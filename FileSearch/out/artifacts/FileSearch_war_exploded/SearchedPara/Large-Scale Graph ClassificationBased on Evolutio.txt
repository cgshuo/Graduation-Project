 amounts of data modeled as graphs. There is a great need for building an automated For instance, chemical compounds can be stored as graphs, and chemists are interested in predicting which chemical compounds are active and which compounds are inactive.
The graph classification framework bein g widely used is first to select a set of sub-graph features from graph databases, and the n to build a generic classification model using the set of subgraph features selected. Discriminative subgraphs that are frequent in one class labeled graph set but infrequent in the other class labeled graph sets are more suitable for classification requirement . Therefore, the key problem of graph clas-sification is how to mine and select the discriminative subgraph features efficiently.
Several main memory-based approaches [21,16,12,13,11] have been proposed to mine discriminative subgraphs in small-scale graph databases, but they are both time and memory costly when applied to process large-scale graph databases. Even worse, a single machine may not be able to handle such a process due to its limited memory. Cloud computing and the widespread MapReduce framework can be used to solve the scalability and computationally-intensive pr oblems in discriminative subgraph pattern mining.
 Many data analysis techniques require iterative computations in MapReduce. HaLoop [4], a modified version of the Hadoop framework can be used for iterative proach to obtain global solutions which can be used for discriminative subgraph mining in large-scale graph databases.

In this paper, we employ the HaLoop MapReduce framework and evolutionary com-putation techniques to find discriminative subgraphs efficiently and propose a large-scale graph classification algorithm with MapReduce, named MRGAGC 1 . To our best knowledge, this work is the first attempt to design an algorithm for discriminative sub-graph mining based on evolutionary computation with MapReduce. We summarize our contributions as follows:  X  We propose a large-scale discriminative subgraph mining algorithm based on evo- X  We propose three evolutionary strategies to generate a set of locally optimal dis- X  We propose to use subgraph coverage rules as graph classifiers using the mined  X  Extensive experimental results on both real and synthetic datasets show that our
The rest of the paper is organized as follows. Section 2 describes the discriminative subgraph mining problem and introduces the MapReduce framework. In Section 3, we describe the iterative programming model and mechanisms of the evolutionary com-putation. Section 4 explains how we can employ the subgraph coverage rules to build graph classifiers. In Section 5, we evaluate our experiment results. Finally, we briefly review the related work in Section 6 and make a conclusion in Section 7. In this section, we first describe the concept of discriminative subgraph, and then Sec-tion 2.2 provides a brief MapReduce prime r and formalizes the problem statement. 2.1 Discriminative Subgraph An undirected graph can be modeled as G =( V,E,L ) where V is a set of vertices and E is a set of edges connecting the vertices. Both nodes and edges can have labels and L is the labeling function on vertices and e dges. Each graph can be attached with an unique class label.
In this paper, we use the two classes graph database as a discussible example (we emphasize that our method can be used to process multi-classes graph database with a little modification, not only two classes graph database). Fig. 1 shows a two classes graph database D contains 4 positive graphs and 4 negative graphs. The set of positive graphs in graph database D denoted as D + and the set of negative graphs denoted as Definition 1 (Subgraph Isomorphism). Given two graphs G =( V ,E ,L ) and G =( V,E,L ) , G is subgraph isomorphic to G , if there exists an injective function  X  : V  X  V , such that (1) for  X  u, v  X  V and u = v ,  X  ( u ) =  X  ( v ) , (2) for  X  u  X  V ,  X  ( L ( u, v ) = L (  X  ( u ) , X  ( v )) .

If graph G is subgraph isomorphic to graph G , we call that G is a subgraph of G , denoted as G  X  G . Here, G is a supergraph of G or G supports G . In this paper, a subgraph is also called a pattern.
 Example 1 . In Fig. 1, G 2 is a subgraph of G 4 and G 4 is a supergraph of G 2 . Definition 2 (Frequency). Given a graph database D = { G 1 ,G 2 ,  X  X  X  ,G n } and a graph pattern G , D = D +  X  X   X  , the supporting graph set of G is D G = { G i | G  X  G ,G i  X  X } . The support of G in D is |D G | , denoted as sup ( G, D ) , the support of G in denoted as freq ( G, D + ) and freq ( G, D  X  ) , respectively.
 Example 2 . In the two classes graph database D in Fig. 1, the supporting set of sub-graph A-B-C is { G 1 ,G 3 } . That is, the support of subgraph A-B-C is 2 , and its fre-0 , respectively.
 Definition 3 (Discriminative Subgraph). Discriminative subgraph G is a subgraph pattern that occur with disproportionate frequency in one class versus others. The dis-criminate power score d ( G ) can be calculated by a given discrimination function. 2.2 MapReduce MapReduce which is a distributed framework for processing large-scale data contains three phases: map, shuffle, and reduce. In the map and reduce phases, users can gener-ally specify the map function and the reduce function. The three phases as follows:  X  Map . In this phase, each worker node reads data from a distributed file system  X  Shuffle . In this phase, the key-value pairs with the same and similar keys are  X  Reduce . In this phase, each worker node examine the keys and corresponding list
With the MapReduce framework, users can implement a map function and a reduce function to process their applications. Fig.2 shows our MapReduce framework for dis-criminative subgraph mining. First, the master node assign the map and the reduce tasks to each worker. Then, discriminative subgr aphs was searched based on evolutionary computation and the discriminative subgrap h results was aggregated in each worker. At last, each worker outputs the results. We employ the HaLoop MapReduce framework because our method needs iterative computa tions to obtain the disc riminative subgraph results. In order to accommodate requirements o f iterative data analysis applications. HaLoop made several changes to the Hadoop MapReduce framework. These changes as follows:  X  First, HaLoop exposes a new application programming interface to users for itera- X  Second, the master node of HaLoop contains a new loop control module that re- X  Third, HaLoop uses a new task scheduler for iterative applications that leverages  X  Fourth, HaLoop caches and indexes application data on worker nodes.

HaLoop not only extends MapReduce with programming support for iterative ap-plications, it also dramatically improves th eir efficiency by making the task scheduler loop-aware and by adding various caching mech anisms. Fig.3 shows the discriminative subgraph mining framework of HaLoop that contains two map-reduce pairs.
 Problem Statement: Let D = { G 1 ,G 2 ,  X  X  X  ,G n } be a graph database that consists of graph G i ,for 1  X  i  X |D| . Large-scale discriminative subgraph mining problem is to find a set of subgraph patterns with which are more discriminative and can be built for graph classifiers.
 In this section, we present the solution for large-scale discriminative subgraph mining. As shown in Fig.2, we next describe the iterative MapReduce programming model. 3.1 Evolutionary Computation Iterative Model The iterative programs that HaLoop support can be distilled in the following core con-struct in Eq. (1).

Where R 0 is an initial result data and L ia an invariant relation. A program in this form terminates when a fixpoint is reached when the result does not change from one iteration to the next, i.e. R i +1 = R i ,andthe fixpoint is typically defined by exact equality between iterations. But HaLoop also supports an approximate fixpoint ,where the computation terminates when either the difference between two consecutive itera-tions is less than a user-specified threshold, or the maximum number of iterations has been reached.

HaLoop extends MapReduce and is based on two simple intuitions. The first one and then reuse them in later iterations. Second, a MapReduce worker node can cache reducer outputs, which makes checking for a fixpoint more efficient, without an extra MapReduce job.

In order to use HaLoop program for discriminative subgraph pattern mining, we specify the loop body (as two map-reduce pairs) and rewrite the map function, reduce function and the termination condition.
 Map Function: In the map step, each worker node M i ( i =1 ,  X  X  X  ,m ) reads a subset D M i . Then, for each discriminative subgraph G the key is G and the value indicates the corresponding discrimination score d i ( G ) . Reduce Function: With the key and value list obtained from the shuffle phase, M i inspects the list of values with key G and computes the sum discrimination scores. Then, M i outputs a key-value pair with key G and value equal to the sum of discrimination scores.
 Stop Condition: With the key-value pairs outputs from the reduce phase, we can obtain than the  X  threshold, the iteration continues. we also assign a maximum number of iterations  X  to terminate the discriminative subgraph mining program. 3.2 Subgraph Pattern Evolutionary Computation Strategy In the map step, randomly divide graph database D into m disjoint subsets D i ( i = 1 ,  X  X  X  ,m ) and sent D discriminative subgraphs in each worker node M i ( i =1 ,  X  X  X  ,m ), each positive graph in
D We achieve this goal in each worker node by exploring candidate subgraph patterns in a process resembling biological evolution (evolutionary commutation) which consist of several evolutionary mechanisms such as reproduction, selection and competition. Evolutionary computation begins with a set of sample points in the search space and gradually biases to the regions of high quality fitness [6,18].

In the problem of discriminative subgraph mining with MapReduce, we need choose a fitness function to measure the potential discriminative power of a subgraph pattern. Larger subgraph pattern with stronger discriminative power can be generated by sub-tion in Eq. (2) for MapReduce.
The discrimination score in Eq. (2) is used to measure the fitness of subgraph patterns pattern is.

In the MapReduce framework, for the positive graph set D + i that stored in M i ,we With a given discrimination score function, we can rank all subgraph patterns by their scores. We first enumerate all subgraph patterns with 2 nodes which the discrimination scores are equal or greater than 0 as the representative subgraph candidates (a set of sample points in the search space in evolutionary computation). ( 1 ) Subgraph Pattern Reproduction Strategy: All the representative subgraph can-didates should have a probability of being s elected for subgraph pattern reproduction to generate a larger subgraph. During each HaLoop iteration, we give a reproduction threshold  X  ( 0 &lt; X   X  1 ) to randomly select a subset of the representative subgraph candidates for reproduction. When a subgraph pattern g was selected to reproduce a new larger subgraph pattern by extending one edge, it also have probabilities to gen-pattern g  X  g can be calculated by Eq. (3). We only consider the larger subgraph patterns g that have the nonnegative discriminative power.
The probability is always between 0 and 1 . This reproduction strategy is commonly used in evolutionary algorithms [6]. The int uition here is that candidate subgraph pat-terns with higher discriminative scores are more likely be extended to larger subgraph patterns with high scores. If a subgraph pattern g reproduce a new larger subgraph pat-survival in the representative subgraph candidates. Among the representative subgraph candidates, there are too many competitions for survival.
 Example 3 . Subgraph graph pattern g in the graph database D in Fig. 1 have four d ( g 3 )=2 and d ( g 4 )=1 . The probability of each larger subgraph pattern reproduced from graph pattern g is shown in Fig. 4.

Extensions of different subgraph patterns can produce the same subgraph pattern be-cause a subgraph pattern can be directly extended from all of its supergraphs. There, we should determine whether a subgraph pattern has already been generated to avoid repet-itive examination of the same subgraph pattern. We use the maximal CAM code [10] for avoiding the same subgraph pattern generation. ( 2 ) Subgraph Pattern Selection Strategy: With the new representative subgraph can-didates generated from subgraph pattern reproduction step in M i , the goal of subgraph pattern selection is to find a subset of discriminative subgraph patterns among which cation. We fist introduce the subgraph-cover definition.
 Definition 4 (Subgraph-Cover). Given a graph database D = { G 1 ,G 2 ,  X  X  X  ,G n } and a subgraph pattern g , a graph G i  X  X  is covered by subgraph pattern g if there is a subgraph isomorphic from g to G i . We denote the set of graphs in D covered by subgraph pattern G as c ( g, D ) .
 tive subgraph candidates, we should select a subset of representative subgraphs R i = { g simultaneously satisfy Eq. (4) and Eq. (5). So, the values k can not be the same for different workers.
We use a heuristic algorithm to select R i from the set of representative subgraph candidates. First, the representative subgraph candidates should be sorted in descend-ing order of their discrimination scores, and we choose some highest scores subgraph patterns which satisfy Eq. (4) as the select result R i . Apparently, when we choose the top-k highest scores subgraph patterns, Eq. (5) is satisfied. ( 3 ) Subgraph Pattern Competition Strategy: In most cases, the representative sub-graph candidate set become bigger and bi gger with the subgraph pattern reproduction and the input subgraph candidates from the previous reduce X  X  output, sooner or later the number of patterns will exceed worker node X  X  capacity. In order to stabilize the sub-graph candidate set, some competitive rule s are needed to determine which subgraph pattern should survive in the representativ e candidate set or not. During each HaLoop iteration, we give a competition threshold  X  ( 0 &lt; X   X  1 ) to delete the vulnerable subgraph candidates.

First, a subgraph pattern that has already been reproduced should not live in the can-didate set any longer because it has served its role in generate a new stronger subgraph pattern. Second, some subgraph patterns with low discrimination scores that are lower than the competition threshold ratio will be deleted from the candidate set. The compe-tition strategy commendably reflect the biol ogical evolutionary phe nomenon:  X  X urvival of the fittest X .
With the three evolutionary strategies, we could quickly identify a set of locally op-timal discriminative subgraph results R i based on evolutionary computation to cover the set of positive graphs in D i that stored in M i . Then, Map function showed in al-gorithm 1 output the key-value pairs that the key is a subgraph pattern and the value is phase, M i inspects the list of values with the key and computes the sum discrimination scores. At last, M i outputs a key-value pair with key equal to the subgraph pattern and the value equal to the sum of discrimination scores.
 Algorithm 1. Map Function
The stop condition calculate by the key-value pairs outputs from the Reduce phase, erations is less than the user-specified  X  threshold or not. we also assign a maximum number of iterations  X  to terminate the discriminative subgraph mining program.
We introduce an algorithm named MRGAGC for discriminative subgraph mining with MapReduce, and MRGAGC consists of two main steps. The fist step is the map step showed in algorithm 1. Through three evolutionary strategies, the map function outputs the representative subgraphs and their corresponding discrimination scores ef-ficiently. The next reduce step showed in algorithm 2 aggregates the key-values pairs iterative stop condition, meanwhile, outputs the results.
 Algorithm 2. Reduce Function Example 4 . Fig. 5 shows an integrated example about discriminative subgraph mining for MapReduce with two worker nodes and three iterations in graph database shown in Fig. 1. First, the graph database was randomly divided in two graph sets, each worker reads one graph set and execute evolutionary computation to obtain a set of discrimina-tive subgraphs. Then, the two worker nodes aggregate the discriminative subgraphs and sc values (sum of subgraph discriminative scores) between two consecutive iterations meet the stop condition threshold (  X  =0 ) and the result discriminative subgraphs (A-B-C and A-B-D) were obtained.
 3.3 Reducing Communication Overhead Discriminative subgraph mining with MapReduce produces a large number of key-value pairs which must be transferred via network communication. This may incur sig-nificant overhead. But, benefit for the HaL oop MapReduce framework, many invariant data could be cached in the local disk. Cach ing the invariant data will reduce the I/O cost and network communication cost. Mean while, we also use the same compression technique mentioned in [14] to reduce the communication cost. In this section, we will introduce the subgraph coverage rule and then build graph clas-sifiers using the discriminative subgraphs mined by MRGAGC.
 Definition 5 (Subgraph Coverage Rule). Given a subgraph set R , a subgraph cover-age rule is defined as a classification rule in the form of G  X  L ,where G  X  X  and L is the class label. If a graph has the subgraph pattern G , then it is classified as L .
When the graph database only have positive and negative class labels, L is always positive since we only mining subgraph patterns whose positive supports are higher than their negative supports. If a graph G has a subgraph pattern in subgraph coverage rules indicates G is a positive graph, whereas, it is a negative graph. We measure the classification accuracy by normalized accuracy, which is defined as follows: We generate subgraph coverage rules with the discriminative subgraphs mined by MRGAGC, and build the classifiers for large-scale graph classification. For some unla-beled graphs, we use the subgraph coverage rules and Eq. (7) to calculate the normalized accuracy about the classifiers and evaluate the quality of the classifiers. In this section, we first evaluate the efficiency of MRGAGC on the synthetic dataset and next evaluate experimentally MRGAGC against two competitors on both synthetic and real-world graph datasets. 5.1 Experimental Setup PC as the Master node, the others as the worker nodes. Each PC has an Intel Quad Core 2 . 66 GHZ CPU, 4 GB memory and GentOS Linux 5 . 6 . We use HaLoop (a modi-fied Hadoop 0 . 20 . 201 ) to run MRGAGC and the default configuration of Hadoop, i.e., dfs.replication = 3 and fs.block.size = 64 MB .
As competitors, we consider two main memor y-based discriminative subgraph min-ing algorithms LEAP [21] and LTS [11], and we adapt the two competitors namely MRLEAP and MRLTS to run on MapReduce, respectively. For the LEAP and LTS algorithms, each worker performs many times algorithm in the map step until all the corresponding positive graphs could be covered. So, we can obtain a set of locally dis-criminative subgraphs mined by these two algorithms in each worker. And, in the reduce step, the two algorithms aggregate the union of discriminative subgraphs as the results. We perform several types of measurements for MRGAGC, MRLEAP and MRLTS, and run each experiment three times for the aver age result. The parameter values used in the three algorithms are shown in Table 1, with default values in bold. According to the rules of the biological evolution process,  X  =0 . 3 and  X  =0 . 3 are the most reasonable values. 5.2 Data Description ( 1 )Synthetic: We use a synthetic graph data generator 2 to generate 100000 undirected, labeled and connected graphs, and randomly select half of the graphs to be positive graphs. We set the number of unique nodes labels to 20 , the average number of nodes and edges in each graph to 20 and 30 respectively. ( 2 ) NCI: The NCI cancer screen data sets are widel y used for graph classification eval-uation [21,16]. We download ten NCI data sets from the PubChem database 3 . Each data the summary of the ten data sets. Since the data sets are unbalanced (about 5% positive graphs), for each bioassay, we randomly select a negative data set with the same size we obtain ten balanced data sets, which comprise 56000 graphs in total. We combine them to construct a new NCI-A dataset (balanced dataset). Meanwhile, we also use all the graphs from the ten data sets ( 400000 graphs) to construct a new NCI-B dataset (unbalanced dataset) in our experiment. 5.3 Efficiency of MRGAGC In this subsection, we study the efficiency of MRGAGC on the synthetic dataset with respect to three parameters:  X  (reproduction threshold),  X  (competition threshold), and  X  (stop condition threshold).

Fig. 6 shows the running times vs three parameters. From the result, we can dis-cover that when the reproduction threshold  X  increases, the running time of MRGAGC observably increases, but with the competition threshold  X  increases, the time cost de-creases. In addition, we observe that the bigger the stop condition threshold  X  is, the less time will be cost.

The phenomena above demonstrate that the most time susceptible operation is the subgraph pattern reproduction in evolutionary computation, which is the same as the traditional main memory-based subgraph pattern mining algorithms. The more sub-graphs are explored, the more time will be cost.
 5.4 Comparison with Other Methods We compare MRGAGC with two other representative discriminative subgraph mining methods: MRLEAP and MRLTS. We run the three algorithms on both synthetic and real-world datasets and show the efficienc y and effectiveness in Fig. 7 and Table 3.
Fig. 7 shows that MRGAGC clearly outperforms MRLEAP and MRLTS in terms of all the datasets. The three methods all decrease their time cost when the worker nodes increase, and MRLEAP need more worker nodes to complete the mining task and have the worst performance in the three methods.

Fig. 8 demonstrates that the number of representative discriminative subgraph pat-terns mined by MRGAGC is the smallest in terms of all the datasets, which indicate that representative subgraph patterns mined by MRGAGC have the stronger discriminative powers. We randomly select half of each data set to be the corresponding training graph demonstrates the quality of representative di scriminative subgraphs mined by the three methods from the aspect of average normali zed classification accuracy. On average, MRGAGC achieves normalized accuracy of 0 . 031 and 0 . 045 higher than MRLTS and MRLEAP respectively.

Fig. 9 shows the communication cost vs number of workers, with the worker nodes increase, the communication cost always increases slowly for the three methods be-cause MRGAGC is benefit for the HaLoop framework and the other two methods only perform simple map-reduce jobs.
 have been developed, such as LEAP [21], GAIA [13], LTS [11], GraphSig [16], and COM [12]. Due to the computation and I/O i ntensive characteristic of graph pattern ming in large-sale graph database, more an d more efforts are geared towards solving for large-scale parallel data analysis and Hadoop is an open-source implementation of MapReduce. Several frequent subgraph mining approaches [14,8] have been proposed in MapReduce. The work in [14] proposes a two-step filter-and-refinement approach to mine the frequent subgraphs which is more s uitable to massive parallelization. The work in [8] takes an iterative approach for frequent subgraph mining and it is the first one to employ MapReduce for mining a large collection of graphs. However, many data analysis techniques require iterative computations in MapReduce, such as PageR-ank [15], recursive relational queries [1] and so on. HaLoop [4], a modified version of the Hadoop MapReduce framework that is desi gned to serve the iterative applications can be used for iterative data analysis applications. Evolutionary computation [6,18] is mining. In this paper, we integrate MapReduce with evolutionary computation to mine discriminative subgraph pattern mining efficiently. In this paper, we first propose the problem for large-scale graph classification base on evolutionary computation with MapReduce. We propose an efficient iterative MapRe-duce method base on HaLoop framework to mine discriminative subgraphs efficiently and explore candidate subgraph pattern space in a biological evolution way. Through evolutionary computation and three effective evolutionary strategies, the method could tive HaLoop framework could reduce signifi cant communication cost. Experiments on both real and synthetic datasets show that our method obviously outperforms the other approaches in terms of classificatio n accuracy and runtime efficiency.
 Acknowledgment. This work was supported by National Natural Science Foun-dation of China ( 61272182 , 61100028 , 61173030 and 61173029 ), 973 program ( 2011 CB 302200 -G ), State Key Program of National Natural Science of China ( 61332014 , U 1401256 ), 863 program ( 2012 AA 011004 ), New Century Excellent Tal-N 130504001 ).
