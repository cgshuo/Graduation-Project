 Automatically describing images in natural lan-guage is an intriguing, but complex AI task, re-quiring accurate computational visual recogni-tion, comprehensive world knowledge, and natu-ral language generation. Some past research has simplified the general image description goal by assuming that relevant text for an image is pro-vided (e.g., Aker and Gaizauskas (2010), Feng and Lapata (2010)). This allows descriptions to be generated using effective summarization tech-niques with relatively surface level image under-standing. However, such text (e.g., news articles or encyclopedic text) is often only loosely related to an image X  X  specific content and many natu-ral images do not come with associated text for summarization.

In contrast, other recent work has focused more on the visual recognition aspect by de-tecting content elements (e.g., scenes, objects, attributes, actions, etc) and then composing de-scriptions from scratch (e.g., Yao et al. (2010), Kulkarni et al. (2011), Yang et al. (2011), Li et al. (2011)), or by retrieving existing whole descriptions from visually similar images (e.g., Farhadi et al. (2010), Ordonez et al. (2011)). For the latter approaches, it is unrealistic to expect that there will always exist a single complete de-scription for retrieval that is pertinent to a given query image. For the former approaches, visual recognition first generates an intermediate rep-resentation of image content using a set of En-glish words, then language generation constructs a full description by adding function words and optionally applying simple re-ordering. Because the generation process sticks relatively closely to the recognized content, the resulting descrip-tions often lack the kind of coverage, creativ-ity, and complexity typically found in human-written text.

In this paper, we propose a holistic data-driven approach that combines and extends the best aspects of these previous approaches  X  a) using visual recognition to directly predict indi-vidual image content elements, and b) using re-trieval from existing human-composed descrip-tions to generate natural, creative, and inter-esting captions. We also lift the restriction of retrieving existing whole descriptions by gather-ing visually relevant phrases which we combine to produce novel and query-image specific de-scriptions. By judiciously exploiting the corre-spondence between image content elements and phrases, it is possible to generate natural lan-guage descriptions that are substantially richer in content and more linguistically interesting than previous work.

At a high level, our approach can be moti-vated by linguistic theories about the connection between reading activities and writing skills, i.e., substantial reading enriches writing skills, (e.g., Hafiz and Tudor (1989), Tsang (1996)). Analogously, our generation algorithm attains a higher level of linguistic sophistication by read-ing large amounts of descriptive text available online. Our approach is also motivated by lan-guage grounding by visual worlds (e.g., Roy (2002), Dindo and Zambuto (2010), Monner and Reggia (2011)), as in our approach the mean-ing of a phrase in a description is implicitly grounded by the relevant content of the image.
Another important thrust of this work is col-lective image-level content-planning , integrating saliency, content relations, and discourse struc-ture based on statistics drawn from a large image-text parallel corpus. This contrasts with previous approaches that generate multiple sen-tences without considering discourse flow or re-dundancy (e.g., Li et al. (2011)). For example, for an image showing a flock of birds, generating a large number of sentences stating the relative position of each bird is probably not useful.
Content planning and phrase synthesis can be naturally viewed as constraint optimization problems. We employ Integer Linear Program-ming (ILP) as an optimization framework that has been used successfully in other generation tasks (e.g., Clarke and Lapata (2006), Mar-tins and Smith (2009), Woodsend and Lapata (2010)). Our ILP formulation encodes a rich set of linguistically motivated constraints and weights that incorporate multiple aspects of the generation process. Empirical results demon-strate that our final system generates linguisti-cally more appealing and semantically more cor-rect descriptions than two nontrivial baselines. 1.1 System Overview Our system consists of two parts. For a query image, we first retrieve candidate descriptive phrases from a large image-caption database us-ing measures of visual similarity (  X  2). We then generate a coherent description from these can-didates using ILP formulations for content plan-ning (  X  4) and surface realization (  X  5). For a query image, we retrieve relevant candi-date natural language phrases by visually com-paring the query image to database images from the SBU Captioned Photo Collection (Ordonez et al., 2011) (1 million photographs with asso-ciated human-composed descriptions). Visual similarity for several kinds of image content are used to compare the query image to images from the database, including: 1) object detections for 89 common object categories (Felzenszwalb et al., 2010), 2) scene classifications for 26 com-mon scene categories (Xiao et al., 2010), and 3) region based detections for stuff categories (e.g. grass, road, sky) (Ordonez et al., 2011). All content types are pre-computed on the mil-lion database photos, and caption parsing is per-formed using the Berkeley PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007).

Given a query image, we identify content el-ements present using the above classifiers and detectors and then retrieve phrases referring to those content elements from the database. For example, if we detect a horse in a query im-age, then we retrieve phrases referring to vi-sually similar horses in the database by com-paring the color, texture (Leung and Malik, 1999), or shape (Dalal and Triggs, 2005; Lowe, 2004) of the detected horse to detected horses in the database images. We collect four types of phrases for each query image as follows: [1] NPs We retrieve noun phrases for each query object detection (e.g.,  X  X he brown cow X ) from database captions using visual similar-ity between object detections computed as an equally weighted linear combination of L 2 dis-tances on histograms of color , texton (Leung and Malik, 1999), HoG (Dalal and Triggs, 2005) and SIFT (Lowe, 2004) features. [2] VPs We retrieve verb phrases for each query object detection (e.g.  X  X oy running X ) from database captions using the same mea-sure of visual similarity as for NPs, but restrict-ing the search to only those database instances whose captions contain a verb phrase referring to the object category. [3] Region/Stuff PPs We collect preposi-tional phrases for each query stuff detection (e.g.  X  X n the sky X ,  X  X n the road X ) by measuring visual similarity of appearance (color, texton, HoG) and geometric configuration (object-stuff rela-tive location and distance) between query and database detections. [4] Scene PPs We also collect prepositonal phrases referring to general image scene context (e.g.  X  X t the market X ,  X  X n hot summer days X ,  X  X n Sweden X ) based on global scene similarity computed using L 2 distance between scene clas-sification score vectors (Xiao et al., 2010) com-puted on the query and database images. For each image, we aim to generate multiple sentences, each sentence corresponding to a sin-gle distinct object detected in the given image. Each sentence comprises of the NP for the main object, and a subset of the corresponding VP, region/stuff PP, and scene PP retrieved in  X  2. We consider four different types of operations to generate the final description for each image: T1. Selecting the set of objects to describe (one T2. Re-ordering sentences (i.e., re-ordering ob-T3. Selecting the set of phrases for each sen-T4. Re-ordering phrases within each sentence. The ILP formulation of  X  4 addresses T1 &amp; T2, i.e., content-planning, and the ILP of  X  5 ad-dresses T3 &amp; T4, i.e., surface realization. 1 First we describe image-level content planning, i.e., abstract generation . The goals are to (1) se-lect a subset of the objects based on saliency and semantically compatibility, and (2) order the se-lected objects based on their content relations. 4.1 Variables and Objective Function The following set of indicator variables encodes the selection of objects and ordering: where k = 1 ,...,S encodes the position (order) of the selected objects, and s indexes one of the objects. In addition, we define a set of variables indicating specific pairs of adjacent objects: The objective function, F , that we will maxi-mize is a weighted linear combination of these indicator variables and can be optimized using integer linear programming: where F s quantifies the salience/confidence of the object s , and F st quantifies the seman-tic compatibility between the objects s and t . These coefficients (weights) will be described in  X  4.3 and  X  4.4. We use IBM CPLEX to optimize this objective function subject to the constraints introduced next in  X  4.2. 4.2 Constraints Consistency Constraints: We enforce consis-tency between indicator variables for indivisual objects (Eq. 1) and consecutive objects (Eq. 2) To avoid empty descriptions, we enforce that the result includes at least one object: To enforce contiguous positions be selected: Discourse constraints: To avoid spurious de-scriptions, we allow at most two objects of the same type, where c s is the type of object s : 4.3 Weight F s : Object Detection In order to quantify the confidence of the object detector for the object s , we define 0  X  F s  X  1 as the mean of the detector scores for that object type in the image. 4.4 Weight F st : Ordering and The weight 0  X  F st  X  1 quantifies the compat-ibility of the object pairing ( s , t ). Note that in the objective function, we subtract this quan-tity from the function to be maximized. This way, we create a competing tension between the single object selection scores and the pairwise compatibility scores, so that variable number of objects can be selected.
 Object Ordering Statistics: People have bi-ases on the order of topic or content flow. We measure these biases by collecting statistics on ordering of object names from the 1 million im-age descriptions in the SBU Captioned Dataset (Ordonez et al., 2011). Let f ord ( w 1 ,w 2 ) be the number of times w 1 appeared before w 2 . For instance, f ord ( window,house ) = 2895 and f ord ( house,window ) = 1250, suggesting that people are more likely to mention a window be-fore mentioning a house/building 2 . We use these ordering statistics to enhance content flow. We define score for the order of objects using Z-score for normalization as follows: We then transform  X  F st so that  X  F st  X  [0,1], and then set F st = 1  X   X  F st so that smaller values correspond to better choices. Recall that for each image, the computer vi-sion system identifies phrases from descriptions of images that are similar in a variety of aspects. The result is a set of phrases representing four different types of information (  X  2). From this assortment of phrases, we aim to select a subset and glue them together to compose a complete sentence that is linguistically plausible and se-mantically truthful to the content of the image. 5.1 Variables and Objective Function The following set of variables encodes the selec-tion of phrases and their ordering in construct-ing S 0 sentences. where k = 1 ,...,N encodes the ordering of the selected phrases, and j indexes one of the four phrases types (object-NPs, action-VPs, region-PPs, scene-PPs), i = 1 ,...,M indexes one of the M candidate phrases of each phrase type, and s = 1 ,...,S 0 encodes the sentence (object). In addition, we define indicator variables for adjacent pairs of phrases: x sijkpq ( k +1) = 1 if x we define the objective function F as: where F sij weights individual phrase goodness and F sijpq adjacent phrase goodness. All coeffi-cients (weights) will be described in Section 5.3 and 5.4.

We optionally prepend the first sentence in a generated description with a cognitive phrase . 3 These are generic constructs that are often used to start a description about an image, for in-stance,  X  X his is an image of... X . We treat these phrases as an additional type, but omit corre-sponding variables and constraints for brevity. 5.2 Constraints Consistency Constraints: First we enforce consistency between the unary variables (Eq. 11) and the pairwise variables so that x sijkpqm = 1 iff x sijk = 1 and x spqm = 1: Next we include constraints similar to Eq. 8 (contiguous slots are filled), but omit them for brevity. Finally, we add constraints to ensure at least two phrases are selected for each sentence, to promote informative descriptions.
 Linguistic constraints: We include linguisti-cally motivated constraints to generate syntacti-cally and semantically plausible sentences. First we enforce a noun-phrase to be selected to en-sure semantic relevance to the image: Also, to avoid content redundancy, we allow at most one phrase of each type: Discourse constraints: We allow at most one prepositional scene phrase for the whole de-scription to avoid redundancy: We add constraints that prevent the inclusion of more than one phrase with identical head words:  X  s,ij,pq with the same heads, 5.3 Unary Phrase Selection Let M sij be the confidence score for phrase x sij given by the image X  X hrase matching al-gorithm (  X  2). To make the scores across dif-ferent phrase types comparable, we normalize them using Z-score: F sij = norm 0 ( M sij ) = ( M sij  X  mean j ) /dev j , and then transform the values into the range of [0,1]. 5.4 Pairwise Phrase Cohesion In this section, we describe the pairwise phrase cohesion score F sijpq defined for each x sijpq in the objective function (Eq. 12). Via F sijpq , we aim to quantify the degree of syntactic and semantic cohesion across two phrases x sij and x spq . Note that we subtract this cohesion score from the objective function. This trick helps the ILP solver to generate sentences with varying number of phrases, rather than always selecting the maximum number of phrases allowed.
 N-gram Cohesion Score: We use n-gram statistics from the Google Web 1-T dataset (Brants and Franz., 2006) Let L sijpq be the set of all n-grams (2  X  n  X  5) across x sij and x spq . Then the n -gram cohesion score is computed as:
Where NPMI is the normalized point-wise mu-tual information. 4 Co-occurrence Cohesion Score: To cap-ture long-distance cohesion, we introduce a co-occurrence-based score, which measures order-preserved co-occurrence statistics between the head words h sij and h spq 5 . Let f  X  ( h sij ,h spq ) be the sum frequency of all n-grams that start with h sij , end with h spq and contain a prepo-sition prep ( spq ) of the phrase spq . Then the co-occurrence cohesion is computed as: Final Cohesion Score: Finally, the pairwise phrase cohesion score F ijpq is a weighted sum of n-gram and co-occurrence cohesion scores: where  X  and  X  can be tuned via grid search, for comparability. Notice that F sijpq is in the range [0,1] as well. TestSet: Because computer vision is a challeng-ing and unsolved problem, we restrict our query set to images where we have high confidence that visual recognition algorithms perform well. We collect 1000 test images by running a large num-ber (89) of object detectors on 20,000 images and selecting images that receive confident ob-ject detection scores, with some preference for images with multiple object detections to obtain good examples for testing discourse constraints. Baselines: We compare our ILP approaches with two nontrivial baselines: the first is an HMM approach (comparable to Yang et al. (2011)), which takes as input the same set of candidate phrases described in  X  2, but for de-coding, we fix the ordering of phrases as [ NP  X  VP  X  Region PP  X  Scene PP] and find the best combination of phrases using the Viterbi algorithm. We use the same rich set of pairwise phrase cohesion scores (  X  5.4) used for the ILP formulation, producing a strong baseline 6 .
The second baseline is a recent Retrieval based description method (Ordonez et al., 2011), that searches the large parallel corpus of im-ages and captions, and transfers a caption from a visually similar database image to the query. This again is a very strong baseline, as it ex-ploits the vast amount of image-caption data, and produces a description high in linguistic quality (since the captions were written by hu-man annotators).
 Automatic Evaluation: Automatically quan-tifying the quality of machine generated sen-tences is known to be difficult. BLEU score (Papineni et al., 2002), despite its simplicity and limitations, has been one of the common choices for automatic evaluation of image de-scriptions (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Ordonez et al., 2011), as it correlates reasonably well with human evalu-ation (Belz and Reiter, 2006).

Table 1 shows the the BLEU @1 against the original caption of 1000 images. We see that the ILP improves the score over HMM consistently, with or without the use of cognitive phrases. Human Evaluation I  X  Ranking: We com-plement the automatic evaluation with Mechan-ical Turk evaluation. In ranking evaluation, we ask raters to choose a better caption between two choices 7 . We do this rating with and with-out showing the images, as summarized in Ta-ble 2 &amp; 3. When images are shown, raters evalu-ate content relevance as well as linguistic quality of the captions. Without images, raters evaluate only linguistic quality.

We found that raters generally prefer ILP gen-erated captions over HMM generated ones, twice as much (67.2% ILP V.S. 32.8% HMM), if im-ages are not presented. However the difference is less pronounced when images are shown. There could be two possible reasons. The first is that when images are shown, the Turkers do not try as hard to tell apart the subtle difference be-tween the two imperfect captions. The second is that the relative content relevance of ILP gen-erated captions is negating the superiority in lin-guistic quality. We explore this question using multi-aspect rating, described below.

Note that ILP generated captions are exceed-ingly (71.8 %) preferred over the Retrieval baseline (Ordonez et al., 2011), despite the gen-erated captions tendency to be more prone to grammatical and cognitive errors than retrieved ones. This indicates that the generated captions must have substantially better content relevance to the query image, supporting the direction of this research. Finally, notice that as much as 16% of the time, ILP generated captions are pre-ferred over the original human generated ones (examples in Figure 2).
 Human Evaluation II  X  Multi-Aspect Rat-ing: Table 4 presents rating in the 1 X 5 scale (5: perfect, 4: almost perfect, 3: 70  X  80% good, 2: 50  X  70% good, 1: totally bad) in three different aspects: grammar , cognitive correctness , 8 and relevance . We find that ILP improves over HMM in all aspects, however, the relevance score is no-ticeably worse than scores of two other criteria. It turns out human raters are generally more critical against the relevance aspect, as can be seen in the ratings given to the original human generated captions.
 Discussion with Examples: Figure 1 shows contrastive examples of HMM vs ILP gener-ated captions. Notice that HMM captions look robotic , containing spurious and redundant phrases due to lack of discourse constraints, and often discussing an awkward set of objects due to lack of image-level content planning. Also notice how image-level content planning under-pinned by language statistics helps correct some of the erroneous vision detections. Figure 3 shows some example mistakes in the ILP gen-erated captions. Although not directly focused on image descrip-tion generation, some previous work in the realm of summarization shares the similar problem of content planning and surface realization. There are subtle, but important differences however. First, sentence compression is hardly the goal of image description generation, as human writ-ten descriptions are not necessarily succinct. 9 Second, unlike summarization, we are not given with a set of coherent text snippet to begin with, and the level of noise coming from the visual recognition errors is much higher than that of starting with clean text. As a result, choosing an additional phrase in the image description is much riskier than it is in summarization.

Some recent research proposed very elegant approaches to summarization using ILP for col-lective content planning and/or surface realiza-tion (e.g., Martins and Smith (2009), Woodsend and Lapata (2010), Woodsend et al. (2010)). Perhaps the most important difference in our approach is the use of negative weights in the objective function to create the necessary ten-sion between selection (salience) and compatibil-ity, which makes it possible for ILP to generate variable length descriptions, effectively correct-ing some of the erroneous vision detections. In contrast, all previous work operates with a pre-defined upper limit in length, hence the ILP was formulated to include as many textual units as possible modulo constraints.

To conclude, we have presented a collective approach to generating natural image descrip-tions. Our approach is the first to systematically incorporate state of the art computer vision to retrieve visually relevant candidate phrases, then produce images descriptions that are sub-stantially more complex and human-like than previous attempts.
 Acknowledgments T. L. Berg is supported in part by NSF CAREER award #1054133; A.
 C. Berg and Y. Choi are partially supported by the Stony Brook University Office of the Vice President for Research. We thank K. Yam-aguchi, X. Han, M. Mitchell, H. Daume III, A. Goyal, K. Stratos, A. Mensch, J. Dodge for data pre-processing and useful initial discussions.
