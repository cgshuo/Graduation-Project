 Zhenguo Li zgli5@ie.cuhk.edu.hk Jianzhuang Liu jzliu@ie.cuhk.edu.hk Xiaoou Tang xtang@ie.cuhk.edu.hk Learning from both labeled and unlabeled data, known as semi-supervised learning, has attracted considerable interest in recent years (Chapelle et al., 2006), (Zhu, 2005). The key to the success of semi-supervised learn-ing is the cluster assumption (Zhou et al., 2004), stat-ing that nearby objects and objects on the same mani-fold structure are likely to be in the same class. Differ-ent algorithms actually implement the cluster assump-tion from different viewpoints (Zhu et al., ), (Zhou et al., 2004), (Belkin et al., 2006), (Chapelle &amp; Zien, 2005), (Zhang &amp; Ando, 2006), (Szummer et al., 2002). When the cluster assumption is appropriate, we can properly classify the whole data set with only one la-beled object for each class.
 However, the distributions of real-world data are of-ten more complex than expected, where there are cir-cumstances that a class may consist of multiple sep-arate groups or manifolds, and different classes may be close to or even overlap with each other. For ex-ample, a common experience is that face images of the same person under different poses and illumina-tions can be drastically different, while those with sim-ilar appearances may originate from two different per-sons. To handle the classification problems of such practical data, additional assumptions should be made and more supervisory information should be exploited when available.
 Class labels of data are the most widely used super-visory information. In addition, pairwise constraints are also often seen, which specify whether two objects belong to the same class or not, known as the must-link constraints and the cannot-link constraints. Such pairwise constraints may arise from domain knowledge automatically or with a little human effort (Wagstaff &amp; Cardie, 2000), (Klein et al., 2002), (Kulis et al., 2005), (Chapelle et al., 2006). They can also be ob-tained from data labels where objects with the same label are must-link while objects with different labels are cannot-link. Generally, we cannot infer data labels from only pairwise constraints, especially for multi-class data. In this sense, pairwise constraints are in-herently weaker and thus more general than labels of data. Pairwise constrains have been widely used in the contexts of clustering with side information (Wagstaff et al., 2001), (Klein et al., 2002), (Xing et al., 2003), (Kulis et al., 2005), (Kamvar et al., 2003), (Glober-son &amp; Roweis, 2006), (Basu et al., 2004), (Bilenko et al., 2004), (Bar-Hillel et al., 2003), (Hoi et al., 2007), where it has been shown that the presence of appro-priate pairwise constraints can often improve the clus-tering performance.
 In this paper, we consider a more general problem of semi-supervised classification from pairwise con-straints and unlabeled data, which includes the tra-ditional semi-supervised classification as a subprob-lem that considers labeled and unlabeled data. Note that the label propagation techniques, which are often used in the traditional semi-supervised classification (Zhou et al., 2004), (Zhu et al., ), (Belkin et al., 2006), cannot be readily generalized to propagate pairwise constraints. Recently, two methods (Goldberg et al., 2007), (Tong &amp; Jin, 2007) are proposed to incorporate dissimilarity information in semi-supervised classifica-tion, which is similar to the cannot-link constraints. It is important to notice that the similarities between objects are not identical to the must-link constraints imposed on them. The former reflects their distances in the input space while the latter is often obtained using domain knowledge or specified by the user. We propose an approach, called pairwise constraint propagation (PCP), that can effectively propagate pairwise constraints to the whole data set. PCP in-tends to learn a mapping that is smooth over the data graph and maps the data onto a unit hypersphere, where two must-link objects are mapped to the same point while two cannot-link objects are mapped to be orthogonal. Such a mapping can be implicitly achieved using the kernel trick via semidefinite programming, which is convex and can be solved globally. Our ap-proach can be directly applied to multi-class classifica-tion and can handle data labels, pairwise constraints, or a mixture of them in a unified framework. Given a data set of n objects X = { x 1 , x 2 , ..., x n } and two sets of pairwise must-link and cannot-link constraints, denoted respectively by M = { ( x i , x j ) } where x i and x j should be in the same class and C = { ( x i , x j ) } where x i and x j should be in differ-ent classes, our goal is to classify X into k classes such that not only the constraints are satisfied, but also those unlabeled objects similar to two must-link ob-jects respectively are classified into the same class and those similar to two cannot-link objects respectively are classified into different classes.
 To better illustrate our purpose, let us consider the classification task on a toy data set shown in Fig. 1(a). Although this data set consists of three separate groups (denoted by different colors and symbols in Fig. 1(a)), it has only two classes (Fig. 1(b)). We argue that the must-link constraint asks merging the outer circle and the inner circle into one class, instead of just merging the two must-link objects; and the cannot-link constraint asks for keeping the middle circle and the outer circles into different classes, not just keeping the two cannot-link objects into different classes. Con-sequently, the desired classification result is the one shown in Fig. 1(b). It is such a global implication that we interpret the pairwise constraints.
 From this simple example, we can see that the cluster assumption is still valid, i.e., nearby objects tend to be in the same class and objects on the same manifold structure also tend to be in the same class. However, this assumption does not concern those objects that are not close to each other and do not share the same manifold structure. We argue that the classification for such objects should accord with the input pairwise constraints. For example, any two objects on the outer and inner circles in Fig. 1(a) should be in the same class because they respectively share the same man-ifold structures with the two must-link objects, and any two objects on the outer and middle circles should be in different classes because they respectively share the same manifold structures with the two cannot-link objects. We refer to this assumption as the pairwise constraint assumption .
 In this paper, we seek to implement both the cluster assumption and the pairwise constraint assumption in a unified framework. A dilemma is that one may spec-ify nearby objects or objects sharing the same manifold structure to be cannot-link. In this case, we choose to respect the pairwise constraint assumption first and then the cluster assumption, considering that the prior pairwise constraints are from reliable knowledge. This is true in most practical applications. 3.1. A General Framework As mentioned in the last section, our goal is to propa-gate the given pairwise constraints to the whole data set in a global implication for classification. Intu-itively, it is hard to implement our idea in the in-put space. Therefore, we seek a mapping (usually non-linear) to map the objects to a new and possi-bly higher-dimensional space such that the objects are reshaped in this way: two must-link objects become close while two cannot-link objects become far apart; objects respectively similar to two must-link objects also become close while objects respectively similar to two cannot-link objects become far apart.
 Let  X  be a mapping from X to some space F , The above analysis motivates us to consider the fol-lowing optimization framework: where S (  X  ) is a smoothness measure for  X  such that the smaller is S (  X  ), the smoother is  X  ;  X  is a small positive number;  X  is a large positive number; k X k F is a distance metric in F ; M is the set of the must-link constraints; C is the set of cannot-link constraints. The inequality constraints (3) and (4) require  X  to map two must-link objects to be close and two cannot-link objects far apart. By enforcing the smoothness on  X  (minimizing the objective (2)), we actually require  X  to map any two objects respectively similar to two must-link objects to be close and map any two ob-jects respectively similar to two cannot-link objects far apart. Hopefully, after the mapping, each class be-comes relatively compact and different classes become far apart. Once such a mapping is derived, the classi-fication task can be done much easier.
 This optimization framework is quite general and the details have to be developed. We propose a unit hy-persphere model to substantialize it in Section 3.2, and then solve the resulting optimization problem in Sec-tion 3.3. 3.2. The Unit Hypersphere Model Recall that our goal is to find a smooth mapping that maps two must-link objects close and two cannot-link objects far apart. To this end, we consider it better to put the images of all the objects under a uniform scale. The unit hypersphere in F is a good choice be-cause there is a natural way to impose the pairwise constraints on it. Our key idea is to map all the ob-jects onto the unit hypersphere in F , where two must-link objects are mapped to the same point and two cannot-link objects to be orthogonal. Mathematically, we require  X  to satisfy where &lt;  X  ,  X  &gt; F denotes the dot product in F . Next we impose smoothness on  X  using the spectral graph theory where the graph Laplacian plays an es-sential role (Chung, 1997). Let G = ( V, W ) be an undirected, weighted graph with the node set V = X and the weight matrix W = [ w ij ] n  X  n , where w ij is the weight on the edge connecting nodes x i and x j , denot-ing how similar they are. W is commonly assumed to be symmetric and non-negative. The graph Laplacian L of G is defined as L = D  X  W , where D = [ d ij ] n  X  n a diagonal matrix with d ii = P j w ij . The normalized graph Laplacian  X  L of G is defined as where I is the identity matrix. W is also called the affinity matrix, and  X  W = D  X  1 / 2 W D  X  1 / 2 the nor-malized affinity matrix.  X  L is symmetric and posi-tive semidefinite, with eigenvalues in the interval [0 , 2] (Chung, 1997).
 Following the idea of regularization in spectral graph theory (e.g., see (Zhou et al., 2004)), we define the smoothness measure S (  X  ) by where  X  ( x i )  X  X  , and k X k F is a distance metric in F . Note that F is possibly an infinite-dimensional space. By this definition, we can see that S (  X  )  X  0 since W is non-negative, and the value S (  X  ) penalizes the large change of the mapping  X  between two nodes linked with a large weight. In other words, minimizing S (  X  ) encourages the smoothness of a mapping over the data graph. Next we rewrite S (  X  ) in matrix form. Let k ij = &lt;  X  ( x i ) ,  X  ( x j ) &gt; F . Then the matrix K = [ k ij ] n  X  n is symmetric and positive semidefinite, de-noted by K 0, and thus can be thought as a kernel over X (Smola &amp; Kondor, 2003). From (9), we have
S (  X  ) = where  X  denotes the dot product between two matrices, defined as A  X  B = P n i =1 P m j =1 a ij b ij , for A = [ a 3.3. Learning a Kernel Matrix With the above analysis (5) X (7), and (12), we have arrived at the following optimization problem: which can be recognized as a semidefinite program-ming (SDP) problem (Boyd &amp; Vandenberghe, 2004). This problem is convex and thus the global optimal solution is guaranteed. To solve this problem, we can use the highly optimized software packages, such as SeDuMi (Sturm, 1999) and CSDP (Borchers, 1999). We can also express the above SDP problem in a more familiar matrix form. Let E ij be a n  X  n matrix con-sisting of all 0 X  X  except the ( i, j ) th entry being 1. Then the above SDP problem becomes It should be noted that we have transformed the prob-lem of learning a mapping  X  stated in (2) X (4) into the problem of learning a kernel matrix K such that  X  is the feature mapping induced by K . The kernel trick (Sch  X olkopf &amp; Smola, 2002) indicates that we can implicitly derive  X  by explicitly pursuing K . Note that the kernel matrix K captures the distribution of the point set {  X  ( x i ) } n i =1 in the feature space. The equality constraints (14) constrain  X  ( x i ) X  X  to be on the unit hypersphere, the inequality constraints (15) and (16) force  X  ( x i ) =  X  ( x j ) if x i and x j are must-link and  X  ( x i ) and  X  ( x j ) to be orthogonal if x i and x are cannot-link. By minimizing the objective function (13), which is equivalent to enforcing smoothness on  X  ,  X  ( x i ) and  X  ( x j ) will move close to each other if x and x j are similar (lie on the same group or manifold). This process will continue until a global stable state is achieved (the objective function is minimized and the constraints are satisfied). We call this process the pair-wise constraint propagation . It is expected that after the propagation, each class becomes compact and dif-ferent classes become far apart (being nearly orthog-onal on the unit hypersphere). This phenomenon is also observed by our experiments (see Section 5.2). The idea of reshaping the data in a high-dimensional space by propagating the spatial information among objects is previously appeared in our recent work (Li et al., 2007) where the problem of clustering highly noisy data is addressed. 3.4. Classification Let K  X  be the kernel matrix obtained by solving the SDP problem stated in (18) X (22). The final step of our approach is to obtain k classes from K  X  . We apply the kernel K-means algorithm (Shawe-Taylor &amp; Cristian-ini, 2004) to K  X  to form k classes. Based on the previous analysis, we develop a semi-supervised classification algorithm listed in Algorithm 1, which we called the Pairwise Constraint Propaga-tion (PCP). The scale factor  X  in Step 1 needs to be tuned, which is discussed in Section 5.1.
 Algorithm 1 Pairwise Constraint Propagation Input : A data set of n objects X = { x 1 , x 2 , ..., x n the set of must-link constraints M = { ( x i , x j ) } , the set of cannot-link constraints C = { ( x i , x j ) } , and the number of classes k .
 Output : The class labels of the objects in X . 1. Form the affinity matrix W = [ w ij ] n  X  n with w ij = 2. Form the normalized graph Laplacian  X  L = I  X  3. Obtain the kernel matrix K  X  by solving the SDP 4. Form k classes by applying the kernel K-means to In this section, we evaluate the proposed algorithm PCP on a number of synthetic and real data sets. By comparison, the results of two notable and most re-lated algorithms, Kamvar et al. X  X  spectral learning al-gorithm (SL) (Kamvar et al., 2003) and Kulis et al X  X  semi-supervised kernel K-means algorithm (SSKK) (Kulis et al., 2005), are also presented. Note that most semi-supervised classification algorithms cannot be directly applied to the tasks of classification from pairwise constraints we consider here, because they perform classification from labeled and unlabeled data and cannot be readily generalized to address classifi-cation from pairwise constraints and unlabeled data. In order to evaluate these algorithms, we compare the results with available ground-truth data labels, and employ the Normalized Mutual Information (NMI) as the performance measure (Strehl &amp; Ghosh, 2003). For two random variables X and Y , the NMI is defined as: where I ( X , Y ) is the mutual information between X and Y , and H ( X ) and H ( Y ) are the entropies of X and Y , respectively. Note that 0  X  NMI  X  1, and NMI = 1 when a result is the same as the ground-truth. The larger is the NMI, the better is a result. To evaluate the algorithms under different settings of pairwise constraints, we generate a varying num-ber of pairwise constraints randomly for each data set. For a data set of k classes, we randomly gen-erate j must-link constraints for each class, and j cannot-link constraints for every two classes, giving to-tal j  X  ( k + k ( k  X  1) / 2) constraints for each j , where j ranges from 1 to 10. The averaged NMI is reported for each number of pairwise constraints over 20 different realizations. Since all the three algorithms employ the K-means or kernel K-means in the final step, for each experiment we run the K-means or kernel K-means 20 times with random initializations, and report the av-eraged result. 5.1. Parameter Selection The three algorithms are all graph-based and thus the inputs are assumed to be graphs. We use the weighted graphs for all the algorithms, where the similarity ma-trix W = [ w ij ] is given by The most suitable scale factor  X  is found over the set the set of m linearly equally spaced numbers between r 1 and r 1 , and r denotes the averaged distance from each node to its 10-th nearest neighbor.
 to solve the SDP problem in the proposed PCP. For SSKK, we use its normalized cut version since it per-forms best in the experiments given in (Kulis et al., 2005). The constraint penalty in SSKK is set to n/ ( kc ), as suggested in (Kulis et al., 2005), where n is the number of objects, k is the number of classes, and c is the total number of pairwise constraints. All the algorithms are implemented in MATLAB 7.6, running on a 3.4 GHz, 2GB RAM Pentium IV PC. 5.2. A Toy Example In this subsection, we illustrate the proposed PCP us-ing a toy example. We mainly study its capability of propagating pairwise constraints to the whole data set. Specifically, we want to see how PCP reshapes the data in the feature space according to the original data structure and the given pairwise constraints. The classification task on the Three-Circle data is shown in Fig. 2(a), where the ground-truth classes are de-noted by different colors and symbols, and one must-link (solid red line) and one cannot-link (dashed red line) constraints are also provided. At first glance, Three-Circle is composed of a mixture of curve-like and Gaussian-like groups. A more detailed observa-tion is that there is one class composed of separate groups.
 The distance matrices for Three-Circle in the input space and in the feature space are shown in Figs. 2(b) and (c), where the data are ordered such that all the objects in the outer circle appear first, all the objects in the inner circle appear second, and all the objects in the middle circle appear finally. Note that this ar-rangement does not affect the classification results but only for better illustration of the distance matrices. We can see that the distance matrix in the feature space, compared to the one in the input space, ex-hibits a clear block structure, meaning that each class becomes highly compact (although in the input space one of the two classes consists of two well-separated groups) and the two classes become far apart. Our computations show that the distance between any two points in the feature space in different classes falls in [  X  2  X  1 . 9262  X  10  X  5 , that the two classes are nearly which comes from the requirement that two cannot-link objects are mapped to be orthogonal on the unit hypersphere. 5.3. On Sensory Data Four sensory data sets from the UCI Machine Learn-The data sets are described in Table 1. These four data sets are widely used for evaluation of the classifi-cation and clustering methods in the machine learning community.
 The results are shown in Fig. 3, from which two ob-servations can be drawn. First, PCP performs better than SSKK and SL on all the four data sets under dif-ferent settings of pairwise constraints, especially on the Ionosphere data. Second, as the number of constraints grows, the performances of all the algorithms increase accordingly on Soybean, but vary little on Wine and Ionosphere. On Iris, as the number of constraints grows the performance of PCP improves accordingly but those of SSKK and SL are almost unchanged. 5.4. On Imagery Data In this subsection, we test the algorithms on three im-age databases USPS, MNIST, and CMU PIE (Pose, Il-lumination, and Expression). Both USPS and MNIST consist of images of handwritten digits with signifi-cantly different fashion and of sizes 16  X  16 and 28  X  28. The CMU PIE contains 41,368 images of 68 people, each person with 13 different poses, 43 different illu-mination conditions, and 4 different expressions. From these databases, we draw four data sets, which are de-scribed in Table 2. The USPS0123 and MNIST0123 are drawn respectively from USPS and MNIST, and PIE-10-20 and PIE-22-23 are drawn from CMU PIE. USPS0123 (MNIST0123) consists of digits 0 to 3, with first 100 instances from each class. PIE-10-20 (PIE-22-23) contains five near frontal poses (C05, C07, C09, C27, C29) of two individuals indexed as 10 and 20 (22 and 23) under different illuminations and expres-sions. Original images in PIE-10-20 and PIE-22-23 are manually aligned (two eyes are aligned at the fixed po-sitions), cropped, and then down-sampled to 32  X  32. Each image is represented by a vector of size equal to the product of its width and height.
 The results are shown in Fig. 4, from which we can see that the proposed PCP consistently and significantly outperforms SSKK and SL on all the four data sets under different settings of pairwise constraints. As the number of constraints grows, the performance of PCP improves more significantly than those of SSKK and SL.
 We also look at the computational costs of different algorithms. For example, for each run on USPS0123 (of size 400) with 100 pairwise constraints, PCP takes about 17 seconds while both SSKK and SL take less than 0.5 second. PCP does take more execution time than SSKK and SL since it involves solving for a kernel matrix with SDP, while either SSKK or SL uses pre-defined kernel matrix. The main computational cost in PCP is in solving the SDP problem. A semi-supervised classification approach, Pairwise Constraint Propagation (PCP), for learning from pair-wise constraints and unlabeled data is proposed. PCP seeks a smooth mapping to map the data onto a unit hypersphere, where any two must-link objects are mapped to the same point and any two cannot-link objects are mapped to be orthogonal. Consequently, PCP simultaneously implements the cluster assump-tion and the pairwise constraint assumption stated in Section 2. PCP implicitly derives such a mapping by explicitly finding a kernel matrix via semidefinite pro-gramming. In contrast to label propagation in tra-ditional semi-supervised learning, PCP can effectively propagate pairwise constraints to the whole data set. Experimental results on a variety of synthetic and real data sets have demonstrated the superiority of PCP. Note that PCP falls into semi-supervised learning since it performs learning from both constrained and uncon-strained data. Most previous metric learning methods, however, belong to supervised learning. PCP always keeps every two must-link objects close and every two cannot-link objects far apart. Therefore it essentially addresses hard constrained classification.
 Although extensive experiments have confirmed the ef-fectiveness of the PCP algorithm, there are several is-sues worthy to be further investigated in future work. One issue is to accelerate PCP where solving the asso-ciated SDP problem is the bottleneck. Another issue is to handle noisy constraints effectively.
 We would like to thank the anonymous reviewers for their valuable comments. The work described in this paper was fully supported by a grant from the Re-search Grants Council of the Hong Kong SAR, China (Project No. CUHK 414306).
 Bar-Hillel, A., Hertz, T., Shental, N., &amp; Weinshall, D. (2003). Learning distance functions using equiva-lence relations. ICML (pp. 11 X 18).
 Basu, S., Bilenko, M., &amp; Mooney, R. (2004). A prob-abilistic framework for semi-supervised clustering. SIGKDD (pp. 59 X 68).
 Belkin, M., Niyogi, P., &amp; Sindhwani, V. (2006). Man-ifold regularization: A geometric framework for learning from labeled and unlabeled examples. Jour-nal of Machine Learning Research , 7 , 2399 X 2434. Bilenko, M., Basu, S., &amp; Mooney, R. (2004). In-tegrating constraints and metric learning in semi-supervised clustering. ICML .
 Borchers, B. (1999). CSDP, a C library for semidefinite programming. Optimization Methods &amp; Software , 11-2 , 613 X 623.
 Boyd, S., &amp; Vandenberghe, L. (2004). Convex opti-mization . Cambridge University Press.
 Chapelle, O., Sch  X olkopf, B., &amp; Zien, A. (2006). Semi-supervised learning . MIT Press.
 Chapelle, O., &amp; Zien, A. (2005). Semi-supervised clas-sification by low density separation. AISTATS . Chung, F. (1997). Spectral graph theory . American Mathematical Society.
 Globerson, A., &amp; Roweis, S. (2006). Metric learning by collapsing classes. Advances in Neural Information Processing Systems (pp. 451 X 458).
 Goldberg, A., Zhu, X., &amp; Wright, S. (2007). Dissim-ilarity in graph-based semisupervised classification. AISTATS .
 Hoi, S., Jin, R., &amp; Lyu, M. (2007). Learning nonpara-metric kernel matrices from pairwise constraints. ICML (pp. 361 X 368).
 Kamvar, S., Klein, D., &amp; Manning, C. (2003). Spectral learning. IJCAI (pp. 561 X 566).
 Klein, D., Kamvar, S., &amp; Manning, C. (2002). From instance-level constraints to space-level constraints:
Making the most of prior knowledge in data cluster-ing. ICML (pp. 307 X 314).
 Kulis, B., Basu, S., Dhillon, I., &amp; Mooney, R. (2005).
Semi-supervised graph clustering: A kernel ap-proach. ICML (pp. 457 X 464).
 Li, Z., Liu, J., Chen, S., &amp; Tang, X. (2007). Noise robust spectral clustering. ICCV .
 Sch  X olkopf, B., &amp; Smola, A. (2002). Learning with ker-nels: support vector machines, regularization, opti-mization, and beyond . MIT Press.
 Shawe-Taylor, J., &amp; Cristianini, N. (2004). Kernel methods for pattern analysis . Cambridge University Press.
 Smola, A., &amp; Kondor, R. (2003). Kernels and regular-ization on graphs. COLT .
 Strehl, A., &amp; Ghosh, J. (2003). Cluster ensembles  X  a knowledge reuse framework for combining multiple partitions. Journal of Machine Learning Research , 3 , 583 X 617.
 Sturm, J. F. (1999). Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones. Op-timization Methods &amp; Software , 11-2 , 625 X 653. Szummer, M., Jaakkola, T., &amp; Cambridge, M. (2002).
Partially labeled classification with markov random walks. Advances in Neural Information Processing Systems .
 Tong, W., &amp; Jin, R. (2007). Semi-supervised learning by mixed label propagation. AAAI .
 Wagstaff, K., &amp; Cardie, C. (2000). Clustering with instance-level constraints. ICML (pp. 1103 X 1110). Wagstaff, K., Cardie, C., Rogers, S., &amp; Schroedl, S. (2001). Constrained k-means clustering with back-ground knowledge. ICML (pp. 577 X 584).
 Xing, E., Ng, A., Jordan, M., &amp; Russell, S. (2003).
Distance metric learning, with application to clus-tering with side-information. Advances in Neural Information Processing Systems (pp. 505 X 512). Zhang, T., &amp; Ando, R. (2006). Analysis of spectral kernel design based semi-supervised learning. Ad-vances in Neural Information Processing Systems (pp. 1601 X 1608).
 Zhou, D., Bousquet, O., Lal, T. N., Weston, J., &amp;
Sch  X olkopf, B. (2004). Learning with local and global consistency. Advances in Neural Information Pro-cessing Systems .
 Zhu, X. (2005). Semi-supervised learning literature survey (Technical Report 1530). Computer Sciences, University of Wisconsin-Madison.
 Zhu, X., Ghahramani, Z., &amp; Lafferty, J. Semi-supervised learning using gaussian fields and har-
