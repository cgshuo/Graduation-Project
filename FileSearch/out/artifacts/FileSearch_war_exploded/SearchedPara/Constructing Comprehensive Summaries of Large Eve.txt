 Event sequences capture system and user activity over time. Prior research on sequence mining has mostly focused on discovering local patterns. Though interesting, these pat-terns reveal local associations and fail to give a comprehen-sive summary of the entire event sequence. Moreover, the number of patterns discovered can be large. In this paper, we take an alternative approach and build short summaries that describe the entire sequence, while revealing local asso-ciations among events.

We formally define the summarization problem as an op-timization problem that balances between shortness of the summary and accuracy of the data description. We show that this problem can be solved optimally in polynomial time by using a combination of two dynamic-programming algorithms. We also explore more efficient greedy alterna-tives and demonstrate that they work well on large datasets. Experiments on both synthetic and real datasets illustrate that our algorithms are efficient and produce high-quality results, and reveal interesting local structures in the data. H.2.8 [ Database Management ]: Database Applications X  Data mining ; I.5.3 [ Pattern Recognition ]: Clustering X  Algorithms ;E.4[ Coding and Information Theory ]: [Data Compaction and compression] Algorithms, Experimentation, Theory event sequences, summarization, log mining
Monitoring of systems X  and users X  activities produces large event sequences , i.e., logs where each event has an associated time of occurrence. Network traffic data, alarms in telecom-munication networks, logging systems are examples of ap-plications that produce large event sequences. Off-the-shelf data-mining methods for event sequences though successful in finding recurring local structures, e.g., episodes, can prove inadequate to provide a global model of the data. Moreover, data-mining algorithms usually output too many patterns that may be overwhelming for the data analysts. In this paper, we bring up a new aspect of event sequence analysis, namely how to concisely summarize such event sequences.
From the point of view of a data analyst, an event-sequence summarization system should have the following properties.
Despite the bulk of work on the analysis of event-sequences, to the best of our knowledge, there is no technique that sat-isfies all requirements discussed above. In this paper, we present a summarization technique that exhibits all these characteristics. More specifically, Figure 1: Visual representation of an event sequence
Example 1. Figure 1 shows an example of an input event sequence and the output of our method for this particular sequence. The input sequence is shown in Figure 1(a); it contains three event types { A, B, C } and it spans timeline [1 , 30] that consists of 30 discrete timestamps.

Figure 1(b) shows the actual segmental grouping that our method finds. Three segments are identified: [1 , 11] , [12 and [21 , 30] . Within each segment the events are grouped into two groups; event types with similar frequency of ap-pearance within a segment are grouped together. In the first segment, the two groups consist of event types { A, B } and { C } -A and B are grouped together as they appear much more frequently than C in the interval [1 , 11] . Similarly, the groups in the second segment are { A } and { B, C } and in the third segment { A, C } and { B } .

Finally, Figure 1(c) shows what the output of the summa-rization method conceptually looks like. The coloring of the groups within a segment is indicative of the probability of ap-pearance of the events in the group; darker colors correspond to higher occurrence probabilities.
We address the following problem: assume an event se-quence S that records occurrences of events over a time in-terval [1 ,n ]. Additionally, let E denote the distinct event types that appear in the sequence. Our goal is to partition the observation interval into segments of local activity that span [1 ,n ]; within each segment identify groups of event types that exhibit similar frequency of occurrence in the segment. We use the term segmental grouping to describe such data-description model. For the purposes of this paper we only consider discrete timelines. We additionally assume that events of different types are generated at every distinct timestamp independently from some stationary probability that depends on the event type and the segment itself.
We formally define the problem of finding the best seg-mental grouping as an optimization problem. By penalizing both complex and simple models, we develop a parameter-free methodology and provide polynomial-time algorithms that optimally solve the above summarization problem. Dynamic-programming is at the core of these optimal algorithms. The computational complexity of our algorithms depends only on the number of timestamps at which events occur and not on the total length of the timeline n .

Although the main motivation for our work is the forensic analysis of large audit logs, we conjecture that the tech-niques presented herein can also be applied to other diverse domains. For example, useful summaries can be constructed for biological data, data streams, and large documents using our methodology.
The rest of the paper is organized as follows; in Section 2 we give some basic notational conventions. In Section 3 we formally describe our summarization scheme and the corre-sponding optimization problem of finding the best summary. The algorithms for solving the problem are presented in Sec-tion 4 and experiments are given in Section 5. We review the related work in Section 6 and conclude in Section 7.
Event sequences consist of events that occur at specific points in time. That is, every event in the sequence has an associated time of occurrence. We assume a set E of m different event types .Aneventisapair( E, t ), where E  X  X  is an event type and t is the (occurrence) time of the event on a timeline. We consider discrete timelines in which occurrence times of events are positive integers in the interval [1 ,n ]. That is, the timeline consists of n different evenly spaced timestamps at which events of different types might occur.

We represent an event sequence by an m  X  n array S such that S ( i, t )=1ifaneventoftype E i has occurred at time point t . At a certain time t , events of different types can occur simultaneously. That is, each column of S can have more than one 1-entries. However, at any time t , only one event of each type can occur (If multiple events of the same type do occur at a point t , they can be ignored as duplicates).
Figure 1(a) shows an event sequence S on which events of m = 3 different types appear; E = { A, B, C } . The events oc-cur on the timeline [1 , 30]. That is, there are 30 timestamps at which any of the three event types can occur.

Given interval I  X  [1 ,n ], we use S [ I ]todenotethe m  X | I | projection of S on the interval I . Finally, for event type E  X  X  and interval I  X  [1 ,n ] we denote the number of occurrences of events of type E within the interval I with n ( E, I ).

The core idea is to find a segmentation of the input time-line [1 ,n ] into contiguous, non-overlapping intervals that cover [1 ,n ]. We call these intervals segments . More for-mally, we want to find a segmentation of S into segments denoted by S =( S 1 ,..., S k ). Such a segmentation is de-fined by k +1 boundaries { b 1 , b 2 , ...,b k , b k +1 } b k +1 = n +1 and each b j ,with2  X  j  X  k takes integer values in [2 ,n ]. Therefore, the j -th segment corresponds to the sub-sequence S [ b j ,b j +1  X  1]. A segmentation of the input event sequence of Figure 1(a) is shown in Figure 1(b). The input sequence is split into 3 segments defined by the boundaries { 1 , 12 , 21 , 31 } .

We now focus our attention on the data of a specific seg-ment S i defined over the time interval I .Thatis, S i = S [ We describe the portion of the data that corresponds to S by the local model M i . We consider model M i to be a par-titioning of event types E into groups { X i 1 ,...,X i } that X ij  X  X  and X ij  X  X ij =  X  for every j = j with 1  X  j, j  X  .Eachgroup X ij is described by a single pa-rameter p ( X ij ) that corresponds to the probability of seeing an event of any type in X ij within data segment S i .
Consider for example the first segment of the output seg-mentation in Figure 1(b) (or Figure 1(c)) that defines seg-ment I 1 =[1 , 11], with length | I 1 | = 11. In this case the local model M 1 that describes the data in S 1 = S [ I 1 ] partitions E into groups X 11 = { A, B } and X 12 = { C } with and The Summarization Problem. Our overall goal is to iden-tify the set of boundaries on the timeline that partition S into segments ( S 1 ,..., S k ) and within each segment S tify a local model M i that best describes the data in S i
The partitioning of S into segments ( S 1 ,..., S k )andthe corresponding local models M 1 ,...,M k constitute the seg-mental grouping or summary of S . For the rest of the dis-cussion we use the terms summary and segmental grouping interchangeably.

In order to be able to devise algorithms for the Summa-rization problem we first need to define the optimization function that best describes the objective of this informal problem definition. Our optimization function is motivated by the Minimum Description Length (MDL) principle. Before formally developing our model, we first review the Minimum Description Length (MDL) principle. Then, we show how to apply this principle to formalize the Summa-rization problem.
The MDL principle [15, 16] allows us to transform the requirement of balance between over-generalizing and over-fitting into a computational requirement.

In brief the MDL principle states the following: assume two parties P and P that want to communicate with each other. More specifically, assume that P wants to send event sequence S to P using as less bits as possible. In order for to achieve this minimization of communication cost, she has to select model M from a class of models M , and use M to describe her data. Then, she can send P model M plus the additional information required to describe the data given the transmitted model.

Thus, party P has to encode the model M andthenencode the data given this model. The quality of the selected model is evaluated based on the number of bits required for this overall encoding of the model and the data given the model.
MDL discourages complex models with minimal data cost and simple models with large data costs. It tries to find a balance between these two extremes. It is obvious that the MDL principle is a generic principle and it can have mul-tiple instantiations that are determined by a set of model-ing assumptions. It has been previously successfully applied in a variety of settings that range from decision-tree classi-fiers [11], genetic-sequence modeling [8], patterns in sets of strings [7] and many more. We devote the rest of the section to describe our instantiation of the MDL principle.
Recall that we model event sequences using a segmen-tation model that partitions the input observation interval [1 ,n ] into contiguous, non-overlapping intervals I 1 ,...,I Therefore, S is split into ( S 1 ,..., S k ), where S i = S [ The data in each S i are described by local model M i ;the local model is in fact a grouping of the event types based on their frequency of appearance in S i .
 Local encoding scheme: We start by describing the pro-cedure that estimates the number of bits required to encode the data within a single segment S i .

Let model M i partition the rows of S i (which correspond to events of all types, present or not in S i )into groups X ,...,X .Eachgroup X j is described by a single param-eter p ( X j ), the probability of appearance of any event type in
X j within subsequence S i . Given the X j  X  X , and corre-sponding p ( X j ) X  X  for 1  X  j  X  , and assuming independence of occurrences of events and event types, the probability of data S i given model M i is given by The number of bits required to describe data S i given model M i is  X  log (Pr ( S i | M )) 1 . Therefore, local data cost of S given M i is Equation (1) gives the number of bits required to describe data in S i given model M i . For the encoding of S i we also need to calculate the number of bits required to encode the model M i itself. We call this cost (in bits) the local model cost Lm ( M i ). In order to encode M i we need to describe the event types associated with every group X j (1  X  j  X  ), and for each group X j we need to specify parameter p (
X j ). By standard arguments [15], we need log m bits to describe each one of the p ( X j ) X  X . Since there are groups we need a total of log m bits to encode the different p ( X j The encoding of the partitioning is slightly more tricky; first we observe that if we fix an ordering of the event types
By standard arguments the number of bits required for encoding an event with probability q is  X  log( q ). that is consistent with the partitioning X 1 ,...,X , 2 then we need m log m bits to specify the ordering and log m bits to identify the partition points on that fixed order. This is because for this fixed order the partition points are integers in the range [1 ,m ] and thus log m bits are necessary for the description of each partition point. Summing up these costs we get the local model cost for M i that is Therefore, the total local cost in bits for describing segment S is the number of bits required to describe S i given model M i and the cost of describing model M i itself. By summing Equations (1) and (2) we get the valuation of the local cost Ll ,whichis Generative model: The above encoding schemes assume the following data-generation process; within segment S events of different types are generated independently. For each event type E  X  X j ,with1  X  j  X  ,aneventoftype E is generated at every time point t  X  I independently with probability p ( X j ).
 Global Encoding Scheme: The global model is the seg-mental model M that splits S into segments S 1 ,..., S k ;each segment is specified by its boundaries and the correspond-ing local model M i . If for every segment i , the data in S is described using the encoding scheme described above, the only additional information that needs to be encoded for describing the global model is the positions of the segment boundaries that define the starting points of the segments on timeline [1 ,n ]. Since there are n possible boundary po-sitions the encoding of k segment boundaries would require k log n bits. Therefore, the total length of the description in bits would be where Ll ( S i ,M i ) is evaluated as in Equation (3). We are now ready to give the formal definition of the Summarization problem.

Problem 1. ( Summarization ) Given event sequence S over observation period [1 ,n ] in which event types from set occur, find integer k and a segmental grouping M of S into ( S 1 ,..., S k ) and identify the best local model M i for each S such that the total description length is minimized.

Problem 1 gives the optimization function that consists of the number of bits required to encode the data given the model, and the model itself. Note that the total model cost can be decomposed in the cost for encoding the global seg-mentation model k log n plus the cost for encoding the dif-ferent local models evaluated as in Equation (2). The cost of
A trivial such ordering is the one that places first all the event types in X 1 , followed by the event types in X 2 and so on. encoding the data given the model is simply the summation of the local data costs for every segment.

We use Ll  X  ( S i ) to denote the minimum value of Ll ( S over all possible local models M i . Similarly, we use Tl to denote the minimum value of Tl ( S ,M ) over all possible summaries M .

Since the definition of the optimization function is formed based on the MDL principle, the function is such that: (a) complex summaries are penalized because they over-fit the data and (b) simple summaries are also penalized since they over-generalize and fail to describe the data with the desired accuracy. Moreover, using the MDL principle allows for a problem formulation that is parameter-free; no parameter setting is required from the analyst who is attempting to extract knowledge from the input event sequence S .
Despite the apparent interplay between the local models picked and the positions of the segment boundaries on the timeline, we can show that, in fact, Problem 1 can be solved optimally in polynomial time.

Given data segment S i we call the problem of identify-ing the local model that minimizes Ll ( S i ,M i )the Local-Grouping problem, and we formally define it as follows.
Problem 2. ( LocalGrouping ) Given sequence S and interval I  X  [1 ,n ] find the optimal local model M i that min-imizes the local description length of S i = S [ I ] given That is, find M i such that Our algorithmic approach exploits the following statement.
Statement 1. If the LocalGrouping problem (Prob-lem 2) can be solved optimally in polynomial time, then the Summarization problem (Problem 1) can also be solved op-timally in polynomial time.

In the rest of this section we give optimal polynomial-time algorithms for the Summarization problem. The al-gorithms exploit the above statement. Moreover, we pro-vide alternative sub-optimal, but practical and efficient, al-gorithms for the Summarization problem.
We first present an optimal dynamic-programming algo-rithm for the Summarization problem. We also show that not all possible segmentations of interval [1 ,n ] are candidate solutions to the Summarization problem.

Theorem 1. For any interval I  X  [1 ,n ] , let Ll  X  ( S [ I min M i Ll ( S [ I ] ,M i ) . Then, Problem 1 can be solved opti-mally by evaluating the following dynamic-programming re-cursion. For every 1  X  i  X  n , The proof of optimality is omitted due to space constraints. However, a similar proof can be found in [2]. We call the dynamic-programming algorithm that implements Recur-sion (5) the Segment-DP algorithm. If T L is the time re-quired to evaluate Ll  X  ( S [ I ]), then the running time of the Segment-DP algorithm is O ( n 2 T L ).

Not all points on the interval [1 ,n ] are qualified to be seg-ment boundaries in the optimal segmentation. In fact, only the timestamps on which an event (of any type) occurs are candidate segment boundaries. The following proposition summarizes this fact.

Proposition 1. Consider event sequence S that spans interval [1 ,n ] and let T  X  X  1 , 2 ,...,n } be the set of times-tamps at which events have actually occurred. Then, the segment boundaries of the optimal segmentation model are subset of T .

Proposition 1 offers a speedup of the Segment-DP algo-rithm from O n 2 T L to O | T | 2 T L ,where | T | X  n .Thatis, the evaluation of Recursion (5) does not have to go through all the points { 1 ,...,n } , but rather all the points in which events actually occur. Although in terms of asymp-totic running time Proposition 1 does not give any speedup, in practice, there are many real data for which | T | &lt;&lt; n and therefore Proposition 1 becomes extremely useful. In our experiments with real datasets we illustrate this fact.
The Greedy algorithm is an alternative to Segment-DP and computes a summary M of S in a bottom-up fashion. The algorithm starts with summary M 1 , where all data points are in their own segment. At the t -th step of the algorithm, we identify boundary b in M t whose removal causes the max-imum decrease in Tl S ,M t . By removing boundary b we obtain summary M t +1 . If no boundary that causes cost reduction exists, the algorithm outputs summary M t .
Since there are at most n  X  1 boundaries candidate for removal the algorithm can have at most n  X  1 iterations. In each iteration the boundary with the largest reduction in the total cost needs to be found. Using a heap data structure this can be done in O (1) time.

The entries of the heap at iteration t are the boundaries of summary M t . Let these boundaries be { b 1 ,...,b l } entry b j is associated with the impact , G ( b j ), of its removal from M t . The impact of b j is the change in Tl S ,M t that is caused by the removal of b j from M t . The impact may be positive if Tl S ,M t is increased or negative if the total description length is decreased. For every point b iteration t the value of G ( b j )is
The positive terms in the first row of the above equation correspond to the cost of describing data S [ b j  X  1 ,b j +1 after removing b j and merging segments [ b j  X  1 ,b j ]and[ 1] into a single segment. The negative costs correspond to the cost of describing the same portion of the data using the two segments [ b j  X  1 ,b j ]and[ b j ,b j +1  X  1].
Upon the removal of boundary b j at iteration t ,theim-pacts of boundaries b j  X  1 and b j +1 need to be updated. With the right bookkeeping this requires the evaluation of Ll  X  two different intervals per update, and thus O (2 T L )time. In addition to that, one heap update per iteration is required and takes O (log n ) time. Therefore, the total running time of the Greedy algorithm is O ( T L n log n ). Proposition 1 can again speedup the running time of the Greedy algorithm to O (
T l | T | log | T | ).
In this section we show that the LocalGrouping can also be solved optimally in polynomial time using yet another dynamic-programming algorithm. We call this algorithm the Local-DP algorithm. The following proposition is at the core of the Local-DP algorithm.
 Proposition 2. Consider interval I and let S i = S [ I ] . Without loss of generality assume that the events in E are ordered so that n ( E 1 ,I )  X  n ( E 2 ,I )  X  ...  X  n ( E tionally assume that the optimal local model M i constructs groups X 1 ,...,X . Then, we have the following: if E j 1 and E j 2  X  X l ,with j 2 &gt;j 1 , then for all E j  X  X  such that j  X  X  j 1 +1 ,...,j 2  X  1 } we have that E j  X  X l .
Proposition 2 states that the grouping of the event types in interval I respects the ordering of event types with respect to their frequency of appearance in S [ I ].

The next proposition states that finding the optimal pa-rameters p ( X j ) that minimize Ld for local model M i that partitions E into X 1 ,...,X is simple. More specifically, the value of p ( X j ) is the mean of the occurrence probabilities of each event type E  X  X j within segment I .

Proposition 3. Consider interval I  X  [1 ,n ] , and local model M i for data in S i = S [ I ] .Let M i partition E into groups X 1 ,...,X . Then, for every X j ,with 1  X  j  X  ,the value of p ( X j ) that minimizes Ld ( S i | M i ) is
For the rest of this section we will assume that event types in E are ordered according to the ordering described in Proposition 2. Given this ordering, we use E ( j ) to denote the event type at the j -th position of the order and E ( denote the set of event types at positions j, j +1 ,...,l  X  that order. Moreover, given data segment S i we use S i [ to denote the subset of the events in S i that correspond to event types in E ( j, l ).

Given the ordering of the event types in E (Proposition 2) the following dynamic-programming recursion computes the minimum number of bits required to encode S i .

Ll  X  ( S i [1 ,j ]) = m log m +(6) where and by Proposition 3 p  X  is given by
The m log m term in Recursion (6) corresponds to the cost of encoding the ordering of the event types in S i , while the term 2 log m encodes the number of bits required to encode the occurrence probability of any event type in the group E ( l +1 ,j ) and the group itself. Note that the order of the event types needs to be sent only once per segment, while the probability of event appearance per group and the group information needs to be sent once per group.

Theorem 2. The Local-DP algorithm that evaluates Re-cursion (6) finds the optimal local model for the data segment S i in polynomial time.
 The running time of the Local-DP algorithm is O m 2 . For every index j the algorithm recurses over all values of l in the interval 1  X  l  X  j . Since the largest value of j is m , the running time of the algorithm is O ( m 2 ). This quadratic running time is under the assumption that in a preprocessing step we can compute the values of the U() function for all the combination of indices j and l . In fact, the asymptotic term O ( m 2 ) also contains the hidden cost of sorting the event types in E based on their frequency of occurrence in S i ,whichis O ( m log m ).

Note that a proposition similar to Proposition 1 of Sec-tion 4.1 can also be applied here. Informally, this means that event types that do not occur in S i can be ignored when evaluating Recursion (6).
Similar to the Greedy algorithm for finding the optimal segment boundaries in [1 ,n ] (see Section 4.2), we give here a greedy alternative to the Local-DP algorithm that we call the LocalGreedy algorithm. By using the same data struc-tures as the ones described in Section 4.2 the running time of the LocalGreedy algorithm is O ( m log m ).

As the Greedy algorithm, LocalGreedy computes the global partitioning X of S i in a bottom-up fashion. It starts with grouping X 1 , where each event type is allocated its own group. At the t -th step of the algorithm grouping X t is considered, and the algorithm merges the two groups that introduce the maximum decrease in Ll ( S i ,M i ). This merge leads to partition X t +1 . If no merging that causes cost re-duction exists, the algorithm stops and outputs partition .
Both Segment-DP and Greedy algorithms require a func-tion that evaluates Ll  X  for different data intervals. The value of Ll  X  can be evaluated using either Local-DP or LocalGreedy algorithms. This setting creates four differ-ent alternative algorithms for solving the Summarization problem; the DP-DP that combines Segment-DP with Local-DP ,the DP-Greedy that combines Segment-DP with Local-Greedy ,the Greedy-DP that combines Greedy with Local-DP and Greedy-Greedy that combines Greedy with Local-Greedy . DP-DP gives the optimal solution to the Summa-rization problem. However, all other combinations also provide high-quality results, while at the same time they give considerable computational speedups.

In terms of asymptotic running times the DP-DP algorithm requires O ( n 2 m 2 )time,the DP-Greedy O ( n 2 m log m ), the Greedy-DP O ( m 2 n log n )andthe Greedy-Greedy algorithm time O ( nm log n log m ).
In this section we report our experiments on a set of synthetic and real datasets. The main goal of the exper-imental evaluation is to show that all four algorithms we developed for the Summarization problem (see Section 4) give high-quality results. That is, we show that even our non-optimal greedy-based algorithms ( DP-Greedy , Greedy-DP and Greedy-Greedy ) use close to the optimal number of bits to encode the input event sequences, while producing meaningful summaries. Moreover, the greedy-based meth-ods provide enormous computational speedups compared to the optimal DP-DP algorithm.

The implementations of our algorithms are in Java Version 1.4.2. The experiments were conducted on a Windows XP SP 2 workstation with a 3GHz Pentium 4 processor and 1 GB of RAM.

We evaluate the quality of the solutions produced by an algorithm A , by reporting the compression ratio CR( A where A is any of the algorithms: DP-DP , DP-Greedy , Greedy-DP and Greedy-Greedy .If M A is the summary picked by algorithm A as a solution to the Summarization problem with input S , then, we define the compression ratio of algo-rithm A to be
Summary M unit is the model that describes every event on S separately; such a model has n segment boundaries (one segment per timestamp) and m groups per segment and it corresponds to the model where no summarization is done. By definition, compression ratio takes values in [0 the smaller the value of CR( A ) the better the compression achieved by algorithm A . In this section we give experiments on synthetic datasets. The goal of these experiments is threefolds. First to demon-strate that our algorithms find the correct model used for the data generation; second to show that they significantly compress the input datasets; third to show that the greedy alternatives, though not provably optimal perform as well as the optimal DP-DP algorithm in practice.
 The datasets: We generate synthetic datasets as follows: we first fix n , the length of the observation period, m ,the number of different event types that appear in the sequence and k , the number of segments that we artificially  X  X lant X  in the generated event sequence. In addition to { 0 } and { n +1 } we select k  X  1 other unique segment boundaries at random from points { 2 ,...,n } . These boundaries define the k segments. Within each segment I i =[ b i ,b i +1 )we randomly pick the number of groups to be formed. Each such group X ij , is characterized by parameter p ( X ij ), that corresponds to the probability of occurrence of each event type in X ij in segment I i .Thevaluesof p ( X ij ) are normally distributed in [0 , 1].

Parameter V is used to control the noise level of the gener-ated event sequence. When V = 0, for every segment I i and every X ij in I i ,eventsofanytype E  X  X ij are generated Figure 2: Synthetic datasets: n = 1000 , m =20 , k =10 ; Figure 3: Synthetic datasets: n = 1000 , V =0 . 04 , k =10 ; independently at every timestamp t  X  I i with probability p (
X ij ). For noise levels V&gt; 0, any event of type E  X  X is generated at each point t  X  I i with probability sampled from the normal distribution N ( p ( X i,j ) ,V ). Accuracy of the algorithms: Figure 2 shows the com-pression ratio of the four different algorithms ( DP-DP , DP-Greedy , Greedy-DP and Greedy-Greedy ) as a function of the increasing noise level V that takes values in [0 . 01 , 0 this experiment we fix n = 1000, k =10and m = |E| =20. In addition to our four algorithms, we also show the com-pression ratio of the ground-truth model ( GT ). This is the model that has been used in the data-generation process. From Figure 2 we observe that all four algorithms provide summaries with small CR values, close to 7%. 3 Further-more, this compression ratio is very close the compression ratio achieved by the ground-truth model. In fact for high noise levels ( V =0 . 3 , 0 . 4 , 0 . 5) the CR achieved by our al-gorithms is better than the CR of the ground-truth model. This is because for high noise levels, the data-generation model is less likely to be the optimal model to describe the data. Overall, even the greedy-based algorithms exhibit per-formance almost identical to the performance of the optimal DP-DP algorithm in terms of the number of bits required to encode the data.

Figure 3, shows the compression ratio of our algorithms as a function of the number of event types m that appear in the sequence. For this experiment, we vary m to take
Recall that the smaller the value of CR the better the sum-mary produced by an algorithm. values { 2, 4, 8, 16, 32, 128 } and fix the rest of the param-eters of the data-generation process to n = 1000, k =10 and V =0 . 04. As in the previous experiment, we can ob-serve that the compression ratio achieved by our algorithms is almost identical to the compression ratio achieved by the corresponding ground-truth model. Furthermore, we can observe that all our algorithms exhibit the same compres-sion ratio and thus can be used interchangeably. Notice that as the number of event types increases, the compression ra-tio achieved by both the ground-truth model as well as the models discovered by our algorithms decreases, i.e., better summaries are found when compared to the raw data. This is because the more event types appear in the data, the more local patterns there are, which are discovered by our sum-marization methods. M unit on the other hand, is oblivious to the existence of local groupings. As a result, for large number of event types the denominator in Equation 7 grows much faster than the numerator.
In this section we further illustrate the utility of our algo-rithms in a real-life scenario. By using event logs managed by Windows XP we show again that all four algorithms con-siderably compress the data and that produce equivalent and intuitive models for the input sequences.

The real datasets consist of the application log ,the se-curity log and the system log displayed by the Windows XP Event Viewer on our machines 4 .The application log contains events logged by application programs. The se-curity log records events such as valid and invalid logon attempts, as well as events related to usage of resources. Fi-nally, the system log contains events logged by Windows XP system components. Each one of the three log files we use stores log records with the following fields: ( Event_Type , Date , Time , Source , Category , Event , User , Computer ). We exported each one of the three log files into a separate file and processed them individually.
 Our application log spans a period from June 2007 to November 2007, the security log the period from May 2007 to November 2007 and the system log the period from November 2005 to November 2007. For all these files we consider all the logged events found on our computer, with-out any modification.
 Considering as event types the unique combinations of Event_Type , Source and Event and as timestamps of events the combination of Date and Time , we get the datasets with characteristics described in Table 1 (upper part). Note that the system records the events at a millisecond granular-ity level. Therefore, the actual length of the timelines ( for the application , security and system logs are n = 12 , 313 , 576 , 000, n =14 , 559 , 274 , 000 and n =61 , 979 respectively. However, this fact does not affect the perfor-mance of our algorithms which by Proposition 1 only de-pends on the number of unique timestamps N on which events actually occur; the values of N for the three datasets are are N = 2673, N =7548and N = 6579 respectively.
Elapse times for the computations are reported in sec-onds in Table 1. For example, the elapse time for the DP-DP method with the system dataset is roughly 10 hours; which makes this method impractical for large datasets containing a large number of event types. We see that the Greedy-
We use the default Windows configuration for logging, so similar datasets exist on all Windows machines. Greedy algorithm ran in 24 seconds for the same dataset.
Finally, the compression ratio (CR) achieved for the three datasets by the four different algorithms are also reported in Table 1. The results indicate that the greedy-based meth-ods produce as good summaries as the optimal DP-DP algo-rithm. Therefore, the results of Table 1 further illustrate that despite the optimality of the solutions produced by DP-DP , the latter algorithm can prove impractical for very large datasets. Greedy-based algorithms on the other hand, give almost as accurate and condensed summaries and are much more efficient in practice.
 Structural similarity of the results .Wehaveobserved that all our algorithms achieve almost identical compres-sion ratios for the same data. A natural question to ask is whether the actual models they output are also structurally similar. In other words, do the reported segmental group-ings have the same segment boundaries and are the groups within the reported segments similar?
The goal of Figure 4 is to answer this question in an af-firmative way. This figure visualizes the output segmental groupings reported by algorithms DP-DP , DP-Greedy , Greedy-DP and Greedy-Greedy (Figures 4(a), 4(b), 4(c), and 4(d) respectively) for the application log dataset.

Each subfigure corresponds to the output of a different algorithm and should be interpreted as follows: the x-axis corresponds to the timeline that is segmented, with the ver-tical lines defining the segment boundaries on the timeline. Within each segment, different groups of event types are rep-resented by different colors (darker colors represent groups that have higher probability of occurrence within a seg-ment). The vertical length of each group is proportional to its size. The main conclusion that can be drawn from Figure 4 is that the output segmental groupings of DP-DP and DP-Greedy algorithms are almost identical, and the out-put of all four algorithms are very close to each other. The apparent similarity is that all segmentations have a large segment in the beginning of the observation period and an even larger segment towards its end. In these segments the same number of groups are observed. In the interval that is in-between these two large segments the outputs of DP-DP , DP-Greedy and Greedy-DP exhibit very similar structure, by identifying almost identical segment boundaries. Seemingly different are the boundaries found by Greedy-Greedy algo-rithm. However, a closer look shows that these latter bound-aries are not far from the boundaries identified by the other three algorithms; Greedy-Greedy in fact identified boundary positions very close to the boundary positions identified by the other three algorithms. Figure 4: Output segmental groupings of different algo-
Although we are not aware of any work that proposes the same summarization model for event sequences, our work clearly overlaps with work on sequence mining and time-series analysis.

Closely related to ours is the work on mining episodes and sequential patterns ([1, 3, 10, 13, 19]). That work mostly focuses on developing algorithms that identify configura-tions of discrete events clustered in time. Although those algorithms identify local event patterns, known as frequent episodes, they do not provide a global description of the event sequence neither do they care about the conciseness of the produced patterns.

Summarization of event sequences via a segmentation model is proposed in [9]. However, the technique presented there can only model sequences of single event types; within each local interval, the appearances of events are modelled by a constant intensity model. In fact, one can think of our model as a generalization of the model proposed in [9] since in fact we split the event types into groups of constant intensities.
Also related is the segmentation framework developed by [8] in order to identify block structures in genetic sequences. A minimum description length approach is also used there for identifying the number and positions of segment boundaries. However, the models built within each block serve the partic-ular modelling requirements of the genetic sequences under study. For example, in the case of [8] finding the local model in each segment is an NP-hard task, while in our case this task is polynomial.

At a high level there is an obvious connection between our model and the standard segmentation model used for time-series segmentation (see [4, 5, 6, 18] and indicative, though not complete, set of references). Similarly, there is an equally interesting line of work that deals with the discovery of local patterns in time-series data, e.g., [12, 17, 20]. However, the connection to our work remains at a high level since we focus on event sequences and not on time series, while at the same time the local models we consider per segment are quite distinct from the models considered before. Same high-level connection exists between our model and HMMs [14]. However, the assumptions behind HMMs are different from the assumptions we make in this model.
We proposed a framework and an algorithmic solution to the problem of summarizing deluging event sequences that continuously record the activities of systems and individu-als. Our framework is based on building segmental group-ings of the data. A segmental grouping splits the timeline into segments; within each segment events of different types are grouped based on their frequency of occurrence in the segment. Our approach is based on the MDL principle that allows us to build summaries that are short and accurately describe the data without over-fitting.

Our contribution is in the definition of the segmental group-ings as a model for summarizing event sequences. This model when combined with the MDL principle allowed us to naturally transform the event-sequence summarization prob-lem to a concrete optimization problem. We showed that this problem can be solved optimally in polynomial time us-ing a combination of two dynamic-programming algorithms. Furthermore, we designed and experimented with greedy al-gorithms for the same problem. These algorithms, though not provably optimal, are extremely efficient and in practice give high-quality results. All our algorithms are parameter free and when used in practice produce meaningful sum-maries. [1] R. Agrawal and R. Srikant. Mining Sequential [2] R. Bellman. On the approximation of curves by line [3] D. Chudova and P. Smyth. Pattern discovery in [4] S. Guha, N. Koudas, and K. Shim. Data-streams and [5] P. Karras, D. Sacharidis, and N. Mamoulis. Exploiting [6] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani. An [7] P. Kilpel  X  ainen, H. Mannila, and E. Ukkonen. Mdl [8] M. Koivisto, M. Perola, T. Varilo, et al. An MDL [9] H. Mannila and M. Salmenkivi. Finding simple [10] H. Mannila and H. Toivonen. Discovering generalized [11] M. Mehta, J. Rissanen, and R. Agrawal. Mdl-based [12] S. Papadimitriou and P. Yu. Optimal multi-scale [13] J. Pei, J. Han, and W. Wang. Constraint-based [14] L. R. Rabiner and B. H. Juang. An introduction to [15] J. Rissanen. Modeling by shortest data description. [16] J. Rissanen. Stochastic Complexity in Statistical [17] Y. Sakurai, S. Papadimitriou, and C. Faloutsos. Braid: [18] E. Terzi and P. Tsaparas. Efficient algorithms for [19] J. Yang, W. Wang, P. S. Yu, and J. Han. Mining long [20] Y. Zhu and D. Shasha. Statstream: Statistical
