 STEVEN BURROWS, MARTIN POTTHAST, and BENNO STEIN , Bauhaus-Universit  X  at The recognition and generation of paraphrases forms the heart of numerous analysis and synthesis tasks in information retrieval, information extraction, and natural lan-guage processing. Given two texts, t 1 and t 2 ,then t 1 is a paraphrase of t 2 if a fact  X  that can be inferred from t 1 can also be inferred from t 2 , and vice versa.
The symbol  X  refers to a world (a domain theory or background knowledge) in the form of a set of relations that readers of t 1 and t 2 agree upon. Paraphrase recognition means to analyze whether two texts t 1 and t 2 fulfill Eq. (1); paraphrase generation means to construct a text t 2 given a text t 1 such that Eq. (1) is fulfilled; paraphrase acquisition refers to the task of compiling a set of paraphrase pairs. The difficulty of recognizing paraphrases results from the complexity of semantic entailment [Androutsopoulos and Malakasiotis 2010], as well as from the fact that it is impossible to state  X  completely. 1 This argumentation also applies to paraphrase generation and paraphrase acquisition.
Typically, t 1 and t 2 are of similar length, and most of the existing research is in fact on sentence-level paraphrasing. Purely syntactical approaches to paraphrase recognition (and generation as well) disregard semantic relationships and compare t 1 and t 2 using surface-level features. If semantics are brought in, they are often in the form of  X  being a thesaurus or a synonym dictionary.

Humans outperform machines in paraphrasing and thus finally decide whether t 1 and t 2 stand in a paraphrase relation. This fact explains the utmost importance of well-formed paraphrase corpora, since they are necessary to evaluate and benchmark the progress of research groups working on the foundations of paraphrasing, on new algorithms, and on new tools. But, as pointed out by Dolan and Brockett [2005], the compilation of reference corpora is more difficult than in other fields.
In our research, we investigated whether crowdsourcing can be exploited in order to compile a reference corpus of realistic order of magnitude. Our undertaking was not of purely theoretical interest, but driven by the PAN 2010 International Plagiarism Detection Competition [Potthast et al. 2010a] for which an extensive and high-quality corpus had to be developed.

Plagiarism detection is a paraphrase recognition task that has received much recent attention. Plagiarism refers to the use of another author X  X  information, language, or writing, when done without proper acknowledgment of the original source. The detec-tion of plagiarism refers to the task of unveiling such cases. Note in this regard that most paraphrase recognition tasks are of  X  X xternal X  nature: one is given a text t 1 along with a corpus D wherein a paraphrase t 2 of t 1 is to be found. For intrinsic plagiarism detection tasks even t 1 is not given, but must be identified due its different writing style. A particularity of plagiarism detection is that t 1 and t 2 are assumed to be of passage length or section length.
Figure 1 organizes a broad spectrum of relevant text analysis and text synthesis tasks that deal with paraphrase recognition or generation. Paraphrase acquisition, which is the focus of our article, combines aspects from both. This article discusses the role of paraphrasing in plagiarism detection (Section 2), outlines the construction of a paraphrase corpus via crowdsourcing (Section 3), and reports on the parts that can be automated and the associated cost trade-offs (Section 4). Its main contributions can be summarized as follows. (1) We identify plagiarism detection as a relevant paraphrase recognition task that, in (2) We construct a paraphrase corpus with 7859 positive and negative samples, para-(3) We tackle the problem of automatic quality assurance and turn crowdsourcing into (4) Our classification model combines the best state-of-the-art features and different (5) We give a comprehensive cost and time analysis showing excellent savings that can
We consider our research also as a gap-bridging piece of work between different com-munities, as it shows common ground between paraphrasing and plagiarism research. Existing Corpora . The MSRPC corpus [Dolan and Brockett 2005], the KMC corpus [Knight and Marcu 2002], an unnamed corpus by Cohn et al. [2008], and the METER corpus [Clough et al. 2002] have all featured prominently in the paraphrasing community and closely related areas. The first three corpora are all comprised of sentence-level paraphrases [Madnani and Dorr 2010]. The MSRPC 2 (Microsoft Re-search Paraphrase Corpus) contains 5801 sentence pairs, of which 3900 are positives and 1901 are negatives. The corpus was developed using automated extraction tech-niques of probable sentence-level paraphrases from news articles [Dolan and Brockett 2005], and candidate paraphrases were assessed by two human judges. The KMC 3 (Knight and Marcu Corpus) corpus contains 1067 sentence pairs, of which all are positives. This design has motivated others to supplement the corpus with negative samples [Fernando 2007]. This corpus was again developed with automated extraction techniques on news articles, and the candidate paraphrases were identified from the articles and their corresponding abstract summaries. We note that this corpus is an application of entailed sentences, therefore it is less relevant to our work, but we still mention it for completeness. The unnamed corpus by Cohn et al. [2008] 4 contains 900 sentence pairs evenly taken from the MSRPC corpus, Chinese journalism stories, and a novel. Several automated and manual approaches are used to annotate the pairs as either  X  X ure X  or  X  X ossible X , but it is unclear which set should be taken as the gold standard. Finally, the METER 5 (Measuring Text Reuse) corpus [Clough et al. 2002] for journalistic text reuse has  X  X ress agency X  and  X  X ewspaper X  copies of the same news stories. It contains 1717 texts judged by one professional journalist, with annotations about the level of derivation and reuse between samples. In summary, three of the corpora are limited to sentence-level paraphrases, and the last corpus was for related work dealing with larger samples in journalistic text reuse.

Crowdsourcing . Brabham [2008a] defines crowdsourcing as an  X  X nline, distributed problem-solving and production model X . It is essentially a form of human computa-tion [Quinn and Bederson 2011]. A benefit of this distributed model is that the work can be shared amongst a wide variety of demographics, skill sets, and political invest-ments [Brabham 2008b], where such diversity would be difficult to obtain otherwise. A disadvantage is that with such anonymity, there are some that will exploit the model, and work is always needed to review submissions to separate the legitimate work from the rest. However, there are many genuine workers out there, who are motivated by rea-sons such as money, personal development, or credibility gained amongst peers. Previ-ous research has demonstrated the successful application of crowdsourcing in a variety of areas including user studies [Kittur et al. 2008], relevance assessment [Alonso and Mizzaro 2009], machine translation [Ambati et al. 2010; Callison-Burch 2009], ideas competitions [Leimeister et al. 2009], and annotating speech and text [Callison-Burch and Dredze 2010; Snow et al. 2008]. Amazon X  X  Mechanical Turk [Barr and Cabrera 2006] is one such crowdsourcing service that is gaining much popular attention, which also forms the crowdsourcing service used in this article.

Crowd Paraphrase Acquisition . Recently, other researchers have considered crowdsourc-ing to collect paraphrases, such as Buzek et al. [2010] who use Mechanical Turk to col-lect paraphrases of parts of sentences that are difficult to translate automatically thus making their translation easier. Denkowski et al. [2010] also study the impact of using paraphrases in machine translation, employing Mechanical Turk to filter automati-cally identified paraphrases of sentences to be translated. Both contributions resulted in a comparably small sample of paraphrases between 1000 and 2500 samples, allowing for manual double-checking of the results. By contrast, Chen and Dolan [2011] follow up on Dolan and Brockett [2005] who first observed the lack of a large paraphrase corpus: they have collected a large-scale corpus of 85000 sentence-level paraphrases using Mechanical Turk. Paraphrases were generated independently as one-sentence descriptions of specific topics defined by a short video. Filtering was accomplished man-ually by discarding judging workers by checking samples of their paraphrases (i.e., not the whole dataset was reviewed manually). While the order of magnitude is similar to our article, differences include that our paraphrases are on passage level and that we introduce an automatic means of quality control, which has been suggested as future work in the aformentioned paper.

Paraphrase Similarity and Classification . Since the list of metrics used in previous work to compute the similarity between paraphrases is very lengthy, we instead broadly describe the three main categories of paraphrase similarity metrics: word-level met-rics, information retrieval metrics, and metrics specifically designed for paraphrasing. First, the word-level metrics make use of the semantic similarity between individual words, and the WordNet::Similarity software [Pedersen et al. 2004] has shown to be a highly used implementation in the paraphrasing community. Numerous contributions have proposed the combination of individual scores obtained from measuring the sim-ilarity of individual terms between samples [Corley and Mihalcea 2005; Fernando and Stevenson 2008]. Second, information retrieval metrics have been applied directly to paraphrasing, such as the Manhattan distance, Euclidean distance, and cosine similar-ity by Malakasiotis [2009], and a probabilistic model by Das and Smith [2009]. Finally, other measures designed specifically for paraphrasing include measures based on n-grams [Cordeiro et al. 2007a] and asymmetric measures [Cordeiro et al. 2007b]. All the measures overviewed before have either been applied individually or in combination as part of machine learning experiments. In summary, we stress that there has been no prior work in applying these measures for paraphrases beyond the sentence level in this body of work. Previous studies on the prevalence of plagiarism are numerous. Sheard et al. [2002] summarize three studies with some of the highest rates of dishonesty reported over student university lifetimes at 88%, 90%, and 91.7%. It is unclear how often specific types of resources are plagiarized, perhaps due preferences within different discipline areas. However, a study by Dordoy [2002] about perceptions in dishonest behavior shows that  X  X opying a few paragraphs of an essay from a book/[I]nternet X  is one of the most common perceived behaviors.

Plagiarising by paraphrasing is one of many types of plagiarism identified by Maurer et al. [2006]. Hamilton et al. [2004] demonstrate that paraphrasing practices are poorly understood using an online test comprising a pool of 25 questions in a university aca-demic integrity workshop. Results show that the correct response rate for the question that asked students to distinguish acceptable and unacceptable paraphrasing exam-ples was the third lowest of all questions. Not only do students poorly understand how to paraphrase correctly, but other studies have shown that this academic integrity skill is not rated as very important compared to others. For example, in a list of 22 types of cheating behavior, Franklyn-Stokes and Newstead [1995] showed that para-phrasing (without references) was the most common behavior type of all, and deemed the third least serious from a combined survey of 112 students and 20 staff. McCabe [2005] discovered similar trends with perceptions of incorrect paraphrasing behavior ranking highest among a group of nine behaviors in a large USA and Canadian survey exceeding 9000 staff.

The act of plagiarizing often happens as follows (see Figure 2): the plagiarist retrieves a source text, copies passages from that source, and optionally paraphrases the copied passages in order to disguise the plagiarism. Paraphrases found in plagiarism cases are hence clearly beyond sentence level.

While the plagiarist may copy from a nondigital source, these days copying from a digital source found on the Web is very quick. This is also why we deem the step of paraphrasing the copied texts to be optional as it requires quite some effort in itself. There are no statistics on the amount of verbatim plagiarism compared to paraphrased plagiarism; however, as the technology to detect plagiarism progresses, paraphrasing may become a major obstacle that stands in between detecting a case of plagiarism or not. This is one of the conclusions that can be drawn from the results of the PAN 2010 plagiarism detection competition. The annual PAN workshop and competition series is about uncovering plagiarism, authorship, and social software misuse. 6 Since the first workshop in 2007, PAN has become a platform for the interdisciplinary exchange of researchers and practition-ers who address these problems from different angles, and PAN has been organized as evaluation competitions now since 2009. The automatic detection of plagiarism has been at the center of attention from the start, and 33 different plagiarism detectors have been evaluated within the 2009, 2010, and 2011 competitions, some of them thrice in a row. For the purpose of these competitions, we have researched and developed the first large-scale evaluation framework for plagiarism detection [Potthast et al. 2010b].
The evaluation framework consists of the PAN plagiarism corpus 7 in the versions of 2009 (PAN-PC-09), 2010 (PAN-PC-10), and 2011 (PAN-PC-11) as well as tailored detection performance measures. The corpora comprise generated plagiarism cases that have been inserted automatically into plain text documents; the 2009 version comprises 94202 cases in 41223 documents, the 2010 version comprises 68558 cases in 27073 documents, and the 2011 version comprises 61064 cases in 26939 documents. Several parameters have been varied during corpus construction, such as the length of a plagiarism case and the percentage of plagiarism per document; however, one of the most revealing parameters proved to be the type of paraphrase in a plagiarism case. In 2009, only automatic paraphrase generation approaches were employed, but 3671 manually generated paraphrases were introduced for the first time in 2010 and another 938 in 2011. Our rationale for doing so was to investigate the difference in detectability of manually paraphrased plagiarism cases compared to automatically paraphrased plagiarism cases.

As an aside, three heuristics were employed to automatically generate paraphrased plagiarism. Given a source passage t 1 , a paraphrase t 2 is created as follows (see Table I for examples).  X  Random Text Operations . The paraphrase t 2 is created from t 1 by shuffling, removing, inserting, or replacing words or short phrases at random. Insertions and replace-ments are taken from the document in which t 2 is to be inserted, which serves as part of the world knowledge  X  .  X  Semantic Word Variation . The paraphrase t 2 is created from t 1 by replacing words with one of their synonyms, antonyms, hyponyms, or hypernyms, chosen at random from
WordNet, which serves as part of the world knowledge  X  . A word is kept if none is available.  X  POS -Preserving Word Shuffling . The sequence of parts of speech of t 1 is determined and the paraphrase t 2 is created from t 1 by shuffling words at random while retaining the original POS sequence.
 To generate various degrees of similarity between sources and paraphrases, the heuris-tics have been adjusted by varying the number of operations made on a source passage, and by limiting the range of affected phrases. Unlike some of the more advanced para-phrase generation methods proposed in the literature [Barzilay and Lee 2003; Chevelu et al. 2009; Zhao et al. 2009], these heuristics do not produce well-formed paraphrases that are semantically equivalent to their sources. However, our rationale to use them, anyway, was to create texts that would be considered highly similar under a bag-of-words model, which most plagiarism detectors employ. Furthermore, most of the existing methods require significant amounts of training data, or they cannot be easily scaled, or both.

Figure 3 shows the detection precision and recall of 18 plagiarism detectors that have been evaluated in PAN 2010, dependent on the type of paraphrase: no paraphrasing, paraphrasing by machine translation from Spanish and German sources to English using the Google Translate API, 8 automatic paraphrasing, and manual paraphras-ing. The first type serves as a baseline for comparison on which the best performing detector of Kasprzak achieved more than 0.99 recall at 0.95 precision. Only two other detectors came close to these figures, whereas the others performed increasingly worse. On machine translated paraphrases, again, the detector of Kasprzak performed best, and only four of the others achieved noteworthy recall. On the automatic paraphrases, the performance characteristics of all detectors are very similar to those of the non-paraphrased plagiarism cases, while the recall is 30% lower on average. Regarding the manually paraphrased plagiarism cases, the precision performance varied greatly; however, more importantly, no detector achieved more than 0.28 recall. These results indicate that there is a wide gap between what automatic plagiarism detectors can detect and what plagiarists can do to disguise their plagiarism. Verbatim copying is easy to detect, whereas manually paraphrased plagiarism cases are quite difficult. However, automatically paraphrased plagiarism sits somewhere in the middle, since the aforementioned paraphrasing heuristics are obviously not sufficient to generate paraphrases with characteristics that come close to those of manual paraphrases. This section details our approach to crowdsourcing paraphrases and outlines our find-ings from applying it. As a result, we introduce the Webis Crowd Paraphrase Corpus 2011 (Webis-CPC-11). 9 The corpus comprises 4067 text samples with lengths ranging from 28 to 954 words, and their corresponding paraphrases. A further 3792 cases were rejected and form negative samples. After a brief introduction to Mechanical Turk, we detail the construction of the corpus within the two steps paraphrase acquisition and manual filtering. Creating a corpus of over 4000 paraphrases is still an enormous task, which cannot be easily accomplished by a single person in a reasonable time frame and at reasonable cost. Even if a single person were to write that many paraphrases, such a corpus would inevitably be biased (for example, by the writing style preferences of that person). Recently, crowdsourcing has become a viable alternative to alleviate these problems: distributing paraphrase generation to a crowd of workers reduces the time and costs to accomplish this task, and it introduces variance into the paraphrases. Our approach to scale up the generation of paraphrases is based on Amazon X  X  Mechanical Turk (AMT), a commercial crowdsourcing service [Barr and Cabrera 2006].

In short, AMT acts as a broker between workers and so-called requesters, who offer tasks and payment for their successful completion. Since real money is involved and workers have anonymity, the platform attracts scammers who try to get paid without actually working. Hence, requesters have the opportunity to check submitted results and reject those that are unsatisfactory. Besides saving money, rigorous result check-ing is of course a necessity to ensure quality. Crowdsourcing via AMT has gathered considerable interest in research and practice; it has also been demonstrated useful for proofreading, writing, and translating texts [Ambati et al. 2010; Bernstein et al. 2010]. As original texts we have used 4067 excerpts chosen at random from about 7000 books downloaded from Project Gutenberg. 10 The excerpts have been reviewed manually in order make sure they consist of passages of text which are amenable to be paraphrased, instead of, for instance, tables or enumerations.

A number of pilot experiments were conducted to refine the task design, the task description, and to determine the pay per task. Unsurprisingly the workers at AMT do not well understand technical terms like  X  X araphrase X  so that in the end, the task was described to them as follows [Potthast et al. 2010b].

The task Web page comprised the task description, the original text to be rewritten, and a multiline text input field beside it for the paraphrase. Furthermore, a background script monitored the workers, recording their keystrokes. This way it was ensured that workers did not simply copy and paste text as the paraphrase.

As an example, Table II contrasts an original passage and its paraphrase obtained from AMT. The workers have rewritten the original text quite thoroughly, replacing words with synonyms and rearranging the sentence structure while maintaining the original semantics.

Workers were required to be fluent in English reading and writing, and they were informed that every result was to be reviewed. Workers from all nationalities were accepted. A questionnaire displayed alongside the task description asked about the worker X  X  age, education, gender, native speaking ability, and whether the worker is a professional writer. Completing the questionnaire was optional in order to minimize false answers, but still, these numbers have to be taken with a grain of salt: AMT is not the best environment for such surveys. Table III overviews the worker demographics and task statistics. The average worker completing our tasks appears to be a well-educated male or female in their twenties, whose first language is English. Each paraphrase submitted by a worker has been checked manually, and those which did not meet our requirements have been rejected. Every submitted paraphrase has been read to ensure that it has the same meaning as the original, and that it is well-formed using proper English. However, since only parts of every paraphrase were checked this way, the reviewer was also given additional information to help with the decisions:  X  X ength ratio of paraphrase to original,  X  n -gram vector space model similarity of original and paraphrase for n  X  X  1 , 5 , 10 } ,  X  X umber of keystrokes with the full keylog, and  X  X ork time of the worker.
 No submitted paraphrases have been accepted that are grossly longer or shorter than the original, which eliminates many entailment relationships X  X ntailment is not the focus of this study. Moreover, paraphrases with high n -gram similarities for n &gt; 1 have been rejected, while the 1-gram similarity was expected to be high. Therefore we also rejected paraphrases which contained large chunks of unmodified text. Next, if a worker just copied and pasted some text as the paraphrase or worked extremely fast compared to others, the text was checked more closely in order to identify spammers. After completing the tasks on Mechanical Turk and after further double-checking by a student assistant, 4067 pairs of original text and paraphrase remained, whereas 3792 submitted paraphrases have been rejected. During manual reviewing, we observed many paraphrasing patterns applied by the workers. For instance, some workers rewrote the text sentence by sentence, whereas others combined or split them. Some exchanged words on a one-for-one basis, while others reorganized sentence structures. Some stuck closely to the line of thoughts of the original, while others made the paraphrase more concise or more verbose. These observations, however, were not recorded systematically, so some doubts remained as to whether the paraphrases produced by the workers are indeed passage-level para-phrases as opposed to simply a sequence of sentence-level paraphrases. Put another way, the question is whether workers mostly resorted to repeated sentence-level para-phrasing to solve the task or not.

To gain better insight into whether the accepted paraphrases are actually worthwhile on passage level, we have checked the corpus with regard to the following hypothesis: if most workers applied sentence-level paraphrasing to solve our task, then most para-phrases will consist of the same number of sentences compared to their respective originals. For each original and paraphrase pair, the number of sentences was counted using the OpenNLP maximum entropy sentence extractor for plotting as shown in Figure 4. From this experiment, we observed that 38.7% of the paraphrases have the same number of sentences as their originals, while the majority of paraphrases are of a different length. Moreover, the length distributions shown in the figure, which depend on the length of the originals, tell something about how much the paraphrases have been shortened or lengthened by the workers. Discounting errors made by the sentence extractor, and the fact that some pairs have the same length by coincidence, we can safely reject the hypothesis that workers mostly resort to sentence-level paraphrasing. Having introduced the Webis Crowd Paraphrase Corpus, we now study whether and how the corpus acquisition can be further automated. After acquiring paraphrases from a crowd, the next step is to review them for quality, which turns out to be a time-consuming task as well. Automating the quality assessment bears the potential of further scaling up corpus construction in the future.

We tackle this problem with machine learning, and consider the six often applied paraphrasing patterns collated by Boonthum [2004] as a working definition of what con-stitutes a  X  X ood X  paraphrase. These patterns are synonym use, changes between active and passive voice, changing word forms and parts of speech, breaking sentences down, replacing words with definitions or meanings, and varying sentence structures. How-ever, in the context of crowdsourcing paraphrases it is important to also define what constitutes a  X  X ad X  paraphrase or a nonparaphrase. From the submissions rejected during human assessment, we have identified cases of duplicates, near-duplicates, unrelated texts, empty samples, automated one-for-one word substitution, and doing something unrelated due to failing to follow instructions, such as attempting to improve the quality of the text instead of paraphrasing it. By the definition of paraphrases given in the introduction, many of these examples must be considered paraphrases (e.g., du-plicating a text is a trivial form of paraphrasing it), however, keeping them as positive examples in our corpus would spoil the goal of creating a useful resource for research. In our experiments, we hence investigate whether well-known similarity measures that are tailored to paraphrase identification can serve as features to distinguish between good and bad paraphrases, using the human assessments of approval and rejection as ground truth. To measure the similarity between the original samples and the accepted/rejected paraphrases, we use ten established paraphrase similarity metrics from the literature. For our analysis, we chose the Edit/Levenshtein Distance (normalized), Simple Word N-gram Overlap, Exclusive LCP (Longest Common Prefix) N-gram Overlap, the BLEU Metric, and the Sumo Metric by Cordeiro et al. [2007a], as well as the Trigonometric, Parabolic, Entropic, Gaussian, and Linear metrics by Cordeiro et al. [2007b]. We note our novel application of these metrics as they were previously used for recognizing existing paraphrases, and not for generating new ones. We do not repeat the details of these metrics, since they are already explained at length in the two aforementioned papers.

The results in Figure 5 indicate many differences between the accepted and rejected groups. First, the normalized Edit Distance for the rejected group is much higher than for the accepted group, which suggests that the workers creating the rejected paraphrases generally made less changes, resulting in higher similarity. Therefore, the time taken to complete the task proportional to the length of the paraphrase may be a feature of interest.

The Simple Word N-gram Overlap scores indicate that the rejected paraphrases contain less original content, as these scores are clearly higher. This observation is reiterated with the Exclusive LCP N-gram Overlap scores, but the difference is less profound, since this metric removes much redundancy from the former metric. Consid-ering that any matching 4-gram also includes two matching 3-grams, three matching 2-grams, and four matching 1-grams, only one matching n-gram of each type is taken for the Exclusive LCP N-gram Overlap metric.

The BLEU Metric is also based on n-gram comparisons. This metric takes into ac-count the difference in lengths of the samples, and applies larger penalties to larger discrepancies [Papineni et al. 2002]. Like the two previous n-gram metrics, the results indicate that there is less original content in the rejected paraphrases.
 The Sumo Metric also clearly separates the accepted and rejected paraphrase groups. A key feature of the Sumo Metric is that it is built with identifying duplicates and near-duplicates as a key design decision [Cordeiro et al. 2007a], which makes it ideal for evaluating the effectiveness of translation systems [Cordeiro et al. 2007b]. This is par-ticularly important for the Webis-CPC-11 corpus, as we need to deal with dubious cases dealing with duplicates, near-duplicates, and automated single-word substitutions. We believe the results shown reflect this quality. Note that the Sumo Metric would be less suited to the MSRPC and KMC corpora, as these are not concerned with duplicates and near-duplicates.

The remaining five metrics demonstrate high variance for the rejected paraphrase group. First, the long tails of the trigonometric, parabolic, and entropic metrics for the accepted paraphrases indicate that we are likely to be dealing with data that does not follow a normal distribution, as perhaps do many of the other measurement sets. Also, the high variance in these results highlights the difficulty of our task, as we need to be able to reject submitted paraphrases that present a very wide range of similarity scores. This finding suggests that the previous work where others have  X  X sed numer-ous thresholds to decide definitely whether two sentences are similar and infer the same meaning X  [Kozareva and Montoyo 2006] is not suitable for crowdsourcing, since almost all possible scores are appearing in the rejected paraphrase group. This does not change the fact that they may still be effective for noncrowdsourced paraphrase corpora, but the thresholds applied may not hold between corpora. This conclusion leads us to believe that machine learning is our best option for automatically classify-ing crowdsourced paraphrases as legitimate paraphrases or otherwise by pooling the similarity metrics together, as explored in the next section.
 Using the ten metrics explored in the previous section, we now explore using them as features for classifying crowdsourced paraphrases as legitimate or otherwise. We chose to test the five classification algorithms that we identified as having been applied to classification in the paraphrasing literature: decision trees [Fernando 2007; Wan et al. 2006], k-nearest-neighbor [Kozareva and Montoyo 2006; Wan et al. 2006], maximum entropy [Kozareva and Montoyo 2006; Malakasiotis 2009], naive Bayes [Wan et al. 2006], and support vector machines [Brockett and Dolan 2005; Dolan and Brockett 2005; Kozareva and Montoyo 2006; Qiu et al. 2006; Wan et al. 2006].

We ran ten iterations of a tenfold cross-validation experiment using the aforesaid classifiers for our experiments. Weka version 3.6.4 implementations for each were cho-sen as listed in the Table IV caption. We used the default Weka parameters for all classifiers with a few exceptions. First we wanted our classifiers to produce the best distribution of class probability estimates possible (for Section 4.3), so we built a lo-gistic model for the support vector machine, used the distance weighting measure for the k-nearest-neighbor classifier that generated the largest distribution, and used an unpruned decision tree. For the k-nearest-neighbor classifier, we experimented with k  X  X  1 , 2 , 5 , 10 , 20 , 50 , 100 , 200 , 500 , 1000 } and only report k = 50 for brevity, which had the highest positive predictive value at 0.25 true positive rate. It was clear that low values of k were poor for our data with noise. Table IV provides performance data for all five classifiers expressed in terms of confusion matrix data, positive predictive value (or precision), true positive rate (or recall), other error rates, and accuracy. With regards to our corpus construction goals, precision is the most important column in this table, as it describes the hit rate with positive samples. Negative samples are trivial to generate. Consequently, accuracy is a less suitable choice since it includes both positive and negative samples together. The Table IV results show that the k-nearest-neighbor classifier has the highest precision at 0.81 (the difference with the decision tree was sig-nificant ( p = 6 . 27  X  10  X  6 ) based upon a two-sample test for equality of proportions with continuity correction), but this is clearly not good enough for automating paraphrase acquisition. One option to improve this score is to trade away recall for precision. We now explore if this is indeed a viable and cost-effective proposition in the next section.
 Figure 6 shows the precision-recall trade-off for all five classifiers. For the generally highest precision k-nearest-neighbor classifier, at 0.25 recall we get 0.987 precision, at 0.5 recall we get 0.966 precision, and precision drops away more steadily for higher recall levels. Reading this the other way, if we want an automatically generated para-phrase corpus with these precision levels, we will need to discard 75% or 50% of our legitimate paraphrases respectively.

Other results show that the highest precision classifier at the lowest recall scores is unclear, but the results are visibly less reliable at this point, as it only takes very few false positives to disturb the initial trends shown in the graphs. The other anomaly is the decision tree classifier that has the poorest distribution of class probabilities, as can be seen by the straight line sections on the graph. Using an unpruned decision tree was the best option we identified for generating good probability estimates [Su 2007].
So far, we have very high precision results in Figure 6 at many reasonable recall levels, so for anyone who wants to perform a task similar to our own, our collection can be used as training data in order to eliminate the crowdsourcing filtering phase and achieve similar precision. On the other hand, others may want to build a completely different corpus, such as a sentence-level corpus instead of a passage-level corpus. New training data is needed in this case. The experiment demonstrated in Figure 6 uses a tenfold cross-validation experiment design based on a 90% training and 10% testing split. It is now of interest to explore if smaller training partitions produce results with similar precision, so that at least part of the crowdsourcing filtering stage can be automated. Figure 7 shows that precision does not vary much when the training set size is reduced incrementally down to only 25% (1965 samples), particularly for lower recall levels. For example, at 0.25 recall, precision drops from 1.000 to 0.982 (  X  0.018), and at 0.50 recall, precision drops from 0.990 to 0.952 (  X  0.038). The precision drop when 10% training data (786 samples) is used is, however, more profound.

Regardless if our data is used for training or if some new training data must be developed, it becomes necessary to analyze the monetary and time trade-offs. Table V shows a detailed breakdown of our expenditures for building WebIS-CPC-11 with each main task aggregated in the second column of Table VI. These costs are then compara-ble to a range of automated scenarios in the last six columns of Table VI. We note that the new  X  X iscarded HITs X  row is added to Table VI as we are proceeding with the plan to accept all bad HITs in Mechanical Turk, in order to easily eliminate most or all of the university employee time spent on manually checking off the HITs. The goal is for machine learning to replace this phase to save as much time and money as possible in building a corpus of acceptable paraphrases.

First, let X  X  compare the costs of our corpus (second column) with the costs associated with repeating our procedure to build similar corpora (third and fourth column) in Table VI. Building similar corpora assumes that the Webis-CPC-11 corpus is used for training data. When building Webis-CPC-11, we first note that we had zero expense for rejected HITs, as manual filtering was performed completely for building the ground truth that forms the experiments in our article. In order to save the manual filtering time, we have to pay the workers who did bad work ($1193.06). Furthermore, we incur extra costs for discarding the acceptable HITs that are assigned low class probabilities in the precision/recall trade-off ($875.58 or $992.05) and increasing the experiment size by that proportion. However, all of the manual filtering stage is saved with the automated method ($2782.92). Depending on the desired precision of the corpus, the cost savings are $967.24 or $850.77 for 0.950 and 0.980 precision, respectively, with 111 hours of saved time for the university research staff in both cases.

It is of course possible that a corpus dissimilar to Webis-CPC-11 is required, and that our corpus is not suitable for training data. In this scenario, it becomes necessary to do some portion of the filtering manually to generate sufficient training data for a machine learning classifier to process the remainder automatically. For the scenarios in the last four columns of Table VI we still incur the same cost for paying the workers for the bad work, and in addition the costs for the discarded HITs go up because these precision levels are achieved at lower rates of recall for lower amounts of training data. Furthermore, savings for the manual filtering step are now only partial (75% or 50%). As a result, Table VI shows that our expenses are generally quite neutral when using 25% training data ($25.93 saving and $275.47 loss respectively), but the costs start to become prohibitive for 50% training data (losses of $465.81 and $946.24 respectively). Nevertheless, we still have excellent time savings in all cases, so these results provide important new options for cost-time compromises. The starting point for our research was the observation that there is no benchmark corpus for paraphrases that are longer than a single sentence. However, passage-level paraphrasing is a frequent and naturally occurring phenomenon as in text plagiarism, for example. That such a benchmark corpus is necessary and that the quality of its para-phrases matters is one of the lessons learned at the PAN 2010 international plagiarism detection competition. Though the compilation of a new benchmark corpus follows a common two-step approach, namely, paraphrase acquisition followed by paraphrase filtering, we had to break new ground: for the first step no algorithms are at hand and we have applied crowdsourcing, which immediately raised the question whether the filtering step can be accomplished with a combination of the existing paraphrase analysis metrics.

This question can be answered with  X  X es X : in our experiments we achieved a preci-sion of 0.980 precision at 0.523 recall. In related work, we note that Wan et al. [2006] achieved 75% accuracy in a paraphrase recognition experiment using a support vector machine with 17 features on the MSRPC corpus. However, we explained that precision is the most appropriate metric, so our results form a new benchmark. Moreover, since our paraphrases provide a much higher variance in regard to sample length and vocab-ulary compared to single-sentence paraphrases, our classification model proves robust as well. With the approach presented in this article we have compiled the Webis-CPC-11 corpus comprising 7859 positive and negative pairs, which is now available to the public. 11
With regards to economics, we have excellent cost and time savings when our corpus can be used as training data in the construction of similar corpora. In the case where only the methodology can be repeated, but around 2000 training samples is required (about 25% of Webis-CPC-11), the costs are fairly neutral, but we still have excellent time savings. In future work, we propose to also conduct some machine learning on the bad workers, in order to try to recoup some of the money that was paid to them that they are not entitled to. Indeed, crowdsourcing could also be applied to the filtering phase, to provide another trade-off. Flagging borderline cases for manual review is also of interest.

An important avenue for future research is the analysis of the commonalities and differences between the classification of sentence-level paraphrases and passage-level paraphrases. From such an analysis we can learn to develop an even more robust clas-sifier and better understand the paraphrase recognition process. The MSRPC corpus is a good candidate for this work.

We can also consider the classification step as a one-class problem [Tax 2001] in-stead of a two-class problem. This is different to the view in previous paraphrasing work where it has been stated that  X  X araphrase recognition reduces to a two-class problem X  [Kozareva and Montoyo 2006]. We suggest that our accepted paraphrases represent the target class, and all rejected paraphrases represent the outlier class. We recommend this approach because we have a large variety of negative samples as dis-cussed earlier, potentially including some cases that may still be not well understood.
A final idea for future work would be to investigate empirically the question of paraphrasing versus entailment, to find which one is better for modeling plagiarism due to being more frequent in practice.

