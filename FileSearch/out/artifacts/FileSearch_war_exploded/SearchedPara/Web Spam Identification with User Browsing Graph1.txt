 According to a study in 2009 [1], 68% of Web users use search engines frequently and 84.5% regard search engines as a dominant way to discover built-up Web sites. Although a search engine usually returns thousands of results for a certain query, most search engine users only view the first few pages in result lists according to [2]. As a consequence, ranking position has become a major concern of internet service providers. Hence Web spam pages use various techniques to achieve higher-than-deserved rankings in search engines X  results, from which search engines and users suffers a lot. 
State-of-the-art anti-spam techniques usually make use of Web page features, either link-based anti-spam methods are based on hyperlink graph analysis. Well-known link-based detection algorithms include TrustRank [3], SpamRank[4], and so on. Generally, hyperlink analysis algorithms are based on two basic assumptions proposed by [5]: recommendation assumption and topic locality assumption. It assumes that if two pages (i.e., recommendation) and the two pages sh are a similar topic (i.e., topic locality). 
However, in practical Web environment, hyperlinks can be easily added or re-moved by Web page authors or even by users (for Web2.0 sites). As a consequence, Web is filled with spam and advertising links so the assumptions as well as the hyper-link analysis algorithms meet lots of troubles in current Web environment. These entire situations make the link graph unreliable data source for hyperlink analysis algorithms. In order to solve this problem, Liu et al. [6] constructed a  X  X ser browsing browsing graph is more reliable than hyperlink graph because users actually follow links in the browsing graph. In this paper, we study the effectiveness of user browsing graph on spam filtering. The results are compared with whole hyperlink graph, and an improved hyperlink graph is proposed, which combines user browsing information and hyperlink informa-tion. Comparative experimental results show that the improved hyperlink graph with user behavior data improves spam classification performance by 5% compared to the same classifier which uses the whole hyperlink graph. What X  X  more, the improved hyperlink graph contains much less vertices than the whole hyperlink graph, which will increase the efficiency of spam classifier. 
The rest of the paper is organized as follows. In Section 2, we review related work in spam detection. Section 3 introduces how to construct user browsing graph and combined graph. Section 4 describes the calculation of TrustRank on user browsing graph and the distribution of the TrustRank values. Section 5 compares the perform-ance of the TrustRank algorithm on several different graphs in spam detection. Conclusions and future work are given in Section 6. Most spam detection approaches utilize link and/or content information to help detect formation [12]. Gy X ngyi et al . [3] propose a algorithm called TrustRank for link spamming detection. Wu et al. [9] use trust and distrust propagation through web pages by building up a classification model which combines multiple heuristics based on page content analysis. 
Recently, the wisdom of the crowd is paid much attention in Web search re-searches, e.g. [6, 7, 8]. For instance, Liu et al. [6] constructed  X  X ser browsing graph X  with Web access log data. They proposed a page importance estimation algorithm called BrowseRank which performs on the user browsing graph. It is believed that the Bayesian Learning, which can detect newly-appeared and various kinds of spam. 
Most linking based spam detection algorithms are based on hyperlink graph. How-ever, with the explosive growth of Web pages, the whole hyperlink graph is too large linking analysis algorithm on different graph using the approach in [8]. 3.1 Web Access Log Data Set As search engines have developed, Web-browser tool bars have become more and more popular. Most toolbar servers collect anonymous click-through data from users X  browsing behaviors, which is used in extensive studies of Web search by more and more researchers, e.g., [7] adopts click-through data to improve ranking performance, graph, because of its low cost and no-int erruption to users. Information shown in Table 1 can be recorded using browser toolbars by commercial search engine systems. 3.2 User Browsing Graph Construction the edge set. The construction process is described as Figure 1: 
After the construction process, V includes all Web pages visited by users during the period when access logs were collected; and E records the users X  browsing behaviors. Each edge in E is also assigned a weight that represents how many times Web users visited site B from site A . 
With the help of a widely-used commercial Chinese search engine, Web access logs were collected from Aug.3 rd , 2008 to Sep. 2 nd , 2008 (30 days). Over 1.4 billion click-through events on 4.2 million Web sites were recorded in these logs. In our experiments, a site-level UG (V, E) was constructed with V containing 4,252,495 Web sites and E containing 10,564,205 edges. We constructed a site-level UG (V, E) data. 
In order to show the advantages of user browsing graphs compared with over hy-perlink graphs, we also construct a hyperlink graph for comparison. With the help of the same search engine which collected Web access log for us, we obtained hyperlink graph constructed by the log data which contains hyperlink relations of over 3 billion tracted-HG(V,E) . This graph was also constructed as a site-level graph to compare with the user browsing graph and contains 139,125,250 edges. 4.1 Trustrank Algorithm In 2004, Gy X ngyi et al . [3] proposed TrustRank algorithm to semi-automatically sepa-rate reputable from spam pages. According to [3], TrustRank relies on an important empirical observation called approximate isolation of the good set: good pages seldom point to bad ones. In other words, TrustRank assumes that if page A pointing to page B reliabilities of the link relationship, which TrustRank computes on. 4.2 Seeds Set Selection At the very beginning of TrustRank algorithm, we need to select a seed set. The pur-pose of seed selection is to identify desirabl e pages for the seed set. Pages in the seed set should be useful in identifying additio nal good pages. Two strategies for seed set selection were described in [3]. According to these two strategies, we defined the following seed set selection criteria for TrustRank on user browsing graph. z Out-degree of a seed site should be neither too high nor too low, i.e., higher than z The PageRank value of the seed site should be higher than P3 to ensure the z Search Engine cannot be seeds because links out from SE don X  X  necessarily z The seed set should not contain blog, forum sites and Web 2.0 sites. Because Web sites in the link graph. These Web sites also contain appropriate amount of reliable out-links. With this method, we select altogether 1,153 Web sites to form the seed set. 4.3 Trust Score Calculation After seed set construction, we can perform TrustRank algorithm on user browsing both in user browsing graph UG(V,E) and hyperlink graph extracted-HG(V,E) , re-spectively. In UG(V,E) , we can get 3,951,485 sites X  TrustRank values, but only 2,658,345 sites X  value can be obtained by algorithm on extracted-HG(V,E) . There are fewer sites which cannot be computed in UG(V,E) than in HG(V,E) . sampled 15,941 out of 1,594,150 (1%) non-valued sites in the extracted-HG(V,E) and checked them manually finding that most of them were in low-quality, spam or non-GBK encoded sites. 4.4 Distribution of TrustRank Scores Distributions of TrustRank scores obtained from UG(V,E) and extracted-HG(V,E) are shown in Figure 2.(a) and 1.(b). Because TrustRank values are little decimal fraction, we computed the value LT = -Log 10 (TrustRank(S)) (S is a Web site) for all sites in the two graph. 
From Figure 2, we find that most sites X  TrustRank scores are between 10 -6 and 10 -12 in both graphs, but there are also some differences between these two distribu-while a number of sites in extracted-HG(V,E) get LT scores less than 8 or larger than sparser than hyperlink graph. 5.1 Experiment Settings  X  X rowseRank X ) could get better performance on user browsing graphs in page quality estimation than state-of-the-art link analysis algorithms which perform on hyperlink graphs. We want to find out whether this improvement comes from different algo-rithm design or the different graph structures. To answer this question, we build four graphs and compare how TrustRank algorithm performs on them. The details of the four graphs are shown in Table 2. 
After performing TrustRank algorithm on these graphs, we adopted two methods to evaluate the performance of Web spam identification. The primary one is based on ROC/AUC metric which is often used to evaluate performance of machine learning algorithms and classifiers, e.g . [11]. The other one is based on analysis into the distri-bution of TrustRank scores for Web spam sites on different graphs. 
In order to evaluate the performance of Web spam detection, we construct three access logs at different time points and annotated with the tags shown in Table 3 by 3 product managers from a search engine company. The amount of Web sites in these 39% are  X  X igh quality X ; 19% are  X  X eb spam X  or  X  X llegal X  and the others are anno-tated as  X  X ow quality X ,  X  X on-GBK encoded X . As for the spam detection evaluation process, illegal sites are also regarded as spam sites because illegal sites usually adopt SEO or spamming techniques to improve their rankings in search result lists. 5.2 ROC/AUC Test ROC/AUC metric is a method which is commonly used to evaluate performance of classifiers and it is adopted by many previous spam detection researches such as Web spam challenge workshop (http://webspam.lip6.fr/). With the Web spam annotation test sets proposed in Section 5.1, we chose ROC (Receiver Operating Characteristic) curves and corresponding AUC (Area under the ROC Curve) values to evaluate the performance of the TrustRank algorithm. The AUC has an important statistical prop-will rank a randomly chosen positive instance higher than a randomly chosen negative instance. 
Table 4 shows the spam page identification performances of BrowseRank algo-rithm [6] on UG(V,E) and TrustRank algorithms on 4 different graphs which we constructed in Table 2 as well as the BrowseRank algorithm on UG(V,E) . 
From Table 4 we can find several interesting results. Firstly, we can see that in all spam page identification. A possible reason is that, although Extracted-HG (V,E) shares a same vertex set of UG(V, E) , the edges among the vertexes inherit from the whole hyperlink graph, which are denser and more complete than UG(V,E) . So extracted-HG(V,E) can get better performance than UG(V,E) for TrustRank algorithm to identify spam sites. What X  X  more, there is another result to support this explanation. CG(V,E) , a combination of UG(V,E) and extracted-HG(V,E) , which contains all edges in them, results better than UG(V,E) . It means both user browsing graphs and hyper-link graph can provide useful information that the other graph does not contain. 
Secondly, in each of the three test sets, we found that BrowseRank doesn X  X  perform as well as TrustRank on user browsing graph for the task of Web spam identification. Liu et al proposed in [6] that BrowseRank performs better than TrustRank and Pag-eRank in spam fighting. However, according to our experimental results, TrustRank out-performs BrowseRank when both algorithms are performed on user browsing better than specially-designed algorithms on user browsing graphs. Moreover, Extracted-HG(V,E) , as a sub-graph of whole-HG(V,E) ( extracted-plained by the fact that extracted-HG(V,E) can be regarded as the user-accessed part of Whole-HG(V,E) . It shares the same vertex set of UG(V,E) and therefore reduces possible noises (i.e., unreliable Web sites) in the whole hyperlink graph. 5.3 Distribution of TrustRank Scores for Web Spam Sites In order to compare spam detection performance of TrustRank algorithm on different graphs, we examined the distribution of spam sites X  TrustRank scores on both graphs. As shown in Section 4.4, we computed the value LT = -Log 10 (TrustRank(S)) (S is a Web site.) for Web sites in both graphs. the TrustRank scores. Experimental results are shown in Table 5. Quality score of each sors give different annotation, one other assessor will join in to vote for net result. 
From Table 5 we can see that most sites with highest TrustRank scores are high-quality ones. Meanwhile, most sites with lowest TrustRank scores are low-quality or spam ones. It means that TrustRank scores can represent the quality of Web sites and this validates the conclusion of [3] that Tr ustRank algorithm is effective in the detec-tion of Web spam. 
We further looked into the distribution of TrustRank scores while the algorithm was performed on different graphs. Figure 4 provides a side-by-side comparison of TrustRank on different graphs w.r.t ratio of bad sites in each bucket. In order to get a suitable sample set with enough data points, we adopted the sampling method pro-posed by Gy X ngyi et al . in [3]. 
Firstly, We generated the list of sites in decreasing order of their TrustRank scores on user browsing graph UG (V, E) , and we segmented it into 20 buckets. Each of the buckets contained a different number of sites, with scores summing up to 5 percent of the total TrustRank score. Therefore, the first bucket contained 53 sites with the high-est TrustRank scores, bucket 2 contains the next 126 sites, while the 20th bucket con-tains 3.9 million sites that are assigned the lowest TrustRank scores. 
After that, we constructed a sample set of 1000 sites by selecting 50 sites randomly from each bucket. We manually examined the sampled Web sites and annotated whether they are spam or not. As for TrustRank scores obtained from Whole-HG (V, E) number of sites as buckets on UG (V, E) . The vertical axis of the graph corresponds to stance, we can derive from Figure 3(a) that on the graph Whole-HG (V, E) , 30% of the Web sites in TrustRank bucket 9 are not high quality ones. From Figure 3 (a)-(c) we see that TrustRank algorithm can perform better on UG(V,E) and Extracted-HG(V,E) than Whole-HG (V, E) graph. In particular, we note tracted-HG(V,E) , while there is a marked increase in spam concentration in the lower (V, E)  X  X  bucket is Web spam or low quality ones. It can be explained by the fact that hyperlinks in Whole-HG(V,E) are unreliable and the trust propagation process is therefore not reliable, either. 5.4 Performance of Spam Identification on Filtered User Browsing Graphs According to the construction process of the user browsing graph given in section 3.2, A. It is usually believed that more visit times mean more reliable recommendation. 
For a given threshold N, we reduce the edges in the user browsing graph whose weights are no more than N. By this means, we get a graph UG_FN(V,E) . 
We calculated TrustRank scores for sites in UG_F1(V, E) and UG_F3(V, E) re-spectively, with the same parameter MB = 20 (iteration times),  X  = 0.85 (decay factor) in algorithm. In UG_F1(V,E) , we got 1,586,142 sites X  TrustRank values, nevertheless only 774,029 sites X  value can be obtained by algorithm on HG_F3 (V,E) . Compared which cannot be computed in the UG_F1 (V, E) than in unfiltered user browsing graph. 
In order to find out whether filtration can improve reliability of user browsing graphs, we compared how TrustRank algorithm performed in the task of spam identi-fication on the three graphs: UG (V,E) , UG_F1 (V,E) and UG_F3 (V,E) . 
Firstly, we adopted the approach proposed by Liu Y. et al. [8] as the baseline, which identified Web spam with user behavior features extracted from Web access logs. Second, we calculated TrustRank scores on three Graphs and syncretize them to the results of baseline approach. Test set includes 400 sites and we adopted the third set which contained 1000 sites described in section 5.1 as training set. Table 6 shows tered graphs. 
The results in Table 6 show that the accuracy rate increases with the rising thresh-old N. This means TrustRank algorithm performs better on filtered user browsing graphs, which proves  X  X ore visit times, more reliable recommendation X . What X  X  time and storage. With the swift growth of Web, we believe that filtered user brows-ing graphs can be helpful in improving the efficiency of spam identification. calculate some Web sites X  TrustRank scores. By merging TrustRank and user behav-ior features with learning algorithm proposed in [8], we calculated sites X  P(Spam) scores, representing probabilities of being Web spam. Then we sorted the Web sites in decreasing order of their P(Spam) scores. Table 7 shows how many top-ranked sites X  TrustRank scores can be calculated in UG-F1(V,E) but not in UG-F3(V, E) . 
From Table 7 we can see that in the 5,000 sites at the top of the possible spam list, there are only 8 sites whose TrustRank scores can not be calculated on UG-F3(V,E) graph. It means that although we filtered some low weight edges from the user brows-ing graph, it does not influence the performance of spam detection a lot. Current search engines are seriously threatened by malicious Web spam that attempts to obtain unbiased ranking in search result lists. State-of-the-art hyperlink-based anti-spam techniques do not work well on whole hyperlink graph because practical Web is in Web contents. In this paper, we focused on spam detection techniques with user browsing graphs. Firstly, we constructed a user browsing graph with Web access log data which con-tained huge amount of user click-through information. Secondly, we adopted the with user behavior features extracted from Web access logs, and calculated the Trus-tRank scores on different Graphs and syncretize them to the results of baseline ap-proach. Then we compared the spam identi fication performances of above approach with TrustRank algorithm on different graphs: UG(V,E) , CG(V,E), extracted-HG(V,E) and whole-HG(V,E) . 
Experimental results show that, although vertices of the user browsing graph is much less than whole hyperlink graph, which means higher efficiency, the improved hyperlink graph with user access information ( extracted-HG(V,E) ) performs better than other graphs. From this result, we see that both of user browsing graph and hy-perlink graph can provide useful information. And combining the vertex set of user browsing graph with edge set of hyperlink graph can improve performance in task of spam detection. What X  X  more, TrustRank also outperforms BrowseRank algorithm while both algorithms were performed on user browsing graph in our results. We believe that the user browsing graph is useful for link-based spam detection algo-rithms such as TrustRank. 
There are still several technical issues which need to be addressed as future work: reliability of the user browsing graph and corresponding spam detection performance. In graph construction process. (2) According to our experimental results, UG(V,E) and Extracted-HG(V,E) achieve better detection performance than whole-HG(V,E) . What X  X  more, sometimes Extracted-HG(V,E) , with UG(V,E)  X  X  vertexes and whole-HG(V,E)  X  X  edges, has the best effectiveness out of all graphs. It will be interesting to look into how to yield the greatest returns on investment of user browsing graph. 
