 The automatic summarization of long-running events from news steams is a challenging problem. A long-running event can contain hundreds of unique  X  X uggets X  of information to summarize, spread-out over its lifetime. Meanwhile, infor-mation reported about it can rapidly become outdated and is often highly redundant. Incremental update summarization (IUS) aims to select sentences from news streams to issue as updates to the user, summarising that event over time. The updates issued should cover all of the key nuggets con-cisely and before the information contained in those nuggets becomes outdated. Prior summarization approaches when applied to IUS can fail, since they define a fixed summary length that cannot effectively account for the different mag-nitudes and varying rate of development of such events. In this paper, we propose a novel IUS approach that adap-tively alters the volume of content issued as updates over time with respect to the prevalence and novelty of discus-sions about the event. It incorporates existing state-of-the-art summarization techniques to rank candidate sentences, followed by a supervised regression model that balances nov-elty, nugget coverage and timeliness when selecting sentences from the top ranks. We empirically evaluate our approach using the TREC 2013 Temporal Summarization dataset ex-tended with additional assessments. Our results show that by adaptively adjusting the number of sentences to select over time, our approach can nearly double the performance of effective summarization baselines.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval Keywords: Temporal Summarization, Adaptive Models, Machine Learning
When a significant event occurs, it is reported in a va-riety of streams such as newswire, microblogs/blogs and forums. However, given the large number of these media Figure 1: An illustration of Incremental Update Summarization (IUS) task over a time-ordered doc-ument stream. streams and the vast quantities of articles/posts produced each day, monitoring an event poses a challenging problem for end-users. Multi-document summarization (MDS) tech-niques have been proposed to tackle event summarization. These techniques generate a concise, on-topic summary from a set of related documents [20], often by extracting infor-mative sentences from those documents [5, 7, 15, 19, 21, 26]. However, these approaches are designed to retrospec-tively summarize an event given a (high-quality) set of on-topic newswire articles about it and hence are not suitable to summarize an ongoing event over time. Some MDS ap-proaches were later extended to the task of update summa-rization [10], where the aim is to produce a fixed-length  X  X p-date X  summary about the event given an initial summary and a set of documents containing new information about the event. However, due to the fixed-length nature of the update summary produced, these approaches are not well equipped to summarize long-running events, particularly for use-cases that require updates to be issued in a timely manner.
For instance, consider a user that had heard about the gar-ment factory fires that took place in Pakistan 1 and wanted to access a summary of that event, which automatically up-dates over time with new content. For this event, we con-sider there to be a set of unique pieces of information that the user would like to know about e.g. that it was  X  X ak-istan X  X  deadliest indus trial disaster X , or that the  X  X otal killed http://en.wikipedia.org/wiki/2012_Pakistan_ garment_factory_fires was 315 X . However, this event ran over a 6-day period, with many important nuggets emerging after the initial event, e.g. that a  X  X ake safety certificate had been obtained [by the factory owners] to fulfil developed countries demands X . De-ploying a traditional MDS system using only the first articles would result in many missed nuggets, since they will occur much later in the event timeline. Meanwhile, iteratively ap-plying an update summarization system will result in update summaries that contain irrelevant and/or redundant content during periods when no new information emerged. Hence, we need an alternative summarization approach that can automatically extract novel updates from news streams in a timely manner, while minimising irrelevant, uninformative or redundant content.

In this paper, we introduce the task of Incremental Up-date Summarization (IUS), which aims to produce a series of sentence updates about an event over time to issue to an end-user tracking the event. Figure 1 gives an example of an IUS system. As we can see from Figure 1, documents pub-lished over time relating to an event are first processed by a traditional update summarization system at regular time in-tervals, producing fixed-length update summaries comprised of sentences. However, during periods when no new pieces of information emerge, the update summary produced may contain irrelevant or redundant sentences. The aim of an IUS system therefore is to take the update summaries and extract only those sentences from them that are on-topic and provide new previously unseen information, such that they can be issued to an end-user as updates about the event. IUS is highly useful for users wanting short timely updates pushed to them, for example in the form of an RSS feed or updating timeline.

To tackle IUS, we propose a supervised approach, where we treat sentence selection from the update summary as a rank-cutoff problem. Here the aim is to predict  X  based upon the current update summary an d any sentences previously issued as updates  X  how many sentences from the current update summary to issue to the end-user. We train a re-gression model that combines over 300 features, describing the prevalence of the event, the novelty of content contained, and overall sentence quality across the sentences in the in-put update summary. These features are used to balance the cost of selecting a deeper cutoff in terms of returning redundant co ntent against the risk of missing important in-formation by selecting a shallower cutoff.

The contributions of this paper are three-fold. First, we propose a new model for IUS that can adaptively adjust the amount of summary content to select over time. Sec-ond, through experimentation using the TREC 2013 Tem-poral Summarization dataset  X  expanded with over 22,000 new assessments to combat incompleteness [3] in the original dataset  X  we show that our proposed approach can improve IUS performance by up-to 47% in comparison to the up-date summarization baselines tested. Third, we provide an in-depth analysis, identifying the most informative features and highlighting directions for future enhancement.
The remainder of this paper is structured as follows. Sec-tion 2 provides a background into prior works examining multi-document summarization and summarization over time. In Section 3, we define the IUS task, while Section 4 details our proposed approach to tackle it. Section 5 describes our experimental setup in terms of the dataset, the development of additional assessments, training regime and evaluation measures, while in Section 6, we detail our results and anal-ysis. Concluding remarks are provided in Section 8.
As described above, the IUS task that we introduce builds upon traditional multi-document/update summarization sys-tems from the literature, using them to produce intermedi-ate summaries over time. In this section, we describe prior works in the field of multi-document summarization (Sec-tion 2.1) and update/temporal summarization (Section 2.2).
Multi-document summarization (MDS) approaches take as input a set of documents about a topic to be summarized and produce a (typically fixed length) summary of those doc-uments. The MDS task was investigated at both the Docu-ment Understanding Conference (DUC) 2 and Text Analysis Conference (TAC) [10]. MDS approaches can be categorised as either extractive  X  where unmodified sentences from the input documents are selected for inclusion into the sum-mary [5, 7, 15, 19, 21, 30, 33, 36], or abstractive  X  where new sentences are generated from the input documents, e.g. via sentence compression or information fusion techniques [15, 32, 36]. Summarization approaches that also take as input a query (in a similar way to a Web search engine) relating to the topic are referred to as  X  X uery-focused X  approaches. The IUS task that we introduce takes as input, summaries produced by a query-focused multi-document summariza-tion system and extracts sentences from them to produce updates for long-running events, issuing them to users who wish to track those events.

Extractive summarization techniques can be defined in terms of three stages [20]: the generation of an intermediate representation of the input documents, e.g. the identification of key terms [17]; the scoring of each sentence with respect to its preference for inclusion into the summary, e.g. by top-icality [29], text quality [26] or aspect coverage [19]; and the selection of sentences from the ranked list, e.g. selecting the top n , or applying a redundancy sensitive technique such as MMR [4]. One of the most widely used approaches to score sentences for inclusion into a summary is clustering with re-spect to the centroid of the sentences within the input docu-ments [18, 23], thereby selecting those sentences most central to the topic first. Early systems that take this approach in-clude NeATS [18] and MEAD [23]. Other approaches score sentences with respect to only highly informative terms [7, 17, 21] or based upon their coverage of latent sub-topics extracted from the input documents [19]. More recent ap-proaches have used graph-based methods that model tex-tual/aspect overlap between sentences, and in some cases in-fer sentence hierarchies from input documents [5, 11, 22, 33]. For example, LexPageRank [11] builds upon the well known PageRank algorithm to identify the most important sen-tences in the graph. Finally, machine-learned sentence rank-ing approaches have been shown to be effective for MDS [15, 16, 26, 32]. For instance, the Pythy summarization system proposed by Toutanova et al. [26] linearly combines a series of weighted summarization features to score each sentence, while Li et al. proposed a structural learner that jointly op-timises diversity and coverage when scoring [15]. Most re-cently, Wang et al. [32] used a learning-to-rank technique, combining sentence features and the output scores of other summarization approaches for sentence scoring. In our later experiments, we leverage some of the extractive summariza-tion approaches described above to produce the summaries that our IUS approach takes as input, namely the learning-http://www-nlpir.nist.gov/projects/duc/ to-rank approach by Wang et al. [32] and the classical Sum-Basic [21], SumFocus [26] and Classy [7] approaches.
One limitation of MDS is that it is retrospective in na-ture, in that there is an assumption that all of the relevant documents about the event are available as input before-hand [1]. The task of update summarization was designed to address this issue through the generation of further fixed-length  X  X pdate X  summaries from documents published later in time [10]. An update summarization system takes as in-put the original summary of the event and a new set of documents, with the aim to produce an update summary that contains only new information not already contained within the original summary. Prior approaches to tackle up-date summarization generally apply an existing MDS tech-nique over the new documents to produce a candidate rank-ing of summary sentences [10]. They then use a redun-dancy removal technique to remove sentences that are simi-lar to those already contained within the original summary. Effective redundancy removal techniques include using an upper-bound on the pair-wise sentence similarity [9] or using MMR [28] to identify and filter out excessively similar sen-tences. In our later experiments, we use a learning-to-rank approach to produce an initial summary at periodic inter-vals. Then, following [9], we employ an upper bound  X  on permissible similarity between sentences, removing any that exceed this upper bound, forming the update summaries that our IUS approach takes as input.

An alternative approach to event summarization over time is timeline generation, where the aim is to produce a times-tamped list of updates covering the most important nuggets about that event. Swan and Allan [25] proposed an early approach to timeline generation by extracting clusters of named entities and noun phrases relating to events over time. This approach was later extended to select related sentences for each date [1]. Chieu et al. [6] also examined the creation of sentence timelines for news events. They studied the effect of using different centrality and bursti-ness estimates to rank sentences for inclusion into a timeline. Recently, Yan et al. [35] investigated evolutionary timeline generation (ETS) for an event. They extracted sentences from newswire articles crawled from the Web at 24 hour in-tervals, using a combined utility function that incorporates relevance, coverage, coherence and diversity (with respect to sentences selected previously) for sentence scoring. A timeline is iteratively updated over time by selecting sen-tences from each interval based upon local and global opti-misations aiming to maximise the summary utility. IUS is similar to ETS in that both aim to produce a timeline of sentences summarising an event. However, rather than pro-ducing a high precision summary with respect to only the most important updates at the end of each day, IUS focuses on timely updating about the event throughout each day.
In this paper, we tackle the problem of how to extract sentences about a long-running event from news streams to issue to a user wishing to track that event. However, avoid-ing the selection of redundant or irrelevant content is chal-lenging, since the amount of new content relating to an event varies over time. For example, Hurricane Sandy 3 started out http://en.wikipedia.org/wiki/Hurricane_Sandy Figure 2: The number of manually identified nuggets about the Hurricane Sandy over time. as a tropical storm in the Caribbean but grew into a huge event as it strengthened and passed over densely populated areas along the U.S. east coast. To illustrate, Figure 2 shows the distribution of manually identified information nuggets about Hurricane Sandy over the lifetime of the event. As we can see, there is a large burst of updates around the 29th of October, when Sandy made landfall on the U.S. east coast. Hence, to summarize long-running events effectively, we need an approach that can adaptively alter the amount of new sentences extracted over time.

To tackle this issue, we introduce the task of incremental update summarization (IUS), which aims to issue periodic updates about an event to a user in a timely manner. In particular, we use the following definitions: Long-running Event e : A newsworthy happening in the world that was significant enough to be reported on over multiple days.
 Event representation (Query) Q : A short textual repre-sentation of the long-running event provided by an end-user. For example, a user wanting to t rack the progress of Hurri-cane Sandy might issue the query  X  X urricane sandy X . Time Interval t : A period of time during the lifetime of an event e . An event is split into a series of equal size time intervals t = { t 0 ,t 1 ,t 2 ... } .
 Update Summary S t : A fixed length query-focused up-date summary for the time interval t and event representa-tion Q , produced by an update summarization system like those described in Section 2.2. Each update summary S t is comprised of a ranked list of sentences s  X  S t published during time interval t . The ranking criteria for sentences in S t is relevance with respect to Q and the intrinsic informa-tiveness/quality of those sentences.
 Sentence Update u : A sentence to be issued to the end-user about the event from an update summary S t . Issued Updates U t : The timeline of all sentence updates issued to the user before t .

Using these definitions, we formalise the IUS task as fol-lows. Given a query Q issued by a user about a long-running event e to be tracked, for each time interval t over the life-time of e , the IUS system takes as input an update summary S t about e and selects sentences from it to issue as a series of sentence updates u (given the previous selections U t ).
We propose a new approach that treats IUS as a rank-cutoff problem , inspired by prior work in the Web search do-main on determining when to stop reading a ranked list [2]. Assuming that we are at time interval t within an event e . If there is new information available in the current up-sentences to issue as updates, achieved by selecting a deep rank-cutoff for sentences within S t . However, during time in-tervals with low activity, the update summary S t will mostly contain redundant or irrelevant sentences, particularly in the Figure 3: Illustration of how prevalence and novelty relates to different event states. lower ranks. Hence, in this case, an IUS system should select a shallow (or in the extreme a zero-rank) cutoff. The aim of our proposed approach therefore is to predict the optimal rank-cutoff for S t , denoted as  X  t .

To accurately predict the optimal  X  t for a given update summary S t , we leverage two concepts: prevalence  X  X he proportion of a population found to belong to a class; and the novelty  X  the degree to which a population is similar to a second population. In the context of IUS, we define prevalence to be the proportion of the update summary S t to arank r (in S t ) that relates to the event. Meanwhile, novelty is the proportion of sentences in the update summary S t to arank r that are similar to sentences previously issued as exhibits a high degree of both prevalence and novelty. To illustrate, Figure 3 shows how prevalence and novelty map to different states of an event. As we can see from Figure 3, only in the case of high prevalence and novelty is there likely to be new information available that we would want to issue as updates to the user.

From a user perspective,  X  t is a prediction of where to stop reading the update summary S t produced for time interval t given the sentence updates read previously ( U t ). A non-zero  X  like to read. In contrast, a predicted  X  t value of 0 indicates that no new information is available and hence, no sentence updates should be issued. To illustrate, consider the follow-ing update summary S t containing four sentences about the aforementioned Pakistan factory fire disaster, assuming that we have not issued any sentence updates so far:
The first sentence in this ranking contains two useful pieces of information, namely that the event was Pakistan X  X  dead-liest industrial accident and that Pakistan has poor safety conditions. Hence, up-to rank 1, the event is prevalent (1/1 sentences are on-topic) and the summary is novel (1/1 sen-tences contain new information). The second sentence also provides more new on-topic information about the safety conditions, i.e. up-to rank 2, the event is prevalent (2/2 sen-tences) and the summary is novel (2/2 sentences). On the other hand, the third sentence, while containing novel infor-mation, is not on-topic. As a result, up-to rank 3, we can characterise the update summary as having medium preva-lence (2/3 sentences) and high novelty (3/3 sentences). Fi-nally, the fourth sentence is on-topic but redundant given sentence 2. Hence, up-to rank 4, we describe the summary as having medium prevalence (3/4 sentences) and medium novelty (3/4 sentences). Given that we want high prevalence and novelty, the optimal  X  t for this ranking would therefore be 2. Of course, during subsequent time intervals when we have already issued updates to the user, these would also need to be considered when estimating the novelty of an update summary.
To perform the actual prediction of  X  t we use a supervised regression model that combines multiple features extracted from the update summary S t and the previously issued sen-tence updates U t , describing the event prevalence and nov-elty within S t . Under this approach, a series of training instances  X  each comprised of an update summary S t ,pre-viously issued sentence updates U t and a pre-calculated op-on a continuous scale. This model can then be applied to predict  X  t values for each time interval t for unseen events.
To produce this supervised model, there are two prereq-uisites. First, a set of prediction features about each update value) and those without (where a low or zero  X  t should be predicted). Second, a loss function that balances the redun-dancy cost of issuing the top r sentences from an update summary as sentence updates, against the added value in terms of new information contained (the best r value found lowing two sections.
To facilitate accurate predictions of  X  t , we define 333 pre-diction features. For clarity, we describe each feature with respect to three distinct properties of that feature, namely: the aspect modelled; the input source; and the feature depth. We summarize these three properties below.
 Aspect Modelled: Represents the property of the input that the feature aims to captur e. Our features each belong to one of three broad aspects, namely: prevalence features ; nov-elty features ;or quality features . Prevalence features mea-surethedegreetowhichtheevent(asspecifiedinthequery) is represented in the input, while novelty features estimate the extent to which the input contains previously unseen information. On the other hand, quality features measure the writing quality and cohesiveness of the input. The idea of using quality features for rank-cutoff prediction is that should the sentences within the input be of poor quality, then they are less valuable for inclusion and hence a smaller  X  Input Source: Denotes whether the feature is calculated using the update summary S t , the previously issued sentence updates U t ,orboth.
 Feature Depth: Each feature is defined at the level of the update summary S t toarank r . However, there are two ways that a feature can be calculated: via aggregation of sentence-level evidence; or when considering all terms from sentences between rank 1 and r as belonging to a single document. For instance, consider the tf-idf similarity of a sentence with respect to the user query. The average of these similarities over all sentences in the update summary S whole to the event (in this case, producing a feature similar to a post-retrieval query performance predictor [13]). Fea-ture depth indicates whether a feature is calculated as the average of the scores for each sentence to a rank depth r (denoted  X  X VG[1.. r ] X ), or calculated from the virtual docu-ment produced by combining the sentences to rank depth r (denoted  X  X D[1.. r ] X ).

Table 1 summarizes the features that we use to predict  X  t , separated by the feature aspects. Note that during learning, features calculated from different events and time periods will be compared. Hence, to make these features compara-ble, we apply a standard feature normalisation at the gran-ularity of each time interval as follows: where f is a feature and min ( f, S t )and max ( f, S t )arethe minimum and maximum observed scores for f across the sentences within S t , respectively. 1: Input 2: Output define sentences previously selected, U t = { X  X  define training instance set, I e = { X  X  for each time interval t loop return I e Figure 4: Training instance generation pseudo-code.
To train the regression model using the features described above, we define a loss function that balances the redun-dancy cost of issuing the top r sentences from an update summary as sentence updates, against the added value in terms of new information contained. takes as input the r value to try. It estimates the expected loss for issuing the top r sentences from S t (denoted S t [1 ..r ]) as follows: ( S t ,U t ,r )= where Gain measures the amount of new information added by each sentence in S t [1 ..r ]and U t . Coverage measures the total number of information nuggets that are covered by S is the harmonic mean between these two performance mea-sures. Notably, Gain and Coverage can be considered anal-ogous to the information retrieval precision and recall met-rics, respectively within the context of summarization. Gain measures the number of  X  X elevant X  updates (where relevance incorporates additional novelty, quality and timeliness com-ponents) issued over all updates, while Coverage measures the number of  X  X elevant X  updates issued over the total num-ber of  X  X elevant X  updates available. Hence, the cost function can be seen as playing a similar role to the popular F 1 mea-sure [27], penalising systems that place too much focus on either Gain or Coverage . For our later experiments, we use the official TREC 2013 Temporal Summarization track 4 measures, namely Expected Latency Gain (ELG) for the Gain component and Latency Comprehensiveness (LC) for the Coverage component. The optimal r value for the cur-rent update summary can then be calculated as follows: Having defined the feature set and loss function that we use, givenanevent e we can generate a set of training instances I , where each instance represents a time interval within e . Importantly, since the loss function is dependant upon the previously selected sentences U t to calculate the optimal  X  t , we generate training instances in a greedy manner. In this case, we assume that at each time interval, the optimal  X  is selected. Figure 4 illustrates the training instance genera-tion process, while in the next section we define the dataset, training data and measures we use within our evaluation. http://www.trec-ts.org/ Table 2: Example event  X 2012 Aurora shooting X  (6/205 nuggets shown). Dataset: To evaluate our approach for IUS, we use the TREC 2013 Temporal Summarization sequential update sum-marization (SUS) task dataset. Notably, SUS is a stream processing task, where the aim is to select sentences from the stream. This contrasts with IUS, which focuses on se-lecting sentences from update summaries already produced from the underlying stream. T he SUS 2013 dataset uses the 2013 Knowledge-Based Acceleration (KBA) stream corpus  X  containing over 1 billion timestamped Web documents (e.g. news articles, blogs and forum posts) spanning the period of Oct 2011 to Feb 2013.
 Events: The dataset also contains 10 topics representing long-running events from during this period, along with a user query Q representing each. Each event has a pre-defined 10-day timespan. A typical 10-day period contains around 8-9 million documents. Each event also has a predefined set of information nuggets (describing key pieces of infor-mation that a summary about the event should cover) and nugget timestamps (indicating approximately when that in-formation emerged) that were manually extracted from the updates made to the Wikipedia page about each event. Ta-ble 2 illustrates an example event.
 Update Summary Generation: In contrast to the SUS task, our IUS approach takes as input  X  X pdate X  summaries produced over time intervals from the underlying document stream. These update summaries are produced as follows. First, we cluster the 8-9 millio n documents published from each 10-day event period into 1 hour intervals, i.e. our in-tervals t = { t 0 ,t 1 ,t 2 ... } . This results in 240 intervals t per event. We index each document cluster, creating 240 doc-ument indices. Stopword removal and Porter stemming is applied. For each document index, we use the event rep-resentation Q asaquery,retrievingthetop10documents relating to the event using the BM25 document weighting model, and applying query expansion with the Kullback-Leibler (KL) term weighting model to improve search effec-tiveness. For each set of 10 documents retrieved, we process them using an update summarization approach to produce an update summary comprised of 10 sentences. As they have previously been shown to be effective [26], we adopt a state-of-the-art learning-to-rank approach for update summariza-tion. In particular, we train a LambdaMART [34] list-wise learning-to-rank model separately from our IUS approach, using the 63 events from the Document Understanding Con-ference (DUC) datasets from 2005/2006/2007 that have Se-mantic Content Unit (SCU) marked sentences [8]. 5 We use
These DUC datasets total 45,683 labelled sentences. 57 sentence features based on those reported in [32], using NDCG as the target measure. Finally, to remove redun-dancy and following [9], we then employ an upper bound  X  on the permissible similarity between sentences, removing any sentences that exceed this upper bound. We calculate the pairwise similarity between each sentence in a greedy time-ordered manner against those ranked above it. Simi-larity is calculated via matching terms, weighted using the tf-idf term weighting model. A Wikipedia snapshot pre-dating the events is used to obtain the background term fre-quencies. Sentences with similarity scores exceeding  X  are discarded.  X  was optimised for NDCG on a separate corpus using 0.1 increments, resulting in an upper bound tf-idf sim-ilarity of 0.5. We then take the top 10 remaining sentences to form each update summary (i.e. summary length is 10 sentences).
 Sequential Update Summarization Evaluation: For the SUS task, participants performed an extractive summa-rization for each event over that event X  X  timespan, returning a timeline of sentences from the corpus summarising that event with confidence scores for each. Unlike prior summa-rization tasks such as MDS [10], the identification of on-topic documents from the stream was left to the participants. The top 60 sentences (by confidence score) returned by each par-ticipating system were then sampled to form an assessment pool. TREC assessors manually compared each sentence against the information nuggets extracted for the event, cre-ating a matching between each sentence and zero or more nuggets. Assessors were also allowed to define new nuggets from sentences they assessed, if a sentence was on-topic but no existing nugget matched. A sentence returned by a par-ticipant system is only considered relevant if it covers infor-mation nuggets not already covered by previously returned sentences, i.e. the update contains new information. Tackling Assessment Incompleteness: Importantly, we observed little ( &lt; 1%) overlap between the top sentences identified by our system and those assessed as matches dur-ing the TREC task, i.e. assessment completeness [3] was low. To counteract this, we used crowdsourcing to generate an ad-ditional set of assessments to augment those created by the TREC assessors. In particular, for each of the 10 events, we pooled the 10 sentences contained within the update sum-maries we take as input for each 1 hour interval over the 10 days, resulting in 22,424 sentences (10 events * 240 hours *10sentences 6 ). Next, we assessed the 22,424 sentences in three stages. First, an IR expert manually labelled each sen-tence as relevant or not to the event that they summarised, based upon the information nuggets defined by the TREC task. 6,073 of the 22,424 sentences were labelled as relevant (27.1%). Second, the 6,073 relevant sentences were subject to duplicate detection. If multiple sentences were textu-ally identical, only the earliest was kept, resulting in 4,463 unique sentences. For later evaluation, the assessment la-bels obtained for each unique sentence are propagated to its duplicates. Third, the remaining 4,463 relevant and unique sentences were matched against the information nuggets for their respective events (as per the TREC task) using crowd-sourcing. For example, for the event illustrated in Table 2, the sentence  X  X heatres across the country are increasing se-curity after 12 people were shot in Denver X  would be labelled as covering nugget 1 and 6.
 Crowdsourced Assessments: For each of the 4,463 sen-tences, three crowdsourced assessors matched that sentence
Not all hours returned 10 sentences, hence the total number of sentences is less than 24,000. Figure 5: Sentence assessment interface (only a sub-set of the information nugget options are shown). against the list of information nuggets for the event, in a similar manner to the TREC assessors. Crowdsourced as-sessors were also able to specify new information nuggets, when the existing nuggets did not provide a good match to the information provided in a sentence. Finally, each as-sessor also rated the quality of each sentence for inclusion intoasummaryabouttheeventona5-pointLikertscale.
 A sentence is considered to match a nugget if two or more assessors select that nugget. A sentence X  X  quality rating is the average of the ratings assigned. Figure 5 illustrates the sentence assessment interface used.
 Crowdsourcing Setup and Statistics: We use the Crowd-Flower crowdsourcing service to recruit workers for our eval-uation. Following best practices in crowdsourcing [24], we have three individual assessors match a sentence against the information nuggets. Each assignment involves the assess-ment of 5 sentences. We paid US $0.08 for each assignment. To detect bots and spammers, for each sentence, the assessor enters a word from a specified position within that sentence into a pre-provided text box (acting as a form of captcha). Workers who fail this test more than three times are barred from completing more assignments. Workers are also sub-ject to an entry test where they perform a single assignment, their performance is measured against a gold standard [14]. A worker X  X  agreement with the gold standard must exceed 70% to qualify for the task. The total crowdsourcing cost was US $305.64. Automatic validation resulted in the rejec-tion of 2.45% of assignments. Inter-worker agreement was 50.4% under Fleiss  X  , indicating that the resultant work was of reasonably good quality.
 Measures: We report the IUS performance using the of-ficial TREC 2013 Temporal summarization track (TREC-TS) measures  X  Expected Latency Gain (ELG) and Latency Comprehensiveness (LC). ELG measures for each update is-sued to the user, the sum of latency-discounted relevance of the nuggets for which that update is the earliest issued, in doing so it accounts for the proportion of updates that con-tain new information (nuggets) and the timeliness of that content with respect to when those nuggets emerged. LC measures the coverage of an event by the updates issued with an additional timeliness component, i.e. it measures nugget recall over all updates issued, where the score for a nugget is discounted if it is reported late. Finally, we also report the macro harmonic mean of ELG and LC, denoted Retrospective vs. Live: Since IUS is a time-oriented task, there are two settings under which the above measures can be calculated. First, in a retrospective manner, where per-formance is calculated at the end of each 10-day period based upon the final aggregated output, denoted Retrospective . This provides a measure of summary effectiveness at the close of the event. However, retrospective effectiveness may not reflect the performance that the end-user sees, since they will access the summaries produced while the event is ongo-ing. Hence, we also report summary effectiveness as an av-erage of the performance calculated incrementally each hour across the 10-days, denoted Live and examine how summary performance varies over time in more detail in Section 6.3. Training Regime: We train our IUS approach using a 10-fold-cross validation. 7 We test both the model produced by a classical linear regression learner and that produced by a learner based on Model Trees [31]. When reporting the most influential features using a feature ablation study, we average the performance loss observed across all 10 folds. Baselines: We compare our IUS approach with traditional update summarization approaches that produce fixed-length summaries. In particular, we compare against SumBasic [21], SumFocus [26], and the Classy [7] sentence scorers config-ured as per their original papers, in addition to the update summaries produced by the learning-to-rank model used as input for our proposed IUS approach, denoted Lamb-daMART. For these baselines, we report performance when selecting one or three sentences from each time interval. reference, the TREC 2013 be st system under the ELG/LC However, in contrast to the TREC systems, it is of note that performance under our approach is reported using the expanded assessment set. Hence, it is not possible to directly compare performances.
In this section, we investigate whether by adaptively alter-ing the number of updates to issue about an event over time we can enhance the effectiveness of IUS over the baselines described earlier. In particular, we examine the following three research questions, each in a separate (sub)section: We begin by evaluating the performance of our proposed IUS approach against a series of traditional update summa-rization systems from the literature. If our proposed IUS approach outperforms these baselines, then we will be able to conclude that predicting the number of sentences to se-lect from each update summary is more effective than select-ing a fixed number of top sentences only. Furthermore, we will have shown that our machine learned approach is able to learn how to predict an effective cutoff based upon the event prevalence and novelty within the update summaries produced over time.
 Table 3 reports the performance of the SumBasic [21]; SumFocus [26]; Classy [7] and learning-to-rank-based [32]
Equivalent to a leave-one-out setting over the 10 events.
Selecting more than 3 sentences reduces overall perfor-mance for this task due to decreased ELG. paired t-test p &lt; 0.05 are denoted and , respectively. extractive MDS approaches with redundancy removal based upon a per-sentence upper-bound similarity threshold, where the top 1 or 3 sentences are selected during each time inter-val to issue as updates. Table 3 also reports the perfor-mance of our proposed IUS approach (denoted IUS Adap-tive) when selecting sentences from the learning-to-rank-based update summaries and trained using the Linear Re-gression and Model Trees learners when trained using 10-fold cross validation. The oracle performance (where the opti-mal rank-cutoff was selected) is also reported. Performance is measured in terms of Expected Latency Gain (ELG), La-tency Comprehensiveness (LC), and the combined ELG/LC
From Table 3, we observe the following three points of in-terest. First, comparing the baseline approaches that select a fixed number of sentences to issue as updates (rows 4-11), we see that the most effective underlying update summa-rization approach under ELG/LC Macro F 1 is Classy [7]. Indeed, it is interesting to note that the generative Classy model outperforms the LambdaMART model trained on DUC data (when using a fixed cutoff). This would indicate that the effective sentence ranking features differ between the DUC newswire documents and the Web documents re-trieved from the KBA corpus being summarized here. Sec-ond, comparing the LambdaMART baseline (rows 10-11) with our proposed IUS approach when trained using Lin-ear regression (row 12) under ELG/LC Macro F 1 ,wesee that our approach marginally under-performs the baseline when evaluating retrospectively, but outperforms that base-line by a small (but statistically significant) margin under the live setting. This lack of substantial improvement over the baseline indicates that a simple linear function is not sophisticated enough to predict an effective rank-cutoff for IUS. However, comparing the LambdaMART baseline (rows 10-11) with our approach when the Model Trees learner is used (row 13), we see that our approach outperforms LambdaMART by a large margin under both retrospective (13.7% absolute ELG/LC Macro F 1 ) and live (11.6% abso-of the Model Trees learner is over 47%/14.4% more effec-tive under retrospective/live ELG/LC Macro F 1 than the best baseline (Classy Top 1 -row 8). Third, comparing the baselines (rows 4-11) with the oracle performance (row 14), we observe that improvements over the best baseline of up-to 28.8%/30.6% absolute ELG/LC Macro F 1 arepossible-highlighting the promise of the approach and the scope for future improvement.

To answer our first research question, as illustrated by the performance of our proposed IUS adaptive approach, by adaptively altering the number of sentences to select from the update summaries, we are able to outperform the best baseline by up-to 47% (under ELG/LC Macro F 1 ). This shows that it is possible to predict the number of sentences to issue as updates to the end-user over time. In the next section, we analyse the most influential feature subsets.
We next examine the features subsets that contribute most to the prediction of an effective rank-cutoff. Identifying the most influential features can provide insights into what prop-erties an update summary should have if we are going to select sentences from it to issue to the user as updates. To identify the most influential features, we perform an abla-tion study, where we remove a subset of the features and re-evaluate the performance of the model produced. When an influential feature subset is removed, we would expect performance to decrease. In contrast, when an uninfluential feature subset is removed, we would expect performance to remain approximately unchanged.
 Table 4 reports the ELG/LC Macro F 1 performance of our IUS approach using the Model Trees learner when leveraging the feature subsets described previously in Table 1. In par-ticular, we report performanc ewhenweablateeachfeature subset for both the live and retrospective settings. From Table 4, we observe the following. First, as a sanity check, comparing the performance of the models produced under the two evaluation settings, we observe that performance de-creases when features are removed in all cases, following our expectations. Indeed, the performance decreases observed as features are ablated are statistically significant (paired t-test p &lt; 0.01) with respect to using all features under the live evaluation setting.

In terms of the most influential feature subsets divided by aspect, we see that the quality features contribute the most, followed by the novelty and prevalence features. To illustrate why this is the case, Figure 6 shows the number of sentences selected by the ablated models produced for the Pakistan factory fires event during the first 24 hours of that event. As we can see from Figure 6, in comparison to using all features (front-most curve), using a model without qual-ity features (rear-most curve) follows the same pattern of selection (sentences are selected during the same hour inter-vals). However, it selects many more sentences during each interval. This results in irrelevant or redundant sentences being issued as updates from deeper within the update sum-maries (see the example from earlier in Section 4).
To answer our second research question, all of the feature groups that we proposed in Section 4.2 are influential, since all reduce prediction performance when removed. The most influential of these were the quality features, followed by Table 4: Influential IUS features identified. in-dicates a statistically significant decrease (paired t-test p &lt; 0.01) with respect to using all features. the prevalence and novelty features. In the next section, we analyse how IUS performance varies as an event evolves.
In Section 6.1, we reported the combined performance of our IUS system under both retrospective and live settings. However, it is also important to examine how summary per-formance varies over time. To e valuate this, we report the in comparison to the oracle system at each hour interval t , on a per-event basis. Importantly, ELG/LC Macro F 1 for a time interval t measures summarization performance based upon sentences emitted both prior to and during t .Perfor-mance will increase as sentences covering new relevant in-formation nuggets are emitted, but will decrease if off-topic or redundant sentences are emitted. Due to space limita-tions, we report only two of the ten events that illustrate two common performance distributions observed.

Figure 7 (a) and (b) report the p erformance distribution of our approach (Model Trees with leave-one-out training) and the oracle under ELG/LC Macro F 1 for each hour interval. The events reported are the Wisconsin Sikh Temple shooting and the Buenos Aires Rail Disaster. From Figure 7 (a) and (b), we make the following observations. Considering the oracle performance, for both events, we see a sharp increase in performance within the first few hours of the event. This shows that a large proportion o f the important information appears during the first few hours. Next, examining the per-formance of our approach, we see that for the Wisconsin Sikh Temple shooting example (Figure 7 (a)), starting from the second day, IUS performance begins to degrade. This shows that the model is continuing to issue more sentence updates over time even though little new information is available. From this, we can make two conclusions. First, the predictor appears to make the most prediction errors when the optimal tive early-on in an event X  X  lifetime than later in that lifetime. The performance distribution of Figure 7 (a) is common to 6 of the 10 events in our dataset. Hence, assuming that users are more interested in tracking an event early in that event X  X  lifetime, the performance of our approach reported earlier in Table 3  X  that considers all updates issued over the 10 days  X  may be an under-estimate. In contrast, from Figure 7 (b), we see a different performance distribution. In this case, performance increases early as initial sentence updates are issued. However, after around 12 hours, no new updates are issued. Contrasting this distribution to the oracle for the same event, we see that new information is still emerging for this event after the 12 hour period, albeit at a slow rate. This indicates that the learned model lacks sensitivity when new information arrives slowly, i.e. when rank-cutoff  X  t val-ues in the range 0-1 are predicted. In answer to our third Figure 6: The number of selected sentences over time using our approach when Prevalence, Novelty and Quality features are ablated. research question, we conclude that our IUS approach can be effective over time, as illustrated by its overall effective performance of the Buenos Aires Rail Disaster event and its early performance for the Wisconsin Sikh Temple shooting event. However, we also observed that IUS performance can degrade once an event ends, as for some events, redundant sentences continue to be selected. To illustrate the qual-ity of the summaries produced by our approach over time, Table 5 shows an extract from the summaries produced for the Wisconsin Sikh Temple shooting and Buenos Aires Rail Disaster events discussed above.
In this paper, we introduced the task of incremental up-date summarization (IUS), which aims to select sentences from update summaries about an event over time to be is-sued to a user tracking that event. We proposed a new approach to this task that treats IUS as a rank-cutoff selec-tion problem from the update summaries. We trained a re-gression model comprised of over 330 features that balances the cost of selecting a deeper cutoff in terms of returning redundant co ntent, against the risk of missing important in-formation by selecting a shallower cutoff. Through empirical evaluation using the TREC 2013 Temporal Summarization dataset expanded with over 22,000 additional assessments, we show that our proposed approach can improve the IUS performance by 47% in comparison to effective update sum-marization baselines from the literature. Hence, we conclude that adaptively altering the number of top sentences to select from the update summaries over time is critical to achieve an effective IUS performance.

From the results of this evaluation as a whole, we believe that the generation of timely updates about long-running events is still a challenging problem. To effectively summa-rize events over time, approaches need to adapt to changes at the rate at which events are reported, accounting for pe-riods when no new information emerges. For future work, we aim to investigate how to increase the sensitivity of the supervised prediction model when small amounts of new in-formation arrive over time, as well as to further examine IUS from an information filtering perspective.
This work has been carried out in the scope of the EC co-funded projects SMART (F P7-287583) and SUPER (FP7-606853). shooting and Buenos Aires Rail Disaster events.
 Aires Rail Disaster events
