 Exploratory data analysis tries to discover novel dependen-cies and unexpected patterns in large databases. Traditional-ly, this process is manual and hypothesis-driven. However, analysts can come short of patience and imagination. In this paper, we introduce Claude, a hypothesis generator for data warehouses. Claude follows a 2-step approach: (1) It detects interesting views, by exploiting non-linear statisti-cal dependencies between the dimensions and the measure. (2) To explain its findings, it detects local patterns in these views and describes them with SQL queries. Technically, we derive a model of interestingness from fundamental informa-tion theory. To exploit this model, we present aggressive ap-proximations and heuristics, allowing Claude to be fast and more accurate than state-of-art view selection algorithms. H.2.8 [ Database Applications ]: Data mining Data exploration; Query recommendation; Feature selec-tion; Subgroup discovery
Businesses and scientists willing to extract knowledge from databases face a dilemma. On one hand, software editors have commercialized quantity of visual Business Intelligence tools, such as Tableau or Qlik. Yet, these rely almost en-tirely on users X  expertise, intuition, and patience. In an ex-ploratory scenario, when users know little about their data, such tools can involve time-consuming cycles of trial and er-ror. On the other hand, automated, machine learning meth-ods have gained much popularity, as shown by the recent success of industrial  X  X ata scientists X . Nevertheless, at the time of writing this paper, these experts are still a rare and expensive resource. Can we find a middle way? Can we de-c  X  sign a flexible, accessible method to automatically analyze data warehouses?
Consider the following scenario. A government analyst studies criminality in US cities. The database describes how a measure , the number of crimes, varies along several dozen dimensions , e.g., the population, unemployment rate, or lo-cation. Which cities are prone to crime? Can we identify causes, correlations, or patterns? For our analyst, inspect-ing how every possible combination of dimensions correlates with crime is close to impossible. But important obser-vations could hide behind unexpected dependencies. Our aim is to automatically generate hypotheses about what in-fluences the measure. We want to synthesize SQL queries which highlight what causes crime to vary, and how these variations occur. Given such queries, even lay users could explore their data and detect unexpected patterns.

We believe that this problem is extremely common in both business and science, where multi-dimensional data ware-houses have been deployed for years. Addressing it with an automated method would allow analysts and researchers to make discoveries more quickly, focusing on interpretation rather than writing queries. Such a technique could also pro-voke serendipitous findings, e.g., highlighting a surprising correlation or confronting an ill-posed hypothesis. Besides, the problem is getting more pressing as data warehouses grow in size and complexity: manual exploration can turn out to be a painful exercise when dealing with hundreds of columns.

Automating data exploration is a difficult problem for two reasons. First, how do we recognize  X  X nteresting X  queries? There is, to our knowledge, no universal measure of inte-restingness. The challenge is to devise a theoretically well-founded model, general enough to cover a wide range of use cases. The second problem is efficiency: given a measure of interest, how can we explore the space of all database queries quickly enough to support large datasets? We must design efficient methods to traverse all possible combinations of dimensions.

Several semi-automated exploration frameworks were pro-posed in the past, in the context of OLAP data cubes [17, 10]. They assumed that databases contained less than a dozen well-known dimensions. The whole challenge was to drill-in correctly, in order to locate  X  X xceptions X , e.g., local anomalies. We believe that many of these assumptions do not hold anymore. In our model, databases can contain more than a hundred dimensions with mixed types. Analysts have little to no knowledge about the effects of these dimensions, even less about their combination and their effect on a given In this query, X1 ,..., Xd represent the variables of the view, [L1, H1] ... [Ln, Hn] represents the bounds of the POI, and T represents the target.
We presented views and POIs without specifying any mea-sure of dependence S or dissimilarity D . In the following section, we instantiate these quantities using fundamental information theory.
A set of dimensions is interesting if its columns are jointly dependent to the target. To quantify this dependence, we use mutual information . This measure presents many advan-tages: it is sensible to non-linear dependences, it can cope with any kind of variables, and it is practical to compute.
The entropy H ( X ) of a variable X describes its variabi-lity [3]. If X has a constant value, then H ( X ) = 0. In contrast, if X is highly unpredictable (e.g., X is the outcome of flipping a perfectly balanced coin) then H ( X ) is maximal. Formally, if X is a discrete variable with sample space  X , then we have: If X is continuous with density p , we define it as follows: We can use the entropy to describe how variables interact. If two variables are dependent, then conditioning (e.g., re-stricting the range of values) on one variable will affect the other. In our example, cities with high unemployment have high levels of crime. Therefore, conditioning on the vari-able Unemployment decreases the uncertainty of the variable Crime . This causes a loss in entropy, and the value of this loss is the mutual information. Formally, if X and T are two random variables, the expression H ( T|X = x ) describes the entropy of T given X = x . If we average this expression over all possible values of x , we obtain the conditional en-tropy: H ( T|X ) = E x [ H ( T|X = x )]. We define the mutual information I as follows: The mutual information is the loss in entropy between T and T|X . It is symmetric, and it is always positive or null. We can generalize this quantity to joint distributions, which gives us our new, refined version of view strength: A view is strong if the mutual information between the tar-get and its dimensions is high.

In practice, we do not know the distributions of the vari-ables X n , but we have access to the samples X n . Therefore, we estimate the strength. If the variables are discrete, we set  X  P ( X n = x ) to be the proportion of tuples with X Figure 2: Example of view with three discrete variables V = { X 1 ,X 2 ,X 3 } . The average divergence of the cells  X  ( x ) equals the strength of the view  X  ( V ).
 We  X  X lug X  this estimator in Equations 3, to obtain the es-timated entropy  X  H ( X ). We then use this approximation in Equation 5. Dealing with continuous vari X  X bles is more diffi-cult, for two reasons. First, estimating the density function in Equation 4 is costly, especially with multivariate distri-butions. Second, it is not clear how to deal with mixed datasets, e.g. continuous and discrete dimensions, or dis-crete target and continuous dimensions. Therefore, Claude bins all the continuous variables, and treats them like dis-crete columns.
Let us now refine our definition of divergence. We es-tablished that the divergence of a POI is the dissimilarity between the target X  X  distribution within this POI, and the target X  X  distribution in the whole database. To measure this dissimilarity, we use the Kullback-Leibler divergence (KL). The KL divergence measures the difference between two probability distributions. It is null if the two distributions are similar, and it grows when the distributions differ. For-mally, if X and Y are two discrete random variables with the same sample space  X , we have: As Claude discretizes the continuous variables, we ignore the continuous case (cf. Section 3.1). Our region R is a good point of interest if KL ( T| R kT ) is as large as possible:
In our model, view strength and POI divergence are closely related. Consider a view V , made of discrete variables. If we compute the divergence of each tuple and average the results, we obtain the V  X  X  strength. We illustrate this prop-erty with Figure 2. Therefore, strength and divergence are  X  X wo sides of the same coin X . We formalize this property with the lemma below.

Lemma 1. If V is a view with d discrete variables and x  X   X  1  X  ...  X  D is a tuple from this view, then: Proof. The random vector X describes the columns of V . Applying Bayes X  X  theorem to Cover and Thomas, Equa-tion 2.35 [3], we obtain I ( X ; T ) = E x [ KL ( T |{ x }k T )]. Sub-stituting the left side with Equation 6, and the right side with Equation 8, we obtain the lemma. Suppose that we obtained a view V by discretizing a set of continuous variables V  X  . The average divergence of V  X  X  bins equals the strength of V , but not that of V  X  . Fortunately, these quantities converge as the bins get small.
 Lemma 2. The view V is a set of continuous variables, V b is a discretized version of V in which each variable is binned with bin size b , and x b is a tuple from V b . We have E x b  X  ( { x b } )  X   X  ( V ) as b  X  0 .

Proof. Let the D -dimensional random vector X describe the (continuous) variables of V , and X b describe the (dis-crete) variables of V b . By generalizing Cover and Thomas, Theorem 8.3.1 [3], we infer that H ( X b ) + D  X  log b  X  H ( X ) as b  X  0 . Thus, using Equation 5, we have I ( X b I ( X , T ). We conclude that  X  ( V b )  X   X  ( V ). We apply Equa-tion 9 to obtain the lemma.
We now have a functional definition of view strength. At this point, we could easily envision a greedy heuristic to detect the top K views in a database. We start with simple views, based on one dimension. We then add columns, one by one. To test if a column X is worth adding to a view V , we compute the strength  X  ( V  X  X  X } ). If the result is high enough, we keep the candidate. If not, we discard it. We will present such an algorithm in Section 5. However, we must first discuss how to compute  X  ( V  X  X  X } ).

Equations 6 and 9 describe several methods to compute the strength of a view. Nevertheless, none of these fit ite-rative algorithms. Suppose that we wish to compute the strength of a view V , then the strength of another view V  X  X  X } . These equations give us no opportunity to share computations, we must obtain  X  ( V ) and  X  ( V  X  X  X } ) sepa-rately. Furthermore, both expressions are expensive, as they involve group-by queries over the whole database. There-fore, we need an alternative recursive formulation for the strength of a view:
Lemma 3. Consider a view V = { X 1 ,...,X i } , and a tar-get T . For any column X i +1 :
Proof. This lemma is a consequence of the Mutual Infor-mation X  X  chain rule, Cover and Thomas, Theorem 2.5.2 [3]. This lemma describes how adding a column impacts the strength of a view. For any random variables X i , X the notation I ( X j ; T|X i ) expresses the conditional mutual information . The conditional mutual information is a con-ditioned version of the mutual information: it describes the dependency between X j and T given restrictions on X i . To obtain it, we compute the mutual information between X j and T given all the possible values of X i , and average the results. Formally: The influence of X i can go either way: it can weaken the dependency between X j and T , or it can strengthen it. The conditional mutual information is positive or null, and it is bounded by the entropy of X j and T .

Unfortunately, we cannot directly exploit Lemma 3 in our algorithm: estimating I ( X i +1 ; T|X 1 ,..., X i candidate { X 1 ,...,X i +1 } is as expensive as computing the Figure 3: Example of co-dependency graph with 5 dimen-sions. To approximate the strength of V  X  X  X 5 } , we add the weight of edge ( X 4 ,X 5 ) to V X  X  strength -in this case 0.5. strength directly. However, we can use an approximation. Recall that V = { X 1 ,...,X i } , we exploit the following ob-servation: The idea behind this approximation is naive: we assume that the high order dependencies. Thanks to this assumption, we can compute the strength of our candidates much faster.
Our new approach operates in two steps, an offline step and an online step. Offline, we compute the conditional mu-tual information I ( X j ; T|X i ) between every pair of variable ( X i , X j ). We call the resulting structure co-dependency graph . In this graph, the vertices represent the dimensions, and the edges represent the conditional mutual information. The co-dependency graph is oriented and weighted. Online, we run a greedy algorithm as described previously, but we use Equation 12 to evaluate new candidates. To compute the strength of a view V  X  X  X i +1 } with V = { X 1 ,...,X i } , we fetch the value of I ( X i +1 ; T|X i ) in the co-dependency graph and add it to V  X  X  strength. We illustrate this method in Fig-ure 3. Previously, computing  X  ( V  X  X  X i +1 } ) involved heavy groupings and aggregations on the whole dataset. Now, we simply perform a lookup in a graph with N edges, where N is the number of columns in the database.

Note that our approximation has a drawback: it depends on the order in which we include the variables in the view. If we enrich a view by successively adding variables X 1 , X and X 3 , then we obtain a different strength than if we incor-porate X 3 , X 2 then X 1 . Similarly, in Equation 12, we obtain different approximations if we change the indexing of the di-mensions X 1 ,..., X i . For more robustness, we introduce a  X  X essimistic X  variant: Instead of adding the strength I ( X i +1 ; T|X i ), where X the last variable inserted, we add I ( X i +1 ; T |X n ), where X is the variable which weakens X i +1 the most. We will use this version in the rest of the paper.
This section presents our view search strategy. Our aim is to find the top K views with at most D columns. If our database includes N dimensions, our search space contains P Therefore, we resort to a greedy, level-wise heuristic. Figure 4: Example of Beam Search, with D = 3 and beam size B = 2 Algorithm 1 Beam Search for view selection function TopViews ( K , D , B , DB ) end function Figure 5: Limit cases of the beam search strategy. The variables X 1 and X 2 represent two dimensions. The symbol and color of the plots represent the value of the target.
Our algorithm is based on beam search , illustrated in Fig-ure 4. To initialize the algorithm, we compute the strength of each variable separately. We sort the candidates, and keep the top B elements. We call this set the beam , greyed in the figure. Then, we generate new candidates, by ap-pending each variable of the database to each variable in the beam. We obtain views with two columns. We compute the strength of these views, keep the top B strongest and discard the others. This gives us a new beam. We repeat the procedure until the views in the beam contain D variables, or the views stop improving. Algorithm 1 presents the full procedure.

Thanks to our strategy, we avoid exploring an exponen-tially large search space. Instead, we compute the strengths of at most N.B candidates at each level. The size of the beam lets us control the trade-off between accuracy and run-time. With a small beam, we evaluate less candidates, and thus terminate earlier. Oppositely, a large beam lets us ex-plore more candidates. Let us explain why this is necessary. Figure 6: Beam search augmented with a deduplication step. We display in italic the size of the intermediate results. Note that N  X  B  X  B 0  X  B .
 At each level of the algorithm, we discard the views which are too weak to reach the top B candidates. We assume that if a combination of columns is weak at level i , then it will be weak at all subsequent levels. Unfortunately, this assumption does not hold: we can form strong views by com-bining weak columns; there are  X  X umps X  in the search space. Consider for instance the two classic scenarios pictured in Figure 5. The dimensions X 1 and X 2 taken in isolation are weak: we can infer no useful information about the target from either of them. However, their combination is very in-teresting. Equivalently, the views { X 1 } and { X 2 } have a very poor strength, but { X 1 ,X 2 } is an excellent candidate. If the beam is too tight, we may discard { X 1 } and { X early because of low scores. We lose the opportunity to dis-cover { X 1 ,X 2 } . Therefore, we recommend to set B &gt; K . During our experiments, we obtained excellent results with B  X  2  X  K (cf. Section 7.3).
In total, we evaluate the strength of B  X  N candidates for the D levels of the beam search. To carry out this computa-tion, we can either use the exact formulation of strength, as shown in Equation 6, or use the approximation scheme pre-sented in Section 4. In Claude X  X  implementation, we opted for a hybrid approach. We perform the first two levels of search with the exact strength (which is equivalent to build-ing the co-dependency graph). Then, for all subsequent steps, we use the approximations. Finally, we revert to the exact strength for the top k ranking, at the very end of the procedure. Thanks to this method, we obtain significant speed gains at little accuracy cost.
Our algorithm seeks strong views. In some cases however, it may be preferable to have weaker but more diverse views. To deal with those cases, we introduce an optional dedupli-cation step, during which we reduce the number of views with an algorithm from the literature. As pictured in Fig-ure 6, we run this procedure at the end of each beam search iteration. By definition, deduplication reduces the number of candidates. Therefore, to obtain B views at the end of the algorithm, we must generate B 0 &gt; B views beforehand. A low B 0 yields more variety, while a high B 0 may lead to stronger views.

Authors have proposed quantity of methods to dedupli-cate itemsets in the pattern mining literature [25, 21]. We opted for a simple compression-based approach. First, we compute the dissimilarity between every pair of views with the Jaccard dissimilarity. Given two views V i and V is defined as follows: d J ( V i ,V j ) = | V i  X  V j then cluster the resulting matrix with Partitioning Around Medoids, an algorithm of type k-medoids. We refer the in-terested reader to the literature for more details [11]. Table 1: Characteristics of the datasets. The last two columns are used for comparison with 4S, cf. Section 7.3.
We previously described how to find strong views. We now explain how to identify P points of interests for each of these views.

We instantiate POIs by a well-known analysis called sub-group discovery [13, 23], which can be formulated as follows: given a set of tuples, a target column and a measure of excep-tionality, detect sets of tuples for which the target behaves exceptionally. In our case, we instantiate the exceptionality measure with divergence.

As pointed our by van Leeuwen and Knobbe [20], we can also solve the subgroup discovery problem with beam search. Let V represent the view to analyze. As the variables are binned, can form a grid over V , as shown in Figure 2. We denote by b the number of bins for each variable. To initialize the algorithm, we compute the divergence of each cell and keep the top B P OI most divergent. We obtain our beam. We then  X  X rill X  into each of these cells: we decompose them into smaller cells by splitting the edges into b bins. We evaluate the new candidates and keep the top B P OI most divergent. We reiterate until the algorithm converges. As shown in the subgroup discovery literature [23, 20], we can generalize this method to binary and nominal data. For each distinct level x of a variable X , we create two groups: tuples for which X = x i , and tuples for which X 6 = x i .

In practice, KL-based approaches tends to favor smaller regions. Therefore, Beam Search may converge late, or not at all. A practical solution is to set a minimum count threshold. Alternatively, we can alter our model to take the size into account [20]. Let R represents a region with count | R | , and | DB | represent the number of tuples in the database. We introduce the weighted deviation  X  w ( R ) = | R | / | DB | X   X  ( R ). This new score introduces a penalty for small POIs.
We now present our experimental results. All our experi-ments are based on 8 datasets from the UCI Repository, de-scribed in Table 1. The files are freely available online several experiments, we report the normalized view strength instead of the usual strength; if V is a view with entropy H ( V ), we obtain it as follows:  X  norm ( V ) =  X  ( V ) /H ( V ).
In this section, we showcase Claude with a real-life exam-ple: we analyze the Communities and Crime dataset form the UCI repository 2 . Our aim is to understand which US archive.ics.uci.edu/ml/ archive.ics.uci.edu/ml/datasets/Communities+and+Crime Table 2: Example of views generated by Claude for the US Crime dataset. Figure 7: Heatmaps of the US Crime Dataset, based on Claude X  X  output. Each box represents a Point of Interest. cities are subject to violent crimes. Our database compiles crime data and socio-economic indicators about 1994 com-munities, with a total of 128 variables. The data comes mostly from the 90 X  X , and it was provided by official US sources -among others, the 1990 US census and the 1995 FBI Uniform Crime Report. All the variables are normal-ized to have a minimum of 0 and a maximum of 100.

We generated K = 100 views with up to D = 3 dimen-sions, both with and without deduplication. We present a selection of views in Table 2, along with 2-dimension heat maps in Figure 7. Observe that strong views have a visual signature: in the top two maps, the blue and red areas are neatly separated. In the bottom two views, the distinction is less clear. Figure 8: Strength vs. Classification accuracy for 500 ran-dom views. We obtained the blue and red lines with simple linear regression.
 The first view of Table 2 is the best one we found: Police. Overtime, Pct.Race.White, Pct.Vacant.Boarded . It has a score of 0.51. which means that these three variables con-tain 51% of the target X  X  information. The columns Police. Overtime and Pct.White.Race respectively describe the av-erage time overworked by the police and the percentage of caucasian population. The third variable, Pct.Vacant.Boar-ded was surprising to us: it describes the percentage of va-cant houses which are boarded up. How does this relate to crime? We could assume that boarded houses are associated with long term abandon, and thus, poverty. The top-left plot of Figure 7 shows the relation between race, boarded houses and crime. Observe that the variables complement each other: a high proportion of caucasians may or may not lead to low crime. However, a high proportion of caucasians combined with a low rate of boarded house correspond to safe areas, while little caucasians and many boarded houses correspond to more violent communities.

Our second view shows that cities with more monoparental families tend to be more violent: the correlation is clearly visible, and both POIs point to the bottom of the chart. However, close inspection also reveals surprises: a few com-munities have a relatively high number of two-parents fami-lies, but also high indicators of police requests and crime (in the top right corner of the chart). Manual queries reveals that many of these cities are located in the suburbs of Los Angeles, and contain a majority of Hispanics. Does this ex-plain the peculiarity? We leave this question open for future investigations. We see that some findings come from the re-commendations directly while others are serendipitous. But in both cases, Claude lets us discover  X  X uggets X  with little prior knowledge and few assumptions.
In this section, we show experimentally that our notion of view strength  X  X orks X , e.g. that strong views effectively pro-vide information about the target column. To verify this as-sumption, we simulate users with statistical classifiers. Con-sider a view V over a database. If a classifier can predict the value of the target from V  X  X  columns, then V is informa-tive. Oppositely, if the classifier fails, then V is potentially uninteresting. In a nutshell, we should observe a positive correlation between views strength and classification accu-racy.

We now detail our experiment. We chose three datasets from the UCI repository. For each dataset, we generated 500 random views and measured their strengths. We then trained classifiers on each of these views, and measured their performance. We report the results in Figure 8. We chose two classification algorithms: Naive Bayes, and 5-Nearest Neighbors. We chose those because they contain no built-in mechanism to filter out irrelevant variables (as opposed to, e.g., decision trees). We measure classification performance with 5-fold validation, to avoid the effects of overfitting.
In all three cases, we observe a positive correlation be-tween the strengths of the views and the accuracy of the predictions. We confirm these observations with statistical tests: the coefficients of determination ( R 2 ) vary between 0.11 and 0.84, which indicates the presence of a trend (de-spite some variability). Furthermore, the p-values associated to the coefficients are all under 10  X  3 , this gives us excellent confidence that the strength influences positively the predic-tion accuracy. In conclusion, strong views are indeed more instructive. We now evaluate Claude X  X  output and runtime in detail. In this section, we verify if Claude X  X  algorithm produces good views in a short amount of time. To do so, we compare it to four methods, three of which come from the machine learn-ing literature. Our first baseline, Exact , is similar to Claude, but we removed the approximation scheme presented in 4 -instead we compute the exact the mutual information, as in Equation 6 The method should be slower, but more accu-rate.

The second algorithm, Clique , is a top-down approach inspired by recent work on pattern mining [24]. We build a graph where each vertex i represents a column D i , and each edge ( i,j ) represents the view { D i ,D j } . We then eliminate all the edges except those which represent the top B views. To detect views with D &gt; 2 columns, we seek cliques in this degenerated graph. We used the igraph package from R. We expect this algorithm to be very fast, but less accurate.
The third method, Wrap 5-NN , is a classic feature selec-tion algorithm [9]. The idea is to train a 5-Nearest Neighbor classifier with increasingly large sets of variables. We first test each variable separately, and keep the column which led to the best prediction. Then we keep adding variables in a breadth-first manner, until the quality of the predictions stops increasing or we reach n variables. Our implementa-tion is based on the class package from R. We modified the original algorithm to maintain and update q distinct sets of variables instead of just one. We chose the nearest neighbor algorithm because it is fast, and it gave us good performance, as shown in 7.2. We expect this algorithm to be very slow, but close to optimal.

Finally, the last method, 4S is a state-of-the-art subspace search method from the unsupervised learning literature [14]. The aim of the algorithm is to detect  X  X nteresting X  subspaces in large databases, independently of a target variable. To do so, it seeks groups of variables which are mutually corre-lated, with sketches and graph-based techniques. We used the author X  X  implementation, written in Java. We expect the algorithm to be very fast and reasonably accurate.
We use 8 public datasets, presented in Section 7. For a fair comparison, we must ensure that each algorithm generates the same number of views ( K ) with the same number of variables ( D ). However, we have no way to specify these parameters a priori with 4S, because the algorithm has a scores, the bars represent the lowest and greatest scores. 3,600 seconds. built-in mechanism to pick optimal values. Therefore, we run 4S first on each dataset, we let it chose K and D , and we use these values for the remaining algorithms. We report the obtained parameters in Table 1.

We implemented Claude in R, except for some information theory primitives written in C. For practical reasons, we interrupted all the experiments which lasted more than 1 hour. Our test system is based on a 3.40 GHz Intel(R) Core(TM) i7-2600 processor. It is equipped with 16 GB RAM, but the Java heap space is limited to 8 GB. The operating system is Fedora 16.

Accuracy. In Figure 9, we compare the quality of the views returned by each algorithm. For each competitor, we generate q views with n variables, train a classifier on each view and measure the quality of the predictions. For the classification, we use both Naive Bayes and 5-Nearest Neighbors, and report the highest score. We measure accu-racy with the F1 score on 5-fold cross validation; higher is better.

The method Wrap 5-NN comes first for all the datasets on which it completed. This is not surprising since the algorithm optimizes exactly what we measure: Wrap 5-NN is our  X  X old standard X . Our two algorithms, Claude and Exhaustive , come very close. This indicates that both algo-rithms find good views, and that our approximation scheme works correctly. The algorithms 4S and Clique come much lower. As 4S is completely unsupervised, we cannot expect it to perform as well as the other approaches. The assump-tions behind Clique are apparently too naive.

Runtime. Figure 10 shows the runtime of our exper-iments. The algorithms Exact and Wrap 5-NN are orders of magnitude slower than the other approaches. The re-maining three approaches are comparable: depending on the datasets, either Clique or 4S come first. Claude comes first for MuskMolecules , and close second for all the other Figure 11: Impact of the beam size on the execution time and view strength. For each dataset, we generated 25 views with beam size 25, 50, 100 and 250. The points represent the medium scores, the bars represent the lowest and greatest scores. datasets. In conclusion, Claude is comparable to its com-petitors in terms of runtime, but it generates better views.
Impact of the beam size. Figure 11 shows the impact of the beam size B on Claude X  X  performance, for 4 databases. To obtain these plots, we ran Claude with K = 25 and D = 5, and varied B between 25 and 250. We observe that smaller beams lead to lower execution times, while larger beam lead to stronger views. However, the heuris-tic converges fast: we observe little to no improvement for B greater than 50.

Impact of the deduplication. We show the impact of our deduplication strategy in Figure 12. We ran Claude with K = 25 and D = 5 and increased the level of deduplication, i.e., varied the value of B 0 between B and N.B (cf. Section 5.3). A level of 0% means that B 0 = B . A level of 100% bars represent the lowest and greatest scores.
 Figure 12: Impact of the deduplication. We generated 25 views for each dataset. The y-axis presents the average Jac-card dissimilarity between every pair of views. means that B 0 = N.B . To measure the diversity of the views, we measured the Jaccard dissimilarity between every pair of views and averaged the results. We observe that the strategy works in all four cases, but with different levels of efficiency. In the BankMarketing case, our strategy almost doubles the pairwise dissimilarity of the views. The effect is much lighter on datasets with few columns, such as USCensus and MAGICTelescope .
In this section, we evaluate Claude X  X  POI detection strat-egy. We compare two approaches. The first approach is the algorithm presented in the paper: first we search K views, then we return P POIs per view. The second approach, FullSpace , is the method used in much of the recent Sub-group Discovery literature [20, 7]. The idea is to apply Beam Search on the whole database directly. Instead of seeking P POIs in K projections, we seek K.P selections from the full column space; we skip the view selection step. We use the same datasets as previously. Our default parameters are K = 25, D = 5, B = 50 and P = 10. To set the beam size, we use a rule of thumb: B P OI = 2 .k ( B P OI is the beam used for POI detection, not for view search). To gather sufficient data, we raise our time limit to 2 hours.

Figure 13 compares the quality of the POIs found by both algorithms. The strategy FullSpace gives slightly better re-sults on Crime and PenDigits , but the difference is close to null. The scores are similar on all the other datasets. We conclude that Claude X  X  POIs are very close to those found by a state-of-the-art Subgroup Discovery approach. Figure 14 compares the runtimes of both approaches. We observe that Claude is much faster than FullSpace . The difference grows with the number of columns: the runtimes are almost sim-ilar for datasets with few columns ( MAGICTelescope ), but Claude is considerably faster for larger databases (more than an order of magnitude difference for MuskMolecules ). This is a positive side-effect of our approach: decoupling view search and POI extraction allows us to find subgroups faster in high dimension datasets.
SQL Query Recommendation. We identify two types of approaches: human-driven systems and data-driven sys-tems. Human-driven systems learn from user feedback. For instance, Chatzopoulou et al. make recommendations from query logs, similarly to search engines [2]. In Explore-by-Example, the system infers queries from examples provided by the user [5]. With Charles, the engine decomposes user queries into smaller queries, which can then be decomposed further [18]. Sarawagi X  X  method builds a maximum entropy model over the database from the user X  X  history [16]. Boni-fati et al. propose a similar method to recommend joins [1]. Claude competes with neither of these approaches, since it uses the content of the database only.

Our work is closer data-driven data recommendation. The general idea is to build a statistical model of the database, and find regions which behave unexpectedly. Sarawagi et al. have published seminal work on this topic [17]. Their system requires that the data is organized in an OLAP cube (with hierarchical dimensions), it supposes that the users know which variables to use, and it seeks thin-grained deviations. Oppositely, our system uses regular tables, it recommends views (not only selections) and it seeks large trends. Sim-ilarly, constrained gradient analysis [10, 6] focuses only on hierarchical data cubes. More recently, Dash et al. have proposed a method to reveal surprising subsets in a faceted search context [4]. This method is related to Claude, but it targets document search, it does not recommend views.
Projection Search. Authors from the data visualization literature have proposed methods to detect the  X  X est X  pro-jections of multidimensional data sets, such as Projection Pursuit [8], Scagnostics [22], or Tatu et al. X  X  relevance mea-sures [19]. Such methods would form excellent complements for Claude X  X  recommendations. Nevertheless, most of them focus on 2-dimensional scatterplots, are limited to continu-ous variables, and involve materializing and analyzing every possible 2D projection of the data.

Feature Selection, Subspace Search. Choosing which variables to use for classification or regression is a crucial problem, for which dozens of methods were proposed [9]. Similarly to Claude, some of these methods rely on mutual information [15]. Nevertheless, the objective is different. A feature selection algorithm seeks one set of variables, on which a statistical predictor will perform optimally. Claude seeks several, small sets of variables, simple enough to be in-terpreted by a humans. In fact, Claude is halfway between inference and exploration. On the unsupervised learning side, our work is close to subspace search. The idea is de-tect subspaces where the data is clustered distinctly [12, 14]. We compare Claude to state-of-the-art methods in our Ex-periments section.
We formalized what makes a query  X  X nteresting X , using the mutual information and the Kullback-Leibler divergence. We presented practical methods to detect these queries, us-ing carefully designed approximations. Finally, we presented and evaluated Claude, a system based on these ideas. The methods we developed for this study have broader applica-tions than the strict realm of query recommendation. Our column selection scheme competes with state-of-the-art fea-ture selection methods. Also, the idea to decouple column selection selection from subgroup search could benefit a wide range of subgroup discovery algorithms.

We are genuinely excited about the possible extensions of this study. Future work includes dealing explicitly with the structure of the data, e.g., hierarchical dimensions and relational joins. We will study how to integrate Claude more tightly with visualizations, for a fully interactive experience. Finally, we will refine our framework with causation models.
This publication was supported by the Dutch national program COMMIT.
