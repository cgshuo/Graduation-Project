 In tar get detection the objecti ve is to determine whether or not a given example is from a tar get class. Obtaining ground truth for the tar get class usually involv es a tedious process of manual labeling. If samples belonging to the tar -get class are labeled as positi ve, then negati ve class covers everything else. Due to the nature of the problem and the labeling process, the number of samples representing the tar get class is usually scarce whereas abundant data is po-tentially available to represent the negati ve class. In other words the data is highly unbalanced between classes favor-ing the negati ve class.
 In this process the actual labels of the counter -examples are ignored and the negati ve class is formed by pooling sam-ples of potentially dif ferent characteristics together within a single class. In other words samples of the negati ve class do not cluster well since the y can belong to dif ferent sub-classes.
 One promising approach that has been hea vily explored in this domain is the one-class classiers. One-class classi-cation simply omits the negati ve class (if it exists) and aims to learn a model with the positi ve examples only . Sev-eral techniques have been proposed in this direction. Sup-port vector domain description technique aims to t a tight hyper -sphere in the feature space to include most of the positi ve training samples and reject outliers (Tax &amp; Duin, 1999). In this approach the nonlinearity of the data can be addressed implicitly through the kernel evaluation of the technique. One-class SVM generates an articial point through kernel transformation for representing the negati ve class and then using relaxation parameters it aims to sepa-rate the image of the one-class from the origin (Scholk opf et al., 1999). Compression Neural Netw ork constructs a three-layer feed-forw ard neural netw ork and trains this net-work with a standard back-propogation algorithm to learn the identity function on the positi ve examples (Mane vitz &amp; Yousef, 2001).
 Discriminati ve techiques such as Support Vector Machines (Vapnik, 1995), Kernel Fisher Discriminant (Mika et al., 2000), Rele vance Vector Machines (Tipping, 2000) to name few are also used in this domain. These techniques deal with the unbalanced nature of the data by assigning dif ferent cost factors to the negati ve and positi ve samples in the objecti ve function. The kernel evaluation of these techniques yields nonlinear decision boundaries suitable for classifying multi-mode data from the tar get class. In this study we aim to learn a polyhedron in the feature space to describe the positi ve training samples. Polyhedral decision boundaries such as boundaries that are dra wn par -allel to the axes of the feature space as in decision trees or skewed decision boundaries (Murth et al., 1994) have existed for quite some time. Our approach is similar in some sense to the Support vector domain description tech-nique but there are two major dif ferences. First instead of a hypersphere, a polyhedron is used to t positi ve training samples. Second, positi ve and negati ve samples are used together in this process. The tar get polyhedron is learned through joint optimization of multiple hyperplane classi-ers, each of which is designed to classify positi ve samples from a subgroup of negati ve samples. The number of such hyperplane classiers is equi valent to the number of sub-classes identied in the negati ve class. The proposed tech-nique requires labeling of a small portion of the negati ve samples to collect training data for the subclasses that exist in the negati ve class.
 Our approach does not intend to precisely identify each and every subclass in the dataset. By manual labeling we aim to identify major subclasses. Subclasses with similar char -acteristics or with only few labeled samples can be grouped together . During annotation one may also encounter posi-tive look alik es, i.e. samples do not appear as negati ve but not yet conrmed as positi ve. A new subclass can be intro-duced for these samples.
 In Figure 1 the proposed algorithm is demonstrated with a toy example. Positi ve samples are depicted by the dark cir -cles in the middle, whereas negati ve samples are depicted with the numbers with each number corresponding to a dif-ferent subclass. All eight classiers are optimized simulta-neously and polygon sho wn with dark lines is obtained as a decision boundary that classies positi ve samples from the negati ve ones.
 Kernel-based classiers have the capacity to learn higly nonlinear decision boundaries allo wing great exibility . Ho we ver it is well-kno wn that in real-w orld applications where feature noise and redundanc y is a problem, too much capacity usually hurts the generalizability of a classier by enabling the classier to easily overt the training data. The proposed approach is capable of addressing nonlinear -ities by tting the positi ve class through a series of linear hyperplanes, all of which are optimized jointly to form a polyhedral decision boundary . The at faces pro vides ro-bustness whereas multiple faces contrib utes to the exibil-ity.
 The problem described abo ve is commonly encountered in areas lik e content-base image retrie val (Chen et al., 2001), document classication (Mane vitz &amp; Yousef, 2001) and speech recognition (Bre w et al., 2007). A similar scheme is also observ ed in Computer Aided Detection (CAD). In this study we explore the proposed idea for a CAD application, namely Colon CAD. We are given a training dataset f ( x &lt; d are input variables and y i 2 f 1 ; 1 g are class labels. We consider a class of models of the form f ( x ) = T x , with the sign of f ( x ) predicting the label associated with the point x . An hyperplane classier with hinge loss can be designed by minimizing the follo wing cost function. where the function : &lt; ( d ) ) &lt; is a regulariza-tion function or regularizer on the hyperplane coef cients and ( k ) f w i : w i 0 ; 8 i g is the weight preassigned to the loss as-sociated with x unbalanced data it is a common practice to weight positi ve and negati ve classes dif ferently , i.e. f w and f w responding sets of indices for the positi ve and negati ve classes respecti vely .
 The function 1 T y weighted sum of con vex functions is also con vex. There-fore for a con vex function ( ) (1) is also con vex. The problem in (1) can be formulated as a mathematical pro-gramming problem as follo ws: For ( ) = k k 2 in the con ventional Quadratic-Programming-SVM, and for ( ) = j j , where j : j is the 1-norm it yields the sparse Linear -Programming-SVM. 3.1. Training a Classifier with an AND Structur e We aim to optimize the follo wing cost function where e the i -th training example f ( x by classier k . C samples in subclass-k. Note that classier k is designed to classify positi ve examples from the negati ve examples in the subclass-k. The rst term in (3) is a summation of the regularizers for each of the classiers in the cascade and the second and third terms accounts for the losses induced by the negati ve and positi ve samples respecti vely . Unlik e (1) the loss function here is dif ferent for the positi ve samples. The loss induced by a positi ve sample i , i 2 C + is zero only if 8 k : 1 T operation. The problem (3) can be formulated as follo ws where the rst two constraints are imposed for 8 i 2 C k = 1 ; :::; K and the last two constraints are imposed for 8 i 2 C + , k = 1 ; :::; K . Note that for a con vex function ( ) the problem in (4) is con vex. In a nutshell we de-signed K classiers, one for each of the binary classica-tion problems, i.e. positi ve class vs subclass-k of the nega-tive class. Then we construct a learning algorithm to jointly optimize these classiers such that the cost induced by a positi ve sample is zero if and only if all of the K classiers classies this sample correctly , i.e. 8 k : 1 T Since each negati ve sample is only used once for training the classier k , the cost induced for a negati ve sample is zero as long as it is classied correctly by the correspond-ing classier k , i.e. 1 + T an arbitrary subset of the original feature set. This pro vides run time adv antages in real-time when the classication ar-chitecture is implemented in a cascaded frame work. This will be explained later in the paper . For now to keep the no-tation clean and tractable we assumed each classier uses the entire feature set in the formulation (4) abo ve. 3.2. Training a Classifier with an AND-OR Structur e The AND algorithm is developed with the assumption that the negati ve class is fully labeled. That is to say , the sub-class membership of each of the negati ve sample is kno wn apriori. For most real world applications this is not a very realistic scenario as it is almost impractical to label all of the negati ve samples due to the time limitations. Ho we ver one can label a small subset of the negati ve class to disco ver dif ferent type of subclasses as well as pool the training data for each subgroup. To accommodate for the unlabeled sam-ples we modify the equation (3) for the unlabeled negati ve samples as follo ws.
 where C is the set of indices of the unlabeled negati ve samples. The rst term in (5) requires a labeled sample from a subclass-k to be correctly classied by the classi-er k . In other words if a sample is kno wn to be a mem-ber of subclass-k, ideally it should be classied as negati ve by the corresponding classier k . On the other hand the second term requires an unlabeled negati ve sample to be correctly classied by any of the classiers. As long as an unlabeled sample is classied as negati ve it does not matter which classier does it, i.e. 9 k : 1 T corresponds to a  X OR X  operation. The third term requires a positi ve sample is classied as positi ve by all of the classiers, i.e. 8 k : 1 T the  X AND X  operation.
 Due to the product operation in the objecti ve function for unlabeled samples, unlik e equation (3), equation (5) can not be cast as a con vex programming problem. In the next section we propose an efcient alternating optimization al-gorithm to solv e this problem. We develop an iterati ve algorithm which, at each iteration, carries out K steps, each aiming to optimize one classier at a time. This type of algorithms is usually called alternat-ing or cyclic optimization approaches. At any iteration, we x all of the classiers but the classier k . The x ed terms have no effect on the optimization of the problem once the y are x ed. Hence solving (5) is equi valent to solving the fol-lowing problem by dropping the x ed terms in (5): where w constrained problem as follo ws where and ` problem in (5) is a con vex problem and dif fers from the problem in (2) by two small changes. First the weight as-signed to the loss induced by the negati ve samples is now adjusted by the term w multiplies out to zero for negati ve samples correctly clas-sied by one of the other classiers. For these samples e ik &lt; 0 dundant. As a result there is no need to include these sam-ples when training for the classifier -k , which yields signif-icant computational adv antages. Second the lower bound for is now max 0 ; e This implies that if any of the classiers in the cascade misclassies x relaxing the constraint on x 4.1. An Algorithm for AND-OR Lear ning (0) Initialize e (i) Fix all the classiers in the cascade except classi-(ii) Compute J c ( (iii) Stop if J c J c 1 is less than some desired tolerance. The initial objecti ve function in (5) is neither con vex nor twice dif ferentiable due to the product of the hinge loss term. Therefore the con vergence theorem introduced in (Bezdek &amp; Hatha way, 2003) for cyclic optimization does not hold here. On the other hand the subproblem in (5) is con vex and hence at each iteration J c &lt; = J c 1 holds and also (5) is bounded belo w. These guarantee con ver-gence of the algorithm from any initial point to the set of suboptimal solutions. The solution is suboptimal if the ob-jecti ve function J can not be further impro ved follo wing any directions. For a more detailed discussion on this topic please see (Dundar &amp; Bi, 2007).
 An unseen sample x can be classied as positi ve if max (1 T 1 x; : : : ; 1 T K x ) and as negati ve if vice versa for a threshold . The recei ver operating character -istics (ROC) curv e can be plotted by varying the value of . In Figure 2 a cascade classication scheme is sho wn. The key insight here is to reduce the computation time and speed-up online learning. This is achie ved by designing simpler classiers in the earlier stages of the cascade to re-ject as man y negati ve candidates as possible before calling upon classiers with more features to further reduce the false positi ve rate. A positi ve result from the rst classier acti vates the second classier and a positi ve result from the second classier acti vates the third classier , and so on (Vi-ola &amp; Jones., 2004). A negati ve outcome for a candidate at any stage in the cascade leads to an immediate rejection of that candidate. Under this scenario T and T candidates labeled as positi ve and negati ve respecti vely by classier k .
 The proposed algorithm learns a polyhedron through jointly optimizing a series of sparse linear classiers. Since the order these classiers are executed in real-time does not matter in terms of the overall prediction accurac y of the system, we can arrange the execution sequence of these classiers in a way to optimize the run-time. Let the set of indices of nonzero coef cients for classifier -k , be the time required to compute featur e i for a given candi-date, i = 1 ; : : : ; d and n T of the algorithm is The sets hedral classier and are x ed for online execution. Ho w-ever the sets T classifier k-1 . Therefore this is a combinatorial optimiza-tion problem with K ! dif ferent outcomes, where K ! is the factorial of K . For small K one can try the exhausti ve number of orderings between classier to nd the optimum sequence. Ho we ver when K is lar ge we can start with the most spar se classier , i.e. the linear classier with the least number of nonzero coef cients and choose the next classi-er as the one that will require computing least number of additional features. The goal of a Computer Aided Detection (CAD) system is to detect potentially malignant tumors and lesions in med-ical images (CT scans, X-ray , MRI etc). In an almost uni-versal paradigm for CAD algorithms, this problem is ad-dressed by a 3 stage system: A typical CAD system con-sists of a candidate generation phase, a feature extraction module and a classier . The task of the candidate gener -ation module is to create a list of potential polyps with a high sensiti vity but low specicity . Features are then ex-tracted for each candidate and eventually passed to a classi-er where each candidate is labeled as normal or diseased. In order to train a CAD system, a set of medical images (eg CT scans, MRI, X-ray etc) is collected from archi ves of community hospitals that routinely screen patients, e.g. for colon cancer . These medical images are then read by ex-pert radiologists; the regions that the y consider unhealthy are mark ed as ground-truth in the images. After the data collection stage, a CAD algorithm is designed to learn to diagnose images based on the expert opinions of the radi-ologists on the database of training images. First, domain kno wledge engineering is emplo yed to (a) identify all po-tentially suspicious regions in a candidate generation stage, and (b) to describe each such region quantitati vely using a set of medically rele vant features based on for example, texture, shape, intensity and contrast. Then, a classier is trained using the features computed for each candidate in the training data and the corresponding ground truth. When training a classier for a CAD system for detection of colonic polyps, the only information that is usually avail-able is the location of polyps, since radiologists only mark unhealthy regions when the y are reading cases. This, of course, is very important for training a CAD system. But for all other structures that are pick ed up during candidate generation phase that are not pointing to a kno wn lesion there is no other information available and the y all have to be treated equally as negati ve examples. This introduces two complications. First all the negati ve candidates are clustered in one group although variation in size and shape among them is very big and valuable information about those candidates, e.g. type cate gory , is not used. Second, radiologists only mark lesions of clinical importance, i.e. polyps greater than 6mm in colon. It is also possible that some lesions are overlook ed during clinical evaluation. So potentially there are unidentied lesions in our dataset with no matching ground truth. If the candidate generation al-gorithm generates candidates for these lesions then these candidates are also mark ed as negati ve together with all the other candidates with no corresponding radiologist marks. In other words negati ve class may also contain unidentied samples of the tar get class.
 In the rest of this section we will discuss some moti vation for the proposed algorithm within the scope of a CAD sys-tem designed to detect colorectal cancer . Colorectal can-cer is the second leading cause of cancer -related death in the western world (Jemal et al., 2004). Early detection of polyps through colorectal screening can help to pre vent colon cancer by remo ving the polyps before the y can turn malignant.
 Typical examples of dif ferent polyp morphologies are given in Figure 3.
 The commonly encountered false positi ve types in colon are fold, stool, tagged stool, meniscus, illeocecal valv e, rec-tum etc. Some of these are sho wn in Figure 4. Ideally we can label all of the negati ve candidates in the training data and use the proposed AN D algorithm in (4) to jointly train classiers one for each of the subclasses of negati ve sam-ples. Ho we ver an exhausti ve annotation of all negati ve ex-amples is not feasible. Therefore we rst select a very small subset of the negati ve candidates and annotate them manu-ally through visual inspection. Then this set together with the positi ve samples and the remaining negati ve samples, i.e. unlabeled samples is used in the proposed AN D OR frame work to train the classiers. We validate the proposed polyhedral classier ( polyhedral) with respect to its generalization performance and run-time efcienc y. We compared our algorithm to a Support Vector Domain Description technique ( svdd) (Tax &amp; Duin, 1999), nonlinear SVM with Radial Basis Function ( rbf), and one-norm SVM ( sparse). To achie ve sparseness we set the 7.1. Data and Experimental Settings The database of high-resolution CT images used in this study were obtained from two dif ferent sites across US. The 370 patients were randomly partitioned into two groups: training (n=167) and test (n=199). The test group was se-questered and only used to evaluate the performance of the nal system.
 Training Data Patient and Polyp Info: There were 167 pa-tients with 316 volumes. The candidate generation (CG) al-gorithm identies a total of 226 polyps at the volume level across all sizes while generating a total of 64890 candi-dates or an average of 205 false positi ves per volume. Test-ing Data Patient and Polyp Info: There were 199 patients with 385 volumes. The candidate generation (CG) algo-rithm identies a total of 245 polyps at the volume level across all sizes while generating an average of 75946 sam-ples or 194 false positi ves per volume (fp/v ol). A total of 98 features are extracted to capture shape and intensity characteristics of each candidate. The proposed algorithm requires a small set of false positi ves anno-tated. Rather than labeling false positi ves randomly across a dataset with 64890 samples we used the output of the most recent prototype classier for labeling. This classi-er is trained using a nai ve SVM and optimized for the 0-5 fp/v ol range. This way we only focus on the most challeng-ing false positi ves. This classier marks a total of 1432 candidates as positi ve. Out of these candidates 1249 are false positi ves. A small subset of the volumes from this set is chosen for labeling and a total of 177 false positi ves (out of 1249) are annotated and ten dif ferent subcate gories are identied. 7.2. Perf ormance Ev aluation The classiers are trained with the combination of 1249 false positi ves generated by the prototype classier and all the polyps the candidade generation detects in the training data. A total of 1560 candidates are used for training. Clas-siers are evaluated on the 1920 candidates the prototype classier mark ed as positi ve in the testing data. The corre-sponding Recei ver Operating Characteristics (ROC) curv es for each algorithm is plotted in Figure 5.
 The classier parameters are estimated using a 10-fold pa-tient cross validation from a set of discrete values using the training data. These are namely the width of the kernel ( =[0.01 0.03 0.05 0.1 0.3 0.5 1 5]) for rbf , svdd , the cost factor ( c =[5 10 15 20 25 50 75 100]) for rbf , polyhedr al , spar se and the =[0.001 0.005 0.01 0.05 0.1 0.2] parame-ter for svdd . The desired tolerance value for Algorithm 4.1 is set to 0.001. The algorithm con verged in less than 10 iterations.
 As sho wn in Figure 5 the ROC curv e corresponding to the proposed polyhedr al classier is consistently dominating all the other curv es. The curv e associated with the spar se SVM is almost linear implying a random beha vior . This is not surprising to a greater extent as both the training and testing data sets used in this experiment are deri ved from the initial datasets via a linear SVM classier . In other words the datasets are composed of samples mark ed as pos-itive by the linear SVM, a signicant portion of which are false positi ves. The one-class SVM is only slightly better than the spar se SVM . Ev en though the rbf SVM is the best of the three competitor algorithms, the dif ference in sen-siti vity between the rbf SVM and the proposed polyhedr al classier can be as high as 5% ( 10 polyps) in favor of the proposed algorithm. 7.3. Run-time Ev aluation As stated earlier in the paper , run-time speedups can be achie ved as a by-product of the proposed algorithm when the real-time classication is implemented in a cascaded frame work as in Figure 2. For a more detailed discussion of cascade classiers in the CAD domain we refer the in-terested readers to these recent works (Dundar &amp; Bi, 2007), (Bi et al., 2006).
 To avoid any delays in the worko w of a physician the CAD results should be ready by the time physician com-pletes his own revie w of the case. Therefore there is a run-time requirement a CAD system needs to satisfy . Among the stages involv ed during online processing of a volume, feature computation is by far the most computational stage of online processing. A cascaded frame work for executing the classier in the order of increasing feature comple x-ity may bring signicant computational adv antages in this case. In this frame work the cascade is designed so as to execute classiers with less number of features earlier in the sequence. This way the additional features required for the succeeding classiers will only be computed on the re-maining candidates, i.e. candidates mark ed as positi ve by all of the pre vious classiers.
 In this section we evaluate the speedups achie ved by the proposed classier . We set the operating point at 60% specicity , around 2.2fp/v ol. At this specicity the pro-posed polyhedral classier yields 85% sensiti vity (see Fig-ure 5). Assuming each feature tak es on the average t sec-onds per candidate to compute we came up with the table in 1. This table sho ws the aggre gate number of features used, feature computation time and number of candidates rejected at each stage in the sequence.
 Feature computation for the proposed approach on average tak es 452t secs per volume. On the other hand for svdd and rbf , which require computation of all the features at once, this stage tak es 595t secs. This represents a roughly 25% impro vement in run-time execution speed of the system. For the one-norm SVM spar se , this time is 437t secs. Ho w-ever the corresponding sensiti vity at this operating point for one-norm SVM is around 40% vs 85% for the proposed technique. In this study we have presented a methodology to tak e ad-vantage of the subclass information information available in the negati ve class to achie ve a more rob ust description of the tar get class. The subclass information which is ne-glected in con ventional binary classiers pro vides a bet-ter insight of the dataset and when incorporated into the learning mechanism acts as an implicit regularizer on the classier coef cients. We belie ve this is an important con-trib ution for applications where feature noise is pre valent. Highly nonlinear kernel classiers pro vides exibility for modeling comple x data but the y tend to overt when there are too man y redundant and irrele vant features in the data. Linear classiers on the other hand do not have enough capacity to model comple x data but the y are more rob ust when there is noise. The polyhedral classier is proposed as a midw ay solution. The linear faces of the polyhedron achie ves rob ustness whereas multiple faces pro vides exi-bility .
 The order in which the classiers are executed during on-line execution does not matter . Ev en though nding the globally optimum sequence is an open research problem for a lar ge number of subclasses, the ordering of the clas-siers can be arranged in a cascaded manner to reduce the total run-time of the system.
 Bezdek, J., &amp; Hatha way, R. (2003). Con vergence of alter -nating optimization. Neur al, Parallel Sci. Comput. , 11 , 351 X 368.
 Bi, J., Periasw amy , S., Okada, K., Kubota, T., Fung, G.,
Salganicof f, M., &amp; Rao, R. B. (2006). Computer aided detection via asymmetric cascade of sparse hyperplane classiers. Proceedings of the Twelfth Annual SIGKDD International Confer ence on Knowledg e Disco very and Data Mining (pp. 837 X 844). Philadelphia, PA.
 Bre w, A., Grimaldi, M., &amp; Cunningham, P. (2007). An evaluation of one-class classification tec hniques for speak er verification (Technical Report UCD-CSI-2007-8). Uni versity Colle ge Dublin.
 Chen, Y., Zhou, X. S., &amp; Huang, T. S. (2001). One-class svm for learning in image retrie val. ICIP (1) (pp. 34 X 37). Dundar , M., &amp; Bi, J. (2007). Joint optimization of cascaded classiers for computer aided detection. Proceedings of the IEEE Confer ence on Computer Vision and Pattern Reco gnition (CVPR) (pp. 1 X 8).
 Jemal, D., Tiwari, R., Murray , T., Ghafoor , A., Saumuels,
A., Ward, E., Feuer , E., &amp; Thun, M. (2004). Cancer statistics.
 Mane vitz, L. M., &amp; Yousef, M. (2001). One-class svms for document classication. Journal of Mac hine Learning Resear ch , 2 , 139 X 154.
 Mika, S., R X  atsch, G., &amp; M  X  uller , K.-R. (2000). A mathe-matical programming approach to the kernel sher algo-rithm. NIPS (pp. 591 X 597).
 Murth, S. K., Kasif, S., &amp; Salzber g, S. (1994). A system for induction of obligue decision trees. Journal of Artificial Intellig ence Resear ch , 2 , 1 X 33.
 Scholk opf, B., Platt, O., Sha we-T aylor , J., Smola, A., &amp;
Williamson, R. (1999). Estimating the support of a high-dimensional distrib ution.
 Tax, D. M. J., &amp; Duin, R. P. W. (1999). Support vector do-main description. Pattern Reco gnition Letter s , 20 , 1191 X  1199.
 Tipping, M. E. (2000). The rele vance vector machine. In
S. Solla, T. Leen and K.-R. Muller (Eds.), Advances in neur al information processing systems 12 , 652 X 658. Cambridge, MA: MIT Press.
 Vapnik, V. N. (1995). The natur e of statistical learning theory . Ne w York: Springer .
 Viola, P., &amp; Jones., M. (2004). Rob ust real-time face de-
