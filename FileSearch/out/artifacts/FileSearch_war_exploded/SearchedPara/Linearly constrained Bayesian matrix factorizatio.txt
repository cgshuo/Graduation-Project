 correlations, maximizing statistical independence, or fit ting data subject to constraints. as a matrix factorization problem, where the subscripts below the matrices denote their dimens ions. The columns of A represent K unknown sources, and the elements of B are the mixing coefficients. Each of the J columns of matrix factorizations with different characteristics can be devised. b factor analysis and (d) non-negative matrix factorization . convex polytope.
 factorization techniques.
 Bayesian matrix factorization based on Gibbs sampling has b een demonstrated [7, 8] to scale up of the algorithm proposed here.
 Section 4, the method is applied to an unsupervised source se paration problem and compared to other existing matrix factorization methods. We discuss ou r results and conclude in Section 5. for efficient inference based on Gibbs sampling. 2.1 Noise model We choose an iid. zero mean Gaussian noise model, where, in the most general formulation, each matrix element has its own variance, v is given by p ( x |  X  ) = where  X  = A , B , { v we choose conjugate inverse-gamma priors, 2.2 Priors for sources and mixing coefficients we specify the matrices by vectors a = vec( A  X  ) = [ a [ b and equality constraints, R , actual mean and covariance of a and b depends on the constraints.
 are biaffine maps, that define N ically, each inequality constraint has the form By rearranging terms and combining the N to a linear form in b . The equality constraints, R , are defined analogously. both through their covariance matrix,  X  indicated indices. restrict the model such that a and b are independent a priori by setting  X  and restricting q ( a ) the elements of A , or groups of elements such as rows or columns, by choosing  X  have an appropriate block structure. Similarly we can decou ple elements of B . 2.3 Posterior distribution is given by tion of the model is given in Figure 2. Markov chain Monte Carlo (MCMC). 3.1 Gibbs sampling We propose an inference procedure based on Gibbs sampling. G ibbs sampling is applicable when posterior.
 we consider the noise variances, v inverse-gamma, from which samples can be generated using standard acceptan ce-rejection methods. the model. Conditioned on b , the prior density of a is a constrained Gaussian, case when a and b are independent in the prior, we simply have  X   X  where V = diag( v repetitions of B .
 and third, b is generated from a constrained Gaussian analogous to Eq. (1 3). 3.2 Sampling from a constrained Gaussian constraints. With a slight change of notation, we consider g enerating x  X  R N from the density A similar problem has previously been treated by Geweke [2], who proposes a Gibbs sampling sampling procedure that handles any number of equality and i nequality constraints. compute an orthonormal basis, T , for the constraints, as well as its orthogonal complement, T where S variable, y , that is related to x by where x inverse, x equality constraints, which is Gaussian subject to inequal ity constraints, where  X  = T We introduce a second transformation with the purpose of red ucing the correlations between the Gaussian. To this end, we define the transformed variable, z , given by where L is the Cholesky factorization of the covariance matrix, LL  X  =  X  is then a standard Gaussian subject to inequality constrain ts, We can now sample from z using a Gibbs sampling procedure by sweeping over the elemen ts z Gaussian, efficient mixed rejection sampling algorithm proposed by Ge weke [2]; or slice sampling [5]. The upper and lower points of truncation can be computed as where [ Q vector of all elements of z except the i th.
 formed into a sample of the original variable, x , using The sampling procedure is illustrated in Figure 3. component analysis (ICA) and non-negative matrix factoriz ation (NMF). Data We used a subset from the MNIST dataset which consists of 28  X  28 pixel grayscale images arranged the vectorized images as the columns of the matrix X  X  R I  X  J , where I = 784 and J = 4 , 000 . Examples of the image mixtures are shown in Figure 4.b. x given x current value in the other dimensions.
 4 exemplars on average for each digit.
 Method For comparison we factorized the mixed image data using two s tandard matrix factor-ization techniques: ICA, where we used the FastICA algorith m, and NMF, where we used Lee and in Figure 4.c X  X .
 image mixtures, 0  X  a to be non-negative, b are shown in Figure 4.e, which displays a single sample of A at the last iteration. the estimated sources lack a clear interpretation.
 The sources computed using NMF (see Figure 4.d) have the prop erty which Lee and Seung [3] different digits can be constructed by combining these part s.
 Compared to the original data, the computed sources are a bit bolder and have slightly smeared (a) Original dataset: MNIST digits (b) Training data: Mixture of digits (c) Independent component analysis (d) Non-negative matrix factorization (e) Linearly constrained Bayesian matrix factorization text). another is an all white feature, which are useful for adjusti ng the brightness. our method finds sources that visually resemble handwritten digits. data domain, i.e., on the product AB .
 As a general framework for constrained Bayesian matrix fact orization, the proposed method has to develop methods for learning relevant constraints from d ata. [5] R. M. Neal. Slice sampling. Annals of Statistics , 31(3):705 X 767, 2003.
