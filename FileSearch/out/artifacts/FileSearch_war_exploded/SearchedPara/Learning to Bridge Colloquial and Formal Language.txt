 We study the problem of linking information between differ-ent idiomatic usages of the same language, for example, col-loquial and formal language. We propose a novel probabilis-tic topic model called multi-idiomatic LDA (MiLDA). Its modeling principles follow the intuition that certain words are shared between two idioms of the same language, while other words are non-shared, that is, idiom-specific. We demonstrate the ability of our model to learn relations be-tween cross-idiomatic topics in a dataset containing prod-uct descriptions and reviews. We intrinsically evaluate our model by the perplexity measure. Following that, as an ex-trinsic evaluation, we present the utility of the new MiLDA topic model in a recently proposed IR task of linking Pin-terest pins (given in colloquial English on the users X  side) to online webshops (given in formal English on the retailers X  side). We show that our multi-idiomatic model outperforms the standard monolingual LDA model and the pure bilingual LDA model both in terms of perplexity and MAP scores in the IR task.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval-Information filtering topic models; unstructured data; user interests; recommen-dation systems; user-generated data; personalized linking
Colloquialisms are words or phrases employed in conver-sational or informal language, but not in formal writing. As more users become producers of content, a large part of the Web, like blogs, reviews and social media sites, may be full of colloquial expressions. In contrast, the language used elsewhere on the Web is more formal. News sites, online retailers or sites for expert knowledge are typical examples. For a given news article, an event may be described using formal language. On the other hand, users are free to com-ment on the article. Users X  comments refer to the same event, but the language tends to be more informal. A simi-lar situation applies to online retailers, where products tend to be described in a formal way. The reviews refer to the same product, but the language use is rather informal.
In this work, the goal is to bridge these two idioms of what is essentially the same language: colloquial and formal. This is, for instance, particularly important for retrieval tasks in e-commerce. For example, to find products, users may is-sue queries on retail sites like Amazon.com and eBay.com . A common problem is that the language and words chosen by the user may differ significantly from those in the product description. Suppose a user is looking for a particular kind of lamp, one that looks like a mushroom. To this user, it may seem like  X  X ushroom lamp X  would be a good query to find it. Note that the phrase  X  X ushroom lamp X  is actually found on social media sites to describe such items. However, on the retail site, the same product is described as  X  X ramble Toadstool Nightlight Red X  . It is clear that users and retail sites may be talking about the same objects, but they choose different words to describe them. Although the product is relevant for the query, it may not be retrieved under a tradi-tional document representation, such as bag-of-words. The idea is to develop a model that is able to learn how these two  X  X anguages X  are related, and consequently has the ability to link knowledge from the users X  side which uses the colloquial language to the target side given in the formal language.
The contributions of this paper are as follows. We pro-pose, describe and evaluate a novel unsupervised topic model called multi-idiomatic LDA (MiLDA) which is able to deal with such multi-idiomatic data, taking into account both the knowledge of shared and non-shared words across two dif-ferent idioms of the same language. We train our model on a dataset that contains Amazon.com  X  X  product descriptions paired with the corresponding product reviews, and then test it in the task of linking users X  Pinterest pins to relevant webshops [7]. After the inference on the test dataset, we demonstrate that our new model obtains lower perplexity as well as better mean average precision (MAP) scores than standard monolingual LDA (which does not distinguish be-tween different idioms in the same language) and bilingual LDA (which treats two different idioms of the same language as two completely separate languages) which were trained on the same training collection. Moreover, we also outperform the best results previously reported in [7].
Monolingual and multilingual probabilistic topic models have been proven as a powerful unsupervised toolkit to an-alyze large text collections. However, no prior work has fo-cused on capturing the evidence coming from multi-idiomatic data such as products descriptions (given in a formal lan-guage) which are naturally linked to their reviews (given in a colloquial user-centered language). On one hand, standard monolingual topic models such as LDA [1] do not distinguish between two different idioms of the same language at all. On the other hand, standard multilingual topic models such as bilingual or polylingual LDA [2, 4, 3] treat two different id-ioms of the same language as two totally separate languages. They do not take into account that a significant portion of words and phrases does not exhibit idiomatic usage and is shared across two idioms. The utility of multilingual topic models has been demonstrated when they are trained on multilingual aligned Wikipedia articles, but here we show that they have limited capability of dealing with different, multi-idiomatic data.

In [7], a new task of linking users X  Pinterest pins to web-shops has been proposed. Combining Pinterest data -given in the colloquial user-generated language-with Amazon web-shops -given in the formal language-implies the need for multi-idiomatic text processing. [7] already demonstrated the utility of topical representations for linking models in this task, but they reported only preliminary results where a monolingual LDA model was trained on the target col-lection to improve overall retrieval results. In contrast, in this paper, we target to learn true  X  X ross-idiomatic X  topics on a training collection consisting of Amazon X  X  product de-scriptions aligned to their respective reviews, and then infer these topics on the target collection for retrieval. Intuition. Assume you have a collection of document pairs. Each pair comprises a product description coupled with the corresponding users X  reviews. The product description in the pair is written in formal language (e.g. written by retailers or experts) and the other in colloquial language (e.g., written by laymen or users). Given such collection, the goal is to find how these two language idioms (colloquial vs. formal) relate to each other. This knowledge might prove its potential in a task such as linking user-generated data (e.g., Pinterest pins) to online webshops [7].

Given this setup, our new multi-idiomatic LDA (MiLDA) model takes into account two main points: (1) A pair of documents shares the same distribution over topics (i.e., in essence, they talk about the same product). This is concep-tually similar to a requirement from bilingual topic modeling [2, 4, 3], where a pair of news articles or Wikipedia articles discusses the same topics in two different languages; (2) A portion of words in the colloquial idiom differs from those in the formal idiom and vice versa. Moreover, a portion of words is shared between the two language idioms, and each document may be observed as a bag of shared and non-shared words combined together. This is conceptually different from [2, 4, 3], where it was assumed that each lan-guage has a unique set of words and no words are shared. In that case, given two languages, these bilingual topic models induce two unique sets of per-topic word distributions, each for one language. Here, we introduce the third set of per-topic word distributions, taking into account the knowledge of shared words. As a result, each latent  X  X ross-idiomatic X  topic is represented as a mixture of: (i) its idiom-specific per-topic word distributions over non-shared words; and (ii) idiom-shared per-topic word distributions (i.e., distributions Figure 1: Graphical representation of the multi-idiomatic LDA (MiLDA) model in plate notation. over shared words). Description of the Model. Fig. 1, shows the plate representation of our new MiLDA model. To address point (1) above, we consider that both documents in a pair have the same topic distribution  X  , which is sam-pled from a symmetric Dirichlet with hyperparameter  X  . To address point (2), we consider three sets of per-topic word distributions: one unique to the colloquial idiom, (  X  ); one unique to the formal idiom, (  X  ); and one common to both idioms, (  X  ). These distributions are independently drawn from a symmetric Dirichlet distribution with hyper param-eter  X  .
 We use a superscript S or T to differentiate the idioms or lan-guages, e.g., colloquial vs. formal, or more generally source ( S ) vs. target ( T ). s S ji is a precomputed indicator that re-veals whether the word at position i is shared ( s S ji = 1) or unique ( s S ji = 0) to this idiom. As a simple heuristic, we as-sume that all words which occur on both sides of the given document collection are shared words. As a consequence in our model, s S ji and s T ji are fully observed because once we see the full corpus, it is trivial to determine whether a word position contains a shared word or not. Finally, algorithm 3.1 shows the full generative story of the MiLDA model. Training and Output. We train our MiLDA model using Gibbs sampling [5]. For the source language S , if s S ji where  X  = ( z S  X  ji , z T , w T , w S , X , X  ). Whereas, if s Analogous equations can be derived for documents in the target language. V S and V T denote vocabularies of non-shared words for the respective source and the target idioms, while V C is a vocabulary of words shared between the two idioms. n j,k denotes the number of times topic k is assigned within document d j ; while n j,k,  X  i has the same meaning but not counting the current assignment to w ji . v k,w ji ,  X  the number of times a word w ji has been assigned to topic k , not counting the current position. When a dot ( X . X ) appears in the subscript of a variable, it means that we sum over all the possible values of the corresponding variable 1 .
After the burn-in period, we obtain the document-topic distribution, P ( z k | d j ), which indicates the probability of each topic k in a document d j .
 We also obtain three per-topic distributions: one for the colloquial (or source) idiom words; one for the formal (or target) idiom words; and one for the shared words between the languages. Each of these indicate the probability of a word given a topic, as shown by eq. (4): where the index L may refer to S , T or C . Evaluation Task: Linking Pins to Webshops. To eval-uate the utility of the new multi-idiomatic topic model, we perform the same retrieval task as proposed in [7]. The task is to link users X  pins posted on the social media site Pin-terest.com given by their textual description to a set of relevant webshops where the user might search for or buy the  X  X inned X  product. It is essentially a retrieval task in which, given the pin as a query [7], one obtains a list of relevant webshops.
 Target Collection and Queries. The target collection is obtained from [7] and consists of 19, 955 product descriptions from Amazon.com grouped into 1, 171 webshops. One prod-uct may belong to more than one webshop. The query set consists of 50 users X  pins in text format. An example query is  X  X e daring, go all out in red! Modern Jessica Rabbit X . Training Collection. We use a training dataset that or-ganically aligns documents written in colloquial and formal language. The training dataset consists of 15, 566 aligned document pairs, where each pair consists of: (1) a descrip-tion of a product acquired from Amazon.com (formal lan-guage), (2) a merged collection of top 10 most helpful users X  reviews for that particular product (colloquial language). Models for Comparison. We experiment with three dif-ferent topic models that capture three different paradigmatic approaches to the data: (1) monolingual LDA (which does not exploit the natural links in the training dataset and treats all documents as given in only one language), (2) bilin-gual LDA (which observes two idioms as two separate lan-guages, and uses the alignment in the training dataset), (3) our new multi-idiomatic LDA. All models have been trained with the same number of topics ( K =100, 200, 500, 800, 1000, 1200, 1500) with the same number of iterations (1000) on the same training collection with the same parameter setup, as hyperparameters are set to the standard values  X  = 50 /K and  X  = 0 . 01, according to [6, 5]. The trained models are then inferred on the previously unseen target collection and we test their utility in the task of linking pins to webshops. Retrieval Models . To perform the actual retrieval, we adapt a well-known LDA-based probabilistic unigram re-trieval model from [6], which combines a topical represen-tation ( tr ) with the regular count-based bag-of-words ( bow ) document representation (see [6] for more details and pa-rameter settings): where Q S is a query (i.e., a pin) containing m query words q ,...,q S m given in the source idiom (i.e., colloquial lan-guage), d T j a j -th document from the target collection given in the target idiom (i.e., formal language), and  X  is the inter-polation parameter. The  X  X opical X  contribution P tr ( q S is computed as follows: P tr ( q S i | d T j ) = The probability P ( z k | d T j ) is known after a topic model is in-ferred on a target document d T j . When performing retrieval with MiLDA, we introduce one major difference when com-puting P tr ( q S i | d T j ). Namely, we can propagate the knowl-edge of shared and non-shared words from the training to our target collection. When the query word q S i happens to be a shared word in the training collection, we compute the probability by using the estimated per-topic word distribu-tions for shared words (  X  , see eq. (4)). Otherwise, we rely on the idiom-specific per-topic word distributions (  X  ). Test 0: Qualitative Evaluation. Tab. 1 shows example lists of the top 5 frequent words in the word distributions for three topics: photography, coffee and tanning. For instance, the shared vocabulary distribution (  X  ) contains words likely expected in a photography topic, such as lens , focus and canon . In the users X  language side, we -as non experts in the subject-learned new words that belong to this topic. For example, bokeh means blur or the aesthetic quality of the blur of an image. It refers to  X  X he way the lens ren-ders out-of-focus points of light X . For the topic coffee , the shared word distribution shows again typical words, such as espresso , press or beans . On the users X  side, we again learn new words associated with the topic. Illy , tierra , ro-busta and gaggia are all brands related to coffee or coffee machines. This is also useful for merchants interested in learning about leading or most-talked-about brands. Test I: Perplexity. A standard way to compare the qual-ity of topic models is the perplexity measure, an intrinsic evaluation metric that evaluates the topic model X  X  capabil-ity of predicting previously unseen documents [1]. A lower perplexity score implies that the model provides a better explanation and a better representation for unseen docu-ments. Fig. 2(a) compares the perplexity scores for the three models in comparison (LDA vs. BiLDA vs. MiLDA). We distribution, users X  (reviews-only) vocabulary distribution may observe that MiLDA consistently outscores the other two models which implies that the true multi-idiomatic text processing as modeled by MiLDA is more beneficial than monolingual (LDA) or multilingual (BiLDA) in this setting. Test II: Linking Pins to Webshops. In another eval-uation test, we measure the ability of the retrieval models from sect. 4 to link relevant webshops to queries/pins given the three different topical representations of target collection documents (again, LDA vs. BiLDA vs. MiLDA). We can again observe that MiLDA outperforms the other two mod-els, which again implies that distinguishing between idiom-specific and idiom-shared words is essential and yields bet-ter scores in this application. The maximum MAP scores of 0.396 ( K = 1000) and 0.398 ( K = 1500) with MiLDA were obtained by  X  = 0 . 5. It reveals that both document rep-resentations are important for retrieval. Furthermore, since the MAP scores when only the bow representation or only the tr representation is used in retrieval are 0.341 and 0.359 ( K = 1200), we observe that combining the two representa-tions leads to a positive synergy in the combined model.
We could further improve the results in a slightly altered retrieval setup, where, instead of retrieving webshops di-rectly, we have first ranked single products according to their relevance to the pin and then provided a score for a webshop as an average over its top B best scoring products. Here, we have not observed any major qualitative change with re-spect to the relation of MAP scores for LDA-, BiLDA-, and MiLDA-based retrieval models. However, our best scoring MiLDA-based model in this setup produced a MAP score of 0.441 ( K = 1500 ,B = 5,  X  = 0 . 7) which is better than the previous best reported result in [7] (MAP: 0.414), despite that their LDA was trained directly on the entire target col-lection. We have proposed a new topic model called multi-idiomatic MiLDA which is capable of dealing with multi-idiomatic data and linking information between different idiomatic us-ages of the same language, for example colloquial and formal language. We showed that we can learn true cross-idiomatic topics on a training collection consisting of Amazon X  X  prod-uct descriptions aligned to their respective reviews, and then infer these topics on the target collection for retrieval.
Our results in the task of linking pins to webshops reveal the advantage of the multi-idiomatic MiLDA model over the monolingual LDA model and the bilingual LDA model both in terms of perplexity and MAP.

Since this work on multi-idiomatic text processing is only a start, we strongly believe that our modeling approach will ignite more future applications. For instance, we foresee that the MiLDA model might prove extremely valuable in opinion mining, sentiment analysis, e-commerce applications, social media analysis, dialect mining, or cross-lingual information retrieval for closely related languages. [1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [2] W. De Smet and M.-F. Moens. Cross-language linking [3] D. Mimno, H. Wallach, J. Naradowsky, D. A. Smith, [4] X. Ni, J.-T. Sun, J. Hu, and Z. Chen. Mining [5] M. Steyvers and T. Griffiths. Probabilistic topic [6] X. Wei and W. B. Croft. LDA-based document models [7] S. Zoghbi, I. Vuli  X c, and M.-F. Moens. Are words
