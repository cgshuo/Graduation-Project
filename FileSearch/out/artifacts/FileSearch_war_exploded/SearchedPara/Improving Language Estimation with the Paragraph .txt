 Incorporating topic level estimation into language models has been shown to be beneficial for information retrieval (IR) models such as cluster-based retrieval and LDA-based document representation. Neural embedding models, such as paragraph vector (PV) models, on the other hand have shown their effectiveness and efficiency in learning semantic representations of documents and words in multiple Natu-ral Language Processing (NLP) tasks. However, their ef-fectiveness in information retrieval is mostly unknown. In this paper, we study how to effectively use the PV model to improve ad-hoc retrieval. We propose three major im-provements over the original PV model to adapt it for the IR scenario: (1) we use a document frequency-based rather than the corpus frequency-based negative sampling strategy so that the importance of frequent words will not be sup-pressed excessively; (2) we introduce regularization over the document representation to prevent the model overfitting short documents along with the learning iterations; and (3) we employ a joint learning objective which considers both the document-word and word-context associations to pro-duce better word probability estimation. By incorporating this enhanced PV model into the language modeling frame-work, we show that it can significantly outperform the state-of-the-art topic enhanced language models.
 Retrieval Model; Language Model; Paragraph Vector
Language models have been successfully applied to IR tasks[8, 14]. The core of this approach is to estimate a language model for each document and rank documents ac-cording to the likelihood of observing a query given the es-timated model. The simple language model approach rep-resents documents and queries under the bag-of-words as-sumption. This approach fails when query words are not observed in a document. A typical solution to this issue is to apply smoothing techniques by incorporating a corpus lan-guage model for  X  X nseen X  words, such as the Jelinek-Mercer method, absolute discounting, and Bayesian smoothing us-ing Dirichlet priors [14]. However, smoothing every docu-ment with the same corpus language model is intuitively not optimal since we essentially assume that all the unseen words in different documents would have similar probabili-ties [13].

One way to improve the smoothing techniques is to in-troduce document dependent smoothing that can reflect the content of the document, for example by representing docu-ments and queries in a latent topic space and estimating the generation probability accordingly. By incorporating topic level estimation into language model approaches, previous work such as the cluster-based retrieval model [6] and the LDA-based retrieval model [12] obtained consistent improve-ments over the basic language models. Nonetheless, the ex-isting topic model based approaches have several drawbacks. Firstly, the model estimation relies on the predefined num-ber of topics. Secondly, the topic models typically assign high probabilities to frequent words. Finally, the learning cost (of the LDA model) is expensive on a large corpus.
Recent advances in Natural Language Processing (NLP) have shown that semantically meaningful representations of words and documents can be efficiently acquired by neu-ral embedding models. In particular, a paragraph vector (PV) model [4] has been proposed to jointly learn word and document embeddings by directly optimizing the generative probabilities of each word given the document. In contrast to existing topic models, PV can automatically cluster topic related words and documents without explicitly defining the number of topics a priori. The negative sampling based op-timization strategy makes PV assign high probabilities to discriminative words rather than frequent words. Moreover, the online learning algorithm enables PV to learn over a large-scale corpus efficiently. Existing work has shown that PV can outperform the LDA model on several linguistic tasks [1], but its effectiveness for IR remains mostly un-known.

In this paper, we study how to effectively use the PV model in the language model framework to improve ad-hoc retrieval. Specifically, we use the Distributed Bag of Words version of PV (PV-DBOW) because it naturally constructs a document language model that fits the framework of the lan-guage modeling approach. However, the original PV-DBOW model is not designed for IR, and we find there are three inherent problems make the original PV-DBOW less effec-tive for ad-hoc retrieval. Firstly, the learning objective of PV-DBOW makes it suppress the importance of frequent words excessively. Secondly, PV-DBOW is prone to over-fit short documents during the training iterations. Finally, PV-DBOW does not model word-context associations, making it difficult to capture word substitution relationships that are important in IR. To address these problems, we pro-posed three modifications to enhance PV-DBOW model for ad-hoc retrieval, including document-frequency based nega-tive sampling, document regularization and a joint learning objective. Empirical results show that consistent and signif-icant improvements over baselines can be obtained with our enhanced PV model.
Previous work has shown that generative topic models are beneficial for language model estimation. For example, Liu and Croft [6] showed that document clustering can signifi-cantly improve retrieval effectiveness when incorporated in language smoothing. The cluster model, also known as the mixture of unigrams model, groups documents into a finite set of clusters (topics) and associates each cluster with a multinomial distribution over the vocabulary. Later, Wei and Croft [12] proposed an LDA-based retrieval model by combining language estimation based on LDA with query likelihood model. Their results showed that the LDA-based retrieval model can consistently outperform the clustering based model.

Recently, there have been several studies exploring the application of word embeddings in the IR scenario. For ex-ample, Vuli  X c and Moens [11] construct dense representations for queries and documents by aggregating word vectors and rank results based on the fusion of cosine similarities and query likelihood scores. Ganguly et al. [2] proposed a gen-eralized language model based on word embeddings by con-sidering three term transformation processes. In contract to these studies that construct retrieval models based on bag of word embeddings, our work mainly focuses on how to effec-tively use the paragraph vector model to improve estimation in the language model approach.
In this section, we describe the details of how we enhance the PV model for language estimation in ad-hoc retrieval.
PV-DBOW maps words and documents into low-dimension dense vectors. Each document vector is trained to predict the words it contains. Under the bag-of-words assumption, the generative probability of word w in document d is ob-tained through a softmax function over the vocabulary: where ~w and ~ d are vector representations for w and d ; and V w is the vocabulary of the training collections.

In training, negative sampling is used to approximate the softmax function in Equation (1). Formally, the local objec-tive function for each ( w,d ) pair in PV-DBOW with negative sampling is where  X  ( x ) = 1 / (1 + exp (  X  x )), k denotes the number of negative samples, w N denotes the sampled word, and P n ( w ) denotes the distribution of negative samples. In [7], P n is defined as the unigram distribution raised to the power 0.75: where #( w ) denotes the corpus frequency of w and | C | = P
From the learning objective of PV-DBOW, we can see that it can be naturally applied in the probabilistic language model framework for IR. With the learned word and docu-ment embeddings, we can directly estimate the generative probability of each word given the document in a latent se-mantic space. Therefore, we can incorporate the language estimation of PV-DBOW into the query likelihood model as a document dependent smoothing technique: where P QL ( w | d ) and P PV ( w | d ) represent the word probabil-ity estimated with QL and PV-DBOW respectively.  X  is the parameter that controls the weights of QL and PV-DBOW.
Now we describe in detail the major problems of the orig-inal PV-DBOW model that makes it less effective for IR, as well as the techniques we employ to solve these issues. Document Frequency Based Negative Sampling.
 Following the idea in [5], we can see that PV-DBOW with negative sampling is implicitly factorizing a shifted matrix of point-wise mutual information between words and docu-ments: where #( w,d ) is the term frequency of w in d ; #( d ) is the length of d and k is the number of negative instances. From Equation (5), we can see that the original PV-DBOW model implicitly weights words according to inverse corpus frequen-cies (ICF). However, previous studies have shown that term weighting with ICF may over-penalize frequent words, and often performs worse than term weighting with inverse doc-ument frequency (IDF) [9]. Inspired by this, we propose a novel document-frequency based negative sampling strategy for PV-DBOW to better fit the IR scenario. More formally, we replace P n ( w ) with a new sample distribution: where # D ( w ) represents the document frequency of w . We can find that the new learning objective of PV-DBOW with document-frequency based negative sampling is equal to the following factorization: Since k and P w 0  X  V cess of PV-DBOW with document-frequency based negative sampling is actually factorizing a shifted tf-idf matrix.
In practice, the exact value of the inverse document fre-quency is too aggressive for tf-idf weighting and its logarith-mic version is more widely used. To achieve similar effects, we adapt a power version of document frequency that uses # D ( w )  X  (  X   X  1) instead of # D ( w ) .

Document Regularization. The original PV-DBOW does not handle the varied lengths of documents, making it prone to over-fit short documents during the training it-erations. Specifically, through the training process of PV-DBOW, vector norms of long documents remain roughly the same while vector norms of short documents keep grow-ing. Increasing vector norms affect the dot product value in Equation (1) and make the language estimation concentrate on the observed words. This in turn significantly decreases the smoothing power of the PV-DBOW model on short doc-uments. To solve this problem, we propose to introduce document regularization into the learning objective to avoid the ever-growing norm of short documents. Specifically, we add an L2 constraint on the document norm to the learning objective of PV-DBOW: ` = log(  X  ( ~w  X  ~ d ))+ k  X  E w N  X  P n [log  X  (  X  ~w N where #( d ) is the number of words in d , || ~ d || is the norm of vector ~ d and  X  is a hyper-parameter that control the strength of regularization. Each iteration of the stochastic gradient descent in PV-DBOW goes through each word exactly once, so we use the document length 1 / #( d ) to ensure equal reg-ularizations over long and short documents.

Joint Objective. The original PV-DBOW model learns over the word-document co-occurrence information as shown in Equation (2), making it focus on capturing syntagmatic relations between words (i.e., words that frequently co-occur in same documents). It lacks the modeling of paradigmatic relations between words (e.g.  X  X ar X  and  X  X ehicle X ) since no word-context information is leveraged in its learning process. As suggested by [1, 10], by modeling both word-document and word-context information, one can usually obtain better word and document vectors for NLP tasks. Following the same idea as [10], we introduce a joint learning objective to the PV-DBOW model. Specifically, we apply a two-layer structure that first uses the document to predict the target word and then uses the target word to predict its context. The new objective function is as follows: where ~c j is the context vector for word w j , c N denotes the sampled context and L represents the context window size.
Experimental Setup. We evaluate three baselines: query likelihood model (QL), LDA-based retrieval model (LDA-LM) and original PV-DBOW model (PV-LM). We add doc-ument frequency based negative sampling (D), document regularization (R), and joint objective (J) to PV-DBOW one by one, and refer to the enhanced PV based retrieval model as EPV-D-LM, EPV-DR-LM, and EPV-DRJ-LM re-spectively. We use two TREC collections, Robust04 and GOV2. We report the results of different versions of en-hanced PV based retrieval models on Robust04, but only the full model (EPV-DRJ-LM) on GOV2 due to the space limitation. We use the Galago search engine 1 to index the corpus and report results for both the title and description of each TREC topic (stop words removed). Queries and doc-uments are stemmed with the Krovetz stemmer. For test ef-ficiency, we adopt a re-ranking strategy. An initial retrieval is performed with QL to obtain 2,000 candidate documents, and then a re-ranking is performed with both LDA-LM and EPV based retrieval models. The final evaluation is based on the top 1,000 results. We use a 5-fold cross validation in the same way as [3]: 4 folds are used to tune  X  in smooth-ing process and 1 fold is used to test retrieval performance. We includes three evaluation metrics: mean average preci-sion (MAP), normalized discounted cumulative gain at 20 (nDCG@20) and precision at 20 (P@20).

Parameter Settings. We train both LDA and EPV on the whole Robust04 collection. However, for the GOV2 col-lection, due to the prohibitive training time, we train both LDA and EPV on a randomly sampled subset with 500k documents for fair comparison. The topic number ( K ) in LDA and the vector dimension in PV-DBOW/EPV are em-pirically set as 300. For LDA, we set the hyper-parameters  X  and  X  to 50 /K and 0 . 01 as described in [12]. For EPV, we tuned  X  from 1 to 100 ( 1, 10 and 100), and  X  from 0.1 to 0.9 (0.1 per step). The final value for  X  is 10 (Robust04/GOV2), for  X  is 0.1 (Robust04) and 0.2 (GOV2).

Results. The results on Robust04 are shown in the top part of Table 1. As we can see, by incorporating topic level estimation, LDA-LM can outperform the QL model on both topic titles and descriptions. Meanwhile, by estimating the language model using the original PV-DBOW model, PV-LM obtains very similar results as LDA-LM. By adding the proposed techniques one by one to enhance the PV-DBOW model for IR, we obtain better and better retrieval perfor-mance. The results indicate the effectiveness of the pro-posed techniques for the PV based retrieval model. Finally, the full enhanced model EPV-DRJ-LM can outperform both QL and LDA-LM significantly on both topic titles and de-scriptions. For example, the relative MAP improvement of EPV-DRJ-LM over QL and LDA-LM in Robust04 is 5.5% and 3.5% on titles, 2.5% and 2.4% on descriptions, respec-tively.

From the results on GOV2, however, we find that the in-corporation of the LDA model may even hurt the retrieval performance in most cases. A major reason is that GOV2 is a large Web collection with many diverse and noisy topics. By using only 300 topics, the learned topics in LDA might be too coarse and noisy, which can hurt the language model estimation. Therefore, one may observe better performance with LDA-LM by increasing the number of topics (with cor-respondingly lower efficiency). On the other hand, although the vector dimension of our enhanced PV model is also 300, the potential number of topics is not limited to that num-ber. Therefore, EPV-DRJ-LM can capture much finer topic relations between words and documents, and produce bet-ter language estimation in the latent semantic space. We http://www.lemurproject.org/galago.php  X  0.247 0.392 0.336 GOV2 collection  X  + 0.252  X  + 0.371 0.472 observe much better performance with EPV-DRJ-LM com-pared with both QL and LDA-LM.

The results in Table 1 also show that the topic level smooth-ing is more effective on short queries (topic titles) than long queries (topic descriptions). For example, the relative im-provement of LDA-LM over QL is 2.0% on titles and 0.4% on descriptions in terms of MAP respectively; while the relative improvement of EPV-DRJ-LM over QL is 5.5% on titles and 2.5% on descriptions in terms of MAP. With fewer words in a query, the language model estimation would be more diffi-cult based on exact matching. Therefore, by involving topic level estimation, the smoothing technique can bring larger benefits by alleviating the vocabulary mismatch problem.
In this paper, we study how to effectively use the PV model to improve ad-hoc retrieval. We identify several prob-lems that make the original PV-DBOW model less effective for the IR scenario. To solve these issues, we proposed three techniques to enhance the original PV model. The experi-mental results demonstrate the effectiveness of our enhanced PV based retrieval model compared with the state-of-the-art topic enhanced language models. This is also the first study to show that a PV model can work better than a topic model on language model estimation for IR.
This work was supported in part by the Center for Intelli-gent Information Retrieval, in part by NSF IIS-1160894, and in part by NSF grant IIS-1419693. Any opinions, findings and conclusions or recommendations expressed in this ma-terial are those of the authors and do not necessarily reflect those of the sponsor.
