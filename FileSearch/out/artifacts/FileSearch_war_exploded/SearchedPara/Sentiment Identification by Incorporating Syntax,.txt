 This paper proposes a method based on conditional ran-dom fields to incorporate sentence structure (syntax and se-mantics) and context information to identify sentiments of sentences within a document. It also proposes and evalu-ates two different active learning strategies for labeling sen-timent data. The experiments with the proposed approach demonstrate a 5-15% improvement in accuracy on Amazon customer reviews compared to existing supervised learning and rule-based methods.
 H.4 [ Information Systems Applications ]: Miscellaneous; D.2.8 [ Software Engineering ]: Metrics X  complexity mea-sures, performance measures Sentiment, Syntax, Semantic, CRF, Active Learning
Understanding the sentiment of sentences allows us to summarize opinions which could help people make informed decisions. All of the state-of-the-art algorithms perform well on individual sentences without considering any context in-formation, but their accuracy is dramatically lower on the document level because they fail to consider context and the syntactic structure of sentences at the same time. There are many difficulties owing to the special characteristics and di-versity in sentence structure in the way people express their opinions, including mixed sentiments in one sentence, sar-castic sentences, and opinions expressed indirectly through comparison, etc. In addition, complicated sentence struc-ture and Internet slang make sentiment analysis even more challenging. In this work, we not only consider syntax that may influence the sentiment, including newly emerged In-ternet language, emoticons, positive words, negative words, and negation words, but also incorporate information about sentence structure, like conjunction words and comparisons. The context around a sentence also plays an important role in determining the sentiment. Therefore, we employ a con-ditional random field (CRF) [2] model to capture syntactic, structural, and contextual features of sentences. Our exper-iment results on customer reviews and Facebook comments show better accuracy compared to supervised and rule-based methods. Furthermore, we also employ active learning to help collect more labeled data. We propose two different strategies to select data with high uncertainty for human beings to label, and our experimental results on customer reviews show faster convergence compared to baselines.
CRF and Features Different subjectivity can generate different or even reversed sentiments for sentences. There-fore, the input is a set of m documents: { d 1 ,d 2 along with the specified subject: { sub 1 ,sub 2 ,...,sub d contains n i sentences S i : { s i 1 ,s i 2 ,...,s i n i } . The output for all documents is that for the j th sentence in the i th ument s i j , it will assign a sentiment o i j  X  { P : positive,N : negative,O : objective } . Conditional Random Fields (CRF) provides a probabilistic framework for calculating the proba-bility of label sequences Y globally conditioned on sequence data X to be labeled. Parameters  X  = {  X  k , X  l } are esti-mated by maximizing the conditional log-likelihood function L ( X )of the training data.
 P ( Y | X ) = 1 where Z X is the normalization constant.

L ( X ) = X
Various features have been widely extracted from sentences for sentiment classification and can be leveraged through CRF model. In this paper, we use features based on two as-pects: syntactic and semantic structure of sentences (listed in the Table 1).
 Data Collection Table 2 shows the data collected from Amazon Mechanical Turk (AMT). For each of these reviews, we asked 10 different workers from AMT to label the sen-tences as positive, negative, or objective. We used majority vote to determine the final label for each sentence. We also randomly selected 500 sentences from each of the camera and TV reviews and checked the labeling accuracy. The average response accuracy for all workers for the camera and TV re-views was 0 . 66 and 0 . 62 respectively. We also manually la-beled 500 Facebook comments. We did some preprocessing tasks on the original data, including word correction (e.g., changing  X  X uv X  to  X  X ove X ) and part-of-speech (POS) tagging.
Experimental Results We compare our proposed method against the following rule-based algorithms and supervised n pos words Number of positive words (a positive word list: 1948 words) n neg words Number of negative words (a negative word list: 4550 words) post neg words Position of negative words occurring. Same as above. post negation words Position of negation words. Same as above. Table 2: Data distribution. nrc | ns | nps | nns | nos: # of reviews/comments | sentences | positive sentences | negative sentences | objective sentences Table 3: Accuracy results of CRF model comparing to other methods (CSR, SVM, LR, and HMM) with semantic features only (SO) and with semantic and syntactic features (SS).
 methods: compositional semantic rules (CSR) [1], support vector machine (SVM), logistic regression (LR), and hidden Markov models (HMM). Table 3 shows that CRFs outper-form the other four methods in all cases on the Amazon review dataset. Using our CRF-based method with seman-tic and syntactic features is 5-15% more accurate than the other methods tested. However, CSR performs the best on the Facebook comments dataset, while all other methods generated similar results. We believe that this result is due to the length of the Facebook comments, which provide lit-tle to no context for our CRF-based method, as well as the use of emoticons, which convey sentiments directly.
Active Learning Since collecting labeled data is expen-sive, we use active learning to collect the most valuable la-beled examples. The fundamental step of active learning procedure is to choose what data to present to the oracle. When we apply our trained model on inferring unlabeled data, we get a sequence of label probabilities for a docu-ment which has m sentences : { p 1 ,p 2 ,...,p m } . Each p the probability for the most probable label. In Strategy 1 (S1), we rank documents based on the average probability:
P m i =1 p i and select the document with the smallest value Figure 1: The convergence speed of classification accuracy ( 10 -fold cross validation). to present to oracle. In Strategy 2 (S2), we rank sentences based on the probability in an ascending order and calcu-late the average of the probabilities in the smaller half P . We then rank the document based on P and present the document with the smallest P to oracle. We start from a training size of 10 documents and add one document at a time. We compare these strategies against two baselines, (B1) selecting a document at random and (B2) selecting a document based on the minimum probability of its sen-tences. In this paper, we use customer reviews to test the convergence speed. Figure 1 shows that S2 achieves the same accuracy faster than S1. Because documents with the smallest average probability may have some sentences with high probability, which do not need to be disambiguated.
