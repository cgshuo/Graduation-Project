 Stephane Ross stephaneross@cmu.edu Jiaji Zhou jiajiz@andrew.cmu.edu Yisong Yue yisongyue@cmu.edu Debadeepta Dey debadeep@cs.cmu.edu J. Andrew Bagnell dbagnell@ri.cmu.edu Many problem domains, ranging from web applica-tions such as ad placement or content recommenda-tion to identifying successful robotic grasp trajectories require predicting lists of items. Such applications are often budget-limited and the goal is to choose the best list of items, from a large set of possible items, with maximal utility. In ad placement, we must pick a small set of ads with high click-through rate. For robotic ma-nipulation, we must pick a small set of initial grasp tra-jectories to maximize the chance of finding a successful trajectory via more extensive evaluation or simulation. In all of these problems, the predicted list of items should be both relevant and diverse. For example, recommending a diverse set of news articles increases the chance that a user would like at least one article (Radlinski et al., 2008). As such, recommending mul-tiple redundant articles on the same topic would do lit-tle to increase this chance. This notion of diminishing returns due to redundancy is often captured formally using submodularity (Guestrin &amp; Krause).
 Exact submodular function optimization is in-tractable, but simple greedy selection is known to have strong near-optimal performance guarantees and typi-cally works very well in practice (Guestrin &amp; Krause). Given access to the submodular reward function, one could simply employ greedy to construct good lists. In this paper, we study the general supervised learning problem of training a policy to maximize a submodu-lar reward function. We assume that the submodular reward function is only directly measured on a finite training set, and our goal is to learn to make good predictions on new test examples where the reward function is not directly measurable.
 We develop a novel agnostic learning approach based on new analysis showing that a single no-regret learner can produce a near-optimal list of predictions. 1 We use a reduction approach to  X  X ift X  this result to contextual hypothesis classes that map features to pre-dictions, and bound performance relative to the opti-mal sequence of hypotheses in the class. In contrast to previous work, our approach ensures both data-efficiency as well as performance guarantees in the fully agnostic setting. Moreover, our approach is simple to implement and easily integrates with conventional off-the-shelf learning algorithms. Empirical evaluations show our approach to be competitive with or exceed the state-of-the-art performance on a variety of prob-lems, ranging from trajectory prediction in robotics to extractive document summarization. The problem of learning to optimize submodular re-ward functions from data, both with and without con-textual features, has become increasingly important in machine learning due to its diverse application areas. Broadly speaking, there are two main approaches for this setting. The first aims to identify a model within a parametric family of submodular functions and then use the resulting model for new predictions. The sec-ond attempts to learn a strategy to directly predict a list of elements by decomposing the overall problem into multiple simpler learning tasks.
 The first approach (Yue &amp; Joachims, 2008; Yue &amp; Guestrin, 2011; Lin &amp; Bilmes, 2012; Raman et al., 2012) involves identifying the parameterization that best matches the submodular rewards of the training instances. These methods are largely limited to learn-ing non-negative linear combinations of features that are themselves submodular, which often restricts their expressiveness. Furthermore, while good sample com-plexity results are known, these guarantees only hold under strong realizability assumptions where submod-ular rewards can be modeled exactly by such linear combinations (Yue &amp; Guestrin, 2011; Raman et al., 2012). Recent work on Determinental Point Processes (DPPs) (Kulesza &amp; Taskar, 2011) provide a probabilis-tic model of sets, which can be useful for the tasks that we consider. These approaches, while appealing, solve a potentially unnecessarily hard problem in first learning a holistic list evaluation model, and thus may compound errors by first approximating the submod-ular function and then approximately optimizing it. The second, a learning reduction approach, by con-trast, decomposes list prediction into a sequence of simpler learning tasks that attempts to mimic the greedy strategy (Streeter &amp; Golovin, 2008; Radlinski et al., 2008; Streeter et al., 2009; Dey et al., 2012). In (Dey et al., 2012), this strategy was extended to the contextual setting by a reduction to cost-sensitive clas-sification. Essentially, each learning problem aims to best predict an item to add to the list, given features, so as to maximize the expected marginal utility. This approach is flexible, in that it can be used with most common hypothesis classes and arbitrary features. Be-cause of this decomposition, the full model class (all possible sequences of predictors) is often quite ex-pressive, and allows for agnostic learning guarantees. 2 This generality comes at the expense of being signifi-cantly less data-efficient than methods that make real-izability assumptions such as (Yue &amp; Guestrin, 2011; Raman et al., 2012), as the existing approach learns a different classifier for each position in the list. Compared with related work, our approach enjoys the benefits of being both data-efficient while ensuring strong agnostic performance guarantees. We do so by developing new analysis for online submodular op-timization which yields agnostic learning guarantees while learning a single data-efficient policy. Let S denote the set of possible items to choose from ( e.g. ads, sentences, grasps). Our objective is to pick a list of items L  X  X  to maximize a reward function f that obeys the following properties: 3 1. Monotonicity: For any lists L 1 ,L 2 , f ( L 1 )  X  2. Submodularity: For any lists L 1 ,L 2 and item Here,  X  denotes the concatenation operator. Intu-itively, monotonicity implies that adding more ele-ments never hurts, and submodularity captures the notion of diminishing returns ( i.e. adding an item to a long list increases the objective less than when adding it to a shorter sublist). We further assume for sim-plicity that f takes values in [0 , 1], and that f (  X  ) = 0 where  X  denotes the empty list. We will also use the shorthand b ( s | L ) = f ( L  X  s )  X  f ( L ) to denote the marginal benefit of adding the item s to list L . A simple example submodular function that repeat-edly arises in many domains is one that takes value 0 until a suitable instance is found, and then takes on value 1 thereafter. Examples include the notion of  X  X ultiple choice X  learning as in (Dey et al., 2012; Guzman-Rivera et al., 2012) where a predicted set of options is considered successful if any predicted item is deemed correct, and abandonment in ad placement (Radlinski et al., 2008) where success is measured by whether any predicted advertisement is clicked on. We consider reward functions that may depend on some underlying state x  X  X (e.g. a user, environ-ment of the robot, a document, etc.). Let f x denote the reward function for state x , and assume that f x is monotone submodular for all x . 3.1. Learning Problem Our task consists in learning to construct good lists of pre-specified length k under some unknown distri-bution of states D (e.g. distribution of users or docu-ments we have to summarize). We consider two cases: context-free and contextual.
 Context-Free. In the context-free case, we have no side-information about the current state (i.e. we do not observe anything about x ). We quantify the per-formance of any list L by its expected value: Note that F ( L ) is also monotone submodular. Thus the clairvoyant greedy algorithm with perfect knowl-edge of D can find a list  X  L k such that F (  X  L k )  X  though D is unknown, we assume that we observe sam-ples of the objective f x during training. Our goal is thus to develop a learning approach that efficiently converges, both computationally and statistically, to the performance of the clairvoyant greedy algorithm. Contextual. In the contextual case, we observe side-information in the form of features regarding the state of the world. We  X  X ift X  this problem to a hypothesis space of policies ( i.e. multi-class predictors) that map features to items.
 Let  X  denote our policy class, and let  X  ( x ) denote the prediction of policy  X   X   X  given side-information de-scribing state x . Let L  X ,k = (  X  1 , X  2 ,..., X  k ) denote a list of policies. In state x , this list of policies will pre-performance using the expected value: It can be shown that F obeys both monotonicity and submodularity with respect to appending poli-cies (Dey et al., 2012). Thus, a clairvoyant greedy algorithm that sequentially picks the policy with high-est expected benefit will construct a list  X  L  X ,k such that F (  X  L  X ,k )  X  (1  X  1 /e ) F ( L  X   X ,k ), where L  X  argmax L velop a learning approach (for learning a list of poli-cies) that efficiently competes with the performance of the clairvoyant greedy algorithm.
 Algorithm 1 Submodular Contextual Policy (SCP) Algorithm in context-free setting.
 We first consider the context-free setting. Our algo-rithm, called Submodular Contextual Policy (SCP), is described in Algorithm 1. SCP requires an online learning algorithm subroutine (denoted by Update ) that is no-regret with respect to a bounded positive loss function, 4 maintains an internal distribution over items for prediction, and can be queried for multiple predictions (i.e. multiple samples). 5 In contrast to prior work (Streeter &amp; Golovin, 2008), SCP employs only a single online learning in the inner loop. SCP proceeds by training over a sequence of states x ,x 2 ,...,x T . At each iteration, SCP queries the on-line learner to generate a list of m items (via Pre-dict , e.g. by sampling from its internal distribution over items), evaluates a weighted cumulative benefit of each item on the sampled list to define a loss re-lated to each item, and then uses the online learner (via Update ) to update its internal distribution. During training, we allow the algorithm to construct lists of length m , rather than k . In its simplest form, one may simply choose m = k . However, it may be beneficial to choose m differently than k , as is shown later in the theoretical analysis.
 Perhaps the most unusual aspect is how loss is defined using the weighted cumulative benefits of each item: where L t,i  X  1 denotes the first i  X  1 items in L t , and Intuitively, (1) represents the weighted sum of benefits of item s in state x t had we added it at any intermedi-ate stage in L t . The benefits at different positions are weighed differently, where position i is adjusted by a factor (1  X  1 /k ) m  X  i . These weights are derived via our theoretical analysis, and indicate that benefits in early positions should be more discounted than benefits in later positions. Intuitively, this weighting has the ef-fect of rebalancing the benefits so that each position contributes more equally to the overall loss. 6 SCP requires the ability to directly measure f x in each training instance x t . Directly measuring f x t enables us to obtain loss measurements ` t ( s ) for any s  X  S . For example, in document summarization f x corresponds to the ROUGE score (Lin, 2004), which can be evalu-ated for any generated summary given expert annota-tions which are only available for training instances. In principle, SCP can also be applied in partial feed-back settings, e.g. ad placement where the value f x t is only observed for some items ( e.g. only the displayed ads), by using bandit learning algorithms instead (e.g. EXP3 (Auer et al., 2003)). 7 As this is an orthogonal issue, most of our focus is on the full information case. 4.1. Theoretical Guarantees We now show that Algorithm 1 is no-regret with re-spect to the clairvoyant greedy algorithm X  X  expected performance over the training instances. Our main theoretical result provides a reduction to an online learning problem and directly relates the performance of our algorithm on the submodular list optimiza-tion problem to the standard online learning regret incurred by the subroutine.
 Although Algorithm 1 uses only a single instance of an online learner subroutine, it achieves the same perfor-mance guarantee as prior work (Streeter &amp; Golovin, 2008; Dey et al., 2012) that employ k separate in-stances of an online learner. This leads to a surprising fact: it is possible to sample from a stationary distri-bution over items to construct a list that achieves the same guarantee as the clairvoyant greedy algorithm. 8 For a sequence of training states { x t } T t =1 , let the se-quence of loss functions { ` t } T t =1 defined in Algorithm 1 correspond to the sequence of losses incurred in the re-duction to the online learning problem. The expected regret of the online learning algorithm is where p t is the internal distribution of the online learner used to construct list L t . Note that an online learner is called no-regret if R is sublinear in T . pected value of constructing lists by sampling (with replacement) m elements from distribution p , and let bution found by the algorithm.
 We define a mixture distribution p over lists that con-structs a list as follows: sample an index t uniformly in { 1 , 2 ,...,T } , then sample m elements (with replace-ment) from p t . Note that F ( p,m ) = 1 T P T t =1 F ( p and F (  X  p,m )  X  F ( p,m ). Thus it suffices to show that F ( p,m ) has good guarantees. We show that in expec-tation p (and thus  X  p ) constructs lists with performance guarantees close to the clairvoyant greedy algorithm: 9 Theorem 1. Let  X  = exp(  X  m/k ) and k 0 = min( m,k ) . For any  X   X  (0 , 1) , with probability  X  1  X   X  :
F ( p,m )  X  (1  X   X  ) F ( L  X  k )  X  E Corollary 1. If a no-regret algorithm is used on the sequence of loss ` t , then as T  X  X  X  , E [ R ] T  X  0 , and: Theorem 1 provides a general approximation ratio to the best list of size k when constructing a list of a different size m . For m = k , we obtain the typical (1  X  1 /e ) approximation ratio (Guestrin &amp; Krause). As m increases, this provides approximation ratios that converge exponentially closer to 1.
 Naively, one might expect regret E [ R ] /T to scale lin-early in k 0 as it involves loss in [0 ,k 0 ]. However, we show that regret actually scales as O ( Weighted Majority (Kalai &amp; Vempala, 2005)). Our result matches the best known results for this setting Algorithm 2 Submodular Contextual Policy (SCP) Algorithm.
 (Streeter &amp; Golovin, 2008) while using a single online learner, and is especially beneficial in the contextual setting due to improved generalization (see Section 5). Corollary 2. Using weighted majority with the opti-mal learning rate guarantees with probability  X  1  X   X  : We now consider the contextual setting where features of each state x t are observed before choosing the list. As mentioned, our goal here is to compete with the best list of policies (  X  1 , X  2 ,..., X  k ) from a hypothesis class  X . Each of these policies are assumed to choose an item solely based on features of the state x t . We consider embedding  X  within a larger class,  X   X   X   X , where policies  X   X  are functions of both state and a par-tially chosen list. Then for any  X   X   X   X ,  X  ( x,L ) corre-sponds to the item that policy  X  selects to append to list L given state x . We will learn a policy, or distri-bution of policies, from  X   X  that attempts to generalize list construction across multiple positions. 10 We present an extension of SCP to the contextual set-ting (Algorithm 2). At each iteration, SCP constructs a list L t for the state x t (using its current policy or by sampling policies from its distribution over policies). Analogous to the context-free setting, we define a loss function for the learner subroutine ( Update ). We rep-resent the loss using weighted cost-sensitive classifica-tion examples { ( v ti ,c ti ,w ti ) } m i =1 , where v ti tures of the state x t and list L t,i  X  1 , w ti = (1  X  1 /k ) is the weight associated to this example, and c ti is the cost vector specifying the cost of each item s  X  X  c ti ( s ) = max The loss incurred by any policy  X  is defined by its loss on this set of cost-sensitive classification examples, i.e. These new examples are then used to update the pol-icy (or distribution over policies) using a no-regret al-gorithm ( Update ). This reduction effectively trans-forms the task of learning a policy for this submod-ular list optimization problem into a standard online cost-sensitive classification problem. 11 Analogous to the context-free setting, we can also extend to partial feedback settings where f is only partially measurable by using contextual bandit algorithms such as EXP4 (Auer et al., 2003) as the online learner ( Update ). 12 5.1. No-Regret Cost-Sensitive Classification Having transformed our problem into online cost-sensitive classification, we now present approaches that can be used to achieve no-regret on such tasks. For finite policy classes  X   X , one can again leverage any no-regret online algorithm such as Weighted Majority (Kalai &amp; Vempala, 2005). Weighted Majority main-tains a distribution over policies in  X   X  based on the loss ` t (  X  ) of each  X  , and achieves regret at a rate of for k 0 = min( m,k ). In fact, the context-free setting can be seen as a special case, where  X  =  X   X  = {  X  s | s  X  S} and  X  s ( v ) = s for any v . However, achieving no-regret for infinite policy classes is in general not tractable. A more practical approach is to employ existing reductions of cost-sensitive clas-sification problems to convex optimization problems, for which we can efficiently run no-regret convex op-timization (e.g. gradient descent). These reductions effectively upper bound the cost-sensitive loss by a con-vex loss, and thus bound the original loss of the list prediction problem. We briefly describe two such re-ductions from (Beygelzimer et al., 2005): Reduction to Regression We transform cost-sensitive classification into a regression problem of pre-dicting the costs of each item s  X  S . Afterwards, the policy chooses the item with lowest predicted cost. We convert each weighted cost-sensitive example ( v,c,w ) into |S| weighted regression examples.
 For example, if we use least-squares linear regression, the weighted squared loss for a particular example ( v,c,w ) and policy h would be: Reduction to Ranking Another useful reduction transforms the problem into a  X  X anking X  problem that penalizes ranking an item s above another better item s . In our experiments, we employ a weighted hinge loss, and so the penalty is proportional to the dif-ference in cost of the misranked pair. For each cost-sensitive example ( v,c,w ), we generate |S| ( |S| X  1) / 2 ranking examples for every distinct pair of items ( s,s 0 where we must predict the best item among ( s,s 0 ) (po-tentially by a margin), with a weight of w | c ( s )  X  c ( s For example, if we train a linear SVM (Joachims, 2005), we obtain a weighted hinge loss of the form: where  X  s,s 0 = c ( s )  X  c ( s 0 ) and h is the linear policy. At prediction time, we simply predict the item s  X  with highest score, s  X  = argmax s  X  X  h &gt; v ( s ). This reduction proves advantageous whenever it is easier to predict pairwise rankings rather than the actual cost. 5.2. Theoretical Guarantees We now present contextual performance guarantees for SCP that relate performance on the submodular list optimization task to the regret of the corresponding online cost-sensitive classification task. Let ` t :  X   X   X  compute the loss of each policy  X  on the cost-sensitive classification examples { v ti ,c ti ,w ti } m i =1 collected in Al-gorithm 2 for state x t . We use { ` t } T t =1 as the sequence of losses for the online learning problem.
 For a deterministic online algorithm that picks the se-quence of policies {  X  t } T t =1 , the regret is For a randomized online learner, let  X  t be the distribu-tion over policies at iteration t , with expected regret the expected value of constructing lists by sampling (with replacement) m policies from distribution  X  (if  X  is a deterministic policy, then this means we use the same policy at each position in the list). Let tribution found by the algorithm in hindsight. We use a mixture distribution  X  over policies to construct a list as follows: sample an index t uni-formly in { 1 , 2 ,...,T } , then sample m policies from  X  t to construct the list. As before, we note that F (  X ,m ) = 1 T P T t =1 F (  X  t ,m ), and F ( X   X ,m )  X  F (  X ,m ). As such, we again focus on proving good guarantees for F (  X ,m ), as shown by the following theorem. Theorem 2. Let  X  = exp(  X  m/k ) , k 0 = min( m,k ) and pick any  X   X  (0 , 1) . After T iterations, for deter-ministic online algorithms, we have that with probabil-ity at least 1  X   X  : Similarly, for randomized online algorithms, with probability at least 1  X   X  :
F (  X ,m )  X  (1  X   X  ) F ( L  X   X ,k )  X  E Thus, as in the previous section, a no-regret algorithm must achieve F (  X ,m )  X  (1  X   X  ) F ( L  X   X ,k ) with high probability as T  X   X  . This matches similar guar-antees provided in (Dey et al., 2012). Despite having similar guarantees, we intuitively expect SCP to out-perform (Dey et al., 2012) in practice because SCP can use all data to train a single predictor, instead of being split to train k separate ones. We empirically verify this intuition in Section 6.
 When using surrogate convex loss functions (such as regression or ranking loss), we provide a general result that applies if the online learner uses any convex upper bound of the cost-sensitive loss. An extra penalty term is introduced that relates the gap between the convex upper bound and the original cost-sensitive loss: Corollary 3. Let  X  = exp(  X  m/k ) and k 0 = min( m,k ) . If we run an online learning algorithm on the sequence of convex loss C t instead of ` t , then af-ter T iterations, for any  X   X  (0 , 1) , we have that with probability at least 1  X   X  :
F (  X ,m )  X  (1  X   X  ) F ( L  X   X ,k )  X  where  X  R is the regret on the sequence of convex loss C , and G is defined as and denotes the  X  X onvex optimization gap X  that mea-sures how close the surrogate C t is to minimizing ` t . This result implies that using a good surrogate con-vex loss for no-regret convex optimization will lead to a policy that has a good performance relative to the optimal list of policies. Note that the gap G often may be small or non-existent. For instance, in the case of the reduction to regression or ranking, G = 0 in realiz-able settings where there exists a  X  X erfect X  predictor in the class. Similarly, in cases where the problem is near-realizable we would expect G to be small. 13 6.1. Robotic Manipulation Planning We applied SCP to a manipulation planning task for a 7 degree-of-freedom robot manipulator. The goal is to predict a set of initial trajectories so as to maximize the chance that one of them leads to a collision-free trajectory. We use local trajectory optimization tech-niques such as CHOMP (Ratliff et al., 2009), which have proven effective in quickly finding collision-free trajectories using local perturbations of an initial tra-jectory. Note that selecting a diverse set of initial tra-jectories is important since local techniques such as CHOMP often get stuck in local optima. 14 We use the dataset from (Dey et al., 2012). It consists of 310 training and 212 test environments of random obstacle configurations around a target object, and 30 initial seed trajectories. In each environment, each seed trajectory has 17 features describing the spatial properties of the trajectory relative to obstacles. 15 Following (Dey et al., 2012), we employ a reduction of cost-sensitive classification to regression as explained in Section 5.1. We compare SCP to ConSeqOpt (Dey et al., 2012) (which learns k separate predictors), and Regression (regress success rate from features to sort seeds; this accounts for relevance but not diversity). Figure 1 (left) shows the failure probability over the test environments versus the number of training envi-ronments. ConSeqOpt employs a reduction to k clas-sifiers. As a consequence, ConSeqOpt faces data star-vation issues for small training sizes, as there is lit-tle data available for training predictors lower in the list. 16 In contrast, SCP has no data starvation issue and outperforms both ConSeqOpt and Regression. 6.2. Personalized News Recommendation We built a stochastic user simulation based on 75 user preferences derived from a user study in (Yue &amp; Guestrin, 2011). Using this simulation as a training oracle, our goal is to learn to recommend articles to any user (depending on their contextual features) to minimize the failure case where the user does not like any of the recommendations. 17 Articles are represented by features, and user prefer-ences by linear weights. We derived user contexts by soft-clustering users into groups, and using corrupted group memberships as contexts.
 We perform five-fold cross validation. In each fold, we train SCP and ConSeqOpt on 40 users X  preferences, use 20 users for validation, and then test on the held-out 15 users. Training, validation and testing are all performed via simulation. Figure 1 (middle) shows the results, where we see the recommendations made by SCP achieves significantly lower failure rate as the number of recommendations is increased from 1 to 5. 6.3. Document Summarization In the extractive multi-document summarization task, the goal is to extract sentences (with character budget B ) to maximize coverage of human-annotated sum-maries. Following the experimental setup from (Lin &amp; Bilmes, 2010) and (Kulesza &amp; Taskar, 2011), we use data from the Document Understanding Conference (DUC) 2003 and 2004 (Task 2) (Dang, 2005). Each training or test instance corresponds to a cluster of documents, and contains approximately 10 documents belonging to the same topic and four human reference summaries. We train on the 2003 data (30 clusters) and test on the 2004 data (50 clusters). The budget is B = 665 bytes, including spaces.
 We use the ROUGE (Lin, 2004) unigram statistics (ROUGE-1R, ROUGE-1P, ROUGE-1F) for perfor-mance evaluation. Our method directly attempts to optimize the ROUGE-1R objective with respect to the reference summaries, which can be easily shown to be monotone submodular (Lin &amp; Bilmes, 2011).
 We aim to predict sentences that are both short and informative. Therefore we maximize the normalized marginal benefit, where l ( s ) is the length of the sentence s . 18 We use a reduction to ranking as described in Section 5.1 using (5). While not performance-optimized, our approach takes less than 15 minutes to train.
 Following (Kulesza &amp; Taskar, 2011), we consider fea-tures f i for each sentence consisting of quality features q i and similarity features  X  i ( f i = [ q T i , X  T i ] T quality features, attempt to capture the representa-tiveness for a single sentence. Similarity features q for sentence s i as we construct the list L t measure a notion of distance of a proposed sentence to sentences already included in the set. 19 Table 1 shows the performance (Rouge unigram statis-tics) comparing SCP with existing algorithms. We ob-serve that SCP outperforms existing state-of-the-art approaches, which we denote SubMod (Lin &amp; Bilmes, 2010) and DPP (Kulesza &amp; Taskar, 2011).  X  X reedy (Oracle) X  corresponds to the clairvoyant oracle that directly optimizes the test Rouge score and thus serves as an upper bound on this class of techniques. Figure 1 (right) plots Rouge-1R performance as a function of the size of training data, suggesting SCP X  X  superior data-efficiency compared to ConSeqOpt.
 Acknowledgements Auer, Peter, Cesa-Bianchi, Nicol  X o, Freund, Yoav, and
Schapire, Robert. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing , 32 (1):48 X 77, 2003.
 Beygelzimer, Alina, Dani, Varsha, Hayes, Thomas,
Langford, John, and Zadrozny, Bianca. Error lim-iting reductions between classification tasks. In ICML . ACM, 2005.
 Dang, Hoa Trang. Overview of duc 2005. In DUC , 2005.
 Dey, Debadeepta, Liu, Tian Yu, Hebert, Martial, and
Bagnell, J. Andrew (Drew). Contextual sequence optimization with application to control library op-timization. In RSS , 2012.
 Feige, U., Mirrokni, V. S., and Vondrak, J. Maxi-mizing non-monotone submodular functions. SIAM Journal on Computing , 40(4):1133 X 1153, 2011.
 Guestrin, Carlos and Krause, Andreas. Beyond con-vexity: Submodularity in machine learning. URL www.submodularity.org .
 Guzman-Rivera, Abner, Batra, Dhruv, and Kohli,
Pushmeet. Multiple choice learning: Learning to produce multiple structured outputs. In NIPS , 2012. Joachims, Thorsten. A support vector method for mul-tivariate performance measures. In ICML . ACM, 2005.
 Kalai, Adam and Vempala, Santosh. Efficient algo-rithms for online decision problems. JCSS , 71(3): 291 X 307, 2005.
 Kulesza, Alex and Taskar, Ben. Learning determinan-tal point processes. In UAI , 2011.
 Lin, Chin-Yew. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches Out: ACL-04 Workshop , 2004.
 Lin, Hui and Bilmes, Jeff. Multi-document summa-rization via budgeted maximization of submodular functions. In Annual Conference of the North Amer-ican Chapter of the Association for Computational Linguistics , 2010.
 Lin, Hui and Bilmes, Jeff. A class of submodular func-tions for document summarization. In ACL-HLT , 2011.
 Lin, Hui and Bilmes, Jeff. Learning mixtures of sub-modular shells with application to document sum-marization. In UAI , 2012.
 Littlestone, Nick and Warmuth, Manfred. The Weighted Majority Algorithm. INFORMATION AND COMPUTATION , 1994.
 Radlinski, Filip, Kleinberg, Robert, and Joachims,
Thorsten. Learning diverse rankings with multi-armed bandits. In ICML , 2008.
 Raman, Karthik, Shivaswamy, Pannaga, and
Joachims, Thorsten. Online learning to diver-sify from implicit feedback. In KDD , 2012.
 Ratliff, Nathan, Zucker, Matt, Bagnell, J. Andrew, and Srinivasa, Siddhartha. Chomp: Gradient op-timization techniques for efficient motion planning. In ICRA , May 2009.
 Ross, Stephane and Bagnell, J. Andrew. Agnostic system identification for model-based reinforcement learning. In ICML , 2012.
 Ross, Stephane, Gordon, Geoff, and Bagnell, J. An-drew. A reduction of imitation learning and struc-tured prediction to no-regret online learning. In AISTATS , 2011a.
 Ross, Stephane, Munoz, Daniel, Bagnell, J. Andrew, and Hebert, Martial. Learning message-passing inference machines for structured prediction. In CVPR , 2011b.
 Streeter, M. and Golovin, D. An online algorithm for maximizing submodular functions. In NIPS , 2008. Streeter, Matthew, Golovin, Daniel, and Krause, An-dreas. Online learning of assignments. In NIPS , 2009.
 Yue, Yisong and Guestrin, Carlos. Linear submodular bandits and their application to diversified retrieval. In NIPS , 2011.
 Yue, Yisong and Joachims, Thorsten. Predicting di-
