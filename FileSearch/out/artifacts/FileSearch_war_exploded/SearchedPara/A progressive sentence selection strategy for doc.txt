 1. Introduction
Document summarization can be loosely d10.1016/j.ipm.2012.05.002efined as producing a short summary for one or more document(s). As a short version of the input documents, a good summary should convey the most important content in the documents. This naturally requires the concepts in the summary to be the most salient. On the other hand, a good summary should also be able to cover as many salient concepts as possible. Here we identify these two characteristics as saliency and coverage respectively. In the study of this paper, we mainly consider the problem of how to construct summa-ries with good saliency and coverage.

Summaries are composed by sentences. So the basic idea of composing a salient summary is to include the sentences that contain more salient concepts. Moreover, to compose a summary with good coverage, a summary sentence should provide some salient concepts that are different from the other summary sentences. In other words, every selected sentence is ex-ensured by sentence ranking methods that rank the sentences by their saliency scores. Based on the sentence ranking result, salient sentences can be identified for composing the summary. After sentence ranking, sentence selection strategies are ap-plied to ensure the coverage of the summary. Since there are actually many overlapping concepts in the input documents, it  X  is indeed unnecessary and redundant to repeatedly mention one concept in the summary. A typical technique in the after-ranking selection process is the redundancy control approaches, such as the maximal marginal relevance (MMR) approach the remaining sentences that share similar concepts with them should be reduced since the concepts in them are already mentioned. In this way, the next selected sentence is more likely to contain novel concepts.

In our opinion, the value of a sentence lies in the novel concepts it brings to the current summary. Thus we X  X  like to directly examine the saliency of the unmentioned concepts in the sentence to judge whether it should be selected. This is a natural way to ensure the novelty of the summary sentences and obtain a summary with maximum coverage. However, it is interesting that no current summarization method chooses this framework. In the MMR-style methods, a parameter is used to control the penalty degree of repeating concepts. Usually the degree is not full-penalty. Moreover, it is even quite small in most methods. This means that existing methods still take the mentioned concepts into account when measuring the saliency of new sentences. We attribute this to the fact that it is nearly impossible to accurately measure the true saliency of the concepts. The various measures in existing sentence ranking methods are just estimations of the true sal-iency. Generally, the most important concepts in the documents are easier to identify since they are more distinctive; in contrast, the less important ones are harder to identify. Therefore, it is common for most sentence ranking methods to rely on only a small set of core concepts to ensure the saliency of most summary sentences. For example, Katragadda and Var-ma (2009) pointed out that most query-focused summarization methods actually rely on the query terms to achieve good performance. They reported that more than 75% sentences picked by any automatic summarization system are  X  X  X uery-biased X  X  (containing at least one query term). For human summaries, the proportion of query-biased sentences is only about 50%. This shows that automatic summarization methods rely more on the same extent of query terms than human summarizers. Therefore, it is not a good choice for automatic summarization methods to simply ignore the mentioned concepts in the sentence selection process. Though the novelty of the sentences can be maximized in this way, the sal-iency may seriously drop.

Different from the existing methods, in our study we X  X  like to explore the idea of directly examining the uncovered part of the sentences for saliency estimation in order to maximize the coverage of the summary. To avoid the possible saliency prob-lem, we make use of the subsuming relationship between sentences to improve the saliency measure. The idea is to use the salient general concepts that are more significant to help discover the salient supporting concepts. For example, once we have selected a general word  X  X  X chool X  X  in a sentence of the summary, we would like to select  X  X  X tudent X  X  or  X  X  X eacher X  X  in the next sentences. The example is provided below to illustrate this idea.
 Sentence A : the schools that have vigorous music programs tend to have higher academic performance . Sentence B : among the lower-income students without music involvement, only 15.5% achieved high math scores .
Assuming that sentence A is already selected, we attribute the selection of sentence B to the new concept  X  X  X tudent X  X  in it mance X  X . Therefore, we can define a conditional saliency for the subsidiary sentence B , which is determined by the related concepts. It can also be viewed as an asymmetric relationship between the sentences, which indicates the recommendation degree of a sentence by another.

Based on the asymmetric sentence relationship, we can develop a progressive sentence selection strategy. In the selection process, when a new sentence is selected, it can either be a new general sentence, or a supporting sentence for an existing summary sentence. All the sentences in the final summary can be selected progressively from general to specific. Different from existing methods, our method achieves the target of selecting novel and salient sentences by: (1) considering the unmentioned concepts only for saliency estimation to ensure the novelty of the sentences and (2) meanwhile, using the sen-tence relationship to improve the saliency measure. It is a more straightforward way that does not just rely on several core measure. We believe that our sentence selection strategy is able to discover more salient concepts than traditional methods such as MMR, which just make compromises between the novelty and saliency of sentences. 2. Related work
A variety of summarization methods have been proposed over the years. Currently, most methods are extractive methods for which sentence ranking is essential. Many sentence ranking methods have been proposed in previous studies, including feature-based methods, graph-based methods, learning-based methods, etc. ( Kupiec, Pedersen, &amp; Chen, 1995; Luhn, 1958; of the sentences in order to construct more salient summaries.
 Sentence selection strategies are also very important for generating better summaries from the sentence ranking results.
Carbonell and Goldstein (1998) proposed the famous maximal marginal relevance (MMR) approach that for the first time introduced the redundancy control issue in the summarization area. In each round of sentence selection, the saliency score of every unselected sentence is reduced according to its similarity to the selected sentence. Then the unselected sentences are re-ranked for the next selection round. It is shown that MMR is able to reduce the redundancy in the summary sentences and thus improve the coverage of the summary. Later, redundancy control is recognized as a necessary step after sentence ranking. MMR or its variations are widely adopted for sentence selection in many successful summarization systems ( Radev,
Jing, &amp; Budzikowska, 2000; Wan et al., 2007 ). Usually, a parameter is used to make a proper compromise between the sal-iency and coverage of the summary.
 Later, other approaches are considered to better balance the saliency and coverage of the summary. For example, Conroy,
Schlesinger, and O X  X eary (2006) adopted a method from numerical linear algebra, the pivoted QR decomposition, to solve the problem. Given a set of candidate sentences, the final sentences are selected to best cover the words in the candidate sen-tences. They reported that pivoted QR was able to outperform MMR. Later, optimization methods are also considered to simultaneously optimize the saliency and coverage. In ( Yih, Goodman, Vanderwende, &amp; Suzuki, 2007 ), sentence selection was cast as a typical Knapsack problem, i.e., selecting the sentences to maximize the informative content-words in the input documents. Instead of the greedy algorithm used in MMR, they use dynamic programming to solve the Knapsack problem and thus can obtain the best sentence combination which is optimized on the whole summary. However, as well acknowl-edged, Knapsack problem is indeed a NP-complete problem. Since optimization methods attempt to optimize the whole summary, they have to face the exponential number of possible sentence combinations which limits the efficiency. 3. Methodology
In this section, we first define the subsuming relationship between two sentences. Then we introduce the progressive sen-tence selection strategy based on the subsuming relationship. 3.1. The subsuming relationship between two sentences
According to the motivation, the sentence relationship is explained as the recommendation degree of a sentence by an-
According to the prior example, the relationship between two sentences is determined by the relations between the concepts problem of word relation identification in our study. 3.1.1. Identifying word relations
Generally, word relations can be obtained either from linguistic relation databases such as WordNet ( Banerjee &amp; Peder-is to study the subsuming relations between the words in the input documents. So this is indeed a local problem in that the word senses are confined in the local context. Thus we regard statistics-based approaches to be more suitable to our problem than database-based approaches that mainly consider general word relations. Here we consider a coverage-based measure a subset, or nearly a subset, of the documents in which a occurs. We follow their idea to identify the subsuming word rela-call the general one the subsuming word and the other one the subsumed word. (1) Sentence-level coverage . In document summarization, sometimes a document set just consists of only a few docu-(2) Set-based coverage . Sentence-level co-occurrence is sparser than document-level co-occurrence due to the shorter (3) Transitive reduction . The subsuming relation between two words also reflects the recommendation status between
To define the word relations, let us first introduce some necessary measures. The Spanned Sentence Set ( SPAN for short) of a word w in a document set D , whose sentence set is denoted by S level. Given an existing non-empty word set W ={ w 1 , w 2 over W is devised to reflect to what extent w brings new information relative to the known information provided in W . Based
SPAN ( W ), i.e., COV ( w | W )=| SPAN ( w ) \[ i SPAN ( w information to W and thus be regarded as introducing a new topic beside the existing topics of W .

With the defined measures, we propose a progressive process to discover all the word relations. We make an assumption here that the more important word in a word pair is always the subsuming word. To identify the word relations, we first order all the words in the input documents by their importance. The size of SPAN is used as a rough estimation of the word importance. Following the descending order of importance, we compare each word with the former words to identify the word(s) that can form a subsuming relation with it.

When comparing the word w to a former word w 0 that already subsumes a set of words S to align a relation between w and w 0 , w should be recognized as covered by w 0 . Moreover, it should not be recognized as subsumed by the words in S to ensure the transitive characteristic. Therefore, there are two constraints here for determining the subsuming relation be-tween w and w 0 , i.e., (1) COV ( w |{ w 0 }) P k 1 ;(2) COV ( w | S )&lt; k the constraints. In our method, k 2 is set as a constant parameter whereas k coverage of w by each candidate word, i.e. k 1  X  k 0 1 Max from 0 to 1. The effect of parameters will be examined in the evaluation section.

To a document set, the identified word relations can organize all the words in it as a directed acyclic graph (DAG, a di-rected graph with no directed cycles). Fig. 1 provides an example graph to illustrate this idea. 3.1.2. The definition of the subsuming relationship
Given the subsuming word relations, now we can define the relationship between two sentences, i.e., the measure of how much a sentence s can be attached to another sentence s 0 by the words in s that can be attached to s 0 .
Denoting the word set of s as W ={ w 1 , ... , w l } and the word set of a sentence s 0 as W cept of  X  X  Connected Word  X  X . A word w i in W is regarded to be  X  X  X onnected X  X  to a word w 0 2 w l 1 ; ... ; w lk # W [ W 0 ; s : t : w i &lt; w l 1 ^ w that directly connects w i is w l 1 . The weight of the edge COV ( w tion between w i and w 0 j . It is denoted as CON w i j w 0 garded to be connected to s 0 .
 sum of the importance of all the  X  X  X onnected words X  X  in s to s 0 , i.e., where score ( w i ) is the importance score of w i . Here, we again use | SPAN ( w probability of continuously referring sentence s given the condition that sentence s X  is selected. 3.2. Progressive sentence selection strategy
With the sentence relationship, now we introduce the progressive sentence selection strategy. It can be viewed as a ran-dom walking process on the DAG from the center to its surrounding nodes. The central words are covered first and then more words are reached from the covered words through the word relations. Here we introduce a virtual word besides the real words that do appear in the input documents. The virtual word is used as the center of the DAG (denoted as ROOT-W ). To every word that is not subsumed by any other word, we regard it as a general word and attach it to ROOT-W . Semantically, we can view it as a virtual word that spans the whole sentence set so that it can perfectly cover any actual word. ROOT-W also serves as the starting point of the walking process. With the virtual word, we further define a virtual sentence ROOT-S that contains ROOT-W only. This virtual sentence ROOT-S is regarded as being already selected at the beginning of the sen-general ideas of the input documents because the words attached to ROOT-W are the general words. Based on the virtual sentence, we can unify the identification of the general sentences and the supporting sentences, and thus make the sentence selection process more compact and consistent.

The sentence selection process is cast as: first adding ROOT-S to the initial summary and then iteratively adding the sen-tence that best supports the existing sentence(s) (denoted as S sentence based on its conditional saliency to each selected sentence. The selected sentence to which s has the maximum saliency is deemed as the one subsuming s . This maximum saliency indicates how much supporting information s can bring to the current summary, i.e., Max s t e S old { CS ( s , s sentence and thus is added to S old in the round.

When different sentences contain the same  X  X  X onnected words X  X , they have equal scores. In this case, we need further cri-teria to judge the best one, which we regard as the most efficient sentence in supporting the current summary. For example, ized value of the position of s in the document. This adjustment reflects a preference for shorter or textually earlier sentences. 3.3. Redundancy control by penalizing repetitive words
To ensure that the selected sentence always brings new concepts, a damping factor a is applied to the word importance during the sentence selection process. Once a sentence is selected into the hierarchy, the importance of each word in it is treme case when a equals 0, an effective  X  X  X onnected word X  X  is required not to appear in any selected sentence. According to the motivation, the effect of the progressive sentence selection strategy in this extreme case is an important issue in our study to be shown in the experiments. Moreover, the effect of different values of a will also be examined to more clearly demonstrate the advantages of the progressive sentence selection strategy. 4. Experiments and evaluation
We conduct the experiments on the evaluation series organized by Document Understanding Conference (DUC). The pro-posed summarization methods are first evaluated on a generic multi-document summarization data set and then extended to several query-focused multi-document summarization data sets. In all the experiments, the texts are pre-processed by removing the stop-words and stemming the remaining words with GATE ( Cunningham, Maynard, Bontcheva, &amp; Tablan, 2002 ). 4.1. Evaluation metrics
In the experiments, we use the automatic evaluation toolkit ROUGE
Oriented Understudy for Gisting Evaluation) is a state-of-the-art automatic summarization evaluation method that mainly makes use of N -gram comparison. It evaluates the system summaries by comparing them with human summaries. For example, ROUGE-2 evaluates a system summary by matching its bigrams against the human summaries.
 it works well in DUC. For example, in the DUC 2005, ROUGE-2 had a Spearman correlation of 0.95 and a Pearson correlation of 0.97 compared with human evaluation. 4.2. Experiments on generic summarization
Experiments are first conducted on the DUC 2004 generic multi-document summarization data set which includes 45 document sets, with each set consisting of 10 documents. Systems are required to produce a summary for each document set with a pre-given summary length. We develop a progressive summarization system that follows the proposed sentence selection strategy to iteratively select the summary sentences until the length limit is reached. Besides, we also implement another summarization system for comparison, which does not consider the sentence relationship. It is called the  X  X  X equen-the progressive system. The difference is that it sums the importance of all the words in the sentence instead of the  X  X  X on-gressive system, this can be viewed as a general saliency measure. For the sequential system, the damping factor a is also applied to redundancy control. Each time when a new sentence is selected, the word importance is renewed and all the unse-lected sentences are re-ranked. The whole summary is constructed by iteratively selecting the highest-ranked sentence in each round until the length limit is reached.

To compare the progressive system and the sequential system, we run them with different summary lengths and damp-the sequential system that it significantly outperforms the sequential system under the same parameters (95% more signif-icant under paired sample t -tests).

Now let us look at the effect of the damping factor a . With smaller a , which means higher penalties on the repetitive words, the resulting summary tends to include more diverse words and thus stands a better chance to share more words with the reference summaries, which may lead to a higher ROUGE-1 score. However, meanwhile the average saliency of the words may become lower and thus the number of salient words may decrease. Consequently, the ROUGE-1 score may not improve or even drop. Moreover, the ROUGE-2 score may decrease even more as it requires matching two contin-uous words, which is stricter than matching single words. Looking at the results in Table 1 , for 200-word and 400-word sum-maries, the sequential system can obtain the highest ROUGE-1 score with full penalty on repetitive words ( a equals 0).
However, the ROUGE-2 scores drop significantly. Also, the ROUGE-1 score drops for 100-word summaries. In contrast, the progressive system can cope with smaller a by using the conditional saliency measure instead of the general saliency mea-sure. In the extreme case when a equals 0, the ROUGE-1 score is still improved. This proves that the progressive system is able to discover more salient words with the idea of considering only the novel words for sentence ranking. Though the high-est ROUGE-2 scores are obtained when a equals 0.5, we can observe that the dropping rate is much lower for the progressive system. This clearly demonstrates the advantages of the progressive sentence selection strategy guarantees the novelty and in dealing with very small a  X  X .

In our method, the damping factor is used to handle the redundancy issue instead of the MMR approach. Here we also conduct an experiment to compare it to MMR. The results of the systems with different redundancy control methods are provided in Table 2 (95% more significant under paired sample t -tests). According to the results, the damping factor method performs better than MMR for both systems. The reason is that it is more consistent with the word importance estimation method used in the systems and thus it is better in handling the redundancy for the systems.

Next, the effects of the two parameters used in word relation identification, k the ROUGE-1 and ROUGE-2 scores of the progressive system versus k 0.1.
 k means looser constraint for judging  X  X  X he new word is covered by the selected words X  X  and thus more selected words are compared to the new word. Consequently, more relations may be discovered for the new word. However, if k many unrelated words may also be wrongly associated, which unavoidably impairs the reliability of the word relations and leads to the worse performance. On the other hand, when using a too large k limited and thus weaken the progressive system. Therefore, an appropriate value of k can see that the best k 2 is around 0.7 and 0.8. Similar to k 4 . The best value is observed at around 0.5 and 0.6. However, its influence is not as significant as k chosen. 4.3. Experiments on query-focused summarization
From 2005, DUC initiates a series of evaluations on query-focused multi-document summarization, which requires cre-ating a summary from a set of documents related to a given query. This task is specified as the main evaluation task for 3 years (2005 X 2007). The data set in each year contains about 50 topics, with each topic consisting of 25 X 50 documents 250 English words in length.

Intuitively, it is necessary to consider the effect of the query in the sentence selection process for query-focused summa-now indicates a word that spans all the sentences that can be spanned by any non-stop word in the query. Subsequently, we also define a query-based importance measure by the size of the Query-based Spanned Sentences Set , i.e., the sentences contain both the word and at least one non-stop query word. This importance measure is used for both the progressive sys-tem and the sequential system.

Table 3 provides the ROUGE-1 and ROUGE-2 scores of the progressive system and the sequential system with the original word importance (denoted by Gene ) and the query-based word importance (denoted by Query ) on the DUC 2005, 2006 and (denoted by Average ) in each year are also included for reference. The results on the query-focused data sets further prove the advantage of the progressive system. It consistently performs better than the sequential system on every data set (95% more significant under paired sample t -tests) and competes comparably with the best submitted systems. It is also shown that incorporating the query to refine the word importance is effective for both the progressive system and the sequential system. 5. Conclusion and future work
In this paper, we propose a progressive sentence selection strategy for multi-document summarization. In the process, a sentence can be selected either as a general sentence or as a supporting sentence. The sentence relationship is used to im-prove the saliency of the supporting sentences. In the progressive sentence selection strategy, we can measure the sentence saliency by the novel part only and thus the novelty of the sentences is naturally guaranteed. The resulting summarization systems are evaluated and compared with a typical sequential system on several DUC data sets. The results clearly demon-strate the advantages of the progressive sentence selection strategy in constructing summaries with better saliency and coverage.

Generally, the effectiveness of the strategy mainly depends on the accuracy of the subsuming sentence relationship, plex concept and the sense of a word can be ambiguous in a document set. Certainly, it may be more accurate to use phrases suming relationship between the sentences.

Another issue is that we have followed the extractive style to construct the summary. A selected sentence may unavoid-ably contain unexpected words besides the  X  X  X onnected words X  X  and thus it may not always be ideal to express the concepts imbedded in the  X  X  X onnected words X  X . However, due to the limitation of the current natural language generation techniques, automatic summarization systems still cannot freely compose ideal sentences like human do. In the future, we X  X  like to investigate other means to break the limitation of the original sentences, such as sentence compression or sentence fusion, which can generate additional candidate sentences in order to more accurately express the desired concepts. Acknowledgements
The work described in this paper was partially supported by Hong Kong RGC Projects (PolyU5217/07E), NSFC programs (Nos: 60603093 and 60875042) and 973 National Basic Research Program of China (2004CB318102).
 References
