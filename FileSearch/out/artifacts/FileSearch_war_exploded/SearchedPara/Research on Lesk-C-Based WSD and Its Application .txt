 Cross-Language Information Retrieval (CLIR) provides a convenient way that can solve the problem of crossing the language boundary, and users can submit queries which are written in their familiar language and retrieve documents in another lan-guage. Among current CLIR systems, most of them select Query Translation as the main strategy [1]. Query Translation refers to translate users X  queries written in a single language into queries in different languages used by retrieval documents from a multi-lingual pool of documents [2]. This pattern has become a method with the lowest cost and the smallest difficulty. In order to improve the performance of CLIR, it is neces-sary to solve the problem of ambiguities that exists in the translation of query [3]. For the same query term in source language, although there may be several translations, in which only part of them are suitable. The disambiguation for query translation is to decide the specific sense in the particular context of query term [4]. With the develop-ment and growth of large-scale corpus and machine-readable dictionary, there is a good platform for the research on Word Sense Disambiguation (WSD) and its applica-tion in CLIR [5]. 
CLIR combines the traditional IR technique and Machine Translation (MT) tech-nique. There are many aspects related to the problem of polysemy, which are good cut-in points for the application of WSD in CLIR. Therefore, this paper attempts to apply WSD in English-Chinese Bi-Directional CLIR and make in-depth discussion from two points of view. On the one hand, WSD is applied in query expansion, which aims to make the expanded content having the higher relativity with the original query. On the other hand, WSD is also applied in retrieval process, which aims to improve the precision of retrieval. Although limited improvement on WSD can be obtained, query expansion and disambiguation based on the related strategies of WSD are beneficial to CLIR, and can improve th e whole retrieval performance. Specially, by considering the  X  X oordinate Terms X , the proposed Lesk-C WSD algorithm shows better performance and has more extensive applicability on CLIR. In recent decades, the performance of WS D has been increasing steadily. Corpus-based unsupervised methods are usually a clustering task, and can efficiently avoid the reality of inadequate trai ning corpus [6]. Besides corpus, dictionary is another kind of knowledge source. Its development trends to contain more and more semantic information. Hence, dictionary-based meth ods have become the hotspot research. The classical thesauruses include WordNet, HowNet, etc. Although there are many differ-ent kinds of WSD methods, the difference between their performance and the upper bound acquired by human (&gt;90%) is still very large. Therefore, it is difficult for these methods to be put into real application. In the 3 rd Sense Evaluation Conference (SEN-SEVAL-3), the precision of the best system based on the English samples is 72.9% (Fine-grained). There is still a large space to improve the whole performance. 
For a long time, researchers in the field of Natural Language Processing (NLP) have been considering ambiguity of word sense as one of the main factors which attempts to accurately evaluate the performance of WSD alone. In the same way, there are a few reports about the combination of WSD and IR. A few works about integration pattern show that the application of WSD in IR cannot bring the improve-ment in performance, but has some negative effects. This kind of case attributes to the lack of useful resource that is suitable for evaluation. Generally the evaluation process can only be established based on small-scale training and testing corpus which lacks of representativeness. In recent years, with the stepwise development of large-scale training corpus which has been disambiguated manually and the emergence of many famous international evaluation conferences related to the field of IR, such as Text Retrieval Conference (TREC), not only large-scale IR testing sets become efficient proposed. Hence, the resolution of available resource can provide a feasible route for WSD evaluation and reliability validation that WSD can improve IR performance. Recently, more and more researchers concern the ambiguity problem in IR corpus and try to probe into the benefits of performing WSD in IR. However, for CLIR that inte-grates MT and monolingual IR, there is still little related research work. As an impor-tant technique in MT and bottom processing of NLP, WSD can benefit from CLIR, which is a good application platform [7]. 3.1 Lesk Algorithm and Its Variation The classical Lesk algorithm views a sense definition as an unordered bag of words based on the assumption that related word senses are often defined in a dictionary shortcoming of Lesk algorithm is that it may cause data sparseness problem due to proposed. 
Existing extended Lesk algorithms overcome the data sparseness problem to some extent by expanding the definition of a word sense. EKEDAHL and GOLUB modi-fied the Lesk algorithm by adding two nearest hypernyms X  definitions to the original synset, which obviously enhanced the chances of overlapping [8]. However, their experimental result (45%) was still below the baseline (60%). Pedersen adopted an-other extended method that utilized the definitions of all the synsets directly con-nected to the target synset, including hypernyms, hyponyms and more [9]. In addition, they assigned more weights to phrases. The authors declared that their method was much more efficient than the Lesk algorithm. 
The information commonly used in extended Lesk algorithms is hypernym infor-mation, that is, parent node in WordNet. This information is the further abstraction of word sense. The hypernyms used in the experiment of this paper include direct hypernym and two nearest hypernyms. 3.2 Extended Lesk Algorithm Based on Coordinate Terms (Lesk-C) Existing extended Lesk algorithms mainly concentrate on direct information con-nected to a synset, especially hypernyms, but neglect some useful indirect informa-tion. The Lesk Algorithm based on Coordinate Terms (Lesk-C) expands further the sense definition of a synset by using coordinate terms, that is, sibling nodes in Word-Net hierarchies. The coordinate terms of English noun  X  basketball  X , for example, are  X  football  X  and  X  volleyball  X . It is obvious that a synset and its coordinate terms have a common parent. 
We suppose that any feature has the consistent effect with its coordinate terms on determining the sense of an ambiguous word. Based on this assumption, the Lesk-C definitions of its coordinate terms. Hence, the ambiguous word  X  play  X  as in  X  play  X  participate in games or sport  X . For the Lesk algorithm, if there is no overlap between the sense  X  participate in games or sport  X  and the sense definition of  X  volley-ness problem. In comparison with the Lesk algorithm, the Lesk-C algorithm may work better because the coordinate terms can expand the definition of  X  volleyball  X . The formalization description of the Lesk-C algorithm is shown in Algorithm 1. 
Coordinate Terms are sibling nodes in WordNet. They are not the abstraction of the original word sense, even there are not direct relations between both. However, under the assumption that the effect of any word sense and its coordinate terms on proves that coordinate terms and hypernyms are both significant in the same way. 4.1 WSD for Query Expansion At first, based on query disambiguation, word senses in query are determined. Sec-ondly, the expansion proceeds according to the related resources, such as thesaurus and corpus. Because restricting the specific word senses, the result with expansion is better than the result without expansion. Ho wever, there are some drawbacks. One is that errors in disambiguation will influence the performance seriously. The other is order to compare the effect of WSD on query expansion, the cases based on WordNet and corpus are considered respectively, as shown in Table 1. Being a polysemous  X  fishing net  X , etc. The column of  X  Without Disambiguation  X  describes the result be expanded based on WordNet, but the expansion result based on corpus is more extensive and noisy. Through the disambiguation process for  X  net income  X , the word sense for  X  net  X  can be confirmed and then expanded further. The final result is im-proved to a great extent. 4.2 WSD for Query and Document IR models commonly used, such as Boolean Model and Space Vector Model, all ignore word senses of feature terms in documents. These models often emphasize particularly on whether feature terms exist, and do not care whether word senses match. There are some deficiencies in this manner. Take  X  net income  X  as an example, many documents related to  X  network  X  will be returned by these models above, rather than documents related to  X  net income  X . Therefore, WSD is introduced into CLIR model, and used to disambiguate the translation of query and retrieval results. The retrieval precision can be improved through the irrelevant document filtering. Only when feature terms in a document are matching with the corresponding word senses of query terms, this document will be merged into the final retrieval document list. Otherwise, such a document will be filtered out. 5.1 Experiment on WSD Algorithms Data Set and Evaluation Metrics. The corpus of the English Lexical Sample Task in SENSEVAL-3 is utilized as the data set, which contains 57 ambiguous words (com-posed of 20 nouns, 32 verbs and 5 adjectives). At the same time, two evaluation parameters are introduced to evaluate the performance of WSD algorithms. Precision (P) is computed by summing the scores over all the instances handled through WSD processing, and divided by the number of the handled instances. Recall (R) is com-puted by summing the system X  X  scores over all the instances including the unhandled instances, and divided by the total number of the instances in the evaluation data set. are handled. Experimental Results. Based on the selected mainstream WSD algorithms and the established Lesk-C algorithm, ambiguous wo rds with different POSs are tested under different preprocessing strategies. The experimental results are shown in Table 2, in which  X  X ine X  represents  X  X ine-grained X ,  X  X oarse X  means  X  X oarse-grained X , and  X  X topwords/Stemming (Y/N) X  represents wh ether the contexts are processed through stopword filtering and stemming.  X  X esk1 X  is the extended Lesk algorithm that uses its direct hypernyms, and  X  X esk2 X  uses two nearest hypernyms.  X  X P X  and  X  X C X  represent the other two kinds of classic unsupervised WSD algorithms, in which WP is Wu and Palmer X  X  approach based on pa th information [10] and JC is Jiang and Conrath X  X  measurement based on information content [11]. Analysis and Discussion. It can be observed from Table 2 that the performance of the algorithm often leads to the data sparsene ss problem, and then the most frequently used word sense will be chosen automatically. At the same time, it also shows that the extended Lesk algorithms are more accurate than the Lesk algorithm itself. According to the experimental results, whether using the stemming process or not, the stopword filtering will obviously improve the precision. This indicates that the stopword filter-ing can eliminate some useless information. The performance of Lesk-C is better than those of Lesk1 and Lesk2, which proves the role coordinate term plays is equivalent to that hypernyms do. Through introducing the coordinate terms, the Lesk-C algo-rithm can not only get the better performance, but also be suitable for the disambigua-tion for both noun and verb. The performance of the WP algorithm is similar to Lesk1, better than Lesk2, but still worse than Lesk-C. The JC algorithm outperforms the other unsupervised algorithms on nouns, and its precision reaches 50.4%. How-ever, it is unable to handle words with other POSs except noun, which is the main restriction and shortage of this algorithm. 5.2 Experiment on the Application of WSD in CLIR Data Set. Based on the English Corpus of Financial Times (1991-1994) from the Ad hoc task in TREC-7, the performance of English Monolingual IR and Chinese-English CLIR is tested. This corpus includes 210,158 documents (about 560M), and the average length of a document is 2,338 bytes. Based on the Chinese Corpus from the CLIR task in TREC-9, the performance of Chinese Monolingual IR and English-Chinese CLIR is tested. This corpus is made up of three news document sets from Hongkong Commercial Daily, Hongkong Daily and Ta Kun Pao respectively, which contains 127,938 documents (about 360M) in all. The English Query Set is established based on the query set from the Ad hoc task in TREC-7, and the Chinese Query Set based on the query set from the CLIR task in TREC-9. 
For English-Chinese CLIR, the corresponding Chinese translation of English query is provided in TREC-9. Therefore, it is no t necessary to translate query manually. For Chinese-English CLIR, English corpus and query set are reused. The title of English query is manually translated into Chinese, and then the acquired Chinese query is used in Chinese-English query translation, retrieval and the final evaluation. Experimental Results. For English Monolingual IR, four runs are tested based on the query as the original query, and takes the English corpus as the retrieval object. (1) E-E_Base  X  without disambiguation and query expansion; (2) E-E_WnAll  X  using WordNet to expand all the word senses of query terms in (3) E-E_WnFirst  X  using WordNet to expand the most frequently used word sense (4) E-E_Lesk-C  X  firstly using the Lesk-C WSD algorithm based on WordNet to The results about all the runs above are shown in Figure 1. 
For Chinese-English CLIR, four runs are implemented based on the related strate-gies of WSD. These runs are tentative application for WSD in CLIR. Every run util-izes the content in the title field of Chinese query as the original query in source lan-guage, which will be converted into the query in target language. The English corpus is taken as the retrieval object. (1) C-E_Base  X  without disambiguation and query expansion; (2) C-E_WnAll  X  using WordNet to expand all the word senses of query terms in the (3) C-E_WnFirst  X  using WordNet to expand the most frequently used word sense (4) C-E_Lesk-C  X  firstly using the Lesk-C WSD algorithm based on WordNet to The results about all the runs above are shown in Figure 2. 
For English-Chinese CLIR, four runs are implemented based on the related strate-original query in source language, which will be converted into query in target lan-guage. The Chinese corpus is taken as the retrieval object. (1) E-C_Base  X  without disambiguation and query expansion; (2) E-C_WnAll  X  firstly using WordNet to expand all the word senses of query terms (3) E-C_WnFirst  X  firstly using WordNet to expand the most frequently used word (4) E-C_Lesk-C  X  firstly using the Lesk-C WSD algorithm based on WordNet to The results about all the runs above are shown in Figure 3. Analysis and Discussion. It can be observed from Figure 1 that there are the following cases in the runs on English Monolingual IR.  X  E-E_Base has the best performance, and its average precision is up to 38.2%.  X  E-E_WnAll has the worst performance, and its average precision is only 30.7%.  X  E-E_WnFirst only expands the most frequently used word senses. The word set  X  E-E_Lesk-C is similar to E-E_WnFirst . Firstly, the specific word senses of query 
For Chinese-English CLIR, because Chinese-English query translation module is not perfect, the effect of Chinese query translation is not very satisfied. At present, the translation module can not correctly translate many proper names, such as  X   X  X  X  ( marginal sea ) X ,  X   X  X  X  X  ( cyanide ) X ,  X   X  X  X  X  X  ( El Nino ) X ,  X   X  X  X  X  X  X  X  X  X  X  ( sick building syndrome ) X ,  X   X  X  X  X  X  X  ( Amazon ) X  and  X   X  X  X  X  X  ( rain forest ) X , etc. This disadvantage directly results in the whole performance of CLIR falling rapidly. In order to reduce the influence of the quality of Chinese-English translation on the per-formance of CLIR and measure the whole performance of CLIR based on WSD strat-egy, some manual adjustments have been done for the translation of the original query in source language beforehand. On the one hand, the correct or basically correct trans-translation result of the query  X   X  X  X  X  X  X  X  X  X  ( Nobel Laureate ) X ,  X  win Nobel prize  X , any modification will not be made. On the other hand, some words with more ambiguity are removed. For example,  X   X  X  X  X  X  ( El Nino ) X  is translated be observed from Figure 2 that in comparison with Figure 1, C-E_Base no longer has the best performance and the average precision is lower than those of and C-E_Lesk-C . At the same time, this case fully shows that query expansion and disambiguation can indeed improve the whole performance of Chinese-English CLIR. The compari-son of the average precision between English Monolingual IR and Chinese-English CLIR based on WSD strategy is shown in Table 3. It can be seen from Table 3 that although E-E_Base is the best one in English Monolingual IR, C-E_Base falls to the third one in Chinese-English CLIR. The main reason is that the lost or noisy information has the direct influence on the performance of C-E_Base . However, the other patterns except C-E_Base can make up this prob-lem. Thus it can be seen that the effect of query expansion is very obvious. Among the methods based on WordNet, C-E_Lesk-C based on WSD is the best one, and its performance is evidently better than those of C-E_WnAll and C-E_WnFirst . This shows that WSD is useful to some extent in Chinese-English CLIR, but this effect is not obvious in English Monolingual IR. Because Chinese query is not expanded by Chinese dictionary or other resources, Chinese Monolingual IR is only tested in the C-C_Base manner based on Chinese query set and corpus. The average precision of this manner is 27.7%, which is less 10.5% in comparison with the average precision 38.2% of English Monolingual IR. Certainly, owing to the corpus used differently, these two manners have no direct comparableness. Similarly, in English-Chinese CLIR, the direct Chinese query expan-sion is still not used. Firstly, English query is expanded by using WordNet, and then converted into Chinese query through query translation. Due to the faultiness of the query translation module used, too much noisy information is introduced. Therefore, as shown obviously in Figure 3, even if some noisy information is cut down manu-ally, the precision reduces sharply. The comparison of the average precision between Chinese Monolingual IR and English-Chinese CLIR based on WSD strategy is shown in Table 4. Through specially introducing and considering the  X  X oordinate Terms X , the Lesk-C algorithm exhibits the better performance among various popular unsupervised WSD algorithms, and has more extensive applicability. This paper attempts to apply WSD in CLIR and evaluate the whole retrieval performance in the round. Although being restricted by the performance of WSD methods, query expansion and disambiguation based on the related strategies of WSD are beneficial to CLIR, and can improve the whole performance of CLIR to some extent. 
In addition, the relatively objective evaluation metrics are also proposed. In order to measure the performance of various WSD methods, the corpus from SENSEVAL-3 are used in training and testing. In order to measure the performance after applying WSD in CLIR, the query set, corpus and result set provided by TREC are utilized to make general evaluation. This pattern makes our experimental results relatively fair and objective, and comparable. 
WordNet is a thesaurus annotated manually. The existing algorithms do not fully utilize semantic information in WordNet, and more useful semantic information should be mining from it. At the same time, current research work focuses on English query expansion and WSD, Chinese query expansion and WSD based on Chinese thesaurus (e.g., HowNet) are not concerned. Though there is some commonness between English and Chinese WSD, they have their own characteristics and Chinese WSD is likewise important. These aspects above will be the focus problems in our future research work. 
