 Clustering has been studied for decades and recognized as an important and valuable capability in various fields. Recently, instead of clustering in the full dimensions, research in data mining has been in the direction of finding clusters which are embedded in subspaces. The increase of research attention for subspace clustering comes from the recent report of  X  the curse of dimensionality  X  X 1], which points out that the distances between data points will be indiscriminate in the high dimensional space. Due to the infeasibility of clustering in high dimensional data, discovering clusters in subspaces becomes the mainstream of cluster research, including the work of p rojected clustering [8] and subspace clustering [2][3][5].

The CLIQUE algorithm is one of the state-of-the-art methodology in the literature, which essentially relies on the monotonicity property in the partition of grid-based regions: if a region/grid is called dense, i.e., its coverage (count of points) exceeds a specified threshold, all of its projection units will be alos dense . Therefore, after binning input data into disjointed grids according to the coordinate of each data point, Apriori-based manners are able to efficiently identify dense grids level by level.

The measure of what is a dense region and the issue of how to efficiently and precisely identify dense regions ha ve been comprehensively studied in re-cent years [2][4][5]. However, identifying clusters from connected dense grids, is deemed reasonable but does not be systema tically evaluated yet. In fact, the rigid definition of subspace clusters as connect ed grids needs further justification in terms of two general criteria of clustering quality: (1) inter-cluster dissimilarity 1 ; and (2) intra-cluster similarity 2 . We note that rashly connecting dense grids as clusters inevitably faces the compromise b etween inter-cluster dissimilarity and intra-cluster similarity, since the naive approach will amplify the side-effect from the misadjustment of two subtle input parameters, i.e., (1) the binning width of a grid and (2) the density threshold for identifying whether a region/grid is dense. With an inappropriate parameter setting, a true cluster is likely to be separated into two or more clusters, whereas many true clusters may be merged into a fat but improper cluster.

Consider the illustrative examples shown in Figure 1(a), which contain four situations in a two-dimensional space with different input parameters in CLIQUE. It is clear to see that different paramete r settings result in highly divergent results when we straightforwardly link dense grids and construct clusters. Since dense grids may distribute apart from each other when the connectivity between dense grids is relatively sparse, clusters could be separated into lots of slivers in CLIQUE, such as the case in Figure 1(a).2, or the shapes of clusters could be distorted, such as the case in Figure 1(a).4. As a result, the inter-cluster dimssimilarity is strikingly sacrificed. On the other hand, true clusters could be merged into a few fat clusters when we h ave the crowded connection between dense grids, such as the result in Figure 1(a).1 and Figure 1(a).3. In such cases, we have the undesired loss of intra-cluster similarity in the clustering result.
Figure 1(b) illustrates another critical limitation in current subspace cluster-ing algorithms. Essentially, users could identify a set of parameters which is able to precisely discover all clusters embedded in a subspace, such as the result in the 2-dimensional subspace { X, Y } shown in Figure 1(b). However, there are numerous subspaces and using the same parameter setting is difficult to capture the best clustering charact eristics for different subs paces due to variety of their distributions. Consider the 2-D subspaces { X, Z } and { Y, Z } in Figure 1(b) as examples, where the result in { X, Z } is expected to have two separated clusters without linkages, and the result in { Y, Z } is expected to have three clusters with near-circular shapes instead of a set of small clusters with irregular shapes. As a result, we present in this paper an approach, called QASC (standing for Q uality-A ware S ubspace C lustering) to accurately co nstruct subspace clusters from dense grids. Specifically, in order to conserve data characteristics within each subspace clusters, QASC takes the data distribution into consideration. Given a set of dense grids, QASC is devised as a two-phase algorithm to merge dense grids: (1) dense grids are partiti oned into numerous small groups, where neighbor grids are located in the same group iff they are identified belonging to the same area influenced by a density function; (2) deliberately merge these small groups according to their distances and density functions by a hierarchical clustering manner.

The remaining of the paper is organized as follows. In Section 2, related works on subspace clustering are presented. In Section 3, we give the model and algo-rithm of QASC. Section 4 presents the exp erimental results. The paper concludes with Section 5. Without loss of generality, previous works on density-based subspace cluster-ing for high dimensional data can be classified into two categories according to whether the grid structure is applied or not. Most of these algorithms utilize the grid structure, and the CLIQUE algorithm is the representative of such grid-based algorithms. On the other hand, a few works, e.g., the SUBCLU algorithm, can identify subspace clusters without use of grids.

Specifically, in the first step of CLIQUE, the data space is binned into equi-sized and axis-parallel units, where the width  X  of each dimension of a unit is one user-specified parameter. Afterwa rd, the second step of CLIQUE exploits an apriori-like method to recursively identify all dense units in a bottom-up way, where a dense unit is a unit whose de nsity exceeds another user-specified threshold  X  .

The use of grids can greatly reduce the computational complexity [6]. How-ever, CLIQUE inevitably incurs many limitations from (1) using the support as a measure of interesting grids and (2) setting the subtle grid width. Consequently, the SUBCLU algorithm [3] and its extension utilize the idea of density-connected clusters from the DBSCAN algorithm without the use of grids. Giving two pa-rameters and m in SUBCLU, the core objects are defined as the data points containing at least m data points in their -neighborhood. Since the definition of core objects also has the monotonicity property, clusters can be considered as a number of density-connected core obj ects with their surrounding objects, and identified in a bottom-up manner like CLIQUE. In general, SUBCLU can avoid the limitations of grids. However, the computation is higher than grid-based so-lutions. In addition, it also leaves the user with the responsibility of selecting subtle parameters. Even though users can empirically set parameter values that will lead to the discovery of acceptable c lusters in a subspace, SUBCLU also has the problem illustrated in Figure 1(b) that clustering quality in other subspaces may be strikingly unsatisfactory.

Several variants of CLIQUE have been proposed to resolve the limitation of using the support as the measure of interesting grids. The ENCLUS algo-rithm in [2] utilizes entropy as a measure for subspace clusters instead of using support. The basic idea behind ENCLUS is that entropy of any subspace with clusters is higher than that of any subspace without clusters. The SCHISM algorithm is proposed to discover statistically  X  X nteresting X  subspace clusters, where a cluster is interesting if the number of points it contains is statistically significantly higher than the number in the uniform distribution according to Chernoff-Hoeffding bound [7]. In addition, the MAFIA algorithm solves another limitation in CLIQUE. It uses adaptive, variable-sized grids whose widths are determined according to the distribution of data in each dimension [5]. As such, the side-effect from the rigid setting of grid widths in CLIQUE can be minimized. However, these new algorithms all merge dense/interesting grids as the same as CLIQUE. Depending on the connectivity between dense grids, they will face the same trade-off between inter-cluster dissimilarity and intra-cluster similarity in different subspaces as we show in Figure 1. 3.1 Problem Description We first introduce the notations used hereafter and then formalize the problem. Without loss of generality, we formalize the grid-based model by following the definition in CLIQUE. Specifically, let S = A 1  X  A 2  X  ...  X  A d be the d -dimensional data space formed by the d data attributes. A k -dimensional subspace is the space with the k dimensions drawn from the d attributes, where k  X  d .
In the grid-based subspace clustering, the data space S is first partitioned into a number of non-overlapping rectangular units by dividing each attribute into  X  intervals, where  X  is an input parameter. Consider the projection of the dataset in a k -dimensional subspace. A  X  k -dimensional grid  X , u is defined as the intersection of one interval from each of the k attributes, and the density, or said support , of u is defined as the number of data points contained in u .InCLIQUE, agridissaida dense grid if its density exceeds a threshold  X  ,where  X  is called  X  the density threshold X  . Note that the definition of density grids is different be-tween various subspace clustering algorithms, but subspace cluster is generally considered as disjointed sets of dense grids in CLIQUE and all its successors. 3.2 The QASC Algorithm We then describe our algorithm, called QASC (the Q uality-A ware S ubspace C lustering algorithm), to deliver high-quality subspace clusters while considering the generality of the proposed model. We aim at improving the strategy of merging grids for the generality issue wh ile conserving two g eneral criteria of clustering quality, i.e., inter-cluster dissimilarity and intra-cluster similarity. To achieve this, the data distribution is taken into account. The basic idea behind our model is to construct small and disjointed groups of dense grids initially, where grids in each group are influenced by the same density function. Therefore, we are able to guarantee the intra-cluster similarity in the first phase. Afterward, we merge groups for improving the inter-cluster dissimilarity. We then formally present these two steps in the fo llowing sections, respectively.
 Phase I of QASC: Identify Seed Clusters. The first step of QASC is to identify highly condensed group of dense grids, called seed clusters in this paper. We first have to present necessary definitions before introducing the solution to identify seed clusters.
 Definition 1 (Grid Distance): Suppose that V y =[ a 1 ,a 2 , ..., a k ] represents the center vector of grid y in the k -dimensional space S k ,where a i denotes its index in the i -th dimension in the grid coordinates. The grid distance between grid y and grid y in the k -dimensional space S k is defined as the normalized Manhattan distance in the grid coordinates: According to the definition, a grid y is said a neighbor grid of y in S k if Dist ( y, y )=1 .
 Definition 2 (Seed Gird): Given the set of dense grids D in the k -dimensional space S k , agrid g is called a seed grid iff its density sup ( g ) is larger than the density of any of its neighbor grids in D .

Essentially, a seed grid is a local maximum in terms of the density in the k -dimensional space, and we are able to identify the set of seed grids in each subspace by a hill-climbing procedure.
 Definition 3 (Density Function): A density function of a grid y wrt a seed grid y sg in the k -dimensional space S k is a function f ( y sg ,y ): S d  X  R + 0 which shows the degree of the influence from y sg in y ,and
In principle, the density function can b e arbitrary. However, to conserve the nature characteristics in the input data without the assumption of the data dis-tribution, the density function is specified according to the support distribution: f ( y sg ,y )=
Based on the foregoing, we can define the seed cluster, which is used to denote the region influenced by a density function: Definition 4 (Seed Cluster): Given the set of dense grids D in the k -dimensional space S k , a seed cluster c i wrt a seed grid y sg is the maximum set of dense grids in which each grid y has f ( y sg ,y ) &gt; 0 , i.e., c i = y sg  X  { X  y  X  D | f ( y sg ,y ) &gt; 0 } .

Figure 2 shows the identification of seed grids and seed clusters, where these sets of dense grids in Figures 2(a)  X Figure 2(d) are discovered with different pa-rameters in CLIQUE. Clearly, a seed grid, e.g., grid A, B, C, or D, in Figure 2(a), has a local maximum density in the density d istribution. In addition, a seed clus-ter wrt a seed grid y sg covers a set of grids surrounding y sg which are with the same distribution trend, indicating that grids within a seed cluster are highly condensed. Clearly, seed clusters can be c onsidered as a set of most strictly de-fined clusters and the intra-cluster sim ilarity can be entirely conserved in seed clusters.

Note that seed clusters inherently cannot contain grids with the density smaller than the density threshold in CLIQUE even though these grids may satisfy the definition of density function. It is the natural limitation from the process of identifying dense grids. Nevertheless, as can be seen in Figure 2, four major seed clusters are all identified in various situations, showing the identifi-cation of seed clusters can robustly distinguish characteristics in groups of grids. On the other hand, clusters cannot be separ ated in Figure 2(c) and Figure 2(d) if we rashly connect dense grids into subspace clusters. The intra-cluster similarity is inevitably sacrificed.
 The whole procedure to identify seed clusters in a subspace is outlined in Procedure Seed Identify() . Specifically, the given set of dense grids should be sorted in order of decreasing grid densit y. Therefore, we can identify the seed grid from the root of the list and utilize a hill-climbing manner to search if a grid belongs to the generated s eed grid. If a connected grid y i is identified satisfying Definition 4, we set y i .cluster pointed to the corresponding seed cluster. The next grid in the sorted list is skipped if it has been identified belonging to a seed cluster. Otherwise, the procedure is itera tively executed until we have identified the cluster index for each grid. Finally, the set of seed clusters is returned. Phase II of QASC: Merge Seed Clusters. In essence, the seed clusters conserve the intra-cluster similarity. The inter-cluster dissimilarity is not con-sidered yet. As shown in Figure 2(a), i t is expected that seed clusters A and B belong to the same cluster because they have the same trend of distribution and are quite close to each other. Similarl y, seed clusters C and D follow the same distribution. Note that the gap with the grid distance equal to one between seed clusters C and D may occur due to noise and the choice of the grid cutting-line. It is desired to have the clustering result with only two clusters in terms of both the intra-cluster similarity and the inter-cluster dissimilarity.

The second step of QASC is thus to delib erately merge seed clusters by a hierarchical clustering manner, where the distance between clusters is taken into consideration. Here we define the cluster grid distance first.
 Definition 5 (Cluster Grid Distance): Given two clusters c i and c j ,the cluster grid distance between c i and c j is defined as
The general criterion to merge two seed clusters is that they should be close to each other, i.e., they have the small CDist ( c i ,c j ). In addition, their seed grids should also be close to each othe r and the difference of the cluster sizes should be significantly large, meaning that they are likely to belong to the same distribution. As such, we build a global heap for merging clusters. The heap is sorted by the weight defined as: where size ratio ( c i ,c j )=max { | c i | | c points in clusters c i and c j , respectively. In addition, M inSeedGridDist ( c i ,c j )= min { Dist ( y i ,y j ) } , where y i is a seed grid in c i and y j is a seed grid in c j .
Importantly, clusters with a quite large CDist ( c i ,c j )arenotpermittedto be merged even though weight ( c i ,c j ) is large. Note that it is reasonable to consider merging clusters with a small distance gap such as the example of seed clusters C and D in Figure 2(a). It is sufficient to avoid the influence from noise or the choice of the grid cutting-line if we permit a tolerant grid distance equal to one. As such, the prerequisite to insert the cluster pair into the heap is CDist ( c i ,c j )  X  2 k, where k is the number of dimensions in the subspace S k .
The procedure in QASC to hierarchically merge clusters is outlined in Pro-cedure Seed Merge() , where the input is the set of seed clusters in S k .Note that while two clusters c i and c j are merged, all information of cluster pairs in the heap related to c i and c j should be updated according to their new weight value.
 Another criterion to determine if two clusters should be merged is shown in Line 11 in Seed Merge() . Essentially, it is not desired to merge two clusters if they have similar cluster sizes because they are difficult to follow a single dis-tribution trend. We set  X  =1 . 2 in default to ensure th e merged clusters are of variant sizes. Finally, the set of remaining clusters are returned when the heap is empty.
 We assess the result of QASC in Windows XP professional platform with 1Gb memory and 1.7G P4-CPU. In this section, we call the methodology to rashly mergedensegridsasthe naive approach, which is used in CLIQUE and all grid-based subspace clustering algorithms. For fair comparison, we generate dense grids by the first step in CLIQUE for QASC and the naive approach. Note that our goal is to provide an effective approach for merging grids, and the current grid-based subspace algorithms all utilize the naive approach. The benefit from QASC for these variant algorithms is expected if we can gain good clustering quality for CLIQUE. All necessary code s are implemented by Java and complied by Sun jdk1.5.

Note that various approaches to identify dense grids in subspaces introduce various parameters which would affect th e clustering quality. We study the sen-sitivity of the QASC algorithm and the naive algorithm in various parameter setting of CLIQUE. For visualization reasons, the sensitivity analysis is studied in two dimensional spaces as the evaluation method used in traditional clustering algorithms. The result of the first study is shown in Figure 3, where a synthetic data with 6,547 points is used. Note that CLIQUE introduces two parameters, i.e., (1) the number of grids in each dimensions and (2) the density threshold, which are specified as  X  X rid X  and  X  X insup X  in figures. Clearly, two clusters with similar diamond-like shapes are expected in the clustering results. However, the naive approach cannot capture the best result in this datasets wrt different pa-rameter setting of CLIQUE. Figure 3(a) shows that the naive approach tends to merge clusters in high connectivity bet ween dense grids, whereas Figures 3(b) and 3(c) show that many clusters are reported if dense grids distribute sparsely. On the other hand, Figures 3(d)  X (f) sho w that QASC results in acceptable re-sults with two expected clusters. Specific ally, the two clusters are separated in Figure 3(d) because QASC does not merge two clusters with similar sizes. In addition, it is worth mentioning that QASC permits the combination of clusters when they are distributed with a gap equal to one. Therefore, QASC can report two acceptable clusters, as shown in Fig ure 3(f), to avoid the side-effect from the improper parameter setting of subspace clustering algorithms, indicating the robustness of QASC.

We study the sensitivity issue in another 7-dimensional synthetic data with 6,500 points. The data is generated by embedding clusters in two 2-dimensional spaces and a 3-dimensional space. The clustering results in these subspaces are shown in Figure 4. In this case, we set grid=20 and the density threshold equal to 0.08%, which is able to correctly retrieve three clusters in the first subspace for the naive approach. However, similar to the example illustrated in Figure 1(b), this parameter setting is difficult to make correct clustering result for other subspaces. In contrast, QASC can retriev e accurate subspace clusters in other subspaces since the data distribution is taken into consideration.
 In this paper, we proposed an effective algorithm, QASC, to merge dense grids for generating high-qua lity subspace clusters. QASC is devised as a two-step method, where the first step generates seed clusters with high intra-cluster simi-larity and the second step deliberately m erges seed clusters to construct subspace clusters with high inter-cluster dissim ilarity. QASC is devised as a general ap-proach to merge dense/interesting grids, and can be easily integrated into most of grid-based subspace clustering algorithms in place of the naive approach of rashly connecting dense grids as clust ers. We complement our analytical and algorithmic results by a thorough empirical study, and show that QASC can retrieve high-quality subspace clusters in various subspaces, demonstrating its prominent advantages to be a practicable component for subspace clustering.
