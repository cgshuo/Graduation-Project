 In the recen t past, there has been a focus on information managemen t from scien tific litera-ture. In the genetics domain, for instance, in-formation extraction of genes and gene X  X rotein interactions helps geneticists scan large amoun ts of information (e.g., as explored in the TREC Genomics trac k (Hersh et al., 2004)). Elsewhere, citation indexes (Garfield, 1979) pro vide biblio-metric data about the frequency with whic h par-ticular pap ers are cited. The success of citation indexers suc h as CiteSeer (Giles et al., 1998) and Google Scholar relies on the robust detection of formal citations in arbitrary text. In bibli-ographic information retriev al, anc hor text, i.e., the con text of a citation can be used to charac-terise (index) the cited pap er using terms out-side of that pap er (Bradsha w, 2003); O X  X onnor (1982) presen ts an approac h for iden tifying the area around citations where the text focuses on that citation. And automatic citation classifi-cation (Nan ba and Okum ura, 1999; Teufel et al., 2006) determines the function that a cita-tion plays in the discourse.

For suc h information access and retriev al pur-poses, the relev ance of a citation within a pap er is often crucial. One can estimate how imp or-tan t a citation is by simply coun ting how often it occurs in the pap er. But as Kim and Webb er (2006) argue, this ignores man y expressions in text whic h refer to the cited author X  X  work but whic h are not as easy to recognise as citations. They address the resolution of instances of the third person personal pronoun  X  X hey X  in astron-omy pap ers: it can either refer to a citation or to some entities that are part of researc h within the pap er (e.g., planets or galaxies). Sev eral appli-cations should profit in principle from detecting connections between referring expressions and citations. For instance, in citation function clas-sification, the task is to find out if a citation is describ ed as flawed or as useful. Consider: The three citations above are describ ed as flawed (detectable by  X  X o es not provide a very satis-factory account X  ), and thus receiv e the lab el Weak . However, in order to detect this, one must first realise that  X  X his appr oach X  refers to the three cited pap ers. A con trasting hypoth-esis could be that the citations are used (thus deserving the lab el Use ; the cue phrase  X  X ased on X  migh t mak e us think so (as in the con text  X  X ur work is based on X  ). This, however, can be ruled out if we kno w that  X  X he speaker X  is not referring to some asp ect of the curren t pap er. We define an attribution task where possible ref-eren ts are mem bers of the reference list (i.e., eac h cited pap er), the Current-P aper , and a bac k-off category No-Specific-P aper for mark ables that are not attributable to any spe-cific pap er(s). Our mark ables are as follo ws: all definite descriptions (e.g.,  X  X he hearer X  , and including demonstrativ e noun phrases suc h as  X  X hese intentions X  ), all  X  X ork X  nouns 1 , and all pronouns (possessiv e, personal and demonstra-tive); c.f., underlined strings in the above exam-ple. Our notion of attribution link encompasses two relations: 1. Anaphoric : The referen ts are entire re-2. Subp art : The referen ts are some comp o-
There are two tasks: attributing a linguistic expression to the righ t pap er (including the cur-ren t pap er)  X  a task we call scientific attribution  X  and deciding whether or not the expression is anaphoric to the entiret y of the pap er, or just to some subpart of it.

Kim and Webb er (2006) solv e the problem of distinguishing between these relations for one case. They decide whether the pronoun  X  X hey X  anaphorically refers to the authors of a cited pa-per, or whether it refers to some entity that is discussed in (a subpart of) a pap er (e.g.,  X  X alax-ies X  ). In this pap er, we tackle the other problem of scien tific attribution.

We do not distinguish between the two types of links stated above, but only iden tify whic h ci-tation(s) a linguistic expression is attributable to. For tasks of interest to us, it is not enough to only consider anaphoric references to entire pap ers; authors often mak e statemen ts compar-ing/using/criticising aspects or subp arts of cited work. We therefore consider a far wider range of mark ables than Kim and Webb er X  X  single pro-noun  X  X hey X  .

Our attribution task differs from the tradi-tional anaphora resolution task in that we have a fixed list of possible referen ts (the reference list items, Current-P aper or No-Specific-Paper ) that are kno wn upfron t. Also, we do not form co-reference chains; we attribute a re-ferring expression directly to one or more ref-eren ts. Ours is therefore a multi-lab el classi-fication task, where the citations, Current-Paper and No-Specific-P aper are the lab els, and where one or more lab els are assigned to eac h mark able.

We evaluate intrinsically by comparing to human-annotated attribution, and extrinsically by sho wing that automatically acquired kno wl-edge about scien tific attribution impro ves per-formance on a discourse classification task X  Argumentative Zoning (Teufel and Mo ens, 2002), where sen tences are lab elled as one of f Own, Other, Background, Textual, Aim, Basis, Contrast g according to their role in the author X  X  argumen t.

We describ e our data in x 3 and metho dology in x 4, discuss evaluation metrics in x 5, and eval-uate intrinsically in x 6 and extrinsically in x 7. We used data from the CmpLg (Computation and Language archive; 320 conference articles in computational linguistics). The articles are in XML format.

We pro duced an annotated corpus (10 arti-cles, 4290 data points, i.e., mark ables) based on written guidelines. The task was found to be quite intuitiv e by our annotators, and this was reflected in high agreemen t -Kripp endorff  X  X  al-pha 2 of more than 0.8 (2 annotators, 3 pap ers, 1429 data points) on the attribution task. The distribution of classes was, as exp ected, quite skewed: 69% of mark ables are attributable to Current-P aper , 7% to no specific pap er and 24% to specific references (on average, 1.7 per reference). Details about the annotation pro-cess and human agreemen t figures can be found in Siddharthan and Teufel (2007). We frame the attribution problem as a classi-fication task: Giv en a mark able (the definite description/pronoun/w ork noun under consid-eration), a binary yes/no decision is made for eac h cited pap er, and a binary yes/no decision is made for whether the mark able is attributable to the curren t pap er. The list of lab els for the mark able is compiled by including all the cita-tions for whic h the mac hine learner returns yes, and Current-P aper if the learner returns yes. If the list is empt y (learner returns no for every-thing), the lab el is No-Specific-P aper .
Since the mo del for whether a mark able is at-tributable to the curren t work is likely to be differen t from the mo del for whether it is at-tributable to a citation, we trained separate mo dels for the two problems. 4.1 Deciding attribution to a citation For eac h data point to be classified (called NP below), we create a mac hine learning instance for eac h reference list item by automatically computing the follo wing features from POS-tagged text:
We have a chicken and egg problem with cal-culating the distance of a reference to curren t work in 2(e). Unlik e citations, these are not un-ambiguously mark ed in the text. We calculate distance from the closest first person pronoun (even though these could possibly refer to a self citation, rather than curren t work) or the phrase  X  X his pap er X , whic h can again refer to other cita-tions but predominan tly refers to curren t work. 4.2 Deciding attribution to curren t work We use the same features for the second clas-sifier that mak es the decision on whether the data point refers to Current-P aper , with the follo wing changes: Features 1(b,c) are remo ved as they are meaningless; 1(d) chec ks Hobbs X  prediction for a first person pronoun/ X  X his pa-per X , rather than CIT; in 2(a X  X ), the distance is measured between the closest first person pro-noun/ X  X his pap er X  and the mark able, rather than a citation and the mark able; similarly , in 3(b,c) we coun t instances of first person pro-noun/ X  X his pap er X ; for 2(e), we now calculate the distance of the closest citation instance. In short, the same features are used, but curren t work and citations are swapp ed. We consider two evaluation metrics. The first is the scoring system used for the co-reference task in the Message Understanding Conferences MUC-6 and MUC-7. The second is Kripp en-dorff  X  X   X  . We briefly discuss both below. 5.1 The MUC-6/MUC-7 Metric The MUC-6/MUC-7 Co-reference evaluation metric (Vilain et al., 1995) works by compar-ing co-reference classes across two annotated files. Calling one annotation the  X  X o del X  and the other the  X  X ystem X , for eac h co-reference class S in the mo del, c ( S ) is the minimal num-ber of co-reference links needed to generate the class (this is one less than the cardinalit y of the class; c ( S ) = j S j 1). m ( S ) is the num ber of  X  X issing X  links in the system annotation rela-tive to the co-reference class as mark ed up in the mo del. In other words, this is the minim um num ber of co-reference links that need to be added to the system annotation to fully gener-ate the co-reference class S in the mo del. Recall error is then RE ( S ) = m ( S ) /c ( S ) and Recall is tire file (or set of files) is calculated by summing over all co-reference classes in the mo del:
Precision ( P ) is calculated by swapping the mo del and system and the f-measure ( F = 2 R P/ ( R + P )) is symmetric with resp ect to both annotations. 5.2 Kripp endorff  X  X  Alpha We follo w Passonneau (2004) and Poesio and Artstein (2005) in using Kripp endorff (1980) X  X   X  metric to compute agreemen t between anno-tations. The adv antage of  X  over the more com-monly used  X  metric is that  X  allo ws for par-tial agreemen t when annotators assign multiple lab els to the same mark able; in this case calcu-lating agreemen t on a mark able requires a more graded agreemen t calculation than the  X 1 if sets are iden tical and 0 otherwise X  pro vided for by  X  . Kripp endorff  X  X   X  measures disagreemen t, and allo ws for the use of distance metrics to calculate partial disagreemen t. Follo wing Passonneau, we presen t results using four distance metrics: 1. (N)ominal: Tw o sets have distance N = 0 2. (J)accard: Tw o sets A and B have dis-3. (D)ice: Tw o sets A and B have distance 4. (M)ASI: This is the Jaccard distance J
As an example, consider two sets f a, b, c g and f b, c, d g . The distances between these sets are N = 1, J = 1 2 / 4 = 0 . 5, D = 1 2 2 / (3+3) = 0 . 33 and M = 0 . 67 0 . 5 = 0 . 33.

Kripp endorff  X  X   X  is defined as  X  = 1 D o /D e , where D o is the observ ed disagreemen t and D e is the disagreemen t that is exp ected by chance:
In the above form ulae, c is the num ber of coders, n jk is the num ber of times item j is classed as category k , n k is the num ber of times any item is classed as category k and d kk 0 is the distance between categories k and k 0 .

Lik e  X  , Kripp endorff  X  X   X  is 1 when there is perfect agreemen t, 0 when the observ ed agree-men t is only what was exp ected by chance, neg-ativ e when observ ed agreemen t is less than ex-pected by chance and positiv e when observ ed agreemen t is greater than exp ected by chance. We ran a mac hine learning exp erimen t us-ing 10-fold cross-v alidation and the memory-based learner IBk 3 (with k=6), using the Weka toolkit (Witten and Frank, 2000). The perfor-mance is sho wn in Tables 1 and 2. To position these results we compare them with three base-line lower bounds and the human performance upp er bound in Table 3. We use three baselines: Table 1: Agreemen t with Human Gold Standard
As Table 3 sho ws, our mac hine learning ap-proac h performs much better than the base-lines on all the agreemen t metrics, and is indeed closer to human performance than to any of the baselines. The MUC evaluation app ears to pro-duce highly inflated results on our task  X  when there is a small set of co-reference classes and one of these classes con tains 70% of data points, it tak es only a small num ber of missing links to correct annotations. This results in unreason-ably high values, particularly for the ma jorit y class baseline of lab elling every data point as Current-P aper . We believ e that the  X  met-rics pro vide a much more realistic estimate of the difficult y of the task and the relativ e perfor-mances of differen t approac hes.

Table 4 sho ws the performance of the ma-chine learner for eac h of the three types of lin-guistic expressions considered. Pronouns are the easiest to resolv e, with on average 90% re-solv ed correctly (an agreemen t with the human gold standard of  X  = . 71). This drops to 85% (  X  = . 68) for definite descriptions and demon-strativ es, and further to 78% (  X  = . 63) for re-Table 2: Evaluation using MUC-6/7 soft ware Table 3: Comparison with Baselines and Human Performance (Av eraged results) maining work nouns (i.e., those not already in a definite noun phrase).

While all the features con tributed to the re-ported results, the most imp ortan t features (in terms of information gain) for deciding attribu-tion to a citation were the paragraph level cita-tion coun t 3(b), the distance features 2(a,b,c,d), the rank 3(a) and the Hobbs X  prediction 1(d). The most imp ortan t features for deciding attri-bution to the curren t pap er were the distance features 2(a,c,e), the rank 3(a) and the Hobbs X  prediction 1(d). To demonstrate the use of automatic scien tific attribution classification, we studied its util-ity for one well kno wn discourse annotation task: Argumen tativ e Zoning (Teufel and Mo ens, 2002). Argumen tativ e Zoning (AZ) is the task of applying one of seven discourse level tags (Fig-ure 1) to eac h sen tence in a scien tific pap er.
These categories mo del several asp ects of sci-entific pap ers: from the distinction of segmen ts by who an idea is attributed to ( Own  X  Other  X  Background ), to the judgemen t of how the au-Table 4: Results for differen t mark able types thors relate to other work ( Contrast  X  Basis ) to the rhetorical status of high-lev el discourse goals (statemen t of Aim ; overview of section structure ( Textual )). Some of these categories (
Background , Other and Own ) occur in zones that span man y sen tences. Other categories typ-ically occur in short zones, often just a single sen tence ( Textual , Aim , Contrast , Basis ).
In all work to date, classification of sen tences into one of the AZ categories has been performed on the basis of features extracted from within the sen tence, and a few con textual features suc h as section heading and location in documen t. Scien tific attribution links previously unresolv ed noun phrases or pronouns in the sen tence to cita-tions. As this pro vides the mac hine learner with more information, AZ results should impro ve. 7.1 AZ Data The evaluation corpus used is the one from Teufel and Mo ens (2002). It consists of 80 con-ference pap ers in computational linguistics, con-taining around 12000 sen tences. Eac h of these is man ually tagged as one of f OWN, OTH, BK G, BAS, AIM, CTR, TXT g . The reliabilit y observ ed is reasonable (Kappa=0.71)). 7.2 Features Follo wing Teufel and Mo ens (2002), we used su-pervised ML using features extracted by shallo w pro cessing (POS tagging and pattern matc hing): Some categories tend to occur in blo cks (e.g., Own, Other, Background ), and the con text in terms of the lab el of the previous sen tence has good predictiv e value. We mo del this (the Table 5: Impro vemen t on AZ from using auto-matic scien tific attribution classification. so-called History feature ) by running the clas-sifier twice, and including the prediction for the previous sen tence as a feature the second time.
Due to practical considerations, we obtained our linguistic features using the RASP part of speech tagger (Brisco e and Carroll, 1995), when in previous work we used the LT TTT (Gro ver et al., 2000). We would not exp ect this to in-fluence results much, however. Another differ-ence is that we use around 1700 additional cue phrases acquired from previous work on another discourse task 4 (Teufel et al., 2006).

In addition to these features, we use four features obtained from the scien tific attribution task describ ed in this pap er: Scien tific Attribution Features :
Our aim is to explore whether these features obtained from the scien tific attribution task in-fluence mac hine learning performance on AZ. 7.3 AZ results We ran five differen t mac hine learners with and without the four scien tific attribution features (c.f., x 7.2). Note that our lab elled data for the attribution task does not overlap with the 80 pa-pers in the AZ corpus, and all attribution pre-dictions used in features for this AZ exp erimen t Table 6: Best AZ results using Stac ked classifier: with and without Attribution Features. are obtained entirely from unseen (and indeed unlab elled) data based on the mo del learn t on 10 pap ers (c.f., x 6). The learners we used (with default Weka settings) are:
As men tioned under History featur e above, we run eac h learner twice, the second time includ-ing the mac hine learning prediction for the pre-vious sen tence (as we found in Teufel and Mo ens (2002) for NB, we noticed a sligh t impro vemen t in performance when using the history feature (between .005 and .01 on both  X  and Macro-F for all learners)). We found an impro vemen t from including the four reference features with all the learners, as sho wn in Table 5.

For a more detailed view of where the im-pro vemen t comes from, refer to Table 6, whic h sho ws precision, recall and f-measure per cate-gory for our best learner. The biggest impro ve-men ts from using attribution features are for the categories Other , Aim and Bas . The impro ve-men t in Other was to be be exp ected, as this zone is directly related to the attribution classi-fication. The large impro vemen ts in Aim and Table 7: Teufel and Mo ens (2002) X  X  best AZ re-sults (Naiv e Bayes Classifier).
 Bas is good news, as these are amongst our most informativ e rhetorical categories for down-stream tasks. Our best results of Kappa=0.48 and Macro-F=0.53 are better than the best pre-viously published results on task (Kappa=0.45 and Macro-F=0.50 in Teufel and Mo ens (2002)). Our results impro ve on the results of Teufel and Mo ens (2002) (repro duced in Table 7)  X  both overall and for eac h individual category . We have describ ed a new reference task -decid-ing scien tific attribution, and demonstrated high human agreemen t (  X  &gt; 0 . 8) on this task. Our mac hine learning solution using shallo w features achiev es an agreemen t of  X  M = 0 . 68 with the human gold standard, increasing to  X  M = 0 . 71 if only pronouns need to be resolv ed. We have also demonstrated that information about scien-tific attribution impro ves results for a discourse classification task (Argumen tativ e Zoning).
We believ e that similar impro vemen ts can be achiev ed on other discourse annotation tasks in the scien tific literature domain. In particular, we plan to investigate the use of scien tific at-tribution information for the citation function classification task.
 Ackno wledgemen ts This work was funded by the EPSR C pro ject SciBorg (EP/C010035/1, Extracting the Science from Scien tific Publications).

