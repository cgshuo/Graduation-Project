 The rapid growth of information sources on the Web has intensified the problem of data quality. In particular, the same real world entity may be described by different sources in various ways with overlapping information, and possibly conflicting or even erroneous values. In order to obtain a more complete and accurate picture for a real world entity, we need to collate the data records that refer to the entity, as well as correct any erroneous values. We observe that these two tasks are often tightly coupled: rectifying erroneous val-ues will facilitate data collation, while linking similar records provides us with a clearer view of the data and additional evidence for error correction.

In this paper, we present a framework called Comet that interleaves record linkage with error correction, taking into consideration the source reliabilities on various attributes. The proposed framework first utilizes confidence based match-ing to discriminate records in terms of ambiguity and source reliability. Then it performs adaptive matching to reduce the impact of erroneous values. Experiment results demon-strate that Comet outperforms the state-of-the-art tech-niques and is able to build complete and accurate profiles for real world entities.
 H.2.8 [ Information Systems ]: Database Management X  data mining Record linkage; source reliability; truth discovery; entity profiling
In the age of Big Data, a real world entity X  X  information is, more often than not, published by more than one data source. Each of the data source may describe the same entity with name variations and provide incomplete and overlap-ping information. In order to obtain a complete picture of a real world entity, we need to collate the data records that refer to this entity. To complicate matters, not all the data sources are reliable and may publish erroneous information.
While the problem of record linkage has been well-studied [7, 15], linking and merging information from multiple sources remains a challenging task [20, 18]. The work in [20] ob-serves that sources have varying semantic ambiguity and proposes a framework to apply either a relaxed or a conser-vative matching criteria depending on how ambiguous is the source. On the other hand, [18] develops a transfer learning approach to learn a unified matching function to link records from multiple sources. These works do not explicitly con-sider the erroneous information in the data.

The work in [12] is the first attempt to link records in the presence of erroneous values. The authors transform the problem into a k-partite graph clustering problem where each node in the graph represents an attribute value and each edge associates a pair of values from the same record. This approach is computationally expensive and its perfor-mance degrades when the percentage of erroneous values increases, as shown in our experiments.

In this paper, we present an effective and efficient frame-work called Comet to collate data records from multiple sources, correct any erroneous attribute values, and con-struct profiles for real world entities.

Example 1. Suppose we want to collate information on researchers in Computer Science. We could first obtain a set of reference records from some well-established source such as the acm.org website. Table 1 shows the names and affil-iations of selected Computer Science researchers. Since the information in these reference records is limited, we would look at other data sources, such as university home pages, object-level search engines, LinkedIn, to harvest more infor-mation. Table 2 shows the information crawled from differ-ent sources, after transforming them into structured records. Note that records may contain ambiguous name representa-tions and conflicting attribute values.

In order to get a more complete profile of each researcher, a typical solution consists of two main steps. First, compute the similarity between the records in Tables 1 and 2, and form three clusters: Then determine the correct attribute values within each clus-ter by majority vote, and construct the following profiles: Table 2: Input Records from Various Data Sources r 1 Rakesh Agrawal Bell DM Wisconsin src 1 r 2 Alon Halevy Google DB Stanford r 3 Rakesh Agrawal MS DM src 2 r 4 A. Halevy Google DB r 5 Agrawal MS Wisconsin src 3 r 6 Charu Aggarwal IBM MIT r 7 Agrawal IBM Wisconsin r 9 Charu Aggarwal UIC DM MIT r 10 Agrawal IBM DM Wisconsin src 5
However, a closer examination would reveal that records r and r 9 which are published by source src 4 contain affiliations that differ from the corresponding profile records. This leads us to suspect that the affiliation information in record r highly likely to be incorrect since it is provided by the same source src 4 .

On the other hand, the education information published by src 4 for r 8 and r 9 are both correct, giving us the confidence that the value X  X isconsin X  X n r 7 can be trusted. With this key insight on src 4 providing unreliable affiliation information, and more reliable education information, we could infer that r 7 is more likely to refer to  X  X akesh Agrawal X  rather than  X  X haru Aggarwal X .

The above example illustrates that rectifying errors in at-tribute values and taking into consideration the reliability of data sources can provide additional evidence for linking records, leading to a more complete and accurate profile of an entity. So how can we utilize this observation to im-prove the accuracy of data collation, and do that efficiently in practice?
First, we introduce the notion of a reliability matrix to capture the reliability of each source for various attributes. Second, we interleave the processes of record linkage and er-ror correction so that they can benefit from each other: recti-fying errors in attribute values will facilitate record compar-ison and linkage, while linking similar records will provide a clearer view of the data and additional evidence of any erroneous values.

The proposed framework Comet consists of two main phases. The first phase is a confidence based matching that links each input record to one or more reference records. This yields a soft clustering of records and reduces the search space for the next phase. Based on the clustering results, we obtain an initial assessment of the reliability of the sources. Then the second phase, adaptive matching, leverages on the source reliability to iteratively determine the correct attribute values within each cluster and eliminate unlikely matches for the input records.

Experiment results on real world multiple source datasets demonstrate that Comet can build more complete and ac-curate entity profiles efficiently, and outperforms the state-of-the-art techniques.

The rest of the paper is organized as follows. Section 2 summarizes the related work. Section 3 gives the problem definition. The Comet framework is presented in Section 4. Section 5 describes the experimental evaluation and we con-clude in Section 6.
Constructing structured profiles of real world entities is a challenging task and has been addressed from various as-pects. [17, 10] extract entity-related facts from web pages, which form the input records in our framework. [21] pro-poses a holistic approach to solve the schema matching prob-lem of web tables. Our work builds upon these information extraction and schema matching techniques, and we focus on resolving the ambiguous references and erroneous values contained in the data records.

Record linkage, also known as entity resolution, aims to identify records that refer to the same real world entity [7, 15]. Techniques to match records can be categorized into learning-based algorithms and non-learning methods. Learning-based algorithms [1] train a classifier to label each record pair as match or unmatch, while non-learning meth-ods [9, 8] utilize a set of rules to link records. Based on the pairwise matching results, clustering algorithms are used to find sets of records where each set refers to a unique en-tity [14, 3]. The works in [20, 18] examine the problem of linking records from multiple sources. [20] transforms the records into a graph to estimate the ambiguity of each source and finds an optimized matching execution plan. [18] adopts transfer learning to learn a classifier which can cap-ture the common characteristics of all the sources as well as the specific characteristics of individual source pairs. All these methods are not aware of source reliabilities and their performance deteriorates when the data is noisy.

There is a line of research to resolve conflicts and find true values from data provided by different sources [13, 16]. These truth discovery algorithms usually adopt an iterative approach to let the data sources and attribute values vote for each other. The similarity between values [22] and the dependence between sources [6] can be taken into account to better model the complexities in the real world. Apart from these voting-based algorithms, the work in [19] constructs probabilistic models where the true values are regarded as latent variables, while [5] deduces the relative accuracy of attributes based on the available master data and a set of pre-defined rules. These works assume that record linkage has already been done and the input set of records refer to the same real world entity.

The work closest to ours, [12], proposes to link records with uniqueness constraints and erroneous values by model-ing it as a k-partite graph clustering problem. This approach assumes the existence of a key attribute that can uniquely identify an entity. It is computationally expensive as it com-pares all the records in each iteration, and its recall rate suffers when the percentage of erroneous values increases. In contrast, the two-phase approach in our proposed frame-work is efficient and remains robust when the percentage of errors and the degree of ambiguity vary.
In this section, we define the notations used as well as a formal definition of the problem addressed.

Let E be a set of real world entities .Eachentity e  X  X  is described by a set of attributes A .

Let S be a set of data sources . Each source s  X  X  may publish some information on a subset of the entities in E
Let R be a set of input records with attributes A .Each input record r  X  X  is published by some source s  X  X  , and refers to some entity e  X  X  . A published record may have missing or erroneous attribute values. Note that the records from different sources have been mapped to a uni-form schema.
 Let Q be a set of reference records with attributes A Q  X  A . Each reference record q  X  X  is known to be clean, and refers to some unique entity e  X  X  .

Let C be a set of clusters. Each cluster c  X  X  comprises of a reference record q  X  X  and a set of input records R c  X  X  Each cluster c has a signature H c = { &lt;a, v, pr&gt; | X  where pr is the probability for v being the correct value on attribute a .
 Problem Definition. Given a set of reference records Q and a set of input records R published by data sources S , the goal is to augment the records in Q with the true values (if any) of the attributes A X  X  Q .
This section describes the proposed framework Comet that collates data from different sources, corrects any erro-neous values, and constructs entity profiles. An overview of
Comet is given in Figure 1. The framework is designed to facilitate robust record matching, and leverage on the knowledge from truth discovery to improve its performance. Comet has two main phases: (a) confidence based matching and (b) adaptive matching.
The first phase finds a set of entities that an input record may potentially describe by linking each input record to one or more reference records. Each reference record and its set of associated records form a cluster. At the same time, it initializes a matrix to capture the reliability of the data sources for various attributes.

The second phase aims to determine the correct attribute values for each entity. It first finds the most accurate at-tribute values within each cluster to form the cluster signa-ture, and then updates the source reliability. Subsequently, clusters are refined by removing the records whose attribute values deviate significantly from the cluster signatures. This process is repeated until there is no change to the clusters.
Finally, we augment the reference record in each cluster with the values in cluster signature to construct the profile for the corresponding entity.
In this phase, we generate a set of candidate reference records for each input record. Due to the possible erroneous values contained in the records, we would need a relatively low similarity threshold so that the true matches are not missed. However, the low threshold will affect the quality of the resulting clusters, which will in turn impact the discovery of correct attribute values in the next phase (as shown in Section 5.3).

We overcome this dilemma by adopting a confidence based matching approach such that records which clearly reference some real world entity are linked first, while the decisions on difficult cases are postponed until we have gathered more information about the reliability of the data sources. This leads to a smaller set of candidate matches without sacrific-ing the recall.

We first create a cluster for each reference record q .Then we bootstrap the framework with a small set of confident matches between the input records R and reference records Q . Specifically, we measure the similarity between records using any existing record linkage method, e.g., the Fellegi-Sunter algorithm [9], and link a record r to a reference record q if r is highly similar to q and differs greatly to the records in
Q\{ q } . In other words, we are confident that r refers to q . We formalize this into the following definition.
Definition 1. Given two thresholds  X  H and  X  L ,where  X  H  X  , and a similarity function sim(), an input record r and a reference record q form a confident match if  X  q  X  X  where sim( r, q ) &gt; X  H ,and  X  q  X  X \{ q } ,sim( r, q ) &lt; X  L record r is called a discriminative record .

For each confident match, we compare the attribute values of record r with its matching reference record q to obtain an initial assessment of the reliability of the corresponding data source.

We define a reliability matrix M where each entry M [ s, a ] follows. Let D s be the set of discriminative records published by s . We first process the sources that have published some discriminative records, that is, D s =  X  .Let D a s  X  X  s be the set of records where both the attribute values r.a and its matching q.a are not null. Then we have Then for each attribute a where D a s =  X  ,wesetits M [ s, a ] to be the average of the non-null entries in M [ s ]. Finally, for the sources that have not published any discriminative record, we set their entries in M to some small value . Table 3: Clusters Obtained by Confidence Based Matching
Example 2. Consider the reference records in Table 1 and the input records in Table 2. Suppose the set of confi-dent matches are { ( r 1 ,q 1 ) , ( r 3 ,q 1 ) , ( r 6 ,q We compute the reliability of the five sources on attribute Affiliation by comparing these record pairs. Based on the reference records, we observe that records r 1 and r 9 provide wrong affiliations, while the others are all correct. Then by Equation 1 we have M [ src 1 , Affiliation] = 0 . 5 , M [ src 2 , Affiliation] = M [ src 3 , Affiliation] = 1 . 0 , M [ src 4 , Affiliation] = 0 . 2 , M [ src 5 , Affiliation] = .

After initializing the reliability matrix, we want to dis-tinguish records that originate from the sources which are significantly more unreliable than the others. If we consider the source reliability 1 as a random variable X ,thenwesay asource s is unreliable if where  X  and  X  is the mean and standard deviation of X . Otherwise, we consider the source as reliable .

For each record r published by a reliable source, we use all the attribute values to compare r with the reference records Q . We link r with a q  X  X  if the similarity between them exceeds a pre-defined threshold  X  .
 Finally, we process the records from less reliable sources. For these records, since their attribute values are expected to be more error-prone, we may miss the correct matchings if we simply compare the attribute values. As such, we use the name references, and compare each record r with all the records in each cluster on name. We add r to a cluster if it is similar to some record in that cluster. This approach is similar to the merge closure described in [2].
This is the average reliability of a source on all the at-tributes.

Algorithm 1: Confidence Based Matching input : Input records R from data sources S with output : Set of clusters C , reliability matrix M 1foreach q  X  X  do 2 create cluster c containing q ; 3 add c to C ; /* confident matches */ 4foreach r  X  X  do 5if r and q form a confident match then 6 add r to the cluster of q ; /* initialize reliability matrix M */ 7 let D s be the set of discriminative records from s ; 8foreach s  X  X  where D s =  X  do 9 let D a s  X  X  s be the set of records where 10 foreach a  X  X  where D a s =  X  do 11 set M [ s, a ] using Equation 1; 12 foreach a  X  X  where D a s =  X  do 13 let n be the number of non-null entries in M [ s ]; 14 M [ s, a ]= 1 n a  X  X  M [ s, a ]; 15 foreach s  X  X  where D s =  X  do 16 foreach a  X  X  do 17 M [ s, a ]= ; /* records from reliable sources */ 18 foreach unprocessed r  X  X  do 19 if Equation 2 does not hold then 20 foreach q  X  X  do 21 if sim( r, q ) &gt; X  then 22 add r to the cluster of q ; /* records from unreliable sources */ 23 foreach unprocessed r  X  X  do 24 foreach c  X  X  do 25 foreach r in cluster c do 26 if sim( r.N, r .N ) &gt; X  then 27 add r to cluster c ; 28 break;
Example 3. Table 3 shows the three clusters obtained by the confidence based matching phase. Discriminative records r 1 and r 3 are assigned to cluster c 1 , r 6 and r 9 are put in c ,while r 2 is placed in c 3 . We next compare these records with their associated reference records to obtain the source reliabilities . Sources src 4 and src 5 are found to be unreliable while the others are reliable. Thus records r 4 and r 5 are processed first. Record r 4 is put in cluster c 3 ,while r placed in both c 1 and c 2 as it is similar to both q 1 and q
For the remaining records r 7 , r 8 and r 10 published by unre-liable sources ( src 4 and src 5 ), we compare them with all the records in each cluster on attribute Name. Records r 7 and r 10 have identical name with r 5 and are assigned to clusters c and c 2 ; r 8 is highly similar to r 4 and is put in c 3 .
Algorithm 1 gives the details of this confidence based matching phase. We first create a cluster for each reference record (lines 1-3). Then we establish the confident matches between reference records and input records (lines 4-6). Lines 7-17 initialize the reliability matrix. Based on the source reliabilities, we process records that originate from the reli-able sources (lines 19-22) before those from the less reliable sources (lines 23-28). The output is a set of clusters. Note that an input record may not be placed in any cluster if it cannot be linked to any reference record.
After clustering the records, the next phase iteratively re-fines the clusters by interleaving truth discovery with record matching. There are three main steps in this adaptive match-ing phase: (a) compute the cluster signatures, (b) update the reliability matrix and (c) refine the clusters.
First, for each cluster c obtained in the previous phase, we build its signature H c based on the accuracy of the attribute values of the records in c . The reliability matrix is updated subsequently. Then each record r is compared with the sig-natures of the clusters it is associated with, and pruned from the cluster that it is the most dissimilar to. The above steps are repeated until there is no change to the clusters.
In order to form a cluster signature, we need to assign an accuracy score to each attribute value in the cluster. Intu-itively, a value published by more reliable sources tends to be more accurate, while a source that provides more accu-rate values tends to be more reliable. Thus we can let the values and the sources vote for each other. However, since our framework allows records to belong to multiple clus-ters, these records may skew the value accuracy and source reliability computations with repeated votes. Therefore a likelihood function is introduced to address the issue.
Let L ( r, c ) be the likelihood of a record r belonging to a cluster c ,and q be the reference record in c . L ( r, c )is initialized as follows:
We now discuss how we use this likelihood function to compute the cluster signatures and update the reliability matrix.

Let V a c be the set of values on attribute a within cluster c . The accuracy of a value v  X  X  a c is given by the sum of the reliabilities of its sources, weighted by the likelihood: where s r is the source that publishes r . The value accuracies are normalized such that they represent the probabilities of the values being true.
 for each a  X  X  such that &lt;a, v, pr&gt; = &lt;a, q.a, 1 . 0 &gt; if q.a = null where v m =argmax
Example 4. Let us calculate the accuracy of the two Ed-ucation values  X  X IT X  and  X  X isconsin X  in cluster c 2 .Sup-pose all the sources are equally reliable with a score of 0 . 8 , racy of  X  X IT X  and  X  X isconsion X  are 1 . 6 and 1 . 2 respectively. We normalize them into probabilities and insert the triplet &lt; Education ,  X  X IT X , 0 . 6 &gt; into the cluster signature of c
After obtaining the signatures for all the clusters in C ,we update the reliability of a source s for attribute a as the average accuracy of the values it publishes: where R s is the set of records published by s ,and C r is the set of clusters that contain r .

Note that a value may have different accuracy scores in different clusters. Hence for a value r.a , we calculate its accuracy as the sum of all its accuracy scores weighted by the likelihood.

Example 5. Consider the entry M [ src 4 , Affiliation] .Sup-pose we have determined acc (Affiliation , X  X BM X , c 1 )=0 . 4 and acc (Affiliation , X  X BM X , c 2 )=0 . 6 . Then the accuracy of the Affiliation value in r 7 is given by 0 . 5  X  0 . 4+0 . 5  X  We repeat the computation for the other records published
Once we have computed the cluster signatures and up-dated the reliability matrix, we prune the clusters by re-moving records that are unlikely to belong to them. While a record is unlikely to belong to a cluster if its similarity with the cluster signature is low, however, a direct comparison of the record with a cluster signature may lead to incorrect pruning due to the possible erroneous values in the record. As such, we incorporate the reliability matrix to adaptively lower the impact of inaccurate attributes on the matching decisions.

Given a record r and a cluster c ,wedefineamatching function match( r, c ) as follows: where sim( r.a, H c .a ) is the similarity between the values in record r and signature H c on attribute a . Note that this similarity may be weighted.

For each record r that associated with multiple clusters, we compute its match scores with all the clusters in C r using Equation 7, and remove r from the cluster with the lowest match score.

Example 6. Consider record r 7 which is associated with clusters c 1 and c 2 . We have determined Further, we have We compare r 7 with both H c 1 and H c 2 to calculate the match scores by Equation 7. Clearly, the match score with cluster c is higher, so we remove r 7 from c 2 .

Finally, we update the likelihood L ( r, c ) for the record r w.r.t. each cluster c  X  X  r as follows:
Algorithm 2 shows the details of this adaptive matching phase. We first initialize the likelihood for each record in the clusters (lines 1-4). Then we repeat the steps of cluster signature computation (lines 7-11), reliability matrix update (lines 12-13) and cluster pruning (lines 14-21) until there is no more change to the clusters.
Algorithm 2: Adaptive Matching input : Reliability matrix M , set of clusters C output : Set of refined clusters C 1foreach r  X  X  do 2 Let C r be the set of clusters that contain r ; 3foreach c  X  X  r do 4 initialize L ( r, c ) using Equation 3; 5 repeat 6foreach a  X  X  do 7foreach c  X  X  do 8f oreach v  X  X  a c do 9 calculate acc ( a, v, c ) using Equation 4; 10 normalization; 11 update cluster signature using Equation 5; 12 foreach s  X  X  do 13 update M [ s, a ] using Equation 6; 14 foreach r  X  X  do 15 if |C r | &gt; 1 then 16 foreach c  X  X  r do 17 calculate match( r, c ) using Equation 7; 18 c  X  arg min 19 remove r from c ; 20 foreach c  X  X  r do 21 update L ( r, c ) using Equation 8; 22 until there is no change to the clusters C ;
Let n be the number of input records R , m be the num-ber of reference records Q ,and p be the number of non-discriminative records from unreliable sources.

In the confidence based matching phase, the time com-plexity for matching the discriminative records and records from reliable sources are both O ( nm ), as they compare record pairs from R and Q . In contrast, matching the records from unreliable sources requires comparing each record to every other record in all the clusters. Let k is the largest number of clusters that a record is associated with. Then this step requires a time complexity of O ( knp ). Hence the time complexity of the confidence based matching phase is O ( nm + knp ).

For the adaptive matching phase, each iteration involves cluster signature computation and cluster pruning. Each of these steps has a complexity of O ( kn ). Since this phase takes k iterations to terminate, the time complexity is O ( k 2
In practice, k is usually a small constant. Hence the com-plexity of Comet is O ( nm + np ).
In this section, we present the results of extensive experi-ments to evaluate the performance of Comet .
 Datasets . We use two real world multiple source datasets:  X  Restaurant dataset .
  X  Football dataset .

Based on the Football dataset, we generate a series of datasets by varying the accuracy of attribute values and the ambiguity of name references. We vary the following parameters to obtain different datasets.  X  % err , the ratio of erroneous values to the total number  X  % ambi , the probability to abbreviate the name reference Table 4 shows the default values and the ranges for these parameters.

When varying % err , if the desired error rate is higher than that of the original data, we introduce errors to the at-tribute values; otherwise, we correct some errors contained in the original data. We generate errors as follows: for at-tribute Birthday, we randomly generate an erroneous value; for the other 3 attributes, we randomly select a value from the domain of that attribute. The erroneous values of the generated data have the same distribution as the original data, that is, the relative accuracy among different sources and attributes remain unchanged.

When varying % ambi , we abbreviate the names of the records in the original data and keep the other attributes unchanged. We use one of the following forms of abbrevia-tion with equal probability: remove first name; remove last
The original dataset provides the full name of each player. name; keep first name initial and last name; remove last name and the last letter of first name.
 Methods. We compare the following methods in our per-formance study:  X  Match [12]. This is the state-of-the-art method that  X  Pipeline . This is the baseline method that performs  X  Comet , the proposed framework. To be fair, we also uti-
Note that the reference records are used in both Match and Pipeline where the attribute values in these records are regarded as true values. The same blocking criteria is ap-plied to all the methods to reduce the number of record com-parisons. TF-IDF metric is used to measure the similarity of strings, where each token is measured by Jaro-Winkler dis-tance. Levenshtein distance is used for Birthday and Phone. We set  X  =0 . 8,  X  H =0 . 95 and  X  L =0 . 65 for Comet .
All the algorithms were implemented in Java, and the ex-periments were conducted in a Windows 7 machine with 3.40 GHz Intel CPU and 8 GB of RAM. Each experiment was repeated 3 times and the average performance is reported.
We first evaluate whether each input record can be linked to the correct reference record. The evaluation metrics in-clude Precision and Recall .

Let Ground be the set of correct matchings between in-put records and reference records, and Result be the set of matchings returned from the algorithms. Then we have
Table 5 shows the performance of various methods on the Restaurant dataset. We observe that both Comet and Match outperform Pipeline , demonstrating that combin-ing record linkage with truth discovery can lead to more robust record comparisons. Comet gives the best precision and recall since it leverages on the more reliable attributes to reduce the impact of erroneous values.
 Pipeline fails to link a considerable amount of records. We found that 57% of these records contain wrong phone Table 5: Record Linkage on Restaurant Dataset numbers, yet the Transfer model still assigns a high weight to this attribute. This indicates that the learning-based record linkage methods do not perform well in the presence of erroneous values. In contrast, Comet explicitly considers the source reliabilities and improves the performance signif-icantly.

Figure 2 shows the results on the Football dataset as we vary % err and % ambi . We observe that the performance of Match and Pipeline drop significantly as the percentage of errors increases. However, Comet remains robust and outperforms the others. Even when % err =0 . 5, Comet still achieves a recall of 0.77, which is 97% higher than the second best. This demonstrates its ability to correctly link records in the presence of erroneous values. Compared to Match , Pipeline is less sensitive to % ambi but more sensitive to % err . That is because the various ambiguities can be learnt more effectively.
Next, we evaluate whether Comet constructs an accurate and complete profile for each entity. The evaluation metrics are Accuracy and Coverage .

Let TrueValue be the set of attribute values in the ground truth that contained in the input records, and ReturnV alue Table 6: Truth Discovery on Restaurant Dataset be the set of attribute values returned by the methods and are not present in the reference records. Then we define
Table 6 shows the results on Restaurant dataset. We ob-serve that Comet outperforms Match by 15% on accuracy and 8% on coverage. One of the reasons for the low accu-racy of Match is that it is unaware of the source reliabilities. Pipeline gives the lowest coverage since many input records are not linked to any reference record. As a result, it fails to find all the attribute values of an entity.

Figure 3 shows the results on Football dataset as we vary % err and % ambi . Again, both Comet and Match out-perform Pipeline , indicating the advantage of integrating record linkage with truth discovery. However, Match is highly sensitive to the percentage of erroneous values. We observe that Comet improves the coverage significantly when % err and % ambi become large. Even when % err =0 . 5, Comet can still recover 78% of the correct attribute val-ues. This contributes to its robust record linkage. Overall, Comet consistently constructs both complete and accurate profiles for the entities when % err and % ambi vary.
In this section, we study the impact of the two main components in the Comet framework, namely, confidence based matching and adaptive matching. We use the Foot-ball dataset for this set of experiments and implement the following variants of Comet :  X  Com-CM . This method does not utilize confidence based  X  Com-AM . This method does not utilize adaptive match-
To evaluate the effect of confidence based matching, we measure the recall of the matchings established in the first phase of the framework, as the goal of this phase is to achieve a high recall.

Figure 4 shows the recall of record linkage after the first phase as we vary % err and % ambi . We observe that the confidence based matching has more impact when % err in-creases. It discriminates records from unreliable sources and thus significantly improves the recall. When % ambi becomes higher, by postponing the decisions to link the ambiguous records, Comet remains robust.

To evaluate the effect of adaptive matching, we measure the error rate of the cluster pruning process. We define er-ror rate as the ratio of the number of true matchings being pruned to the total number of pruned matchings. The main purpose of the adaptive matching is to reduce errors in clus-ter pruning.

Figure 5 shows the error rate of the cluster pruning when we vary % err and % ambi .When% err increases, adaptive matching avoids up to 59% of the errors by incorporating the reliability matrix to reduce the impact of erroneous values on matching decisions. We observe the gap between Comet and Com-AM widens as % ambi increases. This is because a record will be associated with more clusters, while the likelihood function in Comet is able to alleviate the bias brought by these records.

We also examine the effect of the similarity threshold  X  in confidence based matching of Comet .Wevary  X  from 0.7 to 0.9 and report the performance of record linkage on both the Restaurant and Football datasets in Figure 6. In both datasets, we obtain the best results at 0.8. A lower  X  generates a larger set of candidates for each input record, which hurts the purity of the resultant clusters. Then the cluster signatures computed in the subsequent phase will be less accurate, leading to incorrect pruning. On the other hand, if  X  is too high, correct matchings may be missed in the first phase, resulting in lower recall. We observe similar behaviour when varying  X  H and  X  L and we omit the graphs due to lack of space.
We also carried out experiments to compare the scalability of Comet , Match and Pipeline . We generate synthetic records using the Football dataset schema by varying the number of entities and number of data sources. Each source will publish a record on each entity. We partition the records and process them in parallel using a cluster with 10 nodes.
We first fix the number of sources at 100 and vary the number of entities from 2,500 to 10,000, thus increasing the number of records from 250k to 1 million. Figure 7(a) shows that the runtime of all the methods increase almost linearly, since the cluster size for each entity remains the same.
Figure 7(b) shows the runtime as we fix the number of entities at 2,000 and increase the number of sources from 50 to 500. We see that Comet remains scalable, while Match becomes computationally expensive when the num-ber of sources is large. This is because there will be more nodes in the encoded graph and its complexity is quadratic to the number of nodes. Further, Match will require more iterations to terminate.
Finally, we demonstrate the effectiveness of Comet by case studies. Table 7 shows a sample of the reference records extracted from www.yellowpages.com . Each reference record has two attributes: Name and Phone. (a) Vary number of entities
Table 8 shows a subset of the input records obtained from 6 websites. We transform the crawled data into a uniform schema, standardize attributes such as Address and Phone, and map range values to categorical values. We observe that the input records have ambiguous names and each provides a subset of attributes. Further, incorrect values are found in the address, phone, opening hours, etc.

Table 9 shows the profile records of two restaurants con-structed by Comet after collating data records from the various sources and correcting the erroneous values. We see that each profile record contains more complete information than any of the input record in Table 8 and is more accu-rate. Further, the prices and ratings in the profiles reduce the bias found in the different websites. Interestingly, we notice that TripAdvisor provides more reliable rating infor-mation, while Foursquare and Urbanspoon tend to provide higher ratings.
In this paper, we have addressed the problem of build-ing entity profiles by collating data records from multiple sources in the presence of erroneous values. We have de-signed a framework called Comet that interleaves record linkage with truth discovery to facilitate robust record match-ing and takes into account the reliability of the data sources to improve performance. The proposed framework first gen-erates a soft clustering of records, and then iteratively de-termines the cluster signatures and refines the clusters. Ex-tensive experimental results show that our approach signifi-cantly outperforms the state-of-the-art techniques on record linkage and truth discovery, and is both scalable and effec-tive. Case studies demonstrate that the proposed framework can construct entity profiles that are both complete and ac-curate.

In the future, we plan to integrate the information extrac-tion and schema matching processes into the framework, to provide a holistic approach. We believe that taking into ac-count the source reliabilities can improve the performance of them, and an integrated approach will lead to more effective knowledge discovery. [1] K. Bellare, S. Iyengar, A. G. Parameswaran, and [2] O. Benjelloun, H. Garcia-Molina, D. Menestrina, [3] I. Bhattacharya and L. Getoor. Collective entity [4] L. Blanco, V. Crescenzi, P. Merialdo, and P. Papotti. [5] Y. Cao, W. Fan, and W. Yu. Determining the relative [6] X. L. Dong, L. Berti-Equille, and D. Srivastava. [7] A. K. Elmagarmid, P. G. Ipeirotis, and V. S. Verykios. [8] W. Fan, H. Gao, X. Jia, J. Li, and S. Ma. Dynamic [9] I. P. Fellegi and A. B. Sunter. A theory for record [10] N. Garera and D. Yarowsky. Structural, transitive and [11] J. Gemmell, B. I. Rubinstein, and A. K. Chandra. [12] S. Guo, X. L. Dong, D. Srivastava, and R. Zajac. [13] M. Gupta and J. Han. Heterogeneous network-based [14] O. Hassanzadeh, F. Chiang, H. C. Lee, and R. J. [15] H. K  X  opcke, A. Thor, and E. Rahm. Evaluation of [16] X. Li, X. L. Dong, K. Lyons, W. Meng, and [17] X. Liu, Z. Nie, N. Yu, and J.-R. Wen. Biosnowball: [18] S. N. Negahban, B. I. Rubinstein, and J. G. Gemmell. [19] J. Pasternack and D. Roth. Latent credibility analysis. [20] W. Shen, P. DeRose, L. Vu, A. Doan, and [21] M. Yakout, K. Ganjam, K. Chakrabarti, and [22] X. Yin, J. Han, and P. S. Yu. Truth discovery with
