 Dept. of Computer Science, Cornell University, Ithaca NY 14853, U.S.A. IBM Watson Research Center, Yorktown Heights, NY 10598, U.S.A. Dept. of Computer Science, Cornell University, Ithaca NY 14853, U.S.A. Many fundamental problems in machine learning and statistics, such as evaluating expectations or partition func-tions, can be stated as computing integrals in very high dimensional spaces. Since exact integration is generally intractable due to the curse of dimensionality (Bellman, 1961), a number of approximation techniques have been introduced. Sampling (Jerrum &amp; Sinclair, 1997; Madras, 2002) and variational methods (Wainwright &amp; Jordan, 2008; Jordan et al., 1999) are perhaps the most popular, but rarely provide formal guarantees in practice. Pertur-bation based methods (Papandreou &amp; Yuille, 2011; Hazan et al., 2013) exploiting extreme value statistics provide a novel way of using MAP queries to recover the exact par-tition function and samples. However, their correctness re-lies heavily on fully independent Gumbel perturbations ap-plied to exponentially many configurations.
 A new family of provably approximately correct proba-bilistic inference and discrete integration algorithms seeks to overcome these limitations (Chakraborty et al., 2013a;b; Ermon et al., 2013a;b; Gomes et al., 2006b; 2007b). These techniques are all based on universal hash functions and require access to an optimization oracle to, e.g., answer a small (low degree polynomial) number of MAP inference queries. Since these oracle queries are NP-easy optimiza-tion problems, this strategy is desirable because discrete integration is a # P-hard problem (Valiant, 1979), a com-plexity class believed to be even harder than NP.
 This reduction based on universal hashing has been well studied in theoretical computer science (Valiant &amp; Vazi-rani, 1986; Bellare et al., 2000; Sipser, 1983). The focus is often on constructions that are optimal in terms of the number of random bits used. Typically, constructions ex-ploit modular arithmetic and can also be interpreted as par-ity or XOR constraints. In practice, one implements the optimization oracle with a combinatorial reasoning tool, such as a Boolean satisfiability (SAT) (Gomes et al., 2006b) or Integer Programming (Ermon et al., 2013a) solver. Al-though requiring exponential time in the worst-case, real-world instances are often solved quickly, and the imple-mentation of the hash function heavily affects the runtime. One must thus design hash functions that not only have good statistical properties but are also easy to reason with in practice. This is closely related to coding theory, where one seeks codes that have good properties assuming NP-hard optimal decoding is possible, but which also lead to prac-tical, efficient suboptimal decoding algorithms (MacKay, 1999).
 We introduce a new class of hash functions, termed Aver-age Universal, that are statistically weaker than traditional ones but strong enough to be used for discrete integra-tion and counting. Specifically, they retain the desirable property that for large enough sets, the size of each hash  X  X ucket X  is sufficiently concentrated around its mean. The main advantage is that they can be implemented with sparse low-density parity constraints, which are empirically much easier to handle. Thus, our construction provides a trade-off between statistical properties and computational effi-ciency of combinatorial reasoning with such constraints. This idea applies to a wide range of probabilistic inference and counting techniques that are based on universal hash-ing (Ermon et al., 2013b; Chakraborty et al., 2013a; Gomes et al., 2006b; 2007b), which would all benefit from our re-sults.
 To illustrate the gains, we empirically demonstrate that the WISH algorithm for discrete integration (Ermon et al., 2013b) benefits enormously from sparse parity constraints when an Integer Programming solver is used as optimiza-tion oracle (Ermon et al., 2013a). Specifically, given the same computational power, we obtain much better esti-mates on the partition function of graphical models thanks to the sparser parity constraints. The improvement over WISH comes at no cost, as we maintain the same theo-retical properties of the original algorithm. Further, we show that model counting techniques based on modern SAT solvers (Gomes et al., 2006b) also greatly benefit from our new hash function construction. We start with definitions of standard universal hash func-tions (cf. Vadhan, 2011; Goldreich, 2011).
 Definition 1. A family of functions H = { h : { 0 , 1 } n  X  { 0 , 1 } m } is -SU (Strongly Universal) if the following two conditions hold when H is chosen uniformly at random from H . 1)  X  x  X  { 0 , 1 } n , the random variable H ( x ) is uniformly distributed in { 0 , 1 } m . 2)  X  x 1 ,x 2  X  { 0 , 1 } n x 1 6 = x 2 ,  X  y 1 ,y 2  X  { 0 , 1 } m , it holds that P [ H ( x y ,H ( x 2 ) = y 2 ]  X  / 2 m .
 It can be verified that  X  1 / 2 m , and the case = 1 / 2 m corresponds to pairwise independent hash functions. Definition 2. A family of functions H = { h : { 0 , 1 } n  X  { 0 , 1 } m } is pairwise independent if the following two con-ditions hold when H is chosen uniformly at random from H . 1)  X  x  X  { 0 , 1 } n , the random variable H ( x ) is uni-formly distributed in { 0 , 1 } m . 2)  X  x 1 ,x 2  X  { 0 , 1 } n x x , the random variables H ( x 1 ) and H ( x 2 ) are indepen-dent.
 Statistically optimal functions can be constructed by con-sidering the family H of all possible functions from { 0 , 1 } fully independent functions. However, functions from this family require m 2 n bits to be specified, making this con-struction not very useful for large n . On the other hand, pairwise independent hash functions can be specified com-pactly. They are generally based on modular arithmetic constraints of the form Ax = b mod 2 , referred to as par-ity or XOR constraints.
 Proposition 1. Let A  X  { 0 , 1 } m  X  n , b  X  { 0 , 1 } m family H = { h A,b ( x ) : { 0 , 1 } n  X  { 0 , 1 } m } where h
A,b ( x ) = Ax + b mod 2 is a family of pairwise inde-pendent hash functions. 2.1. Probabilistic Inference by Hashing Recently there has been a range of probabilistic inference, discrete integration and counting, and sampling meth-ods relying heavily on universal hash functions, such as WISH (Ermon et al., 2013b;a), ApproxMC (Chakraborty et al., 2013b), MBound and Hybrid-MBound (Gomes et al., 2006b), and XORSample (Gomes et al., 2006a). These methods all build upon the original theoretical ideas by Valiant &amp; Vazirani (1986); Bellare et al. (2000); Sipser (1983).
 The key idea is that one can reliably estimate properties of a very large (high-dimensional) set S (such as the integral of a function) by randomly dividing it into cells using a hash function h and looking at properties of a randomly chosen, lower-dimensional cell h  X  1 ( y )  X  S . Remarkably, although h  X  1 ( y ) is exponentially large, by the properties of the hash functions used it is possible to specify and represent this set in a compact way, without having to enumerate each individual element. For example, applying the hash func-tion to all possible variable assignments for some proba-bilistic model (e.g., a discrete graphical model) is achieved by adding to the model a set of randomly generated parity constraints (e.g., extra factors), that can be specified in a compact way.
 To work with high probability, this family of algorithms relies on good statistical properties of the hash functions. Specifically, they need to behave in a uniform and concen-trated way, so that it is possible to predict | h  X  1 ( y )  X  S | (as a function of unknown | S | ) with high probability, no mat-ter what the structure of S is. Full independence would be ideal, as it would make the structure of S irrelevant. How-ever, fully independent hash functions are computational intractable. If there is high correlation between { h ( x ) } things might not work. In the extreme case, even if H is uniform (property 1), it is possible that all the random vari-ables h ( x ) ,x  X  S are identical, i.e., the division into cells does not break up S in a nice way  X  S is contained in a single cell in this extreme case. It turns out that pairwise in-dependence suffices and is also computationally tractable. All schemes mentioned earlier rely on pairwise indepen-dence. 2.2. ( k, X  ) -Concentration and Counting Formally, let S  X  { 0 , 1 } n and H = { h : { 0 , 1 } n  X  { 0 , 1 } m } be a family of hash functions. Let h be a hash function chosen uniformly from H . Then, for y  X  X  0 , 1 } define the following random variable: Let  X  ( h,S,y ) = E[ X ( h,S,y )] denote its expected value and  X  ( h,X,y ) the standard deviation. Note that if H is universal, then  X  ( h,S,y ) = | S | / 2 m . For brevity, we will sometimes use X and  X  when h , S , and y are implicit in the context. Our main interest is in understanding how con-centrated X is around  X  , under various choices of H . We introduce a general notion of concentration that will come handy: Definition 3. Let k  X  0 and  X  &gt; 2 . Let X be a random variable with  X  = E[ X ] . Then X is strongly ( k, X  ) -concentrated if Pr[ | X  X   X  |  X  weakly ( k, X  ) -concentrated if both Pr[ X  X   X   X  1 / X  and Pr[ X  X   X  + For a given  X  , smaller k corresponds to higher con-centration. Clearly, strong ( k, X  ) -concentration implies weak ( k, X  ) -concentration and, for k 0 &gt; k , ( k, X  ) -concentration implies ( k 0 , X  ) -concentration. Further, by union bound, weak ( k, X  ) -concentration implies strong ( k, X / 2) -concentration.
 As an illustrative example of how ( k, X  ) -concentration can be used for estimating sizes of high-dimensional sets, con-sider the following simple randomized algorithm A for approximately computing | S | with high probability. Let H i = { h : { 0 , 1 } n  X  { 0 , 1 } i } for i = 1 , 2 ,...,n be uni-versal families of hash functions. For i increasing from 1 to m , compute  X  is the set h  X  1 t (0)  X  S empty  X  T times, each time with a different hash function h t chosen uniformly from H i . If the answer is  X  X es X  for a majority of the T times, stop increasing i and return 2 i  X  1 as the estimate of | S | .
 Proposition 2. Let S  X  { 0 , 1 } n . If H i = { h : { 0 , 1 } n  X  { 0 , 1 } i } , i  X  { 1 , 2 ,...,n } are universal families of hash functions such that X ( h,S,y ) is weakly (  X  2 , 4) -concentrated, then A using H i and T  X  8 ln(8 n ) correctly computes | S | within a factor of 4 with probability at least 3 / 4 . We discuss how the statistical properties of various hash families H influence the strength of ( k, X  )-concentration of X = | h  X  1 ( y )  X  S | . Proofs may be found in the appendix. Without any assumptions on the nature of H , Chebychev X  X  inequality and Cantelli X  X  one-sided inequalities yield the following general observation for strong and weak concen-tration, resp.: Proposition 3. Let  X  &gt; 2 and H be a family of hash func-tions. For any S  X  { 0 , 1 } n and y  X  { 0 , 1 } m , and for h  X  R H , X ( h,S,y ) is strongly (  X  X  2 , X  ) -concentrated and weakly ((  X   X  1)  X  2 , X  ) -concentrated.
 Ideally, one would like to choose a family of fully indepen-dent hash functions, which results in very strong concen-tration guarantees from Chernoff X  X  bounds: Proposition 4. Let  X  &gt; 2 , c &gt; 3 and H be a family of fully independent hash functions. For any S  X  { 0 , 1 } n and y  X  { 0 , 1 } m , and for h  X  R H , X ( h,S,y ) is strongly (( c ln  X  )  X , X  ) -concentrated and weakly ((3 ln  X  )  X , X  ) -concentrated.
 However, as discussed earlier, it is often impossible to con-struct such a family. One commonly uses a family of only pairwise independent hash functions, which have compact constructions involving objects such as parity or XOR con-straints. The concentration guarantees we get are much weaker but still very powerful: Proposition 5. Let  X  &gt; 2 and H be a family of pairwise independent hash functions. For any S  X  { 0 , 1 } n and y  X  { 0 , 1 } m , and for h  X  R H , X ( h,S,y ) is strongly (  X  X , X  ) -concentrated and weakly ((  X   X  1)  X , X  ) -concentrated. This follows from observing that pairwise independence implies  X  2 = | S | / 2 m (1  X  1 / 2 m ) &lt;  X  and then applying Prop. 3.
 Although one can compactly represent pairwise indepen-dent hash functions as m parity constraints (cf. Prop. 1), these constraints have average length n/ 2 . Such long parity constraints are often particularly hard to reason about us-ing standard inference methods (see Experimental section below). To start addressing this issue, we observe that in many applications, O (  X  ) -concentration is unnecessary and it suffices to have only O (  X  2 ) -concentration. An example of this is Proposition 2. We next discuss how one can ex-ploit -SU hash functions to explore this wide spectrum of possible concentrations by varying .
 Theorem 1. Let  X  &gt; 2 ,  X  1 / 2 m , and H be a family of -SU hash functions. For any S  X  X  0 , 1 } n and y  X  X  0 , 1 } and for h  X  R H , X ( h,S,y ) is strongly (  X  X  (1 + ( | S | X  1)  X   X  ) , X  ) -concentrated and weakly ((  X   X  1)  X  (1 + ( | S | X  1)  X   X  ) , X  ) -concentrated.
 Proof. The variance of X can be computed as follows. E[ X ( h,S,y ) 2 ] = X Therefore,  X  2 = E[ X 2 ]  X   X  2  X   X  (1 + ( | S | X  1)  X   X  ) . The result now follows from Prop. 3.
 Corollary 1. Let  X  &gt; 2 ,  X  1 / 2 m , and H be a family of -SU hash functions. For any S  X  { 0 , 1 } n and y  X  { 0 , 1 } m , and for h  X  R H , X ( h,S,y ) is strongly (  X  2 , X  ) -concentrated whenever  X  (  X   X  +  X   X  1) / ( | S |  X  1) and weakly (  X  2 , X  ) -concentrated whenever  X  (  X   X   X  1 +  X   X  1) / ( | S | X  1) .
 Thus, by increasing , we can achieve lower (but still ac-ceptable) levels of concentration of X . In practice, how-ever, it is not easy to construct -SU families that allow ef-ficient inference. Simply using sparser parity constraints (i.e., with fewer than n/ 2 variables on average), for in-stance, does not lead to SU hash functions because if s,s 0  X  S are close in Hamming distance, sparser parity-based hash functions will act on them in a very correlated way. The next section provides a way around this. We now define a new family of hash functions that have the same statistical concentration properties as -SU (namely, the guarantees in Theorem 1) but are computationally much more tractable for inference methods.
 Definition 4. A family of functions H = { h : { 0 , 1 } n  X  { 0 , 1 } m } is ( ,i ) -AU (Average Universal) if the following two conditions hold when H is a function chosen uniformly at random from H .  X   X  x  X  X  0 , 1 } n , the random variable H ( x ) is uniformly  X   X  S  X  { 0 , 1 } n , | S | = i ,  X  y 1 ,y 2  X  { 0 , 1 } m , the fol-In other words, we allow pairs of random variables H ( x 1 ) ,H ( x 2 ) to be potentially heavily correlated, for in-stance Pr[ H ( x 1 ) = y 1 ,H ( x 2 ) = y 2 ] could be much larger than / 2 m . However, it needs to balance out so that the average correlation among configuration pairs on (large enough sets) S is smaller than / 2 m .
 Proposition 6. Let H be a family of hash functions. (a) If H is ( , 2) -AU, then H is also -SU. (b) If H is -SU, then H is also ( ,i ) -AU for all i  X  2 . (c) If H is ( ,i ) -AU, then H is also ( ,i + 1) -AU.
 Theorem 2. Theorem 1 and Cor. 1 also hold when H is a family of ( ,i ) -AU hash functions and | S | X  i . Proof. The proof is close to that of Theorem 1. The key observation is that whenever | S |  X  i , under an ( ,i ) -AU hash family we obtain the same bound on the variance,  X  2 as in the -SU hash family case. Our main technical contribution is the following: Theorem 3. Let A  X  { 0 , 1 } m  X  n be a random matrix whose entries are Bernoulli i.i.d. random variables of pa-rameter f  X  1 / 2 , i.e., Pr[ A ij = 1] = f . Let b  X  { 0 , 1 } be chosen uniformly at random, independently from A . Let w  X  = min n w | P w ( n,m,q,f ) = Then the family H f = { h A,b ( x ) : { 0 , 1 } n  X  { 0 , 1 } where h A,b ( x ) = Ax + b mod 2 and H  X  H f is cho-sen randomly according to this process, is a family of ( ( n,m,q,f ) ,q ) -AU hash functions.
 Proof. Let S  X  X  0 , 1 } n and y 1 ,y 2  X  X  0 , 1 } m . Then, For brevity, let  X  = y 1  X  y 2 . The probability Pr [ A ( x 1  X  x 2 ) =  X  ] depends on the Hamming weight w of x 1  X  x 2 , and is precisely the probability that the w columns of the (sparse) matrix A corresponding to the bits in which x 1 and x 2 differ sum to  X  (mod 2 ).
 In order to compute this probability, we use an analysis similar to MacKay (1999), based on treating the m random entries in each of the w columns of A as defining w steps of a biased random walk in each of the m dimensions of the m -dimensional Boolean hypercube. The probability that A ( x 1  X  x 2 ) equals  X  , when viewed this way, is nothing but the probability that starting from the origin and taking these w steps brings us to  X  . Note that this is a function of only w , f , and  X  ; the exact columns in which x 1 and x 2 differ do not matter. Let us denote this probability r ( w,f ) (0 , X  ) . Unlike MacKay (1999), each row of our matrix A is sam-pled independently. So we can model the random walk with m independent Markov Chains (one for each of the m dimensions) with two states { 0 , 1 } and with transition probabilities p Observing that the eigenvalues are 1 and 1  X   X   X   X  , it is easy to verify that
Pr[ X n = 0 | X 0 = 0] = and Pr[ X n = 1 | X 0 = 0] = 1  X  Pr[ X n = 0 | X 0 = 0] . Setting  X  =  X  = f and  X  = y 1  X  y 2 we get r because f  X  1 / 2 . Plugging into the previous expression we get where h ( w | x ) is defined as the number of vectors in S that are at Hamming distance w from x . Clearly, h ( w | x )  X  w . Since r we can derive a worst case bound on the above expression by assuming all n w vectors at distance w from x are ac-tually present in S for small w . Recalling the definition of w  X  from the statement of the theorem, for all | S |  X  q this gives:
X = 2  X  m | S | which proves that H is ( ( n,m,q,f ) ,q ) -AU.
 The significance of this result is that given n,m, and q , we have a family of hash functions parameterized by f for which we can control the  X  X verage correlation X  across all pairs of points in any set of size at least q . These range from rather dense but fully pairwise independent families when f = 0 . 5 , to statistically useful but much sparser and hence much more tractable families when f &lt; 0 . 5 . Algo-rithms for probabilistic inference that use universal hash-ing often do not need full or even pairwise independence but only (  X  2 , X  ) -concentration for success with high prob-ability. Section 4 thus prescribes a value of &gt; 1 2 m suffices. Using the above theorem, we can therefore look for the smallest value of f that is compatible with the re-quirement. For example, using Thm. 3 and Cor. 1 applied to AU hash families as Thm. 2, we can look for the smallest value f  X  such that the resulting family H f = { h A,b ( x ) : { 0 , 1 } n  X  { 0 , 1 } m } guarantees (  X  2 , X  ) -concentration for sets of size at least 2 m +2 and for  X  = 9 / 4 .
 In the top panel of Figure 1 we plot the corresponding f ( n ) as a function of the number of variables n , for m = n/ 2 constraints. In the bottom panel, we plot f  X  ( m ) as a function of m , for n = 100 . We see that we can ob-tain hash functions with provable concentration guarantees using constraints with average length scaling empirically as n 1  X  0 . 72 as opposed to n/ 2 for the pairwise independent construction. We also plot the smallest value of f that guar-antees concentration when the set S in Definition 3 is not an arbitrary set of size q = 2 m +2 but is instead restricted to be an ( m + 2) -dimensional hypercube, for which we have an exact expression for h ( w | x ) . This is clearly a lower-bound for f  X  (which is guaranteed to work for any set, hypercube included). The hypercube is intuitively close to a worst-case distribution for h ( w | x ) because points have small av-erage distance and hence high correlation. The compari-son with the hypercube case highlights that our bounds are fairly tight. 5.1. Alternative Construction We consider a related construction where we generate each row independently by fixing exactly t randomly chosen el-ements to 1 . In contrast, the previous construction has on average nf non-zero elements per row, but the number can vary. We can use an analysis similar to the one for Theo-rem 3, the main difference being that we need to substitute r Then we can obtain a closed form expression for by again looking at the worst-case distribution of the neighbors in terms of Hamming distance w . Our technique and analysis based on Average Universal hash functions applies to a range of probabilistic infer-ence and counting techniques such as WISH (Ermon et al., 2013b;a), ApproxMC (Chakraborty et al., 2013b), and MBound (Gomes et al., 2006b). While preserving all their theoretical properties in terms of approximation guaran-tees, by substituting pairwise independent hash functions with sparse Average Universal ones we obtain significant improvements in terms of runtime (see experimental sec-tion below). For concreteness and brevity, we discuss its application to the recent WISH algorithm for discrete inte-gration.
 Algorithm 1 SPARSE-WISH ( w,n = log 2 |X| ,  X  , X  ) for i = 0 ,  X  X  X  ,n do end for WISH is an algorithm used to estimate the partition func-graphical model by solving a small number of MAP queries on the original model augmented with randomly generated parity constraints. WISH uses a universal hash function to partition the space of all possible variable assignments into 2 cells, and then searches for the most likely variable as-signment within a single cell. By varying the number of cells used to partition the state space, it estimates the tail of the weight distribution, finding its quantiles that can then be used to obtain a constant factor approximation of the in-tractable partition function within any desired degree of ac-curacy, with high probability and using only a polynomial number of MAP queries.
 The original WISH (Ermon et al., 2013b) is based on pair-wise independent hash functions, constructed using ran-dom parity constraints of average length n/ 2 for a problem with n binary variables. Our previous analysis allows us replace them with sparser constraints as in Theorem 3, ob-taining an extension that we call SPARSE-WISH of which we provide the pseudocode as Algorithm 1. The key prop-erty is that Lemma 1 from Ermon et al. (2013b) still holds. Lemma 1. Fix an ordering  X  i , 1  X  i  X  2 n , of the config-urations in { 0 , 1 } n such that for 1  X  j &lt; 2 n , w (  X  w (  X  j +1 ) . For i  X  { 0 , 1 ,  X  X  X  ,n } , define b i , w (  X  M i = Median( w 1 i ,  X  X  X  ,w T i ) be defined as in Algorithm 1. Then, for 0 &lt;  X   X  0 . 0042 , The proof is based on a variance argument. Intuitively, the hash functions used at iteration i are by Thm. 3 ( , 2 i +2 AU with chosen such that by Cor. 1 and Thm. 2 they guar-antee weak (  X  2 , 9 / 4) -concentration for sets of size at least 2 i +2 . In particular, this guarantees that the hash functions will behave nicely on the set formed by the 2 i +2 est configurations (no matter what is the structure of the set), and M i will not underestimate b i by too much. See Appendix for a formal proof. We then have the following result analogous to Theorem 1 of Ermon et al. (2013b): Theorem 4. For any  X  &gt; 0 , positive constant  X   X  0 . 0042 , and the hash families H f given by Proposition 3, SPARSE-WISH makes  X ( n ln n ln 1 / X  ) MAP queries and, with prob-ability at least (1  X   X ) , outputs a 16-approximation of This means that if we carefully choose the density f  X  as in the pseudocode (which is a function of the number of con-straints i , and in general much smaller than 0 . 5 ; cf. bottom panel of Fig 1), we maintain the same accuracy guarantees but using much sparser constraints. In contrast, the analy-sis of short XORs in Ermon et al. (2013a) only guaranteed that the output is an approximate lower bound for Z , not a constant factor approximation. We evaluate SPARSE-WISH using the Integer Linear Pro-gramming (ILP) formulation from Ermon et al. (2013a) to solve the MAP inference instances in the inner loop of the algorithm. We use the Integer Programming solver CPLEX with a timeout of 10 minutes on Intel Xeon 5670 3GHz machines with 48GB RAM, obtaining at the end a lower bound and, by solving a sequence of LP relaxations, an up-per bound on the optimization instances. These translate into bounds for the generally intractable partition function Z 1 (Ermon et al., 2013a) . We evaluate these bounds on M  X  M grid Ising models for M  X  { 10 , 15 } . In an Ising model, there are M 2 binary variables, with unary poten-tials  X  i ( x i ) = exp( f i x i ) and (mixed) binary interactions  X  ij ( x i ,x j ) = exp( w ij x i x j ) , where w ij  X  R [  X  w,w ] and f  X  R [  X  f,f ] . The external field is f  X  X  0 . 1 , 1 . 0 } . In Figure 2 we show the median integrality gap 2 (over 500 runs) for the ILP formulation of the MAP inference in-stances, for i  X  { 10 , 15 , 20 , 25 , 30 } random parity con-straints generated at various density levels f . We see that problems with short XORs (generated with small f ) typi-cally have smaller integrality gaps, which confirms the fact that short XORs are easier to reason about. This is not sur-prising, because the optimizations involved are analogous to max likelihood decoding problems, and sparse codes are known to be easier to decode empirically (MacKay, 1999). We compare SPARSE-WISH with WISH (based on the same ILP formulation, but with denser f = 0 . 5 con-straints), with Loopy BP (Murphy et al., 1999) which es-timates Z without providing any accuracy guarantee, Tree Reweighted BP (Wainwright, 2003) which gives a provable upper bound, and Mean Field (Wainwright &amp; Jordan, 2008) which gives a provable lower bound. We use the implemen-tations in the LibDAI library (Mooij, 2010), allowing 1000 random restarts for Mean Field. Figure 3 shows the error in the resulting estimates, where ground truth is from Junction Trees (Lauritzen &amp; Spiegelhalter, 1988).
 We see that SPARSE-WISH provides significantly better bounds compared to the original WISH algorithm. Since they are both run for the same amount of time using the same combinatorial optimization suite, the improvement is to be attributed entirely to the sparser constraints employed by SPARSE-WISH, which are easier to reason about. Intu-itively, as seen in Figure 2 the LP relaxation obtained using shorter XORs is much tighter, hence improving the quality of the bounds, and yielding overall the best provable up-per and lower bounds among all algorithms we considered. Notice the improvement in terms of lower bound is smaller because in both cases the bounds are quite tight (with an error close to 0 ). Remarkably, SPARSE-WISH is the only method that does not deteriorate as the coupling strength is increased. We emphasize that the improvement over WISH comes at no cost, because thanks to our carefully chosen density thresholds f  X  , we maintain the same theoretical properties without trading off accuracy for speed. 7.1. Model counting for SAT Long parity constraints are difficult to reason about not only for Integer Programming solvers but also for SAT solvers. In fact, SAT solvers can be substantially faster on sparser parity constraints than those of length n/ 2 (Gomes et al., 2006b).
 The use of short parity constraints for model counting (i.e., count the number of solutions of a SAT instance) was inves-tigated by Gomes et al. (2007a), where it was empirically shown that short XORs perform well on a wide variety of instances a number of problem domains. Our analysis pro-vides the first theoretical basis for this observed empirical phenomenon, while also providing a principled way to es-timate a priori a suitable length of parity constraints to use. Table 1 reports the bounds obtained with our analysis on the benchmark used by Gomes et al. (2007a). The best pre-viously known theoretical bound on the length was n/ 2 , based on the pairwise independent construction (Prop. 1). The best previously known empirical bound was computed by finding the smallest XOR length such that the vari-ance of the resulting model count estimate is the same as what one would obtain with pairwise independent func-tions, which can be easily computed analytically. The new provable bound is computed by looking for the short-est XOR length that gives (  X  2 , X  ) -concentration and there-fore provides a  X  X orrect X  answer more than half the time (as in Prop. 2). Specifically, by Theorem 2, we look for the minimum XOR length satisfying the weak (  X  2 , 9 / 4) -concentration condition given by Corollary 1, where the number of variables ( n ), the log of the set size ( log 2 and the number of XORs ( m ) are taken from Gomes et al. (2007a) and reported in the first three columns of Table 1. The new empirical bound is also based on the shortest XOR length yielding weak (  X  2 , 9 / 4) -concentration, but us-ing Prop. 3 for general hash families and taking the sample variance as a proxy for the true variance,  X  2 .
 On this diverse benchmark spanning a variety of domains (Latin square completion, logistic planning, hardware veri-fication, random, and synthetic), the new theoretical bound on the minimum XOR length is significantly smaller than n/ 2 . The empirical bound we achieve (last column, of-ten in single digits and thus extremely efficient for SAT solvers) is also much smaller than the previously reported empirical bound. The gap between our new provable and empirical bounds is due to the intricate structure of the set S of solutions. If the solutions in S are far away on average (hence less correlated when using sparse parity constraints), we obtain an empirical variance that is much tighter than our provable worst-case bound. Overall, our new bounds significantly improve upon the previous best known bounds in all cases considered. We introduced a new class of hash functions, called Av-erage Universal, that can be constructed using low-density (sparse) parity constraints. While statistically weaker than traditional hash functions, these are still powerful enough to be used in hashing-based randomized counting and dis-crete integration schemes. Sparse parity constraints are em-pirically much easier to do inference with, a well-known fact in the context of low-density parity check codes. By substituting dense parity constraints with sparser ones, we obtain variations of inference and counting techniques that have the same provable guarantees but are empirically much more tractable. We show that this leads to signif-icant improvements in the bounds obtained by the recent WISH algorithm and in model counting applications.
