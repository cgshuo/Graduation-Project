 Recommender systems are widely deployed on the Web with the aim to improve the user experience [1]. Well-known e-commerce providers, such as Hulu (video) and NetFlix (movies), are highly relying on their developed recommender sys-tems. These systems attempt to recommend items that are most likely to attract users by predicting the pref erence (ratings) of the user s to the items. In the last decade, collaborative filtering (CF) methods [2 X 4], which utilize users X  past rat-ings to predict users X  future tastes, have been most popular due to their generally superior performance [5, 6]. Unfortunately, most of previous CF methods have not fully addressed the sparsity issue, i.e ., the target data are extremely sparse.
Recently, there has been an increasi ng research interest in developed transfer learning methods to alleviate the data sparsity problem [7 X 13]. The key idea behind is to transfer the common knowledge from some dense auxiliary data to the sparse target data. The common knowledge can be encoded into some common latent factors, and extracted by the latent factorization models [14]. Good choices for the common latent factors can be the cluster-level codebook [8, 9], or the latent tastes of users/latent features of items [11 X 13]. The transfer learning methods have achieved promising performance over previous methods.
However, when the target data are extremely sparse, existing transfer learning methods may confront the following issues. 1) Since the target data are extremely sparse, the latent factors extracted from the auxiliary data may transfer negative information to the target data, i.e., the latent factors are likely to be inconsistent with the target data, which may result in overfitting [15]. We refer to this issue as negative transfer . 2) As the target data are ext remely sparse, it is expected that more common knowledge should be transferred from the auxiliary data to reduce the sparsity of the target data [16]. We refer to this issue as insufficient transfer . Solving these two issues both can reduce the sparsity more effectively.
In this paper, we propose a novel approach, Twin Bridge Transfer Learning (TBT), for sparse collaborative filtering. TBT aims to explore a twin bridge cor-responding to the latent factors which encode the latent tastes of users/latent features of items, and the similarity graphs which encode the collaboration infor-mation between users/items. TBT consists of two sequential steps. 1) It extracts the latent factors from dense auxiliary data and constructs the similarity graphs from these extracted latent factors. 2) It transfers both the latent factors and the similarity graphs to the target data. In this way, we can enhance sufficient transfer by transferring more common k nowledge between domains, and allevi-ate negative transfer by filtering out the negative information introduced in the latent factors. Extensive experiments on tworeal-worlddatasetsshowpromising results obtained by TBT, especially when the target data are extremely sparse. Prior works using transfer learning for collaborative filtering assume that there exist a set of common latent factors between the auxiliary and target data. Rating Matrix Generative Model (RMGM) [9] and Codebook Transfer (CBT) [8] transfer the cluster-level codebook to the target data. However, since codebook dimension cannot be too large, the codebook cannot transfer enough knowledge when the target data are extremely sparse. To avoid this limitation, Coordinate System Transfer (CST) [12], Transfer by Collective Factorization (TCF) [11], and Transfer by Integrative Factorization (TIF) [13] extract both latent tastes of users and latent features of items, and transfer them to the target data. However, these methods do not consider the negative transfer issue, which is more serious when the target data are extremely sparse. Also, they only utilize a single bridge for knowledge transfer. Different from a ll these methods, our approach explores a twin bridge for transferring more useful knowledge to the sparse target data.
Neighborhood structure has been widely used in memory-based CF [17] or graph regularized methods [4]. Neighborhood-Based CF Algorithm [17] predicts missing ratings of target users by their nearest neighbors. Graph Reg ularized Weighted Nonnegative Matrix Factorization (GWNMTF) [4] encodes the neighborhood information into latent factor models, which can preserve the similarity between user tastes or item features. However, the se methods depend largely on dense rat-ings to compute accurate neighborhood structure, which is impossible when the data are extremely sparse. Different from these methods, our approachextractsthe latent factors of users/items from some dense auxiliary data, and then constructs the neighborhood structure from these late nt factors. In this way, the constructed neighborhood structure is more accurate for sparse CF. We focus on sparse collaborative filtering where the targ et data are extremely sparse. Suppose in the target data, we have m users and n items, and a set of integer ratings ranging from 1 to 5. R  X  R m  X  n is the rating matrix, where R ij is the rating that user i gives to item j . Z  X  X  0 , 1 } is the indicator matrix, Z ij =1 if user i has rated item j ,and Z ij = 0 otherwise. In order to reduce the sparsity in the target data, we assume that there exist some dense auxiliary data, from which we can transfer some common knowledge to the target data. Specifically, we will consider two sets of auxiliary data, whose rating matrices are R 1 and R 2 , with indicator matrices Z 1 and Z 2 , respectively. R 1 shares the common set of users with R , while R 2 shares the common set of items with R . For clarity, the notations and their descriptions are summarized in Table 1.
 Definition 1 (Learning Goal). The goal of TBT is to construct 1) latent fac-tors U 0 , V 0 from R 1 , R 2 and 2) similarity graphs L U , L V from U 0 , V 0 (Figure 1), and use them as the twin bridge to predict missing values in R (Figure 2). In this section, we present our proposed TBT approach, which is comprised of two sequential steps: 1) twin bridge construction, and 2) twin bridge transfer. 4.1 Twin Bridge Construction In the first step, we aim to construct the latent factors and similarity graphs of users and items, which will be used as the twin bridge for knowledge transfer. Latent Factor Extraction. We adopt the graph regularized weighted nonneg-ative matrix tri-factorization (GWNMTF)methodproposedbyGuetal.[4]to extract the latent factors from the two sets of dense auxiliary data R 1 and R 2 where  X  U and  X  V are graph regularization parameters, L U  X  , L V  X  are the graph Laplacian matrices,  X  is the Frobenius norm of matrix. U  X  =[ u  X   X  1 ,...,u  X   X  k ]  X  R m  X  k are the k latent tastes of users , with each u  X  codebook representing the association between U  X  and V  X  ,  X   X  X  1 , 2 } . Since auxiliary data R 1 share the common set of us ers with the target data R , we can share the latent tastes of users between them, that is, U 0 = U 1 , where U 0 denotes the common latent tastes of users. Similarly, since auxiliary data R 2 share the common set of it ems with the target data R , we can share the latent features of items between them, that is, V 0 = V 2 ,where V 0 denotes the common latent features of items. U 0 and V 0 are the common latent factors , which will be used as the first type of bridge for knowledge transfer.
It is worth noting that, the latent fac tors extracted by GWNMTF can respect the intrinsic neighborhood structure better than those extracted by Sparse Sin-gular Value Decomposition (SVD) [12], thus can fit sparse target data better. Similarity Graph Construction. Collaboration information between users or items contained in the rating matrix has been widely explored for CF. However, in extremely sparse target data, two users may rate the same item with low probability though they have the same taste. To avoid this data sparsity issue, we construct the similarity graphs from dense auxiliary data since they share common users or items with target data. Because U 0 and V 0 are denser than R 1 and R 2 and can better respect the collaboration information underlying the target data, we propose to construct similarity graphs of users and items from U 0 and V 0 , respectively. We define the distanc e between users/items as follows: symmetric matrices W U and W V are the weight matrices for the similarity graphs , which will be used as the second type of bridge for knowledge transfer. 4.2 Twin Bridge Transfer In the second step, we introduce the objectives for latent factor transfer and similarity graph transfer respectivel y, and then integrate them into a unified optimization problem for twin bridge transfer learning.
 Latent Factor Transfer. Since the latent factors of users/items may not be shared as a whole between domains, we propose to transfer latent factors U 0 , V 0 to the target data via two regularization terms U  X  U 0 2 F and V  X  V 0 2 F , instead of enforcing U  X  U 0 , V  X  V 0 . This leads to the latent factor transfer where  X  U , X  V are regularization parameters indicating our confidence on the latent factors. Since the target data are extremely sparse, the latent factors U 0 , V 0 may transfer some negative information to the target data. Therefore, we introduce similarity graph transfer as a complementary method in the sequel. Similarity Graph Transfer. Basedonthe manifold assumption [18], similar users/items should have similar latent tastes/features. Therefore, preserving the neighborhood structure underlying the users/items in the target data are reduced to the following graph regularizers encoding the constructed similarity graphs G G where D U = diag j ( W U ) ij , D V = diag j ( W V ) ij . L U = D U  X  W U and L V = D V  X  W V are the graph Laplacian matrices for the similarity graphs.
Incorporating the graph regularizers into the objective function of nonnegative matrix tri-factorization, we obtain the following similarity graph transfer where  X  U , X  V are regularization parameters indicating our confidence on the similarity graphs. With similarity graph transfer, we can essentially filter out the negative information in the latent factors by using the nearest neighbor rule. However, since the target data are extremely sparse, only transferring knowledge from the similarity graphs may be insufficient. Thus we seek an integrated model. Twin Bridge Transfer. Considering the aforementioned limitations, we in-tegrate the latent factor transfer and similarity graph transfer into a unified twin bridge transfer (TBT) method. In TBT, firstly, we extract the latent fac-tors of users/items and construct the similarity graphs from them. Secondly, we transfer latent factors as the first bridge to solve the insufficient transfer issue. As mentioned before, these latent factors may contain negative information for the target data and result in the negative transfer issue. So thirdly, we transfer the similarity graphs as the second bridge to alleviate negative transfer and to transfer more knowledge to the target data. It is worth noting that, each of the twin bridge can enhance the learning of the other bridge during the optimiza-tion procedure due to their complementary property. Therefore, we incorporate Equations (3) and (5) into a unified Twin Bridge Transfer optimization problem where  X  U , X  V , X  U , X  V are regularization parameters. If  X  U , X  V &gt; X  U , X  V ,the transferred latent factors dominate the recommendation results; otherwise, the transferred similarly graphs dominate the recommendation results. With TBT, we can transfer more knowledge from dense auxiliary data to sparse target data to reduce the data sparsity, and substantially avoid the negative transfer issue. 4.3 Learning Algorithm We solve the optimization problem in Equation (6) using the gradient descent method. The derivative of O with respect to U (the other variables are fixed) is Algorithm 1. TBT: Twin Bridge Transfer Learning for Collaborative Filtering Using Karush-Kuhn-Tucker (KKT) condition [19] for nonnegativity of U ,weget Since L U may take mixed signs, we replace it with L U = L + U  X  L  X  U ,where L + U = ( | L Likewise, we derive V and B in similar way and get the following update rules According to [4], updating U , V and B sequentially by Equations (7)  X  (9) will monotonically d ecrease the objective function in Eq uation (6) until convergence.
The learning algorithm for TBT is summarized in Algorithm 1. The time complexity is O ( maxIt  X  kmn + mn 2 + m 2 n ), which is the sum of TBT cost plus the time cost for the latent factor extraction and similarity graphs construction. In this section, we conduct experiments to compare TBT with state-of-the-art collaborative filtering methods on benchmark data sets with high sparsity level. 5.1 Data Sets MovieLens10M 1 MovieLens10M consists of 10,000,054 ratings and 95,580 tags given by 71,567 users to 10,681 movies. The preference of the user for a movie is rated from 1 to 5 and 0 indicates that the movie is not rated by any user.
Epinions 2 In epinions.com , users can assign products with integer ratings ranging from 1 to 5. The dataset used in ou r experiments is collected by crawling the epinions.com website during March, 2012. It consists of 3,324 users who have rated a total of 4,156 items, and the total number of ratings is 138,133.
The data sets used in the experiments are constructed using the same strategy as [12]. For the MovieLens10M data set, we first randomly sample a 10 4  X  10 4 dense rating matrix X from the MovieLens data. Then we take the submatrices as the user-side auxiliary data and R 2 = X 5001  X  10000 , 1  X  5000 as the item-side auxiliary data. In this way, R and R 1 share common users but not common items, while R and R 2 share common items but not common users. We use the same construction strategy for the Epinions data. In all experiments, the target ratings R are randomly split into a training set and a test set. To investigate the performance of each method on sparse data, we sampled training set randomly with a variety of sparsity levels from 0.01% to 1%, as summarized in Table 2. 5.2 Evaluation Metrics We adopt two evaluation metrics, Mean Absolute Error (MAE) and Root Mean Square Error (RMSE), which are widely used for evaluating collaborative filter-ing algorithms [4, 12]. MAE and RMSE are defined as Where R ij is the rating that user i gives to item j in test set, while -R ij is the predicted value of R ij by CF algorithms, | T E | is the number of ratings in test set. We run each method 10 repeated trial s with randomly generated training set and test set from R , and report the average MAE and RMSE of each method. 5.3 Baseline Methods &amp; Parameter Settings We compare TBT with Probabilistic Matrix Factorization (PMF) [20], Weight Nonnegative Matrix Factorization (WNMF) [21], Graph Regularized Weighted Nonnegative Matrix Tri-Factorization (GWNMTF) [4], Coordinate System Trans-fer (CST) [12], our proposed TBT UV (defined in Equation (3)) and TBT G (defined in Equation (5)). Different numbers of latent factors { 5, 10, 15, 20 } and different values of regularization parameters { 0.01, 0.1, 1, 10,100 } of each method are tried, with the best ones selected for comparison. Since all the algorithms are iterative ones, we run each of them 1000 iterations and report the best results. 5.4 Experimental Results The experimental results on MovieLens10M and Epinions are shown in Table 3 and Table 4 respectively. From the tables, we can make several observations.  X  TBT performs better than all baseline methods under all sparsity levels. It confirms that simultaneously transferring latent factors and similarity graphs as twin bridge can successfully reduce the sparsity in the target data.  X  It is interesting to see that, for our method, the sparser the target data are, the more improvement can be achieved. This can be explained by two reasons. First, when the rating data are sparse, matrix factorization methods that rely on neighborhood information is ineffective due to: 1) it is impossible to compute the similarity between users/items when dat a are extremely sparse, and 2) even if some similarity information is calculated, this information may be too limited to recommend correct items. So its prediction performance gets worse when rating data are coming more spare. To handle this problem, we construct a twin bridge, which can transfer more knowledge from dense auxiliary data to the target data.
Secondly, towards the negative transfer issue, CST encounters the problem that the transferred latent factors extracted from the auxiliary data may con-tain some negative information for the target data. Our twin bridge transfer mechanism alleviates this problem by using similarity graphs as regularization when training the recommendation model. Notably, the graph regularization can filter out the negative information introduced by the transferred latent factors. With this advantage, TBT is made more robust to the extremely sparse data.  X  Although TBT is a combination of our proposed TBT outperforms both of them. This proves that the twin bridge transfer can reinforce the learning of both bridges. Each bridge emphasizes different properties of the data and enriches the matrix tri-factorization with complementary information.  X  Unfortunately, transfer learning method CST has not consistently out-performed non-transfer learning methods GWNMTF. CST takes advantage on moderately sparse data (0.1%  X  1%) by knowledge transfer. However, under ex-tremely sparsity levels (0.01%  X  0.1%), the latent factors transferred by CST may contain some negative information for the target data. In this case, GWNMTF performs better than CST, since it is not a ffected by the negative transfer issue. 5.5 Parameter Sensitivity We investigate the parameter sensitivity by varying the values of k ,  X  and  X  under different sparsity levels. As Figure 3 shows, under a given sparsity level, the more latent factors used, the better the performance is. Typically, we set k =20as used by baseline methods. For  X  and  X  ,Weset  X  =  X  in TBT for easy comparison with baselines, and report average MAE in Figure 4. Each column corresponds to a specific sparsity level of training ratings while each row corresponds to a different dataset. We observe that, 1) TBT method performs much more stably under different parameter values and sparsity levels than the baseline methods, and 2) neither non-transfer method GWNMTF or single bridge transfer methods CST/TBT G can perform stably under different parameter values, sparsity levels, or data sets. Luckily, our TBT benefits from its twin bridge mechanism and performs much more robustly under all of these experimentation settings. In this paper, we proposed a novel approach, Twin Bridge Transfer Learning (TBT), to reduce the sparsity in the target data. TBT consists of two sequential steps: 1) TBT extracts latent factors of users and items from dense auxiliary data and constructs similarity graphs from them; 2) TBT utilizes the latent factors and similarity graphs as twin bridge to transfer knowledge from dense auxiliary data to sparse target data. By using the twin bridge, TBT can enhance sufficient transfer by transferring more knowledge, while alleviate negative transfer by regularizing the learning model with the similarity graphs, which can naturally filter out the negative information contained in the latent factors. Experiments on real-world data sets validate the effectiveness of the proposed TBT approach. Acknowledgement. The work is supported by the National HGJ Key Project (No. 2010ZX01042-002-002-01), the 973 Program of China (No. 2009CB320706), the National Natural Science Foundation of China (No. 61050010, No. 61073005, No. 60972096).

