 The  X  X ag of Visual Words X  (BoW) framework has been widely used in query-by-example video retrieval to model the visual content by a set of quantized local feature descriptors. In this paper, we propose a novel technique to enhance BoW by the selection of Word-of-Interest (Wo I) that utilizes the quantified temporal motion coherence of the visual words between the adjacent frames in the query example. Experiments carried out using TRECVID datasets show that our technique improves the retrieval performance of the classical BoW-based approach. H.3.3 [ Information Storage and Retrieval ]: Information search and Retrieval  X  Multimedia Information Retrieval Algorithms, Performance, Theory Bag of visual Words, Words-of-Interest, Video Retrieval, Temporal Motion Coherence 
Content-based video retrieval is an important and challenging task in multimedia information retrieval. We are especially interested in the case of query-by-example, i.e. copyright infringement detection, to find videos relevant to an example video issued by the user. 
Inspired by the success in language modeling and text retrieval, visual content representation based on Bag of Visual Words (BoW) has been wildly investigated and has shown some impressive results in content-ba sed video retrieval [2], image classification and object recognition. The BoW model is based on independently, and the spatial-te mporal relationships between the visual words are ignored to achieve better computational and representational efficiency. In the context of video understanding and retrieval, however, a single visual word contains only small amount of information, and the temporal relatio nships among the different words are sometimes critical to represent the visual information on how the objects behave. 
Recently, several techniques have been proposed to address these limitations. Cao et al. [1] proposed a Spatial Coherence Latent Topic Model (Spatial-LTM) that groups visual words with respect to latent topics in orde r to improve the representative power. Liu and Chen [6] argued that spatial and temporal information could be used to extract the object of interest for video retrieval. 
In this paper, we aim to discove r the temporal relations between visual words and explore the possibility to enhance the BoW representation for video retrieval. Inspired by the method proposed by Wang et al. [5] which classifies the relative motion of visual words to represent the temporal patterns in a video, we propose to utilize relative motion to model the temporal relation of visual words. 
According to [3], a subset of visual words is more descriptive for certain catalogue of objects th an others. In context of query-by-example video retrieval, we believe that some selected visual words, called Words-of-Interest (W oI), are more related to the user X  X  interest than others. The WoI should be given higher weights for similarity match be tween query and video data. 
In this paper, we hypothesize that the WoI would appear and move in a relatively coherent manner in a video, while non-WoI occurs more singularly and randomly. Here, the temporal motion coherence can be defined as the degree to which a visual word moves coherently with other words on the temporally aligned frames in the query video. 
The reminder of this paper is orga nized as follows. In section 2, our temporal motion coherence ba sed WoI generation algorithms and the application in video retrieval are presented. The experimental results are discussed in Section 3. Finally, Section 4 concludes the paper and highlights some future research directions. 
In the BoW model, a number of local feature descriptors are automatically extracted, e.g., us ing the Speed Up Robust Feature algorithm (SURF) [4]. A visual vocabulary  X  X   X  Voc e.g., by the K-means algorithm. Then a video  X  is represented as a where each  X   X  is a weighted vector of visual words, and the weight of each visual word is often its Term Frequency (TF) in the frame. Similarly, a given query example is also represented as a set of level similarity match. In this section, the temporal motion coherence defined in Section 1 is quantified by the relative motion between the visual words pair in the given query. 
The motion of instances of a visual word can be extracted between every two neighboring frames by an algorithm based on L norm [8] in the next frame. Specifically, for each instance  X  of a visual word  X  that can be successfully tracked, its motion  X   X   X  X  X  X   X   X ,  X   X  is calculated as a vector, where  X  identify the vertical and horizontal motion respectively. Each instance  X  is associated with a motion vector  X  word Voc X  X  . 
To evaluate how coherently a pair of visual words moves in neighboring frames, we calculate a relative motion vector for them. The relative motion vector  X  X   X , X  of the visual words pair  X  and  X   X  in the query video is formulated as follows: where  X   X , X   X  and  X   X , X   X  are the motion vectors of the k of  X   X  and the n th instance of  X   X  , and N i and N of instances of  X   X  and  X   X  respectively. 
The smaller the relative motion vector, the higher the degree of motion coherence of the visual words pair. In order to quantify the temporal coherence of a visual word, a simplification is proposed. The relative motion vectors of a visual word with all other words are averaged as: where  X  X  X   X   X  is the conditional probability that  X  query  X  ,  X  X   X  is the term frequency of the visual word  X  the number of instances of the visual words in  X  . 
Based on our hypothesis in Section 1, the WoI with high motion coherence can be select ed when the corresponding r lower than an empirical threshold. However, a pre-fixed empirical threshold may not be suitable fo r every query. To tackle this problem, the EM algorithm [7] is used to classify the visual words based on temporal motion coherence adaptively. 
More formally, each visual word  X   X  is associated with a hidden variable  X  X   X   X   X   X .  X   X  . Here,  X   X  denotes that  X  denotes that  X   X  is out of user interest. Naturally,  X   X   X  the probability of a visual word belonging to WoI. The motion definitions, the joint distribution of  X  X , X  X  X  is defined as  X   X   X , X   X   X   X  X  X  X  X  X   X   X  |  X   X  , and we simplify the problem by assuming  X  and  X  are independent variables. All distributions are unknown yet, and the parameters should be estimated using the EM algorithm. 
The steps of the EM algorithm for estimating the unknown distribution is given as follows: where  X  is a set of parameters to be estimated, c temporal motion coherence space. We choose the visual words which are located within the standard deviation of the Gaussian distribution  X   X   X  |  X   X   X  as WoI:  X  X   X   X  X  X   X   X  ,  X  X  X  X 
The frame level similarity based on the WoI is measured by the distance: where  X   X   X  represents the term frequency of WoI in the key frame of the video c and  X   X   X  represents the key frame of the query, and  X  is an empirically sele cted weighting factor. 
Finally, the videos with higher number of key frames similar to the key frames from the query are ranked higher in the results. 
To evaluate our temporal motio n coherence based method, we select a dataset from TRECVI D 2002, which consists of 3000 videos and 6 query topics. The evaluation is based on common criteria used in the information retrieval community: Precision, Recall and Mean Average Precision (MAP). As shown in Figure 1, the WoI enhanced BoW model (denoted WoI) outperforms the classical BoW model for the query-by-example video retrieval task. Most points of the WoI curve are above the curve of classical BoW. 
Moreover, for 5 out of 6 t opics, the WoI based method outperforms the classical BoW in term of Average Precision. The MAP (over all the 6 test topics) of WoI outperforms BoW by 2.3%, which is statistically significant ( P-value = 0.02188). 
This result demonstrates the feas ibility of modeling the interest of a user by temporal motion coherence. We believe further improvement could be achieved by better quantification methods, as currently the approach in Se ction 2.1 may have over-simplified the motion coherence of visual words. 
In this paper, we propose a technique for WoI selection based on the quantified temporal motion coherence in the query video example. The experimental results show that the selection is effective and the generated WoI could improve query-by-example video retrieval based on classical BoW model. 
Our future work will be focused on more descriptive user interest representation model which will incorporate spatial-temporal information. [1] L. Cao and L. Fei-Fei.  X  X patiall y coherent late nt topic model [2] J. Sivic and A. Zisserman, "Efficient visual search for objects [3] S. Zhang, Q. Tian, G. Hua, Q. Huang, S. Li, 'Descriptive [4] H. Bay, A. Ess, T. Tuytelaars , L. Van Gool, "SURF: Speeded [5] F. Wang, Y. Jiang, and C. Ngo.  X  X ideo event detection using [6] D. Liu and T. Chen.  X  X ideo retrieval based on object [7] A. P. Dempster , N. M. Laird , D. B. Rubin.  X  X aximum [8] B. D. Lucas and T. Kanade,  X  X n Interative Image 
