 Existing data cleaning metho ds w ork on the basis of com-puting the degree of similarit y bet w een nearb y records in a sorted database. High recall is ac hiev ed b y accepting records with lo w degrees of similarit y as duplicates, at the cost of lo w er precision. High precision is ac hiev ed analogously at In this pap er, w e prop ose a generic kno wledge-based frame-w ork for e ectiv e data cleaning that implemen ts existing cleaning strategies and more. W e dev elop a new metho d to compute transitiv e closure under uncertain t y whic h handles the merging of groups of inexact duplicate records. Exp eri-men tal results sho w that this framew ork can iden tify dupli-cates and anomalies with high recall and precision. The amoun t of data organizations are handling to da y is increasing at an explosiv e rate. The task of k eeping the data in these large databases consisten t and correct is in-surmoun table. There are man y causes to dirt y data: misuse of abbreviations, data en try mistak es, con trol information hiding, missing elds, sp elling, outdated co des etc. Due to the `garbage in, garbage out' principle, dirt y data will dis-tort information obtained from it. Clean data is critical for man y industries o v er a wide v ariet y of applications [5]. Researc h has sho wn the imp ortance of domain kno wledge in data cleaning. Ho w ev er, there has been little w ork on kno wledge managemen t issues for data cleaning. What t yp e of kno wledge is suitable? Ho w can w e represen t the domain kno wledge in a form that w e can use for data cleaning? Ho w do w e manage the kno wledge? Data cleaners using existing call is ac hiev ed b y relaxing the criteria for records to be considered duplicates. This leads to lo w er precision as more records whic h are not duplicates are classi ed as suc h. Anal-ogously , w e can ha v e higher precision, but at the cost of lo w er recall. This pap er addresses the main shortcomings of existing metho ds.
 The rest of the pap er is organized as follo ws. Section 2 giv es a conceptual mo del and benc hmarking metrics for data cleaning. Section 3 surv eys related w orks. Section 4 de-scrib es our prop osed kno wledge-based framew ork. Section 5 giv es the p erformance results and w e conclude in Section 6. Figure 1 sho ws a high lev el conceptual data cleaning mo del. W e start with a \dirt y" dataset with a v ariet y of errors and anomalies. Cleaning strategies are applied to the dataset with the ob jectiv e of obtaining consisten t and correct data as the output. The e ectiv eness of the cleaning strategies will th us be the degree b y whic h data qualit y is impro v ed through cleaning. W e de ne t w o metrics that b enc hmark the e ectiv eness of data cleaning strategies : 1. Recall. This is also kno wn as p er c entage hits . It 2. F alse-P ositiv e Error. This is the an tithesis of the Pre-pro cessing dirt y data prior to the data cleaning pro cess leads to more consisten t data and b etter de-duplication. [6] prop ose that record equiv alence can be determined b y view-ing their similarit y at three lev els: tok en, eld and record lev els. External source les are used to remo v e t yp ographi-cal errors and inconsisten t use of abbreviations.
 The most reliable w a y to detect inexact duplicates is to compare ev ery record with ev ery other record. The Sorte d
Permission to make digital or hard copies of part or all of this work or permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 plexit y b y sorting the database on an application-sp eci c k ey and making pairwise comparisons of nearb y records b y sliding a windo w of xed size o v er the sorted database. If the windo w size is w , then ev ery new record en tering the windo w is matc hed with the previous w 1 records. The rst record then mo v es out of the windo w. This metho d requires only wN comparisons, but its e ectiv eness dep ends hea vily on the abilit y of the c hosen k ey to bring the inexact (DE-SNM) [4] impro v es on the SNM b y pro cessing records with exact duplicate k eys rst. But the dra wbac k of the SNM still p ersists. The clustering metho d a v oids sorting the database b y partitioning the database in to indep enden t clusters of appro ximately duplicate records [4]. Ho w ev er, this will result in man y singleton clusters as the prop ortion of duplicates in a database is t ypically small.
 Multi-pass algorithms assumes that no single k ey is unique enough to bring all inexact duplicates together and emplo ys the SNM cycle sev eral times, eac h time using a di eren t k ey to sort the database to reduce the c hances of missed duplicates. These algorithms also compute the transitiv e closure o v er the iden ti ed duplicates [4, 8], that is, if A is equiv alen t to B, and B is equiv alen t to C, then A is also equiv alen t to C under the assumption of transitivit y . In this w a y , A and C can be detected as duplicates without b eing in the same windo w during an y of the SNM runs. While this can increase the recall, it is also lik ely to lo w er the precision. The Incremen tal Merge/Purge Pro cedure [9] use \represen-tativ e records" to a v oid re-pro cessing data when incremen ts of data need to be merged with previously pro cessed data. The main dicult y with this metho d is the c hoice of repre-sen tativ e records whic h c haracterize the database. [8] giv es a domain-indep enden t tec hnique to detect appro ximate du-plicate records whic h is applicable to textual databases. Although domain kno wledge has been iden ti ed as one of the main ingredien ts for successful data cleaning [7], the issue of kno wledge represen tation to supp ort data cleaning strate-gies has not been w ell dealt with. In this section, w e presen t a kno wledge-based framew ork for in telligen t data cleaning. This framew ork pro vides a systematic approac h for repre-sen tation standardization, duplicate elimination, anomaly detection and remo v al in dirt y databases. The framew ork (see Figure 2) consists of three stages: Figure 2: A Kno wledge-Based Cleaning F ramew ork 1. Pre-pro cessing Stage.
 2. Pro cessing Stage.
 3. V alidation and V eri cation Stage.
 The Rete Algorithm [1] is an ecien t metho d for comparing a large collection of patterns to a large collection of ob-jects. A basic pro duction system c hec ks eac h if-then state-men t to see whic h ones should be executed based on the facts in the database, lo oping bac k to the rst rule when it has nished. The computational complexit y is of the order O ( RF P ), where R is the n um ber of rules, P is the a v erage n um ber of patterns in the condition part of the rules, and F is the n um ber of facts in the kno wledge base. This escalates dramatically as the n um ber of patterns per rule increases. The Rete Algorithm a v oids unnecessary recomputation b y remem bering what has already b een matc hed from cycle to cycle and then computing only the c hanges necessary for the newly added or newly remo v ed facts [3]. As a result, the computational complexit y per iteration drops to O ( RFP ), or linear in the size of the fact base [2]. Sa ving the state of the system ma y consume considerable memory , but this trade-o for sp eed is often w orth while.
 In the exp ert system engine of the prop osed framew ork, data records are the facts. Rules are activ ated when records matc h the \patterns" sp eci ed in the rules. The actions in the consequen t part of the rule will be executed. Rules, whic h form the kno wledge-base of the framew ork, are written in the Ja v a Exp ert System Shell (JESS) [2] language. A rule will generally be of the form: The action part of the rule will be activ ated or r e d when the conditions are satis ed. W e note here that complex predi-cates and external function references ma y be con tained in both the condition and action parts of the rule. The rules are deriv ed naturally from the business domain. The busi-ness analyst with sub ject matter kno wledge is able to fully understand the go v erning business logic and can dev elop the appropriate conditions and actions.
 The complexit y of the rule language is an imp ortan t issue in suc h applications. Although the declarativ e st yle of the JESS language mak es it in tuitiv e, some kno wledge of the w orking of the Rete Algorithm is required for optimizing rule eciency . Note that rules can be re-used without mo d-i cation b y di eren t applications if they p ossess similar busi-ness rules. This mak es domain-sp eci c rule pac k ages feasi-ble. Giv en the structure of the rule language, simple rules ma y be generated automatically when supplied with neces-sary parameters though rule optimization for more complex rules migh t require hand-co ding.
 W ell-dev elop ed rules are e ectiv e in iden tifying true du-plicates but are strict enough to k eep out similar records whic h are not duplicates. Higher recall can then be ac hiev ed with more rules. Hence, an increase in the n um ber of w ell-dev elop ed rules can ac hiev e higher recall without sacri cing precision, thereb y resolving the recall-precision dilemma. The rationale for computing the transitiv e closure after run-ning m ulti-pass data cleaning algorithms has b een discussed in Section 3. Ho w ev er, w e note that this pro cedure can raise the false-p ositiv e error as incorrect pairs are merged due to the fact that w e are dealing with inexact e quivalenc e Supp ose w e ha v e t w o groups of v ery similar \duplicate" w ords: (F A THER,MA THER) and (MA THER,MOTHER).
 The transitiv e closure step will then merge the t w o groups in to a single group of (F A THER,MA THER,MOTHER), whic h is ob viously wrong since \F A THER" and \MOTHER" are t w o di eren t w ords. This problem will magnify as more groups are merged in to a single group due to di eren t com-mon records linking v arious pairs. In this case, the rst pair will probably be v ery di eren t from the last pair. W e can reduce the n um ber of wrongly merged duplicate groups b y mo difying the structure of Duplicate Iden ti ca-tion Rules to the form: where 0 &lt; cf 1. cf represen ts our con dence in the rule's e ectiv eness in iden tifying true duplicates. W e can assign high certain t y factors to rules w e are con den t of b eing able to iden tify true duplicates. During the computation of the transitiv e closure, w e compare the pro duct of the certain t y factors of the merge groups against a user-de ned thresh-old. This threshold represen ts ho w con den t w e w an t the merges to be. An y merges that will result in a certain t y factor less than the threshold will not be carried out. This is b ecause as more groups are merged during the transitiv e closure step, w e are less con den t that all the records in the resultan t group are represen tations of the same real-w orld en tit y . W e presen t 2 examples here for clarit y . CF and TH represen t the certain t y factor and threshold resp ectiv ely . Example 1. Merge Records (A B) with CF =0.8, TH =0.7. Records A and B will be merged as CF &gt; TH .
 Example 2. Merge Records (A B) with CF =0.9, (B C) with CF =0.85, (C D) with CF =0.8, TH =0.5.
 The groups (A B) and (B C) will be considered rst, as these groups ha v e higher CF s. They will be merged to form (A B C) with CF = 0.765 (since 0 : 9 0 : 85 = 0 : 765). Then, this group will be merged with (C D) to form (A B C D) with CF =0.612 (since 0 : 765 0 : 8 = 0 : 612), and the CF is still greater than TH . Ho w ev er, if TH = 0.7, (A B C) and (C D) will remain separate, as the resultan t CF of the merged group (0.612) will be less than TH . Based on the ideas and tec hniques presen ted in this pap er, w e implemen ted the In telliClean system using Ja v a 1.2 on a P en tium 200 MMX system with 64 MB RAM running Windo ws NT 4.0. Calls to the Oracle 8 database system are made using JDBC (Ja v a Database Connectivit y). The database serv er is a SUN En terprise 450 serv er lo cated in a remote mac hine connected via a lo cal area net w ork. W e tested In telliClean using t w o real w orld datasets : a Compan y dataset and a P atien t dataset. The former re-quires complex matc hing logic and data manipulation, while the latter is a m uc h larger dataset con taining man y errors. The compan y dataset has 856 records. Eac h record has 7 elds: Compan y Co de, Compan y Name, First Address, Sec-ond Address, Currency Used, T elephone Num ber, and F ax Num ber. Human insp ection rev eals 60 duplicate records. Problems encoun tered include large n um ber of empt y elds, incomplete data, t yp ographical errors, di eren t represen ta-tions for names and addresses, misuse of data elds and v ery similar represen tations for di eren t en tities. 1. Pre-pro cessing. The t w o address elds are joined in to 2. Pro cessing. Runs of SNM are conducted with v ary-3. V alidation and V eri cation. The results are v eri ed The results sho w that recall increases with the n um ber of rules, and more complex rules iden ti ed more true dupli-cates. The run time of the system dep ends on the complexit y of the rules rather than the n um ber of rules. W e p ostulate that an increase in the n um ber of e ectiv e rules giv en addi-tional business kno wledge go v erning this dataset can lead to a further rise in recall while main taining the false-p ositiv e error at lo w lev els.
 The windo w size do es not ha v e m uc h e ect on recall, unless the windo w size is comparable to the size of the dataset. This is b ecause SNM's e ectiv eness dep ends on inexact du-plicates b eing close to one another after sorting on a c hosen k ey , and that their pro ximit y dep ends only on the rst few critic al char acters of the k ey . Consider a simple case of a n -c haracter k ey string and that t w o records will nev er be in the same windo w during the SNM windo w scan if an y of the rst m c haracters are \far apart". If there is a single error in one of the k eys, and the error is equally lik ely to be at an y c haracter of the string, then the probabilit y of the c haracter being one of the rst few critical c haracters is m=n . In the case of a 100-c haracter k ey and the critical c haracters are the rst 3, the probabilit y of t w o duplicates not being in a same windo w due to an error in the critical c haracters is 0.03. Hence, ev en if erroneous data is in the k ey , the records are still lik ely to app ear in a same windo w during the SNM scanning pro cess. In our example ab o v e, the probabilit y that the 2 duplicate records b eing detected is 97%. This estimate will be ev en higher if w e use m ultiple passes of the SNM with eac h pass emplo ying a di eren t k ey . Figure 4 sho ws the results of our exp erimen ts when w e v ary the windo w size and n um ber of passes. Note that recall do es not increase m uc h as windo w size increases from 5 to 30. The patien t dataset con tains 22122 records. This database has 60 elds, including the NRIC Num ber 1 , Sex, Date of Birth, Date of Screening, the Clinic Num ber and v arious elds con taining co des for the results of a diab etes screen test.
 Three rules w ere used for this dataset : 2 duplicate iden ti -cation rules and 1 merge/purge rule: 1. Duplicate Id. Rule 1. Certain t y F actor 1.
 2. Duplicate Id. Rule 2. Certain t y F actor 0.9.
 3. Merge/Purge Rule. F or all duplicate record groups 1 The Singap ore National Registration Iden ti cation Card (NRIC) Num ber is the lo cal equiv alen t of the So cial Securit y Num ber of the United States. Figure 4: The E ect of Windo w Size on Recall T able 1: Results of Pro cessing the P atien t dataset A total of 129 alerts, including c hec k digit errors, format and domain v alue violations, w ere raised during the pre-pro cessing stage whic h to ok 118.5 seconds. The results of the pro cessing stage are sho wn in T able 1. Man ual insp ec-tion of the database rev ealed that w e ac hiev ed 100% accu-racy and precision for this exp erimen t. Although the records iden ti ed b y Rule 1 form a subset of those iden ti ed b y Rule 2, w e still include this rule as it iden ti es exact duplic ates whic h the Merge/Purge Rule can handle automatically . The 8 records (4 pairs) that w ere exact duplicates w ere automat-ically pro cessed in this manner. The rest w ere mark ed for man ual pro cessing. In this pap er, w e ha v e presen ted a generic kno wledge-based framew ork that can e ectiv ely p erform data cleaning on an y textual database. This framew ork pro vides for e-cien t represen tation and easy managemen t of kno wledge for data cleaning in an exp ert system. The recall-precision dilemma can also be resolv ed b y sp ecifying e ectiv e rules. W e ha v e also outlined ho w the in tro duction of uncertain t y in to the computation of transitiv e closure can increase pre-cision. Consisting of three stages, this framew ork can sup-port a wide v ariet y of cleaning strategies, of whic h existing metho ds form a subset. W e are curren tly extending this framew ork to de-duplicate results returned b y w eb searc h engines. [1] C. F orgy . Rete: A fast algorithm for the man y [2] E. J. F riedman-Hill. Jess, the ja v a exp ert system shell, [4] M. Hernandez and S. Stolfo. The merge/purge problem [5] R. Kim ball. Dealing with dirt y data. DBMS Online , [6] M. L. Lee, H. Lu, T. W. Ling, and Y. T. Ko. Cleansing [7] A. Ma ydanc hik. Challenges of ecien t data cleansing. [8] A. E. Monge and C. P . Elk an. An ecien t [9] M. J. W aller. A comparison of t w o incremen tal
