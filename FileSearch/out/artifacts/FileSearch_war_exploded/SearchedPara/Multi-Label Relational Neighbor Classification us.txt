 Networked data, extracted from social media, web pages, and bib-liographic databases, can contain entities of multiple classes, in-terconnected through different types of links. In this paper, we focus on the problem of performing multi-label classification on networked data, where the instances in the network can be assigned multiple labels. In contrast to traditional content-only classification methods, relational learning succeeds in improving classification performance by leveraging the correlation of the labels between linked instances. However, instances in a network can be linked for various causal reasons, hence treating all links in a homogeneous way can limit the performance of relational classifiers.

In this paper, we propose a multi-label iterative relational neigh-bor classifier that employs social context features (SCRN). Our classifier incorporates a class propagation probability distribution obtained from instances X  social features, which are in turn extracted from the network topology. This class-propagation probability cap-tures the node X  X  intrinsic likelihood of belonging to each class, and serves as a prior weight for each class when aggregating the neigh-bors X  class labels in the collective inference procedure. Experi-ments on several real-world datasets demonstrate that our proposed classifier boosts classification performance over common bench-marks on networked multi-label data.
 H.2.8 [ Database Management ]: Database applications X  Data Min-ing ; J.4 [ Social and Behavior Sciences ]: Sociology Algorithm, Experimentation Relational learning; Collective classification; Social Dimensions
Recently, much attention has been paid to the problem of learn-ing from networked data, where instances are interconnected by implicit or explicit relationships [1, 5, 21]. Relational learning [6] can learn models of this data structure by utilizing the correlation between labels of linked objects; networks resulting from social processes often possess a high amount of homophily, such that nodes with similar labels are more likely to be connected [15]. Many of the algorithms developed for relational classification are heuristic methods that do not necessarily correspond to formal prob-abilistic semantics [17]. In other approaches, during the infer-ence process the probability distribution is structured as a graph-ical model based on the assumption that the structure of the net-work corresponds at least partially to the structure of the network of probabilistic dependencies [14]. Relational learning enhances the tractability of estimating the full joint probability distribution of the data by making a first-order Markov assumption that the la-bel of one node is dependent on that of its immediate neighbors in the graph. Collective inference in relational classification makes simultaneous statistical estimations of the unknown labels for in-terrelated entities, and finds a equilibrium state such that the incon-sistency between neighboring nodes is minimized. By exploiting network connectivity information, relational classification models have been shown to outperform traditional classifiers [18, 25].
The conventional relational classification model focuses on the single-label classification problem, which assumes that each in-stance is only associated with one label among a finite set of can-didate classes. However, in many real relational datasets, each in-stance is associated with multiple labels. For instance, in document networks, one document can describe multiple topics. In social net-works, people often belong to a large set of interest groups. Clas-sifying this type of dataset can be regarded as a multi-label clas-sification task. In previous work, edges in the network are treated homogeneously; the implicit assumption is that the edges are en-gendered from similar social processes. However, in multi-label relational datasets, connections between instances are driven by various casual reasons. In the familiar example of collaboration networks, scientific authors usually have multiple research inter-ests and seek to collaborate with different co-authors for different types of work. For instance, Author A cooperates with author B on publishing papers in machine learning conferences whereas his/her interaction with author C is mainly due to work in the data mining area. The heterogeneity in connection causality makes the classifi-cation problem more difficult.

Collective classification becomes particularly challenging in multi-label settings since the label dependencies among related instances are more complex. Currently, most collective inference models do not differentiate in their treatment of connections between in-stances; however, treating links in a homogeneous way may nega-tively affect the classification result [23]. The relational neighbor classifier ( RN ) [13] provides a simple yet effective way to solve single-label relational classification problems. In this paper, we present a multi-label relational classifier that accounts for this in-homogeneity in connections and is designed for classification prob-lems on multi-label networked datasets. Our proposed method, SCRN , extends RN by introducing a node class-propagation prob-ability that modulates the amount of propagation that occurs in a class specific way based on the node X  X  similarity to each class. Al-though the class propagation probability can be determined by the node X  X  intrinsic features, it can also be based on node X  X  social fea-tures. These features capture link patterns between a node and its neighbors and can be extracted from network topology in instances when the node lacks intrinsic features. SCRN  X  X  ability to differ-entiate between classes during the inference procedure allows it to outperform previous methods in several real-world multi-label re-lational datasets.

The multi-label collective classification problem that we address here is related to the within-network classification problem: entities whose labels are known are linked to entities for which the class has to be estimated. In this paper, we aim to simultaneously predict the label sets of a group of related instances within the same network. The multi-label networked dataset is represented as a graph G = ( V,E,C,L ) , where V = { v 1 ,v 2 ,...,v n } is a set of nodes, E is a set of edges that connect pairs of nodes. Let C = { c 1 ,c be the finite set of m possible classes that each node can possess. Given a node v i  X  V , its class label is represented by a binary label assignment to each node. l m i = 1 iff v i belongs to class c The set of nodes, V , is further divided into two disjoint parts: nodes with known class labels, V K , and V U , nodes whose labels need to be determined. L K = { L i | v i  X  V K } indicates the observed multi-label set assigned to V K . Our task is to use V K as the training data to infer the labels, L U , for nodes in V U .

In multi-label classification problems, a popular approach is to decompose the multi-label classification problem into multiple bi-nary classification problems (one for each class). Conventional multi-label classification approaches (e.g., the ones used on non-networked data), usually assume the instances are i.i.d., and that the inference for each instance is performed independently:
In this paper, we propose a multi-label relational classifier that models the correlations between inter-related instances in the net-work. We start by constructing a social feature space, an edge-based representation of social dimensions using the network topol-ogy to capture the node X  X  potential affiliations as described in [23]. A class-propagation probability is assigned to each node to describe its intrinsic correlation to each class. The class-propagation prob-ability is calculated from the similarity between the node X  X  social features and the class reference vector. The multi-label relational classifier estimates a node X  X  label set based on its neighbors X  class labels, the similarity between connected nodes, and its class prop-agation probability. SCRN iteratively classifies the labels of the unlabeled nodes until all the label predictions are fixed or the max-imum number of iterations is reached. In the next section, we de-scribe the basic idea behind relational neighbor classifiers before describing our proposed method.
The Relational Neighbor ( RN ) classifier proposed by [13], is a simple relational probabilistic model that makes predictions for a given node based solely on the class labels of its neighbors, with-out machine learning or additional features. RN estimates class-membership probabilities by assuming the existence of homophily in the dataset, entities connected to each other are similar and likely belong to the same class. Suppose each instance in the network only belongs to a single class c  X  C . Given v i  X  V U , the relational-neighbor classifier estimates P ( L i = c | v i ) , the class-membership probability of a node v i belonging to class c , as the (weighted) pro-portion of nodes in the neighborhood that belong to the same class. We define neighbors N i as the set of labeled nodes that are linked to v i . Thus: where Z = P v between node v i and v j and I ( . ) is an indicator variable.
Instead of making a hard labeling during the inference proce-dure, the weighted-vote relational neighbor classifier ( wvRN) ex-tends RN by tracking changes in the class membership probabili-ties. wvRN estimates P ( L i | v i ) as the (weighted) mean of the class membership probabilities of the entities in the neighborhood ( N where Z is the usual normalization factor.

In both RN and wvRN , entities whose class labels are unknown are either ignored or are assigned a prior probability, depending on the choice of the local classifier. Since only a small portion of the nodes in G have known labels, a collective inference procedure is needed to propagate the label information through the network to related instances, using either the RN classifier or wvRN classifier in its inner loops.

As shown in [13], both RN and wvRN perform surprisingly well on relational datasets, even compared to more complex models, such as the Probabilistic Relational Model and Relational Proba-bilistic Tree. wvRN assumes that each node only has one single label, and that the class labels of linked nodes are likely to be the same. How-ever, in multi-label relational networks, the existence of heteroge-neous relationships gives rise to nodes with neighbors from mul-tiple classes. The diversity of the connections indicates two con-nected nodes might only share a subset of labels. The inference procedure in the RN classifier and wvRN classifier treat all links homogeneously, and this may cause problems when propagating the label information across the network, especially when collec-tive inference originates from the overlapping nodes (nodes with multiple labels) in the network. A toy example with two class la-bels is shown in Figure 1. Imagine the case where all the nodes on the left-hand side of node 1 belong to group 1, while those on the right-hand side of node 1 are from group 2; node 1 serves as a bridge, weakly connecting both groups. If we commence inferring the label sets of all the other nodes using node 1 X  X  label informa-tion, without differentiating between the connections, the collective inference in RN classifier will expect all the nodes in the graph to have the same class label as node 1.

To address this problem, instead of uniformly aggregating the neighbor X  X  labels along each class like wvRN does, we propose to assign each node a class propagation probability distribution, which represents its likelihood of maintaining the neighbor X  X  class label set. A node will be more likely to share a class with neighbors that have a high class-propagation probability. Take the toy graph Figure 1: A simple example of a coauthorship network. The solid line represents the act of publishing a paper in a data min-ing conference and the dashed line represents the activity of col-laborating on a machine learning paper. To express the nodes using edge-based social features, each edge is first represented by a feature vector where nodes associated with the edge denote the features. For instance, here the edge  X 1-3 X  is represented as [1,0,1,0,0,0,0,0,0,0]. Then, the node X  X  social feature (SF) vec-tor is constructed based on edge cluster IDs. Suppose in this example the edges are partitioned into two edge clusters (rep-resented by the solid lines and dashed lines respectively), then the SFs for node 1 and 3 become [3,3] and [0,3] using the count aggregation operator. for example, when inferring the labels of node 2 from node 1, we want to keep its estimation of class 2 X  X  probability much higher than class 1 to make a more accurate prediction. Therefore, a node X  X  class-propagation probability can be regarded as its prior probabil-ity for each class. Learning the class-propagation probability dis-tribution is critical in order to achieve better discrimination during the inference procedure. Fortunately the structure of the network can be highly informative, and we capture this information through using the network topology to construct social features .
In the proposed method, we first extract the social features (SF) from the network topology using the edge clustering method de-scribed in Section 2.2. These social features capture the nodes X  in-volvement patterns in different potential affiliations, and the node X  X  class propagation probability can be constructed from the social features in the following way. An initial set of reference features for class c can be defined as the weighted sum of social feature vectors for nodes known to be in class c : where V K c = { v i | v i  X  V K } , which represents the nodes whose labels are known as class c .

Then node v i  X  X  class propagation probability for class c condi-tioned on its social features, P CP ( l c i | SF ( v i )) , can be calculated by the normalized vector similarity between SF ( v i ) and class c  X  X  reference feature vector, RV ( c ) : where sim ( a,b ) is any normalized vector similarity function (e.g., cosine or inner product).

Our proposed multi-label relational classifier then estimates the class-membership probability of node v i belonging to class c : based on the class labels of its neighbors, { L j | v weight between v i and its directed neighbors v j , w ( v Input: { G , V , E , C , L K }, Max_Iter
Output: L U for nodes in V U 1. Construct the social feature space using scalable K-means 2. Initialize the class reference vectors, RV , for each class 3. Calculate the class-propagation probability for each test 4. Repeat until # iterations &gt; Max_Iter or predictions converge conditional class propagation probability, P CP ( l c i | SF ( v multi-label relational classifier model is defined as follows: where Z is the normalization factor. Similar to the RN and wvRN classifiers, our multi-label relational classifier iteratively classifies the nodes in V U using the model defined in Equation 6 in its inner loop. Since the label predictions change in each iteration, the class reference feature vector is updated based on the feature vectors of nodes (both training and testing nodes) whose labels belong to class c in the current iteration. In this paper, we adopt the Relaxation Labeling (RL) approach [3, 27] in the collective inference frame-work. During each iteration, RL updates the prediction probability by taking account of the probability estimates from the previous iteration. The general update procedure for relaxation labeling is shown in Equation 7 [14]: where  X  (0) = k and  X  ( t +1) =  X  ( t )  X   X  . Both k and  X  are con-stants in the range 0 to 1; t is the iteration count and M notes the relational model. The inference procedure in SCRN ter-minates when it meets the stopping criteria; possible stopping cri-teria include the stability of all label predictions between iterations or reaching a fixed budget of iterations. A summary of the SCRN framework is shown in Table 1.
The notion of edge-based social dimensions was created to ad-dress the classification problem in networked data with multiple types of links. Connections in human networks are mainly affiliation-driven, and each connection can often be regarded as principally resulting from one affiliation. Hence, links (connections) possess a strong correlation with affiliation classes. Moreover, since each person usually has more than one connection, the involvements of potential groups related to one person X  X  edges can be utilized as a Figure 2: Visualization of edge clustering using a subset of DBLP with 95 instances. Edges are clustered into 10 groups, with each shown in a different color. representation for his/her true affiliations. Because this edge class information is not readily available in most social media datasets, an unsupervised clustering algorithm can be applied to partition the edges into disjoint sets such that each set represents one potential affiliation [23]. The edges of actors who are involved in multiple group affiliations are likely to be separated into different sets which in turn facilitates the multi-label classification task.

In this paper we construct the node X  X  social feature space using the scalable edge clustering method proposed in [23]. Specifically, we first represent each edge in a feature-based format, where each edge is characterized by its adjacent nodes, as shown in Figure 1. Based on the features of each edge, K-means clustering is used to separate the edges into groups. Each edge cluster represents a po-tential affiliation, and a node will be considered involved in one affiliation as long as any of its connections are assigned to that af-filiation. Since the edge feature data is very sparse, the clustering process can be accelerated wisely. In each iteration a small portion of relevant instances (edges) that share features with cluster cen-troids are identified, and only the similarity of the centroids with the relevant instance need to be recomputed. By using this pro-cedure introduced by [23], the clustering task can be completed within minutes even for networks with millions of nodes. Figure 2 shows a result of the edge clustering method on a small sample of DBLP dataset. Edges are clustered into 10 separate groups, and each edge group is marked in one color. As we can see, the edge clustering method is able to maintain the correlation between con-nected nodes; nodes and their neighbors usually share the same type of edge. Also, nodes with high degree are more likely to asso-ciate with different types of edges since they are usually involved in multiple affiliations.

After clustering the edges, we can easily construct the node X  X  social feature vector using aggregation operators such as count or proportion on edge cluster IDs. In [23], these social dimensions are constructed based on the existence of the node X  X  involvements in different edge clusters. Although aggregation operators are sim-ply different ways of representing the same information (the his-togram of edge cluster labels), alternate representations have been shown to impact classification accuracy based on the application domain [20].
We evaluate the classification performance of our proposed multi-label relational classifier on three real-world multi-label relational
Data DBLP IMDb YouTube # of nodes 8,865 11,476 15,000 # of links 12,989 323,892 136,218 # of categories 15 27 47 Network Density 3 . 3  X  10  X  4 4 . 7  X  10  X  3 1 . 2  X  10  X  3 Maximum Degree 86 290 14,999 Average Degree 3 55 9
Average Category 2.3 1.5 2.1 datasets, DBLP, IMDb, and YouTube, whose properties are sum-marized in Table 2.
The first real-world dataset we studied in this paper is extracted from the DBLP dataset. 1 The DBLP dataset provides bibliographic information for millions of computer science references. In this paper, we construct a weighted collaboration network for authors who have published at least 2 papers during the 2000 to 2010 time-frame. In this network, the author is represented by the node, and two authors are linked together if they have collaborated at least twice. The weight of the link is defined as the number of times these two authors have co-authored papers. Each author can have multiple research interests. For our dataset, we selected 15 repre-sentative conferences in 6 research areas. An author is interested in a research area if he/she has published a paper in any of the confer-ences listed under that area, and our classification task is to asso-ciate each author with the correct set of conferences. The selected conferences are listed below: The second dataset studied in this paper is IMDb. 2 The Internet Movie Database (IMDb) is an online database of information re-lated to movies, television programs, and video games, including information about directors, actors, and plots. Our classification task is to predict the movie X  X  genres based solely on the collab-oration network. Each movie can be assigned to a subset of 27 different candidate movie genres in the database such as  X  X rama",  X  X omedy",  X  X ocumentary" and  X  X ction". In our experiment, we extract movies and TV shows released between 2000 and 2010, and those directed by the same director are linked together. We only retain movies and TV programs with greater than 5 links.
The third dataset is extracted from YouTube, which is a popular website for sharing videos. Each user in YouTube can subscribe to different interest groups and add other users as his/her contacts. In this paper, we select a subset of data (15000 nodes) from the original YouTube dataset 3 in [23] using snowball sampling , and re-http://www.informatik.uni-trier.de/~ley/db/ http://www.imdb.com/interfaces http://www.public.asu.edu/~ltang9/social_ dimension.html tain 47 interest groups as our class label. Unlike DBLP and IMDb, YouTube is not a collaboration network and thus exhibits different network properties.
In this paper, we compare our proposed multi-label relational classifier to four related methods: EdgeCluster , wvRN , Prior and Random . A short description of these methods follows:  X  Edge (EdgeCluster) captures the node X  X  correlation to different classes by extracting social dimensions from network structure us-ing the edge clustering representation [23]. The edge-based social features are constructed using the count operator on the edge clus-ter IDs, and a linear SVM is used as the classifier. To achieve good performance with EdgeCluster, it is necessary to balance the sizes of the positive and negative training sets; this can be accomplished using resampling.  X  wvRN , weighted-vote Relational Neighbor Classifier [13], makes predictions based solely on the class labels of the given node X  X  linked neighbors; the node X  X  predicted class memberships are con-structed as the weighted mean of its neighbors. Our implementa-tion of wvRN uses the same relaxation labeling procedure as used in SCRN .  X  Prior generates a class membership estimate according to the fraction of instances in the labeled training data with the given class label. Thus, all nodes (regardless of network connectivity) share the same class estimates which are assigned to multi-label nodes in rank order.  X  Random generates class membership estimates randomly for each node in the network using neither network nor label informa-tion.

In our proposed method, the edge clustering method is initially adopted to construct the social features. We use cosine similarity while performing the clustering; the dimensionality of the edge-based social features is set to 1000 for DBLP and Youtube datasets and 10000 for the IMDb dataset; these parameters are selected be-cause they give the best results for EdgeCluster and therefore pro-vide the fairest comparison.

In SCRN , the class-propagation probability is calculated by the similarity between the node X  X  social feature and class reference features. We evaluated several similarity measures, including Co-sine , Inner Product and Generalized Histogram Intersection Ker-nel , and we observe that the Generalized Histogram Intersection Kernel (GHI) [2] outperforms the other measures in grouping sim-ilar instances and is therefore used in the rest of this paper.
Since our problem is essentially a multi-label classification task, we assume that the number of labels for the unlabeled nodes is already known (e.g., based on the output of a separate classifier) and assign the labels according to the top-ranking set of classes at the conclusion of the inference process. Such a scheme has been adopted for multi-label evaluation in social network datasets [23, 26]. In our work, we sample a small portion of nodes uniformly from the network as training instances. The fraction of the train-ing data ranges from 5% to 30% for DBLP dataset, 1% to 20% for IMDb dataset, and 1% to 9% for the YouTube dataset. We em-ploy the network cross-validation (NCV) method [16] to reduce the overlap between test samples, which produces fair comparisons between different within-network classification approaches. The NCV method starts by creating k disjoint test sets. Then for each test set fold, the remaining folds are merged together, and the train-ing set is randomly sampled from the merged set. The collective inference is executed over the full set of unlabeled nodes (the infer-ence set), but model performance is only be evaluated on the nodes assigned to the test set. The classification performance is evaluated Input: G , propLabeled , k , S = total number of instances in G F =  X 
Split data into k disjoint folds for fold 1 to k uniform probability from trainPool end for output: F using three standard measures: Macro-F1, Micro-F1, and Ham-ming Loss. Table 3 summarizes the NCV procedure [16]. In this section, we explain the details of the evaluation criteria: Macro-F1, Micro-F1 and Hamming Loss. Given the dataset X  X  R
N  X  M , let y i ,  X  y i  X  { 0 , 1 } K be the true and predicted label sets, respectively, for the instance x i .  X  Macro-F1 [4] is the averaged F1 score over categories: For a category C k , if P k and R k denote the precision and the re-call, respectively, Macro-F1 is defined as the harmonic mean of precision and recall:  X  Micro-F1 [4] is computed using F k 1 while considering the pre-cision as a whole. Specifically, it is defined as follows: Macro-F1 is more sensitive to the performance of rare categories (since all categories are weighted evenly) while Micro-F1 is af-fected more by the common categories (since this measure weights instances evenly).  X  Hamming Loss [28] is one of the most frequently used criteria, which counts the number of labels that are incorrectly predicted. where  X  denotes the Hamming distance (XOR operation), and || X || denotes the l 1 -norm. The smaller the value, the better the perfor-mance of the classifier.
We perform two studies to evaluate the performance of our pro-posed multi-label relational classifier. First, we study the perfor-mance of SCRN under different measures of calculating the node similarity, w ( v i ,v j ) . Then we compare the classification results of Table 4: SCRN results using different node similarity measures on DBLP (10% training data) SCRN against four baseline methods on the DBLP, YouTube and IMDb datasets. 4
Both the wvRN and SCRN classifiers consider the similarity of linked nodes, w ( v i ,v j ) , when estimating the label of node v measures the similarity between linked nodes; note that the weight matrix W is not necessarily symmetric (i.e., w ( v i ,v j ferent from w ( v j ,v i ) ). In this experiment we compare three differ-ent approaches for determining the node similarity using the infor-mation contained in the network structure.  X  Degree calculates the weight w ( v i ,v j ) by the normalized frac-tion of connections between v i and v j among all of v i  X  X  connec-tions. In our weighted DBLP dataset, we normalize the original weight of the link, w 0 ( v i ,v j ) , by the total weight summed over the neighbors of node v i , P j  X  N of the node X  X  similarity to v j .  X  Cosine Similarity uses the cosine function to normalize the number of common neighbors between two nodes in the graph.  X  Pearson Correlation Coefficient is an alternative way to nor-malize the count of common neighbors by comparing it with the expected value that the count would have in a network in which nodes select their neighbors at random [19].

Table 4 shows the classification performance of SCRN using different node similarity measures. The Degree similarity mea-sure clearly achieves the highest accuracy rate (Macro-F1 score of 49.35%); the Pearson Correlation Coefficient performs slightly worse than Degree ; and Cosine Similarity is poorest at capturing the relationship between two nodes. Based on this experiment, we select the Degree method to measure node similarity for the re-maining experiments in the paper.
Table 5 shows the classification performance, under Macro-F1 and Micro-F1 measures, on the DBLP dataset averaged over 10 cross-validation folds. We make several observations. First, we confirm that all the network classification approaches, which con-sider the correlations between linked nodes ( SCRN , EdgeCluster and wvRN ) always outperform the two baseline methods, Random and Prior . wvRN , which takes advantage of the correlation between the labels of linked nodes, significantly outperforms the baselines. EdgeCluster , which uses social features in a supervised learning framework, performs worse than wvRN on this dataset, since it is less able to exploit label homophily. Our proposed method SCRN , which leverages both social features and neighboring labels, con-sistently outperforms the others. The class-propagation probability in SCRN captures the node X  X  intrinsic likelihood of belonging to each class, enabling a more accurate inference procedure. Table 6 shows results on the IMDb dataset. We observe that SCRN consistently has the best performance on Micro-F1. On
Our open-source implementation of SCRN and the base-line methods is available at: http://code.google.com/p/ multilabel-classification-on-social-network/ .
 Figure 3: Classification on DBLP Dataset (Hamming Loss); lower score corresponds to better performance. SCRN is sig-nificantly better.
 Macro-F1, SCRN and wvRN are tied and significantly outperform the non-relational methods. In contrast, we observe that EdgeClus-ter performs surprisingly well on the YouTube dataset, as seen in Table 7. In particular, under the Macro-F1 measure, SCRN is out-performed by EdgeCluster , although SCRN is still the best under the Micro-F1 measure for most conditions. We attribute this to the fact that the YouTube network is not a true collaboration network with strong causal links between authors; also it has a large number (47) of highly skewed classes with a less informative link structure. The relatively low correlation between labels of linked nodes pe-nalizes relational classifiers such as SCRN and wvRN .

Our results confirm that in multi-label collaborative networks, such as DBLP and IMDb, the correlation between connected nodes can be a great asset for relational learning. However, we argue that it is important to correctly exploit this information. For instance, in previous work by Tang and Liu [22, 24], combining labeled node features and relational features aggregated from neighbors in a link-based classifier performed poorly. Thus, in our proposed approach, rather than simply concatenating these two types of features, we translate the similarity between two connected nodes X  social fea-tures into a class propagation probability and see that this signifi-cantly boosts the performance of collective classification.
Figures 3, 4, and 5 compare the classification performance of the various methods on the DBLP, IMDB, and YouTube datasets, respectively, under the commonly used Hamming Loss measure for multi-label classification. SCRN significantly outperforms the other methods on both DBLP and IMDb, particularly with fewer training samples. All three methods perform equivalently on the YouTube dataset, as discussed above.
Multi-label classification (MLC) is a variant of classification where each instance is associated with multiple labels. Given a set of training samples, each of which is associated with a set of labels, MLC aims to learn a model that outputs a bipartition of the labels into those relevant and irrelevant with respect to a query instance. One simple way of addressing multi-label learning is to transform the multi-label classification problem into a set of independent, single-label classification problems, e.g., the most intuitive one-vs-rest learning methods [11]. More sophisticated approaches focus on exploiting the correlations between different labels to improve the label set prediction performance. For instance, Guo and Gu [8] Figure 4: Classification on IMDB Dataset (Hamming Loss); lower score corresponds to better performance. SCRN is sig-nificantly better.
 Figure 5: Classification on YouTube Dataset (Hamming Loss); lower score corresponds to better performance. All methods perform equivalently. proposed a generalized conditional dependency network model for multi-label classification. Their conditional dependency network exploits the dependencies of multiple labels, and the conditional distributions are defined using binary classifiers.

Like other traditional classifiers, MLC assumes that instances are independent and identically distributed. When learning on net-worked data, relational classifiers can improve on the performance of traditional classifiers by taking advantage of dependencies both among labels, and sometimes among attributes, of related labeled instances [12, 14, 17]. Most of the previous work in collective in-ference for relational learning uses network connectivity for pre-diction under the assumption that the connections in the network are homogeneous. However, many real-world networks can be re-garded as heterogeneous information networks composed of multi-ple types of nodes and links. Conventional learning methods do not distinguish the type differences among objects and links in the het-erogeneous network. Ji et al. [10] proposed a ranking-based clas-sification model ( RankClass ) for heterogeneous information net-works. While classifying the data objects, the model simultane-ously ranks each object according to its importance within each class, in order to provide informative class summaries.

Goldberg et al. [7] observe that in social media, nodes may link to one another even if they do not have similar labels. They use two edge types to denote the affinity or disagreement in the class labels of linked objects and incorporate the link type information into discriminant learning. Heatherly et al. [9] introduced a Link Type Relational Bayes Classifier that predicts the node X  X  class la-bels according to the neighbors X  labels as well as their link types. The SocDim framework was created specifically to address the link heterogeneity problem [23]. In this framework, latent social di-mensions are extracted from the network using modularity max-imization to capture the potential affiliations of each entity, and then a discriminant classifier is trained using the instances X  social dimensions. Social features were also employed by Wang and Suk-thankar [26] in conjunction with Fiedler embedding to uncover the relations between nodes and their links.
In this paper, we tackled the problem of classifying multi-label networked datasets, where each instance in the network is asso-ciated with a subset of multiple labels from the candidate label set. We proposed a multi-label relational classifier ( SCRN ) that addresses the issues that arise when directly applying the relational neighbor classifier ( RN ) on network data.

SCRN combines the ability of relational neighbor classifiers to exploit label homophily while simultaneously leveraging feature similarity through the introduction of class propagation probabil-ities. Although this paper focuses on the use of social features extracted from the network, it is straightforward to extend our ap-proach to also employ content features constructed from node (e.g., document) properties.

The intuition behind SCRN is straightforward: wvRN uses the network solely through class-independent pairwise link strengths during label propagation. In contrast, SCRN utilizes an additional observation that strives to capture, on a per class basis, how the given node resembles other nodes based upon the network struc-ture. This observation term thus modifies the probabilities of the node belonging to the different classes. Empirical studies on sev-eral real-world tasks demonstrate that our proposed approach sig-nificantly boosts classification performance on collaboration net-works.
This research was supported in part by DARPA award D13AP00002 and NSF IIS-08451. [2] B OUGHORBELY , S., T AREL , J.-P., AND B OUJEMAA , N. [3] C HAKRABARTI , S., D OM , B., , AND I NDYK , P. Enhanced [4] F AN , R., AND L IN , C. A study on threshold selection for [5] F AN , Y., AND S HELTON , C. R. Learning continuous-time [6] G ETOOR , L., AND T ASKAR , B. Introduction to Statistical [7] G OLDBERG , A., Z HU , X., AND W RIGHT , S. Dissimilarity [8] G UO , Y., AND G U , S. Multi-label classification using [9] H EATHERLY , R., K ANTARCIOGLU , M., AND L I , X. Social [10] J I , M., H AN , J., AND D ANILEVSKY , M. Ranking-based [11] L EWIS , D. D., Y ANG , Y., R OSE , T. G., AND L I , F. RCV1: [12] L U , Q., AND G ETOOR , L. Link-based classification. In [13] M ACSKASSY , S. A., AND P ROVOST , F. A simple relational [14] M ACSKASSY , S. A., AND P ROVOST , F. Classification in [15] M C P HERSON , M., S MITH -L OVIN , L., AND C OOK , J. M. [16] N EVILLE , J., G ALLAGHER , B., E LIASSI -R AD , T., [17] N EVILLE , J., AND J ENSEN , D. Iterative classification in [18] N EVILLE , J., J ENSEN , D., F RIEDLAND , L., AND H [19] N EWMAN , M. Networks: An Introduction . Oxford [20] S EN , P., N AMATA , G., B ILGIC , M., G ETOOR , L., [21] S INGH , A., AND G ORDON , G. A Bayesian matrix [22] T ANG , L., AND L IU , H. Relational learning via latent social [23] T ANG , L., AND L IU , H. Scalable learning of collective [24] T ANG , L., AND L IU , H. Leveraging social media networks [25] T ASKAR , B., A BBEEL , P., AND K OLLER , D.
 [26] W ANG , X., AND S UKTHANKAR , G. Extracting social [27] Y EDIDIA , J. S., F REEMAN , W. T., AND W EISS , Y. [28] Z HANG , X., Y UAN , Q., Z HAO , S., F AN , W., Z HENG
