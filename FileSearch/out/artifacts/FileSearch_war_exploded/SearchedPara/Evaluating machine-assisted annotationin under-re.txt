 Abstract Machine assistance is vital to managing the cost of corpus annotation projects. Identifying effective forms of machine assistance through principled evaluation is particularly important and challenging in under-resourced domains and highly heterogeneous corpora, as the quality of machine assistance varies. We perform a fine-grained evaluation of two machine-assistance techniques in the context of an under-resourced corpus annotation project. This evaluation requires a carefully controlled user study crafted to test a number of specific hypotheses. We show that human annotators performing morphological analysis of text in a Semitic language perform their task significantly more accurately and quickly when even mediocre pre-annotations are provided. When pre-annotations are at least 70 % accurate, annotator speed and accuracy show statistically significant relative improvements of 25 X 35 and 5 X 7 %, respectively. However, controlled user studies are too costly to be suitable for under-resourced corpus annotation projects. Thus, we also present an alternative analysis methodology that models the data as a combination of latent variables in a Bayesian framework. We show that modeling the effects of interesting confounding factors can generate useful insights. In par-ticular, correction propagation appears to be most effective for our task when implemented with minimal user involvement. More importantly, by explicitly accounting for confounding variables, this approach has the potential to yield fine-grained evaluations using data collected in a natural environment outside of costly controlled user studies.
 Keywords Annotation Corpus annotation Machine assistance Syriac studies Bayesian data analysis User study Language resource evaluation 1 Introduction The current success and widespread use of data-driven techniques for processing human language make annotated corpora an essential language resource. Many popular natural language processing (NLP) algorithms require large amounts of high-quality (often human-annotated) training data in order to perform effectively. Also, text whose linguistic structure has been disambiguated with annotations is useful in its own right as a means of exploring and understanding the text itself. For example, one might use part-of-speech (POS) annotations and syntactic dependen-cies to study the diachronic prominence of ideas in a language [e.g., Smith et al. ( 2000 )].

There is an urgent need to produce more annotated corpora. Because data-driven disambiguation techniques learn to mimic patterns found in training data, they perform best when the data from which they learn are very similar to the data they must process. For example, an automatic grammatical tagger trained on hand-labeled news articles will likely perform well when used to automatically label similar articles but may perform poorly on transcribed spontaneous vocal utterances. Therefore, for best performance, it may be necessary to label corpora for each domain of interest. In addition, a growing number of linguistic tasks have been proposed including part-of-speech tagging, named entity recognition, constituent parsing, dependency parsing (Manning and Schu  X  tze 1999 ), supertagging and deep grammatical parsing (Bangalore and Joshi 1999 ), co-reference resolution (Soon et al. 2001 ), sentiment analysis (Pang and Lee 2008 ), information extraction (Cowie and Lehnert 1996 ), logical form extraction (Liang et al. 2013 ; Zettlemoyer and Collins 2005 ), and many more. Solving high-level language problems may involve integrating state-of-the-art solutions for many different NLP tasks, such as IBM X  X  recently publicized Watson project, designed to accomplish deep question answering (Ferrucci et al. 2010 ).

The number of annotated corpora required to adequately cover an interesting subset of the cross product of domains and tasks poses a problem even for highly resourced languages (e.g., English) and is far more daunting for the many under-resourced languages of the world. For the purposes of this paper, we call a language, task, or corpus  X  X  X nder-resourced X  X  if there there are strong constraints imposed on the amount of manual work that can be done. These constraints may arise from insufficient funding or from limits on the number of experts qualified to do the work. Because limited resources are available to annotate corpora in under-resourced domains, it is critical to find ways of reducing the cost of creating annotated corpora.

In Sect. 2 we discuss previous work that uses machines to assist humans in the work of annotation. In Sect. 3 we discuss the value of performing evaluations at multiple levels of machine assistance quality for under-resourced annotation projects, and discuss requirements for annotation tools that support this goal. Section 4 describes the large under-resourced annotation project that motivates this work. Section 5 evaluates the effects of pre-annotation and correction propagation on the morphological analysis of text in an under-resourced Semitic language. It presents a traditional analysis based on mean value comparisons, then discusses the infeasibility of extending this analysis technique to meet the needs of large under-resourced annotation projects, and finally presents a more scalable analysis approach using latent variables and a Bayesian data analysis framework. Section 6 outlines conclusions and future work. 2 Machine-assisted annotation Modern corpora are rapidly becoming too large to annotate by hand. When annotations are required, they are provided by automatic taggers trained on smaller hand-labeled corpora (Leech et al. 1994 ). The British National Corpus (BNC) contains over 100 million words of written and spoken British English (Leech et al. 1994 ). The Corpus of Contemporary American English (COCA) and the Corpus of Historical American English (COHA) contain approximately 425 and 400 million words, respectively (Davies 2009 , 2010 ). The Google Books Corpus contains over 500 billion words and comprises roughly 4 % of all of the books ever published (Michel et al. 2011 ). Even in under-resourced, domains, corpora are growing large. In Sect. 4 we introduce the Syriac Electronic Corpus (SEC), a corpus of classical Syriac, a dialect of Aramaic used since late antiquity by Middle Eastern Christians. The SEC will consist of approximately 50 million words. At a rate of 50 s per word (a generous estimate based on our experience with an expert annotator), this corpus will take approximately 80 dedicated annotation years to completely annotate. With only a limited community of experts in the world qualified to do this work, it is clear that this task, like many other annotation projects, will be impossible without a good deal of automatic assistance.

This section sketches general approaches that have been taken to reduce the cost of creating labeled corpora. It also points out some of the strengths and weaknesses of those approaches.

A popular way of avoiding the costs of a traditional labeling project is to enlist a large body of willing online participants. Although this approach is not strictly machine-assisted, it represents an important recent development in dealing with corpus annotation costs. A large body of work solves annotation problems using crowd-sourcing systems such as Amazon X  X  Mechanical Turk to collect noisy labels at a very low cost (Kittur et al. 2008 ). 1 Others have used cleverly constructed games to elicit labels from participants (Von Ahn and Dabbish 2008 ). Similarly, a labeled corpus may be constructed for free if one can discover data that has been implicitly labeled, such as movies that have been reviewed online (Pang et al. 2002 ). By enlisting such volunteers, corpus developers effectively turn what might have been under-resourced tasks in terms of funding into highly resourced tasks in terms of manpower. However, these techniques are not applicable to tasks which require a good deal of expertise (Dandapat et al. 2009 ).

When manual corpus creation is unavoidable, various flavors of machine assistance have been proposed to increase annotator speed and accuracy. Marcus et al. ( 1993 ) (famously, in the case of the Penn Treebank) and many others have used pre-annotation, presenting annotators with automatically labeled sentences so that they need merely correct errors rather than annotate from scratch. Kristjansson et al. ( 2004 ) proposed correction propagation as an extension to pre-annotation in which multi-part pre-annotations are dynamically revised whenever the human annotator corrects an erroneous portion of a pre-annotation. Active learning addresses the problem from the machine X  X  point of view: rather than assist annotators to work quickly, active learning aims to assist the automatic labeler to learn quickly by presenting the human annotator/expert with examples that are likely to be of the most value to the automatic labeler, relative to their cost (Settles 2010 ; Haertel et al. 2008a , b ). Higher-quality pre-annotations may, in turn, reduce annotation cost. In a similar vein, several researchers have explored ways of training a high-quality automatic annotator cheaply by allowing experts to inject expert knowledge into the model, thereby accelerating the training process (Liang et al. 2009 ; Chang et al. 2007 ; Druck et al. 2008 ; Ganchev et al. 2008 ; Naseem et al. 2010 ).

All machine-assistance techniques rely on the existence of an automatic helper of some kind, and, as previously noted, most state-of-the-art automatic learners rely on the existence of already labeled data. The result of this circular dependency is that machine assistance tends to be of a poor quality to begin with, slowly improving as labels are accumulated. If the quality is sufficiently poor, it is entirely possible that machine assistance could do more harm than good by distracting and antagonizing human annotators. For example, Barque et al. ( 2010 ) report that low-quality dictionary segmentation pre-annotations do not increase annotator speed at all. In addition, the best implementation of a given machine-assistance technique is not guaranteed to be the same for every linguistic task. For simple tasks like POS tagging, presenting the single most likely pre-annotation could be the most effective pre-annotation strategy (Marcus et al. 1993 ), while for tasks like parsing, it may be more effective to present the top k possibilities (Carter 1997 ; Oepen et al. 2002 ; Ganchev et al. 2007 ; Ringger et al. 2008 ). A crucial question, then X  X egarding machine-assistance techniques X  X s determining when and how they ought to be used for each linguistic task and domain. 3 Fine-grained evaluation of machine assistance With some exceptions (Fort and Sagot 2010 ; Alex et al. 2008 ), most work that we are aware of in evaluating machine assistance focuses on evaluating the highest possible assistance quality (Marcus et al. 1993 ; Ngai and Yarowsky 2000 ; Brants and Plaehn 2000 ; Chiou et al. 2001 ; Baldridge and Osborne 2004 ; Kristjansson et al. 2004 ; Tanaka et al. 2005 ; Ganchev et al. 2007 ; Tomanek et al. 2007 ; Rehbein et al. 2009 ). For highly resourced tasks and large, homogeneous domains, this approach is not unreasonable. When a model X  X  quality is plotted against the amount of data it was trained on, the resulting plot, or learning curve, usually grows quickly in the early stages and then slowly in the later stages. Therefore, if the quality of automatic assistance grows according to a typical learning curve, we would expect that during the majority of the annotation project annotators would have access to high-quality assistance.

However, we cannot assume we will enjoy such high-quality assistance in the context of under-resourced languages or a highly heterogeneous corpus. For one thing, most models suitable for machine assistance were developed against data from highly resourced languages. When we apply these models to a language with different characteristics, performance often suffers, resulting in learning curves that climb more slowly. Also, if a corpus contains portions with different characteristics, then models trained on one part of the corpus may perform poorly on other parts. For an example of such a corpus see the Syriac Electronic Corpus described in Sect. 4.1 . Developers of under-resourced and heterogeneous corpora must be concerned with the success of machine assistance at a variety of quality levels.

Broadly, this paper contributes to the field of labeled corpus development, especially in under-resourced domains, by presenting tools and methodologies for performing fine-grained evaluations of machine-assisted annotation. Such an evaluation equips corpus developers to determine at what quality levels a particular form of machine assistance warrants use in their domain. 3.1 Collecting data for fine-grained evaluation In order to analyze the effect of machine assistance on the performance of human annotators, it is necessary to keep detailed records of the annotators X  actions as they work. For example, we commonly wish to know how machine assistance affects annotation speed and/or accuracy. To answer this question, it is necessary to record the times and accuracies of annotators working under a variety of conditions. As currently existing annotation tools did not meet our needs, we created CCASH (Cost-Conscious Annotation Supervised by Humans) and made it available to the community under the AGPL license at http://ccash.sourceforge.net . Although Felt et al. ( 2010 ) have described CCASH in detail, we briefly restate here some of the features that make this tool suitable for evaluating machine-assistance tech-niques. CCASH is a web application written using the Google Web Toolkit, 2 which compiles to standard JavaScript on the client. Thus CCASH enables users running a variety of operating systems and browsers to collaborate on a shared annotation project. Both CCASH X  X  GUI and its data structures are customizable, allowing users to implement machine-assistance techniques that we have not considered. Most importantly, CCASH can record not just annotations, but also detailed timing sta-tistics describing the conditions under which each annotation was produced. This instrumentation is implemented as timing hooks in the client-side JavaScript code rather than on the server, which only has access to coarse-grained transaction information.

Development has been ongoing since CCASH was originally released, and there have been several important changes. Most notably, we have implemented a generic data structure consisting of interlinked trees of strings suitable for representing textual data and many kinds of annotations. The availability of a flexible, pre-implemented data structure reduces the burden on corpus developers who are not interested in defining and persisting their own data structures. In addition, we are actively redesigning our user management and permissions system in order to allow users to upload and administer their own data. Data owners will be able to allow other users to view, annotate, or administer various portions of their data in a fine-grained fashion. 3 3.2 A simple example: tag dictionaries For tasks with a limited vocabulary, machine assistance can be provided to annotators by using labeled data to build dictionaries of memorized annotations. For example, imagine a system that assists users to annotate words by prompting them correct annotation is in the prompt, this approach can save time. On the other hand, if the prompt does not contain the correct annotation, it costs the annotator extra time to read and then dismiss the prompt.

Carmen et al. ( 2010 ) used CCASH to test the effectiveness of annotation dictionaries in the context of English part-of-speech tagging. Thirty-three first-semester graduate students in linguistics annotated 18 sentences, assisted by tag dictionaries of different qualities for each sentence. Hypothesis testing showed that tag dictionaries significantly improve annotation speed without sacrificing accuracy when tag dictionaries contained at least one entry (not necessarily correct) for at least 60 % of the words in a sentence. This 60 %  X  X  X overage level X  X  represented a surprisingly low bar for machine assistance to be useful, especially given the simplicity of the approach.
 The sentences in this study (18 in all) were randomly selected from the Penn Treebank such that sentences were of three lengths: short (12 tokens), medium (23 tokens), and long (36 tokens); 6 sentences were selected per length bucket. Participants were presented the same sentences in the same order, this order having been pre-established randomly. However, the coverage level of the dictionary was randomized for each sentence presented to the participant, under the constraint that a given user be assigned a unique coverage level for each of the 6 sentences in every length bucket. This method ensured that each sentence was annotated at a maximum number of distinct coverage levels by different participants. Before beginning the study, participants were presented with a tutorial consisting of four randomly selected sentences. This helped mitigate the potential effects of the human learning curve and, in addition, it helped familiarize the participants with the user interface and the Penn Treebank tag set.

Although manual English part-of-speech tagging is a fairly well solved problem, this study serves to illustrate the basic process of conducting a simple fine-grained analysis. Detailed instrumentation data is collected by the annotation framework as human annotators complete a task assisted by machine models of varying qualities. performance is either held fixed or else evenly averaged over. In this scenario, the analysis may be conducted by simply comparing the average time and accuracy of annotators under each condition. 4 Case study: Syriac morphological analysis In this section we describe a large corpus annotation project and use it to motivate a fine-grained evaluation of the effects of machine assistance on its annotators. We extend the analysis of data described by Felt et al. ( 2012 ) in order to overcome some important limitations affecting our initial analysis of the data. 4.1 The Syriac electronic corpus Scholars at the Neal A. Maxwell Institute for Religious Scholarship at Brigham Young University and at the Oriental Institute at the University of Oxford are jointly working on a project called the Syriac Electronic Corpus (SEC) project, with the goal of creating a comprehensive, labeled corpus of classical Syriac. Classical Syriac ( X  kthobonoyo  X ) is an under-resourced Semitic language of the Christian Near East and a dialect of Aramaic. It is currently in use as a liturgical language but was a true spoken language up until the eighth century when it was largely supplanted by Arabic. Because many prolific and gifted authors wrote in classical Syriac, SEC texts include both prose and poetry and span more than a millennium. One goal of the SEC project is to annotate these texts with morphological analyses to facilitate systematic study of classical Syriac by historians, linguists, and language learners (Fig. 1 ).

Morphological analysis of Syriac is the process of segmenting a word into its constituent morphemes and labeling each with its grammatical function(s). For our purposes, the primary morpheme is the  X  X  X tem X  X , namely the remainder of the token after removing morphological suffixes and prefixes. The dictionary citation form (or  X  X  X aseform X  X ) and the root are identified from the stem. Although the task of Syriac morphological analysis meets the needs of the SEC and is interesting in its own linguistic structure prediction problem, and so inhabits a space that is the subject of a good deal of active research in NLP (Smith 2011 ). We anticipate that methods developed for this task may be more generally applicable.

In contrast to English, where searching for a few forms of a word or using simple query-expansion is often sufficient for discovering patterns reflecting the word X  X  usage and meaning, in Semitic languages search and discovery are not so straightforward. If we could search Syriac texts on citation forms or even on roots, we could search for and discover patterns as easily as in English; however, Semitic roots are altered significantly by expressive inflectional and derivational morpho-logical processes. Consequently, forms of any given Semitic root are numerous. As a result, searching for text written in languages such as Syriac, Hebrew, and Arabic is impaired since one must either limit one X  X  query to a single inflected form or use heuristics to expand the query, buying higher recall at the price of much lower precision.

A morphologically annotated digital corpus of a lesser studied language such as classical Syriac lends itself to search and therefore to careful study in a way that formerly only experts could attempt based on long years of familiarity. Such annotated corpora enable scholars to study and discover the contributions of and trends in historical documents. One outstanding example of such a corpus is the Dead Sea Scrolls Electronic Library (DSSEL), assembled by the Center for the Preservation of Ancient Religious Texts (CPART) in the Neal A. Maxwell Institute for Religious Scholarship at Brigham Young University (Tov and Reynolds 2006 ). The Bar Ilan University Responsa project 4 is another excellent example of an annotated corpus of historical texts. The Syriac Corpus will be an artifact of similar value: useful to linguists, Syriac students, and scholars of Syriac, the Near East, and Eastern Christianity.

Unfortunately, as we have already noted, creating annotated corpora can be extremely time consuming. The Way International Foundation, a Biblical research, teaching, and fellowship ministry, spent 15 years labeling the Syriac New Testament with morphological annotations (Kiraz 1994 ). The Syriac New Testa-ment consists of approximately 100,000 words. Similarly, two Syriac scholars recently required 18 months to hand label less than half of the Old Testament (Heal 2011, Personal communication). By contrast, the Syriac Corpus aims to encompass approximately 50,000,000 words, by the latest estimate. To achieve this goal in a timely manner it will be necessary to increase the speed of annotation.
 4.2 Pre-annotation and correction propagation The annotated Syriac New Testament provides sufficient training data to create reasonable machine assistance. We use Syromorph, a probabilistic morphological analyzer for Syriac developed by McClanahan et al. ( 2010 ), to provide pre-annotations. Syromorph consists of a joint pipeline of classification and transduction tasks. Each task in the pipeline relies on the data and on a set of the n -best hypotheses produced by tasks preceding the current task in the pipeline. Syromorph first segments each word into its parts: prefix, stem, and suffix. Syromorph then predicts a grammatical category and a baseform, or dictionary citation form, for the stem. Finally, Syromorph predicts the morphological attributes of the stem and suffix.

In order to apply correction propagation to Syriac morphological analysis, our model must be capable of constraining its predictions to match partial labelings. Kristjansson et al. ( 2004 ) propose a constrained Viterbi decoding algorithm for linear conditional random field models (CRFs). Because Syromorph is a pipelined sequence model, we adapt Kristjannson X  X  method of constrained decoding, with the difference that we use an n -best beam decoder instead of Viterbi decoding. 4.3 User study Although Syromorph performs well on SEC data that is similar to the New Testament on which it was trained, its performance suffers considerably when applied to text from different time periods and genres. Stylistic word-order differences between prose and poetry challenge a probabilistic sequence model like Syromorph that relies on consistent patterns among grammatical sequences. Also, classical Syriac literature was produced over a period of more than a millenium, and the vocabulary of later texts differs from that of early texts. Although Syromorph X  X  design includes the capability of handling novel tokens, it is necessarily more error-prone when analyzing previously unseen tokens. Also, despite a good deal of valuable work on the subject, there is as yet no globally accepted morphological annotation scheme in the Syriac Studies community. The initial stages of annotation have already involved over a dozen major and minor changes to the annotation scheme, each of which negatively impacts Syromorph X  X  performance as the target annotation scheme diverges from Syromorph X  X  training data. For these reasons it is imperative that we understand how Syromorph X  X  performance interacts with annotator efficiency at a variety of quality levels.

We designed a controlled user study using the CCASH annotation framework in order to conduct a fine-grained analysis of the effectiveness of pre-annotation and correction propagation applied to Syriac morphological analysis. The study is first described by Felt et al. ( 2012 ). We repeat here the details of the study in order to help readers fully understand the data dealt with in subsequent analyses.

This section will proceed as follows: Sect. 4.3.1 gives an overview of the user study layout. Section 4.3.2 describes the training and evaluation of the automatic annotation models used in the study; Sect. 4.3.3 explains our method of assigning experimental conditions to participants. Section 4.3.4 describes the user study participants; Sect. 4.3.5 describes the framework used to conduct the study and the study X  X  graphical user interface; Sect. 4.3.6 describes the data collected by the user study 4.3.1 User study overview We designed a web-mediated user study using CCASH, an open source web application framework for linguistic annotation tasks described in Sect. 3.1 . In the study, annotators took a survey, participated in an interactive training, and then worked through four practice sentences. After each practice sentence, participants received feedback on how their annotations differed from the annotation guidelines they were given. They were required to achieve a high level of accuracy on the final practice sentence before proceeding. Finally, participants annotated 30 sentences under a sequence of randomly assigned experimental conditions, explained in Sect. 4.3.3 . For each word in the study, CCASH recorded the time each annotator took as well as the number of correct and incorrect decisions he or she made.

The choice to have all participants annotate the same 30 sentences does not limit our ability to collect large amounts of data and identify statistical trends associated with different annotation conditions. It does limit the applicability of our results to new data; however, that is a problem inherent in any focused study.

A gold standard annotation was constructed by two expert Syriac linguists who completed the study, then discussed and resolved all disagreements in their annotations. It should be noted that annotated Syriac text already exists: The Syriac Peshitta New Testament has been labeled with morphological information (Kiraz 1994 ). However, reference copies of this data have been published which could bias the results of our study. Accordingly, the 30 sentences for the study were selected uniformly at random from The Acts of Judas Thomas, an apocryphal text that is similar, but not identical, to the New Testament (Wright 1871 ).

When constructing a gold standard, it is important to acknowledge that there are some inherently ambiguous cases on which even experts have difficulty agreeing (Klebanov and Beigman 2009 ). However, the disagreements between our experts indicated that only around 20 of the 1,289 decisions in the user study were ambiguous. This rate is low enough that it should not greatly affect our results. 4.3.2 Model training and metrics We trained Syromorph models on various random subsets of the Syriac New Testament data assembled by Kiraz ( 1994 ) and augmented with suffix data by McClanahan et al. ( 2010 ), consisting of approximately 100,000 labeled tokens. We calculated model accuracy against the 30 Judas Thomas sentences in the study X  X  gold standard. This slight mismatch between model training and test data caused model accuracy to suffer. Thus our most accurate model, trained on all of the New Testament data, achieved an accuracy of only slightly above 90 %. In order to obtain models with the target accuracies specified in Sect. 4.3.3 , we trained Syromorph on random subsets of the training data until a model was found which achieved the desired accuracy  X  0.01 %, measured against the gold standard.
In a structured annotation task like Syriac morphological analysis, accuracy can be calculated at the sentence level, the word level, or the decision level. These accuracy metrics are highly correlated, but not identical. Furthermore, since decisions can be partitioned into classes according to their sub-task, it is possible to calculate decision-level accuracy either as a macro-average or as a micro-average across decision types. A macro-average is computed by first averaging the decisions for a sub-task, then averaging the resulting averages. A micro-average is computed by averaging the decisions for all sub-tasks at once. Decision-level accuracy using a micro-average is an appropriate accuracy metric since it is computed over the exact set of choices that an annotator must make while annotating. All accuracies mentioned in this paper are decision-level micro-averages calculated against the 30 sentence gold standard set. 4.3.3 Experimental conditions Pre-annotations were supplied to annotators at the following accuracy levels: none , 25, 36, 47, 58, 68, 79, 90, and 100 %. In the none case, no pre-annotations were given. In the 100 % case, gold standard annotations were given. In all intermediate cases, Syromorph models trained to the indicated accuracy provided pre-annota-tions. The accuracy levels between 25 and 90 % inclusive were chosen to span the range of accuracies achievable by Syromorph trained on the Peshitta New Testament.

Additionally, participants annotated sentences both with and without the assistance of correction propagation. Note that correction propagation requires a model; consequently it could not be applied to the none or 100 % cases. In all, there were or 16 parameter combinations to test. We refer to each parameter combination as an experimental condition .

Experimental conditions were assigned to participants and sentences using the matrix in Table 1 where Participant1 was the first participant to take the study, Sent1 was the first sentence in the study, and cell values indicate a pre-annotation quality (25 X 100) and the optional presence of correction propagation ( ? CP).
This matrix was replicated and extended on the bottom in order to accommodate the 30 sentences of the study. That is, Sentence 17 was assigned to the same row as Sentence 1, and so on. This parameter assignment scheme has some desirable properties. It guarantees that annotators encounter each experimental condition roughly the same number of times because conditions are evenly distributed within each column. It also ensures that sentences are annotated under each condition roughly the same number of times because conditions are evenly distributed within each row.

However, this parameter assignment scheme has an important flaw: annotators encounter sentences of steadily increasing quality. Such an apparent trend might affect the way that annotators interact with the pre-annotations. This problem was resolved without sacrificing the desirable properties of the assignment matrix by first randomly permuting the rows of the matrix and afterwards the columns. Randomly permuting the rows scrambles the order in which participants encounter experimental conditions, but does not change which conditions are members of each column, ensuring that participants still encounter each condition roughly the same number of times (see Table 2 for a simple illustration). Row properties remain entirely unchanged. Permuting columns similarly leaves column and row distribu-tions unchanged. Annotators thus encountered the study X  X  sentences in a fixed order and under every experimental condition but without a discernible pattern.

It was expected that annotators would begin to annotate slowly then move more quickly as they grew accustomed to the task; this could potentially have a confounding effect on our timing data. We dealt with this human learning effect in two ways. First, the training and practice at the beginning of the study allowed participants to familiarize themselves with the task and user interface. Second, the parameter assignment scheme ensured that the sentences annotated under a given experimental condition would include approximately equal numbers of sentences annotated early and late in the annotation process.
 4.3.4 User study participants Nine Syriac experts, invited by colleagues associated with CPART and the Oriental Institute at the University of Oxford, successfully completed the study. Their answers to the survey at the beginning of the study indicated that all participants consider themselves reasonably proficient in Syriac and comfortable using computers. 4.3.5 Graphical user interface The graphical user interface used to conduct Syriac morphological analysis, implemented in CCASH, is an important part of this study since it affects annotation speed and also the applicability of this study to other tasks. The interface was refined in close consultation with Syriac experts to make sure it would be reasonably efficient for the study.

Annotators work through a sentence at a time. Half of the screen shows the sentence currently being annotated along with some surrounding context (Fig. 2 a). Note that the text reads from right to left. Annotators navigate from one word to another in the sentence either by clicking on the desired word or by holding down [ctrl] on the keyboard and navigating with the arrow keys. The word being annotated comes into focus in the right half of the screen. Within each word, annotators begin by identifying prefixes and suffixes using either mouse clicks or a keyboard shortcut (Fig. 2 b). Then a grammatical category is chosen (Fig. 2 c; in this case, NOUN), after which a set of stem and suffix tags appear (Fig. 2 d) that are applicable for the chosen segmentation and grammatical category. Annotators set tag values either by clicking on them with a mouse and selecting a value from the resulting drop-down list or else by typing them using a keyboard. For annotators who choose to type, the text is autocompleted for them based on the values that are applicable to that field. Finally, annotators may input Syriac text either by using their mouse to click keys on a virtual keyboard or by using their keyboard directly (Fig. 2 e).
 Once an annotator changes a field value, that field X  X  background changes color. When correction propagation is active, each time the annotator changes a field, the model is queried for a new prediction constrained by all of the decisions that the annotator has made so far in the sentence. In the scope of the word currently being annotated, if the new pre-annotation differs from the old pre-annotation, the new hyperlinks are clicked, their value is substituted into the corresponding field. For all other words in the sentence, pre-annotation values are updated in place without notifying the annotator or requiring any human intervention.

As annotators proceed, CCASH records detailed information about each word including accuracy, the time each element spent in focus, mouse clicks, and the number of keystrokes. To ensure that timing information is accurate, participants are instructed to press the pause button on the bottom left of Fig. 2 whenever they take a break. When the task is paused, the screen is also obscured.
 4.3.6 The data Although participants labeled a sentence at a time, it is problematic to do time analysis at the sentence level because the length of each sentence affects its annotation time, making times difficult to compare across sentences. Controlling sentence length could alleviate this problem but introduces a new problem since the length-controlled sentences are not representative of the data as a whole. We avoid these difficulties by conducting analysis at the word level.

To estimate word annotation times, we record the time that each word was in focus in the GUI. This time is not a perfect stand-in for the time an annotator spent actually working on each word, since it is possible for an annotator to consider a word that is not actually selected. Also, the first word of each sentence will naturally tend to be selected longer than other words in the sentence as an annotator orients herself by reading the sentence and context. However, given sufficient response data, these times should be an acceptable approximation for the true time spent annotating each word.

We compute word annotation accuracies by calculating the accuracy of the decisions applicable to the word, as explained in Sect. 4.3.2 .

The study X  X  9 participants each annotated 30 sentences, or 152 words, resulting in 1,368 word-level data points both for annotation time and accuracy. We subsequently filtered out 20 outliers leaving us with 1,348 data points. Outliers were defined as words that took longer than 5 minutes to annotate. Although it is conceivable that a tricky word could take more than 5 min of thought, it is more likely that annotators were distracted from the task during those times. This conjecture is supported by the fact that these outliers were particles X  X asy cases.
Since there are 16 experimental conditions, each condition has roughly 85 data points. Figure 3 uses standard box plots to summarize the data collected under each pre-annotation condition. Notice that for each condition there is considerable variance in both the accuracy of words annotated (Fig. 3 a, c) and the time required to annotate each word (Fig. 3 b, d). Notice also that the variance in accuracy diminishes with higher pre-annotation quality, and both the mean and variance of time diminish at higher pre-annotation qualities. In order to assess these visible trends, we require more careful analysis.

For the purposes of this analysis we treat all decisions as equal when calculating accuracy. However, it can be seen in Table 3 that some types of decisions are more difficult than others for humans and machines alike. Also, note that as Syromorph X  X  overall accuracy changes, its performance on the various decision types does not vary smoothly, but rather is an unpredictable function of the characteristics of Syromorph X  X  learning algorithm and the randomly selected set of data it was trained on (see Sect. 4.3.2 ). Clearly, overall decision accuracy is an imperfect measure of model quality. It is, however, a useful approximation, as the results of the analysis indicate. 5 Fine-grained evaluation In this section, we perform a fine-grained evaluation of pre-annotation correction propagation applied to Syriac morphological analysis using the data gathered in Sect. 4.3 , in order to identify the quality of machine assistance required to make these techniques effective. In Sect. 5.1 we use simple mean comparisons and hypothesis testing to answer this question. In Sect. 5.2 we discuss the practical limitations of 5.1 X  X  methodology in the context of the SEC project, and in Sect. 5.3 we investigate an alternative evaluation scenario with the potential to avoid costly user studies such as that described in Sect. 4.3 . In Sect. 5.4 we discuss the analysis requirements of large under-resourced annotation projects and argue that the approach introduced in Sect. 5.3 has the potential to fulfill these requirements. 5.1 Fine-grained evaluation using mean comparisons We expended considerable effort in Sect. 4.3 to control for confounding effects by either holding them fixed or uniformly averaging over them. This allows us to evaluate our variables of interest, annotator speed and accuracy, by simply comparing the mean values of these variables in subsets of the data. These differences may be tested for significance using null hypothesis tests. We posit three pairs of null hypotheses.
The first pair of null hypotheses is that annotator speed and accuracy are not significantly different for words annotated with and without pre-annotations. Testing these hypotheses at each of the eight pre-annotation accuracy levels indicates when the pre-annotation ought to be used.

The second pair of null hypotheses is that annotator speed and accuracy are not significantly different for words annotated without assistance and those annotated with the combination of pre-annotation and correction propagation. Testing this hypothesis at each pre-annotation accuracy level indicates when combined pre-annotation and correction propagation ought to be used.

The third pair of null hypotheses attempts to tease apart the effects of correction propagation and pre-annotation: assuming pre-annotations are being used, annotator speed and accuracy are not significantly different for words annotated with and without correction propagation. Testing this hypothesis at each pre-annotation accuracy level indicates when correction propagation ought to be used above and beyond pre-annotation.

Each null hypothesis is tested using both a standard two-sided Student X  X  t test as well as a permutation test (Menke and Martinez 2004 ). The Student X  X  t test is used since it is widely understood and used. A two-sided t test is appropriate since there is the possibility that accuracy and annotation time will either increase or decrease. The permutation test is used since it does not rely on assumptions about any underlying distribution. Note that with 48 null hypotheses being tested, we expect a few spurious rejections. This can be seen by recalling that if we draw two sets of data from the same process, we expect a standard t test with a p value threshold of 0.05 to incorrectly reject the null hypothesis one time in twenty. However, if pre-annotation and correction propagation do indeed improve annotator time or accuracy, there should be clear trends in the rejections.

Table 4 shows the difference between the mean annotator accuracies (a) and times (b) of words annotated under the control condition and of words annotated under the test condition at various levels of pre-annotation quality. Increases in accuracy are good and decreases in time are good. Removing outliers has little effect on the outcomes, so we leave them in for all analyses.

The first row of Table 4 a compares the accuracy of words annotated without the assistance of pre-annotations to that of words annotation with the assistance of pre-annotations. There is a clear block of significant results. It appears that pre-annotations generated by models of quality 70 % or higher increase average annotator accuracy by 5 X 7 % relative to the average annotator accuracy of 90 %, and that increase is usually greater than can be explained by the natural variance of the data. This is an encouraging result for those contemplating using pre-annotation on similar tasks. Although 70 % appears relatively high in the range of model accuracies that we have presented, it is actually quite low for a reasonable predictive model. That is, 70 % accurate models can be attained with relatively little data for most tasks (in our case roughly 50 annotated sentences), resulting in a low barrier to entry for those wishing to employ pre-annotation on similar tasks.

The second row in Table 4 a shows a similar positive trend for the combination of pre-annotation and correction propagation, but with weaker significance. It is unclear whether this trend is explained entirely by the presence of pre-annotation, or whether correction propagation is playing a role in helping or hurting accuracy. The third row of Table 4 a shows mixed signs with no statistical significance, preventing us from drawing any conclusions about the effect of correction propagation above and beyond that of pre-annotation.

The first row in Table 4 b shows the difference between the mean time required to label words with and without pre-annotations. Pre-annotations generated by models of quality 70 % or better decrease average word annotation time by 25 X 35 % relative to the average annotator speed of 53 s per word. Pending additional evidence to strengthen the outcome, it is reasonably clear that moderately good pre-annotation reduces the time required for annotation. The contribution of correction propagation is again unclear. 5.2 Limitations of mean value comparisons The approach described in Sect. 5.1 has some desirable properties. It is natural to compare average performance observed under competing treatments. This makes the values in Table 4 relatively easy to understand and interpret. Also, when data is collected in a carefully controlled experiment, it lends weight to the conclusions drawn from it. For these reasons, there is a large historical body of literature that relies on hypothesis testing to inform important decisions. However, this approach to analysis suffers from a number of practical limitations that prevent it from being usable in the context of an ambitious, resource-limited project such as the SEC.
Some minor limitations of the analysis methodology in Sect. 5.1 stem from the fact that we are using hypothesis tests. Traditional hypothesis testing has been criticized for lacking interpretability and leading to probabilistic paradoxes (Berger and Berry 1988 ). In Sect. 5.1 we looked for patterns in Table 4 and argued that we could use them to shape our beliefs about more general trends. Although this is a natural thing to do, hypothesis testing is equipped only to test specific hypotheses X  not to quantify belief, which is what we are actually interested in doing when we try to understand how machine assistance interacts with annotator efficiency. Also, the t test assumes normally distributed data, an assumption which is clearly violated by our accuracy data. More importantly for our purposes, the hypothesis testing framework does not, strictly speaking, allow us to use our data in order to test hypotheses beyond those which the experiment was originally designed to test. Since we have finely instrumented data, we would like to derive additional information from it, such as whether some kinds of words (e.g., nouns) are more difficult or time consuming than others.

The most important limitation of an analysis based on mean comparisons is that it relies on creating an environment in which confounding effects are artificially controlled for by being either fixed or distributed evenly across experimental conditions. This involves a good deal of planning, preparation, and wasted annotation effort (from the perspective of corpus annotators). For example, in order to control for the fact that some sentences are inherently more difficult than others, all annotators had to redundantly annotate the same thirty sentences. Although we expect the results in Table 4 to generalize to some degree, we would like to update this analysis as the SEC project annotation task and user interface changes over time. Moreover, we would like to evaluate other machine-assistance techniques, such as active learning or allowing annotators to specify expert knowledge (see Sect. 2 ). Annotators involved in the study expressed frustration at doing work that did not directly add annotations to the corpus, and in under-resourced domains such as the SEC, the pool of annotators qualified to participate in such a study is extremely limited. It is unlikely that we enjoy the resources or participation necessary to carry out repeated user studies in the style of Sect. 4.3 . We must be able to do these evaluations on-the-fly as corpus annotation proceeds, without any kind of artificial controls. 5.3 Fine-grained evaluation using a latent variable model One way of dealing with confounding factors is to explicitly model them. Although the values of confounding variables are not directly measurable, we can model them as latent explanatory variables and then estimate their values based on the way they affect the observed data. This goal may be accomplished using a number of frameworks. For reasons explained in Sect. 5.4 , we elect to use a Bayesian data analysis framework. Although space constraints prevent a detailed introduction to Bayesian data analysis, we sketch the idea here and refer the interested reader to the many excellent tutorials and references on the subject such as those by Barry ( 2011 ) or Gelman ( 2004 ).
Conducting a Bayesian data analysis involves proposing a probabilistic model over some partially observed system, and then using the laws of probability together with any available observations to derive probability distributions over the values of hidden variables of interest. The resulting information about hidden variable state is informed both by the expert knowledge built into the model by the analyst as well as by the available observed data.

Let X be a set of observed variables, or data, and Y be a set of parameters. The parameters, although their values are unknown, are hypothesized to have affected the production of the data. For example, if the data consist of error-prone heat measurements in a factory environment, the parameters might represent the unknowable true mean temperature, or the true temperature variance. Or if the data consisted of car insurance claims, one parameter of interest might represent individual driver recklessness X  X  quantity that is difficult to measure directly.
In many situations it is possible to define a function that assigns probability to values of X in a plausible way given value assignments to Y ,or p  X  X  X  x j Y  X  y  X  . This function is known as the data likelihood. The likelihood function encodes our knowledge about the way that the parameters affect the data.

The likelihood alone tells us nothing about the state of the parameters. However, if we use our domain knowledge about likely parameter settings in order to formulate p  X  Y  X  y  X  , known as the prior , then we can use Bayes X  law to calculate the true value of interest, namely, This function is known as the posterior probability distribution and calculates the probability of our parameter state in light of both the information in the observed data X as well as the assumptions built into our likelihood and prior.

Notice that the posterior is built from the product of the likelihood and the prior as well as an additional factor 1 p  X  X  X  that acts as a normalizing constant. In practice the normalizing constant is often intractable to calculate, preventing us from finding closed-form posteriors. However, a variety of algorithms exist in the literature (and are implemented in statistical computing libraries) that allow us to obtain samples from the posterior distribution without computing the normalizing constant. For the practitioner of Bayesian data analysis, then, the focus is not on math, but on modeling. For the posterior distribution to be accurate and helpful, both the likelihood and the priors must do a good job of encoding the practitioner X  X  expert domain knowledge about the system they are analyzing.

We walk through this process first for our time data in Sect. 5.3.1 , and then for our accuracy data in Sect. 5.3.2 . We discuss the results at some length in Sect. 5.3.3 . 5.3.1 Time model with latent variables We begin by proposing a probability distribution over our time data y . We model the number of seconds that it takes to annotate a word by starting with some base time j and then adding or subtracting from that time based on who is annotating, under what experimental condition they are annotating, what kind of word is being annotated, etc. If we finish by adding symmetric noise then the time required to annotate word y may be modeled using a normal distribution whose mean is the sum of the base annotation time j and values (negative or positive) associated with variables representing the conditions under which the word was annotated: y * N ( j ? ... , r 2 ). Note that a normal likelihood is not a perfect choice, since it allocates some probability to negative times, but it is convenient and works well in practice. We indicate which conditions apply to a particular y by indexing it with subscripts { h , a , t , b , r , o }. The subscripts on y index into the following unobserved random variables:  X  h h : The effect of the h th annotator. There are nine h variables, one for each  X  a a : The effect of the a th experimental condition. There are 16 a variables.  X  s t : The effect of the t th grammatical category (noun, verb, etc). There are 6 s  X  b b : The effect of the a word X  X  position within the sentence. We combine all word  X  q r : The effect of correction propagation hyperlinks being shown or not. There  X  x o : The effect of correction propagation hyperlinks being clicked or not. There
The time required to annotate a word y annotated under conditions hatbro is then distributed as and the likelihood of our data set is obtained by taking the product of the density of each observation.

It remains to specify our prior belief about the values of each latent random variable in the form of probability distributions. An expert annotating data outside of the user study took around 90 seconds per word and his times varied by as much as a minute. We use this, along with the knowledge that normal distributions allocate most of their mass to within about 3 standard deviations of the mean, to inform our priors over j and r 2 . j * N (90, 50/3), allowing our offset to vary as much as 50 s on either side. Because r 2 cannot be negative, we model it using a Gamma parameterized by shape and scale. If r 2 were equal to 2,500, that would allow annotation times to vary by about 3 reasonable that annotation times could vary by as much as a couple of minutes given the difficult nature of the task. We therefore let r 2 * Gamma (50,50) which has a mean value of 2,500 with a fair amount of spread, reflecting our uncertainty about this quantity.

We model the effect variables a ; h ; b ; s ; q ; and x as normally distributed around 0, so as to have no effect by default. We don X  X  have good intuitions about what effect each latent variable will have X  X hese are the variables we would like to learn X  X o we set their priors to be relatively uninformative normal distributions, N  X  0 ; 40 3  X  ,allowing each effect to potentially take on values between -40 and ? 40 seconds. Recall that negative effect values contribute to reducing the total annotation time of the words they affect, corresponding to increased annotation speed.

We now have specified everything we need in order to draw samples from posterior probability distributions over the unobserved random variables in light of the data and our priors. We implemented our own sampler and also tried RStan (Stan Development Team 2013 ), which we found to generate equivalent results far more efficiently and conveniently. RStan is one of several available statistical computing packages that allows practitioners to express models in a high level modeling language, abstracting away from mathematical details and reducing the likelihood of implementation errors. In addition, RStan assists with important details like eliminating the effects of poor sampling configurations during the early stages of the process (known as burn-in) and measuring sampling convergence. For simplicity X  X  sake, after validating that RStan X  X  automatic metrics coincided well with our own, we chose to trust them in all cases. The complete conditionals required to manually sample from this model may be found in  X  X  Appendix 3  X  X  section, and the RStan code we used may be found in  X  X  Appendix 1  X  X  section. 5.3.2 Accuracy model with latent variables We now model our word-level accuracy data x . Because accuracies take on values between 0 and 1 and we do not anticipate any multi-modality in our data, they can be modeled using a beta distribution. We reparameterize the beta distribution to use mean l and sample size v rather than the typical shape parameters a and b using the following substitution: a = l v and b = (1 -l ) v .
 constrained by 0 and 1, a sum of normals no longer makes sense. Instead we model l as an average of effects that are, themselves, beta distributions. In this way, effects that increase annotator accuracy will tend towards 1, and those that decrease annotator accuracy will tend to center at lower accuracies. The density of a single word X  X  accuracy is then where BetaMS is a beta distribution parameterized by mean and sample size.

The quantity j represents an average accuracy value. We expect that annotators tend to have high accuracies, so we let j * Beta (3,1.1). This reflects a relatively weak prior belief that we will see about 3 correct decisions per each incorrect decision. The 1.1 shapes the distribution to give very little probability to words that are entirely incorrect. The sample size parameter v represents the total number of decisions (correct or incorrect) that we expect to see per word. Simple words like particles involve few decisions and are numerous; more complex words like nouns and verbs can require as many as 8 or 10 decisions. We therefore let v * Gamma (2,2) parameterized by shape and scale, which spreads a fair amount of mass between 1 and about 12 or so.
 We model the effect variables a ; h ; b ; s ; q ; x with uninformed beta distributions, Beta (1,1). This gives equal probability to every value between 0 and 1, reflecting our lack of prior knowledge about what effect each condition will have on annotator accuracy. This will allow our posterior estimates to be shaped more by the data than model may be found in  X  X  Appendix 2  X  X  section. 5.3.3 Posteriors and discussion We used RStan to obtain 100,000 samples from the joint posterior distribution, where each sample specifies a value for each parameter in our model. These samples allow us to calculate posterior joint, marginal, and conditional probability distributions over unseen variables simply by aggregating over the subsets of values sampled for those variables.
Figures 4 and 5 show the posterior marginal probability densities we estimated for the a variables, indicating the effects of experimental conditions on the speed and accuracy of words. In all graphs the density corresponding to a 0 (the none condition) is plotted as a dashed line for comparison. In Fig. 4 , notice that as pre-annotation levels increase, curves shift farther and farther to the right. This indicates that the model learned that the presence of high-quality machine assistance is associated with annotations of higher accuracy. Similarly, in Fig. 5 curves shift farther and farther to the left as pre-annotation quality increases. This indicates that high-quality machine assistance is associated with smaller annotation times. When we compare conditions with and without correction propagation, it appears that more often than not the curves associated with correction propagation are shifted in the direction of being more helpful (right for accuracy and left for time). In order to assess these visible trends, we require more careful analysis.

We can quantify the differences between a 0  X  X he none condition variable X  X nd the other a variables in a table similar to Table 4 , which we used to summarize our mean comparison tests. In Table 5 we record the average difference between the values of each a variable and a 0 . The probability that a given variable is greater than a is calculated by counting the number of times that this is true in our samples. We use this probability rather than a p value to assess significance in this table. Note that although we choose two arbitrary probability thresholds, 0.9 and 0.95, to report significance, we have an informative probability value for every cell in Table 5 , whereas in Table 4 only the extreme p values are meaningful.
 It is encouraging to see that Table 5 looks similar to Table 4 for the most part. Because of the effort that went into controlling for confounding effects when the data was gathered, a good analysis should mostly agree with Table 4 . Strictly speaking, the two tables are not directly comparable. Table 4 contains differences between average data statistics, whereas Table 5 contains differences between average posterior variable values. Also Table 4 indicates significance using p values, whereas Table 5 indicates significance using probabilities. However, both tables tell the same story regarding pre-annotation: pre-annotations begin to help when the model accuracy approaches 70 %.

On the other hand, Tables 4 and 5 tell rather different stories regarding correction propagation (CP). Correction propagation appears far more promising in the latent variable analysis. We hypothesize that this is because our latent variable models include separate variables to account for when correction propagation was affecting the current word by showing hyperlinks, and when it was invisibly affecting other words in the sentence. When correction propagation was active but operating invisibly in the background, only a variables explain its effect. However, when hyperlinks appeared or were clicked as a part of correction propagation, the variables q and x compete to explain that word (see Sect. 5.3.1 ). Under this explanation, one could say that there were actually two different machine-assistance techniques operating under the name  X  X  X orrection propagation X  X  in the user study: hyperlink-driven CP and invisible CP. The effects of the two approaches X  X ne helpful and the other not X  X ere being confounded in the original analysis because we did not properly control for them.

We can validate this hypothesis somewhat by examining the posterior densities of our q and x variables in Fig. 6 . In our model, hyperlinks being shown and clicked is associated with degraded annotation time and accuracy. For accuracy (Fig. 6 a) this can be seen by observing that the densities in the left column, which are associated with hyperlinks not being shown or clicked, are smashed up against the right side of the plot, column, which are associated with hyperlinks being shown or clicked, are lower, indicating that they contribute to lower accuracies. Similarly, for time (Fig. 6 b) this can be seen by noticing that the densities associated with hyperlinks not being shown or clicked are pushed quite a bit farther left than their counterparts (nearly 20 s in the case of hyperlinks being shown), indicating that they contribute to reducing the time required to annotate a word. One explanation for the poor behavior associated with hyperlinks is that the appearance of hyperlinks is a visual distraction, causing annotators to second-guess themselves or pause in order to assimilate new visual information. Another related explanation is that hyperlinks require annotators to perform two tasks instead of just one: evaluating the initial annotation and also evaluating the corrected one. In either case, the analysis in Fig. 6 suggests that hyperlinks are an inappropriately expensive mechanism for implementing correction propagation, especially in light of the (at best) modest gains afforded by correction propagation.

The posterior densities of the s variables in Fig. 7 estimate the time contributions of different grammatical categories of words. Most of these values are unsurprising. We expected that particles would require little time to label and would tend to be labeled accurately. Similarly we expected that nouns and verbs would require more time to label and would tend to be labeled less accurately. However, the fact that numerals tend to be slow and inaccurate is surprising. Collaborating Syriac experts tell us that labeling numerals ought to be relatively straight-forward, so this data indicates that there is some ambiguity in the labeling documentation or interface that we can address to improve the efficiency of our annotators. Sifting through free-form participant feedback confirmed that participants did feel the need for improving numeral training specifications.

In the posterior densities over the b variables in Fig. 8 , the variable associated with being the first word in a sentence is centered over a high positive value, indicating that being the first word in a sentence tends to increase the time required to annotate the word by a considerable amount. This penalty is probably the result of annotators pausing to orient themselves within the sentence and context before beginning. We anticipated this effect when we designed our model, and modeling it should help account for some of the variance in the data. However, it was unclear before the analysis whether to expect the second and third words to participate in this  X  X  X earning curve X  X  effect. It appears that after the first word in the sentence, all other word positions incur approximately the same time cost.

The posterior densities over the h variables represent effects associated with particular annotators (Fig. 9 ). Notice that annotators who annotate very quickly also tend to be slightly less accurate, such as annotators two and nine . Conversely, some annotators who annotate slowly are very accurate, such as annotators four and five . 5.4 Iterative evaluation in a practical setting As we have defined them, under-resourced annotation projects lack the resources to be successful without judicious use of machine assistance. Our driving motivation is to equip corpus developers working on challenging and dynamic annotation projects to understand the effects of the machine-assistance method-ologies, allowing them to rapidly investigate numerous assistance strategies during the course of annotation. Accordingly, we require that evaluation methodologies support a set of minimal requirements that we anticipate will be necessary in such a scenario.

The annotation tool being used for production must, like CCASH, allow detailed annotation statistics to be constantly gathered. This way when changes are introduced to the user interface or new machine-assistance techniques are tried, data is automatically gathered under the new conditions, immediately available for analysis.

In a natural annotation environment, confounding factors are inevitably introduced into the data. Some annotators may contribute more data than others. Some data may contain different kinds of data than other settings (one author might prefer using difficult and obscure vocabulary). Annotations produced late at night or at the end of a long annotation session may be more prone to error. Bugs associated with particular browsers or versions of the user interface could adversely affect annotator speed.

Accordingly, successful evaluation of natural data must account for confounding factors. For example, suppose that some of the slower annotators had chosen not to annotate under the none case. Table 6 shows that mean comparison analysis fails badly in this case. In this analysis, pre-annotation appears extremely unpromising at almost all qualities, which we know is not the case.

Our latent variable model, on the other hand, does reasonably well. Although the numbers in Table 6 b are not identical to those in Table 5 b, they show the same trends. Since our model has a notion of the effect of annotator identity on annotation speed, it discounts its estimate of the effect of the none case, since it sees that only faster annotators participated in that case. In this way, latent variable models can account for confounding factors in annotator accuracy and timing data by instrumenting the confounding factors and including them as explicit variables in the analysis. Whether or not this approach is successful in practice will depend largely on whether attributes associated with confounding factors are captured in the data, and on whether the model X  X  structure allows the confounding effects to be estimated.

Finally, data is precious in practical under-resourced annotation projects. We require a mechanism for incorporating all of the data collected so far into analyses. We chose to conduct latent variable analysis in a Bayesian setting partly because Bayesian data analysis provides such a mechanism in the form of prior distributions. We note that a Bayesian approach also comes with some important theoretical constraints. Importantly, data that has had some influence on our model X  X  structure or prior may not be analyzed using the model they helped to shape. Procedurally, we may iterate between analyzing data and modifying the user interface or machine-assistance models and mechanisms. However, as soon as we use insights from our analyses to change the way we are doing analysis, we must set aside our data and start afresh to collect data for subsequent analyses. We may, however, use insights or statistics from the set-aside data to inform our priors. This allows us to incorporate all applicable data gathered so far into our analyses and gain increased confidence in our estimates over time (Gelman 2004 ). 6 Conclusions and future work Machine assistance is vital to managing the cost of corpus annotation projects. Identifying effective forms of machine assistance through principled evaluation is particularly important and challenging in under-resourced domains and highly heterogeneous corpora, as the quality of machine assistance varies.

We performed a fine-grained evaluation of two machine-assistance techniques in the context of an under-resourced corpus annotation project. At first, this evaluation required a carefully controlled user study crafted to test a number of specific hypotheses. We showed that human annotators morphologically analyzing text in a Semitic language perform their task more accurately and quickly when even mediocre pre-annotations are provided. When pre-annotations were at least 70 % accurate, annotator speed and accuracy showed statistically significant relative improvements of 25 X 35 and 5 X 7 %, respectively. However, we argued that controlled user studies are too costly to be suitable for under-resourced corpus annotation projects.

Thus, we also presented an alternative analysis methodology that modeled the data as a combination of latent variables in a Bayesian framework. We showed that modeling the effects of interesting confounding factors could generate useful insights. In particular, correction propagation appeared to be most effective for our task when implemented without hyperlinks requiring annotator attention and interaction. More importantly, we demonstrated that by explicitly accounting for confounding variables, this approach has the potential to yield fine-grained evaluations using data collected in a natural environment outside of costly controlled user studies.

In Sect. 2 we pointed out that at a rate of 50 s per word, it would take approximately 80 annotation years to complete the SEC. The best machine assistance identified by the fine-grained analyses presented in this paper, 90 ? CP , shaves off around 20 s per word. If we are extremely optimistic and assume the availability of a 90 % accurate model over the entire corpus, our new estimate of 30 s per word still leaves us with approximately 50 annotator years. Clearly there is much work to be done if the small group of annotators working on the project is to be successful in a timely manner.

Our next steps are to begin work on the Syriac Electronic Corpus project using the insights we have gleaned so far and to apply latent variable analyses in an iterative fashion to the data collected during the course of annotation. Ideally, this will allow us to maintain an up-to-date understanding of the way that machine assistance interacts with annotator efficiency as annotators move from domain to domain within the corpus and as the user interface is iteratively improved. In particular, we are interested in extending the annotation task to involve linking tokens to entries in an online lexicon, and also in using active learning in order to direct annotators X  efforts towards promising areas of the corpus.

Because of the complexity of large-scale annotation projects, it will not be possible to systematically evaluate the methodology we have advocated in this paper as we attempt to apply it. However, there are established methodologies for qualitatively measuring the effectiveness of tools in enabling experts to accomplish complex tasks. Multi-dimensional In-depth Long-term Case Studies (MILCS) involve working in a close feedback loop with a group of participants over an extended period of time, and applying creative analyses to systematically gathered data in order to qualitatively determine what is working well and generate improvements to the system (Shneiderman and Plaisant 2006 ). We plan to apply this pragmatic approach to evaluating and improving the methods proposed in this paper. Appendix 1: RStan time model Appendix 2: RStan accuracy model Appendix 3: Derivation of complete conditionals This appendix walks through a mathematical derivation of the complete conditional distributions necessary to use Gibbs sampling to obtain samples from the posterior probability distributions over the unobserved parameters in the latent variable model of annotation time first presented in Sect. 5.3.1 .

Note that the following derivation is not required by those simply wishing to formulate and run a model such as that described in Sect. 5.3 using existing statistical computing libraries (see  X  X  Appendix 2  X  X  section). The following derivation is for those readers whose background may not be in statistics, but wish to acquaint themselves with the details required to implement own Gibbs sampler.
 Introduction A complete conditional distribution for a variable represents the probability of that variable given the priors, the data, and values for every other parameter in the model. These distributions are obtained by first calculating ln ( g ), the unnormalized joint posterior distribution. After it is defined, ln ( g ) is used as a basis for finding the complete conditional distributions over each parameter.

We first take a moment to summarize the model. We model the number of seconds taken to annotate a word y hatbro as a combination of the following variables: Variables  X  r 2 Variance common to all words  X  h h Annotators  X  a a Current condition  X  s t Grammatical Category  X  b b Bucketed word position (0,1,2,3 ? )  X  q r Hyperlinks clicked  X  x o Hyperlinks shown  X  j Offset common to all words Priors Prior justifications may be found in Sect. 5.3.1 . Gamma distributions are parameterized by shape and scale.  X  r 2 * Gamma (50,50)  X  h h N  X  0 ; 40  X  a a N  X  0 ; 40  X  s t N  X  0 ; 40  X  b b N  X  0 ; 40  X  q r N  X  0 ; 40  X  x o N  X  0 ; 40  X  j N  X  90 ; 50 Likelihood N ( h h ? a a ? s t ? b b ? q r ? x o ? j , r 2 ) Assuming that the probability of each data point is independent, the likelihood, or probability of the data set, may be written as the product of the probability of each data point.
 Joint posterior Our end goal is to estimate the joint posterior distribution over all of our parameters given the evidence provided by the data, p  X  H j y  X  , where H represents all of our parameters. Using Bayes X  rule, we write the posterior as a combination of the likelihood of the data and our prior probability distributions. denominator of this quantity involves integrating over all H  X  X  it is intractable to compute. Fortunately, using Gibbs sampling to get samples from the joint posterior does not require being able to compute the normalizing constant. We drop the constant and calculate the numerator of our joint posterior, which we will call g . Recall that because the denominator of the posterior is a constant, g is proportional to the posterior distribution. We will similarly drop any other constants we find as we derive g . Finally, because of machine precision issues when working with small probabilities, it is most useful to work directly with the logarithm of g . Now insert our own parameter names. Because our parameters are all independent of one another, we can write their joint prior probabilities as a product of individual prior probabilities. Distribute the logarithm. Now substitute the numerical form of the likelihood and priors. Distribute logarithms deeper into terms and drop additive constants. Complete conditionals Now it remains to derive the complete conditional distributions of each parameter. A complete conditional distribution over parameter H represents the probability of that parameter given the data and the value of every other parameter in the graph, and is necessary for Gibbs sampling to function correctly. It turns out that we can use g , which we have already calculated, to derive the complete conditional of each parameter simply by treating all variables except the parameter of interest as constants, and dropping as many of these constants as possible. Because g is not normalized, and complete conditionals are defined as proper probability distribu-tions, we symbolically add to each complete conditional the constant c that would correctly normalize it. However, this is only for form X  X  sake, since Gibbs sampling does not require normalized complete conditionals. Again, because of machine precision issues, we express our conditionals in log space. Because these distribution are not in the form of a known distribution that we can easily sample from, it is necessary to do Metropolis-Hastings within Gibbs sampling. For details on the Metropolis-Hastings algorithm, we refer the reader to Gelman ( 2004 ). r 2  X  X  49  X  X   X  X   X  X   X  X   X  X   X  X  j  X  X  References
 Abstract Machine assistance is vital to managing the cost of corpus annotation projects. Identifying effective forms of machine assistance through principled evaluation is particularly important and challenging in under-resourced domains and highly heterogeneous corpora, as the quality of machine assistance varies. We perform a fine-grained evaluation of two machine-assistance techniques in the context of an under-resourced corpus annotation project. This evaluation requires a carefully controlled user study crafted to test a number of specific hypotheses. We show that human annotators performing morphological analysis of text in a Semitic language perform their task significantly more accurately and quickly when even mediocre pre-annotations are provided. When pre-annotations are at least 70 % accurate, annotator speed and accuracy show statistically significant relative improvements of 25 X 35 and 5 X 7 %, respectively. However, controlled user studies are too costly to be suitable for under-resourced corpus annotation projects. Thus, we also present an alternative analysis methodology that models the data as a combination of latent variables in a Bayesian framework. We show that modeling the effects of interesting confounding factors can generate useful insights. In par-ticular, correction propagation appears to be most effective for our task when implemented with minimal user involvement. More importantly, by explicitly accounting for confounding variables, this approach has the potential to yield fine-grained evaluations using data collected in a natural environment outside of costly controlled user studies.
 Keywords Annotation Corpus annotation Machine assistance Syriac studies Bayesian data analysis User study Language resource evaluation 1 Introduction The current success and widespread use of data-driven techniques for processing human language make annotated corpora an essential language resource. Many popular natural language processing (NLP) algorithms require large amounts of high-quality (often human-annotated) training data in order to perform effectively. Also, text whose linguistic structure has been disambiguated with annotations is useful in its own right as a means of exploring and understanding the text itself. For example, one might use part-of-speech (POS) annotations and syntactic dependen-cies to study the diachronic prominence of ideas in a language [e.g., Smith et al. ( 2000 )].

There is an urgent need to produce more annotated corpora. Because data-driven disambiguation techniques learn to mimic patterns found in training data, they perform best when the data from which they learn are very similar to the data they must process. For example, an automatic grammatical tagger trained on hand-labeled news articles will likely perform well when used to automatically label similar articles but may perform poorly on transcribed spontaneous vocal utterances. Therefore, for best performance, it may be necessary to label corpora for each domain of interest. In addition, a growing number of linguistic tasks have been proposed including part-of-speech tagging, named entity recognition, constituent parsing, dependency parsing (Manning and Schu  X  tze 1999 ), supertagging and deep grammatical parsing (Bangalore and Joshi 1999 ), co-reference resolution (Soon et al. 2001 ), sentiment analysis (Pang and Lee 2008 ), information extraction (Cowie and Lehnert 1996 ), logical form extraction (Liang et al. 2013 ; Zettlemoyer and Collins 2005 ), and many more. Solving high-level language problems may involve integrating state-of-the-art solutions for many different NLP tasks, such as IBM X  X  recently publicized Watson project, designed to accomplish deep question answering (Ferrucci et al. 2010 ).

The number of annotated corpora required to adequately cover an interesting subset of the cross product of domains and tasks poses a problem even for highly resourced languages (e.g., English) and is far more daunting for the many under-resourced languages of the world. For the purposes of this paper, we call a language, task, or corpus  X  X  X nder-resourced X  X  if there there are strong constraints imposed on the amount of manual work that can be done. These constraints may arise from insufficient funding or from limits on the number of experts qualified to do the work. Because limited resources are available to annotate corpora in under-resourced domains, it is critical to find ways of reducing the cost of creating annotated corpora.

In Sect. 2 we discuss previous work that uses machines to assist humans in the work of annotation. In Sect. 3 we discuss the value of performing evaluations at multiple levels of machine assistance quality for under-resourced annotation projects, and discuss requirements for annotation tools that support this goal. Section 4 describes the large under-resourced annotation project that motivates this work. Section 5 evaluates the effects of pre-annotation and correction propagation on the morphological analysis of text in an under-resourced Semitic language. It presents a traditional analysis based on mean value comparisons, then discusses the infeasibility of extending this analysis technique to meet the needs of large under-resourced annotation projects, and finally presents a more scalable analysis approach using latent variables and a Bayesian data analysis framework. Section 6 outlines conclusions and future work. 2 Machine-assisted annotation Modern corpora are rapidly becoming too large to annotate by hand. When annotations are required, they are provided by automatic taggers trained on smaller hand-labeled corpora (Leech et al. 1994 ). The British National Corpus (BNC) contains over 100 million words of written and spoken British English (Leech et al. 1994 ). The Corpus of Contemporary American English (COCA) and the Corpus of Historical American English (COHA) contain approximately 425 and 400 million words, respectively (Davies 2009 , 2010 ). The Google Books Corpus contains over 500 billion words and comprises roughly 4 % of all of the books ever published (Michel et al. 2011 ). Even in under-resourced, domains, corpora are growing large. In Sect. 4 we introduce the Syriac Electronic Corpus (SEC), a corpus of classical Syriac, a dialect of Aramaic used since late antiquity by Middle Eastern Christians. The SEC will consist of approximately 50 million words. At a rate of 50 s per word (a generous estimate based on our experience with an expert annotator), this corpus will take approximately 80 dedicated annotation years to completely annotate. With only a limited community of experts in the world qualified to do this work, it is clear that this task, like many other annotation projects, will be impossible without a good deal of automatic assistance.

This section sketches general approaches that have been taken to reduce the cost of creating labeled corpora. It also points out some of the strengths and weaknesses of those approaches.

A popular way of avoiding the costs of a traditional labeling project is to enlist a large body of willing online participants. Although this approach is not strictly machine-assisted, it represents an important recent development in dealing with corpus annotation costs. A large body of work solves annotation problems using crowd-sourcing systems such as Amazon X  X  Mechanical Turk to collect noisy labels at a very low cost (Kittur et al. 2008 ). 1 Others have used cleverly constructed games to elicit labels from participants (Von Ahn and Dabbish 2008 ). Similarly, a labeled corpus may be constructed for free if one can discover data that has been implicitly labeled, such as movies that have been reviewed online (Pang et al. 2002 ). By enlisting such volunteers, corpus developers effectively turn what might have been under-resourced tasks in terms of funding into highly resourced tasks in terms of manpower. However, these techniques are not applicable to tasks which require a good deal of expertise (Dandapat et al. 2009 ).

When manual corpus creation is unavoidable, various flavors of machine assistance have been proposed to increase annotator speed and accuracy. Marcus et al. ( 1993 ) (famously, in the case of the Penn Treebank) and many others have used pre-annotation, presenting annotators with automatically labeled sentences so that they need merely correct errors rather than annotate from scratch. Kristjansson et al. ( 2004 ) proposed correction propagation as an extension to pre-annotation in which multi-part pre-annotations are dynamically revised whenever the human annotator corrects an erroneous portion of a pre-annotation. Active learning addresses the problem from the machine X  X  point of view: rather than assist annotators to work quickly, active learning aims to assist the automatic labeler to learn quickly by presenting the human annotator/expert with examples that are likely to be of the most value to the automatic labeler, relative to their cost (Settles 2010 ; Haertel et al. 2008a , b ). Higher-quality pre-annotations may, in turn, reduce annotation cost. In a similar vein, several researchers have explored ways of training a high-quality automatic annotator cheaply by allowing experts to inject expert knowledge into the model, thereby accelerating the training process (Liang et al. 2009 ; Chang et al. 2007 ; Druck et al. 2008 ; Ganchev et al. 2008 ; Naseem et al. 2010 ).

All machine-assistance techniques rely on the existence of an automatic helper of some kind, and, as previously noted, most state-of-the-art automatic learners rely on the existence of already labeled data. The result of this circular dependency is that machine assistance tends to be of a poor quality to begin with, slowly improving as labels are accumulated. If the quality is sufficiently poor, it is entirely possible that machine assistance could do more harm than good by distracting and antagonizing human annotators. For example, Barque et al. ( 2010 ) report that low-quality dictionary segmentation pre-annotations do not increase annotator speed at all. In addition, the best implementation of a given machine-assistance technique is not guaranteed to be the same for every linguistic task. For simple tasks like POS tagging, presenting the single most likely pre-annotation could be the most effective pre-annotation strategy (Marcus et al. 1993 ), while for tasks like parsing, it may be more effective to present the top k possibilities (Carter 1997 ; Oepen et al. 2002 ; Ganchev et al. 2007 ; Ringger et al. 2008 ). A crucial question, then X  X egarding machine-assistance techniques X  X s determining when and how they ought to be used for each linguistic task and domain. 3 Fine-grained evaluation of machine assistance With some exceptions (Fort and Sagot 2010 ; Alex et al. 2008 ), most work that we are aware of in evaluating machine assistance focuses on evaluating the highest possible assistance quality (Marcus et al. 1993 ; Ngai and Yarowsky 2000 ; Brants and Plaehn 2000 ; Chiou et al. 2001 ; Baldridge and Osborne 2004 ; Kristjansson et al. 2004 ; Tanaka et al. 2005 ; Ganchev et al. 2007 ; Tomanek et al. 2007 ; Rehbein et al. 2009 ). For highly resourced tasks and large, homogeneous domains, this approach is not unreasonable. When a model X  X  quality is plotted against the amount of data it was trained on, the resulting plot, or learning curve, usually grows quickly in the early stages and then slowly in the later stages. Therefore, if the quality of automatic assistance grows according to a typical learning curve, we would expect that during the majority of the annotation project annotators would have access to high-quality assistance.

However, we cannot assume we will enjoy such high-quality assistance in the context of under-resourced languages or a highly heterogeneous corpus. For one thing, most models suitable for machine assistance were developed against data from highly resourced languages. When we apply these models to a language with different characteristics, performance often suffers, resulting in learning curves that climb more slowly. Also, if a corpus contains portions with different characteristics, then models trained on one part of the corpus may perform poorly on other parts. For an example of such a corpus see the Syriac Electronic Corpus described in Sect. 4.1 . Developers of under-resourced and heterogeneous corpora must be concerned with the success of machine assistance at a variety of quality levels.

Broadly, this paper contributes to the field of labeled corpus development, especially in under-resourced domains, by presenting tools and methodologies for performing fine-grained evaluations of machine-assisted annotation. Such an evaluation equips corpus developers to determine at what quality levels a particular form of machine assistance warrants use in their domain. 3.1 Collecting data for fine-grained evaluation In order to analyze the effect of machine assistance on the performance of human annotators, it is necessary to keep detailed records of the annotators X  actions as they work. For example, we commonly wish to know how machine assistance affects annotation speed and/or accuracy. To answer this question, it is necessary to record the times and accuracies of annotators working under a variety of conditions. As currently existing annotation tools did not meet our needs, we created CCASH (Cost-Conscious Annotation Supervised by Humans) and made it available to the community under the AGPL license at http://ccash.sourceforge.net . Although Felt et al. ( 2010 ) have described CCASH in detail, we briefly restate here some of the features that make this tool suitable for evaluating machine-assistance tech-niques. CCASH is a web application written using the Google Web Toolkit, 2 which compiles to standard JavaScript on the client. Thus CCASH enables users running a variety of operating systems and browsers to collaborate on a shared annotation project. Both CCASH X  X  GUI and its data structures are customizable, allowing users to implement machine-assistance techniques that we have not considered. Most importantly, CCASH can record not just annotations, but also detailed timing sta-tistics describing the conditions under which each annotation was produced. This instrumentation is implemented as timing hooks in the client-side JavaScript code rather than on the server, which only has access to coarse-grained transaction information.

Development has been ongoing since CCASH was originally released, and there have been several important changes. Most notably, we have implemented a generic data structure consisting of interlinked trees of strings suitable for representing textual data and many kinds of annotations. The availability of a flexible, pre-implemented data structure reduces the burden on corpus developers who are not interested in defining and persisting their own data structures. In addition, we are actively redesigning our user management and permissions system in order to allow users to upload and administer their own data. Data owners will be able to allow other users to view, annotate, or administer various portions of their data in a fine-grained fashion. 3 3.2 A simple example: tag dictionaries For tasks with a limited vocabulary, machine assistance can be provided to annotators by using labeled data to build dictionaries of memorized annotations. For example, imagine a system that assists users to annotate words by prompting them correct annotation is in the prompt, this approach can save time. On the other hand, if the prompt does not contain the correct annotation, it costs the annotator extra time to read and then dismiss the prompt.

Carmen et al. ( 2010 ) used CCASH to test the effectiveness of annotation dictionaries in the context of English part-of-speech tagging. Thirty-three first-semester graduate students in linguistics annotated 18 sentences, assisted by tag dictionaries of different qualities for each sentence. Hypothesis testing showed that tag dictionaries significantly improve annotation speed without sacrificing accuracy when tag dictionaries contained at least one entry (not necessarily correct) for at least 60 % of the words in a sentence. This 60 %  X  X  X overage level X  X  represented a surprisingly low bar for machine assistance to be useful, especially given the simplicity of the approach.
 The sentences in this study (18 in all) were randomly selected from the Penn Treebank such that sentences were of three lengths: short (12 tokens), medium (23 tokens), and long (36 tokens); 6 sentences were selected per length bucket. Participants were presented the same sentences in the same order, this order having been pre-established randomly. However, the coverage level of the dictionary was randomized for each sentence presented to the participant, under the constraint that a given user be assigned a unique coverage level for each of the 6 sentences in every length bucket. This method ensured that each sentence was annotated at a maximum number of distinct coverage levels by different participants. Before beginning the study, participants were presented with a tutorial consisting of four randomly selected sentences. This helped mitigate the potential effects of the human learning curve and, in addition, it helped familiarize the participants with the user interface and the Penn Treebank tag set.

Although manual English part-of-speech tagging is a fairly well solved problem, this study serves to illustrate the basic process of conducting a simple fine-grained analysis. Detailed instrumentation data is collected by the annotation framework as human annotators complete a task assisted by machine models of varying qualities. performance is either held fixed or else evenly averaged over. In this scenario, the analysis may be conducted by simply comparing the average time and accuracy of annotators under each condition. 4 Case study: Syriac morphological analysis In this section we describe a large corpus annotation project and use it to motivate a fine-grained evaluation of the effects of machine assistance on its annotators. We extend the analysis of data described by Felt et al. ( 2012 ) in order to overcome some important limitations affecting our initial analysis of the data. 4.1 The Syriac electronic corpus Scholars at the Neal A. Maxwell Institute for Religious Scholarship at Brigham Young University and at the Oriental Institute at the University of Oxford are jointly working on a project called the Syriac Electronic Corpus (SEC) project, with the goal of creating a comprehensive, labeled corpus of classical Syriac. Classical Syriac ( X  kthobonoyo  X ) is an under-resourced Semitic language of the Christian Near East and a dialect of Aramaic. It is currently in use as a liturgical language but was a true spoken language up until the eighth century when it was largely supplanted by Arabic. Because many prolific and gifted authors wrote in classical Syriac, SEC texts include both prose and poetry and span more than a millennium. One goal of the SEC project is to annotate these texts with morphological analyses to facilitate systematic study of classical Syriac by historians, linguists, and language learners (Fig. 1 ).

Morphological analysis of Syriac is the process of segmenting a word into its constituent morphemes and labeling each with its grammatical function(s). For our purposes, the primary morpheme is the  X  X  X tem X  X , namely the remainder of the token after removing morphological suffixes and prefixes. The dictionary citation form (or  X  X  X aseform X  X ) and the root are identified from the stem. Although the task of Syriac morphological analysis meets the needs of the SEC and is interesting in its own linguistic structure prediction problem, and so inhabits a space that is the subject of a good deal of active research in NLP (Smith 2011 ). We anticipate that methods developed for this task may be more generally applicable.

In contrast to English, where searching for a few forms of a word or using simple query-expansion is often sufficient for discovering patterns reflecting the word X  X  usage and meaning, in Semitic languages search and discovery are not so straightforward. If we could search Syriac texts on citation forms or even on roots, we could search for and discover patterns as easily as in English; however, Semitic roots are altered significantly by expressive inflectional and derivational morpho-logical processes. Consequently, forms of any given Semitic root are numerous. As a result, searching for text written in languages such as Syriac, Hebrew, and Arabic is impaired since one must either limit one X  X  query to a single inflected form or use heuristics to expand the query, buying higher recall at the price of much lower precision.

A morphologically annotated digital corpus of a lesser studied language such as classical Syriac lends itself to search and therefore to careful study in a way that formerly only experts could attempt based on long years of familiarity. Such annotated corpora enable scholars to study and discover the contributions of and trends in historical documents. One outstanding example of such a corpus is the Dead Sea Scrolls Electronic Library (DSSEL), assembled by the Center for the Preservation of Ancient Religious Texts (CPART) in the Neal A. Maxwell Institute for Religious Scholarship at Brigham Young University (Tov and Reynolds 2006 ). The Bar Ilan University Responsa project 4 is another excellent example of an annotated corpus of historical texts. The Syriac Corpus will be an artifact of similar value: useful to linguists, Syriac students, and scholars of Syriac, the Near East, and Eastern Christianity.

Unfortunately, as we have already noted, creating annotated corpora can be extremely time consuming. The Way International Foundation, a Biblical research, teaching, and fellowship ministry, spent 15 years labeling the Syriac New Testament with morphological annotations (Kiraz 1994 ). The Syriac New Testa-ment consists of approximately 100,000 words. Similarly, two Syriac scholars recently required 18 months to hand label less than half of the Old Testament (Heal 2011, Personal communication). By contrast, the Syriac Corpus aims to encompass approximately 50,000,000 words, by the latest estimate. To achieve this goal in a timely manner it will be necessary to increase the speed of annotation.
 4.2 Pre-annotation and correction propagation The annotated Syriac New Testament provides sufficient training data to create reasonable machine assistance. We use Syromorph, a probabilistic morphological analyzer for Syriac developed by McClanahan et al. ( 2010 ), to provide pre-annotations. Syromorph consists of a joint pipeline of classification and transduction tasks. Each task in the pipeline relies on the data and on a set of the n -best hypotheses produced by tasks preceding the current task in the pipeline. Syromorph first segments each word into its parts: prefix, stem, and suffix. Syromorph then predicts a grammatical category and a baseform, or dictionary citation form, for the stem. Finally, Syromorph predicts the morphological attributes of the stem and suffix.

In order to apply correction propagation to Syriac morphological analysis, our model must be capable of constraining its predictions to match partial labelings. Kristjansson et al. ( 2004 ) propose a constrained Viterbi decoding algorithm for linear conditional random field models (CRFs). Because Syromorph is a pipelined sequence model, we adapt Kristjannson X  X  method of constrained decoding, with the difference that we use an n -best beam decoder instead of Viterbi decoding. 4.3 User study Although Syromorph performs well on SEC data that is similar to the New Testament on which it was trained, its performance suffers considerably when applied to text from different time periods and genres. Stylistic word-order differences between prose and poetry challenge a probabilistic sequence model like Syromorph that relies on consistent patterns among grammatical sequences. Also, classical Syriac literature was produced over a period of more than a millenium, and the vocabulary of later texts differs from that of early texts. Although Syromorph X  X  design includes the capability of handling novel tokens, it is necessarily more error-prone when analyzing previously unseen tokens. Also, despite a good deal of valuable work on the subject, there is as yet no globally accepted morphological annotation scheme in the Syriac Studies community. The initial stages of annotation have already involved over a dozen major and minor changes to the annotation scheme, each of which negatively impacts Syromorph X  X  performance as the target annotation scheme diverges from Syromorph X  X  training data. For these reasons it is imperative that we understand how Syromorph X  X  performance interacts with annotator efficiency at a variety of quality levels.

We designed a controlled user study using the CCASH annotation framework in order to conduct a fine-grained analysis of the effectiveness of pre-annotation and correction propagation applied to Syriac morphological analysis. The study is first described by Felt et al. ( 2012 ). We repeat here the details of the study in order to help readers fully understand the data dealt with in subsequent analyses.

This section will proceed as follows: Sect. 4.3.1 gives an overview of the user study layout. Section 4.3.2 describes the training and evaluation of the automatic annotation models used in the study; Sect. 4.3.3 explains our method of assigning experimental conditions to participants. Section 4.3.4 describes the user study participants; Sect. 4.3.5 describes the framework used to conduct the study and the study X  X  graphical user interface; Sect. 4.3.6 describes the data collected by the user study 4.3.1 User study overview We designed a web-mediated user study using CCASH, an open source web application framework for linguistic annotation tasks described in Sect. 3.1 . In the study, annotators took a survey, participated in an interactive training, and then worked through four practice sentences. After each practice sentence, participants received feedback on how their annotations differed from the annotation guidelines they were given. They were required to achieve a high level of accuracy on the final practice sentence before proceeding. Finally, participants annotated 30 sentences under a sequence of randomly assigned experimental conditions, explained in Sect. 4.3.3 . For each word in the study, CCASH recorded the time each annotator took as well as the number of correct and incorrect decisions he or she made.

The choice to have all participants annotate the same 30 sentences does not limit our ability to collect large amounts of data and identify statistical trends associated with different annotation conditions. It does limit the applicability of our results to new data; however, that is a problem inherent in any focused study.

A gold standard annotation was constructed by two expert Syriac linguists who completed the study, then discussed and resolved all disagreements in their annotations. It should be noted that annotated Syriac text already exists: The Syriac Peshitta New Testament has been labeled with morphological information (Kiraz 1994 ). However, reference copies of this data have been published which could bias the results of our study. Accordingly, the 30 sentences for the study were selected uniformly at random from The Acts of Judas Thomas, an apocryphal text that is similar, but not identical, to the New Testament (Wright 1871 ).

When constructing a gold standard, it is important to acknowledge that there are some inherently ambiguous cases on which even experts have difficulty agreeing (Klebanov and Beigman 2009 ). However, the disagreements between our experts indicated that only around 20 of the 1,289 decisions in the user study were ambiguous. This rate is low enough that it should not greatly affect our results. 4.3.2 Model training and metrics We trained Syromorph models on various random subsets of the Syriac New Testament data assembled by Kiraz ( 1994 ) and augmented with suffix data by McClanahan et al. ( 2010 ), consisting of approximately 100,000 labeled tokens. We calculated model accuracy against the 30 Judas Thomas sentences in the study X  X  gold standard. This slight mismatch between model training and test data caused model accuracy to suffer. Thus our most accurate model, trained on all of the New Testament data, achieved an accuracy of only slightly above 90 %. In order to obtain models with the target accuracies specified in Sect. 4.3.3 , we trained Syromorph on random subsets of the training data until a model was found which achieved the desired accuracy  X  0.01 %, measured against the gold standard.
In a structured annotation task like Syriac morphological analysis, accuracy can be calculated at the sentence level, the word level, or the decision level. These accuracy metrics are highly correlated, but not identical. Furthermore, since decisions can be partitioned into classes according to their sub-task, it is possible to calculate decision-level accuracy either as a macro-average or as a micro-average across decision types. A macro-average is computed by first averaging the decisions for a sub-task, then averaging the resulting averages. A micro-average is computed by averaging the decisions for all sub-tasks at once. Decision-level accuracy using a micro-average is an appropriate accuracy metric since it is computed over the exact set of choices that an annotator must make while annotating. All accuracies mentioned in this paper are decision-level micro-averages calculated against the 30 sentence gold standard set. 4.3.3 Experimental conditions Pre-annotations were supplied to annotators at the following accuracy levels: none , 25, 36, 47, 58, 68, 79, 90, and 100 %. In the none case, no pre-annotations were given. In the 100 % case, gold standard annotations were given. In all intermediate cases, Syromorph models trained to the indicated accuracy provided pre-annota-tions. The accuracy levels between 25 and 90 % inclusive were chosen to span the range of accuracies achievable by Syromorph trained on the Peshitta New Testament.

Additionally, participants annotated sentences both with and without the assistance of correction propagation. Note that correction propagation requires a model; consequently it could not be applied to the none or 100 % cases. In all, there were or 16 parameter combinations to test. We refer to each parameter combination as an experimental condition .

Experimental conditions were assigned to participants and sentences using the matrix in Table 1 where Participant1 was the first participant to take the study, Sent1 was the first sentence in the study, and cell values indicate a pre-annotation quality (25 X 100) and the optional presence of correction propagation ( ? CP).
This matrix was replicated and extended on the bottom in order to accommodate the 30 sentences of the study. That is, Sentence 17 was assigned to the same row as Sentence 1, and so on. This parameter assignment scheme has some desirable properties. It guarantees that annotators encounter each experimental condition roughly the same number of times because conditions are evenly distributed within each column. It also ensures that sentences are annotated under each condition roughly the same number of times because conditions are evenly distributed within each row.

However, this parameter assignment scheme has an important flaw: annotators encounter sentences of steadily increasing quality. Such an apparent trend might affect the way that annotators interact with the pre-annotations. This problem was resolved without sacrificing the desirable properties of the assignment matrix by first randomly permuting the rows of the matrix and afterwards the columns. Randomly permuting the rows scrambles the order in which participants encounter experimental conditions, but does not change which conditions are members of each column, ensuring that participants still encounter each condition roughly the same number of times (see Table 2 for a simple illustration). Row properties remain entirely unchanged. Permuting columns similarly leaves column and row distribu-tions unchanged. Annotators thus encountered the study X  X  sentences in a fixed order and under every experimental condition but without a discernible pattern.

It was expected that annotators would begin to annotate slowly then move more quickly as they grew accustomed to the task; this could potentially have a confounding effect on our timing data. We dealt with this human learning effect in two ways. First, the training and practice at the beginning of the study allowed participants to familiarize themselves with the task and user interface. Second, the parameter assignment scheme ensured that the sentences annotated under a given experimental condition would include approximately equal numbers of sentences annotated early and late in the annotation process.
 4.3.4 User study participants Nine Syriac experts, invited by colleagues associated with CPART and the Oriental Institute at the University of Oxford, successfully completed the study. Their answers to the survey at the beginning of the study indicated that all participants consider themselves reasonably proficient in Syriac and comfortable using computers. 4.3.5 Graphical user interface The graphical user interface used to conduct Syriac morphological analysis, implemented in CCASH, is an important part of this study since it affects annotation speed and also the applicability of this study to other tasks. The interface was refined in close consultation with Syriac experts to make sure it would be reasonably efficient for the study.

Annotators work through a sentence at a time. Half of the screen shows the sentence currently being annotated along with some surrounding context (Fig. 2 a). Note that the text reads from right to left. Annotators navigate from one word to another in the sentence either by clicking on the desired word or by holding down [ctrl] on the keyboard and navigating with the arrow keys. The word being annotated comes into focus in the right half of the screen. Within each word, annotators begin by identifying prefixes and suffixes using either mouse clicks or a keyboard shortcut (Fig. 2 b). Then a grammatical category is chosen (Fig. 2 c; in this case, NOUN), after which a set of stem and suffix tags appear (Fig. 2 d) that are applicable for the chosen segmentation and grammatical category. Annotators set tag values either by clicking on them with a mouse and selecting a value from the resulting drop-down list or else by typing them using a keyboard. For annotators who choose to type, the text is autocompleted for them based on the values that are applicable to that field. Finally, annotators may input Syriac text either by using their mouse to click keys on a virtual keyboard or by using their keyboard directly (Fig. 2 e).
 Once an annotator changes a field value, that field X  X  background changes color. When correction propagation is active, each time the annotator changes a field, the model is queried for a new prediction constrained by all of the decisions that the annotator has made so far in the sentence. In the scope of the word currently being annotated, if the new pre-annotation differs from the old pre-annotation, the new hyperlinks are clicked, their value is substituted into the corresponding field. For all other words in the sentence, pre-annotation values are updated in place without notifying the annotator or requiring any human intervention.

As annotators proceed, CCASH records detailed information about each word including accuracy, the time each element spent in focus, mouse clicks, and the number of keystrokes. To ensure that timing information is accurate, participants are instructed to press the pause button on the bottom left of Fig. 2 whenever they take a break. When the task is paused, the screen is also obscured.
 4.3.6 The data Although participants labeled a sentence at a time, it is problematic to do time analysis at the sentence level because the length of each sentence affects its annotation time, making times difficult to compare across sentences. Controlling sentence length could alleviate this problem but introduces a new problem since the length-controlled sentences are not representative of the data as a whole. We avoid these difficulties by conducting analysis at the word level.

To estimate word annotation times, we record the time that each word was in focus in the GUI. This time is not a perfect stand-in for the time an annotator spent actually working on each word, since it is possible for an annotator to consider a word that is not actually selected. Also, the first word of each sentence will naturally tend to be selected longer than other words in the sentence as an annotator orients herself by reading the sentence and context. However, given sufficient response data, these times should be an acceptable approximation for the true time spent annotating each word.

We compute word annotation accuracies by calculating the accuracy of the decisions applicable to the word, as explained in Sect. 4.3.2 .

The study X  X  9 participants each annotated 30 sentences, or 152 words, resulting in 1,368 word-level data points both for annotation time and accuracy. We subsequently filtered out 20 outliers leaving us with 1,348 data points. Outliers were defined as words that took longer than 5 minutes to annotate. Although it is conceivable that a tricky word could take more than 5 min of thought, it is more likely that annotators were distracted from the task during those times. This conjecture is supported by the fact that these outliers were particles X  X asy cases.
Since there are 16 experimental conditions, each condition has roughly 85 data points. Figure 3 uses standard box plots to summarize the data collected under each pre-annotation condition. Notice that for each condition there is considerable variance in both the accuracy of words annotated (Fig. 3 a, c) and the time required to annotate each word (Fig. 3 b, d). Notice also that the variance in accuracy diminishes with higher pre-annotation quality, and both the mean and variance of time diminish at higher pre-annotation qualities. In order to assess these visible trends, we require more careful analysis.

For the purposes of this analysis we treat all decisions as equal when calculating accuracy. However, it can be seen in Table 3 that some types of decisions are more difficult than others for humans and machines alike. Also, note that as Syromorph X  X  overall accuracy changes, its performance on the various decision types does not vary smoothly, but rather is an unpredictable function of the characteristics of Syromorph X  X  learning algorithm and the randomly selected set of data it was trained on (see Sect. 4.3.2 ). Clearly, overall decision accuracy is an imperfect measure of model quality. It is, however, a useful approximation, as the results of the analysis indicate. 5 Fine-grained evaluation In this section, we perform a fine-grained evaluation of pre-annotation correction propagation applied to Syriac morphological analysis using the data gathered in Sect. 4.3 , in order to identify the quality of machine assistance required to make these techniques effective. In Sect. 5.1 we use simple mean comparisons and hypothesis testing to answer this question. In Sect. 5.2 we discuss the practical limitations of 5.1 X  X  methodology in the context of the SEC project, and in Sect. 5.3 we investigate an alternative evaluation scenario with the potential to avoid costly user studies such as that described in Sect. 4.3 . In Sect. 5.4 we discuss the analysis requirements of large under-resourced annotation projects and argue that the approach introduced in Sect. 5.3 has the potential to fulfill these requirements. 5.1 Fine-grained evaluation using mean comparisons We expended considerable effort in Sect. 4.3 to control for confounding effects by either holding them fixed or uniformly averaging over them. This allows us to evaluate our variables of interest, annotator speed and accuracy, by simply comparing the mean values of these variables in subsets of the data. These differences may be tested for significance using null hypothesis tests. We posit three pairs of null hypotheses.
The first pair of null hypotheses is that annotator speed and accuracy are not significantly different for words annotated with and without pre-annotations. Testing these hypotheses at each of the eight pre-annotation accuracy levels indicates when the pre-annotation ought to be used.

The second pair of null hypotheses is that annotator speed and accuracy are not significantly different for words annotated without assistance and those annotated with the combination of pre-annotation and correction propagation. Testing this hypothesis at each pre-annotation accuracy level indicates when combined pre-annotation and correction propagation ought to be used.

The third pair of null hypotheses attempts to tease apart the effects of correction propagation and pre-annotation: assuming pre-annotations are being used, annotator speed and accuracy are not significantly different for words annotated with and without correction propagation. Testing this hypothesis at each pre-annotation accuracy level indicates when correction propagation ought to be used above and beyond pre-annotation.

Each null hypothesis is tested using both a standard two-sided Student X  X  t test as well as a permutation test (Menke and Martinez 2004 ). The Student X  X  t test is used since it is widely understood and used. A two-sided t test is appropriate since there is the possibility that accuracy and annotation time will either increase or decrease. The permutation test is used since it does not rely on assumptions about any underlying distribution. Note that with 48 null hypotheses being tested, we expect a few spurious rejections. This can be seen by recalling that if we draw two sets of data from the same process, we expect a standard t test with a p value threshold of 0.05 to incorrectly reject the null hypothesis one time in twenty. However, if pre-annotation and correction propagation do indeed improve annotator time or accuracy, there should be clear trends in the rejections.

Table 4 shows the difference between the mean annotator accuracies (a) and times (b) of words annotated under the control condition and of words annotated under the test condition at various levels of pre-annotation quality. Increases in accuracy are good and decreases in time are good. Removing outliers has little effect on the outcomes, so we leave them in for all analyses.

The first row of Table 4 a compares the accuracy of words annotated without the assistance of pre-annotations to that of words annotation with the assistance of pre-annotations. There is a clear block of significant results. It appears that pre-annotations generated by models of quality 70 % or higher increase average annotator accuracy by 5 X 7 % relative to the average annotator accuracy of 90 %, and that increase is usually greater than can be explained by the natural variance of the data. This is an encouraging result for those contemplating using pre-annotation on similar tasks. Although 70 % appears relatively high in the range of model accuracies that we have presented, it is actually quite low for a reasonable predictive model. That is, 70 % accurate models can be attained with relatively little data for most tasks (in our case roughly 50 annotated sentences), resulting in a low barrier to entry for those wishing to employ pre-annotation on similar tasks.

The second row in Table 4 a shows a similar positive trend for the combination of pre-annotation and correction propagation, but with weaker significance. It is unclear whether this trend is explained entirely by the presence of pre-annotation, or whether correction propagation is playing a role in helping or hurting accuracy. The third row of Table 4 a shows mixed signs with no statistical significance, preventing us from drawing any conclusions about the effect of correction propagation above and beyond that of pre-annotation.

The first row in Table 4 b shows the difference between the mean time required to label words with and without pre-annotations. Pre-annotations generated by models of quality 70 % or better decrease average word annotation time by 25 X 35 % relative to the average annotator speed of 53 s per word. Pending additional evidence to strengthen the outcome, it is reasonably clear that moderately good pre-annotation reduces the time required for annotation. The contribution of correction propagation is again unclear. 5.2 Limitations of mean value comparisons The approach described in Sect. 5.1 has some desirable properties. It is natural to compare average performance observed under competing treatments. This makes the values in Table 4 relatively easy to understand and interpret. Also, when data is collected in a carefully controlled experiment, it lends weight to the conclusions drawn from it. For these reasons, there is a large historical body of literature that relies on hypothesis testing to inform important decisions. However, this approach to analysis suffers from a number of practical limitations that prevent it from being usable in the context of an ambitious, resource-limited project such as the SEC.
Some minor limitations of the analysis methodology in Sect. 5.1 stem from the fact that we are using hypothesis tests. Traditional hypothesis testing has been criticized for lacking interpretability and leading to probabilistic paradoxes (Berger and Berry 1988 ). In Sect. 5.1 we looked for patterns in Table 4 and argued that we could use them to shape our beliefs about more general trends. Although this is a natural thing to do, hypothesis testing is equipped only to test specific hypotheses X  not to quantify belief, which is what we are actually interested in doing when we try to understand how machine assistance interacts with annotator efficiency. Also, the t test assumes normally distributed data, an assumption which is clearly violated by our accuracy data. More importantly for our purposes, the hypothesis testing framework does not, strictly speaking, allow us to use our data in order to test hypotheses beyond those which the experiment was originally designed to test. Since we have finely instrumented data, we would like to derive additional information from it, such as whether some kinds of words (e.g., nouns) are more difficult or time consuming than others.

The most important limitation of an analysis based on mean comparisons is that it relies on creating an environment in which confounding effects are artificially controlled for by being either fixed or distributed evenly across experimental conditions. This involves a good deal of planning, preparation, and wasted annotation effort (from the perspective of corpus annotators). For example, in order to control for the fact that some sentences are inherently more difficult than others, all annotators had to redundantly annotate the same thirty sentences. Although we expect the results in Table 4 to generalize to some degree, we would like to update this analysis as the SEC project annotation task and user interface changes over time. Moreover, we would like to evaluate other machine-assistance techniques, such as active learning or allowing annotators to specify expert knowledge (see Sect. 2 ). Annotators involved in the study expressed frustration at doing work that did not directly add annotations to the corpus, and in under-resourced domains such as the SEC, the pool of annotators qualified to participate in such a study is extremely limited. It is unlikely that we enjoy the resources or participation necessary to carry out repeated user studies in the style of Sect. 4.3 . We must be able to do these evaluations on-the-fly as corpus annotation proceeds, without any kind of artificial controls. 5.3 Fine-grained evaluation using a latent variable model One way of dealing with confounding factors is to explicitly model them. Although the values of confounding variables are not directly measurable, we can model them as latent explanatory variables and then estimate their values based on the way they affect the observed data. This goal may be accomplished using a number of frameworks. For reasons explained in Sect. 5.4 , we elect to use a Bayesian data analysis framework. Although space constraints prevent a detailed introduction to Bayesian data analysis, we sketch the idea here and refer the interested reader to the many excellent tutorials and references on the subject such as those by Barry ( 2011 ) or Gelman ( 2004 ).
Conducting a Bayesian data analysis involves proposing a probabilistic model over some partially observed system, and then using the laws of probability together with any available observations to derive probability distributions over the values of hidden variables of interest. The resulting information about hidden variable state is informed both by the expert knowledge built into the model by the analyst as well as by the available observed data.

Let X be a set of observed variables, or data, and Y be a set of parameters. The parameters, although their values are unknown, are hypothesized to have affected the production of the data. For example, if the data consist of error-prone heat measurements in a factory environment, the parameters might represent the unknowable true mean temperature, or the true temperature variance. Or if the data consisted of car insurance claims, one parameter of interest might represent individual driver recklessness X  X  quantity that is difficult to measure directly.
In many situations it is possible to define a function that assigns probability to values of X in a plausible way given value assignments to Y ,or p  X  X  X  x j Y  X  y  X  . This function is known as the data likelihood. The likelihood function encodes our knowledge about the way that the parameters affect the data.

The likelihood alone tells us nothing about the state of the parameters. However, if we use our domain knowledge about likely parameter settings in order to formulate p  X  Y  X  y  X  , known as the prior , then we can use Bayes X  law to calculate the true value of interest, namely, This function is known as the posterior probability distribution and calculates the probability of our parameter state in light of both the information in the observed data X as well as the assumptions built into our likelihood and prior.

Notice that the posterior is built from the product of the likelihood and the prior as well as an additional factor 1 p  X  X  X  that acts as a normalizing constant. In practice the normalizing constant is often intractable to calculate, preventing us from finding closed-form posteriors. However, a variety of algorithms exist in the literature (and are implemented in statistical computing libraries) that allow us to obtain samples from the posterior distribution without computing the normalizing constant. For the practitioner of Bayesian data analysis, then, the focus is not on math, but on modeling. For the posterior distribution to be accurate and helpful, both the likelihood and the priors must do a good job of encoding the practitioner X  X  expert domain knowledge about the system they are analyzing.

We walk through this process first for our time data in Sect. 5.3.1 , and then for our accuracy data in Sect. 5.3.2 . We discuss the results at some length in Sect. 5.3.3 . 5.3.1 Time model with latent variables We begin by proposing a probability distribution over our time data y . We model the number of seconds that it takes to annotate a word by starting with some base time j and then adding or subtracting from that time based on who is annotating, under what experimental condition they are annotating, what kind of word is being annotated, etc. If we finish by adding symmetric noise then the time required to annotate word y may be modeled using a normal distribution whose mean is the sum of the base annotation time j and values (negative or positive) associated with variables representing the conditions under which the word was annotated: y * N ( j ? ... , r 2 ). Note that a normal likelihood is not a perfect choice, since it allocates some probability to negative times, but it is convenient and works well in practice. We indicate which conditions apply to a particular y by indexing it with subscripts { h , a , t , b , r , o }. The subscripts on y index into the following unobserved random variables:  X  h h : The effect of the h th annotator. There are nine h variables, one for each  X  a a : The effect of the a th experimental condition. There are 16 a variables.  X  s t : The effect of the t th grammatical category (noun, verb, etc). There are 6 s  X  b b : The effect of the a word X  X  position within the sentence. We combine all word  X  q r : The effect of correction propagation hyperlinks being shown or not. There  X  x o : The effect of correction propagation hyperlinks being clicked or not. There
The time required to annotate a word y annotated under conditions hatbro is then distributed as and the likelihood of our data set is obtained by taking the product of the density of each observation.

It remains to specify our prior belief about the values of each latent random variable in the form of probability distributions. An expert annotating data outside of the user study took around 90 seconds per word and his times varied by as much as a minute. We use this, along with the knowledge that normal distributions allocate most of their mass to within about 3 standard deviations of the mean, to inform our priors over j and r 2 . j * N (90, 50/3), allowing our offset to vary as much as 50 s on either side. Because r 2 cannot be negative, we model it using a Gamma parameterized by shape and scale. If r 2 were equal to 2,500, that would allow annotation times to vary by about 3 reasonable that annotation times could vary by as much as a couple of minutes given the difficult nature of the task. We therefore let r 2 * Gamma (50,50) which has a mean value of 2,500 with a fair amount of spread, reflecting our uncertainty about this quantity.

We model the effect variables a ; h ; b ; s ; q ; and x as normally distributed around 0, so as to have no effect by default. We don X  X  have good intuitions about what effect each latent variable will have X  X hese are the variables we would like to learn X  X o we set their priors to be relatively uninformative normal distributions, N  X  0 ; 40 3  X  ,allowing each effect to potentially take on values between -40 and ? 40 seconds. Recall that negative effect values contribute to reducing the total annotation time of the words they affect, corresponding to increased annotation speed.

We now have specified everything we need in order to draw samples from posterior probability distributions over the unobserved random variables in light of the data and our priors. We implemented our own sampler and also tried RStan (Stan Development Team 2013 ), which we found to generate equivalent results far more efficiently and conveniently. RStan is one of several available statistical computing packages that allows practitioners to express models in a high level modeling language, abstracting away from mathematical details and reducing the likelihood of implementation errors. In addition, RStan assists with important details like eliminating the effects of poor sampling configurations during the early stages of the process (known as burn-in) and measuring sampling convergence. For simplicity X  X  sake, after validating that RStan X  X  automatic metrics coincided well with our own, we chose to trust them in all cases. The complete conditionals required to manually sample from this model may be found in  X  X  Appendix 3  X  X  section, and the RStan code we used may be found in  X  X  Appendix 1  X  X  section. 5.3.2 Accuracy model with latent variables We now model our word-level accuracy data x . Because accuracies take on values between 0 and 1 and we do not anticipate any multi-modality in our data, they can be modeled using a beta distribution. We reparameterize the beta distribution to use mean l and sample size v rather than the typical shape parameters a and b using the following substitution: a = l v and b = (1 -l ) v .
 constrained by 0 and 1, a sum of normals no longer makes sense. Instead we model l as an average of effects that are, themselves, beta distributions. In this way, effects that increase annotator accuracy will tend towards 1, and those that decrease annotator accuracy will tend to center at lower accuracies. The density of a single word X  X  accuracy is then where BetaMS is a beta distribution parameterized by mean and sample size.

The quantity j represents an average accuracy value. We expect that annotators tend to have high accuracies, so we let j * Beta (3,1.1). This reflects a relatively weak prior belief that we will see about 3 correct decisions per each incorrect decision. The 1.1 shapes the distribution to give very little probability to words that are entirely incorrect. The sample size parameter v represents the total number of decisions (correct or incorrect) that we expect to see per word. Simple words like particles involve few decisions and are numerous; more complex words like nouns and verbs can require as many as 8 or 10 decisions. We therefore let v * Gamma (2,2) parameterized by shape and scale, which spreads a fair amount of mass between 1 and about 12 or so.
 We model the effect variables a ; h ; b ; s ; q ; x with uninformed beta distributions, Beta (1,1). This gives equal probability to every value between 0 and 1, reflecting our lack of prior knowledge about what effect each condition will have on annotator accuracy. This will allow our posterior estimates to be shaped more by the data than model may be found in  X  X  Appendix 2  X  X  section. 5.3.3 Posteriors and discussion We used RStan to obtain 100,000 samples from the joint posterior distribution, where each sample specifies a value for each parameter in our model. These samples allow us to calculate posterior joint, marginal, and conditional probability distributions over unseen variables simply by aggregating over the subsets of values sampled for those variables.
Figures 4 and 5 show the posterior marginal probability densities we estimated for the a variables, indicating the effects of experimental conditions on the speed and accuracy of words. In all graphs the density corresponding to a 0 (the none condition) is plotted as a dashed line for comparison. In Fig. 4 , notice that as pre-annotation levels increase, curves shift farther and farther to the right. This indicates that the model learned that the presence of high-quality machine assistance is associated with annotations of higher accuracy. Similarly, in Fig. 5 curves shift farther and farther to the left as pre-annotation quality increases. This indicates that high-quality machine assistance is associated with smaller annotation times. When we compare conditions with and without correction propagation, it appears that more often than not the curves associated with correction propagation are shifted in the direction of being more helpful (right for accuracy and left for time). In order to assess these visible trends, we require more careful analysis.

We can quantify the differences between a 0  X  X he none condition variable X  X nd the other a variables in a table similar to Table 4 , which we used to summarize our mean comparison tests. In Table 5 we record the average difference between the values of each a variable and a 0 . The probability that a given variable is greater than a is calculated by counting the number of times that this is true in our samples. We use this probability rather than a p value to assess significance in this table. Note that although we choose two arbitrary probability thresholds, 0.9 and 0.95, to report significance, we have an informative probability value for every cell in Table 5 , whereas in Table 4 only the extreme p values are meaningful.
 It is encouraging to see that Table 5 looks similar to Table 4 for the most part. Because of the effort that went into controlling for confounding effects when the data was gathered, a good analysis should mostly agree with Table 4 . Strictly speaking, the two tables are not directly comparable. Table 4 contains differences between average data statistics, whereas Table 5 contains differences between average posterior variable values. Also Table 4 indicates significance using p values, whereas Table 5 indicates significance using probabilities. However, both tables tell the same story regarding pre-annotation: pre-annotations begin to help when the model accuracy approaches 70 %.

On the other hand, Tables 4 and 5 tell rather different stories regarding correction propagation (CP). Correction propagation appears far more promising in the latent variable analysis. We hypothesize that this is because our latent variable models include separate variables to account for when correction propagation was affecting the current word by showing hyperlinks, and when it was invisibly affecting other words in the sentence. When correction propagation was active but operating invisibly in the background, only a variables explain its effect. However, when hyperlinks appeared or were clicked as a part of correction propagation, the variables q and x compete to explain that word (see Sect. 5.3.1 ). Under this explanation, one could say that there were actually two different machine-assistance techniques operating under the name  X  X  X orrection propagation X  X  in the user study: hyperlink-driven CP and invisible CP. The effects of the two approaches X  X ne helpful and the other not X  X ere being confounded in the original analysis because we did not properly control for them.

We can validate this hypothesis somewhat by examining the posterior densities of our q and x variables in Fig. 6 . In our model, hyperlinks being shown and clicked is associated with degraded annotation time and accuracy. For accuracy (Fig. 6 a) this can be seen by observing that the densities in the left column, which are associated with hyperlinks not being shown or clicked, are smashed up against the right side of the plot, column, which are associated with hyperlinks being shown or clicked, are lower, indicating that they contribute to lower accuracies. Similarly, for time (Fig. 6 b) this can be seen by noticing that the densities associated with hyperlinks not being shown or clicked are pushed quite a bit farther left than their counterparts (nearly 20 s in the case of hyperlinks being shown), indicating that they contribute to reducing the time required to annotate a word. One explanation for the poor behavior associated with hyperlinks is that the appearance of hyperlinks is a visual distraction, causing annotators to second-guess themselves or pause in order to assimilate new visual information. Another related explanation is that hyperlinks require annotators to perform two tasks instead of just one: evaluating the initial annotation and also evaluating the corrected one. In either case, the analysis in Fig. 6 suggests that hyperlinks are an inappropriately expensive mechanism for implementing correction propagation, especially in light of the (at best) modest gains afforded by correction propagation.

The posterior densities of the s variables in Fig. 7 estimate the time contributions of different grammatical categories of words. Most of these values are unsurprising. We expected that particles would require little time to label and would tend to be labeled accurately. Similarly we expected that nouns and verbs would require more time to label and would tend to be labeled less accurately. However, the fact that numerals tend to be slow and inaccurate is surprising. Collaborating Syriac experts tell us that labeling numerals ought to be relatively straight-forward, so this data indicates that there is some ambiguity in the labeling documentation or interface that we can address to improve the efficiency of our annotators. Sifting through free-form participant feedback confirmed that participants did feel the need for improving numeral training specifications.

In the posterior densities over the b variables in Fig. 8 , the variable associated with being the first word in a sentence is centered over a high positive value, indicating that being the first word in a sentence tends to increase the time required to annotate the word by a considerable amount. This penalty is probably the result of annotators pausing to orient themselves within the sentence and context before beginning. We anticipated this effect when we designed our model, and modeling it should help account for some of the variance in the data. However, it was unclear before the analysis whether to expect the second and third words to participate in this  X  X  X earning curve X  X  effect. It appears that after the first word in the sentence, all other word positions incur approximately the same time cost.

The posterior densities over the h variables represent effects associated with particular annotators (Fig. 9 ). Notice that annotators who annotate very quickly also tend to be slightly less accurate, such as annotators two and nine . Conversely, some annotators who annotate slowly are very accurate, such as annotators four and five . 5.4 Iterative evaluation in a practical setting As we have defined them, under-resourced annotation projects lack the resources to be successful without judicious use of machine assistance. Our driving motivation is to equip corpus developers working on challenging and dynamic annotation projects to understand the effects of the machine-assistance method-ologies, allowing them to rapidly investigate numerous assistance strategies during the course of annotation. Accordingly, we require that evaluation methodologies support a set of minimal requirements that we anticipate will be necessary in such a scenario.

The annotation tool being used for production must, like CCASH, allow detailed annotation statistics to be constantly gathered. This way when changes are introduced to the user interface or new machine-assistance techniques are tried, data is automatically gathered under the new conditions, immediately available for analysis.

In a natural annotation environment, confounding factors are inevitably introduced into the data. Some annotators may contribute more data than others. Some data may contain different kinds of data than other settings (one author might prefer using difficult and obscure vocabulary). Annotations produced late at night or at the end of a long annotation session may be more prone to error. Bugs associated with particular browsers or versions of the user interface could adversely affect annotator speed.

Accordingly, successful evaluation of natural data must account for confounding factors. For example, suppose that some of the slower annotators had chosen not to annotate under the none case. Table 6 shows that mean comparison analysis fails badly in this case. In this analysis, pre-annotation appears extremely unpromising at almost all qualities, which we know is not the case.

Our latent variable model, on the other hand, does reasonably well. Although the numbers in Table 6 b are not identical to those in Table 5 b, they show the same trends. Since our model has a notion of the effect of annotator identity on annotation speed, it discounts its estimate of the effect of the none case, since it sees that only faster annotators participated in that case. In this way, latent variable models can account for confounding factors in annotator accuracy and timing data by instrumenting the confounding factors and including them as explicit variables in the analysis. Whether or not this approach is successful in practice will depend largely on whether attributes associated with confounding factors are captured in the data, and on whether the model X  X  structure allows the confounding effects to be estimated.

Finally, data is precious in practical under-resourced annotation projects. We require a mechanism for incorporating all of the data collected so far into analyses. We chose to conduct latent variable analysis in a Bayesian setting partly because Bayesian data analysis provides such a mechanism in the form of prior distributions. We note that a Bayesian approach also comes with some important theoretical constraints. Importantly, data that has had some influence on our model X  X  structure or prior may not be analyzed using the model they helped to shape. Procedurally, we may iterate between analyzing data and modifying the user interface or machine-assistance models and mechanisms. However, as soon as we use insights from our analyses to change the way we are doing analysis, we must set aside our data and start afresh to collect data for subsequent analyses. We may, however, use insights or statistics from the set-aside data to inform our priors. This allows us to incorporate all applicable data gathered so far into our analyses and gain increased confidence in our estimates over time (Gelman 2004 ). 6 Conclusions and future work Machine assistance is vital to managing the cost of corpus annotation projects. Identifying effective forms of machine assistance through principled evaluation is particularly important and challenging in under-resourced domains and highly heterogeneous corpora, as the quality of machine assistance varies.

We performed a fine-grained evaluation of two machine-assistance techniques in the context of an under-resourced corpus annotation project. At first, this evaluation required a carefully controlled user study crafted to test a number of specific hypotheses. We showed that human annotators morphologically analyzing text in a Semitic language perform their task more accurately and quickly when even mediocre pre-annotations are provided. When pre-annotations were at least 70 % accurate, annotator speed and accuracy showed statistically significant relative improvements of 25 X 35 and 5 X 7 %, respectively. However, we argued that controlled user studies are too costly to be suitable for under-resourced corpus annotation projects.

Thus, we also presented an alternative analysis methodology that modeled the data as a combination of latent variables in a Bayesian framework. We showed that modeling the effects of interesting confounding factors could generate useful insights. In particular, correction propagation appeared to be most effective for our task when implemented without hyperlinks requiring annotator attention and interaction. More importantly, we demonstrated that by explicitly accounting for confounding variables, this approach has the potential to yield fine-grained evaluations using data collected in a natural environment outside of costly controlled user studies.

In Sect. 2 we pointed out that at a rate of 50 s per word, it would take approximately 80 annotation years to complete the SEC. The best machine assistance identified by the fine-grained analyses presented in this paper, 90 ? CP , shaves off around 20 s per word. If we are extremely optimistic and assume the availability of a 90 % accurate model over the entire corpus, our new estimate of 30 s per word still leaves us with approximately 50 annotator years. Clearly there is much work to be done if the small group of annotators working on the project is to be successful in a timely manner.

Our next steps are to begin work on the Syriac Electronic Corpus project using the insights we have gleaned so far and to apply latent variable analyses in an iterative fashion to the data collected during the course of annotation. Ideally, this will allow us to maintain an up-to-date understanding of the way that machine assistance interacts with annotator efficiency as annotators move from domain to domain within the corpus and as the user interface is iteratively improved. In particular, we are interested in extending the annotation task to involve linking tokens to entries in an online lexicon, and also in using active learning in order to direct annotators X  efforts towards promising areas of the corpus.

Because of the complexity of large-scale annotation projects, it will not be possible to systematically evaluate the methodology we have advocated in this paper as we attempt to apply it. However, there are established methodologies for qualitatively measuring the effectiveness of tools in enabling experts to accomplish complex tasks. Multi-dimensional In-depth Long-term Case Studies (MILCS) involve working in a close feedback loop with a group of participants over an extended period of time, and applying creative analyses to systematically gathered data in order to qualitatively determine what is working well and generate improvements to the system (Shneiderman and Plaisant 2006 ). We plan to apply this pragmatic approach to evaluating and improving the methods proposed in this paper. Appendix 1: RStan time model Appendix 2: RStan accuracy model Appendix 3: Derivation of complete conditionals This appendix walks through a mathematical derivation of the complete conditional distributions necessary to use Gibbs sampling to obtain samples from the posterior probability distributions over the unobserved parameters in the latent variable model of annotation time first presented in Sect. 5.3.1 .

Note that the following derivation is not required by those simply wishing to formulate and run a model such as that described in Sect. 5.3 using existing statistical computing libraries (see  X  X  Appendix 2  X  X  section). The following derivation is for those readers whose background may not be in statistics, but wish to acquaint themselves with the details required to implement own Gibbs sampler.
 Introduction A complete conditional distribution for a variable represents the probability of that variable given the priors, the data, and values for every other parameter in the model. These distributions are obtained by first calculating ln ( g ), the unnormalized joint posterior distribution. After it is defined, ln ( g ) is used as a basis for finding the complete conditional distributions over each parameter.

We first take a moment to summarize the model. We model the number of seconds taken to annotate a word y hatbro as a combination of the following variables: Variables  X  r 2 Variance common to all words  X  h h Annotators  X  a a Current condition  X  s t Grammatical Category  X  b b Bucketed word position (0,1,2,3 ? )  X  q r Hyperlinks clicked  X  x o Hyperlinks shown  X  j Offset common to all words Priors Prior justifications may be found in Sect. 5.3.1 . Gamma distributions are parameterized by shape and scale.  X  r 2 * Gamma (50,50)  X  h h N  X  0 ; 40  X  a a N  X  0 ; 40  X  s t N  X  0 ; 40  X  b b N  X  0 ; 40  X  q r N  X  0 ; 40  X  x o N  X  0 ; 40  X  j N  X  90 ; 50 Likelihood N ( h h ? a a ? s t ? b b ? q r ? x o ? j , r 2 ) Assuming that the probability of each data point is independent, the likelihood, or probability of the data set, may be written as the product of the probability of each data point.
 Joint posterior Our end goal is to estimate the joint posterior distribution over all of our parameters given the evidence provided by the data, p  X  H j y  X  , where H represents all of our parameters. Using Bayes X  rule, we write the posterior as a combination of the likelihood of the data and our prior probability distributions. denominator of this quantity involves integrating over all H  X  X  it is intractable to compute. Fortunately, using Gibbs sampling to get samples from the joint posterior does not require being able to compute the normalizing constant. We drop the constant and calculate the numerator of our joint posterior, which we will call g . Recall that because the denominator of the posterior is a constant, g is proportional to the posterior distribution. We will similarly drop any other constants we find as we derive g . Finally, because of machine precision issues when working with small probabilities, it is most useful to work directly with the logarithm of g . Now insert our own parameter names. Because our parameters are all independent of one another, we can write their joint prior probabilities as a product of individual prior probabilities. Distribute the logarithm. Now substitute the numerical form of the likelihood and priors. Distribute logarithms deeper into terms and drop additive constants. Complete conditionals Now it remains to derive the complete conditional distributions of each parameter. A complete conditional distribution over parameter H represents the probability of that parameter given the data and the value of every other parameter in the graph, and is necessary for Gibbs sampling to function correctly. It turns out that we can use g , which we have already calculated, to derive the complete conditional of each parameter simply by treating all variables except the parameter of interest as constants, and dropping as many of these constants as possible. Because g is not normalized, and complete conditionals are defined as proper probability distribu-tions, we symbolically add to each complete conditional the constant c that would correctly normalize it. However, this is only for form X  X  sake, since Gibbs sampling does not require normalized complete conditionals. Again, because of machine precision issues, we express our conditionals in log space. Because these distribution are not in the form of a known distribution that we can easily sample from, it is necessary to do Metropolis-Hastings within Gibbs sampling. For details on the Metropolis-Hastings algorithm, we refer the reader to Gelman ( 2004 ). r 2  X  X  49  X  X   X  X   X  X   X  X   X  X   X  X  j  X  X  References
