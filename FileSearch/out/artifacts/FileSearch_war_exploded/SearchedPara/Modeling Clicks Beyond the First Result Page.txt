 Most modern web search engines yield a list of documents of a fixed length (usually 10) in response to a user query. The next ten search results are usually available in one click. These documents either replace the current result page or are appended to the end. Hence, in order to examine more documents than the first 10 the user needs to explicitly express her intention. Although clickthrough numbers are lower for documents on the second and later result pages, they still represent a noticeable amount of tra c.
We propose a modification of the Dynamic Bayesian Net-work (DBN) click model by explicitly including into the model the probability of transition between result pages. We show that our new click model can significantly better capture user behavior on the second and later result pages while giving the same performance on the first result page. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Click models; evaluation; user behavior
A result page of a modern search engine (whether com-mercial or not) usually has buttons leading to more results; Figure 1 shows an example of such buttons. There, the user can switch to the next result page either by using the page number (e.g.,  X  2  X ) or by clicking the  X  Next  X  button. We have a similar setup in our experiments.

By analyzing the click log of the Yandex search engine we learned that one third of all users uses the pagination buttons at least once a week. At the query level, with prob-ability 5 X 10%, a user will go to the second result page. This  X  Now at Google Switzerland.
 corporate mouse movement information into a click model and show that it leads to better click prediction. Background. For each query q we define a query session to be a set of actions performed by the user starting when she issued a query and continuing until she issues another query or abandons the search engine completely (e.g., by closing the browser window). The DBN model [1] considers the following binary events for every ranking position k :  X  E k  X  the user examines the k -th URL (result snippet)  X  A k  X  the user is attracted by the k -th URL  X  C k  X  the user clicks the k -th URL (observed variable)  X  S k  X  the user is satisfied by the k -th URL We use the Simplified DBN model described in [1, Section 5] and refer to it as SDBN. This model assumes that the user examines documents one by one starting from the first doc-ument ( E 1  X  1). She first examines the document snippet and if she has been attracted by the document ( A k  X  1), it receives a click ( C k  X  1). If, after being clicked, the result satisfies the user ( S k  X  1), the user stops examining docu-ments ( E j  X  0 for j  X  k ). Otherwise she proceeds to the next result. Formally, where a q p u k q is the attractiveness of the k -th document snip-pet and s q p u k q is the satisfaction probability for the k -th document. The a and s parameters are learned from the clicks using Algorithm 1 in [1]. By learning these parame-ters we can then estimate the document relevance: As was shown in [1, Section 4.3] this predicted relevance can be combined with a standard editorial-based o  X  ine learn-ing to rank approach to get better retrieval performance (a 2% gain in DCG score is reported). This combination with click model-based prediction becomes especially important for low ranks X  X hen using depth-K pooling [10] low-rank results may never be judged by human editors.
 Beyond the first result page. The most straightforward way of extending a click model beyond the first 10 result items is to simply apply the model to the entire list of doc-uments, not just the first 10. This model implicitly assumes that the user always clicks on the pagination button when-ever she is not satisfied/attracted by the current 10 docu-ments. Below, we use it as our baseline method.

The first model we propose is an extension of the SDBN model. Essentially, we introduce a fake  X  X ocument X  p cor-responding to a pagination button. This fake document is shared across all the result pages, i.e., model parameters for this document depend only on the user query q . Because this is not a real document we assign a di  X  erent meaning to the model parameters: the attractiveness a q p p q of the fake document p models the probability of not stopping after ex-
We also introduce a second model, SDBN(P-Q), which uses equations (1) X (6) for normal documents and (3), (4), (6), (8), (9) for the fake  X  X ocument X  p representing the pag-ination button: i.e., we assume that the attractiveness and  X  X atisfaction X  pa-rameters do not depend on the query (hence,  X  X inus Q  X  X n our notation SDBN(P-Q)) and are global constants.
In this section we compare our SDBN(P) and SDBN(P-Q) models to the original SDBN model. To train the model we use Algorithm 1 from [1]. We then apply the model and see how the adjustments due to pagination buttons correct the click probabilities of the documents below the top-10. The source code of our implementation is available at https: //github.com/varepsilon/clickmodels .

To compare click models we adopt the commonly used perplexity measure. For each document position k we put: where C j k is a binary value corresponding to an observed click on the k -th position 2 in the j -th query session, q j k is the predicted probability of a click on the k -th position in the j -th query session given the observed previous clicks (com-puted using Eqs. 1 through 6 for SDBN or corresponding equations for other models). The lower the perplexity of the model, the better. The perplexity of the ideal prediction is 1, while the perplexity of the random coin flip model is 2.
We tested the click models by looking at how well they can predict future events. We recorded the query sessions per-formed by a small percent of the users of the Yandex search engine during the first 11 days of February, 2013. We had 37,163,170 query sessions resulting in 3,378,470 sessions per day on average. We trained our model using day k and tested it using the next day k ` 1. The average perplexity values across ten train-test pairs are reported in Figure 3. Vertical bars correspond to the confidence intervals computed using the bootstrap method [5] with 1000 samples and 95% con-fidence level. We can see that the perplexity values of the models do not di  X  er for positions 1 through 10. However, we do see that the perplexity is much lower for SDBN(P) and SDBN(P-Q) for all positions below rank 10.

Another interesting observation is that we no longer ob-serve constantly decreasing perplexity for k  X  10. It may well be the case that the underlying cascade hypothesis ( X  X he user examines the documents one by one X ) needs to be re-vised for the second and later result pages. We leave this matter as future work.

Next, we contrast the perplexity gains of our two new models over the baseline. If we compute the average perplex-ity value for positions 1 through 40, then the SDBN(P-Q) and SDBN(P) models yield a 20 . 5% and 20 . 2% gain over the baseline, respectively. 3
Here we use continuous numbering, so for the first docu-ment of the second page k  X  11.
To compare the perplexity gain of model B over model A See, for example, [2, 7].

The quality of click prediction is still relatively low com-pared to the first page, so there is room for improvement. One might want to distinguish between the  X  Next  X  button and buttons with a page number (see Figure 1). Our internal log studies showed that less than 10% of the users switch-ing to the next page used both the  X  Next  X  button and page numbers during the one week period covered by our log sam-ple. Most of the users use page numbers (  X  70%), less than one third use the  X  Next  X  button. One could model these two events separately in order to improve performance.
One limitation of our work is that we are focused on a cascade-like DBN model. While the ranks of user clicks gen-erally appear in ascending order [1], out-of-order clicks are more prominently visible when analyzing pagination buttons  X  X sers often skip the next result page or return to the pre-viously viewed page (  X  10% of the users do it at least once a week, which forces us to ignore 13% of query sessions). It might be interesting to analyze such sessions in detail.
One direction for future work is to consider mobile and/or tablet search. The mobile/tablet version of a search result page usually has a di  X  erent design from the desktop version of the result page so as to facilitate user input. In particu-lar, the pagination button may look di  X  erently: sometimes mobile pagination buttons append new result items to the end of the same result page instead of switching to a new page. It would be interesting to check whether our models are applicable for mobile and tablet search interfaces.
Another future direction is an analysis of di  X  erent query classes and di  X  erent users. By having some prior informa-tion about the query and the user we can further refine the model by adjusting the probabilities related to the pagina-tion buttons. We can go even further and make all model parameters depend on the user.

Another interesting direction for future work is to link pagination button usage with search engine switching [9]. Use cases for the pagination button may be similar to the cases when users switch to another search provider (dissat-isfaction with the results, need for better coverage, etc.).
