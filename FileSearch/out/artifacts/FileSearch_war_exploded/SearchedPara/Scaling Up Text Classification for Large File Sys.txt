 We combine the speed and scalability of information retrieval with the generally superior classification accuracy offered by machine learning, yielding a two-phase text classifier that can scale to very large document corpora. We investigate the effect of different methods of formul ating the query from the training set, as well as varying the query size. In empirical tests on the Reuters RCV1 corpus of 806,000 documents, we find runtime was easily reduced by a factor of 27x, with a somewhat surprising gain in F-measure compared with traditional text classification. I.5.2 [ Pattern Recognition ]: Design Methodology X  classifier design and evaluation, feature evaluation and selection ; H.3.3 [Information Stor age and Retrieval] : Information Search and Retrieval X  query formulation, selection process. Algorithms, Performance, Experimentation. machine learning, text classifi cation, document categorization, information retrieval, enterpri se scalability, forensic search. Consider using a trained document classifier to search for  X  X elevant X  files from one X  X  pers onal file system X  X ypically containing hundreds of thousands of files X  X r from within an enterprise containing billions of files spread across distributed servers worldwide. For such scale, it would be nearly infeasible to pump every single file through the document classifier. Yet such scalability could become essential for future Information Lifecycle Management (ILM) appli cations seeking to verify, for example, that corporate retention and file protection policies are being followed for certain classes of confidential documents. Likewise, this scalability could be demanded by future e-discovery or forensic searches to find all files related to a legal matter. Note that in such applications, the objective is both precision and recall . This is in contrast to most information retrieval settings where only high precision in the top few search results is needed. Information retrieval methods ex cel in scalability, as evidenced by their success in web search engines. However, text classification via machine learning is generally called for if one needs to balance precision and recall, assuming a training set is available. Note that for large scale corpora, the time to train the classifier is dwarfed by the cumulative classification time. The computational workload of classification is linear in the number of documents to be classified: each document is fetched from disk, its text features extracted, and then the classifier makes its class prediction. Even were thousands of CPU cores cheaply available to classify documents in parallel, it would place tremendous bandwidth demands on the disks and the I/O paths. In this paper we improve the scalability of text classification by leveraging a full-text index over the corpus of documents. (The availability of such indices is becoming more common in personal and corporate file systems.) The basic concept is simple: we first use the index to quickly extract a small subset of documents that are potentially relevant, and then pass only these to the traditional text classifier. The workload of such a classifier is proportional to the size of the query hit list, yielding excellent speedup in the common case where only a small fraction of the documents are sought. This enables the system to scale up to very large document corpora. Our overall pur pose is to optimize the design choices appropriate for querying one or more file systems, each with its own static full-text index. Our research objective is to minimize runtime while maximizing F-measure X  X he harmonic average of precision and recall. The research questions include how to generate an effective query from the training set, how large a query is ideal, and how great is the savings in time vs. the tr adeoff in accuracy? Although we expected a tradeoff, it turns out that the two-phase process can be both much faster and more accurate than a single text classification pass over all the documents. For reproducibility, our experiments use publically available data and software. We use 140 classe s of the large Reuters RCV1 corpus [9] indexed by Apache Lucene software v2.2.0 [6], and classified via the Weka v3.4 linear Support Vector Machine (SVM) model [13]. We take care th at the operating system begins each timing experiment with a cold file cache, so that we accurately measure the performance of the whole system. A typical machine learning experiment conducts many runs in succession, but for this sort of in formation retrieval experiment, if one does not clear the cache, the pertinent data becomes cached in RAM, hiding the substantial co st of slow disk seeks. Section 2 describes the problem scope, and Section 3 describes a variety of design choices around our solution. Sections 4 and 5 summarize a suite of experime nts we performed. Section 7 discusses related work, and S ection 8 concludes and offers perspective on future work. For the applications we are interested in, the volume of classification workload dwarfs th e initial training time. This is especially so in view of recent breakthroughs in training state-of-the-art linear Support Vector M achine (SVM) models for text classification in near linear time, e.g. [7]. The rise of multi-core parallelism can aid with training time, but has little impact on the classification process, which is fundamentally I/O bound in fetching files for classification. We expect that the full-text indices have been previously constructed for other purposes, with no special attention to the classification labels or tasks for which we leverage them. Our research scope is limited to binary text classification tasks where the positive class of interest is usually rare, e.g. &lt;1% of the population. Such high class imbalance is often a difficult operating region for machine learning. In particular, if the positive training examples are too rare, the learning process may select a decision threshold that classifies all items as negative, optimizing its training accuracy under uncertainty. Common techniques used in such situati ons are to over-sample the positives in training or to under-sampl e the negatives [11]. At some point after indexing, a training set is provided with labeled positive and negative training examples from which to learn a classifier to select all relevant documents from the entire training set that represents a true random sample of the population of all files. For one thing, since positives are rare, asking a domain expert to provide binary labels to a stream of random samples is not an effective way to obtain a sufficiently large set of positive examples. If 1% of a random sample is positive, a domain expert would have to c onsider 10,000 randomly selected training cases to build up a se t of 100 positive examples X  X any fewer would likely under-represent th e diversity of positives. In practice, positive training cases are sometimes gathered by various ad hoc keyword searches, or have already been gathered in a directory by someone with an unknown, organic method. Considering this, the percenta ge of positives may be over-represented in the training set, but this is tantamount to under-sampling training negatives anyway [11]. Our approach consists of two pha ses: The first phase executes a query against a full-text index to determine a list of filenames that are likely positive. The second phase retrieves the file contents for each specified file, extracts its feature vector from the text content, and then classifies it. We list a number of design choices for each phase. Phase 1: First, what is the space of query terms that may be used? Most text classification research focuses on the universal bag-of-words representation, although it has been shown repeatedly that including phrases can help substantially. In phase 1, we may only query for terms that have been pr eviously indexed. Even so, most indexing packages provide the ability X  X t some additional overhead X  X o query for phrases. This gives us the flexibility to form our query from words only, or also to include phrases X  adjacent pairs of words. We experimented with both options. Figure 1. Block diagram of training and test phase where the design choices involved in each task are listed. Next, given the large number of terms in the training set, how shall we select the best terms? And how many terms Q should we include in the query? We vary Q widely (1 X 16K), and evaluate term goodness via Bi-Normal Separa tion (BNS, our default) or via Information Gain (IG, only where stated) X  X wo feature selection methods that have been shown to perform well [5]. We ignore terms that occur fewer than three times in the training set. The computation for BNS is simply |F -1 (tpr) -F represents the inverse cumulative Normal function from statistical tables, tpr represents the  X  X rue pos itive rate X  of the feature, i.e. what percentage of positives th e feature occurs in, and fpr represents the  X  X alse positive rate X  of the feature X  X hat percentage of negatives the feature occurs in. The computation for IG is better known and more involved: IG = H(pos,neg)  X  [P(ter m) H(tp,fp) + (1 X  X (term)) H(fn,tn)] where Next, we have a design choice for the form of the query. We experiment with the two logical choices: (1) a straightforward Boolean query, namely Lucene X  X  default disjunction chosen terms, or (2) a weighted query, where each term is associated with a real number. The latter amounts to a linear classifier, especially if we allow the weights to be negative and only select documents that end up having a positive score. This opens the issue of how to set the weights. To limit our scope, we assume X  X ot unreasonably X  X hat a linear SVM provides state-of-the-art classification accuracy [7]. Thus, to obtain the weights for Q terms (whether words only, or words and phrases), we filter the training set so that it contains only the Q best terms according to BNS or IG, train a linear SVM classifier Weka v3.4 SMO using Although Google, Microsoft and Yahoo! Search each default to a conjunction of terms, such a query focuses on precision to the exclusion of recall and would be unworkable for our phase 1. default parameters, and then extract its learned weight for each term. This will naturally include negative weights, e.g. for features whose presence is correla ted with the negative class. Our last option in phase 1 is wh ether to just focus on providing the best F-measure, or to try to bias toward higher recall and expect the phase 2 classifier to restore the precision to optimize our objective, F-measure. Phase 2: We have the same choices with respect to the feature set to provide for training the final classifier. That is, we can optionally include phrases, select via BNS or IG, and can control the number of classifier features C used to train the final classifier. Except where explicitly stated otherwise, our typical phase 2 classifier used C=16,384 features selected via BNS from among the set of words and two-wo rd phrases; these choices were selected after some preliminary experimentation. Because in phase 2 we have the complete file contents in memory, we can cheaply afford to use many features as far as it improves F-measure. More generally, phase 2 is not restricted to indexed features, so it could easily include other feature generators for improved accuracy, such as n-grams or domain-specific features. We leave this option for future work. A final consideration is what training set to use for the phase 2 classifier. It will only encounter the files that were classified positive by phase 1, correctly or in correctly. It seems proper then to train the phase 2 classifier only on labeled training cases that the phase 1 classifier finds positive. But it turns out this is impractical. There is a limited s upply of training data, and the phase 1 classifier mostly excludes the negative training examples. Hence, the phase 2 training set w ould have most of the original positives, but would only sometimes contain a handful of negatives. It would be impractical in our setting to perform phase 1, and then ask the user to label a new large set of negatives from the query hits. Hence, we are left with the simple option to train phase 2 on the full training set. For a publically available corpus that includes ground truth classification labels, we use Reuters RCV1 [9], which has 806,791 news articles in XML forma tted text files. We removed from all these files the metadata tags that reveal their true class labels, and saved this information in an isolated file. The average file size is 4 KB, which closely matches typical file systems historically [3]. We indexed all the Reuters files using the default Lucene text analyzer, which does not give any special consideration to the XML structure of the text. The indexing took just over two hours, including index optimization. Because indexing is slow, we did not consider rebuilding the index for Hence, the training positives are among the query hits found in each run. (In a real deployment, it may well be the case that the training examples are already included in the index of the local distributed file systems throughout an enterprise.) Despite training examples being incl uded among the query hits, we explicitly remove them before phase 2 and whenever we compute F-measure performance, in accord ance with accepted practice for measuring performance in machine learning research. Recall that F-measure is the harmonic average of precision and recall: 2*p*r/(p+r). It drops rapidly if either precision or recall is poor. Except where stated otherwise, each data point we show represents results averaged over a large set of separate classification tasks, i.e. macro-averaged. We selected all Reuters classes that have a prevalence &lt;= 5% positive overall and have over 1000 examples (500 for training and over 500 others to find). This leads to 140 classes in all, ranging from 1001 to 37,410 examples, 6854 on average (0.1% to 4.6% positive, averaging 0.8% positive). The classes in clude Reuters geography/country codes, industry codes, and topic codes. In each training set, we provide 500 positive examples and 5000 negative examples, selected at random just once from the ground truth labels. We want to be su re to provide enough training data to learn a decent classifier, so that we might avoid potentially useless  X  garbage-in, garbage-out X  results. That said, for many classes, decent discrimination could have been learned with fewer examples. Exploring these tradeoffs is outside the scope of this paper. The query-time benefits of our methods are largely independent of the size of the training set. Note that each training set has 9% positives, whereas the actual prevalence is typically ~1%. (We briefly tried training with 49,500 negatives to match 1% pos itive, but the Weka software crashed when it exhausted the 2 GB of available heap memory.) Instead of having a huge set of negative examples, we set the Weka instance weights such that the total weight of the positives amounts to 1% (alternately, some SVM implementations let one adjust the relative misclassification costs of positives vs. negatives). We do this for all phase 1 classifiers we train. Assuming the phase 1 classifier achieves decent precision, the phase 2 classifier should expect a much higher rate of positives. Thus, there is no need for weighting the training data for the phase 2 classifier. We conf irmed this experimentally. As mentioned in the introduction, it is important that we clear the file cache between tests, otherwise realistic disk delays are completely hidden. The ability for a user with root privileges to drop the file cache has recently been added to the Linux 2.6.16 kernel via  X  echo 3 &gt;/proc/sys/vm/drop_caches  X . (Even so, this novel capability is still buggy as of 2.6.18-8.el5 and causes CPU soft lockups occas ionally, requiring power-cycle reboots.) Specifically, we drop the cache before each query. We verified that without dropping the cache, we get wildly erroneous timings. Hardware: HP Proliant DL360 G3 server, with dual 2.8GHz Xeon CPUs and 4GB RAM. It has a locally attached disk: a 36.4 GB, 10K RPM Ultra320 SCSI disk with an HP SmartArray 5i controller. We actually used 20 such servers independently to complete the many experime nts involved; there was no communication or interference between them. Our first set of results present the main take-home message of this paper: that two-phase classifica tion greatly improves the speed as well as the final accuracy, compared to the baseline of simply testing every file with the (phase 2) classifier. Refer to Figure 2, which shows the overall F-measur e on the y-axis, and the total elapsed time on the x-axis. The elapsed time includes the time to run the query, fetch the file contents for each query hit (excluding training cases), and extract its text features. Each file is effectively classified with no additional time at the completion of its text feature extraction. On the far right, we see the baseline method took ~66 minutes on average, and achieved 0.545 F-measure averaged over all 140 classes. The baseline method consis ts of the phase 2 classifier (16,384 words and phrases selected via BNS) applied to every file except the 5500 training files. Since no phase 1 is involved, the baseline classifier includes reweighting the training positives, which brings up its precision and F-measure substantially. To the left in the graph, we see that Boolean queries of words alone (or words and phrases together) can greatly cut down on the number of documents to process in phase 2. The different points climbing up each of these curves correspond to Q=1, 2, 4, 8, 16, 32, or 64 query terms. With enough query terms, 100% recall is achieved on the positive class, and the phase 2 classifier obtains the same baseline F-measure, but 2 X 3 times faster. As Q increases, we see a rapid increase in the elapsed time: Once we have achieved 100% recall, additi onal query terms only serve to increase the number of false positives that need to be discarded by the phase 2 classifier. Lastly we note that because phrases are more specific, recalling fewer documents each, we see that more terms are required to achieve a given level of recall compared to the words only curve. The pair of curves furthest le ft indicates the greatly improved overall performance of using a weighted query for phase 1. The different points represent Q=16, 64, 256, 1024, or 4096 terms, as selected by BNS, and the weights are determined from a trained SVM. Despite the extremely large number of query terms to process, in most cases we see much improved speed vs. the Boolean query X  X he weights give the phase 1 classifier much better control to exclude negatives while selecting positives. This increased precision cuts down on the irrelevant files that must be retrieved for phase 2. Furthermore, this more accurate, weighted phase 1 classifier excludes some ne gatives that otherwise get past the phase 2 classifier. Because it eliminates some complementary negatives, the effect is that the two-phase classifier obtains higher precision overall, improving the fi nal F-measure average for all 140 classes. By using Q=1024 word &amp; phrase terms in the weighted query, the process comp letes in just 2.4 minutes on average X 27x faster than the ba seline X  X ith an F-measure 0.597 averaged over all 140 classes. Comparing these two curves, we see that including phrases consistently improves performance at any given number of query terms. This effect is known, although most text classification research is done with a si mple bag-of-words only. We made sure to extend Q far e nough to verify that having more terms is not always better. This is consistent with feature selection literature, which usually shows a benefit to limiting the number of terms, e.g. [5]. But note that the weighted classifier can benefit from many more terms than the Boolean classifier (Q=1024 vs. ~32). Having such a large number of query terms slows down the query, but yields a speedup overall because phase 1 is more discriminating in which files to fe tch for phase 2. As a result, large Q values lead to great savings in overall retrieval time. The best performing weighted query se tting is 3x faster than the best performing Boolean query setti ng while achieving 10% better F-measure (6 points). Next, we break down the elapsed time of the x-axis of Figure 2 into its constituent parts: the time taken (1) to query the index, (2) to fetch the files that satisfy the query, and (3) to analyze the file contents for specific text features and thereby obtain its final classification. These times corre spond to the three segments in each column of Figure 3. Boolean queries achieve nearly 100% recall with few query terms, and hence the query time is too small to see with respect to the total time. Boolean queries produce a large number of false positives, which leads to very high fetching and analyzing time. The fetching time averaged 12 ms per file, and the analysis time averaged 3 ms per file. Given that we end up fetching and analyzing thousands of files, it is relatively cheap to increase Q: an additional 7 ms per word on average, or 29 ms if we allow phrases. This relatively low incremental cost of adding terms opens an opportunity for weighted queries. They can be much more accurate, but they require significantly more query terms for good performance. The right half of Figure 3 shows weighted queries up to Q=1024 terms, where we begin to see the query time take a visually percep tible amount of the overall time. And because of their superior accuracy to Boolean queries, they waste much less time fetching and analyzing false positives. Figure 4 further illustrates this e ffect: For weighted queries there is a sharp increase in phase 1 recall rate with very large Q, and yet with very little increase in fals e positives. By contrast, Boolean bad precision. 
F-measure Figure 2. F-measure vs. elapse d time for various methods. Figure 3. Time taken by different methods for varying query sizes. The time columns have been segmented into querying, fetching and analyzing time . Overall F-measure for each setting is shown atop each bar. A natural question that arises at th is point is whether the complete classification task can be done more efficiently in a single phase 1 pass. That is, given the trained lin ear SVM text classifier to be applied to the entire corpus, extr act its weights and execute it only on the search engine, with no follow-up phase to further test the files. This basic idea has been tried [1], but not compared to the baseline, nor to two-phase classifi cation. Note that the phase 2 classifier does not pay a time pena lty for having a large number of features: the file fetch time and the feature extraction time depend on the disk performance and the file size, not on the number of terms to be extracted for classification . In our experiments, we found that 16K word and phrase terms was superior for the phase 2 classifier. But executing a query with such a large number of terms would pay a significant time penalty. In fact, we conducted this experiment and found it took 450 seconds, averaged over the 140 classes. This is 3x slower than using our two-phase system with Q=1024 in the first phase a nd 16K terms in the second. With such a large Q, the query time greatly exceeds the time it would take to fetch the few likely positive files and classify them. The two-phase classifier performs a balancing act in terms of the querying time vs. fetching and analyzing time. But besides time, there is a further disadvantage to running a single, high-dimensional classi fication on the search engine: recall the baseline classifier did not achieve as good F-measure as the two-phase system. We discuss this effect next. Cascaded classifiers have been used extensively in face detection from images where there is a huge computational cost involved in determining for every window in an image whether it contains a face or not [12][10]. The computational load is overcome by cascading several classifiers, where the complexity of classifiers increases as we go further down the cascade. The first few classifiers of the cascade, which are very cheap, help in removing most of the  X  X asy X  negatives, and the more accurate, complex classifiers at the end of the cascade polish up with excellent discrimination, yielding good overa ll performance. Our approach in this work is similar in spir it, where we additionally use the index to quicken the early classi fication phase. Figure 5 illustrates the impact of our two-phase cla ssification scheme in terms of F-measure, precision and recall. Th e x-axis varies the number of word+phase terms used, while the number of features in the final classifier is 16K, which yielded excellent performance on average. Two-phase classification has the consistent effect of improving precision and lowering recall. This is natural, since a case will be classified negative if either classifier rejects it. The improvement in precision is genera lly more than the decrease in recall, which is reflected in the overall increase in F-measure. This can be attributed to the low correlation of classification errors of the two classifiers in the cascade which has been well studied in [8]. The average F-measure is highest at Q=1024 query terms. Note that at the far right point, where Q matches the number of terms in the final classifier, that the two-phase computing the same function in different ways, and the final decisions match that of a traditi onal, one phase classifier. The benefit of the two phases only ha ppens when the two classifiers have a somewhat different persp ective on the training data. We have run additional experiments ( not shown, but we could if the reviewers request) that vary the num ber of features in phase 1 and phase 2 independently, and they find a consistent plummet in F-measure whenever the number of features matches. Figure 6 illustrates the two-phase cl assifier effect in terms of average F-measure over different groups of Reuters categories. The plots indicate a consistent F-measure improvement across all categories by using the two-phase classifier. Figure 6 also illustrates the F-measure variati on based on the query term count Q. For the country based cate gories, good F-measure performance is obtained with just 64 query terms. On the other hand, the industry and the economy categories require a lot more query terms, around 1024 to achieve good F-measure. The impact of the two-phase classifier is also more pronounced in these difficult cases. All the experimental results shown so far present the F-measure averaged over the 140 classes (or some subset) for different parameter settings (Q and C). In a real-world setting, we are interested in picking parameters that maximize the F-measure for the single class at hand. We a dopt a dynamic scheme using the cross-validation performance m easures obtained during the learning phase. The cross-validation results of performance measures such as F-measure, precision and recall can be used to devise policies for identifying good parameter settings for each phase separately. We limit Q to be a power of 2 &lt;= 1024 terms, in order to avoid inordinate que ry time. Table 1 shows the F-measure obtained for some of the top performing policies. The 
Recall (tp rate) 40 50 60 70 80 90
Figure 5. Effect of two-phase classification on each performance measure. results indicate that the best performing cross-validation based method is very similar to an oracle having access to the F-measure test set averaged over all 140 classes. Oracle I has access to the F-measures on test data for each class and hence, picks a parameter setting that maximizes the F-measure for each class. Clearly, this represents the highest achievable F-measure in this setting. Oracle II has access to the F-measure averaged over all classes for all possible settings. In Section 3 we described a palette of design choices for the phase 1 and phase 2 classifiers. Here we briefly present their effect. Table 1: Comparison of F-me asure obtained through cross-validation based policies to choose parameters compared with two different oracle methods.
 Weighting Instances to Approximate the Test Distribution: A practical problem that arises in machine learning is one of changing class distributions from training to test phase, although it is typically avoided in most machine learning research [4]. As mentioned earlier, this problem ar ises in our problem setting as well. The class distribution of th e test data of the first phase classifier is different from its tr aining set. We adopt the approach of weighting instances appropriate ly to overcome the difference. While training the first phase classifiers, we weight the instances in such a way that the total weight of the positive training set is 1% X  X  rough estimate of the prevalence we expect for typical searches. (It would give an unfair advantage to determine the exact prevalence of positives in the test set and then set the weight exactly. Yet, this might be estimated via a quantifier [4].) The positive effect of such a weighti ng on the first phase F-measure is shown in Figure 7. On deeper inspection, the reweighting is consistently improving the precisi on of the classifier, naturally. Phase 1 Performance: Figure 8 shows the F-measure of th e first phase alone, as we vary the cross-product of the design choices. For clarity of presentation , we hold Q=1024 fixed X  X  reasonably good choice outperformed IG, consistent with past studies [5], (b) weighting the training positives to 1% significantly outperformed using the more balanced training set provide d (right vs. left), and (c) the addition of phrases to the availa ble terms improves performance. A paired t-test indicates very st rong statistical significance, even between the two closest points (B NS vs. IG with phrases and without 1% weighting). In addition to these differences, we also analyzed the effect on query time (not depicted). We f ound that IG queries consistently took longer than BNS queries, e.g. 2x slower. This is because IG tends to select terms that are more common and therefore have longer posting lists to process: 2.9x longer on average. The preference of BNS for rarer features is known [5], but here we have exposed a side benefit: it se lects terms with shorter posting lists that are speedier information retrieval queries (as the research of [2] sought to do with IG-hybrid methods that somewhat preferred terms with shorter posting lists). Phase 2 Performance: These results so far only indicate the F-measure of phase 1. One might reasonably wonder whether these differences are mirrored by the final F-measure after both phases. Figure 9 presents the final F-measure as we vary the cross-product of the design choices (holding fixed Q=1024 query terms and C=16K classifier terms). Each design choice is ke pt consistent between phase 1 Figure 6. Average F-measure for different Reuters categories Figure 7. Weighting instances improves phase 1 F-measure. and phase 2. (One could conceive of the cross product of phase 1 and phase 2 design choices separa tely, but such high dimensional results are expensive to compute and difficult to present.) The leading design choice (BNS +phras es and 1% weighted) continues to dominate, but the picture has changed somewhat. All the F-measures have improved vs. the F-measures of phase 1 in Figure 8. The superiority of each design choice for phase 1 is now less pronounced, but each is still present. One design choice we discussed ear ly on was to focus the phase 1 query on high recall, and expect the phase 2 classifier to increase the precision to the point that we obtain optimal F-measure. We experimented with such a strategy by leaving off the 1% weighting of the positive training examples; the 9% prevalence of positives in the training set therefore make the phase 1 classifier less conservative, and it yields higher recall. But then we found the phase 2 classifier was not able to bring the precision as high. We found that the highest average F-measure obtained was 57.9%, which is inferior compared to the F-measure obtained in the weighted case. We have shown in this work that usage of phrases improves the F-representation. Clearly, there are other features X  X uch as n-grams, file metadata, file-format-sens itive features (e.g. stripping XML before indexing), and domain-specific features X  X hich could potentially aid in improving classification performance and can be added to the index. On the other hand, there are cases where the file needs to be explicitly fetched. Consider a case, where someone is searching for articles about XML on their disk, or info about a merger with XML Inc. co mpany. The index, being made with a generic bag of words parser that does not first strip XML, would have XML tags in every single Reuters article. Clearly, a query working on the index would do badly in terms of precision and this scenario furthers th e case for having a second phase classifier which operates on file-format specific features, e.g. parse the XML and make features from the non-tag text. Typical search engines today limit the number of query terms allowed to 20 or 30 maximum. Viewed from their perspective, they want to limit the resource c onsumption of customer queries. But as businesses begin to get greater value from their (central or someday, federated) search infrastructure via text classification queries, there will be business jus tification to raise term limits into the hundreds or thousands. This leads to a qualitatively different operating region for sear ch engines, and we may see a substantial increase in computer resources used for text classification searches. There is a single prior work in the literature that is highly relevant. Anagnostopoulos, Broder, and Punera [1] considered a classifier executed via a weighted search engine query, which they refer to as Weak AND (WAND). This is equivalent to our weighted queries of phase 1 alone . Their concern was with using a search engine for classificati on, with no phase 2 follow-up to improve accuracy. Our experiments in section 5.1 found that operating entirely on the index took 3x longer than if we included a separate phase 2. Even with respect to phase 1 alone, there are a number of dimensions in wh ich our work adds further contributions: They only consider a bag-of-words model, where we also measure the benefit of phrases, which search engines easily facilitate. They only consider up to Q=100 terms, where we extend to Q=16,384. They did not consider BNS, which we found superior to IG. Their results are reported in terms of area under the ROC curve, which is mainly insensitive to improvements in F-measure when positives are very ra re, e.g. 1%. And significantly, they did not run their experiments with a cold cache, so their reported results do not take disk performance into account. Additionally, we address the cla ss distribution difference between the training set and test set us ing weighting of training data instances and we show signifi cant improvement in F-measure performance. They consider the nove l angle of biasing the feature selection according to the length of the term posting lists, in order to reduce processing time. techniques for speeding up a traditional information retrieval query. Their experiments showed substantial speedup for queries averaging 2.46 to 7 terms, suffering little loss in precision. Coincidentally, they also use two phases, but the similarity is superficial. Their first phase re trieves the posting lists of the more important terms (as determ ined by their method), and the second phase retrieves all the rema ining terms, but tracking scores only for documents that scored well in the first phase. Note that the work does not address classifi cation, but their technique could be leveraged to speed up our phase 1 query on the index with Q  X  1024. Our current software simp ly scores every document that is mentioned by the posting lists. 40 45 50 55 60 F-measure 40 45 50 55 60 F-measure Our goal was to make it text cla ssification scalable enough to scan distributed enterprise file system s for relevant documents. Ideally such an application would also include infrastructure to efficiently federate the search over many full-text indices throughout a corporation. In a real-world deployment, the size of the datasets would be much larger than the publicly available Reuters RCV1 collection, which occupies only 3.5 GB of disk space, and whose Lucene index occupies only ~500 MB of disk space. The index is small enough to fit entirely in RAM, although by dropping the file cache, we ensured that we were measuring realistic disk delays. For real-world usage, the index would be much larger and the reverse posting lists would be longe r. While this suggests longer query times and perhaps a desire to reduce the query size Q to save time, keep in mind that with a larger, distributed infrastructure also comes mu ch longer latency and lower bandwidth to fetch the actual files for phase 2. Hence, if anything, we anticipate the time a nd workload savings of our two-phase technique to be proportionate ly more important for larger environments. The World Wide Web is an extreme thereof X  X  query to a fast, central index server such as Google can efficiently provide a list of relevant docum ents, but actually fetching them for a second phase test will be very slow. Moreover, on a large scale it is likely to suffer from web servers being unavailable and intermittently poor network performance. In view of this, text classification on the web may be st be served by extensive computation with the reverse i ndices and perhaps Google X  X  large file cache. In the end, it may still be important to fetch and check the final list of hits, since URLs often become stale when web pages are deleted. Also, a web page may have changed substantially since its indexing, and no longer match the matching criterion. An interesting direction of future research would be to devise schemes for using an index in an active learning setting. In the real-world where training data is so scarce, active learning plays a crucial part in acquiring labele d data through an oracle who has time constraints. In such a time constrained setting, it is crucial that the processing to identify useful training examples for labeling has to be done quickly. The index can be exploited to quicken the processing step, which normally involves identifying the most informative instance(s) to be labeled towards improving the current classifier. In this paper, we excluded the training time from interest, for we are targeting settings where it is dwarfed by the volume to classify. But with active learning, this training time goes into the user X  X  interactive loop. The rising multi-core revolution can decimate this training time, as well as the time to analyze file content for text features. Ho wever, it does not address the primary bottleneck for this application: disk seek time involved in executing queries and fetching files. In fact, the relative speed between CPU and disk is becoming larger, so the techniques described in this paper shoul d continue to be relevant. Considering the enormous interest in ranking problems, a related research direction would be to analyze the impact of two-phase processing through an index in the context of ranking. In our experiments, we noticed a consis tent improvement in precision in the top 20 from first phase to second phase. It would be interesting to analyze the performance of two-phase ranking schemes on rigorous ranking accuracy measures. Our thanks to Hernan Laffitte and Eric Anderson for their invaluable support in setting up racks of machines with the updated Linux kernel. [1] Anagnostopoulos, A., Broder, A. Z., and Punera, K. 2006. [2] Broder, A. Z., Carmel, D., Herscovici, M., Soffer, A., and [3] Douceur, J. R. and Bolosky, W. J. 1999. A large-scale study [4] Forman, G. 2006. Quantifying trends accurately despite [5] Forman, G. 2003. An extensive empirical study of feature [6] Hatcher, E. and Gospodnetic, O. 2004 Lucene in Action (In [7] Joachims, T. 2006. Training lin ear SVMs in linear time. In [8] Kittler, J., Hatef, M., Duin, R. P. W., and Matas, J. 1998. On [9] Lewis, D. D.; Yang, Y.; Rose , T.; and Li, F. 2004. RCV1: a [10] Luo, H. 2005. Optimization de sign of cascaded classifiers. [11] Van Hulse, J., Khoshgoftaar, T. M., and Napolitano, A. [12] Viola, P. and Jones, M. J. 2002. Robust real-time object [13] Witten, I. and Frank, E. 2005. Data Mining: Practical 
