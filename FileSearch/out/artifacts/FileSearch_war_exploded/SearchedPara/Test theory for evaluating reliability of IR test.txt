 1. Introduction
Information retrieval (IR) research has an impressive tradition of knowledge accumulation, in which high-performing methods are recognized and disseminated throughout the research community. A key ingredient in this impressive tradition is the availability of standard test collections and tasks. If an algorithm performs well on a test collection, it may imply that the model it implements will perform well in some more general sense.
But in order to make that inference, the test must meet the twin challenges of internal reliability and external validity. External validity is the extent to which that performance would extend to other testing environments of interest, e.g. other tasks and data. A typical example of the question of a test collection X  X  external validity is
Soboroff X  X  work  X  X  X o TREC Web Collections Look Like the Web?  X  ( Soboroff, 2002 ). The work presented here focuses instead on the issue of internal reliability, which we will refer to simply as  X  X  X eliability  X  . Reliability is the extent to which a result reflects a real difference that is not due to chance. We note that the question of document  X  X  X ooling  X  is another issue that has received much attention in the literature on IR evaluation.
Although the connection to reliability and validity is not always explicit in that literature, the implicit goal of pooling is to achieve a good level of test reliability and validity while limiting the number-and expense-of manual relevance judgments.

The question of reliability can be addressed at the level of an individual performance comparison, or at the level of a whole test collection. For example, when an individual researcher finds that his/her model out-per-forms a baseline, he/she typically invokes a statistical test to show that the result is statistically significant and not due to chance ( Hull, 1993 ). Our focus is not on the reliability of a given performance comparison, but the reliability of a whole test collection. We address the question, how do we define and measure the reliability of a test collection as a whole? The importance of the question is due to the fact that the accumulation of results in IR depends on the reliability of the test collections.

Our point of departure is the observation that the IR literature makes almost no use of Test Theory, which is a set of statistical tools specifically designed to rigorously define and measure the reliability of a test collec-tion. Test Theory is in widespread use in the field of educational testing and in social sciences in general. It is used to measure the reliability of existing standardized tests, and to help plan the design of new ones. The basic insight of this paper is that the testing of algorithms on a set of test queries, is comparable to the testing of (human) students on a set of exam questions. Once this comparison is made, Test Theory suggests itself as a primary contender for the definition and calculation of the reliability of a test collection. There are many benefits from using Test Theory to assess test reliability. These benefits will become clear after we have reviewed the methods in current use and presented Test Theory as it applies to IR.
 This paper is divided into two main parts. In the first part, which includes Sections 1 X 4 , we introduce Test
Theory, show how it applies to Information Retrieval test collections, and report specific results for a number of test collections and test designs. In the separable second part, which includes Sections 5,6 , we explore two more advanced topics. Section 5 shows how to use Test Theory to consider the effect of documents, in addition to the effect of queries and assessors that we will analyze in Sections 1 X 4 . Section 6 is more conceptual. It uses concepts from Generalizability Theory to illuminate a long-standing (previously implicit) problem in evalua-tion research, regarding how to model the effect of assessors on test reliability. This discussion clarifies the meaning of our own reliability results from Sections 1 X 4 as they relate to the effect of assessors, as well as clar-ifying the meaning previous research results.

The contributions of this paper are: (1) presentation of Test Theory and its proven set of tools that can be used to measure and improve IR test collection reliability. These tools complement the existing swap rate approach (reviewed in the next section) with a very different type of measure that also allows totally new types of analysis beyond the  X  X  X ere  X  reporting of post-hoc reliability (2) presentation of results for a number of test collections. Results include both after-the-fact reliability and before-the-fact guidance for test collection designers (3) conceptual clarification of the variance that comes from assessors. Regardless of how one mea-sures reliability  X  e.g. whether using a swap rate as in previous literature or Test Theory as proposed here  X  assessors are a main source of potential error in test collections, and we use ideas from Test Theory to illuminate a conceptual difficulty that has confronted previous work in this area. 1.1. Previous approach: the swap rate
We use the term  X  X  X eliability  X  to refer to the extent to which a result is not due to chance. Our use of this term reflects a bias toward statistical methods, where this term has a technical meaning. However, in reviewing the literature, we will use  X  X  X eliability  X  to refer also to non-statistical approaches such as the swap rate, which measure the effects of chance.

In a series of papers, Voorhees has analyzed reliability of TREC results using a data-driven concept of  X  X  X wap rate  X  . The swap rate of a set of test results can be calculated with respect to queries, relevance judges, performance measures, or other aspects of the collection. with respect to queries ( Voorhees &amp; Buckley, 2002 ). Suppose we have a set of performance results  X  let us say, MAP scores  X  for a set of algorithms on the full set of (say) 50 queries in a given task collection. The swap rate method randomly chooses a subset of n q (say 15) queries, and finds all pairs of algorithms X and
Y , where algorithm X out-performs algorithm Y by amount d on those queries. For every one of those cases, the method probes whether algorithm Y would have out-performed X had we chosen a single, random, dif-ferent (disjoint) subset of n q queries. If so, that is a  X  X  X wap  X  . The average percentage of swaps across all such cases and across all subsets of n q queries is the swap rate for those values of n different values of n q and d , for a complete three-dimensional graph of overall swap rate as a function of n and d . Similar approaches are used to graph swap rates with respect to the choice of n and magnitude d of performance difference ( Voorhees, 1998; Voorhees, 2000 ), or with respect to the choice of performance measure and d ( Buckley &amp; Voorhees, 2000 ).

In addition to the series of papers by Voorhees and co-authors, other researchers have also adopted this approach, making swap rate the predominant method for calculating reliability of IR test results. For exam-ple, building on this method and in light of the fact that most researchers don X  X  usually draw conclusions unless algorithm X out-performed Y with statistical significance, Sanderson and Zobel (2005) offer a modified version that calculates the swap rate only for those cases where X out-performs Y by amount d and with sta-tistical significance p * on n q queries. Another recent paper has used the swap rate method to measure the reli-ability of performance comparisons when graded (non-binary) relevance assessments are used to assess ranked output ( Sakai, 2006 ).

This swap rate approach has three distinguishing characteristics: It is a (1) data driven (2) aggregate (3) measure of results. Test theory, which we introduce, is different on all three counts. We will elaborate on these three characteristics because they imply three orthogonal dimensions, which define a space of possible approaches to measuring the reliability of a test collection. The swap rate and its variants represent one point in this space; test theory is another. But in fact, the majority of possible approaches have not been discussed in the IR literature, so our discussion intends to provide a framework for broader discussions of the concept of reliability in IR, for future reference. 1.2. Data-driven versus statistical definition of reliability
The first characteristic of the swap rate approach is that it is completely data-driven, rather than theory driven. This potentially has the advantage that the result is totally faithful to the data, and requires no assumptions. In the particular case of swap rate, this potential is not fully delivered. The reason is that the calculation of swap rate anyhow requires extrapolations, and due to the lack of theory, these are not theoret-ically justified. This undermines the supposed benefit of faithfulness to the data. As pointed out by the authors of ( Voorhees &amp; Buckley, 2002 ) and again by ( Sakai, 2006 ), extrapolations are necessary because the swap rate calculation is a kind of split-samples method and does not work for values of n greater than half the number in the collection. For example, if the collection has 50 queries and X out-per-forms Y by amount d on a set of 26 queries (more than half the collection size), it is impossible to calculate the swap rate, since there is no other disjoint set of 26 to see what would have happened. Thus, extrapolation is necessary, not to make inferences outside the test collection, but just in order to  X  X  X xtrapolate  X  from half the test collection to its actual size. Unfortunately, the lack of theoretical basis means that there is no basis for extrapolation, besides a gut feeling that it seems reasonable.

The alternative to a data-driven approach is a statistical or theory-driven approach. In order to keep this characteristic orthogonal to other dimensions, we present a version of the swap rate method that is statistical.
We call it the  X  X  X verage p -value  X  method, and it would work as follows: We take each case where X out-per-forms Y by amount d on a set of n q queries up to the full size of the collection, and find the statistical signif-icance. The statistical test can be parametric or non-parametric . For example, we could take all cases where X out-performs Y by an amount d on n q queries, and for each such case, we calculate the statistical significance of a pairwise t -test or non-parametric Wilcoxon Signed-rank test. By averaging over all such cases, we get the average p -value for cases where one algorithm out-performs another by amount d on n statistical definition of a p -value, (1 p ) indicates the probability that the true mean of the one algorithm is higher than the true mean performance of the other. This approach retains the spirit of the swap rate method, but it replaces the non-theoretical and extrapolated swap rate with a theoretical and non-extrapolated p -value.
We undertook such an analysis using pairwise t -tests and the results are shown in Fig. 1 , which shows average p -value as a function of n q (the x -axis) and performance difference d (legend) The result is very similar to the original version in ( Voorhees &amp; Buckley, 2002 ), lending additional validity to both variants. This statistical swap rate is not related to the (statistical) Test Theory that will be our main focus, but we present it in our outline of the full variety of possible approaches. 1.3. Aggregate versus holistic reliability measure
The second characteristic of the swap rate approach is that the swap rate of the full set of results is derived as an average of the swap rate of the individual pairwise comparisons. That is, this is not a direct and holistic measure of the reliability of the set of results, as much as it is a derived aggregate of the reliability of all indi-vidual pairwise comparisons. This characteristic applies to both the data-driven (original) swap rate method, and to the statistical version we outlined above. This characteristic is not necessarily a shortcoming, but we point out this aspect because it stands in contrast to the approach taken by Test Theory, as we will see below. 1.4. Reliability of results, versus reliability of the collection
The third prominent characteristic of the swap rate approach is that it is a measure of the reliability of a set of reported performance results , not a measure of the reliability of the test collection or of any property of the test collection itself. We will see below that a branch of Test Theory called Generalizability Theory (GT) assigns reliability to a  X  X  X est design  X   X  a characteristic of the test  X  and not to a particular set of results. 1.5. Summary of approaches to IR test reliability
Table 1 summarizes the possible definitions of reliability as it applies to a collection or a set of results, by referring to the distinctions we have discussed. There are practical as well as theoretical differences between these approaches. For example, let us consider how we might use an average swap rate to decide which of two test collections is more reliable and should therefore be preferred. From the outset, a swap rate approach is problematic for this purpose. We could get performance results for a single set of algorithms on each of the two candidate test collections, to see which was more reliable, but this would only tell us which collection was more reliable for this particular set of algorithms, and would not apply to other algorithms. This problem arises because of the final characteristic enumerated above, that the swap rate is a characteristic of test results, not of the test itself. But there are also more immediate practical problems. After analyzing swap rates using a single set of algorithms on the two candidate collections, we would have a set of graphs showing swap rate as a function of n q and d , for each of the two sets of results. Comparing the two test collections would then involve comparing two sets of (3-dimensional) graphs. Unless all the lines in one set of graphs dominate all the lines in the other, there is no clear way to compare the reliability of the two sets of results. We would be left with un-useful and unconvincing conclusions such as  X  X  X ne set of results is more reliable with these combinations of number of queries and magnitudes of performance difference, but the other set of results is more reliable with other combinations  X  . This practical difficulty arises from the second characteristic enumerated above, i.e. that average swap rate is an aggregate (not holistic) measure over pairwise comparisons. Since those pair-wise comparisons are a function of d ,  X  X  X he  X  reliability of the results can only be reported as a function of d .In sum, it is difficult to see how swap rates could be used to help decide which of two candidate test collections to use. This example is not meant to show that swap rate in inferior to the Test Theory approach we will propose.
Rather, the example is meant to show that different approaches to reliability have different practical uses as well as different theoretical assumptions and implications. One approach will not serve all purposes.
We have explored three dimensions of reliability measures in IR. The first row of Table 1 represents the swap rate approach that is widely used. The second row represents a statistical variant that we introduced above, whose results differ only slightly from the swap rate. We will not pursue this direction any further.
The remainder of this paper is dedicated to the final two rows of Table 1  X  Classical Test Theory and Gen-eralizability Theory  X  which complement the swap rate approach. 2. Test theory paradigm Test Theory has two streams. The first stream includes Classical Test Theory (CTT) and Generalizability
Theory (GT) which we will be our focus, while the second stream is Item Response Theory (IRT), sometimes known as Latent Trait Theory. IRT has been occasionally used in IR. For example, ( Banks et al., 1999 ) uses an analysis of variance to show the effects of query and assessor on score variance. But we are not aware of any work that applies either stream of test theory to assessing IR test reliability. This paper will focus on the first stream, CTT and GT. In applying CTT and GT to the case of IR, we are viewing the test collection as a performance test, the various algorithms as  X  X  X tudents  X  who are taking the test, and each query (topic) as a question on the test. The  X  X  X nswer key  X  for each query is provided by the relevance judges.

CTT and GT are based on statistical theory, but not on a hypothesis-testing paradigm. Instead, they are based on an analysis-of-variance paradigm. A hypothesis-testing approach asks whether data supports a hypothesis such as  X  X  X here is a difference in true means between two systems X  performance  X  . The swap rate approach fits roughly in this tradition, though it eschews the strictly statistical concept of true means in favor of the swap counts (the statistical variant we presented above fits exactly in the hypothesis-testing tradition).
Unlike the hypothesis-testing paradigm, the field of Test Theory has no concept of true means. question of whether two algorithms have different true means, is not a sensible question in Test Theory.
And since there is no hypothesis, there is no hypothesis-testing. Rather, CT and GTT provide tools to answer the question, to what extent does test score variance reflect variance of examinees X  true observable scores? This is a ratio. Technically it is a kind of R -square, and it is instructive to consider the different perspectives pro-vided by p -values and R -square. Hypothesis testing is concerned with p -values, and this is a natural approach when applied to an individual performance comparison: Is A really better than B? Analysis of variance is con-cerned with R -square, and we believe this is a natural approach when applied to the test as a whole: To what extent does the test capture differences between the examinees, as opposed to other sources of variance? 2.1. Technical introduction to classical test theory in IR
CTT ( Crocker &amp; Algina, 1986 ) adopts the view that each student X  X  (algorithm X  X ) observed test score Xj is the realization of a random variable with a true mean Tj : Xj = Tj + Ej . However, this  X  X  X rue  X  value is not a mysterious hidden parameter, but just an average over all possible test conditions. CTT addresses the question of the test X  X  reliability, defined as the proportion of variance in examinees X  scores that is due to differences in true scores, as opposed to random error: q XX 0  X  r 2 T
One analytical result has made CTT practically useful and widely used: The reliability coefficient can be estimated from a single administration of a test, by analyzing the variance of individual test items and total test scores. We conform to Test Theory notation and terminology, which denotes a test item as i , rather than q for queries or topics as in IR terminology. Cronbach X  X  alpha is the best-known measure of this kind. The cal-culation of Cronbach X  X  alpha is: where k is the number of items on the exam, ^ r 2 i is the estimated variance for item i , and ^ r 2 variance of the totals X (each algorithm X  X  sum over all items) scores. An example is shown in Appendix A .In this way, CTT analytically relates a measure of consistency to a definition of true score. CTT provides this theoretical meaning without requiring any distributional assumptions. It does assume  X  X au-equivalence X , which means that a participant X  X  true score is the same for all items, but even if this assumption is violated, then
Cronbach X  X  alpha remains a conservative lower bound on reliability. Note that as promised, both the theoret-comparison. There is no concept here of a pairwise comparison, or of whether X out-performed Y . CTT de-fines and calculates a holistic measure of reliability of the set of test results, as a ratio of true score variance to total score variance. Cronbach X  X  alpha estimates this as depending on the ratio of the sum of item variances to the variance of sums.
 Hersh et al. (2002) use Cronbach X  X  alpha, in order to assess inter-rater reliability, i.e. the effect of assessors. This is a somewhat unusual use of Cronbach X  X  alpha. We understand that they defined ^ r 2 the sum of each judge X  X  assessments, and ^ r 2 i as the variance of document scores. CTT in the IR literature, of which we are aware.

Because it is theoretically derived, CTT provides other more constructive analyses, beyond the mere report-ing of post-hoc reliability coefficients. For example, it follows directly from the analytical formula for Cron-bach X  X  alpha that high reliability follows from high ^ r 2 ^ r individual variances, and (2) high correlations between them. Loosely speaking, the requirement of high vari-ances means that all questions should be of medium difficulty. The second requirement would suggest that we analyze the full diagonal matrix of item X  X tem correlations, but this makes it difficult to identify which item is suspicious in general . Instead, the requirement of high item X  X tem correlations can be translated into a require-ment of high item-total correlations. A low or negative item-total correlation is evidence that that item is test-ing something different from what the test is intending. An item with a low item-total correlation contributes little to test reliability; an item with a negative item-whole correlation actually reduces it. It may be possible to attend to these guidelines before the fact. For example, if we are choosing among candidate queries to include in a test collection, we may have some basis for believing  X  in advance of any test results  X  that certain queries will be of medium difficulty and others will not; to the extent we are interested in increasing the reliability of test results, we would favor the items that we anticipate to be medium. On the other hand, it is usually not possible to know in advance which queries might turn out to have low or negative item-whole correlations, so that criterion is usually applied after we have a set of test results, and are looking to refine the test by omit-ting or editing any questionable queries. In any case, CTT goes beyond reporting how good the test reliability is, and sheds light on the features that affect reliability levels. This is a distinct advantage over a swap rate. 2.2. CTT results
We calculated Cronbach X  X  alpha for the adhoc task in TREC3-TREC8, and Web track in TRECs 9 and 10, using the top 75% of runs. Table 2 shows results.

In absolute terms, a reliability coefficient of .80 or higher is considered as acceptable in most social science applications, but we are not aware of any similar standard for engineering studies. This result is easy to cal-culate, but it is ex-post. It can serve as a basic measure for test collection developers, to alert them to any macro-level trends in reliability.

More constructively, item-total correlations can help to identify problematic queries as described above. As an exercise, we calculated these correlations for each query in ad hoc tracks of TREC3-TREC6. The average item-total correlation was about .6, with some individual queries as low as .12. The queries with low item-total correlations tended to be difficult queries. From the point of view of increasing test reliability, these items con-tribute little. That means, the test would have been almost as good without them. If queries are found with negative item-total correlations, then they are detrimental to test reliability and one should consider removing them. One may argue that such anomalous queries are useful because they reveal interesting phenomena, and so should not be excluded from use. For example, we may discover that certain low-performing methods sud-denly out-perform others on very long queries. But the meaning of such discovery, and our reaction to it, should be studied with care. If, even after observing the anomaly, we still believe that the task involves a single, indivisible challenge on which algorithms are being judged, then a query with a low correlation with the total can only indicate that it is some sort of trick question where performance depends on luck, or, in the case of negative correlations, that it especially fools the good performers. In this case, there is a clear argument that the query should be dropped completely. On the other hand, upon seeing the anomaly we may re-conceptu-alize that the task involves multiple independent or inversely related aspects. In this case, the query may be a meaningful test, but of some different aspect of ability than the main one being elicited via the other queries. In this case, the item should preferably be relocated to a separate task, together with other queries that test that same sort of ability. Only the collection X  X  gatekeepers can ultimately make these conceptual and practical judg-ments. But CTT identifies the questionable queries, and informs us that as long as a query with negative cor-relation is retained in the original task, it undermines the reliability of the test, while items with low correlations contribute little.

CTT has strengths but also limitations. It provides a clear definition of reliability. It is useful as a tool for improving the reliability of a test, since we can analyze a set of results and identify those queries that are not helpful to test reliability. Because CTT reports reliability as a single number, rather than high-dimensional graphs as in the swap rate, it allows a direct comparison between two test collections. We would run the same two groups of algorithms on each of the two test collections, and calculate and compare the reliability of each set of results. But such analysis, because it pertains to the sets of results rather than to the tests themselves, only applies to that particular group of algorithms. And because CTT relates to test results, it is mostly limited to post-hoc analysis. Also, CTT is a simple model that allows only a single error term. This means, for exam-ple, that it can either be used to measure reliability with the understanding that queries are a source of var-iance, or else with the understanding that relevance judges are the source of variance (inter-rater reliability), but not both. Also as a result of its simplicity, CTT calculations only regard relative reliability.
The meaning of these limitations will be clearer after we introduce GT. 3. Generalizability theory in IR
GT ( Shavelson &amp; Webb, 1991; Brennan, 2001 ), which is technically a generalization of CTT, measures the reliability of a test design, independent of any results. Its primary use is to help the test planner figure out, in advance of any test administration (use of the test), how reliable the intended test design will be. This allows the test designer to compare various test designs in advance.

What do we mean by a  X  X  X est design  X  ? GT considers that a test has many facets in addition to the algorithms being tested. A facet is a source of variance in scores, and GT allows us to model any number of these facets.
For example, a GT analysis of IR test designs might include a query facet and also a relevance-assessor facet, because both of these can affect an algorithm X  X  scores. As we will see below, it is also possible to add a doc-ument facet. In principle, any source of variance can be analyzed as a test facet. A  X  X  X est design  X  in GT is a pair of decisions related to these facets: (1) How many items there will be in each facet, e.g. how many queries, how many relevance assessors, how (2) How the various facets will be related structurally:
GT is a theory for calculating the reliability of various test designs under consideration, with each test design differing along the above two dimensions  X  how many items in each facet, and how the facets are related. For each proposed design, GT calculates a reliability coefficient. What is the meaning of assigning reliability to a test design as we have defined it? GT connects the issue of reliability to the issue of test design, by defining reliability as the ratio of variance due to differences between participants (algorithms), to variance that is due to other pertinent facets.

GT has two main stages. In the first stage called  X  X  X  Study  X  , we analyze representative old performance data to estimate the variance components of the test facets. Then, in a second stage called  X  X  X  studies  X  ,we use those results to calculate the reliability of various test designs that we might be considering, e.g. a test design (not necessarily in existence yet) with 20 queries crossed with two assessors, or a test design with one assessor nested within 40 queries, etc. The implied goal of this analysis  X  which is done prior to developing a new test collection  X  is to choose a test design that has the most variance due to participants, relative to other sources of variance. GT does not tell us what the query topic should be, or provide any other guidance, other than the reliability of each possible design, defined as the number of items in each facet and the structural rela-tionship between the facets. We now present an overview of the technical details of GT. 3.1. G-study
GT views a test design as including a number of facets. Suppose we conceptualize a planned IR test design as having a query facet and a relevance-assessor facet. In this case, the score of a participating algorithm on a single query as measured using the relevance assessments of one judge, would be modeled as the sum of a grand mean, a person effect, a query effect, an assessor effect, all interactions, and a residual effect. We denote separate effects is modeled as a random variable with its own mean and variance. We denote the variances of variance  X  is reserved for variance components that we view as noise (see below). Regarding interpretation, r 2 for example is the expected difference between the mean (over participants and assessors) of a single item and the average of all items in the  X  X  X niverse  X  (like a population). The variance of observed scores is then modeled as the sum of these variance components:
A G-study  X  the first of two steps when using Generalizability Theory  X  is an estimation of the variance components of Eq. (1) . This is accomplished by an analysis of variance of some actual test results from a set of algorithms on a representative IR test collection. In this sense, GT really does need some data to exist from the past. The purpose of the G-study is not to calculate the reliability of those old test results or that old test design. Rather, the G-study simply uses representative test results from a previously deployed IR test col-lection, in order to estimate how much test score variance seems to come from each of the various facets and their interactions, i.e. to estimate the typical variance contributions of each component in Eq. (1) . Appendix B shows a straightforward manual method to find these variance components. Appendix C mentions software to calculate the variance components. Once we have estimated these variance components, we have no further interest in the data that was used to calculate them. We are not calculating the reliability of that old data. We are just using it to get an idea of how much variance typically comes from each facet.

Table 3 provides an example of results of a crossed G-study based on 48 queries crossed with 2 assessors, for 33 participants; in Section 4 we will discuss these results in more detail.

The purpose of this stage is to estimate how much variance typically comes from the different facets. We are not trying to maximize anything at this stage. For example, there is no point to excluding problematic queries or taking any other remedial action at this stage. On the contrary, we want that these results should be rep-resentative. We may hope to see a larger participant variance relative to other sources of variance, but remedial actions are not appropriate at this stage.

For simplicity, we have assumed that for the purpose of conducting the G-study, there was available a data sample in which all the facets were crossed, which means that every participant got a score for every assessor on every query. This is ideal, because then the G-study can isolate every effect and report results as in Table 3 .
Most often, such data is available or possible to construct. The reason is that we can usually find a subset of an available data set that fully crossed. For example, suppose there is a large data set in which a pool of assessors helped to assess all queries. If we consider the whole set of queries and the entire pool of assessors, it will not be the case that every assessor judged every query. But we will likely still be able to find a subset of queries  X  say, queries 15, 16, 21, 35, and 48  X  which are fully crossed with a subset of assessors  X  say, assessors B and G.
This would simply mean that individuals B and G happened to have both assessed these five queries. This would yield a perfectly viable, fully crossed data sample, as the basis for a G-study. Even such a small data sample often provides stable estimates. If no crossed data is available even using this approach, it may be pos-sible to construct it for the purpose of conducting the G-study, since only a relatively small sample is required.
If even this is not possible, and the only available data has a nested structure, then the types of analysis are more limited. We will see such an example in Section 5 . 3.2. D-study calculations, and relationship to G-study
Once we have derived estimates of variance components, the hard work is done and we begin to reap the benefits of GT. We proceed to perform  X  X  X  studies  X  . D-studies do not involve collecting any data or admin-istering any tests. Each  X  X  X -study  X  is just an exercise of calculating what the reliability would be of any test  X  regardless of whether it exists yet  X  as a function of its planned  X  X  X est design  X  , which, as explained above, is defined by: (1) the number of items in each facet  X  e.g. how many queries, how many assessors, etc.; and (2) how the two facets will be related structurally  X  e.g. will they be  X  X  X rossed  X  , nested, etc. For example, we might plug in the G-study variance results into formulas to calculate what the reliability would be if we were to create a collection with 20 queries and the same 2 assessors providing judgments for all 20 (20 queries  X  X  X rossed with  X  2 assessors). Doing that calculation is called a D-study. The idea is that once we know, from the G-study analysis of representative IR test data, what the typical relative variance contributions are from each facet, then we can calculate the reliability of a proposed test based only on its (proposed) design. As we will see in the formulas below, if a facet  X  say, the queries facet  X  is responsible for more variance, then a design with more queries will have the greater reliability. Unlike the G-study, the mentality in the D-studies is very much of optimization. We are trying to find the test design that will result in a high reliability  X  and that will also be affordable, or meet other criteria.

To repeat this fundamentally different approach to calculating reliability, we can calculate the reliability of any IR test collection we can imagine, even if it has never been used, or doesn X  X  even exist yet. The process has two steps. First, we find or create a sample of real performance data, from which we calculate the variance components. At that point, we are completely finished with that data, and are no longer interested in it. Then, in a second stage called a  X  X  X -study  X  , we use those variance components to calculate what the reliability would be of a test collection we are considering to build, based solely on its design. There is no direct connection between the design of the test collection that was used in the G-study, and the designs of the studies whose reliability we calculate in D-studies. For example, in the G-study that we used to calculate variance compo-nents, the data might have been results from 5 algorithms on 10 queries with 2 assessors for each query. Then in the D-study, we are free to use those variance components to calculate what the reliability would be if we were to build a new test collection with (say) 30 queries and 2 assessors per query, or 1 million queries and a different single assessor for each, or 1 million queries and the same single assessor for all, or any other test design we might fancy. The only connection between the G-study data and the D-studies is that the D-study calculations only apply to tests for which the G-study data is representative (see Section 3.3 ). For example, if the G-study reported variance components based on a set of experienced relevance judges, then any D-study calculations would only apply to a collection with experienced relevance judges, and so on. 3.2.1. D-Study formula of absolute reliability for fully crossed designs
We now present the technical details of D-studies, which help to clarify its intuition. Whereas the G-study was interested in estimating the variance that comes from a single query, assessor, measure, etc. the D-study asks, given those results, what will be the effect of using a set of n queries, a set of m assessors, etc. By con-vention, these sets are denoted by capital letters such as I,A . The variances of the separate effects are denoted (single) participant X  X  observable score on a set of items I , and his/her grand mean. Note that (only) p is still in lower case, because we are still interested in variance due to each single participant.

We first present the formulas for calculating the reliability of a proposed design in which assessors will be result by the number of individuals we are considering for that facet. For example, if our G-study showed var-calculations can be done for interactions. The straightforward formulas are: where n 0 i ; n 0 a denote the number of queries and assessors in the D-study being considered.

Based on the formulas in (2) , GT defines  X  X  X bsolute error variance  X  of a crossed design as
This is the variance of all effects, except the main participant effect. Finally, GT defines the ratio of absolute reliability as
For example, assuming we have the G-study results as in Table 1 above, we present D-study calculations of two fully crossed test designs being proposed. Fig. 2 presents result for a proposed design with 50 queries and one assessor per ( X  X  X rossed with  X  ) query, and Fig. 3 for a design with 20 queries and 3 assessors per ( X  X  X rossed with  X  ) query.

The numbers of queries n 0 i and assessors n 0 a in the D-study are denoted with a  X  X  X rime  X  symbol, in order to emphasize that there is no inherent connection between the number and identity of queries and assessors that were used in the G-study, and the number of queries and assessors in the test designs whose reliability we are estimating in the D-studies. For example, as noted above, it is possible to conduct a G-study using reported performance data from a test of 5 participants on 10 queries with a single assessor, to estimate variance com-ponents based on this, and then to calculate reliability for tests with any numbers of queries and assessors we might fancy.

GT is not used to measure how good a test administration was , but to explore the reliability of various test designs in advance, in order to help the test designer to choose a design that maximizes reliability . This is a very different tool than ex-post swap rate calculations. GT is intended to be used in advance, at the time of test design, in order to explore which design is the most reliable in the sense of having the highest ratio of partic-ipant score variance to total score variance.

In general, as the D-study formula should make clear, a test design will be more reliable if we include more elements in the facets (beside the participant facet) that contribute the most variance. This follows straightfor-wardly from the definition of reliability in GT and CTT, as a ratio of participant variance to other sources of variance. As we will see in the next section, reliability may also be affected by the other aspect of test design, i.e. the structural relationship between the various facets, whether nested or crossed. 3.2.2. D-study formula of absolute reliability for nested design Until now, we have only demonstrated how to use the G-study results to calculate the reliability of crossed
D-study designs, i.e. to appraise in advance the reliability of test designs that will cross all facets. But it can also be used to consider the reliability of nested designs, in which, for example, there will be different relevance judges for each query. The formulas are a bit different:
To calculate absolute error where assessors are nested within queries, we remove from (3) any error com-ponent that contains a or A without i or I . The formula becomes:
We now need formulas for computing the nested variance components. The formulas are:
By (5) X (7) , for D-study with different assessors for each query:
Then the results are plugged into Eq. (4) to find the absolute reliability for these nested designs. As usual, the purpose of calculating these nested D-studies is to consider tests with this design. Perhaps the analysis will reveal that a nested design has higher reliability than a crossed study. Or we might want check that its reliabil-ity is satisfactory, even if not optimal, in cases where we have some other reason to favor a nested design over a crossed one, such as a logistical or other constraint that makes it impractical to design a new collection in which every assessor submits judgments on every query. 3.2.3. D-study formula for relative error
Yet another powerful feature of GT is that it distinguishes between absolute and relative error. Absolute error variance, which we have discussed till here, includes all error components besides the participant main effect. This kind of error is important regarding absolute decisions, such as whether a production system reaches an acceptable level of performance for deployment in a real setting. But if our main purpose is to gain insights into which methods work better than others, then we care only about relative error. Any source of variance that is equal to all participants  X  e.g. variance of item (difficulty), etc.  X  does not affect such compar-isons, and so is excluded from the relative error variance. To formulate relative error variance, simply remove from the corresponding equation of absolute error variance, any component that has no reference to  X  X  p  X  . For example, the formula for relative error variance of a fully crossed test design is (compare with Eq. (3) ) while the formula for relative error variance of a design with assessors nested within queries is (compare with
Eq. (5) ) is:
Because r 2 d includes a subset of the terms in r 2 r , the relative error variance is always equal or smaller than the absolute error variance. Finally, the relative reliability coefficient (compare with (4) ) is:
In sum, GT can help guide our choice of test design by reporting two criteria that we may be interested to maximize, absolute reliability and relative reliability, for any conceivable test design. 3.3. Representativeness
The G-study estimates drive the D-study reliability calculations of test designs we are considering. Results are limited to tests for which the original G-study data is representative; this is essentially a question of exter-nal validity. This requirement of representativeness means that although the statistical approach supports inferences beyond the historical data, these inferences are not without limits. In the context of GT, represen-tativeness is defined in terms of relative variance magnitudes. That is, results may be applied across all kinds of differences in testing conditions, except differences that might affect the relative magnitudes of variance com-ponents. Identifying such differences ultimately depends on our domain understanding. For example, suppose that the original G-study data was based on a sample of independently developed algorithms, but the final test whose design we are planning is going to be used on a set of algorithms that are not independent, e.g. they include multiple  X  X  X uns  X  from each basic model. In such a case, we would have reason to question the repre-sentativeness of the variance components. We will empirically explore this particular question in Section 5.2.6 below. In any case, if the testing conditions that resulted in a certain data set are not representative of the conditions under which we will be using the test now being planned, then we simply won X  X  do a G-study on that data set. Once we have an idea for the conditions under which our new test will be deployed, we will naturally seek historical data that is representative of those conditions. 3.4. Extrapolation
The question arises whether it is fair to characterize the D-study calculations as extrapolations, which was one of the criticisms leveled earlier at the swap rate approach. But note first that GT does not make extrap-olations about reliability but about variance components. For example, suppose the data on which the
G-study was based only had 5 queries, and we use those results in a D-study to calculate the reliability of a test design with (say) 20 queries. In this case, we are not taking the reliability of a 5-query test and extra-polating it to estimate the reliability of a 20-query test. In fact, we would have never calculated the reliability of the 5-query G-study. Rather, we are taking the estimate of variance components from the test conditions (queries, assessors, etc.) of the G-study and we are using it in the D-study. But even this is not an extrapolation from one case to another, but a statistical inference from one case to a larger universe. A G-study makes a theoretically derived inference about variance components, from available data to a larger universe of admis-sible observations. Those estimates then apply to all other tests whose conditions (queries, etc.) come from the same universe. This is not a case of extrapolating from case A to case B, but from case A (the G-study conditions) to a larger universe, of which B (the test whose design we are now considering) is one example.
And crucially, that inference from the G-study conditions to the universe is theoretically derived, and has con-fidence intervals, etc. In sum, GT replaces atheoretical extrapolations about reliability with theoretical infer-ences about variance components. 3.5. GT relationship to CTT
We saw earlier that the main constructive element in CTT is the ability to identify problematic queries. This is only possible after the test has been used, but can at least help to refine the collection for the future. GT provides a different mechanism for improving reliability, i.e. choosing a design that has high reliability. It is possible to use both methods, i.e. to choose a reliable test design a priori, and then after the fact, conduct an item-total analysis to identify and consider removing or modifying problematic queries. Technically, GT reduces to CTT X  X  Cronbach X  X  alpha if we do the G-study and D-study both on the existing test (this under-mines the whole strength of GT, but is possible), use a crossed D-study design, consider only the query facet, and calculate the relative reliability. If one were to use item-total correlations after the fact to identify and possibly remove troublesome queries, then (a) if one re-computed Cronbach X  X  alpha without those queries it would be higher, (b) if one were to re-compute the G-study on that data then the variance from queries would be lower, and (c) if one were to re-compute the D-study reliability for that same test, it would be higher. 3.6. Summary of GT theory
This concludes our introduction to GT. GT evaluates the reliability of a test design. This can serve as a complement to a swap rate approach, which measures the reliability of a set of performance comparisons.
GT uses past experience as a guide to calculating the extent to which a proposed test design captures the var-iance of interest, as opposed to other sources of variance. It is theory-driven. It accounts for variance from all facets, and integrates the various facets into a single, overall reliability, so that alternate proposed designs can be directly compared. It also differentiates between absolute and relative reliability. Additional benefits are the possibility of analytical results, which we will see below. It allows calculation of reliability before the test col-lection exists, based on an analysis of variance with a sample of data. This is the ultimate contrast to the data-driven approach; the two can be used to complement one another. 4. G-Study results
We begin by introducing the data set used for the G-study. Recall that this is the only  X  X  X eal data  X  that we will need to use GT. After that, we will be calculating the reliability of various test designs. 4.1. Adhoc data for a G-study
We use Voorhees X  specially constructed adhoc data set from ( Voorhees, 2000 ) as the basis for our first application of GT to design of a test collection for an adhoc task. This data is unusually rich, as three different assessors judged each query. Table 4 shows the identity of the judges for each query by letter, which were pro-vided by Voorhees. In the table, the column labeled  X  X  X econdary 1  X  just means  X  X  X ne of the secondary assessors for that query, and  X  X  X econdary 2  X  means  X  X  X he other secondary assessor for that query  X  . This gives us a score for each of 33 participants (algorithms), for each of 48 queries, for each of 3 relevance assessors. A sample of the data is as shown in Table 5 .

It seems like a straightforward application for GT with three fully crossed facets: participants, queries, and assessors. In Section 6 we will discuss the subtle conceptual difficulties of the assessor facet. For reasons dis-cussed there, we conduct the G-study based only on 2 assessors  X  the primary and one secondary assessor  X  rather than all 3. We now proceed to report results. 4.2. G-study results of adhoc data
Using the primary and one secondary assessor from the data whose excerpt is shown in Table 5 , we con-ducted a G-study. Results are shown in Table 6 (this reproduces Table 3 ).

Because all subsequent calculations depend on the estimate of variance components, we may report confi-dence intervals around the G-study estimates. In this paper we always report G-study results with confidence intervals as provided by the urGENOVA package. The method is based on one discussed in ( Burdick &amp; Gray-bill, 1992 ) for confidence intervals around linear combinations of expected mean squares. It is clear from the equations in Appendix B that each variance component is in fact a linear combination of expected mean squares. The final column of Table 6 shows the confidence interval around the corrected variance estimates. It is clear that our confidence intervals are extremely narrow.

Table 6 shows that the participant effect is a relatively small minority of total variance. This might seem troubling, considering that the primary use of these tests is to bring out variance due to participants. But recall that the technical definition of these numbers is the variance due to a single participant, query, assessor, etc.
This is precisely why typical exams have more than one question, and why typical test collections have more than one query. The D-study calculations that we do next will show test reliability when we have many queries and assessors. 4.3. D-study reliability results
Based on the G-study results of Table 6 , we proceed to D-studies, which calculate the absolute and relative reliability of various test collection designs. We consider only crossed designs, i.e. in which every query is assessed by every assessor, in cases where there is more than one. To calculate reliability, we use formulas (3,4) and (9,11) to give the absolute and relative error, and absolute and relative reliability, for each design. Table 7 shows results for some reasonable levels of queries and assessors. We previously presented confidence intervals around the results of the G-study. But we are ultimately interested in confidence intervals around these D-study results. Unfortunately, constructing confidence intervals around the final reliability coefficients involves advanced procedures, and this is not a standard part of GT. However, we calculated standard errors for the D-study error variances r 2 D and r 2 d , which are the basis for the reliability coefficients U and E q  X  X  X ormal procedure  X  ( Brennan, 2001 ). These standard errors are shown in parentheses in Table 7 . The details of this procedure are far beyond our scope.

As Table 7 shows, at these levels of queries and assessors, doubling the number of queries is much more productive than doubling the number of assessors roles. This result holds true for test tasks/collections for which these queries and assessors are representative. In the following section, we will explain why we imply that the fair comparison is between multiplying queries or assessors by an equal factor. In any case, this result  X  that compounding queries gives better bang for the buck than compounding assessors  X  this can change in other parts of the grid, depending on the numbers of queries and assessors involved. Regarding the difference between absolute and relative reliability, in the depicted example, the best choice  X  out of the four possibilities presented  X  for absolute reliability is also the best choice for relative reliability; this is not always the case.
Table 7 presents a typical set of basic GT reliability results. It is not a post-hoc analysis of the reliability of a given test after it has been used, but an a priori reliability coefficient of various proposed test designs, based on past experience with the amount of variance coming from each facet. It provides guidance about how to design a test for maximum reliability, which it defines as the ratio of true variance to total variance. It shows the sen-sitivity of reliability to the numbers of elements in each facet. It provides this guidance about both relative and absolute reliability. In the following few sections we present some of the more advanced results that GT offers. 4.4. Reliability results: analytical optimum for crossed design
Is there any point at which the returns from adding assessors begin to out-perform returns from adding queries? We can find this using the trial-and-error approach of Table 7 . But if we introduce a cost model then we can apply a more focused analysis, justify our comparisons, and find an analytically optimal design. This is a further strength of the theory-based nature of GT. Assume that the total cost of constructing set of relevance judgments is proportional to n 0 i n 0 a , i.e. the number of assessors per query times the numbers of queries. This model, which omits any reference to the number of documents, is appropriate for pooling methods of rele-vance assessments, where a fixed number of documents gets manually assessed for each query. Suppose this budget is fixed, and we want to find the ratio n 0 i = n 0 how to apportion our budget as between queries and assessors-per-query, in a way that minimizes error var-iance and thereby maximizes test reliability. Recall that by Eqs. (2) and (3) above, absolute error variance is:
It can be derived that the optimal ratio  X  n 0 i = n 0 a to minimize this type of error, is:
This means, for a fixed budget and this cost model, the optimal ratio of queries to assessors is 1826 to 1. In other words, we would not even think of spending money to add a second assessor per query until we had well more than 1826 queries. Given the typical starting point of 50 queries and 1 or more assessors, it is more important to invest in increasing the number of queries. A similar approach can also yield an optimal ratio for minimizing relative error, where the result is: n 0 i dual problem of minimizing cost for a given desired value of each coefficient. 4.5. Reliability results: nested D-studies
We have so far assessed the reliability of fully crossed test designs under consideration. GT can also con-sider alternative designs in which, for example, assessors will be nested within queries, which means having different assessors for the different queries. We now show that this is necessarily results in higher reliability, compared with using the same assessor(s) for all queries. With the crossed design, we presented above Eq. (9) while for the nested design, we presented Eq. (10) :
It is easy to see that the relative error in the nested design is weakly lower. Thus, for the purpose of distin-guishing good methods, it is always advantageous to use different assessors  X  people or roles  X  for each query.
This may seem counter-intuitive, as one might guess that it is better to stick with the same assessor individuals or roles for as many queries as possible, to  X  X  X eep it constant  X  . But the intuition fails to distinguish G-study from D-study. For the purposes of estimating error, it is indeed better to use the same assessors for each query, i.e. a crossed study. We have already noted that doing so allows us to estimate all the individual various com-ponents. But when it comes to limiting error in the actual test collection, it is better to use different assessors for each query. This is an additional example of the sort of result that a theoretically-based approach can offer. 4.6. Representativeness
The G-study data must be representative of the test whose design we wish to evaluate. One aspect of rep-resentativeness, regards the set of participants. The data used above for our G-study ( Table 5 ) consisted of 33 participants, but there is a sub-structure: some of the 33 participants are actually multiple runs from one research group, and are based on a single basic mode, and differ only in minor ways. It seems possible that the variance due to participants may be higher in cases where each run represents a different basic model.
If this is the case, it would mean that the G-study results reported in Table 6 might only be a good basis for calculating reliability in cases where the test we are designing will be used in this same way, with multiple runs from each model. To explore the sensitivity to this sub-structure, we conducted an additional G-study, in which we randomly selected only one run from each of the 18 participating organizations, instead of all 33 runs as above. Results ( Table 8 ) show that the variance component due to participants was only marginally higher, and impact on D-study results in small. So, this aspect of the testing condition does not pose issues of representativeness. In general, the test designer must use domain knowledge to consider characteristics of the testing condition that might alter the relative magnitudes of the variance components. 4.7. Summary of test theory in IR
This concludes our presentation of results for Test Theory, including both CTT and GT. Test Theory pro-vides holistic reliability measures to complement the aggregate measures of swap rates. CTT provides a holis-tic, ex-post measure of the reliability of a set of test results. Among other benefits, it allows  X  X  X tem analysis  X  ,in which we can identify which queries harm the overall test reliability. In this way, although it is post-hoc, it at least offers suggested ways to refine the test for improved reliability. GT is a generalization of CTT. GT pro-vides an a priori, holistic measure of any imaginable test design , on the basis of previous experience with the magnitude of variance from each facet of the test. Specifically, it allows a priori sensitivity analysis  X  and even optimization  X  of reliability as a function of the numbers of assessors, queries (and any other source of var-iance) in a planned test design.

We envision that the tools of Test Theory can complement swap rate analyses. GT provides specific  X  X  X dvice  X  about the test X  X  design, and after the fact, CTT provides specific advice about ways the test can be profitably refined. Such tools can be seen as part of the effort to minimize costs, since this guidance opti-mizes the use of limited resources. In the case of the data set analyzed here, this guidance mostly pertains to the identification of potentially troublesome queries, and the prescription that limited budgets be spent on queries rather than on assessors. But the particular results will of course vary, when these techniques are used on other types of tasks and data sets that differ from the adhoc collection explored here.

This concludes our presentation of basic results. In the following two sections, we explore two additional issues. Section 5 introduces a second data set as a vehicle for exploring an additional facet that we have not seen so far, the document facet. Section 6 uses the theoretical framework of GT to illuminate a long-standing conceptual difficulty surrounding the assessor facet. This discussion clarifies the meaning of our own GT results reported above, as well the meaning of other research results, as they regard the effect of assessors on test reliability. 5. Adding a document facet
We have until now ignored one critical facet: documents. All IR tasks are in effect asking participants about the relevance (or novelty, etc.) of documents. And, there certainly exists a main document effect and as well as document X  X  (e.g. document X  X uery, document X  X ssessor, etc.) interaction effects. For example, it is undoubt-edly true that some documents present more problems to automatic prediction than other documents; this is a document main effect. It is undoubtedly true that for a given query, some relevant (non-relevant) docu-ments are more difficult to find (avoid); this is a document X  X uery interaction. It is undoubtedly true that a document may pose more difficulties for some algorithms than for others; this is a document X  X articipant inter-action effect. And so on. Why, then, have we avoided this facet?
The reason is that the adhoc task does not give a performance score to a participant with respect to each document, but only a single performance measure with respect to a whole (possibly ranked) list of documents.
This makes it impossible and irrelevant to analyze the document facet for this task. But in some tasks, it is possible to conceive of a single-document-based performance measure, in which case we would add a docu-ment facet to the GT analysis, and GT would provide reliability estimates as a function of how many queries, assessors, and documents in a planned test design, as well as the structure (nested or crossed) among these facets.

This section introduces a filtering data set in which a per-document performance measure is possible. Once we have introduced a per-document measure, then we can show how to calculate reliability with facets for queries, assessors, and documents. We will compare reliability when using a traditional measure such as an
F -measure, as against using the per-document-based performance measure that we introduce. 5.1. Data set #2: novelty track
The data comes from the TREC 2004 Novelty track ( Soboroff, 2004 ). Task 1 of the Novelty track is to identify all relevant and novel sentences. We ignore the novelty aspect, and consider only the relevance pre-dictions. Each topic was judged by two assessors, the topic author and a single secondary assessor, and this yields data of the same rich sort as was analyzed above for the adhoc task.

If we don X  X  consider the document facet, then all the procedures are the same as were presented above for the adhoc data. Using the F -measure as the performance measure as in the TREC Novelty track, we con-ducted a G-study to calculate the variance components based on past data. Results are shown in Table 9 .
Table 10 presents D-study reliability results for a variety of reasonable test designs. Note that although abso-lute reliability is low, relative reliability remains reasonably high. This is a direct reflection of the fact that the main sources of variance on this collection are equal to all participants, i.e. are not the participant X  x interactions. 5.2. Towards a single-document-based measure
In order to include a separate document facet, we want a performance measure that allows us to cross doc-uments with participants. That is, we want to assign a performance score for every participant with respect to every one of a set of documents. Of course, when comparing algorithms or otherwise reporting results, we would take the sum or average over all the per-document scores. The F -measure is not a per-document measure.

A linear utility measure ( Robertson &amp; Soboroff, 2001 ) gets us a bit closer to our goal of a per-document performance measure. The linear utility measure implies a score for each document a participant retrieves, i.e. +2 if the document is relevant, 1 if not.

Unfortunately, we still cannot directly adopt linear utility as a per-document performance measure for the purpose of including the document facet in a G-study. The reason is that we need that every participant receives a score on all documents in a set. The linear utility implies a score for each document that a particular participant retrieves, but different participants retrieve different sets of documents! So, we still do not have a measure that assigns a score to every participant on every one of a common set of documents.

We propose the following definitions of a common document set and a per-document performance mea-sure: Regarding the document set, for each query we construct a union of all retrieved documents across all participants. Then we also add to this, all relevant documents. This is the common set of documents on which each participant will be scored for that query. Assuming that our analysis will also include the assessor facet as before, we do this separately for each assessor.

Now that we have defined the common set of documents on which all participants will be judged for the given query, we proceed to propose the per-document performance measure:  X  If the document is relevant and is retrieved by the participant, score is +1  X  If the document is relevant and is not retrieved by the participant, score is 1  X  If the document is non-relevant and is retrieved by the participant, score is 1  X  If the document is non-relevant and is not retrieved by the participant, score is 0
The average of this performance measure maintains the rank order of participants X  performance with the lin-ear utility measure ,
In both, the retrieval rule is: retrieve if P (rel) &gt; 1/3. This allows meaningful comparison and interpretation of any differences in reliability. We will compare reliability of a test using linear utility, versus reliability of a test using this per-document measure.

Note that in the per-document measure we propose, the common set of documents is separate for each query, so this is a nested G-study design. Participants are crossed with documents and assessors, but docu-ments are nested within queries. 4 In GT notation, the design is p a ( d : i ). Thus, we will not be able to iso-late a document effect. Rather, the document effects will always be conflated with the query effects. Although this limits the full range of D-studies we can conduct, we can still assess reliability of designs in which docu-ments are nested within queries. 5.3. Reliability using set-based utility measures vs. per-document utility measure
Which performance measure is better, one based on single documents or one based on document-sets? This cannot be answered solely in statistical terms, as the different approaches simply mean different things. For example, in discussing his choice among set-based measures, Hull (1996) points out the complex interaction between performance on individual documents, performance on individual queries, and overall performance.
Assuming set-based measures, performance on a single relevant document will have more dramatic influence on a query with fewer relevant documents. If we then take straight averages over all queries, then that dispro-portionate effect propagates to overall performance. In the context of Hull X  X  discussion, in which set-based measures were assumed, this argues in favor of not taking straight performance averages across queries.
But in principle, if a per-document measure makes sense, this same problem can be addressed at the source, by using a per-document performance measure; averaging over these per-document scores would give equal weight to each document-query pair, not to each query. Interestingly, Robertson and Soboroff (2001) raise the exactly opposite concern. Discussing a linear utility measure that indeed gives a score for each document in a filtering task, they complain that simple averaging of these scores would  X  X  X ive each retrieved document equal weight  X  , which in turn means that  X  X  X he average scores will be dominated by the topics with large retrieved sets  X  . In other words, they seem to take the opposite assumption as Hull; whereas Hull assumed that every document should have equal weight in determining which algorithm is better overall, Robertson assumed that every query should have equal weight. In any case, there is room for meaningful debate about the wisdom of using a per-document versus per-query performance measure, in those tasks where both make sense.

While GT cannot tell us what it is we want the performance measure to capture, it can tell us which approach results in higher reliability. Tables 11,12 present G-study and D-study results for linear utility, a tra-ditional document-set based measure.

Robertson and Soboroff (2001) already point to some troubles with this measure, so we use the improved normalized utility, in order that our comparison with a single-document-based measure will be conservative. Tables 13, 14 show G-study and D-study results using the normalized utility.
 We now turn to the per-document measure, to compare against the document-set-based utility measures.
We used the publicly available  X  X  X nput  X  files for participants of this track to compute their per-document scores on the set of documents defined above. Then we ran a G-study of the nested design p a ( d : i ). This result, shown in Table 15 , presents a very different picture than the G-study results that ignored the document facet (confidence intervals unavailable because the design is unbalanced, i.e. different numbers of documents for each query; details are beyond our scope).
This analysis results in a large residual. In fact, this variance component that we label as  X  X  X esidual  X  may either be due to four-way interaction P X  X  X  X :Q or to some other source of variance that we fail to capture.
In the latter case, results would lack generalizability. But in this case, we already know that if we ignore the document facet, the residual is small. Therefore, the interpretation of this residual is that it is due to the four-way interaction and not to some other source of error that we failed to model. Specifically, the (large) residual reflects differences in participants X  performance on different documents within a query. This was to be expected. The extended model with document facet is sufficiently elaborate to capture differences in perfor-mance between documents, but not sufficiently elaborate to capture the underlying events that cause them, i.e. what is it about the different documents. This kind of large residual is not a threat to generalizability of results. We now calculate D-studies based on this result. Because documents were nested within queries in the
G-study we can only consider D-studies in which documents are nested within queries. In fact, we will only consider D-study designs that have the same structure of the G-study, p a ( d : i ). We calculate the reliability with various values for n 0 i ; n 0 a ; n 0 d . With our method, the  X  X  X umber of documents per query  X  n determined by our choice. Rather, it represents the number of documents in the union of all participants X  retrieved documents plus the relevant documents. This is not something under our direct control, but it is a number we can estimate. In this sense, this is a somewhat non-standard use of GT. In the data we analyzed for the G-study, this averaged about 1000 per query. Assuming this number in the calculation of the D-studies, we get the D-study reliability results shown in Table 16 . 5.4. Discussion of document facet analysis
The G-study analysis of variance components offers important insights into the structure of the data. With-out the per-document facet, it always seemed that there is so much variance from the query effect and the query X  X articipant interaction, compared with other sources, and especially compared with the main partici-pant effect. The comparable assessor effects had seemed small by comparison. This had led us to prescribe investing in queries rather than in assessors. But when we introduce the document effect, we see that in fact the query main effect is rather tame. The main source of variance is the three-way participant X  X uery X  X ssessor interaction. The reason for this difference in results, is that the earlier analyses omit the document facet and aggregate over sets of documents. When aggregating over documents, assessor-related effects averaged-out and had little effect on the bottom line of algorithms X  overall performance. But when we introduce a per-doc-ument measure, we reveal that in fact there are major assessor-related effects.

We also set out to answer which type of measure yields higher reliability. A glance at Tables 14 and 16 already shows that reliability will be higher with the per-document measure, at least for absolute reliability and at these levels of queries to assessors. An even more conservative comparison adopts the ratio of queries ratio that favors linear utility. For a ratio of 161:1, absolute reliability with normalized linear utility is .892 while using a per-document measure it is .966. Using that same ratio (the ratio of 20,082 is not realistic), rel-ative reliability was .981 for the traditional measure and .986 for the per-document measure. In summary, there is evidence that a per-document measure has higher absolute reliability, but relative reliability is quite high for both measures. 5.5. Relationship to pooling literature
We began this work by mentioning the twin challenges of internal reliability and external validity. There is one additional topic that has been the focus of evaluation research in IR, and that is the question of how to choose which documents that will undergo explicit relevance assessment for each query. Some of the better known methods are variants of pooling ( Cormack et al., 1998 ), while other impressive methods have recently been introduced ( Carterette et al., 2006 ). At the philosophical level, GT has one thing in common with these methods, i.e. that they aim to provide guidance on how to construct a test collection before the fact. But at the practical level, GT is only concerned with the question of how many documents an algorithm should be scored on. If we assume that algorithms are only scored on documents for which we have a manual judgment, then
GT directly tells us what the reliability will be depending on how many documents are judged. In that case, the relationship between GT and pooling methods is complementary  X  GT can offer guidance about how many documents to judge, and pooling methods can offer guidance about which documents to judge. The relation-ship with the approach in ( Carterette et al., 2006 ) is trickier, since that approach suggests both how many and which documents to judge. GT would be providing that guidance even before the documents and queries are designed, while the Carterette et al. approach would provide that guidance after the algorithms have been run. 6. Assessor error in IR literature
The assessor facet presents conceptual difficulties, regardless of the methods used to define and measure test reliability. Previous researchers have tried  X  explicitly or implicitly  X  to grapple with these issues. We use the theoretical basis of GT to illuminate the source of this long-standing complication, and to outline possible solutions.

The assessor facet is conceptually difficult. The problem is that included in the  X  X  X ssessor  X  facet are two pos-sible sources of systematic variance: differences between roles, as well as differences between individuals. Dif-ferences between roles reflect the different scores that derive because a given person served as a primary versus secondary assessor. Differences between individuals reflect the different scores that result because different peo-ple served in a given role.

How has the previous literature dealt with this facet? Soboroff and Harman (2003) don X  X  present detailed swap rate analyses, but their basic presentation of  X  X  X ssessor effects  X  explicitly separates two separate effects, an individuals effect (their Fig. 2 ) and a roles effect (their Fig. 3 a). Voorhees (2000) who does present a detailed swap rate analysis, does not specifically write of a roles effect to be distinguished from an individuals effect.
However, her analysis is like Soboroff and Harman X  X  roles analysis in that it averages over individuals and focuses on the effect of roles. But Voorhees X  data has one primary and two secondary assessors. Thus, the meaning of her analysis is less obvious, because the difference between the primary and either of the secondary assessors, is of a different nature than between secondary-A and secondary-B. So, the nature of that analysis is not immediately obvious, as it seems to combine both the effect of roles and the effect of individuals. We will return below to explain the nature of this analysis.

GT helps us to organize our understanding of this facet, and of these previously reported results. In order to completely separate the roles effect, the individuals effect, and all interactions involving these with one another and all other facets, we would need to consider these two as full-fledged facets, and conduct a G-study that replaces the data structure we previously assumed, a i p where a denotes assessors, with data that has the fully crossed design: v r i p where v denotes individuals and r denotes roles. This means having every judge (individual) serve in every possible role in every query. If we could do this, then GT could not only defin-itively isolate the effects of individuals and roles and all their interactions, but would also allow us to address actionable questions related to these effects, e.g. how reliability would be affected if were to use n individuals in each role for n 0 q queries, or if we were to use n viduals in each of n 0 r roles etc., the whole range of questions that GT can answer with respect to a test X  X  facets.
But of course it is impossible to have data of this kind. The first reason is that it is not possible for a given individual to serve in more than one role on a given query  X  an assessor cannot be both the topic author and also not the topic author! This means that the design is necessarily not fully crossed but is instead v :( r i ) p .
In words, this means that individuals are nested with a query-role. And on top of that, for one of the roles, namely the primary assessor on a given query, it is not possible to have more than one individual, so it is not even possible to conduct a proper G-study with the structure (( v :( r i )) p ). In a restricted case like this, it is possible to derive a small set of conflated effects, but it is not possible to isolate the whole set of individual effect, roles effect, query effect, and all possible interactions. Although conceptually all these effects exist, they are not totally separable given the particular and necessary structure of the data in this application. If GT tells us that it is impossible to isolate the various effects, then we are left with two broad questions: (1)
What approaches are possible within a GT framework to measure the effect of individuals and roles on an IR test collection data? (2) What have previous researchers done?
Broadly speaking, the answer to the first question is that many approaches are possible, and each one addresses a slightly different question. One possible approach is to isolate the individuals effect by totally elim-inating the roles aspect. We can do this by using only data from multiple secondary assessors per query, and totally omitting any primary assessments, leaving a design of v i p or ( v : i ) p . To construct the crossed design v i p , we find a subset of the data, queries {206,222,226,242}, in which two individuals known as  X  X  X   X  and  X  X  X   X  both served as the secondary assessors. A sample of that data appears as Table 17 . This data allows us to conduct a G-study in which the assessors effect is actually a pure individual effect, with no effect of roles. The assessor facet in such a G-study would reflect differences between individuals serving as second-ary assessors. This is a perfectly valid choice, except that like all the choices we will list, the result is that the reliability analysis has this one specific meaning, and no other. Specifically, a D-study based on this approach would indicate the reliability of a test, as a function of how many secondary assessors (1 or more) we intended to use, assuming that only secondary assessors are used. Such a study would have nothing to say about a design that included both primary and secondary assessors. This may not appear to be the most useful approach, since one normally assumes that the test designer will use the topic author (primary assessor) as one judge. We gently suggest that it might be worthwhile to change current practice  X  i.e. to use only second-ary assessors  X  for the purpose of being able to use GT. If we use only secondary assessors, then GT is straight-forwardly able to report the reliability of a test design as a function of how many individual assessors are used, and as a function of whether the design is crossed or nested. Practitioners may feel that changing practice  X  using only secondary assessors  X  in order to measure reliability is putting the cart before the horse. But it should be recalled that GT does not merely supply a post-hoc measure, but can tell us which test design will be more reliable. Thus, the benefit of being able to use GT is the benefit of being able to design a reliable study .
Unless secondary assessors can be shown to be  X  X  X orse  X  than primary assessors in some well defined sense, then the ability to improve reliability through the use of GT may be worth this change to using only secondary assessors.

Hypothetically, one might be interested in the opposite approach, focusing only on the effect of individuals and not roles. But it is impossible to isolate the roles effect in a similar way, since we cannot omit all but one individual per query, and still have that single person serve multiple roles for the query! Alternatively, we can isolate a roles effect it we have queries in which there are multiple individuals for each role, but this requires multiple primary judges (it is unclear whether co-authors of a topic would behave like two primary authors, but in any case we have no such data). Thus, it is not possible to isolate this effect within an analysis of var-iance framework.

On the other extreme, rather than trying to isolate each effect, we might embrace the problem and purposely yet cleanly conflate the effects of individuals and roles, by selecting a subset of data in which there is a one-to-one correspondence between individuals and roles. For example, Table 18 shows such a layout, which would be a subset of the full data set. Once again, results of such a G-study are perfectly valid, except that the reli-ability results would only have the corresponding meaning and no other. Specifically, a reliability analysis based on such a G-study would indicate reliability of a test design as a function of whether it uses one person who is either the primary author for all queries or a secondary assessor for all queries, or two people, one of whom is the primary for all queries and a second who is a secondary for all queries. But it is unclear if such a
G-study can provide stable estimates, since only a single pair of individuals can be used. In our particular case, the data situation was worse, since the largest subset with this structure consisted of pairs of individuals who played consistent roles (primary or secondary) over just three queries. There were six such pairs to choose from, but results were not consistent across the pairs, showing that any one pair does not provide enough data for a stable estimate of variance components.

Turning to the second question posed above, if these various effects are conflated, then what have previous researchers done? By applying the ideas of GT to this previous work, we uncover a major conceptual issue confronting the IR evaluation community. The two  X  X  X ides  X  of the issue are represented by Soboroff and Har-man (2003) on the one hand, and Voorhees (2000) on the other, as their positions are understood in terms of GT theory.

Recall that Soboroff and Harman (2003) focus on roles by ignoring (averaging over) the individuals that happened to serve in those roles. The GT perspective indicates that those reported effects actually conflate an individuals effect (because different individuals were serving in one role or the other), an individuals-roles interaction (because the effect of serving in a particular role may have been specific to the particular individ-uals who were serving in those roles), an individual X  X uery interaction (because the effect of the individuals who happened to be serving in those roles may have been particular to those queries), and a three-way indi-vidual-role X  X uery interaction (because the individuals who happened to be serving in those roles happen to have that behavior for those particular queries). What, then, is the meaning of their analysis, which claims to focus on roles? The answer is that this is not the pure  X  X  X oles  X  approach shown in Table 17 , which isolates the effect of roles by using the same individuals for many queries, as would normally be done within a strict
GT approach. Rather, it emphasizes the roles effect by averaging over many individuals. This implicitly appeals to assumptions of independent errors, large numbers, and random sampling of individuals to serve in various roles for various queries. Under the right set of assumptions, the individual X  X  variance components  X  X  X ash out  X  in some statistical sense, leaving mostly a roles effect. This is apparently the sense in which their reported results indeed represent a roles effect. It would also be possible in principle to do the reverse, i.e. orga-nize the data by a given set of individuals, ignoring (averaging over) which roles they happened to play and hoping that the roles effect and other interactions wash out. For example, individuals A and M served together in various roles for queries {202,209,214,230,242} But this approach seems less appealing, because it would necessarily be based on much less data.

Voorhees (2000) analysis implies yet another valid approach to the data, but one that implies a very differ-ent conceptualization of the nature of assessor error in IR test collections. Recall that her swap rate analysis analyzes the effects of assessors, based on comparing all three assessors as in Table 19 .

At first glance, it is difficult to give a clear meaning to such an analysis, since it reflects neither a pure roles effect, nor a pure individuals effect, nor any one kind of conflation. The differences between the primary and either of the secondary assessors, are of a different kind then any differences among the two secondary asses-sors. What, then, is the meaning of this approach that combines both? The answer is that it measures idiosyn-cratic differences between the three opinions on each query, without trying to track individuals or roles across queries. It conceives of assessor error as always interacting with the particular query involved, and not as deriving systematically across queries because of one X  X  role or individualness. In this conceptualization of error, there is only idiosyncratic error, and no attempt is made to measure systematic role effects or individ-ual-identity effects. We might refer to this as an assessment facet, rather than an assessor facet, since it views error as coming from the particular assessments, and not from systematic effects that derive from assessors qua roles or assessors qua individuals.

This conceptualization can be formalized within GT, as a G-study in which participants are crossed with assessors nested within queries, p ( a : q ). The nesting of assessors within queries captures the philosophy of this approach, by de-linking the trio of judgments in one query from the trio of judgments in other queries, regardless of any equivalence in individual-identity or roles. Actual G-study results using this approach are depicted in Table 20 . Note that this is our first encounter with a nested G-study. Such a nested G-study cannot separate the assessor effect from its query interaction, because it conceives of assessors as being nested inside the query, in the sense that an individual person X  X  behavior is not considered to carry over systematically from one query to another, nor is a role considered to be source of systematic behavior from one query to another. Any assessor effects are therefore considered to be conflated with the particular query.

As with all the other choices, this approach has its own meaning and limitations. This approach has the benefit that it is conservative  X  it tracks the effects of any variance due to assessors, rather than only systematic variance, so it tends to  X  X  X ind  X  larger variance in the assessor component. This is conservative in the sense that it leads to rather lower reliability results. Another advantage is that such an approach can be conducted entirely within the GT framework. However, as noted at the conclusion of Section 3.1 and in Section 5.3 ,a nested G-study such as this does not allow the full range of analyses that are possible with a crossed G-study.
For example, it does not allow calculation of reliability of D-study designs in which assessors (individuals or roles) are crossed with queries, something we can do if we use a crossed G-study. The GT formality has shown the Voorhees approach  X  which at first seems overly na X   X  ve in combining role effects and individual effects  X  to be a valid approach. It sidesteps the difficulty of tracking consistent individual and role effects in IR qrels, by conceiving of assessor variance as an interaction between a judgment and a query, with no notion of a person (individual) or a role.

We have thus discovered two very different conceptualizations of assessor error in previous literature. One possibility is that the error is systematic, owing either to systematic differences between individuals, or to sys-tematic differences between roles, or both. This conceptualization of error is reflected in by Soboroff and Har-man X  X  analysis. It is impossible to isolate a pure roles effect, but Soboroff and Harman approximate this by appealing to laws of large numbers. A second conceptualization is that there is only idiosyncratic assessor error that comes from the particular assessment on a particular query, with no notion of systematic differences due to either individuals or roles. This conceptualization is reflected in Voorhees X  analysis. In one sense, the different conceptualizations measure different things and address different questions. But on the other hand, they imply error models, and one of the conceptualizations can be more right than the other, i.e. fit the data more closely. Is Voorhees correct that there is idiosyncratic error even after accounting for systematic error due to roles or individuals? If so, then  X  X  X nly  X  measuring the systematic effects will be under-estimating the error. On the other hand, are Soboroff and Harman correct that there exist systematic effects due to roles and individuals? If so, then these can be addressed through having more roles or individuals per query, which would be overlooked in Voorhees X  approach. It is possible to use GT and analysis of variance to shed light on this empirical question, but this is outside the scope of this article.

Finally, we must explain the meaning of our approach in Section 4 , where we reported GT results consid-ering query and assessor facets. Which conceptualization were we using? Table 6 showed results of a G-study in which queries are crossed with assessors (compare the structure of that output with the structure of Table 20 , where assessors are nested within queries). This already reveals that we had performed our calculations based on a conceptualization of systematic error. But which kind, roles or individuals? We had wanted to focus on the effect of roles. This is not right or wrong X  we simply had wanted to present an analysis of reli-ability in terms of the number of different assessor roles that are employed. But as explained, it is not possible to use a pure analysis of variance approach to isolate the effect of roles, since that would require having the same individuals in different roles for the same query. Therefore, in deriving the results of Table 3 , we orga-nized the input to our G-study as in Table 21 .

This is essentially Soboroff and Harman X  X  approach. When conducting our analysis of variance on this data, the resulting  X  X  X ssessor  X  facet mostly reflects any systematic effect of roles. It is not a pure roles effect, since even with quasi-random sampling of individuals to fill these roles, some of the variance may be due to the interaction between the particular individuals who happened to populate each role in the various que-ries, i.e. role X  X ndividual interactions and role X  X ndividual X  X uery interactions. That is, the  X  X  X ashing out  X  of individual effects may be imperfect. Nevertheless, the reported assessor variance reflects mostly a roles effect.
Regardless of the interpretation , the results are valid when the question is whether to use only a single kind of assessor  X  primary or secondary  X  to judge all queries, or whether to use both roles for each query. In Table 7 , for example,  X  X  X ne assessor  X  meant a design that uses either the primary assessor or the secondary assessor for all queries, while  X  X  X wo assessors  X  meant a design that uses both the primary and a secondary assessor for each query. And as we saw, this source of error was relatively smaller than the query facet, and so all our reliability results were derived. 7. Summary and conclusion
We have presented Test Theory, including both Classical Test Theory and Generalizability Theory, as a method for assessing reliability of IR test collections. Classical Test Theory X  X  Cronbach X  X  alpha is in very wide-spread use as a holistic reliability measure, and complements nicely the swap rate approach that measures sen-sitivity of pairwise performance comparisons. Generalizability Theory introduces a totally different concept of reliability, as it calculates the reliability of a test based solely on its design, even before the test has been used or even developed. We have reported reliability results using both CTT and GT. They show fairly good reli-ability overall  X  for CTT, this means reliability of actual test results over the years; for GT, this means reli-ability of typical test designs. Relative reliability is clearly higher than absolute reliability (by definition, it cannot be lower), and reliability for the more mature adhoc data set was understandably higher than for the newer filtering data. One of the main benefits of Test Theory is that because of its theoretical basis, it can provide guidance about how to improve a collection. Classical Test Theory identified particular queries that lower overall reliability, and which can be the focus of re-evaluation. Generalizability Theory showed that on the basis of sample data, queries are a much larger source of variance than assessor role, and this suggests focusing any available resources on designing more queries. Results from a second data set showed that reliability could be increased with a per-document performance measure, where appropriate, though there are myriad other conceptual issues that enter into the choice between single-document-based and document-set based measures. Also, apart from reporting all these reliability results, we used GT concepts to explore and illuminate the complex nature of the assessor facet. We found that previous researchers, who have also grap-pled with this issue, implicitly offer very different conceptualizations about the source and nature of assessor error. We hope to have illuminated the nature of this choice and of previously reported analyses. Acknowledgements
The author would like to thank Dr. Ellen Voorhees for the special data set and other helpful communica-tion, and an anonymous reviewer for substantial comments. Any errors are the author X  X  sole responsibility. Appendix A. Example of Cronbach X  X  alpha calculation Student 1 0.7 0.5 0.6 1.8 0.10817 Student 2 0.8 0.6 0.76 2.16 Student 3 0.94 0.82 0.89 2.65 Student 4 0.75 0.7 0.5 1.95 Student 5 0.75 0.8 0.75 2.3 Item variances 0.00847 0.01828 0.02305 Sum of item variances (.00847 + .01828 + .02305) = .0498 Total-score variance Variance(1.8, 2.16, 2.65, 1.95, 2.3) = .10817 Alpha (3/2) times (1 .0498/.10817) = .80942 Appendix B. Calculating variance components The MSE equations for a crossed p i a design is: where n i , n a , n p denote the (known) numbers of items (queries), assessors, and participants in the data sample being used in the G-study. The variance components on the right-hand-side are solved for in a bottom-up fashion. To start things off, the MSE of the three-way interaction directly gives the residual component r is always conflated with the residual error, and this is why it is denoted as r estimate of all the variance components.
 Appendix C. Software For the case with participants, queries, and assessor facets, we can make available our spreadsheets and
Matlab programs to calculate variance components for fully crossed G-studies, and spreadsheets to calculate crossed and nested D-studies. For the case with the additional (and nested) document facet, we relied on the urGENOVA package ( http://www.education.uiowa.edu/casma/GenovaPrograms.htm ) developed by Bren-nan to calculate the G-study.
 For calculating variance components, it is possible to use a general-purpose statistical package such as SPSS, but here we need to identify one practical point. Using the SPSS VARCOMP command (via menus:
Analyze ... GLM ... Variance Components), it is necessary to specify a custom model, in which one includes all but the highest-order effect. For example, with participants, queries, and assessors (and no documents), one specifies a custom model including all main effects and two-way interactions, but not three-way interac-tion. The reason is that the highest-order interaction is always conflated with the residual, so if one asks for a  X  X  X ull model  X  , SPSS will do nothing.
 References
