 (SSL) from general unlabeled data, which may contain ir-relevant samples. Within the binary setting, our model man-ages to better utilize the information from unlabeled data by formulating them as a three-class (  X  1 , +1 , 0 ) mixture, where class 0 represents the irrelevant data. This distin-guishes our work from the traditional SSL problem where unlabeled data are assumed to contain relevant samples only, either +1 or  X  1 , which are forced to be the same as the given labeled samples. This work is also different from another family of popular models, universum learning (universum means  X  X rrelevant X  data), in that the universum need not to be specified beforehand. One significant con-tribution of our proposed framework is that such irrelevant samples can be automatically detected from the available unlabeled data, even though they are mixed with relevant data. This hence presents a general SSL framework that does not force  X  X lean X  unlabeled data. More importantly, we formulate this general learning framework as a Semi-definite Programming problem, making it solvable in poly-nomial time. A series of experiments demonstrate that the proposed framework can outperform the traditional SSL on both synthetic and real data. portant topic in machine learning and data mining. Given a sufficiently large quantity of labeled instances called train-ing data, one can exploit the traditional Supervised Learning (SL) algorithms to handle this task [23, 8, 10]. However, in many real world applications, the labeled data may be very few due to the expensive cost of manual labeling. On the other hand, the number of unlabeled instances could be very large since they are generally much easier to obtain. SL, taking only advantages of the labeled data, might not work appropriately in these cases. In contrast, Semi-supervised Learning (SSL), making use of both labeled data and unla-beled data, proves to be an effective solution in addressing this problem [29, 4]. Undoubtedly, SSL has achieved a great success in many domains involving machine learning and data mining. To guarantee good performance, SSL usually assumes that the unlabeled data should share the same labels as the labeled training samples. Although this assumption can be well satisfied in some cases, it appears still strong in certain other domains. In fact, it is very common that unla-beled data are collected by using automatical tools. This is actually frequently seen in the earlier stages of data collec-tion. It is usually inevitable that those collected unlabeled data contain irrelevant samples. Feeding such  X  X orrupted X  unlabeled data to SSL may significantly affect the overall performance and incur severe problems consequently. SSL framework capable of learning from general unlabeled data systematically, where the unlabeled data may contain irrelevant samples. Our model manages to better utilize the information from unlabeled data by formulating them as a three-class (  X  1 , +1 , 0 ) mixture. 1 This hence distinguishes our work from the traditional SSL problem where unlabeled data are assumed to contain the same labels as the labeled training samples [28, 7].
 can be seen in Figure 1 and Figure 2. In both Figures, all the filled points (  X   X  X  and  X  X ) are unlabeled data, while the  X   X  X  and  X  X  are the two classes of labeled training sam-ples. Clearly, Figure 1(a) illustrates that SSL can outper-form the boundary given by the Support Vector Machines (SVM) [3, 23], the current state-of-the-art SL algorithm. However, SSL may encounter problems if the unlabeled data contain the  X  X rrelevant X  data. This can be observed in Figure 1(b): The boundary of SSL is obviously unreason-able. A more reasonable decision plane should pull away the SVMs. the  X  X elevant X  data (maximizing the margin among the neg-ative and positive data) while predicting the values of the  X  X rrelevant X  data as close to zero as possible (clustering the  X  0  X -data around the decision line). Such a boundary (the dashed red line) can be observed in Figure 1(b).
 tive can actually remedy the negative impact when both the unlabeled data and the labeled data are limited. Such a case can be seen in Figure 2. Assume the ground truth bound-ary is given as the dashed line in Figure 2(a). However, due to the limited training data (including both the labeled and relevant unlabeled data), the learned SSL boundary may be deviated from the actual one (as observed in Figure 2(a)). Sometimes, there are perhaps some  X  X rrelevant X  instances (  X  X  in Figure 2(b)), being neither positive nor negative, mixed into the unlabeled data. By appropriately detecting and using these irrelevant data (trying to cluster such irrel-evant unlabeled data around the decision plane), one can actually learn a more reasonable boundary as seen in Fig-ure 2(b).
 to the work proposed in [19, 24], where the irrelevant data are called universum . However, they designed their system only within the Supervised Learning framework. In addi-tion, these universum data need to be specified beforehand and are merely used as the labeled third class of samples. In other words, one needs to know which instances are uni-versum data in advance so as to build a decision boundary. In comparison, we propose to exploit such irrelevant data in the semi-supervised context. More importantly, we do not need to specify which samples belong to the universum. Instead, we can learn from general unlabeled data, which means those relevant data or irrelevant data are mixed in the unlabeled data. Our novel model can output a more rea-sonable decision boundary, while simultaneously detecting the relevant data and irrelevant data automatically after the learning is finished.

Indeed, as far as we know, this work presents a novel study on how to perform learning from general unlabeled data consisting of both relevant and irrelevant instances. When the irrelevant data are known as prior knowledge by the user, this is the idea of  X  X SL with universum X  proposed in [27]. In contrast, our work presents a more difficult and general SSL framework, where irrelevant data are mixed with the relevant unlabeled data, without any knowledge on which samples are relevant or irrelevant beforehand. As a major contribution, we successfully formulate such a diffi-cult problem as a Semi-definite Programming (SDP) prob-lem [13, 6, 21], making the framework solvable in polyno-mial time. Both theoretical analysis and empirical inves-tigations demonstrate that the proposed framework outper-forms the traditional SSL in many cases.

The rest of this paper is organized as follows. In the next section, we discuss the related work. In Section 3, we detail the proposed framework including the model definition, the theoretical analysis, and the practical solving method. In this section, we will demonstrate how the proposed model can be formulated in a Mixed Integer Programming (MIP) problem [16] and finally relaxed to be an SDP problem. In Section 4, we conduct a series of experiments to validate our novel approach. Finally, we set out the conclusion with final remarks.
Researchers have devoted a lot of efforts on how to uti-lize unlabeled data via the effective semi-supervised learn-ing [4, 28]. One assumption for SSL is that unlabeled data unlabeled data. The decision plane of the SSL is given by the SVM. are required to share the same distribution as the labeled data. The above assumption is relaxed in [2], where the distribution of training data is allowed to be arbitrarily dif-ferent from the distribution of the unlabeled data. Unfor-tunately, although tolerating different distributions, it still requires the unlabeled data share the same class categories as the labeled training data. In fact, such requirement is enforced in most of SSL algorithms [28].

Alternatively, [24] studied the universum data, a special kind of unlabeled data that do not belong to any classes of the problem at hand, and showed that the universum data could boost the classification performance by encoding the prior knowledge of the domain. However, this interesting work is conducted in the context of Supervised Learning. The universum data need to be specified beforehand and are just used as the third class of samples. [17] proposed a Self-taught Learning (STL) and showed that weakly-related unlabeled data sharing a little structural information with the current task could also benefit the classification perfor-mance. The problem is that those weakly-related data are only exploited for extracting feature patterns and they are not involved in optimizing the decision boundary. Empir-ical study shows that STL sometimes extracts misleading patterns and hence might hurt the performance.

In addition, [11] and [27] studied the case that unlabeled data are a mixture of both relevant data, which are from the same domain of the current task, and irrelevant data, which are from a different task or the background. More specifi-cally, [27] assumed that the prior knowledge about the com-position of the mixture, i.e., the universum data and the data from the same distribution as the training data, is clear be-fore learning a semi-supervised classification model. How-ever, the application of the above methods requires the as-sumption that the prior knowledge of the composition of data should be known before learning. In contrast, this pa-per leverages the above requirement by learning from gen-eral unlabeled data which we do not know are relevant or not.

As a brief summary, our proposed framework presents a novel SSL framework that can learn from general unlabeled data. Such unlabeled data could consist of both relevant and irrelevant data. More importantly, we do not need to know which instances are relevant or irrelevant data. Based on solving an SDP problem, the proposed algorithm is able to automatically detect them, and consequently outputs a classification boundary that can exploit the unlabeled data more appropriately and more reasonably.
In this section, we first present the problem definition and the notation used in the paper. We then introduce the model definition, the theoretical analysis and the practical solving method in turn.
Given a training data set D , consisting of l labeled sam-certain distribution S .Here x i  X  X  n ( i =1 , 2 ,...,l ) de-scribes an input feature vector, and y i  X  X  X  1 , +1 } is the category label for x i . In addition, assume that m ( m l ) unlabeled data samples { x l +1 , x l +2 ,..., x l + m } are also available (for brevity, we denote n = l + m ). The unlabeled data contain both the relevant data sharing the same labels i.e., { X  1 , +1 } as the labeled data, and the irrelevant data which are different from the labeled data. Moreover, there are no prior knowledge on which instances are relevant or irrelevant.

The basic task here can be informally described as seek-ing a hypothesis h : R n  X  X  X  1 , +1 } that can predict the label y  X  X  X  1 , +1 } for the future input data sample z  X  X  n sampled from S by appropriately exploiting both the labeled data and the general unlabeled data. The hy-pothesis usually takes the linear form of h = sign ( f ( z )) , where f ( z )= w  X  z + b ( w  X  X  n , b  X  X  ). Note that the linear form can be easily extended to the non-linear form based on the standard kernelization trick [18].
The novel framework is introduced in the following. We first present the model definition followed by the theoreti-cal analysis showing the inner justifications of our model. Finally, we show how to transform the problem to an SDP problem. 3.2.1 Model Definition The novel model is formulated as the following Problem I:
Problem I : ) where x i , i =1 ,...,l are the labeled training samples. Namely, y i  X  X  X  1 , +1 } i =1 ,...,l is known before-hand. x j , j = l +1 ,...,n are the unlabeled data, where the associated labels are unknown, but restricted in the set of { X  1 , 0 , +1 } . C L and C U are two positive penalty para-meters used to trade-off the margin and the training loss.  X  is a small positive parameter describing the insensitiveness level.
 Constraint (1) describes the loss for the labeled data. Constraint (2) provides the loss if x j is judged as the  X  1 (i.e., the relevant data), while (3) presents the loss if x j is judged as the class of 0 (i.e., the irrelevant class). The loss incurred by the unlabeled sample x j is finally given by the minimum loss that it is judged as the class of  X  1 or 0 .This can be seen in the objective function of Problem I. Intu-itively, the above model attempts to maximize the margin among the positive relevant data and negative relevant data, while predicting the values of the irrelevant data as close to zero as possible simultaneously. In addition, our model can automatically detect or assign the unlabeled samples to ei-ther  X  1 (relevant classes) or 0 (irrelevant class) by choosing the smaller cost associated with the assigned label. lem I. The loss function for the relevant data is the hinge loss H  X   X  = max { 0 ,t  X   X  } as seen in (2), where t =1 .On the other hand, the loss function of the irrelevant data is de-fined as the  X  -insensitive loss U [ t ]= H  X   X  [ t ]+ H  X  [ t ] .Both loss functions are plotted in Figure 3. When a data point is judged as a relevant instance, we should push it as faraway as possible from the margin f ( z )=  X  1 . Hence a hinge loss is more appropriate for such a setting. When the data point belongs to the irrelevant class, it should be around the de-cision plane f ( z )=0 . In this sense, an  X  -insensitive loss function is more suitable. An analogy can also be seen in choosing the loss functions for SVM (using hinge loss) and Support Vector Regression (using  X  -insensitive loss) [20]. the operator of min . However, by introducing an integer variables d j = we can transform Problem I to the following problem: Problem II : In the above, M is a large positive constant. When d j is equal to 0 , M (1  X  d j )= M is a big value. Hence (6) will naturally be satisfied, leading  X  i =0 and further min(  X  j , X  j )=  X  j +  X  j . A similar analysis can be obtained when d j =1 . Therefore, we can know that Problem II is strictly equivalent to Problem I, provided that M is set to a sufficiently large value. Problem II is a Mixed Integer Pro-gramming problem [1, 16].
 solve the MIP problem. In the following, we will first de-rive a theorem showing the justification of our proposed al-gorithm. We then revisit the optimization and propose our practical solving method. 3.2.2 Analysis
In this section, we conduct some analysis showing that the utilization of irrelevant data has a nice theoretical justifica-tion. For clarity, we slightly modify Problem II to the fol-lowing optimization problem. Based on the modified prob-lem, we then derive the analysis. Problem II is changed as follows:
C r U represents the penalty parameter for the relevant sam-ples, while C i U describes the penalty imposed on the irrel-evant data points. We first present the following theory.
Theorem 1 The above learning machine with C i U =  X  and  X  =0 is equivalent to training a standard Transductive
SVM [5] with the training points projected onto the orthog-onal complement of span { z j  X  z 0 , z j  X  X } , where z 0 is an arbitrary element of the space spanned by the irrelevant samples denoted by U .

Sketch of Proof : C i U =  X  and  X  =0 implies that any w yielding the optimal solution of (8) satisfies w  X  z + b =0 for any z judged as irrelevant samples. Hence, we have w  X  ( z  X  z 0 )=0 , implying w is orthogonal to the subspace spanned by all the irrelevant samples. Hence the optimiza-tion of (8) intends to find a traditional transductive SVM in a subspace which contains only the relevant samples, while the irrelevant samples are suppressed. In addition, from the previous argument, the space U spanned by the irrelevant samples can also benefit the classification, since it is U that decides the optimization subspace.

Theorem 1 shows that the optimization of our proposed algorithm actually tries to find the most suitable subspace in which the margin can be maximized while the overall er-ror can be minimized. The irrelevant data do not contribute to the final accuracy directly. However, it determines the subspace where the resultant decision boundary is derived and will consequently affect the final performance. Theo-rem 1 clearly shows how the irrelevant data can affect and eventually improve the overall performance. 3.2.3 Practical Solving Method We now revisit the optimization of Problem II. Although there are softwares that are able to deal with MIP involved in Problem II, the computational complexity is usually high. It is even difficult to perform optimization with more than 50 { 0 , 1 } integer variables. Hence we would like to re-lax the problem to other solvable optimization forms. To achieve this purpose, we first reformulate Problem II to its dual form.
Problem III :
In the above,  X  j is defined as  X  j = grangian multiplier for (5) and (6) associated with x j , and z  X  j and z  X  j correspond the Lagrangian multipliers for (7) when the abs operator is expanded. And K is the kernel present some notation first. We denote a new vec-tor  X  =(  X  ; z  X  ; z + ) . We further define P 1 = ( sisting of the columns of X from k 1 to k 2 , and X  X  Diag ( y ) represents the element-wise matrix multiplication of X and Diag ( y ) . We further define a =( 1 l ; 1 m  X  M ( 1  X  d );  X  M d ;  X  M d ) , where 1 k represents a k -dimension col-umn vector with all the elements as 1 . We denote the matrix 0 , C elements as C L . Other symbols are similarly defined. lem by using the above notation. jective can be written to the following problem: where  X ,  X   X  0 are the Lagrangian multipliers.

We can easily obtain the optimal  X  =( P 1 P T 1 )  X  1 ( a +  X   X  B T  X  ) . Substituting the optimum value of  X  into (14), we further get the optimization problem as follows: max
Finally, the above optimization problem can equivalently be transformed to a form similar to the Semi-definite Prob-lem (SDP) by using Schur Complement Lemma [12, 13].
Problem IV : min
Here P is defined as  X   X  and a matrix A 0 means that A is a Semi-definite matrix.
Similar to the work presented in [13], we relax ( yy T ) as rank-one matrix M . We further relax d j  X  X  0 , 1 } to 0  X  d j  X  1 . We can finally write the optimization problem as Problem V:
Problem V : min
Following most optimization methods in SSL [25, 26, 5, 22], we further remove the rank-one constraint, the above problem is exactly an SDP problem. Note that Diag ( y ) ap-pearing in the matrix P can be represented by the elements of M . For example, assume y 1 =1 , then Diag ( y ) can be written as Diag ( M 11 ,M 12 , ..., M 1 n ) . This SDP problem can be solved in polynomial time by some packages such as
Sedumi[21].
In this section, we evaluate our proposed framework on both synthetic and real data. A synthetic example will be firstly presented in order to illustrate the model clearly. We then compare our model with the traditional SSL and the Universum Support Vector Machine (USVM) [24] on benchmark data sets, USPS 2 and MNIST data 3 . For brevity, we name our model as Universum Semi-supervised Learn-ing, in short, USSL from now on. However, we should keep in mind that it is significantly different from the work pre-sented in [27] in that the universum must be known before-hand in their work, while we do not have such requirement. Hence our proposed model presents a more general SSL framework. We implement our model by using a generic convex programming solver CVX. 4 The traditional SSL and the universum SVM are solved based on the package Uni-verSVM. 5
Figure 4. Comparison of SSL and the pro-posed USSL on the synthetic data
We generate three synthetic data sets to validate our pro-posed algorithm. In more details, we obtain the training data for all the three data sets from three two-dimensional Gaussian distributions, which are centered at  X  0 . 3 , 0 , and +0 . 3 respectively. The two types of relevant data are cen-tered at  X  0 . 3 both with the standard deviations as 0 . 13 for each data set, while the irrelevant data are located around 0 , but with standard deviations as 0 . 1 , 0 . 2 , and 0 . 3 respectively for three data sets. The number of training samples for the labeled data and the relevant unlabeled data is respectively set to 5 and 30 for each class in all the three sets. The num-ber of irrelevant unlabeled data samples for all the three cases is also set to 30 . The test data consists of 500 sam-ples for each class, generated from the same distributions as the labeled data. We train our proposed model USSL in comparison with SSL on the training data consisting of both irrelevant and relevant data samples, and evaluate its perfor-mance on the test data sets. In both SSL and USSL, C U and C
L are set to 100.  X  is set to 0 . 2 . Note that again, we do not know which data samples are irrelevant or irrelevant be-forehand. They are merely input as the unlabeled data for training in both USSL and SSL. The above process is re-peated for 20 times and the average accuracy is reported in Figure 4.

It is obvious that the proposed general framework USSL demonstrates much better performance than SSL. The mean error rates of USSL are significantly lower than those of SSL in all the three cases. On the other hand, when the standard deviation increases, USSL tends to approximate the SSL in terms of the error rate, since it is difficult to detect irrelevant data in such cases.
 In order to have a closer examination on the proposed USSL, we also draw the training set including the labeled and unlabeled data, the test data, and the decision bound-aries for one of 20 evaluations in Figure 5. Figure 5(a), (b), and (c) show the training samples for the three sets, where the labeled samples are plotted as  X   X  X  and  X  X  for +1 and  X  1 class respectively, while  X  X  depict the unlabeled in-stances consisting of both relevant and irrelevant samples. Figure 5(d), (e), and (f) show the final class labels for the unlabeled data and the decision boundary given by the tra-ditional SSL. The filled points represent the unlabeled data, but their shapes imply their class, i.e., the filled  X  X  are judged as  X  1 class, while the filled  X   X  X  are classified as +1 class. Similarly, we show the decision boundary given by USSL and the associated final class labels of the unlabeled samples for the three cases in Figure 5(g), (h), and (i) re-spectively. We use the similar symbols to describe different points. The difference is that our proposed USSL is able to indicate which samples are irrelevant. Such irrelevant samples are finally marked as . It is interesting that al-most all the irrelevant samples can be correctly detected by our proposed USSL as observed in these three sub figures. Moreover, the decision boundaries given by USSL are actu-ally more reasonable than the ones derived by the traditional SSL. This can be also observed in Figure 5(j), (k), and (l), which show the test results for the three cases respectively.
In this section, we evaluate the proposed novel model in comparison with the traditional SSL and the USVM [24] the traditional SSL. on real data, the USPS and the MNIST data. We fol-low [24, 19] and exploit the digits of 5 and 8 as the la-beled data and use the remaining digits as the irrelevant data. Hence we have 8 data sets, depending on which cat-egory of digits is used as the irrelevant data. We randomly extract 20 labeled samples and 30 random data points as relevant unlabeled samples from 5 and 8 respectively. We further obtain 30 samples randomly extracted from a cer-tain category of digits other than 5 and 8 . The test data set contains 400 digits randomly extracted from the 5 and 8 digits. The parameters involved in SSL and USSL are searched via cross validation. More specifically, C L and C
U are searched in { 1 , 10 , 100 , 1000 } , while  X  is searched in result averaged on the 10 random evaluations for both USPS and MNIST. In addition, as verified by many researches in Optical Character Recognition [14, 15], especially in hand-written numeral recognition, kernel based methods are just slightly better than the linear classifier, but with signifi-cantly heavier computational cost. 6 Hence, we only con-duct the comparisons based on the linear version of USVM, USSL and SSL in the following.

The evaluation results are reported in Table 1 and Ta-ble 2 for USPS and MNIST respectively. Once again, our proposed USSL outperforms the traditional SSL and the USVM. More specifically, the proposed USSL demon-strates significantly better performance than SSL and USVM in the 0 , 1 , 2 , 3 , 6 , and 7 data sets of USPS ac-cording to a t-test at the 5% significance level. Similarly, a t-test indicates that the result of USSL is also significantly different from those of SSL and USVM in the 0 , 1 , 3 , 4 , 6 , 7 , and 9 data sets of MNIST at the significant level of 5% . SSL simply regards all the unlabeled data as relevant data, while USVM considers all the unlabeled data as uni-versum. Hence it is inappropriate for them to deal with the general unlabeled data containing both relevant and irrele-vant data. In comparison, our proposed approach can auto-matically model the impact caused by the relevant and irrel-evant data into the final decision boundary. It demonstrates superior performance and is more appropriate in handling Semi-supervising Learning from general data.
We have proposed a novel framework that can learn from general unlabeled data. In contrast to the traditional Semi-supervised Learning that requires unlabeled data to share the same category labels as the labeled data, the proposed framework is able to learn from unlabeled data with irrele-vant samples. Moreover, we do not need the prior knowl-edge on which data samples are relevant or irrelevant. Con-
Data set USVM SSL USSL
Data Set USVM SSL USSL sequently it is significantly different from the recent Semi-supervised Learning with universum or the Universum Sup-port Vector Machines. As an important contribution, we have successfully formulated this new learning approach as a Semi-definite Programming problem, making it solv-able in polynomial time. We have also presented theoreti-cal analysis to justify our model. A series of experiments demonstrate that this novel framework has advantages over the Semi-supervised Learning on both synthetic and real data in many facets.

The work described in this paper was fully supported by two grants from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. CUHK4150/07E and No. CUHK4125/07).

