 1. Introduction and music. Finding the requested information from large amounts of multimedia data is challenging. Two types of approaches, i.e., content-based and text-based, are usually adopted in image retrieval ( Goodrum, 2000 ). Content-based image retrieval (CBIR) uses low-level visual features such as color, texture, and shape to represent images. Users can employ example images as queries, or directly specify the weight of low-level visual features to retrieve images. Images that are visually similar to an example image or contain the specified visual features are returned.
In text-based approaches, text is used to describe images and formulate queries. Because images and image representations are in different types of media, media transformation is required. Images are transformed into text, and a text retrieval system is used to index and retrieve images. Textual features can also be derived from the text accompanying an image such as a caption or the surrounding text. Text-based approach encounters the following problems: (1) Image captions are usually short. The short annotation cannot represent the image content completely. (2) Image captions are not always available. Manually assigning captions to images is time consuming and (3) Some visual properties cannot be described directly in captions. For example, the styles of images, e.g., (4) Users X  queries may have different levels of semantics. Users may search for images at a higher semantic
Since images are produced by people familiar with their own language, they can be annotated in different languages. In this way, text-based image retrieval has a multilingual nature. In addition, images are under-standable by different language users. They can resolve the major argument in cross-language information retrieval, i.e., users that are not familiar with the target language cannot understand the retrieved documents. In such a situation, cross-language image retrieval has attracted researchers X  attentions recently and is orga-nized as one of evaluation tasks in the Cross-Language Evaluation Forum (CLEF) ( Clough, Sanderson, &amp; Mu  X  ller, 2005 ). In addition to media transformation, language translation is also necessary to unify the language usages in queries and documents in cross-language image retrieval.

Textual and low-level visual features have different semantic levels. Textual feature is highly semantic, while low-level visual feature is less semantic and is more emotive. These two types of features are complementary and provide different aspects of information about images. In this paper, we explore the integration of textual and visual information in cross-language image retrieval. An approach that automatically transforms textual que-ries into visual representations is proposed. The generated visual representation is treated as a visual query to retrieve images. The retrieved results using textual and visual queries are combined to generate the final result.
The rest of this paper is organized as follows. Section 2 introduces the proposed model. The integration of textual and visual information is illustrated. Section 3 models the relationships between text and images. How to generate visual representation of a textual query is introduced. Section 4 specifies the experimental mate-rials. Section 5 shows the experiment designs. Here, the selection of suitable textual query terms to construct visual queries is the major issue. In addition, three types of experiments are evaluated, including monolingual image retrieval, cross-language image retrieval and ideal visual queries. Finally, we conclude our work in Section 6 . 2. Integrating textual and visual information
Several hybrid approaches that integrate visual and textual information have been proposed. A simple approach conducts text-and content-based retrieval separately and merges the retrieval results of the two runs parallel approach, a pipeline approach employs textual or visual information to perform initial retrieval, and then uses the other feature to filter out the irrelevant images ( Lowlands Team, 2001 ). In the above two approaches, users have to issue two types of queries, i.e., textual and visual. In these approaches, sometimes it is not intuitive to find an example image or to specify low-level visual features.

We take another approach as shown in Fig. 1 with our cross-language image retrieval system. This system automatically transforms textual queries into visual representations. First, the relationships between text and images are mined from a set of images annotated with text descriptions. A trans-media dictionary which is similar to a bilingual dictionary is set up from the training collections. When a user issues a textual query, the system automatically transforms the textual query into a visual one using the trans-media dictionary. The generated visual representation is treated as a visual query and is used to retrieve images. In this way, we have both textual and visual queries.
 is textual index of image descriptions, and the other one is visual index of images. A textual query is used to retrieve image descriptions using the textual index. A visual query transformed from the textual query retrieves images using the visual index. The retrieval results of textual and generated visual queries are merged together. language information retrieval, translation ambiguity and target polysemy problems ( Chen, Bian, &amp; Lin, 1999 ) have to be tackled in the translation process. If a word is not translated correctly, we cannot capture the correct meaning of the word. If the translation is polysemous, the undesired documents that contain the translation with other senses could be reported even if the translation is correct. Visual queries could be helpful to reduce these problems. 3. Visual representation of text original (or the translated) text. For an image, a term in the description may relate to a portion of an image. If we divide an image into several smaller parts, e.g., blocks or regions, we could link the terms to the corre-sponding parts. This is analogous to word alignment in a sentence aligned parallel corpus. Here the word alignment is replaced with the textual-term/visual-term alignment. If we treat the visual representation of an image as a language, the textual description and visual parts of an image are an aligned sentence. The cor-relations between the vocabularies of two languages can be learned from the aligned sentences. Given a picture of sunset, for example, we can link textual feature  X  X  X unset X  X  to visual feature  X  X  X ed circle X  X . and visual representation, and generate text descriptions from images. Mori, Takahashi, and Oka (1999) divided images into grids, and then the grids of all images are clustered. Co-occurrence information is used to estimate the probability of each word for each cluster. Duygulu, Barnard, Freitas, and Forsyth (2002) used blobs to represent images. First, images are segmented into regions using a segmentation algorithm. All regions are clustered and each cluster is assigned a unique label (blob token). EM algorithm constructs a prob-ability table that links blob tokens with word tokens. Jeon, Lavrenko, and Manmatha (2003) proposed a cross-media relevance model (CMRM) to learn the joint distribution of blobs and words. They further proposed continuous-space relevance model (CRM) that learned the joint probability of words and regions, rather than blobs ( Lavrenko, Manmatha, &amp; Jeon, 2003 ).
 This paper considers blobs as a visual representation of images, and adopts Blobworld ( Carson, Belongie, Greenspan, &amp; Malik, 2002 ) to segment an image into regions. Blobworld groups pixels in an image into regions which are coherent in low-level properties such as color and texture, and which roughly correspond to objects or part of objects. For each region, a set of features such as color, texture, shape, position, and size are extracted. The regions of all images are clustered by the K-means clustering algorithm. Each cluster is assigned a unique number, i.e., blob token, and each image is represented by the blob tokens.

Given the textual descriptions and blob tokens of images, we mine the correlation between textual and visual information. Mutual Information (MI) is adopted to measure the strength of correlation between an image blob and a word. Let x be a word and y be an image blob. The Mutual Information of x and y is defined as follows: where p ( x ) is the occurrence probability of word x in text descriptions, p ( y ) is the occurrence probability of blob y in image blobs, and p ( x , y ) is the probability that x and y co-occur in aligned image-text description pairs.
 After the MIs between words and blobs are computed, we can generate related blobs for a given word w i . The blobs whose MI values with w i exceed a threshold are associated to w i . The generated blobs can be regarded as the visual representation of w i . In this way, a trans-media (word-blob) dictionary is established.
Fig. 2 shows an example image, which has nine regions. These regions cluster into four groups, each of which is assigned a blob visual term. Hence the nine regions can be represented by the following visual terms: B01, B02, B02, B03, B03, B04, B04, B04, and B04. The corresponding text description of the image is:  X  X  X are and foal in field, slopes of Clatto Hill, Fife X  X . After textual-term/visual-term alignment, the following possible pairs remain: hill () B02, mare () B03, foal () B03, field () B04, slope () B04 4. Experimental materials In the experiments, we adopt the 2004 and 2005 ImageCLEF test sets ( Clough et al., 2005, in press ). The image collection consists of 28,133 photographs from St. Andrews University Library X  X  photographic collection, which is one of the largest and most important collections of historic photography in Scotland. The majority of images (82%) are in black and white. All images are accompanied by a caption written in English by librarians. The information in a caption ranges from specific date, location, and photographer to a more general description of an image. Fig. 3 shows an example of an image and its caption in the St. Andrews image collection. The text descriptions are semi-structured and consist of several fields including document number, headline, record id, description text, category, and file names of images in a 368  X  234 large version and 120  X  76 thumbnail version.
 sentence or phrase describing the search request in a few words), and (2) a narrative (a description of what constitutes a relevant or non-relevant image for each request). In addition to the text description for each topic, one and two example images are provided for 2004 and 2005 topic sets, respectively.
 experiments, queries are in Chinese. Fig. 4 illustrates a topic in English and in Chinese.
 set into five categories, including queries modified by photographer or date (1 X 4), queries modified by location (5 X 11), queries related to specific events (12 X 15), queries related with known-items (16 X 18), and queries related to general topics (19 X 25). Queries in the 2005 topic set are more general and more visual than those in the 2004 topic set ( Clough et al., in press ). More than half of them not only consider the objects in the images, but also their spatial or event relationships. The followings show some examples: (5) Query  X  X  X ishermen in boat X  X  specifies at least two people fishing in a boat or ship. Pictures of just one fish-5. Experiments 5.1. Monolingual image retrieval In the experiments, we adopt title field in topics to retrieve images. Okapi IR system ( Robertson, Walker, &amp; the English captions, are used for indexing. All words are stemmed and stopwords are removed. For visual index, the blob tokens of each image are indexed. The weighting function used is BM25.

We evaluate our approach in monolingual image retrieval on the 2004 topic set at first. The correlations between text and images are learned from St. Andrews image collection. The title field of a topic is used as a query to retrieve images. For each textual query, a visual query is generated from the query terms according to the mined relationships. The first issue is which query terms are adopted to generate the visual query. Intu-itively, we can generate visual representation for each query term. However, not all query terms are related to the visual content of images. Here, we employ part-of-speech (POS) to select suitable query terms to generate visual representations. Brill tagger ( Brill, 1995 ) is used to tag English topics. Different types of POSes are explored to tell which types of query terms are useful. Nouns only (without named entities), nouns with named entities, verbs only, adjectives only, or nouns, verbs, along with adjectives are experimented.

For each selected query term, the top n blobs of MI values exceed a threshold t are regarded as its visual representation. The values of parameter n from 10 to 40 and t from 0.1 to 0.4 are experimented. The blobs corresponding to the selected query terms form a visual query. It is used to retrieve images using visual index. The results of textual and generated visual queries are merged into the final result. For each image, the sim-ilarity scores of textual and visual retrieval are normalized and linearly combined using weights 0.9 and 0.1 for the textual and visual runs, respectively. The top 1000 images of the highest combined scores are reported.
The performance of the proposed approach is shown in Fig. 5 . Fig. 5 (a) X (d) demonstrate 10, 20, 30, and 40 blobs are selected for each query term, respectively. Mean average precision (MAP) measures the retrieval per-formances. The approach of using nouns only, higher threshold and more blobs has better performance than that of using verbs and adjectives. The performances of using verbs or adjectives only in different setting of n tives in four topics and nine verbs in eight topics, and the MI values of blobs with verbs and adjectives tend to be low. When using nouns, verbs and adjectives, the performance is slightly worse than using nouns only. The performance is dropped when named entities are added. It is even worse than using all words with stopword removal (ALL-SW).
 using textual query only, the MAP is increased. The performances of textual query and generated visual query are shown in Table 1 . The results show that the proposed approach increases retrieval performance. Although the generated visual queries are not so good enough, the integration of them is useful to improve retrieval per-formance. Several factors may affect the visual query construction. First, the image segmentation has a large effect. Because the majority of images in the St. Andrews image collection are in black and white, that charac-teristic makes image segmentation more difficult. Second, clustering affects the performance of the blobs-based approach. If image regions that are not similar enough are clustered together, the cluster (blob) may have sev-eral different meanings. That is analogous to the polysemy problem on word level ( Chen et al., 1999 ).
Table 1 also shows the performance of the same approach on the 2005 topic set. The MAP of using textual queries is decreased to 0.3952. It confirms that the 2005 topic set containing more general and visual queries is more challenging than the 2004 topic set. The MAP of the generated visual queries is 0.0215, which is larger than that of the 2004 topic set. However, the overall performance of integrating textual and generated visual queries is a little worse than that of textual queries only. This might be due to that the spatial and action relationships among objects in the images are crucial in the 2005 topic set and our approach only lists the pos-sible objects, but does not consider their relationships. 5.2. Cross-language image retrieval
In the experiments of cross-language image retrieval, Chinese queries are used as source queries and trans-lated into English to retrieve English captions of images. First, we deal with the 2004 topic set. The Chinese queries are segmented by a word recognition system and tagged by a POS tagger. Named entities are identified by a Chinese NER tool ( Chen, Ding, Tsai, &amp; Bian, 1998 ). For each Chinese query term, we find its translation equivalents using a Chinese X  X nglish bilingual dictionary. If a query term has more than one translation, the first two translations with the highest frequency of occurrences in the English image captions are considered as the target language query terms.

For those named entities that are not included in the dictionary, a similarity-based backward transliteration name and the keyword parts of a named entity. The keyword parts are general nouns, and are translated by dictionary lookup as described above. The name parts, which are transliterations of foreign names, are trans-literated into English using similarity-based backward transliteration. Total 3599 English names from the image captions are extracted. Given a transliterated name, 300 candidate names are selected from the 3599 names using an IR-based candidate filter ( Lin et al., 2005 ). We transform the transliterated name and candi-date names to International Phonetic Alphabet (IPA), and compute the similarities between IPA representa-tions of the transliterated name and candidate names. The top 6 candidate names with the highest similarity are chosen as the original names.

Visual queries are generated from Chinese queries. In order to learn the correlations between Chinese words and blob tokens, English image captions are translated into Chinese by SYSTRAN machine translation sys-tem. Similarly, POS selects query terms for visual query construction. Fig. 6 (a) X (d) shows the values of param-eter n from 10 to 40 and t from 0.01 to 0.04 are experimented. The performances of term selection strategies are similar to that of monolingual image retrieval. Using nouns only to generate visual query has better performance than using verbs and adjectives only. When n P 30, using nouns, verbs and adjectives together performs better than using nouns only. The best performance is 0.4441 when using nouns, verbs and adjec-tives, n = 30, and t = 0.02. The performances of textual query and generated visual query are shown in Table 2 . In cross-language experiment, the improvement of retrieval performance is not as well as monolingual experiment. One of the reasons is that the quality of training data is not good. We use a famous machine trans-lation system to translate image captions. However, there are still many translation errors that affect the cor-rectness of learned correlations. Table 2 also lists the performance on the 2005 topic set. The MAP of the 2005 test set is worse than that of the 2004 test set. That is consistent to the monolingual case due to that the former topic set covers more general and visual information needs than the latter one ( Clough et al., in press ).
The improvement is verified by Wilcoxon signed-rank test, which considers the sign and the magnitude of the rank of the difference between pairs of measurements. In the 2004 test set, we can generate visual queries for 18 topics. Table 3 shows that 14 topics have nonzero performance differences after integrating visual que-ries. The observed value of z is computed as follows, where W is the sum of signed ranks, and N is sample size.
For the example, with N =14, W = 61, and r W = 31.86, the result 1.90, which passes the critical value 1.645, is significant beyond level 0.05.
 use only a part of query terms to generate visual query, thus some information is lost. In some topics, the retrieved images are not relevant to the topics, while they are relevant to the query terms that are used to gen-erate visual query. Take query 13 of the 2004 topic set, i.e.,  X  X 1939  X  X  (The
Open Championship golf tournament, St. Andrews 1939), as an example. Terms  X  X   X  X  (St),  X  X   X  X  (golf) and  X  X   X  X  (Open Championship) are tagged as nouns, thus they are selected to generate visual query. Of the top 10 returned images shown in Fig. 7 , nine images are about the Open Championship golf tournament, but they are not the one held in 1939. The date/time expression is hard to be captured by visual features. It shows that using visual information only is not enough, integrating textual information is needed.
Since the performance of generated visual query depends on image segmentations, blob clustering, and so on, we create an ideal query from relevant images to test if a visual query can help increase the performance of image retrieval. A useful visual query will exist if the relevant images for a query share some image features. The common image features can help us retrieve the relevant images well. We use v 2 score to select blobs from relevant images of the 2004 topic set. For each query we generate 10 blobs whose v 2 scores are larger than 7.88 ( v =1, p = 0.005). The selected blobs form a visual query to retrieve images. The retrieval result is combined with that of a textual query. The performances are shown in Table 4 . The results show that a good visual query can improve performance of image retrieval. 5.3. More experiments and discussions
Fig. 8 shows the different combinations of media transformation (denoted by thick lines) and language translation (denoted by thin lines) using a trans-media dictionary approach. The first column and the last column denote the query and the data collection, respectively. In addition, the first row and the last row denote text and image, respectively. Query 1 is a textual query in source language, and Query 5 is a visual query. The text descriptions of images are in target language. There are two alternatives (denoted by dotted lines) to construct a trans-media dictionary. The first one is translating the text description through a bilingual dictionary, which is similar to document translation in traditional CLIR, and then mining the relationship between textual terms and visual terms. The second one is: mining the relationship between textual terms and visual terms directly without translation. Trans-Media Dictionaries 1 and 2 in Fig. 8 belong to the first and the second alternatives, respectively.
 query. In the counter part, Query 5, a visual query, is transformed into Query 6, a textual query in source lan-guage, and then Query 6 is translated into Query 7, a textual query in target language. Before looking up Trans-Media Dictionary 2, Query 1 has to be translated into Query 3, a textual query in target language. Then
Query 3 is transformed into Query 4, a visual query. Consulting Trans-Media Dictionary 2, Query 5 can also be transformed into Query 8, a textual query in target language. The combinations are summarized as follows. tion retrieval. Because the experiments in Section 5.2 depict that selecting nouns, verbs and adjectives for transformation and adopting more blobs (e.g., 40) has the better performance, the following experiments only consider those cases. Table 5 shows the experimental results. The MAPs of Query 3 and Query 5 on the (2004, 2005) topic sets are (0.4395, 0.2399) and (0.0523, 0.0633), respectively. Query 2 and Query 4 are in the same medium, i.e., image. However, Query 4 is generated through both query translation and medium transforma-tion, thus more noise is introduced and the MAP of Query 4 is worse than that of Query 2. Similarly, Query 8 and Query 7 are in the same medium, i.e., text. Query 7 is generated from Query 5 by two operations, so that the MAP of Query 7 is worse than that of Query 8. Compared Query 4 and Query 7, both resulting from query translation and medium transformation, the former is worse than the latter. In other words, medium trans-formation first and then language translation is better than language translation first and then medium transformation. 6. Conclusion
This paper explores the uses of both textual information and visual features for cross-language image retrie-val. We conduct English monolingual and Chinese X  X nglish cross-language retrieval experiments to evaluate our approach. Experimental results show that combining retrieval results of textual and generated visual que-ries improves retrieval performance. The generated visual query has little impact in the cross-lingual experi-ments. One of the reasons is that using a machine translation system to translate English captions into Chinese introduces many translation errors that affect the correctness of learned correlations. We also con-struct an ideal visual query from relevant images. Using the ideal visual query increases retrieval performance about 12.3% in monolingual and 8.8% in cross-language image retrieval. The results show that a good visual query can improve performance of image retrieval.

We use POS to select query terms for constructing a visual query. Experiments show that nouns are appro-priate to generate visual queries, while using named entities is useless. Nouns usually indicate the objects in images, which is the kernel of an image, thus it is reasonable to link nouns to the image regions which corre-spond to objects. Named entities, such as personal name, location name, and date, do not have strong rela-tions with image regions, and cannot be represented well by visual representations. In this way, the visual representations of named entities introduce noise and decrease the retrieval performance. Similarly, verbs that indicate actions are rarely represented by visual features. Thus, verbs are not feasible for visual query gener-ation. Some adjectives that are relative to visual features could be used to generate visual queries. For exam-query terms. Semantic information which may provide more clues for term selection is not used. We will inves-tigate query term selection on the semantic level in the future.
 Acknowledgement Research of this paper was partially supported by National Science Council, Taiwan, under the contracts NSC94-2213-E-002-076 and NSC95-2752-E-001-001-PAE.
 Appendix. Topics in the experiments enclosed in parentheses are translated from English ones by human. References
