 1. Introduction trieved documents is relevant and then learn from the pseudo-relevant documents to expand terms or to assign better weights to the original query. This is similar to the process used in relevance feedback, when actual relevant documents the top 10 documents (P@10) is 0.5, this means that five of them are non-relevant. This is common and even expected in all retrieval models. When combined with pseudo-relevance feedback, this noise, however, can cause the query representa-tion to  X  X  X rift X  X  away from the original query.

This paper describes a deterministic sampling method based on overlapping clusters to select better documents for pseudo-relevance feedback. The sampling unit is a document cluster from the initial retrieval set which can represent an aspect of a  X  overlapped clusters for the top-retrieved documents and repeatedly using the dominant documents that appear in multiple highly-ranked clusters, we expect that an expansion query can be represented to emphasize the core topics of a query.
This is not the first time that clustering has been suggested as an improvement for relevance feedback. In fact, clustering use clusters have not improved effectiveness. The work presented here is based on a new approach to using the clusters that produces significantly better results.

Our motivation for using overlapping clusters and resampling is as follows: the top-retrieved documents are a query-ori-ented ordering that does not consider the relationship between documents. We view the pseudo-relevance feedback prob-decision boundary, depending on training examples. There are two stages to expand query terms based on the pseudo-rel-evance feedback framework: (1) Classifying pseudo-relevant documents for the initial retrieval set. (2) Extracting an expan-sion query for the classified pseudo-relevant documents. Here, we focus on unsupervised learning with pseudo-relevant documents. We approach this problem by repeatedly selecting dominant documents biased toward good representative doc-hard examples to change the decision boundary toward hard examples. For the query expansion problem, it is important to use good representative documents with high agreement in the initial set, resulting in better representative expansion terms for the topic of a query.

The hypothesis behind using overlapped document clusters is that a good representative document for a query may have forming clusters, this document may be dominant for this topic. Repeatedly sampling dominant documents can emphasize the topics of a query, rather than randomly resampling documents for feedback.

We show that resampling feedback documents based on clusters contributes to higher relevance density for feedback documents on a variety of TREC collections. The results on large-scale web collections such as the TREC WT10g and GOV2 collections show significant improvements over the baseline relevance model.

The rest of the paper is organized as follows: Section 2 presents related work. Section 3 describes a cluster-based resam-the experimental results by studying relevance density. We conclude in Section 6 . 2. Related work
Our approach is related to previous work on pseudo-relevance feedback, resampling approaches, and the cluster hypoth-esis in information retrieval.

Relevance feedback (RF) and pseudo-relevance feedback (PRF) have been shown to be effective ways of improving retrie-val accuracy by reformulating an original query using relevant or pseudo-relevance documents from the initial retrieval re-judgment procedure, core reformulation algorithm, and multiple iterations on the terabyte document collection. In TREC (2008) , the relevance feedback task was to compare RF algorithms with exactly the same relevance judgments, and RF results for multiple amounts of relevance information  X  i.e. one relevant document or 10 judged documents with at least three rel-2008 ). In TREC (2009) , the focus shifted to methods for finding good documents for each topic and the impact of different documents on algorithms. Specifically, sites compared results using the documents they found with those found by other sites X  algorithms. Preliminary results indicate that the relationship between which documents are used and success of an workshop ( Buckley &amp; Harman, 2004 ), there were comparative experiments on the effects of several factors for pseudo-rel-evance feedback. The report provides the effects of the number of documents, the number and source of terms used, the ini-tial set of documents, and the effects of swapping documents and clusters by document clustering and passage-level clustering. The experimental setup is too complex to see the individual effects of clusters, since an outside source factor itself affects the performance. Thus the analysis for the comparative experiments is inconclusive.

Traditional pseudo-relevance feedback algorithms ( Robertson et al., 1996 ) are based on the assumption of relevancy for the top-retrieved documents.Research has been conducted to improve traditionalPRF by using passages ( Allan, 1995; Yeung et al., tionmethod( Tao&amp;Zhai,2006 ),bydrawingstatisticsfromexternalcorpora( Diaz&amp;Metzler,2006 ),andbyusinglatentconcepts
Recently there has been some work on sampling and resampling techniques for the initial retrieval set. A selective sam-pling method by Sakai, Manabe, and Koyama (2005) skips some top-retrieved documents based on a clustering criterion. The cluster is generated not by document similarity but all members of a cluster containing the same subset of query terms. The sampling purpose is to select a more varied and novel set of documents for feedback. Their assumption is that the top-ranked documents may be too similar or redundant. However, their results did not show significant improvements on NTCIR col-lections. Our approach of repeatedly and deterministically using dominant documents is based on a different assumption.
A deterministic sampling is a sampling which behaves predictably ( Vishkin, 1991 ). Given a particular input, it will always produce the same output. In applications such as motion planning and verification problems in robotics and graphics, deter-2004 ). A resampling method suggested by Collins-Thompson and Callan (2007) uses bootstrap sampling on the top-retrieved documents for the query and variants of the query obtained by leaving a single term out. The assumption behind query vari-ants is that one of the query terms is a noise term. From their experimental analysis, the main gain is from the use of query variants, not document resampling. Their results on robustness and precision at 10 documents (P@10) show improvements, but the performance in terms of mean average precision (MAP) is lower than the baseline relevance model on TREC collec-tions. Our approach primarily focuses on the effects of resampling the top-retrieved documents. Recently, a boosting ap-proach proposed by Lv, Zhai, and Chen (2011) combined different document weighting strategies using a loss function defined to directly measure both robustness and effectiveness to improve the overall effectiveness of pseudo-relevance feed-back without sacrificing the performance of individual queries too much.
 On the other hand, many information retrieval techniques have adopted the cluster hypothesis to improve effectiveness. by the likelihood of generating the query. The results show improvements over the query-likelihood retrieval model on TREC so that topically related documents receive similar scores. The results on TREC collections show that regularized scores are significantly better than the initial scores. Our work is closely related to document re-ranking using cluster validation and mation ( Kurland &amp; Lee, 2004 ), re-ranking method using cluster-based language models within a graph-based framework 2011 ).

There has also been work on term expansion using clustering in the vector space model ( Buckley, Mitra, Walz, &amp; Cardie, 1998; Lynam, Buckley, Clarke, &amp; Cormack, 2004; Yeung et al., 2004 ). At TREC 6, Buckley et al. (1998) used document clus-tering on SMART though the results of using clusters did not show improvements over the baseline feedback method. Re-cently, a cluster-based query expansion method ( Kalmanovich &amp; Kurland, 2009 ) combined document clusters that are created offline and the top-retrieved documents for pseudo-relevance feedback in the relevance model. and average precision, and analyzing results for each query. 3. A cluster-based deterministic resampling framework for feedback
This section describes the rationale for the method for a deterministic sampling and our sampling procedure. 3.1. A deterministic sampling approach
The main issues in pseudo-relevance feedback are how to select (likely) relevant documents from the top-retrieved doc-uments, and how to select expansion terms. Here we deal with the problem of selecting better feedback documents.
The problem in traditional pseudo-relevance feedback is obtaining a set of expansion terms from the top-retrieved doc-uments that may have low precision. If a method can select better documents from the given sample, it can almost certainly contribute better expansion terms. For pseudo-relevance feedback, the initial retrieval set can be seen as the sample space from which we estimate the sampling distribution of documents.

In statistics, resampling (bootstrapping) ( Efron, 1979 ) is a method for estimating the precision of sample statistics by sampling randomly with replacement from the original sample, leading to robust estimates. If a method is available for selecting better examples from the original sample space, deterministic resampling will perform better than random sam-pling. In some cases, deterministic sampling has shown benefits compared to random sampling ( Vishkin, 1991; Yershova &amp; ative procedure used to adaptively change the distribution of training examples so that the weak learners focus on examples that previous weak learners misclassified. In contrast, in the query expansion problem, it is important to change the distri-bution toward good representative documents for a query. We assume that a dominant document for a query is one with ters, a dominant document will appear in multiple highly-ranked clusters. Since a topic can contain several subtopics, the topic clusters, so we call that document dominant . From such a dominant document, terms that retrieve documents related to all subtopics can be selected as an expanded query.

Based on the above assumption, we deterministically sample documents for feedback using k -nearest neighbors ( k -NN) clustering to generate overlapped clusters from the given top-retrieved documents space. 3.2. Deterministically resampling feedback documents using overlapping clusters A cluster-based resampling method to get novel pseudo-relevant documents is based on the language model ( Ponte &amp; works. The essential point of our approach is that a document that appears in multiple highly-ranked clusters will contribute more to the query terms than other documents. The resampling process proceeds as follows: (1) Deterministically constructing a sample space by selecting top-ranked N documents for each query based on language (2) Deterministically constructing sampling units by k -NN clustering based on the similarities of documents in the sample (3) Deterministically sampling clusters by selecting top-ranked M clusters based on the cluster-based language model. All (4) Deterministically sampling expansion terms by selecting top-ranked E terms based on the relevance model.
The following are the details of each step. 3.2.1. Constructing a sample space First, documents are retrieved for a given query by the query-likelihood language model ( Ponte &amp; Croft, 1998 ) with of documents. (In our experiments, the size of sample space N is set to 100.)
A statistical language model is a probabilistic distribution over all the possible word sequences for generating a piece of text. In information retrieval, the language model treats documents themselves as models and a query as strings of text gen-erated from these document models. The popular query-likelihood retrieval model estimates document language models using the maximum likelihood estimator. The documents can be ranked by their likelihood of generating or sampling the query from document language models: P ( Q | D ).
 where q i is the i th query term, m is the number of words in a query Q , and D is a document model.
Dirichlet smoothing is used to estimate non-zero values for terms in the query which are not in a document. It is applied to the query likelihood language model as follows.
 denote the frequency of a word w in D and Coll , respectively. The smoothing parameter is learned using training topics on each collection in experiments. 3.2.2. Constructing sampling units the sample space to find dominant documents. Here, a sampling unit is that cluster considered for selection in the next stage of sampling. Note that one document can belong to several clusters.
 In k -NN clustering, each document plays a central role in making its own cluster with its k closest neighbors by similarity.
We represent a document using tf.idf weighting and cosine normalization. The cosine similarity is used to calculate similar-ities among the top-retrieved documents.

Our hypothesis is that a dominant document may have several nearest neighbors with high similarities, participating in several clusters. On the other hand, a non-relevant document ideally makes a singleton cluster with no nearest neighbors with high similarity, though in practice it will have neighbors due to noise such as polysemous or general terms. Document member of several clusters and the clusters are highly related to the query, we assume it to be a dominant document. The cluster-based resampling method repeatedly uses such dominant documents based on document clusters. 3.2.3. Deterministically sampling clusters described below. The top-ranked M clusters are selected. All the documents in the sampled clusters are used for feedback with redundancy. Note that clusters are used only for selecting feedback documents.

A cluster can be treated as a large document so that we can use the successful query-likelihood retrieval model. Intui-tively, each cluster can be represented by just concatenating documents which belong to the cluster. If Clu represents such a cluster, then: where freq ( w , Clu ) is sum of freq ( w , D ) for the document D which belongs to the cluster Clu . 3.2.4. Sampling expansion terms
Finally, expansion terms are selected using the relevance model for each document in the sampled clusters. Note that the set of feedback documents chosen from the selected clusters are used to estimate the relevance model with their initial query-likelihood probabilities.

A relevance model is a query expansion approach based on the language modeling framework. Relevance models have been shown to be a powerful way to construct a query model from the top-retrieved documents ( Diaz &amp; Metzler, 2006; Lav-query Q . In the model, the query words q 1 ... q m and any word w in relevant documents are sampled identically and inde-pendently from a distribution R . Following that work, we estimate the probability of a word in the distribution R using where R is the set of documents that are pseudo-relevant to the query Q . We assume that P ( D ) is uniform over the set.
After this estimation, the most likely e terms from P ( w | R ) are deterministically selected as the expansion terms for an original query. The final expanded query is combined with the original query using linear interpolation, weighted by a parameter k . The combining parameter is learned using training topics on each collection in the experiments.
The original relevance model and traditional pseudo-relevance feedback methods use the initial retrieval set to get expan-sion terms directly after the first step. The problem is that the top-retrieved documents contain non-relevant documents, which add noise to expansion terms. Our effort uses overlapping clusters to reuse dominant documents and to skip non-dominant documents for the query to emphasize good representative terms in dominant documents and to deemphasize terms in non-dominant documents. It may still find non-relevant documents, but we will show it finds fewer of them. 4. Experiments
To validate the proposed method, we performed experiments on five TREC collections and compared the results with a baseline retrieval model, a baseline feedback model, and an upper-bound model. 4.1. Experimental set-up 4.1.1. Test collections
We tested the proposed method on homogeneous and heterogeneous test collections: the ROBUST, AP and WSJ collec-tions are smaller and contain newswire articles, whereas GOV2 and WT10G are larger web collections. For all collections, the topic title field is used as the query. A summary of the test collections is shown in Table 1 .
 are stemmed using a Porter stemmer. A standard list of 418 common words is removed at retrieval time. 4.1.2. Training and evaluation to 100), is deterministically constructed by the language model for each query from the document collection.
In order to find the best parameter settings, we sweep over values of the smoothing parameter for the language model ( l 2 {500,750,1000,1500,2000, ... ,5000}), the number of feedback documents which can be deterministically selected from the sample space for the relevance model (| R | 2 {5,10,25,50,75,100}), the number of expansion terms, which is the final re-sult of drawing samples by the relevance model ( e 2 {10,25,50,75,100}), and the weight of the original query ( k 2 {0.1,0.2, ... ,0.9}). To train the proposed model, we sweep over the number of feedback clusters that can be determin-documents since one cluster can have at most five documents as a member ( k = 5; the maximum size of a sampling unit is 5) following Indri query form: where q 1 ... q m are the original query terms, t 1 ... t combining the original query and the expanded query.
 All comparison methods are optimized on the training set using mean average precision (MAP) defined as, where AP ( q ) is (uninterpolated) average precision for a query q in the topic set Q .

The best parameters on training for each test collection shown in Table 2 are used for experimental results with the test topics. 4.1.3. Comparisons 4.1.3.1. Baselines. We provide two baselines: the language model and the relevance model.

Language model ( LM ): The performance of the baseline retrieval model. The relevance model and the resampling method use this baseline result as the starting point for selecting documents for feedback and clustering.

Relevance model ( RM ): The performance of the baseline pseudo-relevance feedback model. The expanded query is com-bined with the original query; this formulation of relevance modeling is commonly referred to as RM3 ( Diaz &amp; Metzler, 2006 ). The resampling method is based on the relevance model framework. The difference is the pseudo-relevant docu-ments used. 4.1.3.2. Upper-bound: true relevance feedback. To investigate the performance of the upper-bound of the proposed method, we compare with true relevance feedback.

True relevance feedback ( TrueRF ): The performance using actual relevant documents in the top-retrieved 100 documents, where relevance is determined by the provided TREC judgments. This performance presents the upper-bound when using the relevance model. cluster-based reranking method.

Reranking using clusters ( Rerank ): The performance of reranking by combining query likelihoods for documents and clus-ters based on k -NN clusters for the top-retrieved N documents. N and k are set to 1000 and 5, respectively. Since a dominant document can be a member of several clusters, we choose the maximum query likelihood for the clusters
Clu which the document D belongs to. The cluster-based reranking method ( Lee et al., 2004 ) based on the vector-space model has shown good results. Here we applied the reranking method to the language model.
 We also compared the cluster-based language model (CBLM) by linear combination of Liu and Croft (2004) . the cluster-based language model corresponds to Eq. (2) of the language model.

The reranking method shows the effects of dominant documents without the feedback procedure. 4.2. Experimental results
The results for the comparison methods on five test collections are presented in Table 3 . The Resampling method signif-icantly outperforms LM on all test collections, whereas RM does not significantly outperform LM on WT10g collection. For
GOV2 and WT10g heterogeneous web test collections, Resampling significantly outperforms RM . The relative improvements over RM are 6.28% and 19.63% on GOV2 and WT10g, respectively. For the ROBUST newswire collection, Resampling shows slightly lower performance than RM . For AP and WSJ newswire collections, Resampling shows small, but not significant improvements over RM . In the precision at 5 (P@5) evaluation metric as shown in Table 6 , Resampling shows 14.8%, 24.7%, 3.9%, 20.0%, and 11.9% improvements over LM , whereas RM shows 7.1%, 7.4%, 1.6%, 18.8% and 7.4% improvement on GOV2, WT10g, ROBUST04, AP and WSJ collections, respectively.

The Rerank method using clusters shows significant improvements over LM on all test collections. In fact, Rerank outper-forms RM on WT10g collection. The results indicate that document clustering can help find relevant document groups for the ing relevant groups. As shown in Table 4 , we conducted an experiment to compare the cluster-based language model by lin-ear combination of Liu and Croft (2004) in Eq. (11) to show a strong baseline for cluster-based reranking. The proposed reranking method outperforms the reranking by linear combination.

TrueRF shows significant improvements over all methods on test collections. The results provide upper-bound perfor-mance on each collection, showing what might happen if we are able to choose better pseudo-relevant documents, approaching the set of true relevant documents.

We have also examined the effectiveness as the number of feedback documents varies. The best parameters ( l , e , and k ) learned on each collection are set for test queries since our efforts is on selecting pseudo-relevant documents. As shown in Fig. 1 , Resampling achieves better performance over RM regardless of the number of feedback documents on GOV2 and
WT10g collection. On ROBUST, AP and WSJ collection, RM shows better performance in 25 feedback documents, but Resam-pling shows better performance as the number of feedback documents increases.

Fig. 2 shows the performance of each query for LM , RM , Resampling , and TrueRF . TrueRF is an upper-bound performance of the relevance model with the true relevant documents contained in the top 100 retrieved documents. On GOV2 collection,
Resampling shows relatively stable improvements over RM overall, although some queries perform worse using Resampling compared to RM . On WT10g collection, Resampling shows some variations in performance depending on queries. Basically, the performance of Resampling depends on the accuracy in clustering stage. 4.3. Retrieval robustness
We analyze the robustness of the relevance model and the resampling method over the baseline language model. Here, retrieval robustness is defined as the number of queries whose performance is improved (i.e. # of improved queries/# of total 2007 ).

Overall, our resampling method improves the effectiveness over the language model for 82%, 61%, 63%, 66%, and 70% of the queries for GOV2, WT10g, ROBUST, AP and WSJ, respectively.

Fig. 3 presents an analysis of the robustness of the relevance model, the resampling method and true relevance feedback model. The resampling method shows strong robustness for GOV2. For the GOV2 collection, Resampling improves 41 queries and hurts 9, whereas RM improves 37 and hurts 13, and TrueRF improves 45 and hurts 5. For the WT10g and WSJ collection, although RM improves the performance of 2 more queries than Resampling , the amount of improvement in average precision for each query exhibited by Resampling is significantly larger for WT10g. For the ROBUST collection, Resampling improves 63 and hurts 36, whereas the relevance model improves 64 and hurts 35.
 The average effectiveness (i.e. performance improvement in average precision for each query) of overall queries of RM and Resampling over the language model are 12.45% and 23.09% on GOV2, 9.07% and 55.04% on WT10g, 39.28% and 30.96% on ROBUST, 35.65% and 36.23% on WSJ, and 48.79% and 81.09% on AP collection.

On GOV2, WT10g, and AP collections in the amount of improvement, our resampling method shows strong effectiveness, although it is less robust for the ROBUST collection. However, RM and Resampling do hurt some queries since the pseudo-relevance feedback model tends to sensitive to the initial retrieval result for a query. Resampling method showed higher var-iance in performance improvement than RM on WT10g since non-relevant documents can be repeatedly used for feedback in the cluster-based resampling method. It can be overcome by finding optimal clusters for a query. 5. Justification by relevance density
In this section, we aim to develop a deeper understanding of why expansion by the cluster-based resampling method helps. To justify the cluster-based resampling approach using overlapping clusters, we have analyzed relevance density with dominant documents and the performance of feedback without redundant documents.

The rationale for the proposed method is that resampling documents using clusters is an effective way to find dominant documents for a query from the initial retrieval set. We measure relevance density to justify our assumption that dominant documents are relevant to the query and redundantly appear over the top-ranked clusters.
 Relevance density is defined to be the proportion of the feedback documents that contain relevant documents. A higher relevance density implies greater retrieval accuracy, ultimately approaching true relevance feedback.
If a resampling method is effective, it will produce higher relevance densities for pseudo-relevant documents than a set of top-retrieved documents. To justify the cluster-based resampling method, we will examine the relevance density of feedback documents through experimental analysis. 5.1. Relevance density of feedback documents
We can expect that higher relevance density produces higher performance since more relevant documents are used for feedback.

As shown in Fig. 4 , the resampling method shows higher density compared to the relevance model on 100 feedback doc-uments for all test collections. Relevance density for TrueRF is measured with all true relevant documents in the relevance judgment set.

When the number of feedback documents is set to 100, we can expect that the resampling method outperforms the rel-evance model since the resampling method uses more relevant documents for feedback.

To verify our expectation for density, we compared performance with the number of feedback documents and terms set to 100. The performance of feedback on fixed documents is shown in Table 5 . The resampling method outperforms the relevance model for all collections. The results show that the density of relevant documents supports the improvements from the resampling approach which extracts better feedback documents from the top-ranked 100 documents.
From the results of density as related to the number of feedback documents and effectiveness on all collections, we can see that the redundant dominant documents help the density of the relevant documents.
 Table 5 shows performance on fixed feedback documents and terms. Resampling outperforms RM for all collections. Resampling takes better feedback documents (with higher relevance density) from the top-ranked 100 documents. beled MAP (mean average precision) is the score of the language model on each collection. When relevance density is higher than Mean Relevance Density , it tends to have higher average precision than MAP for all test collections. The distribution shows the general pattern that higher relevance density produces higher average precision though some queries with low relevance density show high average precision for all test collections. 5.2. Redundancy in feedback documents
To support the observation of relevance density and performance, we have examined performance by removing redun-dant documents in feedback. That is, a document is not repeated in the feedback even if it occurs in multiple clusters.
We assumed that dominant documents for the initial retrieval set are relevant and redundant documents that play a cen-tral role in making overlapping clusters. Table 6 shows the performance of Sampling without Replacement . It outperforms RM , but is worse than Resampling for GOV2, WT10g, AP, and WSJ collection except for ROBUST collection. The results show that dominant documents give positive effects for feedback.
 We examined how many documents are redundantly used for feedback for each query.

For example, the unique documents are six among ten feedback documents which means four documents redundantly appear in more than one clusters. Then redundancy ratio is 0.4.

Fig. 6 shows that redundancy ratio for overall queries are high for all test collections. The average redundancy ratio is 0.38, 0.41, 0.42, 0.57, and 0.59 on GOV2, WT10g, ROBUST, AP and WSJ, respectively.
 5.3. Result analysis: Inside the clusters sampled for a example query
We have also examined inside the clusters how redundancy affects the number of relevant documents in the feedback 10 clusters, and  X  X  X  in  X  X 6 #1 X  means non-relevant. If we look at the top 5, 10, 25, 50, 75, and 100 documents, we find the retrieved results include 23 relevant documents for the top 100 documents. For Resampling , however, the relevance densities where the top 10 clusters contained 50 documents, 49 of which were relevant: 37 of those relevant documents appeared in other clusters. One relevant document appeared in nine of the top 10 clusters and another was in seven. The ten dominant documents are sampled 47 times repeatedly for feedback. The three non-dominant documents are sampled 3 times.
Such dominant documents that appear in multiple highly-ranked clusters and their redundancy contribute to query expansion terms. 6. Conclusions
Resampling the top-ranked documents using clusters is effective for pseudo-relevance feedback. All the procedures of the proposed method for constructing the sample space, building sampling units, sampling clusters, and sampling expansion terms are deterministic. The improvements obtained were consistent across nearly all test collections, and for large web col-lections, such as GOV2 and WT10g, the approach showed substantial gains. The relative improvements on GOV2 collection are 16.82% and 6.28% over LM and RM, respectively. The improvements on the WT10g collection are 19.63% and 26.38% over
LM and RM, respectively. We showed that the relevance density was higher than the baseline feedback model for all test collections as a justification of why expansion by the cluster-based resampling method helps. Experimental results also show that deterministically resampling clusters are helpful for identifying pseudo-relevant documents for a query.
For future work, we will study how the resampling approach can adopt query variants by considering query character-istics. Additionally, in our experiments we simply represent a cluster by concatenating documents. Using a better represen-tation of a cluster to improve cluster ranking and combining weak learners based on several cluster representations for boosting should improve the performance of pseudo-relevance feedback without sacrificing the performance of individual queries.
 Acknowledgements This research was supported in part by Basic Science Research Program through the National Research Foundation of Korea (NRF) grant funded by the Ministry of Education, Science and Technology (MEST) (611-2006-1-D00025), and by the
Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
 References
