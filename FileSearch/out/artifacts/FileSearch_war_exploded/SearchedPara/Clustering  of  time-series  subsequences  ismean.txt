
A large fraction of attention from the data-mining community has focuses on time-series data (Keogh and Kasetty 2002; R oddick and Spiliopoulou 2002). This is plau-sible and highly anticipated because time-se ries data is a by-product in virtually every human endeavor, including biology (Bar-Joseph et al. 2002), finance (Fu et al. 2001;
Gavrilov et al. 2000; Mantegna 1999), geology (Harms et al. 2002b), space explo-ration (Honda et al. 2002; Yairi et al. 2001), robotics (Oates 1999) and human motion analysis (Uehara and Shimada 2002). Of all the techniques applied to time series, clustering is perhaps the most frequently used (Halkidi et al. 2001), being useful in its own right as an exploratory technique and as a subroutine in more complex data-mining algorithms (Bar-Joseph et al. 2002; Bradley and Fayyad 1998). Given these two facts, it is hardly surprising th at time-series clustering has attracted an extraordinary amount of attention (Bar-Joseph et al. 2002; Cotofrei 2002; Cotofrei and Stoffel 2002; Das et al. 1998; Fu et al. 2001; Gavrilov et al. 2000; Harms et al. 2002a, 2002b; Hetland and Satrom 2002; Honda et al. 2002; Jin et al. 2002a, 2002b; Keogh 2002a; Keogh et al. 2001; Li et al. 1998; Lin et al. 2002; Mantegna 1999; Mori and Uehara 2001; Oates 1999; Osaki et al. 2000; Radhakrishnan et al. 2000;
Sarker et al. 2002; Steinback et al. 2002; Tino et al. 2000; Uehara and Shimada 2002; Yairi et al. 2001). The work in this area can be broadly classified into two categories:  X  Whole clustering: The notion of clustering here is similar to that of conven- X  Subsequence clustering: Given a single time series, sometimes in the form of
Subsequence clustering is commonly used as a subroutine in many other algorithms, including rule discovery (Das et al. 1998; Fu et al. 2001; Harms et al. 2002a, 2002b;
Hetland and Satrom 2002; Jin et al. 2002a, 2002b; Mori and Uehara 2001; Osaki et al. 2000; Sarker et al. 2002; Uehara and Shimada 2002; Yairi et al. 2001), indexing (Li et al. 1998; Radhakrishnan et al. 2000), classification (Cotofrei 2002; Cotofrei and Stoffel 2002), prediction (Schittenk opf et al. 2000; Tino et al. 2000) and anomaly STS (subsequence time series) clustering.
 is meaningless! In particular, clusters e xtracted from these time series are forced to obey certain constraints that are pathologically unlikely to be satisfied by any dataset, and because of this, the clusters extracted by any clustering algor ithm are essentially random.
 the time to define this term. All useful al gorithms (with the sole exception of random-number generators) produce output that depends on the input. For example, a decision-tree learner will yield very different outputs on, say, a credit-worthiness domain, a drug-classification domain and a music domain. We call an algorithm meaningless if the output is independent of the input. As we prove in this paper, the output of STS clustering does not depend on input and is therefore meaningless. of papers. In fact, the existence of so much work based on STS clustering offers an obvious counterargument to our claim. It could be argued: Because many papers have been published that use time-series subsequence clustering as a subroutine and these papers produced successful results, time-series subsequence clustering must be a meaningful operation.
 the results are consistent with what one w ould expect from random cluster centres.
We recognize that this is a strong assertion, so we will demonstrate our claim by reimplementing the most successful (i.e. the most referenced) examples of such work and showing with exhaustive experiments that these contributions inherit the property of meaningless results from the STS clustering subroutine.

The rest of this paper is organized as follows. In Sect. 2, we will review the necessary background material on time series and clustering, then briefly review the body of research that uses STS clustering. In Sect. 3, we will show that STS clus-tering is meaningless with a series of simple intuitive experiments; then in Sect. 4, we will explain why STS clustering cannot produce useful results. In Sect. 5, we show that the many algorithms that use STS clustering as a subroutine produce re-sults indistinguishable from random cluste rs. Because the main contribution of this paper may be considered negative, Sect. 6 demonstrates a simple algorithm that can find clusters in at least some trivial datasets. This algorithm is not presented as the best way to find clusters in time-series subs equences; it is simply offered as an ex-istence proof that such an algorithm exists and to pave the way for future research.
In Sect. 7, we conclude and summarize so me comments from researchers that have read an earlier version of this paper and verified the results.
In order to frame our contribution in the proper context, we begin with a review of the necessary background material. We begin with a definition of our data type of interest, time series:
Definition 1. Time series: A time series T = t 1 ,... , real-valued variables.

Data-mining researchers are typically not interested in any of the global properties of a time series; rather, researchers confine their interest to subsections of the time series, called subsequences.

Definition 2. Subsequence: Given a time series T of length m , a subsequence C of T is a sampling of length w&lt; m of contiguous positions from T ,thatis, C t ,... , t
In this work, we are interested in the case where all the subsequences are ex-tracted and then clustered. This is achieved by use of a sliding window.
Definition 3. Sliding window: Given a time series T of length m and a user-defined subsequence length of w , a matrix S of all possible subsequences can be built by sliding a window across T and placing subsequence C p size of matrix S is ( m  X  w + 1) by w .
 Figure 1 summarizes all the above definitions and notations.

Note that, while S contains exactly the same information nificantly more storage space.
One of the most widely used clustering appr oaches is hierarchical clustering due to the great visualization power it offers (Keogh and Kasetty 2002; Mantegna 1999).
Hierarchical clustering produces a nested hierarchy of similar groups of objects, ac-cording to a pairwise distance matrix of the objects. One of the advantages of this method is its generality because the user does not need to provide any parameters such as the number of clusters. However, its application is limited to only small datasets due to its quadratic computational complexity. Table 1 outlines the basic hierarchical clus tering algorithm.

The basic intuition behind k -means (and a more general class of clustering algorithms known as iterative refinement algorithms) is shown in Table 2.
 is the number of clusters specified by the user, r is the number of iterations until convergence and D is the dimensionality of time series (in the case of STS clus-tering, D is the length of the sliding window, w ). While the algorithm is perhaps the most commonly used clustering algorithm in the literature, it does have several shortcomings, including the fact that the number of clusters must be specified in advance (Bradley and Fayyad 1998; Halkidi et al. 2001).

It is well understood that some types of high-dimensional clustering may be meaningless. As noted by Agrawal et al. (1993) and Bradley and Fayyad (1998), in high dimensions, the very concept o f nearest neighbor has little meaning because the ratio of the distance to the nearest neighbor over the distance to the average neighbor rapidly approaches 1 a s the dimensionality increases. However, time series, while often having high dimensionality, typically have a low intrinsic dimensionality (Keogh et al. 2001) and can therefore be meaningful candidates for clustering.
The last decade has seen an extraordinary int erest in mining time-series data, with at least one thousand papers on the subject (Keogh and Kasetty 2002). Tasks addressed by the researchers include segmentation, indexing, clustering, classification, anomaly detection, rule discovery and summarization.

Of the above, a significant fraction use subsequence time-series clustering as a subroutine. Below we enumerate some representative examples.  X 
There has been much work on finding asso ciation rules in time series (Das et al. 1998; Fu et al. 2001; Harms et al. 2002a, 2002b; Jin et al. 2002a, 2002b; Keogh and Kasetty 2002; Mori and Uehara 2001; Osaki et al. 2000; Uehara and Shimada 2002; Yairi et al. 2001). Virtually all work is based on the classic paper of Das et al. that uses STS clustering to conver t real-valued time series into symbolic values, which can then be manipulated by classic rule-finding algorithms (Das et al. 1998).  X 
The problem of anomaly detection in tim e series has been generalized to in-clude the detection of surprising or interesting patterns (which are not necessarily anomalies). There are many approaches to this problem, including several based on STS clustering (Yairi et al. 2001).  X 
Indexing of time series is an important problem that has attracted the attention of dozens of researchers. Several of the proposed techniques make use of STS clustering (Li et al. 1998; Radhakrishnan et al. 2000).  X 
Several techniques for classifying time s eries make use of STS clustering to pre-process the data before passing to a standard classification technique such as a decision tree (Cotofrei 2002; Cotofrei and Stoffel 2002).  X 
Clustering of streaming time series has also been proposed as a knowledge-discovery tool in its own right. Researchers have suggested various techniques to speed up the STS clustering (Fu et al. 2001).

The above is just a small fraction of the work in the area; more extensive surveys may be found in Keogh (2002a) and Roddick and Spiliopoulou (2002).
In this section, we will demonstrate the meaninglessness of STS clustering. In order to demonstrate that this meaninglessness is a result of the way the data is obtained by sliding windows and not some quirk of the clustering algorithm, we will also do whole clustering as a control (Gavrilov et al. 2000; Oates 1999). We will begin by using the well-known k -means algorithm because it accounts for the lion X  X  share of all clustering in the time-series data-mining literature. In addition, the k -means algorithm uses Euclidean distance as its underlying metric, and again the Euclidean distance accounts for the vast majority of all published work in this area (Cotofrei 2002; Cotofrei and Stoffel 2002; Das et al. 1998; Fu et al. 2001; Harms et al. 2002a; Jin et al. 2002a; Keogh et al. 2001), and as empirically demonstrated in Keogh and
Kasetty (2002), it performs better than t he dozens of other recently suggested time-series distance measures.
Because k -means is a heuristic, hill-climbing al gorithm, the cluster centres found may not be optimal (Halkidi et al. 2001). That is, the algorithm is guaranteed to converge on a local, but not necessarily global, optimum. The choices of the initial centres affect the quality of results. One technique to mitigate this problem is to do multiple restarts and choose the best set of clusters (Bradley and Fayyad 1998). An obvious question to ask is how much variability in the shapes of cluster centres we get between multiple runs. We can measure this variability with the following equation:  X  Let A = (  X  a  X  Let B = (  X  b  X  Let dist (  X  a
Then the distance between two sets of clusters can be defined as
The simple intuition behind the equation i s that each individual cluster centre in A should map onto its closest counterpart in B , and the sum of all such distances tells us how similar two sets of clusters are.
 sets of clusters derived for the same dataset but also two sets of clusters that have been derived from different data sources. Given this fact, we propose a simple experiment. saved the three resulting sets of cluster centres into set random restarts on random-walk dataset, saving the three resulting sets of cluster of exposition; larger values do not change the substance of what follows. each set of cluster centres in  X  X , to each other set of cluster centres in this number within_set_  X  X_distance .
We also measured the average cluster dis tance between each set of cluster centres in  X  X , to cluster centres in  X  Y ; we call this number between_set_
We can use these two numbers to create a fraction,
We can justify calling this number clustering meaningfulness because it clearly mea-sures just that. If, for any dataset, the clu stering algorithm finds similar clusters each time regardless of the different initial seeds, the numerator should be close to 0. In contrast, there is no reason why the clusters from two completely different, unre-lated datasets should be similar. Therefore, we should expect the denominator to be relatively large. So, overall, we should expect that the value of clustering meaning-different datasets.

As a control, we performed the exact same experiment on the same data, but using subsequences that were randomly extracted, rather than extracted by a sliding window. We call this whole clustering.

Because it might be argued that any resu lts obtained were the consequence of a particular combination of k and w , we tried the cross product of k and w ={ 8 , 16 , 32 } . For every combination of param eters, we repeated the entire process 100 times, and averaged the results. Figure 2 shows the results.
The results are astonishing. The cluster centres found by STS clustering on any particular run of k -means on the stock-market dataset are not significantly more simi-lar to each other than they are to cluster centres taken from random-walk data! In other words, if we were asked to perform clustering on a particular stock-market dataset, we could reuse an old clustering obtained from random-walk data, and no one could tell the difference! whole clustering in this experiment (and all experiments in this work) are due ex-clusively to the feature-extraction step. In particular, both are being tested on the same datasets with the same parameters of w and k using the same algorithm. ant to our results. In our definition, each cluster centre in A maps onto its closest match in B . It is possible, therefore, that two or more cluster centres from A map to one centre in B , and some clusters in B have no match. However, we tried other variants of this definition, including pairwise matching, minimum matching and max-imum matching, together with dozens of other measurements of clustering quality ference to the results.
The previous section suggests that k -means clustering of STS time series does not produce meaningful results, at least for stock-market data. Two obvious questions to other types of data? We will answer the former question here and the latter question in Sect. 3.3.
 reuse the experimental methodology from the previous section exactly; however, we can do something very similar.
 tering, by cutting the first k links (Mantegna 1999). Figure 3 illustrates the idea. The resultant time series in each of the k subtrees can then be merged into single cluster prototypes. When performing hierarchical clustering, one has to make a choice about how to define the distance between two clusters; this choice is called the linkage method (cf. step 3 of Table 1).
 (Halkidi et al. 2001). We can use all three methods for the stock-market dataset and place the resulting cluster centres into set X . We can do the same for random-walk data and place the resulting cluster centres into set Y . Having done this, we can extend the measure of clustering meaningful ness in Eq. (4) to hierarchical clustering and run a similar experiment as in the last sec tion, but using hierarchical clustering. The results of this experiment are shown in Fig. 4.
 of linkage method can have minor effects on the clustering found, the results above tell us that, when doing STS clustering, the choice of linkage method has as much effect as the choice of dataset! Another way of looking at the results is as follows: If we were asked to perform hierarchical clus tering on a particular dataset, but we did not have to report which linkage method we used, we could reuse an old random-walk clustering and no one could tell the difference without rerunning the clustering for every possible linkage method.
The results in the two previous sections are extraordinary, but are they the con-sequence of some properties of stock market data, or as we claim, a property of the sliding-window feature extraction? The latter is the case, which we can simply demonstrate. We visually in spected the UCR archive of tim e-series datasets for the two time-series datasets that appear the least alike (Keogh 2002b). The best two candidates we discovered are shown in Fig. 5.

We repeated the experiment of Sect. 3.2 us ing these two datasets in place of the stock market data and the random-walk data. The results are shown in Fig. 6.
In our view, this experiment sounds the death knell for clustering of STS time series. If we cannot easily differentiate between the clusters from these two vastly dif-ferent time series, then how could we possibly find meaningful clusters in any data? periments we performed. We tested other clustering algorithms, including Expecta-tion Maximization (EM) and Self-Organizing Maps (SOMs) (van Laerhoven 2001).
We tested on 42 different datasets (Keogh 2002a; Keogh and Kasetty 2002). We experimented with other measures of clus tering quality (Halkidi et al. 2001). We tried other variants of k -means, including different seeding algorithms. Although Eu-clidean distance is the most commonly used distance measure for time-series data mining, we also tried other distance measures from the literature, including Man-hattan, L  X  , Mahalanobis distance and dynamic time warping distance (Gavrilov et al. 2000; Keogh 2002a; Oates 1999). We tried various normalization techniques, in-cluding Z-normalization, 0-1 normalization, amplitude only normalization, offset only normalization, no normalization, etc. In every case, we are forced to the inevitable conclusion: whole clustering of time series is usually a meaningful thing to do, but sliding-window time-series clustering is never meaningful.
Before explaining why STS clustering is meaningless, it will be instructive to visu-alize the cluster centres produced by both whole clustering and STS clustering. By definition of k -means, each cluster centre is simpl y the average of all the objects series within that cluster. Appa rently, because the objective of k -means is to group similar objects in the same cluster, we should expect the cluster centre to look some-what similar to the objects in the cluster. We will demonstrate this on the classic cylinder-bell X  X unnel data (Keogh and Kasetty 2002). This dataset consists of random instantiations of the eponymous patterns, with Gaussian noise added. Note that this dataset has been freely available for a decade and has been referenced more than 50 times (Keogh and Kasetty 2002). While each time series is of length 128, the onset and duration of the shape is subject to random variability. Figure 7 shows one instance from each of the three patterns.

We generated a dataset that contains 30 instances of each pattern and performed k -means clustering on it, with k = 3. The resulting cluster centres are shown in
Fig. 8. As one might expect, all three clusters are successfully found. The final centres closely resemble the three different patterns in the dataset, although the sharp edges of the patterns have been somewhat softened by the averaging of many time series with some variability in the time axis.

To compare the results of whole clustering to STS clustering, we took the 90 time series used above and concatenated them into one long time series. We then performed STS clustering with k -means. To make it simple for the algorithm, we used the exact length of the patterns ( w = 128) as the window length and k the number of desired clusters. The cluster centres are shown in Fig. 9. any of the patterns in the data; what X  X  more, they appear to be perfect sine waves. of the clustering algorithm, the number of clusters or the dataset used! Furthermore, although the sine waves are always exactly out of phase with each other by 1 period, overall, their joint phase is arbitrary and will change with every random restart of k -means.
 cluster centres for every dataset, then clearly it will be impossible to distinguish one dataset X  X  clusters from another. Although we have now explained the inability of STS clustering to produce meaningful results, we have revealed a new question:
Why do we always get cluster centres with this special structure? To explain the unintuitive results above, we must introduce a new fact.
Theorem 1. For any time-series dataset T with an overall trend of 0, if T is clus-tered using sliding windows, and w m , then the mean of all the data (i.e. the special case of k = 1), will be an approximately constant vector.
 case, but perfectly legal), we will always end up with a horizontal line as the cluster centre. The proof of this fact is straightforward but long, so we have elucidated it in a separate technical report (Truppel et al. 2003). Note that the requirement that the overall trend be 0 can be removed, in which case, the k still a straight line, but with slope greater than 0.
 a visual proof in Fig. 10.
 somewhere in the time series T , such that w  X  i  X  m  X  w + much longer than the window size, then virtually all datapoints are of this type. What contribution does this datapoint make to the overall mean of the STS matrix S ?As the sliding window passes by, the datapoint first appears as the rightmost value in the window, then it goes on to appear exactly once in every possible location within the sliding window. So the t i datapoint contribution to the overall shape is the same everywhere and must be a horizontal line. Only those points at the very beginning and the very end of the time series avoid contributing their value to all of S , but these are asymptotically irrelevant. The average of many horizontal lines v in the mean vector, 1  X  i  X  w , is computed by averaging essentially every value in the original time series; more precisely, from t of m = 1 , 024 and w = 32, the first value in the mean vector is the average of t [ 1 ... 993 ] ; the second value is the average of t [ 2 ... only datapoints not being included in every computation are the ones at the very beginning and at the very end, and their effects are negligible asymptotically.
The implications of Theorem 1 become cl earer when we consider the following well-documented fact. For any dataset, the we ighted (by cluster membership) average of k clusters must sum up to the global mean. The implication for STS clustering is profound. Because the global mean for STS clustering is a straight line, then the weighted average of k -clusters must in turn sum to a straight line. However, there is no reason why we should expect this to be true of any dataset, much less every dataset. This hidden constraint limits the utility of STS clustering to a vanishingly small set of subspace of all d atasets. The out-of-phase si ne waves as cluster centres that we get from the last section conforms to this theorem, b ecause their weighted average, as expected, sums to a straight line.
There are further constraints on the types of datasets where STS clustering could possibly work. Consider a subsequence C p that is a member of a cluster. If we examine the entire dataset for similar subsequences, we should typically expect to find the best matches to C p to be the subsequences ... , In other words, the best matches to any subs equence tend to be just slightly shifted versions of the subsequence. Figure 11 illustrates the idea and Definition 4 states it more formally.

Definition 4. Trivial match: Given a subsequence C beginning at position p ,amatch-ing subsequence M beginning at q , and a distance R , we say that M is a trivial match to C of order R , if either p = q or there does not exist a subsequence M beginning at q such that D ( C , M )&gt; R , and either q elsewhere (Lin et al. 2002).
 different numbers of trivial matches. In particular, smooth, slowly changing sub-sequences tend to have many trivial matches, whereas subsequences with rapidly changing features and/or noise tend to have very few trivial matches. Figure 12 il-lustrates the idea. The figure shows a time series that subjectively appears to have a cluster of three square waves. The bottom plot shows how many trivial matches each subsequence has. N ote that the square waves have v ery few trivial matches, so all three taken together sit in a sparsely populated region of consider the relatively smooth Gaussian bump centred at 125. The subsequences in the smooth ascent of this feature have more than 25 trivial matches and thus sit in a dense region of w -space; the same is true for the s ubsequences in the descent from the peak. So if clustering this dataset with k -means, k = tres will be irresistibly drawn to these two shapes, simple ascending and descending lines.

The importance of this observation for STS clustering is obvious. Imagine we have a time series where we subjectively see two clusters: equal numbers of a smooth, slowing changing pattern and a noisier pattern with many features. In space, the smooth pattern is surrounded by many trivial matches. This dense volume will appear to any clustering algorithm an extremely promising cluster centre. In contrast, the highly featured, noisy pattern has very few trivial matches and thus sits in a relatively sparse space, all but ignored by the clustering al gorithm. Note that it is not possible to simply remove or factor out the trivial matches because there is no way to know beforehand the true patterns.

We have not yet fully explained why the cluster centres for STS clustering de-generate to sine waves (cf. Fig. 9). However, we have shown that for STS clustering, algorithms do not really cluster the data. If not clustering, what are the algorithms doing? It is instructive to note that, if we perform singular value decomposition on time series, we also get shapes that seem to approximate sine waves (Keogh et al. 2001). This suggests that STS clustering algorithms are simply returning a set of basis functions that can be added together in a weighted combination to approximate the original data.

An even more tantalizing piece of evidence exists. In the 1920s, data miners were excited to find that, by preprocessing their data with repeated smoothing, they could discover trading cycles. Their joy was shattered by a theorem by Evgeny Slutsky (1880 X 1948), who demonstrated that any noi sy time series will converge to a sine wave after repeated applications of moving window smoothing (Kendall 1976). While
STS clustering is not exactly the same as repeated moving window smoothing, it is clearly highly related. For brevity, we will defer future discussion of this point to future work.
Having gained an understanding of the fact that STS clustering is meaningless and having developed an intuition as to why this is so, it is natural to ask if there is a sim-ple modification to allow it to produce meaningful results. We asked this question, not just among ourselves, but also of dozens o f time-series clustering researchers with whom we shared our initial results. While we considered all suggestions, we discuss only the two most promising ones here.

The first idea is to increment the slid ing window by more than one unit each time. In fact, this idea was suggested by Das et al. (1998), but only as a speed-up mechanism. Unfortunately, this idea does not help. If the new step size s is much smaller than w , we still get the same empirical results. If s is approximately equal to or larger than w , we are no longer doing subsequence clustering, but whole would become a critical parameter, and choices that differ by just one timepoint can give arbitrarily different r esults. As a concrete exampl e, clustering weekly stock market data from Monday to Sunday will give completely different cluster patterns and cluster memberships from a Tuesday-to-Monday clustering.

The second idea is to set k to be some number much greater than the true number of clusters we expect to find, then do some postprocessing to find the real clusters.
Empirically, we could not make this idea work, even on the trivial dataset introduced in the last section. We found that, even if k is extremely large, unless it is a significant fraction of T , we still get arbitrary sine waves as cluster centres. In addition, we note that the time complexity for k -means increases with k .
 the definition of the problem is itself intrinsically flawed.
We conclude this section with a summary of the conditions that must be satisfied for STS clustering to be meaningful.
 length w . Further assume that we happen to know k and w in advance. A necessary (but not necessarily sufficient) condition for a clustering algorithm to discover the k patterns is that the weighted mean of the patterns must sum to a horizontal line and each of the k patterns must have approximately equal numbers of trivial matches.
As we noted in the introduction, an obvious counter argument to our claim is the fol-lowing. Because many papers have been published that use time-series subsequence clustering as a subroutine and these papers produce successful results, time-series subsequence clustering must be a meaningful operation. To counter this argument, we have reimplemented the most influential such work, the time-series rule-finding algorithm of Das et al. (1998) (the algorithm is not named in the original work, we will call it TSRF here for brevity and clarity).
The algorithm begins by performing STS clustering. The centres of these clusters are then used as primitives to convert the real-valued time series into symbols, which are in turn fed into a slightly modified version of a classic association-rule algorithm (Agrawal et al. 1993). Finally, the rules are ranked by their J -measure, an entropy based measure of their significance.

NASDAQ data. The high values of support, confidence and J -measure are offered as evidence of the significance of the rules. The rules are to be interpreted as follows: then we can expect to see within 20 time units, a pattern of rapid decrease followed by a leveling out . X  (Das et al. 1998).
 walk data, using exactly the same parameters? Because no such rules should exist by definition, we should get radically different results. experiment; the support, confidence and J -measure values are essentially the same as in Fig. 13! have created a random-walk time series that happens to have some structure to it.
Therefore, for every result shown in the original paper, we ran 100 recreations using different random-walk datasets, using quantum mechanically generated numbers to insure randomness (Walker 2001). In every case, the results published cannot be distinguished from our results on random-walk data.

The above experiment is troublesome, but perhaps there are simply no rules to be found in stock market data. We devised a simple experiment in a dataset that does contain known rules. In particular, we tested the algorithm on a normal, healthy electrocardiogram. Here, there is an obvi ous rule that one heartbeat follows another.
Surprisingly, even with much tweaking of t he parameters, the TSRF algorithm cannot find this simple rule.

The TSRF algorithm is based on the classic rule-mining work of Agrawal et al. (1993); the only difference is the STS ste p. Because the rule-mining work has been carefully vindicated in 100s of experiments on both real and synthetic datasets, it seems reasonable to conclude that the STS clustering is at the heart of the problems with the TSRF algorithm.
 referenced paper, and many of the dozens of extensions researchers have proposed (Das et al. 1998; Fu et al. 2001; Harms et al. 2002a, 2002b; Hetland and Satrom 2002; Jin et al. 2002a, 2002b; Mori and Uehara 2001; Osaki et al. 2000; Sarker et al. 2002; Uehara and Shimada 2002; Yairi et al. 2001). However, in retrospect, this result should not really be too surprising. Imagine that a researcher claims to have an algorithm that can differentiate between three types of Iris flowers (Setosa, Virginica and Versicolor) based on pet al and sepal length and width is not so extraordinary, given that it is wel l known that even amateur botanists and gardeners have this skill (British Irish Society 1997). However, the paper in question is claiming to introduce an algorithm that can find rules in stock-market time series.
There is simply no evidence that any human can do this, in fact, the opposite is true: every indication suggests that the patterns much beloved by technical analysts such as the calendar effect are completely spurious (Jensen 2000; Timmermann et al. 1998).
The results presented in this paper thus fa r are somewhat downbeat. In this section, we modify the tone by introducing an algorithm that can find clusters in some time series. This algorithm is not presented as the best way to find clusters in time series; for one thing, its time complexity is untenable for massive datasets. It is simply offered as an existence proof that such an algorithm exists and to pave the way for future research.
 to cluster every subsequence produces an unrealistic constraint and that considering trivial matches causes smooth, low-detail subsequences to form pseudo-clusters. Motifs are overrepresented sequences in discrete strings, for example, in musical or
DNA sequences (Reinert et al. 2000). Classi c definitions of motifs require that the underlying data be discrete, but in recent wor k, the present authors have extended the definitions to real-valued time series (Chiu et al. 2003; Lin et al. 2002). Figure 15 illustrates a visual intuition of a motif and definition 5 defines the concept more concretely.

Definition 5. k-motifs: Given a time series T and a distance range R ,themost significant motif in T (called 1-Motif ) is the subsequence C count of nontrivial matches. Subsequently, the J th most significant motif in T (called
J-Motif ) is the subsequence C J that has the highest count of nontrivial matches and satisfies D ( C J , C i )&gt; 2 R ,forall1  X  i &lt; J .

Figure 16 provides a visual explanation of why motifs are required to be at least 2 R apart.

Although motifs may be considered similar to clusters, there are several important differences, a few of which we enumerate here.  X 
When mining motifs, we must sp ecify an additional parameter R .  X 
Assuming the distance R is defined as Euclidean, motifs always define circular regions in space, whereas clusters may have arbitrary shapes.  X 
Motifs generally define a small subset of th e data and not the entire dataset. Note that, in Fig. 16, there are several time series that are not included in any motif.  X  The definition of motifs explicitly eliminates trivial matches.

Note that, while the first two points appear as limitations, the last two points explic-itly counter the two reasons that STS clust ering cannot produce meaningful results.
We cannot simply run a k -motif detection algorithm in place of STS clustering because a subset of the motifs discovered might really be a group that should be clustered together. For example, imagine a true cluster that sits in a hyperellipsoid.
It might be approximated by 2 or 3 motifs that cover approximately the same vol-ume. However, we could run a K -motif detection algorithm, with K k to extract promising subsequences from the data and then use a classic clustering algorithm to cluster only these subsequences. This simple idea is formalized in Table 3.
Step 2 of the algorithm requires a call to a motif discovery algorithm; an op-timized exact algorithm for this appears in Lin et al. (2002) and a linear time-approximate algorithm appears in Chiu et al. (2003).
We have seen in Sect. 5 that the cluster cen tres returned by STS have been mistaken for meaningful clusters by many researchers. To eliminate the possibility of repeating this mistake, we will demonstrate the proposed algorithm on the dataset introduced in Sect. 4, which consists of the concaten ation of 30 examples each of the cylinder, bell, funnel shapes, in random order. We would like our clustering algorithm to be able to find cluster centres similar to the ones shown in Fig. 8. fair because we also gave th ese two correct parameters to all the algorithms above.
We needed to specify the value of R ; we did this by simply examining a fraction of our dataset, finding ten pairs of subsequences we found to be similar, measuring the Euclidean distance between these pairs , and averaging the results. The cluster centres found are shown in Fig. 17.
 tering. The fact that this is achieved by working with only a subset of the data and explicitly excluding trivial matches furt her supports our explanations in Sects. 4.1 and 4.2 of why STS clustering is meaningless.

As one might expect with such an unintuitive and surprising result, the original version of this paper caused some controversy when first published. Some suggested that the results were due to an implementation bug. Fortunately, many researchers have since independently confirmed our findings; we will note a few below. times series. After reading an early draft of our paper she wrote  X  At first we didn X  X  understand what the problem was, but after reading your paper this fact we experi-mentally confirmed that (STS) clustering is meaningless!!  X  (Nanni 2003). Dr. Richard
J. Povinelli and his student Regis DiGiacomo experimentally confirmed that STS clustering produces sine wave clusters regardless of the dataset used or the setting of any parameters (Povinelli 2003). Dr. Miho Ohsaki reexamined work she and her group had previously published and confirmed that the results are indeed meaning-less in the sense described in this work (Ohsaki et al. 2002). She has subsequently been able to redefine the clustering subroutine in her work to allow more meaningful pattern discovery (Ohsaki et al. 2003). Dr. Frank H X ppner noted that he had observed a year earlier than us that  X ... when using few clusters the resulting prototypes appear very much like dilated and translated trigonometric functions ... X  (Hoppner 2002); however, he did not attach any significance to this. Dr. Eric Perlman wrote to tell us that he had begun to scale up a project of astronomical time-series data mining (Perlman and Java 2003); however, he abandoned it after noting that the results were consistent with being meaningless in the sense described in this work. Dr. Anne Den-ton noted,  X  X  X  X e experimented myself, (and) the central message of your paper X  X hat subsequence clustering is meaningless X  X s very right, X  and  X  X t X  X  amazing how similar the cluster centres for widely distinct series look! X  (Denton 2003)
We have shown that a popular technique for data mining does not produce mean-ingful results. We have further explained the reasons why this is so.
Although our work may be viewed as negative, we have shown that a reformula-tion of the problem can allow clusters to be e xtracted from (streaming) time series.
In future work, we intend to consider several related questions; for example, whether or not the weaknesses of STS clustering described here have any implications for model-based, streaming clustering of tim e series or streaming clustering of nominal data (Guha et al. 2000).

