 E-commerce and intranet search systems require newly ar-riving content to be indexed and made available for search within minutes or hours of arrival. Applications such as file system and email search demand even faster turnaround from search systems, requiring new content to become avail-able for search almost instantaneously. However, incremen-tally updating inverted indices, which are the predominant data structure used in search engines, is an expensive oper-ation that most systems avoid performing at high rates.
We present JiTI, a Just-in-Time Indexing component that allows searching over incoming content (nearly) as soon as that content reaches the system. JiTI X  X  main idea is to invest less in the preprocessing of arriving data, at the expense of a tolerable latency in query response time. It is designed for deployment in search systems that maintain a large main index and that rebuild smaller stop-press indices once or twice an hour. JiTI augments such systems with instant retrieval capabilities over content arriving in between the stop-press builds. A main design point is for JiTI to demand few computational resources, in particular RAM and I/O.
Our experiments consisted of injecting several documents and queries per second concurrently into the system over half-hour long periods. We believe that there are search applications for which the combination of the workloads we experimented with and the response times we measured present a viable solution to a pressing problem.
 H.3.3 [ Information Storage and Retrieval ]: [Informa-tion Search and Retrieval] Algorithms, Performance Incremental indexing, inverted indices, search engines Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00.
Web search engines index billions of pages, which are gath-ered by high-volume crawlers capable of collecting millions of pages per day. Still, crawl cycles may take weeks to com-plete [10], and so a search engine X  X  view of the Web, as reflected by its index, is always stale to some extent. Many new pages are created daily, other pages are modified, and some pages are deleted. Such changes in the content of the Web are not instantaneously captured by search engines. The relatively long turnaround between the time content is created and the time it becomes searchable through one X  X  favorite search engine is not dictated solely by crawl cycles. Even if the content would have somehow been made avail-able to the engine upon creation, incrementally updating indices over billions of pages with volumes of changes in the order of millions of pages per day is a daunting task, es-pecially while having to serve millions of queries per day. Nevertheless, for many general information needs, perhaps even for a large majority of them, searching over Web snap-shots a few weeks old is not an issue. The information need can be perfectly satisfied without the index containing an up-to-the second view of the Web [20].

However, many search applications require much faster turnaround between the time new content is created, and the time it becomes available for searching [28, 9]. Enter-prise search engines are often required to index new con-tent within hours of its creation. Online news outlets must make items searchable within minutes of the items coming off the wire. E-commerce and trading sites must keep up with content updates from the vibrant marketplace. Mail systems should support search over incoming mail practi-cally instantaneously, even before the messages are read by their recipients. File system and desktop search applica-tions must reflect content creation, deletion, modification and movement as file operations occur. Applications in the intelligence community require speedy search capabil-ities over various types of incoming data. Note that while the update rates required by these applications are far more demanding than those required of Web search engines, the volumes of data involved are usually much smaller. With the exception of perhaps very high volume intelligence informa-tion systems or central mail servers of large organizations, document arrival rates are orders of magnitude smaller than those faced by Web search engines. Likewise, query rates for these applications are much lower than those experienced by Web search engines. This is certainly true for personal search applications on one X  X  own desktop or mailbox, but it is also true of most intranet search engines. IBM, with over 300 , 000 employees and arguably one of the larger intranets, rarely loads its intranet search engine with rates exceeding 10 queries per second, or 100 , 000 queries per day.
This paper addresses the problem of shortening the con-tent turnaround time in medium scale search systems. We present JiTI, a Just-in-Time Indexing system that allows searching over incoming content (nearly) as soon as that content reaches the system. JiTI is intended to augment search applications which batch index updates at (about) hourly intervals, by providing search capabilities over con-tent arriving between index updates. JiTI X  X  main idea is to invest less in the preprocessing of arriving data, at the ex-pense of a tolerable latency in query response time. Since JiTI is designed to fit as a component within existing search applications, special care was taken to design a solution de-manding few computational resources, in particular RAM.
The rest of the paper is organized as follows: Section 2 surveys related work. Section 3 presents the intuition behind the JiTI approach. Section 4 discusses our design guidelines, and describes the system X  X  architecture. Section 5 details our experimental setup and reports our results. Section 6 brings our conclusions and directions for further research.
The inverted index (or inverted file ) is the data structure most commonly used in large scale full-text indexing systems [29, 2, 1]. Given a corpus (an ordered set of documents), an inverted index for the corpus consists of postings lists , also known as as inverted lists , corresponding to each term ap-pearing in the corpus. There are many flavors of postings lists; for the purpose of this paper, we choose to abstract the postings list for term t as an array of payloads , with each pay-load corresponding to a single occurrence of t in the corpus. A payload encodes the position of that occurrence in the corpus -the ID of the document it appears in, and its posi-potentially encode some information regarding the context of the occurrence [5, 12] -whether the occurrence is in plain text, bold text, the title or other special field of the docu-ment, etc. An important feature of postings lists is that the payloads are sorted by corpus position.

Since each occurrence of each term is encoded in the in-verted index, its is inevitably linear in the size of the corpus. Consequently, a vast body of work has focused on index com-paction and compression methods (see appropriate chapters in [29, 2, 8]). Such methods can reduce the size of the index to 30% of the size of the corpus. The final compressed post-ings lists are typically written contiguously, with no gaps, to disk [7, 15]. Even implementations of memory-resident inverted indices try to keep the data in large contiguous blocks, to reduce memory fragmentation.
The above description of the inverted index has several implications. First, building an index from scratch requires significant computational resources. Second, incrementally adding new content to an existing index is quite costly. Consider a document containing n distinct terms -naively adding it to an existing inverted index will require manipu-lating n postings lists (at least). This typically includes disk phrase matching and proximity-based scoring.
 I/O and compression/decompression of O ( n ) data blocks. To reduce the cost of naive incremental updates, most sys-tems index new documents in a RAM-resident buffer, which when full is used to batch-update the disk-resident index. Updates can be done in-place or by merging the disk-based index and the RAM index to a new location on disk -see a comprehensive discussion in [19]. Performance (indexing throughput) increases with the size of the in-memory buffer, as the random I/O operations required for disk updates are amortized over larger batches of new content [24]. The spe-cific mechanics of several approaches to incremental indexing are detailed below.

Brown et al. [7] implemented incremental index updates in the INQUERY system. Instead of packing postings lists, bytes. Each list was stored in the smallest block in which it fit (large lists were stored by chaining blocks of size 8192 bytes). Often, append operations to postings lists were done in-place since the appendage fit in the same block. When a list grew beyond the capacity of its block, it was moved to a larger block (or, in the case of long lists, chained to a new block of size 8192). Still, document additions were usu-ally done in large batches, and not in an individual manner. Shieh and Chung [25] extended this idea by using statistics on the update frequency of each term to estimate the extra gap size that should be left for the growth of a term X  X  post-ings list whenever it exhausts its current block and must be moved. Tomasic et al. [28] suggested maintaining a small in-memory inverted index in an incremental manner, merging that index with the large disk-resident index when it reaches a certain size. On disk, short postings lists (corresponding to infrequent terms) were physically organized in a different manner than the longer lists, with lists migrating from one storage mode to the next as they grow.

Brewer [3] suggests partitioning inverted indices into chu-nks, which are essentially stand-alone indices over a con-time to time in a batched manner, i.e. the entire chunk is refreshed in a single atomic operation. In the Apache Lucene open-source search library [13], a chunk (segment) is written to disk for each n new documents, and every k seg-ments containing nk i documents are merged to form a single segment containing nk i +1 documents. In both approaches, query evaluation involves federating over the various seg-ments.

Lim et al. [21] focus on updating a memory-resident in-dex to reflect changes in the content of already indexed doc-uments. They index term positions within each document relative to multiple landmarks rather than relative to the beginning of the document. This allows many postings to remain valid following changes to the document, as long as the edit-distance change to the content is relatively small.
Chiueh and Huang [9] augmented the Codir system to support real-time updates of individual documents. Codir maintains its main index on disk, and evaluates queries by loading the (entire) postings lists of query terms into a memory-resident cache. In order to support dynamic doc-ument insertions, a transient in-memory index is maintained over newly added content, and queries federate over the two RAM-based structures. The authors examine two ap-proaches to merging the transient index with the main index. nodes, each responsible for a contiguous range of chunks. The first approach batch-merges the indices whenever the transient index grows above a certain threshold. The sec-ond approach lazily updates the main index when its post-ings lists are evicted from the cache. More concretely, let t denote a term contained in a new document being added to the system. If t  X  X  main postings list is currently cached, that list is updated in memory. Otherwise, t  X  X  transient postings list is updated. Whenever a main postings list is evicted from the cache, it is written back to disk if it was modified while cached. Whenever a main postings list is read into the cache, it is merged in memory with the corresponding tran-sient list. See the end of Section 3 for more on the differences between [9] and this paper.
The section above surveyed mechanisms of incrementally updating inverted files. However, most large scale systems do not incrementally update their operational indices, due to the large cost of update operations and the interference of incremental updates with the capability to keep serving queries at high rates [23, 8]. Rather, they manage updates at a higher level.

Several papers mention the shadowing technique as a com-mon index replacement scheme [1, 10]: while one immutable index is serving queries, a second index is built from the newly crawled content. Once the new index is ready, the engine shifts its service from the older index to the newly built one. In this approach, indexed content is updated only when a new index generation is rolled out.

Another approach, that performs updates at a finer level of granularity than shadowing, suggests using stop-press or delta indices [8, 12, 29]. Here, the engine maintains a large main index, which is rebuilt at relatively large intervals, along with a smaller delta index which is rebuilt at a higher rate and reflects the new content that arrived since the main index was built. Query evaluation in this approach is a fed-erated task, requiring the merging of the results returned by both the main and delta indices. When building the next main index, the existing main index and the latest corre-sponding delta index are merged.
Several works have examined the locality of reference pres-ent in search engine query logs. Two types of behaviors have been examined -repetitions of entire queries, and repetitions of terms. The distribution of query frequencies submitted to Web search engines has been found to follow a power-law [18, 22]. Since Web indices are typically stable over long periods of time, high volumes of repeating queries result in the same search results being produced. This implies that caching search results can significantly improve query throughput, and indeed even naive caching schemes can achieve hit rates exceeding 30% [18, 22, 27]. Consequently, result caching is an important optimization mechanism in search engines [5]. Another result of query log analysis is the study of the pop-ularity of individual search terms in queries. Note that non-repeating queries often share some terms, both as the result of overlapping information needs among different users, as well as from query reformulations that occur within a single user X  X  search session [26, 16]. Popular query terms motivate caching at yet another level in search engines -caching of the oft-accessed postings lists corresponding to the popular terms [8, 17]. However, in highly updatable settings, naively caching either postings lists or search results is not very ef-fective, since arriving documents invalidate the cached lists of the terms they contain (and the cached results of queries for which they are relevant). One advantage of using a stop-press or delta index is that all caching done within the main index can be kept highly effective.
The basic premise behind the inverted index structure is to invest in an expensive preprocessing step, so that later on, each query evaluation will be relatively cheap. This makes sense when the inverted index is expected to serve millions of queries over extended periods of time. Those queries will typically need to access millions of distinct postings lists. However, if the indexing outcome is ephemeral, it seems that preparing posting lists for all terms is excessive, given that the number of distinct terms expected to appear in queries over the near future is quite small.

To exemplify our case, consider an engine that uses a stop-press index that is rebuilt every 1800 seconds (half an hour). Assume that documents and queries both arrive at a rate of 5 per second. Overall, 9000 documents and queries reach the system in between rebuilds of the stop-press index. We shall now examine the work involved in maintaining an inverted index over these 9000 documents, as well as answering the queries, in real time. As mentioned in the previous section, updating an inverted index upon arrival of a new document involves updating the postings lists of all distinct terms in the document. Answering queries involves traversing the postings lists of the queries X  terms. The following para-graphs report our observations on the number of distinct terms found in typical streams of documents and queries.
In order to estimate the percentage of distinct terms in a typical corporate Web document, we examined the .gov .gov collection, which consists of about 1 , 240 , 000 English Web pages, was found to contain on average 1728 terms per document, of which about 450 (26%) were distinct. Terms were counted after lower-casing the full text of the docu-ments, with minimal additional normalization. Thus, index-ing 9000 .gov documents individually, with each document containing about 450 distinct terms, amounts to over four million manipulations of postings lists. We then proceeded to estimate the number of distinct terms that will be found in 9000 documents of length 1800 terms each (rounding up the average length of .gov documents), by examining a set of IBM intranet documents containing a total of 16 . 2 million terms (16 . 2 M = 9 K  X  1 . 8 K ). The volume of data was about 100 MB, and it contained over 285000 distinct terms.
Turning our attention to query streams, we measured term recurrence in queries submitted to the AltaVista search en-by dividing the log into batches containing an equal number of queries. In each batch we counted the total number of distinct terms, and divided by the batch size to arrive at the average number of distinct terms per query. We can also in-terpret this number as the rate of distinct terms per query. Note that the average query length in the log was about 3. When batches contained just 200 queries, the rate of distinct terms per query was about 2 . 6. However, as the number of http://trec.nist.gov/ ftp://ftp.archive.org/pub/AVLogs queries per batch grew, the rate lowered. When 1000 queries were batched, the rate was 2 . 2 distinct terms per query, and when 5000 queries were batched, it fell to 1 . 7 distinct terms per query. The same analysis conducted on a log of queries submitted to IBM X  X  internal search engine resulted in similar trends. We estimate by extrapolation that 9000 queries will contain roughly 27000 terms, but only about 14500 distinct terms (implying 1 . 61 distinct terms per query).
We now return to the example of 9000 documents and queries arriving within half an hour. The above paragraphs concluded that those documents will contain 285000 distinct terms, while the queries will contain about 14500 distinct terms. We believe that there is little reason to build or update 285K distinct postings lists in between stop-press rebuilds, when only about 5% of them (14500) will actu-perform minimal preprocessing on the arriving documents, lazily building postings lists only for terms actually appear-ing in queries, during query evaluation time. The prepared postings lists will not be written to disk or merged with the main or stop-press index; rather, they will be cached in RAM and will potentially be reused by future queries con-taining the same terms (although future queries may need to append more data to those lists). Since the larger indices in the system remain unchanged, all cached data pertaining to those indices remains valid as the corpus grows.
Essentially, the main difference between JiTI and the in-cremental indexing works surveyed in Section 2 is that those works aim to build long-lasting structures for answering queries over extended periods of time, and thus invest in-dexing resources that are recouped over the long run. JiTI, on the other hand, builds ephemeral structures that are re-cycled upon the next delta-index build, and therefore shifts work from the indexing phase to the query evaluation phase.
There are two major differences between our philosophy and the lazy approach of [9] (see Section 2.1). First, we do not update disk-resident postings lists at all. Today X  X  search engines operate predominantly in a document-at-a-time evaluation mode [5, 6], in which most queries do not re-quire the scanning of the entire postings lists of their terms. Therefore, caching full postings lists (on which the update policy of [9] is based) is not necessarily optimal. Second, our approach is more lazy in the sense that we do not even build transient postings lists for terms, unless those terms participate in queries.
JiTI is intended for deployment in search systems that maintain a large main index, and that rebuild smaller stop-press indices once or twice an hour. We assume that both these indices are kept on disk. JiTI will augment such sys-tems with instant retrieval capabilities over content arriving in between the stop-press rebuilds.

An important guideline in large scale search systems is to conserve resource consumption, such as RAM, I/O opera-tions, the number of open file handles, and the number of concurrent threads. Since JiTI will handle retrieval respon-sibilities for only a small fraction of the indexed content, our goal is to design it in a manner that only consumes a query terms will overlap with the set of 285000 distinct terms in the new documents.
 small fraction of the available resources. While incremen-tal indexing and query evaluation of hourly volumes may be more easily manageable in an isolated environment (e.g. a machine dedicated solely to this task), it is a challenge if the implementation is to share computational resources with a search application operating over a large corpus.
The JiTI system, and its interfaces to the rest of the search engine, are depicted in Figure 1. We begin by describing JiTI X  X  two infrastructure components, the DBF (Document Blocks File) and the lexicon , showcased in Figure 2.
The DBF is a file that holds, per document, a block of data representing the tokens of that document along with their payloads. As new documents arrive to the system, their blocks are appended sequentially to the file. Each doc-ument X  X  block is essentially a small inverted index of that document. Let t denote a term appearing (perhaps more than once) in document d ; the information associated with t in d  X  X  DBF block contains the following information: Note that neither the string form nor any other represen-tation of t itself is saved. We will shortly explain how the chaining data is obtained and updated.

The lexicon is a memory resident open-addressing hash ta-ble [11] containing an entry for each distinct term injected into the JiTI system. Each entry consists of the term X  X  hash value, its assigned term Id, the last injected document in which the term appeared, and the offset of the term X  X  oc-currences in that document X  X  DBF record. Essentially, the document and offset fields are pointers into the DBF.
Figure 2 shows the state of the DBF and lexicon after three documents have been injected into the system. Over-all, the documents contained 9 distinct terms, and so the lexicon contains 9 entries. The lexicon entry of term t points to the postings list of t in the last document that t has ap-peared in. That postings list has a similar pointer to the previous document containing t . In this manner, all doc-uments containing t are chained from last to first, and t  X  X  occurrences can be enumerated by a scan (from end to start) of the DBF. Upon arrival of a new document d , it is inverted and transformed into DBF block structure, thus determining the offset of each term X  X  postings list relative to the start of d  X  X  block. Now, the lexicon entry of each term t appearing in d is both read and updated: if t has appeared in previously injected documents, the last such document (and the offset within it) are updated in d  X  X  block. If d is the first injected document to contain t , a lexicon entry for t is created. In either case, the lexicon entry of t is updated to reflect its oc-currence in d . Finally, d  X  X  updated DBF block is appended in a single sequential write operation to the DBF. The two other main components of the JiTI system are: Postings Cache The postings cache saves prepared post-Query Engine The query engine accepts queries, and re-JiTI interacts with the other components of the search en-gine, depicted on the left side of the dashed line in Figure 1, as follows. It indexes incoming documents that also accumu-late in the staging area for the next stop-press index build. Once the stop press index is built, JiTI X  X  storage (lexicon, DBF and postings cache) is reset and it begins afresh to in-dex the next incoming documents. The central query engine sends queries to its three active indices (the main index, the latest stop-press index and JiTI) and merges the results.
We proceed to describe the two query engine flavors we experimented with. We then offer a short comparison be-tween the two models.
With this approach, each injected query is evaluated in its own dedicated thread. Evaluation begins by looking up the lexicon entry of each query term. For terms not appearing in the DBF, the lexicon will return an invalid entry. Terms that appear at least once in the DBF are called active terms, and for each one, the lexicon will return its term Id, along with the  X  document, offset  X  pair corresponding to its last occurrence. Queries consisting of inactive terms only are returned immediately with empty postings lists. Therefore, we now focus on queries having at least one active term. Let t , . . . , t k denote the k active terms of the query; for each such term t j , we now know the latest document ` j in which it has appeared, along with the offset within that document X  X  DBF block where the information pertaining to t j begins.
The postings cache is queried for a possible cached list associated with each active term. Terms that have not ap-peared yet in queries will surely not have cached postings lists. Terms that were queried before may have cached post-ings lists (depending on the history of cache evictions). Let e denote the latest document included in the cached post-ings list of term t j (whenever t j does not have a cached list, let e j = 0). By these notations, we need to collect all the payloads of t j appearing within the document range [ e j + 1 , ` j ]. Note that this range is empty whenever e j i.e. when the cached postings list for t j is up to date. Eval-uation proceeds as follows: the set of document identifiers { ` j : ` j &gt; e j } is entered into a max-heap. As long as the heap is not empty, its maximal element is removed -denote that element by  X  d . The DBF block of document  X  d is read and the payloads of all active query terms appearing in it are collected. Furthermore, for each such term t j appear-ing in document  X  d , we insert into the heap the identifier of the latest document prior to  X  d to contain t j , as long as that identifier is larger than e j . Thus, the DBF is scanned from tail to head, where only required blocks containing at least one active query term are read. Once the heap is empty, we have for each active term t j , its payloads from all documents 1 , . . . , ` j . Those payloads are concatenated into postings lists, which are returned to the system X  X  cen-tral query engine, and are also cached. Naturally, the cache replaces any postings lists from previous evaluations of the same terms with these fresh and up-to-date lists. Our implementation used a pool of 20 query threads. Queries arriving when all threads were busy were queued and served in FIFO order as threads became available.
The following approach was inspired by the work of Brin et al. [4] on frequent itemset counting. Imagine a public transportation bus , constantly circling the DBF from the most recently appended document block to the first injected document. The passengers on the bus are query terms, be-longing perhaps to multiple queries. As the bus proceeds along its route, each term riding the bus asks the driver to make a stop at the next DBF block containing payloads it needs to collect. With each such stop, the term finds out which block it should stop at next. A term disembarks the bus once the last document to contain it has been reached, by which time its postings list is complete. Once all terms of all queries have disembarked, the bus begins its next cycle.
The above description is missing the process by which query terms board the bus. The boarding process is as fol-lows: injected queries are pushed onto a queue. Every time the bus stops at a document (at the request of some term), the query queue is emptied, and all queued queries are exam-ined. Let t 1 , . . . , t k be the terms of such a query. The lexicon entry of each term is looked up; as in Subsection 4.2.1, let ` denote the last document containing term t j . Further-more, let d denote the ID of the current  X  X us station X . If any of ` 1 , . . . , ` k are greater than d , the query will not board the bus on this cycle; rather, all k terms will wait for the start of the next cycle. Otherwise, all terms satisfy ` j ( j = 1 , . . . , k ), and all terms board the bus at intermediate station d . Note that in either case, all query terms always board a single bus, and they do so when the bus has yet to pass any of ` 1 , . . . , ` k . A query is retired (fully evaluated) once all its terms disembark the bus. Note that although a query is retired within one bus ride, it may have to wait (up to a full DBF scan) before it boards the bus.

The bus scheme interacts with the postings cache in the same manner as in the threads approach. When a term boards the bus, the cache is probed for an existing post-ings list. If such a list exists, the term will disembark the bus after picking up the payloads from the documents that were added to the DBF since the cache entry was created. Furthermore, retired queries update the cache with the post-ings lists of all their terms. Finally, note that when queries with overlapping terms board the same bus, each overlap-ping term occupies a  X  X ingle bus seat X , i.e. the work required for collecting its payloads is done just once.
The main advantage of the threads approach is its sim-plicity. Queries start processing immediately as they arrive, and processing is straightforward, with each query spending the time to read only the portions of the DBF required by its own terms. With the bus approach, queries may spend some time in the queue and then wait for the next bus cycle. Fur-thermore, within a bus ride, the bus may stop at documents that are not required for a particular query. Bookkeeping is
However, the multiple evaluating threads compete with each other on access to critical sections in the postings cache and the lexicon. This overhead is saved in the bus scheme, which interacts with those resources in a single thread. Fur-thermore, the number of DBF blocks that are read by the threads is higher than the number read by the bus, as terms from different queries that appear in the same documents will generate multiple reads of those blocks. This is espe-cially true as the query rate grows, or as documents become longer. Finally, context switching itself may take a toll on performance when the number of concurrent threads grows.
As mentioned at the top of the section, JiTI is designed to run in tandem with much larger indices on the same plat-form. In light of this we now revisit its design, summarizing the resources it consumes and contrasting it with prior art. RAM We chose not to build a memory-resident index over Disk I/O JiTI only reads from and writes to the DBF. The Query evaluation threads Typically in search engines, dedicated to a single query, which is launched as soon as the query arrives.
This section reports on our experiments with the JiTI system. JiTI was implemented in Java, and all experiments were done on a dual Intel Xeon 2.4GHz machine (512KB L2 cache) with 4GB of RAM, running Linux, on a single JVM. We begin by describing our experimental setup -the queries and documents used, and the measurements we performed. We then describe our two baseline runs (one for each query engine flavor), and analyze their results. Finally, we measure the sensitivity of the JiTI system to various parameters.
We developed a launcher component that simulates the asynchronous events received by a JiTI deployment within an operational search engine, where queries are submitted by users and documents arrive through the engine X  X  crawler. The launcher is repsonsible for injecting queries and docu-ments into the system. It is governed by parameters spec-ifying the query and document Poissonic arrival rates [14], as well as a range of allowed documents lengths (measured in indexable tokens). Within this range, document lengths are distributed uniformly. The queries we used were taken from a log of queries submitted to IBM X  X  intranet search en-gine. We stripped them of syntax elements (plus and minus signs, quotation marks, etc.), leaving each query as an array of terms. Evaluation time was measured from the injection of the query until JiTI X  X  query engine returned the posting lists corresponding to all terms of the query.
 For documents, we used about 25000 parsed and tokenized IBM intranet pages. The words (tokens) of those docu-ments were normalized (mainly lower-cased), and concate-nated into one long sequence of tokens. The launcher read through this sequence, packing blocks of neighboring to-kens into documents according to a distribution of document lengths. Thus, the documents generated by the launcher are a random segmentation of a sequence of real IBM pages.
Note that the scope of our experiments does not cover doc-ument parsing and tokenization, nor do we time the evalu-ation process that search engines perform once posting lists for all query terms are available. Our goal is to measure the impact of the JiTI philosophy (minimal preprocessing of documents, building posting lists on the fly at query time) in any instantly-updatable search system based on inverted indices. Since any such system would require documents to be parsed and tokenized as they arrive, that part of the pre-processing is common to all possible approaches. We begin our experiments at the point at which we diverge from other solutions. Likewise, query processing in any inverted-index based search system involves traversing the posting lists of the query terms. Our system is different in the way those posting lists are acquired, but once those lists are available, we converge with the flow of any other system. In order to isolate our experiments from any implementation issues in the parts that are common between JiTI and any search sys-tem, our experiments focus only on the novel aspects of our approach. Essentially, one can interpret our measurements as the added latency that our approach introduces into the query processing flow -the latency that allows the search results to include the most recent documents.
We performed two baseline runs, one for each query en-gine flavor. Each run simulated 30 minutes of document and Figure 3: Eval. time vs. number of terms in query query injection. The query rate in both cases was approxi-mately 4 . 66 queries per second, and the document injection rate was approximately 4 . 62 documents per second. Doc-ument length was distributed uniformly at random in the range of (1400 , 2200) tokens per document, for an average document length of 1800 tokens -slightly higher than the average number of tokens found on average in TREC X  X  .gov collection (see Section 3). This represents raw data volumes of about 100MB per half hour. Finally, the runs used a in Table 1. The threads engine outperformed the bus engine on average, although its longest evaluation time was higher.
To further investigate the behavior of both flavors, we re-ran the experiments with the postings cache disabled and measured the performance characteristics over the last 10 minutes of each run. Figure 3 displays the average evalua-tion time for queries as a function of the number of terms in the query. The threads evaluate single term queries faster than the bus, and since those terms account for over two-thirds of the queries in the log, the threads attain higher overall throughput. However, for queries of three or more terms, the advantage is clearly with the bus approach. Fig-ure 4 plots the relationship between the query response time and the total length of the output (sum of payloads in all postings lists). The right hand side of the figure, corre-sponding to the threads engine, shows an almost linear de-pendency on a log-log scale between the response time and the output length. The corresponding behavior of the bus engine (on the left) is more random, with many short-output queries requiring relatively long evaluation times. This is rather intuitive, since evaluation times of queries in the bus approach depend on the other queries sharing the same bus ride, and even on queries that have ridden the previous bus.
Figure 5 shows the average response time of both schemes as a function of the document insertion rate. In the range tested, the slowdown in bus performance is linear with the increase in the rate of document insertion, while the per-formance of the threads degrades more rapidly. The same trends are visible in Figure 6, where we maintained the fixed document insertion rate of the baseline runs, but in-creased the average number of tokens per document. Both approaches achieve close performance figures for documents containing up to 2700 tokens, but the bus scheme is better suited for handling larger documents.

Figure 5: Eval. time vs. document insertion rate
Figure 7 plots the average evaluation time attained by both approaches as a function of the length of the experi-ment, in 15 minute (900 second) intervals. As can be seen, the throughput of the threads approach is better for the first 2700 seconds (45 minutes), but then starts to deteri-orate. The threads manage to maintain a slightly higher average throughput over the first hour of the experiment, but are overtaken by the bus soon thereafter (see the 4500 second mark). After 90 minutes (5400 seconds), the aver-age bus evaluation time is still less than 500 ms, whereas the threads reach an average of 1442 ms, which is beyond the scope of the graph. These results are quite consistent with those of Section 5.3: the threads X  performance starts deteriorating rapidly at data volumes that are about twice those of the baseline runs. Note that data may double by doubling the document insertion rate, doubling the average document size, or doubling the duration of the experiment. In either case, the performance trends are the same.
For reference, Figure 7 also plots the number of queries and documents submitted up to each point in time. Those are the two nearly identical linear plots -the plot of the number of documents is the lower of the two.
Figure 8 plots the average response times of both ap-proaches as a function of the query rate. The threads run slightly faster when loaded with up to 10 queries per sec-ond, while the bus handles higher loads better. This is due to the fact that with more concurrent queries, more terms board the same bus ride and so the overlap between the documents containing those terms increases. Consequently, a bus ride can read significantly fewer DBF blocks than the corresponding independent thread runs. Figure 7: Evaluation time vs. experiment length
Figure 9 measures the sensitivity of the average evaluation time to changes in the size of the postings cache. Here, both approaches behave very consistently -the improvement in performance is gentle at small cache sizes up to those of the baseline runs, and more pronounced with larger caches.
Note that the X-axis of the figure represents the capacity of the cache in terms of the total number of payloads. In all runs, the capacity in terms of the number of postings lists was the former figure divided by 8.
This paper presented a new approach, called JiTI, for in-cremental indexing of documents within a search engine that builds hourly delta indices in conjunction with a larger and more stable main index. JiTI indexes and searches over the documents arriving in between delta index builds. The in-tuition behind JiTI is that over such short periods of time, keeping an inverted index up to date is not cost-effective since the expected number of terms that will actually ap-pear in queries is quite small as compared with the vocabu-lary found in the arriving documents. Instead, we advocate maintaining a data structure that requires less processing per arriving document, at the cost of more overhead dur-ing query processing. Specifically, we maintain a DBF -a file consisting of inverted documents in order of arrival, with term appearances in each document pointing to the previous document containing the same term. An in-memory lexicon, containing the vocabulary of all arriving documents, links each term to the latest document in which it has appeared so far, thus enabling the enumeration of all appearances of each term by a backward-motion scan of the DBF.

We proposed two schemes for processing queries. The first scheme employs independent query threads that oper-ate over the DBF, with each thread collecting postings lists for the terms appearing in its associated query. The sec-ond scheme, called the bus approach, is more centralized, and runs in a single thread. It performs repeated scans of the DBF, with each scan building posting lists for terms of several queries. Both approaches maintain a cache of pre-pared postings lists, recouping the investment in building a postings list when the same term appears in later queries.
JiTI is meant to fit within large-scale search systems. Ac-cordingly, our design attempts to maintain low footprints in terms of RAM, open file handles, and I/O demands. Fur-thermore, the bus approach does not require spawning of multiple threads  X  an advantage in large-scale systems.
We experimented with both schemes over a wide range of parameters, including query and document arrival rates, document lengths, caching volumes, and more. Our results show that there are parameters for which both approaches deliver satisfactory query throughput. The more simple threads approach is more efficient at lighter loads, while the bus approach economizes on system resource utilization, and thus scales more gracefully and better handles higher loads. We believe that there are search applications for which the load characteristics we experimented with are sufficient, and so those applications can benefit from our proposed system. The following directions are left for future work: [1] A. Arasu, J. Cho, H. Garcia-Molina, A. Paepcke, and [2] R. Baeza-Yates and B. Ribeiro-Neto. Modern [3] E. A. Brewer. Combining systems and databases: A [4] S. Brin, R. Motwani, J. D. Ullman, and S. Tsur. [5] S. Brin and L. Page. The anatomy of a large-scale [6] A. Broder, D. Carmel, M. Herscovichi, A. Soffer, and [7] E. W. Brown, J. P. Callan, and W. B. Croft. Fast [8] S. Chakrabarti. Mining the Web -Discovering [9] T. Chiueh and L. Huang. Efficient real-time index [10] J. Cho and H. Garc  X  X a-Molina. The evolution of the [11] T. H. Cormen, C. E. Leiserson, and R. L. Rivest. [12] M. Fontoura, J. Zien, E. Shekita, S. Rajagopalan, and [13] A. S. Foundation. Apache lucene search library. [14] R. G. Gallager. Discrete Stochastic Processes . Kluwer [15] S. Heinz and J. Zobel. Efficient single-pass index [16] B. J. Jansen, A. Spink, and T. Saracevic. Real life, [17] B. T. J  X onsson, M. J. Franklin, and D. Srivastava. [18] R. Lempel and S. Moran. Predictive caching and [19] N. Lester, J. Zobel, and H. Williams. Efficient online [20] M. Lifantsev and T. Chiueh. I/O-concious data [21] L. Lim, M. Wang, S. Padmanabhan, J. S. Vitter, and [22] E. P. Markatos. On caching search engine query [23] S. Melnik, S. Raghavan, B. Yang, and [24] S. Mitra, W. W. Hsu, and M. Winslett. Trustworthy [25] W.-Y. Shieh and C.-P. Chung. A statistics-based [26] C. Silverstein, M. Henzinger, H. Marais, and [27] F. Silvestri. High Performance Issues in Web Search [28] A. Tomasic, H. Garc  X  X a-Molina, and K. Shoens. [29] I. Witten, A. Moffat, and T. Bell. Managing
