 I  X  lker Kocabas  X   X  Bekir Taner Dinc  X er  X  Bahar Karaog  X  lan Abstract In this article, we introduce an out-of-the-box automatic term weighting method for information retrieval. The method is based on measuring the degree of divergence from independence of terms from documents in terms of their frequency of occurrence. Divergence from independence has a well-establish underling statistical the-ory. It provides a plain, mathematically tractable, and nonparametric way of term weighting, and even more it requires no term frequency normalization. Besides its sound theoretical background, the results of the experiments performed on TREC test collections show that its performance is comparable to that of the state-of-the-art term weighting methods in general. It is a simple but powerful baseline alternative to the state-of-the-art methods with its theoretical and practical aspects.
 Keywords Information retrieval Nonparametric index term weighting Statistical dependence Pearson X  X  Chi-Square statistics 1 Introduction Any text-based IR system has two integral parts: (1) a term weighting method, such as TFxIDF, and (2) a retrieval model (or ranking model), such as the vector space model. Words constituting the text do not contribute evenly to the informative content of the document. In this context, the research question of interest in term weighting can be stated as: which terms are related to which documents with respect to informative content? We base our answer to this question on a frequentist point of view by using the simple assumption that in a document written in natural language, some words are used due to grammatical necessity (i.e., semantically nonselective or function words), while some are used to reflect the document X  X  contents (i.e., semantically selective or content bearing words). Given a collection of documents, this implies that function words are supposed to appear in every document in contrast to content bearing words (keywords), provided that the collection is composed of documents with different topics. This assumption directly leads to a well-known method of distinguishing keywords from function words, called the Inverse Document Frequency -IDF (Sparck Jones 1972 ; Robertson and Sparck Jones 1976 ).
In order to identify the terms that contribute to the informative contents of documents, every frequentist approach to term weighting requires making a second assumption about language use which may be stated as: there is a causal relation between frequency of a word occurrence and its contribution to informative content. The question then arises on how to model such a relation, in order to quantify the contribution of a word to informative content based on its frequency of occurrence. On this account, Luhn ( 1958 ) states that  X  X  X he frequency of word occurrence in an article furnishes a useful measurement of word sig-nificance X  X , and claims that  X  X  X eywords tend to occur at mid-range frequencies in docu-ments, rather than at low or high frequency ranges. X  X  If Luhn X  X  claim holds perfectly true, then keywords would completely be distinguished from function words by means of a method utilizing within document term frequencies . Unfortunately, this is not the case in practice.

We can further make a third assumption so as to have a better model: Keywords have frequency distributions different from that of function words in the population of docu-ments. On this account, Harter ( 1975a , b ) claims that both keywords and function words follow a Poisson distribution in the population of documents but with different means. If Harter X  X  claim holds perfectly true, then keywords would completely be discriminated from function words by means of a method utilizing term frequency distributions . Again as in Luhn X  X  claim, this is not the case either. In the same line of research, a recent answer comes from Amati and van Rijsbergen ( 2002 ). They claim, in principle, that a semantically selective word occurs in a semantically related content with a frequency different from the frequency of the word in common use, which can be determined by a model of random-ness, such as Poisson distribution, e.g. PL2.

Our divergence from independence (DFI) hypothesis which brings another point of view content with a frequency different from the frequency of a word in common use, which can be determined by the saturated model of independence. According to the notion of inde-pendence, if the ratio of the frequencies of two particular words remains constant across all documents, then the occurrences of those words are said to be independent from the documents. Assuming that the amount of contribution of a word to the information content of a document is proportional to the observed frequency of the word in the document, we can further say that both words evenly contribute to the information content of every document. As the function words are the ones which make equal contribution to the contents of every document, one can therefore discriminate the keywords from the function words by measuring the divergence of the observed frequency of each term in the docu-ments from the frequency expected under independence.

With a system using DFI term weighting functions, we participated in TREC 2009, settings of indexing was not ideal for DFI in TREC 2009 (i.e., stop-word elimination was applied to the ClueWeb09-T09B data set), average retrieval performance with respect to other systems has been achieved. On the other hand, in our system run on the TREC 2010 Web track data set, stop-words were preserved and two supplementary techniques were used: (1) n -gram phrase matching and (2) spam-page filtering. The official TREC results show that it is one of the best performing runs (Clarke et al. 2010 ). In this respect, this article presents an in-depth description of how the concept of independence can be exploited in term weighting, and the retrieval performance comparisons with the state of-the-art term weighting methods/models using past TREC test collections.

The organization of this article is as follows. The previous works related to DFI term weighting are summarized in the  X  X  X elated Work X  X  section.  X  X  X odels of independence and measures of dependence X  X  and  X  X  X erm weighting based on DFI X  X  are given afterwards. The design of experiments, the experimental results, a short discussion, and the conclusions are presented subsequently. 2 Related works Early approaches to the term weighting problem, from the viewpoint of statistics, date back to the pioneering work of Maron and Kuhns ( 1960 ). Since then, several attacks have been made, including the works of Damerau ( 1965 ), Bookstein and Swanson ( 1974 ), Robertson and Sparck Jones ( 1976 ), Cooper and Maron ( 1978 ), Croft and Harper ( 1979 ), Fuhr ( 1989 ), Margulis ( 1992 ), Wong and Yao ( 1995 ), Ponte and Croft ( 1998 ), and Amati and van Rijsbergen ( 2002 ). Among all, Harter ( 1975a , b ) is the first researcher who introduced the notion of eliteness , a notion that gave rise to the most successful probabilistic approaches to term weighting problem. According to Harter, there are  X  X  X pecialty words X  X  and  X  X  X on-specialty words X  X . Specialty words are the ones which occur densely in  X  X  X lite X  X  documents whose informative contents are composed of the meaning conveyed by those speciality words. In contrast, non-specialty words are the ones which occur randomly, and which do not contribute to the contents of the document. Harter claims that specialty words differ from non-specialty words in distribution on a collection of documents, and both the spe-cialty and non-specialty words follow a Poisson distribution with different means k 1 and k , respectively, where k 1 [ k 2 .

In spite of the fact that the original  X  X 2-Poisson model X  X  of Harter is plausible in theory, Robertson et al. ( 1981 ), and generalized by Robertson and Walker ( 1994 ) as a series of successful implementations called BMs (e.g., BM25). In the same line of research, Amati and van Rijsbergen ( 2002 ) developed a sophisticated probabilistic model/framework, called the divergence from randomness -DFR, by further refining the notion of eliteness on the basis of the semantic information theory (Hintikka 1970 ) and the Popper X  X  ( 1995 ) notion of informative content. Finally, in their recent work, Clinchant and Gaussier ( 2010 ) incorporate the burstiness notion (Church 1995 ) with the notion of eliteness so as to result in better and simpler information-based methods of term weighting, such as the method with a priori log-logistic distribution called LGD.

Among all the previous works in this line of research, DFI-based term weighting is mostly related to DFR-based term weighting. Randomness in occurrence characterizes non-speciality words within the Harter X  X  paradigm, and it is quantified in the context of DFR by means of a basic model of randomness , which corresponds to a Poisson distri-bution in the paradigm. In order to model the actual (chance) distribution of frequency of word occurrence in documents on the population, Amati and van Rijsbergen ( 2002 ) examine several probability density functions, such as Hyper-Geometric, Bose-Einstein, etc., as well as Poisson distribution. They claim that speciality words are the ones which occur in documents with a frequency different from the frequency suggested by the assumed model of randomness; and complementarily, non-speciality words are the ones which occur in documents with a frequency that can be attributed to chance under the assumed model of randomness.

On the other hand, statistical independence takes the place of randomness in the context of DFI, such that non-speciality words are characterized by independence in occurrence rather than randomness. In this respect, DFI is the nonparametric counterpart of DFR. From the viewpoint of statistics, a term weighting method based on DFR or DFI can be considered as a method/procedure of (inductive) inference from observations to popula-tion, where observations are observed frequencies of words in documents and population is the collection of frequencies that could be generated by a model of randomness or inde-pendence. As an inferential procedure, term weighting based on DFR is of parametric type in contrast to DFI, since it is necessary to make an assumption about the precise shape/ form of the actual distribution of frequency of word occurrence in the population of documents in order to define what is random (Wolfowitz 1942 ; Bradley 1968 ). It is worth mentioning in particular that DFR models are also qualified as nonparametric, but the term  X  X  X onparametric X  X  means no-parameter or parameter-free in this context. On this account, it can be said that DFI models are nonparametric in both senses. 3 Measures of divergence from independence Divergence from independence (DFI) implies statistical dependence. In brief, statistical dependence refers to a relation between two random variables that makes one of the random variables less or more probable to take on a value when the value of the other one is given. To measure the degree of statistical dependence between two random variables, there needs to be a model of independence that expresses the relation between the random variables in terms of probability models. The model of independence that is used in the saturated model in statistics. 3.1 Saturated model of independence Suppose that the cells of a r 9 c contingency table contain densities or proportions, p ij for i  X  1 ; 2 ; ... ; r and j  X  1 ; 2 ; ... ; c . Then, the marginal density for row i is given by p i = and the marginal density for column j is given by p j = categorical random variables X and Y can be characterized in terms of probability models relating cell densities. Multiplication rule of probability states that if two random variables are independent of each other in distribution, the bivariate probability distribution of the random variables, f X , Y ( x , y ), can be expressed as the product of their marginal probability events, one of which is defined in the sample space of X (i.e., X = i ) and the other one is defined in the sample space of Y (i.e., Y = j ), can be expressed as the product of the marginal probabilities of occurrences of individual events: where Pr ( X = i , Y = j ) corresponds to the cell density p ij , Pr ( X = i ) the marginal density p j  X  1 ; 2 ; ... ; c .

Under independence, the observed density in the cell ( i , j ), tf ij / N , is therefore expected observed marginal density for column j , D j / N : for i  X  1 ; 2 ; ... ; r and j  X  1 ; 2 ; ... ; c .

Among all, the simplest model of independence is the equal cell probability model , implying that all r 9 c possible events are equally likely. Other models which assume constant densities across rows and constant densities down columns are given by
In the constant column density model , the marginal density for each column is p j = 1/ c .Also in the constant row density model , the marginal density for each row is p i = 1/ r .
Multiplication rule of probability may be expressed in terms of the three simple models, such that The first term represents the density for cell ( i , j ) under the constant cell density model. The second term represents the ratio of the marginal density for row i to the marginal density under the constant row marginal model. The third term represents the ratio of the marginal density for column j to the marginal density under the constant column marginal model.

This independence model can also be interpreted as the product of an average effect [1/ rc ], a row effect  X  r p i , and a column effect  X  c p j . Under independence, the row effect and the column effect account for all the variation in cell densities.

If this independence model does not hold, cell densities can be expressed by including a residual as given by to the statistical dependence between the row and column categories.

This model of independence is called the saturated model , because it perfectly fits any given contingency table: for i  X  1 ; 2 ; ... ; r and j  X  1 ; 2 ; ... ; c .

The degree of divergence of the residual from the value 1 gives the magnitude and pendence in the observed frequency of occurrence of t i in d j can therefore be measured as principle, for the purpose of term weighting.

The residual of the saturated model of independence can also be regarded as the ratio of the relative frequency of occurrence of term t i in a document d j to the relative collection wide frequency of that term: ( tf ij / D j )/( TF i / N ), which expresses the DFI hypothesis in the true sense:  X  X  X o what degree does the frequency of occurrence of a term in a document diverge from the frequency of occurrence of the term in common use? X  X  The DFI hypothesis assumes that a function word has a relative frequency ratio that is equal to 1. This means that a function word is a term whose collection frequency distributes on documents proportional to the length of the documents: then for a function word t i for j  X  1 ; 2 ; ... ; c . 3.2 Normalized chi-squared distance from independence Saturated model of independence is the underlying model of the Pearson X  X  chi-squared test of independence (Agresti 2002 ). The test statistic used in chi-squared tests is given by
Since Pearson X  X  chi-square statistic is simply the sum of squares of the standardized degree of divergence from independence in observed cell frequencies, each component of G 2 is usually referred to as the normalized chi-squared distance from independence. In addition to the DFI formula given in (1), both standardized and chi-squared distance from independence can be used as a measure of DFI.

In general, for any experiment whose sample space can be divided into two disjoint subsets, success refers to the event that an outcome from the subset of interest occurs, and failure refers to the event that an outcome from the other subset occurs.

Suppose that outcome of an experiment is the classification of a given observation into one of the r 9 c cell categories in a contingency table, such that the sample space of the experiment is composed of r 9 c outcomes: classifying a given observation into the first cell category, classifying the same observation into the second cell category, and so on. Let success be the classification of an observation into a particular cell category, and failure be the classification of the same observation into one of the other ( r 9 c ) -1 cell categories. Then, cross-classification of a document collection of size N with respect to the two categorical random variables X and Y can be modeled as a random experiment, a series of N identically and independently repeated classification over r 9 c cell categories.
A Bernoulli trial is an experiment that has dichotomous outcomes such as  X  X  X ead X  X  which is usually referred to as success , and  X  X  X ail X  X  which is usually referred to as failure in the case of throwing a coin. If we denote the probability of success in a Bernoulli trial by p and the number of successes in a series of N identically and independently repeated Bernoulli trial by k , then the expected number of successes E ( k ) is given by N 9 p . For the case of the cross-classification of a document collection of size N , the expected number of value one gives the direction and magnitude of the degree of divergence from randomness variables X and Y in the population determines the probability of classifying an observation in cell category ( i , j ), p ij , and one would therefore measure the degree of divergence from randomness in cell frequencies. On the other hand, since the probability of classifying an observation in cell category ( i , j ) can be expressed under independence as the product of the marginal probability of classifying the observation in row category i and the marginal probability of classifying the observation in column category j , p i 9 p j , we can calculate the degree of divergence from independence in cell frequencies without knowing or making any assumption about the actual bivariate distribution of the random variables X and Y on the population. This is the reason why divergence from independence is considered the nonparametric counterpart of divergence from randomness, from the viewpoint of statistics.

It can be shown that the number of successes k in a series of N identically and inde-pendently repeated Bernoulli trial follows a binomial distribution with mean l = Np and variance r 2 = Np (1 -p ). Let K be a binomial random variable. Then, it can also be shown that the random variable Z  X  X  K l  X  =  X  binomial random variable K , follows a binomial distribution with zero mean and unit variance. This transformation, which is commonly referred to as the standardization in statistics (i.e., standard scores or z -scores), transforms any given set of data to a set of data original data.

In consequence, the standardized frequency in cell ( i , j ), j  X  1 ; 2 ; ... ; c .

Now suppose, without loss of generality, that we have a contingency table with only two cells. Let tf be the number of observations in the first cell, N be the total number of observations, and p be the probability of classifying an observation in the first cell. Then, the degree of divergence from randomness in the first cell and the degree of divergence from randomness in the second cell are respectively given by E ( N -tf ) for the second cell is N (1 -p ). The total degree of divergence from randomness in this contingency table with two cells can be calculated as
This suggests that the total degree of divergence from randomness in a given ( r 9 c ) contingency table can be measured as the sum of squares of the standardized cell frequencies:
Now consider the case of the cross-classification of a large document collection, where magnitude. In such a case, the mean and the variance of the binomial distribution asso-ciated with each cell would approximately be equal to each other: Np ij &amp; Np ij (1 -p ij ) for approach to zero, the total degree of divergence from randomness in cell frequencies can be approximated as It follows that the total degree of divergence from independence can be calculated as where e ij = N ( p i p j ) = Np ij under independence. Each component of G 2 , which measures the normalized chi-squared distance from independence in cell frequencies, can be used for measuring the degree of DFI in cell frequencies as well as the stan-dardized distance, 3.3 Term frequency normalization Term frequency normalization is an important issue for the state-of-the-art term weighting methods, and it has shown that it has a significant impact on retrieval performance (Singhal et al. 1996 ; He and Ounis 2003 , 2005 ).

The ratio of the observed cell frequency tf ij to the frequency expected under indepen-dence can be rewritten as where tfn ij is the normalized frequency of term t i , as defined in the work of Amati and van Rijsbergen ( 2002 ), k i is the sample estimate of the population mean frequency of occurrence of term t i in a document, and D is the average length of a document in the collection given. This shows that DFI term weighting models implicitly apply term fre-quency normalization, which fosters our proposed DFI measures further. 4 Scoring documents: an information theoretic approach Our past TREC experiences on DFI-based term weighting suggest that the way how to aggregate DFI measurements over all terms of a given query for getting the final scores of documents is one of the factors that largely influences retrieval performance. The docu-ment scoring function which best accords, by design, with the DFI measures in (1), (2), and (3) is the one that is stemmed from Shannon X  X  information theory ( 1949 ). On the basis of the theory of probability, Shannon ascribes a measure to the information conveyed by a where Pr ( s ) denotes the probability of observing signal s in the channel. The information that is conveyed by a signal, I( s ), increases as the probability of occurrence of that signal in the channel, Pr ( s ), decreases. This measure of information quantifies the self information value of a signal in terms of the number of  X  X  X its X  X  required to represent that signal within the finite set of signals to which the signal belongs.

Assume that a message M of length N is a series of N signals each of which is identically and independently emitted from a source to a destination. Cross-classification of a docu-ment collection into r 9 c cell categories can then be modeled as a transmission. The set of r 9 c cell categories described herein corresponds to the finite set of r 9 c signals, where times in the channel.

In this information theoretic model, emission of a signal refers to a Bernoulli trial, emission of one of the other ( r 9 c ) -1 signals. Thus, the emission probability p ij associated with each signal s ij is again determined by a binomial distribution with mean Np ij and variance Np ij (1 -p ij ), for i  X  1 ; 2 ; ... ; r and j  X  1 ; 2 ; ... ; c . On the basis of the model given, self-information value of a signal s ij can be expressed as model of independence which would in general provide a better fit to the log-transformed cell densities than that of the original model to the raw cell densities (Hoaglin et al. 1983 ). and itself gives the magnitude and direction of DFI in self-information. It follows that, for information is given by for ( dfi ij [ 0) and zero elsewhere, where  X  X  ldfi  X  X  stands for  X  X  X og-DFI X  X  and dfi is the measure of DFI given in (1). In a general sense, the lesser the probability of observing a degree of DFI in tf ij equal to dfi ij the higher is the amount of information on the content of d j that we would get by observing term t i , and vice versa.

As a result, given a query q , the simplest information theoretic, DFI-based document scoring function can be formulated as document scoring function basically measures the contribution of document d j to the information on q under the assumption of independence. 4.1 On the two sources of information The fact that the measures of DFI presented in this article are defined from a cell-centric viewpoint suggests that they utilize only the first source of information,  X  X  X ithin document term frequencies X  X . In scoring documents with DFI, in order to take into account the second source of information,  X  X  X erm frequency distributions X  X , the measured degree of DFI in the self-information value of each query term needs to be weighted in accordance with the contribution of the terms to total inertia .

The total inertia can be thought of as a measure of the magnitude of the total row squared deviations from independence or equivalently the magnitude of the total column squared deviations. The total inertia associated with the system represented by a given data inertia (CTI) is for i  X  1 ; 2 ; ... ; r . Under independence, contribution of each term to total inertia is nec-essarily zero.

Under independence, the collection frequency TF i of t i is expected to be distribute on documents proportional to the lengths of documents D j s, resulting in a zero contribution to which the use in documents is due to a reason other than serving to impart knowledge would have no contribution to total inertia. Thus, the weight that would be assigned by means of G i 2 / N for term t i is expected to be close to zero in magnitude when the use of t i in documents is of function type: otherwise, a weight that is greater than zero is expected.
As a result, to take into account the second source of information in the sense of term specificity, one can use CTI as a weighting factor in scoring documents: where G i 2 / N is equivalent to G i 2 with respect to document ranking. 4.2 A DFI 9 IDF weighting scheme Essentially, the function of CTI component in DFI 9 CTI weighting scheme is the same as the function of the IDF component of TF 9 IDF weighting scheme (Salton and Buckley 1988 ): both are a measure of term specificity. Thus, it is also possible to use IDF instead of CTI for scoring documents based on DFI.

Robertson ( 2004 ) states that  X  X  X  query term which occurs in many documents is not a good discriminator, and should be given less weight than one which occurs in few doc-uments X  X . Although both IDF and CTI are a measure of term specifity, they are different in that the weight of a term occurred in whole documents of a collection is measured zero by IDF. In measuring term specificity based on CTI, a term may be given a zero weight only if its frequency of occurrence is independent of documents. Otherwise, a weight that is greater than zero will be given, no matter how many documents the term occurs in. In theory, this property of CTI provides a certain stability in measuring term specificity. Suppose that the term of interest is  X  X  X nd X  X . Since it is likely that the term  X  X  X nd X  X  occurs in every document, IDF would fail to identify its specificity to a document that could provide information on the linguistic properties of  X  X  X nd X  X  as a conjunction, whereas CTI would not, to a certain extent.

Besides, IDF suffers, at least in theory, from a certain weakness for which CTI can also be a remedy. Church and Gale ( 1995 ) state this weakness as  X  X  ... favoring extremely rare words, no matter how they are distributed . X  X  4.3 The relation between DFR and DFI Amati and van Rijsbergen ( 2002 ) express the DFR term weighting model in terms of probability models as  X  X  X he weight of a term in a document is a function of two probabilities Prob 1 and Prob 2  X  X  : w = (1 -Prob 2 ) 9-log 2 ( Prob 1 ), where Prob 1 refers to the con-ditional probability of the frequency of occurrence of a term in a document to the whole document collection, and Prob 2 refers to the same probability but conditional to the elite set. In the DFR framework, it is assumed that elite set of a term is the set of all documents domness model, a probability distribution function with mean k 1 , and Prob 2 is determined by a probability distribution function, which is different from the basic randomness model assumed, with mean k 2 .

In the family of Pareto distributions, a Type-I Pareto distribution has two parameters (Arnold 1983 ), a (necessarily positive) minimum possible value of K , k min [ 0, and a positive valued parameter n [ 0 (i.e., Pareto index), and it is given by for k C k min , and zero elsewhere. A Type-I Pareto distribution with k min = Np ij and n = 1 can be considered as the characteristic distribution function of the degree of divergence from randomness in observed cell frequencies tf ij for i  X  1 ; 2 ; ... ; r and j  X  1 ; 2 ; ... ; c : for tf ij C Np ij , and zero elsewhere. Assuming that the minimum possible frequency of occurrence of a function word in a document is k min , the probability of being a function word for term t i in document d j can therefore be calculated under independence as information can thereby be rewritten as
Suppose that the basic randomness model is a Type-I Pareto distribution with k min = k 1 and n = 1:
Then, the informative content of a term t i in document d j , which is defined by Amati and divergence from randomness in self-information: where k 1 = Np ij for i  X  1 ; 2 ; ... ; r and j  X  1 ; 2 ; ... ; c :
The total information contained in a message M of N independently and identically emitted signals from a source with emission probability p ij is given by This also gives the total information that would be contained in a document collection D of length N under randomness or a pure chance distribution. The observed total infor-mation in a given document collection can be calculated as
It follows that the total degree of divergence from randomness in self-information is given by where the third line is rank equivalent to the fourth line.
Thus, assuming a Type-I Pareto distribution for Prob 2 with k min  X  k 2  X  k 1 ; I DFR  X  D  X   X  X  can be rewritten as In here, gives the probability of observing term t i in document d j less than k 1 times. In one sense, it refers to the risk of accepting the term t i as a descriptor of the document d j . The odds in favor of accepting t i as a descriptor of d j is given by
Given a pair of terms in a particular document, the odds ratio gives the risk of accepting one of the terms relative to the other as the descriptor of the document, i.e., relative risk.
This suggests, as a result, that DFR models can be reduced to DFI models under independence. To give a concrete example, consider the DFR model PL2 which assumes a Poisson distribution with mean l = k 1 for the basic randomness model, and a probability distribution based on Laplace law of succession for Prob 2 . Then, which in fact gives the probability of observing a term t i that was already occurred tf ij times in document d j , one more time, under a Type-I Pareto distribution with k min = k 2 = tf ij and n = 1. This suggests that PL2 can be reduced to a nonparametric model based on DFI under independence. 5 The DFI retrieval functions This study proposes three measures of statistical dependence for the purpose of term weighting: based on the saturated model of independence; based on standardization and normalized chi-squared distance, respectively, for i  X  1 ; 2 ; ... ; r and j  X  1 ; 2 ; ... ; c .

Based on a measure of statistical dependence, the degree of relevance of a document d j to a given query q can be measured as for ( tf ij -e ij ) [ 0 and zero elsewhere. In here, w ij represents a possible weighting scheme for weighting query term t i with respect to document d j based on DFI. In this study, we examined 9 possible DFI weighting schemes as listed in Table 1 . 6 Experimental design and materials In our experiments, TERRIER retrieval platform version 3.0 (Ounis et al. 2007 ) is used to index and search TIPSTER disks 1 &amp; 2, TREC disks 4 &amp; 5, and Clueweb09-T09B data sets. TIPSTER disks 1 &amp; 2 consist of about 740,000 documents from the Wall Street Journal, the Federal Register, the Associated Press, Department of Energy abstracts, the Computer Select disks of Ziff-Davis, which are used in the ad hoc tracks of TREC 1 through 3. TREC disks 4 &amp; 5 consist of about 550,000 documents from the Financial Times, the Congressional Record of the 103 rd Congress, the Federal Register, the Foreign Broadcast Information Service, and the Los Angeles Times, which are first used in TREC 6 ad-hoc track. They are also used in the ad-hoc tracks of TREC 7 &amp; 8 and the robust tracks of TREC 2003 and TREC 2004 but without the Congressional Record (about 28,000 documents). Clueweb09-T09B consists of over 50 million (English) WEB pages crawled from the Internet in between January 2009 and February 2009, which is used in the WEB tracks of TREC 2009, 2010, and 2011 as  X  X  X ategory B X  X  document collection.

During indexing and searching, terms are stemmed by the TERRIER X  X  implementation of Porter X  X  stemmer but no stop-word elimination is applied to the documents: DFI models depend on the lengths of documents and stop-word elimination degenerates documents in length, arbitrarily. Nevertheless, performance evaluations also carried out on the stop-word eliminated versions of TIPSTER disks 1 &amp; 2 and TREC disks 4 &amp; 5 document collections for the comparison.

During indexing, a cascaded term filter is applied to the TERRIER X  X  default term validity checker, which can be expressed in a regular expression as  X  a zA Z  X nn 0 nn &amp;  X  a zA Z  X  X  0 9 f 0 ; 4 gj X  0 9 f 1 ; 4 g .

This regular expression accepts such symbol sequences as valid terms which are ampersand surrounded by letters (e.g.  X  X  X T&amp;T X  X ), or letters followed by numbers of maximum 4 digit long (e.g.  X  X  X S2 X  X ,  X  X  X M25 X  X ,  X  X  X REC2004 X  X , etc), or numbers of maximum 4 digit long.

In each track of TREC, a set of 50 topics is used to measure and compare the retrieval performances of the runs submitted to the track, which cumulatively sum up to 550 topics over 11 tracks: the topics 51 X 200 of TREC 1, 2, and 3 ad hoc tracks; the topics 301 X 450 of TREC 6, 7, and 8 ad hoc tracks; the topics 601 X 700 of TREC 2003 and 2004 robust tracks; and the topics 1 X 150 of TREC 2009, 2010 and 2011 WEB tracks. But, there are actually 545 effective topics in total due to the lack of success in discovering relevant documents to some topics at the time of construction. The topics with no relevant documents are the topic 672 of TREC 2004 robust track and the topics 19, 20, 95, and 100 of TREC 2009 WEB tracks.
 While scoring a document, TERRIER retrieval platform ignores the terms that have low IDF (i.e., it ignores a term if the marginal frequency of the term is greater than the number of documents in a given document collection: TF i [ c ) by default. This feature is rea-queries, it would be either ineffective whenever such a query term is not present or dramatically effective in the negative sense whenever most of the query terms would be ignored in scoring, such as topic 42  X  X  X he music man X  X , or topic 70  X  X  X o be or not to be that is the question X  X , or topic 92  X  X  X he wall X  X . Thus, it is not used for very short queries.
Every term weighting method is allowed to return a result set of maximum 1,000 documents from a corresponding data set to an associated query. For the TREC 2009, 2010, and 2011 Web track topics, spam-page filtering (Cormack et al. 2010 ) is applied to the result sets returned by each term weighting model, as given by where 0 B d B 1 is a weighting factor (where d = 1 means no spam-page filtering) and 0 B / B 0.50 is the percent of spammness calculated for document d j in Cormack et al. ( 2010 ): the documents with / [ 0.50 are assumed not spam in this study, where / = 0 refers to 100 % chance of being a spam document and / = 0.50 means 50 % chance of being a spam document. For example, d = 0.5 and / = 0 yields the half of the original score of document d j , and if / = 0.50 (implying 50 % chance of being a spam document), then the original score is left intact at d = 0.5. The resultant sds( q , d j ) scores are then used to re-rank the result sets. The base run of each term weighting method which employs no spam-page filter is allowed to return a result set of maximum 10,000 documents. Spam-page filtering is then applied to that result sets of size 10,000, and the resultant re-ranked result sets are reduced to 1,000 in size.

The retrieval performance of DFI is compared with the retrieval performances of 5 standard term weighting methods as implemented in TERRIER. We follow the TERRIER X  X  method naming convention for those weighting methods. Out of the DFR-based term weighting methods, there are 3 methods considered:  X  X  X M25 X  X ,  X  X  X -LM X  X , and  X  X  X -LM X  X .  X  X  X M25 X  X  is the OKAPI X  X  BM25 (Robertson et al. 1999 ).  X  X  X -LM X  X  is the Hiemstra X  X  language model (Hiemstra 2000 ).  X  X  X -LM X  X  is also, a language model but it uses Bayesian smoothing with Dirichlet Prior (Zhai and Lafferty 2004 ). The remaining 2 are DFR-based methods (Amati and van Rijsbergen 2002 ; Macdonald et al. 2005 ; He and Ounis 2003 ):  X  X  X Free X  X  and  X  X  X L2 X  X .  X  X  X Free X  X  is a parameter-free DFR method that assumes a Hypergeometric term distribution.  X  X  X L2 X  X  is a parametric DFR method that assumes a Poisson term distribution as a basic randomness model with Laplace after-effect, and tailored to the tasks that require early precision.

In the experiments, we measure retrieval performance on the out-of-the-box runs of those 5 term weighting methods with the default values of parameters as defined in TERRIER, since DFI models are of out-of-the-box type, by design. One may therefore obtain different performance scores than the ones presented in this article by tuning the values of necessary parameters for individual data sets, or by changing the operational settings of the experiments.

For the methods based on parametric models, change of operational settings, such as the change of document collection or query, would in general invalidate the tuned values of parameters and hence causes a reduction in their expected retrieval performance. The benefit of DFI approach is, in this respect, that a nonparametric model is expected to be robust against such changes (Sect. 8 ). In addition to the out-of-the-box runs of  X  X  X M25 X  X  and  X  X  X L2 X  X , the runs with the best parameter values for the TIPSTER disks 1 &amp; 2 and the TREC disks 4 &amp; 5 document collections, which are obtained by optimizing MAP using a simulated annealing process 1 , are also taken into account to have strong baseline perfor-mance scores in comparisons. The best b parameter values of  X  X  X M25 X  X  are respectively 0.3277 and 0.3444, and the best c parameter values of  X  X  X L2 X  X  are 4.607 and 9.150. For the Clueweb09-T09B document collection, we used the best parameter values obtained on  X  X  X T10G, TREC9-10 Web Tracks X  X : b = 0.2505 and c = 12.33. The best runs are denoted by  X  X  X M25B X  X  and  X  X  X L2B X  X  in the presentation of the experimental results. 7 Experimental results The runs of the term weighting methods under consideration are evaluated using two performance measures: Mean Average Precision ( X  X  X AP X  X ) and the total number of rele-vant documents retrieved ( X  X  X R X  X ) over all queries.

The results of the experiments performed are presented in two parts. First, DFI-based models are compared with the 5 standard term weighting models using test collections with stemming but applying no stop-word elimination. Second, to see whether stop-word elimination affects the retrieval performance, the same analysis is repeated by applying stop-word elimination to TIPSTER disks 1 &amp; 2 and TREC disks 4 &amp; 5 document collections. 7.1 Part I Table 2 lists the observed performance scores of the runs of the weighting models under consideration for both the very short (topic only) versions and the short (topic and description) versions of the (first) set of ad-hoc topics 51-200 (denoted by TS-I) and the (second) set of ad-hoc topics 301-450&amp;601-700 (TS-II).

Figure 1 shows the multiple comparisons after Friedman X  X  test (Hollander and Wolfe 1999 ) based on MAP scores for the very short versions of TS-I and TS-II. Friedman X  X  test is the nonparametric counterpart of the balanced two-way ANOVA test: it tests only for row effect (i.e., methods) after adjusting for possible column effects (i.e., topics). It refers to the balanced one-way ANOVA with homogenous blocks. In this respect, it only tests the run effect or the topic effect, at a time, which in fact makes it more appropriate for the multiple comparison of the results of IR experiments than ANOVA (Hull 1993 ). The multiple comparisons shown in Fig. 1 use Tukey X  X  honestly significant difference criterion (Tukey X  X  HSD), which is based on the Studentized range distribution.

For the very short version of the TREC 1, 2, and 3 ad-hoc topics, 51-200 (TS-I), it appears that  X  X  X -IDF X  X  model has a retrieval performance that is not significantly different from that of the baseline runs with best parameter values,  X  X  X M25B X  X  and  X  X  X L2B X  X . For this set of 150 topics, IDF component seems to contribute to the retrieval performance of DFI models significantly higher than that of CTI component, and when CTI component con-sidered separately it does not result in a significant effect on the retrieval performance of DFI models.

For the very short version of the TREC 6, 7, and 8 ad-hoc topics and TREC 2003 and 2004 robust track topics, 301-450&amp;601-700 (TS-II), the case is slightly different. For this set of 250 topics,  X  X  X FIZ X  X  model itself has a retrieval performance that is not significantly nificant effect on the retrieval performance of  X  X  X FIZ X  X  model as well as  X  X  X FIB X  X  and  X  X  X FIC X  X  models. Moreover, in contrast to TS-I, the effect of CTI component and the effect of IDF component are also not significantly different from each other for this topic set. For the short versions of TS-I and TS-II, the multiple comparisons are given in Fig. 2 . The case of short queries (topic and description) is similar to the case of the very short queries (topic only): DFI models have a retrieval performance that is not significantly different from that of baseline runs.

For the TREC 2009, 2010, and 2011 Web track topics 1-150 (TS-III), the observed performance scores are given in Table 3 for the varying values of spam-page filtering weight d . Multiple comparison is given in Fig. 3 .

Relating all the analyses performed, it would appear, as a result, that nonparametric DFI models perform as well as parametric models. The DFI model  X  X  X -IDF X  X  seems to be a good choice among the alternatives for well-structured, controlled document collections like the ones in TIPSTER disks 1 &amp; 2 and TREC disks 4 &amp; 5. On the other hand, the DFIZ or DFIC model with or without IDF/CTI component seems to be a good choice for non-controlled document collections like the collection of Web pages, after applying spam-page filtering. 7.2 Part II This part presents the evaluation of the runs of the term weighting methods using the stop-word eliminated versions of TIPSTER disks 1 &amp; 2 and TREC disks 4 &amp; 5. Table 4 lists the observed performance scores for both the very short versions and the short versions of TS-I and TS-II.

For the term weighting models,  X  X  X L2B X  X  and  X  X  X M25B X  X , the parameter values are tuned based on MAP for very short versions of topic sets using the stop-word eliminated versions of document collections. Thus, the observed MAP scores are the maximum ones for those runs.

Together with the results of the multiple comparisons given in Fig. 4 , it can be said, all retrieval performance of DFI models. DFI models are still comparable with the baseline runs in retrieval performance.

It is seen clearly that the CTI component has a significant contribution to the retrieval performance of DFI models, especially for the model DFIZ. Interestingly, in contrast to CTI, IDF component results in a consistent negative effect on retrieval performance. 8 Discussion In the context of statistical inference, the advantage of a nonparametric procedure is that, even when the parametric assumptions about the distribution of the sampled population hold perfectly true, a nonparametric procedure is only slightly less powerful than its parametric counterpart. But, if the parametric assumptions are failed to hold, only the nonparametric test procedures are valid (Mosteller and Tukey 1977 ). This implies that a parametric term weighting model should outperform its nonparametric counterpart on the document collections in which the frequency of term occurrence perfectly follows the distribution assumed by the parametric model. For example PL2 assumes a Poisson term distribution, and it should outperform DFI models on those document collections where the actual frequency of term occurrence follows a Poisson distribution. On the other hand, it also implies that nonparametric term weighting methods tend to approach and stay close to their parametric counterparts in retrieval performance. A nonparametric term weighting model is, in this respect, expected to outperform its parametric counterpart on a document collection in which the frequency of term occurrence imperfectly follows the assumed average, tend to approach and stay close to the parametric models in average retrieval impossible, to single out a parametric model whose assumptions hold perfectly true for all document collections. Change of operational settings, such as the change of document collection or query, would in general invalidate parametric models, and hence cause a reduction in their expected retrieval performance, whereas, in contrast, a nonparametric model is expected to be robust against such changes. 9 Conclusion We present a nonparametric term weighting method based on measuring the divergence from independence -DFI, upon which we build an information theoretic document scoring function. We propose here three basic measures of DFI: (1)  X  X  X FIB X  X  based on the saturated model of independence, (2)  X  X  X FIZ X  X  based on standardization, and (3)  X  X  X FIC X  X  based on the normalized chi-squared distance.

The objective of this study is first to introduce nonparametric term weighting methods, and second to show that the theoretical expectation that nonparametric term weighting methods tend to approach and stay close to parametric methods in average retrieval per-formance over a large set of topics and document collections holds true in practice. The empirical results overall confirms this expectation to be true, at least in the body of the data at hand. In this regard, the DFR model  X  X  X FRee X  X , for example, assumes a hyper-geometric hypothesis that assuming a hyper-geometric distribution, relative to making no assumption formance. The same is true for the DFR model  X  X  X L2 X  X , which assumes a Poisson term distribution.

Here, we also propose a DFI-based measure of term specificity, CTI (contribution of terms to total inertia), as an alternative to the well-known term specificity measure, called the IDF, inverse document frequency, and examine the retrieval performance of the cor-responding DFI 9 CTI weighting scheme in comparison with the DFI 9 IDF weighting scheme, to see (1) whether CTI contributes to the retrieval performance of DFI models, and (2) whether there is a significant difference between CTI and IDF in contribution to the performance of DFI models. The empirical results show that IDF and CTI are indifferent in contribution to retrieval performance of DFI models.

As a result, we conclude that DFI-based term weighting promises a new direction in IR research, as being a simple but powerful baseline alternative to the state-of-the-art para-metric models.
 References
