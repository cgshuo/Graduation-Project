 Image clustering has been a critical preprocessing step for vision tasks, e.g., visual concept discovery, content-based image retrieval. Conventional image clustering methods use handcraft visual descriptors as basic features via K-means, or build the graph within spectral clustering. Recently, rep-resentation learning with deep structure shows appealing performance in unsupervised feature pre-treatment. How-ever, few studies have discussed how to deploy deep repre-sentation learning to image clustering problems, especially the unified framework which integrates both representation learning and ensemble clustering for efficient image cluster-ing still remains void. In addition, even though it is widely recognized that with the increasing number of basic parti-tions, ensemble clustering gets better performance and lower variances, the best number of basic partitions for a given data set is a pending problem. In light of this, we propose the Infinite Ensemble Clustering (IEC), which incorporates the power of deep representation and ensemble clustering in a one-step framework to fuse infinite basic partitions. Gen-erally speaking, a set of basic partitions is firstly generated from the image data. Then by converting the basic parti-tions to the 1-of-K codings, we link the marginalized auto-encoder to the infinite ensemb le clustering with i.i.d. basic partitions, which can be approached by the closed-form so-lutions. Finally we follow the layer-wise training procedure and feed the concatenated deep features to K-means for final clustering. Extensive experiments on diverse vision data sets with different levels of visual descriptors demonstrate both the time efficiency and superior performance of IEC com-pared to the state-of-the-art ensemble clustering and deep clustering methods.
  X 
Information systems  X  Clustering;  X  Theory of com-putation  X  Unsupervised learning and clustering;  X  Computing methodologies  X  Ensemble methods; Ensemble Clustering, Marginalized Denoising Auto-encoder, K-means
Image clustering has been identified as one of the most critical steps for many vision applications, e.g., storyline reconstruction from photo streams [23, 24], automatic vi-sual concept discovery [25], 3D construction from image collections [15], finding iconic images [6, 36] and web-scale fast image clustering [1, 19]. However, most existing im-age clustering methods use either feature engineering based features [11, 33] or discriminant deep features trained by well-established deep Convolutional Neural Network (CNN) model [8, 14]. As a result, they did not intentionally learn the representation along with the clustering, or in other words, build a joint clustering and representation learning framework. It is especially useful for high-performance im-age clustering systems, where the learned representation and clustering strategy need substantial interactions for better performance [31, 32, 28].

Recently, representation learning attracts substantial re-search attention, which has been widely adopted as the un-supervised feature pre-treatment [4]. The layer-wise training and the followed deep structure are able to capture the visual descriptors from coarse to fine [5, 20]. Notably, there are a few deep clustering methods proposed recently, working well with either feature vectors [37] or graph Laplacian [21, 26], towards high-performance generic clustering tasks. There are two typical problems with regard to the deep clustering approaches: (1) how to seamlessly integrate the  X  X eep X  con-cept into the conventional clustering framework, (2) how to solve it efficiently. Few attempts have been made for the first problem [21, 40], however, most of which sacrifice the time efficiency. They follow the conventional training strategy for deep models, whose complexity will be in super-liner with respect to the number of samples. A recent deep linear cod-ing framework attempts to handle the second problem [37], and preliminary results demonstrate its time efficiency with comparable performance on large-scale data sets. However, its performance on vision data has not been thoroughly eval-uated yet, given different visual descriptors and tasks.
In order to obtain a robust clustering result for computer vision applications, ensemble clustering is often used due to its high performance and robustness. Tremendous efforts have been devoted to thrive this area in different perspec-tives [16, 39, 45]. These studies can be roughly generalized into two categories: the methods based on co-association Figure 1: Framework of IEC. We apply marginalized Denoising Auto-Encoder to generate infinite ensem-ble members by adding drop-out noise and fuse them into the consensus one. The figure shows the equiv-alent relationship between IEC and mDAE. matrix or utility function. Although ensemble clustering outperforms the tradition clustering methods, [45] pointed out that different generation strategies for basic partitions have great impacts on the success of ensemble clustering, besides the number of basic partitions determines the ro-bustness [35]. It has been widely recognized that with the increasing number of basic partitions, ensemble clustering achieves better performance and lower variance [45, 35]. However, the best number of basic partitions for a given data sets still remains an open problem. Too few basic partitions cannot exert the capacity of ensemble clustering, while too many basic partitions lead to unnecessary computational re-source waste. Here comes the third problem that (3) can we use the infinite ensemble basic partitions to maximize the capacity of ensemble clustering with a low computational cost?
In this work, we simultaneously manage to tackle the three problems mentioned above, and conduct extensive experi-ments on image data sets with different visual descriptors for demonstration. Our new model links the marginalized denoising auto-encoder to ensemble clustering and leads to a natural integration named  X  X nfinite Ensemble Clustering X  (IEC), which is simple yet effective and efficient. To that end, we first generate a moderate number of basic parti-tions, as the basis for the ensemble clustering. Second, we convert the preliminary clustering results from the basic par-titions to 1-of-K codings, which disentangles dependent fac-tors among data samples. Then the codings are expanded infinitely by considering the empirical expectation over the noisy codings through the marginalized auto-encoders with the drop-out noises. Finally, we run K-means on the learned representations to obtain the final clustering. The frame-work of IEC is demonstrated in Figure 1. The whole process is similar to marginalized Denoising Auto-Encoder (mDAE). Several basic partitions are fed into the deep structure with drop-out noises in order to obtain the expectation of the co-association matrix. It is worthy to note that IEC has roughly linear O ( n ) time complexity, which can be used for large-scale data sets. Extensive results on diverse vision data sets show that our IEC framework works fairly well with different visual descriptors, in terms of time efficiency and clustering performance, and moreover some key impact factors are thoroughly studied as well. Although we focus on image clustering in this work, IEC is general enough to handle clustering problems of other domains.

We highlight our contributions as follows.
Here we introduce the related work in terms of image clus-tering, ensemble clustering and auto-encoder, and highlight the difference between existing methods and ours.
Image clustering has been a fundamental problem for many vision applications, in particular with the popularity of photo sharing websites such as Facebook, Instagram and Twitter. Most of existing works focus on either specific vision prob-lems, e.g., automatic visual concept discovery [25], 3D con-struction from image collections [15], storyline reconstruc-tion from photo streams [23, 24], finding iconic images [6, 36], or on web-scale memory efficient fast clustering [1, 19]. In this work, we develop a generic image clustering method that naturally integrates the X  X eep X  X hought with the ensem-ble clustering. It can be easily deployed to different vision tasks as the basic clustering tool [15, 23, 24, 25], and well balance the performance and running time.

On the other hand, the hashing or binary tricks for memory-efficient web-scale clustering manage to accelerate linear clus-tering methods [1, 19] and achieve comparable performance on large-scale systems. Our method is a competitive comple-ment to those methods in terms of accuracy, as ours usually performs better than those with K-means by a large-margin (See Table 2 and 3). In addition, our method can be eas-ily deployed in a distributed system as the time consuming basic partitions are generated independently, binarized and hashed with further optimizations [18, 44]. However, these discussions are beyond the scope of this paper.

Ensemble clustering, also known as consensus clustering or cluster aggregation, aims to fuse several existing basic partitions into an integrated one. Tremendous efforts have been made to solve ensemble clustering. The existing work can be roughly generalized into two categories according the different levels of similarity. The first category employs the utility function to measure the similarity at the partition-level between the consensus clustering and multiple basic partitions. This kind of methods usually finds the final clustering result by maximizing the utility function value. For instance, [41] proposed a Quadratic Mutual Informa-tion based objective function for consensus clustering, and further extended their work to use the EM algorithm with a finite mixture of multinomial distributions for consensus clustering [42]. Along this line, Wu et al. [45] proposed K-means-based Consensus Clustering (KCC) and transferred the consensus clustering into a K-means clustering problem with different KCC utility functions. In addition, there are some other interesting objective functions for the consensus clustering, such as [27, 34]. The second category summarizes the information of basic partitions into a co-association ma-trix, which counts how many times two instances occur in the same cluster. Therefore, the co-association matrix can be regarded as the similarity matrix based on the instance-level, base on which any graph partition algorithm can be conducted for the final clustering. For example, [16] applied the agglomerative hierarchical clustering, and [39] developed three graph-based algorithms for consensus clustering. Re-cently, Liu et al. [29] proposed a spectral ensemble clustering method to transform it as a weighted K-means problem and built the connection of these two kinds of ensemble clustering methods. Other methods include Relabeling and Voting [2], Locally Adaptive Cluster based methods [13], genetic algo-rithm based methods [48], and still many more. Although much efforts have been made to design effective and efficient algorithms, the number of basic partitions is still an open unsolved problem.
Auto-Encoder (AE) is a building block of deep struc-ture that learns hidden and compressed representations (i.e., codings) from data [3]. The motivation of auto-encoder is to transform inputs into outputs with the least possible amount of deformation. Moreover, stacked Auto-Encoder (SAE) can be developed by inserting multiple hidden lay-ers to auto-encoder, which is one of the most popular deep learning architectures. In reality, learning from the noisy or corrupted data is a challenging problem. Denoising Auto-Encoder (DAE) and stacked DAE learn effective represen-tations by reconstructing input data from artificial corrup-tions [43]. Marginalized Denoising Auto-Encoder (mDAE) approximately marginalizes out the corruptions during train-ing, taking into account infinitely many corrupted copies of training data [9, 10]. Due to the flexibility and impressive learning capability, auto-encoder and its variants have been successfully applied to many scenarios, such as face recogni-tion [22], domain adaptation [10, 12], and image classifica-tion [47].

Most recently, a few auto-encoder based methods have been proposed for graph clustering. [38] augmented the loss function of auto-encoder by incorporating a constraint of the distance between samples and centroids. [21] built a deep embedding network using auto-encoder, and incorporated locality-preserving and group sparsity constraints to the loss function of deep network for clustering-oriented representa-tions. [40] revealed the similarity between auto-encoder and spectral clustering, and presented a GraphEncoder method based on sparse auto-encoder. [37] proposed a deep learning coding approach, which jointly learns feature transforms and discriminative codings for fast graph clustering. However, the connection between auto-encoder and ensemble cluster-ing has not been explored.

In this paper, we aim to build the connection between en-semble clustering and auto-encoder, and apply marginalized Denoising Auto-Encoder to fuse infinite basic partitions for image clustering. Figure 2: Performance distribution of basic parti-tions. Y-axis represents the number of basic parti-tions in each bin, and X-axis is the external measure-ment Normalized Mutual Information (NMI). Here Random Parameter Selection strategy is used to ob-tain 100 sub data sets, on which K-means is con-ducted with the cluster number varying from K to 2 K ,where K is the true cluster number; and finally IEC is employed to fuse these basic partitions into the consensus one.
In this section, we introduce the preliminary knowledge in terms of ensemble clustering and marginalized Denoising Auto-Encoder, and then formulate the research problem.
The goal of ensemble clustering is to find a single par-tition which agrees with existing basic partitions as much as possible. It has been widely recognized that ensemble clustering can help generate robust partitions, find bizarre clusters, handle noise, outliers and sample variations, and integrate solutions from multiple distributed or incomplete sources of data or attributes [39, 41].

Before giving the formulation of ensemble clustering, we demonstrate the power of ensemble clustering on Caltech101 and ORL in Figure 2. As can be seen, the performance of all the basic partitions on Caltech101 locates in a wide range from 0.5 to 0.9 in terms of Normalized Mutual Information (NMI) and most of basic partitions are around 0.7, few of them exceed 0.85; surprisingly, IEC produces a high-quality partition which exceeds the best in all basic partitions. The similar phenomenon also occurs on ORL . This indicates that IEC can learn from the low-quality partitions and take use of the rich diversity of basic ones to provide a high-quality partition. In the following, we introduce how ensemble clus-tering fuses different basic partitions and obtains the final one.
 Given a set of r basic partitions of the data matrix X : H = { H (1) , H (2) ,  X  X  X  , H ( r ) } with the cluster number of H to be K i , the goal is to fuse all the basic partitions into a consensus partition H  X  . Here basic partitions might be gen-erated by the same clustering algorithm with different pa-rameters, or by the same clustering algorithm with different features or even by several diff erent clustering algorithms. Although ensemble clustering can be roughly generalized into two categories, based on co-association matrix or utility function, Liu et al. [29] built a connection between the meth-ods based on co-association matrix and utility functions and pointed out the co-association matrix plays a determinative role in the success of ensemble clustering. Thus, here we focus on the methods based on co-association matrix. Figure 3: Performance of different numbers of basic partitions via KCC on mm and reviews datasets. X-axis is the number of basic partitions. With increas-ing numbers of basic partitions, the performance goes up and the variance becomes narrow.

For the ensemble clustering with co-association matrix, therepresentativemethodssummarize r basic partitions into a co-association matrix as follows: where  X  (  X  ) denotes the Kronecker delta function, which re-turns 1 with two identical input values and returns 0 with different input values. We can regard S as a similarity ma-trix between a pair of instances, which simply counts the co-occurrence number in the same cluster in each basic par-tition. By this means, ensemble clustering problem is rede-fined as a classical graph partition problem, so that based on the co-association matrix S , some clustering rules or loss functions can be derived in order to obtain the final consen-sus clustering.

Next, we introduce the impact of the number of basic partitions by the following theorem.

Theorem 1. (Stableness [35]) For any &gt; 0 ,thereexists amatrix S 0 ,suchthat where ||  X  || 2 F denotes the Frobenius norm.

From the above theorem, we have the conclusion that al-though basic partitions might be greatly different from each other due to different generation strategies, the normalized co-association matrix becomes stable with the increase of the number of basic partitions r . From our previous experimen-tal results [30] in Figure 3, it is easy to observe that with the increasing number of basic partitions, the performance of en-semble clustering goes up and becomes stable. However, the best number of basic partitions for a given data set is difficult to set. Too few basic partitions can not exert the capacity of ensemble clustering, while too many basic partitions lead to unnecessary computational resource waste. Therefore, fus-ing infinite basic partition is addressed in this paper, instead of answering the best number of basic partitions for a given data set. According to Theorem 1, we expect to fuse infinite basic partitions to maximize the capacity of ensemble clus-tering. Since we cannot generate infinite basic partitions, how to obtain a stable co-association matrix S and calcu-late H  X  in an efficient way is highly needed, which is also one of our motivations. In Section 4, we employ mDAE to equivalently obtain the X  X nfinite X  X asic partitions and achieve the expectation of co-association matrix.
Denoising Auto-Encoders (DAEs) have been successfully used to learn new representations for a wide range of ma-chine learning tasks [17, 7]. Usually DAE is implemented as a single-hidden-layer neural network where the input is the corrupted data by certain noises and the output is the clean data. The goal of DAE is to make the output to be as close as possible to the clean data x after learning. Usually a loss function ( x , y ) is employed to measure the reconstruction error as follows. where n is the number of data points, m is the times of corrupted data, x i is the i -th clean data point and x j i -th data point in j -th corruption, and f ( x j i )) = g the output of x j i ,where g and h are the encoder and decoder, respectively.

After getting the one-layer hidden representation z = h ( x ), we can continue to use this strategy by using z as the input to obtain deep representation for feature generation, which is called Stacked Denoising Auto-encoder.

The disadvantage of DAE is to explicitly corrupt x by m times to get multiple x , which enlarges the training sam-ples and increases the computational cost. Recently, Chen et al. [9, 10] proposed the marginalized Denoising Auto-Encoder (mDAE) to overcome this challenge by taking use of the expected average loss as follows,
For a long time, auto-encoder and its variants are regarded as a powerful feature generation tool. Actually it can also be used as an optimization tool. In the following, we will give another interpretation of auto-encoder.
Deep structure and clustering techniques are powerful tools for computer vision and data mining applications. Espe-cially, ensemble clustering attracts a lot of attention due to its appealing performance. However, these two powerful tools are usually used separately. Notice that the perfor-mance of ensemble clustering heavily depends on the basic partitions. As mentioned before, co-association matrix S is the key factor for the ensemble clustering and with the in-crease of basic partitions, the co-association matrix becomes stable. According to Theorem 1, the capability of ensemble clustering goes to the upper bound with the number of basic partitions r  X  X  X  , Then we aim to seamlessly integrate deep concept and ensemble clustering in a one-step framework: Can we fuse infinite basic partitions for ensemble clustering inadeepstructure?
The problem is very straightforward, but it is quite diffi-cult. The challenges of the problem lie in three folds:
Here we first uncover the connection between ensemble clustering and auto-encoder. Next, marginalized Denoising Auto-Encoder is applied for the expectation of co-association matrix, and finally we propose our method and give the cor-responding analysis.
It seems that there exists no explicit relationship between ensemble clustering and auto-encoder due to their respec-tive tasks. The aim of ensemble clustering is to find a clus-ter structure based on basic partitions, while auto-encoder is usually used for better feature generation. However, by taking a close look at the objective function in Eq. 2 and Eq. 3, we find that auto-encoder can be regarded as an op-timization method for minimizing the loss function.
Recalling that the goal of ensemble clustering is to find a single partition which agrees the basic ones as much as possible, we can understand it in the opposite way that the consensus partition has the minimum loss to present all the basic ones. After we summarize all the basic par-titions into the co-association matrix S , spectral clustering or some other graph partition algorithms can be conducted on the co-association matrix to obtain the final consensus result. Taking spectral clustering as an example, we aim to find a n  X  K low-dimensional space to represent the original input. Each column of low-dimensional matrix is a base for spanning the space. Then K-means can be run on that for the final partition. Similarly, the function of auto-encoder is also to learn a hidden representation with d dimensions with  X  X arrying X  as much as possible information with the in-put, where d is a user pre-defined parameter. Therefore, to some extent spectral clustering and auto-encoder have the similar function to learn new representations according to minimizing certain objective function; the difference is that in spectral clustering, the dimension of new representation is K , while auto-encoder produces d dimensions. From this view, auto-encoder is more flexible than spectral clustering.
Therefore, we have another interpretation of auto-encoder, which not only can generate robust features, but also can be regarded as an optimization method for minimizing the loss function. By this means, we can feed the co-association ma-trix into auto-encoder to get the new representation, which has the similar function with spectral clustering, and run K-means on that to obtain the consensus clustering. For the efficiency issue, it is not a good choice to use auto-encoder on the ensemble clustering task due to the large space com-plexity of co-association matrix O ( n 2 ). We will address this issue in the next subsection.
According to Theorem 1, with the number of basic par-titions going to infinity, the co-association matrix becomes stable. Before answering how to generate infinite ensem-ble members, we first solve how to increase the number of basic partitions given the limited ones. The naive way is to apply some generation strategy on the original data to produce more ensemble members. The disadvantages lie in two folds: (1) time consuming, (2) sometimes we only have the basic partitions, and the original data are not accessi-ble. Therefore, without the original data, producing more basic partitions with the limited one is like a clone problem. However, simply duplicating the ensemble members does not Algorithm 1 The algorithm of Infinite Ensemble Clustering Input: H (1) ,  X  X  X  , H ( r ) , : r basic partitions; Output: optimal H  X  ; 1: Build the binary matrix B ; 2: Apply l layers stacked mDAE with p noise level to get 3: Run K-means on BW T to get H  X  . work. Here we make several copies of basic partitions and corrupt them with erasing some labels in basic partitions to get new ones. By this means, we have extra incomplete ba-sic partitions and Theorem 1 also holds for incomplete basic partitions.

By this strategy, we just amply the size of ensemble mem-bers, which is still far from the infinity. To solve this chal-lenge we use the expectation of co-association matrix in-stead. Actually, S 0 is just the expectation of S ,whichmeans if we obtain the expectation of co-association matrix as an input for auto-encoder, our goal can be achieved. Since the expectation of co-association matrix cannot be obtained in advance, we intend to calculate it during the optimization.
Inspired by the marginalized Denoising Auto-Encoder [10], which involves the expectation of certain noises during the training, we corrupt the basic partitions and marginalize it for the expectation. By adding drop-out noise to basic partitions, some elements are set to be zero, which means some instances are not involved during the basic partition generation. By this means, we can use marginalized Denois-ing Auto-Encoder to finish the infinite ensemble clustering task. If we take a look at Eq. 3, the function f can be lin-ear or non-linear. In this paper, for efficiency issue we use the linear version for mDAE [10] since it has a close-form formulation.
So far, we solve the infinite ensemble clustering problem with marginalized Denoising Auto-Encoder. Before con-ducting experiments, we notice that the input of mDAE should be the instances with independent and identically distribution; however, the co-association matrix can be re-garded as a graph, which disobeys this assumption. To solve this problem, we introduce a binary matrix B .

Let B = { b ( x ) } be a binary data set derived from the set of r basic partitions H as follows: b ( x )= b ( x ) 1 ,  X  X  X  ,b ( x ) r ,b ( x ) i = b ( x ) i
We can see that the binary matrix B is a n  X  d matrix, where d equals r i =1 K i . It concatenates all the basic parti-tions with 1-of-K i coding, where K i is the cluster number in the basic partition H ( i ) . With the binary matrix B ,wehave BB T = S . It indicates that the binary matrix B has the same information with the co-association matrix S .Since B obeys the independent and identically distribution, we can put the binary matrix as input for marginalized Denoising Auto-Encoder.
The algorithm is summarized in Algorithm. 1. The first line is to build the binary matrix B . Then we add the con-stant 1 at the last column of B and corrupt it with p level drop-out noise. Let q =[1  X  p,  X  X  X  , 1  X  p, 1]  X  R d +1 and  X  = B T B . The corresponding mapping for W between input and hidden representations is in closed form as: where E [ P ] ij =  X  ij q j and E [ Q ] ij =  X  ij q i  X  ( i, j, q  X  ( i, j, q j ) returns 1 with i = j , and returns q j with i = j . After getting the mapping matrix, BW T is used as the new representation. By this means, we can recursively apply marginalized Denoising Auto-Encoder to obtain deep hidden representations. Finally, K-means is called to run on the hidden representations for the consensus partition. Since only r elements are non-zeros in each row of B ,itisvery efficient to calculate  X  .Moreover, E [ P ]and E [ Q ]areboth ( d +1)  X  ( d + 1) matrixes. Finally, K-means is conducted on all the hidden representations. Therefore, our total time complexity is O ( ld 3 + IKnld ), where l is the number of layers of mDAE, I is the iteration number in K-means, K is the cluster number, and d = r i =1 K i n . This indicates our algorithm is linear to n , which can be applied for large-scale clustering. Since K-means is the core technique in IEC, the convergence is guaranteed.
In this section, we first introduce the experimental set-tings, then showcase the effectiveness and efficiency of IEC compared with the state-of-the-art deep clustering and en-semble clustering methods. Finally, some impact factors of IEC are thoroughly explored.
Data Sets. 13 real-world image data sets with true clus-ter labels are used for experiments. Table 1 shows their important characteristics, where #MinClass, #MaxClass, CV and Density denote the instance number of the smallest and biggest clusters, Coefficient of Variation statistic that characterizes the degree of class imbalance, and the ratio of non-zeros elements, respectively. In order to demonstrate the effectiveness of our IEC, we select the data sets with dif-ferent levels of features, such as pixel, Surf and deep learning features. The first two are characters and digits data sets the middle ones are the objects and digits data sets 23 and the last four data sets are with the deep learning features In addition, these data sets contain different types of im-ages, such as digits, characters, objects. Figure 4 shows some samples of these data sets.

Comparative algorithms. To validate the effectiveness of the IEC, we compare it with several state-of-the-art meth-ods in terms of deep clustering methods and ensemble clus-tering methods. In the ensemble clustering framework, we employ Random Parameter Selection (RPS) strategy to generate basic par-titions. Generally speaking, k-means is conducted on all features with different numbers of clusters, varying from K to 2 K . To show the best performance of the comparative algorithms, 100 basic partitions via RPS are produced for boosting the comparative methods. Note that we set 5 layers in deep clustering methods and for all clustering methods, we set K to be the true cluster number for fair comparison.
Validation metric. Since the label information is avail-able to these data sets, here we use two external metrics ac-curacy and Normalized Mutual Information (NMI) to mea-sure the performance. Note that accuracy and NMI are both positive measurements, which means the larger, the better. The computation details can be found in [46].

Environment. All the experiments were run on a Win-dows standard platform of 64-bit edition,which has two Intel Core i7 3.4GHz CPUs and 32GB RAM.
Table 2 and 3 show the clustering performance of different algorithms in terms of accuracy and NMI. The best results are highlighted in bold font.  X  X /A X  denotes there is no re-sult due to out of memory. As can be seen from the tables, Figure 5: Running time with different layers and instances. three observations are very clear. (1) In the deep clustering method, MAEC1 performs the best and the worst on Ama-zon and COIL100 , respectively; on the contrary, MAEC2 gets reasonable result on COIL100 but low quality on Ama-zon . Although we try our best to tune the number of neurons in the hidden layers, GEncoder suffers from the worst per-formance in all the comparative methods, even worse than K-means. The high computational cost prohibits MAEC2 and GEncoder from handling large-scale data sets. Since clustering belongs to the unsupervised learning, only re-Table 4: Execution time of different ensemble clus-tering methods by second lying on deep structure makes little effect to improve the performance. Instead DLC jointly learns the feature trans-form function and discriminative codings in a deep structure, which has the satisfactory results. (2) In most cases, ensem-ble clustering is superior to the baseline method, even better than deep clustering methods. The improvement is obvious when applying ensemble clustering methods on the data sets with high-level features, since high-level features have more structural information. However, ensemble methods do not work well on SUN09 . One of the reasons might be the un-balanced class structure, which prevents the basic clustering algorithm K-means from uncovering the true structure and further harms the performance of ensemble methods. (3) Our method IEC gets the best results on most of 13 data sets. It is worthy to note that the improvements are over nearly 8%, 8% or 22% on Dslr , USPS and Caltech101 ,re-spectively, which are rare in clustering field. Usually the per-formance of ensemble clustering goes up with the increase the number of basic partitions. In order to show the best performance of the comparative ensemble clustering meth-ods, we use 100 basic partitions. Here we can see that there still exists large space to improve via infinite ensemble mem-bers.

For efficiency, to make fair comparisons here we only re-port the execution time of ensemble clustering methods. Al-though additional time is needed for generating basic parti-tions, k-means and parallel computation make it quite effi-cient. Table 4 shows the average time of ten runs via these methods. GCC runs three methods on small data sets but runs two methods on large data sets, and HCC runs fast on data sets containing few instances but struggles as the num-ber of instances increases due to its O ( n 3 ) time complexity. KCC, SEC and IEC are all K-means-based methods, which are much faster than other ensemble methods. Since our method only applies mDAE on basic partitions which has the closed-form solution and then runs K-means on the new representations, therefore IEC is suitable for large-scale im-age clustering. Moreover, Figure 5 shows the running time on MNIST and letter with different number of layers and instances. We can see that the running time is linear to the layer number and instant number, which verifies the high efficiency of IEC. Therefore, if we only use one layer in IEC, the execution time is similar to KCC and SEC.
Next we thoroughly explore the impact factors of IEC in terms of the number of layers, the generation strategy Figure 6: Performance of IEC with different layers. Figure 7: Impact of basic partition generation strategies. of basic partitions, the number of basic partitions, and the noise level, respectively.

Number of layers. Since stacked marginalized Denois-ing Auto-Encoder is used to fuse infinite ensemble members, here we explore the impact of the number of layers. As can be seen in Figure 6, the performance of IEC goes slightly up with the increase of layers. Except that the second layer has large improvements over the first layer on Caltech101 , IEC demonstrates the stable results on different layers, be-cause only one-layer marginalized Denoising Auto-Encoder calculates the expectation of co-association matrix. Usually the deep representation is successful in many applications on computer vision, here the default value of the number of layers is set to be 5.

Generation strategy of basic partitions. So far we rely solely on Random Parameter Selection (RPS) to gen-erate basic partitions, with the number of clusters varying in [ K, 2 K ]. In the following, we demonstrate whether the generation strategy will impact the performance of IEC.
Here Random Feature Selection (RFS) is proposed as a comparison, which still uses k-means as the basic cluster-ing algorithm with random selecting 50% original features to obtain 100 basic partitions. Figure 7 demonstrates the performance of KCC and IEC via RPS and RFS on 5 data sets. As we can see that, IEC exceeds KCC in most cases of RPS and RFS. When we take a close look, the performance of IEC via RPS and RFS is almost the same, while KCC produces large gaps between RPS and RFS on Caltech101 and Sun09 (See the ellipses). This indicates that although the generation of basic partitions is of high importance to the success of ensemble clustering, we can take use of infinite ensemble clustering to alleviate the impact.

Number of basic partitions. The key problem of this paper is to use limited basic partitions to achieve the goal of fusing infinite ensemble members. Here we discuss the impact of the number of basic partitions to ensemble clus-tering. Figure 8 shows the performance of 4 ensemble clus-tering methods on USPS . Generally speaking, the perfor-mance of HCC, KCC and GCC goes up with the increase Figure 8: Impact of the number of basic partitions via different ensemble methods on USPS . Figure 9: Performance of IEC with different noise levels. of the number of basic partitions and becomes stable when enough basic partitions are given, which is consistent with Theorem 1. It is worthy to note that IEC enjoys the high performance even with 5 ensemble members. It is worthy to note that for large-scale data sets, generating basic par-tition suffers from high time complexity even with ensemble process. Thus, it is appealing that IEC uses limited basic partitions and achieves the high performance, which is suit-able for tons of image clustering.

Noise level. The core idea of this paper is to obtain the expectation of co-association matrix via adding the drop-out noise. Figure 9 shows the results of IEC with different noise level on four data sets. As can be seen that, the performance of IEC is quite stable even to 0.5 noise level. Note that if we set the noise level to zero, IEC will equivalently degrade into KCC.
In this paper, we proposed a novel ensemble clustering al-gorithm Infinite Ensemble Clustering (IEC) for image clus-tering. Generally speaking, we built a connection between ensemble clustering and auto-encoder, and applied marginal-ized Denoising Auto-Encoder to fuse infinite incomplete ba-sic partitions. Extensive experiments on 13 data sets with different levels of features demonstrated our method IEC had promising performance over the state-of-the-art deep clustering and ensemble clusterings methods; besides, we thoroughly explored the impact factors of IEC in terms of the number of layers, the generation strategy of basic parti-tions, the number of basic partitions, and the noise level to show the robustness of our method.
This research is supported in part by the NSF CNS award 1314484, ONR award N00014-12-1-1028, ONR Young Inves-tigator Award N00014-14-1-0484, and U.S. Army Research Office Young Investigator Award W911NF-14-1-0218. We thank KDD anonymous reviewers and SPC for their con-structive comments, which help to improve this work to a new level in the final revision. [1] Y. Avrithis, Y. Kalantidis, E. Anagnostopoulos, and [2] H. Ayad and M. Kamel. Cumulative voting consensus [3] Y. Bengio. Learning deep architectures for ai. [4] Y. Bengio, A. Courville, and P. Vincent.
 [5] Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle, [6] T. Berg and A. C. Berg. Finding iconic images. In [7] M. Carreira-Perpin  X  A an and R. Raziperchikolaei. [8] K. Chatfield, K. Simonyan, A. Vedaldi, and [9] M. Chen, K. Weinberger, F. Sha, and Y. Bengio. [10] M. Chen, Z. Xu, K. Weinberger, and F. Sha.
 [11] N. Dalal and B. Triggs. Histograms of oriented [12] Z. Ding, M. Shao, and Y. Fu. Deep low-rank coding [13] C. Domeniconi and M. Al-Razgan. Weighted cluster [14] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, [15] J.-M. Frahm, P. Fite-Georgel, D. Gallup, T. Johnson, [16] A. Fred and A. Jain. Combining multiple clusterings [17] M. Ghifary, W. Kleijn, M. Zhang, and D. Balduzzi. [18] Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin. [19] Y. Gong, M. Pawlowski, F. Yang, L. Brandy, [20] G.E.Hinton,S.Osindero,andY.-W.Teh.Afast [21] P. Huang, Y. Huang, W. Wang, and L. Wang. Deep [22] M. Kan, S. Shan, H. Chang, and X. Chen. Stacked [23] G. Kim, L. Sigal, and E. P. Xing. Joint summarization [24] G. Kim and E. P. Xing. Reconstructing storyline [25] Y. J. Lee and K. Grauman. Shape discovery from [26] S. Li, Y. Jiang, and Z. Zhou. Partial multi-view [27] T. Li, D. Chris, and M. Jordan. Solving consensus and [28] H. Liu and Y. Fu. Clustering with partition level side [29] H. Liu, T. Liu, J. Wu, D. Tao, and Y. Fu. Spectral [30] H. Liu, J. Wu, D. Tao, Y. Zhang, and Y. Fu. Dias: A [31] T. Liu and D. Tao. On the performance of manhattan [32] T. Liu, D. Tao, and D. Xu. Dimensionality-dependent [33] D. G. Lowe. Distinctive image features from [34] Z. Lu, Y. Peng, and J. Xiao. From comparing [35] D. Luo, C. Ding, H. Huang, and F. Nie. Consensus [36] R. Raguram and S. Lazebnik. Computing iconic [37] M. Shao, S. Li, Z. Ding, and Y. Fu. Deep linear coding [38] C. Song, F. Liu, Y. Huang, L. Wang, and T. Tan. [39] A. Strehl and J. Ghosh. Cluster ensembles  X  a [40] F. Tian, B. Gao, Q. Cui, E. Chen, and T. Liu. [41] A. Topchy, A. Jain, and W. Punch. Combining [42] A. Topchy, A. Jain, and W. Punch. A mixture model [43] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. [44] Y. Weiss, A. Torralba, and R. Fergus. Spectral [45] J. Wu, H. Liu, H. Xiong, J. Cao, and J. Chen. [46] J. Wu, H. Xiong, and J. Chen. Adapting the right [47] G.-S. Xie, X.-Y. Zhang, and C.-L. Liu. Efficient [48] H. Yoon, S. Ahn, S. Lee, S. Cho, and J. Kim.
