 Shie Mannor shie@mit.edu Duncan Simester simester@MIT.EDU Peng Sun psun@duke.edu Fuqua School of Business, Duke University, Durham, NC 27708 John N. Tsitsiklis jnt@mit.edu Keywords: A common method when analyzing data obtained from a Markov Process (MP) is to estimate the transition probabilities and the reward function based on an em-pirical sample. The cost-to-go (or the profit-to-go in a maximization problem) is then estimated by plug-ging the empirical model instead of the true transition probability and the reward function. A fundamental question regarding such an estimate concerns its bias and variance. Knowledge of the bias and variance is essential for evaluating the quality of the cost-to-go es-timate, as well as for determining the amount of data required in order to achieve a certain confidence level. Surprisingly, little attention was given to date to the bias and variance of the cost-to-go estimate which is the topic of this paper.
 From a statistical point of view, an informative estima-tor should be accompanied by confidence bounds. The bias and the variance of an estimator naturally lead to such confidence bounds (e.g., using a Chebychev bound). A useful bias and variance estimate should be expressed as a function of the available statistics (counts of state transitions and statistics of the re-ward). We will show that when assuming that the empirical model is reasonable (i.e., not  X  X oo far X  from the real model) such bias and variance estimates can be developed based on a second order approximation. A common framework for decision making under un-certainty used in decision theory and machine learn-ing is the Markov Decision Process (MDP) framework (e.g., Puterman, 1994). In this framework there are several possible actions in every state and a decision maker is required to choose the best action. In the con-text of estimating the value of an MDP from data, it is common to estimate the conditional transition proba-bilities (conditioned on the current state and the deci-sion maker X  X  action) and the conditional reward func-tion. Once the empirical model is estimated the best policy is computed by optimizing over the policy space. The value of each state in the MDP is the cumulative expected reward obtained from that state on, if the de-cision maker follows the optimal policy. The results in this paper are developed for Markov Processes (MPs), but are valid for Markov Decision Processes (MDPs) as long as the policy is fixed.
 The paper is organized as follows. We start with de-scribing the model in rigor and defining the problem of interest in Sec. 2. In Sec. 3 we illustrate the magni-tude of the variance in both an artificial MP and in real data obtained from a mail-order catalog firm. We sug-gest two different approaches for estimation of the bias and variance in MPs. The first approach is a  X  X lassi-cal X  statistical (frequentist) approach, and the second approach is Bayesian. We provide the essential details in Sec. 4. We demonstrate the variance estimates for both types of data in Sec. 5. Some concluding remarks are given in Sec. 6. In this section we specify the problem of interest. We start with defining the problem setup. We then point to two types of variances, one that is related to uncer-tainty in the parameters, and another which is inherent to the stochastic nature of the problem. 2.1. Problem Setup We consider both MPs and MDPs. Let us define the latter, as the former is a special case. An MDP is a 4-tuple ( S, A, P, R ), where S is a set of the states, A is a set of actions, P a ij is the transition probability from state i to state j when performing action a  X  A in state i , and R ia is the reward received when per-forming action a in state i . We assume that S and A are finite sets and that R ia is a random variable. We further let | S | = m . At time t , the current state is s , the decision maker chooses some action a t . As a result of this action the next state s t +1 is determined and the decision maker obtains a reward r t which is distributed according to R s attention to discounted reward. The discount factor will be denoted by  X  , where it is assumed that  X  &lt; 1. A strategy for an MDP assigns, at each time t , for each state i a probability for performing action a  X  A , given a history which includes the states, actions and rewards observed until time t  X  1 and the state in time t . A strategy is stationary if it only depends on the current state. It is well known that there exists an optimal stationary strategy for discounted reward. An MP can be considered as a special case of an MDP, where a stationary strategy is fixed by the decision maker. We will denote such a strategy by  X  . The expectation operator under strategy  X  starting from state i will be denoted by IE  X  i .
 We will consider a nominal (empirical) model of the MDP. This model is typically the result of interacting with the environment. We denote the nominal transi-tion probability by  X  P , and the nominal reward function by  X  R . Typically, the sampling procedure provides ad-ditional statistics, such as the variance of the reward and the counts of the transitions. We now distinguish between two types of variances. 2.2. Two Types of Variance There are two different types of variance which are of interest in learning and planning. Let  X  be a specific stationary strategy such that  X  ( a | i ) is the conditional probability of choosing action a in state i . 1. Internal variance -consider the random variable 2. Parametric variance -Suppose that there is some Each type of variance is related to a different type of uncertainty and can be associated with a different experiment. The internal variance is the variance of the cumulative discounted cost-to-go in an experiment where the decision maker starts many times from a certain state i and follows a policy  X  , and the model is assumed to be perfectly known. The parametric variance is the variance of the empirical cost-to-go es-timate when one obtains nominal models many times. In this paper we only consider the parametric variance. We will consider artificial MPs that are randomly gen-erated. Those MPs have m = 10 states, and a ran-domly generated transition probability according to the following rule: we sample m random numbers from a uniform distribution on [0 , 1]. We take the two largest numbers and normalize them to sum to 1 / 2, we take the rest m  X  2 numbers and normalize them to sum to 1 / 2 as well. As a result we have a transition probability that sums to one and has two states that contain 50% of the mass. The reward R ia is a Nor-mal random variable whose mean and variance were sampled from N (0 , 1) and U [0 , 1 / 4], respectively. To demonstrate the effect of variance we run the follow-ing experiment. We constructed a random MP using the procedure just described. We sampled this MP n times. We calculated the cost-to-go of each empirical model, and weighted the different states according to the steady-state distribution (of the true model), so that we considered c &gt; Y  X  where c is the steady state vector. The reason for weighing the cost-to-go is that the vector Y  X  is m dimensional and we want to con-sider just a one dimensional summary. We set the dis-count factor to  X  = 0 . 9. Fig. 1 presents the empirical standard deviation of the weighted value function for ten randomly generated MPs as a function of number of times each state was sampled. In order to calculate the standard deviation we sampled each MP n = 50 times. The weighted cost-to-go was in the range [  X  3 , 3] for all MPs. It is clear from Fig. 1 that the variance in the cost-to-go estimate is significant. As expected, this variance decreases as the number of samples per state increases. In Fig. 2 we focus on a single MP that was generated in the same way. The weighted cost-to-go of the true model was c &gt; Y  X  = 1 . 78, and each state i was sampled N i = 200 times, we repeated the experiment n = 1000 times. The histogram of c &gt;  X  Y (where  X  Y is the empirical estimate of Y ) shows that the deviation in the cost-to-go is significant.
 We were fortunate to have access to a large transac-tional database of a mail-order catalog firm. Every time a new catalog is produced, the mail-order catalog firm has to make a decision X  X o mail or not to mail to each customer. The cost of producing and mailing the catalogs is not negligible, and the firm looks for a strat-egy that maximizes its expected revenue. The firm logs all the purchasing and mailing history for every customer, and can therefore make informed decisions. The database we used includes about 1.72 million cus-tomers and more than 160 million transactions. Fol-lowing G  X on  X ul and Shi (1998), the decision problem (for every customer) can be modelled as an MDP, where the state is a summary of the customer X  X  history, and the action at each time epoch is either to mail or not to mail. The construction of the state space is an interest-ing problem which we will not consider here. We have used the so-called RFM (Recency, Frequency, Mone-tary value) scales which is common in the mail-order catalog industry (e.g., Bult &amp; Wansbeek, 1995; Bitran &amp; Mondschein, 1996). In the RFM parametrization, the history of each customer is summarized by three scales: the recency of the last purchase, the frequency of purchases, and the average monetary value. We constructed an MDP model from the data by quan-tizing each of the RFM scales to 4 discrete levels, so that the state space has m = 4 3 = 64 states. Since there are many customers, the internal variance is av-eraged out, so the firm only cares about the parametric variance. Estimation of the parametric variance of the current policy is extremely important for the firm, so it can have confidence in projected revenues. In addi-tion, the firm is interested in estimating the variance of new mailing policies, which might promise higher profit at the expense of larger variance.
 In Tab. 1 we present the empirical standard deviation of the profit-to-go for the real data, weighted uniformly over the states, for the policy used by the firm. The empirical standard deviation was calculated by divid-ing the data to roughly equal segments (since the his-tory of every customer is integral, we did not split a customer between segments). The empirical profit-to-go of the whole data is $5.88. It can be seen that the standard deviation is rather significant, and accounts for as much as 5% of the profit-to-go for as many as 1.5 million transactions.
Number of transactions STD of value Relative per segment (millions) error Suppose that we have access to the counts that gen-erate  X  P and to the statistics of  X  R (i.e., its empirical variance and mean). Given those statistics we consider two approaches for estimating the parametric bias and variance of the cost-to-go. In this section we assume that a fixed stationary policy  X  is used so we therefore drop the superscript  X  . 1. The  X  X lassical X  (or frequentist) approach. Ac-2. The Bayesian approach. Since (  X  P,  X  R ) are given, 4.1. The classical approach In the classical approach we assume the existence of true P and R that generate data (a collection of sam-ple trajectories). Using these data we generate the nominal model, i.e.,  X  P and  X  R . Both,  X  P and  X  R , are random variables that depend on the true P and R . We will provide expressions for the bias and variance of the cost-to-go estimate if P and R were known, and later suggest to replace P with  X  P and R with  X  R in those estimates.
 We assume that the number of transitions out of each state, N i , is given. The number of transitions from state i to all states j , N ij  X  X , thus follows a multinomial distribution. We use the estimate  X  P ij = N ij N have that IE[  X  P ] = P . We use zero mean random vari-ables  X  P and  X  R to represent the difference between the true model and nominal model, i.e.,  X  P = P +  X  P and  X  R = R +  X  R . Note that the random variables  X  P and  X  R may be correlated. We therefore write the expectation of  X  Y := ( I  X   X   X  P )  X  1  X  R as: We use notation X 4 = ( I  X   X P )  X  1 and let f k ( X  P ) 4 X ( X  PX ) k = ( X  X  P ) k X . The following technical lemma will be useful: Lemma 4.1 Proof. where we repeatedly used the definition of X and the fact that X is invertible. 2 Substituting Lemma 4.1 in Eq. (2), and separating the first term in the sum ( k = 0) from the rest of the terms, we obtain: IE[  X  Y ] = ( I  X   X P )  X  1 R + There are three terms in Eq. (3). The first term is the cost-to-go of the true model. The second term reflects the uncertainty in P and the third term represents the correlation in errors between the estimates of R and P . The immediate implication of Eq. (2) is that using the nominal model induces bias.
 The estimation of IE[ f k ( X  P )] involves k th order mo-ments of multinomial distributions. This can be conducted however is rather tedious. We will con-sider a second order approximation and assume that IE[ f k ( X  P )]  X  0 for k &gt; 2. As a justification, no-tice that as long as k  X  P k &lt; (1  X   X  ) / X  (where k  X  k is any matrix norm) we have that q :=  X  k  X  P k  X  k k  X  P k k ponentially with increase of k .
 In many cases, the correlation between  X  P and  X  R can be modelled as the result having a true model whose rewards come from an m  X  m matrix R m while R is just the aggregated values from R m in the following way  X  R Here  X  R m ij  X  X  and  X  P are independent. Un-der this modelling assumption, we have  X  R i = P notations:  X  R = ( R m  X   X  P +  X  R m  X  P +  X  R m  X   X  P ) e, (5) where  X  is the Hadamard multiplication, e is an m  X  1 vector of ones, and  X  R m is the m  X  m difference matrix between the true R m and the empirical estimate  X  R m . Let Q be the m  X  m matrix satisfying: When  X  P is  X  X mall X  one can use a second order ap-proximation and estimate the bias and variance of the cost-to-go. The following proposition prescribes the bias. The proof of the proposition is technical and lengthy. This proof and other proofs are deferred to the full version of this paper.
 Proposition 4.1 Assume that  X  P and  X  R are cor-related according to Eq. (5). Then the expectation of  X  Y satisfies: where X := ( I  X   X P )  X  1 , Y = ( I  X   X P )  X  1 R is the true cost-to-go, and Q is computed according to Eq. (6). Since we can calculate IE[  X  Y ], it suffices to calculate IE[  X  following proposition provides this estimation. Proposition 4.2 Using the same notations and un-der the same assumptions of Prop. 4.1, the second mo-ment of  X  Y is approximately IE[  X   X  trices such that Q Q Q Q Note that as the N i  X  X  increase to  X  all the terms in-volving Q decrease to 0, so that both the bias and the variance decrease to 0. The true model ( P and R ) is used in the above estimates. According to the classical approach we plug in  X  P and  X  R in place of P and R (and the empirical variance of R ik instead of var( R ik ) for are roughly of the same order of magnitude. Since both are typically a number much smaller than 1, this im-plies that the standard deviation (which is the square root of the variance) will be typically much larger than the bias. The conclusion is that de-biasing the cost-to-go estimate is not useful since the noise caused by the variance is typically more significant. 4.2. The Bayesian Approach In this section we describe a Bayesian approach. As before, we assume that the data is the number of tran-sitions out of each state N i , the number of transitions from state i to all states j , N ij  X  X . We also observe the sample of the rewards obtained when moving between the states. We assume that for every pair of states i, j the reward moving from state i to state j , R ij is a ran-dom variable with a Normal prior. We further assume that the probability P is a random variable with a Dirichlet prior (as in Strens, 2000). See Dearden et al. (1998) for a somewhat different Bayesian formulation in the context of Q-learning. An additional assump-tion is that the priors of P and R is not correlated between states.
 We first recall the following properties of a Dirich-let distribution with parameters  X  1 , . . . ,  X  m (here we define  X  0 := man et al. (1995) for further details. For a vector (1 /Z (  X  )) stant. Some useful properties of the Dirichlet distri-bution are: 1. Mean of the k th component:  X  k / X  0 . 2. Variance of the k th component: var( P k ) = 3. Covariance between the k th and ` th components: Assume that P i  X  , the prior transition probability distri-bution out of state i is Dirichlet with initial parameters , . . . ,  X  i m . After observing sample trajectories, sum-marized by N ij transitions out of state i to state j and N It then follows that the posterior distribution for P i terior. This motivates us to define the nominal model, which is also an unbiased estimator for the posterior of P , to be  X  P ij = (  X  i j + N ij ) / (  X  i 0 + N i ). The difference between the nominal and the true model is then a zero mean random matrix  X  P := P  X   X  P . The following lemma is a result of the useful facts regarding the properties of the Dirichlet distribution. Lemma 4.2 Under the assumption of a Dirichlet prior we have that: We note that if  X  i j = 0 (for j = 0 , . . . , m ) then we get the same estimates as in the classical approach (up to the +1 in the denominator of the variance and the covariance).
 Similarly, we define the prior distribution for R m . No-tice that R m could be drawn from any family of dis-tributions that has a close form Bayesian updates. As a special case, here we assume the prior distribution for R m is Normal with parameters  X  ij ,  X  ij and denote the sample variance by s ij .
 If for each component R m ij we observe a series of N ij observations  X  x ij 1 , . . . ,  X  x ij N for R m ij is also Normal with expectation:  X  post ij =  X   X  variance:  X  post ij = 1 / man et al., 1995. So we define the nominal model  X  R m to be the m  X  m matrix whose ij -th entry is  X   X  R = ( R m  X   X  P +  X  R m  X  P +  X  R m  X   X  P ) e . Using a second order approximation and following sim-ilar derivation as in Section 4.1, we have the following results.
 Proposition 4.3 Assume that random matrices  X  R m and  X  P are independent, the expectation (w.r.t. the posterior) of Y := ( I  X   X P )  X  1 R satisfies: according to where cov ( P ik , P ij ) and var ( P ij ) are computed accord-ing to Lemma 4.2.
 Proposition 4.4 Using the same notations and un-der the same assumptions of Prop. 4.3, the second mo-ment of Y := ( I  X   X P )  X  1 R is approximately IE ) +  X  , where  X  X := ( I  X   X   X  P )  X  1 ,  X  Y :=  X  X  X  R , and  X  Q (1) ,  X   X  Q  X  Q  X  Q  X  Q and  X  Q is calculated according to Eq. (7). To validate the variance estimation, and to show that we can compute confidence bounds with reasonable accuracy, we performed the following experiment. We generated random MPs as in Sec. 3. For each such ran-dom MP we sampled the process 1000 times. We then compared the difference between the empirical cost-to-go and the true cost-to-go (weighted by the steady-state frequency of the true model) and divided the difference by the estimated standard deviation (based on the empirical sample and the classical approach). In Fig. 3 we show the percentage of experiments that were within 1 standard deviation (marked by  X + X ) and within 2 standard deviations (marked by  X  X  X ) from the true weighted cost-to-go, as a function of the number of samples per state. Under a Gaussian distribution assumption these percentages would ideally equal 68% and 95%, respectively. It can be seen that the variance estimation is rather accurate.
 We performed a similar experiment using the mail-order catalog data. We divided the data to 1000 groups. We then randomly chose different segments, and based on the empirical model for each segment estimated the variance of a certain fixed policy (the policy used by the firm). We then compared the differ-ence between the empirical profit-to-go of the segment and the profit-to-go of the model that uses all of the data (weighted equally across all the 64 states) and normalized by one standard deviation using the vari-ance estimate (based on the classical approach). In Fig. 4 we show the percentage of customer segments that were within 1 standard deviation (marked by  X + X ) and within 2 standard deviations (marked by  X  X  X ) from the profit-to-go as calculated based on all the data, as a function of the number of transactions per seg-ment. In Fig. 5 we present a histogram of the normal-ized difference between the estimated profit-to-go and the profit-to-go based on all of the data, across dif-ferent partitions of the data. The difference appears to be Gaussian with high confidence (as validated by a Kolmogorov-Smirnof test). The experiments pre-sented validate the accuracy of the variance estimate. We note that the variance estimate appears less tight for the mail-catalog data than for the simulated MPs. We attribute this lack of tightness to the fact the data was not sampled from a  X  X eal X  Markov process. In this paper we provided explicit expressions for the bias and variance of the cost-to-go in MPs using both classical and Bayesian approaches. We assumed that we have access to a rather accurate estimate of the model, which allowed us to use a second order approx-imation. It is not clear how to go about estimating the variance when a second order approximation is not valid. A natural question is how to assess the validity of the second order approximation given data that are suspected to be generated from a Markov process. In this paper we did not address the maximization problem encountered in MDPs, where an additional maximization over the space of policies is performed. This maximization may introduce an additional bias to the value function estimation, since actions that are not sampled enough may appear better than they really are. Estimating this bias and accounting for it as part of the optimization process are important research questions.
 The statistical setup of this paper assumes that the sample trajectories are provided. A learning setup, where an agent can actively sample trajectories, is a natural extension. In such a setup, a learning agent may guide the exploration to minimize the parametric variance. We note that in model-based reinforcement learning (e.g., Kearns &amp; Singh, 2002) one typically as-sumes that the current estimation of the model is ac-curate enough to allow accurate policy evaluations. By estimating the parametric variance we may, perhaps, allow the agent to focus on sampling critical areas of the state space where the variance can be reduced sig-nificantly.
 Acknowledgements We are grateful for the helpful suggestions made by three anonymous reviewers. This work was partially supported by the National Science Foundation under grant 0322823.

