 Almost every representation of music used in the field of Music Information Retrieval (MIR) involves extracting features from music transformed into the frequency domain. These features include chromatic, melodic, harmonic, rhythmic, and timbral measures.
Thus, [1] represents the distribution of chroma within a song as a histogram. The songs with similar chroma histogram distributions are considered similar. The temporal aspects of pitch are taken into account by [2] and [3]. In [4] authors try to capture the rhythm by constructing a self-similarity ma trix based upon the similarity of each short time frequency spectra extracted from the audio. Bello [5] presents a method to describe a novelty function (a common method for identifying onsets) of a waveform inspired by Foote X  X  [4] similarity measure. One of the challenges in MIR is how to interpret the classification confusions. Some incorrectly classified instances cast doubt on whether the ground truth is correct (for example a pop song that could be labeled as funk). The solution to this problem might be the incorporation of fuzzy logic.

Our approach implements methods from both visual IR (VIR) and MIR research areas. The motivation behind this work was the hypothesis, that 2D images of music tracs (spectrograms) perceived as similar would correspond to the same music genres (perhaps even similar music tracs). Conver sely, we can treat real life images as spec-trograms and utilize music-based features to represent these images in a vector form. This would point to an interesting interchangeability between visual and music infor-mation retrieval. Thus, instead of extracting features directly from frequency domain, we generate an image of each song that shows h ow the spectral density of a signal varies with time. Two geometric dimensions represent frequency and time, and the colour of each point in the image indicate the amplitude of a particular frequency at a particular time. The advantage of such visual representation is that it does not rely on musical concepts (melody, timbre, rhythm, etc.). It is also intuitive that two songs can be com-pared visually based on their spectrogram representations. The next step involves the application of local features from Content B ased Image Retrieval and representation of music in the form of histograms of visual words counts. Thus obtained histograms, characterizing individual songs, are used for genre music classification task.
Because global approaches find it hard to capture all the properties of an image, the implemented local features are based on th e  X  X ag of visual words X  approach. The first step in the  X  X ag of features X  1 method is to localize the points of interest (point-like, region-like) by using corner or blob detectors. Other sampling techniques include ran-dom and dense sampling. The second step involves the representation of regions around the sample points in a form of multidimensional vectors. There are various existing de-scriptors, the SIFT (Scale Invariant Feature Transform) being one of the most widely used. The initial extraction is performed on a training set of images and the K-means clustering is applied to it. Each cluster will correspond to one  X  X isual word X , a local pattern. Finally, each image in a data collec tion can be characterized by a histogram of  X  X isual words X  counts.

The most common interest points detectors a re: Harris-affine, Hessian-affine, SIFT (Scale Invariant Feature Transform), Ma ximally Stable Extremal Regions (MSER). Among descriptors we have SIFT (detector and descriptor), local jets (image deriva-tives), steerable filters, generalized mome nt invariants. One of the variations of  X  X ag of features X  (B.O.F) method based on SIFT detector and descriptor was first proposed by Lowe in [6]. Other good sources of information about scale space and local features are [7] and [8]. In [9] authors applied the local features to nude or pornographic images de-tection. Instead of using the well known SIFT descriptor, they implemented Hue-SIFT method in order to take colour into account. [10] is a comparison of different tech-niques used in B.O.F. approach for image sampling, visual dictionary generation and normalization of the histograms. Yang et al. [11] incorporated and tested some methods derived from textual information retrieval domain into CBIR (B.O.F.): term weighting, stop word removal, feature selection. They also conducted experiments testing the influ-ence of the vocabulary size (number of clusters) and spatial information on the retrieval performance.

In this paper we utilize fast and easy to implement method based on local features [15]. Because quite often randomly generated s ample points are more discriminant than the points detected by corner detector (especially when it comes to a large number of sample points when the set of keypoints detected by corner detectors becomes satu-rated), we decided to implement a hybrid sampling technique. Comparison between random, dense, pure corner-based, and hybrid sampling showed the superiority of this type of sampling. Our descriptor is based on co-occurrence matrix and colour moments. For the description of the image patches around the sample points we used separately: co-occurence matrix computed at eight different orientations, and three colour moments to capture the local colour properties. The c o-occurence matrix has proven to be an ef-fective way of texture representation and by considering multiple orientations we make it invariant to rotation. Colour moments are fairly insensitive to changes in viewpoint, and their computation is trivial. Moreover, patches characterized by colour moments are also able to capture the local textural information. Despite the relative simplicity of the model, this method was able to obtain results comparable with current state-of-the-art (ImageCLEF2010 Wikipedia Retrieval Task).

One of the main advantages of our model is that by switching to visual domain we can abstract from the musical concepts and still obtain results comparable with current state-of-the-art. Moreover, the proposed approach could be used for the characterization of signal in general as an alternative to common techniques.
The paper is organized as follows. In Section 2 we present the model and describe the developed algorithm in detail. It consists of the sub-sections introducing the local fea-tures based on the  X  X ag of visual words X , the Fast Fourier Transform and a sub-section on spectrograms generation. Section 3 is devoted to the experiments and discussion. It describes the data collection used in the e xperiment, the detailed experimental setup and results with their analysis. We draw conclusions in Section 4 and finally present some ideas for future research (Section 5). Here, we give a detailed description of the proposed model. The framework establishes the link between MIR and VIR research areas.

Our algorithm consists of the following stages: 1. Transformation to frequency domain: Transform the music data from the time 2. Music representation, visual data generation: Generate spectrograms in two di-3. Image sampling: 4. Description of local patches: Characterize local patches in the form of co-occurence 5. Feature vector construction: Represent local patterns as 9 dimensional (moments) 6. Visual dictionary generation: Apply K-means clustering to the training set in 7. Histogram computation: Create a histogram of visual words counts by calculating 8. Music genre classification: The classification of music data into music genres is Steps 3 to 7 are related to generation of visual representations of the spectrograms. In the course of this research, global methods like colour moments, co-occurrence ma-trix (texture), colour correlograms were also tested. We utilize local features because of their superior performance over various global methods. The local features may also have another advantage over other models. An interesting future work would be to in-vestigate if image patches identified by corner detectors (roughly speaking -locations of a sudden change of pixel intensities) and  X  X i sual words X  correspond to some important characteristics of audio signal. 2.1 The Fast Fourier Transform Let x 0 , ..., x N  X  1 be complex numbers. The Discrete Fourier Transform (DFT) is de-fined by Computing DFT requires O ( N 2 ) operations, while FFT reduces the number to O ( N log N ) . The implemented FFT method incorporates Cooley-Tukey algorithm, which breaks down a DFT into smaller DFTs. The audio sampled size is 65536 bytes (1.486 seconds), with sampling rate of 44100Hz. 2.2 Spectrogram Generation The resulting spectrum is split into 512 bins (64Hz / bin). The power of each bin is converted into a pixel as follows: Let colour = power / meanPower IF {colour &gt; 1} ELSE ELSE ENDIF The horizontal dimension of each image represents the time (1 pixel = 1.486sec), ver-tical dimension represents frequency (1 pixel = 64Hz), and pixel intensities represent power.

This method of spectrogram generation pr oduces images that varies in colour and texture. These properties make the images suitable for application of visual features. An interesting observation is that some genres are easily recognizable directly from our spectrograms. Classical music, for instan ce, is characterized by a presence of the blue colour joining the top and the bottom part of an image. 2.3 The Sampling Technique Recently, an approach based on local feat ures extraction has become quite popular in Visual Information Retrieval. Global appro aches find it hard to capture all the properties of an image. The most recent state-of-the-art in Image Retrieval is based on so-called  X  X ag of visual words X . The first step in the  X  X ag of features X  method is to localize the points of interest (point-like, region-like) by using corner/blob detectors. Other sam-pling techniques include random and dense sampling. The second step involves the rep-resentation of regions around the sample points in a form of multidimensional vectors. There are various existing descriptors, the SIFT (Scale Invariant Feature Transform) being one of the best. The initial extraction is performed on a training set of images and the K-means clustering is applied to it. Each cluster will correspond to one  X  X isual word X , a local pattern. Finally, each imag e in a data collection can be characterized by a histogram of  X  X isual words X  counts. The most popular interest points detectors are: Harris-affine, Hessian-affine, Scale Invar iant Feature Transform (SIFT), Maximally Stable Extremal Regions (MSER). Among descriptors we have SIFT (detector and de-scriptor), local jets (image derivatives), steerable filters, generalized moment invariants.
Let us now return to the local features utilized in this paper. As aforementioned, the implemented hybrid sampling method combines Shi and Tomasi [14] corner detection with a random number generator. The Shi and Tomasi method is based on the Harris corner detector. The change of pixel intensities is characterized as From Taylor approximation of the first order we get Substituting (2) in (1) we obtain We can rewrite equation (3) in the following form where A = We define  X  X ornerness X , a measure of corner response as We assume that the corner was detected if M c is sufficiently large. Shi and Tomasi found that the good corners can be obtained by se tting a minimum threshold and checking if the smaller of the eigenvalues is greater than the threshold.

Another sampling technique implemented for comparison purposes was dense sam-pling. In this case, each image was divided i nto the same number of 900 identical rect-angular sub-images. 2.4 Region Descriptors Each local patch in an image was represented as  X  The 8 orientational co-occurence matrix.  X  Colour moments.
 A simple co-occurence matrix is defined as follows The matrix describes the way certain grayscale pixel intensities occur in relation to other grayscale pixel intensities. It counts the number of such patterns. The most discrim-inating statistics extracted from co-occurenc e matrix are: contrast, inverse difference moment, entropy, energy, homogeneity, and variance.

The method based on three colour moments assumes that the distribution of colour can be treated as probability distribution. Three statistics extracted from individual colour channels are  X  Standard Deviation  X  i = 1 N N j =1 ( p ij  X  E i ) 2 The first moment can be interpreted as an average colour value, second as a square root of the variance of the distribution, and third as the measure of asymmetry in the distri-bution. One can construct the weighted similarity measure as an analogy to manhattan metric, for example: Colour moments can also capture the textural properties of an image and are fairly insensitive to viewpoint changes. By computing them in HSV colour space we can make the statistics insensitive to illumination changes. 2.5 Feature Vector Construction, Visual Dictionary Generation, and Histogram The local patches are represented as multidimensional vectors constructed from differ-ent statistics, extracted from individual colour channels. By taking a sample training set consisting of collection X  X  representative images, we can generate so-called visual vocabulary. The K-means clustering algor ithm has been used for that purpose. Each cluster characterizes a local pattern, representing specific  X  X isual word X . The histogram of visual words counts is created by computing the manhattan distance between individ-ual patches and cluster centroids, and cal culating how many patches belong to specific clusters. 2.6 Music Genre Classification The classification of music data into music genres is performed by k-nearest neighbour algorithm, using Minkowski X  X  fractional similarity measure where x =( x i ) and y =( y i ) are the n dimensional feature vectors. It was exper-imentally proven (see [12]) that the fractional measures from Minkowski X  X  family of distances yield good results in VIR. Figure 2 shows the query by visual example retrieval based on the local features and our music representation.

For the experimental purposes we used a data collection consisting of 4759 music tracks. The genre distribution is presented in table 1.

Genre labels were extracted from iTunes. The local feature algorithm uses 900 sam-ple points per image, for each sample poi nt we open a square window 10 by 10 pixels wide. The dimensionality of the histogram s of visual words counts is 40. The applied k-nearest neighbour algorithm uses 9-fold cross-validation, 12 nearest neighbours, dis-tance weighting and manhattan metric. Th e classification accuracy we obtained with dense sampling was approximately 46 per cent (2176 tracks) of correctly classified in-stances. The hybrid sampling scored lower, resulting in 43 per cent (2051 tracks) of retrieval accuracy. The reason for this lies i n the worse performance of corner detector in this domain. The local features with hybrid sampling performed better than the one with dense sampling on ImageCLEF2007 and MIRFlickr25000 collections, consisting of real-life images.
From the confusion analysis we observed that most incorrectly classified instances were confused with similar genres, and the song-genre correspondence was arguable and subjective. That is why it is so hard to improve the retrieval performance. Good, natural solution to this problem could be the incorporation of fuzzy logic, and associate each song with certain probability of it being in one of the classes. The problem with comparisons with other methods arises b ecause of the lack of the standardized data collections in MIR. Many data collections have unequally distributed data sets, different number of genres, more specia lized or generalized classes.

All of that affects the behavior of classifier. Meng and others [13] used a multivariate autoregressive feature model, considered as current state-of-the-art in MIR, to capture the temporal information in the window. The data set used consisted of 1210 music tracks with 11 genres. The best mean classi fication accuracy they obtained were 44 and 40 per cent for the LM and GLM classifiers. It should be noted though that the accuracies obtained by the automatic classifi cation need to be relative to the theoretical baseline for random classification which is 9% for [13], and 5% for our collection. It means that the performance of our method is actually much better. There are also other aspects, mentioned previously, that make the evaluation difficult.

In his PhD thesis on music genre classification, Serra presents a  X  X on exhaustive list for the most relevant papers presented in journals and conferences for the last years X  [16]. He concludes that  X  X lthough accur acies are not completely comp arable due to the different datasets the authors use, similar approaches have similar results. This suggest that music genre classification, as it is known today, seems to reach a  X  X lass ceiling X  X . The reported accuracies were then plotted with resp ect to the number of genres (Figure 3).
The human performance in classifying music genres (10 genres) is around 53% cor-rectly cassified for 250ms samples and around 70% for samples longer than 3s [17]. Thus, the performance of current state of the art models for genre classification is com-parable with the human performance. In this paper we propose a novel approach to MIR. Having represented the music tracks in the form of two dimensional images, we apply the  X  X ag of visual words X  method from visual IR in order to classify the songs into 19 genres. The motivation behind this work was the hypothesis, that 2D images of music tracs (spectrograms) perceived as similar would correspond to the same music genres (perhaps even similar music tracs). Conversely, we can treat real life images as spectrograms and utilize music-based features to represent these images in a vector form. This would point to an interesting interchangeability between visual and music information retrieval.
 First, songs are represented as images generated from Fourier frequency domain. The next step involves indexing thus obtained data collection by applying the following method, derived from content based image r etrieval. Because images often consist of different patches of uniform patterns, global features find it hard to capture all the prop-erties. Initially, when it comes to the imple mented method, about ha lf of sample points is detected by a corner detector and another half is picked at random. For relatively small number of sample points this technique proved to give better results than sampling based purely on corner detectors, random generators, or dense sampling. Detector-based keypoints tend to concentrate on objects, wh ich is good for object instance recognition but not necessarily good for generic image categorization. For characterization of local patches the regions around selected points are represented as four different statistics extracted from co-occurrence matrix, computed for individual colour channels. We also experiment with a different descriptor ba sed on three colour moments, which is able to capture local textural properties as well. Finally the k-means algorithm is applied to the training set to generate the visual dictionary and the images from the database are characterized with a histogram of visual words counts. Thus obtained histograms, characterizing individual songs, are used for genre music classification task.
We obtained classification accuracy of 46% (with 5% theoretical baseline for random classification) which is comparable with existing state-of-the-art approaches. Moreover, the novel features characterize different properties of the signal than standard methods. Therefore, the combination of them should further improve the performance of existing techniques.

The main advantages of our method are: more intuitive, easy way to automatic music classification, classification accuracy comparable with state-of-the-art and new promising research direction. The future work may include incorporation of the spatial information for local im-age patches, experimentation with differen t sampling techniques and incorporation of temporal information (short time Fourier transform, wavelets), which should further improve the classification accuracy. Additionally, an interesting future work would be to investigate if image patches identified by corner detectors and  X  X isual words X  corre-spond to some important characteristics of audio signal. In other words, new specialized visual features can be developed for this particular task.

