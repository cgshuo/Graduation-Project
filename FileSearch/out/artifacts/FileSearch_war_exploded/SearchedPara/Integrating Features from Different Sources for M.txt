
Efficient and intelligent music information retrieval is a very important topic of the 21st century. With the ulti-mate goal of building personal music information retrieval systems, this paper studies the problem of identifying  X  X im-ilar X  artists using both lyrics and acoustic data. In this pa-per, we present a clustering algorithm that integrates fea-tures from both sources to perform bimodal learning. The algorithm is tested on a data set consisting of 570 songs from 53 albums of 41 artists using artist similarity pro-vided by All Music Guide. Experimental results show that the accuracy of artist similarity classifiers can be signifi-cantly improved and that artist similarity can be efficiently identified.
In multimedia information retrieval the data are natu-rally multi-modal, in the sense that they are represented by multiple sets of features. For example, the representation of a movie has three modes: (i) the personnel (the producer, the director, the editor, the scenario writer, the music com-poser, the cast, etc.), (ii) the visual features (which sum-marize the scenarios and the actions), and (iii) the acoustic features (which summarize the voice and the background audio). The representation of popular music is also tri-modal in some sense, where the second feature set is re-placed by the lyrics. The personnel feature set of the rep-resentation of music, however, is significantly smaller than that of movies, since many music artists produce, com-pose, and perform themselves. This compels one to take the standpoint that the representation of popular music is bimodal, consisting of the acoustic features, which sum-marize the sound, and the text features, which summarize the words put into the music.

Two fundamental problems in dealing with multime-dia data are classification and clustering. Classification is the problem of assigning predefined class labels to the data, while clustering is the problem of dividing the data into classes based on their similarity without predefined class labels. These concepts are interchangeably called su-pervised learning and unsupervised learning , respectively. Since the proportion of predefined class labels available as part of input is 0% for clustering and 100% for clas-sification, one naturally wonders about the special cases of these two fundamental problems in which only a part of the data has predefined labels. This problem is called semi-supervised learning . The main question in semi-supervised learning is whether it is possible to use the un-labeled data to produce something better than the one pro-duced using only the labeled data. In particular, for semi-supervised learning of multi-modal data, i.e., data with het-erogeneous sets of features, a natural question is whether multi-modality can be effectively utilized in learning and, if so, whether such multi-modal learning methods produce better results than unimodal methods.

The celebrated paper of Blum and Mitchell [6] is the first to address formally this question. In this paper, Blum and Mitchell study the problem of incorporating unlabeled data in building classifiers in the presence of two feature sets. In particular, they propose a strategy for constructing classifiers called co-training for the purpose of making use of unlabeled data. The co-training algorithm proceeds in rounds in the following way: In each round a classifier is built on each of the two feature sets using the current train-ing set, which is initially set to the set of data whose labels are given as input. Then, for each feature set, the point among the unlabeled data for which the classifier with re-spect to the feature set provides the most confident asser-tion is selected and is added to the training set of the other feature set along with the assertion. (Note that the two classifiers may select an identical point and disagree on its class label). Blum and Mitchell show that under a certain  X  X ndependence X  assumption about the joint distribution of the feature sets their co-training algorithm converges in the sense of PAC-learning. Many research efforts have been done for the purpose of extending and generalizing the idea of co-training [1, 9, 16, 25, 28].

It is also possible to design an interactive (or ensemble) learning algorithm (that exploits interactions among clas-sifiers to improve accuracy) for supervised learning (that is, all the data are already labeled). For example, the co-boosting algorithm of Collins and Singer [8] uses the indi-vidual boosting of the feature sets with the weight adjust-ments influenced by the labeling of the other classifier(s). The approach can be used not only for supervised learning but for semi-supervised learning (indeed co-boosting algo-rithm was originally conceived for semi-supervised learn-ing). Although such algorithms may fall into pitfalls due to the highly simple mutual boosting structure, Collins and Singer point out, such multi-modal learning can be very powerful and thus is worth while.

The work of Blum and Mitchell and that of Collins and Singer study the design of effective algorithms multi-modality through interaction for semi-supervised learning and for supervised learning, respectively. This naturally leads to the question of whether multi-modal interactive methods can be more powerful than unimodal methods in the case of unsupervised learning, namely, clustering. The purpose of this paper is to study this question on bimodal clustering (we of course anticipate that bimodal cluster-ing techniques can be naturally extended to general multi-modal clustering). We present a clustering framework for integrating the features based on minimizing disagreement. It is known that in bimodal learning minimizing disagree-ment between two classifiers can improve the performance of learning [3, 12].

In this paper we present a formalization of the prob-lem of minimizing disagreement in bimodal learning in the Bayesian framework. In the framework, minimizing disagreement can be thought as a simple common theme of multi-modal information retrieval: individual feature sets interact to help each other by reducing disagreement among their outputs. We then present a bimodal cluster-ing algorithm based on the common theme  X -initialize the cluster layout using the output of the counterpart and try to minimize the disagreement between two modes. We apply the bimodal clustering algorithm to the problem of clustering popular music songs.

The rest of the paper is organized as follows: Section 2 introduces the underlying principle of minimizing the dis-agreement, Section 3 presents the clustering algorithm of utilizing the general principle, Section 4 describes the two heterogeneous feature sets extracted from the lyrics and acoustics data, Section 5 presents the results of experi-ments. Finally Section 6 concludes.
In this section, we introduce the basic principle of min-imizing disagreement, i.e., minimizing the disagreement between two individual models could lead to the improve-ment of learning performance of individual models.
Our data are bimodal: let X 1 and X 2 be the space of the first mode and the space of the second mode, respectively. Let X = ( X 1 , X 2 ) be the product space of X 1 and X 2 Let 0 and 1 be the class labels of these data, which we will often denote by Y . For each u  X  X  0 , 1 } , we use  X  u to its opposite class label, that is, 1  X  b . Suppose that the data in X is subject to a distribution D . Let f be our class label function and let f 1 and f 2 be our class label functions based on the first mode and on the second mode, respectively. The ( x ) in f and Y are often dropped  X  we will write f = u to mean f ( x ) = u and Y = u to mean Y ( x ) = u , etc.
 Definition 1 We say that f is a nontrivial classifier if for all u  X  X  0 , 1 } , where the probability is subject to D .
 Remark 1 The above nontrivial condition can be restated as (  X  u  X  X  0 , 1 } )[Pr( f = u | Y = u ) &gt; 1 / 2] and as (  X  u  X  { 0 , 1 } )[Pr( f 6 = Y )  X  P r ( f = u )] .

In [6], it is assumed that x 1 and x 2 are conditionally independent given the labels, i.e.,
Pr( x 1 = x 0 1 | x 2 = x 0 2 ) = Pr( x 1 = x 0 1 | f 2 ( x The independence assumption is rather strong, but has been used by many successful applications. Suppose we build hypotheses f 0 1 on X 1 and f 0 2 on X 2 . Thus, if x x 2 are conditional independent given the labels, then f 0 1 and f 0 2 are also conditional independent. The conditional independence of f 0 1 and f 0 2 can be interpreted as follows: Pr( f 0 1 ( x 1 ) = u | f 0 2 ( x 2 ) = v, Y = y ) = Pr( f where u, v, y  X  X  0 , 1 } . In other words, The conditional independence implies that (i) for all S 1  X  X 1 such that the probability of ( S 1 , X 2 ) is non-zero, the distribution of X 2 in which the first mode is restricted to S 1 is identical to the distribution of X 2 with no restriction; and that (ii) for all S 2  X  X 2 such that the probability of ( X 1 , S 2 non-zero, the distribution of X 1 in which the first mode is restricted to S 2 is identical to the distribution of X 1 no restriction.
 One can show the following (proof omitted): Theorem 1 Under conditional independence assumption, the disagreement upper bounds the misclassification error for the nontrivial classifiers.

In essence, this indicates that, under certain conditions, the disagreement upper bounds the misclassification error. Thus, minimizing disagreement will ideally decrease the upper bound on the misclassification error and could boot-strap the learning algorithm. It should be pointed out that although the principle was originally proved in the context of supervised learning [12], it can be thought as a simple common theme of multi-modal information retrieval: indi-vidual feature sets interact to help each other by reducing disagreement among their outputs. Let x = ( x 1 , x 2 ) be an observation vector. Then the Bayes decision rule for the first mode is: This implies that if the posteriori probability of class 1 (re-spectively, class 0 ) given x 1 is larger than the probability of class 0 (respectively, class 1 ), x 1 is assigned to class 1 . Using the Bayes theorem and eliminating the common term Pr( x 1 ) , we get Pr( Y = 1) Pr( x 1 | Y = 1)  X  0 1 Pr( Y = 0) Pr( x 1 | Y = 0) .
The Bayes error can be computed as: 2 Here L 1 1 is the area in which
Pr( Y = 1) Pr( x 1 | Y = 1) &gt; Pr( Y = 0) Pr( x 1 | Y = 0) and L 1 0 is the area in which
Pr( Y = 1) Pr( x 1 | Y = 1) &lt; Pr( Y = 0) Pr( x 1 | Y = 0) . In other words, if an observation x 1  X  L 1 1 , it will be clas-class 0 .

Under the conditional independence assumption, the disagreement between two components can be computed as where p 0 ( x 1 , x 2 ) = Pr( Y = 0) Pr( x 1 | Y = 0) Pr( x 2 | Y = 0) , and p 1 ( x 1 , x 2 ) = Pr( Y = 1) Pr( x 1 | Y = 1) Pr( x 2 | Y = 1) . Here L 2 1 is the region where
Pr( Y = 1) Pr( x 2 | Y = 1) &gt; Pr( Y = 0) Pr( x 2 | Y = 0) and L 2 0 is the region where Pr( Y = 1) Pr( x 2 | Y = 1) &lt; Pr( Y = 0) Pr( x 2 | Y = 0) . Similarly, if an observation x 2  X  L 2 1 , it will be classified as
Observe that = Pr( Y = 1) = Pr( Y = 1) = Thus, to ensure that  X  E ( x 1 , x 2 ) , it is sufficient that Z and Z The above formula can be reduced to The formulas in Eq. (1) and (2) in the above are essentially the same as those in Definition 1 of Section 2. Hence, the disagreement upper bounds can also be derived from the Bayes perspective.
 Remark 2 When the conditional independence condition (e.g., equation 1) doesn X  X  hold, to guarantee that disagree-ment upper bounds the misclassification error, we need
Pr( f 0 1 = 0 | f 0 2 = 0 , Y = 1)  X  Pr( f 0 1 = 1 | f 0 2
Pr( f 0 1 = 1 | f 0 2 = 1 , Y = 0)  X  Pr( f 0 1 = 0 | f 0 2 In other words, if then the disagreement still upper bounds the misclassifica-tion error without the conditional independence condition.
In this section, we present a clustering algorithm that integrates different features based on the principle of min-imizing disagreements. Let D = { d 1 , d 2 ,  X  X  X  , d n } be a set of n data points. Suppose we are given two clusterings P 1 and P 2 with each consists of a set of clusters: where k i is the number of clusters for clustering P i , and D = S k i j =1 C j i . The first question is how to measure the agreements between the two clusterings.

We use adjusted Rand index to compute the agreement between clusterings. Adjusted Rand Index is a statistic to assess the clustering quality compared against assigned known classes. The Rand Index is defined as the number of pairs of objects which are both located in the same cluster and the same class, or both in different clusters and dif-ferent classes, divided by the total number of objects [36]. Adjusted Rand Index which adjusts Rand Index is set be-tween [0 , 1] [17]. The higher the Adjusted Rand Index, the more resemblance between the two clusterings.
 Formally, the adjusted Rand index, ARI , is defined as Here n ij denotes the number of objects belonging to both C i and C
We present a bimodal clustering approach based on the minimizing the disagreement principle. The algorithm is an extension of the EM method [13]. In each iteration of algorithm, an EM type procedure is employed to bootstrap the model obtained from one data source by starting with the cluster assignments obtained in the previous iteration using the other data source. Upon convergence, the two individual models are used to construct the final cluster as-signment. Table 1 listed the notions used for the algorithm and the algorithm procedure is presented in Figure 1. We assume parameterized models, one for each cluster. Typically, all the models are from the same family, e.g., multivariate Gaussian. The algorithm described above is a variant of the EM algorithm. It performs an iterative op-timization process for each data source by using the clus-ter assignments from the other sources. Note that in each iteration, one data source is picked and every data point is reassigned to one of the clusters based on information from that data source and on its previous assignment. At the end of each iteration, the algorithm explicitly checks whether the agreement between two clusterings (one clus-tering from each data source) has been improved. If it is improved, the algorithm then continues to iterate. Other-Algorithm 1 : Bimodal Clustering Input: S , K Output: Cluster assignment Y as well as the trained model structure 1: Initialization: Initialize the model structure ( X  1 ,  X  2: while the stopping criterion does not meet do 3: Step I: 4: Step II: 5: Step III: 6: Step IV: 7: end while 8: Return Y as well as the trained models ( X  1 ,  X  2 ) wise, the algorithm will go back to the allocation step and hopefully get a new clustering.
We addresses the issue of identifying the artist style us-ing both content and lyrics. Ellis et al. [26] point out that similarity between artists reflects personal tastes and sug-gest that different measures have to be combined together so as to achieve reasonable results in similar artist discov-ery. Recently, [35] shows the usefulness of multi-modal learning for music artist style classification. In this sec-tion, we describe the feature sets extracted from the lyrics and the acoustic content.
Recently, there has appeared some work that exploits the use of non-sound information for music information retrieval. Whitman and Smaragdis [35] study the use of the descriptions (obtained from All Music Guide) and the sounds of artists together to improve classification. Whitman, Roy, and Vercoe [34] show that the mean-ings the artists associate with words can be learned from the sound signals. A number of researchers also pre-sented probabilistic approaches to model music and text jointly [5, 7, 22, 29]. From these results, it can be hypoth-esized that by analyzing how words are used to generate lyrics, artists can be distinguished from others and similar artists can be identified.

Previous study on stylometric analysis has shown that statistical analysis on text properties could be used for text genre identification and authorship attribution [2, 18, 30] and over one though stylometric features (style makers) have been proposed in variety research disciplines [32]. To choose features for analyzing lyrics, one should be aware of the characteristics of popular song lyrics. For instance, song lyrics are usually brief and are often built from a very small vocabulary. In song lyrics, words are uttered with melody, so the sound they make plays an important in de-termination of words. The stemming technique, though useful in reducing the number of words to be examined, may have a negative effect. In song lyrics, word orders are often different from those in conversational sentences and song lyrics are often presented without punctuation.
To account for the characteristics of the lyrics, our text-based feature extraction consists of four components: bag-of-words features, Part-of-Speech statistics, lexical fea-tures and orthographic features.  X  Bag-of-words : We compute the TF-IDF measure for  X  Part-of-Speech statistics : We also use the output of  X  Lexical Features : By lexical features, we mean fea- X  Orthographic features : We also use orthographic fea-
There has been a considerable amount of work in ex-tracting descriptive features from music signals for mu-sic genre classification and artist identification [14, 19, 27, 33, 21]. In our study, we use timbral features along with wavelet coefficient histograms. The feature set consists of the following three parts and totals 35 features. 4.2.1 Mel-Frequency Cepstral Coefficients (MFCC) MFCC is a feature set popular in speech processing and is designed to capture short-term spectral-based features. To obtain the feature, we first compute, for each frame, the logarithm of the amplitude spectrum based on short-term Fourier transform, where the frequencies are divided into thirteen bins using the Mel-frequency scaling. (The  X  X epstrum X  is the name coined for this logarithm.) After taking the logarithm of the amplitude spectrum, the fre-quency bins are grouped and smoothed according to Mel-frequency scaling, which is design to agree with percep-tion. MFCC features are generated by decorrelating the Mel-spectral vectors using discrete cosine transform. In this study, we use the first five bins, and compute the mean and variance of each over the frames. 4.2.2 Short-Term Fourier Transform Features (FFT) This is a set of features related to timbral textures and is not captured using MFCC. It consists of the following five types. More detailed descriptions can be found in [33].
Spectral Centroid is the centroid of the magnitude spec-trum of short-term Fourier transform and is a measure of spectral brightness. Spectral Rolloff is the frequency be-low which 85% of the magnitude distribution is concen-trated. It measures the spectral shape. Spectral Flux is the squared difference between the normalized magnitudes of successive spectral distributions. It measures the amount of local spectral change. Zero Crossings is the number of time domain zero crossings of the signal. It measures noisiness of the signal. Low Energy is the percentage of frames that have energy less than the average energy over the whole signal. It measures amplitude distribution of the signal.
We compute the mean for all five types and the variance for all but zero crossings. 4.2.3 Daubechies Wavelet Coefficient Histograms Daubechies wavelet filters are ones that are popular in im-age retrieval (see [10]). To extract DWCH features, the db 8 filter with seven levels of decomposition is applied to thirty seconds of sound signals. After the decomposi-tion, the histogram of the wavelet coefficients is computed at each subband. Then the first three moments of a his-togram, i.e., the average, the variance, and the skewness, are used [11, 21] to approximate the probability distribu-tion at each subband. In addition, the subband energy, de-fined as the mean of the absolute value of the coefficients, is also computed at each subband. A few trials reveal that of the seven subbands of db 8 (1: 11025 X 22050 Hz, 2: 5513 X 11025Hz, 3: 2756 X 5513Hz, 4: 1378 X 2756Hz, 5: 689 X 1378Hz, 6: 334 X 689Hz, 7: 0 X 334Hz), subbands 1, 2, and 4 show little variation. We thus choose to use only the remaining four subbands, 3, 5, 6, and 7, for our exper-iments. In fact, The subbands match the models of sound octave-division for perceptual scales [20].
In this section, we perform experiments to evaluate whether the clustering algorithms based on minimizing disagreement can be more powerful than unimodal meth-ods.
Our experiments are performed on the dataset consist-ing of 570 songs from 53 albums of a total of 41 artists. The sound recordings and the lyrics from them are ob-tained.

To obtain the ground truth of song styles, we choose to use similarity information between artists available at All Music Guide artist pages (http://www.allmusic.com), assuming that this information is the reflection of multi-ple individual users. By examining All Music Guide artist pages, if the name of an artist X appears on the list of artists similar to Y, it is considered that X is similar to Y. The clus-ters are listed in Table 2. Our goal is to identify the song styles using both content and lyrics, i.e., cluster the 570 songs into the four different clusters. We use the cluster information of the artists as the labels for their songs.
As discussed above, we use the cluster structures ob-tained from All Music Guide as labels to evaluate the clus-tering performance. We use Purity, Entropy and Accu-racy [38] as our performance measures. We expect these Clusters Members No. 1 { Fleetwood Mac, Yes, Utopia, Elton John, No. 2 { Carly Simon, Joni Mitchell, James Taylor, No. 3 { AC/DC, Black Sabbath, ZZ Top, No. 4 All the remaining artists measures would provide us with good insights on how our algorithm works.

Purity measures the extent to which each cluster con-tained data points from primarily one class [38]. The pu-rity of a clustering solution is obtained as a weighted sum of individual cluster purity values and is given by where S i is a particular cluster of size n i , n j i is the number of documents of the i -th input class that were assigned to the j -th cluster, K is the number of clusters and n is the total number of points 4 . In general, the larger the values of purity, the better the clustering solution is.
Entropy measures how classes distributed on various clusters [38]. The entropy of the entire clustering solution is computed as: where m is the number of original labels, K is the number of clusters. Generally, the smaller the entropy value, the better the clustering quality is.

Accuracy discovers the one-to-one relationship between clusters and classes, therefore to measure the extent to which each cluster contained data points from the corre-sponding class. It sums up the whole matching degree be-tween all pair class-clusters. Accuracy of the clustering can be represented as: where C k denotes the k -th cluster, and L m is the m -th class. T ( C k , L m ) is the number of entities which belong to class m are assigned to cluster k . Accuracy computes the maximum sum of T ( C k , L m ) for all pairs of clusters and classes, and these pairs have no overlaps. The larger accuracy usually means the better clustering performance.
We compare the results of the bimodal clustering algo-rithm with the results obtained when the clustering is ap-plied on the two sources of data separately.

We also compare the bimodal clustering algorithm with the following clustering strategies on integrating different information sources:  X  Feature-Level Integration: Feature-level integration  X  Cluster Integration: Cluster integration refers to the  X  Sequential Integration: Sequential integration is an
Figure 1 illustrates and summarizes various strategies for integrating different information sources.
We compare the results of bimodal clustering with the results obtained when clustering is applied on content and lyrics separately, and with the results of other integration strategies. Table 3 presents the experimental results. Feature-Level Integration 0.425 0.729 0.380 Sequential Integration I 0.431 0.724 0.434
Sequential Integration II 0.438 0.734 0.407
Table 3. Performance Comparison. The num-bers are obtained by averaging over ten tri-als.

From the table, we observe the following:  X  The performance of purity, entropy, and accuracy rel- X  The purity and accuracy of feature-level integration  X  Cluster Integration: The cluster integration performs  X  Sequential Integration: the results of sequential inte- X  Our bimodal clustering outperforms all other meth-
Experimental comparisons show that our bimodal clus-tering can efficiently identify song styles. For example, in our experiments, two songs from the album Utopia / Anthology : Overture Mountain Top And Sunrise Commu-nion With The Sun and The Very Last Time would be put into two different clusters based on their contents or lyrics only. However, using both the content and lyrics, our bimodal clustering algorithm identifies them to be in the same cluster with similar styles. Similarly, bimodal clus-tering identifies two songs from the album Peter-Gabriel / Peter Gabriel : Excuse Me and Solsbury hill to be in the same cluster while other methods don X  X . In our experi-ments, we have identified around 50 such pairs and they give good anecdotal evidence that our bi-modal clustering algorithm can efficiently identify song styles.

To investigate the relationship between the clustering performance and the agreement with respect to the two sources, we take a closer look at our experiments. Figure 2 shows the cluster performance (entropy and purity values) and the (dis-)agreements between two sources in a trial. Each unit on the X-axis represents five iterations of the al-gorithm and the Y-axis shows the performance value. We can observe from Figure 2 that as the agreement between the two sources increases, the clustering quality also tends to increase (i.e., entropy is generally decreasing while pu-rity is increasing).
In this paper, we study the problem on whether multi-modal interactive methods can be more powerful than uni-modal methods in the case of clustering. In particular, we present a clustering framework for integrating the features based on minimizing disagreement. Experimental results on a data set consisting of 570 songs from 41 artists of 53 albums show the effectiveness of our approach.

There are two natural avenues for future research. The first natural direction is on music annotation. How can we automatically and efficiently generate music style or simi-larity information? Note we did not agree completely with the artist similarity obtained from All Music Guide, but nonetheless used it as the ground truth to evaluate our algo-rithms in the experiments. Can we incorporate the opinions Figure 2. Relationships Between Clustering
Performance and Agreements. Each unit on the X-axis represents 5 iterations of the algo-rithm and the Y-axis shows the performance value. from music experts or take into account the views from in-dividual users? Second, it would also be interesting to ex-tend the bimodal algorithm by using statistical inference techniques to adaptively weight different data sources dur-ing the clustering process. This work is supported in part by NSF Career Award IIS-054680, NSF grants EIA-0080124, and EIA-0205061.
