 In this paper, we look into the problem of filtering problem so-lution repositories (from sources such as community-driven ques-tion answering systems) to render them more suitable for usage in knowledge reuse systems. We explore harnessing the fuzzy nature of usability of a solution to a problem, for such compaction. Fuzzy usabilities lead to several challenges; notably, the trade-off between choosing generic or better solutions. We develop an approach that can heed to a user specification of the trade-off between these crite-ria and introduce several quality measures based on fuzzy usability estimates to ascertain the quality of a problem-solution repository for usage in a Case Based Reasoning system. We establish, through a detailed empirical analysis, that our approach outperforms state-of-the-art approaches on virtually all quality measures. Categories and Subject Descriptors: H.3 [Information Systems]: Information Storage and Retrieval  X  Content Analysis and Index-ing, Information Search and Retrieval General Terms: Algorithms Keywords: Compaction, QA, Case-based Reasoning
Internet based knowledge markets 1 have been increasingly gain-ing popularity in recent years. Of these, community driven QA (CQA) sites are considered a valuable source of opinions , facts and experiences 2 , whereas Wikipedia-like sources are mostly for facts . Here, we focus on approaches for pre-processing problem-solution datasets to filter out knowledge based on the quality and genericity criteria that we will outline. Research on the utility problem in CBR systems [1] suggests that such compaction is necessary for building efficient Case-Based Reasoning (CBR) systems. Data from CQA websites contain multiple solutions to each problem, with each so-lution tagged with the number of votes it has acquired through user voting.
We motivate the problem using a series of examples. The outer black boundary in Figure 1(a) is meant to represent a space of prob-lems. A1, A2 and A3 represent solutions; corresponding ovals rep-resent the subset of problems (called the coverage space ) that they can solve . Assuming all solutions are of indistinguishable quality, consider the problem of choosing two solutions from among them. One woul intuitively want to choose A2 and A3 since they have the largest coverages. The same approach, applied to Figure 1(b) would choose A4 and A5. However, the large overlap between the coverage spaces of A4 and A5 does not augur well with our rather intuitive intent of preserving solutions that together solve as many problems as possible; one may want to choose A 4 and A 6 hence. A greedy incremental heuristic [2] starts from an empty set of solutions, and keeps adding solutions that are useful for the maximum number of problems outside the set of problems already covered by the solutions selected so far. Similar effects could also be achieved by weighing problems in the intersection of A4 and A5 half as much as those which are in the coverage area of only one solution [3].

Unlike Figure 1(a&amp;b), usabilities of solutions to problems are often fuzzy. Consider an example in Figure 1(c); the degree of darkness of the coverage area is directly related to the usability of the solution to that problem subset. Let the coverage areas of A1 and A2 be roughly the same (  X  60% of the problem space) and their darkness representing medium usability; on the other hand, A3 and A4 cover two disjoint 30% regions with high usability. Consider choosing one solution from among { A 1 ,A 2 ,A 3 ,A 4 } . We could choose one from either of { A 1 ,A 2 } or { A 3 ,A 4 } ; the former leads to preserving a medium usability solution for 60% of the problems, and the latter preserves a high usability solution for half as many. This trade-off is tricky, and depends on the intelligence in the down-stream adaptation engine (that adapts the retrieved solution to help solve the user posed problem), user preferences etc.

Now, consider choosing two solutions from { A 1 ,A 2 ,A 3 ,A 4 each combination of two solutions leads to a coverage of 60% of the problem space. One option is to choose A1 and A2 that leads to retaining two medium usability solutions for each problem in that 60% space; at the other extreme is choosing A3 and A4 that retains one high usability solution for each problem in the same space. The choice between these is dependent on various factors. Real scenarios are often more complex than that in Figure 1(c); a solution that is of medium usability for a part of the problem space may be of high usability to other problems. Thus, coverage areas for solutions are often composed of various shades. This trade-off between solution usability and solution abundance is at the core of the problem addressed in this paper.
Problem-Solution pairs are often referred to as cases in CBR lit-erature; case bases are collections of cases. Consider a case base that contains objects of the form d i = { p i ,s i } . Each object d composed of a problem p i and a set of solutions s i (we model s a set since multiple solutions to a problem is typical of CQA sys-tems). We would like to condense D to a form D whose objects are of the form d i = { p i ,s i } where  X  i, s i  X  s i . Informally, we would like to remove some solutions from the solution set of each object in
D based on a user-specified preference of the quality-abundance trade-off. Our compaction process decides on which solutions are to be removed from D based on a user-specified trade-off between usability and abundance of solutions; we allow the user to specify this using a parameter  X  , a high value for  X  indicating that the usage of the compacted dataset D would be in scenarios where the user is interested in getting more solutions (even at the expense of some reduction in usability) to newly posed problems and vice versa. p (  X  ) denotes the problem associated with a particular solution,  X  . In heeding to the user specified trade-off, we would make use of usability estimates for solutions to specific problems. The usability of a solution  X  to a problem p j is denoted by u (  X , p j of a solution to its own problem (i.e., values for u (  X , p (  X  )) )is often readily available from user voting systems such as those in Yahoo! Answers ; more votes correspond to more usability. We will elaborate on our notion of usability and how we estimate it in the absence of voting data, in Section 4.

We do not remove any problems from D since retrieving similar problems to a new problem is a text retrieval problem and state of the art techniques for text retrieval easily scale to terabytes utility problem is more manifested in downstream processes that are handled by the CBR system core, making the compactness of the solution set an important priority.
The utility problem in case-based reasoning systems [1] suggests that the efficiency of a case based reasoning system often degrades when the size of the case-base increases. In response, case-base compaction techniques have been developed considering aspects such as classification accuracy [4, 5], coverage and reachability [6], retrieval models [7] and competence retention [8]. In this paper, we focus on certain trade-offs in case base compaction that stem from the nature of fuzzy usability of solutions. Fuzzy usability scores are not unique to unstructured problem-solution pairs; even http://www-nlpir.nist.gov/projects/terabyte/ in structured cases, assessments of whether a plan would work for a travel request may be subjective, leading to fuzzy usability assess-ments. To the best of our knowledge, case base compaction with fuzzy usability scores is being considered for the first time. We will elaborate on the notion of usability in Section 4.
Usability of a solution for a problem quantifies the ease of adapt-ing the solution for solving the problem. The CBR system relies on the assumption that similar problems have similar solutions. Thus, given a new problem, a CBR system retrieves solutions of similar problems since they are deemed to be usable . However, it could so happen that such solutions are not so usable; thus, the real usability of the solution is determined based on voting systems where users vote whether the deemed to be usable solutions presented to them are really usable or not. Thus, this actual usability, a supervised concept (derived from user feedback), is distinct from deemed us-ability, an unsupervised measure.

CQA systems often have voting scores for problem-solution pairs where the solution was proposed for the problem; we use these as a proxy for the usability. We have to fall back to using statistical measures to estimate the usability of an solution  X  to a different problem p j (i.e., p j = p (  X  ) ). We compute the usability of the so-lution  X   X  s i , to the problem p j (whose true solutions are available as s j )as: where sim ( ., . ) is a text similarity function such as cosine use s (  X  ) as a shorthand for u (  X ,p (  X  )) and will follow the conven-tion of using s ( . ) when the usability is estimated from user feed-back such as votes and u ( ., . ) when the usability is estimated using statistical measures as above. The above formulation is based on the intuition that  X  is more usable for p j if it is highly similar to highly usable solutions associated with p j .

Usage of a CBR system starts with receiving a user-posed prob-lem and invoking a retrieval engine to retrieve the top-k similar problems. Thus, the usabilities of solutions associated with prob-lems that are not retrieved need to be squished to zero. Thus, our refined formulation (that also uses a threshold,  X  ,thatwesetto 1 . 0 ) stands as:
We use u ( ., . ) to denote this refined notion hereafter.
As outlined in Section 2, we would like to design a compaction technique that can make use of a user parameter specifying the trade-off between solution usability and solution abundance .At one extreme, the users of the target CBR system are only inter-ested in getting the maximally usable solution to their queries; at the other, they are interested in getting as many usable solutions as possible . Instead of modeling it as an awkward parameter ranging from 0 to 1 (the ends for each extreme), we model the trade-off parameter as the tipping point , the number of solutions upto which a user may consider abundance useful. For example, a user may suggest that she may be interested in looking at upto 3 different http://en.wikipedia.org/wiki/Cosine_similarity solutions, beyond which she attaches no value for abundance; the compaction engine could then focus on enough abundance that en-sures close to 3 solutions for problems from the problem space, beyond which it could optimize usability. When the parameter is set to 4, the end-user could then expect to receive more number of solutions (closer to 4); however, the trade-off forces it to deliver solutions of lesser usability than the former case with 3.
We now lay down evaluation criteria to measure the goodness of compaction. These are run-time characteristics, that would be mea-sured for problems from the test set posed to a CBR system built on the compacted dataset. The evaluation is done on a test dataset of cases for which true solutions are known; more specifically, the criteria that we describe here all make use of usability scores that are estimated using the true solutions of the test problem (these true solutions come with user feedback data such as # votes). Consider a set of test problems, T = { t 1 ,t 2 ,...,t k } . Let the set of solu-tions returned by the CBR system for a test problem t i be denoted by R ( t i ) . We evaluate the various compaction techniques on how they perform on these measures, based on the average of these mea-sures across problems in T .

Usability of the Maximally Usable Solution (max): max ( t i ,R ( t i )) = max { u (  X , t i ) |  X   X  R ( t i ) where u ( ., . ) is estimated as shown in Section 4.

Number of Usable Solutions Presented (#ans): # ans ( t i ,R ( t i )) = |{  X  |  X   X  R ( t i )  X  u (  X  ij
Total Usability of Solutions (tot):
Coverage (cov): This measures the number of solutions that pass the usability threshold (which we set to 1 . 0 , the quantity equivalent to one user vote) cov precisely denotes the notion of competence of case bases [7]; this is mostly the only metric used in evaluating case base com-paction techniques in lietarature.

Diversity (div): This measures the diversity of the solution set; diverse result sets may be intuitively preferred.
Our approach is a simple greedy decremental approach that re-moves solutions one-by-one, each iteration removing the solution that minimally affects a specified objective function.

Consider an intermediate dataset D  X  ( D  X  is to be understood as
D after removal of some solutions) and the input compaction parameter  X  . Since the specification of the compaction parameter suggests that the user is interested only in the top- X  solutions for a problem, our objective function will also concern with only the top- X  solutions. Consider a case d  X  i (used to denote the case corre-sponding to d i in D ) from D  X  whose problem p i is posed to a CBR system built on D  X   X  X  d  X  i } . Our objective function for this specific p is defined as the following: where R ( p i ) denotes the top- X  solutions to p i when posed on a Alg. 1 Greedy Decremental Optimization Input. D , X , X  Output. D 1. D  X  = D 2. while D  X  has more than  X  solutions 3. a =argmin 4. remove a from among the solutions in D  X  5. return D  X  as D CBR system that works on the dataset D  X   X  X  d  X  i } .Weusetheorig-inal solution set s i (from D , Ref. Section 2) to estimate the u ( ., . ) for usage in computation of max ( ., . ) , tot ( ., . ) and R ( . ) . The de-sign of O ( D  X  ,p i ) counts the usability of the maximally usable so-lution twice, once in the first term and once in the second term; the other solutions are only counted once (in the second term, as part of the tot ( ., . ) ). For very high  X  , the overall sum is contributed to, by mostly the second term since | R ( . ) | increases with  X  leading to a corresponding increase in tot ( ., R ( . )) . This property reduces the relative contribution by the maximally usable solution for high  X  . The overall measure of fit for D  X  is then defined as: Let  X  be the number of solutions desired in the compacted dataset D ; the greedy decremental algorithm is given in Algorithm 1. Although the objective function only includes max ( ., . ) and tot ( ., . ) , Algorithm 1 indirectly optimizes on coverage. This is because the removal of the only usable solution to a problem (i.e., the operation that would reduce coverage) would affect the objective function through both the max ( ., . ) and tot ( ., . ) (whereas most other oper-ations only affect tot ( ., . ) ) and hence is less preferred by design. This is so since the only solution is also the maximally usable so-lution. The objective function and the algorithm are not designed to optimize for div ( ., . ) ; however, we will see that the resultant dataset D fares well on the diversity of solutions too.
Making use of fuzzy solution usabilities in case base compaction is a new problem, and has not been addressed in literature. Thus, we can only compare our techniques with those that work with crisp usabilities by quantizing the fuzzy usability with a threshold. We use the incremental approach from [9] (ADD) and the decremen-tal approach from [6] (DEL) as baseline techniques to compare our technique to. We incorporate the  X  parameter in these approaches by assigning the utility of adding (or penalty of removing) a solu-tion which is usable for a problem that has fewer than  X  solutions to 0 . 5 . The raw version of these approaches are equivalent to as-signing utility of 0 . 0 to any solution beyond the first.
We use three datasets; (1) MSR QA Dataset 5 ( encarta ), (2) Ya-hoo! Answers Data from the Lincoln cars category ( lincoln )and(3) Yahoo! Answers Data from the Delhi and NCR category ( delhi ). Each of these have thousands of problems and multiple solutions per problem. We randomly chose 100 problems for each to use as the test set, and report the performance on the evaluation measures averaged over the test problems. We present charts only on the en-carta dataset since the trends were mostly similar across datasets.
Firstly, we analyze the performance of the various techniques with varying  X  . We set the number of solutions to be preserved to be equal to the number of problems in D ensuring one solution per problem in D ,onanaverage . We expect that the performance on the max measure would deteriorate with increasing  X  ,since more and better are often contradictary goals. We plot the performance of the various techniques on the encarta dataset in Figures 2,3,4,5 and6(  X  is plotted on the X-Axis). The OPT line (wherever it is visible on the scale) shows the value of the appropriate metric when no compaction has been performed on the dataset; this signi-fies the upper bound. From the charts, it is easy to see that GDO is seen to outperform the other techniques on each and every mea-sure for small values of  X  , while the difference between the various techniques starts to fade away at high values of  X  .The GDO tech-nique remained the absolute best on each of the settings on the other datasets; the quantum of improvements is summarized in Table 1. It is interesting to note that heeding to the usability while compaction also automatically optimizes better on the # ans and div measure as compared to DEL and ADD that seek to optimize only # ans (since they do not use the fuzzy usability scores). This underlines the importance of heeding to fuzzy usabilities since it leads to more and diverse result sets across problems.
We now analyze the performance of the compaction techniques with varying ratios of # solutions # problems in the compacted dataset, run two sets of experiments, one on a dataset compacted with  X  = 1 and the other with  X  = 3 . The charts for the encarta dataset with  X  = 1 are plotted against the ratio of solutions to problems (in in Figures 7,8,9, 10 and 11 (ratio is plotted in the X-axis). The difference in effectiveness between the techniques manifest more as more solutions can be chosen for retention. GDO still outperforms the other techniques in each and every measure of interest.
In this paper, we considered the novel problem of harnessing fuzziness in the usability of solutions to problems in compacting Figure 10: cov vs. fraction (encarta) (  X  = 1 ) problem solution repositories. We outline various evaluation mea-sures, and show that the technique we propose outperforms the state of the art techniques in each of them. This underlines the impor-tance of heeding to any available fuzzy usability scores, while per-forming compaction of problem solution repositories.

