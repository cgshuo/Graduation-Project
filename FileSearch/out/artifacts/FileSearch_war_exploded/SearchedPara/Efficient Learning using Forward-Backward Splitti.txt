 We use k x k mize the following sum of two functions: they are into R a sequence of loss functions  X  for excessively complex vectors, for instance r ( w ) =  X  k w k tor w according to the update rule w t +1 = w t  X   X  size and g f more general method than the above is the projected gradient method, which iterates where  X  f ( w  X  ) = O (1 / minimize Eq. (1) gives simple iterates of the form w t +1 = w t  X   X  functions such as r ( w ) = k w k the true minima of the function. Furthermore, with  X  known [1]. [6] give a general and efficient projected gradien t method for  X  sparsity patterns through  X  particular the runtime and sparsity selection performance of the derived algorithms.  X  In the above, g f actual value of  X  new vector that interpolates between two goals: (i) stay clo se to the interim vector w an interim step size, denoted  X  value of  X  belong to subgradient set of the objective at the optimum w t +1 , that is, Since w anteed to obtain a vector g r where g f gradient-based and online convex programming algorithms. provide convergence rate and regret analysis. To derive con vergence rates we set  X  we show in the sequel, it is sufficient to set  X  what follows, define k  X  X  ( w ) k , sup the fairly general assumption [10, 11] that the subgradient s are bounded as follows: (iii) r ( 0 ) = 0 , and (iv) 1
X We provide one corollary below as it underscores that the rat e of convergence  X   X  T . for a predefined T iterations with  X  functions f compared to a single optimal predictor w  X  . Formally, let f t To achieve an online bound for a sequence of convex functions f begin with a slightly different assignment for  X  following theorem, whose proof we provide in Appendix B.
  X  X  with  X  for F OBOS when the sequence of loss functions f to [8], by using the curvature of f w be rewritten as min F computed from w and offline method for minimizing a convex f with  X  the support of g f F
OBOS with  X  2 2 regularization: When r ( w ) =  X  problem, min F vector. By setting r ( w ) =  X   X  k w k we obtain the following problem: min  X  -regularization results in a zero weight vector under the co ndition that k w t  X   X  condition is rather more stringent for sparsity than the con dition for  X   X  / X  2 -norm as the regularization, as we show in the sequel.
 F quadratic function is a quadratic function and the conjugat e of the  X  we immediately obtain that the dual of the problem in Eq. (8) i s max  X   X  linear time algorithm for finding the optimal  X  to this  X  there shows the optimal solution to Eq. (8) is w solution satisfies  X  = 0 iff k w Mixed norms: We saw above that when using either the  X  obtain an all zeros vector if || w where k is the number of classes, and the predicted class is argmax to the same input feature: we would to zero the row of weights w 1  X  the  X  Given the specific variants of norms described above, the F OBOS update for the  X  regularization, we use the shorthand V = W numerous zero rows. We demonstrate the merits of F OBOS with mixed-norms in Sec. 6. of straightforward. We postpone the proof of the following pro position to Appendix C. problems for t = 1 , . . . , T , For q  X  X  1 , 2 ,  X  X  the vectors w T and w  X  are identical.
 iteration by omitting the terms of w t (or whole rows of the matrix W are outside the support of g f coordinates of w or W . Let  X   X  X  solving w t +1 = argmin can simply cache the last time t when it is needed, solve w t +1 = argmin entire rows so long as the row index corresponds to a zero entr y of the gradient g f sums  X   X  and  X  with an  X  2 function. We thus experimented with both f ( w ) = target values, we set y noise. In all experiments, we used 1000 training examples of dimension 400 . performs in the  X  In the next experiment, whose results are given in Fig. 2, we s olved  X  the  X  k w k specifically for solving  X  objective function (empirical loss plus the  X  learning rate was set to  X   X  regularization whereas F OBOS enjoys a broad range of applicable settings. Mixed-norm experiments: Our experiments with mixed-norm regularization (  X  and we believe could serve as benchmarks for other methods to solve mixed-norm problems. Our experiments compared multiclass classification with  X  example is x input 36 features into 1296 features by taking the product of all features. time (number of single-example gradient calculations) for the  X  stochastic gradient. Each used the same learning rate  X  but for MNIST with 10000 training examples and  X  for longer training times, we do indeed see similar behavior .  X  the MNIST data the  X  of structural sparsity. Moreover, for a given level of struc tural sparsity, the  X  effective only when the set of parameters indeed exhibit nat ural grouping. [1] D.P. Bertsekas. Nonlinear Programming . Athena Scientific, 1999. [11] S. Shalev-Shwartz and A. Tewari. Stochastic methods fo r  X  [15] K. Koh, S.J. Kim, and S. Boyd. An interior-point method f or large-scale  X  [17] R.T. Rockafellar. Convex Analysis . Princeton University Press, 1970.
