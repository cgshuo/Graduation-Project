 Please check http://mlg.eng.cam.ac.uk/andrew/ for updates on code release. A stationary kernel k ( x,x 0 ) is the inverse Fourier transform of its spectral density S ( s ), where  X  = x  X  x 0 .
 First suppose where s, X , X  and  X  = x  X  x 0 are scalars. Substituting (2) into (1), Noting that the spectral density S ( s ) must be symmetric (Rasmussen and Williams, 2006), we let Closely following the above derivation, substituting (11) into (1) gives tor  X  = x  X  x 0 , then the integral in (1) becomes a sum of a product of the one dimensional integrals we encountered to derive (12), from which it follows that Generally, we have had success naively training kernel hyperparameters using conjugate gradients (we use Carl Rasmussen X  X  2010 version of minimize.m ) to maximize the marginal likelihood p ( y |  X  ) of the data y given hypers  X  , having analytically integrated away a zero mean Gaussian process. We have found subtracting an empirical mean from the data prior to training hyperparameters (with conjugate gradients) undesirable, sometimes leading to local optima with lower marginal likelihoods, particularly on small datasets with a rising trend.
 Rasmussen, C. E. and Williams, C. K. (2006). Gaussian processes for Machine Learning . The MIT Press.
