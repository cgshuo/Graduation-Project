 Moreover, none of these methods has been experimented with large datasets. In this paper, we depart from a kernel-based approach, which has a established theoretical foundation, and propose a new novelty detection machine that scales up seamlessly with data.
 new. At its crux, geometry of the normal data is learned from data to define the domain of novelty. One-Class Support Vector Machine (OCSVM) [ 12 ] aims at constructing an optimal hyperplane that separates the origin and the data sam-ples such that the margin, the distance from the origin to the hyperplane, is max-imized. OCSVM is used in case data of only one class is available and we wish to learn the description of this class. In another approach , Support Vector Data Description (SVDD) [ 14 ], the novelty domain of the normal data is defined as an optimal hypersphere in the feature space, which becomes as a set of contours tightly covering the normal data when mapped back the input space.
 ods to linear complexity including Pegasos [ 13 ] (kernel case), SV M Perf [ 10 ]. However, to our best of knowledge, none has attempted at novelty detection problem.
 this paper a new approach for novelty detection. Our approach is a combination of cutting plane method applied to the well-known One-Class Support Vector Machine [ 12 ]  X  we term our model Fast One-Class SVM (FOSVM). We propose two complementary solutions for training and detection. The first is an exact solution that has linear complexity. The number of iterations to reach a very close accuracy, termed as  X   X  precision (defined later in the paper) is O 1  X  , mak-ing it attractive in dealing with large datasets. The second solution employed a sampling strategy that remarkably has constant computational complexity defined relatively to probability of approximation accuracy. In all cases, rigorous proofs are given to the convergence analysis as well as complexity analysis. Most closely to our present work is the Core Vector Machine (CVM) [ 18 ]and its simplified version Ball Vector Machine (BVM) [ 17 ]. CVM is based on the achievement in computational geometry [ 3 ] to reformulate a variation of L2-SVM as a problem of finding minimal enclosing ball (MEB). CVM shares with ours in using the principle of extremity in developing the algorithm. Nonetheless, CVM does not solve directly its optimization problem and does not offer the insight view of cutting plane method as ours.
 to train SV M struct ;the N  X  slack optimization problem with N constrains is converted to an equivalent 1  X  slack optimization problem with 2 N constrains (binary classification); the constrains are subsequently added to the optimization problem. In [ 15 , 16 ], the bundle method is used for solving the regularized risk Algorithm 1. Cutting method to find the  X  -approximate solution. where first n constrains C i (1  X  i  X  n ) are activated. By convenience, we assume that the chosen index i n +1 = n + 1. The satisfaction of condition o n +1  X  X  X   X  X  n where  X   X  (0 , 1) means that the current solution ( w n , X  n ) is that of the relax-ation of the optimization problem in Eq. ( 3 ) while the constrains C i ( i&gt;n )are replaced by its relaxation C i ( i&gt;n ) as follows: the current solution s n =( w n , X  n ) and the feasible set of the full optimiza-tion problem in Eq. ( 1 ) since C n +1 is added to the active constrains set (see Figure 1 ). Furthermore, the dis-tance from the current solution as shown in Eq. ( 4 ) to the chosen hyperplane is maximized so that the current feasible set rapidly approaches the destined one (see Figure 1 ). 3.3 Approximate Hyperplane We present an interpretation of the proposed Algorithm 1 under the framework of approximate hyperplane. Let us start with clarifying the approximate hyperplane notion. Let A  X  [ N ] be a subset of the set including first N positive integer numbers and D A be a subset of the training set D including samples whose indices are in A .
 approximate notion can be interpreted as slack variables of soft model. It is worthwhile to note that the stopping criterion in Algorithm 2 means that  X  N i = n +1 : m ber of iterations independent of the training size N . To simplify the derivation, we assume that isometric kernel, e.g., Gaussian kernel, is used which means that
 X  ( x ) = K ( x, x ) 1 / 2 =1 ,  X  x . Furthermore, let us define w  X  = w  X  = lim and d  X  = d  X  = lim current active set D [ n ] and d  X  is the margin of the general dataset. Theorem 1. Algorithm 2 terminates after at most n 0 iterations where n 0 depends only on  X  and the margin of the general data set d  X  = d  X  . Proof. We sketch the proof as follows. The main task is to prove that: constant which is independent with N such that m n ( x j ) &gt; 1  X   X  for all n  X  n 0 . Theorem 2. The number of iterations in Algorithm 2 is O (1 / X  ) .
 1  X  g ( n )  X  1  X   X   X  2 Now applying the inequality ln(1  X  x )  X  X  X  x,  X  x  X  [0 , 1), we have: Therefore, Pr( -top)  X  1  X  e  X  mn N =1  X  e  X  m . If we wish to have at least 95% probability in approximate accuracy (to the exact solution) with = 5%, we can calculate the number of sampling points m from this bound: Pr( -top)  X  1  X  e  X  m &gt; 0 . 95. With =0 . 05 we have m  X  20 ln 20  X  59. Remarkably, this is a constant complexity w.r.t. to N . We note that the disappearance of N is due to the fact that the accuracy of our approximation to the exact solution is defined in terms of the probability. What remarkable is that with a very tight bound required on a accuracy (e.g., within 95 %), the number of points required to be sampled is remarkably small (e.g., 59 points regardless of the data size N ). In the first set of experiments, we demonstrate that our proposed FOSVM is comparable with both OCSVM and SVDD in terms of the learning capacity but are much faster. OCSVM and SVDD are implemented by using the state-of-the-art LIBSVM solver [ 5 ]. All comparing methods are coded in C# and run on a computer with 4 GB of memory.
 normal class, appointing the remaining classes as the abnormal class, and ran-domly removing the samples from the abnormal class such that the ratio of data in the normal and abnormal classes are 50 : 1. The details of the experimental datasets are given in Table 1 .
 positive samples to train the model and we used all samples of the testing folds for testing. Gaussian kernel, which is given by K ( x, x )= e  X   X  x  X  x The width of kernel  X  was searched in the grid 2  X  15 , 2  X  13 ,..., 2 5 . The trade-off parameter C was varied in the grid 2  X  15 , 2  X  13 ,..., 2 5 .For FOSVM2 ,we set the parameter  X  to 0 . 05 and m = 100 corresponding to the confidence of 99 . 33% for obtaining the top = 5% furthest samples. To measure the accuracy, we applied the measurement given by acc = acc + + acc  X  2 where acc + ,acc  X  are the accuracies on the positive and negative classes, respectively. This measurement is appropriate for ND because it encourages the high accuracies for both two classes.
 diverse set of datasets. As can be seen from Table 2 , FOSVM1 and FOSVM2 are comparable with others in terms of the accuracies but much faster. Particularly, FOSVM2 is superior in its time complexity as expected from our theoretical result. Surprisingly, FOSVM2 also gains the highest accuracies on 9 out of 13 evaluated both the training accuracies and times for each sub-dataset. To ensure the stability of the proposed methods, we ran each case ten times. To explicitly observe the complexities of FOSVM1 and FOSVM 2, we took average of the training times of ten times for each percentage and then plotted them. As seen in Figure 3 (left), the training times of FOSVM2 is increased at first and eventually does slightly change when the training size is sufficiently large. As shown in Figure 3 (right), the training time of FOSVM1 is approximately linear. In this paper, we integrate the formulation of cutting-plane method and the well-known One-Class Support Vector Machine to propose Fast One-Class Support Vector Machine (FOSCM) for novelty detection. We actually propose two com-plementary solutions FOSVM1 and FOSVM2 for training and detection. The first is an exact solution thas has linear complexity. The number of iterations to reach a very close accuracy, termed as  X  -precision is O 1  X  , making it attractive in dealing with large datasets. The second solution employed a sampling strat-egy that remarkably has constant computational complexity defined relative to probability of approximation accuracy. The experiment results indicate that our proposed methods are comparable with OCSVM and SVDD in terms of accu-racy but much faster than them. The speed-up in training time could reach 808 times for some dataset.
 Because of the limit space, in this appendix, we only sketch out the theoretical results used in the paper.
 Lemma 1. The following holds: 2. 1 &gt; w 1  X  w 2  X  ...  X  w n  X  ...  X  w  X  where w  X  = w  X  = lim
