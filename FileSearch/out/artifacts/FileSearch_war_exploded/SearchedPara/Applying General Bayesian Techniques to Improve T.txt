
Free Augmented Naive Bayes (TAN) has shown to be com-
Tree Augmented Naive Bayes (TAN) has shown to be competitive with state-of-the-art machine learning algorithms. In this paper we analyze the TAN induction method proposed in [3]. We have identified three shortcomings where corrections can lead to a more coherent and accurate classifier: 
In the following, we propose solutions that try to over-come these problems. We first introduce bayesian net-works and propose a new approach (the multinomial sampling approach) to the problem of learning bayesian networks in Section 2. In Section 3 we discuss TAN into detail and use the multinomial sampling approach to derive an algorithm for learning maximum likelihood networks that uses a unique coherent probability dis-tribution at every step and competes in accuracy with the one proposed in [3]. In order to overcome problems two and three, we consider bayesian model averaging (BMA) in 4.1; we introduce local bayesian model aver-aging (LBMA) in 4.2 and we make the application of 
LBMA to TAN induction in 4.3. We empirically eval-uate the results of our improvements in Section 5, to finish up pointing some conclusions in Section 6. A longer version of this paper is available [2]. 
Let U = {XI,... ,X,} be a set of discrete random variables. A bayesian network is an annotated directed acyclic graph that encodes a joint probability distribu-tion over U. Formally it is a pair B = (G, 0). G is a DAG whose vertices correspond to the random vari-ables and whose edges represent direct dependencies be-tween the variables. 0 represents the set of parame-ters that quantify the network. It contains a parameter and IIzi of IIx;, where IIx; denotes the set of parents of Xi in G. A bayesian network B defines a unique joint probability distribution over U given by 
The problem of learning a bayesian network has been informally stated as: 
Statement 1 (Friedman et al., 1997) Given a training set D = {us, . . . , UN} of instances of U find the network B that best matches D. 
For classification purposes, the fact of trying to match the data perfectly usually causes overfitting. That is why, in order to use bayesian networks for classification purposes we prefer the following informal statement: 
Statement 2 Given a sample S = (~1,. . . ,UN} of a probability distribution P X  find the network B that best matches P*. 
One of the measures used to learn bayesian networks is the log likelihood: 
This measure has the property that can be decomposed according to the structure of B. Let Counto(X) stand for the number of observations in our sample that ful-fill condition X and FreqD(X) = cOu*~(x). VaZ(Xi) is the set of possible states of the random variable Xi and #VaZ(Xi) the number of different possible states Xi can be in. 
If FreqD(xi, II,;) is strictly positive and defined every-where (i.e. we have at least one observation for each possible pair zi, II,; in our dataset) it is easy to see that LL(BID) is maximized when 
This results allow us to search separately the space of network structures and the space of network parame-ters. 
The dataset D of Statement 1 can be seen as a sample of a probability distribution P X . We can assume that P* is a multinomial distribution with a number of possible states equal to: States(P*) = fi #Val(Xi). Each instance in the dataset is equivalent to the observation of a concrete state as an outcome of a multinomial trial. 
In order to learn a bayesian network we should take two steps: 1. Approximate the distribution P* from the informa-2. Find the bayesian network which better fits P;. 
We need a way to calculate Pi. We adhere to the principle of indiflerence, which says that if we lack better information, we should assign an equal probability to each possible success. Our prior is then a Dirichlet distribution with States(P*) equiprobable possible states. We still have to fix one more parameter, the relevance we are giving to the prior, namely A. Hence: Pi(x1,... ,xn) = From our point of view TAN are the most coherent and best performing enhancement to Naive Bayes up to now. To talk of the classification problem we will use the common notation of for distinguishing between the random variable we want to predict (the class, C) and all the rest (the attributes, Al,. . . , A,). TAN are a restricted family of bayesian networks in which the class variable has no parents and each at-tribute has the class variable and at most one other attribute as parents. The interesting property of this family is that we have an efficient procedure for identi-fying the structure of the network with maximum likeli-hood. In [3] we can find a procedure that builds a TAN BT that maximizes LL(BTID) and has time complex-ity C(n X  X ). The search for the maximum likelihood structure is done using the result of Equation 3. Sur-prisingly, once the structure is found, they empirically noticed that softening the probabilities instead of using exactly the maximum likelihood estimates lead to an improvement in classification accuracy [3]. They take: The explanation given there (summarizing, that the number of observations is not enough to estimate reliably the probabilities) is not theoretically sound, because the results are not assymptotic. A plausible explanation to this phenomenon could be related to the fact (usually disregarded) that the results only hold when FreqD is strictly positive and defined everywhere. From our point of view, the most likely explanation comes from the fact that, by following Statement 1 we are focusing on fitting the data, and not on predicting future events. That is why we propose the usage of the multinomial sampling approach to provide a solution to the problem. In this case, instead of looking for maximum likelihood, we would like to minimize the cross entropy or Kullback-Leibler divergence [6] between the P&lt; and the prob-ability distribution generated by the TAN (following the spirit of Statement 2). Cross entropy is minimized when: where IIx; stands for the set of variables which are not parents of Xi in the network. IIx; does not include Xi. We define SC(Xi) = n #VaZ(Xj). SC(Xi) is the number of different states of the multinomial for which you have to add up the probability in order to calculate 
P;(xi, II,;). From this definition and Equation 6 we get: 
In the case of TAN induction, we have to set three different kind of parameters: Oa;lc,aj , eailc and 8,. In these concrete three cases Equation 7 simplifies to: 
These probabilities estimates are used consistently both in the tree structure induction and in the weight deter-mination. Empirical results are given and explained in 
Section 5. 
The second shortcoming in the TAN induction algo-rithm of [3] is that uncertainty in model selection is ignored. Bayesian modeE averaging (BMA) [5] provides a coherent mechanism for accounting for uncertainty in modelling. 
A coherent approach to solve the classification problem is calculating the probability of each class given the data. If we assume that the data has been generated from a model that is contained in a class of models M, the probability distribution of the class given data, is: where P(CIM,I) is the probability distribution of the class when we know the model M that generated the data, and the value of the attributes for this instance 1. P(MIS) is the probability that M is the model that generated the data given the sample S. 
Equation 9 tells us not to use a single model to classify the data, but instead to use all the models from the class of models, weighting each model prediction by the probability of the model given the sample of data we are analyzing. Using Equation 9 to predict is known as Bayesian Model Averaging or BMA for short. 
BMA produces optimally accurate predictions within the chosen model family from the probability theory point of view. 
In order to use BMA in practice, we need develop Equation 9. It can be expanded as: 
Here P(M) is the prior probability that M is the real model and P(SIM) is the probability that model M generates the data in S. 
In practice, the usage of BMA presents some problems, coming from: l The computational cost of calculating Equation 9. l The difficulty in the specification of P(M), the prior 
In order to handle the first of these problems, we propose LBMA, an heuristic approach to approximate BMA. The idea is similar in spirit to the Occam X  X  
Window method described in [5, 71. To apply LBMA we should have an heuristic h(M, S) such that 
In order to approximate the summation in Equation 9, and given that we have h(M, S), we define our set of interesting models M X  as: y represents a compromise between the prediction accuracy and its computational cost. It should be big enough to make #M X  &lt; #M , but small enough in order for to be accurate approximations. It is interesting to note that maximum likelihood prediction is a concrete case of LBMA where h(M, S) = P(S]M) and y is implicitly set in order for M X  to contain only a model. Once we have the resulting weighted set of models calculated, we can use it to classify by calculating Equation 14 for each class and choosing the one with higher probability. 4.3 Local Bayesian Model Averaging for For this concrete case, our class of models M is 
M = {(G, O)jG E TANStructs, 0 E Params( We perform a first reduction of M by using the results in Equation 8 or the softened method proposed in [3] depending on whether we decide to use the multinomial sampling approach or the ad hoc adjustment proposed in [3]. In any case, we will only average over the struc-tures, fixing the parameters by using the corresponding equation in each case. Our heuristic over structures will be given by the algorithm Construct-TAN, just modify-ing the step where a maximum spanning tree is induced, to generate a set containing the K maximum spanning trees by using Gabow algorithm [4]. In order to cal-culate P X (M), we set a prior over tree structures that assigns the same probability to each possible tree struc-ture (since they can be considered of a similar com-plexity). We also have to provide an implementation for Predict, that is, we have to know how to calculate 
P(C = clM,u). In a TAN: where ui,n(Aj) is adequately set to the value of the parent of Aj in the tree or nothing if Aj does not has parents in it. 
Applying LBMA to TAN induction we are simulta-neously providing a solution for the second and third shortcomings in [3] noticed in the introduction. It is clear that LBMA is an approximation to BMA, and hence focuses on the second point, i. e., taking into account the uncertainty in model selection. The third point was that what should be maximized is the conditional likelihood instead of the likelihood. This shortcoming was already noticed in their paper, where they stated that it was an open question whether good heuristic approaches can be found in order to induce TAN models that maximize conditional likeli-hood. The way LBMA weights the different models is exactly multiplying each model by its conditional likelihood. We are then trusting more those models with a higher conditional likelihood. The problem is far from being solved, but we think this is a good first step. In order to use the algorithm described in Section 4.3, we need to set some parameters. In our experimental setting, we took: We tested three algorithms over 11 datasets from the Irvine repository [l] plus our own credit screening database. To discretize continuous attributes we used equal frequency discretization with 5 intervals. For each dataset and algorithm we evaluated accuracy and LogScore. LogScore gives an idea of how well the classifier is estimating probabilities. For both evaluations we used 10 fold cross validation. 
The error rates appear in Table 1, with the best method for each dataset in cursive. LogScore X  X  appear in Table n The meaning of the column headers are: 
Table 1: Averages and standard deviations of error rates 5.1 Interpretation of the results 
We plot in Figure 1 the percentage of improvement in error rate between using TAN+MS+BMA and STAN, and in Figure 2 the improvement in LogScore. Both figures favor TAN+MS+BMA. Table 2: Averages and standard deviations of LogScore 
Figure 1: Comparison of the error rate of TAN+MS+BMA and STAN 
We have proposed solutions for correcting the three shortcomings we noticed in TAN induction as it is done in [3]. We have introduced the multinomial sampling approach, a new approach to learning bayesian networks that provides a coherent way of estimating probabilities, and have used it to develop a theoretically coherent maximum likelihood TAN induction algorithm. We have introduced local bayesian model averaging and have used it to account for uncertainty in the selection of the model. By using LBMA, we have weighted each model by its conditional likelihood, instead of by its likelihood, thus providing a partial solution to the fact that conditional likelihood and not likelihood is what should be maximized. 
We have provided empirical evidence that shows that in most of the cases the resulting new method provides more accurate predictions and probability estimates. 
Figure 2: Comparison of the LogScore of TAN+MS+BMA and STAN I would like to thank Maria Luisa Barja and Ramon 
Lopez de Mantaras for carefully reviewing the prelim-inary versions of this paper. I would like to specially thank Maria Luisa for accepting an additional workload in order to give me the time to write it. 
PI 
