 While computer scientists are uni quely situated to incorporate privacy protections in the link anal ysis algorithms they construct, most computer scientists are unaware of this opportunity and of ways to think about achieving needed protections. The work presented in this writing introduces a new way for computer scientists to think about providing privacy protection within link analysis and introduces the notion of  X  X rivacy-enhanced linking X  as algorithms that perform link analysis with guarantees of privacy protection modeled after th e Fair Information Practices. In this approach, privacy protection is realized by assessing the validity and interpretation of link analysis results such that inappropriate harm to individuals is provably minimized. Privacy-enhanced technology, privacy, link analysis While law enforcement and counter-terrorism objectives encourage the development of algorithms that learn sensitive information from volumes of dispar ate data left behind as people conduct their daily affairs, the potential for serious harm to innocent individuals evokes grave privacy concerns. More specifically, society has experienced exponential growth in the number and variety of data collected on individuals [6]. This growth has been driven by access to inexpensive computing devices and by the plummeting costs of data storage. The ability to collect more data has impacted policy. Throughout the 1990 X  X  a pattern emerged in American policy in which policymakers responded to many pressing issu es by expanding existing data collections and by starting new data collections on the populace[6]. Having so much readily available data on so many individuals has since ignited a new behavioral pattern in which information collected from various data sources are combined to help solve current issues. For example, following the terrorist events of September 11, 2001 in the Unite d States, government programs sought to gather evidence and intelligence by combining information across various existing data collections [18]. While initial efforts succumbed to privacy concerns [5], on-going efforts to accomplish these goals continue. It is important to note that commercial applications of link analysis are also significant. These include insurance fraud detection, telecommunications network analysis, pharmaceuticals research, and epidemiology. While the focus of this paper is on homeland s ecurity uses, the issues and remedies presented in this pa per are just as relevant to commercial practice though the motivations differ. To achieve the law-enforcement a nd counter-terrorism vision, two main hurdles must be overcome X  X ne is a need for computational methods to combine disparate information accurately and the other is a need to sufficiently address privacy concerns.  X  X ink analysis X  refers to a growing area of computer science that seeks to construct algorithms that learn information from disparate data [2]. Today, a fundamental motivation for link analysis development is law-enfo rcement and counter-terrorism. Example tasks include: name matc hing; link detection and social network analysis; detection and monitoring of intrusion, deception, conspiracy, fraud, and criminal activity; scene identification; person identification; and trend detection [14]. Because of the severity of harm to innocent individuals that can result, such uses immediately evoke privacy concerns (see [5] for an example). Examples of privacy concerns emerging from link analysis for law-enforcement and intelligence purposes include: These concerns are underscored by the government X  X  mammoth power to take away an individual X  X  liberty  X  X .e., to restrict a person X  X  movement and autonomous self-determined behavior, in some cases indefinitely and without legal process [13] [17]. Policymakers and computer scientis ts have previously addressed personal privacy issues in govern ment databases. The dominant policy remedy allows individuals to review and correct personal information. Computer science remedies in other legal settings distort data such that resulting information remains useful while guaranteeing no one can be re-identified. Neither of these approaches may be best for the link analysis setting previously described. A claimed need fo r secrecy may override the policy remedy. An inability to identify beforehand which data elements are the valuable ones that shoul d not be distorted makes prior computer science remedies difficult to consider. Detailed discussion on these issues app ears in the next subsections. Commercial harms to individuals include insurance coverage refusal, loss of credit worthine ss, and denial of employment. Prior to the current era, there was a surge in government collection of information on individuals in the 1970 X  X  made possible by the growing availability of mini-computers. Privacy concerns voiced at that time culminated into a set of principles for privacy protection that have become known as the  X  X air Information Practices. X  These principles form the basis of many policies and practices, most notably, the U.S. Privacy Act of 1974 and the European Union Directive on the Protection of Personal Data (1995). The basic principles are listed in Figure 1. 1 Existence of personal data collections should be public knowledge. 2 Individuals have a right to review and correct their information. 3 The minimum information necessary should be collected, and where 4 Personal data should be accurate and complete and retained only for 5 Data should only be used for the purpose originally intended. 6 Data should be protected by security safeguards against Figure 1. Basic principles of the Fair Information Practices. Some privacy advocates argue that Fair Information Practices should be imposed on informati on learned through link analysis [16]. Opponents argue that the Fair Information Practices are impractical for law-enforcement and counter-terrorism pursuits because potential criminals and terrorists cannot be given the opportunity to alter learned inform ation or change behavior based on the knowledge of what has been learned. Many regulations allow data to be shared beyond the original purpose of its collection, and wit hout further adherence to Fair Information Practices, provided no one whose information is contained in the shared data can be re-identified. Examples include the U.S. medical privacy regulation known as HIPAA and Canadian and European data sharing practices. Data in which the subjects of the data can provably not be re-identified is termed  X  X nonymous data. X  3 Computer scientists have devised methods that guarantee a minimal risk that a subject of the data can be re-identified yet the data remains prac tically useful [9][7]. This is achieved by provably thwarting the ability to reliably link the anonymous information to other in formation that may lead to a re-identification. Therefore, anonymous data cannot be reliably linked to many kinds of data, thereby posing serious limitations on the ability to use anonymized data in link analysis. It has been shown that removi ng explicit identifiers, such as name, address, or Social Secu rity numbers, or replacing them with made-up alternatives (no matter how strong the cryptographic hash) is not sufficient to render the result anonymous [5][2]. One way to provably anonymize data is k-anonymity [5], but a more real-world savvy approach is done using the Privacert Risk Assessment Server [12]. Many other frameworks are possible based on di fferent statistical disclosure control techniques. On the other hand, algorithms emerging under the rubric of  X  X rivacy-pre serving data mining X  have not yet demonstrated their real-world applicability and legal appropriateness in the link analys is settings discussed in this paper. One approach that might be useful when link analysis algorithms are deployed in the real world is Selective Revelation [9], which provides data to a surveillance system with a sliding scale of identifiability, where the level of anonymity matches scientific and evidentiary necessity. During normal operation, surveillance is conducted on sufficiently anonymous data that is provably useful. When sufficient and necessary scientific evidence merits, the system provides increasingly more identifiable data. Under Selective Revelation, human judge s, who make decisions as to whether information will be shared with law-enforcement, are replaced with technology that makes these decisions. The limitation of its use in the li nk analysis setting previously described is that the role of pa rticular data elements must be known beforehand, which is not always practical during the development of algorithms, but may be practical when deployed. One effort that may help is to have the kinds of privacy protections provided in the Fair In formation Practices be realized on results learned from link analysis. Doing so shifts the responsibility of privacy protection to the computer scientists who develop link analysis algorithms and to the experts who deploy them. This is the approach introduced in this paper. In prior work [8], reactions by computer science researchers to privacy issues in their research was characterized by three positions: (1)  X  X echnology trumps privacy; X  (2)  X  X echnology is policy neutral; X  and, (3)  X  X omputer scientists take responsibility. X  In the  X  X echnology trumps privacy X  position, computer science researchers take stock in past accomplishments and computational benefits enjoyed by society, thereby relying on a belief that if society is forced to choose, it will choose advancements in computer technology over privacy . Warnings against this position caution that unforeseen dangers could be unleashed forever or the technology never deployed. In the  X  X echnology is policy neutral X  position, computer science researchers do not contemplate any privacy or social implications that may be inherent in the construction or existence of the technology they seek to build. In stead, these comput er scientists want to pursue their research, leaving any related privacy issues to social scientists, policy makers , lawyers, and others. But some argue that such positions are themselves human value decisions, and computer science research ers cannot escape making them. In the  X  X omputer scientists take responsibility X  position, computer scientists take the initiative to in corporate privacy into their own constructions. Some believe assuming such responsibility is a necessary condition to insure viability of their technology. For those computer scientists, the ne xt sections provide methods for incorporating privacy protections based on Fair Information Practices into newly constructed link analysis algorithms. For a link analysis algorithm to be put into practice in the settings previously described, the devel opers and/or those deploying the algorithm should provide a guarantee related to the utility of the algorithm (a  X  X arranty X ) and a guarantee of privacy protections the algorithm provides (a  X  X rivacy statement X ). The link analysis algorithm along with these accompanying guarantees describe the appropriate use of the algorithm; together, the algorithm and its guarantees are introduced as a  X  X rivacy-enhanced linking X  solution. These are further desc ribed in the next subsections. The term privacy-enhanced technology ( X  X ET X ) has historically been used to generally refer to a technology that performs a task while providing privacy protection [11]. This writing extends the notion of PETs to privacy-enhanced linking (PEL) by dictating that one or more of the Fair Information Practices must be addressed within the link analysis algorithm and/or within the setting in which the algorithm is e xpected to execute. In PEL, two guarantees accompany the algorithm  X  X  warranty statement and a privacy statement, as further described below. 4 A PEL warranty statement addresses the quality of the algorithm as being suitable for, or adaptable to, a particular set of tasks. Computer scientists typically provide proofs of correctness and complexity when introducing an algorithm. These help characterize the utility that may be realized if the algorithm is put into actual practice, and therefor e these will typically form the basis of a warranty statement for a link analysis algorithm. Because it is believed that individuals cannot participate in link analysis settings sufficient to exercise Fair Information Practices, the onus of providing those protections shifts to the technology and is quantified and expressed in the PEL privacy statement. Given a link analysis algorithm deemed appropriate for a particular setting, a PEL  X  X  rivacy assessment X  involves determining which Fair Informa tion Practices are relevant and quantifying and characterizing algorithmic performance in terms of the protection provided. The results of the privacy assessment forms the basis for the PEL privacy statement. The first principle found in the Fair Information Practices listed in Figure 1 may be beyond the scope of what can be accomplished by technical remedy, but the othe r principles, depending in part on the nature of the link analysis program used, can be realized by technology. A PEL privacy statement reports on the validity and interpretation of computed resu lts as they relate to these principles. Here is an example. Government authorities have an explicit list of names of known or suspected terrorists (a  X  X atchlist X ) they want to locate or merely track among the general population. There are vast numbers of locations the government seeks to query as to whether a person has appeared bearing the same explicit identity as one on the Watchlist. The idea is to revi ew transactional data (store purchases, hotel registrations, airp lane manifests, car rentals, school attendance records, etc) and match names to those on the watchlist. The problem is further complicated by the use of nicknames and misspellings [10]. PEL is modeled after a new res earch paradigm (termed  X  X nified computing X ) for constructing technology that is provably appropriate for a given setti ng. The developer provides warranty and compliance statements that show that the resulting technology remains useful while being compliant to the stated standard. (See privacy.cs.cmu.edu/dataprivacy/projects/unified/ index.html for more information). The principles of the Fair Information Practices that seem particular relevant are 2 and 4 in Figure 1. Because subjects of the data cannot review results learned from matching, it becomes extremely important that false positives (names of different people are incorrectly matched together) be rare. Preference should also be given to verified source information (e.g., from a credit card, driver X  X  license) ove r casually acquired information. The current solution involves the simple approach of matching names using soundex, which is a gross hash function in which spellings that may look or sound similar are hashed together [1]. Using soundex [4], the names  X  X ames X  and  X  X ohn X  are hashed to J52 and J5, respectively, but the names  X  X ohn, X   X  X ane X  and  X  X ean X  are all hashed to the same  X  X 5 X  value. An accompanying PEL privacy statement would either include results of matching soundex name s in a general population to report the false positives found or describe tests that should be conducted to determine whether the false positive rate for a given population is at an acceptable threshold. By any reasonable standard on mo st large populations of names, soundex matching is not appropriate for this task, because it lumps too many different names t ogether (see [1] for an example). Producing a PEL privacy statement re vealed its inappropriateness. Notice however, that the false negative rate (names for the same person incorrectly not matched together) is likely to be low (which is good), but this perfo rmance measure relates to the warranty, not the privacy statement. An experiment was conducted to demonstrate the kinds of measurements that are likely to appear in PEL privacy assessments. The experiment i nvolved automatically constructing a dossier on a given subject from information appearing on web pages indexed by Google. Inform ation related to the subject was compiled into a single extended v ita using semi-automated text extraction [12]. Human review was then conducted to assess the kinds of errors found. The subject was Raj Reddy, a dis tinguished computer scientist and a Turing Award recipient. Entering  X  X aj Reddy X  into Google generated 372,000 hits. The first 14, 000 text pages were selected and the information surrounding the occurrence of his name was extracted and catalogued. Human review of the material was conducted. A few highlights s howing ways linking can go wrong from a Fair Information Practices perspective are provided below. There is a Raj Reddy, who is a reporter. Information associated with the reporter X  X  activities were confused with those of Raj Reddy, the computer scientist. There were also situations in which  X  X eddy X  did refer to Raj and other cases where it did not. There were a few cases referring to Helen Reddy, the singer. The only significant financial contribution found on-line was a $8000 gift to an organization. This experience provided a false over-emphasis of his enthusiasm towards this organization because his actual giving includes numerous gifts of larger amounts that were not listed on the web in any obvious manner. There was more than 100 press ar ticles that included his name, but many of them were simply va riations (modified repeats) of fewer original articles. In fact the article having the most variation was neither the most insightful nor useful in learning facts about him. This experience warns that data may not reflect independent events. Among the newspaper articles was one in which there was a quotation attributed to Raj harshly criticizing a computer company. Raj never knew of the existence of the article previously, and further he patently denies ever having made the comments attributed to him. Th is experience underscores a need to handle conflicts, assuming that in the absence of Raj X  X  verbal input, the conflict would have been identified. In conclusion, PEL provides a way for society to enjoy the benefits of link analysis while minimizing harm to individuals. The PEL privacy statement (a scientific assessment of the validity of results and of the appropriate use of the technology) does not actually provide privacy 5 , but is consistent with minimizing the same kinds of harms as do the Fair Information Practices. Support from the link analysis community is necessary if the potential of PEL is to be realized . First, publishing channels for the development of link analysis algorithms should include parts of papers or papers themselv es that contain PEL privacy assessments, even if those asse ssments are critical or expose weaknesses in link analysis algorithms. Second, the nature of PEL priv acy assessments involves activities (e.g. testing the function on real-wor ld data sets) that lie outside the kind of information normally included in computer science presentations of algorithms. These may rely on different scientific research methods (n aturalistic observation, survey, interview, and experimentation) than traditional computer science research. Third, computer scientists tend to exalt one algorithm over another if it solves more tasks. But PEL solutions are optimizations in which maximum utility is achieved while providing as much privacy protection as possible. PEL solutions modeling all relevant Fair Inform ation Practices, while remaining useful, is most preferred. Special thanks to Lise Getoor and Chris Diehl for the opportunity to provide this paper. Much gratitude goes to Raj Reddy for recommending the experiment and for his willingness to be a consenting subject. This work was funded in part by a grant from the Intel Corporation. Additional support came from the Data Privacy Lab in the School of Computer Science at Carnegie Mellon University. [1] Goo, S.  X  X aulty  X  X o-Fly X  System Detailed, X  Washington Data anonymity does offer privacy, but link analysis algorithms many not work with anonymized data. [2] Jensen, D. and Goldberg, H. (eds). Artificial Intelligence and [3] Malin, B. and Sweeney, L. How (Not) to Protect Genomic [4] Russell, R. Soundex. U.S. Pa tent 1,261,167 April 2, 1918. [5] Safire, W.  X  X ear DARPA Diary,  X  New York Times, June 5, [6] Sweeney, L. Information Explosion. Confidentiality, [7] Sweeney, L. k-anonymity: a m odel for protecting privacy. [8] Sweeney, L. Navigating Computer Science Research [9] Sweeney, L. Privacy-Preserving Surveillance using [10] Sweeney, L. Towards a Privacy-Preserving Watchlist. AAAI [11] 6 th Workshop on Privacy-Enhancing Technologies. [12] The AutoVita Project. (privacy.cs.cmu.edu/dataprivacy/ [13]  X  X ourt Orders Due Process for Guantanamo Prisoners. X  [14] International Conference on Artificial Intelligence and Law. [15] Privacert Risk Assessment Server. www.privacert.com [16] Privacy Rules for Access to Personal Data. Center for [17] U.S.A. Patriot Act, HR 3162. October 2001. thomas [18]  X  X our Papers Please, X  Worl d Net Daily, Jan 16, 2003; www 
