 drawn from some domain (the source domain), but then needs to perform the same task on a related depend on both the performance in the source domain and the si milarity between the two domains. This situation arises often in machine learning. For exampl e, we might want to adapt for a new domain), under the assumption that users generally agree on what is spam and what is not. Then, the the source domain will do on the target domain.
 of document types and genres in a given language because of av ailability, cost, and project goals. language under consideration.
 generalization error and the many experimental evaluation s of learning methods. However, the as-approach this challenge by investigating how a common repre sentation between the two domains can make the two domains appear to have similar distribution s, enabling effective domain adapta-and it shows that a representation function should be design ed to minimize domain divergence, as well as classifier error.
 While many authors have analyzed adaptation from multiple se ts of labeled training data [3, 5, 7, plentiful unlabeled data exists for both target and source d omains. As we suggested above, this learning. Indeed recent empirical work in natural language processing [11, 6] has been targeted at exactly this setting.
 We show experimentally that the heuristic choices made by th e recently proposed structural corre-off between source-target similarity and source training e rror.
 adaptation. Section 3 gives our main theoretical results. W e discuss how to compute the bound in section 4. Section 5 shows how the bound behaves for the str uctural correspondence learning domain adaptation based on our theory, in section 6 and concl ude in section 7. 1 . A representation function R is a function which maps instances to features R : X X  X  . A follows:  X  D is the probability of the inverse image of B under R according to D , and the probability that the setting is defined by fixed but unknown D and f , and our choice of representation function R and hypothesis class H X  X  g : Z X  X  0 , 1 }} of deterministic hypotheses to be used to approximate the function f . 2.1 Domain Adaptation We now formalize the problem of domain adaptation . A domain is a distribution D on the instance task in multiple domains. This is quite common in natural lan guage processing, where we might be performing the same syntactic analysis task, such as taggin g or parsing, but on domains with very different vocabularies [6, 11]. We assume two domains, a source domain and a target domain. We denote by D distribution of instances and  X  D notation, D domains, and  X  f is the induced image of f under R .
 the distribution D Similarly,  X  We now proceed to develop a bound on the target domain general ization performance of a classifier term bounds the performance of the classifier on the source domain. The second term is a measure of the divergence between the induced source marginal  X  D natural measure of divergence for distributions is the L where B is the set of measureable subsets under D and D  X  . Unfortunately the variational distance useful to us when investigating representations for domain adaptation on real-world data. A key part of our theory is the observation that in many realis tic domain adaptation scenarios, we do not need such a powerful measure as variational distance. Instead we can restrict our notion of domain distance to be measured only with respect to function in our hypothesis class. 3.1 The A -distance and labeling function complexity A -distance between such distributions is defined as respect to distributions  X  D A function  X  f is  X  -close to H when there is a single hypothesis h  X  X  which performs well on both domains. This embodies our domain adaptation assumption, a nd we will assume will assume that our induced labeling function  X  f is  X  -close to our hypothesis class H for a small  X  . sumption on labeling function complexity. If H has bounded capacity (e.g., a finite VC-dimension), data is generated by some D target domain, T , then one can construct a function which agrees with some h  X  X  with respect to  X 
D S and yet is far from H with respect to  X  D T . Nonetheless we believe that such examples do both the source and target domains. 3.2 Bound on the target domain error let Z In a slight abuse of notation, for a binary function class H we will write d A -distance on the class of subsets whose characteristic func tions are functions in H . Now we can state our main theoretical result.
 Theorem 1 Let R be a fixed representation function from X to Z and H be a hypothesis space of VC-dimension d . If a random labeled sample of size m is generated by applying R to a D sample labeled according to f , then with probability at least 1  X   X  , for every h  X  X  : where e is the base of the natural logarithm.
 Proof: Let h  X  = argmin to
D T and D S respectively. Notice that  X  =  X  T +  X  S . The theorem now follows by a standard application Vapnik-Ch ervonenkis theory [14] to bound the true  X  probability exceeding 1  X   X  , because we can measure this from finite samples from the distr butions  X  D 1 with theorem 3.2 from [9], we can state a computable bound fo r the error on the target domain. Theorem 2 Let R be a fixed representation function from X to Z and H be a hypothesis space of VC-dimension d .
 If a random labeled sample of size m is generated by applying R to a D according to f , and  X  U tively, then with probability at least 1  X   X  (over the choice of the samples), for every h  X  X  :  X  T ( h )  X   X   X  S ( h ) + Let us briefly examine the bound from theorem 2, with an eye tow ard feature representations, R . Under the assumption of subsection 3.1, we assume that  X  is small for reasonable R . Thus the two domains for hypothesis class H . Looking at the two terms, we see that a good representation R is one which achieves low values for both training error and dom ain A -distance simultaneously. set A h . Then h is the classifier which achieves minimum error on the binary c lassification problem of discriminating between points generated by the two distrib utions.
 To see this, suppose we have two samples  X  U distributions as where I forward to show that Unfortunately it is a known NP-hard problem even to approxim ate the error of the optimal hyper-plane classifier for arbitrary distributions [4]. We choose to approximate the optimal hyperplane In the subsequent experiments section, we train a linear cla ssifier to discriminate between points sampled from different domains to illustrate a proxy for the A -distance. We minimize a modified Huber loss using stochastic gradient descent, described mo re completely in [15]. the bound and all of them have the same flavor. First, we choose a representation R . Then we train a classifier using R and measure the different terms of the bound. As we shall see, represenations which minimize both relevant terms of the bound also have sma ll empirical error.
 For instance, in the previous sentence we would the tag for  X  X  peech X  is singular common noun , the tag for  X  X abeling X  is gerund , and so on. PoS tagging is a common preprocessing step in many empirically investigate methods for adpating a part of spee ch tagger from financial news (the Wall Street Journal, henceforth also WSJ) to biomedical abstract s (MEDLINE) [6]. We have obtained for which we have no labeled training data.
 representation and classify in the d -dimensional projected space. 5.1 Random Projections The entries of P are drawn i.i.d. from N (0 , 1) . The Johnson-Lindenstrauss lemma [8] guarantees Figure 1: 2D plots of SCL representations for the (a) A -distance and (b) empirical risk parts of theorem 2 that random projections approximate well distances in the o riginal high dimensional space, as long as d is sufficiently large. Arriaga and Vempala [1] show that one c an achieve good prediction with random projections as long as the margin is sufficiently larg e. 5.2 Structural Correspondence Learning spondence learning (henceforth also SCL). SCL uses unlabel ed data from both domains to induce independent  X  X ivot X  features which occur frequently in the unlabeled data of both domains. Other nally they use a low-rank approximation to the co-occurence count matrix as a projection matrix P . 5.3 Results We use as our source data set 100 sentences (about 2500 words) of PoS-tagged Wall Street Journal from each domain) of unlabeled data to estimate the A -distance between the financial and biomedi-cal domains.
 are empirical and appear in 2(b). In this case we use the Huber loss as a proxy from the empirical training error.
 Figure 1(a) shows one hundred random instances projected on to the space spanned by the best two discriminating projections from the SCL projection matrix for part of the financial and biomedical dataset. Instances from the WSJ are depicted as filled red squa res, whereas those from MEDLINE poorly the A -distance is low. On the other hand, figure 1(b) shows the best two discriminating components for the task of discriminating between nouns and verbs. Note that in this case, a good discriminating divider is easy to find, even in such a low-dim ensional space. Thus these pictures error and small A -distance. In this case theorem 2 predicts good performance . (a) Plot of random projections repre-sentation for financial (squares) vs. biomedical (circles) Figure 2: (a) 2D plot of random projection representation and (b) results summary on large data Figure 2(a) shows one hundred random instances projected on to the best two discriminating pro-jections for WSJ vs. MEDLINE from a random matrix of 200 projec tions. This also seems to be poorly, since it minimizes only the A -distance and not the empirical error.
 Figure 2(b) gives results on a large training and test set sho wing how the value of the bound can maximum possible value.
 The random projections method achieves low A -distance but high Huber loss, and the classifier Huber loss and low A -distance, and the error rate is the lowest of the three repre sentations. Our theory demonstrates an important tradeoff inherent in d esigning good representations for do-main adaptation. A good representation enables achieving l ow error rate on the source domain while also minimizing the A -distance between the induced marginal distributions of th e two domains. The previous section demonstrates empirically that the heuris tic choices of the SCL algorithm [6] do achieve low values for each of these terms.
 Our theory is closely related to theory by Sugiyama and Muell er on covariate shift in regression models [12]. Like this work, they consider the case where the prediction functions are identical, unlabeled samples.
 the accuracy of our approximate H -distance. This is an important area of exploration for futu re work. Finally our theory points toward an interesting new di rection for domain adapation. Rather a representation which directly minimizes a combination of the terms in theorem 2. If we learn mappings from some parametric family (linear projections, for example), we can give a bound on and we are also investigating theory and algorithms for this . tradeoff between low empirical source domain error and a sma ll difference between distributions. distance of Kifer et al [9]. Computing the A -distance is equivalent to finding the minimum-error reasonable approximation to the A -distance.
 Our experiments indicate that the heuristic structural cor respondence learning method [6] does in fact simultaneously achieve low A -distance as well as a low margin-based loss. This provides a an interesting new algorithm for domain adaptation. Instea d of making heuristic choices, we are training margin.
 [1] R. Arriaga and S. Vempala. An algorithmic theory of learn ing robust concepts and random [3] J. Baxter. Learning internal representations. In COLT  X 95: Proceedings of the eighth annual [4] S. Ben-David, N. Eiron, and P. Long. On the difficulty of ap proximately maximizing agree-[7] K. Crammer, M. Kearns, and J. Wortman. Learning from data of variable quality. In Neural [8] W. Johnson and J. Lindenstrauss. Extension of lipschitz mappings to hilbert space. Contem-[10] C. Manning. Foundations of Statistical Natural Language Processing . MIT Press, Boston, [11] D. McClosky, E. Charniak, and M. Johnson. Reranking and self-training for parser adaptation. [13] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Sharing clusters among related groups: [14] V. Vapnik. Statistical Learning Theory . John Wiley, New York, 1998.
