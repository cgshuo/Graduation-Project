 This paper is concerned with top-k ranking problem, which reflects the fact that people pay more attention to the top ranked objects in real ranking application like information retrieval. A popular approach to top-k ranking problem is based on probabilistic models, such as Luce model and Mallows model. However, whether the sequential generative process described in these models is a suitable way for top-k ranking remains a question. According to the riffled inde-pendence factorization proposed in recent literature, which is a natural structural assumption on top-k ranking, we pro-pose a new generative process of top-k ranking data. Our approach decomposes distributions over the top-k ranking into two layers: the first layer describes the relative order-ing between the top k objects and the rest n  X  k objects, and the second layer describes the full ordering on the top k objects. On this basis, we propose a new probabilistic model for top-k ranking problem, called hierarchical order-ing model . Specifically, we use three different probabilistic models to describe different generative processes of the first layer, and Luce model to describe the sequential generative process of the second layer, thus we obtain three different specific hierarchical ordering models. We also conduct ex-tensive experiments on benchmark datasets to show that our proposed models can outperform previous models sig-nificantly.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithm, Performance, Experimentation, Theory Learning to Rank, Top-k, Ranking Model
Ranking is a central problem in a number of machine learning applications, such as information retrieval, recom-mendation system and computational advertising etc. In these real applications, people pay more attention to the top ranked objects, which means that if two ranked lists of ob-jects have the same ranking results for the top k positions, they will likely provide the same experience to the users. This characteristic of ranking problem has been explored in earlier studies in different setting [1, 9, 10, 7], which is referred to as the top-k ranking problem.

Many probabilistic models on permutation have been widely applied to solve the ranking problem, among which, the Luce model [11] and the Mallows Model [5] are the most popular ones. Xia et al. [10] propose Top-k ListMLE to model the top-k ranking data, which can be viewed as an extension of the Luce model to the top-k setting; Qin et al. [8] propose a coset-permutation distance based stagewise (CPS) model to inherits the advantages of both the Luce model and the Mal-lows model, which can also be extended to the top-k setting. Both generative processes of the above methods are sequen-tial. At the i -th stage, an object is selected and assigned to position i with a certain probability. We refer to this approach as sequential generative process. Although these methods have been proven effective empirically, the rational-ity of the sequential generative process remains a question because it is heuristic and lacks theoretical support.
Riffled independence, which was introduced in recent lit-erature by Huang et al. [2], is a generalized notion of prob-abilistic independence for ranked data. It corresponds to ranking disjoint sets of objects independently, then inter-leaving those rankings, which has been proven to be an ap-propriate structural assumption on the top-k ranking data theoretically [3, 4]. Inspired by the assumption of riffled in-dependence, we argue that a more appropriate generative process of the top-k ranking data should be comprised of two layers: in the first layer, the relative ordering between the top k objects and the rest n  X  k objectsisgenerated,then in the second layer, the full ordering on the top k objects is further generated.

On the basis of the aforementioned hierarchical genera-tive process, we propose a new probabilistic model for top-k ranking problem, which is called hierarchical ordering model (HOM for short). In HOM, the distribution of a top-k rank-ing is decomposed to the product of two terms, one term is the distribution of the relative ordering in the first layer and the other is the distribution of the full ordering in the second layer. Specifically, we introduce three different prob-ability distributions to model the relative ordering in the first layer, and use Luce model to describe the full order-ing in the second layer. Combining the two-layer proba-bility distributions, we obtain three different specific hier-archical ordering models. To evaluate the performances of our proposed models, we conduct extensive experiments on benchmark datasets. Our experimental results show that our models consistently outperform previous top-k ranking models, such as Top-k ListMLE [10] and Top-k CPS [8].
In this section, we introduce some backgrounds of top-k ranking problem, including some notations, reviews on existing top-k probabilistic ranking models and riffled inde-pendence assumption.
Let D = { x 1 ,  X  X  X  ,x n } be a set of objects to be ranked, where n is the number of objects. We use  X  to denote a top-k ranking on D , which is comprised of a full ordering of top k objects and the relative ordering of top k objects and the rest n  X  k objects. Let T be the set of top k objects and F be the set of the rest n  X  k objects. The full ordering of T is denoted by  X  T .Weuse f to represent the ranking function to be learned. For evaluation, we relate the top-k ranking to relevance labels by defining y = { y 1 ,  X  X  X  ,y position-aware relevance labels of the corresponding objects. Asdidin[7], y j is defined as k +1  X   X  T ( x j )if x j  X  T ,and y =0otherwise.
Many probabilistic models on permutation have been widely applied to solve the ranking problem, such as the Luce model [11] and the Mallows Model [5]. Here we review two recently proposed probabilistic models based on them, one of which is directly a top-k ranking model and the other can be easily extended to the top-k ranking scenario.

Xia et al. [10] proposed Top-k ListMLE to model the top-k ranking data, which can be viewed as an extension of the Luce model to the top-k setting. In Top-k ListMLE, the probability of top-k ranking is formulated as below:
P (  X  )= Qin et al. [8] introduced a coset-permutation distance based stagewise (CPS) model for rank aggregation, which inherits the advantages of both the Luce model and the Mallows model. It can also be applied to the setting of ranking by treating each feature as a weak ranker. As CPS organizes the probability in the form of Luce model, it can be eas-ily extended to top-k ranking by similar technique as used in Top-k ListMLE. Thus we obtain Top-k CPS as in the following formulation:
P (  X  )= where M is the number of features,  X  m suggests the weight of feature h m and  X  ( h m ) denotes the permutation generated by the ranking function h m .  X  in the right side can be directly viewed as a right coset as defined in [8], and S n  X  i (  X  denotes the right coset including all permutations that rank sitions, respectively. The Kendall X  X  tau coset-permutation distance d (  X  ,  X  ) defined in [8] is adopted in this paper.
Riffled independence defines a novel class of independence structure on ranking [2], which corresponds to ranking dis-joint sets of objects independently, then interleaving those rankings. The formal definition is as follows.

Definition 1. Given a ranking  X   X  S D , S D is the collec-tion of all possible full rankings of object set D , a partition of D into disjoint sets A and B ,let p be a distribution over S
D ,thesets A and B are said to be riffle independent if p decomposes as: where interleaving distribution m AB is defined over inter-leaving of A and B , distributions f A and g B are defined over relative rankings  X  A and  X  B ,respectively. In the top-k ranking scenario, riffled independence indicates that the distribution over top-k ranking has a natural de-composition, which provides us a new perspective to look at top-k ranking.
In this section, we will discuss the generative process of the top-k ranking data. First, we can see that both Top-k ListMLE [10] and Top-k CPS [8] take a sequential view of top-k ranking data. However, whether it is a good choice remains a question. Inspired by the assumption of riffled in-dependence [2], we propose a new hierarchical view of top-k ranking data, which naturally captures the structural char-acteristics of the top-k ranking data.
Figure 1: Different generative processes for top-3 ranking.
From Section 2.2, we can see that both Top-k ListMLE [10] and Top-k CPS [8] are stagewise models, which decom-pose the process of generating the top-k ranking data into k sequential stages. At the i -th stage, an object is selected and assigned to position i according to a probability dis-tribution. For Top-k ListMLE, the probability is computed based on the scores of the unassigned objects by Luce Model. While for Top-k CPS, the probability is based on the dis-tance between a location permutation and the right coset of the Top-k ranking by Mallows Model. Therefore, both generative processes of Top-k ListMLE and Top-k CPS are sequential, referred to as sequential generative process.
A typical sequential generative process on top-k ranking is like this. Given 8 objects { A, B, C, D, E, F, G, H } , the top-3 ranking G | F | A |{ B,C,D,E,H } is generated as follows: first, we generate the object to be ranked at the 1-st position, and we get G ; second, we generate the object to be ranked at the 2-nd position, and we get F ; third, we generate the object to be ranked at the 3-rd position, and we get A . You will find a sketch map of the above generative process in Figure 1.
Although the sequential view of top-k ranking leads to ef-ficient computing, the sequential generative process remains heuristic and lacks theoretical support. We argue that it is not the unique generative process and there is a more appropriate way to look at top-k ranking data.
According to the definition of riffled independence in Sec-tion 2.3, the distribution over the top-k ranking  X  can be decomposed 1 into the interleaving distribution of T and F and the distribution over ranking  X  T . Inspired by the above idea, we propose a new two-layer generative process of top-k ranking, called hierarchical generative process. In the first layer, the relative ordering T  X  F indicating all the k ob-jects in T are ranked before all the other n  X  k objects in F is generated; in the second layer, the full ordering  X  T on all the k objects in T is generated.

Take the aforementioned example of { A, B, C, D, E, F, G, H in Section 3.1, the top-3 ranking G | F | A |{ B,C,D,E,H } generated as follows: first, we generate 3 objects which are ranked higher than the other 5 objects, and we get { A, F, G second, we generate the full ordering of { A, F, G } ,andwe get G | F | A . The above generative process is also depicted in Figure 1.

Compared to the sequential generative process, we can see that our hierarchical generative process can well capture the structural characteristics of top-k ranking. Furthermore, our hierarchical generative process inherits the empirical advan-tages of riffled independence and enjoys theoretical guaran-tees as in previous work on riffled independence [3, 4].
According to the hierarchical generative process of top-k ranking as described in Section 3, we propose a new prob-abilistic model for top-k ranking problem, which is called hierarchical ordering model, HOM for short.

For each set of objects { x 1 ,  X  X  X  ,x n } ,let P TF be the prob-ability distribution of the relative ordering in the first layer of the hierarchical generative process, and P T be the proba-bility distribution of the full ordering in the second layer of the hierarchical generative process. HOM defines the prob-ability of a top-k ranking  X  as the following form: It is easy to verify that the probability defined in HOM naturally forms a distribution. That is, for every possible top-k ranking  X  , we always have P (  X  )  X  0, and  X  P (  X  )= 1.

The above hierarchical ordering model HOM is in fact a general framework. We can obtain different specific forms of HOM by defining different forms of distributions on relative ordering T  X  F and full ordering  X  T ,thatis, P TF and P T We discuss them separately and give three examples of HOM in this paper.
In top-k ranking problem, the ordering among the bottom n  X  k objects do not matter. Thus, we can view the distri-bution over rankings of F as uniform, which can be ignored in the decomposition. For the probability distribution of the relative ordering T  X  F , we can model it in the following three different ways. (1) Taking T as a whole, the probability of T  X  F can be viewed as the energy proportion of T to the whole object set, where the energy of an object set can be computed by adding the scores of all the objects in the set. This approach is like the competition between groups, which is called Group-to-Group. The formulation is as follows: (2) Considering the relationship of each object in T and that in F , the probability of T  X  F canbeviewedastheevent that each object in T is ranked higher than that in F .This approach is like the competition between objects, which is called One-to-One and can be well formulated by the widely used Bradley-Terry model [6]. The formulation is as follows:
P TF ( T  X  F )  X  (3) Considering the relationship of each object in T and the whole F , the probability of T  X  F can be viewed as a multi-step event that for each step an object in T is ranked higher than F . This approach is like the competition between each object and a whole group, which is called One-to-Group and can be well formulated by the widely used Plackett-Luce model [6]. The formulation is as follows: P
TF ( T  X  F )  X 
For the probability distribution of the full ordering in T , it can be computed by some widely used probabilistic mod-els on permutation in previous work, such as Luce model [11] and Mallows model [5]. Considering computational effi-ciency, here we use Luce model as an example in this paper with the following formulation: If the probabilities distribution of P T is fixed to Eq.(8), and P
TF takes the form of Eq.(5), Eq.(6) and Eq.(7), we will call the corresponding HOM as HOM-GG, HOM-OO and HOM-OG, respectively.
In this section, we compare the empirical performance of our proposed three kinds of hierarchical ordering models with state-of-the-art top-k ranking methods on benchmark datasets, including Top-k ListMLE and Top-k CPS. Note that k issetto10inourexperiments.
We use the benchmark LETOR4.0 collection 2 in the ex-periments. There are two datasets containing permutation-level ground-truth in LET OR4.0, MQ2007-list and MQ2008-http://research.microsoft.com/en-us/um/beijing/projects/letor// list. Considering the computational cost of Top-k CPS, we only use top 100 documents for each query in the experi-ments. We construct the top-10 ground-truth by only pre-serving the total order of top 10 documents on each query. We obtain two datasets containing top-10 ground-truth, re-ferred to as top-10 MQ2007 and top-10 MQ2008.

All the models including baselines and our methods, are optimized by gradient descent method. Parameters like the learning rate and stopping precision are tuned on the vali-dation set in each trial of the 5-fold cross validation method.  X  -NDCG and  X  -ERR [7] are employed to measure the effec-tiveness of ranking performance, which are natural general-izations of NDCG and ERR to the top-k ranking scenario.
The performances of different top-k ranking models on these two datasets are shown in Table 1, where  X  -N @ j stands for  X  -NDCG @ j . From the results, we can find that the proposed three kinds of HOMs can consistently and sig-nificantly outperform Top-k ListMLE and Top-k CPS in terms of both  X  -NDCG and  X  -ERR. For example, consider-ing HOM-OG on Top-10 MQ2007, the relative performance improvements over top-k ListMLE, top-k CPS are 4 . 13%, 15 . 5% in terms of  X  -NDCG@10 and 3 . 85%, 15 . 0% in terms of  X  -ERR, respectively. Another example is HOM-OG model on Top-10 MQ2008, the relative performance improvements over Top-k ListMLE, Top-k CPS are 8 . 59%, 16 . 2% in terms of  X  -NDCG@10 and 7 . 46%, 11 . 3% in terms of  X  -ERR, re-spectively. Furthermore, we can see that the proposed three kinds of HOMs reach the best performances (denoted by numbers in bold) under all metrics on the two datasets.
The results indicate that our hierarchical ordering model is more effective than models based on the sequential genera-tive process(i.e. Top-k ListMLE and Top-k CPS). It in some sense reflects the hierarchical generative process is superior to the sequential generative process for top-k ranking.
In this paper, we argue that the sequential generative process of top-k ranking adopted in many previous ranking models is not unique. Such a process is heuristic and lacks theoretical support. On the basis of the recently proposed riffled independence assumption of top-k ranking, we pro-pose a new hierarchical generative process, which is a more appropriate way to look at top-k ranking data. According to the generative process, we propose a hierarchical order-ing model, which is a new probabilistic model for the top-k ranking problem. We conducted extensive experiments on two benchmark datasets, and experimental results show that the proposed models can outperform previous top-k proba-bilistic ranking models significantly. This research work was funded by the National Natural Science Foundati on of China unde r Grant No. 60933005, No. 61173008, No. 61003166 and 973 Program of China under Grants N o. 2012CB316303. [1] S. Cl  X  emen  X  con and N. Vayatis. Ranking the best [2] J. Huang and C. Guestrin. Riffled independence for [3] J. Huang and C. Guestrin. Learning hierarchical riffle [4] J. Huang, A. Kapoor, and C. Guestrin. Efficient [5] G. Lebanon and J. D. Lafferty. Cranking: Combining [6] J.I.Marden. Analyzing and Modeling Rank Data . [7] S.Niu,J.Guo,Y.Lan,andX.Cheng.Top-klearning [8] T. Qin, X. Geng, and T.-Y. Liu. A new probabilistic [9] C. Rudin. Ranking with a p-norm push. COLT X 06, [10] F. Xia, T.-Y. Liu, and H. Li. Statistical consistency of [11] F. Xia, T.-Y. Liu, J. Wang, W. Zhang, and H. Li.
