 Distance function computation is a key subtask in man y data mining algorithms and applications. The most e ec-tive form of the distance function can only be expressed in the con text of a particular data domain. It is also often a challenging and non-trivial task to nd the most e ectiv e form of the distance function. For example, in the text do-main, distance function design has been considered suc h an imp ortan t and complex issue that it has been the focus of intensiv e researc h over three decades. The nal design of distance functions in this domain has been reac hed only by detailed empirical testing and consensus over the qualit y of results pro vided by the di eren t variations. With the in-creasing abilit y to collect data in an automated way, the num ber of new kinds of data con tinues to increase rapidly . This mak es it increasingly dicult to undertak e suc h e orts for eac h and every new data type. The most imp ortan t as-pect of distance function design is that since a human is the end-user for any application, the design must satisfy the user requiremen ts with regard to e ectiv eness. This cre-ates the need for a systematic framew ork to design distance functions whic h are sensitiv e to the particular characteris-tics of the data domain. In this pap er, we discuss suc h a framew ork. The goal is to create distance functions in an automated way while minimizing the work required from the user. We will sho w that this framew ork creates distance functions whic h are signi can tly more e ectiv e than popu-larly used functions suc h as the Euclidean metric. H.2.8 [ Database Managemen t ]: Database Applications| Data Mining Data mining, distance functions, user interaction Cop yright 2003 ACM 1 X 58113 X 737 X 0/03/0008 ... $ 5.00.
Distance function design remains at the core of man y im-portan t data mining applications. Most applications suc h as clustering, classi cation and nearest neigh bor searc h use dis-tance functions as a key subroutine in their implemen tation. Clearly , the qualit y of the resulting distance function signi -can tly a ects the success of the corresp onding application in nding meaningful results. For man y data mining applica-tions, the choice of the distance function is not pre-de ned, but is chosen heuristically . There is not much literature on how distance functions should be designed for arbitrary ap-plications. Man y data mining algorithms and applications use the Euclidean distance metric as a natural extension of its use for spatial applications. The widespread use of dis-tance functions suc h as the L 2 -norm is perhaps rooted in the initial dev elopmen t of index structures for spatial ap-plications. In suc h cases, the special interpretabilit y of the L -norm assumed great imp ortance. However, this inter-pretabilit y is not really relev ant for arbitrary domains of data con taining man y dimensions.

The issue of distance function design becomes even more critical in the con text of high dimensional applications. Re-cen t work [13] has sho wn that in high dimensional space, the sparsit y of the data may a ect the qualit y of the dis-tance function. In particular, it has been indicated in [4, 7] that distance functions suc h as the L p -norm are often not very meaningful for high dimensional data because of the fact that the pairwise distances between the points are very similar to one another. In suc h cases, the poor con trast in the measuremen t of distances reduces the e ectiv eness of the measuremen ts. The overall e ect of high dimensionalit y on nearest neigh bor searc h has been examined in some detail in [2, 11]. In [2], it has been sho wn that nearest neigh bor searc h by visual interaction can often pro vide good qual-ity results. However, this approac h is only designed for the speci c problem of nearest neigh bor searc h and is not fo-cussed on determining the most e ectiv e distance functions in closed form. In fact, the merits of user feedbac k for near-est neigh bor searc h have been well documen ted for a num ber of data domains suc h as text and image retriev al [19, 20, 22]. These pro cedures cannot easily be used in arbitrary data mining applications. For most real applications, an ecien t computation of a closed form distance function is critical to the success of the underlying algorithm.

Some recen t work [3] has discussed the e ects of di eren t kinds of closed-form distance functions in high dimension-alit y, but this metho d is not user cen tered in its approac h. This technique only aims to increase the con trast in the distances of the di eren t points from one another. It is im-portan t to understand that since the qualit y of a distance function may often be determined by the perceptual sig-ni cance to an end-user, the user needs to be kept in the loop during distance function design. This would ensure that the nal distance function can mo del what the user wants, rather than simply satisfy a particular criterion suc h as maximizing the con trast. Consider the follo wing cases for distance function design: Whic h features of the data pro vide the greatest similar-ity? For example, in the image database, should the color histograms be given greater weigh t or do the texture and coarseness pro vide greater similarit y? In most cases, these speci c com bination of weigh ts one must assign to the dif-feren t features cannot be easily speci ed only with domain kno wledge. This mak es distance function design by man-ual and ad-ho c techniques a somewhat impractical solution. In this pap er, we will concen trate on the design of distance functions whic h are sensitiv e to the data beha vior and serv e the needs of the user.

We note that the pro cess of designing distance functions even for a speci c domain suc h as Information Retriev al (IR) has intrigued researc hers over three decades [19]. Suc h a design in the IR comm unit y has been achiev ed by consider-able testing and understanding of the particular character-istics of the data whic h are the most meaningful indicators of similarit y. Unlik e IR applications, we cannot alw ays use suc h domain speci c information about the \typical" nature of the data for arbitrary applications. Therefore, for these cases, designing distance functions is an even more dicult task. Furthermore, since adv ances in hardw are technologies con tinue to increase the num ber of data collection domains at a rapid rate, it is impractical to undertak e suc h an inten-sive researc h e ort for eac h newly disco vered data domain. On the other hand, given the imp ortance and reusabilit y of distance functions as a tool for man y di eren t algorithms in kno wledge disco very, it may be acceptable to allo w a few hours of human lab or in mo delling the distance function, pro vided that a systematic way of constructing the distance function is available. In this pap er, we will pro vide just suc h a framew ork.

This pap er is organized as follo ws. The remainder of this section will discuss imp ortan t asp ects and motiv ations of distance function design. In section 2, we will discuss how distance functions can be mo delled and designed in a systematic way so as to mo del an end-user's perceptions. This section will primarily concen trate on the use of popu-lar canonical forms suc h as the cosine mo del or Mink owski mo del for constructing distance functions. In section 3, we will discuss the use of purely learning strategies for distance function design. Techniques for com bining di eren t strate-gies in order to create the most e ectiv e distance function are discussed in section 4. The empirical results are pre-sen ted in section 5. Finally , the conclusions and summary are con tained in section 6.
The design of distance functions for well studied data do-mains suc h as Information Retriev al pro vides some useful hin ts for the high dimensional case. We list some of the practical desiderata for e ectiv e design of distance functions below. (1) User-Cen tered: Since the success of the data min-ing application ultimately rests with the perception of the user, it is clear that the distance function should mo del the corresp onding requiremen ts as closely as possible. While incorp oration of domain kno wledge in the design of the dis-tance function is often e ectiv e, it lacks the direct systematic approac h that is necessary in order to handle arbitrary ap-plications. In man y cases, for a new data domain, we have little exp erience or understanding of the features whic h are most indicativ e of similarit y. Therefore, a necessary require-men t is to nd a way to solicit the feedbac k from the user whic h can be used to construct the distance function by a systematic mo delling pro cess. (2) Con trasting: Since man y data mining applications are inheren tly high dimensional, it is imp ortan t to ensure that the distance functions pro vide sucien t discriminatory power. The results of [7] indicate that standard distance functions suc h as the L p -norm are not very e ectiv e in this resp ect. This non-con trasting beha vior is because even the most similar records are likely to have a few features whic h are well separated. For distance functions suc h as the L norm, the results of [7] sho w that the averaging e ects of the di eren t dimensions start to dominate with increasing dimensionalit y. The utilization of user-feedbac k serv es as a useful tool in resolving this problem of high dimensional distance functions. This is because it is easier for the user to guide the ultimate form of the distance function in suc h a way that the level of con trast is considerably increased. (3) Statistically Sensitiv e: It is very rare that the data is uniformly distributed along any given dimension. Some of the values may be more sparsely populated than others. This beha vior is commonly noted in Information Retriev al applications in whic h a given documen t usually con tains a small fraction of the available vocabulary . Distance func-tions suc h as dice, cosine or jaccard coecien ts [19] do not treat the attribute values uniformly but normalize using the frequency of occurrence of the words in the documen ts. Em-pirical testing on large collections has established that the normalization pro cess signi can tly a ects the qualit y of the distance function. In this pap er, we will try to achiev e statis-tical sensitivit y by analyzing the user requiremen ts directly , rather than try to guess the imp ortance of frequency distri-butions on the qualit y of similarit y. In other words, we wish to dev elop generic models for distance functions whic h can directly incorp orate user requiremen ts. (4) Compactness: In man y applications, the computa-tion of the distance function itself can be the most exp ensiv e operation because of its rep eated use over a large num ber of iterations. Therefore, a distance function should be ef-cien tly computable. In the event that the distance func-tion uses a pre-stored mo del or statistical information, the storage requiremen ts should be mo dest. We refer to this re-quiremen t as that of comp actness . We note that a distance function can be represen ted either in closed form, or algo-rithmic ally on the feature sets of the two objects. In most cases, closed form distance functions have the adv antage of being more ecien tly computable. (5) Interpretabilit y: The interpretabilit y of a distance function is very valuable for e ectiv e use in man y applica-tions. For example, one would like to kno w whic h features of the data are most in uen tial in a ecting the distance function. This can pro vide considerable insigh t and under-standing in a large num ber of applications. In this resp ect, closed-form distance functions are usually more desirable.
In the next section, we will discuss some useful mo dels for distance function computation. Speci cally we will discuss two generalizations of widely used distance functions suc h as the L p -norm and the cosine distance function. Speci cally , we will prop ose the Par ametric Minkowski model and the Par ametric Cosine model for distance function computation. We will sho w how to use these mo dels in conjunction with human interaction in order to create techniques whic h are signi can tly more friendly to the needs of a given user.
The essen tial idea behind a parametric mo del for distance function computation is that we do not specify the distance function exactly , but specify it incompletely along with some parameters. Then, we utilize user interaction and feedbac k to ll in these incomplete parameters. In order to formalize this concept, we will introduce some additional notations and de nitions.
 We assume that we have a data set D con taining a set of N records, eac h with d dimensions. Let us assume that the records X and Y are dra wn from the database D . We de-ne the parametric distanc e function f ( X; Y ; 1 ; : : : the function of its argumen ts X and Y and the k param-eters 1 : : : k . We will denote the vector ( 1 : : : k Thus, we note that this de nition of the distance function is incomplete since it only assumes a basic form of the distance function and leaves sev eral parameters unsp eci ed. These parameters are then learne d by a series of user interactions in whic h the user feedbac k is utilized in order to solicit the perceptual similarit y between di eren t pairs of objects. If desired, multiple users can be used in order to increase the robustness of the data collection pro cess. These similarit y values are then used in conjunction with a canonical form in order to train the data for a distance function whic h is most re ectiv e of user beha vior. We note that the use of a canonical form has the adv antage that it is able to incorp o-rate some degree of domain kno wledge about the data, while leaving the function sucien tly incomplete, so that its nal form is sucien tly re ectiv e of user beha vior.

Let us assume that the data pairs for whic h the distance function needs to be de ned are given by ( X 1 ; Y 1 ) : : : ( X The interactiv e pro cess assumes that for eac h pair of objects ( X i ; Y i ), the user pro vides the perceptual similarit y value We assume that the user-de ned similarit y values for these di eren t data pairs are given by 1 : : : N . We note that the nature of eac h of these objects and the user feedbac k may dep end upon the particular domain at hand. For example, in an image database, this perceptual similarit y could corre-spond to the similarit y in the patterns between the di eren t images. In this case, the visual perception of the user may be leveraged in order to de ne the nal similarit y values.
Thus, at the end of the pro cess, the values 1 : : : N are the user resp onses to the similarities between the corre-sponding pairs of objects ( X 1 ; Y 1 ) : : : ( X N ; Y N like to choose values of 1 : : : k , so that the error in predic-tion is as low as possible. This error can be de ned in sev eral ways; some of whic h are algorithmically more desirable. For example, one way of de ning the error E ( 1 : : : k ) is the squar e prediction error : A sligh tly di eren t way of de ning the error is the mean p-norm-squar ed error .
 We note that the use of the p-norm squared error is a matter of algorithmic con venience, since the nal objectiv e function often tak es on a simpler form for some of the mo dels that we will prop ose.

In order to determine the value of this objectiv e func-tion we use the gradien t searc h metho d. In the gradien t searc h metho d, we start o with an initial value of the vec-tor = 0 and successiv ely update to 1 , 2 , : : : , m using the gradient vector E . This gradien t vector is de ned as follo ws: For eac h value of = m , we substitute its corresp onding value in the above equation in order to determine the gra-the steep est descen t metho d. In this metho d, the greatest reduction of the objectiv e function value of E ( m ) oc-curs along the gradien t direction given by d m = E ( m ). Therefore, the value of m is updated as follo ws: Here, the choice of m determines the con vergence beha vior of this iterativ e mec hanism. Pic king m e ectiv ely ensures a rapid con vergence rate for the algorithm. To this e ect, we use the line searc h metho d [6] in order to perform the iterations. In the line searc h technique we would like to pick m suc h that the value of ( m ) = E ( m + m d m ) is minimized. In order to achiev e this, we should have 0 ( m ) = 0. This value of m = can be determined using a brac ket bisection technique. It is elemen tary to sho w that direction d m is a descen t gradien t at m , we have 0 (0) &lt; 0. In the event that we are able to determine at least one value of m suc h that 0 ( m ) 0, we kno w that the required value of m must lie in this range (0 ; ). Finding the value of exactly can often be time consuming; therefore we estimate this value within a factor of two. To do so, we rst pick an arbitrary value of m = 1. If 0 (1) &lt; 0, we keep doubling m until we obtain the largest value for whic h 0 ( m ) &lt; 0. Otherwise, if 0 (1) &gt; 0, we keep halving m until we obtain the rst value of m for whic h 0 ( m ) &lt; 0. The nal value of m at the end of this two-pronged searc h metho d is used for the update step of Equation 3. We note that this tech-nique is somewhat similar to the Armijo rule [6] used for line searc h in gradien t metho ds.
The parametric mink owski mo del used in this pap er is basically a generalization of the L p norm. In this mo del, the distance function is de ned by a weigh ted version of the L p norm. The weigh ts on the di eren t dimensions are the parameters for this mo del. Let us assume that the weigh ts for the d dimensions are given by 1 : : : d . Therefore, for a pair of objects X = ( x 1 : : : x d ) and Y = ( y 1 : : : y of the parametric distance is de ned as follo ws: Note that this distance function is easily interpretable in terms of the di eren t dimensions. A higher value of i in-dicates a greater signi cance for a particular dimension. In this case, the use of the p-norm mean square error results in a linear set of equations for 1 : : : k . Let us assume that the coordinates for eac h individual pair of points X i and Y error function in this case is of the follo wing form: Here the value of a ij is given by jj x ij y ij jj p . On using the deriv ativ e with resp ect to eac h j , we obtain the follo wing: Now the gradien t vector E can be de ned directly using Equation 3. This gradien t vector is used in order to update the vector as indicated in Equation 4.
In real applications, man y attributes in the data are cor-related with one another. Inter-attribute correlations have often been used for designing distance functions in categor-ical domains where there is no natural ordering of attribute values. In suc h cases, the use of inter-attribute summary in-formation pro vides the only possible insigh t into the similar-ity of objects by examining whether commonly co-o ccurring inter-attribute values are presen t in the two objects [8]. This insigh t is equally relev ant even for quan titativ e domains of data where a natural ordering of attribute values exists. The use of aggregate data beha vior in order to measure similarit y becomes more imp ortan t for high dimensional data, where there may be considerable redundancies, dep endencies, and relationships among the large num ber of attributes. Since a lot of the pro ximit y information may be hidden in the aggre-gate summary beha vior of the data, the use of the linearly separable parametric mo del may miss man y of the critical characteristics. We also note that some data domains suc h as text factor the correlation beha vior indirectly into the distance function by using data transformation techniques suc h as Laten t Seman tic Indexing [12].

Fortunately , the parametric Mink owski mo del can be di-rectly generalized in a way so as to mak e it more sensitiv e to the correlation beha vior of the data. In order to do so, we use the gener alize d parametric Minkowski model in order to de ne distances. In this case, we use a symmetric d d matrix con taining the parametric values. In this case, the distance between the point vectors X and Y is given by: We note that when the matrix has non-zero entries only on the diagonal, the mo del reduces to the simple Mink owski case. Thus, the non-zero values on the non-diagonal entries pro vide the magnitudes of the correlations between the cor-resp onding dimensions. As in the previous case, the gradi-ent descen t metho d can be used to determine the parameter values.
The parametric cosine mo del is actually a similarit y func-tion rather than a distance function, since higher values im-ply greater similarit y. The basic idea behind the parametric cosine mo del is dra wn from the cosine function used in text databases [19, 21]. We will prop ose this function for the case of boolean data sets, though it can be directly extended to quan titativ e data sets by applying a simple prepro cess-ing phase of discretization. For two boolean data points X = ( x 1 : : : x d ) and Y = ( y 1 : : : y d ), the cosine distance be-tween these points is given by: In man y cases, the attributes are appropriately weigh ted and normalized in order to impro ve the qualit y of the distance function. An example of suc h normalization is the simple inverse documen t frequency normalization used in Informa-tion Retriev al applications. We note that suc h weigh ting or normalization is usually done purely on the basis of the statistical prop erties of the data set, but may often devi-ate signi can tly from how similarit y may be perceiv ed by a given user. In order to achiev e this, we introduce a paramet-ric cosine function whic h is more sensitiv e to the pro cess of actually learning similarit y from user beha vior. The para-metric cosine similarit y of X and Y is de ned in terms of the d parameters = 1 : : : d : We note that this is a similarity function as opp osed to a distance function, since higher num bers imply greater sim-ilarit y. As before, let us assume that the coordinates for eac h individual pair of points X i and Y i are denoted by ( x nd that by using the mean square error function, we again obtain a similar form of the error function: In this case, the value of a ij is equal to x ij y ij . The re-maining analysis is exactly similar to that of the Mink owski mo del.
We note that the above techniques for distance function design naturally lead to a high degree of interpretabilit y of the distance function in a given data domain. For example, unlik e simple Euclidean functions whic h treat all dimensions equally , the parametric Mink owski mo del pro vides a clear idea of whic h features are the most imp ortan t based on the underlying data beha vior. It is clear that any feature i for whic h the weigh t i is small is less relev ant in determining the similarit y value.

Similarly , in the case of the parametric cosine function, the imp ortance of eac h attribute is decided by the corresp ond-ing weigh ts. We note that in the case of the text domain, in whic h similarit y functions have been extensiv ely researc hed, suc h weigh ts are determined by using term/in verse docu-men t frequency normalization [19]. While it is intuitiv ely clear that suc h normalization gives greater weigh t to statis-tically infrequen tly occurring events, it does not incorp orate the characteristics of the particular kind of data at hand. The abilit y to capture suc h relationships lies at the heart of a technique whic h incorp orates suc h feedbac k into the learning pro cess.
In the previous section, we discussed a metho d whic h used purely parametric forms for learning a distance function with a particular canonical form. While suc h a technique has the adv antage of pro viding a closed form represen ta-tion to the distance function, a natural alternativ e is to transform the problem of distance function design to that of pure regression mo delling. In order to illustrate this point, consider the case, when we have a set of N object pairs ( X 1 ; Y 1 ) : : : ( X N ; Y N ) along with the corresp onding user re-sponses 1 : : : N . In this case, we can recreate a new com-posite object con taining 2 d attributes, by concatenating the attributes of X i and Y i for eac h value of i . Therefore, we can create the new object Z i , by concatenating the d attributes eac h of X i and Y i . Now, we can use the set Z 1 : : : Z with 1 : : : N as the training data in order to mo del the distance function.

While the simplicit y of the above mo del is tempting, it has some dra wbac ks as well. In most data domains, dis-tance functions have some naturally desirable qualities in terms of the relationship of the quan titativ e class variable with the feature variables. Parametric techniques suc h as the Cosine or Mink owski mo dels precisely try to enco de suc h information within the training pro cess. Just as the use of a xed form suc h as the L p -norm is in exible, the absence of any canonical form ignores any kind of understanding of the inheren t nature of distance functions. In order to create accurate mo dels in cases where there are no guiding canoni-cal forms, the amoun t of data required is likely to consider-able. This is a signi can t dra wbac k in a system in whic h the generation of eac h data point requires human interv ention. However, it is possible to mak e some compromises in order to incorp orate feature speci c kno wledge into the training pro cess without at the same time using a xed and in exible canonical form. To do so, we prop ose to use feature transfor-mations whic h capture the natural functional prop erties of man y distance functions. Thus, for the set of N object pairs ( X 1 ; Y 1 ) : : : ( X N ; Y N ) along with the corresp onding user re-sponses 1 : : : N , one could create the set of functional fea-tures g 1 ( X 1 : : : X N ; Y 1 ; : : : Y N ), : : : g q ( X Thus, a set of new features are created by the transforma-tion. These new features are created by the user in order to help the learning pro cess to some exten t, while not design-ing precise functional forms for the distance function. While a regression mo del built on the original feature-resp onse tu-ples ( X i ; Y i ; i ) ough t to be the most exible, the realit y is that the constrain ts on training data availabilit y mak e suc h a system dicult to implemen t. The use of functional fea-tures indirectly incorp orates kno wledge about the natural beha vior of distance functions, whic h would otherwise have to be learned. In the next subsections, we will discuss some functional transformations whic h deriv e their motiv ations from the Mink owski and Cosine mo dels resp ectiv ely.
One example of suc h a system is the di erence functional transformation. In this case, we have a total of q = N g ( : : : ) = jj X i Y i jj . In this mo del, smaller values of cate greater similarit y. We note that this mo del is in uenced by the Mink owski mo del, since the latter is also determined completely by the values of jj X i Y i jj . We note that suc h a transformation directly reduces the num ber of features by a factor of two, while adding additional kno wledge in order to impro ve the eciency of the kno wledge disco very pro cess.
This mo del assume that eac h of the values of ( X i ; Y i boolean. However, quan titativ e values can easily be con-verted to boolean by the use of data discretization. As is the case of the cosine functional represen tation, this mo del uses only the pro ducts of corresp onding values of ( X i in order to de ne the new features on whic h the distance function mo del is constructed. In this case q = N new fea-g ( : : : ) = X i Y i . In this mo del, larger values of i cate greater similarit y. This mo del deriv es its motiv ation from the cosine function in whic h the (normalized) pro duct sum de nes the value of the similarit y function. However, a pro duct functional transformation is somewhat more exible than a parametric cosine mo del because it recognizes that the actual similarit y value may be a more complex com bina-tion of feature interactions than is allo wed by the parametric closed form.
Once the functional features have been created, we use them directly for distance function learning. Speci cally , a standard classi cation mo del is constructed on this new set of features in order to relate them to the user-de ned similar-ity values. We note that the classi cation mo del often works much more e ectiv ely on this new set of features, because they often incorp orate the inheren t characteristics of natural similarit y (or distance) functions. For example, the di er-ence functional transformation automatically stores the rela-tionship between two of the columns whic h is often likely to be very relev ant to the nal distance value. While a purely learning strategy may also try to learn this relationship, it is likely to be signi can tly more inaccurate when insucien t amoun ts of data are available. In fact,as our empirical re-sults will sho w, even the feature transformational mo dels do not work as well as the more restricted parametric forms when small amoun ts of training data are available. In gen-eral, a direct learning strategy often nds it more dicult to mo del the natural characteristics of similarit y calculations.
One of the interesting points to be noted is that no partic-ular strategy may pro vide an optim um resp onse. This is be-cause a given feature transformation or canonical form may incorp orate certain adv antages for a particular data domain, while it may not be quite as e ectiv e for other domains. Un-fortunately , without additional domain kno wledge, it is not possible to kno w the most e ectiv e transformation a-priori. In order to handle this problem, it is possible to use a hold-out strategy in picking the most e ectiv e canonical form or transformation.

In order to compare di eren t strategies, we divided the data set D of size N into two subsets D 1 and D 2 of size N 1 and N 2 resp ectiv ely. The set D 2 is de ned as the hold-out set. Once these two data sets have been constructed, we build eac h mo del separately on the data set D 1 . Next, we determine the estimate of the distance function value for eac h record in the data set D 2 , using the mo del constructed on the data set D 1 . The average error in the distance func-tion using eac h canonical form or feature transformation is calculated and used for the determining whic h of the var-ious metho ds is most appropriate for that particular data domain.

We note that a normalization issue needs to be tak en into accoun t while com bining di eren t strategies. Since some of the metho ds discussed above corresp ond to maximization objectiv e functions, whereas others corresp ond to minimiza-tion objectiv e functions, we need to utilize the user feedbac k in a consisten t way. In order to achiev e this, we assume that : : : N are generated using the minimization con vention, whereas the corresp onding maximization coun terparts are generated as M 1 : : : M N . Here M is an upp er bound on any value of i and may be chosen as M = max i f i g . We note that while using this con vention the absolute inaccu-racy in mo delling the maximization objectiv e function value is directly comparable to that of mo delling the minimization value. This mak es it possible to compare di eren t distance function mo dels in order to test the appropriateness of that approac h for the given data set.
The aim of this section is to sho w that the mo dels dis-cussed in this pap er can e ectiv ely capture the distance functions quite accurately for a num ber of real data sets. All results were implemen ted on an AIX4.1.4 system at 233MHz and 200MB of main memory .
In order to test the performance of our metho dology , we used a num ber of data sets from the UCI mac hine learn-ing 1 rep ository . While all data sets were based on real do-mains, the user resp onses were syn thetically generated from within the data. We used one of the feature variables in or-der to syn thetically generate the distance function resp onses : : : N . This feature was stripp ed from the other records. Let the records in the original databases D be denoted by W 1 : : : W r . We will denote the values of the special col-umn from whic h the objectiv e function was generated by z : : : z r . In order to actually generate the objectiv e func-tion values, we randomly picked N pairs of objects from the records W 1 : : : W r . Let us assume that the i th pair of objects picked is denoted by ( X i ; Y i ) = ( W n i ; W m i ). Thus, n m i are indices of the object pairs whic h are sampled. The value of the objectiv e function i was given by jj z m i z while testing for minimization objectiv e functions 2 suc h as the parametric Mink owski distance function or the di erence functional transformation. On the other hand, while dealing with maximization objectiv e functions suc h as the paramet-ric cosine mo del or the pro duct functional transformation, this value was set at M jj z m i z n i jj , where M is an upp er bound on the value of jj z m i z n i jj . The value of M is chosen to be max i f z i g min i f z i g . We note that this metho d of cre-ating the similarit y value is syn thetic as opp osed to man ual. Since a di erence function was used on a particular column to consisten tly generate the similarit y values, this generates a similarit y function with a particular bias, as is the case with most user de ned similarit y perceptions. For example, when the column for \Age" is used in order to de ne the similarit y function, the di erence function on this column creates a function whic h will mak e two objects similar only when they are similar in age. Some of the features are likely to be more imp ortan t than others while making suc h a de-termination. However, the aim of this section is to sho w the e e ctiveness of these techniques in mo delling any kind of similarit y, whether they be man ually de ned or syn thetic. The adv antage of using syn thetic data sets is to be able to pro vide objectiv e measures of accuracy of the metho d. 2 Smaller values are better for the case of minimization ob-jectiv e functions.
In addition to the N data pairs whic h are used for distance function mo delling, we also generate a separate set of N 0 data pairs for testing the e ectiv eness of the distance func-tion. Let us assume that these N 0 data pairs are denoted by larit y values of 1 : : : N 0 . Once the appropriate mo del has been constructed, we use it to predict the similarit y values : : : 0 N 0 for eac h of the N 0 test cases. The absolute di er-ence between eac h value of i and 0 i (denoted by jj i 0 i is the inaccuracy on that particular test example. The over-all inaccuracy I is then determined by averaging the same calculation over all the di eren t test examples. Therefore, we have: We calculated this error value for eac h individual mo del as well as the comp osite mo del by obtaining the best canonical form in the holdout mo del.

While the above measures give insigh t into the absolute errors of the di eren t strategies, the real e ectiv eness of a distance function is determined by how well the distance function orders the di eren t objects in terms of their rela-tive distance values. Thus, for example, if objects A and B are closer to one another than the user perceiv es objects C and D , then we would like that the mo del is able to ac-curately predict this fact. To this e ect, we sampled two pairs of objects at a time and calculated the fraction of time that the correct ordering was main tained. We will refer to this value as the ordering accuracy . Therefore, if n 1 and n 2 be the resp ectiv e num ber of times that the correct and incorrect order was main tained between randomly chosen pairs of objects, then the ordering accuracy was given by n = ( n 1 + n 2 ). Suc h a measure is indep enden t of the abso-lute values of the similarit y and measures qualit y in terms of percen tage of ordering correctness. This mak es it possible to compare distance functions with widely varying absolute values.
In Table 1, we have illustrated the statistics of the di er-ent data sets. For eac h data set, the column whic h was used to generate the distance function value is denoted by colum-nid . This column was alw ays chosen to be numeric in order to de ne the distance function e ectiv ely. This column was also remo ved from the data set, so that the distance func-tion was mo delled using only the remaining features. The resulting dimensionalit y (after remo val of this column) is indicated in Table 1. In the table we have also illustrated the maxim um and minim um values of the target objectiv e function once they were syn thetically generated. These val-ues pro vide an idea of the range of the absolute error in the estimation of the nal objectiv e function.

The absolute error for eac h data set is illustrated in Table 2. Eac h of the di eren t metho ds prop osed in this pap er were tested together with a pure learning metho dology as a base-line. The learning metho d used was a decision (regression) tree classi er [17]. It is interesting to see that the pure learn-ing metho d was nev er as e ectiv e as either the parametric functional metho ds or the transformational learning meth-ods in any case. The reason for this is that the parametric distance functions and the transformational features enco de Figure 1: Accuracy versus Data Set Size (Ecoli Data Set) critical information about the underlying data. The other salien t observ ation is that the e ectiv eness of the di eren t distance functions varies from one data set to another. For example, the parametric distance functions are more e ec-tive than the transform learning functions for the Ecoli and Mac hine data sets, whereas the transformational features are more e ectiv e for the other cases. Even among the dif-feren t kinds of parametric forms, there were sligh t variations for di eren t data sets. For example, for the Ecoli data set the parametric Mink owski function had a lower error than the parametric cosine function, whereas for the glass data set, the rev erse was true. The same observ ation was true about the transformational features. In terms of the abso-lute values of the error rates, the error values for the pure learning strategy are signi can tly higher, whereas the di er-ences between the other four metho ds are relativ ely small. This tends to indicate that any of the latter metho ds would be signi can tly robust for most data sets. We also note that by using the hold out strategy discussed earlier, it is possible to isolate and use the most e ectiv e distance function for a given data set.

In Table 3, we have illustrated the ordering accuracy of the di eren t metho ds for eac h of the data sets. In this case, we have omitted the pure learning strategy , because our re-sults in Table 2 sho w that it is conclusiv ely worse than any of the other techniques in eac h case. Instead, we have used the ordering error on the simple Euclidean function as a base-line in order to sho w the adv antages of this metho d over the traditional Euclidean metric. The results in Table 2 in-dicate that the relativ e ordering accuracies of the di eren t metho ds closely mirror the absolute inaccuracies of the dif-feren t metho ds. Furthermore, in eac h case, the techniques turn out to be more e ectiv e than the traditional Euclidean distance function. The reason for this is that the Euclidean function is not geared to pro vide the particular notion of nearness whic h is application dep enden t and data driv en. Thus, the com bined results of Table 2 and 3 sho w that the careful metho d of designing the distance function is able to achiev e results that either traditional distance functions or pure learning strategies cannot achiev e. Figure 2: Accuracy versus Data Set Size (Glass Data Set) Figure 3: Accuracy versus Data Set Size (Mac hine Data Set) Figure 4: Accuracy versus Data Set Size (Auto-Mpg Data Set) Figure 5: Accuracy versus Data Set Size (Housing Data Set)
The previous section sho wed that while the qualit y of re-sults obtained are appro ximately comparable, they pro vide little guidance about the comparativ e data size requiremen ts in order to achiev e e ectiv e results. We note that since the focus of this pap er is to learn distance functions from do-main speci c data, the amoun t of data available may often be limited. Therefore, it is useful to have an idea of how well the di eren t metho ds compare in terms of the amoun t of data required in order to achiev e accurate mo delling of the distance function. The results for the Ecoli, Glass, Mac hine, Glass, Auto-Mpg and Housing data sets are illus-trated in Figures 1, 2, 3, 4 and 5 resp ectiv ely. On the X-axis, we have illustrated the num ber of data pairs used for train-ing, whereas on the Y -axis, we have illustrated the absolute error resulting from the training pro cedure. In eac h case, the error reduces with increasing num ber of training pairs, but stabilizes at some point. We note that the results in Ta-ble 2 are eac h based on 10,000 data pairs whic h is this stable region. An interesting common characteristic of all these re-sults is that in eac h case, the parametric forms require much few er num ber of records to reac h the region of stabilit y than the functional feature transformations. In most cases only about 500 to 1000 data pairs were sucien t to reac h the region of stabilit y. In fact, in some of the cases suc h as the Glass, Auto-Mpg and Housing data sets (Figures 2, 4 and 5), the error rate of the parametric forms is lower for smaller num ber of records, even though the steady state value for larger num ber of records is higher. This beha vior is particu-larly eviden t in the housing data set of Figure 5. The reason for this is that the parametric mo dels are somewhat more restrictiv e in performing distance function mo deling. Thus, few er num ber of data pairs are required in order to reac h the optimal error value. At the same time, Table 2 sho ws that the qualitativ e results are not very di eren t between the transformational feature techniques and the parametric forms. This indicates an overall adv antage for the paramet-ric metho d since the same overall qualitativ e results can be obtained with much less data. Furthermore, the paramet-ric forms have the adv antages of being compact, since they can be easily computed and expressed in closed form. When com bined with the adv antages of low data size requiremen ts, it is clear that parametric mo deling is a promising technique for systematic design of distance functions.
While distance functions con tinue to be one of the most imp ortan t and widely used elemen t of man y data mining problems, they are often implemen ted using naiv e meth-ods for man y data domains. In this pap er, we discussed a user-cen tered and systematic metho d for mo deling dis-tance functions e ectiv ely. We illustrated the adv antages of a parametric approac h over both a xed distance function and a purely learning metho dology . We also discussed the usefulness of transformational features in distance function mo deling. Giv en the imp ortance and application speci cit y of distance functions, the results in this pap er can be valu-able for a large num ber of data mining problems. [1] C. C. Aggarw al. Re-designing distance functions and distance based applications for high dimensional data.
ACM SIGMOD Record , Marc h 2001. [2] C. C. Aggarw al. Towards Meaningful High Dimensional Nearest Neigh bor Searc h by Human-Computer
Interaction. ICDE Confer ence , 2001. [3] C. C. Aggarw al, P. S. Yu. The IGrid Index: Rev ersing the Dimensionalit y Curse for Similarit y Indexing in High
Dimensional Space. KDD Confer ence , 2001. [4] C. C. Aggarw al, A. Hinneburg, D. A. Keim. On the Surprising Beha vior of Distance Metrics in High
Dimensional Space. ICDT Confer ence , 2001. [5] R. Agra wal, K.-I. Lin, H. S. Sawhney , K. Shim. Fast similarit y searc h in the presence of noise, scaling, and translation in time-series databases. VLDB Confer ence , pages 490-501, 1995. [6] D. Bertsek as. Nonlinear Programming. Athena
Scienti c , 2nd Edition, 1999. [7] K. Bey er, J. Goldstein, R. Ramakrishnan, U. Shaft. When is Nearest Neigh bors Meaningful? ICDT
Confer ence , 1999. [8] G. Das, H. Mannila. Con text-Based Similarit y
Measures for Categorical Databases. PDKK Confer ence , 2000. [9] J. Foote. A Similarit y Measure for Automatic Audio Classi cation. AAI 1997 Spring Symp osium on
Intel ligent Inte gration and Use of Text, Image, Vide o, and Audio Corp ora , 1997. [10] D. Gunopulos, G. Das. Time Series Similarit y Measures and Time Series Indexing. ACM SIGMOD
Confer ence , 2001. [11] A. Hinneburg, C. C. Aggarw al, D. Keim. What is the nearest neigh bor in high dimensional spaces? VLDB
Confer ence , 2000. [12] S. Deerw ester et al. Indexing by Laten t Seman tic Analysis. Journal of the Americ an Society for
Information Scienc e , 41(6): 391-407, 1990. [13] N. Kata yama, S. Satoh. Distinctiv eness Sensitiv e Nearest Neigh bor Searc h for Ecien t Similarit y
Retriev al of Multimedia Information. ICDE Confer ence , 2001. [14] E. Keogh, M. Pazzini. Scaling up Dynamic Time Warping to Massiv e Data Sets. Principles of Data
Mining and Know ledge Disc overy , pages 1-11, 1999. [15] P. Mo en. Attribute, Event Sequence, and Event Type Similarit y Notions for Data Mining. ining. PhD Thesis, Rep ort A-2000-1, Departmen t of Computer Science,
Univ ersit y of Helsinki, February 2000. http://www.cs.helsinki. /TR/A-2000/1/. [16] A. Hinneburg, D. A. Keim, M. Wawryniuk. HD-Ey e: Visual Mining of High Dimensional Data. IEEE Comp.
Graphics and Applic ations , 19(5), pp. 22-31, 1999. [17] M. James. Classi cation Algorithms, Wiley , 1985. [18] M. M. Ric hter. On the notion of similarit y in case-based reasoning. Mathematical and Statistical
Metho ds in Arti cial Intelligence (ed. G. della Riccia et al). Springer Verlag , 1995, p. 171-184. [19] G. Salton, M. J. McGill. Introduction to Mo dern
Information Retriev al. Mc Graw Hill , New York, 1983. [20] T. Seidl, H.-P . Kriegel: Ecien t User-Adaptable Similarit y Searc h in Large Multimedia Databases. VLDB
Confer ence , 1997. [21] A. Singhal, G. Salton, M. Mitra, C. Buc kley . Piv oted Documen t Length Normalization. ACM SIGIR
Confer ence , 1996. [22] L. Wu, C. Faloutsos, K. Sycara, T. Payne. FALCON: Feedbac k Adaptiv e Loop for Con ten t-Based Retriev al.
VLDB Confer ence , 2000.
