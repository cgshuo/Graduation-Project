 In this paper we discuss the important practical problem of customer wallet estimation , i.e., estimation of potential spending by customers (rather than their expected spend-ing). For this purpose we utilize quantile modeling, whose goal is to estimate a quantile of the discriminative condi-tional distribution of the response, rather than the mean, which is the implicit goal of most standard regression ap-proaches. We argue that a notion of wallet can be captured through high quantile modeling (e.g, estimating the 90th percentile), and describe a wallet estimation implementa-tion within IBM X  X  Market Alignment Program (MAP). We also discuss the wide range of domains where high-quantile modeling can be practically important: estimating oppor-tunities in sales and marketing domains, defining  X  X urpris-ing X  patterns for outlier and fraud detection and more. We survey some existing approaches for quantile modeling, and propose adaptations of nearest-neighbor and regression-t ree approaches to quantile modeling. We demonstrate the vari-ous models X  performance in high quantile estimation in sev-eral domains, including our motivating problem of estimat-ing the  X  X ealistic X  IT wallets of IBM customers.
 H.2.8 [ Database Management ]: Database applications X  Data mining ; G.3 [ Mathematics of Computing ]: Proba-bility and Statistics X  Statistical Computing, Nonparametric statistics Algorithms, Management, Performance Quantile Estimation, Customer wallet estimation, Regres-sion trees
In standard regression modeling, we are given n obser-vations on a continuous numeric variable Y and a set of p explanatory variables, or features, x = ( x 1 , ..., x p ) try to estimate the  X  X ependence X  of Y on x , so that in the fu-ture we can observe x only and predict what Y may be. This typically leads us to build a model for a conditional central tendency of Y | x , usually the mean E ( Y | x ). For example, under appropriate model assumptions, modeling based on a least squares loss function (like linear least squares or mo st regression tree approaches), is as a maximum likelihood ap-proach to estimating this conditional mean.

In this paper we address the situations when we are really not interested in estimating a conditional mean, but rather a different property of the conditional distribution P ( Y | x ), in particular a high quantile of this distribution, such as the 0.9 quantile of P ( Y | x ), which is the function c ( x ) such that P ( Y  X  c ( x ) | x ) = 0 . 0. As we discuss in Section 2 below, these problems (of estimating conditional mean vs. conditional high quantile) may be equivalent under certain simplistic assumptions about our models, but in practice they are usually not. We are typically interested in modelin g high quantiles because they represent a desired  X  X redictio n X  in some business and scientific domains.

Our primary motivating application is the problem of cus-tomer wallet estimation, which is of great practical intere st to us at IBM. A customer X  X  wallet for a specific product category (for example, Information Technology) is the tota l amount this customer can spend in this product category. As an IT vendor, IBM observes what the companies that are our customers actually spend with us, but does not typically have access to the customers X  budget allocation decisions, their spending with competitors, etc. Information about our customers X  wallet, as an indicator of their potential fo r growth, is considered extremely valuable for marketing, re -source planning and other tasks. For a detailed survey of the motivation, problem definition, and some alternative so -lution approaches, see [13]. In that paper we propose the definition of a customer X  X  REALISTIC wallet as the 0 . 9 or 0 . 95 quantile of their conditional spending  X  this can be interpreted as a highly optimistic (yet still attainable) e s-timate of what they could spend on buying IT from IBM. This task of modeling  X  X hat we can hope for X  rather than  X  X hat we should expect X  turns out to be of great interest in multiple other business domains, including:
In this paper, we address quantile estimation both as a generic problem and in the specific context of customer wal-let estimation. In Section 2 we discuss the fundamental sta-tistical and practical issues involved in the task of modeli ng high quantiles and evaluating performance of high-quantil e prediction models. We then survey in Section 3 a variety of approaches that have been proposed in the literature for quantile modeling, and propose original approaches based o n the adaptation of arguably the two most common regression approaches used in data mining  X  k-nearest neighbors and regression trees  X  to estimation of high quantiles instead of conditional means. Section 4 is devoted to a case study, where we describe the Market Alignment Program (MAP), aimed at refocusing IBM Sales resources using wallet esti-mates, and present the modeling and evaluation process we went through to determine which approach was most ap-propriate for supplying wallet estimates for MAP. Our main tool in that effort is wallet values obtained from consultati on with IBM experts, which we use to analyze which of a large set of candidate models best captures the experts X  notion of customer wallet. Finally, we conduct and present in Section 5 an extensive experimental study on a number of practi-cal prediction problems where high quantile prediction is a well justified prediction task. This includes our motivatin g application of customer wallet estimation, as well as sever al publicly available datasets. Our main conclusions are:
In this section we review some of the fundamental sta-tistical and algorithmic concepts underlying the two main phases of predictive modeling  X  model building and model evaluation and selection  X  when our goal is ultimately to predict high quantiles well. Figure 1: Quantile loss functions for some quantiles.
Let us start from the easier question of model evaluation and model selection: given several models for predicting hi gh quantiles and an evaluation data set not used for modeling, how can we estimate their performance and choose among them? The first, obvious answer, would be to obtain obser-vations about high quantiles of holdout data (for example, by asking experts). This is in general a difficult or very ex-pensive endeavor, and is often simply impossible (because no experts are available who can estimate this).

A more sound approach to this problem is to find a loss function which describes well our success in predicting hig h quantile and evaluate the performance using this loss func-tion. Clearly, the most important requirement from a loss function for evaluation is that the model which always pre-dicts the conditional quantile correctly will have the best expected performance. Such a loss function indeed exists [8]. Define the quantile loss function for the p th quantile to be:
In Figure 1, we plot the quantile loss function for p  X  Expected quantile loss is minimized by correctly predictin g the (conditional) p th quantile of the conditional distribu-tion. That is, if we fix a prediction point x , and define c to be the p th quantile of the conditional distribution of Y given x : then the loss function is optimized in expectation at every point by correctly predicting c p ( x ): With p = 0 . 5, the expected absolute loss is minimized by predicting the median, while when p = 0 . 9 we are in fact evaluating a model X  X  ability to correctly predict the 90th percentile of the distribution P ( Y | x ).

Another approach to evaluation is to look at the propor-tion of positive and negative residuals on the holdout data. A perfect prediction model for the 0 . 9 quantile will predict a value that is higher than the actual observed holdout re-sponse 90% of the time, on average. Thus we can examine whether the actual percentage of the time that the predic-tions are higher than observed response is indeed close to that, as a way of  X  X valuating X  high-quantile models. This is dangerous, of course, because a model which predicts +  X  90% of the time and  X  X  X  the other 10% would be perfect according to this measure.

On the modeling side, our first observation is that any property of the conditional distribution P ( Y | x ) can be es-timated well if we estimate well the whole distribution. In particular, if we have a parametric model for P ( Y | x ) which we believe is true and which we have enough data to esti-mate, then it is often the best policy to apply all our effort towards estimating this model X  X  parameters well (e.g., usi ng a maximum likelihood approach), regardless of what prop-erty of P ( Y | x ) we are ultimately interested in. For example, if we believe that the distribution of Y | x is homoscedastic gaussian and E ( Y | x ) =  X  t x is linear in x , then a maximum likelihood approach would call for fitting a linear regressi on model of Y on x . Furthermore, this would also trivially imply that the 0.9 quantile of P ( Y | x ) is linear in x , and is simply a fixed offset from the expectation: E ( Y | x )+1 . 28  X   X  .
However, parametric and distributional assumptions are usually over simplifications of realistic modeling problem s, especially those encountered in complex data-mining do-mains, and one should either dispose with them completely (and choose non parametric approaches), or at least treat with skepticism the notion of high quality estimation of com -plete conditional distributions. An alternative is to buil d the model by minimizing an  X  X mpirical risk X  over the training data, which represents well the prediction task. In the case of quantile modeling, the quantile estimation loss functio n L p (1) certainly qualifies (a similar approach leads Friedman et al. [7] to advocate the logistic regression loss function for boosting, for example).

In practice, both of these approaches may have advantages and disadvantages. An additional consideration is one of variance, especially when modeling high quantiles  X  does the high-quantile loss function allow us to make efficient use of the data for modeling? See [8] for detailed discussion of the dependence of variance on quantile for simple quantile regression.

All of this leads us to adopting the following methodolog-ical guidelines in developing and testing high-quantile es ti-mation modeling approaches: 1. Where available, we would like to use expert-supplied 2. Following the concept of  X  X mpirical risk minimization X  3. We believe these approaches should still be compared
Over the recent past, the modeling of quantiles has re-ceived increasing attention. The modeling objectives were either prediction or to gain insights how the statistical de -pendencies for quantiles differ from expected value models. We review some of these efforts in 3.1. Two of the best studied and also practically most common standard regres-sion approaches in machine learning and data mining are k-nearest neighbors and regression trees. We discuss in some detail how these methods can be adjusted to modeling quan-tiles in 3.2 and 3.3.
We are aware of a number of such quantile estimation methods including Linear quantile regression [8], Kernel q uan-tile regression [14], Quanting [9], Quantile regression fo rests [11] and polynomial regression trees [6]. Many of these meth -ods suffer from intractable computational behavior for larg er quantile estimation tasks as in the case of our IBM wallet. We next discuss the two most relevant approaches to our work in some detail.
 Linear quantile regression A standard technique for quantile regression that has been developed and extensively applied in the Econometrics com-munity is linear quantile regression [8]. Linear quantile r e-gression assumes that the conditional quantile function is a linear function of the explanatory variables of the form  X  x and we estimate the parameters  X   X  that minimize the quan-tile loss function (Equation 1). This minimization is a line ar programming problem and that it can be efficiently solved using interior point techniques [8]. Obvious extensions of Linear quantile regression are Kernel quantile regression [14] and nonlinear Spline models [8].
 Quanting Recently, a reduction from quantile regression to classific a-tion has been proposed [9]. The Quanting reduction trans-forms a quantile regression problem into a series of classi-fication problems such that a small average error rate on the classification problems leads to a provably accurate es-timate of the conditional quantile. This allows us to apply any existing classifier learning algorithm to solve quantil e regression problems. The essential idea of quanting is that each classifier c t attempts to answer the question  X  X s the q -quantile above or below t ? X  In the (idealized) scenario where A is perfect, one would have c t ( x ) = 1 if and only if q ( x ) &gt; t for a q -quantile q ( x ), hence the algorithm would output R q ( x ) 0 dt = q ( x ) exactly. The quanting analysis [9] shows that if the error of A is small on average over t , the quantile estimate is accurate.
The traditional k -nearest neighbor model is defined as where N k ( x ) is the neighborhood of x defined by the k closes points x i in the training sample for a given distance measure (e.g., Eucledian). From a statistical perspective we can vi ew the set y j  X  N k ( x ) as a sample from approximated condi-tional distribution of P ( Y | x ). The standard k NN estimator of  X  y is simply the expected value of this conditional distri-bution approximated by a local neighborhood. For quan-tile estimation we are not interested in the expected value (i.e., an estimate of E ( Y | x )) but rather a particular quan-tile c ( x ) of the conditional distribution P ( Y | x ) such that P ( Y  X  c ( x ) | x ) = q . Accordingly we can estimate  X  c ( x ) in a k -nearest neighbor setting as the q X  X h quantile of the em-pirical distribution of { y j : x j  X  N k ( x ) } . If we denote that empirical distribution by: then our k NN estimate of the q X  X h quantile of P ( Y | x ) would be  X  G  X  1 x ( q ).

The interpretation is that the values of Y in the neighbor-hood N k ( x ) are a sample from the conditional distribution P ( Y | x ) and we are empirically estimating its q X  X h quantile.
An important practical aspect of this estimate is that, in contrast to the standard k NN estimates, it imposes a constraint on k . While k = 1 produces an unbiased (while high variance) estimate of the expected value, the choice of k has to be at least 1 / (1  X  q ) to provide an upper bound for the estimate of the qth  X  X igh X  quantile (more generally we have k  X  max(1 /q, 1 / (1  X  q ))). The issue of how exactly to estimate the q X  X h quantile when q/k is not an integer (and hence the quantile of the empirical distribution falls in between observed y j values) also has to be addressed. In our experiments below we simply select the integer closest to q/k as the index for the estimate, although interpolation approaches may be considered as well.

The definition of neighborhood is determined based on the set of variables, the distance function and implicit proper -ties such as scaling of the variables. The performance of a k NN model is very much subject to the suitability of the neighborhood definition as a good approximation of the true conditional distribution  X  this is true for the standard pro b-lem of estimating the conditional mean and no less so for estimating conditional quantiles.
Tree-induction algorithms are very popular in predictive modeling and are known for their simplicity and efficiency when dealing with domains with large number of variables and cases. Regression trees are obtained using a fast divide and conquer greedy algorithm that recursively partitions t he training data into subsets. Therefore, the definition of the neighborhood that is used to approximate the conditional distribution is not predetermined as in the case of the k NN model but optimized locally by the choice of the subsets. Work on tree-based regression models traces back to Morgan and Sonquist [12] but the major reference is the book on classification and regression trees (CART) by Breiman et al. [5]. We will limit our discussion to this particular algorit hm.
A tree-based modeling approach is determined predomi-nantly by three components:
The most common choice for the splitting criterion is the least squares error (LSE). While this criterion is consiste nt with the objective of finding the conditional expectation, i t can also be interpreted as a measure of the improvement of the approximation quality of the conditional distributi on estimate. Tree induction searches for local neighborhood definitions that provide good approximations for the true conditional distribution P ( Y | x ). So an alternative inter-pretation of the LSE splitting criterion is to understand it as a measure of dependency between Y and x i by evaluat-ing the decrease of uncertainty (as measured by variance) through conditioning. In addition, the use of LSE leads to implementations with high computational efficiency based on incremental estimates of the errors for all possible spli ts.
Pruning is the most common strategy to avoid overfit-ting within tree-based models. The objective is to obtain a smaller sub-tree of the initial overly large tree, excludi ng those lower level branches that are unreliable. CART uses Error-Complexity pruning approach which finds an optimal sequence of pruned trees by sequentially eliminating the su b-tree (i.e., node and all its descendents) that minimizes the increase in error weighted by the number of leaves in the eliminated subtree: where E ( T t ) is the error of the subtree T t containing t and all its ancestors, and E ( t ) is the error if it was replaced by a single leaf, and S ( T t ) is the number of leaves in the subtree. E ( . ) is measured in terms of the splitting criterion (i.e., for standard CART it is squared error loss). Given an optimal pruning sequence, Breiman et al. suggest to determine the optimal level of pruning by cross validation on a holdout set .
Finally CART estimates the prediction for a new case that falls into leaf node l similarly to the k NN algorithm as the mean over the set of training responses D l in the leaf: where n l is the cardinality of the set D l of training cases in the leaf.

Given our objective of quantile estimation, the most ob-vious adjustment to CART is to replace the sample mean estimate in the leaves with the quantile estimate using the empirical local estimate  X  G D l ( c ) of P ( Y | x ) as in (3).
A more interesting question is whether the LSE splitting (and pruning) criterion should to be replaced by a quantile loss. On one hand, finding splits that minimize the quantile loss on the training sample in the leaves corresponds direct ly to our prediction objective. On the other hand, having the best possible approximation of the conditional distributi on can be expected to result in the best quantile estimates of th e distribution and minimizing the distribution variance cou ld lead to a better approximation than the direct optimiza-tion of quantile loss, in particular for very high quantiles . In addition, changing the splitting criterion to quantile l oss causes severe computational problems. The evaluation of a split now requires the explicit construction of the two sets of predictions in each leaf, sorting both of them in order to find the correct quantile and the calculation of the loss. We will not consider the issue of efficiency any further, as there is already some related work by Torgo [15]. In our experi-ments below, we investigate the success of the two splitting criteria in terms of predictive performance only.
Our framing of wallet modeling as a quantile estimation task leads to a well-defined machine learning problem. While we can assess the relative model performance for different modeling approaches in terms of quantile loss (as we will do in Section 5), the fundamental question of how well our models perform as wallet estimators remains open. And in particular, we have no obvious choice of the quantile for the IBM customer wallets. In this section we describe the Market Alignment Program (MAP), which demonstrates a major use for wallet estimates within IBM, and which sup-plied us with a unique opportunity to evaluate the success of wallet estimation models in capturing experts X  notion of IBM customer wallets.
In 2005 IBM started an initiative called the Market Align-ment Program ([10]) to address the major challenge of align-ing sales resources with the best revenue-generating oppor -tunities. The main objective of MAP is to drive the sales resources allocation process based on field-validated anal yt-ical estimates of future revenue opportunity. While expert knowledge is crucial to facilitate the sales process, the so le reliance on expert knowledge may lead to an overly strong focus on existing and large customers with limited growth opportunities. In order to facilitate the discussion with s ales experts, initial model-based wallet estimates are used as a starting point in the assessment of future revenue opportu-nity. An integral part of the MAP process is the validation of the analytical estimates via an extensive set of workshop s conducted with sales leaders. These interviews rely on a web-based tool to convey the relevant information, and to capture the expert feedback on the analytical models. The tool allows the sales team to input their estimates of rev-enue opportunity, as well as their reasons for recommending a change to the model results. As a side effect of this in-terview process, we now have at hand  X  X alidated X  customer-wallet estimates against which we can evaluate our models.
The complete MAP process consists of: 1. Developing a consistent data model incorporating all 2. Estimating the wallets for all IBM customers within 3. Developing a web-based tool designed to display his-4. Conducting workshops with sales leaders to validate 5. Shifting sales resources to sales accounts with the large st
For the initial round of workshops conducted in late 2005, the wallet estimates displayed in the MAP tool were gen-Figure 2: Expert Feedback versus the shown Q-k NN Wallet Predictions erated using a simple and intuitive quantile k NN approach that follows our definition of REALISTIC wallet. For each of the approximately 100,000 customers available for this study, we identified a set of 20 similar companies, where similarity is based on the industry and a measure of size (either sales or employees). From this set of 20 firms, we discarded all companies with zero IBM revenue in the par-ticular product group and reported the median of the IBM revenues of the remaining companies as the wallet estimate for this product group. The choice of the median (50th per-centile) reflected a combination of statistical considerat ions and ad-hoc business constraints (such as conforming to a  X  X nown X  total market opportunity, i.e., sum over all compa-nies). This estimated wallet was sometimes smaller than the realized 2004 IBM revenue for some companies. The final reported estimates were therefore taken as the maximum of the k NN model estimate and last year X  X  revenue (we refer to this operation as flooring ). These wallet predictions were aggregated into opportunities by sales account according t o the IBM internal account structure. The MAP workshops covered a total of about 1200 important sales accounts. Fig-ure 2 presents the expert-validated opportunity for a major IBM software brand as a function of the calculated oppor-tunity estimates from the 2005 workshops.

We can make a number of interesting observations here: 1. 45% of the opportunity estimates are accepted with-2. For 15% of the accounts, the experts concluded that 3. Of the remaining 40% of accounts, opportunity esti-4. The horizontal lines reflect the human preference to-5. The opportunities and the feedback appear almost jointly
While the purpose of the MAP workshop was not primar-ily to validate our models, as a side effect we now have 1200  X  X rue X  wallets (based on experts X  opinions) for 2006 at the sales-account level that we can use to evaluate our differ-ent wallet modeling approaches. We decided to eliminate the 15% of accounts where the experts reduced the oppor-tunity to zero for competitive reasons. This information is not available for modeling and we did not want to bias our evaluation. It may be an interesting classification problem to identify accounts without opportunity, but we want to focus the quality of our estimates if there is an opportunity .
It is a well established fact that monetary quantities (like wallets) typically have a very long tailed exponential-typ e distribution. The few largest numbers, corresponding to biggest IBM customers, would typically dominate model-ing and evaluation. And homoscedasticity assumptions un-derlying most modeling and evaluation approaches typicall y do not hold for monetary quantities. This is clearly shown by the experts X  tendency to make wallet adjustments by percents rather than dollars (corresponding roughly to ho-moscedasticity on the log scale), as discussed above. On the other hand, success of models in a business environment is ultimately measured in dollars, not log-dollars. We theref ore chose to evaluate model performance on three scales: error on original (dollar) scale, square root scale, and log scale . In addition to the sum of squared errors for each scale, we also considered the absolute error. This provides us with a total of six different performance criteria.

We adopted the modeling approaches discussed in the pre-vious section, and to account for our lack of knowledge about what truly defines a customer wallet, we allowed the model parameters  X  such as the quantile being modeled, neighbor-hood size, etc.  X  to vary. In total, we built nearly 100 different models, counting all variations of model parame-ters, input variables, and different quantiles. We followed the same aggregation process and calculated the resulting opportunities for 2006 at the sales-account level for each model. We finally ranked all models according to each of the 6 performance criteria, and compared how often a given model appears within the top 20 of all models. Table 1 shows the relative performance of the best variants and includes a s reference points the performance of the shown Q-k NN model and of a very naive model that predicts simply for each ac-ctoun the maximum revenue over the last 3 years.

The results in Table 1 support the following conclusions: Table 1: Model performance in terms of number of times a wallet model was within the top 20 models across the 6 different performance metrics.
 Model Brand 1 Brand 2 Brand 3 Shown Q-k NN 6 5 6 Max Revenue 03-05 1 3 4 Linear Q-Regression 6 4 5 Q-k NN 1 0 2 Q-k NN + Flooring 3 6 6 Q-Tree 1 4 4
Based on this analysis for three major product brands, we concluded that the linear quantile regression model showed the best overall performance, when using a quantile of 0.8. In other words, this quantile regression model provided the best agreement with the expert feedback collected during the initial 2005 MAP workshops. Hence, this model was selected to provide the revenue opportunity estimates for MAP workshops conducted in late 2006. The results from this iteration are not yet available.
In the sequel we compare experimentally the performance of different quantile estimation models that were presented in Section 3:
In addition we generate quantile estimates from the cor-responding traditional modeling approaches including lin -ear regression, k NN, and CART. These models attempt to model the mean and cannot perform well in high quantile estimation unless the distribution is highly skewed and the expected value happens to correspond to a high quantile. However, if we assume homoscedastic gaussian error model, then predicting the mean and predicting a high quantile are equivalent, as discussed in Section 2, and the 0.9th condi-tional quantile of Y would be just E ( Y | x )+1 . 28  X  . We adopt this approach to convert estimates of conditional means int o estimates of 0.9th conditional quantiles in our experiment s. We estimate  X  2 from the test set as E ( y i  X   X  y i ) 2 .
As baseline, we also present the performance of the opti-mal Constant model that predicts for all observations the 90th percentile of the training set.

We collected in addition to the IBM domain a number of public datasets with similar motivation: 1. Adult: available from the UCI Machine Learning Repos-2. California Housing: available from the StatLib repos-3. KDD-Cup 1998: available at the UCI KDD Archive 4. IBM Wallet: a subset of the data discussed in Section
The main domain characteristics including size of train-ing and test sets and the number of numeric and nominal variables are shown in Table 2.
 Table 2: Characteristics of the datasets including training and test size as well as the number of nu-meric and nominal variables.
 Domain Training Test Numeric Nominal KDD98 4840 4870 7 3 California 13760 6880 8 0 Adult 32560 16280 6 8 IBM 20000 63000 14 6
Since all of our datasets are reasonably large, we used the same training-test split for all modeling approaches accor d-ing to the sizes in Table 2. The results in Table 3 report the average quantile loss on the test sample with the standard deviation of this average (estimated as the standard devia-tion of the quantile loss divided by the root of the number of test cases) in parentheses. Given the large size of all tes t sets and the central limit theorem we can argue that the av-erage quantile loss is approximately gaussian and we can use the deviation to assess the uncertainty in these evaluation scores. The bold indication of the best models is based on pairwise t tests to the method with the best results. Any method that was not significantly worse than the best one is in bold.

Looking at Table 3, we can draw several conclusions: 1. On these datasets, the quantile modeling approaches 2. Not surprisingly, there is no clear winner among the 3. The IBM dataset suffers from the highest evaluation 4. The particularly bad performance of all standard mod-5. The two tree-based quantile models (LSE and QL Q-the mean loss in parentheses.
 6. Both tree-based ensemble methods, Bagged LSE and
As discussed in Section 2, we can also evaluate our suc-cess in predicting a high quantile by the percentage of test cases where the model predictions are higher than actual observations.

Table 4 shows the percentage of time that each model predicts a higher value than the observed response on the test set. As we can see, all numbers are between 82% and 91%, and they have a slight tendency for negative bias (i.e., predicting above the response slightly less than 90% of the time). Linear Quantile Regression clearly does the best job in this criterion. We hypothesize that this is due to its be-ing the simplest method, which is least prone to overfitting, hence is most consistent in terms of test-set performance.
Given our motivation for high-quantile estimation, as a way of estimating  X  X hat can be hoped for X , it is interesting to consider the difference between conditional mean of the standard methods and those of the quantile modeling ap-proaches. For example, on the Adult dataset, this can tell us how much higher the salary request would be of a person using a quantile model to assess her prospects compared to someone using a standard approach and trying to estimate their expected salary. We would have liked to analyze these differences on our motivating problem of IBM wallet, but due to the extreme instability of these predictions, do not really trust them. On the Log-IBM dataset, interpretabilit y is an issue. Thus, we chose to analyze the Adult dataset in more detail. We display in Figure 3 a histogram of the difference in predictions between the CART model (without the adjustment to prediction quantiles we discussed above) and Bagged LSE model on this dataset . As expected, prac-tically all differences are non-negative. The mean differenc e is $18643 and the median $16715, representing the typical differences in salary expectations between the humble and the ambitious.

The corresponding dollar mean differences in prediction for KDD98, California and IBM are $6.24, $63757, $612380, respectively. This dramatically demonstrates the differen ce between predicting the mean and predicting the 90th quan-tile on these problems. Figure 3: Histogram of differences in prediction be-tween CART and Bagged LSE on Adult test set.
In this paper we have argued for the practical importance of high-quantile modeling in many problem domains, includ-ing wallet estimation, price/salary prediction and others . We reviewed the statistical considerations involved in de-signing methods for high-quantile estimation and describe d some existing quantile modeling methods, as well as our own adaptations of k NN and CART to quantile modeling.
Next, we described the MAP application and utilized the output from its first iteration to analyze which of a large candidate set of models is most consistent with IBM Sales executives X  notion of wallet. One interesting conclusion f rom our analysis is that the experts relied quite heavily on the numbers we presented to them (which were the output of a simplistic  X  X irst approximation X  model). The second con-clusion is that the experts X  notion of customer wallet seems most consistent with a high-quantile model for the 0.8 quan-tile, compared to our previously proposed definition of the 0.9 quantile [13]. The model which performed best overall was a linear quantile regression model which (naturally) re -lies heavily on the previous-year observed sales revenue to predict current-year wallet.

We then performed an empirical study on several prob-lems where high-quantile modeling is a well motivated goal. Our main conclusions are: where the optimal performance would be 0.9.

In summary, there is a set of algorithms readily available to address the relevant issue of quantile modeling. The nec-essary adjustments to classical machine learning techniqu es such as tree induction are straight forward and result in re-liable, interpretable, and efficient solutions.
 We thank Sholom Weiss, Paulo Costa, Alexey Ershov and John Waldes for useful discussions on the topic of this paper . [1] StatLib: Data, Software and News from the Statistics [2] S. D. Bay. UCI KDD archive. Department of [3] C. L. Blake and C. J. Merz. UCI repository of [4] L. Breiman. Bagging predictors. In Machine Learning , [5] L. Breiman, J. H. Friedman, Olshen, R. A., and C. J. [6] P. Chaudhuri and W. Loh. Nonparametric estimation [7] J. Friedman, T. Hastie, and R. Tibshirani. Additive [8] R. Koenker. Quantile Regression . Econometric Society [9] J. Langford, R. Oliveira, and B. Zadrozny. Predicting [10] R. Lawrence, C. Perlich, S. Rosset, J. Arroyo, [11] N. Meinshausen. Quantile regression forests. Journal [12] Morgan and Sonquist. Problems in the analysis of [13] S. Rosset, C. Perlich, B. Zadrozny, S. Merugu, [14] I. Takeuchi, Q. V. Le, T. Sears, and A. Smola. [15] L. Torgo. Functional models for regression tree leaves . [16] B. Wansink, R. J. Kent, and S. J. Hoch. An anchoring
