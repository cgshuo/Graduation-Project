 classification under wide ly varying amounts of computational resources. For example, if asked to classify an instance taken from a bursty stream, we may have from milliseconds to minutes to return a class prediction. For such problems an anytime algorithm may be especially useful. In this work we show how we can convert the ubiquitous nearest neighbor classifier into an anytime algorithm that can produce an instant classification, or if given the luxury of additional time, can utilize the extra time to increase classification accuracy. We demonstrate the utility of our approach with a comprehensive set of experiments on data from diverse domains. classification under widely varying amounts of computational resources. For example, if asked to classify an instance taken from a bursty stream [2][23], we may have from milliseconds to minutes to return a class prediction. For su ch problems an anytime algorithm may be especially useful. execution time for quality of results [7]. In particular, an anytime algorithm always has a best-so-far answer available, and the quality of the answer improves with execution time. The user may examine this answer at any time, and then choose to terminate the algorithm, temporarily suspend the algorithm, or allow the algorithm to run to completion. The utility of anytime algorithms for data mining has been extensively documented [4][5][22]. of nearest neighbor classifier s in real world settings we will consider some motivating examples below. Audio Sensor Monitoring : In ongoing work we are attempting to build classifier s for  X  X mart X  insect traps that can identify th e species and sex of automatically captured insects [26]. While the single feature of wing beat frequency is quite effective by itself, our best results come from a nearest neighbor algorithm that considers many additional features, including sound amplitude, time of day, season, temperature, humidity etc. Our classifier must run on low-powered embedded computers attached to dis posable smart traps. This computer has very limited computational resources, making exhaustive nearest neighbor search quite demanding even in databases of only a few thousand instances. Furthermore, in empirical studies we found that the insects inter arrival time can vary from tenths of seconds to tens of minutes. In such an environment an anytime algorithm will allow the classifier to make the best possible decision in the time permitted. Industrial Applications : As noted in [3]  X  Many industrial applications require classification of items placed on a moving conveyor  X . The distance measure used may have a complexity as high as O( n 3 ) in order to achieve robustness despite rotation and other distortions [1]. In domains such as manufacturing it may be possible to know the frequency at which the objects in question pass under the camera. However in other applications, such as fruit sorting and grading [27], the inter arrival times can vary greatly. The amount of time the algorithm has to classify an object before it must take action (i.e., blowing the suspect fruit off the conveyer with compressed air) can vary by up to 3 orders of magnitude. Once again using an anytime algorithm can allow us to make the best decision in the time available. ubiquitous nearest neighbor classifier to an anytime algorithm that can produce an instant classification, or if given the luxury of a dditional time, can increase classification accuracy. The framework is based on the simple intuition that important exemplars (instances highly representative of a given class) should be examined first. We show that, for most problems, a simple generic algorithm can produce a high quality ordering of the exemplars. Section 2, we review related work and discuss some background material. In Section 3 we introduce a formal definition of our anytime nearest neighbour classification. We introduce our observations on index ordering in Section 4. Section 5 introduces three general heuristics for ordering the instances for our algorithm. We perform an extensive empirical evaluation in Section 6. In Section 7 we provide a detailed case study of a real world application of our algorithm to streaming data. Finally, Section 8 offers some conclusions and suggestions for future work. algorithms that trade execution time for quality of results [7]. In particular, after some small amount of  X  X etup time, X  an anytime algorithm always has a best-so-far answer available, and the quality of the answer improves with execution time. desirable properties of anytime algorithms:  X  Interruptability : After some small amount of  X  Monotonicity : the quality of the result is a non- X  Measurable quality : the quality of an approximate  X  Diminishing returns : the improvement in solution  X  Preemptability : the algorithm can be suspended anytime classifiers can be used in many application domains. The work of Myers, et al. considers an anytime classification fra mework [19] for topic identification in the Natural Language Processing domains. In computer vision anytime classifiers have been successfully used to perform a fast reconfiguration to a view database as reported by Heidemann et al. [11]. The task is to recognize multiple objects using image processing. Yamada et. al reported an anytime algor ithm that successfully controls web robots to create Personal Web Maps (PWM) using Self Organizing Maps [24]. in supporting teamwork in a multi-agent system [15]. Kotenko et. al have investigated the anytime framework in domains as diverse as virtual soccer (Robocup), simulations of battle operations in autonomous flight vehicles, and simulations of distributed coordinated computer attacks. conventional machine learning algorithms to anytime algorithms, including anytime inductive logic programming [18], the anytime Na X ve Bayes Text Classifier [22], and anytime Bayesian Networks [12]. Others have noted that certain algorithms can be regarded as anytime algorithms even if they were not designed for that purpose [22]. users should interrupt an anytime algorithm to get the best-so-far answer since anytime algorithms provides us  X  a tradeoff between solution quality and computation time that has proved useful in applying artificial intelligence techniques to time-critical problems  X  [10]. We never know when algorithms should optimally be interrupted in advance. Thus algorithms should be equipped with appropriate stopping criteria by monitoring learning performances. Therefore algorithms should achieve better results asymptotically so that they can return the approximately good answers before user interruption [30]. For this reason we do not consider contract anytime classifiers, focusing only on interruptible anytime classifiers in this paper. order the index of training data for asymptotically good results. However this work applies to decision trees [8], and Na X ve Bayes classifiers [25], and to the best of our knowledge, no ordering heuristics has been reported for nearest neighbor. anytime nearest neighbor algorithms that can be interrupted anytime and get the best answers in limited time. We also propose generic heuristics that can sort the index of training data suitable for anytime nearest neighbor classifiers. In an extensive empirical evaluation we will show that our methods work exceptionally well although they are quite simple. which we refer to as Database . We can access the i exemplar from this set with Database.object ( i ), and the Database.class_label ( i ). The instances in the database can be any objects we are in terested in classifying, such as classic database tuples, strings, graphs, time series, webpages etc. Assume that Index is a permutation of the integers from 1 to m , where m is the size of Database . Finally, assume O is an object we wish to classify using an anytime nearest neighbor algorithm with Database as our training data.  X  S  X  m , the algorithm will be interrupted and we report a class label prediction for O . Given the above notation, Table 1 shows our anytime nearest neighbor algorithm. member of each class and tentatively assign its class label to the nearest exempl ar. Note that the algorithm cannot be interrupted during this phase of the algorithm, however this is a very small amount of time. After this setup time, the algorithm can be interrupted at any point. Until it is interrupted, or has exhaustively compared O to the entire database, it will compare O to each object in the database in the order predefined in Index variable, updating the class prediction to reflect the current cumulative results. sorted. The only way we can help the algorithm above is to sort the Index such that  X  X seful X  examples will appear early. In Section 4 we will make some observations about this problem before introducing several concrete algorithms as solutions in Sections 5. additional observations about the anytime algorithm. distance (Database.object(Index p ),O) can be any distance measure, including Euclidean distance, Manhattan distance, correlation, etc. Where appropriate it can also be any specialized distance measure such as string edit distance, Dynamic Time Warping etc. Note also that the time and space complexity for classification has not changed, although we must do some additional preprocessing work in creating the Index variable. producing an anytime nearest neighbor algorithm to finding an ordering of Index . There are m ! such orderings. It might be imagined that for any particular dataset there is an optimal ordering of the Index . Such a speculation is important, because it suggests a possible efficient algorithm. Many similar problems can be efficiently solved by dynamic programming if the optimal arrangement with n objects can be derived from the optimal arrangement with n -1 objects. abandon any hope of such an optimal ordering. Consider Figure 2, which shows a simple two-dimensional classification problem, which we call the Japanese Flag (JF) problem. We will use this problem as a running example throughout this work. imagine that we are allowed to keep just one instance from class A , and we are asked to find the optimal n instances from class B . As illustrated in Figure 3 the chosen instances will form a Voronoi tessellation of the plane. the error rate is equivalent to minimizing the area enclosed by the true class boundary and its Voronoi piecewise linear approximation, which in turn is equivalent to the Archimedean problem of approximating a circle with n straight lines. The implications for the task at hand are obvious. In this case the optimal 4 instances and the optimal 3 instances have zero intersections. We cannot derive the optimal arrangement for n instances from the optimal arrangement with n -1 instances. We could instead attempt to solve the problem of finding the optimal arrangement of instances, given perfect knowledge of the distribution of the interruption times. However a fundamental (and realistic) assumption of anytime algorithms is that we have no way of knowing when an interruption might take place, even on average. algorithm, we note that there are only two ways to modify the algorithm: by changing the distance measure and by changing the Index ordering heuristic. As distance measurement selection is an independent concern well-documented in many other papers, will only focus on the critical Index ordering subroutine intuition is as follows: We begin by finding the  X  X orst X  exemplar (Line 4), and place a pointer to it in the last position in the Index (Line 5). To ensure that we don X  X  consider it again, we replace the exemplar in the original list with a null (Line 6). We then repeatedly find the worst exemplar remaining and place a pointer to it in the last unoccupied place in the Index . contained, except we have not yet explained how we defined the  X  X orst X  exemplar. This is a deliberate omission, which allows us to explore many possibilities, and allows anyone to use our framework simply by creating a definition of the  X  X orst X  exemplar. Before considering this problem in the next section we will make some observations about the ordering algorithm. force the first few exemplars pointed to by the Index to include one of each class. In practice however, this is almost always the case under any definition of exemplar quality. first, X  rather than to search  X  X est-first. X  That is to say, we could in principle search in the opposite direction, find the best instance and place it in the first place in the Index , then find the next best instance and place it in the second place in the Index etc. The main reason for this decision is the difficulty in defining  X  X est X  in the early stages of the algorith m. It is hard to measure the utility of single instance in the early stages of  X  X est-first X  search because we are interested in the decision boundaries, which are defined by (at least) two instances of different classes. techniques for defining the worst exemplar. Random and BestDrop are considered to allow some baseline comparison, and our method SimpleRank , is discussed last.  X  Random : Here the find_location_of_worst_exemplar subroutine on line 4 of Table 2 simply returns a random number between 0 and m-1. We define this algorithm to allow a simple baseline comparison.  X  BestDrop : There has been much work on data editing (numerosity reduction/condensing) for nearest neighbor classification [20][28]. Such algorithms have very similar goals to the current work, except these algorith ms explicitly know in advance the stopping value S . Perhaps the most referenced work in this area is by Wilson and 
Martinez [28], who introduced 3 algorithms for determining the worst exemplar. All these algorithms create some list of nearest neighbors, of both the same class ( associates ) and of different classes ( enemies ), and use a weighted scoring function based on this list to determine the worst exemplar. We have implemented all three variants and we report only the best performing variant for each possible S value.  X  SimpleRank : The intuition behind this algorithm is to give every instance a rank according to its contribution to the classification. We do leave-one-out 1-nearest-neighbor classification for the training data, and the rank of the instance is calculated as the following formula: where x j is an instance having x as its nearest neighbor. This ranking typically produces many ties for the worst instance, which we break by further sorting the instances by thei r distance to their nearest neighbor of the same class. Section 6, we will preview here the utility of the SimpleRank method on the JF classification problem introduced in Section 4. Figure 4 shows the first ten instances encountered by the anytime nearest neighbor algorithm, using both SimpleRank and Random ordering. The ten points were obtained from an original set of 2,000. The testing error rate, assuming we interrupt after seeing ten points, was 7.75% for SimpleRank and 24.36% for Random ordering. SimpleRank create a decision boundary that is a pretty good approximation to the pentagon which is the optimal solution for this number of exemplars. the ordering algorithm in Table 2 is O( m 3 log( m )), because the outer loop on line 4 repeats O( m ) times, and the subroutine find_location_of_worst_exemplar requires sorting m objects, after doing an all-to-all comparison. However, by simply caching the results of the first iteration of the outer loop on line 4, and only updating the instances that where affected by the temporary removal of the worse instance, we can reduce the complexity to O( m 2 ). To concretely ground these numbers, on the largest problem considered in this work (the 11,340 training instances of the Forest Cover Type problem), the simple caching approach reduces the time complexity from 277.7 hours to 53.7 minutes using a Pentium(R)4 3.0GHz. evaluation of our ideas. We made an effort to consider a diverse range of problems in terms of size, feature types, and number of classes. For example, the problems we consider range from 351 to 581,012 instances, and we consider problems with real, categorical, and mixed features in addition to two specialized time series problems. Table 3 lists the properties of the datasets. preexisting training/test split was defined. In order to ensure our experiments are reproducible we have placed the exact data, (with split information) at [14], where we have also placed additional experiments that we could not fit in the paper. The original datasets except JF, Leaf, Face and Two_Pat came from www.ics.uci.edu/~mlearn/MLRepository.html. JF problem, which we have already mentioned in Section 3 (Figure 2). We randomly took 2,000 instances from 20,000 instances as training data, and used the remaining 18,000 as the testing set as described in Table 3. Figure 5 shows the accuracy obtained by both SimpleRank and Random. Note that while the training accuracy is slightly optimistic, it closely shadows the curve for the testing data. Random ordering. For example, the accuracy assuming we interrupt after seeing ten points is 92.25% for SimpleRank but only 75.64% for Random ordering. As with all our experiments, if the algorithm is not interrupted and both methods see all the instances, they will have identical accuracy. Since this dataset has only 690 examples, we used cross validation for the evaluation. This dataset contains both categorical and numerical attributes. We compare the accuracy curves of SimpleRank, BestDrop, and Random in this dataset. We found that BestDrop only slightly beats Random here, a finding which was echoed in all the other datasets. For this reason, and to enhance visual clarity, we will not include the results of the BestDrop in the rest of the figures, however the results are available at [14]. The results show that SimpleRank can achieve good results even if we have smaller datasets (and thus a smaller space to search over), with an accuracy difference of 5-7% between Random and SimpleRank for almost the entire range of possible interruption times. after seeing only 18% of the data, thereafter the accuracy begins to decrease (we see similar behavior in Figure 8). This non-monotonic behavior is undesirable in an anytime algorithm, although we note that it still beats random over the entire range of the dataset. In this case the non-monotonic behavior is caused by noisy (possibly mi slabeled) exemplars. We could improve accuracy by permanently removing exemplars which appear late in the Index (i.e data editing), however for simp licity we leave such considerations for future work. algorithm to the Letter dataset. The Letter dataset has 26 class labels corresponding to the letters from A to Z, of which the attributes are normalized from 0 to 1. The task in this dataset is to recognize letters given 16 features extracted from imag e data. Even in such a multiple-class problem we find that SimpleRank dominates Random ordering up to the first 2,000 training examples. which we consider in Figure 8. We used 7,494 examples for training and 3,498 for test respectively. Because we have a much smaller testing set the curves are less smooth, however, SimpleRank clearly beats Random up to the first 2,000 examples. Figure 9 is a particularly challenging dataset because of its size both in terms of the number of the instances and the number of attributes. The dataset has mixed type of attributes, including both categorical and numerical data. For simplicity, all of the attribute values are transformed to numeric data and normalized by subtracting the minimum value and then dividing by the maximum value on each attribute. We then use Euclidean distance, as with all the other datasets in this section. This is probably not the best choice for distance measure, but we are only interested in comparing the relative merits of our ordering algorithms, not in finding the best distance measure for mixed data types. This expl ains why our best accuracy using by 1-nearest-neighbor is 66.6%, while the best reported accuracy from UCI KDD Archive is higher, 70% using back propagation. Interestingly, after an initial  X  X ump X  in accuracy, th e accuracy curves almost increases linearly from 200 to 11,340. This phenomenon suggests that (unlike, say, Australian Credit or PenDigits) we could greatly benefit from obtaining more data in this domain. problem shown in Figure 10. Note that while the training curve for our algorithm is wildly optimistic our approach still beats Random a majority of the time. generic heuristic dominates random ordering, although the latter sometimes has increasingly competitive accuracy as more time passes before interruption, particularly for  X  X orest Cover Type X  and  X  X en Digits X  datasets. These results strongly support our claim that our generic ordering heuristic works well in a variety of application domains. study of a real world implementation of classification on a streaming problem. understand the behavioural responses of fish to man-made and natural environmental variations [16]. Such monitoring has implications for human as well a fish health, since fish health is an implicit measure of water quality. Currently, monitoring is most often done manually, by on-site human observers, and thus prone to human error and expensive in labour costs. One effort to mitigate the labour co sts is to record the fish migration on video, and allow users to  X  X ast-forward X  through time periods when no fish are observed. However both US Bureau of Reclamation (USBR), and the US Department of Agriculture (USDA) have recently noted that a fully-a utomated fish recognition system is urgently needed [29]. Many species look very similar, the raw data may be corrupted by bubbles and debris, and, as shown in Figure 19, the fish may be  X  X isaligned X  with their representations in the database. for streaming data is difficult is because robust similarity measures for shapes are typically very computationally demanding, for example the O( n 3 ) method of Adamek and Connor 0. Since the average number of fish observed at a monitoring station in a day is typically less than a few thousand, this may not seem like a problem. However, as the agricultural monitoring problem discussed in Section 1, the arrival times can greatly vary and thus the classification time for any given fish may be quite small. time is to reduce the resolution of the fish contour in a principled way. For example, in [16] 1 the authors note:  X  it is first necessary to reduce the number of data points on the contour to a reasonable number that can be evaluated using shape similarity measurement.  X  For example, on the fish contour shown in Figure 12, there are 1,824 raw datapoints. This is reduced down to a mere forty datapoints because they  X  ... found that a reduced data set of 40 points was sufficient to retain the important shape features for comparison  X  [16]. This dramatic data reduction did make the similarity measure more tractable, but we wondered if the assumption that it  X  retain (s) the important shape features  X  was true. We compared their results, which after considerable parameter tuning claimed  X  the highest recognition accuracy of 64% . X , with rotation invariant Euclidean distance on the raw data. Surprisingly this simple, parameter-free method achieves 88.57% accuracy. However, this dramatic improvement comes at a cost. Rotation invariant Euclidean distance requires that we compare one contour with every possible circular shift of another, and thus requires O( n 2 ) time. While there are recent methods to somewhat mitigate this quadratic complexity [13], we can see that this problem is ideally suited to the anytime classification framework. the Leaf classification problem considered above, it differs in one important asp ect. For leafs, the existence of a stem (petiole) gives an unambiguous starting point and thus removes the need for rotation invariance (cf. [13]). In contrast, finding a fixed starting point for fish can be so difficult (depending on the species) that it is simply more accurate to test all rotations. fish have similar shape characteristics, and the problem is non-trivial even for human experts. The seven species are Chinook Salmon , Winter Coho , Brown Trout , Bonneville Cutthroat , Colorado River Cutthroat , Yellowstone Cutthroat , and Mountain Whitefish . As noted above, video data from streams are plagued by debris, so we include images of debris in our experiments. Table 4 lists the properties of the dataset. can achieve 98.0% by always choosing the  X  debris  X  class. The results of the test are shown in Figure 13. Note that a random selection (but one that is guaranteed to have at least one of each class) is initially only slightly better than the default rate, and it slowly improves as more obj ects are seen. In contrast, our approach immediately achieves 99.4% accuracy, and improves more quickly.
 algorithm in the context of Zilberstein and Russell X  X  desirable properties for anytime algorithms [30]. small amount of setup time (the time taken to see one of each class). In general our algorithm is monotonic , however on some problems (Ionosphere, Australian Credit and Leaf) the accuracy actually goes down slightly after some point. Intuitively, this makes sense. If the dataset has mislabeled or noisy instances, those instances will be placed at the end of the Index by the ordering algorithm, although they really should be thrown away. We could address this problem by using an internal cross validation to determine if the instances at the end of the Index should be discarded [20][28]. dramatically satisfied. For almost all problems, we get 90% of the final accuracy after seeing only the first 10% of the data. The measurable quality property is a little more difficult to satisfy. In general the curves for the training error are very similar to those for testing error, but slightly optimistic. The exceptions to this are the very small datasets, and Forest Cover Type, which is known to have a training set that is unrepresentative of the test set. We can th erefore use the training error curves to obtain approxima te expected accuracy for unseen instances. Finally, our algorithm is clearly preemptable . If we wish to stop it temporarily and restart it later, we need only save three numbers, the current class prediction ( best_match_class ), the current best-so-far distance ( best_match_val ), and the number of objects we have seen thusfar ( p ). ask  X  why not use an indexing technique to speed up nearest neighbor? X  , and  X  why not use a faster classification algorithm like a decision tree? . X  We are now in a position to answer these questions. theoretically use a Spatial Access Method (SAM) such as an R-tree [9] to quickly locate the nearest neighbor for our classifier. However there are several reasons why this is not a solution to the task at hand. In the best case, index trees require O(log 2 m ) time to locate the nearest neighbor, however this O(log 2 m ) comes with very high constants (which depend on the dimensionality of the data). The real utility of SAMs comes from minimizing costly disk accesses, but for a main memory problem on high dimensional data (as in Leaf, Face, Two_Pat, Forest Cover, Ionosphere etc) we are able to do a fast linear scan and see a large percentage of the data before the index structure returns an answer. During this time our anytime algorithm is interruptible, whereas an interrupted SAM does not have an answer of any kind. neighbor algorithm is that it is defined for any similarity/dissimilarity measure, including measures that either cannot be indexed, or are indexed only with great difficulty. For example, string, graph or tree edit distance, rotation invarian t Euclidean distance (cf. Section 8) [13], Mahalanobis distance, Earth Movers Distance, compression based similarity [17] etc. the nearest neighbor algorithm by using an eager learner such as a decision tr ee or Bayesian classifier, we can address this id ea with a single word: accuracy . There exist many problems for which the best-known classifier is Nearest Neighbor. As a concrete example, let us consider just time series classification, which we discussed at some length above in Sections 6 and 8. introduced decision trees to cl assify time series. On the Two Patterns dataset, they report error rates of 4.84% and 4.90% respectively. However, in our experiments on the same dataset (cf. [14]) our anytime algorithm achieves the same accuracy after seeing only 47 instances, and thereafter rapidly converges on a 0.0% error rate. While there have been several other attempts to classify time series with decision trees, to the best of our knowledge none of them comes close to the accuracy achieved by Nearest Neighbor. convert the ubiquitous nearest neighbor algorithm into an anytime algorithm. On a highly diverse set of problems, our algorithm can achieve high accuracy even if interrupted after seeing only a small fraction of the dataset. Future work includes investigation of other ordering algorithms and a field study of anytime classification for insect classification [26]. Geoffrey Webb, Jill Brady and Ying Yang for their useful suggestions. We further wish to acknowledge Dennis Shiozawa, Xiaoqian Xua, and Pengcheng Zhana at Brigham Young University, and Robert Schoenberger with Agris-Schoen Vision Systems for their help with the fish monitoring problem. grant IIS-0237918. 
