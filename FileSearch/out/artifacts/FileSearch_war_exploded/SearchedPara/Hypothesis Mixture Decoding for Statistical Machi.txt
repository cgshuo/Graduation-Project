 Besides tremendous effort s on constructing more complicated and accurate mod el s for statistical machine translation (SMT) ( Och and Ney, 200 4 ; Chiang, 2005; Galley et al. , 2006 ; Shen et al., 2008; Chiang 2010 ) , many re searcher s have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT system s as well .

S ystem combination is built on top of the N -best outputs generated by multiple component systems (Rosti et al., 200 7 ; He et al., 200 8 ; Li et al., 2009 b ) which align s multiple hypotheses to build conf u-sion network s as new search space s , and output s the highest scoring path s as the final translation s . C onsensu s decoding , on the other hand, can be based on either single or multiple system s : s ingl e syste m based methods (Kumar and Byrne, 200 4 ; Tromble et al., 2008; DeNero et al., 2009 ; Kumar et al., 200 9 ) re -rank translation s produced by a single SMT model using either n -gram posteriors or expected n -gram counts . Because hypotheses generated by a single model are highly correlated , improvements obtained are usually small ; r ecently, dedicated efforts have been made to extend it from single system to multiple systems ( Li et al., 2009 a ; D eNero et al., 2010; Duan et al., 2010 ) . Such m e-thods select translation s by optimizing consensus models over the combined hypotheses using all component systems X  posterior distributions.
Although these two types of approaches have shown consistent improvements over the standard Maximum a Posteriori (MAP) decoding scheme , most of them are implemented as post -processing procedures over translations generated by MAP decoders . In this sense, the work of Li et al. (2009 a ) is different in that both partial and full hypotheses are re -ranked during the decoding phase directly using consensus between translations from diffe r-ent S MT systems . However , their method does not change component systems  X  search spaces.

T his paper present s hypothesis m ixture d ecoding (HM decoding) , a new decoding scheme that pe r-forms translation reconstruction using hypotheses generated by multiple component systems . H M decoding involves two decoding stages : first , each component system decode s the source sentence independently, with the explored search space kept for use in the next step ; second , a new search space is constructed by composing existing hyp o-theses produced by all component systems usin g a set of rules provided by the HM decoder itself , and a new set of component model independent fe a-tures are used to seek the final best translation from this new constructed search space .

We evaluate by combining two SMT models with state -of -the -art performances on the NIST Chinese -to -English translation tasks . Experimental results show that our approach outperform s the best component S MT syste m by up to 2 . 11 BLEU points . C onsistent improvements can be observed over several related decoding tech niques as well , includ ing word -level system combination, coll a-borative decoding and model combination. 2.1 Motivation and Overview SMT models based on different paradigms have emerged in the last decade using fairly different levels of linguistic knowledge. Motivated by the success of system combination research , the key contribution of this work is to make more effective use of the e xtended search spaces from different SMT models in decoding phase directly , rather than just post -processing their final outputs . We first begin with a brief review of single system based SMT decoding , and then illustrate major challenges to this end .

Given a source sentence , an SMT decoder seek s for a target translation that best matches as its translation by maximizing the following conditional probability : where is the feature vector that includes a set of system specific features, is the weight vector, as a sequence of translation rule applications . Figure 1 illustrates a decoding example , in which the f inal translation is generated by recursively composing partial hypotheses that cover different ranges of the source sentence until the whole input sentence is fully covered , and the feature vector of the final translation is t he aggregation of feature vectors of all partial hypotheses used . 1 However, hypotheses generated by different SMT systems cannot be combined directly to form new translations because of two major issues :
The first one is the heterogeneous structures of different SMT models . For example , a string -to -tree system cannot use hypotheses generated by a phrase -based system in decoding procedure , as such hypotheses are based on flat structures , which cannot provide any additional information needed in the syntactic model.

The second one is the incompatible feature spaces of diff erent SMT model s. For example, even if a phrase -based system can use the lexical forms of hypotheses generated by a syntax -based system without considering syntactic structures , the feature vectors of these hypotheses still cannot be aggre gated together in any trivial way , because the feature sets of SMT models based on different paradigms are usually inconsistent .

To address the se two issues discussed above, w e propose HM decoding that perform s translation reconstruction using hypotheses generated by mu l-tiple component systems . 2 O ur method involves two decoding stages depicted as follows : 1. Independent decoding stage, in which e ach 2. H M decoding stage, where a mixture search 
HM decoding can use lexicalized hypotheses of arbitrary SMT models to derive translation, and a set of component model independent features are used to compute translation confidence. W e di s-cuss mixture search space construction , details of model and feature design s as well as HM decoding algorithms in Section 2.2, 2.3 and 2.4 respectively . 2.2 Mixture Search Space Constructi on Let denote component MT systems , at position and ending at position . We use by , and denot ing the mixture search space of constructed by the HM decod er , which is defined recursively as follows:  X  . This rule adds all comp o- X  , in wh ich 
Figure 2 shows an example of HM de coding , in which h ypotheses generated by two SMT systems are used together to compose new translations . Since search space pruning is the indispensable procedure for all SMT systems, we will omit its explicit expression in the following descriptions and algorithms for convenience . 2.3 Model s and Feature s Following the common practice in SMT research , we use a linear model to formulate the preference of translation hypotheses in the mixture search space . Formally, we are to find a translation that maximizes the weighted linear combination of a set of real -valued features as follows : where is a n HM decoding feature with its corresponding feature weight .

In this paper , the HM decoder does not assume the availability o f any internal knowledge of t he underlying component systems . The HM decoding features are independent of component models as well, which fall into two categories :
The first category contains a set of consensus -based features, which are inspired by the success of consensus decoding appro aches . These features are described in details as follows : 1) : the n -gram p osterior feature of 2) : the stemmed n -gram p ost erior 3) : the n -gram p osterior feature of 4) : the length p osterior feature of the Note here that features in and will be computed when the computations of all the remainder features in two categories have already finished for each in , and they will be used to update current HM decoding model scores .
C onsensus features based on component search spaces have already shown effectiveness (K umar et al. , 2009 ; DeNero et al., 2010; Duan et al., 2010 ). W e leverage consensus features based on the mixture search space newly generated in HM decoding as well . The l ength posterior feature (Zen and Ney, 2006) is used to adjust the prefer ence of HM decod er for longer or shorter translation s , and the stemmed n -gram posterior features are used to provide more discriminative power for HM deco d-ing and to decrease the effects of morphological changes in words for more accurate computation of consensus statistic s .

The second feature category contains a set of general features. Although there are more features that can be incorporated in to HM decoding besides the ones we list below , we only utilize the most representative ones for convenience : 1) : the w ord count feature . 2) : the language model feature . 3) : the dictionary -based feature that 4) and : reordering feature s 5) and : reordering fe a-6) : the feature that count s how many 
The MERT algorithm (Och, 2003) is used to tune weights of HM decoding features. 2.4 Decoding Algorithm s T wo CKY -style algorithms for HM decoding are presented in this subsection . The first one is based on B TG (Wu, 1997) , and the second one is based on SCFG , similar to Chiang ( 2005) . 2.4.1 B TG -based H M Decoding The first algorithm , B TG -H M D , is presented in Algorithm 1 , where hypotheses of two consecutive source spans are composed using two B TG rules :  X  Straight rule . It combines translations of  X  Inverted rule . It combines translation s of These two rules are used bottom -up until the whole source sentence is fully covered. We use t wo reordering rule penalty features, and 
In BTG -HMD , i n order to derive translations for a source span , we compose hypotheses of any two smaller spans and using two B TG rules in line 9 and 10 , denotes the operation s that first ly combine and using one BTG rule and second ly compute HM decoding features for the new ly generated hypothesis . W e compute HM decoding features for hypotheses contained in all existing component search spaces From line 17 to 20, w e update current HM deco d-ing scores for all hypotheses in using the n -gram and length posterior feature s computed based on . When the whole source sentence is fully covered , we return the hypothesis with the maximum model score as the final best translation . 2.4.2 SCFG -based HM Decoding The second algorithm , SCFG -H M D , is presented in Algorithm 2 . An additional rule set , which is provided by the HM decoder, is used to compose hypotheses . It includes hierarchical rules extracted using Chiang ( 200 5 )  X  X  method and glue rule s. T wo reordering rule penalty features , and using hierarchical rules and glue rule s .
 Compar ed to BTG -HMD , the key difference s in SCFG -HMD are located from line 7 to 11, where the translation for a given span is generated by replacing the non -terminals in a hierarchical rule terminal of , is the search space for predicted by the HM decoder. 3.1 Model Combination and M ixture M odel M odel combination (DeNero et al., 2010) is an approach that selects translations from a conjoint search space using information from multiple SMT component models; Duan et al. (2010) present s a similar method , which utilize s a mixture model to combine distributions of hypotheses from different system s for Bayes -risk computation , and selects final translations from the combined search space s using MBR decoding . Both of these two methods share a common limitation: they only re -rank the combined search space , without the capability to generate new translation s . In contrast , by re using hypotheses generated by all component systems in HM decoding , translations beyond any existing search space can be generated . 3.2 Co -D ecoding and Joint D ecoding Li et al. (2009 a ) propose s collaborative decoding, an approach that combines translation syste ms by re -ranking partial and full translations iteratively using n -gram features from the predictions of other member systems . However, in co -decoding, all member systems must work in a synchronous way , and hypotheses between different systems cannot be shared during decoding procedure ; Liu et al. (2009) propose s joint -decoding, in which multiple SMT models are combine d in either translation or derivation levels. However, their method relies on the correspondence between nodes in hypergraph outputs of different models . HM decoding , on the other hand , can u se hypotheses from component search space s directly without any restriction . 3.3 Hybrid Decoding Hybrid decoding (Cui et al., 2010) resembles our approach in the motivation . This method use s the system combination technique in decoding directly to combine partial hypotheses from different SMT models. However, confusion network construction brings high computational complexit y . What  X  s more , partial hypotheses generated by confusion network decoding cannot be as sign ed exact feature val ues for future us e in higher level decoding , and t hey only use feature values of 1 -best hypothesis as an approximation . HM decoding, on the other hand, leverage s a set of enriched features , which are computable for all the hypotheses generated by either component systems or the HM decoder. 4.1 Data and Metric E xperiments are conducted on the NIST Chinese -to -E n glish MT tasks. T he NIST 2004 ( MT04 ) data set is used as the development set , and evaluation results are reported on the NIST 2005 ( MT05 ), the newswire portion s of the NIST 2006 ( MT06 ) and 2008 ( MT08 ) data set s . All bilingual corpora available for the NIST 2008 constrained data track of Chinese -to -English MT task are used as training data, which contain 5.1M sentence pairs, 128M Chinese words and 147M English words after pre -processing. Word a lignments are performed using GIZA++ with the intersect -diag -grow refinement . T he English side of bilingual corpus plus Xinhua portion of the LDC English Gigaword V ersion 3.0 are used to train a 5 -gram l anguage model .
Translation performance is measured in terms of case -insensitive BLEU score s (Papineni et al., 2002), which compute the brevity penalty using the shortest reference translation for each segment . Statistical significance is computed using the boo t-strap re -sampling approach proposed by Koehn (2004 ). Table 1 gives some data statistics . 4.2 Component System s For convenience of comparing HM decoding with several related decoding techniques , we include two state -of -the -art SMT systems as component systems only :  X  PB . A phrase -based system (Xiong et al. ,  X  D H PB . A string -to -dependency tree -based Phrasal rules are extracted on all bilin gual data , h ierarchical rules used in D HPB and reordering rules used in SCFG -HM D are extracted from a selected data set 3 . R eordering model used in PB is trained on the same selected data set as well . A tri gram d ependency language model used in DHPB is trained with the outputs from Berkeley parser on all language model training data . 4.3 Contrast ive Techniques W e compare HM decoding with three multiple -system based decoding techniques :  X  Word -L evel S ystem C ombination ( SC ) . W e  X  Co -decoding ( CD ) . We re -implement it based  X  Model Combination ( MC ). Different from co -4.4 Comparison to Component Systems W e compare d HM decoding with two component SMT system s first (in Table 2) . 30 feature s are used to annotate each hypothesis in HM decoding , including : 8 n -gram posterior features computed from PB/DHPB forests for ; 8 stem med n -gram posterior features computed from stemmed PB/DHPB forests for ; 4 n -gram pos t-erior features and 1 length posterior feature co m-puted from the mixture search space of HM d e-coder for ; 1 LM fea ture ; 1 word count feature; 1 dictio nary -based feature ; 2 grammar -specified rule penalty features for either BTG -HMD or SCFG -HMD ; 4 count features for new ly generated n -grams in HM decoding for .
 All n -gram posteriors are computed using the eff i-cient algorithm proposed by Kumar et al . (2009). From table 2 we can see , both B TG -HM D and SCFG -HM D outperform decoding results of the best component system (DHPB) with significant improvements : + 1.50 , + 1.76 , and + 1.26 BLEU points on MT05 , MT06, and MT08 for B TG -HM D ; + 1 . 43 , + 1. 63 and + 1.09 BLEU points on MT05, MT06, and MT08 for SCFG -HM D . We also notice that BTG -HMD performs slight better than SCFG -HMD on test sets. We think the potential reason is that more reordering rules are used in SCFG -HMD to handle phrase movements than BTG -HMD do; h owever, current HM decoding model lacks the ability to distinguish the qualit ies of different rules. We also investigat e on the effects of different HM -decoding features. F or the convenience of comparison , we divide them into five categories :  X  Set -1 . 8 n -gram posterior features based on 2  X  Set -2 . 8 stemmed n -gram posterior features  X  Set -3 . 4 n -gram posterior features and 1  X  Set -4 . 2 grammar -specified reordering rule  X  Set -5 . 4 count features for unseen n -grams
Except for the dictionary -based feature, all the features contained in Set -1 are used by the latest multiple -system based consensus decoding tec h-niques (DeNero et al., 2010; Duan et al., 2010). W e use them as the starting point. E ach time , w e add one more feature set and describe the ch ange s of performance s by drawing two curves for each HM decoding algo rithm on MT08 in Fig ure 3 .
With Set -1 used only, HM -decoding has already outperform ed the best component system, which shows the strong contribution s of these features as proved in related work ; small gains (+0.2 BLEU points) are achieved by using 8 stemmed n -gram posterior features in Set -2 , which shows consensus statistics based on n -grams in their stem form s are also helpful ; n -gram and length posterior features based on mixture search space bring improvements as well ; reordering rule penalty features and count features for unseen n -grams boost new ly gen erated hypotheses specific for HM decoding , and they contribute to the overall improvements . 4.5 Comparison to System Combination Word -level system combination is state -of -the -art method to improve translation performance using outputs generated by multiple SMT systems . I n this paper , w e compare our HM decoding with the combination method proposed by Li et al. (2009 b ). Evaluation results are shown in Table 3.

Compar ed to word -level system combination , both B TG -HMD and SCFG -HMD can provide significant improvements . We think the potential reason for these improvements is that, system combination can only use a small portion of the component systems  X  search spaces ; HM decoding , on the other hand, can make full use of the entire translation spaces of all component systems. 4.6 Comparison to Consensus Decoding Consensus decodi ng is another decoding technique that motivates our approach . W e compare our HM decoding with two latest multiple -system based consensus decoding approaches , co -decoding and model combination. We list the comparison results in Ta ble 4 , in which CD -PB and CD -DHPB denote the translation results of two member systems in co -decoding respectively , CD -Comb denotes the results of further combination using outputs of CD -PB and CD -DHPB , MC denotes the results of model combination.

Table 4 shows that after an additional system combination procedure , CD -Comb performs slight better than MC. B oth BTG -HMD and SCFG -HMD per form consistent better than CD and MC on all blind test sets , due to its richer generative capability and usage of larger search spaces . 4.7 System Combination over BTG -HMD As BTG -HMD and SCFG -HMD are based on two different decoding grammars , w e could perform system combination over the output s of these two settings ( SC BTG+SCFG ) for further improvements as well , just as Li et al. (2009a) did in co -decoding . We present evaluation results in Table 5 .

After system combination , translation results are significantly better than all decoding approaches investigated in this paper : up to 2.11 BLEU point s over the best component system (DHPB) , up to 1.07 BLEU point s over system combination , up to 0.74 BLEU point s over co -decoding , and up to 0.81 BLEU point s over model combination . 4.8 Evaluation of Oracle Translation s In the last part , we evaluate the quality of oracle translations on the n -best lists generated by HM decoding and all decoding approaches discussed in this paper . Oracle performances are obtained using the metric of sentence -level BLEU score pro posed by Ye et al. (2007), and each decoding approach outputs its 1000 -best hypotheses , which are used to extract oracle translations .

Results are shown in Table 6: compar ed to each single component system, decoding methods based on multiple SMT systems can provide significant improvements on oracle translations ; word -level s ystem combination, collaborative decoding and model combination show similar performances , in which CD -Comb performs best ; BTG -HMD, SCFG -HMD and SC BTG+SCFG can obtain significant improvements than all the other approaches, and SC BTG+SCFG performs best on all evaluation sets. In this paper, w e have present ed the hypothes i s mixture decodin g approach to combine multiple SMT models , in which hypotheses generated by multiple component systems are used to compose new translations. HM decoding method integrates the advantages of both system combination a nd consensus decoding techniques in to a unified framework . E xperimental results across different NIST Chinese -to -English MT evaluation data sets have validate d the effectiveness of our approach .
In the future, we will include more SMT models and explore more features, such as syntax -based features, helping to improve the performance of HM decoding. We also plan to investigate more complicated re ordering models in HM decoding.
