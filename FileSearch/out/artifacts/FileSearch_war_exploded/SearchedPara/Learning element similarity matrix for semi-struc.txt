 Jianwu Yang  X  William K. Cheung  X  Xiaoou Chen Abstract Capturing latent structural and semantic properties in semi-structured documents (e.g., XML documents) is crucial for improving the performance of related document ana-lysis tasks. Structured Link Vector Mode (SLVM) is a representation recently proposed for modeling semi-structured documents. It uses an element similarity matrix to capture the latent relationships between XML elements X  X he constructing components of an XML document. to compute the matrix using the machine learning approach. In addition, we incorporate term semantics into SLVM using latent semantic indexing to enhance the model accuracy, with the element similarity learnability property preserved. For performance evaluation, we applied the similarity learning to k -nearest neighbors search and similarity-based clustering, and tes-ted the performance using two different XML document collections. The SLVM obtained via learning was found to outperform significantly the conventional Vector Space Model and the edit-distance-based methods. Also, the similarity matrix, obtained as a by-product, can provide higher-level knowledge on the semantic relationships between the XML elements. Keywords Semi-structured document analysis  X  Learning similarity matrix  X  Similarity-based clustering  X  Extended Vector Space Model 1 Introduction With the recent proliferation of Extensible Markup Language (XML) technologies, it is getting more common for digital data to be archived [ 1 , 2 ] as XML documents which are semi-structured (i.e., requiring no rigid and fixed schema in advance). The need to develop a new set of tools for analyzing and managing semi-structured documents is immediate. XML is the W3C recommended markup language for representing semi-structured data (or documents). Other than that providing a natural representation for hierarchical structures and repeating fields or structures, XML makes use of document type definitions (DTDs) and XML schemas to allow different degrees of variations in the document structure. Its structural flexibility makes it attractive for representing data including news items (NewsML), mathematical formulae (MathML), vector graphics (SVG), as well as some proprietary designs used by specific enterprises and institutions. 1 The wide adoption of XML documents has already triggered some recent development on document analysis techniques for managing semi-structured documents.

While XML documents can contain elements of different data types, we, in this paper, focus on those with elements containing textual descriptions. In the following,  X  X emi-structured documents X  and  X  X ML documents X  will be used in an interchangeable manner. 1.1 Semi-structured document analysis Even though the tasks of interest for semi-structured document analysis are still clustering, classification and retrieval, conventional document analysis tools developed for unstructured documents [ 3 ] fail to take the full advantage of the structural properties of XML documents at the level of their composing elements.

In contrast with ordinary unstructured documents, XML documents represent their syn-the associated schema specified in either DTD or XML Schema format. In addition, XML documents can be cross-linked by adding IDREF attributes to their elements to indicate the linkage. Thus, techniques designed for semi-structured document analysis normally take into account the information embedded in both the element tags as well as their associated contents for better performance. For example, the structural similarity between a pair of IDREF-free XML documents can be defined as some edit distance between two unordered labeled trees, 2 i.e., to compute the minimum number of operators needed to edit the tree of one form to that of the other form. In the literature, different tree edit distances have been proposed for measuring XML document structural dissimilarity [ 4 , 5 ], which are equivalent in principle except for the edit operators allowed and whether repetitive and optional XML elements were considered. However, computing the edit distance between a pair of unordered labeled trees is NP-complete [ 6 ] in general and yet the distance is not optimal in any sense. This is unde-sirable for large-scale applications. An alternative is to apply some heuristics. For example, one can measure the depth difference with reference to the root element for defining struc-tural dissimilarity between a pair of XML elements [ 7 , 8 ]. The depth differences can then be aggregated for estimating the overall document structural dissimilarity. While the associated computational cost is low, the accuracy is limited. Other than trees, XML documents were represented as time series in [ 9 ], with each occurrence of a tag corresponding to an impulse. Document similarity was then computed by comparing the corresponding Fourier coefficients of the documents. This approach does not take into account the order in which the elements appear and is adequate only when the XML documents are drastically different from each other, i.e., they have very few tags in common. In [ 10 ], WordNet X  X n ontology of general concepts [ 11 ] has been used to measure the semantic similarity of the elements X  names and their values. However, in many applications, domain-specific knowledge is needed instead, which is sometimes not easy to be captured. In [ 12 ], an approach extending a logic-based framework to deal with also uncertainty has been proposed for merging two XML documents at the element level.

Structured Link Vector Mode (SLVM), which forms the basis of this paper, was originally proposed in [ 13 ] for representing XML documents. It was extended from the conventio-nal Vector Space Model (VSM) by incorporating document structures (represented as term-by-element matrices), referencing links (extracted based on IDREF attributes), as well as element similarity (represented as an element similarity matrix). In [ 13 ], the element simi-larity was pre-set to be related to the path difference between two elements as well as their depth difference with reference to the root derived from the document schema.

We here argue that to perform highly accurate XML document analysis, the computation of the document similarity should consider not only structural relationships but also semantic similarity. While the idea of considering the element semantics is obvious, it is still by far a seldom-considered factor.

Ta b l e 1 shows a more complete list of related works and their comparison in terms of representation and the nature of similarity considered. 1.2 Learning element similarity In this paper, we take the machine learning approach for estimating the element similarity matrix of SLVM (cf. kernel learning). By learning the similarity matrix based on a set of training examples collected from a specific domain, the computed similarity measure can be best tuned with respect to the corresponding domain. We expect that the resultant simila-rity matrix can take into account multiple factors including the structural and semantic ones automatically. Depending on whether the training examples are unlabeled or simply labeled with pair-wise similarity information, the unsupervised or semi-supervised [ 21 ] learning algorithms are both studied in this paper. To the best of our knowledge, there does not exist any work in the literature that can automatically incorporate element structural and seman-tic similarities at the same time to achieve performance optimal for a specific application domain. As the XML elements X  contents are unstructured textual descriptions, conventio-nal document/text analysis techniques could still play a role. In particular, we adopt latent semantic indexing (LSI) [ 22 ] X  X n effective statistical method for identifying term semantics, and extend SLVM to SLVM-LSI to further enhance the model accuracy.

We evaluated the proposed approach by using the proposed similarity learning approach for k -nearest neighbor search and clustering, and demonstrated its superior performance when compared with some existing approaches based on two real-world datasets. Also, SLVM-LSI was found to outperform significantly the basic SLVM, showing the importance of conside-ring term semantics in the XML element similarity learning. In addition, we illustrated that the element similarity matrix, obtained as a by-product, is a useful piece of discovered know-ledge about the semantic relationships between the XML elements for subsequent document analysis. 1.3 Paper organization The rest of the paper is organized as follows. Section 2 describes SLVM and its extended ver-sion SLVM-LSI. Related works on document similarity measures together with the measure proposed for SLVM/SLVM-LSI are also included in the same section. Section 3 describes a proposed iterative algorithm for learning the element similarity matrix. Section 4 discusses how term semantics can be incorporated in SLVM without affecting the similarity matrix lear-nability. The experimental results are reported in Sect. 5 . Section 6 discusses some related research areas including link analysis and semi-structured document indexing and retrieval. Section 7 concludes the paper with future research directions. 2 Structured Link Vector Mode (SLVM) In this section, we present a mathematical formulation called Structured Link Vector Mode (SLVM) which was recently proposed for representing XML documents. Also, a correspon-ding document similarity measure is described, where the definition of an element similarity matrix is introduced. 2.1 Basic representation Vector Space Model (VSM) [ 23 ] has long been used to represent unstructured documents approach assumes that the term occurrences are independent of each other.
 Definition 2.1 Assume that there are n distinct terms in a given set of documents D. Let doc x denote the x th document and d x denote the document feature vector such that is the inverse document frequency of w i for discounting the importance of the frequently appearing terms, | D | is the total number of the documents, and DF( w i ) is the number of documents containing the term w i .

Applying VSM directly to represent semi-structured documents is not desirable as the document syntactic structure tagged by their XML elements will be ignored. For example, VSM considers two documents with an identical term appearing in, say, their  X  X itle X  fields to be equivalent to the case with the term appearing in the  X  X itle X  field of one document and in the  X  X uthor X  field of another. As the  X  X uthor X  field is semantically unrelated to the  X  X itle X  field, the latter case should be considered as a piece of less supportive evidence for the documents to be similar when compared with the former case. Using merely VSM, these two cases cannot be differentiated.

Structured Link Vector Mode (SLVM), proposed in [ 13 ], can be considered as an extended version of Vector Space Model for representing XML documents. Intuitively speaking, SLVM represents an XML document as an array of VSMs, each being specific to an XML element (specified by the &lt; element &gt; taginDTD). 3 Definition 2.2 SLVM represents an XML document doc x using a document feature matrix where m is the number of distinct XML elements, x ( i )  X  R n is the TFIDF feature vector forallj=1ton,and TF (w i , doc x . e j ) is the frequency of the term w i in the element e j of doc x .
 Definition 2.3 The normalized document feature matrix is defined as where the factor caused by the varying size of the element content is discounted via norma-lization.
 Example 2.1 Figure 1 shows a simple XML document. Its corresponding document feature vector d x , document feature matrix x , and normalized document feature matrix  X  x are shown in Figs. 2 , 3 , 4 respectively. Here, we assume that all the terms share the same IDF value equal to one.

The form of SLVM studied in this paper is only a simplified one where only the leaf-node elements in the DTD are incorporated without considering their positions in the document DOM tree and their consecutive occurrence patterns. In addition, the interconnectivity bet-ween the documents based on IDREF is also not considered. One obvious advantage is that this simplification makes the subsequent similarity learning much more tractable. Also, this kind of unigram-like approach makes it applicable to most of the unseen semi-documents as long as there are no newly encountered terms. If the consecutive occurrence patterns of the elements are to be taken into consideration, the most extreme case is to have each pos-sible path of the DOM tree corresponds to one column in Fig. 3 . This however will increase the computational complexity exponentially. Also, the generalization capability will be poor (e.g,. a book with three authors cannot be modeled if a maximum of two authors are assumed in the SLVM X  X  document feature matrix). More discussion related to the possible extensions can be found in Sect. 7 . 2.2 Similarity measures Using VSM, the similarity between two documents doc x and doc y is typically computed as the cosine value between their corresponding document feature vectors, given as similarity measure can also be interpreted as the inner product of the normalized document feature vectors.

For SLVM, with the objective to model the latent relationships between XML elements, introduced.
 Definition 2.4 The SLVM-based document similarity between two semi-structured docu-ments doc x and doc y is defined as where M e is a matrix of dimension m  X  m and named as the element similarity matrix .
The matrix M e in Eq. ( 2 ) captures both the similarity between a pair of XML elements as well as the contribution of the pair to the overall document similarity (i.e., the diagonal elements of M e are not necessarily equal to 1). An entry in M e taking a small value means that the two corresponding XML elements are unrelated and same words appearing in the two elements of two different documents will not contribute much to the overall similarity of them. If M e is diagonal, this implies that all the XML elements are not correlated at all with each other, which obviously is not the optimal choice. To obtain an optimal M e for a specific type of XML documents, we adopt the machine learning approach which is to be explained in the next section. 3 Learning XML element similarity matrix Given the SLVM formulation, the accuracy of the document similarity measure is directly affected by that of the XML element similarity matrix, which in turn will affect the subsequent document analysis tasks. In this section, we present how the optimal element similarity matrix can be computed in a problem-specific manner using machine learning methods. 3.1 Related works classification) that works directly on the inner product (commonly termed as kernel or dis-computing explicitly the transformed data representations. There exist in the literature some kernel functions proposed with reasonably good performance on average (e.g., Gaussian kernel, polynomial kernel [ 24 ]). The study of adaptive kernel has attracted some attention in recent year with the objective to obtain an optimal kernel in a problem-specific manner via training. One possible approach is to consider multiple predefined kernels and the lear-ning problem becomes computing an empirical quality functional (also called alignment) for picking the best kernel [ 25  X  29 ]. In this paper, we focus on a more generic approach of adap-ting directly the kernel function. In addition, we assume the kernel function to be a matrix, instead of one taking some parametric form.
 For example, in [ 30 , 31 ], the metric learning problem was posed as a convex optimization problem. In [ 32 ], the Mahalanobis distance (derived based on the training data X  X  covariance matrix) was adopted to define the matrix. Also, a model-based transductive approach was proposed in [ 33 ] for learning the kernel. In [ 34 ], an iterative algorithm was proposed for deriving the document similarity matrix based on the fact that document similarity and term similarity should be highly correlated and substantially affecting each other.
All the aforementioned methods are related to learning the kernel matrix for unstructured data. In this paper, we propose to apply kernel methods to learn the XML element similarity matrix for semi-structured XML documents. The element similarity matrix obtained by the element related semantics from XML document repositories. 3.2 Similarity learning for SLVM Inspired by [ 34 ], we propose an element similarity learning algorithm with the assumptions that ( 1 ) different element tags have different contributions to the overall XML document similarity and that ( 2 ) their relative contributions should be problem-specific, depending on multiple latent factors like elements X  semantics and their relative position in the documents, which are hard to be pre-determined. For example, as depicted in Fig. 4 , the contribution of the  X  X onfYear X  tag should be much less than that of the  X  X uthor X  tag to the overall document similarity measure. Also, terms appearing in one document X  X   X  X itle X  tag and another one X  X  tag pairs like  X  X itle X  and  X  X uthor X . These intuitive requirements are related not only to the XML data structure but also to their semantics, and thus cannot be fully satisfied using only structure related similarity measures like the edit distance [ 4 , 5 ].
 Definition 3.1 Given a set of | D | XML documents with m distinct XML elements, the SLVM-based document similarity matrix is defined as where B i  X  R m  X | D | is an element-document matrix specific to the i th term with its x th
Note that B i can be interpreted as a matrix storing the statistics of the i th term in each XML element given a particular document repository. Each entry of S d corresponds to the similarity between a pair of XML documents and is defined as Eq. ( 2 ), except for the coefficient  X  d . Also, the diagonal elements of M e are not confined to have the value 1 as its upper bound so as to capture the contribution of individual elements to the overall document similarity.
Given Eq. ( 3 ), the remaining task is to estimate the similarity matrix M e using a set of unlabelled XML documents as the training data ( unsupervised learning). With the notion that element similarity should affect document similarity and vice versa [ 33 ], we proposed to complement Eq. ( 3 )byEq.( 4 ), given as where  X  e is a real constant.

On the basis of Eqs. ( 3 )and( 4 ), an iterative algorithm for learning the element similarity matrix M e in SLVM can be obtained as
In addition, we assume that all the document similarity values are normalized to the range of [0, 1]. In other words, two totally different documents should have a similarity value equal to 0 and two identical documents should have a similarity value of 1; otherwise, the similarity should take some value in-between. We can prove (see Sect. 3.3 )thatif  X  d and  X  e satisfy Eqs. ( 7 )and( 8 ), given as all the entries X  values of matrix S d will fall between 0 and 1.

Note that the pair of iterative equations Eqs. ( 5 )and( 6 ) has an obvious trivial solution of having both matrices with all their elements taking 0 as their values. Thus, an additional value of one (the similarity of two identical documents should be one by definition). So, Eq. ( 6 ) is amended as where and I is an identity matrix. For implementation, one can simply set each diagonal element of S d to be always equal to one as shown in Step 4 of Fig. 5 .
 For the initialization of the algorithm, we take The proposed iterative learning algorithm is summarized in Fig. 5 .

Note that this learning algorithm is essentially unsupervised as the information about how the documents should be grouped is not used. To achieve semi-supervised learning, one to the given pair-wise similar documents throughout the learning iterations.
 |
D | ) n 0 t ) ,where n 0 is the average number of different terms in a document and t is the number of iterations. In our experiments, the number of iterations for convergence was found to be in the order of 5 X 10. 3.3 Convergence proof Here, we give the proof for the convergence of the iterative algorithm using a similar approach adopted in [ 34 ].
 Lemma 1 The entries of the similarity matrices S d and M e defined in the iterative equations ( 5 ) and ( 6 ) are bounded.
 Proof According to Eqs. ( 5 )and( 6 ), we can derive Eqs. ( 13 )and( 14 ) for computing the entries of M e and S d , given as Note that the diagonal elements of S d are explicitly set to take the value of 1. AccordingtoEqs.( 11 )and( 12 ), we know that 0  X  M 0 e ( u ,v)  X  1and0  X  S 0 d ( x , y )  X  1. M e and S d in the ( g + 1)th iteration can easily be shown to be bounded by [0,1] as follows: By induction, we can draw the conclusion that the entries of M e and S d after convergence are bounded by [0, 1].
 Lemma 2 The entries of similarity matrices S d and M e defined in the iterative equations are non-decreasing.
 conclusion that S 1 d ( x , y )  X  S 0 d ( x , y ) for all their entries.
 M ( u ,v) for all their entries.
 of matrix M e and all the non-diagonal entries of matrix S d ,wehave, M
S of the similarity matrices M e and S d are non-decreasing. Theorem 1 The iterative algorithm is convergent.
 Proof From Lemmas 1 and 2 , we know that M e and S d are bounded and non-decreasing. So, the iterative algorithm is convergent. 4 Incorporating latent semantics SLVM, taking the vector space approach for representing XML elements, inherits inevitably the limitations of VSM X  X ssuming terms to be independent of each other. Its lacking the cases caused by synonym and polysemy. For example, the meaning of the word  X  X uilding X  is similar to that of  X  X ouse X  or  X  X lat X  (synonym). Also, the word  X  X aper X  can be referred to as the material made of cellulose pulp for printing, or some writings like journal articles (polysemy). In general, the assumption that terms are independent of each other is not correct and the similarity measured by cosine or inner product between conventional document feature vectors failed to take into consideration the semantic relationships. 4.1 Learning term similarity matrix One obvious extension is to learn also the term similarity matrix as in [ 34 ]. But we, later on in this section, show that learning term similarity is not scalable to large problems and LSI could be used as an alternative.
 Definition 4.1 Given a set of | D | unstructured documents, the document similarity matrix is defined as documents, M t is as the term similarity matrix of dimension n  X  n ,and  X  1 is a real constant.
 Using the derivation tricks similar to that presented in Sect. 3.2 , the term similarity matrix M t can readily be estimated in either unsupervised or semi-supervised manners. The corres-ponding iterative algorithm for learning the term similarity matrix M t can be obtained based on the following two equations:
To incorporate this term similarity learning into the XML document similarity formulation, one possibility is to have a separate M t for each XML element. That is, term semantics are assumed to be independent between elements. This however is relatively rare to be the case and contents of different elements within one XML document should normally be describing things under the same context, and thus correlated. Also, this implies the need of learning m different n  X  n matrices, requiring a large amount of training data and a high computational cost. In this paper, we assume only one single M t for all the elements.
 AccordingtoEqs.( 13 )and( 14 ), we can get Eqs. ( 22 )and( 23 ) for computing S d and M formulation, given as The overall learning algorithm (semi-supervised version) is shown in Fig. 6 .

As the number of terms n is used to be much bigger than the number of elements m , the computational complexity of the learning algorithm with the term similarity considered becomes an important concern. The complexity for learning the term similarity matrix can data structure for storing the term-document matrix D (with each document vector stored as on average, where n 0 is the average number of different terms in each document. When n SLVM and found that the quadratic complexity makes it not scalable in practice. To alleviate the complexity problem, latent semantic indexing provides an alternative. 4.2 Using latent semantic indexing Latent semantic indexing (LSI) [ 22 ] is a technique commonly used in information retrieval for exploiting term semantic relationships. In particular, LSI projects documents from the original document feature space onto a corresponding  X  X emantic X  space via singular value decomposition (SVD) so that more robust semantic-based document similarity measure can be resulted.
 singular value decomposition as where U and V contain orthonormal columns and S is diagonal.

By restricting the matrices U , V and S to their first k &lt; min ( n , | D | ) columns, one can obtain the matrix where  X  D is the best square approximation of D by a matrix of rank k [ 6 ].

This newly derived term document matrix contains document feature vectors with term semantics taken into consideration (essentially due to term co-occurrence statistics). To deal with novel documents not included in the training data, one can project the novel document vector onto the semantic space and measure distance directly in the semantic space. According to [ 22 ], this pseudo-document projection can be computed as where U  X  S  X  1 is the transformation for the projection.

The corresponding semantic space is spanned by the column vectors of the associated matrix V . Another alternative is to use simply U as the transformation and the corresponding pseudo-document projection becomes which is equivalent to put d T LSI = d T  X  U  X  S 0 . For this choice, the corresponding semantic space is then spanned by the columns of S  X  V T
To apply LSI to SLVM, XML documents are first partitioned into segments based on the element tags. SVD is then applied to the term-segment matrix. An XML document will then be represented as a matrix x  X  R k  X  m , with each column being the projection of the element-specific feature vector on the semantic space (with a dimension of k ) . Note that we pooled the instances of all the elements together for the term semantic analysis. Unless there exist a way to correspond a set of the semantic spaces X  X ach for one particular element, having a common semantic space for all the elements seems more viable and thus is here adopted.

Regarding the computational complexity of direct implementation of LSI is O ( n 3 +| D | 2 ) [ 33 ]. By exploring some sparse data structure for storing the term-document matrix (e.g., las2 algorithm in SVDPACK X  X  [ 35 ]), the complexity of LSI can be reduced to is O ( zk ), where z is the number of non-zero elements in the matrix (proportional to n o | D | ) and k n is tional complexity of the subsequent iterative algorithm for learning the matrix M e becomes O ( m | D | ( m +| D | ) kt ) . The typical value of k that gives good performance ranges from 30 to 50. The value of n 0 is around 500 for the datasets used in our experiments. While the use of LSI may not be able to result in the optimal term similarity matrix, we adopted it as the semantic extension of SLVM (named SLVM-LSI in sequel) due to its computation tractability. 5 Experiments 5.1 Data sets for evaluation To evaluate the performance of the proposed similarity matrix learning algorithm, we used two collections of XML documents, namely, ACMSIGMOD Record [ 36 ] and  X  X hinese Encyclopedia Database X  (CEDB) [ 37 ].

ACMSIGMOD Record (see Fig. 7 for its corresponding DTD) is composed of around 1,000 documents obtained from the past issues of ACM SIGMOID Record. In order that we can contrast the performance between unsupervised and semi-supervised learning of the similarity matrix, we extracted only the data with category information. Besides, only those categories with 10 or more documents were extracted for fair evaluation. Thus, the final data subset of ACMSIGMOD Record used in our experiments contains 461 documents.  X  X hinese Encyclopedia Database X  (CEDB) is a digital archive of millions of XML docu-ments extracted from a Chinese encyclopedia with 74 categories. Its DTD is shown in Fig. 8 . We randomly extracted around 1,000 documents from it for the subsequent experiments. In order to test the sensitivity of the proposed algorithm on datasets with different sizes, we prepared a number of data subsets for both ACMSIGMOD Record and CEDB as shown in Ta b l e 2 for our experiments.

As mentioned, both ACMSIGMOD Record and CEDB contain elements with category information. In order to fully explore the effectiveness of the semantic learning capability (instead of picking it directly from the category element), we deliberately did not use the tag  X  X ategories X  of ACMSIGMOD Record and  X  X ubject X  of CEDB for training in our expe-riments so as to study how well the proposed setup can analyze document structure and content in the worst scenario. 5 Also, all the documents were preprocessed by (1) conver-ting all the words to lower case (for ACMSIGMOD Record), (2) going through the Porter stemming algorithm (for ACMSIGMOD Record), and (3) removing stop-words (for both ACMSIGMOD Record and CEDB). 5.2 Experiment setup A similarity matrix that can accurately capture the relationships between XML elements (and in turn XML documents) should benefit all the existing similarity-based algorithms for XML document analysis. In this paper, we evaluated the similarity learning algorithms using two applications, namely, (1) similarity-based retrieval and (2) similarity-based clustering .For the retrieval application, k -nearest neighbors ( k NN) search which tries to locate the k most similar documents given a document query was chosen. For the clustering application, agglo-merative hierarchical clustering (AGH) [ 38 ] was chosen but with the document similarity computed based on the similarity matrix learned using our proposed method.

For benchmarking, we implemented (in C++) conventional VSM, basic SLVM, a variant of SLVM with the element similarity estimated using the edit distance as well as the SLVM with similarity learning. For implementing LSI, we used the las2 algorithm implementa-tion in SVDPACK X  X  [ 35 ] for singular value decomposition. In addition, we tested both the unsupervised and semi-supervised versions of the similarity matrix learning algorithm for SLVM. All the experiments were run on a PC with a 3.0G Hz Intel CPU and 512 M RAM. The convergence criterion of the iterative algorithm for the similarity learning being used is
Cross-validation was adopted in our experiments to minimize the sampling bias of the training data. For preparing the training and test datasets, we first performed uniform sampling on documents of different categories to create data partitions of equal size. Then, we prepare training sets of different sizes by including different numbers of the partitions. 5.3 Performance evaluation For evaluating k NN search accuracy, we adopted the commonly used precision and recall rates, defined as category that the i th document is belonged to.

For similarity-based hierarchical clustering like AGH, a measure for computing cluster similarity is needed. In particular, we computed the similarity between a pair of clusters C i and C j as of the k th document in C i .

For evaluating the quality of clustering results, we used F measure [ 39 ] which combines the precision and recall rates as an overall performance metric. Based on a particular cluster-and those in the category are the desired results. Then, the F measure for the cluster-category pair can be computed. More specifically, for the j th cluster and i th category, the F measure associated is given as where N ij is the number of the i th category documents falling into the j th cluster, N j is the number of documents in the j th cluster, and N i is the number of documents in the i th category.
The quality of the j th cluster should then be measured as the maximum of the F measure scores between it and all the possible categories. The overall weighted F measure can then be computed as where N C is the total number of clusters, which is the same as the total number of categories. 5.4 Results and discussion 5.4.1 Performance comparison between learning-based and heuristics-based methods The experimental results corresponding to the use of VSM, edit distance and SLVM based on data subsets of different sizes extracted from ACMSIGMOD Record and CEDB are shown by setting the value of k as 10, 20, 30...100. In all the figures, the learning-based methods have been shown to be significantly better than those heuristic ones for both retrieval and clustering applications. It is especially true when the size of the training set is large.
For k NN search, the conventional VSM was found to be the worst for all the datasets, with the lowest precision X  X ecall curves shown in Figs. 9 , 10 , 11 , 12 , 13 . Using SLVM with the edit distance-based element similarity matrix, an improvement in precision by  X  5 X 10% was observed for ACMSIGMOD Record (see Figs. 9 , 10 ) but a deterioration of  X  1 X 2% for CEDB data subsets (see Figs. 10 , 11 , 12 , 13 ). SLVM adopting the proposed learning method outper-
For XML document clustering, VSM was again found to be the worst for all the datasets with the F measure score ranging from 29 X 42% for ACMSIGMOD Record (see Figs. 14 , 15 ) and from 35 to 46% for CEDB (see Figs. 16 , 17 , 18 ). Using SLVM adopting the edit distance, an improvement in F measure by  X  3 X 5% was observed for ACMSIGMOD Record (see Figs. 14 , 15 ) but a deterioration of  X  1 X 3% for CEDB (see Figs. 16 , 17 , 18 ). SLVM adopting the proposed learning method outperformed that using the edit distance method by 7 X 13% for ACMSIGMOD Record (see Figs. 14 , 15 ) and 3 X 8% for CEDB (see Figs. 16 , 17 , 18 ). 5.4.2 Effectiveness of incorporating LSI By adopting LSI to the proposed learning algorithm as explained in Sect. 4.2 (with the best semantic space dimension selected according to the training data), a significant performance For k NN search, the precision-recall curves moved up by  X  5 X 10% for ACMSIGMOD Record (see Figs. 9 , 10 )and  X  10 X 20% for CEDB (see Figs. 10 , 11 , 12 , 13 ). For clustering, an improvement in F measure score by  X  3 X 13% was observed for ACMSIGMOD Record (see Figs. 14 , 15 )and  X  3 X 28% for CEDB (see Figs. 16 , 17 , 18 ).
 d Record was found to be  X  7 X 8% for VSM,  X  8 X 9% for the editing distance method, and  X  4 X  8% for SLVM (i.e., SLVM-LSI), respectively. For CEDB, the improvements were found to be  X  12 X 25% for VSM,  X  22 X 29% for the editing distance method, and  X  20 X 25% for SLVM (i.e., SLVM-LSI) respectively. The results were consistent to what we expected. 5.4.3 Comparison between unsupervised and semi-supervised similarity learning The results reported in the previous two subsections were based on unsupervised learning. similar to that of the unsupervised version on average (as shown in Fig. 19 ). The performance discrepancies were less than 2% and there were only two cases for which the semi-supervised approach was performing better than the unsupervised approach by  X  4 X 8%. We believe that the use of the pair-wise similar information still has rooms for further improvement. 5.4.4 Efficiency of learning-based methods and incorporating LSI The gain in accuracy in XML document clustering and search reported in the previous sections is not without cost. Table 3 tabulates the computational time required for performing clustering and search on different data sets. As expected, the additional computational cost needed so as to incorporate the term semantics is significant, especially for the clustering application. In general, how to further reduce the computational complexity for deriving latent semantics from a document repository remains to be an open research issue. 5.4.5 High-level knowledge from the similarity matrix Other than performance boosting, the element similarity matrix obtained via learning can in fact provide higher-level knowledge about the conceptual relationships among the elements. The similarity matrix learned based on the ACM SIGMOD Record dataset was shown in Fig. 20 , with the element indices shown in Table 4 . The element pairs: {term, content}, {abstract, term}, {term, content}, {title, term}, {title, content} were found to be the most similar ones, whose validity can easily be validated based on the nature of those elements. In addition, the weightings of different elements contributing to the overall similarity measure were found to be relevant. For example, unimportant elements include conference year and initial page, while important ones include abstract, authors, and categories X  content. 6 Related research areas This paper focuses on analyzing the contents of semi-structured documents. Its theme also has some overlaps with some related research areas, including Web structure analysis and XML retrieval. Drawing relationships between them may result in further extension of this work. 6.1 Web structure analysis One distinct characteristic of semi-structure documents like XML or HTML is the support of adding reference links between documents. Applying link analysis to semi-structured documents has been shown useful for applications like topic distillation [ 40 ], estimating document associations [ 41 ], identifying web communities [ 42 ], web summarization [ 43 ], etc. The incorporation of linkage information via the document reference tags for enhancing the document similarity computation has not yet been explored in this paper, but the extension can easily be supported by SLVM as described in [ 12 ]. 6.2 XML document indexing and retrieval down to the XML element level and how the structural information can be incorporated for language model, Bayesian network, hidden Markov tree) and different element structural information (e.g., element level in the DOM tree [ 8 ], integrating language model with shrin-kage based on the document structure [ 13 ]) have been proposed and found to be effective in improving retrieval performance. However, so far, the research effort in exploring element semantics as what being done in this paper is still rather limited. In [ 11 ], Wordnet was used for computing elements X  similarity based on their names and values. However, Wordnet is a general ontology and may not be useful for some domain-specific applications. Building domain ontologies in many cases may require a large amount of effort from domain experts and thus machine learning capability that can infer the element semantics becomes attractive. 7 Conclusion and future works Measuring semi-structured document similarity is a fundamental issue for XML document analysis. In this paper, we studied in detail a proposed extension of SLVM called SLVM-LSI for representing XML documents so that term semantics, element similarity, as well as elements X  relative importance for a given set of documents can all be taken into account. Also, we formulated an iterative estimation procedure for automatically learning the element similarity matrix associated with SLVM-LSI and applying the similarity learning to k NN search and similarity-based clustering. The proposed method was demonstrated to outperform the conventional Vector Space Model and the edit-distance approach. Also, the proposed way of incorporating term semantics was shown to be simple and yet effective.

For future work, we are currently investigating the consideration of not only the elements at the leaf nodes of the XML document tree, but also frequently appearing element sub-structures so as to further enrich the structural representation of SLVM-LSI. There has been work in the literature that takes into account sub-structures in XML documents to derive their similarity [ 45 , 46 ]. Also, incorporating also link information between XML documents in the SLVMs being studies in this paper is another direction that is worth pursuing. In addition, it will be interesting to study how the similarity matrix obtained via the proposed learning algorithm can be applied to support multiple word sense identification which serves as an important component for automatic ontology generation.
 References Author Biographies
