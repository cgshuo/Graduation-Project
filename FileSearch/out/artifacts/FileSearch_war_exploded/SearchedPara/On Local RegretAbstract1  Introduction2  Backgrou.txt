 Martin Zinkevich MAZ @ YAHOO -INC . COM Yahoo! Research, Santa Clara, CA 95051 USA An online learning task involves repeatedly taking actions and, after an action is chosen, observing the result of that action. This is in contrast to offline learning where the de-cisions are made based on a fixed batch of training data. As a consequence offline learning typically requires i.i.d. assumptions about how the results of actions are generated (on the training data, and all future data). In online learn-ing, no such assumptions are required. Instead, the met-ric of performance used is regret: the amount of additional utility that could have been gained if some alternative se-quence of actions had been chosen. The set of alternative sequences that are considered defines the notion of regret. Regret is more than just a measure of performance, though, it also guides algorithms. For specific notions of regret, no-regret algorithms exist, for which the total regret is growing at worst sublinearly with time, hence their average regret goes to zero. These guarantees can be made with no i.i.d., or equivalent assumption, on the results of the actions. One traditional drawback of regret concepts is that the number of alternatives considered must be finite. This is typically achieved by assuming the number of available ac-tions is finite, and for practical purposes, small. In offline learning this is not at all the case: offline hypothesis classes are usually very large, if not infinite. There have been attempts to achieve regret guarantees for infinite action spaces, but these have all required assumptions to be made on the action outcomes (e.g., convexity or smoothness). In this work, we propose new notions of regret, specifically for very large or infinite action sets, while avoiding any sig-nificant assumptions on the sequence of action outcomes. Instead, the action set is assumed to come equipped with a notion of locality, and regret is redefined to respect this no-tion of locality. This approach allows the online paradigm with its style of regret guarantees to be applied to previ-ously intractable tasks and hypothesis classes. For t  X  { 1 , 2 ,... } , let a t  X  A be the action at time t , and u : A  X  R be the utility function over actions at time t . Requirement 1 For all t , max a,b  X  A | u t ( a )  X  u t ( b ) | X   X  . The basic building block of regret is the additional util-ity that could have been gained if some action b was chosen in place of action a : R T a,b = P T t =1 1( a a ) ( u t ( b )  X  u t ( a )) , where 1( condition ) is equal to 1 when condition is true and 0 otherwise. We can use this building block to define the traditional notions of regret. where x + = max( x, 0) so that R T, + a,b = max( R T a,b , 0) . In-ternal regret (Hart &amp; Mas-Colell, 2002) is the maximum utility that could be gained if one action had been chosen in place of some other action. Swap regret (Greenwald &amp; Jafari, 2003) is the maximum utility gained if each action could be replaced by another. External regret (Hannan, 1957), which is the original pioneering concept of regret, is the maximum utility gained by replacing all actions with one particular action. This is the most relaxed of the three concepts, and while the others must concern themselves with | A | 2 possible regret values (for all pairs of actions) external regret only need worry about | A | regret values. So although the guarantee is weaker, it is a simpler concept to learn which can make it considerably more attractive. These three regret notions have the following relationships. Infinite Action Spaces. This paper considers situations where A is infinite. To keep the notation simple, we will use max operations over actions to mean suprema opera-tions and summations over actions to mean the suprema of the sum over all finite subsets of actions. Since we will be focused on regret over a finite time period, there will only ever be a finite set of actually selected actions and, hence only a finite number of non-zero regrets, R T a,b . The sum-mations over actions will always be thought to be restricted to this finite set.
 None of the three traditional regret concepts are well-suited to A being infinite. Not only does | A | appear in the regret bounds, but one can demonstrate that it is impossible to have no regret in some infinite cases. Consider A = and let u t be a step function, so u t ( a ) = 1 if a &gt; y for some y t and 0 otherwise. Imagine y t is selected so always possible. Essentially, high utility is always just beyond the largest action selected. Now, consider y while 1 T P T t =1 u t ( y  X  ) = 1 (i.e., there is large internal and external regret for not having played y  X  ,) so the average regret cannot approach zero.
 Most attempts to handle infinite action spaces have pro-ceeded by making assumptions on both A and u . For exam-ple, if A is a compact, convex subset of R n and the utilities are convex with bounded gradient on A , then you can min-imize regret even though A is infinite (Zinkevich, 2003). We take an alternative approach where we make use of a notion of locality on the set A , and modify regret concepts to respect this locality. Different notions of locality then result in different notions of regret. Although this typically results in a weaker form of regret for finite sets, it breaks all dependence of regret on the size of A and allows it to even be applied when A is infinite and u is an arbitrary (although still bounded) function. Wide range regret meth-ods (Lehrer, 2003) can also bound regret with respect to a set of (countably) infinite  X  X lternatives X , but unlike our results, their asymptotic bound does not apply uniformly across the set, and uniform finite-time bounds depend upon a finite action space (Blum &amp; Mansour, 2007). Let G = ( V,E ) be a directed graph on the set of actions, i.e., V = A . We do not assume A is finite, but we do assume G has bounded out-degree D = max a  X  V |{ b : ( a,b )  X  E }| . This graph can be viewed as defining a no-tion of locality. The semantics of an edge from a to b is that one should consider possibly taking action b in place of action a . Or rather, if there is no edge from a to b then one need not have any regret for not having taken action b when a was taken. By limiting regret only to the edges in this graph, we get the notion of local regret. Just as with traditional regret, which we will now refer to as global re-gret, we can define different variants of regret.
 Local internal and local swap regret just involve limiting regret to edges in G . Local external regret is more sub-tle and requires a notion of edge lengths. For all edges ( i,j )  X  E , let c ( i,j ) &gt; 0 be the edge X  X  positive length. Define d ( a,b ) to be the sum of the edge lengths on a short-est path from vertex a to vertex b , and E b = { ( i,j )  X  E : d ( i,j ) = c ( i,j ) + d ( j,b ) } to be the set of edges that are on any shortest path to vertex b .
 Global external regret considers changing all actions to some target action, regardless of locality or distance be-tween the actions. In local external regret, only adjacent actions are considered, and so actions are only replaced with actions that take one step toward the target action. The factor of 1 /D scales the regret of any one action by the out-degree, which is the maximum number of actions that could be one-step along a shortest path. This keeps local external regret on the same scale as local swap regret.
 It is easy to see that these concepts hold the same relation-ships between each other as their global counterparts. More interestingly, in complete graphs where there is an edge between every pair of actions (all with unit lengths) and so everything is local, we can exactly equate global and local regret.
 Theorem 1 If G is a complete graph with unit edge lengths R The proofs of the paper X  X  theorems are not included for space reasons. When there is a useful insight, we discuss the proof techniques and implications. The full proofs can be found in the longer version of this work available as a technical report (Bowling &amp; Zinkevich, 2012).
 So our concepts of local regret match up with global regret when the graph is complete. Of course, we are not really in-terested in complete graphs, but rather more intricate local-ity structures with a large or infinite number of vertices, but a small out-degree. Before going on to present algorithms for minimizing local regret, we consider possible graphs for three different online decision tasks to illustrate where the graphs come from and what form they might take. Example 1 (Online Max-3SAT) Consider an online ver-sion of Max-3SAT. The task is to choose an assignment for n boolean variables: A = { 0 , 1 } n . After an assignment is chosen a clause is observed; the utility is 1 if the clause is satisfied by the chosen assignment, 0 otherwise. Note that | A | = 2 n which is computationally intractable for global regret concepts if n is even moderately large. One possi-ble locality graph for this hypothesis class is the hypercube with an edge from a to b if and only if a and b differ on the assignment of exactly one variable, and all edges have unit lengths. So the out-degree D for this graph is only n . Local regret, then, corresponds to the regret for not having changed the assignment of just one variable. In essence, minimizing this concept of regret is the online equivalent of local search (e.g., WalkSAT (Selman et al., 1993)) on the maximum satisfiability problem, an offline task where all of the clauses are known up front.
 Example 2 (Online Disjunct Learning) Consider a boolean online classification task where input features are boolean vectors x  X  { 0 , 1 } n and the target y is also boolean. Consider A = { 0 , 1 } n , to be the set of all disjuncts such that a  X  A corresponds to the disjunct x of a such that a i j = 1 . In this online task, one must repeat-edly choose a disjunct and then observe an instance which includes a feature vector and the correct response. There is a utility of 1 if the chosen disjunct over the feature vector results in the correct response; 0 otherwise. Although a very different task, the action space A = { 0 , 1 } n same as with Online Max-SAT and we can consider the same locality structure as that proposed for disjuncts: a hypercube with unit length edges for adding or removing a single variable to the disjunction. And as before | A | = 2 while D = n .
 Example 3 (Online Decision Tree Learning) Imagine the same boolean online classification task for learning disjuncts, but the hypothesis class is the set of all possible decision trees. The number of possible decision trees for n boolean variables is more than a staggering 2 2 n , which for any practical purpose is infinite. We can construct a graph structure that mimics the way decision trees are typically constructed offline, such as with C4.5 (Quinlan, 1993). In the graph G , add an edge from one decision tree to another if and only if the latter can be constructed by choosing any node (internal or leaf) of the former and replacing the subtree rooted at the node with a decision stump or a label. There is one exception: you cannot replace a non-leaf subtree with a stump splitting on the same variable as that of the root of the subtree. Edges that replace a subtree with a label have length 1, while edges replacing a subtree with a stump (being a more complex change) have distance 1.1. So, we have local regret for not having further refined a leaf or collapsing a subtree to a simpler stump or leaf. Notice that the graph edges in this case are not all symmetric (viz., collapsing edges). In essence, this is the online equivalent of tree splitting algorithms. While | A |  X  2 2 n , the out-degree is no more than ( n + 1)2 n +1 . The maximum size of the out-degree still appears disconcertingly large, and we will return to this issue in Section 5 where we show how we can exploit the graph structure to further simplify learning. We now present an algorithm for minimizing local swap re-gret, similar to global swap regret algorithms (Hart &amp; Mas-Colell, 2002; Greenwald &amp; Jafari, 2003), but with substan-tial differences. The algorithm essentially chooses actions according to the stationary distribution of a Markov pro-cess on the graph, with the transition probabilities on the edges being proportional to the accumulated regrets. How-ever there are two caveats that are needed for it to handle infinite graphs: it is prevented from playing beyond a par-ticular distance from a designated root vertex, and there is an internal bias towards the actual actions chosen. Formally, let root be some designated vertex. Define d to be the unweighted shortest path distance between two vertices. Define the level of a vertex as its distance from root: L ( v ) = d 1 ( root ,v ) . Note that, L ( root ) = 0 , and  X  ( i,j )  X  E , L ( j )  X L ( i ) + 1 . All of the algorithms in this paper take a parameter L , and will never choose actions at a level greater than L . In addition, the algorithms all main-tain values  X  R t i,j (which are biased versions of R t i,j these to compute  X  t j , the probability of choosing action j at time t . These probabilities are always computed accord-ing to the following requirement, which is a generalization of (Hart &amp; Mas-Colell, 2002; Greenwald &amp; Jafari, 2003). Requirement 2 Given a parameter L , for all t  X  T , and (b)  X  j  X  V such that L ( j ) &gt; L ,  X  t +1 j = 0 . (c)  X  j  X  V such that 1  X L ( j )  X  L , (e) If there exists j  X  V such that  X  t +1 j &gt; 0 and  X  t +1 to be the stationary distribution of the transition function whose probabilities on outgoing edges are proportional to their biased positive regret, with the root vertex as the starting state, and all outgoing transitions from vertices in level L going to the root vertex instead. Definition 2 ( b,L ) -regret matching is the algorithm that initializes  X  R 0 i,j = 0 , chooses actions at time t according to a distribution  X  t that satisfies Requirement 2 and after choosing action i and observing u t updates  X  R t i,j =  X  ( u t ( j )  X  u t ( i )  X  b ) for all j where ( i,j )  X  E , and for all other ( k,l )  X  E where k 6 = i ,  X  R t k,l =  X  R t  X  1 There are two distinguishing factors of our algorithm from (Hart &amp; Mas-Colell, 2002; Greenwald &amp; Jafari, 2003):  X  R 6 = R , and past a certain distance from the root, we loop back.  X  R differs from R by the bias term, b . This term can be thought of as a bias toward the action selected by the algorithm. This is not the same as approaching the negative orthant with a margin for error. This small amount is only applied to the action taken, which is very different from adding a small margin of error to every edge. Theorem 3 For any directed graph with maximum out-degree D and any designated vertex root , ( X  / ( L + 1) ,L ) -regret matching, after T steps, will have expected local swap regret no worse than, where E L = { ( i,j )  X  E |L ( i )  X  L } .
 The overall structure of the proof is similar to (Blackwell, 1956; Hart &amp; Mas-Colell, 2002; Greenwald &amp; Jafari, 2003) with a few significant changes. As with most algorithms based on Blackwell, if there is an action you do not regret taking, playing that action the next round is  X  X afe X . If not, the key quantity in the proof is a flow f i,j =  X  t +1 for each edge. On most of the graph, the incoming flow is equal to the outgoing flow for each node in levels 1 to L . Since all the flow out from the nodes on one level is equal to the flow into the next, the total flow into (and out of) each level is equal. Thus, the flow out of the last level is only 1 / ( L + 1) of the total flow on all edges since there are L + 1 levels, including the root.
 Traditionally, we wish to show that the incoming flow of an action times the utility minus the outgoing flow of an action times the utility summed over all nodes is nonpositive, and then Blackwell X  X  condition holds. In traditional proofs, for any given node, the flow in and out are equal, so regard-less of the utility, they cancel. For our problem, the flow out of the last level is really a flow into the ( L + 1) st level, not the zeroeth level, so the difference in utilities between the zeroeth level and the ( L + 1) st level creates a problem. On the other hand, because we subtract b from whatever action we select, we get to subtract b times the total flow. Since exactly 1 / ( L +1) fraction of the flow is going into the ( L + 1) st level, these two discrepancies from the traditional approach exactly cancel. The second term of Equation (9) is a result of the traditional Blackwell approach. In the fi-nal analysis, we must account for the amount b we subtract from the regret each round. This means that if we get  X  R to approach the negative orthant, we only have bT local swap regret left. This is the first term of Equation (9). The local swap regret algorithm in the previous section suc-cessfully drops all dependence on the size of the action set and thus can be applied even for infinite action sets. How-ever, the appearance of | E L | in the bound in Theorem 3 is undesirable as | E L |  X  O ( D L ) , and L is more likely to be 100 than 2, in order to keep the first term of the bound low. The bound, therefore, practically provides little beyond an asymptotic guarantee for even the simplest setting of Ex-ample 1. In this section, we will appeal to (i) the structure in the locality graph, and (ii) local external regret to achieve a more practical regret bound and algorithm.
 Cartesian Product Graphs. We begin by considering the case of G having a very strong structure, where it can be entirely decomposed into a set of product graphs. In this case, we can show that by independently minimizing local regret in the product graphs we can minimize local regret in the full graph.
 Theorem 4 Let G be a Cartesian product of graphs, G = G 1  X  ...  X  G k . Let R regret on the l th component graph, where the action at time t is the l th component of a t and regret is on the edges in G l that transform the l th component. Then, R T localexternal P The implication is that we if we apply independent regret minimization to each factor of our product graph, we can minimize local external regret on the full graph. For ex-ample, consider the hypercube graphs from Example 1 and 2. By applying n independent external regret algorithms (the component graphs in this case are 2-vertex complete graphs), the overall local external regret for the graph is at most n times bigger than the factors X  regrets, so under regret matching it is bounded by n  X  are able to handle an exponentially large graph (in n ) with local external regret only growing linearly (in n ). If the component graphs are not complete graphs, then we can simply apply our local swap regret algorithm from the pre-vious section to the graph factors, which minimizes local external regret as well.
 Color Regret. Cartesian product graphs are a powerful, but not very general structure. We now substantially generalize the product graph structure, which will allow us to achieve a similar simplification for very general graphs, such as the graph on decision trees in Example 3. The key insight of product graphs is that for any vertex b , an edge moves to-ward b if and only if its corresponding edge in its com-ponent graph moves toward b l . In other words, either all of the edges that correspond to some component edge will be included in the external regret sum, or none of the eges will. We can group together these edges and only worry about the regret of the group and not its constituents. We generalize this fact to graphs which do not have a product structure.
 Definition 5 An edge-coloring C = { C i } i =1 , 2 ,... for an arbitrary graph G with edge lengths is a partition of E : C i  X  E , S i C i = E , and C i T C j =  X  . We say that C is admissble if and only if for all b  X  V , C  X  C , and ( i,j ) , ( i 0 ,j 0 )  X  C , d ( i,b ) = c ( i,j ) + d ( i,b )  X  d ( i c ( i 0 ,j 0 ) + d ( j 0 ,b ) . In other words, for any arbitrary target, all of the edges with the same color are on a shortest path, or none of the edges are.
 We now consider treating all of the edges of the same color as a single entity for regret. This gives us the notion of local colored regret.
 Theorem 6 If C is admissible then R So by minimizing local colored regret, we minimize local external regret. The natural extension of our local swap regret algorithm from the previous section results in an al-gorithm that can minimize local colored regret.
 Definition 7 ( b,L, C ) -colored-regret-matching is the al-gorithm that initializes  X  R 0 C = 0 , for all C  X  C , chooses actions at time t according to a distribution  X  t that satis-action i and observing u t at time t for all C  X  C updates  X  R Theorem 8 For an arbitrary graph G with maximum de-gree D , arbitrarily chosen vertex root , and edge coloring C , ( X  / ( L + 1) ,L, C ) -colored-regret matching applied af-ter T steps will have expected local colored regret no worse than, where C L = { C  X  C | X  ( i,j )  X  C s.t. L ( i )  X  L } . The consequence of this bound depends upon the number of colors needed for an admissible coloring. Very small admissible colorings are often possible. The hypercube graph needs only 2 n colors to give an admissible color-ing, which is exponentially smaller than the total number of edges, n 2 n . We can also find a reasonably tight coloring for our decision tree graph example, despite being a com-plex asymmetric graph.
 Example 4 (Colored Decision Tree Learning) Reconsider Example 3. Recall that an edge exists between one decision tree and another if the latter can be constructed from the former by replacing a subtree at any node (internal or leaf) with a label (edge length 1) or a stump (edge length 1.1). We will color this edge with the pair: (i) the sequence of variable assignments that is required to reach the node being replaced, and (ii) the stump or label that replaces it. This coloring is admissible. We can see this fact by considering a color: the sequence of variable assignments and resulting stump or label. If this color is consistent with the target decision tree (i.e., the sequence exists in the target decision tree, and the variable of the added stump matches the variable split on at that point in the target decision tree) then the color must move you closer to the target tree. The previous section presented algorithms that minimize local swap and local external regret (by minimizing local colored regret). The regret bounds have no dependence on the size of the graph beyond the graph X  X  degree, and so pro-vide a guarantee even for infinite graphs. We now explore these algorithms X  practicality as well as illustrate the gen-erality of the concepts by applying them to a diverse set of online problems. The first two tasks we examine, online Max-3SAT and online decision tree learning, have not pre-viously been explored in the online setting. The final task, online disjunct learning, has been explored previously, and will help illustrate some drawbacks of local regret. In all three domains we examine two algorithms. The first minimizes local swap regret by applying ( X  / ( L + 1) ,L ) -regret matching with L chosen specifically for the problem. This will be labeled  X  X ocal Swap X . The second focuses on local external regret by using a tight, admissible edge-coloring and applying ( X  / ( L + 1) ,L, C ) -colored-regret matching. This will be labeled simply  X  X ocal External X . Online Max-3SAT. First, we consider Example 1. We ran-domly constructed problem instances with n = 20 boolean variables and 201 clauses each with 3 literals. On each timestep, the algorithms selected an assignment of the vari-ables, a clause was chosen at random from the set, and the algorithm received a utility of 1 if the assignment satis-fied the clause, 0 otherwise. This was repeated for 1000 timesteps. The locality graph used was the n -dimensional hypercube from Example 1. The admissible coloring used to minimize local external regret was the 2 n coloring that has two colors per variable (one for turning the variable on, and one for turning the variable off). In both cases we set L =  X  and b = 0 , since the bounds do not depend on L once it exceeds 20. This also achieved the best performance for both algorithms. The average results over 200 randomly constructed sets of clauses are shown in Figure 1, with 95% confidence bars.
 Figure 1 (a) shows the time-averaged colored regret of the two algorithms, to demonstrate how well the algorithms are actually minimizing regret. Both are decreasing over time, while external regret is decreasing much more rapidly. As expected, swap regret may be a stronger concept, but it is more difficult to minimize. The local external regret algo-rithm after only one time step can have regret for not having made a particular variable assignment, while local swap re-gret has to observe regret for this assignment from every possible assignment of the other variables to achieve the same result. This is further demonstrated by the number of regret values each algorithm is tracking: local external re-gret on average had 34 non-zero regret values, while local swap regret had 4200 non-zero regret values. In summary, external regret provides a powerful form of generalization. Figure 1 (b) shows the fraction of the previous 100 clauses that were satisfied. Two baselines are also presented. A random choice of variable assignments can satisfy 7 the clauses in expectation. We also ran WalkSAT (Selman et al., 1993) offline on the set of 201 clauses, and on aver-age it was able to satisfy all but 4% of the clauses, which gives an offline lower bound for what is possible. Both sub-stantially outperformed random, with the external regret al-gorithm nearing the performance of the offline WalkSat. Online Decision Tree Learning. Second, we consider Ex-ample 3. We took three datasets from the UCI Machine Learning Repository (each with categorical inputs and a large number of instances): nursery, mushroom, and king-rook versus king-pawn (Frank &amp; Asuncion, 2010). The categorical attributes were transformed into boolean at-tributes (which simplified the implementation of the local-ity graphs) by having a separate boolean feature for each at-tribute value. 1 We made the problems online classification tasks by sampling five instances at random (with replace-ment) for each timestep, with the utility being the num-ber classified correctly by the algorithm X  X  chosen decision tree. This was repeated for 1000 timesteps, and so the al-gorithms classified 5,000 instances in total. The locality graph used was the one described in Example 3. The tight coloring used to minimize local external regret was the one described in Example 4. L was set to 3 for local swap re-gret, and 100 for local external regret, as this achieved the best performance. Even with the far larger graph, the ex-ternal regret algorithm was observing nearly one-eighth of the number of non-zero regret values observed by the lo-cal swap algorithm. The average results over 50 trials are shown in Figure 2(a)-(c) with 95% confidence bars. The graphs show the average fraction of misclassified in-stances over the previous 100 timesteps. Two baselines are also plotted: the best single label (i.e., the size of the ma-jority class) and the best decision stump. Both regret algo-rithms substantially improved on the best label, and local external regret was selecting trees substantially better than the best stump. As a further baseline, we ran the batch algorithm C4.5 in an online fashion, by retraining a deci-sion tree after each timestep using all previously observed examples. C4.5 X  X  performance was impressive, learning highly accurate trees after observing only a small fraction of the data. However, C4.5 has no regret guarantees. As with any offline algorithm used in an online fashion, there is an implicit assumption that the past and future data in-stances are i.i.d.. In our experimental setup, the instances were i.i.d., and as a result C4.5 performed very well. To further illustrate this point, we constructed a simple online classification task where instances with identical attributes were provided with alternating labels. The best label (as well as the single best decision tree) has a 50% accuracy. C4.5 when trained on the previously observed instances, misclassifies every single instance. This is shown along with local regret algorithms in Figure 2 (d).
 Online Disjunct Learning. Finally, we examine online disjunct learning as described in Example 2. This task has received considerable attention, notably the celebrated Winnow algorithm (Littlestone, 1988), which is guaranteed to make a finite number of mistakes if the instances can be perfectly classified by some disjunction. Furthermore, the number of mistakes Winnow2 makes, when no disjunc-tion captures the instances, can be bounded by the number of attribute errors (i.e., the number of input attributes that must be flipped to make the disjunction satisfy the instance) made by the best disjunction. In these experiments we com-pare our algorithms X  performance to that of Winnow2. We looked at two learning tasks. In the first, we generated a random disjunction over n = 20 boolean variables, where a variable was independently included in the disjunction with probability 4 /n . Instances were created with uniform random assignments to all of the variables, with a label be-ing true if and only if the chosen disjunct is true for the instance X  X  assignment. In the second case, we chose in-stances uniformly at random from a constructed set of 21 instances: one for each variable with that variable (only) set to true and the label being true, and one with all of the variables assigned the value of true and the label be-ing false. We call this task Winnow Killer. For both tasks, the n -dimensional hypercube from Example 1 was used as the locality graph with the 2 n coloring as our admissible coloring, and L =  X  and b = 0 . The average results over 50 trials are shown in Figure 3, with 95% confience bars. The graphs plot error rates over the previous 100 instances. Three baselines are plotted: randomly assigning a label (guaranteed to get half of the instances correct on expec-tation), the best disjunct (which makes no mistakes for ran-dom disjunctions and makes 1 21 mistakes on the Winnow Killer task), and Winnow2. Figure 3 (a) shows the results on random disjunctions. Winnow2 is guaranteed to make a finite number of mistakes and indeed its error rate drops to zero quickly. The local regret concepts, though, have diffi-culties with random disjunctions. The reason can be easily seen for the case of local external regret. Suppose the first instance is labeled true; the algorithm now has regret for all of the variables that were true in that instance (some of these will be in the target disjunction, but many will not). These variables will now be included in the chosen dis-junction for a very long time, as the only regret that one can have for not removing them is if their assignment was the sole reason for misclassifying a false instance. In other words, the problem is that there X  X  no regret for not remov-ing multiple variables simultaneously as this is not a local change. Winnow2, though, also has issues. It performs very poorly in the Winnow Killer task (in fact, if the in-stances were ordered it could be made to get every instance wrong), as shown in Figure 3 (b). Since the mistake bound for Winnow2 is with respect to the number of attribute er-rors, a single mistake by the best disjunction can result in n mistakes by Winnow2. A further issue with Winnow is that while its peformance is tied to the performance of dis-junctions, its own hypothesis class is not disjunctions but a thresholded linear function, whereas local regret is playing in the same class of hypotheses that it comparing against. We introduced a new family of regret concepts based on restricting regret to only nearby hypotheses using a local-ity graph. We then presented algorithms for minimizing these concepts, even when the number of hypotheses are infinite. Further we showed that we can exploit structure in the graph to achieve tighter bounds and better performance. These new regret concepts mimic local search methods, which are common approaches to offline optimization with intractably hard hypothesis spaces. As such, our concepts and algorithms allows us to make online guarantees, with a similar flavor to their offline counterparts, with these hy-pothesis spaces.
 This work was supported by NSERC and Yahoo! Research, where the first author was a visiting scientist at the time the research was conducted.

