 Raymond J. Mooney MOONEY @ CS . UTEXAS . EDU In man y learning tasks, unlabeled data is plentiful but la-beled data is limited and expensi ve to generate. Conse-quently , semi-supervised learning , which emplo ys both la-beled and unlabeled data, has become a topic of significant interest. More specifically , semi-supervised clustering , the use of class labels or pairwise constraints on some exam-ples to aid unsupervised clustering, has been the focus of several recent projects (W agstaf f et al., 2001; Basu et al., 2002; Klein et al., 2002; Xing et al., 2003; Bar -Hillel et al., 2003; Segal et al., 2003).
 Existing methods for semi-supervised clustering fall into two general approaches we call constr aint-based and metric-based . In constraint-based approaches, the cluster -ing algorithm itself is modified so that user -pro vided la-bels or pairwise constraints are used to guide the algo-rithm towards a more appropriate data partitioning. This is done by modifying the clustering objecti ve function so that it includes satisf action of constraints (Demiriz et al., 1999), enforcing constraints during the clustering process (W agstaf f et al., 2001), or initializing and constraining clus-tering based on labeled examples (Basu et al., 2002). In metric-based approaches, an existing clustering algorithm that uses a distance metric is emplo yed; howe ver, the met-ric is first trained to satisfy the labels or constraints in the supervised data. Several distance measures have been used for metric-based semi-supervised clustering including Eu-clidean distance trained by a shortest-path algorithm (Klein et al., 2002), string-edit distance learned using Expectation Maximization (EM) (Bilenk o &amp; Moone y, 2003), KL diver-gence adapted using gradient descent (Cohn et al., 2003), and Mahalanobis distances trained using con vex optimiza-tion (Xing et al., 2003; Bar -Hillel et al., 2003). Pre vious metric-based semi-supervised clustering algo-rithms exclude unlabeled data from the metric training step, as well as separate metric learning from the clustering pro-cess. Also, existing metric-based methods use a single dis-tance metric for all clusters, forcing them to have similar shapes. We propose a new semi-supervised clustering al-gorithm deri ved from K-Means, MPCK-M EANS , that in-corporates both metric learning and the use of pairwise con-straints in a principled manner . MPCK-M EANS performs distance-metric training with each clustering iteration, uti-lizing both unlabeled data and pairwise constraints. The algorithm is able to learn indi vidual metrics for each clus-ter , which permits clusters of dif ferent shapes. MPCK-M
EANS also allo ws violation of constraints if it leads to a more cohesi ve clustering, whereas earlier constraint-based methods forced satisf action of all constraints, lea ving them vulnerable to noisy supervision.
 By ablating the metric-based and constraint-based compo-nents of our unified method, we present experimental re-sults comparing and combining the two approaches on mul-tiple datasets. The two methods for semi-supervision indi-vidually impro ve clustering accurac y, and our unified ap-proach inte grates their strengths. Finally , we demonstrate that the semi-supervised metric learning in our approach outperforms pre viously proposed methods that learn met-rics prior to clustering, and that learning multiple cluster -specific metrics can lead to better results. 2.1. Clustering with K-Means K-Means is a clustering algorithm based on iterati ve re-location that partitions a dataset into K clusters, locally minimizing the total squared Euclidean distance between the data points and the cluster centroids. Let X = f x component of x troids, and l l 2 f 1 ; : : : ; K g . The Euclidean K-Means algorithm cre-ates a K -partitioning fX function P It can be sho wn that the K-Means algorithm is essentially an EM algorithm on a mixture of K Gaussians under as-sumptions of identity covariance of the Gaussians, uniform mixture component priors and expectation under a partic-ular type of conditional distrib ution (Basu et al., 2002). In the Euclidean K-Means formulation, the squared L k x and its corresponding cluster centroid  X  tance measure, which is a direct consequence of the identity covariance assumption of the underlying Gaussians. 2.2. Semi-super vised Clustering with Constraints In semi-supervised clustering , a small amount of labeled data is available to aid the clustering process. Our frame-work uses both must-link and cannot-link constraints be-tween pairs of instances (W agstaf f et al., 2001), with an associated cost for violating each constraint. In man y unsupervised-learning applications, e.g., clustering for speak er identification in a con versation (Bar -Hillel et al., 2003), or clustering GPS data for lane-finding (W agstaf f et al., 2001), considering supervision in the form of con-straints is more realistic than pro viding class labels. While class labels may be unkno wn, a user can still specify whether pairs of points belong to same or dif ferent clus-ters. Constraint-based supervision is also more general than class labels: a set of classified points implies an equi v-alent set of pairwise constraints, but not vice versa. Since K-Means cannot directly handle pairwise constraints, we formulate the goal of pairwise constrained clustering as minimizing a combined objecti ve function, defined as the sum of the total squared distances between the points and their cluster centroids, and the cost incurred by violat-ing any pairwise constraints. Let M be a set of must-link pairs where ( x the same cluster , and C be a set of cannot-link pairs where ( x i ; x j ) 2 C ters. Let W = f w violating the constraints in M and C respecti vely . There-fore, the goal of pairwise constrained K-Means is to min-imize the follo wing objecti ve function, where point x assigned to the partition X where is the indicator function, [ true ] = 1 and [ f alse ] = 0. This mathematical formulation is moti vated by the metric labeling problem with the gener alized Potts model (Kleinber g &amp; Tardos, 1999). 2.3. Semi-super vised Clustering with Metric Lear ning While pairwise constraints can guide a clustering algorithm towards a better grouping, the y can also be used to adapt the underlying distance metric. Pairwise constraints effec-tively represent the user X  s vie w of similarity in the domain. Since the original data representation may not specify a space where clusters are suf ficiently separated, modifying the distance metric warps the space to minimize distances between same-cluster objects, while maximizing distances between dif ferent-cluster objects. As a result, clusters dis-covered using learned metrics adhere more closely to the notion of similarity embodied in the supervision. We parameterize Euclidean distance using a symmet-ric positi ve-definite matrix A as follo ws: k x = q ( x was pre viously used by Xing et al. (2003) and Bar -Hillel et al. (2003). If A is restricted to a diagonal matrix, it scales each dimension by a dif ferent weight and corre-sponds to feature weighting; otherwise new features are created that are linear combinations of the original ones. In pre vious work on adapti ve metrics for clustering (Cohn et al., 2003; Xing et al., 2003; Bar -Hillel et al., 2003), metric weights are trained to simultaneously minimize the distance between must-link ed instances and maximize the distance between cannot-link ed instances. A fundamental limitation of these approaches is that the y assume a single metric for all clusters, pre venting them from having dif fer -ent shapes. We allo w a separate weight matrix for each cluster , denoted A a generalized version of the K-Means model described in section 2.1, where cluster h is generated by a Gaussian with covariance matrix A  X  1 that maximizing the complete data log-lik elihood under this generalized K-Means model is equi valent to minimiz-ing the objecti ve function: where the second term arises due to the normalizing con-stant of l 2.4. Integrating Constraints and Metric Lear ning Combining Eqns.(1) and (2) leads to the follo wing objec-tive function that minimizes cluster dispersion under the learned metrics while reducing constraint violations: If we assume uniform constraint costs w straint violations are treated equally . Ho we ver, the penalty for violating a must-link constraint between distant points should be higher than that between nearby points. Intu-itively , this captures the fact that if two must-link ed points are far apart according to the current metric, the metric is grossly inadequate and needs severe modification. Since two clusters are involv ed in a must-link violation, the cor -responding penalty should affect the metrics for both clus-ters. This can be accomplished via multiplying the penalty in the second summation of Eqn.(3) by the follo wing func-tion: Analogously , the penalty for violating a cannot-link con-straint between two points that are nearby according to the current metric should be higher than for two distant points. To reflect this intuition, the follo wing penalty term can be used with violated cannot-link constraints that are assigned to the same cluster ( l where ( x 0 in the dataset according to l sures that the penalty for violating a cannot-link constraint remains non-ne gative since the second term is never greater than the first. The combined objecti ve function then be-comes: Costs w tive importance of the labeled versus unlabeled data while allo wing indi vidual constraint weights. The follo wing sec-tion describes how J mpckm can be greedily optimized by our proposed metric pairwise constrained K-Means (MPCK-M
EANS ) algorithm. Given a set of data points X , a set of must-link constraints M , a set of cannot-link constraints C , corresponding cost sets W and W , and the desired number of clusters K , MPCK-M EANS finds a disjoint K -partitioning fX of
X (with each cluster having a centroid  X  h and a local weight matrix A The algorithm inte grates the use of constraints and metric learning. Constraints are utilized during cluster initializa-tion and when assigning points to clusters, and the distance metric is adapted by re-estimating the weight matrices A during each iteration based on the current cluster assign-ments and constraint violations. Pseudocode for the algo-rithm is presented in Fig.1.
 3.1. Initialization Good initial centroids are critical to the success of greedy clustering algorithms such as K-Means. To infer the initial clusters from the constraints, we tak e the transiti ve closure of the must-link constraints and augment the set M with these entailed constraints (assuming consistenc y of the con-straints). Let  X  be the number of connected components in the augmented set M . These connected components are used to create  X  neighborhood sets f N neighborhood consists of points connected by must-links. For every pair of neighborhoods N least one cannot-link between them, we add cannot-link constraints between every pair of points in N and augment the cannot-link set C with these entailed con-straints. We will overload notation from this point and refer to the augmented must-link and cannot-link sets as M and C respecti vely .
 After this preprocessing step, we get  X  neighborhood sets f N p g  X  p =1 . These neighborhoods pro vide initial clusters for the MPCK-M EANS algorithm. If  X   X  K , we initialize  X  cluster centers with the centroids of all the  X  neighborhood sets. If  X  &lt; K , we initialize the remaining K  X   X  clusters with points obtained by random perturbations of the global centroid of X .
 If  X  &gt; K , we select K neighborhood sets using a weighted variant of the farthest-first algorithm, which is a good heuristic for initialization in centroid-based clustering al-gorithms lik e K-Means. In weighted farthest-first tra versal, the goal is to find K points which are maximally separated from each other in terms of a weighted distance. In our case, the points are the centroids of the  X  neighborhoods, and the weight of each centroid is the size of its correspond-ing neighborhood. Thus, we bias farthest-first to select cen-troids which are relati vely far apart but also represent lar ge neighborhoods, in order to obtain good initial clusters. In weighted farthest-first tra versal, we maintain a set of tra-versed points at every step, and pick the follo wing point having the farthest weighted distance from the tra versed set (using the standard notion of distance from a set: d ( x ; S ) = min y 2 S d ( x ; y ) ), and so on. Finally , we initialize the cluster centers with the centroids of the K neighborhoods chosen by weighted farthest-first tra versal. 3.2. E-step MPCK-M EANS alternates between cluster assignment in the E-step, and centroid estimation and metric learning in the M-step (see Step 2 in Fig.1). In the E-step, every point x is assigned to the cluster that minimizes the sum of the distance of x to the cluster centroid according to the local metric and the cost of any constraint violations incurred by this cluster assignment. Points are randomly re-ordered for each assignment sequence, and once a point x is assigned to a cluster , the subsequent points in the random ordering use the current cluster assignment of x to calculate possible constraint violations.
 Note that this assignment step is order -dependent, since the subsets of M and C rele vant to each cluster may change with the assignment of a point. We experimented with random ordering as well as a greedy strate gy that first as-signed instances that are closest to the cluster centroid and involv ed in a minimal number of constraints. These exper -iments sho wed that the order of assignment does not result in statistically significant dif ferences in clustering quality; therefore, we used random ordering in our evaluation. In the E-step, each point mo ves to a new cluster only if the component of J mpckm contrib uted by this point decreases. So when all points are given their new assignment, J mpckm will decrease or remain the same. 3.3. M-step In the M-step, every cluster centroid  X  using the points in corresponding X contrib ution of each cluster to J mpckm is minimized. The pairwise constraints do not tak e part in this centroid re-estimation step because the constraint violations only de-pend on cluster assignments, which do not change in this step. Thus, only the first term (the distance component) of J mpckm is minimized. The centroid re-estimation step ef-fecti vely remains the same as in K-Means.
 The second part of the M-step performs metric learning, where the matrices f A the objecti ve function J mpckm . Each updated matrix of lo-cal weights A where M link constraints respecti vely that contain points currently assigned to the h -th cluster .
 Since each A covariance matrices in Eqn.(7), A  X  1 not be singular . If any of the obtained A  X  1 lar , the y can be conditioned via adding the identity ma-trix multiplied by a small fraction of the trace of A  X  1 A h = A the A is mended by projecting on the set C = f A : A  X  0 g of positi ve semi-definite matrices as described by Xing et al. (2003) to ensure that it parameterizes a distance metric. For high-dimensional or lar ge datasets, estimating the full matrix A diagonal weight matrices can be used, which is equi va-lent to feature weighting, while using the full matrix cor -responds to feature generation. In the case of diagonal A the d -th diagonal element, a ( h ) of the d -th feature for the h -th cluster metric: Intuiti vely , the first term in the sum, P scales the weight of each feature proportionately to the fea-ture X  s contrib ution to the overall cluster dispersion, analo-gously to scaling performed when computing unsupervised Mahalanobis distance. The last two terms that depend on constraint violations stretch each dimension attempting to mend the current violations. Thus, the metric weights are adjusted at each iteration in such a way that the contrib ution of dif ferent attrib utes to distance is variance-normalized, while constraint violations are minimized.
 Instead of multiple metrics f A a single metric A for all clusters. The metric would be used and updated similarly to the description abo ve, except that summations in Eqns.(7) and (8) would be over X , M , and C instead of X The objecti ve function decreases after every cluster assign-ment, centroid re-estimation and metric learning step till con vergence, implying that the MPCK-M EANS algorithm will con verge to a local minima of J mpckm as long as ma-trices f A A definite or if the maximally separated points f ( x 0 change between iterations, con vergence is no longer guar -anteed theoretically; howe ver, empirically this has not been a problem in our experience. 4.1. Methodology and Datasets Experiments were conducted on three datasets from the UCI repository: Iris , Wine , and Ionospher e (Blak e &amp; Merz, 1998); the Protein dataset used by Xing et al. (2003) and Bar -Hillel et al. (2003), and randomly sampled subsets from the Digits and Letter s handwritten character recogni-tion datasets, also from the UCI repository . For Digits and Letter s , we chose two sets of three classes: f I, J, L g Letter s and f 3, 8, 9 g from Digits , sampling 10% of the data points from the original datasets randomly . These classes were chosen since the y represent dif ficult visual discrimi-nation problems. Table 1 summarizes the properties of the datasets: the number of instances N , the number of dimen-sions D , and the number of classes K .
 We have used pairwise F-Measure to evaluate the clustering results based on the underlying classes. F-Measure relies on the traditional information retrie val measures, adapted for evaluating clustering by considering same-cluster pairs: We generated learning curv es with 5-fold cross-v alidation for each dataset to determine the effect of utilizing the pair -wise constraints. Each point on the learning curv e repre-sents a particular number of randomly selected pairwise constraints given as input to the algorithm. Unit constraint costs W and W were used for all constraints, original and inferred, since the datasets did not pro vide indi vidual weights for the constraints. The clustering algorithm was run on the whole dataset, but the pairwise F-Measure was calculated only on the test set. Results were averaged over 50 runs of 5 folds. 4.2. Results and Discussion First, we compared constraint-based and metric-based semi-supervised clustering with the inte grated frame work as well as purely unsupervised and supervised approaches. Figs.2-7 sho w learning curv es for the six datasets. For each dataset, we compared five clustering schemes:  X  MPCK-M EANS clustering, which involv es both seed- X  MK-M EANS , which is K-Means clustering with the  X  PCK-M EANS clustering, which utilizes constraints  X  K-M EANS unsupervised clustering;  X  S UPERVISED -M EANS , which performs assignment of On the presented datasets, the unified approach (MPCK-M
EANS ) outperforms indi vidual seeding (PCK-M EANS ) and metric learning (MK-M EANS ). Superiority of semi-supervised over unsupervised clustering illustrates that pro-viding pairwise constraints is beneficial to clustering qual-ity. Impro vements of semi-supervised clustering over S
UPERVISED -M EANS indicate that iterati ve refinement of centroids using both constraints and unlabeled data out-performs purely supervised assignment based on neighbor -hoods inferred from constraints (for Ionospher e , MPCK-M
EANS requires either the full weight matrix or indi vid-ual cluster metrics to outperform S UPERVISED -M EANS , results for these experiments are sho wn on Fig.11). For the Wine , Protein , and Letter -IJL datasets, the dif fer -ence between methods that utilize metric learning (MPCK-M EANS and MK-M EANS ) and those that do not (PCK-M
EANS and regular K-Means) with no pairwise constraints indicates that even in the absence of constraints, weight-ing features by their variance (essentially using unsuper -vised Mahalanobis distance) impro ves clustering accurac y. For the Wine dataset, additional constraints pro vide an in-substantial impro vement in cluster quality on this dataset, which sho ws that meaningful feature weights are obtained from scaling by variance using just the unlabeled data. Some of the metric learning curv es display a characteris-tic  X  X ip X , where clustering accurac y decreases when ini-tial constraints are pro vided, but after a certain point starts to increase and eventually rises abo ve the initial point on the learning curv e. We conjecture that this phenomenon is due to the fact that metric parameters learned using few constraints are unreliable, and a significant number of con-straints is required by the metric learning mechanism to es-timate parameters accurately .
 On the other hand, seeding the clusters with a small number of pairwise constraints has an immediate positi ve effect on the final cluster quality , while pro viding more pairwise con-straints has diminishing returns, i.e., PCK-M EANS learn-ing curv es rise slo wly . When both seeding and metric learning are utilized, the unified approach benefits from the indi vidual strengths of the two methods, as can be seen from the MPCK-M EANS results.
 In another set of experiments, we evaluated the utility of using indi vidual metrics for each cluster and the usefulness of learning a full weight matrix A (feature generation) as opposed to a diagonal matrix (feature weighting). We have also compared our methods with RCA, a semi-supervised clustering algorithm that performs metric learning sepa-rately from the clustering process (Bar -Hillel et al., 2003), and that has been sho wn to outperform a similar approach by Xing et al. (2003). Figs.8-13 sho w learning curv es for the six datasets on the follo wing clustering schemes:  X  MPCK-M EANS -S-D, which is same as MPCK- X  MPCK-M EANS -M-D, which involv es both seeding  X  MPCK-M EANS -S-F, which involv es both seeding  X  MPCK-M EANS -M-F, which involv es both seeding  X  RCA clustering, which uses distance metric learning As can be seen from results, both full matrix parameteri-zation and indi vidual metrics for each cluster can lead to significant impro vements in clustering quality . Ho we ver, the relati ve usefulness of these two techniques varies be-tween the datasets, e.g., multiple metrics are particularly beneficial for Protein and Digits datasets, while switching from a diagonal to a full weight matrix leads to lar ge im-pro vements on Wine , Ionospher e , and Letter s . These re-sults can be explained by the fact that the relati ve success of the two techniques depends on the properties of a par -ticular dataset: using a full weight matrix helps when the attrib utes are highly correlated, while multiple metrics lead to impro vements when clusters in the dataset are of dif fer -ent shapes or lie in dif ferent subspaces of the original space. A combination of the two techniques is most helpful when both of these requirements are satisfied, as for Iris and Dig-its , which was observ ed by visualizing these datasets. For other datasets, either multiple metrics or full weight matrix lead to maximum performance in isolation.
 Comparing the performance of dif ferent variants of MPCK-M EANS with RCA, we can see that early on the learning curv es, where few pairwise constraints are avail-able, RCA leads to better metrics than MPCK-M EANS . Ho we ver, as more training data is pro vided, the ability of MPCK-M EANS to learn from both supervised and un-supervised data as well as use indi vidual metrics allo ws MPCK-M EANS to produce better clustering.
 Ov erall, our results indicate that the inte grated approach to utilizing pairwise constraints in clustering with indi vidual metrics outperforms seeding and metric learning indi vidu-ally and leads to impro vements in cluster quality . Extend-ing the basic approach with a full parameterization matrix and indi vidual metrics for each cluster can lead to signifi-cant impro vements over the basic method. In pre vious work on constrained pairwise clustering, Wagstaf f et al. (2001) proposed the COP-KMeans algo-rithm that has a heuristically moti vated objecti ve function. Our formulation, on the other hand, has an underlying gen-erati ve model based on Hidden Mark ov Random Fields (see (Basu et al., 2004) for a detailed analysis). Bansal et al. (2002) also proposed a frame work for pairwise con-strained clustering, but their model performs clustering us-ing only the constraints, whereas our formulation uses both constraints and an underlying distance metric between the points for clustering.
 Schultz and Joachims (2004) recently introduced a method for learning distance metric parameters based on rela-tive comparisons. In unsupervised clustering, Domeni-coni (2002) proposed a variant of K-Means that incorpo-rated learning indi vidual Euclidean metric weights for each cluster; our approach is more general since it allo ws met-ric learning to utilize pairwise constraints along with unla-beled data.
 In recent work on semi-supervised clustering with pairwise constraints, Cohn et al. (2003) used gradient descent for weighted Jensen-Shannon divergence in the conte xt of EM clustering. Xing et al. (2003) utilized a combination of gradient descent and iterati ve projections to learn a Maha-lanobis metric for K-Means clustering. Also, Bar -Hillel et al. (2003) proposed a Redundant Component Analy-sis (RCA) algorithm that uses only must-link constraints to learn a Mahalanobis metric using con vex optimization. All these metric learning techniques for clustering train a single metric first using only supervised data, and then per -form clustering on the unsupervised data. In contrast, our method inte grates distance metric learning with the clus-tering process and utilizes both supervised and unsuper -vised data to learn multiple metrics, which experimentally leads to impro ved results. Finally , a unified objecti ve func-tion for semi-supervised clustering with constraints was re-cently proposed by Segal et al. (2003), howe ver, it did not incorporate distance metric learning. This paper has presented MPCK-M EANS , a new approach to semi-supervised clustering that unifies the pre vious constraint-based and metric-based methods. It is based on a variation of the standard K-Means clustering algorithm and uses pairwise constraints along with unlabeled data for constraining the clustering and learning distance metrics. In contrast to pre viously proposed semi-supervised cluster -ing algorithms, MPCK-M EANS also allo ws clusters to lie in dif ferent subspaces and have dif ferent shapes. By ablating the indi vidual components of our inte grated ap-proach, we have experimentally compared metric learning and constraints in isolation with the combined algorithm. Our results have sho wn that by unifying the adv antages of both techniques, the inte grated approach outperforms the two techniques indi vidually . We have sho wn that using in-dividual metrics for dif ferent clusters, as well as perform-ing feature generation via a full weight matrix in contrast to feature weighting with a diagonal weight matrix, can lead to impro vements over our basic algorithm.
 Extending our approach to high-dimensional datasets, where Euclidean distance performs poorly , is the primary avenue for future research. Other interesting topics for fu-ture work include selection of most informati ve pairwise constraints that would facilitate accurate metric learning and obtaining good initial centroids, as well as methodol-ogy for handling noisy constraints and cluster initialization sensiti ve to constraint costs. We would lik e to thank anon ymous revie wers and Joel Tropp for insightful comments. This research was sup-ported in part by NSF grants IIS-0325116 and IIS-0117308, and by a Faculty Fello wship from IBM Corp.
