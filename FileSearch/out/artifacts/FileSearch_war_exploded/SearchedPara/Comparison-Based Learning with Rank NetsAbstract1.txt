 Amin Karbasi amin.karbasi@epfl.ch EPFL, Lausanne, Switzerland Stratis Ioannidis stratis.ioannidis@technicolor.com Technicolor, Palo Alto, USA Laurent Massouli  X e laurent.massoulie@technicolor.com Technicolor, Paris, France In search through comparisons, a user locates a tar-get object in a database as follows. At each step, the database presents two objects to the user, who then selects among the pair the object closest to the tar-get that she has in mind. This process continues until, based on the user X  X  answers, the database can uniquely identify the target she has in mind.
 This kind of interactive navigation, also known as ex-ploratory search, has numerous real-life applications ( Marchionini , 2006 ; Ruthven , 2008 ), such as naviga-tion in a database of pictures of people photographed in an uncontrolled environment ( Tschopp et al. , 2011 ). Automated methods may fail to extract meaningful features from such photos. Even if this were pos-sible, in many practical cases, images with similar low-level descriptors may have very different seman-tic content, and thus be perceived differently by users ( Smeulders et al. , 2000 ; Lew et al. , 2006 ). On the other hand, a human can easily sort images of peo-ple w.r.t. their similarity to a given person, and her answers can be used to rank images in the database in terms of this similarity.
 Formally, the human user X  X  feedback can be modelled as a  X  X omparison oracle X  ( Goyal et al. , 2008 ). Assum-ing a database N endowed with a distance metric d , capturing the  X  X istance X  or  X  X issimilarity X  between different objects, a comparison oracle answers ques-tions of the kind:  X  X etween two objects x and y in N , which one is closest to t under the metric d ? X . In this paper, we study algorithms for identifying an unknown target with as few queries to such an oracle as possible. Most importantly, the algorithms we con-sider do not rely on a priori knowledge of the distance between objects: they cannot access an embedding of N in a metric space, nor can they compute the dis-tance between two objects. Decisions on which queries to submit to the oracle depend only on (a) ranking re-lationships between objects, which can indeed be ob-tained through a comparison oracle and (b) the prior distribution from which the target is sampled. As discussed in Section 3.3 , content search through comparisons can be framed as an active learning prob-lem. A well-known active learning algorithm is the Generalized Binary Search (GBS) or splitting algo-rithm ( Dasgupta , 2005 ). Using GBS to submit queries to the oracle locates the target in OP T H max ( )+1 queries, where H max ( ) = max x  X  OP T is the number of queries submitted by an opti-mal algorithm. In practice, GBS performs very well in terms of query complexity, suggesting that this bound can be tightened. However, the computational com-plexity of GBS is  X ( n 3 ) for n = |N| , which makes it intractable for most large databases.
 Recently, Karbasi et al. ( 2011 ) proposed an algorithm that determines the target in O c 3 H ( ) H max ( ) queries, whose computational complexity is O (1) per query. Here, H ( ) is the entropy of the prior and c , defined formally in Section 3.2 , is the doubling con-stant of the prior . It captures the dimension of the database, as determined by the underlying distance d ( Clarkson , 2006 ). Karbasi et al. also show that OP T =  X ( cH ( )), indicating that their algorithm is within a c 2 H max ( ) factor from the optimal. We make the following contributions: First, we pro-pose a new adaptive algorithm, RankNetSearch , lo-cating the target with O ( c 6 H ( )) queries to the oracle, in expectation. Our algorithm therefore improves on GBS and Karbasi et al. by removing the term H max  X  which can be quite large in practice X  X t a cost of a higher exponent in the dependence on the constant c . Its computational complexity is O n (log n + c 6 ) log c per query, which is manageable compared to GBS; moreover, this cost can be reduced to O (1) by pre-computing an additional data structure.
 Second, we extend RankNetSearch to the case of a faulty oracle that lies with a probability  X  &gt; 0, and show that it locates the target w.h.p. at an ex-pected query cost O ( P x  X  X  ( x ) log 1 ( x ) log log( ( x )) and, thereby, close to H ( ).
 Third, we evaluate RankNetSearch and prior art algorithms over several datasets. We observe that RankNetSearch establishes a desirable trade-off be-tween query and computational complexity.
 The remainder of this paper is organized as follows. We overview related work in Section 2 , and discuss definitions and preliminaries in Section 3 . Our algo-rithm and the analysis of its complexity are presented in Section 4 , and its robustness to noise in Section 5 . Section 6 includes our numerical evaluations. Search through comparisons was first introduced by Goyal et al. ( 2008 ), and further explored by Lifshitz and Zhang ( 2009 ) and Tschopp et al. ( 2011 ). The above works study the problem in terms of worst-case (prior-free) bounds, so our work departs in introduc-ing a prior and studying query complexity in ex-pectation. All these works introduce a disorder con-stant , that plays the same role as the quantity c in our setup. Lifshitz and Zhang also employ hierarchi-cal data structures similar to the rank-nets we study here. Our upper bound coincides with theirs when is uniform over N and can thus be seen as an extension to the more general Bayesian setting under prior . Cover trees based on nets have been extensively studied in the context of nearest neighbour search ( Clarkson , 1999 ; Beygelzimer et al. , 2006 ). These works too focus on worst-case bounds and, crucially, assume full access to the underlying distance metric d . Our approach thus differs in both of these respects. In earlier work, Fredman ( 1976 ) and others have consid-ered decision trees for determining a complete ordering of objects rather than just the first one in the list. To the best of our knowledge, the work closest to our is ( Karbasi et al. , 2011 ), which was the first to study search through comparisons in a Bayesian setting. Our work improves their bound by the factor of H max and establishes the connection to active learning and GBS. 3.1. Search through Comparisons Consider a large finite set of objects N of size n = |N| , endowed with a distance metric d , capturing the  X  X issimilarity X  between objects. A user selects a target t  X  N from a prior distribution ; our goal will be to design an interactive algorithm that queries the user with the purpose of discovering t .
 Comparison Oracle. Though we assume that the metric d exists, our view of distances is constrained to only observing order relationships. More pre-cisely, we only have access to information that can be obtained through a comparison oracle ( Goyal et al. , 2008 ). Given object z , a comparison oracle O z receives as a query an ordered pair ( x, y )  X  N 2 and answers the question  X  X s z strictly closer to x than to y ? X , i.e. , Note that a tie d ( x, z ) = d ( y, z ) is revealed by two calls O z ( x, y ) and O z ( y, x ). Our algorithm for determining the unknown target t can submit queries to a compar-ison oracle O t  X  X amely, the user. We thus assume, effectively, that the user can order objects w.r.t. their distance from t , but does not need to disclose (or even know) the exact values of these distances. We will first assume that the oracle always gives correct answers; in Section 5 , we relax this assumption by considering a faulty oracle that lies with probability  X  &lt; 0 . 5. Prior Knowledge and Performance Metrics.
 The algorithms we study rely only on a priori knowl-edge of (a) the distribution and (b) the values of the mapping O z : N 2  X  { X  1 , +1 } , for every z  X  N . This is in line with our assumption that, although the dis-tance metric d exists, it cannot be directly observed. Our focus is on adaptive algorithms, whose decision on which query in N 2 to submit next are determined by the oracle X  X  previous answers.
 The prior can be estimated empirically as the fre-quency with which objects have been targets in the past. The order relationships can be computed off-line by submitting  X ( n 2 log n ) queries to a comparison ora-cle, and requiring  X ( n 2 ) space: for each possible target z  X  N , objects in N can be sorted w.r.t. their distance from z with  X ( n log n ) queries to O z . We store the re-sult of this sorting in (a) a linked list, whose elements are sets of objects at equal distance from z , and (b) a hash-map, that associates every element y with its rank in the sorted list. Note that O z ( x, y ) can thus be retrieved in O (1) time by comparing the relative ranks of x and y with respect to their distance from z . We measure the performance of an algorithm through two metrics. The first is the query complexity , deter-mined by the expected number of queries the algo-rithm needs to submit to the oracle to determine the target. The second is the computational complexity , determined by the time-complexity of determining the query to submit to the oracle at each step. 3.2. A Lower Bound Recall that the entropy of is defined as H ( ) = P of . Given an object x  X  N , let B x ( r ) = { y  X  N : d ( x, y )  X  r } be the closed ball of radius r  X  0 around x . Given a set A  X  N let ( A ) = P x  X  A ( x ) . The doubling constant 1 c ( ) of a distribution is the min-imum c &gt; 0 for which ( B x (2 R ))  X  c ( B x ( R )) , for any x  X  supp ( ) and any R  X  0.
 The doubling constant has a natural connection to the underlying dimension of the dataset ( Clarkson , 2006 ; Karbasi et al. , 2011 ), as determined by the distance d . Both the entropy and the doubling constant are also inherently connected to content search through comparisons. Karbasi et al. show that any adaptive mechanism for locating a target t must submit at least  X  c ( ) H ( ) queries to the oracle O t , in expectation. Moreover, they provide an algorithm for determin-ing the target in O c 3 H ( ) H max ( ) queries, where H 3.3. Active Learning Search through comparisons can be seen as a special case of active learning ( Dasgupta , 2005 ; Nowak , 2012 ). In active learning, a hypothesis space H is a set of bi-nary valued functions defined over a finite set Q , called the query space . Each hypothesis h  X  H generates a label from { X  1 , +1 } for every query q  X  Q . A target hypothesis h  X  is sampled from H according to some prior ; asking a query q amounts to revealing the value of h  X  ( q ), thereby restricting the possible candi-date hypotheses. The goal is to determine h  X  in an adaptive fashion, by asking as few queries as possible. In our setting, the hypothesis space H is the set N , and the query space Q is the set of ordered pairs N 2 . The target hypothesis sampled from is the unknown target t . Each hypothesis/object z  X  N is uniquely 2 identified by the mapping O z : N 2  X  { X  1 , +1 } , which we have assumed to be a priori known.
 Generalized Binary Search A well-known algo-rithm for determining the true hypothesis in the gen-eral active-learning setting is the so-called generalized binary search (GBS) or splitting algorithm ( Dasgupta , 2005 ; Nowak , 2012 ). Define the version space V  X  H to be the set of possible hypotheses that are consis-tent with the query answers observed so far. At each step, GBS selects the query q  X  Q that minimizes | P h  X  V ( h ) h ( q ) | . Put differently, GBS selects the query that separates the current version space into two sets of roughly equal probability mass; this leads, in expectation, to the largest reduction in the mass of the version space as possible, so GBS can be seen as a greedy query selection policy.
 A bound on the query complexity of GBS originally obtained by Dasgupta ( 2005 ) and recently tightened (w.r.t. constants) by Golovin and Krause ( 2010 ) is given by the following theorem: Theorem 1. GBS makes at most OP T H max ( )+1 queries in expectation to identify hypothesis h  X   X  N , were OP T is the minimum expected number of queries made by any adaptive policy.
 GBS in Search through Comparisons. In our setting, the version space V comprises all possible ob-jects in z  X  N that are consistent with oracle answers given so far. In other words, z  X  V iff O z ( x, y ) = O ( x, y ) for all queries ( x, y ) submitted to the oracle. Selecting the next query therefore amounts to finding the pair ( x, y )  X  N 2 that minimizes As the simulations in Section 6 show, the query com-plexity of GBS is excellent in practice. This suggests Algorithm 1 RankNetSearch ( O t ) Algorithm 2 RankNet ( x , E ) that the bound of Theorem 1 could be improved in the specific context of search through comparisons. Nevertheless, the computational complexity of GBS is  X ( n 2 | V | ) operations per query, as it requires minimiz-ing f ( x, y ) over all pairs in N 2 . For large sets N , this can be truly prohibitive. This motivates us to propose a new algorithm, RankNetSearch , whose computa-tional complexity is almost linear and its query com-plexity is within a O ( c 5 ( )) factor from the optimal. Our algorithm is inspired by  X  -nets, a structure in-troduced by Clarkson ( 1999 ; 2006 ) in the context of Nearest Neighbor Search (NNS). The main challenge that we face is that, contrary to standard NNS, we have no access to the underlying distance metric . In addition, the query complexity bounds on  X  -nets are worst-case ( i.e. , prior-free); our construction takes the prior into account to provide bounds in expectation. 4.1. Rank Nets To address the above issues, we introduce the no-tion of rank nets , which will play the role of  X  -nets in our setting. For some x  X  N , consider the ball E = B x ( R )  X  N . For any y  X  E , we define to be the radius of the smallest ball around y that maintains a mass above  X  ( E ). Using this definition 3 , we define a  X  -rank net as follows.
 Definition 1. For some  X  &lt; 1 , a  X  -rank net of E = B x ( r )  X  N is a maximal 4 set of objects R  X  E such that for any two distinct y, y  X   X  R For any y  X  R , consider the Voronoi cell V y = { z  X  the radius r y of the Voronoi cell V y as r y = inf { r : V y  X  B y ( r ) } . Critically, a rank net and the Voronoi tessellation it defines can both be computed using only ordering information : Lemma 1. A  X  -rank net R of E can be constructed in O ( | E | (log | E | + |R| )) steps, and the balls B y E circumscribing the Voronoi cells around R can be constructed in O ( | E ||R| ) steps using only (a) and (b) the mappings O z : N 2  X  { X  1 , +1 } for every z  X  E . The proof is in Appendix A . Armed with this result, we turn our attention to how the selection of  X  affects the size of the net as well as the mass of the Voronoi balls around it. Our next lemma, whose proof is in Appendix B , bounds |R| .
 Lemma 2. The size of the net R is at most c 3 / X  . Finally, our last lemma determines the mass of the Voronoi balls in the net.
 Lemma 3. If r y &gt; 0 then ( B y ( r y ))  X  c 3  X  ( E ) . The proof is in Appendix C . Note that Lemma 3 does not bound the mass of Voronoi balls of radius zero. 4.2. Rank Net Data Structure and Algorithm Rank nets can be used to identify a target t using a comparison oracle O t as described in Algorithm 1 . Initially, a net R covering N is constructed; nodes y  X  R are compared w.r.t. their distance from t , and the closest to the target is determined, say y  X  . Note that this requires submitting |R|  X  1 queries to the oracle. The version space V (the set of possible hypotheses) is thus the Voronoi cell V y  X  , and is a subset of the ball B y  X  ( r y  X  ). The algorithm then proceeds by limiting the search to B y  X  ( r y  X  ) and repeating the above process. Note that, at all times, the version space is included in the current ball to be covered by a net. The process terminates when this ball becomes a singleton which, by construction, must contain the target. A question in the above setup is how to select  X  : by Lemma 3 , small values lead to a sharp decrease in the mass of Voronoi balls from one level to the next, hence reaching the target with fewer iterations. On the other hand, by Lemma 2 , small values also imply larger nets, leading to more queries to the oracle per iteration. We select  X  in an iterative fashion, as indicated in the pseu-docode of Algorithm 2 : we repeatedly halve  X  until all non-singleton Voronoi balls B y ( r y ) of the resulting net have a mass bounded by 0 . 5 ( E ). This selection leads to the following bounds on the corresponding query and computational complexity of RankNetSearch: Theorem 2. RankNetSearch locates the target by making 4 c 6 (1 + H ( )) queries to a comparison oracle, in expectation. The cost of determining which query to submit next is O n (log n + c 6 ) log c .
 In light of the  X ( cH ( )) lower bound on query com-plexity by Karbasi et al. ( 2011 ), RankNetSearch is within a O ( c 5 ) factor of the optimal algorithm, and is thus order-optimal for constant c . Moreover, the com-putational complexity per query is O n (log n + c 6 ), in contrast to the cubic cost of GBS. As shown in Sec-tion 6 , in practice, this leads to drastic reductions in the computational costs compared to GBS.
 The computational complexity can be further reduced to O (1) through amortization. In particular, it is easy to see that the possible paths followed by RankNet-Search define a hierarchy, whereby every object serves as a parent to the rank net of its Voronoi ball. This tree can be pre-constructed, and search reduces descending this tree; we elaborate on this in Section 6 . In a noisy setting the search must be robust against erroneous answers. Specifically, assume that for any query O t ( x, y ), the noisy oracle returns the wrong an-swer with probability bounded by  X  , for some  X  &lt; 1 / 2, independently of previous answers . In this context, a problem with RankNetSearch arises in line 4 of Al-gorithm 1 : it is not clear how to identify the object closest to the target among elements in a net. We re-solve this by introducing repetitions at each iteration. Specifically, at the  X  -th step of the search,  X   X  1, and rank-net size m , we define a repetition factor for some design parameter  X   X  (0 , 1). The modified algorithm then proceeds down the hierarchy, starting at the top level for  X  = 1. The basic step at step  X  with a net R proceeds as follows. A tournament is organized among elements of R , who are initially paired. Pairs of competing members are compared k  X  (  X , |R| ) times. The  X  X layer X  from a given pair winning the largest number of games moves to the next stage, where it will be paired again with another winner of the first round, and so forth until only one player is left. Note that the number of repetitions k  X  (  X , m ) increases only logarithmically with the level  X  .
 To find the closest object to target t with the noise-less oracle, clearly we need to make O ( |R| ) number of queries. The proposed algorithm achieves the same goal with high probability by making at most a factor 2 k  X  (  X , |R| ) more comparisons. In this context we have the following Theorem 3. For a comparison oracle with error prob-ability  X  , the algorithm with repetitions ( 5 ) outputs the correct target with probability at least 1  X   X  in O ( 1 with constants depending on c .
 The proof is given in Appendix E . For uniform dis-tribution ( x )  X  1 /n , for all x  X  N , this yields an extra log log( n ) factor in addition to the term of order H ( ) = log( n ) which, by the lower bound by Karbasi et al. , is optimal. We evaluate RankNetSearch over six publicly avail-able datasets: iris , abalone , ad , faces , swiss roll , and netflix . We subsampled the latter two, taking 1000 randomly selected data points from swiss roll , and the 1000 most rated movies in netflix . We map these datasets to R d (categorical variables are mapped to binary values in the standard fashion) for d as shown in Fig. 1 (a). For netflix , movies were mapped to 50-dimensional vectors by obtaining a low rank approxi-mation of the user/movie rating matrix through SVD. For all experiments, the distance metric d is the  X  2 distance and the prior is power-law with  X  = 0 . 4. We evaluated the performance of two versions of RankNetSearch : one as described by Algo. 1 , and another one ( T-RankNetSearch ) in which the hi-erarchy of rank nets is precomputed and stored as a tree. Both propose exactly the same queries to the or-acle, so have the same query complexity; however, T-RankNetSearch has only O (1) computational cost per query. The sizes of the trees precomputed by T-RankNetSearch for each dataset are shown in Fig. 1 (a).
 We compare these algorithms to (a) the policy pro-posed by Karbasi et al. ( 2011 ), denoted by Memo-ryless , and (b) two heuristics based on GBS (the  X ( n 3 ) computational cost of GBS per query makes it intractable over the datasets we consider). The first heuristic, termed F-GBS for fast GBS, selects like GBS the query that minimizes ( 2 ); however, it does so by restricting the queries to pairs of objects in the current version space V . This reduces the computational cost per query to  X ( | V | 3 ), rather than  X ( n 2 | V | ). The second heuristic, termed S-GBS for sparse GBS, exploits rank nets as follows. First, we costruct the rank-net hierarchy over the dataset, as in T-RankNetSeach . Then, we minimize ( 2 ) restricted only on pairs of objects that appear in the same net. Intuitively, S-GBS assumes that an equitable parti-tion of the objects exists among such pairs.
 Query vs. Computational Complexity. The query complexity of different algorithms, expressed as average number of queries per search, is shown in Fig. 1 (b). Although there are no known guaran-tees for either F-GBS nor S-GBS both algorithms are excellent in terms of query complexity across all datasets, finding the target within about 10 queries, in expectation. As GBS should perform as well as these algorithms, these suggest that it should also have low query cost. The query complexity of RankNet-Search is between 2 to 10 times higher; the impact is greater for high-dimensional datasets, as expected through the dependence of the rank net size on the c doubling constant. Finally, Memoryless performs worse compared to all other algorithms. As shown in Fig. 1 , the above ordering is fully reversed w.r.t. com-putational costs, measured as the aggregate number of operations performed per search. Differences from one algorithm to the next range between 50 to 100 orders of magnitude. F-GBS requires close to 10 9 op-erations in expectation for some datasets; in contrast, RankNetSearch ranges between 100 and 1000 opera-tions and, in conclusion, presents an excellent trade-off between query and computational complexity.
 Scalability and Robustness. To study how the above algorithms scale with the dataset size, we also evaluate them on a synthetic dataset comprising ob-jects placed uniformly at random at R 3 . The query and computational complexity of the five algorithms is shown in Fig. 2 (a) and (b).
 We observe the same discrepancies betwen algorithms we noted in Fig. 1 . The linear growth in terms of log n implies a linear relationship between both measures of complexity w.r.t. the entropy H ( ) for all methods (we ommit the relevant figure for lack of space). In Fig. 2 (b), we plot the query complexity of the robust RankNetSearch algorithm outlined in Section 5 . For all simulations, the target success rate was set to 0.9, but the actual success rates we observed were con-siderably higher, close to 0.99. We observe that, even for high error rates  X  , the query complexity remains low. Moreover, the high success rates that we observe, combined with the independence of the cost on n , sug-gest that we can further reduce the number of queries to lower values than the ones required by ( 5 ). We presented RankNetSearch , an algorithm that strikes an excellent balance between query and com-putational costs. Further improving this trade-off, in particular for more general kinds of noise, is an inter-esting future direction for this line of work. Through-out, we assumed that human inference of proximity is accurately captured by a metric space structure. An interesting research direction is assessing the validity of this assumption through user trials.
 Using the ordered list containing the sets of equidistant objects described in Section 3.1 , for any z  X  N , we can partition N into equivalence classes A z 1 , A z 2 , . . . , A such that for any two objects y, y  X   X  N , y  X  A z i and y  X  A z j with i &lt; j if and only if d ( y, z ) &lt; d ( y To construct R , it suffices to show that ( 4 ) can be verified for any z, z  X   X  E using only the above parti-tion and . If so, a  X  -rank net can be constructed in a greedy fashion as a maximal set whose points ver-ify ( 4 ). This can be obtained by adding sequentially an arbitrary object to the net and excluding from fu-ture selections any nodes that violate ( 4 ) w.r.t. this newly added object. Indeed, for all y  X  E , B y ( d y ) = S The statement thus follows as ( 4 ) is equivalent to y  X  /  X  B y ( d y )  X  y /  X  B y  X  ( d y  X  ). To construct the Voronoi balls B y ( r y )  X  E , y  X  R , we initialize each such ball to contain its center y . For each z  X  E \R , let j min be the smallest j such that R  X  A z j 6 =  X  ; the object z is then added to the ball B y ( r y ) of every y  X  N  X  A z j For each y , B y ( d y ) can constructed in O (log | E | ) time via binary search on the ordered list of equidistant objects. Constructing the rank net in a greedy fashion requires determining which objects violate ( 4 ) w.r.t. a newly added object on the net, which may take O ( | E | ) time. Hence, the overall complexity of constructing R is O ( | E | ( |R| + log | E | )). Finally, the construction of the Voronoi balls requires O ( |R| ) steps per object in E to assign each object to a ball.
 Note first that, for all distinct y, y  X   X  R , the balls assume w.l.o.g. that d y  X  d y  X  which implies that d ( y, y  X  )  X  d y  X  d y  X  . This is due to the fact that ( B ( y, d y  X  ))  X   X  ( E ), and hence, by ( 3 ), d y can be at most d y  X  + d ( y, y  X  ). In case d y or d y  X  is zero, clearly d ( y, y  X  ) &gt; d y / 2 &gt; d y / 4+ d y  X  / 4. If 0 &lt; d d ( y, y  X  ) &gt; d y / 2  X  d y / 4 + d y  X  / 4 . . If d y  X  in all cases d ( y, y  X  ) &gt; d y / 4 + d y  X  / 4 and as a result B ( y, d y / 4)  X  B ( y  X  , d y  X  / 4) =  X  .
 To prove Lemma 2 , observe that d y  X  2 R for all y  X  R since ( y, d )  X  ( E ) &gt;  X  ( E ) for d  X  2 R . Therefore, d y / 4  X  R/ 2 and thus B ( y, d y / 4)  X  B ( x, 2 R ) . Hence, by the definition of c ( ), P y  X  X  ( B ( y, d y / 4))  X  ( B ( x, 2 R ))  X  c ( E ) . More-over, P y  X  X  ( B ( y, d y / 4))  X  c  X  2 P y  X  X  ( B ( y, d c  X  2  X  ( E ) |R| . Therefore, |R|  X  c 3 / X  .
 Observe first that, for all z  X  E , there exists a y  X  R such that z  X  B ( y, d y (  X , E )). To see this, assume otherwise. Then for any y  X  R , d ( z, y ) &gt; d y (  X , E )  X  min { d y (  X , E ) , d z (  X , E ) } and we can add z to R , which contradicts its maximality.
 To prove Lemma 3 , we consider the following two cases. Suppose first 0 &lt; r y  X  d y . By ( 3 ), for any lar, ( B ( y, d y / 2) &lt;  X  ( E ) . By the definition of c , ( B ( y, r y )  X  ( B ( y, d y )  X  c X  ( E ) . For the second case, suppose that r y &gt; d y . Let z  X  V y is the point for which d ( y, z ) = r y . By the above observation, we know that there exists a y  X   X  R such that d ( z, y  X  )  X  d y  X  r since z  X  V y . Using the triangle inequality, we get d ( y, y  X  )  X  d ( y, z ) + d ( y  X  , z )  X  2 d ( y  X  , z )  X  2 d know that B ( y, r y )  X  B ( y  X  , d ( y, y  X  ) + r y ) . Since r d ( y, z )  X  d y  X  we can say B ( y, r y )  X  B ( y  X  , 3 d nally, by the definition of c , we have ( B ( y, r y ))  X  ( B ( y  X  , 3 d y  X  ))  X  c 2 ( B ( y  X  , d y  X  ))  X  c 3  X  ( E ) . Note first that, by induction, it can be shown that the version space is a subset of E ; correctness is implied by this fact and the termination condition. To bound the number of queries, we first show that the process RankNet constructs a net with small cardinality. Lemma 4. RankNet terminates at  X  &gt; 1 4 c 3 . Proof. To see that the while loop terminates, ob-serve that, by Lemma 3 , for small enough  X  &lt; min z  X  E ( z ) / ( c 3 ( E )), all Voronoi balls B y ( r  X  -rank net R will be singletons, so I will indeed be empty. Suppose thus that the loop terminates at some  X  =  X   X  . Since it did not terminate at  X  = 2  X   X  , there exists a ball B y ( r y ) of the 2  X  -rank net R such that r y &gt; 0 and ( B y ( r y )) &gt; 0 . 5 ( E ). By Lemma 3 , ( B y ( r y ))  X  c 3 2  X  ( E ), and the lemma follows. Hence, from Lemmas 2 and 4 , we get that the rank nets returned by RankNet have cardinality at most 4 c 6 . On the other hand, by construction, a net cov-ering a ball B y r y consists of either singletons or balls with mass less than 0 . 5 ( B y ( r y )). As a result, at each iteration, moving to the next object either halves the mass of the version space or leads to a leaf, and the search terminates. As at any point the version space has a mass greater than ( t ), the search will terminate after traversing most  X  log 2 (1 / ( t ))  X  iterations. Since, at each level, the number of accesses to the oracle are R  X  1  X  4 c 3 , the total query cost for finding target t is at most 4 c 6  X  log 2 (1 / ( t )  X  , and the query complexity statement follows. Finally, from Lemmas 1 and 4 , the computational complexity of each RankNet call is at most O n (log n + c 3 ) log c .
 We first show the following auxiliary result. Lemma 5. Given a target t and a noisy oracle with error probability bounded by  X  , the tournament among elements of the net R with repetitions k  X  (  X , |R| ) re-turns the element in the set R that is closest to target t with probability at least 1  X  (  X  + 1 / X  )  X  2 . Proof. We assume for simplicity that there are no ties, i.e. there is a unique point in R that is closest to t . The case with ties can be deduced similarly. We first bound the probability p ( k ) that upon repeating k times queries O t ( x, y ), among x and y the one that wins the majority of comparisons is not the closest to t . Because of the bound  X  on the error probability, one has p ( k )  X  Pr(Bin( k,  X  )  X  k/ 2), where Bin( , ) denotes the Binomial distribution. Azuma-Hoeffding inequal-ity ensures that the right-hand side of the above is no larger than exp(  X  k (1 / 2  X   X  ) 2 / 2). Upon replacing the number of repetitions k by the expression ( 5 ), one finds now the games to be played by the element within R that is closest to t . There are at most  X  log 2 ( |R| )  X  such games. By the union bound, the probability that the closest element loses on any one of these games is no less than (  X  + 1 / X  )  X  2 , as announced.
 By the union bound and the previous Lemma we have conditionally on any target t  X  N that Pr(success | T = t )  X  1  X  P  X   X  1 (  X  + 1 / X  )  X  2 ). The latter sum is readily bounded by  X  . The number of comparisons given that the target is term depends only on the doubling constant c . The bound on the expected number of queries follows by
