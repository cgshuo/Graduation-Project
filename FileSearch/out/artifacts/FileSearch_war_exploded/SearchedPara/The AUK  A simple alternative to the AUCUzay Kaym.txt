 1. Introduction Many real world situations involve the use of competing models.
Whether one tries to predict a borrower X  X  financial stability, the throughput of some complex industrial process, or consumers X  behavior in a supermarket  X  one has very often several competing models at hand, each trying to predict the phenomenon. Typically, some models do better than the others, and the user eventually has to rank them and select those with the best performance. Correct ranking or the selection of the best performing model may have a great influence on the unpaid debt, the factory throughput or the revenue of the supermarket. Frequently enough, the ranking of under some conditions while another outperforms it under different circumstances.

Receiver operating characteristic (ROC) curves are among the most popular tools which have been proposed over the years for ranking model performance. ROC curves are two-dimensional graphs in which true positives are plotted against false positives.
Historically, they have been used for finding an optimum operat-ing point for radio and radar transmissions, but the data mining community has found out that they can also be used for studying classifier performance. Detailed information about ROC curves in data mining can be found in Fawcett (2006) and Flach (2004) . A recent comprehensive survey which covers ROC curves as well as other graphical methods for performance evaluation can be found in Prati et al. (2011) .

ROC curves are very useful while assessing model accuracy in classification problems. However, it is often required to encapsu-late the core information they provide within a single scalar quantity. For example, when many ROC curves cross each other it is not always trivial to rank the various models. In such cases the use of scalar measures can help. Scalar performance measures are also used in order to perform statistical ranking tests ( Dem  X  sar, 2006 ). Using scalar performance measures often comes at a risk of losing valuable information, but nevertheless, there are many occasions in which their use is practically unavoidable.
The area under a ROC curve (the so-called AUC index) is one of the most commonly used scalars for ranking model performance ( Wu et al., 2007 ; He and Garcia, 2009 ). The AUC is very simple to calculate and interpret. Models can naturally be ranked according to their AUC index for any given data set. The AUC is independent of class priors. It also ignores misclassification costs ( Hand and get the very same value of AUC given a trained classification model and an identical testing data set. However, AUC has its shortcomings as well. Hand (2009) has shown that using the AUC is equivalent to assuming that misclassification costs depend on the classifier, an assumption which clearly does not make sense, since these costs are problem domain dependent rather than classifier dependent. He proposed a new measure called the H-measure which is dependent on the class priors.

The H-measure is indeed a major improvement over the AUC as it takes the priors into account. This modification is of particular importance for highly class-imbalanced, or skewed, data sets. Nevertheless, the H-measure makes some assumptions regarding the distribution of the loss functions, which may or may not be valid for some application domains. These assumptions can be replaced by others of course, but in this case the proposed
H-measure will lose the fundamental property which is required ent assumptions by different researchers will lead to different
H-measure values). Furthermore, the H-measure must be normal-ized in order to transform it onto a scale that has an intuitive meaning for researchers (such as the interval [0,1]). Although the normalization procedure is fairly simple, it makes the H-measure intuitively less interpretable.

In this work, we propose a new measure as a simple alter-native for the AUC. The newly proposed measure is based upon a well-established measure called Cohen X  X  Kappa ( Cohen, 1960 ).
Cohen X  X  Kappa has been used in major disciplines such as medicine and statistics for a couple of decades for measuring classifiers X  performance ( Zhou et al., 2002 ), but it is still less common in data mining circles. The relation between ROC curves and Cohen X  X  Kappa has been initially studied in Ben-David (2008) .
However, this relation was expressed in terms of several para-meters related to the data set studied and the specific model that is developed. In this paper, we express the relation between the
ROC curves and Cohen X  X  Kappa using the percentage of positive samples in the data as a parameter. Thus, the relation is abstracted away from the specific model one has chosen. This leads to the new insight that Cohen X  X  Kappa can be seen as a non-linear transformation of the difference between the true positive rate and the false positive rate. Furthermore, Ben-David (2008) did not propose a new measure as an alternative to the AUC. In this paper, we do exactly that and propose, for the first time, an alternative measure to the AUC based on Kappa. In this current work, we extend Kappa to two dimensions in a way which resembles how ROC curves are being plotted (i.e., by plotting
Kappa versus the false positive rate). Later we calculate the area under the Kappa curve, resulting in a scalar which we call AUK (stands for the area under the Kappa curve). It turns out that there is an indirect relation between AUC and AUK. Similar to the H-measure, but unlike AUC, the newly proposed measure, the
AUK, is dependent on the priors. This should be the case for any well-performing measure as pointed out by Hand (2009) . How-ever, unlike the H-measure, the AUK does not assume any explicit loss function distribution. Similar to the AUC and the H-measure, the AUK is also objective in the sense mentioned earlier.
Since the AUK has its origins in Kappa, it prefers correct class-ifications of the minority class to that of the majority class. We argue that what might seem a strange feature of a classifier performance measure is actually a virtue for data sets which are highly skewed. Another attractive feature of the newly proposed than the H-measure. Another notable added value of the AUK is In this way, the AUK can be more useful in data mining than AUC.
By showing the relation between the newly proposed AUK and the well-known AUC, the observations from this research can contribute to a better understanding and acceptance of both Cohen X  X  Kappa and the AUK within the data mining community.
Although we restrict our discussion here to binary problems, we think that extensions of the proposed AUK to multi-class pro-blems may be easier than similar extensions to AUC and the H-measure. This is due to the fact that multi-class versions of
Kappa are already known for decades. However this issue needs to be studied separately.
 the preliminaries upon which the analysis depends. The relation between ROC curves and Kappa is discussed in Section 4 , where it is proposed to determine the value of Kappa as a function of the the ROC curve. The AUK index is introduced in Section 5 . The relation between AUC and AUK is explored in Section 6 . Guide-lines for selecting the optimal model from a Kappa curve are discussed in Section 7 . Some illustrative numeric examples are research can be found in Section 9 . 2. Preliminaries tinguish between a positive class  X  and a negative class .Theperfor-mance of a classifier can be assessed by studying the so-called confusion matrix . Table 1 shows a confusion matrix for a binary classification problem. Indicated by p is the fraction of positive examples that are shown to the classifier, and n is the fraction of shows values of a hypothetical example, in which the  X  indicates fraudulent credit card transactions, while the represents legit-imate ones. This common type of applications is typified by p 5 n (it is most likely that the company will file for bankruptcy well before p  X  n ). The company X  X  main goal is to correctly classify the minority class (  X  ), as is the case with many financial, medical, industrial, military and other applications. Nevertheless, both FP and FN do incur costs: FP in terms of lost revenues and/or honest clients X  dissatisfaction, and FN in fraudulent purchases. We assume here that the exact values of these costs are unknown. All we know ing a . This is the case with many real world applications. Since we want our measure to be objective, we avoid making here additional assumptions.

FN and FP . Accuracy a , for example, is computed as a  X  TP  X  TN :  X  1  X  is an unimpressive outcome considering the fact that a random or a majority based classifier would have correctly classified 93% of the cases. More importantly, 37.5% (3/8) of the alarms it raises are false. The disadvantages of accuracy as a performance index will not be discussed here any further since the topic has been well covered in Cohen (1960) , Provost and Fawcett (2001) , Dem  X  sar (2006) , Hand (2009) , Ben-David (2008) and Fawcett (2006) ,to mention a few.
 been suggested over the years. Among them are error rate (1 a ),
Kolmogorov X  X mirnov (KS) statistic ( Djuric and Miguez, 2009 ), specificity and sensitivity ( Written and Frank, 2005 ), precision and recall ( Written and Frank, 2005 ), likelihood ratios (described in many statistical text books), receiver operating characteristic (ROC) curves ( Bradley, 1997 ; Provost and Fawcett, 2001 ), the area under ROC curve (AUC) ( Bradley, 1997 ; Provost and Fawcett, 2001 ), and recently, the H-measure ( Hand, 2009 ). ROC curves and AUC have become very popular in recent years ( Dem  X  sar, 2006 ;
Hand, 2009 ). They are central to our discussion later, so we present the notation we use throughout this paper.

The following relations hold by the way the confusion matrix has been defined: p  X  TP  X  FN ,  X  2  X  n  X  FP  X  TN ,  X  3  X  ^ p  X  TP  X  FP ,  X  4  X  ^ n  X  FN  X  TN :  X  5  X 
The true positive rate t and the false positive rate f are defined as: t  X 
TP p ,  X  6  X  f  X 
FP n :  X  7  X 
Bythesametoken,thefalsepositiverateisthecomplementofthe true positives. Ideally, the model should be both sensitive and specific, but these two goals often conflict, so one needs to make a trade-off between the two objectives.

In a ROC curve R , the true positive rate t (usually on the vertical computes a continuous valued output, it can be turned into a binary one by applying a threshold to the output. If the output
ROC curve for the classifier can be computed by varying the value of the threshold and recording the false positive and true positive rates for all possible thresholds. Many binary classifiers fall into class belonging either to the  X  or to the class, so both t and f can be computed from the corresponding cumulative distributions. A ROC curve R is, thus, a function g of f . In other words, R is characterized by t  X  g  X  f  X  :  X  8  X 
Example ROC curves are shown in Fig. 1 (a). ROC curves are very powerful visualization tools for comparing the performance of two or more classifiers. When a ROC curve of classifier A dominates the one of B (i.e., it has higher t values for all values of f ), one can conclude that A is preferred to B. This case is demonstrated in Fig. 1 (a). ROC curves can generally convey more information than what is possible by a single scalar metric. However, often ROC curves do intersect, making such a judgment less obvious. Very often, such as when one wants to perform a ranking statistical test, a simple objective scalar metric is needed ( Dem  X  sar, 2006 ).

The area under a ROC curve (the so-called AUC index) is a well-known scalar measure of the overall performance of a classifier, averaging across different thresholds that can be used to generate a classifier. In recent years it has gained much popularity in the machine learning and data mining literature. According to our notation, the AUC is defined as AUC  X 
In general, a model with a larger AUC is preferred to a model with a smaller one. The AUC of a random classifier is 0.5, because its ROC curve is the straight line t  X  f . Since one is typically interested in classifiers which have some positive added value over random classifiers, the AUC range which is of interest in the Gini coefficient. The Gini coefficient is equal to twice the area between the ROC curve and the diagonal. Simple geometry dictates that Gini  X  2AUC 1 :  X  10  X 
The AUC has some attractive features, most notably its objectivity. Given a testing data set and a classifier, every researcher will get the same AUC. The AUC is equivalent to the Mann X  X hitney X  X ilcoxon statistic and it also has some very intuitive statistical interpretations which will not be discussed 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 rate of true positives (t) 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Cohen X  X  kappa here ( Hand, 2009 ). However, the AUC suffers from some draw-backs. For instance, when two (or more) ROC curves cross each other, it is not clear whether the classifier with the larger AUC is always preferable. Moreover, a recent study by Hand (2009) has pointed at a much more fundamental problem with the AUC, one which has attracted little attention so far. Hand showed that the
AUC uses different classification cost distributions for different classifiers. He also suggested an alternative to the AUC, called the
H-measure. Here we propose another alternative to the AUC, called the AUK (the area under Kappa curve), one which solves the AUC X  X  key problem without resorting to prior explicit assump-tions about loss function distributions as the H-measure does. In the following sections, we develop the new measure, the AUK, and discuss some of its key features. We also show how the
AUK relates to the AUC, and how by using the AUK one can find an optimal classifier in a straightforward manner. We later show an example comparing it with the AUC. Let us begin with a short introduction to Cohen X  X  Kappa, which our proposed measure relies upon. 3. Cohen X  X  Kappa Since its original proposal in 1960 ( Cohen, 1960 ), Cohen X  X 
Kappa has gained an increasing popularity as a performance measure in disciplines such as medicine, psychology, and statis-tics. It is less popular in data mining circles, though. For this reason, we devote here a section which re-introduces it and highlights some of its key features.

Cohen X  X  Kappa is a scalar defined as:  X  probability of predicting the correct class due to chance: p  X  p ^ p  X  n ^ n :  X  12  X 
Note that the assumption behind (12) is that each test case is randomly classified as  X  or with probabilities ^ p and ^ n , respectively.

Kappa has some characteristics that make it suitable for assessing classifier performance as well as for feature selection which is shared by the H-measure, but not by the AUC and the
Gini coefficient. As can be seen, Kappa is simple to calculate and to interpret. Kappa ranges from 1to  X  1. Any random and majority based classifier results in k  X  0. Negative Kappa values indicate worse than random performance, so these values are usually of no interest to the data mining and machine learning communities. Similar to the Gini coefficient, when its value is larger than 0, Kappa measures the added value of a classifier over a random or a majority based one.

Multi-class versions of Kappa as well as their cost-sensitive variants do exist since the eighties. The basic idea is to add a weight w ij to (11), reflecting the severity or cost of an error topic is outside the scope of this paper, but it is worth mentioning here that these cost-sensitive extensions to Kappa are encapsu-lated in a single formula, which does not require the splitting of a data set into (potentially) many binary problems as suggested, for example, in Hand and Till (2001) . This property makes the multi-class, cost-sensitive, version of Kappa relatively simple to calcu-late and interpret. The interested reader can find more details in Fleiss (1981) .

Perhaps one of the major reasons Kappa has not gained a wide acceptance within the data mining community is the well-known that of the majority one. We demonstrate this feature through an example. We have seen the confusion matrix of classifier A in
Table 1 . Suppose now that another classifier, say B, results in the confusion matrix shown in Table 2 , and that still another classifier, C, gives the confusion matrix shown in Table 3 . Despite of the test cases, the Kappa values are not identical because of the difference in class priors: k A  X  0 : 640, k B  X  0 : 589, and
Note that classifier B was penalized, while the Kappa value resulting from C improved relative to that of A.
 component whatsoever, the results we have just gotten may seem rather strange at a first glance. Although we have not explicitly quantified any trade-off between the benefits of correct classifica-tions, the way Kappa compensates for random successes did have an effect. Of course, these implicit trade-offs may be modified in the cost sensitive versions of Kappa discussed earlier. However, we argue that this property of Kappa is actually a virtue for many real world classification problems in which it is more important to correctly classify the minority rather than the majority class.
Having assumed here that all classification costs or benefits are unknown, and that we only prefer to correctly classify the minority to make sense. In the following sections, we use the basic definition of Kappa, as shown in (11), for binary problems. In the next section, we study the relation between Kappa and ROC. 4. The relation between ROC and Kappa
Cohen X  X  Kappa has been studied in Ben-David (2008) . This analysis has formulated a relation between ROC and Kappa given the percentage ^ p of positively classified samples. This relation, however, is dependent on the model and can be different for each classifier. In this paper, we extend the analysis further by quantifying the relation between ROC and Cohen X  X  Kappa in terms of the percentage p of positive samples in the data set, which is independent of the classifier chosen. This analysis results in a simple and elegant alternative measure to the AUC, which we propose in this paper.
 a  X  tp  X  n  X  1 f  X  :  X  13  X 
Substituting (13) and (12) into (11) gives  X  pt  X  n  X  1 f  X  p ^ p n ^ n
By using the relations (2) X (5), we can re-write (14) as  X  pt  X  1 p  X  f  X  2 p 1  X  ^ p
Hence, for each point in the ROC space, we can compute the corresponding Kappa value using (15). Note that by re-arranging (15), it is possible to express the ROC curve in terms of Kappa as: t  X  1 p p f  X  X  1 k  X 
In (15), Kappa is expressed as a function of ROC in terms of the percentage positive samples predicted by the model. Since this number is different for each model, comparison is easier if Kappa is expressed in terms of the percentage of positive samples in the data. Note that ^ p  X  pt  X  X  1 p  X  f  X  f  X  p  X  t f  X  :  X  17  X 
Then, Kappa can be written as  X  pt  X  1 p  X  f  X  2 p 1  X  X  pt  X  X  1 p  X  f  X 
Although the above equations are useful for converting ROC into Kappa, they are not very illuminative for understanding the relation between ROC and Kappa. This relationship is clearer if we re-arrange the terms in (18) to yield  X  p  X  X  1 2 p  X  f  X  p  X  1 2 p  X  X  t f  X   X  h  X  t f  X  ,  X  19  X  where h is a non-linear function of t f , with parameters f and p .
In other words, Kappa can be seen as a non-linear transformation of the difference between the true positive rate and the false positive rate. Note, however, that the false positive rate is also a parameter of the non-linear function h . Furthermore, the line t  X  f is the ROC of a random model. Hence, Kappa can also be interpreted as a non-linear transformation of the difference between the ROC of the model to be evaluated and the ROC of a random one. Therefore, Kappa is related to the performance improvement that a classifier achieves over the performance of a random classifier with the same false positive rate.
Fig. 1 (b) shows, for two classifiers, the Kappa curves corre-sponding to the ROC curves in Fig. 1 (a). In this simple example, A is closer to the upper left corner of the ROC space than the
ROC curve of classifier B. Similarly, we observe that the Kappa is higher for all values of the false positive rate. In realistic examples, however, such as the one to be discussed in Section 8 , ROC curves frequently do intersect each other, making the visual rankings of the various models rather difficult if not impossible.

The special case of p  X  n  X  0 : 5 for the relation between the ROC and Kappa should be mentioned separately. In that case, the following relation holds:  X  t f :  X  20  X 
Hence, when there are as many positive samples in the data as there are negative samples, Cohen X  X  Kappa is equal to the ROC improvement that a classifier brings over a random model. In this case, Cohen X  X  Kappa provides exactly the same information as the
ROC curve. 5. The AUK index
The above discussion exposes the close relation between a ROC curve (true positive rate plotted against the false positive rate) and the Kappa curve (Kappa values plotted against the false positive rate). Just like the area under the ROC curve (AUC) is an indication of the overall performance of a classifier, the area under a Kappa curve can also be seen as such an indication. Therefore, we propose to plot Kappa against the false positive rate as an alternative way of analyzing the performance of a classifier and to assess its overall performance by computing the area under the Kappa curve. We call this new index the AUK index (AUK stands for the area under the Kappa curve).
 The AUK is defined as: AUK  X  Note that AUK already accounts for the uninteresting area below the main diagonal. Furthermore, Kappa inherently takes into account the class skewness in the data, which we consider to be desirable.
 Since Kappa is a transformation of the difference between the ROC curve of a classifier and the ROC curve of a random model, AUK can be seen as a transformation of the area between the ROC curve and the ROC of a random model, one which compensates for skewness and for random successes. 6. The relation between AUC and AUK Since AUC is the area under a ROC curve and ROC is related to Kappa, AUK is related to the AUC. AUK is a transformation of the area under ROC that is above the main diagonal. It can therefore be expected that AUK is a function of the difference between the model AUC and the AUC of a random classifier (which is equal to the constant 0.5).

For the special case where p  X  n  X  0 : 5, the AUK can be computed straightforwardly as AUK  X  AUK  X  AUC 0 : 5 :  X  23  X 
Hence, in this special case, AUK and AUC differ only by a constant. Furthermore, by substituting (10) and re-arranging it is found that AUK  X  1 2 Gini :  X  24  X  In other words, AUK is equal to half the Gini coefficient when there is no class skew in the data set. 7. Selecting the optimal threshold
There are a number of advantages to using the Kappa curve and the AUK. First of all, Kappa as a measure accounts for correct above a random one can be assessed immediately. Secondly, the Kappa curve inherently accounts for class skewness. From (19) it can be seen that the percentage of positive class samples appears explicitly in the transformation. This introduces a correction for class skew. Thirdly, the convex Kappa curve has a unique max-imum that can be used to select the optimal model (by Kappa) from the set of models that have been used to generate the ROC and/or Kappa curves. Of course, one has to be careful about sampling effects and be aware that the training data is just one realization of the underlying distribution. However, such considerations can be overcome by re-sampling techniques such as bagging. In that case, the expected Kappa curve can be generated by averaging over a large number of samples from the same underlying population.

Finding the maximum of the Kappa curve can be achieved by taking the derivative of the curve and setting it equal to zero. d k df  X  Hence, the value of Kappa is maximized when the gradient of the
ROC curve equals 1. This is a well-known method of selecting an optimal model from a ROC curve ( Flach, 2004 ; Fawcett, 2006 ). The above equation shows that a model which is selected in this way is also optimal in the sense that it maximizes Cohen X  X  Kappa when the data set is balanced. Therefore, as far as balanced data sets are concerned there is no added value in finding the model that maximizes Kappa since it is the very same model with the ROC gradient equal to 1.
 When there is a class skew in the data set, the derivative of the
Kappa curve becomes more complex. By taking the derivative of the Kappa curve and setting it equal to zero, one finds that the following condition should be satisfied to maximize Kappa: dt df  X  1  X   X  1 2 p  X  X  t f  X  p  X  X  1 2 p  X  f :  X  26  X 
Eq. (26) shows that when p o 0 : 5 (which is usually the case), the second term is positive, and hence the optimal point that max-imizes Kappa shifts towards the left of the ROC curve, so that the classifier needs to be more discriminative. 1 By maximizing Kappa, it now becomes possible to compare different classifiers as well, since a unique condition for optimality has been established. In other words, we can find the classifier that maximizes accuracy (discounted by the chance events) in a very elegant way. 8. Examples
In this section, we will expose two examples to illustrate how the implications from Kappa curves and the AUK index may differ from that of ROC curves and the AUC index. As our first example data set we have used the Statlog German Credit data set from the UCI Machine Learning Repository ( Asuncion and Newman, 2007 ).
Since the AUK measure takes into account the skewness of the data, we produced a highly skewed version of the original UCI data set by randomly deleting samples of the minority class until they were 11% of the total. This example demonstrates that although AUK and AUC are related, the rankings they produce may differ.
 randomly to divide it into a training set consisting of 500 patterns and a test set consisting of the remaining examples. By using the training data, we have generated two models. The first model is a linear regression. The second is a neural network with one hidden layer containing five neurons. We used the standard logistic function as the transfer function both in the hidden and the output layer X  X  nodes. The experiments were performed in Matlab by using Mathworks X  Neural Networks Toolbox. The ROC and the Kappa curves have also been computed in Matlab.
 computed by using the algorithm described in Fawcett (2006) .
Fig. 2 (b) shows the Kappa curves computed from the ROC curves by using the relation (19). Note that both the ROC curves and the
Kappa curves intersect. Hence, it is not trivial to determine which model is preferable. In order to rank the models, we have computed the AUC and the AUK. The results are shown in Table 4 . preferable to the neural network. However, AUK shows the reverse ranking. This demonstrates the fact that although there is a relationship between AUK and AUC, the rankings of AUC and
AUK may differ from each other. This is not always the case, of course, since AUK and AUC are correlated. For instance, for the original Statlog German Credit data set (with 30% minority class) the rankings by AUK and AUC were identical, and they are not shown here. rate of true positives (t) Cohen X  X  Kappa
Similar results were obtained when the ROC and Kappa curves were not made convex, as shown in Fig. 3 . These ROC curves have also been computed according to the method discussed in
Fawcett (2006) . The corresponding values of AUC and AUK are shown in Table 5 . Note that the rank reversal shown in Table 4 is also observed here.

The Kappa values from Fig. 2 (b) can be used for identifying the optimal operating point (and the corresponding threshold) for the best performing model (the neural network). The optimal false positive rate that maximizes Kappa is 0.60. This corresponds to a maximum Kappa value of 0.328. At this operating point, the true positive rate is 0.928 as can be seen in Fig. 2 (a).
 As our second example data set we use the Blood Transfusion Services Center (BTSC) data set, again from the UCI Machine
Learning Repository ( Asuncion and Newman, 2007 ). This same data set was also used by Yeh et al. (2009) . The data set contains 748 samples taken from the donor database of the Blood Transfu-sion Service Center in Hsin-Chu City in Taiwan. There are five real valued attributes for each sample. The positive samples constitute 23.8% of the data set. The goal is to predict whether a donor will donate blood within a certain month. Hence, the problem is a classification problem. The scenario we want to use as an example is one of the selection of model structure parameters. We devel-oped two feed-forward neural network models: one with one hidden layer (two nodes) and one with two hidden layers (three and five nodes in each layer, respectively). Therefore, we are interested in selecting the better model structure in terms of the number of hidden layers and the number of nodes in each layer.
Since our goal with this example is just to show the different results for model selection when using the AUC or the AUK, we did not divide the data set into a training and a test set. The results from the training set can be inspected in Fig. 4 . The values of AUC and AUK are shown in Table 6 . The AUC is similar for both models, and hence AUC could not differentiate between the two models. However, with AUK, the two layer model scores clearly better. Hence, the use of the measure has an impact on the selection of the structure parameters. It should be mentioned that for the non-convex versions of these two graphs, the conclusions remain equal.
Summarizing, our first example shows that AUC and AUK might fundamentally disagree on which one of two models is best. In our second example, AUC cannot decide, but AUK can differentiate between the two models. Needless to say, there are also many other cases in which the AUC and the AUK do agree. 9. Conclusions and further research
A new measure for classifier performance, called AUK, has been at the focus of this paper. The relationship between ROC curves and Kappa has been discussed. We have proposed plotting Kappa against the false positive rates to obtain Kappa curves, in a way similar to ROC curves. Our analysis is an extension of the work presented in Ben-David (2008) . We have shown that there is corresponding point  X  f , k  X  on a Kappa curve, and so one can be easily converted to the other. In fact, we have shown that Kappa is a non-linear transformation of the difference between a model X  X  ROC value and the ROC of a random model. When the data set is balanced (i.e., p  X  0.5), the relation between the ROC curves and Kappa curves is the simplest: Kappa curve is then equal to the difference between a model X  X  ROC curve and the ROC curve of a random model.

The close relation between the ROC and Kappa curves implies that there is also a close relation between the area under a ROC curve and the area under a Kappa curve. We have proposed the area under the Kappa curve (AUK) as a new index for quantifying classifier performance. It has been shown that AUC and AUK are related to one another. AUK can be used for model performance evaluation in a similar way to the AUC. The difference is mainly that AUK accounts for class skewness in the data, while AUC does not. Therefore, it seems that AUK is a better measure of a model X  X  performance, especially when there is considerable class skew-ness in the data set. We have demonstrated with examples that using AUK instead of AUC may lead to different rankings, which is very relevant in applications of data mining. Note that when the data set is balanced, the new index, AUK, is simply half the Gini constant only. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 rate of true positives (t) 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Cohen X  X  Kappa Kappa curves can also be used for selecting an optimal model.
The convex Kappa curve has a unique maximum. In this way, the problem of selecting a suitable threshold for a model can be solved. For balanced data, Kappa is maximized at the point where skewness with p o 0 : 5, the maximum of Kappa corresponds to a point where the gradient of the ROC curve exceeds 1.0.
We have also discussed the fact that Kappa prefers correct
This feature has no effect when the data set is balanced, in which case Kappa still compensates for random successes. However, we have argued that when the data set is highly skewed, it is usually this very common case, using AUK is more realistic, and prefer-able to using the AUC as a performance index.

Our analysis in this paper has been focused on binary classification problems, where we have only assumed that one prefers to correctly class. No other explicit assumption about benefits or loss functions have been made. The exte nsion of our analysis to multi-class and/or cost-sensitive classification problems is left for future research. References rate of true positives (t) Cohen X  X  Kappa
