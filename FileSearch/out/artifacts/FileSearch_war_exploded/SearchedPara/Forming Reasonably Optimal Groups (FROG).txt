 Instructors often put students into groups for coursework. Several tools exist to facilitate this process, but they typi-cally limit the criteria one can use for forming groups. We have defined a general mathematical model for group forma-tion: a set of attribute types, group-formation criteria, and fitness measures. We have implemented an optimizer that uses an evolutionary algorithm to create groups according to the instructor X  X  criteria. Our experiments support the hypothesis that, even with a general model, reasonably op-timal solutions to the group-formation problem can be found in reasonable time. Several instructors have used the tool to form groups for their courses. In all cases, they were im-pressed by the expressiveness of the model and pleased with the quality of the groups produced.
 K.3.1 [ Computers and Education ]: Computer Uses in Education X  collaborative learning ; K.4.3 [ Computers and Society ]: Organizational Impacts X  computer-supported col-laborative work ; D.2.2 [ Software Engineering ]: Design Tools and Techniques X  evolutionary prototyping ; I.2.8 [ Ar-tificial Intelligence ]: Problem Solving, Control Methods, and Search X  heuristic methods Algorithms, Human Factors, Experimentation Group formation, courseware, evolutionary algorithms, op-timization, collaborative learning, group work Instructors often put students into groups for coursework. Forming groups by hand can be very time-consuming, es-pecially with large classes and potentially complex criteria. For example, one could imagine wanting groups that are heterogeneous with respect to GPA, cover a range of skill sets, are homogeneous with respect to where students live, leave no woman alone in a group, and have students whose timetables include at least one free time-slot in common. Most instructors would not attempt to satisfy such require-ments by hand, opting to compromise on the criteria or to let students group themselves. Existing tools for group forma-tion are either customized to consider only specific student attributes and criteria or are otherwise constrained.
Our primary goal is to define an expressive, general model that gives the instructor as much control as possible over the criteria to be used in group formation, the attributes on which they are based, the relative weighting of the criteria, and the size of the groups to be formed. By using the model in actual classes, we aim to demonstrate that it can express what instructors want, and do so naturally.

Tractability of the group formation process is a significant concern, given that the number of possible partitions of a class grows explosively with class size. For example, there are more than 1 . 8  X  10 98 possible partitions of a class of 100 into groups of size 4. Like many other group-formation systems, we do not attempt to find an optimal solution, but instead aim to find a reasonably optimal one (hence the  X  X  X  in  X  X ROG X ); the user-interface can then allow the instructor to make adjustments to the groups. Others have demon-strated that optimization can be done effectively when the model is somewhat limited; we intend to show that it can also be done on our more general model in reasonable time. Furthermore, through real class use, we aim to show that instructors will actually use the groups that the FROG tool forms.
There are two main categories of group-formation soft-ware. Some tools are used to assist online learners in form-ing their own support networks that may include students from other networks [11]. Groups are not necessarily non-overlapping and a given student may belong to zero or more groups. Many authors have studied this problem, e.g. , [8, 19, 15]. In particular, Strnad and Guid [15] use a genetic algorithm working on fuzzy attributes to select good groups from a pool of employees, based on criteria specified by a company manager. Their optimizing algorithm is the closest to our own among the systems that we surveyed. Although we do not solve the same problem, our model could support this task because it provides a precise measure of fitness for each possible group.
Our interest is in the second category of tools: those that form non-overlapping groups of students for course work. Such a tool can also be used for other applications, e.g. , dividing all the players from a sports league into teams.
Greene [7] describes the related problem of partitioning a set of integers into subsets with roughly equal sums. Al-though group formation is a more general problem, his ge-netic algorithm had a strong influence on our approach.
Previous work on group formation has not used a standard terminology. We will use the following terms. The instructor has one or more criteria according to which groups should be formed. These refer to attributes of the students, such as GPA, sex, or timetable. An attribute may have several criteria that apply to it. For instance, if one attribute is a student X  X  favourite programming task, one might wish to form teams with heterogeneous preferences and at least one person who likes coding best. A partition is a division of the entire class into non-overlapping groups of the desired size, according to the instructor X  X  criteria X  X n other words, a solution to the grouping problem. Group fitness is a measure of the quality of a group with respect to the criteria, and partition fitness is a measure of the quality of the entire partition.

There are several existing tools for partitioning classes of students, but they generally limit the criteria one can apply. For instance, Graf and Bekele X  X  system [6] prescribes group size as well as exactly what student attributes and criteria will be used. Many can only handle very limited attribute types, for instance only integers [3, 5], only binary values [4], only timetable information [14], or only student preferences for each other [16]. Some limit the number of constraints [3, 18], others the type of constraint. For instance, DIANA [18] attempts to form only heterogeneous groups. The system of Christodoulopoulos et al . [3] can form either homogeneous or heterogeneous groups, but it cannot handle a mixture of homogeneous and heterogeneous criteria simultaneously. Some systems are unable to make important distinctions. For instance, the DLV solver [12] cannot distinguish between degrees of criteria satisfaction: all arrangements of students into groups that satisfy, say, three out of four criteria are treated alike, even though one may nearly satisfy the fourth criterion and another may be very far from satisfying it. Most systems treat all values for an attribute as equidistant from each other. For example, even though OmadoGene-sis [5] uses Likert scales for all student attributes, it treats values of 5 and 2 as equally distant from 1.

By far the most general team-formation software we have encountered is TeamMaker [2, 9, 10]. Their model of group formation has been developed into a tool, adopted both at their university and elsewhere. Still, their system has sig-nificant limitations. (Our analysis is based on the most re-cent implementation, available as part of the CATME sys-tem [1].) Although it has five question types (multiple-choice, choose-any-or-all-of, schedule-compatibility, under-represented-minority, and free-form 1 ) instructors must pick from a predefined set of specific attributes, such as gen-der or GPA. Instructors who find other attributes relevant ( e.g. , interest in writing, year of study) simply cannot use them. In addition, the available criteria for group forma-tion are extremely limited: for multiple-choice, choose-any-or-all-of, and schedule-compatibility attributes, one can re-
Free-form questions do not contribute to the team forma-tion process. quest only similarity or dissimilarity; for underrepresented-minority attributes, one can only request that no one from the underrepresented-minority be alone in a group. There is no way to express, for example, that gender should be similar within groups (since gender is an underrepresented-minority attribute), or that there should be at least one woman in each group (since there is no  X  X t least one X  cri-terion for any attribute type), or that no first-year student should be alone in a group (since gender and race are the only underrepresented-minority attributes defined).
Among all of these systems, many different approaches have been taken for the group formation process itself, in-cluding customized greedy algorithms [16, 14], clustering al-gorithms [3, 5], representing the task as a constraint satis-faction problem and using logic programming [13, 12], cus-tomized SQL queries [17], ant-colony optimization [6], other local search algorithms [2, 10], goal programming [4], and genetic algorithms [5, 15].
Based on a literature survey and discussions with col-leagues, we compiled a list of students X  characteristics used for forming groups. We defined attribute types that repre-sent as many of these characteristics as possible, as simply as possible.

The simplest attribute type in our model is numeric . It can be used to represent characteristics such as GPA, or score on a dimension of a learning style instrument. A nu-meric attribute can be constrained further by specifying up-per and/or lower limits. Some attributes concern a numeric value for which the instructor does not need to know the ex-act value; instead they want to know into what range each student falls (such as age  X  &lt; 18 X ,  X 18 X 21 X ,  X 21 X 30 X ,  X  &gt; 30 X ). A bin attribute is appropriate in this case.

A bin attribute is one that has a discrete set of possible values. Each value is like a bin that the students may fall into. If each student may have only one value on this at-tribute ( e.g. , sex, what college they belong to) we refer to it as a one-of bin ; if they may have several ( e.g. , which clubs they belong to, which prerequisite courses they X  X e taken, which languages they know), we refer to it as a many-of bin . The treatment of one-of bins and many-of bins could have been combined into a single generic bin attribute, re-quiring the instructor to specify minimum and maximum numbers of bins each student may select. We choose not to do this because we do not expect instructors to want more than these two special cases and handling them separately makes the model and the corresponding code easier to un-derstand. This also has implications for the user interface.
Each bin label is merely a string that the instructor speci-fies; there are no semantics to the different values. However, the user can define the distance between bins, allowing bins to correctly represent ordered values (see section 4.2).
The timetable attribute specifies when a student is avail-able or unavailable and can be used for assessing compat-ibility for group meetings. Each student X  X  value on the timetable attribute indicates their availability in each time-slot, on either a two-point ( X  X vailable/unavailable X ) or three-point scale. The instructor controls the meaning of the in-termediate value for a three-point scale (see section 4.3).
A ranking attribute records a student X  X  ordering of cer-tain information e.g. , languages ordered by level of knowl-edge. The teammate preference attribute records a stu-dent X  X  preferences for and against grouping with particular students. The fitness measures for these two attributes have not been finalized and will be discussed in a future paper. As will be seen in section 6, this did not diminish the use-fulness of the tool for the classes on which it was tested: none of the instructors for those classes required ranking or teammate preference attributes to achieve their goals.
The instructor specifies criteria for group formation in terms of the attributes he or she has chosen as relevant. Table 1 lists the possible criteria that the instructor may choose to apply for each type of attribute. If desired, there may be more than one criterion applied to an attribute.
This section defines these criteria, as well as functions that specify the fitness of an individual group and of an entire partition with respect to each single fitness criterion. Fitness values are always scaled so that they fall between 0 (low fitness) and 1 (high fitness), covering as much of this range as possible. Fitness functions make use of common variables listed in Table 2.

Because a student is either in a group or not (as opposed to systems like that of Christodoulopoulos [3] that determine the probability of a student belonging to a group), and be-cause there are exactly N n is the total number of students), fitness values are discrete, not continuous. Because our optimizer does not need to use the derivative of the fitness function, this is not a problem.
Numeric attributes have three possible fitness criteria: ho-mogeneous , heterogeneous and apportioned . Fitness for the first two is based on the average pairwise distance be-tween group members on this attribute, scaled by the max-imum pairwise difference across the entire class. 2 For group g and criterion c , defined on attribute a , let
In fact, this scaling factor is slightly too large since the av-erage pairwise difference between group members can never be as large as this maximum difference.
 F g,c fitness of group g on criterion c
S g set of students in group g n g number of students in group g ( n g = | S g | ) v s,a value of student s on attribute a
B a number of bins for attribute a h b,g,a number of students in group g who selected bin b b g,a number of bins covered by group g on attribute a t b,c target bin population of bin b for criterion c
M g,a maximum number of bins that could be covered
T a number of time-slots for timetable attribute a m c maximum number of common slots to count for w c weight of criterion c (for fitness calculations) F g fitness of group g (over all criteria) where N is the total number of students in the class, v s,a the value of attribute a for student s and n g is the size of group g . Then An instructor may wish for a numeric attribute such as GPA to be apportioned, i.e. , shared as evenly as possible among the groups. By choosing to apportion the numeric at-tribute GPA, students with high GPAs would be distributed across groups, as would students with low GPAs.

Unlike our other fitness criteria,  X  X pportionedness X  is es-sentially a property of the entire partition, rather than of a single group. However, it can be emulated by considering how close each individual group is to the class average. This allows us to compute fitness by group for this criterion, as for all the others. Let A a be the class average on attribute a , X a be the maximum possible group average for attribute a (the average of the n g students with the largest value on attribute a ), and Y a be the minimum possible group aver-age for attribute a (analogous to X a ). If c is an apportioned criterion on attribute a , then
Our model is very expressive in the range of criteria al-lowed for bin attributes. Criteria of type homogeneous or heterogeneous can be applied to one-of-bin attributes. Fitness is defined in the same way as for numeric attributes, but based on pairwise distances between bins. Rather than assume that all bins are equidistant, we use a matrix to spec-ify these pairwise distances. By default all bins are equidis-tant, but the instructor can specify that sequential bins be evenly spaced, or can completely specify the pairwise dis-tances. For example, suppose the bins represent majors of the students taking a course and labels are math , cs , psy-chology and history . The instructor may decide that cs is closest to math , and a little closer to psychology than to his-tory . Distances are required to be symmetric but not tran-sitive. 3 Because we provide this ability to define distances between bins, Likert scales can be properly implemented as one-of bins where for example, bin  X 2 X  can be defined to be one unit away from bin  X 1 X  and three units away from bin  X 5 X . This is in contrast to OmadoGenesis [5] where 1 is no more similar to 2 than to 5.

The correct interpretation of homogeneous and heteroge-neous fitness measures for many-of bins is not obvious. For instance, TeamMaker X  X  measure for its  X  X hoose-any-or-all X  questions tends toward 0 for groups that are supposed to be more homogeneous and toward 1 for groups that are sup-posed to be more heterogeneous. Yet, a group where three students selected no option would be given the same value as a group where each student selected mutually disjoint options X  X ven though the former group is arguably more homogeneous. Also, as described in section 6, users of our tool did not request this measure X  X or their purposes, our measures of coverage were sufficient. For these reasons, we did not define homogeneous and heterogeneous measures for many-of bins.

A bin-in-common criterion can only be applied to a many-of-bin attribute. Its fitness is defined as the fraction of the group who selected the most-commonly-selected bin for that group. This is equivalent to the population of the most-popular bin divided by group size or where h b,g,a is the number of students in group g who se-lected bin b on attribute a (used in criterion c ). If all the group members select at least one bin in common, the fitness becomes 1, regardless of the similarity of their other bin se-lections. This fitness function is useful when each group will have to come to a consensus on exactly one option, such as the topic for a project, but that choice can differ by group. Note that the model does not force different groups to choose different options.

Another type of criterion allows the instructor to specify that as many bins as possible should be covered. Note
That is, d ( a, b ) = d ( b, a ) but d ( a, b ) does not have to equal d ( a, c ) + d ( c, b ), for any bins a, b, c . that this is not the same as heterogeneity. Consider the case where the set of possible attribute values for a one-of bin is { A, B, C } . As-many-bins-as-possible will consider the group AAAAAB to be equivalent to AAABBB while het-erogeneous will assign a higher fitness value to the latter group. If simply getting bin coverage is the goal, and the distribution of students in the group does not matter, it is better to use an  X  X s many as possible X  criterion rather than heterogeneous because it gives the optimizer more possibil-ities for high quality groups.

Similarly, we allow the instructor to specify as few bins as possible , which is not the same as homogeneity.
With as-many-bins-as-possible, fitness is proportional to the number of bins covered; with as-few-bins-as-possible, fit-ness is the complement. We scale by the maximum bin cov-erage possible in order to force the fitness to be at most 1. That maximum depends on whether a single student can select multiple bins, that is, whether it is a one-of bin or many-of bin attribute.

Let B a denote the number of bins for attribute a , and b g,a denote the actual number of bins covered by group g on attribute a . Notice that these values depend only on the attribute, not on the particular criterion specified on that attribute. Then, the maximum number of bins that could possibly be covered by group g on attribute a is and This is essentially the measure used by TeamMaker for its  X  X ultiple-choice X  questions, except that TeamMaker X  X  mea-sure is scaled by n g alone, so it does not assign values from the full [0 , 1] range for groups where the number of students exceeds the number of possible choices.

Criteria of type at-least-t-bins-covered and at-most-t-bins-covered allow the instructor to define how much or how little bin coverage is desired instead of simply looking for as much or as little as possible. An instructor may wish to specify, for example, that teams should have at least two different languages represented.

Let t c represent the target number of bins covered on cri-terion c . One could define fitness as a step function with value 0 if less than t c bins were covered and 1 if t c or more bins were covered. This is not ideal for our optimizer be-cause it provides no information about how close a given group is to the target. Instead, we use a piecemeal linear ap-proximation to this step function where the fitness of groups with coverage below the target increases linearly from 0 to d and the fitness of groups with coverage above the target increases linearly from d to 1, where the  X  X egree of fitness X  d is an instructor-specified constant in [0 , 1]. Therefore, for at-least-t-bins-covered, we have as shown in Figure 1. Notice that while the function could be defined for continuous values of b g,a , this represents the number of bins from attribute a covered by group g and is therefore a discrete value. Figure 1: Fitness functions for at-least-t-bins-cover-ed (left) and at-most-t-bins-covered (right) Figure 2: Fitness functions for at-least-t-members-in-bin-b and at-most-t-members-in-bin-b The analogous case for at-most-t-bins-covered is
Criteria of type at-least-t-members-in-bin-b and at-most-t-members-in-bin-b are concerned with the num-ber of members in a particular bin rather than the number of bins covered. They allow one to specify that at least two group members should have taken a particular related course, or that no more than three students should be part-time students, for example. Let t b,c be the target number of group members in bin b according to criterion c . The maximum possible number of members in a bin is the group size and is the same for both one-of-bins and many-of-bins. Again h b,g,a denotes the number of students from group g who selected bin b on attribute a and we use piecemeal lin-ear functions, shown in Figure 2, to provide the optimizer with more information.
 Fitness for the at-least case is and the analogous case for at-most is
No-one-alone-in-bin-b criteria can be used to specify, for example, that no member of a particular minority is the only one in their group. Fitness is again a function of h the number of students from group g who selected bin b on attribute a . If exactly one student selected bin b , the group does not satisfy the criterion, but if either no student or two or more students selected bin b , the group satisfies the criterion. Figure 3: Fitness function for no-one-alone-in-bin-b
Again, we use a piecemeal linear function shown in Fig-ure 3, resulting in the fitness function
TeamMaker uses a very similar approach for its  X  X inority X  questions, except that their value  X  X latlines X  at 1 for any group whose number of minority students exceeds two.
For timetables, we define only one type of fitness criterion: the compatibility of students X  schedules. Let v s,a [ t ] denote the availability of student s during time-slot t , for timetable attribute a . Possible values are 0 (the lowest availability), 1 (the highest), or an intermediate value A (a constant in the range [0 , 1]), if the instructor chooses to use three avail-ability levels. Fitness of group g with respect to a timetable criterion c is defined in terms of the group X  X   X  X ommon slots X  where T a is the maximum number of time-slots. This is the sum over all time-slots of the product of each student X  X  availability. Each time-slot contributes between 0 (when one or more students give that slot the lowest availability value) and 1 (when all students give that slot the highest availability value) to the sum. Using the product effectively allows everyone a veto.

Having a few common free slots is highly valuable, but having more than five, for instance, may not be important. Therefore, the instructor has the option of setting a target number of common slots, m c . Fitness is again a piecemeal linear function: This function is similar to the one for at-least-t-bins-covered, except that m c plays the role of t c and it is defined in terms of the number of common free time-slots (a continuous quan-tity) instead of the number of bins that are covered (a dis-crete quantity).

Note that if the instructor chooses two levels of availabil-ity, C g is simply a count of the number of slots to which every group member gave top availability and group fitness simplifies to the number of slots where everyone is available, scaled and linearized. In that case, our measure is equiv-alent to TeamMaker X  X  [10] in having a cap, but our fitness function does not flatline at 1 once that cap is reached.
Instructors specify the relative importance of each crite-rion as an integer. The tool scales these to reals w c  X  [0 , 1] such that P criteria c w c = 1, and overall group fitness on all criteria is defined as
For fitness of the whole partition, Wang et al . [18] make a good case that  X  X airness X  (having groups of roughly equal fitness) is just as important as  X  X uality X  (having groups of generally high fitness). Because they restrict attribute val-ues to real numbers in [0 , 1], their system can use a clustering algorithm that pre-processes students to determine an  X  X deal target X  for group fitness. It then tries to form groups that achieve fitness as close as possible to this target.
Even though our attributes cannot all be represented as real numbers in the interval [0 , 1], we would like a measure of partition fitness that similarly maximizes the fitness of each group while minimizing its variability between groups. In addition, we need a measure that is efficient to compute (even at the cost of some accuracy), because this computa-tion will be carried out numerous times during optimization.
Currently, we use average group fitness as our measure of partition fitness. This is not ideal because it does not take outliers into account X  X t would give higher fitness to a partition with higher average group fitness, even if this came at the price of making the worst group even worse.

Alternatively, we could use the fitness of the worst group, like TeamMaker [10]. While this would be computationally efficient, it would not allow the optimizer to make progress independently of changes that affect the worst group.
We could experiment further to compare various defini-tions of partition fitness but experience with instructors us-ing our tool indicated that this basic definition of partition fitness sufficiently met their needs. Furthermore, they were satisfied with the partition proposed by our optimizer (see section 6). If further experience shows the need for a differ-ent measure of partition fitness (or even multiple measures from which a user could choose), this would be easy to ac-commodate. By design, the modularity of the model makes it easy to change fitness functions, and the model is also scalable: new attributes and criteria can be added without affecting its other components.
Our optimizer X  X  evolutionary algorithm maintains a pop-ulation of  X  X olutions X  (partitions of the entire class) that evolves over time, in discrete phases. During each evolu-tionary phase, the algorithm generates a set of  X  X ffspring X  partitions from the  X  X arent X  partitions in the current gen-eration, then replaces a fixed fraction of the worst parent partitions with the best offspring partitions.

Each offspring is generated by selecting two parent parti-tions at random, with probabilities proportional to the fit-ness of each partition, and performing a crossover operation on both parents, followed by a mutation operation on the resulting offspring. This is repeated for some fixed number of generations, and the best partition in the final generation is chosen as our final solution.

There are many variations possible on this basic scheme: mutations could be applied to parents in addition to off-springs, mutations could be applied with some probability (perhaps inversely proportional to the fitness of the partition being mutated), crossover and mutation could be applied in-dependently of the fitness of partitions, etc . However, the choices we made for our optimizer produced excellent results during experimentation and when used for real classes (both addressed in what follows). We plan to explore the space of possible evolutionary algorithms at a later date.
Our mutation operation simply swaps two students chosen uniformly at random from different groups in the partition being mutated. OmadoGenesis [5] uses essentially the same definition for mutation, except that they restrict it to pre-vent swapping students out of the current best group in the partition.
The purpose of a crossover operation is to create a new offspring partition from two parent partitions. Our crossover operation is modelled after the  X  X ager breeder X  crossover op-eration of Greene [7]. This starts by putting the best p groups from both parent partitions into the offspring par-tition (where p is the number of groups in each partition). This may result in overlap between groups that come from different parents ( i.e. , students assigned to two different groups), or in students entirely left out of the offspring ( i.e. , students assigned to no group at all). The offspring parti-tion is  X  X leaned up X  by swapping students who appear in no group with students who appeared in two groups, in an arbi-trary order X  X o effort is made to ensure that the remaining students are put into  X  X ood X  groups.

The crossover operation used by OmadoGenesis [5] is dif-ferent, though similar in intent. In a crossover between par-ents A and B , the offspring starts with every group from parent A for which all members are better off than in par-ent B ( i.e. , their group has higher fitness). Then groups from parent B with no students already in the offspring are copied over and remaining students are put into arbitrary groups. The process is repeated with the roles of the par-ents reversed, so that each crossover operation results in two offspring.
To validate our optimizer, we compared against optimal partitions where possible, and against a benchmark other-wise. We have run a series of experiments on anonymized data from a real course of 280 students using group sizes of 4 and 7 and the following criteria: apportioned GPA ( X  X pa X ), no woman alone in a group ( X  X ender X ), homoge-neous on students X  colleges ( X  X ollege X ), heterogeneous with respect to where students live ( X  X egion X ), at least one group member having taken CSC 209 ( X  X sc209 X ), and all five of these criteria combined ( X  X ll X ). The optimizer was run with a population of 100 partitions, for 200 generations, replac-ing the worst 1 / 10 th of the partitions from each generation to the next. (The same values were used when running our optimizer with actual instructors for use in their classes, dis-cussed in the next section.) Because the number of possible partitions grows so explosively with class size, it was not possible to determine the optimal partition on such a large data set. We used two strategies to deal with this. We ran a brute force algorithm that examines every possible partition to find a best one on sub-classes of size 16, 18, and 20. In all experimental conditions, we also ran a partitioner that simply generates 100 random solutions and picks the best of the lot. Thus, in conditions where we could not compare to the optimal solution, we could at least confirm that our optimizer meets this minimal benchmark.

The 78 experimental conditions we tested are described 4 in Table 3.
 Table 3: Experimental conditions (group size 7 was used
On the smaller datasets where we can compare against brute force, our optimizer found an optimal partition in 16 of the 18 experiments. In the other two cases, it found a partition with fitness within 0.66% of the best possible so-lution. Note that seemingly  X  X ub-optimal X  solutions may, in fact, meet some or even all of the criteria set in their re-spective experiments: our piecemeal linear fitness functions, designed to benefit the optimizer, allow a partition that sat-isfies a criterion to nevertheless have a fitness value less than 1 on that criterion. For example, if the criterion is at least one CSC 209 alumnus in each group, we give a group with exactly one such alumnus a fitness of only d (set by the instructor; for our experiments we used a value of 0 . 75).
For the class of size 280, we found that the optimizer made quick improvements to partition fitness, no matter which criteria were used X  X here was generally little or no improve-ment after 20 generations, if not sooner. But for some crite-ria, the optimizer continues to make marginal progress X  X or example, under one condition the optimizer finds a slightly better partition after spending 50 generations making no improvement to the best partition. However, in all cases, al-though the final partitions are not always optimal ( e.g. , the partition for a no-woman-alone criterion contains 1 group out of 70 with a lone woman), they are generally reason-able, which was our goal.

For some criteria, there are much simpler (and faster) techniques for optimizing than a genetic algorithm. For ex-ample, one can apportion GPAs by  X  X olding X  the class list. However, such techniques do not scale up when one wants to allow multiple simultaneous criteria of different types.
On 21 of the 30 experiments run on the optimizer, it found a partition with higher fitness than the best-of-100-random solver. But on 9 of the experiments, the best-of-100-random solver tied the optimizer. Eight of these cases were for small class sizes, where between 1% and 73% of all possible parti-tions had optimal fitness and both algorithms found a best partition X  X he constraints were simply easy to satisfy. The 9 th case involved a class of 280, too large to compute the per-centage of  X  X est X  partitions, but we can confirm by analysis that the constraint was easy to satisfy in this case also: it involved forming groups of size 7 with at least one CSC 209 alumnus in each group, and 106 of the 280 students had taken CSC 209.
We provide a summary of our experimental results for all 78 conditions at http://www.cs.toronto.edu/frog/
How does the optimizer fare when the problem is more difficult? Under many conditions, very few of the possible partitions are  X  X est X  or nearly best. In nearly half of the small-class conditions where we could compute the fraction of best partitions, that fraction was less than .00005%; our optimizer beat the best-of-100-random solver in all of these cases and several others. It also did better in 11 of the 12 experiments on a class of 280, the exception being again the condition where the constraint was easy to satisfy and was satisfied fully by both solvers. In every single case, our optimizer performed at least as well as best-of-100-random.
The most interesting experiments use the full class of 280 and all the criteria simultaneously. When creating 70 groups of size 4, the partition produced by the optimizer using all criteria was better than the corresponding best-of-100-random partition with respect to every one of the 5 criteria. The optimized partition was particularly superior on the no-woman-alone criterion, creating only two solo-woman groups ( vs . 25 for the best-of-100-random partition). Although we had weighted the criteria equally, because of the design of the fitness functions, having a lone student in a no-one-alone bin makes a large negative contribution to the fitness func-tion (which goes from at least 0 . 75 down to 0 if the  X  X arget fitness X  parameter, d , is set to its default value). In compar-ison, with an as-many-bins-as-possible criterion, the penalty for having only 3 of 4 bins covered is only 0 . 25. This demon-strates the importance of allowing instructors to weight (and re-weight) their criteria.

Our partition of the full class into 40 groups of 7 shows an even more dramatic improvement over the corresponding best-of-100-random solution. The optimized partition has no groups with a solo woman, no groups without a CSC 209 alumnus and 85% of the groups have at least 5 colleges rep-resented. In contrast the best-of-100-random partition has 11 women on their own, 1 group without a CSC 209 alum-nus and only 58% of the groups have at least 5 colleges. The optimized partition is also superior with respect to each of the other two criteria.

Besides the quality of the solutions produced, we were also interested in the efficiency of the optimizer. Running times ranged from 120 to 576 milliseconds for small class sizes, and from 5.1 seconds to 7.8 seconds for the class of 280 students. We had set the the optimizer to run for exactly 200 gener-ations, yet found that the most dramatic improvements in the fitness of the best partition were always made within the first 20 X 30 generations. The optimizer could be set to stop early if there were no significant progress; however, this might miss random mutations that would lead to better par-titions. It would be easy to give the user control over the stopping conditions.

A natural instinct is to compare our optimizer to other group formation systems. However, our goal was not to pro-vide better partitions than other systems, and indeed we would expect that systems with limited expressive power may out-perform our optimizer in those situations to which they are tuned. Our primary goal was to be highly expres-sive, while providing reasonable partitions that instructors would actually want to use. Classroom experience suggests that we have achieved this goal, as discussed next.
The optimizer has been used by five different instructors, in seven course sections ranging from second year to grad-uate level. Three instructors planned to (and did) use the groups we proposed. For the other two, we generated groups post hoc in order to be able to compare optimizer results to what the instructor had generated and used. One of these instructors did not maintain records that would allow for comparison and so will be excluded from further mention.
The courses ranged in size from 13 to 64 students, and included classes in our own department and elsewhere. In-structors used a wide array of attributes related to geogra-phy, course completion, grades, program of study, schedule, sex, personality, and interests, among other things. The cri-teria, which also varied greatly, included most of our criteria types.

We learned a great deal from this experience about the expressive power and usability of the model, the quality and value of the partitions generated by our optimizer, and about the features users need.
One of the primary purposes of using the optimizer in real courses was to find out whether the constraints could express what instructors actually wanted, and the degree to which instructors actually wanted the full expressive power of our model. We found two shortcomings, one of which we remedied immediately. We need more experience before deciding whether to accommodate the other.

In one course, the instructor had gathered information about students X  project preferences and wanted groups with at least one type of project of interest to every member. In other words, she needed a criterion for many-of bins that would maximize the chance that a group shares at least one bin in common. This was not part of our model at the time, but was so important to her group-formation process that we defined the  X  X in-in-common X  criterion (see section 4.2) and implemented its fitness function on the spot. Both the model and the code easily accommodated the extension. The real issue with adding new types of attributes or criteria is to ensure that the number of options is not overwhelming.
With this extension, three of four instructors found the model could express everything they wanted, and in fact two said they were pleased that it could do even more than they had expected. One instructor wished for a different notion of apportioned. He wanted the total amount of one attribute (relevant experience) to be even across groups, whereas our apportioned criterion attempts to make the average within each group as even as possible. The two notions are equiva-lent only if groups have the same size, but this instructor had unequal group sizes. Despite this, he was satisfied with the groups that our optimizer proposed. It would be straightfor-ward to offer this alternative definition of apportioned, but again, we are concerned about providing too many options. With more experience, we will learn which criteria are the most commonly required.

There may be other kinds of desired criteria that will not fit our model as easily, but we are encouraged that we have been able to satisfy the diverse needs of these instructors.
One instructor suggested it would be useful to have the option of saying that a constraint is  X  X ncompromisable X , as opposed to simply giving it a very high weight. This en-hancement to our model would require a significant change to our optimization and we would like to have more experi-ence with users before committing to it.
Although instructors could generally express what they wanted, how easy was it to do so? With a large range of attribute and criterion types, there is the risk that instruc-tors will have difficulty differentiating between the options or choosing the ones that will fit their needs.

Our subjects generally found it straightforward to map their desired attributes and criteria onto our model. These were computer scientists, however, and we were available to help. We want the optimizer to be usable in other contexts and without personal support. It will be critical to have a good user interface that provides sensible default values and sample criteria for regular users, and at the same time, advanced options for knowledgeable users who wish to fine-tune the system X  X  parameters.

For instance, one instructor pointed out that she may want to have groups that are mixed by sex. This can be accom-plished by requiring that each group have at least one man and at least one woman. If desired, one could additionally specify that no man should be alone and no woman should be alone. Our model can easily represent either version of this requirement. If this combination proves popular, it could become a named option in the tool, for convenience.
All of the instructors were satisfied with the groups pro-posed by FROG. One made some small modifications and two used them without change. In the cases where we made post hoc groups, our partition was almost identical to the one the instructor had made and used. All of the instructors said that they would use the optimizer again even in the form in which it was at that time, with no survey or constraint editor, and only extremely rudimentary visualization. 5 The ability to form reasonable groups over multiple complex cri-teria outweighed the at-the-time lack of a convenient tool. Execution time was not an issue, ranging from 0.67 to 18.38 seconds.
All of the instructors had formed groups by hand in a previous course. One had a small enough course that he had been able to apply all of his desired criteria by hand. The others all experienced significant disadvantages when forming groups without the optimizer. One instructor had pared down to only one attribute and one criterion when doing partitioning himself. Even still, due to the size of his class, he wrote a small script to create the groups. For another instructor, group assignment prior to FROG had taken two people a full day, and they could only consider two of their desired nine criteria. Even with this much effort and such reduced criteria, they were not at all confident they had satisfied their own goals. This instructor was extremely pleased that FROG allowed her to optimize over all nine criteria.

All but the instructor with the very small course used significantly more complex criteria when given the opportu-nity to use FROG. To give a sense of the complexity, one instructor used four criteria, two used seven and one used nine.
One instructor X  X  plan to use FROG again was conditional on the output being compatible with Excel. This is an easy feature to provide.
Instructors wanted an online survey to gather student data, and the ability to visualize and modify the proposed partitions. These are features that we had already planned, and have partially implemented (see section 7).

Two instructors made a change after seeing the distribu-tion of attribute values among their students. One had in-tended to request heterogeneity on whether or not students were in a computer science program, however, the survey re-sults showed that there were not enough non-CS students to cover all the groups. Instead he specified that there should be at most one per group, so that at least this scarce re-source was not concentrated within any groups. The other instructor chose to  X  X ock X  one group, due to the rarity of one attribute value. This shows the value of visualizing survey results before setting the group formation criteria.
Every instructor needed to visualize the proposed parti-tion(s) in order to assess their quality. We created ad hoc visualizations, customized to each situation. All instructors wanted to flag groups that violated criteria they considered particularly important ( e.g. , no woman alone). They had no difficulty identifying their top priorities, even among seven or nine criteria. With such a multi-dimensional space, writ-ing a general partition visualizer is a rich problem to explore.
Instructors wanted to be able to interface loosely with other tools and systems. They were happy that FROG was not built directly into any course-management system but were interested in the ability to upload student data from other sources. They also wanted to save their partitions in a format that they could present to students or plug into other tools.
Instructors also requested some features that we had not already planned. We fully expected this, and it was an im-portant reason for doing the experiment.

Three instructors wanted to use an attribute whose value would be derived from other attributes. This was no issue for the optimizer, of course, but required preprocessing of the input data. Two instructors had data about grades in certain key courses and were surprised to find some students were missing one or more of these grades X  X ecause they had transferred from another university. The instructors both decided they would like to control the distribution of these  X  X ransfer students X . We preprocessed their data to add a one-of bin that would act as a proxy for this; its value was derived from the missing grades. Another instructor wanted a many-of bin for each of three background courses, with labels for  X  X ompleted X ,  X  X ot taken X  and  X  X n progress X . In addition to criteria on these individual values, he wanted to apportion students X  total  X  X elevant experience X , a function of the three values. Because he knew this in advance, he could have collected all of the information from the students. However, this would have required them to enter the derived value as well as the three individual values. To avoid this, we created his derived attribute by pre-processing his data.
There may be other scenarios where instructors want a derived attribute. We are considering incorporating derived attributes into the tool.

Two instructors requested the ability to  X  X ock X  some stu-dents or groups and have the optimizer partition the rest of the class. This is easily accommodated by our optimizer if the fixed parts of the partition are full groups, as was desired by our subjects. To specify that a partial group of students should or should not be together, instructors could use a teammate preference criterion (with preferences set by the instructor instead of the students) and, if desired, make that criterion  X  X ncompromisable X .
 One instructor wished to see the top three partitions. With a good partition visualizer and editor, this could prove quite fruitful, and would be trivial for the optimizer to ge-nerate X  X he main issue, again, would be the user interface.
The optimizer, implemented in Python, is at the heart of a larger, web-based FROG software system that we are building (see Figure 4). The modularity of the system has allowed us to implement components independently, focus-ing first on the most critical pieces. Basic implementations are finished for all components except the survey results vi-sualizer and the partition visualizer and editor.

The first step in using the system is to configure a stu-dent survey that will gather the attributes that the instruc-tor considers relevant to group formation. (It is possible to import data from other sources by converting to our XML format.) When students have completed the survey, the in-structor uses the constraint editor to specify criteria, and relative weights, for group formation. The optimizer uses these constraints and the student survey results to propose a partition.

Two additional components are future work. First, a sur-vey results visualizer will present the results of the student survey. This may lead the instructor to fine-tune the in-tended criteria. Second, a partition visualizer and editor will display the proposed partition along with group fitness values and, if desired, the individual values of group mem-bers on chosen attributes. This will help the instructor to assess the proposed partition and then allow the instructor to make adjustments to the groups by hand and re-visualize.
Our implementation of FROG uses the Pylons web devel-opment framework. The survey editor and constraint edi-tor use JavaScript and the Mako templating language. The survey is administered to the students using Mako and the built-in Pylons formencode API. Intermediate files, includ-ing the survey definition, the criteria specifications and the student attribute data, are in XML format.
We plan to complete our model by defining fitness mea-sures for the teammate preference and ranking attributes. We will continue building the full FROG tool, in particular the survey results visualizer and the partition visualizer and editor. Because our initial goal was to show that accept-able groups could be created in reasonable time, we did not explore the space of parameters for the optimization algo-rithm. We can now turn to this task in order to tune the optimizer.

Many people have postulated that groups of one sort are better in some sense. FROG provides researchers in this area with a metric for comparing groups and a tool for forming groups according to the criteria they wish to investigate.
Instructors have varying goals for group formation. We have defined a mathematical model that gives them great flexibility to specify relevant student attributes and the sort of groups that should be formed. Even with this much ex-pressive power, our experiments show that the FROG op-timizer, which implements our model, can be used to form reasonably optimal groups in reasonable time. Under ac-tual use, we found that the model was sufficiently expressive to capture what instructors wanted yet simple enough that they were not overwhelmed by the number of options. Fur-thermore, the optimizer produced groups that instructors were happy to use. [1] http://www.catme.org/. [2] R. Cavanaugh, M. Ellis, R. Layton, and M. Ardis. [3] C. E. Christodoulopoulos and K. A. Papanikolaou. A [4] V. Drnevich and J. Norris. Assigning civil engineering [5] A. Gogoulou, E. Goulie, G. Boss, E. Liakou, and [6] S. Graf and R. Bekele. Forming heterogeneous groups [7] W. A. Greene. Genetic algorithms for partitioning [8] A. Inaba, T. Tamura, R. Ohkubo, M. Ikeda, and [9] R. Layton, M. Loughry, M. Ohland, and H. Pomeranz. [10] R. A. Layton, M. L. Loughry, M. W. Ohland, and [11] I. Liccardi, A. Ounnas, R. Pau, E. Massey, [12] A. Ounnas, H. Davis, and D. Millard. A framework for [13] A. Ounnas, D. E. Millard, and H. C. Davis. A metrics [14] M. A. Redmond. A computer program to aid [15] D. Strnad and N. Guid. A fuzzy-genetic decision [16] S. L. Tanimoto. The squeaky wheel algorithm: [17] C. M. Tobar and R. L. de Freita. A tool for student [18] D.-Y. Wang, S. S. Lin, and C.-T. Sun. Diana: A [19] M. Wessner and H.-R. Pfister. Group formation in
