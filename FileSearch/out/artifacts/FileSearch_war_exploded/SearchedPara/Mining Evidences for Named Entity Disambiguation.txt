 Named entity disambiguation is the task of disambiguating named entity mentions in natural language text and link them to their corresponding entries in a knowledge base such as Wikipedia. Such disambiguation can help enhance readability and add semantics to plain text. It is also a central step in constructing high-quality information net-work or knowledge graph from unstructured text. Previous research has tackled this problem by making use of vari-ous textual and structural features from a knowledge base. Most of the proposed algorithms assume that a knowledge base can provide enough explicit and useful information to help disambiguate a mention to the right entity. However, the existing knowledge bases are rarely complete (likely will never be), thus leading to poor performance on short queries with not well-known contexts. In such cases, we need to col-lect additional evidences scattered in internal and external corpus to augment the knowledge bases and enhance their disambiguation power. In this work, we propose a genera-tive model and an incremental algorithm to automatically mine useful evidences across documents. With a specific modeling of  X  X ackground topic X  and  X  X nknown entities X , our model is able to harvest useful evidences out of noisy in-formation. Experimental results show that our proposed method outperforms the state-of-the-art approaches signif-icantly: boosting the disambiguation accuracy from 43% (baseline) to 86% on short queries derived from tweets. I.7.0 [ Computing Methodologies ]: Document and Text Processing Entity Disambiguation; Evidence Mining; Generative Model; Knowledge Expansion; Semi-supervised Learning
Access to an organized information network or knowledge graph is critical for many real-world tasks. Most real-world information is unstructured, interconnected, noisy, and of-ten expressed in the form of text. This inspires constructing an organized, semi-structured information network from the large volume of noisy text data. Such formal and structural representation of information has the advantage of being easy to manage and reason with, which can greatly facilitate many Artificial Intelligence applications, such as Semantic Search, Reasoning and Question Answering. To achieve this goal, knowledge bases such as DBpedia [1], Freebase [4] were manually constructed. However, due to the laborious, time consuming, and costly extracting and labeling process, these knowledge bases are often restricted by a very limited cov-erage. Recently, automatically constructed knowledge net-works including YAGO [27], NELL [6], Reverb [12], have emerged. But unfortunately, they suffer from the problems of low coverage [6, 27], or poor quality [12]. How to auto-matically construct a high-quality information network from a large amount of unstructured and noisy text data remains an open research problem.

An important component in constructing information net-works is named entity disambiguation (NED) .Giventhe named entity mentions in unstructured text data, the goal of NED is to map them to their corresponding real world entities in a knowledge base such as Wikipedia. Different from entity resolution (ER) , whose goal is to cluster entity mentions into several disjoint groups with each group rep-resenting a unique entity, NED requires explicitly identify-ing which underlying entity a given named entity mention should refer to. The NED task is challenging due to the fact that many named entity mentions are ambiguous: the same mention can refer to various different real world enti-ties when they appear in different contexts. For example,  X  X ichael Jordan X  can refer to the basketball star in NBA, the Machine Learning researcher in Berkeley or some other people. NED plays a critical role in high-quality informa-tion network construction. When new information extracted from text data is ready to be inserted into the network, it is necessary to know which real world entity this piece of information should be associated with. If the system makes a wrong decision here, the network will not only lose some information, but also introduce errors. For example, as shown in Figure 1, if the extracted information  X  X lected as AAAI fellow X  is wrongly associated with the basketball player Michael Jordan , the network will lose the informa-tion that Michael Jordan (Machine Learning) is an AAAI fellow, as well as wrongly including Michael Jordan (Bas-ketball Player) as a fellow of AAAI. Figure 1: Named Entity Disambiguation Example
In recent years, the NED task has received a lot of re-search interests. Many methods [10, 11, 17, 18, 19, 21, 24, 25, 26, 28] have been proposed to disambiguate named en-tity mentions in free text with respect to Wikipedia. Gener-ally speaking, three kinds of features are explored by those methods. The first one is a statistical feature called entity popularity . It is based on the assumption that the most prominent entity for a given entity mention is the most probable underlying entity for that mention. Usually the  X  X ost prominent X  entity is defined as the entity which uses the mention most frequently as a hyperlink anchor text in Wikipedia. Previous study [24] has shown that this sim-ple heuristic is a very reliable indicator of the correct dis-ambiguation. But obviously, methods merely depending on this feature are not robust as they will disambiguate all ap-pearances of an entity mention to a fixed entity, regardless of the contexts along with them.

The second feature is a textual feature called context sim-ilarity . It takes the entity mention X  X  context into considera-tion and defines similarity measures between the text around the entity mention and the document describing the refer-ent entity in Wikipedia. This feature complements the entity popularity prior and is widely used in almost every method. One problem with context similarity is that it requires exact word overlap between the two compared texts, which may become an over-strict constraint due to natural language X  X  usage flexibility. To handle this problem, the third feature  X  X opical coherence X  is proposed. This feature is a structural feature making use of Wikipedia X  X  cross-page links to de-fine two entities X  topical coherence. The intuition for using this feature is that the mention X  X  referent entity should be topical coherent with other entities within the same context. Previous study [24] has proved the effectiveness of using this feature. Recently, several methods [18, 19, 24, 26, 28] also tried to combine together all these three features using a hybrid strategy which can further improve the accuracy.
Almost all of the previously proposed algorithms assume that the knowledge base can provide enough explicit and useful information to help disambiguate a mention to the right entity. However, in many situations, the information contained in the knowledge base is insufficient, thus lead-ing to a connection gap between the keywords in a query (a named entity mention along with its context) and the knowl-edge base. Note that such situations are not rare since the reference knowledge base (e.g. Wikipedia) has a limited cov-erage and therefore cannot capture every aspect of a referent entity. In these cases, the existing state-of-the-art methods will fail to make correct disambiguation decisions because there is not enough information for them to utilize. The following two examples show the cases where key evidences (  X  X ric Xing X  and  X  X aper X  , respectively) are not available ( Ex-ample1 ) in the knowledge base or overwhelmed ( Example2 ) by other evidences (  X  X on, best, award X  ).

Example 1. Eric Xing worked with Michael Jordan from 1999 to 2004.
 Example 2. Michael Jordan won the best paper award.
To solve the above problem, we need to collect additional evidences scattered in internal and external corpus to aug-ment the knowledge base and enhance its disambiguation power. Mining additional evidences is an effective method for improving NED performance because it helps address at least two types of failures in existing approaches: 1. No evidence failure , i.e. the knowledge base does not 2. Insufficient evidence failure , i.e. the important dis-
In this paper, we aim at developing a method to auto-matically mine helpful evidences from internal and exter-nal corpus to boost the NED performance. Mining external evidences is much harder than mining internal ones, since internal documents in a knowledge base are well labeled and linked. Mentions in external documents are not dis-ambiguated; yet it is still possible to extract new evidences from them, through our model. Our method can incremen-tally enrich the useful evidence set, making use of informa-tion both inside and outside the reference knowledge base. With a specific modeling of  X  X ackground topic X  and  X  X n-known entities X , our method can harvest helpful evidences out of noisy information.

Our main contribution is the development of an innovative generative model and a novel incremental algorithm for min-ing additional evidences to help boost the NED performance. To the best of our knowledge, our study is the first work on mining evidences for named entity disambiguation, a critical problem in constructing high-quality information network. Experimental results show that our proposed method can mine additional evidences to significantly improve knowl-edge base X  X  disambiguation ability. Our work is also useful to the work on developing new NED algorithms and the mined evidences can be beneficial to any such algorithms.
We formalize the Named Entity Disambiguation (NED) problem and our Mining Evidences for Named Entity Dis-ambiguation (MENED) task as follows.

Definition 1 (Named Entity Disambiguation). Na-med Entity Disambiguation (NED) is the process of associ-ating an entity name mentioned in a text to an entry, rep-resenting that entity, in a knowledge base (e.g. Wikipedia). Given a textual named entity mention m along with the un-structured text t in which it appears, and a reference knowl-edge base K , the goal is to produce a mapping from the men-tion m to its referent real world entity e in K . Definition 2 (Mining Evidences for NED). Mining Evidences for Named Entity Disambiguation (MENED) is the task of finding additional evidences inside and outside the knowledge base to improve NED accuracy. Given a tex-tual named entity mention m , a reference knowledge base K , and a document corpus C outside K , the task is to mine additional evidences from K and C which can further help the disambiguation of m with respect to K .
 The MENED task is independent of the query context. For each named entity mention m , MENED is performed only once, regardless of different query contexts for the same mention. In practice the set of solvable ambiguous mentions can be pre-calculated from the knowledge base K (e.g. the whole set of entities indexed by K ). Therefore the MENED process shall run offline as a preprocessing step .After MENED, any NED algorithm can make use of the evidences harvested by MENED to disambiguate m .Inthiswork,a component in our MENED model can be reused to perform NED directly (Section 3.4.3).
In this section we formally introduce our proposed model and algorithm, for mining new evidences to help named en-tity disambiguation. We will first describe the intuitions be-hind our model, then provide details about how the model is constructed and how the incremental algorithm works, and finally we will discuss how to perform inference on our model to estimate the document-label(entity) association and the label(entity)-word association.
We first describe the intuitions behind our model, before detailing the model in the next section. The goal of named entity disambiguation is to find a named entity mention X  X  referent entity by utilizing the context along with the men-tion. The reason why context can help disambiguation is that each referent entity candidate can be distinguished by a set of representative words. Those representative words can be seen as the disambiguation evidences for those entity candidates. Therefore it is natural to model each entity as a topic/label 1 and imagine those representative words are generated from such topics. Since we are only interested in those representative words which are highly related to the underlying entities, we model each entity mention X  X  limited size context as a document. Each document can be asso-ciated with only one topic/label corresponding to its entity mention X  X  real referent entity. Figure 2: Entity, Word (Evidence), and Document
Though we have adopted the limited size context con-straint to ensure topic centrality, some words within the context may still be general to some or all topics/labels. To specifically model this phenomenon, we introduce a spe-cial background topic to capture those non-representative words. On the other hand, we also notice that sometimes we may encounter documents whose underlying entities are not within the referent entity candidates. This is due to the fact that currently there is no perfect solution to generate com-plete referent entity candidates for a given entity mention. Obviously it is not appropriate to assign any topic/label to these documents. Therefore we introduce another special topic called  X  X efault X  to capture words from the documents with unknown or unsure underlying entities. With all these intuitions, we are able to properly model the document-label association and the label-word association. Figure 2 shows the association among entities, words, and documents. Evidences are reflected by the words and their association strengths with entities, after discounting  X  X ackground X  and  X  X efault X  topics. In the next section we will introduce our proposed generative model based on these intuitions.
We now explain the details of our generative model. Fig-ure 3 shows the graphical structure of dependencies of our model. Each node in the figure corresponds to a random variable or prior parameter. The shaded nodes represent observed variables while other nodes represent latent vari-ables. A plate means the nodes within it are replicated for multiple times. A directed edge from node a to node b indi-cates that the variable represented by b is dependent on the the variable represented by a .
 Table 3.2 summarizes the notations used in our model. Givenanamedentitymention m , we will first generate all of its possible referent entity candidates. Following previous work [24, 26] on NED, we make use of the structural infor-mation of Wikipedia to find all the entities that m can be mapped to. Each referent entity candidate will be treated as a regular topic/label and the total number of them is K . We denote the set of regular topics/labels as S .Foreach occurrence of m , we model its limited size context (e.g. a
In this paper we use  X  X ntity X ,  X  X opic X  and  X  X abel X  inter-changeably for describing our model. Symbols Descriptions
D the set of documents (e.g. named entity men-K the number of referent entity candidates S the set of regular entity labels
N d the number of words in document d w di the i -th word of document d z di the label associated with the i -th word of docu-y d the label associated with document d t di the background indicator for the i -th word of  X  d the background topic proportion for document d  X  the topic/label distribution  X  bg the word distribution for the background  X  df the word distribution for the default topic/label  X  k the word distribution for the k -th regular  X  df , X  the hyperparameters for Dirichlet prior of  X   X  bg the hyperparameter for Dirichlet prior of  X  bg  X  df the hyperparameter for Dirichlet prior of  X  df  X  the hyperparameters for Dirichlet prior of  X  k  X  the hyperparameters for Beta prior of  X  width-W word window surrounding m )asadocument.For a labeled document (e.g. the document in which the genuine underlying entity for m is already identified), its document label y is fixed and within S . For an unlabeled document (e.g. the document in which the genuine underlying entity for m is not clear yet), its document label y is drawn from S  X   X  default  X , according to a multinomial distribution  X  , which itself is drawn from a Dirichlet prior with  X  and  X  as the hyperparameters. As mentioned in the last section,  X  X efault X  means the label for this document is unknown or unsure (in other words, not within S ). The difference be-tween  X  and  X  df should reflect how conservatively we choose between regular topics/labels and the special  X  X efault X  one.
Initially, all the documents inside the reference knowl-edge base (e.g. Wikipedia) are labeled documents, while all the documents in the external corpus are unlabeled doc-uments. For each word w in both labeled and unlabeled documents, its label z is either the same as the label of the document in which it appears, or the special  X  X ack-ground X  label. The selection is controlled by an indicator variable t drawn from a Bernoulli distribution  X  ,whichit-self is drawn from a Beta prior with  X  1 and  X  2 as the hy-perparameters. The difference between  X  1 and  X  2 should reflect the proportion of background topic. For each label in S  X   X  default  X   X   X  background  X , it is associated with a multi-nomial distribution  X  over words, which is drawn from the Dirichlet prior with  X  ,  X  bg and  X  df as the hyperparameters. The difference among  X  ,  X  bg and  X  df should reflect the con-tent difference among regular labels, the  X  X efault X  label and the  X  X ackground X  label. Finally, each word w is drawn from the multinomial distribution  X  z ,where z is the word label for w . Our goal is to infer the document-label association y and the label-word association  X  from this model. The document-label association helps reveal the entity labels for unlabeled documents, and the label-word association helps demonstrate the disambiguation evidences for each referent entity candidate.

To summarize, the detailed generative process of our model is as follows: 1. Draw the multinomial distribution over words  X  k  X  2. Draw the multinomial distribution over words  X  bg  X  3. Draw the multinomial distribution over words  X  df  X  4. Draw a topic/label distribution  X   X  Dirichlet (  X  ), where 5. For each document d  X  D :
Note that the above generative process is for unlabeled documents. For labeled documents, the document label y d is known and fixed. Thus 5(a) becomes unnecessary and should be skipped. The other steps will remain the same.
Compared with regular topic models like LDA [3], our model is different in three aspects: 1. In our model, the regular topics, the  X  X efault X  topic 2. In our model, each document has only one topic/label 3. In our model, a word can only have two possible labels:
Now we will explain our incremental evidence mining al-gorithm based on the model introduced in the last section. As discussed in Sections 3.1 and 3.2, our model is able to in-fer both the document-label association and the label-word association (the inference details will be discussed in next section). So after a run of our model, each unlabeled docu-ment will be assigned a label with the maximum likelihood (Section 3.4.3), and the words associated with each label will change accordingly (Section 3.4.4). Each run of the model will bring in some new knowledge (e.g. more labeled docu-ments and more comprehensive label-word correspondences) and those new knowledge can further help the model to find more additional evidences. So this is a typical incremental mining scenario. We thus introduce an incremental evidence mining algorithm, as described in Algorithm 1. Algorithm 1 will first do inference only for the labeled documents of named entity mention m in reference knowledge base K (e.g. documents which contain mention m and a hyper-link to m  X  X  real referent entity). Then in each iteration, the algorithm will collect additional documents ( D add )froman external corpus C which have overlapped words with the current labeled documents ( D i  X  1 ). Inference will then be performed on current documents ( D ) and newly added doc-uments ( D add ) together, with a constraint that the labels of the current labeled documents ( D i  X  1 ) will remain un-changed. After inference, documents whose labels are found in the knowledge base will be added into the new labeled document set ( D i ). The incremental process will continue until the iteration limit ( MaxIter ) is reached. Algorithm 1 Incremental Evidence Mining
Input: Reference knowledge base K , external corpus C , named entity mention m , integer MaxIter .
 D 0  X  the set of labeled documents for mention m in K S  X  the set of entity candidates X  labels for mention m D  X  D 0
Do inference for D = D 0 for all i from 1 to MaxIter do end for
The joint likelihood is p ( w w w,t t t,y y y,z z z |  X   X   X , X   X   X , X   X   X  )(1) = We use  X  = {  X   X   X , X   X   X , X   X   X  } to denote the hyperparameters. We an d use the maximal marginal probability to infer each topic assignment y d and z di . In typical topic models with con-jugate prior such as LDA, one can apply collapsed Gibbs sampling to iteratively sample the variables z di ,t di ,y by one, and estimate marginal probabilities with the sam-ples. However, in our model, it is difficult to apply that sampling method due to the fact that every document has only one label y d .Infact,when y d and t di are determined, z di is uniquely decided as either bg or y d . Therefore, if we sample y d and z di alternatively, once y d is assigned some value fg ,allthe z di  X  X  associated with the corresponding document can only take values from { fg,bg } , and hence y d will never be assigned any other value than fg because p ( y d = l | z di  X  X  fg,bg } ) = 0 for any l = fg .Inotherwords, the Gibbs sampler will be trapped in a particular region y = fg and never able to jump out of it.

To overcome that issue, we propose a blocked and col-lapsed Gibbs sampling algorithm with variational approxi-mation.
A blocked Gibbs sampler groups two or more variables to-gether and samples from their joint distribution conditioned on all other variables, rather than sampling from each one in-dividually. In our model, we group the variables z d  X  ,t y for the same document together because of the aforemen-tioned  X  X eterministic trap X  issue. In each blocked sampling stage, we sample the variables z d  X  ,t d  X  ,y d for one document d with all the other variables fixed as given.
 Algorithm 2 Blocked Gibbs Sampling 3: sample { z d  X  ,t d  X  ,y d } together according to
Now we explain how we sample z d  X  ,t d  X  ,y d . First, we notice that z di is determined by t di and y d ,soweonlyneedto sample t d  X  and y d according to p ( t d  X  ,y d | w w w,z z z Second, based on chain rule of joint probability we have: p ( t d  X  ,y d | w w w,z z z  X  d ,t t t  X  d ,y y y  X  d ,  X ) = p ( y So for a particular document d , we can first sample y d and then sample t di for each position i in d .

However, it is hard to compute p ( y d | w w w,z z z  X  d ,t t t exactly because the parameters  X   X   X  and  X   X   X  are hard to be inte-grated out when marginalizing p ( w d  X  ,t d  X  | w w w  X  d To sample y d with the advantage of collapsed sampler, we make a variational approximation p ( t d  X  | w w w,z z z  X  d ,t t t  X  d ,y y y,  X ) = w di . In other words, we temporarily assume the labels t d one document are conditionally independent given y d and all variables in other documents. This approximation is reasonable because our documents are short but the num-ber of documents is large. The conditional probability of p ( t but the calculation of p ( y d | w w w,z z z  X  d ,t t t  X  easy to accomplish 2 . p ( y d = l | w w w,z z z  X  d ,t t t  X  d ,y y y  X  d ,  X )  X   X  1 + |{ t d j =0 ,d = d }| +  X  2 + |{ t d j =1 ,d = d }| After we sample y d ,wecansample t di for each position i in d .If y d = default , Otherwise replace the  X  df in the formula with  X  . Finally, we have the following sampling steps for sampling one block. Algorithm 3 The subroutine for sampling One Block (Line 3 in Algorithm 2)
Sample y d accordingtoEq.(2) for all i from 1 to N d do end for
We infer the document label using maximal marginal prob-ability with one exception: if the maximal marginal proba-bility is smaller than a threshold  X  , we predict the label to be default . With this threshold we can control the noise by only labeling the documents on which our model has suffi-ciently high confidence. y
Since our model can infer the document label for unla-beled document, it can also be directly used for named en-tity disambiguation if we treat the query as an unlabeled document.
This approximation can be avoided if we use a variable elimination trick. However, with this approximation the sampling is more efficient.

We infer the label of each word in each document with maximal marginal probability: And the label-word distribution can be estimated by maxi-mum a posteriori (MAP) inference:
In this section, we evaluate the effectiveness of our pro-posed method for the MENED task on two real-life datasets: one from news, and the other from Twitter. We will: (1) compare the disambiguation accuracy and robustness of our method, to two state-of-the-art NED methods that utilize various kinds of features; (2) analyze the effectiveness of the additional evidences mined by our method; (3) show how the performance of our method changes with respect to the number of the incremental evidence mining iterations. All the experiments, if not specifically mentioned, are conducted on a server with 2.40GHz Intel Xeon CPU and 48GB RAM.
Since our work is the first one to tackle the MENED prob-lem, there is no established publicly available benchmark for us to test. As mentioned before, the goal of MENED is to bridge the potential information gap between a query and the reference knowledge base. Here we use two real-world datasets where such information gap indeed exists, to test the performance of our algorithm.

The first one is derived from the TAC-KBP2009 dataset, which is created for the Entity Linking task [20] in the Knowledge Base Population track at the Text Analysis Con-ference. The TAC-KBP2009 dataset consists of 3,904 queries (again, each query is an entity mention along with its con-text) and entity mentions in 1,675 of them can be linked to their corresponding entries in the knowledge base (Wikipedia). The fact that more than half of the entity mentions can-not find their underlying entities in the knowledge base also proves that the reference knowledge base is usually a limited information source and therefore it may lack some important information to help disambiguate named entity mentions. The original dataset contains many long news articles. In or-der to test the abilities of different algorithms in a challeng-ing scenario where information gap is large, we modify this dataset to keep only a fixed-size word window surrounding the query mention as its  X  X ontext X  (in this work, we choose the word window size as 60). By adding this constraint the information gap is enlarged and the disambiguation diffi-culty is increased. Among the 1,675 resolvable queries, we choose the queries whose named entity mentions have a cor-responding Disambiguation Page in Wikipedia as our first test dataset. This dataset contains 424 queries.
Our second dataset is generated from Twitter. Since tweets have the 140-character constraint and the words used in them are often irregular, the probability of seeing informa-tion gap between tweets and the reference knowledge base is relatively high. Therefore running NED on tweets is much harder than on news. We randomly picked 25 ambiguous entities from the Wikipedia X  X  Disambiguation Page Cate-gory and crawled 500 tweets containing these mentions as queries. After filtering out the queries which are unsolvable (e.g. even human beings cannot specify which entity the mention refers to), 340 queries are left and we treat them as our second test dataset.
In this work, we use Wikipedia as the reference knowledge base and the webpages indexed by Google as the external corpus. For each reference entity candidate, we generate its labeled data ( D 0 in Section 3.3) by utilizing its Wikipedia page and all Wikipedia pages which have hyperlinks to its Wikipedia page. For fetching related documents ( D add in Section 3.3) from the external corpus, we make use of the Google Search API and collect the top 20 webpages for each referent entity candidate.
We first conduct experiments to compare our method with two NED methods utilizing various kinds of features: Wik-ifier [24], a state-of-the-art NED system using a machine learning based hybrid strategy to combine popularity prior, context similarity and topical coherence features together, and AIDA [19], a robust NED system making use of weighted mention-entity graph to find the best joint mention-entity mapping. As explained in Section 3.4.3, our model for MENED can be directly used for NED if we treat the query as an unla-beled document. We test our model under two settings: (1) using the evidences mined from Wikipedia only; (2) using the evidences mined from both Wikipedia and the external corpus. We denote the first setting as MENED(Wiki) and the second setting as MENED(All) .ForbothMENED(Wiki) and MENED(All), we use the following parameter settings:  X  =0 . 001 , X  df =0 . 01 , X  =0 . 001 , X  df =0 . 01 , X  bg =0 . 1 , X  0 . 0003 , X  2 =0 . 001 . These parameters are tuned on a small test dataset containing 15 queries and then reused in all the experiments without any further tuning. The thresh-old for predicting document label is chosen as  X  =0 . 9. For MENED (All), we incrementally mine evidences from exter-nal corpus for 5 rounds. For Wikifier and AIDA, we use the parameter settings suggested by their authors. The same parameter settings were applied to both datasets. Wiki-fier used a Wikipedia repository of 2009 3 . Originally the Wikipedia repository used by AIDA is of 2010, later the au-thors kindly provided us an updated version which used a Wikipedia repository of late 2012. We denote the original one and the updated one as AIDA(2010) and AIDA(2012), respectively. Both MENED(Wiki) and MENED(All) rely on a Wikipedia repository of late 2012.
 Figure 4 shows that MENED(All) slightly outperforms Wikifier and AIDA on TAC-KBP2009 dataset. Compared with Wikifier and AIDA, MENED(All) does not utilize any complicated features (e.g. topical coherence). On the Twit-ter dataset, MENED(All) performs remarkably better than Wikifier and AIDA. Both Wikifier and AIDA get very poor NED accuracy on short and noisy texts like tweets. MENED(All) retains high accuracy on tweets, indicating a much more robust performance. We notice that MENED(Wiki) also
The authors of Wikifier are working on a updated version utilizing recent Wikipedia repository, but unfortunately it cannot be ready at this time. We were also unable to obtain a Wikipedia repository of 2009 to run our method with. greatly outperforms Wikifier and AIDA on the Twitter dataset. This is due to two reasons. First, MENED(Wiki) mines new evidences from the Wikipedia pages that hyperlink to the en-tity candidates X  own Wikipedia pages. Second, the topical coherence feature utilized by both Wikifier and AIDA is less helpful on very short texts like tweets since there are very few entities within the short context.

We also tried to compare our method with TAGME [13], an NED system specifically designed for very short texts like tweets. Different from Wikifier and AIDA, the TAGME API does not allow us to specify the named entity men-tions to disambiguate. As a result, some queries in our test datasets are not properly responded. Without considering the queries which cannot be handled, TAGME obtains the NED accuracy of 78.3% and 61.1% on TAC-KBP2009 data and Twitter data respectively. Our method MENED(All) outperforms TAGME on both datasets.
We then conduct experiments to demonstrate the effec-tiveness of mining evidences from external corpus. As can be seen from Figure 4, MENED(All) outperforms MENED(Wiki) in terms of NED accuracy on both datasets. The accuracy gain illustrates that our method for MENED is effective and the mined evidences from external corpus are indeed very helpful for boosting the NED performance.

Table 2 shows the mined evidences from external corpus for several entities. We can see that the mined evidences can provide complementary knowledge for disambiguating the entities, especially for those entities that are not very popular and therefore do not have many context informa-tion in Wikipedia. For example, in  X  X ichael I. Jordan X  case, the evidences  X  X ayers, nonparametric, nonlinear X  correspond to his research work,  X  X ehong, chen, distinguished X  indicate the fact that he is a Pehong Chen Distinguished Professor at UC Berkeley, and X  X avid, heckerman, kearns, marina, meila X  describe his collaborators. All these evidences are not cap-tured in Wikipedia but available in external sources (e.g. his homepage and DBLP page). Our model and algorithm can successfully dig out these useful evidences scattered across multiple documents in the external corpus.
Next we conduct experiments to illustrate how the perfor-mance of our MENED method changes with respect to the number of incremental evidence mining iterations. Here the parameter settings are the same as those described in Sec-tion 4.3. Figure 5 shows that the NED accuracy increases as Entity Mined Additional Evidences
Michael I. Jordan (Michael Jordan)
Michael B. Jordan (Michael Jordan)
Owen Bieber (Bieber) jobs, automobile, corporation,
General Aircraft Hot-spur (Hotspur)
David Young Cameron (David Cameron) Table 2: Mined Evidences for Michael I. Jordan, Michael B. Jordan, Owen Bieber, General Aircraft Hotspur and David Young Cameron. Words in parentheses are named entity mentions. the number of iterations increases. But the increasing speed slows down as more evidences are collected.
Named entity disambiguation has received a lot of atten-tions in recent years. Approaches that disambiguate named entity mentions with respect to Wikipedia date back to Bunescu and Pasca X  X  work [5]. They defined a similarity measure to compute the cosine similarity between the text around the entity mention and the referent entity candidate X  X  Wikipedia page. The referent entity with the maximum context simi-larity score is selected as the disambiguation result. Several subsequent work incorporated more information into simi-larity comparison: Gottipati and Jiang [14] explored query expansion, while Zhang and Sim [28] considered acronym expansion. To incorporate different types of disambiguation knowledge together, Han and Sun [16] proposed a generative model to include evidences from entity popularity, mention-entity association and context similarity in a holistic way. And to overcome the deficiency of the bag of words model, Sen [25] adopted a latent topic model to learn the context-entity association to help disambiguation. Cucerzan X  X  work [10] is the first one to realize the effectiveness of using topi-cal coherence to help named entity disambiguation. In that work, the topical coherence between the referent entity can-didate and other entities within the same context is cal-culated based on their overlaps in categories and incoming links in Wikipedia. Milne and Witten [22] refined Cucerzan X  X  work by defining topical coherence using Normalized Google Distance [9] and only using  X  X nambiguous entities X  in the context to calculate topical coherence. Several new mea-sures of topical coherence were also proposed in recent years: Bhattacharya and Gatoor [2] modeled the topical coherence as the association of an entity and the latent topics of a document, and Sen [25] modeled the topical coherence us-ing the co-occurrence of entities. Recently, several methods [18, 19, 24, 26, 28] also tried to combine together  X  X ontext similarity X  and  X  X opical coherence X  using a hybrid strategy which could further improve disambiguation accuracy.
Almost all these previous NED algorithms fall into the scope of  X  X ingle document NED X . Their disambiguation de-cisions depend on the comparison (both textual and topical) of the query document (named entity mention along with its context) and the referent entity candidates X  Wikipedia pages. Therefore they cannot handle the cases where there are no enough overlaps between the compared documents. To solve this problem, Chen and Ji [8] proposed the  X  X ol-laborative Ranking X  technique. In their work, they used document clustering to find several  X  X uery collaborators X  (documents which are in the same cluster with the query document) and ran existing NED algorithms on each  X  X uery collaborator X  separately. Their disambiguation results were then assembled together to make the final decision. If most of a query X  X  collaborators exhibit enough overlap with ref-erent entity candidates X  Wikipedia pages, the final disam-biguation decision will likely be reasonable. Han and Sun [17] tackled this problem in another way. They proposed a generative entity-topic model which can jointly model con-text compatibility, topical coherence and their correlations. Since their model was trained on all Wikipedia pages, it made use of not only the contents of referent entity candi-dates X  Wikipedia pages, but also the contents of the Wikipedia pages where those referent entity candidates appear. Com-pared with X  X ingle document NED X  X lgorithms, their method utilized cross-document information. However, both [8] and [17] cannot work well in cases where a query X  X  context infor-mation does not exist in the entire Wikipedia corpus. Our work focuses on such cases and explores information both inside and outside Wikipedia to mine additional evidences for named entity disambiguation.
 Our proposed model is inherited from the Latent Dirichlet Allocation (LDA) model. LDA was first proposed by Blei, Ng and Jordan [3] for finding the document-topic association and the topic-word association in text documents. Ramage, Hall, Nallapati and Manning [23] extended LDA to Labeled-LDA so that each document can have multiple labels and the label-word correspondences can be inferred. Different from both LDA and Labeled-LDA, our model is particu-larly designed for entity disambiguation evidence mining, and our semi-supervised learning model works in an incre-mental manner to control errors. Most LDA-based models require a preprocessing step to remove the stopwords. Oth-erwise, those stopwords will pervade the learnt topics, hiding the real statistically interesting word patterns. However, re-moving stopwords is not trivial as a lot of stopwords are domain-dependent. To cope with this issue, several work [7, 15] introduced a special background topic and assumed all stopwords being generated by this background distribu-tion. Our model incorporates not only such a background distribution to capture stopwords, but also a  X  X efault X  dis-tribution to capture words from documents with unknown or unsure labels. Therefore our model is robust and effective in mining evidences for named entity disambiguation.
In this paper, we studied the problem of mining evidences for named entity disambiguation. We proposed a genera-tive model and an incremental algorithm to automatically mine useful evidences across documents. With a specific modeling of  X  X ackground topic X  and  X  X nknown entities X , our model is able to harvest useful evidences from noisy text. To evaluate the effectiveness of our model and algorithm, a thorough experimental study was conducted. The experi-mental results demonstrated that our proposed method can mine additional evidences to significantly boost the disam-biguation performance. As future work, we plan to extend our approach to mine other type of evidences such as entities and concepts. We would also like to combine the evidences mined by our method with other NED algorithms.
This research was sponsored in part by the Army Re-search Laboratory under cooperative agreements W911NF-09-2-0053, NSF IIS-0954125, an d DARPA under agreement number FA8750-13-2-0008. Chi Wang was supported by Mi-crosoft Research PhD Fellowship. The views and conclu-sions contained herein are those of the authors and should not be interpreted as representing the official policies, ei-ther expressed or implied, of the Army Research Laboratory, DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Govern-ment purposes notwithstanding any copyright notice herein.
