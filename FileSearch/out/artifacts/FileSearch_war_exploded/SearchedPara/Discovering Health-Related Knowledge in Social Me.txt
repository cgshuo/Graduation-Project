 Social media is emerging as a powerful source of communication, information dissemination and mining. Being colloquial and ubiq-uitous in nature makes it easier for users to express their opinions and preferences in a seamless, dynamic manner. Epidemic surveil-lance systems that utilize social media to detect the emergence of diseases have been proposed in the literature. These systems mostly employ traditional document classification techniques that represent a document with a bag of N-grams. However, such tech-niques are not optimal for social media where sparsity and noise are norms. The authors address the limitations posed by the traditional N-gram based methods and propose to use features that represent different semantic aspects of the data in combination with ensemble machine learning techniques to identify health-related messages in a heterogenous pool of social media data. Furthermore, the results reveal significant improvement in identifying health related social media content which can be critical in the emergence of a novel, unknown disease epidemic.
 I.2.6 [ ARTIFICIAL INTELLIGENCE ]: Learning X  Knowledge Acquisition Social Media, Machine Learning, Classification
Social media such as Twitter R  X  and Facebook R  X  are increasingly being used as tools for real-time knowledge discovery relating to social events, emerging threats, epidemics, and even product trends [27]. For example, real time analysis of Twitter users X  tweet content can be or is being used to detect earthquakes and provide warnings [21], to identify needs (e.g., medical emergencies, food and water shortages) during recovery from natural disasters such as the Haiti Earthquake [6], track emergence of specific syndromic characteris-tics of influenza-like illness [8], and collect epidemic-related tweets [16]. In all such applications, systems are needed to automatically, accurately, and efficiently identify and interpret health-related con-tent in short text  X  X icro X  messages.

We propose and test the efficacy of ensemble methods wherein multiple base classifiers are trained with heterogeneous features and used in combination to enhance performance of health-related message classification. A message is said to be health-related if at least one of these two following conditions is met:
Traditional techniques such as N-gram feature extraction limit the ability to recognize high-discriminative terms that include health-related keywords and/or obtain meaning from the topical semantics of the entire text. Hence, our main contribution is to propose the use of other semantic-enhanced features, beyond traditional binary N-grams, that address the challenge of identifying emerging health related terms that are non-standard in nature.

Specifically, this paper has the following key contributions: 1. We propose to use 5 different feature types which extract dif-2. We explore the use of different ensemble methods that allow 3. We validate our proposed methods using empirical evalua-
The literature on text classification is extensive, hence we only discuss works closely related to ours. The major differences be-tween a short message or  X  X icrotext X , and a traditional document includes the length and the formality of language. Classification algorithms that work for traditional documents may not succeed in the microtext domain due to the lower dimension and higher noise characterizing the data.

Sriram et al. [23] pointed out the limitation of bag-of-word strategies for tweet classification, and proposed 8F features, which primarily capture the information about authors and reply-to users. While authorship proves to be a potential source of information, our dataset (see Section 3) does not have such information avail-able. Caragea et al. [6] proposed the system EMERSE for clas-sifying and aggregating tweets and text messages about the Haiti earthquake disaster. They trained an SVM classifier with the com-bination of 4 feature sets: uni-grams, uni-grams with Relief fea-ture selection [15], abstractions [22], and topic words generated by LDA [3]. Since the first 2 feature sets are N-gram based, they en-counter similar limitations as our baseline. The other two feature sets are based on groups of terms, and would partially solve the disambiguation problem, but not the keyword recognition problem.
Paul and Dredze [19] proposed a machine learning based classi-fication algorithm used for identifying health-related tweets. Uni-gram, bi-gram, and tri-gram binary word features were used to train a linear kernel SVM classifier. They further used the collected tweets to mine public health information using a LDA-like tech-nique [20]. The parameters of the classifier are then tuned to obtain 90.04% precision and 32.0% recall, since classifiers with higher precision are preferred in their task which is to collect high qual-ity health-related tweets. Besides using traditional binary N-gram features to train the classifier, which we point out later not to be sufficient and accurate enough for social media settings, their clas-sification model was build and tuned on a small dataset of roughly 5 thousands Twitter messages.
For consistency and scientific comparison, we use the same dataset as [19] which consists of 5,128 manually labeled tweets. Since we want to minimize the assumption about the properties of social text, all hashtags, retweets and user information are removed and only textual content is kept. Future steps of our research involve expand-ing the data sources to include other kinds of social media (such as Facebook, Google+, blogs, etc.), which may not have hashtags and other Twitter-like features, thus we focus on common features (such as textual information and timestamps) to develop a generic algorithm. Each tweet is a tuple of tweet ID and its textual content, and is labelled with either Positive or Negative . A message is pos-itive if it is health-related , and is negative otherwise. The dataset contains 3296 positive (64.27%) and 1832 negative (35.73%) in-stances.

We note that although the size of the dataset may not completely capture the noise and lexical diversity presented in social media, the hundreds of millions tweets generated each day constrain the viability of established ground truth data of substantial proportion. Examining the literature, comparable or smaller sizes of manually labeled tweets are often used to validate the models proposed in many reputable and high-impact works such as [21, 24].
Even though Twitter data is used to verify our model, the ex-pansion into diverse types of social media such as web blogs and Google+ provides a broader foundation for public health surveil-lance. The need to accommodate heterogeneous types of data means that it is important for us to design a method that easily generalizes across data sources with different properties.

We propose to combine 5 different base classifiers, selected from different families of classification algorithms and shown to be state-of-the-art for text classification, each of which is trained with a different feature type explained in Section 4.2. For each feature type, 5 base classifiers listed in Section 4.1 are tried using 10-fold cross validation with different feature extraction parameter config-urations. The base classifier and parameter configuration that yield the highest F-measure is chosen for ensemble experiments outlined in Section 4.3.
On each feature type, we employ 5 classification algorithms drawn from different classification families namely Support Vector Ma-chine ( SVM ) [2] with the linear kernel and C = 1 : 0 , Naive Bayes ( NB ) [12], Multinomial Naive Bayes ( NBM ) [18], Random Forest ( RF ) [5] with 100 trees as suggested by [14], and Repeated Incre-mental Pruning to Produce Error Reduction ( RIPPER ) [7] with the number of folds of 3 and the minimum weight of instances of 2.0. We use LibSVM 1 implementation for SVM, and Weka 2 implemen-tation for the other classifiers.
This section discusses the extraction of the 5 feature sets repre-senting different views of the dataset.
N-gram features have been used extensively in text classification to learn word patterns in the training data. Let a document d be an ordered set of terms. An N-gram is a sequence of contiguous N terms in d . Here we represent a document with a union of its uni-to N -grams. Three different weighting schemes are explored: Binary , Frequency , and TF-IDF . Let S be the set of training documents, V =  X  v 1 ; :::; v M  X  be the vocabulary extracted from S , t be the test document, and F ( t ) =  X  f 1 ; :::; f M  X  be the feature vector of the test document t . We define the weighting schemes as follows: f f i = T F ( v i ; t ) f
T F ( w; d ) is the number of occurrences of term w in document d . Since tweets do not conform with standard English, we also study how data cleaning and stemming have effects on the perfor-mance. Table 1 lists all the configuration parameters and their pos-sible values for the NG feature extraction. Note, the features used in the baseline method proposed by Paul and Dredze [19] uses the  X  clean = F; stem = F; N = 3 ; W = binary  X  configuration.
Two drawbacks of N-gram features are 1) words with multiple meaning are treated the same (Ex. cold can be used in both dis-ease or temperature contexts) and 2) important keywords are treated as normal words (Ex. Xeroderma pigmentosum is a disease http://www .csie.ntu.edu.tw/ cjlin/libsvm/ http://www.cs.waikato.ac.nz/ml/weka/ name, but may not be identified as a discriminative feature by N-Gram approaches since it is a rare disease and appears in only a few documents). Figueiredo et al. [10] propose compound features (c-features) for text classification. A compound of C terms is a group of C terms that occur in the same document. A compound with C = 2 is a generalized definition of term co-occurrence. Like NG features, we represent a document with the union of uni-to N -grams.

Compound features address the disambiguation problem, since they can identify different sets of term used in different scenarios. However, such features would not be able to address the keyword recognition problem as they cannot interpret the meaning of each term. Another problem of using full compound features is that the feature set can grow very large once all possible compounds are enumerated.

To overcome these challenges, we propose a feature selection strategy for the compound feature extraction, which we call Dictionary-based Compound features (DC). Our DC feature extraction algo-rithm first generates all possible compounds from a document. Next, a compound that contains at least one term defined in the dictionary is kept. In our experiment we use 3 vocabularies: disease , symp-tom , and anatomy . We obtain such vocabularies from the Gemina project 3 . The disease and symptom vocabularies contain human disease and symptom names respectively, and are used due to the fact that there is a high chance that authors of the messages use these terms to identify their own or others X  health conditions (i.e.  X  X  think I X  X  havin an asthma attac k....wtf am I tweeting? X  and  X  X eel-ing better. still have a bit of a headache though.  X  ). The anatomy vocabulary contains words used to name physical parts of a hu-man body, and is used because the existence of body organ words may help disambiguating health-related terms (i.e.  X  X  X  X l throw pil-lows from my couch here...my knees ar e burning X  . In this example, burning can mean either very hot or painful . The presence of the word knees may help identify that burning actually has the latter meaning.). Table 2 lists all the configuration parameters and their possible values.
The intuition behind topic modeling is that an author has a set of topics in mind when writing a document. A topic is defined as a distribution of terms. The author then chooses a set of terms from the topics to compose the document. With such assumption, the whole document can be represented using a mixture of different topics. Topic modeling strategies have also been applied in a va-riety of applications such as citation recommendation [13], docu-ment annotation [26, 25], and text classification [6, 11]. We employ the Latent Dirichlet Allocation (LDA) [3] algorithm for modelling topics in our work.

Here we use topic distribution to represent a document. Since a topic is represented by a group of weighted terms, one can think of http://gemina.igs.umaryland.edu a set of topics as a form of compound features, where the weighted terms in a topic represent the components in a compound, and hence we hypothesize that using topic distribution as features can address the term disambiguation problem. For example, the term cold may be the top terms in two topics; one is temperature-related, and the other sickness-related.
 In our work we model topics from the training documents using LDA algorithm implemented in MALLET 4 , a MAchine Learning for Language Toolkit , with 3,000 maximum iterations and using Gibbs sampling. We obtain the topic distribution for each test doc-ument using the inference algorithm proposed by [1]. Table 3 lists all the configuration parameters for TD feature extraction. T able 4: Features used in ST feature extraction, divided into two groups: physical and emotional based.

Our proposed sentiment features can be divided into two groups: physical and emotional based. The physical based ST features quantify the explicit illness by measuring frequency of health re-lated keywords in each document. We use the same sets of vo-cabularies as in Section 4.2.2 for health-related keywords. The emotional based features measure the level of positive and neg-ative emotions in the message, using the SentiStrength algorithm proposed by Thelwall et al. [24]. Table 4 lists all the features.
Our physical based ST features also serve as a dimension reduc-tion of the DC features (with C = 1 ). Hence, such features have the potential to address the keyword recognition problem as they capture the frequency of highly relevant keywords. We also aim to investigate whether emotional based ST features can be discrimi-native as social messages are contaminated with emotions. All the configuration parameters are listed in Table 5. http://mallet.cs.umass.edu/ T able 6: Overlaps between misclassifications (misses) of the baseline and correct classifications (hits) of the classifiers train with proposed feature sets.
Having a classifier that learns all the aspects of the data may be helpful when combined with other one-aspect classifiers. We create such an overall classifier by training a base classifier with combined features generated by merging all the four feature sets discussed above into a single feature set.
In this subsection, we explain the motivation for combining base classifiers and discuss the choices of ensemble methods.
We replicated the feature set used by Paul and Dredze [19] on the original dataset and 10-fold cross validated it with a SVM classifier, which yields precision of 76.68%, recall of 47.63%, and F-measure of 58.76% (we later use these classification results as a baseline). In post-hoc examination we observed that many of the misclassifi-cations had the following characteristics: Keyword Recognition Problem. Messages containing highly dis-Term Disambiguation Problem. Messages containing highly dis-Additionally, we trained 4 classifiers based on DC, TD, ST, and DC-TD-ST (combined) feature sets (See Section 4.2), respectively, and examined the classification results. The magnitude of over-laps between the misses (false positives + false negatives) produced by the classifier trained with the baseline feature set and the hits (true positives + true negatives) produced by the DC (7.21%), TD (9.26%), ST (10.82%), DC-TD-ST (9.95%) based classifiers as seen in Table 6 suggests that the addition of these features may potentially increase overall performance of tweet classification.
The 5 base classifiers trained with different feature types are combined using standard ensemble methods listed below: Majority Voting (VOTE) Each classifier outputs either a  X  X es X  or Weighted Probability Averaging (WPA) Each classifier is given Multi Staging (MS) Classifiers operate in order. If a classifier Reverse Multi Staging (RevMS) Similar to the MS technique, ex-For the VOTE , MS , and RevMS methods, each base classifier classi-fies an instance as positive is the probability estimate is equal to or greater than the probability cutoff, and negative otherwise. For the WPA method, an instance is classified as positive if the final prob-ability estimate is equal to or greater than the probability cutoff, and negative otherwise. The probability cutoff is tuned during the training process using held-out data.
For each feature type, all the parameter configurations are 10-fold cross validated using the 5 different base classifiers listed in Section 4.1. The parameter and probability cutoff are tuned with the 10% held-out data. The best combination of the parameter con-figuration and base classifier in terms of F-measure is chosen. Pa-rameter sensitivity is also investigated. The performance of the best configuration of each feature type summarized in Figure 5. (a) Varying clean/stem parame-ters Figure 1: Parameter comparison of NG feature extraction as the maximum size of grams (N).

SVM is chosen for the NG feature type with configuration  X  T; stem = T; N = 2 ; W = tf idf  X  , with F-measure of 68.19%. To study the parameter sensitivity of the NG feature extraction, we investigate 1) the effects of document preprocessing and 2) how different weighting schemes affect the performance (F-measure) of the SVM classifier. Figure 1 shows the results as a function of the maximum size of grams ( N ). Figure 1(a) compares the per-formance of the feature sets with different clean and stem pa-rameters. According to the results, cleaning and stemming the data lead to higher quality of the feature sets. Figure 1(b) compares the results of NG feature extraction with different weighting schemes. It is clearly seen that features with TFIDF weight outperform the other weighting schemes.
A SVM classifier with the configuration  X  stem = true; vocab = all; N = 1 ; C = 2 ; W = tf idf  X  yields the best F-measure (56.47%). Figure 2 shows the parameter sensitivity analysis (F-measure) as functions of the maximum size of grams (N) on the SVM classifier. Figure 2(a) compares the performances when different vocabular-ies are used. It is evident that combining all the three vocabularies yields the best results. Note that the symptom vocabulary gives (a) Varying vocab parameters Figure 2: Parameter comparison of DC feature extraction as the function of maximum gram size (N). the best results among individual vocabulary sets, this is because a large number of sickness-related tweets only talk about symptoms (headache, stomachache, sore throat, etc.) without mentioning the causing disease names. Figure 2(b) compares the results achieved with different weighting schemes. First point to note, the perfor-mances of all the weighting schemes decrease as N increases. This is because compounds with bigger grams tend to generate sparse and idiosyncratic features. Similar to the NG features, the TFIDF weighting scheme outperforms the others. Figur e 3: Parameter comparison of TD feature extraction as the function of number of topics (Z).

Our results show that the configuration  X  clean = F; Z = 200 with a Random Forest classifier yield the best F-measure (54.50%). As part of the parameter impact on the RF classifier, we vary the number of topics, and also model topics from both  X  X leaned X  and  X  X ncleaned X  datasets. Figure 3 shows that the optimum number of topics is 200. Too few topics may lead to broad topics, hence low discriminative power; whereas, too many topics can result in spuri-ous, meaningless topics consisting of idiosyncratic word combina-tions.

An unexpected research finding was that uncleaned data gives a better performance, contrasting with the analysis of the NG, DC, and ST features which agree that cleaning the data in the preprocess step helps remove noise and boost the performance. One explana-tion could be that the length limitation (a tweet can have at most 140 characters) and informality of social media messages force the users to be creative in expressing their opinions using variants of standard languages. For example,  X  looooove u :)  X  in social media domain would mean  X  X ove you so much that I am smiling right now X  in standard formal language. Unfortunately, most nat-ural language processing techniques would treat such variations as noise and remove them instead of making use of them, causing loss of valuable information. (a) Varying type parameter Figure 4: Parameter comparison of ST feature extraction as the function of maximum gram size (N).

A RIPPER classifier and the configuration  X  stem = T; N = 2 ; type = both  X  yield the best F-measure (51.08%). Figure 4 shows the results from varying type and stem parameters as a function of the maximum size of grams ( N ) when tested with a RIPPER classifier. From Figure 4(a), it is interesting to see that the emotional-based features do not significantly help increasing the performance. This is because most Twitter users who tweet about their sicknesses do not always express negative feelings. Often-times, they make the messages sound humorous by adding positive emotions or use positive tones, e.g. GWS ya bang :P T Oh no I X  X  sick! Gotta use some rest :) LOL .
The combined features include all the previous 4 feature types generated with the chosen configurations mentioned earlier. The 5 base classifiers are tried and SVM is found to perform the best with F-measure of 68.47%.
We evaluate each ensemble method using 10-fold cross valida-tion on the labelled dataset TwitterA , using standard precision, re-call, and F-measure (F1) as the evaluation metrics [17]. Unlike existing approaches in the literature [19] in which the quality of the retrieved data is more important than the amount, we aim to apply our algorithm in disease surveillance situations where the ability to detect non-obvious health-related messages (e.g.  X  I X  X  not feeling good today, and prolly can X  X  go to class.  X ) is also important. Hence, we treat both precision and recall as having equal importance, and F-measure is used to mainly compare the results from each method.
The weight vectors used in the WPA method, the orderings of base classifiers used in the MS and RevMS methods, and the prob-ability cutoff are tuned using 10% held-out data of the training set (the other 90% is used to train the base classifiers).

We compare our proposed methods with the baseline features used in related works trained with a SVM base classifier tuned to achieve the best F-measure. Table 5 lists the results (in terms of precision, recall, F-measure, and F-measure improvement over the baseline) of each ensemble strategy, along with other base classi-fiers and the baseline classifier.
 The best performance in terms of F-measure is yielded by the WPA ensemble method. This method gives some weight to all the base classifiers learning different aspects of the dataset. The MS method gives the best recall of 91.93%. The RevMS yields the best precision of 90.08%. Since we treat precision and recall as equal Figur e 5: 10 fold classification performance of the baseline, proposed base and ensemble classifiers, in terms of precision, recall, F1, and  X  F1. important, we conclude that the WPA ensemble method works best for our task.
Our results show that the WPA method, wherein each base clas-sifier is given some decision weight, yields the best performance. This section further attempts to assess the importance of each base classifier when making collaborative decisions. The dataset is split into a train set (90%) and a test set (10%). All the base classi-fiers are trained with the train set, then all possible weight vec-tors are enumerated with an increment of 0.1, and are tested on the test set. The best performance is yielded by the weight vec-tor  X  N G = 0 : 1 ; DC = 0 : 2 ; T D = 0 : 1 ; ST = 0 : 1 ; CB = 0 : 5 with 74.76% precision, 68.93% recall, and 71.88% F-measure. The CB classifier is given most weight due to having the most exten-sive view of the data. The DC classifier is given a twice higher weight compared to TD and ST classifiers since it addresses both the problems posed by the baseline, while the others address only one problem.
We investigate using 5 heterogeneous feature sets representing different views of the data on machine learning ensemble methods for health-related short text classification problem. We analyse the parameter sensitivity of the feature extraction algorithms in order to obtain the best possible features from each feature type, and study the mutual effects of the feature sets by combining the base clas-sifiers, each of which is trained with a different feature type, using standard ensemble methods. We are able to outperform the base-line by 18.61% using the weighted probability averaging method. Our results are promising and reaffirm our assumption that the lim-itation of the N-gram features on the social media domain can be reduced by combining classifiers that learn different characteristics of the data. Future works could seek to improve the classification algorithm [28, 9] and to employ semi-supervised methods such as the co-training technique [4] to expand the training data with unla-beled data. We gratefully acknowledge financial support from the Center for Integrated Healthcare Delivery Systems (CIHDS).
