 Distance metric learning (DML) aims to learn a distance metric better than Euclidean distance. It has been success-fully applied to various tasks, e.g., classification, cluster-ing and information retrieval. Many DML algorithms suffer from the over-fitting problem because of a large number of parameters to be determined in DML. In this paper, we exploit the dropout technique, which has been successfully applied in deep learning to alleviate the over-fitting prob-lem, for DML. Different from the previous studies that only apply dropout to training data, we apply dropout to both the learned metrics and the training data. We illustrate that application of dropout to DML is essentially equivalent to matrix norm based regularization. Compared with the standard regularization scheme in DML, dropout is advan-tageous in simulating the structured regularizers which have shown consistently better performance than non structured regularizers. We verify, both empirically and theoretically, that dropout is effective in regulating the learned metric to avoid the over-fitting problem. Last, we examine the idea of wrapping the dropout technique in the state-of-art DML methods and observe that the dropout technique can significantly improve the performance of the original DML methods.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining ; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation Distance Metric Learning, Dropout
Learning a good distance metric is essential to distance based algorithms and applications, e.g., k -nearest neighbors, k -means clustering, content-based image retrieval (CBIR), etc. Distance metric learning (DML) aims to learn a lin-ear mapping such that in the mapped space, examples from the same class are closer to each other than those from different classes. Many methods have been developed for DML [6, 7, 16, 26, 27] in the past, and DML has been suc-cessfully applied in various domains, including information retrieval [13], ranking [6], supervised classification [26], clus-tering [27], semi-supervised clustering [5] and domain adap-tation [20].

One problem with DML is that since the number of pa-rameters to be determined in DML is quadratic in the di-mension, it may overfit the training data [26], and lead to a suboptimal solution. Although several heuristics, such as early stopping, have been developed to alleviate the over-fitting problem [26], their performance is usually sensitive to the setting of parameters (e.g. stopping criterion in early stopping), making it difficult for practitioners. Another problem with many existing DML methods is their high computational cost since they have to project intermediate solutions onto the Positive Semi-Definite (PSD) cone at ev-ery iteration to ensure that the learned metric is PSD. In [6], the authors show that it is possible to avoid the high cost of PSD projection by an one-projection paradigm that only needs to project the learned metric onto the PSD cone once at the end of the optimization algorithm. We adopt the one-projection paradigm in this paper to alleviate the high computational cost.

Recently, dropout has been found to be helpful in allevi-ating the over-fitting problem in training deep neural net-works [14]. It randomly omits half of the feature detectors to prevent the complex co-adaptation between those neu-rons. The main intuition behind dropout is that it improves the robustness of individual neurons in order to avoid the over-fitting problem. Theoretical analysis about the regu-larization role of dropout in neural network can be found in [1]. Besides the success in deep learning, dropout has been applied to regression [24] in order to obtain robust fea-ture representations. Features with artificial noises has been a classic topic in machine learning and data mining, and has been examined by many studies [17, 19] with the focus on additive noise that usually leads to a L 2 regularizer [2]. Wager et al. [25] analyze dropout within the framework of regression and find that dropout is first-order equivalent to a L 2 regularizer for the dataset scaled by the inverse diagonal Fisher information matrix. Although dropout has received a lot of interests in machine learning and data mining com-munity, to the best of our knowledge, this is the first study that exploits dropout for alleviating the over-fitting problem in DML.

In this paper, we, for the first time, introduce dropout to DML. Unlike previous studies on dropout that only ap-ply dropout to training data, we apply dropout to both the learned metrics and the training data. By applying appro-priate dropout probabilities to the learned metric, we show that dropout can be equivalent to Frobenius norm and L p in expectation. In addition, we develop a structured regular-izer using the dropout technique. Unlike the conventional regularizer that treats diagonal and off diagonal elements equivalently, the structured regularizer introduces different dropout probabilities for diagonal elements and off diago-nal elements. To verify the effect of dropout in DML, we have conducted a comprehensive study that compares the dropout technique to other regularization techniques used in DML. Experimental results show that dropout significantly improves the classification performance on most datasets. In addition, we observe that dropout behaves like a trace norm based regularizer in DML when applied to training data: it controls the rank of the learned metric and leads to a skewed distribution of eigenvalues. This is in contrast to the previous studies that view dropout as a L 2 regularizer. Finally, we show that the dropout technique can be easily incorporated into the state-of-art DML methods and signif-icantly improve their performance for most cases. The main contribution of this work is summarized as follows.
The rest of the paper is organized as follows. Section 2 introduces the related DML methods and the analysis for dropout. Section 3 describes applying dropout to the learned metric and training data, respectively. Section 4 summarizes the results of the empirical study. Section 5 concludes this work and discusses the future directions.
Distance metric learning has been studied sufficiently dur-ing the past years [6, 7, 22, 26, 27] and detailed investigations could be found in the survey papers [15, 28]. The early works focus on optimizing pair-wise constraints [7, 27] to make sure the distance between examples from the same class is less than a pre-defined threshold while that from the differ-ent classes is larger than the other threshold. Nowadays, triplet constraints, where the distance of examples from the same class should be marginally smaller than that of exam-ples from different classes, are preferred [6, 26] due to their superior performance compared with pair-wise constraints. More analysis shows that triplet constraints have the large margin property and could be explained as learning a set of local SVMs [8]. However, either taking pair-wise constraints or triplet constraints increases the number of training data exponentially, which significantly promotes the risk of over-fitting for DML. In fact, over-fitting phenomenon is reported by many DML methods [6, 26], and most of them try to al-leviate it by PSD constraint. PSD constraint, on one hand, is the feasible set where the optimal metric should live in, and on the other hand, restricts the learned metric in the PSD cone to reduce the complexity of the model. Given a huge number of constraints, stochastic gradient descent (SGD) is widely used to learn the metric and PSD projec-tion occurs at every iteration [23]. Unfortunately, the com-putational cost of PSD projection is cubic to the dimension of data, which significantly limits the application of DML in high dimensional datasets. Additionally, recent empir-ical study [6] demonstrates that one-projection paradigm, which only performs PSD projection once at the end of the algorithm, has the similar performance as projecting at ev-ery iteration. In this paper, we bring dropout to overcome over-fitting in DML. We adopt triplet constraints and one-projection paradigm setting, and show that dropout signifi-cantly improves the performance of existing DML methods.
Dropout is a technique developed for training deep neural networks [14]. Since deep neural network is a very complex dropout randomly omits half of neurons during training to limit the co-adaptation between neurons. Dropout makes sure that each learned neuron is robust. Besides deep learn-ing, dropout has been introduced to more general regression task these years [24, 25]. Maaten et al. [24] use dropout to learn a robust feature representation for bag-of-words fea-tures, which significantly improves the performance over the original features. Although dropout belongs to the classic artificially corrupted features, it is better than other kinds of noises as reported in the work [24]. Recently, Wager et al. [25] analyze dropout within the framework of general regression task, and find that dropout is first-order equiv-alent to the L 2 regularizer on the rescaled data. The ex-periments in the study [25] demonstrate that dropout, as a data-dependent L 2 regularizer, outperforms the standard L 2 norm significantly. In this paper, we introduce dropout for DML. Unlike the previous works that focus on dropout in features [24, 25], we also apply dropout for the learned metric. We observe that dropout is equivalent to Frobenius norm, L 1 norm and trace norm with different dropout strate-gies. Empirical study validates the effectiveness of dropout in DML.
Given the dataset X = [ x 1 ,  X  X  X  , x n ]  X  R d  X  n , distance metric learning is to learn a good Mahalanobis distance met-ric M , so that for each triplet constraint ( x i , x j , x x i and x j are in the same class and x k is from a different class, we have where dist( x i , x j ) is the squared Mahalanobis distance be-tween x i and x j and is measured by Algorithm 1 SGD for DML 1: Input: Dataset X  X  R d  X  n , #Iterations T , Stepsize  X  2: Initial M 0 as an identity matrix 3: for t = 1 ,  X  X  X  ,T do 4: Randomly sample a triplet { x t i , x t j , x t k } 5: M t = M t  X  1  X   X   X  ` 6: end for 7: return  X  psd (  X  M ) Therefore, the objective optimization problem based on min-imizing empirical risk could be written as where S d + is the d  X  d symmetric PSD cone, ` (  X  ) is the convex loss function, and A t = ( x t i  X  x t j )( x t i  X  x t j x ) &gt; .  X  X  ,  X  X  denotes the dot product for matrix.
Since the number of triplets is very large (it can be as high as O ( n 3 )), the optimization problem is usually solved by stochastic gradient descent (SGD) [7, 23]. Instead of projecting the learned metric onto PSD cone at every itera-tion, which can be an expensive operation, we adopt one-projection paradigm [6], which only projects the learned metric onto the PSD cone once at the end of iterations. Empirical studies have shown that one-projection-paradigm is significantly more efficient and yields the similar perfor-mance as SGD with PSD projection performed at every iter-ation. Therefore, in this work, we will focus on the following optimization problem without the PSD constraint.
Algorithm 1 gives the standard SGD algorithm for DML and dropout will be applied to perturb Step 5. We will dis-cuss the details in the next section within this framework. In particular, we will discuss two different applications of dropout, i.e. application of dropout to the learned metric and application of dropout to the training data, in the fol-lowing two subsections.
In this section, we focus on applying dropout to the learned metric. Let M be the metric learned from the previous it-eration. To apply the dropout technique, we introduce a Bernoulli random matrix  X  = [  X  i,j ] d i,j =1 , where each  X  Bernoulli random variable with  X  i,j =  X  j,i . Using the ran-dom matrix  X  , we compute the dropped out distance metric, denoted by  X  M as Note that by enforcing  X  i,j =  X  j,i ,  X  M is ensured to be a symmetric matrix. Below, we will discuss how to design the dropout probabilities for the Bernoulli random matrix  X  to simulate the effect of Frobenius norm based regularization and L 1 norm based regularization, respectively. Frobenius norm is the most widely used regularizer in DML [23, 26], and the standard DML problem with Frobe-nius norm is given by The updating rule in SGD for Frobenius norm based regu-larization is where ` t ( M ) = ` (  X  A t ,M  X  ).

Instead of using the regularizer directly, we could simu-late the effect of Frobenius norm based regularization by applying dropout to the learned metric M t  X  1 . In particu-lar, the Bernoulli random matrix  X  is constructed by sam-pling each  X  i,j : i  X  j independently from a Bernoulli distribu-tion with Pr[  X  = 0] = q and setting  X  j,i =  X  i,j to ensure that  X  is symmetric. It is easy to verify that and the updating rule becomes Theorem 1. Let M  X  be the optimal solution output by Algorithm 1. Let  X  M be the solution output by Algorithm 1 with dropout in Step 5 and q be the probability that dropout occurs in each item of the learned metric. Assume | x | 2 and q = 1 /T , we have The detailed proof can be found in appendix. By setting the stepsize  X  as we have
E [ ` (  X  M )]  X  ` ( M  X  )  X  4 r It is well known that O (1 / rate for SGD, when the loss function is Lipschitz continu-ous [11, 12, 29]. As a result, with the appropriate choice of dropout probabilities, dropout will maintain the same con-vergence rate as the standard SGD method. We also notice that q is suggested to be set as 1 /T in order to achieve O (1 / should not be taken too frequently, which is consistent with the analysis of other corrupted feature methods [2, 24]. Fi-nally, since the derivation of convergence rates keep the same regardless of the sampling probabilities used in dropout, we thus will omit the analysis on convergence for the following cases. Besides Frobenius norm, L 1 norm also could be used in DML as It is known as the composite optimization problem [18] and could be solved by iterative thresholding method
With different design of sampling probabilities, we can apply dropout to the learned metric to simulate the effect of L 1 regularization. In particular, we introduce a data-dependent dropout probability as Now, instead of perturbing M t  X  1 , we apply dropout to M i.e. the matrix after the gradient mapping. It is easy to verify that the expectation of the perturbed matrix  X  given by E h [  X  M 0 which is equivalent to the thresholding method stated above. It is straightforward to extend the method to L p norm by setting the probability as Note that when p = 1, it is equivalent to the probability for L 1 norm.
Although these conventional regularizers have been ap-plied for DML, they cannot exploit the structure of metrics sufficiently. Given a distance metric, the diagonal elements are more important than those from off diagonal. It is due to the fact that diagonal elements represent the importance of each feature (e.g., linear classifier) rather than the inter-actions between different features, and they also control the trace of the learned metric. Therefore, different regularizers should be assigned for diagonal and off diagonal elements, respectively. Fortunately, dropout can serve this purpose conveniently.

Given Q , which is a random matrix with each element from a uniform distribution in [0 , 1], we investigate the ma-trix It is obvious that the diagonal elements of R are still from the same uniform distribution, while elements in off diagonal are from a triangle distribution with cumulative distribution function as Figure 1 illustrates the cumulative distribution function for the diagonal elements of R and those living in off diagonal. The dropout probability based on the random matrix R is defined as
First, we consider dropout with the same probability for each item of the metric as for Frobenius norm. Then, the probability of  X  is Figure 1: Cumulative distribution function for diag-onal elements and those in off diagonal. Algorithm 2 Dropout as Structured Frobenius Norm (SGD-M1) 1: Input: Dataset X  X  R d  X  n , #Iterations T , Stepsize  X  2: Initial M 0 as an identity matrix 3: for t = 1 ,  X  X  X  ,T do 4: Randomly sample a triplet ( x t i , x t j , x t k ) 5: Generate random matrix R as in Eqn. 2 6: Generate dropout parameters  X  by Eqn. 3 8: M t =  X  M t  X  1  X   X   X  ` 9: end for 10: return  X  psd (  X  M ) since q = 1 /T 0 . 5 as indicated in Theorem 1.

Therefore, the expectation of  X  M t  X  1 is which is equivalent to solving the following problem
It is obvious that the L 2 norm of diagonal elements in the metric is penalized quadratically more than those from off diagonal. This regularizer seems complex but the implemen-tation by dropout is quite straightforward and Algorithm 2 summarizes the method.

Then, we consider dropout with the probability based on the elements as It seems too complicated to analyze at the first glance, but Figure 1 could help us to understand the dropout strategy clearly. For the diagonal elements, they are actually shrunk by q as the L 1 regularizer. For the off diagonal elements, if | M i,j | &gt; 2 q , the red dashed curve is under the blue solid one, which means the shrinkage is less than q . When q  X  | M i,j | X  2 q , the red dashed curve stands above the blue solid Algorithm 3 Dropout as Structured L 1 Norm (SGD-M2) 1: Input: Dataset X  X  R d  X  n , #Iterations T , Stepsize  X  2: Initial M 0 as an identity matrix 3: for t = 1 ,  X  X  X  ,T do 4: Randomly sample a triplet ( x t i , x t j , x t k ) 6: Generate random matrix R as in Eqn. 2 7: Generate dropout parameters  X  by Eqn. 4 8: Dropout: M t =  X M 0 t 9: end for 10: return  X  psd (  X  M ) one and the shrinkage on these elements is much faster than the standard L 1 norm. Since q is very small, most of the off diagonal elements have relatively larger values and will be shrunk slower than those with extremely small values. Algorithm 3 summarizes this method. Unlike Algorithm 2, dropout in Algorithm 3 is performed after updating with the current gradient.
Besides dropout within the learned metric, in this section we apply dropout to the training data as many pervious studies [24, 25]. Since the analysis for data highly depends on the loss function, we take the hinge loss, which is the most widely used loss function in DML [6, 7, 23, 26], as an example.

Hinge loss is defined as ` ( z ) = [1 + z ] + , where z =  X  A and
Obviously, the loss function penalizes A t rather than the individual example, so dropout is taken according to the structure of A t . To avoid affecting the decision of hinge loss, we perturb A t after calculating the hinge loss.
We begin with additive noise as  X  A where  X  X  (0 ,qI d  X  d ). So the expectation of  X  A t is By replacing A t in Prob. 1 with  X  A t , the expectation of the problem becomes Note that the trace norm stands outside of the hinge loss, since the noise is added only after computing the hinge loss and only active constraints will contribute to the trace norm. We use the trace norm rather than the trace of the metric, because the final metric will be projected onto the PSD cone, where the trace of metric is equivalent to the trace norm. Algorithm 4 describes the details of the method.

Although the Guassian noise could perform as the trace norm, the external noise may affect the solution. Therefore, we consider dropout as Algorithm 4 Additive Noise as Trace Norm (SGD-D1) 1: Input: Dataset X  X  R d  X  n , #Iterations T , Stepsize  X  2: Initial M 0 as an identity matrix 3: for t = 1 ,  X  X  X  ,T do 4: Randomly sample a triplet ( x t i , x t j , x t k ) 5: if ` ( A t ,M t  X  1 ) &gt; 0 then 6: Generate a Guassian noise vector 7: Add noise as in Eqn. 5 9: end if 10: end for 11: return  X  psd (  X  M ) Algorithm 5 Dropout as Trace Norm (SGD-D2) 1: Input: Dataset X  X  R d  X  n , #Iterations T , Stepsize  X  2: Initial M 0 as an identity matrix 3: for t = 1 ,  X  X  X  ,T do 4: Randomly sample a triplet ( x t i , x t j , x t k ) 5: if ` ( A t ,M t  X  1 ) &gt; 0 then 6: Dropout as in Eqn. 7 8: end if 9: end for 10: return  X  psd (  X  M ) where  X  t is a binary value random variable and It is obvious that E [  X  t ] = 1 and V [  X  t ] = 1 + q/ ( x Similar to the additive noise, we only apply dropout for the first item of A t as Note that when we perform dropout to the training data ac-cording to this strategy, we actually drop the rows and the corresponding columns in the first component ( x t i  X  x t x ) &gt; of A t . Since the expectation of random variables in diagonal is the variance and it is 1 in off diagonal, the ex-pectation of  X  A is
E [  X  A t ] = E [(  X  x t i  X   X  x t j )(  X  x t i  X   X  x t
By taking  X  A t back to Prob. 1, we obtain the same problem as Prob. 6. Algorithm 5 summarizes this dropout strategy for training data.
 Theorem 2. Let M  X  be the optimal solution output by Algorithm 1. Let  X  M be the solution output by Algorithm 5 and q be the probability that dropout occurs in each feature of the dataset. Assume | x | 2  X  r and q = 1 /T , we have E [ ` (  X  M )]  X  ` ( M  X  )  X  The detailed proof is referred in appendix. If we set the stepsize  X  as we have ` (  X  M )  X  ` ( M  X  )  X  1 where O (1 / standard SGD, is also observed as in Theorem 1.

According to Theorem 2, applying dropout to training data with the appropriate component and dropout probabil-ity does not hurt the convergence performance of standard SGD method too. Furthermore, q is required to be suffi-ciently small to avoid the suboptimal solution, which is also consistent with the analysis in Theorem 1.
Six datasets from different application scenarios are used to verify the effectiveness of the proposed method. Table 1 summarizes the information of these datasets. ta is a so-cial network dataset with 6 different categories of terrorist attacks [21]. semeion is a handwritten digit dataset down-loaded directly from the UCI repository [9]. caltech10 is a subset of Caltech256 image dataset [10] with 10 most pop-ular categories and we use the version pre-processed by the study [6], where each image is represented by an 1 , 000-dimension vector. The other datasets are directly down-loaded from LIBSVM database [4]. For dna , protein and sensit , we use the standard training/testing split provided by the original dataset. For the rest datasets, we randomly select 70% of data for training and use the remaining 30% for testing. For each dataset, we randomly select T = 100 , 000 active triplets (e.g., incur the positive hinge loss by Eu-clidean distance) within the range of 3-nearest same class neighbors as suggested by the study [26]. K -Nearest Neigh-bor ( k =3) classifier is applied after obtaining the metric, since we optimize the triplets from 3-nearest neighbors. All experiments are repeated by 5 trials on different randomly generated triplet sets and the average result with standard deviation is reported.
In the first experiment, we compare the standard SGD for DML to five SGD variants including our proposed meth-ods (i.e., SGD-M1, SGD-M2, SGD-D1, and SGD-D2). The methods are summarized as follows.  X  SGD : stochastic gradient descent method as described in  X  SGD-PSD : SGD with PSD projection at every iteration.  X  SGD-M1 : SGD with dropout for the learned metric as  X  SGD-M2 : SGD with dropout for the learned metric as  X  SGD-D1 : SGD with additive Guassian noise in training  X  SGD-D2 : SGD with dropout for training data as trace Euclidean distance is also included as the baseline method and denoted as  X  Euclid  X .

All of these SGD methods are applied on the same triplet set and take one-projection paradigm except SGD-PSD that projects the learned metric onto the PSD cone at every iter-ation. We search the stepsize  X  in { 0 . 1 , 1 , 10 } by cross val-Table 1: Statistics for the datasets used in the em-pirical study. #C is the number of classes. #F is the number of features. #Train and #Test rep-resent the number of training data and test data, respectively. idation and  X  = 1 shows the best performance, so we fix it for all experiments. The dropout probability parameter q for the proposed methods is searched in { 10  X  i : i = 1 ,  X  X  X  , 5 } . All SGD methods are started with an identity matrix in the experiment.

Table 2 shows the classification accuracy of different SGD methods. First, it is not surprising to observe that all DML algorithms improve the performance compared to the Eu-clidean distance. Second, for all datasets, we observe that the proposed SGD methods with dropout (i.e., SGD-M1, SGD-M2, SGD-D1, and SGD-D2) significantly outperform the baseline SGD methods (i.e., SGD and SGD-PSD), which is also demonstrated by the statistical significance exam-ined via pairwise t-tests at the 5% significance level. Con-cretely, on most datasets, the accuracy of SGD with dropout is about 2% improved compared with that of SGD and it is even 4% on protein .

Furthermore, we observe that SGD-M1 shows the best performance on semeion , caltech10 and protein , while SGD-M2 outperforms other methods on ta and dna , and SGD-D2 is the best on sensit . It is because dropout in the learned metric and dropout in the training data represent different regularizers, and different dataset prefers different regular-izer. SGD-D1 and SGD-D2 have the similar performance because they optimize the same trace norm. However, SGD-D2 is a little bit better than SGD-D1 due to the reason that no additional Guassian noise is introduced by SGD-D2. Fi-nally, SGD-PSD performs same as if not worse than SGD, which is consistent with the observation in the study [6].
Then, we investigate if dropout can perform as the reg-ularizers as we expected. Figure 2 compares the effect of different norms with different weights to that of SGD, where only the parameter q of dropout varies and the others are kept the same. First, since SGD-M1 puts more aggressive penalty on the diagonal, Figure 2(a) shows how L 2 of the diagonal elements in the metric learned by SGD-M1 varies as q decreases. We observe that the smaller the parameter is, the larger the L 2 norm is, and even when demonstrates that dropout as structured Frobenius norm restricts the size of diagonal elements well. Second, Fig-ure 2(b) compares the sparsity of the learned metric, where sparsity is defined as Without the constraint of L 1 norm, the sparsity of the met-ric learned by SGD is small as shown by the blue dashed line. proposed methods). is the rank of the metric learned by SGD-D2. There is no over-fitting observed for SGD method with dropout. However, in SGD-M2 as plotted by the red dash dotted line, with the increasing of the probability of dropout as the struc-tured L 1 norm, the learned metric becomes more sparse, which confirms the effectiveness of SGD-M2. Finally, trace norm constraint usually leads to a low-rank metric [3], so we study the rank of the learned metric by SGD-D2 in Fig-ure 2(c). As expected, when q becomes larger, more stress is put on the trace norm and the lower-rank metric is induced.
Since dropout is found to be helpful to overcome over-fitting in deep learning [10], we empirically study the role of dropout for alleviating over-fitting problem in DML. We fix all parameters as above except the number of sampled triplets, to study the changes of training error and test er-ror on the training and test set, with the increasing of the number of triplets. Figure 3 shows the training error and test error of SGD, SGD-PSD and SGD with best dropout strategy on three small datasets (i.e., SGD-M1 on semeion and SGD-M2 on the others), while the number of sampled triplets increases from 20 , 000 to 100 , 000. First, we observe that the training error of SGD with dropout is similar to that of conventional SGD methods as we indicate in Theo-rem 1. However, over-fitting is observed for SGD and SGD-PSD when the number of triplets is up to 40 , 000, while there is no over-fitting phenomenon for SGD with dropout. It further demonstrates the overwhelming performance of dropout strategies in Table 2 and confirms that dropout is also helpful to overcome the over-fitting problem in DML.
Besides the comparison with various SGD methods, we also compare our proposed dropout methods to three state-of-art DML algorithms as follows.  X  SPML [23]: a mini-batch stochastic gradient descent al-strategies in. The best result is bolded. strategies in. The best result is bolded.  X  OASIS [6]: an online learning approach for DML and the  X  LMNN [26]: a batch learning method with Frobenius
SPML and OASIS use the same triplet set as the proposed methods and also adopt one-projection paradigm. LMNN is a batch learning method, and thus there is no limitation for the type and the number of triplets that it could use for each iteration. Specifically, LMNN is not restricted to the set of triplets used by other methods. There is also no constraint for PSD projection in LMNN and it can perform PSD pro-jection whenever it requires. All codes for these methods are from the authors and the recommended parameters are used. Since SPML is a stochastic method, it shares the same setting as the proposed methods, where the parameter for Frobenius norm is searched within the same range as q to obtain the best performance. SPML and OASIS are both initialized with an identity matrix, while LMNN starts with the matrix from PCA without dimension reduction, which usually has a better performance than the identity matrix for the method [26].

Table 3 summarizes the classification accuracy of differ-ent DML algorithms.  X  Dropout  X  denotes the best result of dropout methods adopted from Table 2. It can be easily observed that, although LMNN is a batch learning method and could utilize much more information than our methods, LMNN only has the similar performance on dna and cal-tech10 , while SGD method with dropout significantly out-performs on all other datasets. It further demonstrates the effectiveness of the proposed methods. SPML and OASIS are slightly better than the standard SGD method, but sig-nificantly worse than SGD method with dropout technique. The performance of OASIS could be explained by the fact that it does not include any conventional regularizer and over-fitting could be easily induced. Although SPML com-bines the Frobenius norm as the regularizer, it is worse than SGD-M1 and SGD-M2 shown in Table 2, which implies that the proposed structured norm by dropout is more effective than the standard norm.
In this section, we demonstrate that dropout can be eas-ily wrapped in the existing DML methods and help improve the performance. First, we wrap dropout in SPML, which is a state-of-art mini-batch SGD method for DML. Note that SPML has the Frobenius norm as the regularizer, so we drop it first to make sure that there is only one regularizer at one time. Since it is a SGD method, the dropout on M is the same as Algorithm 2 and Algorithm 3. We denote dropout as the structured Frobenius norm on SPML as  X  X PML-M1 X  and dropout as the structured L 1 norm as  X  X PML-M2 X . Instead of randomly selecting one triplet at each iteration, SPML samples b triplets at one time and updates according to the batch of the gradient. Therefore, when applying dropout to the training data, we simply perform the dropout on differ-ent matrix A in the mini-batch as in Algorithm 5 and the method is denoted as  X  X PML-D X .

Table 4 summarizes the results for wrapped SPML meth-ods. First, it is not surprising to observe that all dropout strategies improve the performance of SPML. On almost all datasets, the improvement on accuracy is more than 1% and it is even about 3% on protein , which is also consistent with the observation in the comparison for various SGD meth-ods. Although SPML applies the standard Frobenius norm as the regularizer, SPML with different dropout strategies outperforms it significantly according to the statistical sig-nificance examined via pairwise t-tests at the 5% significance level, which shows the superior performance of the proposed structured regularizers.

Then, we wrap dropout in OASIS, which is a state-of-art online learning method for DML. Since online learning has the similar process as stochastic gradient descent method, wrapping dropout in is pretty straightforward. Figure 4 il-lustrates the procedures of wrapping different dropout strate-gies in OASIS. Let  X  X ASIS-M1 X ,  X  X ASIS-M2 X , X  X ASIS-D X  denote dropout as the structured Frobeniuse norm, the struc-tured L 1 norm and the trace norm in OASIS, respectively. The comparison of classification accuracy applied by 3-NN is summarized in Table 5. The similar phenomenon as for SPML is observed, that dropout always helps to improve the performance of OASIS significantly according to pairwise t-test at the 5% significance level.

In summary, wrapping dropout in existing DML methods is not only convenient but also very helpful for performance improvement.
In this paper, we propose two strategies to perform dropout for DML, i.e., dropout in the learned metric and dropout in the training data. For dropout in the metric, we propose the structured regularizer, which is simulated with dropout by assigning different dropout probabilities for the diagonal elements and those living in off diagonal. For dropout in the training data, the data-dependent dropout probability is adopted to mimic the trace norm. We develop the theo-retical guarantees for both dropout scenarios to show that dropout will not affect the convergence rate of SGD with the appropriate dropout probability. Furthermore, we demon-strate that the proposed strategies are very convenient to wrap in the existing DML methods. Our empirical study confirms that the proposed methods have the overwhelm-ing performance compared with the baseline methods, and could significantly improve the classification accuracy for the state-of-art DML methods. Since we currently apply dropout to the learned metric and the training data sepa-rately, we plan to examine the performance of the combina-tion in the near future.
 Qian and Jin are supported by ARO (W911NF-11-1-0383), NSF (IIS-1251031) and ONR Award (N000141410631). Hu and Pei X  X  research is supported in part by a NSERC Discov-ery Grant and a BCIC NRAS Team project. All opinions, findings, conclusions and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies. [1] P. Baldi and P. J. Sadowski. Understanding dropout. [2] C. M. Bishop. Training with noise is equivalent to [3] E. J. Cand`es and T. Tao. The power of convex [4] C.-C. Chang and C.-J. Lin. Libsvm: A library for [5] H. Chang and D.-Y. Yeung. Locally linear metric [6] G. Chechik, V. Sharma, U. Shalit, and S. Bengio. [7] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. [8] H. Do, A. Kalousis, J. Wang, and A. Woznica. A [9] A. Frank and A. Asuncion. UCI machine learning [10] G. Griffin, A. Holub, and P. Perona. Caltech-256 [11] E. Hazan, A. Agarwal, and S. Kale. Logarithmic [12] E. Hazan and S. Kale. Beyond the regret minimization [13] X. He, W.-Y. Ma, and H. Zhang. Learning an image [14] G. E. Hinton, N. Srivastava, A. Krizhevsky, [15] B. Kulis. Metric learning: A survey. Foundations and [16] D. K. H. Lim, B. McFee, and G. Lanckriet. Robust [17] K. Matsuoka. Noise injection into inputs in [18] Y. Nesterov et al. Gradient methods for minimizing [19] S. Rifai, X. Glorot, Y. Bengio, and P. Vincent. Adding [20] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. [21] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher, [22] S. Shalev-Shwartz, Y. Singer, and A. Y. Ng. Online [23] B. Shaw, B. C. Huang, and T. Jebara. Learning a [24] L. van der Maaten, M. Chen, S. Tyree, and K. Q. [25] S. Wager, S. Wang, and P. Liang. Dropout training as [26] K. Q. Weinberger and L. K. Saul. Distance metric [27] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. J. Russell. [28] L. Yang and R. Jin. Distance metric learning: a [29] M. Zinkevich. Online convex programming and
Proof. Since the loss function is convex, we have Taking expectation on  X  , we have Since | x |  X  r , k A t k F  X  4 r . Adding iterations from 1 to T and setting q = 1 /T , we have E [ ` (  X  M )]  X  (1  X  1 /T ) ` ( M  X  )  X  1
Proof. Taking expectation on  X  A t , we have Since the loss function is convex, it is
E [ ` ( M t  X  1 )]  X  ` ( M  X  )  X  1 where q  X  X  high order items are omitted since q is a small number. Sum over the iteration from 1 to T , we have E [ ` (  X  M )]  X  ` ( M  X  )  X  The proof is finished by setting q = 1 /T .
