 This work presents a general rank-learning framework for passage ranking within Question Answering (QA) systems using linguistic and semantic features. The framework en-ables query-time checking of complex linguistic and seman-tic constraints over keywords. Constraints are composed of a mixture of keyword and named entity features, as well as features derived from semantic role labeling. The framework supports the checking of constraints of arbitrary length re-lating any number of keywords. We show that a trained ranking model using this rich feature set achieves greater than a 20% improvement in Mean Average Precision over baseline keyword retrieval models. We also show that con-straints based on semantic role labeling features are par-ticularly effective for passage retrieval; when they can be leveraged, an 40% improvement in MAP over the baseline can be realized.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Design, Experimentation, Performance Question answering, passage retrieval, learning to rank, an-notation graphs, text annotations, committee perceptron
Question Answering (QA) systems aim to deliver specific answers to user questions posed in natural human language. A QA system can be thought of as an embedded passage re-trieval process bookended by Natural Language Processing (NLP) components that allow the system, to understand the question, on the front end, and post-retrieval, to locate an-swers among the results. If QA systems are ever to become competitive with the ad hoc keyword search engines that are ubiquitous in the lives of today X  X  internet users, both latency and accuracy must be improved. Both of these goals can be addressed by improving the quality of the embedded passage retrieval component.

Poor passage retrieval quality within QA systems stems in part from a mismatch between what the system wants and what the embedded retrieval component is able to query. Internally, QA systems represent their information needs as sets of linguistic and semantic constraints that a retrieved passage must satisfy if it answers the question. Many pas-sage retrieval approaches commonly used in QA systems can not check these types of constraints at query time. As a result, QA systems are forced to approximate their informa-tion needs in terms of classic ad hoc retrieval primitives such as bag-of-words, proximity and named entity features.
For many questions, the classic feature set poorly approx-imates the information need, resulting in the retrieval of too few answer-bearing passages and/or too many false posi-tives. This degradation in passage retrieval quality overbur-dens the downstream Answer Generation component, which must determine whether each retrieved passage is answer-bearing by comparing it against the linguistic and semantic constraints specified in the information need. Post-retrieval constraint-checking involves potentially slow NLP analysis, which can limit the number of results that can be consid-ered in the time available. In addition to increasing sys-tem latency, poor quality retrieval results can also degrade accuracy when the best answer is ranked so low that the system does not have a chance to consider it. The focus of this paper is on improving the quality of results ranked by a QA system X  X  embedded passage retrieval component, thereby providing the best possible foundation for a fast and accurate end-to-end system.

In this paper, we propose a new approach to passage re-trieval for QA systems, based on rank-learning techniques, that integrates linguistic and semantic constraint-checking into the retrieval process. The approach utilizes a novel method of decomposing the question representation, viewed as a graph, into atomic constraints to be matched in can-didate answer-bearing passages. This decomposition en-ables partial constraint matching, differential weighting of constraint types, and graceful back-off to baseline ranking features such as bag-of-words and named entity matches. Atomic constraints become features for a trained model able Figure 1: Block diagram of a pipelined QA system. This paper studies the impact on passage retrieval quality of re-ranking baseline search results using a trained model capable of checking the linguistic and semantic constraints specified in the system X  X  information need. to judge linguistic and semantic similarity between a re-trieved passage and the QA system X  X  information need. We show that this model provides significant improvements in Mean Average Precision when used to re-rank passages re-trieved by a baseline retrieval approach consisting of bag-of-words and named entity features.
Consider a Question Answering (QA) system with broad coverage on the common types of factual, short-answer ques-tions known as factoids within the research community. For the purposes of this study, we consider only the first two modules of the pipelined QA system shown in Figure 1. The Question Analysis module uses NLP resources to map the question into a representation we call the information need , which consists of the linguistic and semantic constraints that an answer-bearing passage must satisfy. The information need serves as input to a Baseline Search module responsible for retrieving text likely to be relevant to that information need.

The experiments in this paper will focus on measuring the improvement in passage retrieval quality gained by intro-ducing our linguistic and semantic rank-learning approach, depicted as the Trained Ranking Model in Figure 1. The model re-ranks the top 1000 baseline results with respect to the linguistic and semantic constraints expressed in the in-formation need. The evaluation compares the quality of the trained model X  X  passage ranking with respect to the baseline in terms of Mean Average Precision. The Answer Genera-tion module shown in the figure is included for illustration purposes only, as end-to-end system accuracy is not explic-
The baseline passage retrieval approach in our QA system placeholders to represent the expected answer type. When scoring passages, as opposed to entire documents, this ap-proach approximates density-based methods considered to be strong baselines for QA [22]. For example, consider ques-tion 1398 from the QA track at TREC 3 2002, What year terms of MAP can translate to improved end-to-end system accuracy or answer Mean Reciprocal Rank (MRR) [2]. 2 See: http://www.lemurproject.org/indri Figure 2: Formal description of an annotation graph G , consisting of a set of elements E , a set of relations R , and a type system T , which defines element types T e and relation types T r . was Alaska purchased? The answer is 1867 . The baseline Indri query combines question keywords with an #any: op-erator matching occurrences of date , the expected answer type. The extent restriction operator, [sentence] , retrieves and scores sentence-sized passages individually.
We assume that a QA system analyzes its input question into an information need representation containing linguistic and semantic constraints that can be used to rank passages. Furthermore, we make the weak assumption that the sys-tem X  X  information need can be represented as an annotation graph , a generalized formalism described in this section. An-notation graphs make it easy to represent keywords and ar-bitrary linguistic and semantic relationships between them. Figure 2 gives a formal description of an annotation graph G , consisting of a set of elements E , a set of relations R , and a type system T . A type system defines sets of element and relation types, notated T e and T r , respectively. Each el-ement type te i has a name and a pointer to an optional parent element type. Each tr i is a named relation type de-fined to hold over specific domain and range element types, or types that inherit from them. The inheritance mechanism for element types allows for relations defined over element supertypes to be instantiated over instances of subtypes of those elements.

The QA system in this paper represents its information need under the type system T ne + srl , defined in Figure 3. It supports common named entity types and semantic role labeling, in which target verbs are related to their argu-ments , which are assigned PropBank [14] semantic roles by the ASSERT semantic parser [17]. The type system mod-els enclosure relations between field pairs, and between fields and keywords , ordering relations between keyword pairs, and attachment relations between targets and their arguments .
Figure 4 shows the annotation graph representing the in-formation need for our example question. Question keywords year , Alaska and purchased are in the bottom row, and the pairwise ordering relations holding among them are shown with curved arrows. Alaska was recognized as a location , so there is an enclosure relation between them shown as a solid
T e = Figure 3: Definition of the type system used in this paper, which supports named entities and ASSERT semantic role labeling. Note that there is an ele-ment type for each keyword k in the vocabulary V . Leaf element types representing text annotations are given in monospace type . arrow. The sentence participates in an enclosure relation with all of the other fields and keywords in the graph.
The semantic structure in the question is modeled by the target , argm-tmp and arg1 arguments , the enclosure rela-tions between them and the keywords , and the attachment relations between the target and the arguments , shown as dashed arrows. In this question, Alaska is labeled as arg1 because it is the thing being purchased ; the buyer ( arg0 ) is not specified. The answer is expected to be of type date and to occur in the temporal adjunct labeled argm-tmp .
This section describes our approach to learning to rank passages according to their linguistic and semantic similar-ity to an information need. In Section 4.1, we describe how to select, for any given type system, a set of atomic lin-guistic and semantic constraints, which are represented as annotation graph snippets. We show an example of this decomposition for T ne + srl , the type system underlying the experiments in this paper, in Section 4.2. In Section 4.3, we describe how atomic constraints become features useful for ranking and introduce the learning algorithm used for the experiments in this paper.
This section presents an algorithm for the decomposition of a type system into atomic constraints based on the ele-ment and relation types defined in the type system. All con-straints are annotation graphs, and the order of a constraint Figure 4: Information need representation for the question, What year was Alaska purchased? Solid arrows indicate enclosure relations, dashed arrows indicate target -argument attachment relations, and curved ar-rows indicate keyword ordering relations. The asterisk indicates the expected answer type.
 Table 1: Algorithm for Type System Decomposition is equal to the number of relations in the graph. A first-order constraint, therefore, consists of two elements joined by a single relation.

The algorithm shown in Table 1 enumerates in set C 1 all of the possibilities for first-order constraints by read-ing the type system X  X  set of relation definitions T r and in-serting all possible domain and range element types allow-able. The main loop of the algorithm begins at step 3. At each iteration, the algorithm builds a set of order-i con-straints based on the constraints of order i  X  1. The al-gorithm builds higher-order constraints by extending lower-order constraints, adding a single element and relation each time. The algorithm maintains a list of constraints as it traverses the type system in a breadth-first fashion.
To illustrate this process by example, consider the follow-ing first-order constraint, which contains a single enclosure relation between an arg1 and a person : To build a second-order constraint, the algorithm can intro-duce an attachment relation with a target as its source:  X  E = { ( t, target ) , ( a 1 , arg1 ) , ( p, person ) } , The worst-case complexity of this algorithm occurs for a type system for which every relation can be defined over any pair of element types, in either direction. The number of first-order constraints would be: For each constraint of order i  X  1, the algorithm constructs an order-i constraint by adding a new element and a new relation. In the worst case type system, any element could be chosen to be extended by any new relation in any direction and with any new element at the other end of the relation. The number of new constraints added would be: The overall complexity of the algorithm is: In the next section, we show the decomposition for T ne + srl which does not approach the worst case complexity. It is likely that more efficient algorithms exist for type system decomposition, but we leave their design and implementa-tion for future work.
This section describes the linguistic and semantic con-straint types obtained by decomposing T ne + srl , the type system used in the experiments in this study. Though this feature set provides a comprehensive basis for annotation graph similarity, we opt to reduce computational complex-ity in practice. To limit the number of constraints selected by the algorithm, we set the maximum order n = 3. In an effort to avoid redundant or sparsely predictive features, we prune the constraint sets C i for i &gt; 2 according to the NLP tools underlying T ne + srl : 1. A field may participate in at most one enclosure re-2. When attachment relations are present between tar-3. Do not model enclosure of targets and arguments 4. For keyword ordering , keywords must be enclosed in
After running the decomposition algorithm and thinning the number of constraints using the above-described princi-ples, we are left with eight classes of atomic linguistic and semantic constraints, which are shown in Figure 5, and are discussed individually below, with numbers referring to the figure. One additional constraint class mentioned below, Paths( N ) , is not generated using the algorithm, but using a separate procedure described below. 1. KEnc( field ) : This constraint is satisfied by a passage 2. KOrd( field ) : A passage satisfies this constraint if it 3. FEnc( f ield 1 , f ield 2 ) : A passage with an enclosure 4. Att( argument ) : This constraint checks for an at-5. Ans : This constraint is equivalent to  X  X Enc( sen-6. Args( N ) : A passage satisfies this constraint if it con-proposed by Geng, et. al. [10], could be applied here, but we leave that for future work. 7. Ta( argument ) : To satisfy this constraint, a passage 8. Taa( argument 1 , argument 2 ) : This constraint is 9. Paths( N ) : This powerful constraint requires a pas-
The above-described constraints are snippets of annota-tion graphs that can be compared against passages repre-sented as annotation graphs to determine whether the pas-sages satisfy the constraints. A constraint is considered sat-isfied by a passage if there exists a sub-graph alignment of the constraint annotation graph to the passage annotation constraint sub-graph to those of the passage graph such that all mapped elements are of the same type, and all relations that hold between elements in the constraint sub-graph also hold between the mapped elements in the passage graph. Satisfied constraints become features useful for ranking by counting the number of distinct alignments of the constraint to the passage that exist. See Figure 6 for an illustration of a constraint graph aligning to a passage annotation graph.
For T ne + srl , we generate 162 count-based features by count-ing the number of distinct sub-graph alignments to a pas-sage annotation graph for each constraint annotation graph. Leveraging this large number of features requires carefully lem, but the problem instances are small enough to be tractable for this application.
 Table 2: Full Feature Set, with group membership. Features are parameterized by element types defined in Figure 3. N is the path length through the anno-tation graph.
 setting the feature weights based on annotated training data. Many machine learning algorithms that have been applied to document ranking [4, 12] are well suited to this task. In this work we use an efficient, online linear rank learner, the Committee Perceptron [7].

The Committee Perceptron algorithm is a generalization of previous Perceptron variants [5], and is adapted for learn-ing ranking functions based on preferences between pairs of judged passages. This algorithm significantly outper-forms other perceptron variants and performs comparably or better than other linear rank learning algorithms, such as RankSVM, yet requires only a fraction of the training time [7, 12]. Rank learners that minimize the number of mis-ranked passage-pairs such as RankSVM and the Com-mittee Perceptron maximize a lower bound on many com-mon retrieval performance measures such as Mean Aver-age Precision [7]. The algorithm is described in Table 3. Here, we adopt the indexed sampling technique proposed by Scully [19] to avoid the quadratic dependence on the number of sentences in the collection.

The passage feature vectors used as input to this algo-rithm are vectors are represented as: p iq = h f 0 ( p i , q ) , . . . , f ( p i , q ) i where the f j ; j = 1 . . . M are the constraint-count features as described in the previous section and f 0 is the Figure 6: Constraint annotation graph (in bold) aligning to the annotation graph for the answer-bearing passage, In 1867, ... Seward reached agreement ... to purchase Alaska . The constraint is  X  X tt( arg1 ) X , as shown in Figure 5(4). Note that there are two distinct alignments of the constraint to the passage. The enclosing sentence and the ordering relations that exist between keyword pairs are not shown to increase legibility. baseline retrieval score. Features are scaled to zero-mean unit-variance per-question prior to training and testing.
The output of the learning algorithm is a collection of learned weight vectors, w k  X  R M +1 ; k = 1 . . . N com , and quality indicators for each weight vector, c k . These weight vectors parameterize an ensemble of linear passage scoring functions: Score( p iq , w ) = h p iq , w i where h X  ,  X i is the inner product. Scores from the learned ensemble of linear scoring functions, K , are averaged, weighted by the quality indica-tor, c k , to produce the overall score for a passage: For all experiments, we fix the committee size N com = 30 and the number of passage pairs to sample T = 10000, which are known to be effective parameter settings for other tasks.
To evaluate the impact of linguistic and semantic constraint-checking on passage retrieval quality, we introduce the trained ranking model to re-rank the top 1000 sentences retrieved by the Indri baseline, and compare the ranking quality in terms of Mean Average Precision. We evaluate on the AQUAINT corpus [11] used in the TREC 2002 QA track, prepared with sentence segmenta-tion (MXTerminator [18]), named entity recognition (BBN Identifinder [1]) and semantic role labeling (ASSERT [17]). to how much data is required for proper evaluation. One school of thought emphasizes using hundreds of questions and measuring accuracy in terms of matching the TREC-Table 3: Committee Perceptron Algorithm for Pas-sage Ranking.
 the TREC 2002 QA track for which reusable, passage-level relevance judgments are available [13]. The passage-level judgments were aligned to the sentence segmentation on the AQUAINT corpus, excluding relevant passages span-ning more than one sentence. Sentences deemed non-answer-bearing due to unresolved anaphora were also excluded.
Information need representations were built from the ques-tions using BBN Identifinder and ASSERT. ASSERT out-put was hand-corrected to mitigate its poor accuracy on questions, which are relatively rare among its training data. Despite this, only 48 of the 109 information needs (44%) contain ASSERT targets and arguments 7 . This is because ASSERT explicitly does not cover verbs, including be , have and do , common among the TREC questions. In the analy-sis, we will examine these 48 Deep Structure Questions sep-arately from the remaining 61 Shallow Structure Questions .
As described in Section 4.2, the trained ranking model uses features based on the atomic linguistic and semantic constraints in the information need. Table 2 gives the com-plete list of features. To study the effectiveness of different types of features, we refer to feature groups numbered 1 through 8, which are described below, in the analysis that follows the experiments. 1. Keyword Only : bag-of-words features only. 2. Surface Patterns : bag-of-words and relative key-3. NE + Keywords : bag-of-words and named entities. 4. NE + Surface Patterns : all of the above. provided answer pattern, as in [20]. Other researchers, such as Moschitti, et. al. [15], have used question sets nearly as small as this one with human relevance judgments. When measuring the quality of a ranked list, we believe that it is more important to have sufficient depth of relevance judg-ments than it is to have a large number of topics. of TREC factoid questions were answerable using a shallow semantic parsing method based on FrameNet [9]. 5. SRL : verbs, arguments and semantic roles, ignoring 6. SRL + Keywords : keywords participating in SRL 7. SRL + Surface Patterns : bag-of-words, keyword 8. SRL + NE + Surface Patterns : all of the above,
Experimental results analyzed in this section measure the improvement in passage retrieval quality attributable to re-ranking using linguistic and semantic features, in terms of Mean Average Precision (MAP). Feature groups 1 through 8 are studied individually. For all tests we report the p -value according to the two-sided Fisher X  X  randomization test [21]. First, we describe the results of experiments on the set of all questions, and then we take a closer look at the Deep Structure Questions .
For the full set of 109 questions, we perform 5-fold cross validation, with approximately 88/22 training/testing queries in each fold. Re-ranking using the trained model yields at least a 5% improvement improvement in MAP for all feature groups. Complete results are given in Table 4.
 Table 4: Performance results on the full question set.

In Table 4, most feature groups show noticeable improve-ments over the baseline, and those of feature groups 1, 2, 3 and 8 are statistically significant. Groups 5, 6 and 7, which use semantic role features, show large, but not significant, gains. Among these data, few questions are helped by the semantic role features, but for those that are, these features can be powerfully predictive. In feature group 6, for exam-ple, re-ranking helps 30% of the questions, improving MAP by more than 430% on average, and hurts 26%, degrading quality by 35% on average.

Although these semantic role features do not show signif-icant improvements when training and testing on the whole question set, we do see dramatic improvement on a subset of the questions. We hypothesize that by limiting the training and testing to the Deep Structure Questions , we can expect to find larger, more significant improvements in MAP. The Shallow Structure Questions , in contrast, have zero feature values for all semantic role labeling features, because AS-SERT was not able to provide target and argument infor-mation for these questions.
 Table 5: Top 15 (in absolute value) mean feature weights across folds, trained on the full feature set and full question set.

Table 5 shows the 15 features having the greatest mag-nitude weights learned by the model when training on the full question set, averaged across all five cross validation folds. Though some of the semantic role features claimed the largest magnitude weights, 6 of the 15 most influential features drawn from the surface patterns and named entity feature groups. This is a portrait of the model X  X  difficulty deciding between semantic role features, which are key for some questions yet irrelevant for many, and surface patterns and named entity features, which provide modest help for all questions.
To accurately measure the power of the semantic role fea-tures, we have to take a more careful look at the Deep Struc-ture Questions , as defined in Section 5.1. Table 6 reports the results for the model trained and tested on the Deep Struc-ture (top half) and Shallow Structure (bottom half) sets sep-arately. As before, these tests show 5-fold cross-validation results averaged across test sets.

From the top half of Table 6, we can clearly see that, for the Deep Structure Questions , the semantic role features have a significant positive impact on passage retrieval perfor-mance, realizing over 35% improvements in Mean Average Precision when those features are used in combination with keyword information. Feature group 5, which uses semantic role features alone, shows less of an improvement because the semantic role features are most useful when describing linguistic and semantic relationships among keywords that are not implied by surface patterns alone.

For the Shallow Structure Questions the picture is vastly different. In the bottom half of Table 6, we observe that the only features that result in moderate improvement are the surface patterns. As expected, feature group 5 shows zero improvement, because for these questions, the feature values other than the Baseline Retrieval Score are always zeroes. What is interesting about these questions is that named en-tities are of limited use in ranking the passages. It seems that the Baseline Retrieval Score and surface patterns are already capturing most of the information provided by these Table 6: Performance results on the sets of Deep and Shallow Structure Questions .
 features. Much of the small variations in performance across feature groups 4-7 are attributable to random sampling of the passage-pairs during the training process.

Looking at the features that are assigned the highest weight by the learning algorithm, we see that there is a distinct shift towards strongly favoring semantic role features for the Deep Structure Questions . Table 7 shows the top 15 (in absolute value) feature weights learned with the full feature set (8) on the Deep Structure Questions , averaged across all five cross validation folds. The top two of these top 15 features encode long-distance linguistic and semantic relationships between keywords that are not implied by the surface representation. Ten of the top 15 make use of some semantic role informa-tion. It is interesting to note that the most basic surface pattern features, X  X ield-keyword-enclosure( sentence ) X  X nd  X  X eyword-ordering( sentence ) X , are not a part of the top 15 most useful features, all of which make use of some named entity and/or semantic role information. This fact suggests that semantic role features are indeed powerful for ranking passages with respect to Deep Structure Questions .
Cui, et. al. [6], suggest a passage ranking method that rewards passages having a dependency tree structure simi-lar to the structure in the question. This similarity score is based on an application of IBM Model 1, which gives the likelihood that the dependency path between two keywords in the answer candidate sentence represents the same syn-tactic relationship as that which holds between the keyword pair in the question. Cui X  X  method linearly combines this dependency score with a lexical match score from a baseline retrieval method.
 Table 7: Top 15 (in absolute value) mean feature weights across folds, trained on the full feature set and the Deep Structure Questions .

We conducted a set of experiments on our dataset us-dependency path match score and the lexical score to max-imize Mean Average Precision. These experiments showed that our Indri baseline consistently performed as well or bet-ter than the Cui model.

The primary finding is that the Cui dependency path match score correlates strongly with the Indri score, with an average per-question Pearson correlation coefficient of 0.7083. See the top row of Figure 7 for a depiction of the cor-relation between the path match score and the Indri score. Terracing is visible where one score is constant while the other is capturing variation.

The dependency path match score does not appear to offer new information beyond what Indri provides. We hypothe-size that it is capturing primarily local syntactic dependen-cies between phrase heads and their modifiers and between the component words of named entities. Long-distance de-pendencies are explicitly avoided by Cui, et. al., due to poor parser accuracy [6]. Our baseline Indri queries model local keyword dependencies by enforcing co-occurrence of keywords and named entities of the expected answer type within a small text window.

Moschitti, et. al. [15], proposes a special-purpose tree ker-nel for the PropBank-style predicate-argument structures used in the type system in this paper. It is shown that the Shallow Semantic Tree Kernel (SSTK) can be used to classify whether an answer candidate is correct given its se-mantic context and that of the question.

The SSTK maps a predicate-argument structure into a feature space consisting of all possible combinations of tar-get verbs and argument slots. Each slot can contain at most one keyword, corresponding to the syntactic head of the la-beled phrase. SSTK models the question and answer sen-tences in separate feature spaces, but under our approach, a feature value is jointly derived from a question and an asso-ciated answer sentence. Additionally, in contrast to the ap-proach described in this paper, SSTK does not support key-is available from: http://www.cuihang.com/software.html Semantic Tree Kernel (bottom) for three randomly-selected que stions. word ordering or named entities, long-distance relationships between keywords, or constraints that mix feature types.
SSTK can not be applied directly to our annotation graphs because they are not trees. To compare it to our work, we adapted the kernel function as a feature extractor for use with the Committee Perceptron algorithm. The feature ex-tractor generates all possible combinations of slots from the question and compares them against the answer sentence. Binary feature values are set to one if and only if the answer sentence contains a predicate-argument structure containing the slots filled by the appropriate question keywords.
We repeated the cross-validation experiments using the feature set prescribed by the SSTK, with the addition of the baseline Indri score. We found that the resulting rank-ing was not statistically significantly better or worse than our Indri baseline. Indeed, with most of the weight accu-mulating to the Indri score feature, Indri was carrying most of the ranking. The examples in the lower half of Figure 7 show evidence of this, with the Committee Perceptron pre-dictions for some questions correlating near perfectly with Indri and others showing complete confusion. The average per-question Pearson correlation coefficient between the In-dri and SSTK scores is 0.6390.

We were not able to reproduce the answer classification results in [15] for several reasons. In the paper, answers are classified for correctness and are ranked by pushing incor-rect answers down in an ad hoc fashion. Here, in contrast, we have set up the learning problem as one of pairwise-preference classification, which gives us a complete ranking. The evaluation in the paper focused on description (also known as definition) questions, and our system is optimized for factoid questions.

The SSTK method assumes that a predicate-argument structure has at most one keyword in an argument slot, the syntactic head of the labeled phrase. We speculate that, for the task presented in this paper, the assumption that only the phrasal headwords are important is a bad one. Con-sider question 1427, What was the first spaceship on the moon? Here, the modifier first encodes an important con-straint that, if ignored, could result in incorrect answers being retrieved.

With both the Cui, et. al., and Moschitti, et. al., methods statistically indistinguishable from our bag-of-words with named entities Indri baseline, we can be confident that our approach demonstrates the power of long-distance linguistic and semantic relationships for passage ranking.
Rank-learning techniques have been successfully applied to the task of document ranking using features such as term count statistics and baseline retrieval model scores [7, 12]. Recent work applies rank-learning to the task of passage ranking for QA, augmenting these traditional feature types with linguistically-motivated features [23]. Their feature set was based largely on term overlap between the question and answer passage within different syntactic categories, such as verb and subject. A second set of features measured these same overlaps after expansion through WordNet synsets [8].
The Shen and Lapata [20] approach to applying seman-tics to the task of QA is notable in that it is based on FrameNet [9], rather than PropBank [14], which has a much richer roleset and an inheritance hierarchy for frames. They use a graphical approach to determine the distribution over semantic roles that keywords can take in a question or re-trieved sentence. A sentence is assumed to have at most one predicate, chosen heuristically. The similarity function used for ranking answer-bearing sentences requires that the pred-icates match exactly or participate in the same inheritance chain. The degree of match is determined by the divergence between the role distributions of the arguments of the pred-icates in the question and answer-bearing sentence.
Both the Verberne, et. al., and Shen, et. al., fail to model the case of multiple predicates in a sentence or question. Furthermore, the Shen, et. al., model does not allow for variation of the predicate verb except along the FrameNet inheritance hierarchy. The method proposed in this paper differs from previous work in that linguistic and seman-tic constraints between keyword pairs are modeled directly. Keywords labeled with semantic roles can be associated with specific target verbs in the event that there are more than one in the sentence. In addition, our approach models the case of multiple arguments attached to an unknown target .
Single-pass passage retrieval methods have also been pro-posed for QA, which involve pre-annotating the collection and indexing linguistic and semantic features to enable query-time constraint-checking. Bilotti, et. al., use structured queries to retrieve text satisfying PropBank-style semantic constraints [3]. Their method is shown to be effective in cer-tain cases, yet poor at combining evidence from bag-of-words and structured features and not robust to ranking partial matches. Pizzato and Moll  X a apply the vector-space model to a feature space consisting of surface words and their se-mantic role labels, as well as pairwise semantic relationships between keywords [16]. This model performs better at par-tial matching, but does not capture long-distance linguistic and semantic relationships between keywords.
For Question Answering (QA) systems to ever hope to compete with the dominant web search information access paradigm, they must improve both speed and accuracy. Poor quality embedded passage retrieval is one of the primary causes of wrong answers and high latency as perceived by users. This work aims to address the issue of passage re-trieval quality, proposing a novel passage ranking framework for QA that unifies traditional keyword-only ranking fea-tures with deeper linguistic and semantic features in a rank learning framework. Experimental results show that this generalized linguistic and semantic text similarity approach significantly outperforms a high-quality bag-of-words and named entity passage retrieval baseline on common classes of factoid questions, achieving better than a 20% increase in Mean Average Precision (MAP).
 For questions analyzable by ASSERT, the improvement in MAP offered by the proposed method can reach 40%. Cur-rently, fewer than half of the questions in the test collection can be analyzed by ASSERT. Despite the small sample size, the 40% improvement in MAP is statistically significant. For the questions that do not have ASSERT analyses, aver-age precision is not significantly penalized, so there is little risk in deploying the proposed passage retrieval method. As coverage is improved for semantic role labeling tools such as ASSERT, a greater percentage of user X  X  questions will be analyzable, which will result in even greater improvements in passage retrieval quality for a QA system employing the proposed method. The future potential for the proposed ap-proach notwithstanding, even the 20% improvement in MAP available using today X  X  NLP tools could make a substantial impact on the QA research community by enabling faster and more accurate Answer Generation that could result in lower end-to-end system latency and better accuracy that users would notice.
This material is based in part upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract Number W0853736. Any opinions, findings and conclusions or recommendations expressed in this ma-terial are those of the author(s) and do not necessarily re-flect the views of the Defense Advanced Research Projects Agency (DARPA).
