 There are several tools available that are designed to browse the large amount of data available on the Internet via basic text searches. Through the use of standard logical and relational operators, users formulate queries that are applied to the corpus of web data, returning a subset of results that meet their search criteria. However, these tools are limited and are typically performing keyword matching. In an attempt to further research along these lines, the Text Retrieval Conference (TREC) [1] introduced a tra ck called Related Entity Finding (REF), designed to extend the knowledge base in the areas of question answering, entity relationships and dedicated entity home pages. Motivated by the REF track, our research has led to the development of a search engine called NMiner.
The proposed system and the underlying algorithms depend on the extrac-tion of entities and their relationships. The Stanford NER tagger [3] is capable of identifying persons, organizations, and locations, and is used extensively by the research community for entity extraction purposes. In addition to relationships among documents in a corpus, the entities in documents exhibit relationships among themselves. Modeling relationships among entities eases the task of find-ing related entities.

NMiner allows end users to enter a query along with a target entity class and returns related entities for the given query. A sample query might ask for  X  X niversities with an NCAA Division I men X  X  lacrosse team X . NMiner uses two major components. One component indexes the unstructured web documents corpus from which candidate documents for question satisfaction would later be derived. The other component of the system processes the identified documents to obtain a ranked list of entity answers for a given query. The documents are processed using various third party tools for entity recognition purposes. The recognized entities are then ranked using principles of network analysis. We pro-pose two algorithms to achieve best ranking, they are Content-Centric Ranking (CCR) and Cumulative Structural Similarity (CSS). Experiments are run on the system using various queries and configurations. The results are compared against existing algorithms such as PageRank [5] and Hyperlink Induced Topic Search (HITS) [17], it is evident that NMiner offers a performance enhancement over current search techniques.

Key contributions of the work are as follows:  X  Developed a mechanism to identify valid entities using a consensus approach  X  Proposed a method to model sentence and entity relationships as a network.  X  Proposed new ranking methods, Content-Centric Ranking (CCR) and Cu- X  Evaluated the new ranking methods against existing ranking methods such  X  Implemented a complete system for finding related entities of a query Current state of art is presented in Section II, design details for NMiner are discussed in Section III, Network modeling and mining approaches are presented in Section IV, experiments followed by r esults and discussions in Section V and VI respectively. There have been several lines of research concerning Related Entity Finding -Linking Open Data (REF-LOD) in recent y ears. The application of data min-ing techniques to web-based data sources [2] have resulted in data collections useful in REF-LOD tasks. Ontologies such as DBpedia, EntityCube, KnowItAll, ReadTheWeb, and YAGO-NAGA have beco me integral to these efforts by pro-viding comprehensive repositories concerning entities [26]. These repositories consist of reliable information that can be easily accessed and extracted. Ma-chine learning based Natural Language Processing (NLP) tools have long been studied as part of Information Retrieval (IR) systems [23]. Named Entity Recog-nition (NER) is a subset of this research [24]. Meij et al. [21] used a combination of machine learning and IR tools to tie query-based mechanisms to the DBpe-dia data repository. Bonnefoy et al. [6] investigated using only web resources to determine with the finest granularity possible the type to which a given en-tity belongs to. Without a context, the r elationship between any two entities can be ambiguous [22]. Graphs can be used to help visualize these relationships [25]. Graphs also allow for the transmission of contextual information through directionality and weighting. Li and Cunningham [19] explained the advantages of using vector space models (VSMs) as a tool for information retrieval (IR) tasks. Mehler et al. [20] and Dehmer et al. [8] expanded on the concept of using VSMs for data representations by incorporating contextual information derived from the hypertext portion of a web document. Several tools have been devel-oped [18] that act as interfaces for extraction of information about Universal Resource Identifiers (URIs) of entities. Typically, IR systems must sort through a list of possible answers and provide a ranked list [10]. Elbassuoni et al. [9] used a Kullback-Leibler divergence calculation as a method for ranking the results re-turned from such a retrieval tool. A hybrid approach incorporating both context dependent and context independent data is proposed by Mirizzi et al. [7].
Besides the semantic based techniques, there are several network mining al-gorithms to rank vertices in a network. There is no participant (at least for our knowledge) who used network mining algorithms to find related entities in Clueweb09 dataset. PageRank [5] and HITS [17] are two of them and are used in this work to evaluate NMiner. The PageRank algorithm is widely recognized as being an effective tool for discovering and ranking the relationships among web pages and is the basis for the Google search engine. However, its primary method of ranking pages relies on the hyperlinks between pages and not the content of the pages themselves. This lack of contextual reference to the page content limits the effectiveness of the algorithm in searches requiring an exami-nation of the semantic association of the query terms. The HITS algorithm is a variation on PageRank. Like PageRank, HITS examines the hyperlinks between web pages. Unlike PageRank, the HITS algorithm attempts to identify authori-tative pages for a given search term by looking for hubs within the network that  X  X oin X  other pages together. While this variation does provide a more refined picture of the network structure, it still does not give enough detail to reliably answer queries involving semantic content. Unlike PageRank and HITS, in this paper we propose two algorithms in which one bases on content and other ranks a vertex based on structural information o f the vertex. Following section provides illustrative details of NMiner. There are two critical components in NMiner, one for candidate document re-trieval and the other is for processing the candidate documents. The first com-ponent indexes the unstructured web documents corpus from which candidate documents for question satisfaction would later be derived. The other compo-nent examines the structure of each of the candidate documents and identified entities contained therein. The senten ces in candidate documents and entities in the sentences are identified using various third party tools. A bi-modal network is constructed to represent the sentence and entity relationships.

Two novel networking mining algorithms, Content-Centric Ranking (CCR) and Cumulative Structural Similarity (CSS), are proposed to rank the entity vertices in the network. The algorithms proved to be significant in solving the problem of answering complex queries performed against a largely unstructured corpus of web documents. 3.1 Documents Retrieval The objective of this component is to retrieve relevant documents for a given topic/query. The implementation details of the component involves both corpus indexing and candidate documents retrieval.
 Corpus Indexing. The Clueweb09 dataset [12] is indexed using Indri software [16] for efficient retrieval. Since the dataset is too large to implement on a single machine, the dataset was segmented into 46 partitions (each partition with a set of raw web documents) and distributed across 46 processing platforms. Candidate Documents Retrieval. The candidate documents retrieval phase lever-ages the index built in the previous phase to retrieve query related documents. It involves two steps, topic analysis and document retrieval.
 Topics Analysis. A given query typically includes the query narrative and the class of the target entity and the given query is reformulated using topic analysis techniques. Our experiments included both manual and automated topic analysis techniques.
 Manual Query Reformulation. Given topic is converted into Indri Query Lan-guage by hand. Human intervention is considered advantageous for the following reasons;1. Refinement of generic references, 2. To maintain the integrity of multi-word phrases, 3. To introduce synonymic terms.
 Automated Query Reformulation. The automated approach involves complex processing of given topic narrative. The processing consists of the following steps:  X  Named entities are identified within the topic narrative.  X  Stop words are removed except for the term  X  X f X . The resultant phrase con- X  The query keywords are translated to form a weighted query in Indri Query 3.2 Documents Processing The documents obtained from the search are processed in an attempt to find answers to given topic. Figure 1 shows the steps involved in the document pro-cessing. The documents are parsed using a HTML parser obtained from [13] and to obtain text segments of the documents. A text segment is split into sentences using Apache Incubators OpenNLP sentence detection tool [14]. The lack of in-dividual performance of NER systems motivated us to use more than one entity identification technique. Rather than running the entity taggers in parallel, here they run in sequence. The sequence proce ssing improved accuracy by feeding dis-covered knowledge about entities to subsequent taggers. The Stanford NER tag-ger [3] is used for the first phase of entity recognition and the classes assigned by the Stanford NER tagger are ignored. Assuming all nouns in a sentence are enti-ties, Apache Incubator X  X  Parser [14] is used to recognize all noun phrases (includ-ing NP, NN, NNP, NNS, NNPS) in other words entities in given virtual sentence. The entity recognition systems in pre-vious steps introduce noise in terms of false positives (invalid entities) into the dataset. A simple noise cancella-tion is implemented by using an en-tity validation process. DBPedia [15] is chosen as our validation dataset. For faster access, the dataset is indexed us-ing Apache Lucene. The entities that are found in DBPedia and matches with target entity class are potential answers for given query. Each sentence is compared to given query and the comparison is measured using the sim-ilarity measure called relevance score .
 The score of a sentence is computed as the sum of the length of the query key words found in the sentence.

By the end of document processing, we have the following characteristics for each sentence collected from all of the related documents: sentence id, rele-vance score, target entities. 4.1 Network Construction This phase builds a sentence-entity network based on the relationships inher-ent in the document structure from which the data is derived. The network is bimodal but not bipartite, as there were relationships among sentences. The network is constructed using the following criteria.  X  Each sentence is a vertex.  X  Each entity is a vertex.  X  A sentence and an entity have an edge if the entity appeared in the sentence.  X  The weight of a sentence vertex is its associated relevance score.  X  Two sentences are connected if they co-occurred closely in the HTML tree 4.2 Network Mining We propose two novel network mining algorithms for ranking entity vertices in sentence-entity network.
 CCR Algorithm. The Content Centric Ranking (CCR) algorithm is specifi-cally tailored for the entity ranking purposes. As the name suggests, it leverages content of the network to find critical vertices. Domain knowledge such as rele-vance score is used for this purpose. As it is a bi-modal network, the algorithm follows different approaches to compute CCR score for each type of vertex. A CCR score of a sentence varies with vari ous parameters. The parameters are as follows.  X  The score of a sentence varies in proportion to its relevance score.  X  A large number of entities in a sentence leads to less chance of becoming an  X  More generic terms appear in the beginning and end of a document and a In summary, the score of a sentence vertex of the network can be written as
CCR score of a sentence = (relevance score / (countneighbor entities * count neighbor sentences )
A sentence with a high score is assumed to contain answer entities. However, an entity may occur in more than one sentence and can inherit scores from multiple sentence vertices. When there is more than one neighbor sentence for an entity vertex, the maximum value of all neighbor sentences is the entitys CCR score.
 CCR Score of an entity = max score of neighbor sentences The sentence from which the score is inherited is called a supporting sentence. The supporting sentence acts as evidence to support the validity of the entity as a potential answer for the query.

The CCR algorithm computes a CCR score for each entity and sentence in the network. The scores for entities are sorted to produce ranked answer entities. Cumulative Structural Similarity (CSS) Algorithm. This is the other algorithm proposed in this paper and can be applied to any undirected network. Similarity between any two network nodes measures how likely they are to share a neighbor [26,4]. There are various similarity measures; cosine, jaccard and minimum. The robustness of cosine similarity inspired us to consider it in this work. Given two vertices i and j, their similarity can be expressed as . The Cumulative Structural Similarity (CSS) of a vertex is the sum of similarity scores with its neighbors. The CSS of a vertex represents an accumulated similarity with its neighbors. A CSS value was calculated for each vertex in the sentence-entity network by considering it as a single-mode network. There was no difference in processing CSS values for sentence and entity nodes. The CSS values from all the entities are s orted to return a ranked list of entities. Queries from the TREC 2011 competition for the REF/REF-LOD track are used as input for our experiments. There are 50 queries in all and they are from differ-ent domains such as medical, political, educational, etc. For each query, various configurations in the system includes different search engines, different query reformulation techniques, and different result ranking algorithms are tested. Bing. This approach is similar to the nave configuration with the addition of the Microsoft X  X  Bing search engin e as a replacement for Indri.
 Results Ranking. In addition to the various search strategies, we also tested four different ranking algorithms in each configuration. The four ranking algo-rithms are: CCR, CSS, PageRank, and HITS. Therefore, for each query, we have 16 experiments. In each experiment, the outcome is a list of answer entities sorted by their ranks. It is not feasible to manually evaluate the outcome from the experiments, so widely use evaluati on measures such as nDCG (Distributed Cumulative Gain at n) and MAP (Mean Average Precision) and are computed on the result sets.
 feasible to plot charts for all 50 queries for this paper, so the evaluation mea-sures are summarized. The summary is in two phases. One is to find the best topic analysis mechanism and the other is to find the appropriate network algorithm.
 ogy of the network. In contrast to PageRank and HITS, CSS is a local structural measure thus finding best answers around given query keywords. The robustness of the cosine similarity adds stability to CSS. Also, the CSS value of a vertex (entity or sentence) is obtained from 2-hop structural information of the network starting from the vertex where PageRank and HITS values are computed from global structural information. The increased granularity of this scoring method provides a more accurate picture of the entitys relationship with its neighbors.
While PageRank performs well on the manual ground truth, the CSS algo-rithm exhibits the best performance in each case. However, the CSS algorithm is computationally costly due to the similarity calculations (i.e. the dot product of two vertex vectors) when compared to CCR. Even so, is typically faster than PageRank or HITS. CCR, on the other hand, is a relatively light-weight data mining tool. Its performance is lower than the other algorithms, but not pro-hibitively so. From these tests, we surmise that the CCR algorithm is good for limited computational resource environments and CSS performs best if enough computational resou rces are available.

The running time of the NMiner for a query highly depends on document search system. In our experiments, we used a Linux box of 8 core processor with 16GB RAM. A query took 25 seconds on average with distributed Indri search engine while the same took 45 seconds on average using Bing as this configuration involves crawling the documents from web. The query processing time can be reduced by keeping NLP models in memory as most of the query processing time involved NLP models loading. The primary aim of this research is to answer a given question with a ranked list of entities. This paper has detailed a novel methodology for identifying enti-ties and ranking them in context to a given query. With the advantage of third party parsers like HTML parser and sent ence parser, we are able to extract se-mantic relationships from among the entities and sentences in each document. These relationships are used to const ruct a bi-modal network which was then used to discover and rank target entities. The proposed Content-Centric Rank-ing (CCR) and Cumulative Structural Similarity (CSS) algorithms for ranking entities exhibited the best accuracy in comparison to PageRank and HITS rank-ing techniques because prior one is domai n based and later is local measure. The experiments also proved that the CSS algorithm is best in ranking entities pro-vided sufficient resources are available. Th is work reiterates that network mining algorithms can address complex problems with simple solutions.
 Acknowledgments. This work was supported in part by the National Sci-ence Foundation under Grant CR I CNS-0855248, EPS-0701890, EPS-0918970, MRI CNS-0619069, and OISE-0729792, US Office of Naval Research (Award: N000141010091) and the US NSF (Awards: IIS-1110868 and IIS-1110649).
