 Filippo Furfaro  X  Giuseppe M. Mazzeo  X  Domenico Sacc X   X  Cristina Sirangelo Abstract Hierarchical binary partitions of multi-dimensional data are investigated as a basis for the construction of effective histograms. Specifically, the impact of adopting loss-less compression techniques for representing the histogram on both the accuracy and the efficiency of query answering is investigated. Compression is obtained by exploiting the hierarchical partition scheme underlying the histogram, and then introducing further restric-tions on the partitioning which enable a more compact representation of bucket boundaries. Basically, these restrictions consist of constraining the splits of the partition to be laid onto regular grids defined on the buckets. Several heuristics guiding the histogram construction are also proposed, and a thorough experimental analysis comparing the accuracy of histograms resulting from combining different heuristics with different representation models (both the new compression-based and the traditional ones) is provided. The best accuracy turns out from combining our grid-constrained partitioning scheme with one of the new heuristics. Histograms resulting from this combination are compared with state-of-the-art summariza-tion techniques, showing that the proposed approach yields lower error rates and is much less sensitive to dimensionality, and that adopting our compression scheme results in improving the efficiency of query estimation.
 Keywords Data reduction  X  Multi-dimensional data  X  Histograms  X  Query processing 1 Introduction The need to compress data into synopses of summarized information often arises in many scenarios, where the aim is to retrieve aggregate data efficiently, possibly trading off the computational efficiency with the accuracy of query answers.

Selectivity estimation for query optimization in RDBMSs [ 5 , 21 ], range query answering in OLAP services [ 23 , 26 ], statistical and scientific data analysis [ 15 ], window query ans-wering in spatial databases [ 1 , 2 , 17 ], are examples of application contexts where efficiently aggregating data within specified ranges of the domain is such a crucial issue, that high accuracy in query answers becomes a secondary requirement.

For instance, query optimizers in RDBMSs [ 28 ] can build an effective query evaluation plan by estimating the selectivity of intermediate query results, which can be accomplished by retrieving aggregate information on the frequencies of attribute values. Obviously, the execution plan for a given query is effective only if it can be computed efficiently, that is if building the plan takes much less time than answering the query itself: thus fast computation of aggregations is mandatory. Moreover, a dramatic precision in evaluating aggregates is not needed, as knowing the order of magnitude of the selectivity of intermediate queries suffices to build an effective execution plan.

Likewise, in Decision Support Systems [ 20 ], fast answers to OLAP range queries are mandatory to provide useful and timely reports. In this context, pieces of information are represented as points in a multi-dimensional space, where dimensions define different pers-pectives for viewing data. This representation model is suitable to support data exploration, as users can navigate information and retrieve aggregate data by simply specifying the ranges of the data domain they are interested in. In fact, DSS users are often concerned with perfor-ming preliminary explorations of the data domain, to find the portions where a more detailed analysis is needed. In this scenario, high accuracy in less relevant digits of query answers is not needed, as providing their order of magnitude suffices to locate the regions of the database containing relevant information. At the same time, fast answers to these preliminary queries allow users to focus their explorations quickly and effectively, thus saving large amounts of system resources.

A widely accepted approach to the problem of providing fast answers to aggregate queries consists of compressing data into lossy synopses, and executing range queries over compres-sed data rather than over raw ones. Using this approach, answering range queries can be accomplished more efficiently (as the amount of data to be accessed is much smaller), al-though some approximation is introduced, due to the lossy nature of adopted compression strategies. Otherwise, computing exact answers could take a very long time (as it could re-quire large volumes of disk-resident data to be accessed and elaborated), so that the efficiency requirements of the application scenarios cited above would not be met.

Histograms [ 10 , 12 ] are a widely used approach for compressing multi-dimensional data and supporting fast query answering. A histogram over a multi-dimensional data distribution is built by partitioning the data domain into a number of hyper-rectangular blocks (called bu-ckets ), and then storing summary information for each block. The answer of a range query is estimated on the histogram by aggregating the contributions of all buckets, without accessing the original data. That is, every bucket overlapping the query range is located and its contribu-tion to the query answer is evaluated by performing suitable interpolation on its summary data.
As expected, on the one hand, querying the histogram rather than the underlying original data reduces the cost of evaluating answers (as the histogram size is much less than the original data size); on the other hand, the loss of information due to summarization introduces some approximation when queries are estimated on the histogram. Therefore, a crucial issue when dealing with histograms is finding the partition which provides the  X  X est X  accuracy in reconstructing query answers.

In order to better explain the main contributions of this paper, in the following section we describe in more detail the notion of histogram, and focus our attention on well-established results as well as challenging issues of this research area. 1.1 Related work Histograms were originally proposed in [ 14 ] in the context of query optimization in relational databases. Query optimizers compute efficient execution plans on the basis of the estimation the joint frequency distribution [ 21 ] associated with R . The latter can be viewed as a d -dimensional array F whose dimensions represent the attribute domains, and whose cell with coordinates &lt;v 1 ,...,v d &gt; stores the number of tuples of R where A 1 = v 1 ,..., A d = v . The selectivity of the query Q defined above is the sum of the frequencies contained in the multidimensional range [ v 1 ..v 1 ] ,..., [ v d ..v d ] of F . In this scenario, histograms were introduced to summarize the joint frequency distribution in order to make selectivity estimation more efficient [ 5 ], initially in the one-dimensional case [ 14 ] (that is, on the one-dimensional array representing the frequency distribution of a single attribute), and then in the multi-dimensional scenario [ 21 ].

A histogram on a multi-dimensional data distribution consists of a set of non-overlapping buckets (hyper-rectangular blocks) partitioning the overall domain. Each bucket is associated with a multi-dimensional range of the domain and stores aggregate information summarizing the values inside this range. The meaning of the aggregate value associated with each bucket depends on the data distribution on which the histogram is constructed. For instance, when the histogram is constructed on a joint frequency distribution to support selectivity estimation, the value associated with a bucket represents the number of tuples whose attribute values are inside the range of the bucket. Likewise, when the histogram is constructed on an OLAP datacube, each bucket can store either the maximum value, or the minimum value, or the sum of the values within the corresponding multidimensional range.

Aggregation queries issued on the original data distribution can be estimated on the his-togram by exploiting the aggregate values stored in its buckets. For instance, if histogram buckets contain sums, a range-sum query over the multi-dimensional range r (denoted as Sum ( r ) ) can be evaluated on the histogram by summing the values stored in the buckets whose boundaries are completely contained inside r , and then by estimating the  X  X ontri-butions X  of the buckets which partially overlap the query range. These contributions are evaluated by performing linear interpolation, under the assumption that the data distribution inside each bucket is  X  X omogeneous X  (that is, the value of every cell in the multi-dimensional range corresponding to the bucket coincides with the mean value in the bucket). 1 We focus on range-sum queries as they are relevant in the selectivity estimation context: evaluating the selectivity of the query Q defined above is equivalent to computing the range-sum query Sum [ v 1 ..v 1 ] ,..., [ v d ..v d ] on F .

The effectiveness of a histogram (built in a given storage space bound) is measured by evaluating the accuracy of estimating queries on it. As queries are estimated by performing linear interpolation on the aggregate values associated with buckets, the more homogeneous the distribution inside the buckets involved in the query, the better the accuracy of the es-timation. Therefore, the effectiveness of a histogram depends on the underlying partition of the data domain. In [ 19 ] the authors present a taxonomy of different classes of parti-tions: arbitrary , hierarchical and grid-based partitions. Grid-based partitions are built by dividing each dimension of the underlying data into a number of ranges, thus defining a grid on the data domain: the buckets of the histogram correspond to the cells of this grid. Hierarchical partitions are obtained by recursively partitioning blocks of the data domain into non-overlapping sub-blocks. Finally, arbitrary partitions have no restriction on their structure. Obviously, arbitrary partitions are more flexible than hierarchical and grid-based ones, as there are no restrictions on where buckets can be placed. Unfortunately, building the  X  X ost effective X  multi-dimensional histogram based on an arbitrary partition (called V-Optimal [ 10 ]) was shown to be a NP-Hard problem, even in the two-dimensional case [ 19 ]. Therefore, several techniques for building effective histograms (which can be compu-ted more efficiently than the V-Optimal one) have been proposed. Most of these techniques adopt greedy approaches and are not based on arbitrary partitions. In particular, MHIST [ 21 ] and MinSkew [ 2 ] use a hierarchical-partitioning strategy to construct the histogram: they differ on the greedy criterion adopted to choose and split a bucket at each step of the histo-gram construction. Indeed, these two techniques represent buckets by storing their Minimal Bounding Rectangles (MBR), i.e., their minimal hyper-rectangular portions containing all their non-null values. This implies that MHIST and MinSkew histograms do not define a partition of the data domain in the strict sense, as some null regions can be possibly covered by no bucket of the histogram. In [ 6 ], a space efficient representation of MHIST histograms buckets. This representation model does not represent bucket boundaries by means of MBRs, and it can be viewed as the first proposal exploiting the hierarchical partitioning for reducing the storage space consumption of the histogram.

Also STHoles [ 3 ]and GENHIST [ 9 ]histogramsdonotdefineapartitionofthedatadomain, as they allow bucket nesting and overlapping. In particular, STHoles histograms are not constructed by examining the data distribution, but by using query results feedback to refine bucket definition. However, this makes both the time needed to construct an histogram and the accuracy of the histogram strictly depend on the query workload. As regards GENHIST , further details are given in Sect. 4.7 , where this technique is compared with our proposal.
Traditional multi-dimensional histograms can be adapted to support selectivity estimation in spatial databases, where objects have an extent in space. As shown in [ 2 ], this can be done by storing in each bucket the number of objects whose center lies inside it, as well as the average width and height of the MBRs of the contained objects. Then, the selectivity of a spatialselection(alsoknownas window query )isestimatedbyperforminglinearinterpolation on the count value associated with buckets overlapping the window range. In more detail, in order to take into account the extent of objects, interpolation is performed after the edges of the query range are suitably enlarged according to the average values of width and height stored in the buckets overlapping the query window.

More specific approaches to the problem of summarizing spatial data to support the estima-tion of spatial queries have been proposed in [ 1 , 16 , 17 , 25 ]. Among these works, Aboulnaga and Naughton [ 1 ] is the most related to this paper, as it proposes a histogram construction technique based on a hierarchical partition of the data domain. Specifically, the histogram construction is accomplished in three steps. First, a complete quad-tree partition of the data domain is constructed, and each object is associated with the quad-tree node whose volume is the closest to the object extent w.r.t. all the nodes containing the center of the object. Thus, the first step aims at classifying the spatial objects according to their extension and position. Then, the quad-tree is progressively compressed by repeatedly merging the four sibling nodes with the least variation in data distribution (in terms of difference in the number of objects associated with the nodes), until the quad-tree representation fits in the assigned memory. Finally, buckets are built as follows: for each node of the compressed quad-tree, a bucket is defined whose boundaries are those of the MBR including all the objects associated with the node. Hence, the buckets of the histogram may overlap and each bucket does not exactly correspond to a region represented by a quad-tree node. Thus, the quad-tree cannot be used as an index to search the buckets overlapping a given query window. This is a substantial difference from the approach investigated in this paper, where the hierarchy guiding the his-togram construction will be embedded in the histogram representation and exploited as an indexing mechanism for efficiently locating the buckets involved in a given query.
Other approaches to the problem of summarizing multi-dimensional data are the wavelet-based ones. Wavelets are mathematical transformations implementing a hierarchical decom-position of functions. They were originally used in different research and application contexts (like image and signal processing [ 13 , 24 ]), and have been recently applied to selectivity es-timation [ 18 ] and to the approximation of OLAP range queries over data cubes [ 26 , 27 ]. The compressed representation of a data distribution is obtained in two steps. First, a wavelet transformation is applied to the data distribution, and N wavelet coefficients are generated (the value of N depends both on the size of the data and on the particular type of wavelet transform used). Next, among these N coefficients, the m &lt; N most  X  X ignificant X  ones are selected and stored. Issuing a query on the compressed representation of the data essentially corresponds to applying the inverse wavelet transform to the stored coefficients, and then aggregating the reconstructed (approximate) data values. Several approaches to the problem of selecting the most effective m terms approximating the original data set have been propo-on a probabilistic framework [ 7 ] or a more complex deterministic thresholding method [ 8 ] providing error guarantees. 1.2 Main contribution Existing histogram-based summarization techniques do not scale up (in terms of accuracy) to high-dimensionality scenarios. Indeed, some of them were intended to work with data having a specific dimensionality (such as the technique proposed in our previous work [ 4 ], for two-dimensional data). The other techniques, although they were proposed to deal with generic multi-dimensional scenarios, provide satisfiable accuracy only in the low-dimensional case. This is the well-known curse of dimensionality : as the number of dimensions increases, the number of buckets needed to achieve a satisfiable degree of accuracy requires a larger and larger amount of storage space for being stored.

This work is an effort in this direction. We study hierarchical binary partitions as a basis for effective multi-dimensional histograms, focusing our attention on two aspects which turn out to be crucial for histogram accuracy: the representation model and the strategy adopted for partitioning data into buckets. As regards the former, we propose a very specific space-efficient representation model (alternative to MBRs) where bucket boundaries are represented implicitly by storing the partition tree. Histograms adopting this representation model (which will be said to be Hierarchical Binary Histograms X  HBH ) can store a larger number of buckets within a given amount of memory w.r.t. histograms using a  X  X lat X  explicit storage of bucket boundaries. On top of that, we consider the introduction of a constraint on the hierarchical partition scheme, allowing each bucket to be partitioned only by splits lying onto a regular grid defined on it: histograms adopting such a constrained partitioning paradigm will be said to be Grid Hierarchical Binary Histograms ( GHBH ). We show how the introduction of the grid-constrained partitioning of GHBH s can be exploited to further enhance the physical representation efficiency of HBH s. As regards the construction of effective partitions, we introduce some heuristics guiding the data summarization by locating inhomogeneous regions of the data where a finer-grain partition is needed.
The two physical representation schemes adopted by HBH and GHBH can be viewed as a form of lossless compression to be used on top of the summarization accomplished by histograms (which is a form of lossy compression). We show that combining these forms of compression can result in a relevant improvement of histograms effectiveness. On the one hand, our compression-based representation models provide a mechanism for efficiently locating the buckets involved in query estimation, thus reducing the amount of time needed to estimate queries w.r.t. traditional flat representation models. On the other hand, applying lossless compression on top of summarization reduces the loss of information due to sum-marization, as it enables a larger amount of summary data to be stored within a given storage space bound: this turns out to yield lower error rates of query estimates.

By means of experiments, we provide a thorough analysis of different classes of his-tograms based on hierarchical partitions: we study the accuracy provided by combining different heuristics (both our new proposals, as well as the  X  X lassical X  heuristics of MHIST and MinSkew) with either the traditional MBR-based representation model or our specific tree-based ones (both the unconstrained and the grid-constrained one). These results provide an insight into the value of compression in the context of histograms based on hierarchical partitions. Interestingly, we show that the impact of both HBH and GHBH representation models on the accuracy of query estimates is not simply orthogonal to the adopted heuristic. Thus, we identify the best combination of these different features, which turns out from adopting the grid-constrained hierarchical partitioning of GHBH s guided by one of the new heuristics.

Finally, we compare this class of GHBH with state-of-the-art techniques (MHIST, MinS-kew, GENHIST, as well as other wavelet-based summarization approaches [ 26 , 27 ]), showing that our technique results in much lower error rates and satisfiable degree of accuracy also at high-dimensionality scenarios. 2 Histograms based on binary partitions 2.1 Binary partitions Throughout the paper, a d -dimensional data distribution D is assumed. D will be treated as a multi-dimensional array of integers with volume n d (without loss of generality, we assume that all dimensions of D have the same size). The number of non-zero elements of D will be denoted as N .A range  X  i on the i -th dimension of D is an interval [ l .. u ] , such that 1  X  l  X  u  X  n . Boundaries l and u of  X  i are denoted by lb ( X  i ) ( lower bound )and ub ( X  i ) A block b (of D )isa d -tuple  X  1 ,..., X  d where  X  i is a range on the dimension i , for each 1  X  i  X  d . Informally, a block represents a  X  X yper-rectangular X  region of D . A block b of D with all zero elements is said to be a null block . The volume of a block b =  X  1 ,..., X  d is given by size ( X  1 )  X  X  X  X  X  size ( X  d ) and will be denoted as v ol ( b ) . Given a point in the multidimensional space x = x 1 ,..., x d ,wesaythat x belongs to the block b (written x  X  b )if lb ( X  i )  X  x i  X  ub ( X  i ) for each i  X  X  1 .. d ] . The sum of the points inside b will be denoted as sum ( b ) .

Given a block b =  X  1 ,..., X  d ,let x be a coordinate on the i -th dimension of b such b b along the dimension i at the position x ;dimension i and coordinate x are said to be the splitting dimension and the splitting position , respectively.

Informally, a binary partition can be obtained by performing a binary split on D (thus generating the two sub-blocks D lo w and D high ), and then recursively partitioning these two sub-blocks with the same binary hierarchical scheme.
 Definition 1 Given a d -dimensional data distribution D with volume n d ,a binary partition BP of D is a binary tree such that: 1. the root of BP is the block [ 1 .. n ] ,..., [ 1 .. n ] ; 2. for each internal node p of BP the pair of children of p is a binary-split of p .
The root, the set of nodes, and the set of leaves of the binary partition BP will be denoted, respectively, as Root ( BP ) , Nodes ( BP ) ,and Lea v es ( BP ) . An example of binary partition on a two-dimensional data distribution is shown in Fig. 1 . 2.2 Flat binary histograms As introduced in Sect. 1.1 , several techniques proposed in literature, such as MHIST and MinSkew, use binary partitions as a basis for building histograms. In this section, we provide a formal abstraction of classical histograms based on binary partitions. We refer to this class as Flat Binary Histograms , to highlight the basic characteristic of their physical representation model. The term  X  X lat X  means that, classically, buckets are represented independently from one another, without exploiting the hierarchical structure of the underlying partition. Definition 2 Given a d -dimensional data distribution D and a binary partition BP on D ,the Flat Binary Histogram on D based on BP is the set of pairs: where the set { b 1 ,..., b  X  } coincides with Lea v es ( BP ) .
 In the following, given the flat binary histogram: the blocks b 1 ,..., b  X  will be said to be the buckets of FBH ,andtheset { b 1 ,..., b  X  } will be denoted as Buckets ( FBH ) .
Figure 2 shows how the two-dimensional flat binary histogram corresponding to the binary partition of Fig. 1 can be obtained by progressively performing binary splits. The histogram consists of the following set:
A flat binary histogram can be represented by storing, for each bucket of the partition, both its boundaries and the sum of its elements. In order to exploit as much as possible the available storage space, usually the boundaries of the buckets are not represented explicitly (this would require 2  X  d 32-bit words for each bucket): the data domain is linearized and the boundaries of the bucket b =  X  1 ,..., X  d are translated into the one-dimensional coordinates in this li-i.e., the nearest and the farthest corner of the bucket w.r.t. position 0 ,..., 0 , respectively. The amount of storage space needed to represent p 1 and p 2 depends on the size of the data domain. In the following, we will denote as  X  the number of 32-bit words needed to en-code the coordinates of a multi-dimensional point adopting the above-explained strategy. 2 Therefore, the amount of storage space needed to store an FBH with  X  buckets is given by: size ( FBH ) = 32  X  ( 2  X   X  + 1 )  X   X  .

MHIST and MinSkew algorithms use a different representation of buckets: instead of storing the ranges delimiting the leaves of the binary partition, they store, for each leaf, the coordinates of its MBR ( minimal bounding rectangle , see Sect. 1.1 ). For instance, consider the case that D is a two-dimensional data distribution with two points in it, placed at the ends of a diagonal. According to this representation model, splitting D will lead to two single-point MBRs. With respect to the naive representation model introduced above for FBH ,this aims at a higher accuracy in approximating D , and introduces no spatial overhead. In fact, representing the coordinates of the MBR inside a bucket b has the same cost as representing the boundaries of b , but the information provided by the MBR on where nonnull elements are located inside b is more accurate. In Sect. 2.3 , we propose an alternative representation scheme, which does not enable MBRs to be stored, but allows bucket boundaries to be represented more efficiently, so that a larger number of buckets can be stored within the same storage space bound. 2.3 Hierarchical binary histogram The simple FBH representation paradigm defined in Sect. 2.2 does not exploit the hierarchical nature of the partition. In particular, the boundaries of two buckets of an FBH corresponding to a pair of siblings in the underlying binary partition could be represented by representing only the boundaries of their father together with the splitting position and dimension ge-nerating them. We expect that exploiting this characteristic improves the efficiency of the representation.

As for split trees in [ 6 ], the idea underlying Hierarchical Binary Histogram consists of storing the partition tree explicitly, in order to both avoid the explicit storage of bucket boundaries and provide a structure indexing buckets. In particular, storing the structure of the partition enables the boundaries of the buckets (which correspond to the leaves of the partition tree) to be retrieved from the partition itself. Moreover, as storing the partition tree is less costly (in terms of amount of storage space) than storing bucket boundaries (as it will be explained in the following), some storage space can be saved and invested to obtain finer grain buckets.
 Definition 3 Given a d -dimensional data distribution D ,a Hierarchical Binary Histogram on D is a pair HBH ( D ) = P , S where P is a binary partition of D ,and S is the set of pairs { p , sum ( p ) | p  X  Nodes ( P ) } .

In the following, given a hierarchical binary histogram HBH = P , S ,theterm Nodes ( HBH ) will denote the set Nodes ( P ) , whereas Buckets ( HBH ) will denote the set Lea v es ( P ) .
 A hierarchical binary histogram HBH = P , S can be stored efficiently by representing P and S separately, and by exploiting some intrinsic redundancy in their definition. To store of P correspond to ranges of the multi-dimensional space, some information describing the boundaries of these ranges has to be stored. This can be accomplished efficiently by storing, for each non-leaf node, both the splitting dimension and the splitting position which define the ranges corresponding to its children. Therefore, each nonleaf node can be stored using a string of bits, having length 32 + log 2 d + 1, where 32 bits are used to represent the splitting position, log 2 d to represent the splitting dimension, and 1 bit to indicate that the node is not a leaf. On the other hand, 1 bit suffices to represent leaf nodes, as no information on further splits needs to be stored. Therefore, the partition tree P can be stored as a string of bits (denoted as Ar r ay P ( HBH ) ) consisting of the concatenation of the strings of bits representing P nodes.

The pairs p 1 , sum ( p 1 ) ,..., p m , sum ( p m ) of S (where m =| Nodes ( HBH ) | ) can be represented using an array containing sum ( p 1 ),..., sum ( p m ) , where the sums are stored according to the ordering of the corresponding nodes in Ar r ay P ( HBH ) .Indeed,itisworth noting that not all the sum values in S need to be stored, as some of them can be derived. For instance, the sum of every right-hand child node is implied by the sums of its parent and its sibling. Therefore, for a given hierarchical binary histogram HBH ,theset Nodes ( HBH ) can be partitioned into two sets: the set of nodes that are the right-hand child of some other node (which will be called derivable nodes ), and the set of all the other nodes (which will be called non-derivable nodes ). Derivable nodes are the nodes which do not need to be explicitly represented as their sum can be evaluated from the sums of non-derivable ones. The sums associated to non-derivable nodes are stored into the array Ar r ay S ( HBH ) .

The application of this representation paradigm to the HBH shown on the left-hand side of Fig. 3 is shown on the right-hand side of the same figure.

In Fig. 3 non-derivable nodes are colored in grey, whereas derivable nodes are white. Leaf nodes of HBH are represented in the array Ar r ay P ( HBH ) by means of a unique bit, with value 0. As regards non-leaf nodes, the first bit of their representation is 1 (meaning that these nodes are split); the second bit is 0 if the node is split along the horizontal dimension, 1 otherwise.

This representation scheme can be made more efficient by exploiting the possible sparsity of the data. In fact, it often occurs that the size of the multi-dimensional space is large w.r.t. the number of non-null elements. Thus, we expect that null blocks are very likely to occur when partitioning the multi-dimensional space. This leads us to adopt an ad-hoc compact representation of such blocks in order to save the storage space needed to represent their sums. A possible efficient representation of null blocks could be obtained by avoiding storing zero sums in Ar r ay S ( HBH ) and by employing one bit more for each non-derivable node in Ar r ay P ( HBH ) to indicate whether its sum is zero or not. Moreover, observe that we are not interested in HBH s where null blocks are further split since, for a null block, the zero sum provides detailed information of all the values contained in the block, thus no further investigation of the block can provide a more detailed description of its data distribution. Therefore, any HBH can be reduced to one where each null node is a leaf, without altering the description of the overall data distribution that it provides. It follows that in Ar r ay P ( HBH ) non-leaf nodes do not need any additional bit either, since they cannot be null. According to this new representation model, each node in Ar r ay P ( HBH ) is represented as follows:  X  if the node is not a leaf it is represented using a string of length 32 + log 2 d + 1 bits,  X  if the node is a leaf, it is represented using one bit to state that the node has not been split On the other hand Ar r ay S ( HBH ) represents the sums of all non-null non-derivable nodes. A possible representation of the HBH shown on the left-hand side of Fig. 3 according to this new model is provided in Fig. 4 . In particular, both non-leaf nodes and derivable leaf nodes are stored in the same way as in Fig. 3 , whereas non-derivable leaf nodes are represented with a pair of bits. The first one of these has value 0 (which states that the node has not been split), and the second one is either 0 or 1 to indicate whether the node is null or not, respectively. Remark Observe that the physical representation model introduced above cannot be used to represent the coordinates of the MBRs inside buckets. This is due to the fact that MBRs of two sibling nodes of a binary partition in general do not coincide with node boundaries, because the two partitions can be shrunk to eliminate any null spaces around. This means that our approach can be considered an alternative to the idea of storing MBRs. 2.4 Grid hierarchical binary histogram In the previous section it has been shown how the binary partition scheme underlying a hierarchical binary histogram can be exploited to reduce the amount of memory needed to represent bucket boundaries. We now introduce further constraints on the partition scheme adopted to define the boundaries of the buckets: the basic idea is that the use of a constrained partitioning enables a more efficient physical representation of the histogram w.r.t. histograms using more general partition schemes.

Basically, a Grid Hierarchical Binary Histogram ( GHBH ) is a hierarchical binary histo-gram whose internal nodes cannot be split at an arbitrary position of any dimension: every split of a block is constrained to be laid onto a grid, which divides the block into a number of equal-size sub-blocks. This number is a parameter of the partition, and it is the same for every block of the partition tree. In the following, a binary split of a block b = &lt; X  1 ,..., X  d &gt; along the dimension i at the position x will be said to be a binary split of degree k if x = lb ( X  i ) + j  X  size ( X  i ) Definition 4 Given a d -dimensional data distribution D ,a grid binary partition of degree k on D is a binary partition GBP of D such that, for each non-leaf node p of GBP , the pair of children of p is a binary-split of degree k on p .
 Definition 5 Given a d -dimensional data distribution D ,a Grid Hierarchical Binary Histo-gram of degree k on D is a hierarchical binary histogram k-GHBH = P , S on D where P is a grid binary partition of degree k on D .

In the following, we will use GHBH as an acronym of grid hierarchical binary histogram without specifying the degree k of the partition when k is not relevant. Figure 5 shows an example of two-dimensional 4-GHBH .

Constraining each split of the partition to be laid onto a grid defined on the blocks of the histogram enables some storage space to be saved to represent the splitting position. In fact, bits, instead of 32 bits. In the following, we will consider degree values which are a power of 2, so that the space consumption needed to store the splitting position will be simply denoted as log 2 k . Figure 6 shows the representation of the grid hierarchical binary histogram of Fig. 5 .
 2.5 Usage of storage space We now compare the effectiveness of the different physical representation models by evalua-ting the number of buckets of a histogram H of type FBH ,or HBH or GHBH saturating the available storage space B . In the following, given a storage space bound B , a histogram H will be said to be B -maximal if size ( H )  X  B and no split can be performed on any bucket of H , otherwise the storage space consumption of H would exceed B .
 Proposition 1 Let D be a d -dimensional data distribution, B a storage space bound, and T a type of histogram ( where T is either FBH , HBH or k-GHBH ) . The number of buckets  X 
T of a B-maximal histogram H of type T on D is in the range [  X  min T .. X  max T ] where  X  min T and T are reported in Table 1 .
 Proof See Appendix.

Observe that while all possible B -maximal histograms of type FBH have the same number of buckets (for a given B and a given size of the data domain), this does not hold for HBH and GHBH . This is due to the fact that the buckets of an HBH (or, equivalently, a GHBH ) have a different storage space consumption depending on the underlying data distribution.
Comparing the ranges defining the possible number of buckets of the different types of histogram, the main conclusion that can be drawn is that the physical representation scheme adopted for an HBH permits us to store a larger number of buckets w.r.t. an FBH within the same storage space bound, as the denominator of  X  min HBH (i.e., 67 + log 2 d )islessthan the denominator of  X  FBH (i.e., 32  X  ( 2  X   X  + 1 ) )evenfor  X  = 1. Analogously, the constraint on the splitting position of a GHBH further increases the number of buckets that can be represented within B , as we can assume that log 2 k &lt; 32, thus 67 &gt; 35 + log 2 k .
In order to give an idea of the benefits (in terms of number of buckets) introduced by the efficient representation models of HBH and GHBH , consider the case of a data distribution with eight dimensions. In this scenario, it is likely that  X  is at least 2 (as  X  = d  X  log 2 n 32 , a single 32-bit word suffices to encode the coordinates of the data domain only if n  X  16, that is a rather specific case). Therefore, the number of buckets of an HBH is between 2 and 4 times the number of buckets of a FBH on the same data; if we consider the case of 8-GHBH , the number of buckets is between 4 and 18 times that of an FBH on the same data. In Sect. 4.8 (diagrams 13 (d,h)) we will provide some experiments comparing the number of buckets stored by the different classes of histogram within the same storage space bound. 2.6 Estimating query selectivity In order to study in more detail the impact of the hierarchical representation on the efficiency of estimating queries, we first show how queries are estimated on HBH and GHBH .
Let q be a query defined on the range r ,and Sum ( r ) be the range-sum query asking for the selectivity of q . Basically Sum ( r ) is estimated by navigating the partition tree of the histogram and summing the contributions of the nodes which overlap the query range. The visit starts from the root, which corresponds to the whole data domain. When a node u is being visited, three cases may occur: 1. the range corresponding to u is external to r : no contribution to the estimate is given by 2. the range corresponding to u is entirely contained into r : the contribution of u is Sum ( u ) , 3. the range corresponding to u partially overlaps r :if u is a leaf, linear interpolation is
The volume of the range corresponding to each node is evaluated on the basis of the boundaries associated with the node. These boundaries are computed during the navigation, according to the splitting position and the splitting dimension stored in the representation of the parent node.

According to the linearized representation of the binary partition adopted in our approach, the encoding of each node u in Array P is followed by the encoding of the whole tree rooted at from u , linearization results in no mechanism for navigating directly from a node u to its have to skip the bit string corresponding to u left . As the length of the encoding of the sub-tree rooted at u left is not fixed (since it depends on the depth of the sub-tree), we have to scan the encoding of this sub-tree in order to reach the position in Array P where the encoding of u right starts from. However, scanning the representation of the sub-tree rooted at u left results in an overhead only if u left does not contain any node overlapping the query range (otherwise, we should scan it to compute its contribution to the query estimate anyhow). Indeed, even if this is the case, the cost of accomplishing this scanning is unlikely to slow down the query estimation time significantly. In fact, we can rapidly scan the tree rooted at u left by accessing only the bits encoding its structure, that is the bits saying whether the corresponding nodes are leaves or not. Consider two nodes u 1 , u 2 such that u 2 immediately follows u 1 according to the prefix visit of the partition tree. Then the distance between the encodings of u 1 and u 2 is exactly the length of the bit encoding of u 1 . This length is a constant depending on whether u 1 is a leaf or an intermediate node. That is, if u 1 is a leaf this distance can be either 1 or child of its parent node (see nodes D . 1and D . 2inFig. 6 ). Otherwise, if u 1 is an intermediate node, the distance between the starting points of the encodings of u 1 and u 2 in Array P is equal to the length of the encoding of the splitting position and the splitting dimension of u 1 (i.e., log 2 d + 32 for HBH ,and log 2 d + log 2 k for GHBH ). Therefore, when the tree rooted at u left does not overlap the query range, we can scan its encoding by skipping the fragments of Array P encoding the splitting positions and the splitting dimensions of its nodes. The bits encoding the splitting position of nodes must be unpacked only for those nodes giving a non-null contribution to the query estimate, that is those nodes belonging to branches of the partition tree overlapping the query range.

Therefore, a histogram of type HBH or GHBH can be navigated by rapidly scanning the branches of the partition tree corresponding to portions of the data domain which are not involved in the query, and executing the complete unpacking of node boundaries only for those branches overlapping the query range. Moreover, observe that storing the sums at intermediate nodes of the tree further enhances the efficiency of the estimation: if the range of an intermediate node u is completely contained inside the range of the query, the estimated answer is increased by the sum associated with node, and the tree rooted at u is rapidly scanned, without decoding the boundaries of the nodes descending from u and without accessing their sums.

As regards FBH , its flat representation model does not provide any mechanism for effi-ciently locating the buckets involved in the query. Thus, estimating Sum ( r ) requires accessing all the buckets of the histogram to check whether they overlap r . In order to check whether abucket b overlaps r we must accomplish O ( d ) intersections: first we have to check whe-ther b intersects r along dimension 1; if this is the case, we accomplish the same verifying for dimension 2, and so on, until we find a dimension where b and r do not overlap, or all dimensions have been checked. Therefore, O ( b  X  d ) intersections between one-dimensional ranges should be computed to determine the list of buckets giving non-null-contribution to the query answer.

Observe that for both HBH and GHBH the number of one-dimensional intersections which must be computed to locate the buckets involved in the query is less than the latter amount (for the same value of b ). In fact, for each intermediate node visited in the partition tree we must accomplish at most one 1-dimensional intersection, according to the splitting dimension and the splitting position stored in the node representation. Thus the number of intersections to be performed is at most b (as the number of intermediate nodes is the same as the number of leaves). Moreover, the value b is an upper bound: as explained above, intersections must be computed only for those branches descending from nodes overlapping the query range.

The above-reported reasoning does not suffice to state that accomplishing query estimates on HBH and GHBH is more efficient than the case of FBH , as the number of buckets stored by HBH and GHBH is larger than that of FBH within the same storage space bound (as explained in Sect. 2.5 ). Moreover, the cost of the bit-wise decoding of the tree structure should be taken into account. The relevance of this overhead in a practical setting cannot be measured but experimentally. In Sect. 4.8 we provide several experiments comparing the efficiency of answering queries when the FBH flat representation model and the GHBH hierarchical representation model are employed, respectively. Overall, our experimental results show that adopting the compressed hierarchical representation model results in better accuracy and efficiency of query answering. 3 Constructing histograms based on binary partitions 3.1 Optimal histograms As introduced in Sect. 1.1 , one of the most important issues when dealing with multi-dimensional histograms is building the histogram which approximates  X  X est X  the original data distribution, while being constrained to fit in a given storage space bound. The SSE of a histogram is a widely used metric to measure its  X  X uality X , in terms of accuracy of the estimated answers. In particular, independently on the class of the underlying partition, the the SSE of a single bucket is given by SSE ( b i ) = j  X  b bound B , the histogram on D which has minimum SSE among all histograms on D whose size is bounded by B is said to be V-Optimal (w.r.t. B ). This notion of optimality can be trivially specialized to the case of histograms based on binary partitions.
 Definition 6 Let D be a d -dimensional data distribution, B a storage space bound, and T a type of histogram (where T is either FBH , HBH ,ork-GHBH ). A histogram H of type T on D is said to be V-Optimal w.r.t. B if the following conditions hold: 1. size ( H )  X  B ; 2. SSE ( H ) = min H  X  H T ( D , B ) SSE ( H ) where H T ( D , B ) is the set of all histograms of type T on D whose size is less than or equal to B .
 Theorem 1 Let D be a d -dimensional data distribution, B a storage space bound, and T a type of histogram ( where T is either FBH , HBH , or k-GHBH ) . A V-Optimal histogram H of type T on D w.r.t. B can be computed in the complexity bounds reported in Table 2 . Proof See Appendix.

Results for FBH s in Theorem 1 can be viewed as an extension of the results presented in [ 19 ], where the problem of finding the optimal binary hierarchical partition w.r.t. several metrics (including the SSE) has been shown to be polynomial in the two-dimensional case. 3 We stress that this result does not hold for arbitrary partitions, where the problem of finding the V-Optimal histogram has been shown to be NP-hard in the two-dimensional case [ 19 ]. In the one-dimensional case the classes of arbitrary and hierarchical partitions coincide, and thus our result is consistent with that of [ 11 ], where a polynomial-time algorithm for constructing a V-Optimal histogram on a one-dimensional data distribution has been proposed. Comparing results for FBH s, HBH sand GHBH s in Theorem 1 we can observe that the computational complexity of constructing a V-Optimal FBH is less than that of computing a V-Optimal HBH within the same storage space bound. Essentially, this is due to the more complex representation scheme adopted by HBH , whose buckets are represented differently depending on whether they are null or not, derivable or not (see the theorem proof in the appendix for more details). However, the two complexity bounds have the same polynomial degree w.r.t. the volume of the input data; moreover, the aim of introducing HBH is not to make the construction process faster, but to yield a more effective histogram. The complexity of building k-GHBH is less than that of HBH as, in the former case, the number of splits that can be applied to a block are constrained by the grid. Note that, if k = n , the complexities of the two cases coincide. 3.2 Greedy algorithms From the complexity bounds reported in Table 2 we can draw the conclusion that V-Optimal hierarchical histograms can be built in time polynomial w.r.t. the size of the domain of the input data distribution. In particular, both FBH and HBH can be constructed in nearly quadratic time w.r.t. n d , whereas k-GHBH in linear time (since the grid degree k can be assumed as a constant). Indeed, for high-dimensionality scenarios the size of the domain is so large that finding the V-Optimal becomes unfeasible. In order to reach the goal of minimizing the SSE, in favor of simplicity and speed, we adopt a greedy approach for constructing the histogram, accepting the possibility of obtaining a non-optimal solution. As it will be shown later, this approach can work in linear time w.r.t. N (the number of non-null points inside D ), which is generally much smaller than n d (especially in the case of high-dimensionality data).
 Our approach can be viewed as an extension of the standard greedy strategy adopted by MHIST and MinSkew. It starts from the binary histogram whose partition tree has a unique node (corresponding to the whole D ) and, at each step, selects the leaf of the binary-tree which case of a GHBH , the splitting position must be selected among all the positions laid onto the grid overlying the block. Both the choices of the block to be split and of the position where it has to be split are made according to a greedy criterion.

A number of possible greedy criteria can be adopted for choosing the block which is most in need of partitioning and how to split it. The greedy strategies tested in our experiments are reported in the table shown in Fig. 7 . Two of them (namely Max-Red marg and MaxDiff ) were already used by other techniques ( MinSkew and MHIST , respectively) to drive the his-togram construction. Criteria denoted as marginal (marg) investigate marginal distributions of blocks. The marginal distribution of a block b =  X  1 ,..., X  d along the i -th dimension is the  X  X rojection X  of the internal data distribution on the i -th dimension, and will be denoted as marg i ( b ) .Formally,the j -th element of marg i ( b ) is the sum of all elements inside b whose i -th coordinate has value j + lb ( X  i ) (see Fig. 14 in Section B.2 for an example of marginal distribution). In the following, the term marginal SSE will be used to denote SSE marg i ( b ) for some i  X  1 .. d .
 The resulting algorithm scheme is shown below.
 Greedy Algorithm INPUT D : a multi-dimensional data distribution;
OUTPUT H : a histogram of type T on D within B ; begin b 0 := [ 1 .. n ] ,..., [ 1 .. n ] ; &lt; need, dim, pos &gt; = Evaluate ( G , b 0 ) ; q . Insert (&lt; b 0 ,&lt; need, dim, pos &gt;&gt;) ; while ( ! H . overflow ()) end_while; return H ; end
Greedy algorithm uses a priority queue q where nodes of the histogram are ordered according to their need to be partitioned. At each step, the node at the top of the queue is extracted and split, and its children are in turn enqueued. Before adding a new node b to the queue, the algorithm invokes function E v aluate ( G , b ) , G being the adopted greedy criterion. This function returns both a measure of the need of b to be partitioned (denoted as need ), and the position ( dim , pos ) of the most effective split, according to the adopted criterion G . For instance, if G = Max-Var/Max-Red , the function returns the SSE of b into Otherwise, if Max-Red criterion is adopted, the value of need returned by E v aluate on b is the maximum reduction of SSE which can be obtained by splitting b , and the returned pair &lt; dim , pos &gt; defines the position corresponding to this split.

Function BinarySplit takes the following arguments: a bucket b of the histogram, the chosen splitting position and the available storage space B . It returns the pair b low , b high consisting of the sub-blocks obtained by performing the specified binary split of b . Moreover, itevaluatesthestoragespaceconsumptionofadding b low and b high aschildrenof b (thestorage space needed to store these new buckets depending on the histogram type). If the sum of this storage space consumption with the current size of H is smaller than or equal to the space bound B , then buckets b low , b high are actually inserted into H ; otherwise, H is not updated and the next invocation of overflow () will return true : this ends the histogram construction.
As regards function Evaluate , in the case of FBH and HBH , the splitting positions to be evaluated for a bucket b are all the positions between the boundaries of every dimension of b , whereas for GHBH the function computes only all possible splits laid onto the grid.
The complexity of Greedy algorithm strictly depends not only on the type of histogram to be built, but also on the adopted data model. For instance, data can be stored into a multidimensional array (where each cell is associated to a point of the multi-dimensional space), or by adopting a sparse model, where only non-null data are stored. In the latter case, v al the value of non-null points. For the sake of brevity, here we discuss only the complexity of the greedy approach working on the sparse model. In the appendix the adoption of a non-sparse model for the data distribution is analyzed too; furthermore, the use of some pre-computed data which serve as auxiliary structures for improving the algorithm efficiency is also considered. Experimental results comparing construction times for the three approaches (sparse model, non-sparse model, and pre-computation) are also provided in the appendix. Theorem 2 Given a d -dimensional data distribution D with volume n d containing N non-null points, the time complexity of Greedy algorithm computing a histogram of type T (where T is FBH , HBH or k-GHBH ) on D, adopting the sparse data model, are reported in Table 3 , where  X  = nif Max-Var marg /Max-Red marg criterion is adopted, and  X  = k for all the other greedy criteria.
 Proof See Appendix.

The complexity bounds reported in Table 3 show that Greedy algorithm works in linear time w.r.t. both the number of non-null points inside D and the size of the dimensions of D . Notice that, as these bounds hold for all the considered greedy criteria, the idea of working on the one-dimensional marginal distributions of blocks does not provide a relevant benefit on the efficiency of the histogram construction w.r.t. investigating the actual multi-dimensional distributions of blocks in the greedy criterion.
 4 Experimental results Our experimental analysis investigates thoroughly several issues related to the accuracy pro-vided by histograms based on binary partitions. We focus our attention on different issues, in order to study progressively the impact on the histogram accuracy of the following contri-butions: the specific tree-based representation model of HBH , the grid-constrained partition scheme of GHBH , and the heuristics used to accomplish the construction of the histogram. Finally, we study the efficiency of query evaluation using our compressed representation model, comparing it with the FBH representation model.

The experiments were conducted on both synthetic and real-life data. In the following two sections we describe the synthetic data generator and the real-life data sets used to perform experiments. 4.1 Synthetic data Our synthetic data generator is similar to those of [ 27 ]. It takes as argument the following dense regions inside a d -dimensional array D with volume n 1  X  X  X  X  X  n d . These dense regions skew parameter z i (as it will be clear later, z i determines the data distribution inside r i ). The coordinates of the centers c 1 ,..., c m are generated according to a uniform distribution on the domain of D ; the widths l 1 ,..., l m are randomly chosen between l min and l max ,and z ,..., z m are randomly chosen between z min and z max (in our experiments, where it is not only null points), and at the end of the generation process it will contain a number of points divided into m portions T 1 ,..., T m according to a uniform distribution. Each T i represents the sum of the points inside region r i .Region r i is populated in two steps: 1. a number T i of points (namely, p 1 ,..., p T i ) inside the range of r i are generated. Each 2. for each p j (with j  X  X  1 .. T i ] )thevalueof D [ p j ] is increased by one. Thus, each cell p As regards T noise , it is used to simulate noise in the data distribution: it is distributed among randomly generated points inside D (in our experiments we used T noise = 0 . 05  X  T ). This generation paradigm results in a data distribution where, for each r i , the higher z i , the more  X  X oncentrated X  around c i the data points: as z i increases, points having large distances from c are less probable to be generated. 4.2 Real life data In our experiments we considered two real-life data sets. The first will be referred to as Census and was obtained from the US Census Bureau using their DataFerret application for retrieving data. The data source is the Current Population Survey (CPS), from which the March Questionnaire Supplement (1994) file, containing 150,943 tuples, was extracted. Eight attributes have been chosen: Age , Parent X  X  line number , Major Occupation , Marital Status , Race , Family Type , Public Assistance Type , School Enrollment . The corresponding eight-dimensional array has about 4 . 6  X  10 7 cells, and contains 19,129 non-null values (the density is about 4 . 2  X  10  X  4 ).
 The other data set will be referred to as Forest Cover . It was obtained from the US Forest Service and is available at the UCI KDD archive site. It consists of 581,012 tuples having 54 attributes. Among these, ten attributes are numerical. As in [ 9 ], we considered the tuples projected on these numerical attributes, thus obtaining a ten-dimensional data distribution. The corresponding 10-dimensional array has about 4 . 4  X  10 28 cells (the density is about 1 . 3  X  10  X  23 ). 4.3 Measuring the effectiveness of histograms Given a data distribution D ,let r be a range inside the domain of D . We denote with Sum ( r ) the range-sum query asking for the sum of values stored in the cells of D inside r . In our experiments D represents a frequency joint distribution, thus the value of Sum ( r ) represents The relative error is defined as: e rel = e abs Sum ( r ) .

The accuracy of the various techniques was evaluated by measuring the absolute error and the relative error of the estimates of range-sum queries computed by accessing the histogram. The impact of a number of parameters on the accuracy was considered: the amount of storage space used to represent the histogram, the selectivity of queries, as well as several characteristics of the input data (such as dimensionality and domain size). The sensitivity to each of these parameters was analyzed by varying it and fixing the other ones. In particular, in order to generate groups of queries with the same selectivity on a data distribution D , we used the following strategy. First, a number of distinct points were randomly selected inside D . Then, for each of these points p , a set of queries was generated starting from the query whose range coincided with p and by progressively enlarging the query volume. This resulted in queries centered in p with increasing selectivity. Finally, such obtained queries were grouped by their selectivity. 4.4 Experimental plan In this section, we provide an overview on the experimental analysis presented in the rest of the paper, motivating the rationale of each group of experiments.
 Section 4.5 and Section 4.6 . As explained in Sect. 2.3 , the tree-based representation models of HBH and GHBH are an alternative to the MBR-based one of FBH . On the one hand, the tree-based representation model yields a larger number of buckets w.r.t. the MBR-based one (within the same storage space bound); on the other hand, buckets represented by means of their MBRs are likely to provide a more accurate description of the underlying data distribution. Therefore, it is worth investigating which of these alternatives yields the best accuracy and, in more detail, how the accuracy provided by the different representation models of FBH , HBH , GHBH depends on the particular greedy criterion adopted to guide the histogram construction.

The experiments presented in Sects. 4.5 and 4.6 are devoted to investigating this aspect, as well as giving an insight on how several characteristics of the data (such as skewness, dimensionality, size of the domain) affect the error rates of query estimates.
In more detail:  X  in Sect. 4.5 , we momentarily disregard the grid-based approach of GHBH , and we concen- X  in Sect. 4.6 , we study the impact of the grid-constrained partitioning by comparing HBH On the whole, the analysis presented in Sects. 4.5 and 4.6 shows that the best performances among all different combinations type of histogram / greedy criterion is provided by histo-grams of type GHBH constructed by adopting Max-Var / Max-Red .
 Section 4.7 . Here we compare (in terms of accuracy) histograms of type GHBH adopting Max-Var / Max-Red with state-of-the-art techniques. Specifically, we present experiments stu-dying how the accuracy of the various techniques is affected by data dimensionality, showing that histograms of type GHBH are less sensitive on data dimensionality than the state of the art.
 Section 4.8 . In this section, we provide experiments testing the impact of adopting the compression-based representation model of GHBH on query-estimation efficiency. These experiments are complementary to Sect. 2.6 in that they show that the speedup provided by the indexing mechanism encoded in the GHBH representation counterbalances the cost of unpacking the bit-wise encoding of the histogram. 4.5 Comparing FBH and HBH under different greedy criteria Rationale of the experiments. In order to establish how using different greedy criteria and employing different representation models affect the histogram effectiveness, we studied the accuracyofhistogramsobtainedusingdifferentcombinations greedy criterion / representation model . In particular, in this section we provide experimental results studying how error rates change when the same greedy criterion is used with the flat representation model of FBH and the tree-based representation model of HBH . The impact of the use of the grid-constrained partitioning of GHBH will be considered in more detail in Sect. 4.6 .
 Description of the experiments. Diagrams in Fig. 8 a, b were obtained on a synthetic four-dimensional data distribution with volume 400  X  400  X  400  X  400 containing 1 million tuples distributed among 30 dense regions (with l min = 30 and l max = 80), whereas diagrams in Fig. 8 c, d were obtained on the ten-dimensional Forest Cover data set. The accuracy of the various criteria is evaluated w.r.t. the storage space available for the compressed representation.
The main results which turn out from diagrams in Fig. 8 are the following: 1. for both FBH and HBH , Max-Var / Max-Red provides lower error rates than all other 2. the accuracy of histograms built by employing any criterion other than Max-Var /
In order to explain why employing different greedy criteria results in different error rates, we considered a two-dimensional data distribution and studied how data is partitioned depending on the adopted criterion for both HBH and FBH . Partitions resulting from different combinations greedy criterion / representation model are depicted in Fig. 9 .
 By analyzing diagrams in Fig. 9 , the following considerations can be drawn:  X  Max-Red fails in building effective partitions, as it tends to progressively split  X  X mall X   X  the behavior of Max-Red marg can be explained as for Max-Red . In this case, the criterion  X  adopting MaxDiff results in partitions which poorly describe the underlying data, as this  X  Max-Var / Max-Red succeeds in locating dense regions where a finer-grain partition of
In Fig. 9 the partition obtained with Max-Var marg / Max-Red marg is not shown, as it was very similar to that of Max-Var / Max-Red . Indeed, differences in error rates between these two cri-teria arise more significantly in higher dimensionality settings. As dimensionality increases, marginal distributions contain less and less information on the internal content of blocks: in fact, the total size of marginal distributions of a block (i.e., the sum of the lengths of all mar-ginal distributions of the block) grows linearly with d (it is O ( d  X  n ) ), whereas the volume of blocks grows exponentially with d (it is O ( n d ) ). Therefore, investigating the content of mar-ginal distributions to decide whether a block needs to be partitioned is likely to provide less reliable information as dimensionality increases. As a matter of fact, isolated dense regions of the multi-dimensional space can collapse into a single mono-dimensional dense region, when projected on each dimension (for instance, consider two adjacent circular dense re-gions located at the ends of a diagonal in the 2D space). Therefore, it is unlikely to succeed in isolating the dense multi-dimensional regions by only looking at their projections (i.e., the marginal distributions) on space dimensions.

We now focus our attention on explaining why only Max-Var / Max-Red and Max-Red benefits from the adoption of HBH representation model. As regards Max-Var / Max-Red ,the reason for this is that MBRs do not help this criterion in isolating dense regions from null ones: therefore, Max-Var / Max-Red can exploit significantly the increase in number of buckets due to the HBH representation model, investing a larger number of buckets to approximate dense regions in more detail (in fact, error rates for Max-Var / Max-Red are very sensitive to an increase in storage space, as shown in diagrams in Fig. 8 ). As regards Max-Red ,onthe one hand MBRs do not prevent this criterion from generating several small buckets in a few dense regions. On the other hand, the use of MBR reduces the number of splits, so that a larger number of dense regions are likely to be disregarded by this criterion (for instance, the dense region at the bottom right-hand part of the data distribution in Fig. 9 e is not partitioned when MBRs are used).

On the contrary, all other criteria are not effective in assigning distinct dense regions to distinct buckets: in this case, MBRs do provide a more accurate description of the data underlying buckets, but this positive benefit of FBH representation model turns out to be counterbalanced by the smaller number of buckets w.r.t. the HBH representation model. Remark In [ 6 ] a different version of MHIST consisting in adopting a partition-tree-based representation scheme instead of MBRs was proposed. This basically consists in combining the Max-Diff criterion with the HBH representation model. As it emerges from our expe-riments, that naif adoption of a tree-based representation scheme is not likely to achieve significant benefits w.r.t. the original MHIST , which combines the Max-Diff criterion with the FBH representation model.
 Summary of results. On the basis of the experimental results presented in this section, we can draw the following conclusions:  X  the impact of the representation model ( which affects the number of buckets which can  X  the HBH histogram employing Max-Var / Max-Red largely outperforms histograms re-4.6 Comparing HBH with GHBH Rationale of the experiments. In this section, we study how the introduction of a grid constraining block splits affects the accuracy of histograms. Specifically, in Sect. 4.6.1 ,we compare HBH s with GHBH s of different degrees under the same greedy criterion (thus we adopt Max-Var / Max-Red , under which HBH yields the best accuracy). We also investigate how the accuracy resulting from adopting a particular grid degree depends on different characteristics of the data (such as size of data domain and data skewness), and give an insight on how the choice of a particular grid degree affects the depth of the partition tree underlying the histogram. Then, in Sect. 4.6.2 , we briefly discuss the impact of adopting different greedy criteria to guide the construction of a GHBH . 4.6.1 HBH vs. GHBH under Max-Var / Max-Red Description of the experiments. Histograms of type GHBH with different grid degrees have been tested. In the following, the term GHBH ( x ) will be used to denote a GHBH which employs x bits to store the splitting position.

Diagrams in Fig. 10 a, b were obtained on the same four-dimensional synthetic data dis-tribution as Fig. 8 , whereas diagrams in Fig. 10 c, d were obtained on Census data set by performing samples of queries whose selectivity is respectively, 0 . 5 and 3%.
 From diagrams in Fig. 10 it emerges that GHBH yields higher accuracy than HBH .
 Although the HBH algorithm is able to perform more effective splits at each step (as splits are not constrained by the grid), the number of buckets generated by GHBH algorithms is larger.

In order to investigate in more detail how the optimal grid degree depends on the cha-racteristics of data, we also performed experiments studying how the accuracy of GHBH s adopting grids with different degrees is affected by either the size of the domain and the data skewness. As regards the former issue, intuition would suggest that, as the volume of data increases, in order to keep the same accuracy in partitioning data, a higher-degree grid should be adopted. To analyze this aspect, we performed some experiments investigating how changing the grid degree affects the effectiveness of isolating dense regions distributed on larger and larger domains.

To this aim, we tested GHBH s with different grid-degrees on data distributions having the same dimensionality, increasing volume, and containing the same dense regions differently distributed in the data domain. Figure 10 e depicts error rates on three-dimensional cubic data sets with increasing lengths of dimensions, from 200 to 1,600. These data sets will be denoted as D n ,where n is the length of each dimension. D 200 ,..., D 1600 were generated by first creating 30 dense regions, and then distributing randomly the centers of these regions in the different data domains. For instance, each dense region r i (with i  X  X  1 .. 30 ] )inside D 400 contains the same distribution of values as the region r i inside D 200 , but the center c i of r i has different coordinates w.r.t. the center c i of r i ( c i and c i being randomly selected points inside D 400 and D 200 , respectively). For each data set D n a query set QS n is generated as follows. QS 200 contains, for each dense region r i of D 200 , 1,000 hypercubic queries intersecting r i . The centers of these queries are characterized by their relative coordinates to c i . QS 400 contains, for each q  X  QS 200 involving r i , a query q involving r i with the same volume as q , and whose center has the same relative coordinates to c i as q does to c .Querysets QS 800 , QS 1600 have been constructed analogously. Evaluating error rates w.r.t. these query sets allows us to establish whether the optimal grid degree for a GHBH depends on the domain size. Figure 10 f was obtained analogously, but for six-dimensional data.

Diagrams in Fig. 10 e, f show that the effectiveness of adopting a particular grid degree is almost unaffected by the size of the domain.

DiagramsinFig. 10 g,hshowhowerrorratesdependondataskewness.DiagraminFig. 10 g depicts error rates for three-dimensional data distributions with volume 800  X  800  X  800 having 30 dense regions (having the same value z of the skew parameter), whereas diagram in Fig. 10 h was obtained on six-dimensional data distributions with volume 800 6 with 30 dense regions. As for Figs. 10 e, f we considered a query set consisting of queries with the same volume overlapping dense regions. Observe that data density decreases as skewness increases: as z gets larger, dense regions become sparser, since tuples are generated with a decreasing probability of being far from the center of the region. That is, when z = 0the distances of generated tuples from the center are uniformly distributed, while as z becomes larger, large distances from the center become less and less probable. Going to the limit, dense regions collapse to a unique cell.

From diagrams in Fig. 10 g, h, it turns out that  X  X ntermediate X  values of z yield the largest error rates. Indeed, if skew is either very low or very high, dense regions will be rather uniform, or collapse, respectively. In both cases, isolating the dense region into a few buckets suffices to have good accuracy, whereas for intermediate values of z dense regions need more and more splits to be accurately described by the partition.

Figure 11 a, b give an insight on how the adoption of a particular grid degree affects the depth of the partition tree underlying a GHBH . They were obtained for the six-dimensional data distributions with z = 0and z = 2 considered in Fig. 10 d, and they compare the depths of the partition trees of GHBH s of different degrees with those of an HBH and a FBH (employing MBRs) built within the same storage space bound, for different amounts of storage space. From these diagrams, it turns out that: (i) the depth of the hierarchies does not depend on the data skewness; (ii) the hierarchies of GHBH s are deeper than that of an HBH , which, in turn, is deeper than that of an FBH . The latter can be explained by observing that imposing the grid constraint on the splitting positions makes each split less flexible, thus longer sequences of splits are needed to isolate dense regions. The hierarchy overlying the buckets of an FBH is even less deep than that of an HBH since the use of MBRs, as already discussed, facilitates the task of separating dense regions from null spaces around. Observe that the increase in depth of the partition tree underlying a GHBH w.r.t. HBH sand FBH s poses the problem of designing techniques for intelligently fragmenting the partition tree into pages, in the case that its size does not fit in main memory. In fact, more levels of the tree could require more disk accesses. This aspect is not a severe drawback of our approach, since in several application scenarios histograms are designed as memory-resident structures, and their size is chosen not to exceed main memory limits. However, a deeper investigation of this issue goes beyond the scope of this work.
 Summary of results. From the results presented in this section, we can draw the conclusion that the use of grids provides an effective trade-off between the accuracy of splits and the number of splits which can be generated within a given storage space bound. The effectiveness of this trade-off depends on the degree of the allowed binary splits. In fact, when a high degree is adopted, a single split can be very  X  X ffective X  in partitioning a block, in the sense that it can produce a pair of blocks which are more homogeneous w.r.t. the case that the splitting position is constrained to be laid onto a coarser grid. On the other hand, the higher the degree of splits, the larger the amount of storage space needed to represent each split. From our results, it emerges that GHBH ( 3 ) (using binary splits of degree 8) generally gives the best performances in terms of accuracy, and as the number of bits used to define the grid increases, the accuracy decreases.

However, we point out that GHBH s with small degree values do not exhibit large diffe-rences in error rates. Therefore, even if a value for the grid degree yielding the best accuracy in any setting cannot be found, this is not a limit of our approach, as any low-degree grid provides error rates which are close to those of the  X  X ptimal X  degree. In the rest of the paper, all results on GHBH will be presented by using 3 bits for storing splitting positions. 4.6.2 HBH vs. GHBH under different greedy criteria It is worth noting that the improvement of HBH accuracy obtained by introducing the grid constraint is not simply orthogonal to any greedy criterion of the table in Fig. 7 . Although we have discussed the benefits of using grids only when Max-Var / Max-Red is used, from several experiments it turned out that, in the case that another greedy criterion is adopted, the improvement in accuracy when moving from HBH to GHBH is not so relevant. We do not report related diagrams here for the sake of brevity. However, this result is rather expected, as it is worth noting that Max-Var / Max-Red is the only criterion which improves significantly as the available storage space becomes larger (see Fig. 8 ), whereas the other criteria are less sensitive to this parameter. Therefore, it is unlikely that the other criteria exploit the increase of the number of available splits due to the introduction of grids as Max-Var / Max-Red does. Therefore, in the following we will consider only GHBH adopting Max-Var / Max-Red . 4.7 GHBH versus other techniques Rationale of the experiments. We compared the effectiveness of GHBH with the state-of-the-art techniques for compressing multi-dimensional data. Specifically, we compared error rates of query estimates against three parameters: storage space, query selectivity and data dimensionality. Besides considering MHIST and MinSkew, we compared the accuracy of GHBH with GENHIST [ 9 ] and the wavelet-based techniques proposed in [ 26 , 27 ], which are briefly described in the following.
 GENHIST . The idea underlying GENHIST is to progressively locate regions of data which exhibit a inhomogeneous distribution w.r.t. contiguous ones. At each step, a grid based on a  X  -regular partitioning of the multi-dimensional data distribution is constructed and a number of cells of the grid having larger density is chosen. The data distribution is made smoother by randomly removing from each selected cell a number of tuples, so that the density of remai-ning tuples in the cell is the same as neighbor cells. Removed tuples are stored in a bucket, whose boundaries coincide with those of the corresponding cell. The value of  X  defining the grid depends on the step of the algorithm. At the first step, an input parameter is used, and at the following steps its value is iteratively decreased, thus making the regular partitioning of data coarser. This follows from the observation that the data distribution processed at each step of the algorithm is more homogeneous than that processed at previous steps, thus larger buckets suffice to approximate data in detail. The main difference w.r.t. traditional histograms is that buckets returned by GENHIST algorithm can overlap, as each bucket may not contain information on all the tuples within the corresponding multi-dimensional range, but only on some (randomly chosen) tuples which have been removed at some step.
 Wavelet-based compression techniques . We have considered the two wavelet-based tech-niques presented in [ 27 ] (that will be referred as WAVE1) and in [ 26 ] (WAVE2). The former applies the wavelet transform directly on the source data, whereas the latter performs a pre-computation step. First, it generates the partial sum data array of the source data, and replaces each of its cells with its natural logarithm. Then, the wavelet compression process is applied to this array.
 Description of the experiments. Diagrams of Fig. 12 were obtained on four-dimensional synthetic data with volume 8  X  16  X  256  X  1 , 024, with T = 35 , 000 having 30 dense regions (the obtained density is about 0 . 1%). In this case, we could not use the same synthetic data distribution as Fig. 8 a, b and 10 a, b since our prototype implementing wavelet does not work with data distributions with so large volumes. In particular diagrams (a,b) show how the accuracy of the techniques changes as the storage space increases, whereas diagrams (c,d) depict error rates w.r.t. the selectivity of queries.

GHBH exploitstheincreaseofstoragespacebetterthantheothertechniques.Relativeerror rates of all techniques increase as query selectivity decreases: this can be easily explained by considering that as selectivity decreases (i.e., query answers become smaller in value) even a small difference between the actual query answer and the estimated one can lead to a large relative error.
 Diagrams (e X  X ) of Fig. 12 were obtained on Forest Cover data set.

In particular Fig. 12 g refers to very low selectivities and reports absolute errors. In this case relative errors have not been considered as, for small selectivities, relative error is likely be high even if a reasonable approximation is obtained: for instance, while a 300% error rate on a query with selectivity 1 corresponds to a good accuracy of the estimate, the same error rate on a query whose selectivity is high (w.r.t. N ) makes the inaccuracy of the query estimate intolerable. Thus relative error may not be indicative of the actual accuracy. In diagram (h) relative error rates at higher selectivities are reported. Wavelet techniques were not considered on this data set as our prototype does not support data sets with so large volumes.
Diagrams (i X  X ) of Fig. 12 were obtained similarly to diagrams (e X  X ), but on the eight-dimensional Census data set. In particular, in this case we considered a broader range of selectivities, as this data set is less sparse than Forest Cover and thus queries with very low selectivities are unlikely to occur.

All the diagrams in Fig. 12 show that GHBH adopting Max-Var / Max-Red outperforms the other techniques on all the examined data sets.

We also investigated in detail how the accuracy of the various techniques is affected by an increase in dimensionality of the input data. In particular, Fig. 12 m, n refer to synthetic data. These diagrams were obtained by starting from a ten-dimensional data distribution (called D 10 ) containing 10 20 cells (the size of each dimension is equal to 100), where about 53,000 non-null values (density 5 . 3  X  10  X  16 ) are distributed among 1,000 dense regions. The data distributions with lower dimensionality (called D i , with i  X  4 .. 9) were generated by projecting the values of D 10 on the first i of its dimensions. By means of this strategy, we created a sequence of multi-dimensional data distributions, with increasing dimensionality (from 4 to 10) and with decreasing density (from 3 . 9  X  10  X  4 to 5 . 3  X  10  X  16 ). Figure 12 m, n were obtained by considering, for each D i , a sample of range queries whose selectivity is respectively 0.1 and 1%. Both these diagrams were obtained using a storage space of 2,000 words. The same kind of experiments were performed on Census data set. In this case we projected the eight-dimensional data set described in Sect. 4.2 on the first i of its dimensions ( i = 5 .. 7). In Fig. 12 o, p results obtained on samples of queries having selectivity 0.1 and 1% respectively, are depicted.

Both WAVE1 and WAVE2 were not considered on synthetic data, as our prototype does not work on data sets so large in volume. Error rates for WAVE1 are not reported in the diagram in Fig. 12 o as they were out of scale. Diagrams in Fig. 12 show that, for both synthetic and real-life data, error rates of every technique tend to increase as dimensionality increases, but GHBH accuracy gets worse very slightly and outperforms all the other techniques at all considered dimensionalities.
 Summary of results. From the experiments provided in this section, we can draw the conclu-sion that GHBH outperforms state-of-the-art techniques in terms of accuracy. Specifically, GHBH is much less affected by the increase of dimensionality w.r.t. other techniques, and provides high accuracy even for queries with  X  X mall X  selectivity. 4.8 Efficiency of query estimation Rationale of the experiments. In this section, we provide experiments comparing the effi-ciency of estimating queries on histograms adopting the FBH and the GHBH representation models. Thus, these experiments aim at evaluating the impact of both the indexing mecha-nism provided by the partition tree embedded in the GHBH and the computational overhead resulting from the need of unpacking the bit-wise encoding adopted in the GHBH represen-tation. Specifically, we compare estimation times versus (a) the amount of storage space used to represent the histograms; (b) the volume of queries; (c) the data dimensionality. Description of the experiments. Diagrams (a X  X ) in Fig. 13 were obtained by considering the same synthetic data sets D 4 ,..., D 10 used in the experiments of Fig. 12 m, n. In particular, Fig. 13 a depicts query estimation times versus the histogram size for a bulk of 10 , 000 queries issued on histograms of type FBH and GHBH constructed on D 8 (all the queries of the bulk having volume that is 0.1% of the volume of the whole data domain). As expected, for both FBH and GHBH query estimation times increase as the storage space used for representing histograms increases, since the amount of data that must be accessed to estimate queries gets larger. Figure 13 b refers to the same data distribution as Fig. 13 a and depicts query estimation times versus query volumes, for histograms consuming 2,000 words (bulks of queries each consisting of 10,000 queries of the same volume were considered). Figure 13 c depicts query execution times versus data dimensionality. In this experiment both the amount of storage space and the query volume (in terms of percentage of the data domain volume) are kept constant. This diagram shows that the increase in dimensionality slightly affects the query estimation times.

Overall, diagrams (a X  X ) show that estimating queries on histograms adopting the compression-based representation model of GHBH is faster than the case of histograms of type FBH consuming the same amount of storage space. This behavior turns out to be independent on the invested amount of storage space, the query volume, and the data dimen-sionality.

The relevance of this result is strengthened by considering that a GHBH consists of a much larger number of buckets than an FBH built within the same storage space bound. In Fig. 13 d, we report the number of buckets obtained by the histograms of type FBH and GHBH constructed on the four-dimensional and the ten-dimensional data distributions of Fig. 13 c (that is, the data distributions having the lowest and the highest dimensionality, respectively). In the examined cases, adopting the GHBH representation model yields a number of buckets which ranges from 2.5 times to 5 times the number of buckets of an FBH (in the case of four-dimensional data and ten-dimensional data, respectively). Observe that the amount of buckets of a GHBH is almost unaffected by data dimensionality, whereas the number of buckets of an FBH decreases as dimensionality increases. This is in agree with weakly sensitive to d , whereas  X  FBH is inversely proportional to  X  .

Analogous results were obtained on real-life data distributions. In Fig. 13 e X  X  we report results obtained by performing the same kind of experiments on Census data-sets used in Fig. 12 o, p. Also in this context, GHBH representation allows a more efficient query eva-luation compared with FBH , for various values of storage space (Fig. 13 a), query volume (Fig. 13 b) and number of dimensions (Fig. 13 c), and yields a larger number of buckets (Fig. 13 d).
 Summary of results. The experiments provided in this section show that the tree-based repre-sentation of GHBH provides an effective indexing mechanism for locating buckets involved in the queries. Specifically, the positive effect of the indexing mechanism on query estimation efficiency overwhelms the computational cost of decoding the bit-wise representation of the histogram, and results in estimation times smaller than those provided by FBH . This result is even more remarkable when the fact is considered that a GHBH consists of a significantly larger number of buckets than an FBH with the same storage space occupancy. 5 Conclusions We have studied the use of binary partitions as a basis for effective multi-dimensional histo-grams. We have introduced two new classes of histogram (namely, HBH and GHBH )which exploit their hierarchical partition paradigm to make the representation of buckets more ef-ficient w.r.t. the traditional  X  X lat X  representation scheme adopted for classical histograms. In particular, GHBH differs from HBH as it adopts a grid-constrained hierarchical partition, where splits partitioning any block of data must be laid onto a grid dividing the block into a fixed number of equal-size sub-blocks. Both the hierarchical partition and the grid-constraint on split positions are exploited to define a very specific space-efficient physical representa-tion model. This enables HBH and GHBH to store a larger number of buckets within the same storage space bound w.r.t. the traditional MBR-based representation model.

Moreover, we have investigated several heuristics for guiding the construction of histo-grams. By means of experiments we have analyzed the performances (in terms of accuracy) of different combinations heuristic / representation model . That is, we have considered both the traditional MBR-based and the tree-based representation models of HBH and GHBH (for different grid granularities), and we have shown that GHBH with  X  X ow X -degree grids combined with one of the proposed heuristics yields the highest accuracy. Finally, we have provided several experimental results (on both synthetic and real-life data) comparing the best performing GHBH with other state-of-the-art multi-dimensional compression techniques, proving its effectiveness, also for high-dimensional data distributions. Moreover, we have shown by means of experiments that query estimation times are enhanced by the adoption of GHBH representation model.

In our opinion this work issues new research challenges in the direction of improving histograms construction techniques by exploiting the value of lossless-compression-based representation models for storing histograms. Indeed, our specific compression paradigm, as it exploits the hierarchical scheme adopted to construct the histogram, cannot be used to enhance the physical representation of different classes of histogram, such as wavelet-based ones, as well as techniques like GENHIST, which enables bucket overlapping. Therefore, it would be interesting to design ad-hoc compression-based representation models which are suitable for other summarization techniques, and thus to investigate more generally the value of compression in the context of approximate data structures.
 Appendix This appendix is divided into two sections. In the first one we provide the proofs of Proposition 1 (where FBH , HBH and GHBH representation models are compared in terms of number of buckets allowed in a given storage space bound) and Theorem 1 (where the complexity of constructing V-Optimal histograms is addressed). In the second section we discuss several issues related to the cost of Greedy algorithm. In particular we provide the proof of Theorem 2 and further investigation on the complexity of Greedy algorithm, also presenting experimental results on its execution time.
 A Proofs of theorems Proof of Proposition 1 1. T = FBH . The size of an FBH having  X  buckets is precisely 2. T = HBH or T =k-GHBH .An HBH ,aswellasak-GHBH , with  X  buckets has a space According to the physical representation of an HBH described in Sect. 2.3 , the size of an HBH H with  X  buckets can be expressed as the sum of four contributions: size HBH ( H ) = ( 2  X   X   X  1 ) + ( X   X  1 )  X  ( log d + 32 ) + ndl ( H ) + 32  X  ndn + ( H ) ,where ndl ( H ) and ndn + ( H ) stand for the number of non-derivable leaves of H and, respectively, the number of non-null non-derivable nodes of H . Analogously, we will denote by ndl + ( H ) and ndl 0 ( H ) the number of non-null non-derivable leaves and, respectively, the number of Similarly the size of a k-GHBH H having  X  buckets is size GHBH ( H ) = ( 2  X   X   X  1 ) + ( X   X  1 )  X  ( log d + log k ) + 32  X   X  + ndl + ( H )  X  31  X  ndl 0 ( H ) .

The expressions for size T ( H ) (for both T = HBH and T = GHBH ) have minimum value when ndl + ( H ) = 0and ndl 0 ( H ) =  X   X  1, which occurs for a histogram of type T with  X  buckets where all but one leaves are non-derivable and null. Likewise, the expressions for size T ( H ) have maximum value when ndl + ( H ) =  X   X  1and ndl 0 ( H ) = 0, which occurs for a histogram of type T with  X  buckets where all but one leaves are non-derivable and not null. Thus the minimum and maximum storage consumption of an HBH and a GHBH having  X  buckets are, respectively: As said above,  X  max HBH ,  X  max GHBH ,aswellas  X  min HBH ,  X  min GHBH , are straightforward. Proof of Theorem 1 1. T = FBH . The problem of finding the V-Optimal FBH on D can 2. T = HBH . The problem of finding the V-optimal HBH can be formalized and solved 3. T = k-GHBH . The problem of finding the V-Optimal k-GHBH can be formalized by B Cost analysis of Greedy algorithm In this section, we provide complexity bounds and a workspace analysis for Greedy algorithm for the different types of histogram ( FBH , HBH and k-GHBH ). Furthermore we provide experimental results regarding execution times. In particular, in Sects. B.1 and B.2, we will discuss the complexity of evaluating the greedy criteria during the construction of an FBH ,an HBH and a GHBH when either the sparse or non-sparse data model is adopted. In Sect. B.3, we show how the evaluation of greedy criteria can be performed by exploiting suitable pre-computation. Then in Sect. B.4 we provide the proof of Theorem 2, where the complexity of Greedy algorithm is analyzed under the sparse data model. Moreover, we provide complexity bounds for Greedy algorithm under either the non-sparse data model or pre-computation. Finally, in Sects. B.5 and B.6 we discuss some practical issues related to our greedy approach for histogram construction, analyzing the workspace size and execution times of Greedy algorithm.
 B.1 Evaluating greedy criteria when constructing an FBH or an HBH In this section, we analyze the complexity of computing function E v aluate on a block b , during the construction of either an FBH and an HBH , when different greedy criteria G are used, and when either the sparse data model or the non-sparse one are adopted. We show that the order of magnitude of the computational complexity of E v aluate ( G , b ) does not depend on the criterion G . This is due to the fact that the SSE of a block, as well as its reduction due to a split, can be computed by scanning marginal distributions, as explained in the following. Max-Red Let b = &lt; X  1 ,..., X  d &gt; be the block of D on which function E v aluate is invoked. If we be shown that the reduction of SSE ( b ) due to this split is given by: Red ( b , dim , pos ) can be computed by accessing marg dim ( b ) . In particular, notice that all possible splits along the dimension dim can be evaluated progressively, starting from sub-blocks of b obtained by performing the binary split at the i -th position in this sequence, comparing all possible splits along the dimension dim can be accomplished by first compu-ting marg dim ( b ) , and then scanning it once, as
The cost of constructing all marginal distributions is either O ( d  X  n d ) (non-sparse data model) or O ( d  X  N ) (sparse data model). The cost of scanning all marginal distributions to find the most effective splitting position is O ( d  X  n ) , so that the complexity of E v aluate (
Max-Red , b ) is bounded by either O ( d  X  n d ) (non-sparse data model) or O ( d  X  N + d  X  n ) (sparse data model).
 The reduction of SSE ( marg b dim ) due to a split of b at the position dim , pos can be shown to be (by applying the definition of SSE ): where P dim is the ratio between the volume of b and the size of its dim -th dimension. There-forethecomputationof Red marg ( b , dim , pos ) canbeaccomplishedwithinthesameboundas reduction of marginal SSE has the same complexity bound as computing the splitting position corresponding to the largest reduction of SSE. This implies that E v aluate ( Max-Red marg , b ) can be computed within the same bound as the case that Max-Red is adopted.
 Max-Var / Max-Red The value of SSE ( b ) (which is returned as need ) is given by: where sumSquare ( b ) is the sum of the squares of all values contained in b . This implies that SSE ( b ) can be computed by accessing all nonnull elements inside b . Therefore the cost of evaluating SSE ( b ) is O ( n d ) (non-sparse data model), or O ( N ) (sparse data model). The most effective splitting position can be evaluated in the same way as the case that Max-Red is adopted, and this cost dominates the cost of computing SSE ( b ) .
 Therefore, E v aluate ( Max-Var / Max-Red , b ) can be computed within the same bound as the case of Max-Red .

Observe that, when Max-Var/Max-Red is adopted, the strategy used by Greedy algorithm can be modified to make the computation of the histogram more efficient. Instead of compu-ting the most effective splitting position when a bucket is inserted into the queue q ,thevalue of &lt; dim , pos &gt; is evaluated only when the bucket is extracted from q .Infact,whena new bucket is generated and inserted into q , its position inside the queue depends only on its SSE ; similarly, the bucket which is most in need of partitioning is chosen only on the basis of its SSE . Therefore, computing the most effective splitting position &lt; dim , pos &gt; for a bucket b is useful only in the case that b is extracted from the queue. By using this strategy, the most effective splitting position is evaluated half many times as the previous strategy, as the buckets of the returned histogram (which correspond to the leafs of the underlying partition tree) have never been chosen to be split during the algorithm execution. Max-Var marg /Max-Red marg The marginal distributions of b must be constructed to compute both the value of need (i.e., the maximum variance of the marginal distributions) and the most effective splitting position. The value of need can be computed by scanning all marginal distributions, and the reductions of marginal variance can be evaluated as the case in which Max-Red marg is adopted. Therefore, computing E v aluate ( Max-Var marg /Max-Red marg , b ) has the same complexity bound as previous cases.
 B.2 Evaluating greedy criteria when constructing a GHBH The main difference w.r.t. the construction of an HBH is that splitting positions in a GHBH are constrained by the grid, so that the number of possible splits to be compared when processing the block b extracted from q is d  X  k (instead of d  X  n ,asinthe HBH case). The computation of Red ( b , dim , pos ) and Red marg ( b , dim , pos ) corresponding to all the d  X  k splitting positions can be efficiently accomplished after pre-computing d temporary data structures. Differently from the case of HBH , these temporary data structures are not constructed as follows. gr i d ( b ) is a bucket containing k d elements, where each cell contains the sum of the elements of b located in the corresponding cell of the k -th degree grid overlying shows the k -marginal distributions associated to a bucket b w.r.t. a 4th degree grid.
Let &lt; dim , pos &gt; be an admissible splitting position for the bucket b ,and i , i + 1be the corresponding cells of k-marg dim (i.e., the contiguous cells of k-marg dim which would be separated by performing the split). Then, the reduction of SSE ( b ) due to this split can
Obviously, constructing the k -marginal distributions has either cost O ( d  X  N ) (sparse data model) or O ( d  X  n d ) (non-sparse model), but their scanning has cost O ( d  X  k ) (instead of O ( d  X  n ) ,asinthe HBH case). Therefore, by applying the same reasoning explained in the previous section, it is easy to show that E v aluate ( G , b ) has cost O ( d  X  N + d  X   X ) and O ( d  X  n d ) for the two data models, respectively, where  X  = k for all greedy criteria G except Max-Var marg /Max-Red marg . When Max-Var marg /Max-Red marg is adopted,  X  = n : in fact, in order to apply this criterion, it is necessary not only to access the dk -marginal distributions to establish the most effective split, but it is also necessary to access the d marginal distributions (which have size n ) in order to compute the maximum marginal SSE , that corresponds to the value need of the bucket.
 B.3 Using pre-computation for evaluating greedy criteria Each invocation of E v aluate ( G , b ) can be accomplished more efficiently if the array F of partial sums and the array F 2 of partial square sums are available. F and F 2 have volume ( n  X  F [ i 1 ,..., i d ]=  X  each element F 2 [ i 1 ,..., i d ] is either 0 (if i j = 0forsome j  X  X  1 .. d ] )orthesumofall Figure 15 shows an example of arrays of partial sums and partial square sums.

By using F and F 2 both the SSE of a block b and the reduction the SSE due to a split of b can be computed efficiently, as both sum ( b ) and sumSquare ( b ) can be evaluated by accessing 2 d elements of F and F 2 , instead of accessing all the elements of b . For instance, (  X  as follows: and In these expressions:  X  b = &lt; [ l 1  X  1 .. u 1 ] ,..., [ l d  X  1 .. u d ] &gt; ;  X  v rt ( b ) is the set of vertices 4 of b ;  X  uv ( b ) = u  X  C ( i , j ) = d Then, for any splitting position dim , pos , once sum ( b l ) , sum ( b h ) ,and sumSquare ( b ) have been computed, Red ( b , dim , pos ) , Red marg ( b , dim , pos ) and SSE ( b ) can be evalua-ted using formulas ( 1 ), ( 2 ), ( 3 ).
 B.4 Complexity of Greedy algorithm In this section, we discuss the time complexity of Greedy algorithm for constructing histo-grams based on binary partitions under different greedy criteria. In particular, first we provide the proof of Theorem 2, where the complexity of Greedy algorithm is studied under the sparse data model. Then we extend the complexity analysis of Greedy algorithm to the cases that either the non-sparse data model or pre-computation are adopted.
 Proof of Theorem 2 The complexity bounds were obtained by multiplying the maximum number of iterations of Greedy algorithm (which are O ( X  max T ) ) for the cost of each iteration. The cost of each iteration of Greedy algorithm is dominated by the cost of evaluating the greedy criterion G on a bucket b , that is by the cost of computing E v aluate ( G , b ) (which has been computed in Appendix B.2 for FBH and HBH , and in Appendix B.2 for GHBH ). Theorem 3 Given a d -dimensional data distribution D with volume n d containing exactly N non-null points, the time complexity of the Greedy algorithms computing a histogram of type T ( where T is either FBH , HBH or k-GHBH ) on D, adopting either the non-sparse data model or pre-computation, are reported in Fig. 16 , where  X  = nif Max-Var marg /Max-Red marg criterion is adopted, and  X  = k for all the other greedy criteria.
 Proof Complexity bounds when pre-computation is not used were obtained by multiplying the maximum number of iterations of Greedy algorithm (which are O ( X  max T ) ) for the cost of each iteration. The cost of each iteration of Greedy algorithm is dominated by the cost of eva-(which has been computed in Appendix 5 for FBH and HBH , and in Appendix 5 for GHBH ). In the case that pre-computation of F and F 2 is performed, the cost of Greedy algorithm is given by the sum of three contributions: 1. PreComp : the cost of pre-computing F and F 2 ; 2. C U : the cost of all the updates to the priority queue; 3. C E : the cost of computing the function E v aluate for all the nodes to be inserted in the
These contributions can be computed as follows: (1) Both F and F 2 can be constructed  X  X ncrementally X , by accessing only once each cell (2) As to term C U , at each iteration of the algorithm the first element of the priority queue (3) Wedenoteas C ( E v aluate ( G , b )) thecostofcomputingfunction E v aluate ontheblock In the case that Max-Var marg / Max-Red marg is adopted, the cost of computing the d marginal SSEs of the block is O ( 2 d  X  d  X  n ) for either FBH , HBH and k-GHBH , and dominates the cost of computing the reductions of marginal SSE.
 Tosumup,whenpre-computationisadopted,wehavethat C ( E v aluate ( G , b )) = O ( 2 d  X  d  X  n ) for FBH and HBH ,and C ( E v aluate ( G , b )) = O ( 2 d  X  d  X   X ) for k-GHBH (where  X  = k for all the greedy criteria G but Max-Var marg / Max-Red marg , for which  X  = n ).
 Therefore, C E is given by: 1. in the case that T = FBH or HBH , 2. in the case that T = k-GHBH , C E = O ( X  max GHBH  X  2 d  X  d  X   X ) Observe that in the case of FBH and HBH under any greedy criterion, as well as in the case of k-GHBH under Max-Var marg / Max-Red marg , C U is negligible w.r.t. C E . In fact, the case of k-GHBH , when a criterion different from Max-Var marg / Max-Red marg is adopted, the inequalities log  X &lt; 2 d  X  d  X  k could not hold, even though, in practical cases, the number of buckets is likely not to exceed the value 2 2 d  X  d  X  k .

For the sake of presentation complexity bounds for Greedy algorithm under either the sparse representation model, the non-sparse one and pre-computation are summarized in the table of Fig. 17 . We point out that these bounds assume that all steps of Greedy algorithm have the same complexity. In fact it is unlikely that this case occurs, since as the histogram construction goes on, smaller and smaller buckets are generated, and each of these buckets contain fewer tuples than buckets generated at previous steps. Therefore we expect that, after the very first steps, Greedy algorithm deals with buckets whose volume is much smaller than n , whose marginal distributions have size much smaller than n , and containing fewer tuples than N .

Experimental results comparing the efficiency of the three different approaches (the ones based on the sparse data model, the non-sparse one, and pre-computation, respectively) are provided in Sect. 5 .
 B.5 Workspace size for Greedy algorithm Implementing Greedy algorithm with the adoption of pre-computation becomes unfeasible for high-dimensionality data, due to the explosion of the spatial complexity: the space needed to store F and F 2 grows exponentially as dimensionality increases, even if the number of non-null values remains nearly the same. In real-life scenarios, it often occurs that N n d , especially for high-dimensionality data: as dimensionality increases, data become sparser and the size of the data domain increases much more dramatically w.r.t. the number of non-null data.
 On the contrary, Greedy algorithm under the sparse data model is much less sensitive on the increase of dimensionality, also from the point of view of the workspace size. In this case, Greedy algorithm can be implemented by associating to each element of the queue not only the boundaries of the corresponding bucket b , but also the set of tuples belonging to b . Thus, when a bucket b is chosen and split into b lo w and b high , tuples of b are distributed among b b high ] is computed by scanning only the tuples belonging to b lo w [resp. b high ]. That is, the partition underlying the histogram is used as an index to locate the tuples contained in the buckets. Therefore, the algorithm workspace (i.e., the storage space needed to store q )is O ( d  X  N ) (instead of O ( n d ) , as in the case of pre-computation), since each non-null point belongs to exactly one bucket of the partition at each step of the algorithm.
 B.6 Execution time of Greedy algorithm In this section, we present some experimental results studying how the execution times of Greedy algorithms constructing a GHBH depend on several parameters, such as the storage space (i.e., number of buckets), the density, the volume, the dimensionality of D ,andthe grid degree. In particular, we have compared the execution times when either the sparse data model, or the non-sparse one, or pre-computation is adopted.
 Diagrams in Fig. 18 have been obtained for greedy algorithms adopting the Max-Var / Max-Red constructing an 8-GHBH .
 Experimental results reported in Fig. 18 are basically consistent with the complexity bounds of Fig. 17 , and can be summarized as follows:  X  when the sparse model is used, the execution time of Greedy algorithm is sensitive only  X  otherwise (if either the non-sparse data model is adopted or pre-computation is performed)  X  when the sparse data model is used, if data density is smaller than a threshold, Greedy Figure 18 c shows that execution times of greedy algorithms is not very sensitive w.r.t. the size of the available storage space. This can be explained as follows:  X  in the case that pre-computation is performed, the cost of the pre-computation step  X  in the other cases, the largest portion of execution times is devoted to the computation Experimental results analyzing how execution times depend on the grid degree are shown in Fig. 19 . They have been obtained on a three-dimensional data distribution with volume 300  X  300  X  300 containing 135,000 non-null values, using a storage space of 10,000 words. From the diagram above it emerges that execution times slightly depend on the grid degree. This could be due to the fact that, as the grid degree increases, the number of splits to be evaluated at each step of the construction increases, but the number of steps of the algorithm decreases (as the number of buckets which can be stored within the given storage space decreases). For the sake of brevity, we do not provide experimental results on execution times of greedy algorithms constructing an HBH . However it is easy to see that an HBH can be viewed as a GHBH with high degree.
 References Authors Biography
