 Previous research has suggested the permutation test as the theoretically optimal statistical significance test for IR evaluation, and advocated for the discontinuation of the Wilcoxon and sign tests. We present a large-scale study comprising nearly 60 million system comparisons showing that in practice the bootstrap, t-test and Wilcoxon test out-perform the permutation test under different optimality cri-teria. We also show that actual error rates seem to be lower than the theoretically expected 5%, further confirming that we may actually be underestimating significance.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Performance evaluation.
 Evaluation, Statistical significance, Randomization, Permu-tation, Bootstrap, Wilcoxon test, Student X  X  t-test, Sign test.
An Information Retrieval (IR) researcher is often faced with the question of which of two IR systems, A and B ,per-forms better. She conducts an experiment with a test col-lection, and chooses an effectiveness measure such as Aver-age Precision or nDCG. Based on the effectiveness difference she concludes that, for instance, system A is better. But we know there is inherent noise in the evaluation for a wealth of reasons concerning document collections, topic sets, rel-evance assessors, etc. Therefore the researcher needs the conclusion to be reliable, that is, the observed difference un-likely to have happened just by random chance. She employs a statistical significance test to compute this probability (the p -value). If p  X   X  (the significance level, usually  X  =0 .  X  =0 . 01) the difference is considered statistically significant (
A B ). In practice this means that she can be confident that the difference measured with a similar test collection will be (at least) as large as currently observed. If p&gt; X  the difference is not significant ( A B ), and she can not be confident that the observed difference is indeed real.
Unfortunately, there has been a debate regarding statisti-cal significance testing in IR evaluation. Classical tests such as the paired t-test, the Wilcoxon test and the sign test make different assumptions about the distributions, and ef-fectiveness scores from IR evaluations are known to violate these assumptions. The bootstrap test is an alternative that makes fewer assumptions and has other advantages over clas-sical tests, and the permutation or randomization test is an even less stringent test in terms of assumptions that theoreti-cally provides exact p -values. Because IR evaluations violate most of the assumptions, it is very important to know how robust these tests are in practice and which one is optimal. Previous work [4, 5] compared these five tests with TREC Ad Hoc data, reaching the following conclusions: a) the bootstrap, t-test and permutation test largely agree with each other, so there is hardly any practical difference in us-ing one or another; b) the permutation test should be the test of choice, though the t-test seems suitable as well; the bootstrap test shows a bias towards small p -values; c) the Wilcoxon and sign tests are unreliable and should be dis-continued for IR evaluation. However, all these conclusions were based on the assumption that the permutation test is optimal. For example, authors showed that the Wilcoxon and sign tests fail to detect significance when the permuta-tion test does and vice versa. That is, they are unreliable according to the permutation test.

But we may follow different criteria to chose an optimal test. We may want the test to be powerful , that is, to pro-duce significant results as often as possible. Additionally, we may want it to be safe and yield low error rates so that it is unlikely that we draw wrong conclusions. But power and safety are inversely related; different tests show different re-lations depending on the significance level. The lower  X  the lower the power, because we need p  X   X  for the result to be significant. Error rates are expected to be at the nominal  X  level, so the higher the significance level the higher the expected error rate. The test is exact if we can trust that the actual error rate is as dictated by the significance level. If it is below it means we are being too conservative and we are missing significant results; if it is above it means we are deeming as significant results that probably are not.
This paper presents a large-scale empirical study that compares all five tests according to these optimality criteria, providing significance and error rates at various significance levels for 50-topic sets. Our main findings are:
To compare the five statistical significance tests at hand, we employed data from the TREC 2004 Robust Track. A total of 249 topics were used, 100 of which were originally de-veloped in the TREC 7 and 8 Ad Hoc tracks (50 and 50). A total of 110 runs were submitted by 14 different groups. This dataset is unusually large both in terms of topics and runs, given that TREC tracks usually employ 50 topics. The sub-set with the 100 Ad Hoc topics is especially interesting: all 100 topics were developed and judged by the same assessors for the most part, and they were developed using the same methodology and pooling protocol with roughly the same number of runs contributing to the pools [6]. Additionally, all three tracks used disks 4 and 5 as document collection. Therefore, we can consider these two sets of 50 topics as two different samples drawn from the same universe of topics.
We randomly split these 100 topics into two disjoint sub-sets of 50 topics each: T and T . For each of these two subsets we evaluated all 110 runs as per Average Precision. This provides us with 5,995 system pairwise comparisons with T and another 5,995 with T . We ran all five statis-tical significance tests between each of these system pairs This gives us a total of 5,995 pairs of p -values per test, which can be regarded as the two p -values observed with two dif-ferent test collections for any two systems. We performed 1,000 random trials of this experiment, so we have a total of 5,995,000 system pairwise comparisons and the correspond-ing 5,995,000 with another topic subset. Thus, this paper reports results on nearly 12 million p -values for each of the five tests, for a grand total of nearly 60 million p -values. To our knowledge, this is to date the largest study of this type.
Given an arbitrary topic set split, the 5,995 pairs of p -values provided by a test can be used to study its optimality. Consider a researcher that used topic subset T and ran a test to compute a p -value; under the significance level  X  draws a conclusion. What can he expect with a different topic subset T ?. One of these situations can occur:
As in [4, 5], we calculated 100,000 samples in the permu-tation and bootstrap tests.

A powerful test minimizes the non-significance rate, a safe test minimizes the minor and major error rates, and an exact test maintains the global error rate at the nominal  X  level.
For every statistical significance test we computed the non-significance, success, lack of power and error rates at 32 significance levels  X   X  X  0.0001, ..., 0.0009, 0.001, ..., 0.009, ..., 0.1, ..., 0.5 } . Tables 1 and 2 report the results for a selection of significance levels, and Figures 1 and 2 plot detailed views in the arguably most interesting [0 . 001 , range. Please note that all plots are log-scaled.

Non-significance rate. The bootstrap test consistently produces smaller p -values, and it is therefore the most pow-erful of all tests across significance levels. Next are the per-mutation test for  X &lt; 0 . 01 and the Wilcoxon test for the usual  X   X  0 . 01. The t-test is consistently less powerful, though the difference is as small as roughly 1% fewer signifi-cant results at the usual  X  =0 . 05. The sign test is by far the least powerful of all five. Its stair-like behavior is explained by its resolution: p -values depend only on the sign of the score differences, not on the magnitude (see Figure 5 in [4]).
Success rate. The bootstrap and Wilcoxon tests are the most successful overall. For small significance levels  X   X  0 . 001 the bootstrap test shows the highest success rate, but for the more usual levels 0 . 001 &lt; X   X  0 . 05 the Wilcoxon test performs better. Next are again the permutation test and the t-test, with very similar success rates about 0.3% lower than the Wilcoxon and bootstrap tests at the usual  X  levels. The sign test is clearly the worst of all.
Lack of power rate. Most of the unsuccessful compar-isons are due to a lack of power with the second topic subset T . Relative results are comparable to results above: the bootstrap test dominates at small significance levels and the Wilcoxon tests dominates at the usual levels, again followed by the permutation test and the t-test.

Minor error rate. Except for rare occasions where the sign test X  X  step-like behavior results in the smallest minor error rate, the t-test is generally the safest of all five across significance levels. The permutation test follows next with rates about 0.03% higher. The bootstrap test is consis-tently outperformed by the t-test and the permutation test; it yields 0.13% more minor errors at  X  =0 . 05. The Wilcoxon test performs even better than the permutation test for low significance levels, but it performs worse at the usual levels. As mentioned, the sign test wiggles between the other tests.
Majorerrorrate. Similarly the t-test consistently per-forms best in terms of major errors, followed by the permu-tation and bootstrap tests. It is noticeable that for small significance levels neither of these three tests show any ma-jor error at all. For instance, at  X  =0 . 005 the t-test provides as many as 3,006,441 (50.2%) significant comparisons, and yet none of them results in a major error with the second topic subset. The Wilcoxon test outperforms the permu-tation test sporadically, but it performs worse overall. In general though, it is important to the bear in mind the mag-nitudes of the major error rates. For instance, at  X  =0 . the t-test produced 1,082 major errors and the bootstrap test produced 1,523. While the difference may seem small compared to the total of significants (0.0277% vs 0.0383%), this is actually a large +41% relative increase. The sign test is clearly the worst of all, having an extremely large major error rate at small significance levels. (lower is better), and global error rates over total of significants (
Global error rate. Aggregating minor and major errors we have a global error rate that can be used as an overall indicator of test safety and exactness. Given the relative size of minor and major error rates, the trends are here nearly the same as fwith minor errors, but for the sake of complete-ness we plot the full range of significance levels. The t-test approximates best the nominal error rate for low significance levels, but the Wilcoxon test does better for the usual levels and best overall. Surprisingly the permutation test does not seem to be the most exact at any significance level.
Zobel [7] compared the t-test, Wilcoxon test and ANOVA at  X  =0 . 05, though with only one random split in 25-25 topics. He found lower error rates with the t-test than with the Wilcoxon test, and generally lower than the nominal 0.05 level. Given that the latter showed higher power and has more relaxed assumptions, he recommended it over the t-test. Sanderson and Zobel [3] ran a larger study also with splits of up to 25-25 topics. They found that the sign test has higher error rates than the Wilcoxon test, which has itself higher error rates than the t-test. They also suggested that the actual error rate is below the nominal 0.05 level when using 50 topic sets. Voorhees [6] also observed error rates below the nominal 0.05 level for the t-test, but more unstable effectiveness measures resulted in higher rates. Cormack and Lynam [1] used 124-124 topic splits and various significance levels. They found the Wilcoxon test more powerful than the t-test and sign test; and the t-test safer than the Wilcoxon and sign test. Sakai [2] proposed the bootstrap method for IR evaluation, but did not compare it with other tests.
Smucker et al. [4] compared the same five tests we study in this paper, arguing that the t-test, permutation and boot-strap tests largely agree with each other. Nonetheless, they report RMS Errors among their p -values of roughly 0.01, which is a large 20% for p -values of 0.05. Based on the ar-gument that the permutation test is theoretically exact, they concluded that the Wilcoxon and sign tests are unreliable, suggesting that they should be discontinued for IR evalua-tion. They find the bootstrap test to be overly powerful, and given the appealing theoretical optimality of the per-mutation test they propose its use over the others, though the t-test admittedly performed very similarly. In a later paper [5] they found that the tests tended to disagree with smaller topic sets, though the t-test still showed acceptable agreement with the permutation test, again assumed to be optimal. The bootstrap test tended again to produce smaller p -values, so authors recommend caution if using it.
In this paper we ran a large-scale study to revisit these issues under different optimality criteria. In terms of safety, the t-test produced the smallest error rates across signifi-cance levels, followed by the Wilcoxon test for low levels and the permutation test for usual levels. In general, all tests yielded error rates higher than expected for low sig-nificance levels, but much lower for the usual levels. This suggests that we are being too conservative when assessing statistical significance at  X  =0 . 05; we expect 5% of our significant results to be wrong, but in practice only about 1.3% do indeed seem wrong. We must note though that this global error rate, as the sum of minor and major errors, is just an approximation of the true Type I error rates [1].
Table 3 shows the agreement of the five tests with them-selves: p -values with topic subset T compared to those with Table 3: RMS Error of all five tests with themselves (lower is better). Best per bin in bold face. subset T . The Wilcoxon test turns out to be the most sta-ble of all for very small p -values, and generally more so than the permutation test. The t-test is the most stable overall. Indeed, if we compute the difference between the actual and nominal error rates we find that the Wilcoxon test is the one that best tracks the significance level and therefore seems to be the most exact (RMSE 0.1146), followed by the boot-strap, t-test, sign and permutation tests (RMSEs 0.1148, 0.1153, 0.1153 and 0.1155). This is particularly interesting for the bootstrap test: it provides the most significant re-sults and the actual error rate is still lower than expected.
In summary, a researcher that wants to maximize the number of significant results may use the more powerful bootstrap test and still be safe in the usual scenario. Re-searchers that want to maximize safety may use the t-test, and researchers that want to be able to trust the signifi-cance level may proceed with the Wilcoxon test. For large meta-analysis studies we encourage the use of the t-test and Wilcoxon test because they are far less computationally ex-pensive and show near-optimal behavior. Unlike previous work concluded, our results suggest that in practice the per-mutation test is not optimal under any criterion. Further analysis with varied test collections and effectiveness mea-sures should be conducted to clarify this matter, besides devising methods to better approximate what actual Type I error rates we have in IR evaluation. We further support the argument of discontinuing the sign test. [1] G. V. Cormack and T. R. Lynam. Validity and Power [2] T. Sakai. Evaluating Evaluation Metrics Based on the [3] M. Sanderson and J. Zobel. Information Retrieval [4] M. D. Smucker, J. Allan, and B. Carterette. A [5] M. D. Smucker, J. Allan, and B. Carterette. Agreement [6] E. M. Voorhees. Topic Set Size Redux. In ACM SIGIR , [7] J. Zobel. How Reliable are the Results of Large-Scale
