 We consider feature selection for text classification both t he-oretically and empirically. Our main result is an unsuper-vised feature selection strategy for which we give worst-ca se theoretical guarantees on the generalization power of the resultant classification function  X  f with respect to the classi-fication function f obtained when keeping all the features. To the best of our knowledge, this is the first feature selec-tion method with such guarantees. In addition, the anal-ysis leads to insights as to when and why this feature se-lection strategy will perform well in practice. We then use the TechTC-100, 20-Newsgroups, and Reuters-RCV2 data sets to evaluate empirically the performance of this and two simpler but related feature selection strategies against t wo commonly-used strategies. Our empirical evaluation shows that the strategy with provable performance guarantees per -forms well in comparison with other commonly-used feature selection strategies. In addition, it performs better on ce r-tain datasets under very aggressive feature selection. Categories and Subject Descriptors: E.m [Data] : Mis-cellaneous; H.m [Information Systems] : Miscellaneous General Terms: Algorithms, Experimentation Keywords: Feature Selection, Text Classification, Random Sampling, Regularized Least Squares Classification
Automated text classification is a particularly challengin g task in modern data analysis, both from an empirical and from a theoretical perspective. This problem is of central interest in many internet applications, and consequently i t has received attention from researchers in such diverse are as as information retrieval, machine learning, and the theory  X  Work done in part while visiting Yahoo! Research.
 Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. of algorithms. Challenges associated with automated text categorization come from many fronts: one must choose an appropriate data structure to represent the documents; one must choose an appropriate objective function to optimize in order to avoid overfitting and obtain good generalization ; and one must deal with algorithmic issues arising as a result of the high formal dimensionality of the data.

Feature selection, i.e., selecting a subset of the features available for describing the data before applying a learnin g algorithm, is a common technique for addressing this last challenge [4,13,17,20]. It has been widely observed that fe a-ture selection can be a powerful tool for simplifying or spee d-ing up computations, and when employed appropriately it can lead to little loss in classification quality. Neverthel ess, general theoretical performance guarantees are modest and it is often difficult to claim more than a vague intuitive un-derstanding of why a particular feature selection algorith m performs well when it does. Indeed, selecting an optimal set of features is in general difficult, both theoretically and em -pirically; hardness results are known [5 X 7], and in practic e greedy heuristics are often employed [4,13,17,20].
We address these issues by developing feature selection strategies that are: sufficiently simple that we can obtain non-trivial provable worst-case performance bounds that a c-cord with the practitioners X  intuition; and at the same time sufficiently rich that they shed light on practical applica-tions in that they perform well when evaluated against com-mon feature selection algorithms. Motivated by recent work in applied data analysis X  X or example, work on Regularized Least Squares Classification (RLSC), Support Vector Ma-chine (SVM) classification, and the Lasso shrinkage and se-lection method for linear regression and classification X  X h at has a strongly geometric flavor, we view feature selection as a problem in dimensionality reduction. But rather than employing the Singular Value Decomposition (which, upon truncation, would result in a small number of dimensions, each of which is a linear combination of up to all of the orig-inal features), we will attempt to choose a small number of these features that preserve the relevant geometric struct ure in the data (or at least in the data insofar as the particular classification algorithm is concerned). We will see that thi s methodology is sufficiently simple and sufficiently rich so as to satisfy the dual criteria stated previously.

In somewhat more detail:  X  We present a simple unsupervised algorithm for feature selection and apply it to the RLSC problem. The algorithm assigns a univariate  X  X core X  or  X  X mportance X  to every fea-ture. It then randomly samples a small (independent of the total number of features, but dependent on the number of documents and an error parameter) number of features, and solves the classification problem induced on those features .  X  We present a theorem which provides worst-case guar-antees on the generalization power of the resultant classifi -cation function  X  f with respect to that of f obtained by using all the features. To the best of our knowledge, this is the first feature selection method with such guarantees.  X  We provide additive-error approximation guarantees for any query document and relative-error approximation guar-antees for query documents that satisfy a somewhat stronger but reasonable condition with respect to the training docu-ment corpus. Thus, the proof of our main quality-of-approx-imation theorem provides an analytical basis for commonly-held intuition about when such feature selection algorithm s should and should not be expected to perform well.  X  We provide an empirical evaluation of this algorithm on the TechTC-100, 20-Newsgroups, and Reuters-RCV2 data-sets. In our evaluation, we use: the aforementioned univari -ate score function for which our main theorem holds X  X hich, due to its construction, we will refer to as subspace sam-pling (SS); random sampling based on two other score func-tions that are simpler to compute X  X ne based on weighted sampling (WS) and the other based on uniform sampling (US); as well as two common feature selection strategies X  Information Gain (IG) and Document Frequency (DF) X  that have been shown to perform well empirically (but for which the worst case analysis we perform would be quite difficult).  X  We show that our main SS algorithm performs similarly to IG and DF and also to WS (which has a similar flavor to DF). In certain cases, e.g., under very aggressive feature s e-lection on certain datasets, our main SS algorithm does bet-ter than the other methods. In other less aggressive cases, when it does similarly to IG, DF, and WS, we show that the univariate score that our provable SS algorithm computes is more closely approximated by the score provided by WS than it is at more aggressive levels of feature selection.
Learning a classification function can be regarded as ap-proximating a multivariate function from sparse data. This problem is ill-posed and is solved in classical regularizat ion theory by finding a function f that simultaneously has small empirical error and small norm in a Reproducing Kernel Hilbert Space (RKHS). That is, if the data consist of d exam-ples ( z 1 , y 1 ) , . . . , ( z d , y d ), where z i  X  R then one solves a Tikhonov regularization problem to find a function f that minimizes the functional: where V ( ., . ) is a loss function, || f || K is a norm in a RKHS H defined by the positive definite function K , d is the number of data points, and  X  is a regularization parameter [12, 34, 35]. Under general conditions [30], any f  X  H minimizing (1) admits a representation of the form: timization problem (1) can be reduced to finding a set of coefficients x i , i = { 1 , . . . , d } . The theory of Vapnik then justifies the use of regularization functionals of the form a p-pearing in (1) for learning from finite data sets [35]. If one chooses the square loss function, then, by combining (3) with (1) and (2), we obtain the following Regularized Least Squares Classification (RLSC) problem: where the d  X  d kernel matrix K is defined over the finite training data set and y is a d -dimensional { X  1 } class label vector [12,26,27].

As is standard, we will represent a document by an n -dimensional feature vector and thus a corpus of d training documents (where, generally, n  X  d ) as an n  X  d matrix A . Similarly, we will consider an identity mapping to the feature space, in which case the kernel may be expressed as K = A T A . If the Singular Value Decomposition (SVD) of A is A = U  X  V T , then the solution and residual of (4) may be expressed as: The vector x opt characterizes a classification function of the form (2) that generalizes well to new data. Thus, if q  X  R is a new test or query document, then from (2) it follows that our binary classification function is: That is, given a new document q that we wish to classify, if f ( q ) &gt; 0 then q is classified as belonging to the class in question, and not otherwise.
Feature selection is a large area. For excellent reviews, see [4,13,17,20]. Papers more relevant to the techniques we employ include [14, 18, 24, 37, 39] and also [19, 22, 31, 36, 38 , 40,42]. Of particular interest for us will be the Informatio n Gain (IG) and Document Frequency (DF) feature selection methods [39]. Hardness results have been described in [5 X 7] .
RLSC has a long history and has been used for text classi-fication: see, e.g., Zhang and Peng [41], Poggio and Smale [25 ], Rifkin, et. al. [27], Fung and Mangasarian (who call the procedure a Proximal Support Vector Machine) [15], Agar-wal [3], Zhang and Oles [42], and Suykens and Vandewalle (who call the procedure a Least Squares Support Vector Machine) [33]. In particular, RLSC performs comparable to the popular Support Vector Machines (SVMs) for text categorization [15, 27, 33, 42]. Since it can be solved with vector space operations, RLSC is conceptually and theoreti -cally simpler than SVMs, which require convex optimization techniques. In practice, however, RLSC is often slower, in particular for problems where the mapping to the feature space is not the identity (which are less common in text cat-egorization applications). For a nice overview, see [26,27 ].
We note in passing that if a hinge loss function is used instead of the square loss function of (3), i.e., if we set classical SVM problem follows from (1). The proof of our main theorem will make use of matrix perturbation theory and the robustness of singular subspaces to the sampling implicit in our feature selection procedure; see [8, 11] and also [32]. We expect that our methodology will extend to SVM classification if one can prove analogous robustness re-sults for the relevant convex sets in the SVM optimization.
In this section, we describe our main sampling algorithm for feature selection and classification. Recall that we hav e a corpus of d training documents, each of which is described by n  X  d features. Our main goal is to choose a small number r of features, where d . r  X  n , such that, by using only those r features, we can obtain good classification quality, both in theory and in practice, when compared to using the full set of n features. In particular, we would like to solve exactly or approximately a RLSC problem of the form (4) to get a vector to classify successfully a new document according to a classification function of the form (6).

In Figure 1, we present our main algorithm for perform-ing feature selection, the SRLS Algorithm . The algo-rithm takes as input the n  X  d term-document (or feature-document) matrix A , a vector y  X  R d of document labels where sign( y j ) labels the class of document A ( j ) (where A denotes the j th column of the matrix A and A ( i ) denotes the i th row of A ), and a query document q  X  R n . It also takes as input a regularization parameter  X   X  R + , a probability distribution { p i } n i =1 over the features, and a positive integer r . The algorithm first randomly samples roughly r features according to the input probability distribution. Let  X  A be the matrix whose rows consist of the chosen feature vectors, rescaled appropriately, and let  X  q be the vector consisting of the corresponding elements of the input query document q , rescaled in the same manner. Then, if we define the d  X  d matrix  X  K =  X  A T  X  A as our approximate kernel, the algorithm next solves the following RLSC problem: thereby obtaining an optimal vector  X  x opt . Finally, the algo-rithm classifies the query q by computing, If  X  f  X  0, then q is labeled  X  X ositive X ; and otherwise, q is labeled  X  X egative X . Our main theorem (in the next section) of A and  X  ) for relating the value of  X  q T  X  A  X  x opt of q T Ax opt from (6) obtained by considering all n features.
An important aspect of our algorithm is the probability distribution { p i } n i =1 input to the algorithm. One could per-form random sampling with respect to any probability dis-tribution. (Indeed, uniform sampling has often been pre-sented as a  X  X traw man X  for other methods to beat.) On the other hand, as we show in Sections 4 and 6, more intelligent sampling can lead to improved classification performance, both theoretically and empirically. Also, note that rather than using the probability distribution { p i } n i =1 over the fea-tures directly in r i.i.d. sampling trials (which might lead to the same feature being chosen multiple times), the SRLS
Input: A  X  R n  X  d ; y  X  R d ; q  X  R n ;  X   X  R + ; { p i  X  [0 , 1] : i  X  [ n ] , p i  X  0 , P i p i = 1 } , and a positive integer r  X  n .
Output: A solution vector  X  x opt  X  R d ; a residual  X  Z  X  R ; and a classification  X  f . for i = 1 , . . . , n do end Set  X  K =  X  A T  X  A ; Solve  X  x opt = arg min x  X  R d  X   X   X  Kx  X  y  X   X  2 2 +  X x Set  X  Z =  X   X   X  K  X  x opt  X  y  X   X  2 2 +  X   X  x T opt  X  Compute  X  f = f (  X  q ) =  X  q T  X  A  X  x opt .
 Figure 1: SRLS Algorithm: our main algorithm for Sampling for Regularized Least Squares classifica-tion.
 Algorithm computes, for every i  X  { 1 , . . . , n } , a proba-bility  X  p i = min { 1 , rp i }  X  [0 , 1], and then the i is chosen with probability  X  p i . Thus, r actually specifies an upper bound on the expected number of chosen rows of A : if X i is a random variables that indicates whether the i row is chosen, then the expected number of chosen rows is r = E [ P i X i ] = P i min { 1 , rp i } X  r P i p i = r .
In this section, we provide our main quality-of-approx-imation theorems for the SRLS Algorithm . In these the-orems, we will measure the quality of the classification by comparing the classification obtained from (6) using the out -put of an exact RLSC computation with the classification obtained from (8) using the output of the SRLS Algo-rithm , which operates on a much smaller set of features. The proof of these two theorems will be in Section 5.
Before stating these results, we review notation. Recall that: A is an n  X  d full-rank matrix, whose d columns corre-spond to d objects represented in an n -dimensional feature space; y is a d -dimensional class-indicator vector, i.e., the i th entry of y denotes the class membership of the i th ob-ject;  X   X  0 is a regularization parameter. If we denote the SVD of A as A = U  X  V T , then: U is the n  X  d matrix whose columns consist of the left singular vectors of A ;  X  max  X  min denote the largest and smallest, respectively, singular values of A ;  X  A =  X  max / X  min is the condition number of A ; and we will denote by U  X  any n  X  ( n  X  d ) orthogonal matrix whose columns span the subspace perpendicular to that spanned by the columns of U . In this case, a query document q  X  R n may be expressed as: for some vectors  X   X  R d and  X   X  R n  X  d . (Note that the  X  X cale X  of  X  is different from that of  X  , which for simplicity we have defined to account for the singular value informa-tion in A ; this will manifest itself in the coefficients in the expressions of our main results.) Of course,  X  A ,  X  q ,  X  x defined in the SRLS Algorithm of Figure 1, and x opt is an optimum of (4), as defined in (5).
 Our first theorem establishes that if we randomly sam-ple roughly  X  O ( d/ X  2 ) features according to a carefully chosen probability distribution of the form i.e., proportional to the square of the Euclidean norms of the rows of the left singular vectors of the ( n  X  d with n  X  d ) matrix A , then we have an additive-error approximation bound for any query vector q . (We will use the common  X  notation to hide factors that are polylogarithmic in d and  X  for ease of exposition.)
Theorem 1. Let  X   X  (0 , 1 / 2] be an accuracy parame-ter. If the SRLS Algorithm (of Figure 1) is run with r =  X  O ` d/ X  2  X  and with sampling probabilities of the form (9), then, with probability at least 0 . 98 : Note that (except for the trivial case where  X  = 0), our the-orem provides no guideline for the choice of  X  . The second bound holds regardless of the choice of  X  , which we will see is conveniently eliminated in the proof.

Note that the error bounds provided by Theorem 1 for the classification accuracy of our feature selection algori thm depend on: the condition number of A  X  X his is a very com-mon dependency in least squares problems; the amount of the query vector q that is  X  X ovel X  with respect to the train-ing set documents X  k  X  k 2 measures how much of q lies out-side the subspace spanned by the training set documents; as well as the alignment of the class membership vector y with the part of the query document q that lies in the sub-space spanned by the columns of A . In particular, notice, for example, that if  X  = 0, namely if there is no  X  X ovel X  component in the query vector q (equivalently, if q may be expressed as a linear combination of the documents that we have already seen without any information loss), then the error becomes exactly zero if  X  = 0. If  X  &gt; 0 and  X  = 0, then the second term in (10) is zero.

One important question is whether one can achieve rela-tive error guarantees. We are particularly interested in th e case where  X  = 0 (the query vector has no new components), and  X  &gt; 0. The following theorem states that under addi-tional assumptions we get such relative error guarantees. I n particular, we need to make an assumption about how the query vector q interacts with the class discrimination vec-tor y , so that we can replace the product of norms with the norm of products.

Theorem 2. Let  X   X  (0 , 1 / 2] be an accuracy parame-ter, and let  X  &gt; 0 . Assume that the query document q lies entirely in the subspace spanned by the d training doc-uments (the columns of A ), and that the two vectors V T y and ` I +  X   X   X  2  X   X  1 V T  X  are  X  X lose X  (i.e., almost parallel or anti-parallel) to each other, in the sense that  X   X   X  for some small constant  X  . If the SRLS Algorithm (of Figure 1) is run with r =  X  O ` d/ X  2  X  and with sampling prob-abilities of the form (9), then, with probability at least 0 . 98 ,
Recall that the vector  X  contains the coefficients in the expression of q as a linear combination of the columns of A ; hence, the assumption of Theorem 2 is related to the assumption that  X  is  X  X lose X  to the classification vector y . (Notice that V is a full rotation, and ` I +  X   X   X  2  X   X  1 tially discounts the smaller singular values of A .) Thus, The-orem 2 quantifies the intuition that query vectors that are clearly correlated with the class discrimination axis will have smaller classification error. On the other hand, Theorem 1 indicates that ambiguous query vectors (i.e., vectors that are nearly perpendicular to the class indicator vector) will ha ve higher classification errors after sampling since such vect ors depend on almost all their features for accurate classifica-tion.
For notational convenience in the proofs in this section, we define an n  X  n diagonal sampling matrix S . The diagonal entries of this matrix are determined by  X  X oin flips X  of the SRLS Algorithm . In particular, for all i = 1 , . . . , n , S 1 /  X  p i with probability  X  p i = min { 1 , rp i } ; otherwise, S Here, the p i are defined in (9). Intuitively, the non-zero entries on the diagonal of S correspond to the rows of A that the algorithm selects.
Our proof will rely on the following three lemmas from matrix perturbation theory.

Lemma 3. For any matrix E such that I + E is invertible, ( I + E )  X  1 = I + P  X  i =1 (  X  E ) i .
 Lemma 4. Let X and  X  X = X + E be invertible matrices. For a proof, see Stewart and Sun [32], pp. 118.
 Lemma 5. Let D and X be matrices such that the product DXD is a symmetric positive definite matrix with X ii = 1 . Let the product DED be a perturbation such that Here  X  min ( X ) corresponds to the smallest eigenvalue of X . Let  X  i be the i -th eigenvalue of DXD and let  X   X  i be the i -th eigenvalue of D ( X + E ) D . Then, For a proof, see Demmel and Veseli  X c [10].
 Let A = U  X  V T be the SVD of A . Define, Here E denotes how far away U T SU is from the identity. We will apply Lemma 5 on the matrix product  X  U T U  X , which is symmetric positive definite. (Notice that the matri x D of the lemma is  X  and the matrix X of the lemma is U
T U = I , thus X ii = 1 for all i .) Towards that end, we need to bound the spectral norm of E , which has been provided by Rudelson and Vershynin in [28].

Lemma 6. Let  X   X  (0 , 1 / 2] . Let  X  p i = min { 1 , rp i be as in (9) , and let r =  X  O ( d/ X  2 ) . Then, with probability at least 0.99, We can now immediately apply Lemma 5, since the spectral norm of the perturbation is strictly less than one, which is the smallest eigenvalue of U T U = I . Since  X  is symmetric positive definite, the i -th eigenvalue of  X  is equal to the i -th singular value of  X ; also, the i -th eigenvalue of  X  U T U  X  is equal to  X  2 i , where  X  i =  X  ii . Thus Lemma 5 implies
Lemma 7. Let  X  i be the singular values of  X  . Then, with probability at least 0.99, for all i = 1 . . . d .
 The following lemma is the main result of this section and states that all the matrices of interest are invertible. Lemma 8. Using the above notation:  X  2 is invertible;  X  2 +  X I is invertible for any  X   X  0 ;  X  is invertible with probability at least 0 . 99 ;  X  +  X I is invertible for any  X   X  0 with probability at least 0 . 99 ; and I + E is invertible with probability at least 0 . 99 .
 Proof. The first two statements follow trivially from the fact that A is full-rank. The third statement follows from Lemma 7 and the fourth statement follows by an analogous argument (omitted). The last statement follows from the fact that the spectral norm of E is at most  X  , hence the singular values of I + E are between 1  X   X  and 1 +  X  .
In this subsection, we will bound the difference between q
Ax opt and  X  q T  X  A  X  x opt . This bound provides a margin of error for the generalization power of  X  A  X  x opt in classifying an arbitrary new document q with respect to the generalization power of q T Ax opt . The following lemma provides a nice expression for  X  x opt .

Lemma 9. With probability at least 0 . 99 , Sketch of the proof. Writing down the normal equations for the sampled problem, and using the orthogonality of V and the invertibility of  X  +  X I and  X  provides the formula for  X  x opt .

We now expand q into two parts: the part that lies in the subspace spanned by the columns of A and the part that lies in the perpendicular subspace, i.e., in the span of U Using A = U  X  V T and substituting x opt from (5), we get In the above we used the fact that U  X  T U = 0 and the in-vertibility of  X  2 and  X  2 +  X I . We now focus on  X  q T  X  which may be rewritten (using our sampling matrix formal-ism from Section 5.1) as q T SA  X  x opt .  X   X   X  We will bound (17) and (18) separately. Using the formula for  X  x opt from Lemma 9 and  X  =  X  U T SU  X  =  X  ( I + E )  X  we get To understand the last derivation notice that, from Lemma 3, ( I + E )  X  1 = I +  X , where  X  = P  X  i =1 (  X  E ) i . We now bound the spectral norm of  X . k  X  k 2 = using Lemma 6 and the fact that  X   X  1 / 2. We are now ready to bound (17).  X   X   X   X   X   X   X  Using Lemma 4 and noticing that all matrices involved are invertible, we bound the above quantity by der to complete the bound for the term in (17) we bound the spectral norm of  X .
 Since we already have bounds for the spectral norms of  X ,  X   X  1 , and  X , we only need to bound the spectral norm of `  X  2 +  X I +  X   X   X   X  1 . Notice that the spectral norm of this matrix is equal to the inverse of the smallest singular value of  X  +  X I +  X   X . Standard perturbation theory of matrices [32] and (19) imply that, Here  X  i ( X ) denotes the i th singular value of the matrix X . of A , Thus, Here we let  X  min be the smallest singular value of A , and  X  max be the largest singular value of A . Combining all the above and using the fact that k  X  k 2 k  X  k  X  1 2 =  X  max  X 
A (the condition number of A ), we bound (17):  X   X   X   X   X  X  X  A  X  2 We now proceed to bound the term in (18). where the first inequality follows from  X  = U  X  T q ; and the second inequality follows from the lemma below (whose proof is omitted X  X t is similar to Lemma 4 . 3 from [11]).
Lemma 10. Let  X   X  (0 , 1 / 2] . Given our notation, and our choices for  X  p i , p i , and r , with probability at least 0.99, To conclude the proof, we will bound the spectral norm of It is now enough to get a lower bound for the smallest sin-gular value of I +  X   X   X  2 + E . We will compare the singular values of this matrix to the singular values of I +  X   X   X  2 From standard perturbation theory, and hence using  X  max / X  min =  X  A , In the above we used the fact that  X   X  1 / 2, which implies that (1  X   X  ) +  X / X  2 max  X  1 / 2. Combining the above, we get a bound for (18).  X   X   X 
In order to prove Theorem 1 for the case  X  = 0, notice that equation (20) becomes zero. For the case  X  &gt; 0, notice that the denominator  X  2 min +(1  X   X  )  X  in (20) is always larger than (1  X   X  )  X  , and thus we can upper bound the prefactor in (20) by 2  X  X  A (since  X   X  1 / 2). Additionally, using  X   X  k  X  k 2 (since V is a full-rank orthonormal matrix),  X   X   X  The last inequality follows from the fact that the singular values of I +  X   X   X  2 are equal to 1 +  X / X  2 i ; thus, the spectral norm of its inverse is at most one. Combining the above with (20) and using  X   X  V T y  X   X  2 = k y k 2 concludes the proof of Theorem 1.

The proof of Theorem 2 follows by noticing that if  X  is all-zeros the right-hand side of (21) is zero. Since  X  &gt; 0, we can use the aforementioned argument to bound the prefactor in (20) by 2  X  X  A , which concludes the proof.
In this section, we describe our empirical evaluations on three datasets: TechTC-100 [9]; 20-Newsgroups [1,2,21]; a nd Reuters RCV2 [23]. We compare several sampling-based fea-ture selection strategies to feature selection methods com -monly used in Information Retrieval (IR). Our aim is to compare classification results after performing feature se lec-tion with classification results from the original problem. Table 1 summarizes the structure of the three datasets. The TechTC-100 family [9,16] consists of 100 datasets, each Name Classes Terms Train Test
TechTC-100 2 20K 100  X  120 100  X  30 20-Newsgroups 20 62k 15k 4k
Reuters-rcv2 103 47k 23k 10k having roughly 150 documents evenly spread across two classes. The categorization difficulty of these datasets, as measured in [9] (using the baseline SVM accuracy), is uni-formly distributed between 0 . 6 and 1 . 0. Each dataset is stemmed, normalized according to SMART-ltc [29], and then split into four different test-train splits. The ratio of tes t to train documents we used is 1 : 4.

The 20-Newsgroups dataset [1, 2, 21], which consists of postings from 20 Usenet newsgroups, is well used in the IR literature. The dataset consists of 20 classes, each corre-sponding to a newsgroup, containing almost an equal num-ber of documents. We used the document vectors provided by Rennie et al. [1], who applied the usual stemming and ltc normalization to this dataset, and split it into ten test-tr ain splits. We employ only the first five splits for our empirical evaluations.

The last dataset is a subset of Reuters-RCV2 [23], that contains news-feeds from Reuters. We considered only the 103 topic codes as the classes. The class structure in Reuter s is hierarchical, and as a result the sizes of the classes are highly skewed, with the 21 non-leaf classes accounting for 79% of the total number of documents. We considered all 103 topics as separate classes. We use both the ltc -normalized term-document matrix and the one test-train split provided by Lewis et al. [23] for this dataset. For ef-ficiency purposes, instead of using all 800K test documents for classification, we randomly select 10K test documents and report results on these.

Finally, for each of these datasets, we used our SRLS clas-sifier with feature selection in a simple one-vs-all format.
We investigate the following three sampling-based feature selection strategies. Since these strategies are randomiz ed, we need only specify the probability distribution { p i } that is passed to the SRLS Algorithm .  X  Subspace Sampling (SS). The probability of choosing each feature is proportional to the length squared of the corresponding row of the matrix U k consisting of the top k left singular vectors of A , i.e., (Thus, for our empirical evaluation we generalize Equation (9) to permit k to be a parameter.)  X  Weight-based Sampling (WS). The probability of choos-ing each feature is proportional to the length squared of the corresponding row of the matrix A , i.e.,  X  Uniform Sampling (US) . The probability of choosing each feature is equal, i.e., p i = 1 /n , for all i = 1 , . . . , n .
We compare the performance of these three strategies with that of the following two deterministic feature selec-tion methods that are well-known in the IR literature.  X  Document Frequency (DF) . The document frequency of a term is the number of training documents in which it appears.  X  Information Gain (IG) . The IG feature selection method is based on a notion of the amount of information the pres-ence or absence of a particular term contains about the class of the document [39]. It is measured as follows:
From its definition, it is clear that the IG strategy is a su-pervised strategy. That is, it uses the document labels in its choice of features to retain. In contrast, our sampling-bas ed strategies, as well as the DF strategy, are unsupervised .
We investigate precision, recall, and the micro-and macro-averaged F1 measures aggregated over all classes. We are interested in comparing the performance of the classifier on the subsampled instance to the performance on the instance with the full feature set. Thus, we mainly report relative performances of the subsampled instances to the original in -stance. We first describe how the aggregation of the relative performances were done, taking the 20-Newsgroups dataset as an example.

We considered five test-train splits for the 20-Newsgroups dataset. For each split, i = 1 , . . . , 5, we obtained the op-timal (micro-averaged F1) performance MIF max ( i ) of the classifier on the full feature set by varying the regulariza-tion parameter  X  . This procedure essentially determined Figure 2: Performance of various values of  X  for the first split of the 20-Newsgroup dataset. The opti-mal Micro-averaged and Macro-averaged F1 values occur at  X  = 0 . 4 for this split. both the baseline performance MIF max ( i ) and the optimal value of the regularization parameter  X  max ( i ) for split i . Figure 2 plots the micro-and macro-averaged F1, average precision and average recall as a function of  X  for one of the splits and shows the choice of the optimal value  X  max for this split. Next, for each of the randomized sampling strategies; subspace, weighted, and uniform, and for each expected sample size r , we collected the aggregated perfor-mances over five different samples in MIF s ( i, r ), MIF and MIF u ( i, r ), respectively. For the deterministic feature selection strategies of IG and DF, the performance values MIF ig ( i, r ) and MIF df ( i, r ) were obtained using one run for each sample size, as the sample size r is the actual num-ber of features chosen. We then computed the relative per-formances for each feature selection strategy for this spli t; performance RelMIF( r ) curves for each strategy were then obtained by averaging over all the splits. We followed the same strategy of averaging the relative performance RelMIF for the TechTC family, using four test-train splits for each dataset. For ease of exposition, we also average the RelMIF curves across the 100 different datasets. For the Reuters dataset, we used only one test-train split (that of Lewis et al. [23]).
For the TechTC-100 family, Figures 3(a), 3(b) and 3(c) demonstrate the performances of the various feature selec-tion strategies. As mentioned earlier, we aggregated the RelMIF and the relative precision and recall values over all 100 of the datasets in the family. Figure 3(a) presents the RelMIF performance of all the feature selection strategies . All the selection strategies except document frequency (DF ) and uniform sampling (US) achieve 85% of the original (in-volving no sampling) micro-averaged F1 performance with only 500 out of the (roughly) 20K original features. In gen-eral, the subspace sampling (SS) and information gain (IG) strategies perform best, followed closely by weighted sam-pling (WS). For very aggressive feature selection (choosin g less than 0 . 1% of the original features), SS based on k = 10 singular vectors actually dominates both IG and the full-rank SS ( k = rank( A )). Figures 3(b) and 3(c) present the precision and recall ratios averaged over all 100 datasets. The precision plot closely follows the micro-averaged per-formance. The recall performance is very different, with DF scoring much higher than all the other strategies.
In order to understand better the behavior of SS, e.g., why it does relatively well at very aggressive levels of feature selection and why other methods perform similar to it at less aggressive levels, we considered its behavior in more detail. Figure 4(a) demonstrates the variability of the IG and SS strategies across the 100 datasets of the TechTC family. We set the expected sample size parameter r to 1000, and sorted the 100 datasets according to the RelMIF performance of the IG strategy. The two curves show the performance of the IG and the SS strategies according to this ordering. The performance of the SS strategy seems uncorrelated with the performance of the IG-based feature selection. Also, the relative performance of the IG strateg y varies from 0 . 6 to 1 . 1 whereas that of SS varies only from 0 . 8 to 1. The two horizontal lines represent the aggregated performances for the two methods over all the 100 datasets, and they correspond to the points plotted on Figure 3(a) at r = 1000. The aggregated performance of the SS method is marginally better than that of IG at this sample size. Since roughly half of the datasets have worse IG perfomance than the average, it follows from the figure that on roughly half of these 100 datasets, SS performs better than IG.
We also investigate the effect of the choice of the number of singular vectors k used by SS. Figure 4(b) plots the rel-ative micro-averaged F1 of SS for various values of k . At aggressive levels of feature selection, smaller values of k give a better performance whereas for higher number of features, choosing k to be equal to the rank of the training matrix A seems to be the optimal strategy. As expected, using all the singular vectors (i.e. k equals the rank of the training ma-trix) and using the top singular vectors that capture 90% of the Frobenius norm behave similarly.

The performance plots in Figure 3(a) show that both of the weight-based strategies WS and DF perform similarly to SS when k is chosen to be close to the rank of the matrix A . Insight into why this may be the case can be obtained by examining the distance between the probability distribu -tions as a function of k . Given two probability distributions tance between them is the Hellinger distance H (  X  p,  X  q ): Figure 4(c) plots the Hellinger distance between the proba-bility distribution of the WS (weighted sampling) strategy and the probability distribution of SS for various k , rang-ing from 1 to 100. In terms of Hellinger distance, WS is closer to SS for higher values of k . Thus, for less aggressive levels of feature selection, i.e., when the optimal strateg y is to choose k to be as close to the rank of A as possible, the weight-based selection methods (WS, which has a simi-lar flavor to DF) can serve as an efficient substitute for the SS strategy. This observation is in fact corroborated by the performance plots in Figure 4(c).
Figure 5(a) plots the relative micro-averaged F1 perfor-mances RelMIF against the expected number of features chosen by each selection strategy for the 20-Newsgroups dataset.

For the SS strategy, we employed either k = 1500 singular vectors, which captures 35% of the total Frobenius norm, or k = 100, which captures 15%. Both the SS (for k = 1500) and IG strategies achieve about 90% of the micro-averaged F1 performance of the full feature-set with roughly 5K of the total 60K features. However, the classification perfor-mance of this dataset seems to degrade rapidly at aggres-sive levels of sampling. The IG-based strategy dominates the performance, being particulary better than the others at the aggressive levels. We see the same effect of SS with k = 100 being better among all unsupervised methods for aggressive selection. For this dataset, though, the effect i s much less pronounced than it is for TechTC. In general, SS with k = 1500 strategy outperforms the other unsupervised feature selection strategies; however, it is only marginal ly better than the WS and DF methods. As expected, uni-formly random feature selection falls far behind rest.
The relative precision and recall plots (not presented) show that the precision does not increase after selecting ca. 3000 features, while the recall steadily increases with increase in the number of selected features. This is presum-ably because although the terms chosen at this point are discriminative amongst the classes, there are documents in the test set that do not contain these terms and thus affect the average recall performance.
Lastly, Figures 5(b) and 5(c) summarize the performance on the Reuters dataset. For SS strategy, we use either k = 1500, capturing 30% of the Frobenius norm, or k = 100 capturing only 12%. Under feature selection, the perfor-mance of this dataset actually improves marginally over the full set of features. Since the Reuters dataset has a wide skew in the sizes of different classes, we present the relativ e performance both for the micro-averaged and the macro-averaged F1 measures. As with 20-Newsgroups, the IG-based feature selection strategy performs marginally bett er than the others. In fact, for this dataset, the DF selection strategy also slightly outperforms the subspace-based met h-ods.
Several directions present themselves for future work. Fir st, we expect that our analysis is not tight in that we have per-mitted ourselves to sample enough such that  X  has an in-verse. One might expect that we only need to sample enough to  X  X apture X  the part of the space that is not  X  X ut off X  by the regularization parameter  X  , and a more refined analysis might demonstrate this. Second, we have based our analysis on recent work in the theory of algorithms for approximat-ing the  X  2 regression problem [11], but similar methods also apply to the  X  p regression problem, for all p  X  [1 ,  X  ) [8]. One might expect that by considering p = 1 we can apply our methods to the SVMs, which would be of interest due to the ubiquity of SVM-based classifiers in large-scale text analysis. For example, although we use matrix perturbation theory [32] to establish the robustness of certain subspace s to the feature selection process, if one can show that the rel -evant convex sets are similarly robust then our methodology should apply to SVM classification.
We would like to thank Christos Faloutsos for helpful dis-cussions during the initial stages of this project and Evgen iy Gabrilovich for pointing us to [9,16].
