 NLP researchers and practitioners spend a consid-erable amount of time comparing machine-learned models of text that differ in relatively uninteresting ways. For example, in categorizing texts, should the  X  X ag of words X  include bigrams, and is tf-idf weighting a good idea? In learning word embed-dings, distributional similarity approaches have been shown to perform competitively with neural network models when the hyperparameters (e.g., context window, subsampling rate, smoothing con-stant) are carefully tuned (Levy et al., 2015). These choices matter experimentally, often leading to big differences in performance, with little consistency across tasks and datasets in which combination of choices works best. Unfortunately, these differ-ences tell us little about language or the problems that machine learners are supposed to solve.
We propose that these decisions can be auto-mated in a similar way to hyperparameter selection (e.g., choosing the strength of a ridge or lasso regu-larizer). Given a particular text dataset and classi-fication task, we show a technique for optimizing over the space of representational choices, along with other  X  X uisances X  that interact with these de-cisions, like hyperparameter selection. For exam-ple, using higher-order n -grams means more fea-tures and a need for stronger regularization and more training iterations. Generally, these decisions about instance representation are made by humans, heuristically; our work seeks to automate them, not unlike Daelemans et al. (2003), who proposed to use genetic algorithms to optimize representational choices.

Our technique instantiates sequential model-based optimization (SMBO; Hutter et al., 2011). SMBO and other Bayesian optimization ap-proaches have been shown to work well for hyper-parameter tuning (Bergstra et al., 2011; Hoffman et al., 2011; Snoek et al., 2012). Though popular in computer vision (Bergstra et al., 2013), these techniques have received little attention in NLP.
We apply it to logistic regression on a range of topic and sentiment classification tasks. Consis-tently, our method finds representational choices that perform better than linear baselines previously reported in the literature, and that, in some cases, are competitive with more sophisticated non-linear models trained using neural networks. Let the training data consist of a collection of pairs d train =  X  X  d.i 1 ,d.o 1  X  ,...,  X  d.i n ,d.o n  X  X  , where each input d.i  X  I is a text document and each output d.o  X  O , the output space. The overall training goal is to maximize a performance func-tion f (e.g., classification accuracy, log-likelihood, F 1 score, etc.) of a machine-learned model, on a held-out dataset, d dev  X  ( I  X  O ) n 0 .

Classification proceeds in three steps: first, x : I  X  R N maps each input to a vector rep-resentation. Second, a predictive model (typi-cally, its parameters) is learned from the inputs (now transformed into vectors) and outputs: L : ( R N  X  O ) n  X  ( R N  X  O ) . Finally, the resulting classifier c : I  X  O is fixed as L ( d train )  X  x (i.e., the composition of the representation function with the learned mapping).

Here we consider linear classifiers of the form c ( d.i ) = arg max o  X  O w &gt; o x ( d.i ) , where the param-eters w o  X  R N , for each output o , are learned using logistic regression on the training data. We let w denote the concatenation of all w o . Hence the parameters can be understood as a function of the training data and the representation function x . The performance function f , in turn, is a func-tion of the held-out data d dev and x  X  X lso w and d train , through x . For simplicity, we will write  X  f ( x )  X  when the rest are clear from context.
Typically, x is fixed by the model designer, per-haps after some experimentation, and learning fo-cuses on selecting the parameters w . For logistic regression and many other linear models, this train-ing step reduces to convex optimization in N | O | dimensions X  X  solvable problem that is costly for large datasets and/or large output spaces. In seek-ing to maximize f with respect to x , we do not wish to carry out training any more times than necessary.
Choosing x can be understood as a problem of selecting hyperparameter values . We therefore turn to Bayesian optimization, a family of techniques that can be used to select hyperparameter values intelligently when solving for parameters ( w ) is costly. Our approach is based on sequential model-based optimization (SMBO; Hutter et al., 2011). It itera-tively chooses representation functions x . On each round, it makes this choice through a probabilistic model of f , then evaluates f  X  X e call this a  X  X rial. X  As in any iterative search algorithm, the goal is to balance exploration of options for x with exploita-tion of previously-explored options, so that a good choice is found in a small number of trials.
More concretely, in the t th trial, x t is selected using an acquisition function A and a  X  X urrogate X  probabilistic model p t . Second, f is evaluated given x t  X  X n expensive operation which involves training to learn parameters w and assessing per-formance on the held-out data. Third, the surrogate model is updated. See Algorithm 1; details on A and p t follow.
 Acquisition Function. A good acquisition func-tion returns high values for x when either the value f ( x ) is predicted to be high, or the uncertainty about f ( x )  X  X  value is high; balancing between these is the classic tradeoff between exploitation
Input: number of trials T , target function f p 1 = initial surrogate model for t = 1 to T do end for and exploration. We use a criterion called Expected pectation (under the current surrogate model p t ) that f ( x ) = y will exceed f ( x  X  ) = y  X  : A ( x ; p t ,y  X  ) = where x  X  is chosen depending on the surrogate model, discussed below. (For now, think of it as a strongly-performing  X  X enchmark X  discovered in earlier iterations.) Other options for the acquisition function include maximum probability of improve-ment (Jones, 2001), minimum conditional entropy (Villemonteix et al., 2009), Gaussian process up-per confidence bound (Srinivas et al., 2010), or a combination of them (Hoffman et al., 2011). Surrogate Model. As a surrogate model, we use a tree-structured Parzen estimator (TPE; Bergstra density estimation. We seek to estimate p t ( y | x ) where y = f ( x ) , the performance function that is expensive to compute exactly. The TPE approach seeks p t ( y | x )  X  p t ( y )  X  p t and p tions from previous trials that are less than and greater than y  X  , respectively. In TPE, y  X  is defined as some quantile of the observed y from previous trials; we use 15-quantiles.

As shown by Bergstra et al. (2011), the Ex-pected Improvement in TPE can be written as:  X  = p t ( y &lt; y  X  ) , fixed at 0 . 15 by definition of y  X  (above). Here, we prefer x with high probability under p  X  t ( x ) and low probability under p &lt; t ( x ) . To maximize this quantity, we draw many candidates according to p  X  t ( x ) and evaluate them according be given an explicit form. To compute p &lt; t ( x ) and p t ( x ) , we associate each hyperparameter with a node in the graphical model and multiply individ-ual probabilities at every node X  X ee Bergstra et al. (2011) for details. We fix L to logistic regression. We optimize text representation based on the types of n -grams used, the type of weighting scheme, and the removal of stopwords; we also optimize the regularizer and training convergence criterion, which interact with the representation. See Table 1 for a complete list.
Note that even with this limited number of options, the number of possible combinations is pensive. In all our experiments for all datasets, we limit ourselves to 30 trials per dataset. The only preprocessing we applied was downcasing.

We always use a development set to evaluate f ( x ) during learning and report the final result on an unseen test set. We summarize the hyperparam-eters selected by our method, and the accuracies achieved (on test data) in Table 5. We discuss com-parisons to baselines for each dataset in turn. For each of our datasets, we select supervised, non-ensemble classification methods from previous lit-erature as baselines. In each case, we emphasize comparisons with the best-published linear method (often an SVM with a linear kernel with represen-tation selected by experts) and the best-published method overall. In the following,  X  X VM X  always means  X  X inear SVM. X  All methods were trained and evaluated on the same training/testing splits as baselines; in cases where standard development sets were not available, we used a random 20% of the training data as a development set.
 Stanford sentiment treebank (Socher et al., 2013) X  X able 2. A sentence-level sentiment analysis dataset of rottentomatoes.com movie re-binary classification task where the goal is to pre-dict whether a review is positive or negative (no neutral). Our logistic regression model outperforms the baseline SVM reported by Socher et al. (2013), who used only unigrams but did not specify the weighting scheme for their SVM baseline. While our result is still below the state-of-the-art based on the the recursive neural tensor networks (Socher et al., 2013) and the paragraph vector (Le and Mikolov, 2014), we show that logistic regression is comparable with recursive and matrix-vector neu-ral networks (Socher et al., 2011; Socher et al., 2012).
 Amazon electronics (McAuley and Leskovec, 2013) X  X able 3. A binary sentiment analy-sis dataset of Amazon electronics product re-performing methods on this dataset are based on convolutional neural networks (Johnson and Zhang, best of these, outperforming all of the reported feed-forward neural networks and SVM variants Johnson and Zhang used as baselines. They varied the representations, and used log term frequency and normalization to unit vectors as the weighting scheme, after finding that this outperformed term frequency. Our method achieved the best perfor-mance with binary weighting, which they did not consider.
 IMDB movie reviews (Maas et al., 2011) X  Table 3. A binary sentiment analysis dataset of highly polar IMDB movie reviews: results parallel those for Amazon electronics; our method comes close to convolutional neural networks (Johnson and Zhang, 2015), which feed-forward neural networks, the restricted Boltzmann machine approach presented by Dahl et al. (2012), and compressive feature learning Congressional vote (Thomas et al., 2006) X  X able 4. A dataset of transcripts from the U.S. Congressional debates: to previous work (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010), we consider the task to predict the vote ( X  X ea X  or  X  X ay X ) for the speaker of each speech segment (speaker-based speech-segment classification). Our method outperforms the best results of Yessenalina et al. (2010), which use a multi-level structured model based on a latent-variable SVM. We show comparisons to two weaker baselines as well. 20 Newsgroups (Lang, 1995) all topics X  X able 6. 20 Newsgroups is a benchmark topic classifica-There are 20 topics in this dataset. Our method outperforms state-of-the-art methods including the distributed structured output model (Srikumar and baseline from Paskov et al. (2013) uses all 5-grams, heuristic normalization, and elastic net regulariza-tion; our method found that unigrams and bigrams, with binary weighting and ` 2 penalty, achieved far better results.
 rived three additional topic classification tasks from the 20N dataset. The first and second tasks are Wang and Manning (2012) report a bigram na  X   X ve Bayes model achieving 85.1% and 91.2% on these method achieves 86.3% and 92.1% using slightly different representations (see Table 5). The last task is to classify related science documents into four sci.med ; test size = 1 , 899 ). We were not able to find previous results that are comparable to ours on this task; we include our result (95.82%) to enable further comparisons in the future. Optimized representations. For each task, the chosen representation is different. Out of all possi-ble choices in our experiments (Table 1), each of them is used by at least one of the datsets (Table 5). For example, on the Congress vote dataset, we only need to use bigrams, whereas on the Amazon elec-tronics dataset we need to use { 1 , 2 , 3 } -grams. The binary weighting scheme works well for most of the datasets, except the sentence-level sentiment analysis task, where the tf-idf weighting scheme was selected. ` 2 regularization was best in all cases but one. We do not believe that an NLP expert would be likely to make these particular choices, except through the same kind of trial-and-error pro-cess our method automates efficiently.
 Number of trials. We ran 30 trials for each dataset in our experiments. Figure 1 shows each trial accuracy and the best accuracy on develop-ment data as we increase the number of trials for two datasets. We can see that 30 trials are gener-ally enough for the model to obtain good results, although the search space is large.
 Transfer learning and multitask setting. We treat each dataset independently and create a sep-arate model for each of them. It is also possible to learn from previous datasets (i.e., transfer learn-ing) or to learn from all datasets simultaneously (i.e., multitask learning) to improve performance. This has the potential to reduce the number of trials required even further. See Bardenet et al. (2013), Swersky et al. (2013), and Yogatama and Mann (2014) for more about how to perform Bayesian optimization in these settings.
 Beyond supervised learning. Our framework could also be extended to unsupervised and semi-supervised models. For example, in document clus-tering (e.g., k -means), we also need to construct representations for documents. Log-likelihood might serve as a performance function. A range of random initializations might be considered. Inves-tigation of this approach for nonconvex problems is an exciting area for future work. We used Bayesian optimization to optimize choices about text representations for various categoriza-tion problems. Our technique identifies settings for a standard linear model (logistic regression) that are competitive with far more sophisticated meth-ods on topic classification and sentiment analysis. Acknowledgments
