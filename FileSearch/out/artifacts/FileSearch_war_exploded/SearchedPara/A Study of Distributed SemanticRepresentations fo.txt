 problem [1{3] where learning algorithms such as K-nearest neighbor and support vector machines for ranking are applied to learn a rating model for a given essay prompt with a set of training essays rated by human assessors [4]. Currently, the AES systems have been widely used in large-scale English writing tests, e.g. Graduate Record Examination (GRE), to reduce the human e orts in the writing assessments.
 tures that are not linked to intuitive dimensions of semantics or writing quality, such as lexical complexity, grammar errors, syntactic complexity, organization and development, coherence, etc. However, these shallow text features are not able to represent the semantic content of essays, resulting in limited robustness and e ectiveness [5].
 sentations of documents are successfully applied to compute the syntactic and semantic similarity in quite a few natural language processing (NLP) tasks. For example, Tomas et al. propose a method based on representations of words and sentences that achieves promising results for movie rating prediction on a crawl of IMDB [8]. Richard et al. propose a method based on the continuous represen-tations of sentences that obtains good performance on the Stanford background dataset [9]. There are also e orts in developing methods for extracting the se-mantic representations of documents [9{11]. For instance, a simple approach is to use a weighted average of all word vectors in the document [12]. A more sophisticated approach is to learn continuous distributed vector representations for pieces of texts [13]. For the task AES, there has been little success of the application of the semantic features as far as we are aware of.
 ous novel features in indicating the writing quality of essays. The new features are derived based on di erent approaches to generating distributed representa-tions of words, paragraphs, and documents, including latent Dirichlet allocation (LDA) [17], Word2Vec [18], and PV-DBOW [13]. Experimental results on the publicly available dataset ASAP indicate that the new features based on the semantic similarity features and distributed semantic representations of essays achieve higher agreement with human raters than the use of only the common text features. In our evaluation, the use of the new features can achieve up to 12.33% improvement in Kappa, and 18.61% improvement in nRMSE against the baseline. AES methods [1, 7, 14, 15], which are listed in Table 1. The detailed description of the features is given below. acters. These can be indicators for the degree of complexity a writer can master since the unusual words tend to be longer. The number of unique words appeared in an essay, normalized by the essay length in words. words. The variety of the length of sentences potentially re ects the complexity of syntactics. words and the number of characters in an essay. Essays are usually written under a time limit, so the essay length can be a useful predictor of the productivity of the writer. The fourth root of essay length in words is proved to be highly correlated with the essay score [15]. number of sentence in an essay. The maximum number of clauses of a sentence in an essay. The mean length of sentences that contain at least one clause. tence, normalized by sentences in words. The average height of the parser tree of each sentence in an essay. The average of the sum of the depth of all nodes in a parser tree of each sentence in an essay. The more complicated the sentences are, the higher complexity the parser trees exhibit. It is therefore necessary to utilize the sentence structure to indicate the essay quality. indicators of bad essays, which are detected by the spelling check API provided by LanguageTool 1 . can be measured by the mean tf/TF of word bigrams and trigrams [16] ( tf is the frequency of bigram/trigram in a single essay and TF is the frequency of bigram/trigram in the whole essay collection). We assume a bigram or trigram with high tf/TF as a grammar error because high tf/TF means that this kind of bigram or trigram is not commonly used in the whole essay collection but appears in the speci c essay. Part-of-Speech tagging of each word is done by the Stanford Parser 2 . the element is the term frequency multiplied by inverse document frequency (tf-idf) of each word. It is calculated as the weighted mean of cosine similarities and the weight is set as the corresponding essay score. mantic Analysis. The calculation of mean cosine similarity of semantic vectors is the same with word vector similarity.
 section include most of the common text features used in recent studies on AES, which lead to state-of-the-art results [1, 4, 7, 16, 20]. Therefore, the AES system trained by those common text features is used as the baseline in this paper. 3.1 introduces the methods used for learning the semantic representations of essays, from which the semantic features are generated, as in Section 3.2. 3.1 Methods for Vector Representations erate semantic features based on the following recent methods for the vector representations. A brief introduction of how to obtain semantic embeddings of essays through these learning algorithms is given below.
 corpus [17]. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. The probabilistic distribution on all topics of a document is considered as a kind of semantic representation of a document. Using LDA, the i th dimension of the semantic representation of the essay is given by the probability that the essay belongs to topic i . To analyze the e ectiveness of the di erent number of topics, the number of topics is set as 5, 6, 7, 8, 9, 10, respectively. Those settings are found to be the most e ective in the preliminary experiments. Results obtained using the di erent settings above are also presented in Section 5.2. representations that capture syntactic and semantic relationships between words [18]. In continuous skip-gram model, every word is mapped to a unique vector, and all the word vectors are stacked in a word embedding matrix W generated by the model. The weighted mean of word embeddings of words appeared in an essay is used as a semantic representation of the essay, which the weight is set as the tf/TF . The i th dimension of the semantic representation of the essay is given by: where n is the number of unique words in an essay, weight j is the tf/TF of the j th word in the essay, and W ji is the i th dimension of the word vector of the j th word in the essay.
 one is the publicly available ASAP dataset (see Section 4.1 for details), and the other is the GoogleNews dataset 3 . The word embeddings obtained on ASAP is trained by Word2Vec, and the dimension of word embedding is set as 50, 100, 200 and 300, respectively. Those settings are found to be the most e ective in the preliminary experiments. Results obtained using the di erent settings above are also presented in Section 5.2. Using GoogleNews, the word vectors are pre-trained on 100 billion words of Google news dataset and are of length 300. al. propose the distributed bag of words version of paragraph vector model (PV-DBOW) [13]. The PV-DBOW model learns the paragraph vector based on the continuous skip-gram model. A notable di erence between the outcome of the PV-DBOW model and the continuous skip-gram model is that the PV-DBOW model generates the vector representations of paragraphs, in addition to the word vectors. The paragraph vector representations are obtained by the PV-DBOW model trained on the ASAP dataset. GoogleNews is not used as it only comes with word embeddings. The same as the word embeddings, the dimension of paragraph embedding is set to 50, 100, 200, and 300, respectively, for e ectiveness reason.
 3.2 Semantic Features essays for generating the semantic features for AES, namely Vector Similarity and Dimension Extension.
 ities between the given essay and the other essays for a given prompt. Assuming w ; w 2 ; :::; w m are the semantic representations of essays in the speci c essay set, Sim i is the Vector Similarity of the ith essay: where m is the number of essays associated to the given prompt, and r j is the actual rating of the j th essay. Using Vector Similarity, only a single semantic feature is generated from the essay embeddings.
 the entire semantic representations of the essay. Each dimension of the essay embedding is regarded as a semantic feature of the essay. In other words, the size of the feature vector of the given essay is extended by the number of dimensions of the entire semantic representations of the essay.
 the basis of semantic representations of essays introduced in Section 3.1. 2. For example, when the para vec 100 feature is used, the essay feature vector is extended by the semantic paragraph vector with 100 dimensions. Instead, if para sim 100 is used, the learned paragraph embeddings have 100 dimensions, and the essay feature vector is extended by a single dimension, which is the similarity between the semantic paragraph vector of essay and other essays in the same essay set. the evaluation metrics of the AES system, and the learning algorithms. 4.1 Dataset Assessment Prize (ASAP) 4 . Dataset in this competition consists of eight essay sets. Each essay set was generated from a speci c prompt. All essays received a resolved score, namely the actual rating, from professional human raters. As the ocial test data is no longer available, the evaluation is done by 10-fold cross-validation on the training data, split by random partitioning. 4.2 Evaluation Metrics and Learning Algorithms lation coecient and normalized root-mean-squared error to evaluate the agree-ment between the ratings given by the AES system and the actual ratings. They are widely accepted as reasonable evaluation measures for the AES systems [14, 19, 20].
 inter-rater agreement. Quadratic weighted Kappa takes the degree of disagree-ment between raters into account. The kappa metric is computed by the mean of the kappa values across all essay sets after applying the Fisher Transformation 5 , instead of the average of the raw kappa values over all essay sets. linear association between two variables. between two variables can be described using a monotonic function.
 malized root-mean-squared error (nRMSE) [23] measures the prediction error of the essay ratings. The ratings of a given essay topic are normalized to be within [0, 1] such that the errors among di erent prompts are comparable. Di erent from the other three metrics, a lower nRMSE value indicates better e ectiveness. The nRMSE reported in the results is averaged over all test es-says in the whole dataset. All statistical tests are based on Analysis of Variance (ANOVA).
 chines for ranking (SVM-rank) to predict ratings of essays. These two classical algorithms are widely used in recent studies on the AES systems [14, 15]. commonly used in automated essay scoring. Using KNN, we select the K essays in the training collection that are most similar to the test essay. Then the pre-dicted score of the test essay is the average of the scores of the K essays. The parameter K is set by grid search on the ASAP validation set.
 parameter C , which controls the trade-o between empirical loss and regularizer, is set by grid search on the ASAP validation set. To determine the nal rating of a given essay, we take the average rating of k essays whose scores are closest to the given essay. The parameter k is also set by grid search on the ASAP validation set. We use the implementation of SVM-rank in SVMrank package 6 . 5.1 Evaluation Design to AES, the experiments conducted in this paper are organized as follows. dimensions : To investigate the e ectiveness of the semantic features with dif-ferent numbers of dimensions, six sets of experiments are conducted. Each set of experiments corresponds to one speci c semantic feature, in addition to the baseline that uses the common text features in Table 2, as presented in Tables 3-5, respectively.
 mantic features based on Vector Similarity and Dimension Extension : In these experiments, we compare the semantic features to the baseline that uses the common text features. The semantic features are word sim , word vec , lda sim , lda vec , para sim , para vec and sim best+vec best . Out of these semantic fea-tures with di erent embedding dimensions presented in Tables 3-5, we choose the best individual semantic feature in each table to be evaluated against the baseline. sim best+vec best denotes the concatenation of the best Vector Simi-larity feature and the best Dimension Extension feature out of Tables 3-5, which correspond to Word2Vec, PV-DBOW, and LDA, respectively. Baseline uses all the common text features in Table 1 to learn a rating model for AES. The re-sults are listed in Table 6. The last column in Table 6 presents the results of sim best+vec best .
 para sim 100 , para vec 100 are compared with the baseline as they are the best out of the di erent settings of parameter k . sim best+vec best means that the feature set used is the concatenation of Baseline , para vec 100 and google sim 300 . 5.2 Evaluation Results bles 3-5 present the evaluation results brought by the use of individual semantic features in addition to the common text features, with respect to di erent num-bers of dimensions. Each of the tables corresponds to the results of the semantic features generated by a single learning method, i.e. Word2Vec, PV-DBOW, or LDA. In Tables 3-5, the best result of each semantic feature is in bold . al stable with di erent numbers of embedding dimensions in di erent evaluation metrics. Therefore, changing this parameter setting does not have a signi cant impact on the performance of the individual features. Moreover, according to Table 3, the word embeddings learned from ASAP appears to have slight bet-ter performance than those learned from GoogleNews when SVM-rank is used, and the other way around when KNN is used. In addition, comparing the eval-uation results of using Vector Similarity and Dimension Extension of the same embeddings, we nd no conclusive results. When SVM-rank is used, the Vector Similarity features have overall slightly better performance than the Dimension Extension features. However, when KNN is used, the Vector Similarity features have better performance when generated by Word2Vec (Table 3) and LDA (Ta-ble 5), while the Dimension Extension features ave better performance when generated by PV-DBOW. Such diverse results suggest the potential usefulness to combine the best individual semantic features based on Vector Similarity and Dimension Extension, respectively.
 bined in order to make the best use of those semantic features. Table 6 compares the use of the combination of the best semantic features against the baseline. The last column in Table 6 presents the results of sim best+vec best , the concatena-tion of the baseline features, and the best features generated by Vector Similarity and Dimension Extension, respectively. A * indicates a statistically signi can-t improvement over the baseline according to the ANOVA test. According to Table 6, all semantic features we present in this study have improvements over the baseline, and sim best+vec best has the best performance in all cases. This shows that it is bene cial to combine the semantic features generated by both methods. When using SVM-rank, the features generated by Dimension Exten-sion have overall better performance than those generated by Vector Similarity and the e ectiveness of features generated by word embeddings outperform the features generated by PV-DBOW and LDA.
 ated by Vector Similarity and Dimension Extension are statistically signi cant when the e ectiveness is measured by all four evaluation metrics. Using KNN, google sim 300 outperforms para sim 100 and lda sim 6 , and para vec 100 has better performance than google vec 300 and lda vec 6 . According to the ANOVA signi cance test, the improvements brought by google sim 300 , google vec 300 , para sim 100 , para vec 100 , lda sim 6 and sim best+vec best are statistically signi cant when the e ectiveness is measured by Kappa, Pearson and Spearman. All improvements are statistically signi cant when the e ectiveness is measured by nRMSE.
 improve the e ectiveness of AES on top of the common text features. As shown in Table 6, it is particularly encouraging that a combination of the best features can achieve up to 12.33% improvement in Kappa, and 18.61% improvement in nRMSE. Therefore, it is also recommended to combine the best features gen-erated by Vector Similarity and Dimension Extension, in order to achieve the maximized performance of AES. It is widely accepted that the agreement be-tween professional human raters ranges from 0.70 to 0.80, measured by quadratic weighted Kappa or Pearson's correlation [3]. In Table 6, the semantic features achieve a Kappa of 0.8016 and a Pearson's correlation of 0.8374, suggesting their potential usefulness in automated essay scoring. semantic vector representations for the task of automated essay scoring (AES). According to the evaluation results on the standard ASAP English dataset, the e ectiveness brought by our proposed semantic representations of essays de-pends on the learning algorithms and the evaluation metrics used. On the other hand, the e ectiveness of individual semantic features is stable with respect to di erent numbers of dimensions. Results show that statistically signi cant im-provement over the baseline can be achieved by applying our proposed semantic features listed in Table 2. Results also show that the concatenation of the best features generated by Vector Similarity and Dimension Extension, namely fea-ture sim best+vec best has the best e ectiveness among all features involved in this investigation. Moreover, the semantic features based on word embeddings lead to better e ectiveness than those based on LDA embeddings and paragraph embeddings.
 based on di erent sources of information, e.g. the structure of a given essay. We also plan to further improve this work by using the embeddings as input to a deep neural network, in order to learn an AES model.

