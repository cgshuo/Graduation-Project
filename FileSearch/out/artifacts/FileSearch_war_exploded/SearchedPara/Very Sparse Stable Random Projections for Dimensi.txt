 The method of stable random projections is a useful tool for ef-ficiently computing the l  X  ( 0 &lt; X   X  2 ) norms and distances in massive data in one pass. Consider a data matrix A  X  R n  X  D we multiply A with a projection matrix R  X  R D  X  k ( k D ), whose entries are i.i.d. samples of an  X  -stable distribution, then the projected matrix B = A  X  R  X  R n  X  k contains enough informa-tion to approximately recover the l  X  properties in A .
We propose very sparse stable random projections , by replacing the  X  -stable distribution with a (much simpler) mixture of a sym-metric  X  -Pareto distribution (with probability  X  , 0 &lt; X  point mass at the origin (with probability 1  X   X  ). This leads to a sig-nificant 1  X  -fold speedup for small  X  when computing B = and a 1  X  -fold cost reduction in storing R . By analyzing the conver-gence, we show that in  X  X easonable X  datasets  X  often can be very small (e.g., D  X  1 / 2 ) without hurting the estimation accuracy. Some numerical evaluations are conducted, on synthetic data, Web crawl data, and gene expression microarray data.
 H.2.8 [ Database Applications ]: Data Mining Algorithms, Performance, Theory Dimension Reductions, Random Projections, Stable Distributions, Sparse Projections, Asymptotic Analysis, Convergence
The method of stable random projections [48, 43, 45, 9, 77, 56] is a useful tool in data mining and machine learning, for efficiently computing the l  X  ( 0 &lt; X   X  2 ) distances in massive data (e.g., the Web or massive data streams) using a small space in one pass.
There are practical reasons to consider the l  X  norm other than l because real-world large-scale datasets (especially Internet data) Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. are ubiquitously  X  X eavy-tailed X  and follow the  X  X ower-law X  [52, 22, 29, 69]. It is very often the case that one can not use the l directly without carefully term-weighting the original data (e.g., heuristically taking logarithm or square root, or tf-idf)[67, 78, 73, 26, 41, 65]. The term-weighting procedure is often far more impor-tant than fine-tuning the machine learning parameters[53, 72, 50]. Instead of weighting the data, an alternative scheme is to choose an appropriate norm. While the l 1 norm becomes popular, other l norms are also possible as we can basically treat  X  as a tuning parameter. We will comment more on this issue in Section 2.5.
Consider a data matrix A  X  R n  X  D .Both n and D can be very large. For example, n can be the total number of words (or contigu-ous words, shingles [15]) in the dictionary and D can be the total number of Web pages collected by commercial search engines. The idea of stable random projections is to multiply A with a random projection matrix R  X  R D  X  k ( k D ). The resultant (projected) data matrix B = A  X  R  X  R n  X  k will be much smaller than (e.g., B may be small enough to reside in the physical memory).
The projection matrix R typically consists of i.i.d. samples from a symmetric  X  -stable distribution[80], denoted by S (  X , 1) with the index parameter  X  and the scale parameter 1. After the projec-tions (matrix multiplications), the resultant data in B also follow  X  -stable distributions with the scale parameters being the l erties (norms, distances) of the original data in A . Therefore, if all we care about are the l  X  properties, we can  X  X hrow away X  the original data, which are often too massive to handle efficiently.
A random variable z is symmetric  X  -stable if its characteristic function (Fourier transform of the density function) is where d&gt; 0 is the scale parameter. We write z  X  S (  X , d ) ,which in general does not have a closed-form density function (inverse Fourier transform) except for  X  =2 (normal) or  X  =1 (Cauchy).
Given two vectors u 1 and u 2 in D dimensions (e.g., the leading two rows in A ), the individual l  X  norms and the l  X  distance are
X
For many applications in data mining and machine learning, we really only need the l  X  properties (especially the pair-wise dis-tances) instead of the original data. We will comment more on the applications of stable random projections in Section 2.
Applying stable random projections on u 1  X  R D , u 2  X  R D yields, respectively, v 1 = R T u 1  X  R k and v 2 = R T u (We can view v 1 and v 2 as the first two rows in the matrix By the properties of Fourier transforms, the projected differences, v 1 ,j  X  v 2 ,j , j =1 , 2 , ..., k , are i.i.d. samples of S (  X , d Therefore, the task boils down to estimating the scale parameter from k i.i.d. samples x j  X  S (  X , d (  X  ) ) . Because no closed-form density function is available except for  X  =1 , 2 , the estimation task is an interesting research topic. In Section 3, we will review several estimators recently proposed in [56]. First, the processing time O ( nDk ) for the matrix multiplication AR is expensive in modern massive data streams, one of the open questions raised in a recent workshop on data streams. See which cited an arXiv preprint[55] as a plausible solution. We should mention that [55] included most of the results in this paper. Another problem is that we have to store the projection matrix R at the memory cost O ( Dk ) , which is also expensive for large D . We will come back for this issue again in Section 2.

Finally, sampling from the stable distribution is in general ex-pensive. As described in [74, Pr oposition 1. 71.1], we should first sample W 1 uniform on (  X   X  2 ,  X  2 ) and E 1 from an exponential dis-tribution with mean 1. If W 1 and E 1 are independent, then is distributed as S (  X , 1) . Clearly, this procedure is not inexpensive.
In this study, we propose very sparse stable random projections by replacing the  X  -stable random variable S (  X , 1) in the projec-tion matrix R with a mixture of a symmetric  X  -Pareto distribution (with probability 0 &lt; X   X  1 ) and a point mass at the origin (with probability 1  X   X  ), i.e., R consists of i.i.d. entries where P  X  denotes an  X  -Pareto variable, Pr ( P  X  &gt;t )= 1 ; and 0 otherwise. This procedure is beneficial because
With this scheme, the projected data, under reasonable regularity conditions, are asymptotically (instead of exactly)  X  -stable. When the convergence is  X  X ast enough, X  we might treat the projected data as if they were exactly  X  -stable so that we could use the theoretical estimators developed for stable random projections.

We will show, theoretically and empirically, that the sparsity fac-tor  X  can be small (e.g., 0.01, or even D  X  1 / 2 ). In engineering prac-tice, even a 10-fold speedup may be often considered significant.
Fundamentally, very sparse stable random projections can work well because
Random coordinate sampling , as an alternative scheme for di-mension reductions, randomly selects k coordinates from vectors in D dimensions. In other words, k columns are randomly picked in the data matrix A  X  R n  X  D . With this scheme, the processing (sampling) procedure is very efficient. The drawback, of course, is that the accuracy is often not satisfactory in heavy-tailed data (be-cause we often could not afford very large k ). In addition, when the data are highly sparse, random coordinate sampling will likely miss most of the informative (non-zero) components.

With stable random projections , the accuracy is guaranteed re-gardless of the data sparsity or data heavy-tailedness, at the cost of the expensive processing time (for the matrix multiplication).
Our proposed scheme, very sparse stable random projections , can be viewed as a combination of random coordinate sampling and stable random projections in that every time we randomly se-lect only  X D coordinates for applying the stable projections. Un-like the traditional random coordinate sampling which can only af-ford sampling a small number of components, this new scheme picks  X D coordinates for each projection. While  X  can be small (e.g., D  X  1 / 2 ),  X D will still be large (e.g., D 1 / 2 ery projection, a different (overlapping) subset of  X D coordinates will be chosen. Therefore, very sparse stable random projections should be much more robust than random coordinate sampling.
Given this insight, one might worry that, in the extreme scenario when there are only a few non-zero entries in a vector of D dimen-sions, our proposed scheme would not work well unless  X  =1 . This, of course, is not a practical scenario. But even if this sit-uation does occur, we could afford to store all the non-zeros (e.g., using inverted index ) and hence there is no need to use random pro-jections. When the data are just  X  X easonably sparse, X  our scheme is still applicable though we may want to consider  X  less aggressively.
Here we would like to introduce a variant of random coordinate sampling called conditional random sampling (CRS) [57, 58, 59], which works particularly well on massive sparse data. Also CRS has the advantage that the same set of  X  X amples X  can be used to approximate the l  X  distances for any  X  (not even limited to  X 
The method of very sparse stable random projections is inspired by [61], which suggested using a very sparse projection matrix for dimension reduction in the l 2 norm. In this study, we consider the l norm ( 0 &lt; X   X  2 ), a much more general scenario.

Unlike in the l 2 norm case, the analysis of very sparse stable random projections is more involving because now we are dealing with the heavy-tailed case for which the theoretical tools are much more limited. In [61], the conditions for convergence and the rates of convergence were derived from existing statistical theorems.
Like [61], our approach relies on the assumptions on data reg-ularity conditions and hence does not have the worst-case guar-antees. We should mention that our scheme is different from [5], which, for dimension reduction in l 2 , achieves sparsity by more preprocessing without losing the worst-case guarantees.
There have been a lot of applications of stable random projec-tions , in data streams, data mining, and machine learning.
Many applications, including clustering, classifications and ker-nels (e.g., for SVM), require computing all pair-wise distances of A  X  R n  X  D . The exact computations cost O ( n 2 D ) , which can be infeasible for large n and D (e.g., at the Web scale), especially when A is too large for the physical memory.
 With stable random projections, we first obtain B = AR , R  X  R D  X  k , and then estimate all pair-wise l  X  distances from tal cost is reduced to O ( n 2 k + nDk ) , which can be further reduced to O ( n 2 k +  X nDk ) by very sparse stable random projections .
While the original data matrix A  X  R n  X  D may be too large for the physical memory, materializing all pair-wise l  X  distances in A takes space O ( n 2 ) , which can be also too large for the mem-ory. In many applications such as online learning, search engines, databases, online recommendation systems, online market-basket analysis, it may be more efficient if we store B  X  R n  X  k memory and estimate any distances on the fly only when necessary.
Massive data streams are fundamental in many modern data pro-cessing applications. Data streams come from Internet routers, phone switches, atmospheric observations, sensor networks, high-way traffic conditions, finance data, and more[42, 32, 43, 9, 19, 4]. Unlike in the traditional databases, it is not common to store massive data streams; and hence the processing is often done  X  X n the fly. X  For example, in some situations, we only need to  X  X isually monitor X  the data by observing the time history of certain summary statistics, e.g., sum, number of distinct items, or any l
In data stream computations, stable random projections can be used at least for (A): approximating the l  X  frequency moments for individual streams; (B): approximating the l  X  differences between a pair of streams; (C): approximating the number of non-zero items (the Hamming norm) in a stream using very small  X  [19, 20]. We will comment more on (C) in the next subsection. [43, 45] described the procedure of using Cauchy random pro-jections to approximate the l 1 norms (or l 1 differences) in data streams. For a stream, u 1 , which contains pairs ( i , u { 1 , 2 , ..., D } , [43, 45] suggested the following steps:
Obviously, this procedure can be extended to general 0 &lt; X  [19, 20] proposed approximating the Hamming norms of data streams using stable random projections with very small  X  ,be-cause the l  X  norm approximates the Hamming norm well if  X  0+ . The Hamming norm gives the number of non-zero items (dis-tinct terms) present in a single stream; and it is also an important measure of (dis)similarity when applied to a pair of streams[19, 20]. Note that for static data, one could approximate the Hamming norms directly by applying 2-stable (i.e., normal) random projec-tions on the binary-quantized (0/1) data. [19, 20] considered the dynamic setting in that the data may be subject to frequent addi-tions/subtractions. [19, 20] still used the median estimator. For  X  =0+ ,however, the estimator based on the harmonic mean is much more accurate. The choice of  X  depends on the applications and datasets at hand.
The case  X  =2 (i.e., normal random projections ) has been very thoroughly studied[77]. When  X  =2 , we could directly esti-mate the original l 2 distances from the projected l 2 distances (i.e., the arithmetic mean estimator). The Johnson-Lindenstrauss (JL) Lemma says we only need k = between any pair of data points can be estimated within a 1 factor. Many versions of the JL Lemma have been proved for the l case[47, 37, 46, 7, 23, 43, 44, 2, 8, 5]. There are also a large number of publications on applications of normal random projections , e.g., in Machine Learning [7, 12, 3, 36, 34, 10], VLSI layout [76], La-tent Semantic Indexing (LSI) [70, 64], set intersections [6, 18, 71], finding motifs in bio-sequences [16, 54], face recognition [39], pri-vacy preserving distributed data mining [66]. Because large-scale real-world datasets are ubiquitously heavy-tailed (e.g., the l of the original data may not be meaningful), data pre-processing ( term-weighting ) is often a critical step in order to use the l
The case  X  =1 (i.e., Cauchy random projections )isalsocom-mon in practice, as it is well-known that the l 1 norm is far more robust than l 2 against  X  X utliers. X  Success stories include LASSO [75], LARS [28], 1-norm SVM [79], and Laplacian radial basis kernel [17, 33]. However, it has been proved [13, 51, 14] that one can not hope to develop an estimator that is a metric for dimension reduction in l 1 without incurring large errors, i.e., the arithmetic mean estimator does not work for  X  =1 .

Other norms are also possible. In data streams, as  X  increases, the l  X  distance attributes more significance to a large individual component; and therefore varying  X  provides a tunable mecha-nism[35]. This argument applies directly also in the machine learn-ing content. As a concrete example, [17] proposed a family of non-Gaussian radial basis kernels for SVM in the form of K ( x, y )= exp sions. [17] showed that  X  =0 . 5 in some cases gave better results in histogram-based image classifications.

The l  X  norm with  X &lt; 1 is now well-understood to be a nat-ural measure of sparsity[24, 25]. Of course, this is why [19, 20] approximate the Hamming norm with the l  X  norm using small  X  . [21] adopted the similar idea to approximate the max-dominance norm in data streams using very small  X  .
We often need to store the random projection matrix R for im-portant reasons. Firstly, new data points (not in the original data matrix A ) may arrive. Secondly, in data streams, the entries may not arrive in orders. Thirdly, the data may be subject to frequent update (additions/subtractions).

If we sample R  X  R D  X  k from i.i.d. S (  X , 1) , the storage cost would be O ( Dk ) , which can be prohibitive. In fact, this is one of the concerns in [43]. Sampling R from the very sparse projection distribution defined in (5) can reduce the storage cost to O (  X Dk ) .
In this section, we assume k i.i.d. samples x j  X  S (  X , d j =1 , 2 , ..., k . A widely used estimator is based on the sample quantiles [30, 31, 68], which can be simplified to be the sample median estimator by the symmetry of S (  X , d (  X  ) ) [43, 45, 62, 63]
The sample median estimator is not accurate (especially at small k ) and is difficult for theoretical analysis (e.g., tail bounds). Re-cently, considerable progress has been achieved in [56] for new es-timators and theoretical analysis. In particular, [56] recommended various estimators according to  X  .
We suggest a procedure to simplify stable random projections and significantly reduce the processing and storage cost.
Recall the basic fact about stable distributions: If z 1 , z i.i.d. are S (  X , 1) , then for any constants (i.e., the original data) g g , ..., g D ,wehave
As mentioned in Section 1, we suggest replacing z i  X  S (  X , 1) with the following (much simpler) very sparse distribution where P  X  denotes an  X  -Pareto distribution. That is, Pr ( P  X  if t  X  1 ; and 0 otherwise. Sampling P  X  is straightforward. If we sample U uniformly in (0,1), then 1 /U 1 / X  is P  X  .
We call this approach very sparse stable random projections be-cause on average only a  X  -fraction of entries are non-zero, i.e., a  X  -fold speedup in computing O (  X nDk ) . The storage cost is reduced from O ( Dk ) to O (  X Dk ) .
We reiterate the two fundamental reasons why this scheme should work (and work well).
We are inspired by the recent work on very sparse random pro-jections for dimension reduction in l 2 [61], which showed the reg-ularity condition for convergence and rate of convergence from known statistical theorems: the Lindeberg Central Limit Theorem and the Berry-Esseen Theorem.

For  X &lt; 2 , we will also analyze under what condition very sparse stable random projections will converge, as well as the con-vergence rate[38]. By assuming all g i  X  X  were identical and impos-ing more conditions on the density of z i , a fairly recent paper [49] derived the precise rate of convergence. Since we are dealing with the non-i.i.d. case (i.e., g i  X  X  are different), we resort to the first principle by studying the characteristic function of Lemma 1 (proved in Appendix A).

L EMMA 1. Suppose z i , i =1 , 2 , ..., D , are i.i.d. random vari-ables defined in (12). Then, as D  X  X  X  , provided The notation  X  D =  X   X  stands for  X  X onvergence in distribution. X 
The convergence rate factor 1 is
We use the term  X  X onvergence rate factor X  instead of the  X  X ate of convergence X  because strictly speaking we have to conduct the in-verse Fourier transform for the precise  X  X ate of convergence. X  Since the analysis is involving (even for the i.i.d. case, [49] relied on some additional assumptions), we take the convergence rate factor directly from the higher order terms in the Fourier transform.
Note that we do not have to consider  X  =2 in Lemma 1 because we can sample from the sparse distributions suggested in [1, 2, 61]. Also note that the convergence condition (14) is merely a conve-nient sufficient condition, which can be further relaxed at the cost of slower convergence rate.
 At the risk of not being rigorous, we sometimes write (13) in Lemma 1 conveniently as
X
When the convergence is fast, we might assume the projected data after very sparse stable random projections to be  X  X xactly sta-ble X  and we might still use the va rious estimators proposed in Sec-tion 3. Of course, we need to adjust the estimates for the additional constant factor  X (1  X   X  )cos
Lemma 1 is not very interpretable. For convenience, we will assume that the data g i  X  X  are i.i.d. samples of some distribution. Suppose the data have bounded second moments, i.e., can be treated as a constant if  X   X  2 (law of large numbers). We have the following corollary.

C OROLLARY 1. Suppose the data | g i |  X  X  are i.i.d. with bounded second moments, then the convergence condition (14) is satisfied. The convergence rate factor is
If we choose  X  = 1  X  D , then the convergence rate factor would be O
In other words, if the data have bounded second moments, we can achieve a significant still reasonably fast if  X  is not close to 2. On the other hand, when  X  is approaching 2, the convergence will be very slow (if converges at all) even if we let  X  =1 . Therefore, we do not recommend replacing the stable distribution with (12) when  X  is close to 2. We will provide an alternative sparse sampling scheme in Section 7.
Next, we will consider the case when the data do not have bounded second moments or even first moments. To simplify the arguments, we assume the data | g i |  X  X  are i.i.d. and follow an  X  -Pareto distribu-tion with  X &lt; 2 . Recall if a random variable x follows an  X  -Pareto distribution, then E ( x  X  ) &lt;  X  if  X &lt; X  and E ( x  X 
Heavy-tailed data are often routinely modeled by Pareto distri-butions. [69] measured the  X  values for many kinds of datasets. While it is quite often that 1 &lt; X &lt; 2 , it is not very common that  X &lt; 1 . For example,  X  =1 . 20 for the frequency of use of words,  X  =1 . 40 for the number of hits on Web sites,  X  =2 . 04 for the number of citations to papers[69].

C OROLLARY 2. Suppose | g i |  X  X  are i.i.d.  X  -Pareto with  X &lt; 2 , then the convergence condition (14) is satisfied if  X &gt; X  . Assuming  X &gt; X  , the convergence rate factor would be
If we choose  X  = D  X  would be
Proof: This can be shown by the fact that if x i is  X  -Pareto, i.i.d., then ple 2.7.4][61].

Therefore, in order for very sparse stable random projections to converge (for any 0 &lt; X   X  1 ), we have to make sure that the original data should at least have the bounded  X  th moment, a very natural requirement. When the data have bounded higher moments, we can obtain a faster convergence rate and afford a smaller  X  .
We can see that if D is larger enough (say 10 5 ), it is often easy to achieve (e.g.) 100-fold (  X  =0 . 01 ) speedup. A factor of 100 (or even 10) may be significant enough to make a theoretically appeal-ing algorithm become a practical one.
Other than l 2 ,the l 1 norm is probably the most widely used. We re-present the results for the l 1 case, for the convenience of those who are only interested in l 1 .
When  X  =1 , i.e., Cauchy random projections , the estimators are much simplified. Assume k i.i.d. samples x j  X  S (1 ,d j =1 , 2 , ..., k , the geometric mean estimator becomes[62, 63]
Because the Cauchy distribution has a closed-form density, [62, 63] also suggested two other estimators. The bias-corrected sam-ple median estimator has similar accuracy when the sample size k&gt; 20 .The bias-corrected maximum likelihood estimator is more accurate but slightly more complicated numerically.
When  X  =1 ,the very sparse distribution in (12) becomes where U 1 and U 2 are independent uniform random variables in (0 , 1) . Suppose z i , i =1 , 2 , ..., D , are i.i.d. random variables as defined above. Then, as D  X  X  X  , provided the data ( g i  X  X ) have at least bounded first moment. For convenience, we write
Therefore, after very sparse Cauchy random projections , we can (approximately) use any estimators for Cauchy random projections and divide the estimated results by a factor  X  2  X  .
In three subsections, we present some numerical results on the synthetic data, some Web crawl data, and the Harvard microarray data, respectively, only for the l 1 case (  X  =1 ). We will always use the geometric mean estimator (20) (i.e., assuming the projected data are exactly Cauchy). Figure 1: We simulate data from an  X  -Pareto distribution for  X  =1 . 1 (i.e., highly heavy-tailed data), and then apply very sparse stable random projections with  X  =0 . 05 (i.e., a 20-fold speedup) to estimate the l 1 norm of the data. The mean square errors (MSE), computed from 10 6 simulation for each k and D , are plotted against the sample size k for each D =100, 500, 1000, and 5000. We compute the empirical variances from 10 simulations for every k and D . The  X  X heoretic X  curve is the theoretical variance assuming th e data are exactly (instead of asymptotically) Cauchy (i.e., the variance in (21)). When D = 100 , the performance is poor, but as soon as D  X  500 , very sparse Cauchy random projections produce very similar results to regular Cauchy random projections.

We simulate data from an  X  -Pareto distribution, P  X  ,for  X  =1 . 1 ,  X  =1 . 5 and  X  =2 . 0 . We choose  X  =1 . 0 (i.e., the l 1 Corollary 2 recommends  X  = D  X  D which however, are still highly conservative. To make the results more interesting, we choose, more aggressively,  X  =0 . 05 ,  X  = D  X  0 . 6 ,and D  X  0 . 75 , respectively, otherwise all curves will simply overlap. And we will see that D does not have to be very large for the asymptotic theory to work well.

For  X  =1 . 1 , Figure 1 plots the empirical mean square errors (MSE) as a function of the sample size k ,for D = 100 , 500 , 1000 , and 5000 . It also plots the  X  X heoretical X  variance curve which is the variance of the exact Cauchy random projections, e.g., (21). Be-cause  X  =1 . 1 is only slightly larger than  X  =1 , one probably would expect that  X  =0 . 05 should not work well at least when D is not too large. Figure 1 presents, maybe surprisingly, that even when D = 500 only, very sparse stable random projections pro-duce very similar results as exact Cauchy random projections. Be-cause, as tabulated in [69], most natural datasets have moments higher than 1.1, we expect that it should really easy to achieve at least (e.g.) 10-fold speedup for almost any natural data. Figure 2: When  X  =1 . 5 and  X  = D  X  0 . 6 , after D  X  500 , the MSE curves produced by very sparse stable random projec-tions are almost indistinguishable from the theoretical variance curve generated by Cauchy random projections.
 For  X  =1 . 5 and  X  =2 . 0 , Figure 2 and Figure 3 plot the similar MSE curves, illustrating that we can choose  X  very aggressively (  X  = D  X  0 . 6 and  X  = D  X  0 . 75 ) and the projected data still con-verge to Cauchy fairly rapidly. For example, after D  X  500 ,the MSE curves produced by very sparse stable random projections are almost indistinguishable from the theoretical variance curve gener-ated by exact Cauchy random projections. Figure 3: When  X  =2 . 0 and  X  = D  X  0 . 75 , after D  X  500 , the MSE curves produced by very sparse stable random projec-tions are almost indistinguishable from the theoretical variance curve generated by Cauchy random projections.
We apply very sparse stable random projections on some Web crawl data. We pick two pairs of words, THIS-HAVE, and SCHOOL-PROGRAM. For each word (vector), the i th entry ( i =1 to D = 65536 ) is the number of occurrences of this word in the i th Web page. It is well-known that the word frequency data are highly heavy-tailed and highly sparse. Some summary statistics in Table 1 verify that the data are indeed highly heavy-tailed and sparse.
For each pair, we estimate the l 1 distance using very sparse sta-ble random projections with  X  =0 . 1 , 0.01, and 0 . 001 .Asshown in Figure 4, for the pair THIS-HAVE, even when  X  =0 . 001 ,there-sults are indistinguishable from what we would obtain by exact sta-ble random projections. For the pair SCHOOL-PROGRAM, when  X  =0 . 01 , the results are good. However, when  X  =0 . 001 ,we see larger errors. This is because the data are sparse (sparsity = Figure 4: The MSEs (normalized by the l 1 distances) using very sparse stable random projections for two pairs of words. The  X  X heoretic X  curves are the theoretical variances in (21) by as-suming that the projected data are exactly Cauchy. The empir-ical variances are computed from 10 6 simulations for every k and  X  . 0.1279, meaning that the  X  X ffective dimension X  should be much smaller than D = 65536 ) and the data are highly heavy-tailed.
Here is probably a good place to comment more on how data sparsity affects very sparse stable random projections . Since only the non-zero terms contribute to the computation of distances, we really should use the  X  X ffective data dimension X  D = D  X  spar-sity, in choosing  X  and analyzing the convergence. For example, for the pair SCHOOL-PROGRAM, since the sparsity is 0.1279, we may choose  X  = 1  X  D = 1  X  D  X  0 . 1279 =0 . 01 . Of course, in prac-tice, we will only know the  X  X arginal sparsity X  instead of the  X  X oint sparsity. X  However, the  X  X arginal sparsity X  can often be a conser-vative estimate of the  X  X oint sparsity. X  For example, the sparsity for SCHOOL is 0.0695 and the sparsity for PROGRAM is 0.0816, while the joint sparsity = 0.1279.

As the data become more and more sparse, very sparse stable random projections will eventually break (unless we let  X  =1 ). However, as we have previously commented, in truly extremely sparse datasets, we could either compute the distances exactly by (e.g.) the inverted index or using a variate of random coordinate sampling called Conditional Random Sampling (CRS) [57, 58, 59].
Usually the purpose of computing distances is for the subsequent tasks such as clustering, classification, information retrieval, etc. Table 1: For each pair of words ( u 1 v. s. u 2 ), we compute the dif-ference vector u = u 1  X  u 2 and ratios of the empirical moments for illustrating that the data are highly heavy-tailed. Note that Here we consider the task of classifying deceases in the Harvard microarray dataset[11]. The original dataset contains 176 samples (specimens) in 12600 gene dimensions, including 139 lung adeno-carcinomas, 17 normal samples, and 20 pulmonary carcinoids. For each specimen, we subtracted the median (across genes). However, we did not perform any normalization.

A simple nearest neighbor method can classify the samples nearly perfectly using the l 1 distance. When m =1 ,3,5,7,9,the m -nearest neighbor classifier mis-classifies 3, 2, 2, 2, 2, samples, respectively. For comparisons, using the ( l 2 ) correlation distance ( 1  X  correlation coefficient), when m =1 ,3,5,7,9,the m -nearest neighbor classifier mis-classifies 6, 4, 4, 4, 5, samples, respectively.
We conduct both Cauchy random projections and very sparse stable random projections (  X  =0 . 1 , 0.01, and 0.001) and classify the specimens using a 5-nearest neighbor classifier based on the es-timated l 1 distances (using the geometric mean estimator). Figure 5 indicates that (A): stable random projections can achieve similar classification accuracy using about 100 projections (as opposed to the original D = 12600 dimensions); (B): very sparse stable ran-dom projections work well when  X  =0 . 1 and 0 . 01 . Even with  X  =0 . 001 , the classification results are only slightly worse.
The main scheme for very sparse stable random projections us-ing (12) works well when  X   X  1 or  X  not too much larger than 1. This scheme may be improved by various alternatives.
Instead of using the sparse distribution in (12), we could, alter-natively, consider the following more general form: where P  X , X  denotes Pareto (  X ,  X  ) .Thatis, Pr ( P  X , X  &gt;t )= if t  X   X  ;and0otherwise.

Theoretically, (25) is appealing. When we choose a smaller (than 1)  X  , e.g.,  X  =1 /D  X  for some  X &gt; 0 , we can achieve faster con-vergence and a less restrictive condition for ensuring convergence.
However, as  X  decreases, the probability density function (PDF) of P  X , X  becomes more steep near  X  (as shown in Figure 6), i.e., harder to obtain random samples of good quality. Therefore, in or-der to use (25), we have to first analyze how pseudo-random num-ber generation mechanism will affect the sampling quality of (25). We have not conducted this type of theoretical analysis. Figure 5: We apply Cauchy random projections and very sparse stable random projections (  X  =0 . 1 , 0 . 01 , 0 . 001 )and classify the microarray specimens using a 5-nearest neighbor classifier based on the l 1 distances. The curves in Panel (a) show that Cauchy random projections can achieve almost the same misclassification errors with just 100 projections (as opposed to the original 12600 dimensions). Panel (a) also shows that very sparse stable random projections with  X  =0 . 1 and 0 . 01 perform almost indistinguishably from Cauchy random projec-tions. When  X  =0 . 001 , the results are only slightly worse. Each curve is averaged over 1000 runs. Panel (b) presents the standard errors over 1000 runs, indicating that the classifica-tion accuracy does not fluctuate much in different runs.
The following alternative scheme (II) is practical for general 0 &lt;  X   X  2 , although it is not as simple as (12):
The analysis is similar to Lemma 1 (in fact easier). This scheme will work well even when  X  approaches 2. The disadvantage is that we will have to sample from S (  X , 1) , which is quite expensive. Again, assume the data | g i |  X  X  are i.i.d.  X  -Pareto. If  X &gt; X  ,then and the convergence rate factor would be O we choose  X  = D max(1 / 2 , X / X  )  X  1 , then the convergence rate factor would be O Figure 6: The probability density functions of P  X , X  for  X  =1 and  X  =1 ,0.5,and0.1. As  X  decreases, the curve becomes more steep, i.e., harder to obtain good random samples.
In many data mining and machine learning applications, we only need the pair-wise distances (in some appropriate l  X  norm) of the data, instead of the original data. While the l 2 norm remains to be the most popular distance metric, other l  X  norms have also been widely used, especially the l 1 norm and l 0 norm. Basically we can treat  X  as an additional tuning parameter .

The method of stable random projections is a very useful tool for efficiently computing the l  X  ( 0 &lt; X   X  2 ) distances in mas-sive datasets (e.g., the Web or massive data streams), using a small (memory) space in one pass of the data. In this study, we propose very sparse stable random projections , to simplify the sampling procedure, to speedup the processing time (i.e., matrix multiplica-tion), and to reduce the storage cost. As shown both theoretically and empirically, it is evident that we can achieve a significant im-provement (e.g., 100-fold or even the accuracy for recovering the original l  X  distances.
Our results will be useful when applying stable random pro-jections in computing summary statistics in modern massive data streams, clustering massive high-dimensional data, information re-trieval, and speeding up machine learning tasks which require com-puting pair-wise distances.
 Ping Li is supported in part by NSF Grant DMS-0505676. Ping Li thanks Trevor Hastie for his support and many discussions. The author also thanks Kenneth Church, Graham Cormode, Piotr Indyk, Tze Leung Lai, Assaf Naor, Wing Wong, and Tong Zhang.

To simplify the notation, we let c i = g i damental probability theory says that, in order to show the con-vergence in distribution, it suffices to show the convergence of the characteristic function (Fourier transform of the density function). That is, it suffices to show, as D  X  X  X  , By our definition of z i in (12), we have
E =1  X   X  +  X   X  Z  X  Using the integral formula [40, 3.823, page 484], we obtain Also, by the Taylor expansion, Combining the results, we obtain
E =1  X   X  +  X   X  =1  X   X  | t |  X   X (1  X   X  )cos Once we know E log E exp =log =
X  X  =
X If max In order to obtain the very precise rate of the convergence, we need to conduct the inverse Fourier transform and watch out for the higher order terms. In [49], which considered the i.i.d. case (here we have to consider the independent but not identical situa-tion), the inverse Fourier transform was worked out by assuming some condition on the density which does not exist in our case.
In this study, we simply take the higher order terms, which, de-pending on  X  ,  X  , and the data, could be either the | t | 2 | t | 2  X  term, directly from the Fourier transform and call them the  X  X onvergence rate factor, X 
