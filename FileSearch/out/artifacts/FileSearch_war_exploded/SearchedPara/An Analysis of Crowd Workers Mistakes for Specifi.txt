 The TREC 2012 Crowdsourcing track asked participants to crowdsource relevance assessments with the goal of replicat-ing costly expert judgements with relatively fast, inexpen-sive, but less reliable judgements from anonymous online workers. The track used 10  X  X d-hoc X  queries, highly spe-cific and complex (as compared to web search). The crowd-sourced assessments were evaluated against expert judgments made by highly trained and capable human analysts in 1999 as part of ad hoc track collection construction. Since most crowdsourcing approaches submitted to the TREC 2012 track produced assessment sets nowhere close to the expert judge-ments, we decided to analyze crowdsourcing mistakes made on this task using data we collected via Amazon X  X  Mechani-cal Turk service. We investigate two types of crowdsourcing approaches: one that asks for nominal relevance grades for each document, and the other that asks for preferences on many (not all) pairs of documents.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval Crowdsourcing, Preference Judgment
Information Retrieval strategies require ground truth as-sessments on the produced results as a basis for optimiza-tion, training, and evaluation. Traditionally, for example within TREC efforts, the ground truth was obtained through very expensive expert humans looking through the pool of top retrieved documents and judging relevance to the re-spective queries. Since the cost of this manual effort is pro-hibitive in many ways, and is definitely not scalable with web search, many researchers are using alternative assess-ments from crowdsourcing services, like Amazon Mechani-cal Turk (AMT). This effort is a lot cheaper, but less reli-able. It is also far more suitable to trivial yes/no questions like "Does this picture contain a tiger?" than to intel-lectually complex questions like "What motivates racial profiling cases in the U.S.?" . Since crowdsourcing is quickly becoming very popular due to its cheapness, clearly research is needed to understand crowd workers X  behavior on non-trivial questions and in particular to understand the nature of the errors they make.

Previous research is focused on how to fix crowd worker X  X  errors by inferring worker quality and then weight  X  X ood X  and  X  X ad X  workers appropriately  X  all with variations of the Expectation Maximization algorithm [3]. An adversarial approach was proposed to prevent cheating, including an adaptive scheme for  X  X rap questions X  that prevents workers from exploiting easy-hit payable answers [4]. To the best of our knowledge, there is no previous work on analyzing crowd worker mistakes based on characteristics of documents being judged and complexity of task.

We therefore, focus on the nature of mistakes, and ask in this paper why crowdsourcing of particularly complex tasks has not yet succeeded in getting a reasonable approxima-tion of the expert assessments. The TREC 2012 Crowd-Sourcing track [5] targeted crowdsourcing on a highly spe-cific, intellectually complex task. The goal was to reproduce the existing set of answers (QREL) obtained from highly trained experts over 10 queries randomly selected out 50 from the TREC 1999 ad-hoc task. The TREC ad-hoc 1999 task is one of the most complete sets of query-documents ever judged, with an average of about 1,800 documents as-sessed per query. As a result of the complexity and speci-ficity of the track queries, most crowdsourcing approaches submitted to the 2012 TREC track failed to achieve a per-formance good enough to be considered viable substitutes for the expert QREL. Assessments submitted to the TREC track which disagreed with the QREL were analyzed and eventually regarded as crowdsourcing errors rather than er-rors in the QREL. Looking at the crowdsourced errors, for various queries and questions asked, we analyze workers be-havior and mistakes with respect to:
We consider two types of crowdsourced tasks: relevance grades , resulting in a document relevance assessment in the set { 0 , 1 , 2 , 3 , 4 } , and pair preference assessments [2] result-ing in a stated preference between two documents. Typ-ically relevance grade questions are more difficult, but re-quire fewer questions to assess the entire collection. We have run separate nominal(relevance grade) and pair assessments using AMT crowd workers, and our analysis in Section 2 presents each result for both types.
In our crowdsourcing interface, users are first shown in-structions explaining the task and providing general guide-lines for determining the relevance of a document to the query. When the user acknowledges having read the instruc-tions, the interface presented in figure 1 is displayed. The query is shown at the top, along with its description, nar-rative, and the number of remaining questions in the task. When the user selects a relevance grade (or chooses which document is preferred), the next document (or pair) is au-tomatically presented.

The interface for relevance grades (figure 1, left) shows a simple one-to-five star voting scheme with a textual descrip-tion of the meaning of each star appearing both in a tooltip and in text to the right of the stars when the mouse cursor is placed over a particular star. The users are told that the descriptions will appear when the mouse is placed over each star. The pair preferences interface (figure 1,right) has but-tons which allow the user to select the preferred document, or to say that they are both good or both bad to indicate a tie.

The documents in the corpus are unformatted. We pre-sented the document title and byline (when present) in bold and separated the paragraphs, but we did not add any addi-tional formatting. In particular we did not highlight query terms within the documents, so any user bias toward query terms in our findings could not have resulted from any extra emphasis our interface gave them.

We ran our experiments on the 10 queries proposed by the TREC 2012 crowdsourcing track. The official list of documents contains about 18,200 documents, or about 1,820 per query. Out of these the 1999 TREC ad-hoc QREL marks about 650 as relevant by the experts, or about 65 per query. For the purpose of our analysis,  X  X elevant X  or R refers to expert-QREL marked relevant, and  X  X on-relevant X  or NR refers to the rest of the documents, i.e. the ones expert-QREL marked nonrelevant.

For the pair preferences, we collected 5 random pairings for each document. We calculate initial score of each doc-ument using BM25 scoringto produce an initial ranking of the documents for each topic. We collected complete pair-wise preferences between the top six documents; for each document below the top six, we select five documents from the set of documents with higher BM25 scores, uniformly at random. In this scheme documents at the very top of the BM25 ranking are likely to appear in many pairs. For quality control we have used the well known techniques of using gold data to monitor workers, and obtaining multiple labels from different workers for reducing noise through ma-jority rule [4]. For graded relevance judgments, we collected 5 assessments for each document for a total of about 90,000 judgments and used 5 trap questions in batch of 20 docu-ments. For the pair preferences, we collected 5 assessments for each pair and used 5 trap questions in batch of 20 pairs.
Previous research [2] has shown that it is easier for asses-sors to make pairwise assessments as compared to nominal assessments with multiple relevance grades, but the study was done using expert assessors on a very small set of docu-ments (5 documents per topic). We are using a much larger set of assessed documents, more challenging task/queries, and non-expert assessors (crowd workers). The precision results in figure 2 seem contradictory to the previous claim that pair preferences are always easier for assessors than rel-evance grades. We think this is because the nature of the queries: some of the documents could be categorized as ei-ther relevant or non-relevant, and the workers often chose to express a preference between two documents, both expert-assessed as nonrelevant.
Most of our charts present some measurement versus an error rate, per topic. These error rates are the observed probability of a randomly-selected worker making an error of a certain type. We consider a relevance grade to be correct if a worker assigns a grade of 3 or 4 to a relevant document or a grade of 0, 1, or 2 to a non-relevant document, and a pair preference to be correct if a worker prefers a rele-vant document over a non-relevant document. For consis-tency reasons, we exclude preferences between two relevant or two non-relevant documents. X-axis on all plots repre-sents TREC specific topic numbers. We use box and whisker plots in order to present the distribution over workers. The line in the center of each box is its median value; each box covers the inner 50% of the data, and the whiskers extend to the highest and lowest observed values which lie within 1.5 times the range covered by the box. We ran Welch two sample t-tests for the hypothesis that the mean of one distri-bution is greater than the mean of the other, and mark with asterisks (*) those topics which have statistically significant findings ( p &lt; 0 . 05).
We present the precision and recall of our workers in Fig-ure 2. Let R represent set of relevant documents according to gold standard truth (expert judgements) and J(d) repre-sent the crowd worker judgment for document d. For rele-vance grades, these values are computed in a straightforward manner as defined in equations 1 and 2.
 where the documents that were assigned label of 3 or 4 represent documents judged as relevant by crowd workers.
Note that we count individual worker judgements here; we do not attempt to aggregate judgements for the same document. We chose to count in this manner in order to produce numbers which best reflected the decisions of indi-vidual workers. Calculating these values for preference pairs is less straightforward as defined in equations 3 and 4. keywords and description F igure 2: Recall (left) and precision (right) for graded assessments and pair preferences. Recall higher on pair preferences indicates that workers easily identified the relevant doc in the pair; preci-sion higher for relevance grades indicates that fewer non-relevant documents were assessed as relevant.
 where J ( d i , d j ) represents the crowdworker preference be-tween document d i and document d j .

Recall is calculated as the ratio of the number of cor-rect judgements between a relevant and a non-relevant doc-ument to the total number of judgements between a relevant and a non-relevant document, and precision as the ratio of the number of correct judgements between a relevant and a non-relevant document to the number of judgements which were not ties. As with relevance grades, we count individual worker judgements rather than trying to aggregate.
For most topics, across both relevance grades and pair preferences, recall was much higher than precision. That is, workers seem to mislabel a lot of documents as being rele-vant. Note that recall is higher in general for pair prefer-ences than for relevance grades: this supports our belief that it X  X  easier to recognize a relevant document in comparison with a non-relevant document than it is to give a sufficiently high relevance grade to a relevant document provided on its own. While the precision results may seem low overall for both pair preferences and relevance grades, they are above the median results submitted at TREC 2012 Crowdsourcing track; more reason for this analysis. F igure 3: Quality of workers versus time spent. The y axis shows the distribution over average fraction of correct answers per worker. Workers are split into three groups based on their average durations. T-tests were run between the fastest and slowest groups; asterisks indicate low p-value ( p &lt; 0 . 05 ). The best workers spent more time on average per as-sessment on both relevance grades (left) and pair preferences (right), but the effect is smaller for pair preferences.

We calculated both the normalized number of correct an-swers each worker gave and the average time the worker spent producing each answer. We then divided the workers into three groups for each topic: workers whose average du-ration lies within one standard deviation of the mean, work-ers whose average duration was more than a standard devi-ation below the mean, and workers whose average duration was more than a standard deviation above. We found that for most topics, for both relevance grades and pair prefer-ences, workers who spent more time produced more accurate responses. Our results are presented in Figure 3.
In the process of diagnosing the nature of the problem, we observed that many of the non-relevant documents which were preferred over relevant documents were much longer than the relevant documents. Perhaps workers tend to feel that a longer document is more authoritative. In Figure 4, we compare the error rate of workers for comparisons when the relevant document is longer to the error rate when the non-relevant document is longer. As a baseline, we compare this data to the error rate of relevance grades for documents whose lengths are above or below the mean document length for each topic. For relevance grades, we find that in most Figure 4: Document Length vs. Error rate. Longer relevant documents were assessed more accurately with relevance grades (left) and pair preferences (right). Error increased when on pairs of a non-relevant document VS a shorter relevant document. topics worker assessments are more reliable for longer doc-uments.

This trend continues for preference data. The success rates for pairs in which the relevant document was longer (the red bars) are almost all higher than the success rates when the non-relevant document was longer (the blue bars). This suggests to us that many workers will give a preference for a longer document over a shorter, perhaps without care-fully considering the relative information content of those documents.
We observed several cases where workers preferred non-relevant documents containing many query terms over rele-vant documents, so we wanted to measure how often that is the case, by calculating the TF-IDF score for each document and measuring that against assessment quality.

For our baseline, we plot the success rates for workers as-signing relevance grades to documents whose TF-IDF scores are either above or below the mean score for that topic. We find no consistent trend across topics: for many topics, the success rate is similar on both sides of the mean. On some topics, however, there is a statistically significant increase in mistakes for documents with small TF-IDF scores. For pair preferences, on the other hand, there is a clear trend: when the relevant document has a higher TF-IDF score than the non-relevant document, the assessments are more reliable. Figure 5: TF-IDF vs. Error. Only half of the topics for relevance grades show a statistically significant result ( p &lt; 0 . 05 ) for TF-IDF. For pair preferences, workers preferred non-relevant documents over rel-evant documents with lower TF-IDF scores.
For our final plot, we speculated that the presence of query terms in the title of a document would strongly influence whether workers preferred it. This notion is similar to the  X  X itleStat X  notion introduced by Soboroff et al [1]. Note that we empirically tested that the presence of query terms in the title of document has no correlation with Tf-IDF score of document. Therefore, it made sense to analyze the influ-ence of query terms in title of documents separately. We measure document title relevance as the number of query terms appearing in the document X  X  title. Note that many documents either have no title or have no query terms in the title; these documents are assigned a score of zero. In our baseline, we find that query terms in the document ti-tle are well correlated with worker accuracy for most topics. This trend continues with pair preferences: the presence of more query terms in the title of the non-relevant than the relevant document led to uniformly higher error rates. F igure 6: Title Relevance vs. Error. Non-relevant documents are frequently mislabeled (left) or pre-ferred over relevant documents (right) when their titles are rich in query terms.
In this work, we have analyzed the assessments made by crowd workers for two different types of relevance assess-ments: pair preferences and nominal relevance grades. We have observed that crowd workers have biases toward certain features of documents.

We have also found that it is easier to identity a relevant document when it is compared to a non-relevant document as compared to when it is presented on its own. We can conclude from this study that preference judgments are not always a better choice as compared to nominal judgments and one has to choose the type of relevance assessment very carefully based on the characteristics of the collection and nature of task. [1] C. Buckley, D. Dimmick, I. Soboroff, and E. M. [2] B. Carterette, P. N. Bennett, D. M. Chickering, and [3] M. Hosseini, I. J. Cox, N. Mili  X c-Frayling, G. Kazai, and [4] P. G. Ipeirotis, F. Provost, and J. Wang. Quality [5] M. D. Smucker, G. Kazai, and M. Lease. Overview of
