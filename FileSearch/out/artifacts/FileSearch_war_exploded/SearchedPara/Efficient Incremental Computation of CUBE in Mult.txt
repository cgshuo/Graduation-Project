 What-if analysis is a particularly common and very important decision support process. It has applications in marketing, productio n planning, and other areas. What-if analysis is used to forecast future performance under a set of assumptions related to past data. It also enables the evaluation of past performance and the estimation of the opportunity cost taken by not following alternative policies in the past. For example, if the sale price of drink was increased by 10%, how would it affect the profit of the company? What-if analysis can provide important predicting information for decision-maker. by[1][2][3]. [5]analyzed the types of dimension updates, and the method of maintaining cube under dimension updates. Most of what-if analyses are based on the hypothetical fact table updates[6][7][8]. It analyzes the influence of the change of aggregation value on the base data from top to bottom. data form a what-if view. Each what-if view shows in detail the result of a combination of decisions, and allows the decision maker to observe the global effect of those deci-sions. The decision maker can compare the results of multiple what-if views to select the optimal plan. Since what-if view differs only on a few items from its original data, we what-if version can base on another what-if version, how to these what-if versions and original data to get correct what-if view is a problem to be considered. We formulate the problem and propose a novel method to process multiple version what-if update data. house and OLAP system. As assumed changes are made to the schema, dimension or fact table, a new cube, which has same aggregate function, dimension and measure attributes as the materialized cube, must be updated to reflect the changed state of the incremental computed by calculate the changes to the views due to the source changes. The delta cube[7] was proposed to incremental compute the new cube. By the method, propagate and refresh. In the propagate stage, a delta cube  X  C was computed from the changes of the data source. The  X  C has same aggregate function as the cube view. In the refresh stage, the cube view was refreshed by applying the  X  C to it. The method fits the incremental computing self-maintainable[7] aggregate functions, such as SUM, COUNT, etc.. But, the MAX and MIN functions are self-maintainable with respect to insertions, non-self-maintainable with respect to deletions. For deletion operations, when a tuple having the maximum or minimum is deleted, the new maximum or minimum value for the group must be recomputed. The new maximum or minimum value was recomputed from the base table and delta table[7]. Due to the size of base mental computation data cube for MAX and MIN aggregation functions(abbr. MAX and MIN cube): R_NLC. It uses delta cube method incremental computation MAX and MIN cube. If the tuple having the maximum or minimum is deleted, the new maximum or minimum value is recomputed from the next level cuboid. It reduced the access frequency of base table. As a result, the cost of incremental compute for MAX and MIN cube is substantially reduced. 
This paper is organized as following: Section 2 will discuss the related work. Then, in section 3, the method to process multiple-version of what-if update data is presented. In section 4, we will describe the algorithm to incrementally computing MAX and MIN cube in detail. The results of performance experiments are given in section 5. Finally, we conclude our work in section 6. In data warehouse and OLAP system, some cuboids are materialized to improve the query performance. When the data of fact ta ble changed, the materialized cuboids must be updated to reflect the changed state of the data source. There are two strategies to re-compute the data cube: (1) discarding the materialized cuboids, and re-computing new cuboids from the fact table. (2) incremental computing the new cuboids based on the original materialized cuboids. The advantage of first strategy is that it can be used in any situations. Its disadvantage is that it can not make full use of existing resource. The re-computation of data cube consumes a lot of system resources. The advantage of second strategy is that it can make full use of existing cuboids, saving system resources. thetical update of the data sources. [2][3]describe different approaches to track schema changes and enable simultaneous queries on different schema version. [3]proposes a SQL query rewriting technique to support the OLAP query analysis in multidimen-sional database systems modeled on multiversion schema. [4] introduces independ-ently-updated views for creating multiple scenarios. But, they do not consider the cube under hypothetical dimension update are researched by [5][9] [10][11]. They do not consider the incremental maintenance of holistic functions. [7][8] research the incremental maintenance of data cube under hypothetical fact table update. Both of them adopt delta cube method to incremental compute data cube under fact table up-date. [7] describes an approach for incremental maintenance of MAX and MIN cube. By the approach, when the tuple having the maximum or minimum value is deleted, the new maximum or minimum value of the group is recomputed from the base table and proposes an incremental computation method for data cube that can maintain a data cube by using only delta cuboids. Because the amount of delta cuboids is reduced does not discuss how to update the MAX and MIN cube with respect to deletions. In what-if applications, what-if operations are temporary. The what-if update data based on what-if operations does not exist. These what-if update data should not de-stroy the real data in the OLAP system. Delta table is adopted to store what-if update data. This paper uses two types of delta tables to store what-if data. One is insert_delta hypothetical conditions are transformed into deletions and insertions. 3.1 Basic Conceptions about Multiple Versions Upd}}. Where, OP A is an atomic hypothetical update operation. It may be insert(Ins), delete(Del) or update(Upd). That is, a what-if operation is a set of atomic hypothetical update operation. In what-if analysis, there may be many what-if operation based on the same base table. What-if operation may base on tables or what-if views. what-if update data, denoted as D wi . D wi  X  OP wi means that the what-if operation OP wi produces a set of data D wi . Definition 3: base table: A base table T B is the object of what-if operation. In OLAP system, T B may be fact table, view or dimension table. Definition 4: what-if view: The merging of what-if update data D wi with its base table T forms a what-if view, denoted as VI , then VI = T B  X  D wi . what-if version, denoted as V . V  X  OP wi means the what-if version V is produced by The data of base table T B forms the base version V B of what-if analysis. The data of versions may be independently, or relatively. As shown in Figure 1. 
V 3.2 The Processing of Multiple Versions of What-If Data V the what-if view VI n of what-if version V n need to be computed while processing the hypothetical query on VI n . 
Because the number of tuples in base table is much more than the number of tuples in delta table, the second method has higher efficiency than the first method. But, both of them need merge the delta tables and the base table. 
When incremental computing cube by delta method, it only need compute the delta_cube based on the delta tables. Therefore, it need not merge the delta tables and the base table. Only the merge of delta tables is needed. Based on different aggregate discussed as following:  X  processing method is correct to some aggregate functions, for instants, SUM, COUNT, etc.. But, it is not correct to other aggreg ate functions, for instance, MAX, MIN, etc.. For example, the data in  X  1 + ,  X  1 -,  X  2 + and  X  2 -is shown in Figure 2: After delta tables are merged by above method,  X  + and  X  -are shown in Figure 3. 
It is assumed that the aggregate function is MAX. Then, the maximum value of from  X  2 -, so it does not exist. The maximum value 5 is not the correct maximum value of group (1,1,1). 
For aggregate functions, such as SUM and COUNT, this processing method is correct. For SUM, the total of Unit_sales can be computed by subtracting the subtotal of Unit_sales in  X  -from the subtotal of Unit_sales in  X  + . The tuple (1,1,1,5) is added once, and then is subtracted once, so it has no effect on the result. Definition 7. Invertible aggregate function: For a function F, if existing a function F X  algebraic functions, then F is an invertible aggregate function, F X  is the inverse function of F. For example, aggregate functions SUM, COUNT are invertible aggregate function. The inverse function of SUM and COUNT is SUBSTRACT. 
The aggregate functions MAX, MIN and MEDIAN are not invertible aggregate function. 
For different type of aggregate functions, different method can be adopted to process multiple versions of what-if data to improve the efficiency. Rule 1. For invertible aggregate function, using the following method to process multi-version what-if data: F(
 X  ) = F X (F(  X  p + ), F(  X  p -)), then: 
For non_invertible aggregate function,  X  + and  X  -should satisfies the follow condi-t  X   X  j + , i &lt; j &lt;= n. Rule 2: For non_invertible aggregate function,  X  + and  X  -can be computed by following formula[12]: 
The sequence of above operations is based on the sequence of versions. Otherwise, it 
In this result, the tuple(2,2,1,8) is deleted. But, it is deleted from  X  2 -, then inserted into  X  3 + . Because  X  3 + is later than  X  2 -, it should be kept in the result. 
Obviously, the set operations of Rule 1 are much less than Rule 2. It has no demand processing performance. While the number of dimension attributes is n, the CUBE contains 2 n cuboids, and each cuboid is defined by a single SELECT-FROM-WHERE-GROUP BY block, having identical aggregate functions, identical FROM and WHERE clauses, and one of the 2 n subsets of the dimension attributes as the group-by columns. Figure 4 shows a CUBE with three dimension attributes (level i indicates the level of cuboid). exists a materialized view on T B , we can carry out the what-if analysis by incremental computation the new view with the materialized view, T B and D wi . In this section, we discuss the incremental computation of MAX and MIN cube. 
MAX and MIN function are self-maintainable with respect to insertions. The new maximum or minimum value of a group can be computed by comparing the old maximum or minimum value with the maximum or minimum value of the group in delta table. Therefore, for insertions, the incremental computation of MAX or MIN cube is same to the SUM cubes. This paper does not discuss it again. The discussion cremental computation of MAX and MIN cube with respect to deletions. 
MAX and MIN functions are not self-maintainable with respect to deletions, so the incremental computation of MAX and MIN cube are more complex than the SUM or COUNT cube. [7]introduces a method for incremental computing MAX and MIN cube. When the tuple holding maximum or minimum value is deleted, the new maximum or minimum value for the group can be recomputed from the base table and the delta table. Because the number of tuples in the base table is very large, the cost of re-computation of maximum(minimum) value is very high. CUBE: R_NLC(Refreshed by Next_Level Cuboid): The correctness of the algorithm is proofed as following: disjoint subsets of S. S D is a subset of S, denoting the set of tuples which need to be aggregate function, MAX(S -S D ) = MAX({MAX(S 1 -S D ), MAX(S 2 -S D ), ..., MAX(S n -S D )}). Simultaneously, MIN(S -S D ) = MIN({MIN(S 1 -S D ), MIN(S 2 -S D ), ..., MIN(S n -S D )}). End. For example, for a cube C with three dimension attributes: a, b and c, it has eight cu-boid: abc, ab, ac, bc, a, b, c and  X  . The incremental computing procedure for cuboid new_abc(Figure 5(a)) and new_ab(Figure 5(b)) is shown in Figure 5. In the procedure of incremental compute of cuboid new_abc, wh en the tuple of cuboid abc is (1,1,2,5,1), there exists a tuple (1,1,2,5,1) in cuboid  X  abc, that is the tuple with maximum value 5 is deleted. Therefore, the maximum value of group(1,1,2) needs to be recomputed. Be-cause abc is the cuboid of level 0, the new value is recomputed from the base table and delta table. The result of recomputed is4, then inserting tuple(1,1,2,4,1) into new_abc. When incrementally computing of cuboid new_ab and the tuple of cuboid ab is (1,1,5,1), there exists a tuple(1,1,5,1) in cuboid  X  ab. Therefore, the maximum value of group(1,1) needs to be recomputed. Because cuboid new_abc has been computed, we can compute the new maximum value from new_abc to get 4, then inserting the tuple (1,1,4,1) into cuboid new_ab. 4.1 Computing Delta Cube R_NLC algorithm computes a delta cube from delete_delta table Firstly. The delta cube has the same aggregate function, dimension and measure attributes as base cube. In a group, there may be more than one tuples having the same measure values, the algo-rithm adopts count to determine if all of tuples with the same measure values have been function, the delta cube has another measure attribute: COUNT. The values of COUNT maximum or minimum value in the delete_delta table. Therefore, during computing delta cube, not only the maximum value of each group need to be computed, but also the number of tuples having the maximum value in the group need to be computed. 4.2 Incremental Compute the MAX and MIN Cube After the delta cube is computed, we compute the new cube by the base cube and delta cube. The incremental computing procedure for MAX cubes is the following (The algorithm for MIN cubes is similar to it.):  X  t(a 1 ,a 2 ,...,a i , m_del, count_del) which has the same dimension attribute values as t in the compounding delta cuboid: 
When re-computing the new maximum value of the group(a 1 ,a 2 ,...,a i ), if the group belongs to cuboids of level 0, then re-computing the new maximum value from the base re-computing the new maximum value from the cuboid of level i-1. 
If all of the tuples of delete_delta table come from the base fact table, the (2)(iv) of above algorithm need no t to be considered. method R_NLC is compared with an incremental CUBE maintenance method that re-computes the maximum(minimum) value from the source data and delta data. We use an Oracle 10g database system running on a PC with 3GHZ Pentium4 CPU and 1G RAM. The methods used in the experiments are implemented using PL/SQL in Oracle 10g. The FoodMart database of Microsoft Analysis Service is used in the experiments. customer_id and store_id in the sales_fact_1997 are used as dimension attributes, while unit_sales is used as a measure attribute. 
In this experiments, we firstly compare the performance of the two merge meth-ods(denoted as merge_by_rule1 and merge_by _rule2 in Figure 6 respectively). We use Figure 6. It shows that the efficiency of merging delta tables by Rule1 is better than gregate function, the delta table should be merged using Rule1. 
Then, we evaluated the performance of R_NLC. Table 1 shows two cuboids, i.e., C1 and C2 used in the experiments. In the experiments, we compared R_NLC with an compute the new maximum or minimum value when the tuple with the maximum or minimum value is deleted(denoted as R_BFT).

Figure 7 shows the results of performance experiment when we varied update ratio for a fixed size of the base table. In the experiments, the size of base table is 800000 tuples. We varied the update ratio of the fact table from 5% to 20%. In Figure 7, the time of the R_NLC algorithm is less than another algorithm. The performance is improved about 10% by R_NLC. The results of experiments show that reducing the access times to base from the fact that R_NLC uses the result of previous computation. 
We next vary the size of base table from 400000 to 1600000 tuples for a fixed update ratio 10% to evaluate the scalability performance. The result (shown in Figure 8) shows that R_NLC has better performance than R_BFT. The benefit to R_NLC increases when the size of base table increases. This paper proposed a strategy for multiple versions what-if update data: improving the merging efficiency by using different methods to merge multiple versions what-if update data according to different types of aggregate functions. We also proposed an algorithm R_NLC to incrementally compute MAX(MIN) cubes. In this algorithm, the cubes are incrementally computed from botto m to up. It incrementally computes the computed by base table and delta table. In the performance evaluation, the efficiency of the R_NLC algorithm over the previous method is demonstrated. Because of benefit from the reduction of the access times of base table, R_NLC improved the performance effectively. What-if analysis plays an important role in data warehouse and OLAP system. There are many problems about what-if analysis which should be further studied, such as, the framework of what-if analysis, the performance optimization of problems in the future work. 
