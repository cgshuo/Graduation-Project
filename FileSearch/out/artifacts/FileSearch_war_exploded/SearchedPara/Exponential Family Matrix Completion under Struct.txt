 Suriya Gunasekar SURIYA @ UTEXAS . EDU The University of Texas at Austin, Texas, USA In the general problem of matrix completion, we seek to recover a structured matrix from noisy and partial mea-surements. This problem class encompasses a wide range of practically important applications such as recommen-dation systems, recovering gene X  X rotein interactions, and analyzing document collections in language processing, among others. In recent years, leveraging developments in sparse estimation and compressed sensing, there has been a surge of work on computationally tractable estimators with strong statistical guarantees, specifically for the set-ting where a subset of entries of a low X  X ank matrix are observed either deterministically, or perturbed by additive noise that is Gaussian (Candes &amp; Plan, 2010), or more gen-erally sub X  X aussian (Keshavan et al., 2010b; Negahban &amp; Wainwright, 2012). While such a Gaussian noise model is amenable to the subtle statistical analyses required for the ill X  X osed problem of matrix completion, it is not always practically suitable for all data settings encountered in ma-trix completion problems. For instance, such a Gaussian error model might not be appropriate in recommender sys-tems based on movie ratings that are either binary (likes or dislikes), or range over the integers one through five. The first question we ask in this paper is whether we can gen-eralize the statistical estimators for matrix completion as well as their analyses to general noise models? Note that a noise model captures the uncertainty underlying the matrix measurements, and is thus an important component of the problem specification given any application; and it is thus vital for broad applicability of the class of matrix comple-tion estimators to extend to general noise models. Though this might seem like a narrow technical, although important question, it is related to a broader issue. A Gaussian observation model implicitly assumes the ma-trix values are continuous X  X alued (and that they are thin X  tail X  X istributed). But in modern applications, matrix data span the gamut of heterogeneous data X  X ypes, for instance, skewed X  X ontinuous, categorical X  X iscrete including binary, count X  X alued, among others. This, thus gives rise to the second question of whether we can generalize the stan-dard matrix completion estimators and statistical analyses, suited for thin X  X ailed continuous data, to more heteroge-neous data X  X ypes? Note that there has been some recent work for the specific case of binary data by Davenport et al. (2012), but generalizations to other data X  X ypes and distri-butions is largely unexplored.
 The recent line of work on matrix completion, moreover, enforces the constraint that the underlying matrix be either exactly or approximately low X  X ank. Aside from the low X  rank constraints, further assumptions to eliminate overly  X  X piky X  matrices are required for well X  X osed recovery un-der partial measurements (Candes &amp; Recht, 2009). Early work provided generalization error bounds for various low X  rank matrix completion algorithms, including algorithms based on nuclear norm minimization (Candes &amp; Recht, 2009; Candes &amp; Tao, 2010; Candes &amp; Plan, 2010; Recht, 2011), max X  X argin matrix factorization (Srebro et al., 2004), spectral algorithms (Keshavan et al., 2010a;b), and alternating minimization (Jain et al., 2013). These work made stringent matrix incoherence assumptions to avoid  X  X piky X  matrices. These assumptions have been made less stringent in more recent results (Negahban &amp; Wainwright, 2012), which moreover extend the guarantees to approxi-mately low X  X ank matrices. Such (approximate) low X  X ank structure is one instance of general structural constraints which are now understood to be necessary for consis-tent statistical estimation under high X  X imensional settings (with very large number of parameters and very few obser-vations). Note that the high X  X imensional matrix comple-tion problem is particularly ill X  X osed, since the measure-ments are typically both very local (e.g. individual matrix entries), and partial (e.g. covering a decaying fraction of entries of the entire matrix). However, the specific (approx-imately) low X  X ank structural constraint imposed in the past work on matrix completion does not capture the rich va-riety of other qualitatively different structural constraints such as row X  X parseness, column X  X parseness, or a superpo-sition structure of low X  X ank plus elementwise sparseness, among others. For instance, in the classical introductory survey on matrix completion (Laurent, 2009), the authors discuss structural constraints of a contraction matrix , and a Euclidean distance matrix . Thus, the third question we ask in this paper is whether we can generalize the recent line of work on low X  X ank matrix completion to the more general structurally constrained case.
 In this paper, we answer all of the three questions above in the affirmative, and provide a vastly unified framework for generalized matrix completion. We address the first two questions by considering a general matrix completion set-ting wherein observed matrix entries are sampled from any member of a rich family of natural exponential family dis-tributions . Note that this family of distributions encom-pass a wide variety of popular distributions including Gaus-sian, Poisson, binomial, negative X  X inomial, Bernoulli, etc. Moreover, the choice of the exponential family distribu-tion can be made depending on the form of the data. For instance, thin X  X ailed continuous data is typically modeled using the Gaussian distribution; count X  X ata is modeled through an appropriate distribution over integers (Poisson, binomial, etc.), binary data through Bernoulli, categorical X  discrete through multinomial, etc. We address the last question by considering general structural constraints upon the underlying matrix, as captured by a general regulariza-tion function R ( . ) . Our general matrix completion setting thus captures heterogeneous noise X  X hannels, for heteroge-neous data X  X ypes, and heterogeneous structural constraints. In a key contribution, we propose a simple regularized convex M  X  X stimator for recovering the structurally con-strained underlying matrix in this general setting; and moreover provide a unified and novel statistical analysis for our general matrix completion problem. Following a standard approach (Negahban, 2012), we (a) first showed that the negative log X  X ikelihood of the subset of observed entries satisfies a form of Restricted Strong Convexity (RSC) (Definition 4); and (b) under this RSC condition, our proposed M  X  X stimator satisfies strong statistical guar-antees. We note that proving these individual components for our general matrix completion problem under general structural constraints required a fairly delicate and novel analysis, particularly the first component of showing the RSC condition, which we believe would be of independent interest. A key corollary of our general framework is ma-trix completion under sub X  X aussian samples and low X  X ank constraints, where we show that our theorem recovers re-sults comparable to the existing literature (Candes &amp; Plan, 2010; Keshavan et al., 2010b; Negahban &amp; Wainwright, 2012). Finally, we corroborate our theoretical findings via simulated experiments. 1.1. Notations and Preliminaries In this subsection we describe the notations and definitions frequently used throughout the paper. Matrices are denoted by capital letters, X ,  X  , M , etc. For a matrix M , M j M ( i ) are the j th column and i th row of M respectively, and M ij denotes the ( i,j ) th entry of M . The transpose , trace , and rank of a matrix M are denoted by M  X  , tr ( M ) , and rk ( M ) , respectively. The inner product between two matrices is given by  X  X,Y  X  = tr ( X  X  Y ) = P ( i,j ) X For a matrix M  X  R m  X  n of rank r , with singular val-ues  X  1  X   X  2  X  ... X  r , commonly used matrix norms in-clude the nuclear norm k M k  X  = P i  X  i , the spectral norm k M k 2 =  X  1 , the Frobenius norm k M k F = P i  X  2 i , and the Given any matrix norm k X k , the dual norm , k X k  X  is given by k X k  X  = sup k Y k X  1  X  X,Y  X  .
 Definition 1 (Natural Exponential Family) . A distribution of a random variable X , in a normed vector space is said to belong to the natural exponential family , if its probability density function, characterized by the parameter  X  in the dual vector space, is given by: where G ( X ) = log R X h ( X ) e  X  X,  X   X  dX, called the log X  partition function, is strictly convex, and analytic. Definition 2 (Bregman Divergence) . Let  X  : dom (  X  )  X  R be a strictly convex function differentiable in the relative interior of dom (  X  ) . The Bregman divergence (associated with  X  ) between x  X  dom (  X  ) and y  X  ri ( dom (  X  )) is de-fined as: Definition 3 (Subspace compatibility constants) . Given a matrix norm R ( . ) , we define the following maximum and minimum subspace compatibility constants of R ( . ) w.r.t the subspace M :  X ( M ; R ) = sup Thus,  X   X   X  X  Definition 4 (Restricted Strong Convexity) . A loss func-tion L is said to satisfy Restricted Strong Convexity with respected to a subspace S , if for some  X  L &gt; 0 , L ( X  +  X )  X  X  ( X )  X  X  X  X L ( X ) ,  X   X  X  X   X  L k  X  k 2 F ,  X   X   X  S. Definition 5 (Sub X  X aussian Distributions) . A random variable, X , is said to have a sub X  X aussian distribution with parameter b , if  X  s &gt; 0 , the distribution satisfies E [ e sX ]  X  e s 2 b 2 / 2 . Further, if X is sub X  X aussian with pa-rameter b , then E [ X ] = 0 and V ar ( X )  X  b 2 (refer Ver-shynin (2010)). Denote the underlying target matrix by  X   X   X  R m  X  n . We then assume that individual entries  X   X  ij are observed in-directly via a noisy channel: specifically, via a sample drawn from the corresponding member of natural exponen-tial family (see Definition 1):
P ( X ij |  X   X  ij ) = h ( X ij ) exp X ij  X   X  ij  X  G ( X   X  where G : R  X  R , is a strictly convex, and analytic func-tion called the log X  X artition function.
 Consider the random matrix X  X  R m  X  n , where each entry X ij is drawn independently from the corresponding distri-bution in (1); it can be seen that: P ( X |  X   X  ) = Y where we overload the notation to denote G : R m  X  n  X  R as G ( X ) = P ij G ( X  ij ) , and the base measure h ( X ) as h ( X ) = Q ij h ( x ij ) .
 Uniformly Sampled Observations: In a  X  X ully observed X  setting, we would observe all the entries of the observation matrix X  X  R m  X  n . However, we consider a partially ob-served setting, where we observe entries over a subset of indices  X   X  [ m ]  X  [ n ] . We assume a uniform sampling model, so that  X  ( i,j )  X   X  , i  X  uniform ([ m ]) , j  X  uniform ([ n ]) . (3) Given,  X  , we define the following matrix P  X  ( X ) : P The matrix completion task can then be stated as the es-timation of  X   X  from the partially observed matrix P  X  ( X ) , where X  X  P ( X |  X   X  ) . As noted earlier, this problem is ill X  posed in general. However, as we will show, under struc-tural constraints imposed on the parameter matrix  X   X  , we are able to design an M  X  X stimator with a near optimal de-viation from  X   X  . 2.1. Applications Gaussian (fixed  X  2 ) is typically used to model continuous data, x  X  R , such as measurements with additive errors, affinity datasets. Here, G (  X  ) = 1 2  X  2  X  2 .
 Bernoulli is a popular distribution of choice to model bi-nary data, x  X  { 0 , 1 } , with G (  X  ) = log (1 + e  X  ) . Some examples of data suitable for Bernoulli model include so-cial networks, gene protein interactions, etc.
 Binomial (fixed N ) is used to model number of successes in N trials. Here, x  X  { 0 , 1 , 2 ,...,N } , and G (  X  ) = N log (1 + e  X  ) . Some applications include predicting suc-cess/failure rate, survey outcomes, etc.
 Poisson is used to model count data x  X  X  0 , 1 , 2 ,... } , such as arrival times, events per unit time, click X  X hroughs among others. Here, G (  X  ) = e  X  .
 Exponential is often used to model positive valued con-tinuous data x  X  R + , specially inter arrival times between events. Here, G (  X  ) =  X  log (  X   X  ) . 2.2. Log X  X ikelihood Denote the gradient map: g ( X ) ,  X  G ( X )  X  R m  X  n , where g ( X ) ij = It can then be verified that the mean and variance of the distribution P ( X |  X   X  ) are given by E [ X ] = g ( X  Var ( X ) =  X  2 G ( X   X  ) , respectively. The Fenchel conjugate of the log partition function G , is denoted by: F ( X ) , sup  X   X  X,  X   X  X  X  G ( X ) .
 A useful consequence of the exponential family is that the negative log X  X ikelihood is convex in the natural parameters  X   X  , and moreover has a bijection with a large class of Breg-man divergences (Definition 2). The following relationship was first noted by (Forster &amp; Warmuth, 2002), and later established by (Banerjee et al., 2005) [Theorem 4]: 2.3. Discussion and directions for future work We consider the standard matrix X  X ompletion setting where the distribution of the observation matrix X in (2) corre-sponds to its entries X ij being drawn independently from the other entries. Further, the probability of observing a specific entry X ij , under uniform sampling is independent of the noise channel or the distribution P ( X ij |  X   X  ij ever, in some applications, it might be beneficial to have a sampling scheme involving dependencies; for instance, when a user gives a movie a bad rating, we might want to vary the sampling scheme to sample an entirely different region of the matrix. It would be interesting to extend the analysis of this paper to such a dependent sampling setting. The form of the observation random matrix distribution in (2), given the individual observations in (1), can be seen to have connotations of multi X  X ask learning: here recover-ing each individual matrix entry  X   X  ij together with its cor-responding noise model forms a single task, and all these tasks can be performed jointly given the shared structural constraint on  X   X  . It would be interesting to generalize this to form a more general statistical framework for partial multi-task learning.
 We use the general class of exponential family distribu-tions as the underlying probabilistic model capturing the measurement uncertainties. The richness of the class of ex-ponential family distributions has been used in other set-tings to provide general statistical frameworks. Kakade et al. (2010) provide a generalization of compressed sens-ing problem to general exponential family distributions. Note however that results from compressed sensing can-not be immediately extended to matrix completion case, since the sampling operator P  X  does not satisfy the typi-cal assumptions (restricted isometry or restricted eigenval-ues) made in such settings; see (Candes &amp; Recht, 2009) for additional discussion. There have been extensions of classical probabilistic PCA (Tipping &amp; Bishop, 1999) from Gaussian noise models to exponential family distribu-tions Collins et al. (2001); Mohamed et al. (2008); Gordon (2002). There have also been recent extensions of proba-bilistic graphical model classes, beyond Gaussian and Ising models, to multivariate extensions of exponential family distributions (Yang et al., 2012; 2013).
 More complicated probabilistic models have also been pro-posed in the context of collaborative filtering (Mnih &amp; Salakhutdinov, 2007; Salakhutdinov &amp; Mnih, 2008), but these typically involve non X  X onvex optimization, and it is difficult to extend the rigorous statistical analyses of the form in this paper (and in the matrix completion literature) to these models. As noted in the introduction, we consider the matrix com-pletion setting with general structural constraints on the underlying target matrix  X   X  . To formalize the notion of such structural constraints, we follow (Negahban, 2012), and assume that  X   X  satisfies  X   X   X  X  X  M X  R m  X  n , for some subspace M X  M , which contains parameter matri-ces that are structured similar to the target (the correspond-ing structural constraints such as low rankness, low rank-ness+sparsity etc); we also allow the flexibility of work-ing with a superset M of the model subspace that is po-tentially easier to analyze. Moreover, we use their def-inition of a decomposable norm regularization function, R ( . ) : R m  X  n  X  R + , which suitably captures these struc-tural constraints: A 1. (Decomposable Norm Regularizer) We assume that R ( . ) is a matrix norm, and is decomposable over ( M , M  X  ) , i.e. if X  X  X  , Y  X  M  X  , then, We provide some examples of such decomposable regular-izers and structural constraint subspaces, and refer to (Ne-gahban, 2012) for more examples and discussion.
 Example 1. Low X  X ank : This is a common structure as-sumed in numerous matrix estimation problems, particu-larly those in collaborative filtering, PCA, spectral cluster-ing, etc. The corresponding structural constraint subspaces ( M , M  X  ) in this case correspond to a linear span of spe-cific one X  X ank matrices; we discuss these in further detail in Section 3.2, where we derive a corollary of our general theorem to the specific case of recovery guarantees for low X  rank constrained matrix completion. The nuclear norm R ( X ) = k  X  k  X  = P k  X  k , has been shown to be decom-posable with respect to these constraint subspaces (Fazel et al., 2001).
 Example 2. Block sparsity : Another important structural constraint for a matrix is block X  X parsity, where each row is either all zeros or mostly non X  X ero, and the number of non X  zero rows is small. The structural constraint subspaces in this case correspond to a linear span of specific Frobenius X  norm X  X ne matrices that are non X  X ero in a small subset of the rows (dependent on  X   X  ); it has been shown that ` 1 /` ( q &gt; 1) norms (Negahban &amp; Wainwright, 2008; Obozinski et al., 2011) are decomposable with respect to such struc-tural constraint subspaces. Recalling that  X  ( i ) is the i row of  X  , the ` 1 /` q norm is defined as: Example 3. Low X  X ank plus sparse : This structure is of-ten used to model low X  X ank matrices which are corrupted by a sparse outlier noise matrix. The structural constraint subspaces corresponding to these consist of the linear span of weighted sum of specific rank X  X ne matrices and sparse matrices with non X  X ero components on specified positions; and appropriate regularization function decomposable with respect to such structural constraints is the infimum convo-lution of the weighted nuclear norm with weighted elemen-twise ` 1 norm, k M k 1 , 1 = P ij | M ij | (Candes et al., 2011; Yang &amp; Ravikumar, 2013): The second assumption we make is on the curvature of the log X  X artition function. This is required to establish a form of RSC (Definition 4) for the loss function.
 A 2. The second derivative of the log X  X artition function G : R  X  R has atmost an exponential decay, i.e, It can be verified that commonly used members of natural exponential family obey this assumption.
 Finally, we make an assumption to avoid  X  X piky X  target matrices. As Candes &amp; Recht (2009) show with numer-ous examples, low X  X ank and presumably other such struc-tural constraints as above, by themselves are not sufficient for accurate recovery, in part due to the infeasibility of re-covering overly  X  X piky X  matrices with very few large en-tries. Early work (Candes &amp; Plan, 2010; Keshavan et al., 2010a;b), assumed stringent matrix incoherence conditions to preclude such matrices, while more recent work (Dav-enport et al., 2012; Negahban &amp; Wainwright, 2012), relax these assumptions to restricting the spikiness ratio , defined as follows: A 3. There exists a known  X   X  &gt; 0 , such that 3.1. M  X  X stimator for Generalized Matrix Completion We propose a regularized M  X  X stimate as our candidate pa-rameter matrix b  X  . The norm regularizer R ( . ) used is a con-vex surrogate for the structural constraints, and is assumed to satisfy A 1. For a suitable  X  &gt; 0 , b  X  = argmin = argmin The above optimization problem is a convex program, and can be solved by any off X  X he shelf convex solvers. 3.2. Main Results Without loss of generality, suppose that m  X  n . Let R  X  ( . ) = sup larizer R ( . ) . Further, let  X ( M ) and  X  min be the maxi-mum and minimum subspace compatibility constants of R w.r.t the model subspace M (Definition 3)  X  . We next define the following quantity: where the expectation is over the random sampling index set  X  , and over the Rademacher sequence { ij :  X  ( i,j )  X   X  } ; here { e i  X  R m } , { e j  X  R n } are the standard basis. This quantity  X  R ( n, |  X  | ) captures the interaction between the sampling scheme and the structural constraint as cap-tured by the regularizer (specifically its dual R  X  that it depends only on n ( n  X  m ), and on the size |  X  | of  X  .
 Theorem 1. Let b  X  be the estimate from (6) with  X  2  X  |  X  | =  X  ( X  2 ( M ) n log n )  X  , then given a constant c constants C , C 1 , C 2 , and K 1 , such that, with probability In the above theorem,  X  and  X   X   X   X  sp ( X   X  ) k  X   X  k F are con-stants from Assumptions 2 and 3, respectively. Our proof uses elements from Negahban (2012), as well as Negah-ban &amp; Wainwright (2012) where they analyze the case of low X  X ank structure and additive noise, and establish a form of restricted strong convexity (RSC) for squared loss over subset of matrix entries (closely relates to the special case, when the exponential family distribution assumed in (2) is Gaussian). However, showing such an RSC condition for our general loss function over a subset of structured matrix entries involved some delicate arguments. Further, we pro-vide a much simpler proof that moreover only required a low X  X pikiness condition rather than a multiplicative spik-iness and structural constraint. Moreover, we are able to provide an RSC condition broadly for general structures, and the negative log X  X ikelihood loss associated with gen-eral exponential family distribution. 3.3. Corollary An important special case of the problem is when the pa-rameter matrix  X   X  , is assumed to be of a low X  X ank r m . The most commonly used convex regularizer to enforce low X  X ank is the nuclear norm. The intuition behind the low X  X ank assumption on the parameter matrix is as follows: the parameter  X   X  ij , can be thought of as the true measure of affinity between the entities corresponding to row i and col-umn j , respectively; and the data X ij , is a sample from a noisy channel parametrized by  X  ij . It is hypothesized that {  X   X  ij } , are obtained from a weighted interaction of a small number of latent variables as,  X   X  ij = P r k =1  X  k u ik Let { u k  X  R m } and { v k  X  R n } , k  X  [ r ] be the left and right singular vectors, respectively of  X   X  . Let the column and row span of  X   X  be U  X  , col ( X   X  ) = span { u i } and V  X  , row ( X   X  ) = span { v j } , respectively. Define: It can be verified that, M6 = M , however, M X  M .
 Corollary 1. Let  X   X  be a low X  X ank matrix of rank atmost r m . If further,  X  ( i,j ) , ( X ij  X  g ( X   X  ij )) is sub X  X aussian (Definition 5) with parameter b , and |  X  | &gt;  X  ( rn log n ) , then using R ( . ) = k . k  X  and  X  2 := C Remark 1: Note that the above results hold for the min-imizer b  X  of the convex program in (6), optimized for any  X   X   X   X  sp ( X   X  ) k  X   X  k F ; in particular it holds with  X   X  sp ( X   X  ) k  X   X  k F , where 1  X   X  sp ( X   X  )  X  practice  X   X  is chosen through cross X  X alidation, the theoret-ical bound in Corollary 1 can be tightened to the following (if k  X  k F  X  1 ): Similar bound can be obtained for Theorem 1.
 Remark 2: The parameter b 2 is a measure of noise per entry in the system;  X  ij, Var ( X ij  X  g ( X   X  ij ))  X  b 2 In this section, we provide key steps in the proofs of the main results (Sec. 3.2-3.3). 4.1. Proof of Theorem 1 The proof of our main theorem involves two key steps:  X  We first show that, under assumptions A1-3, RSC of  X  When the RSC condition holds, the result follows Let b  X  = b  X   X   X   X  . We state two results of interest. Lemma 1. We define the following subset: where recall that  X  M is the projection of  X  onto the subspace M . If b  X  is the minimizer of (6) , and  X  The proof follows from Lemma 1 of Negahban (2012). Lemma 2. Let b  X  be the minimizer of (6) . If  X  2  X  The proof is provided in Appendix A. 2 .
 Recall the notation  X  sp ( X ) = sider two cases, depending on whether the following con-dition holds for some constant c 0 &gt; 0 : Case 1: Suppose condition in (9) does not hold; so that  X  the optimization problem (6), we have that k b  X  k max k b Case 2: Suppose condition in (9) does hold. Then, the following theorem shows that an RSC condition of the form in Definition 4 holds.
 Theorem 2 (Restricted Strong Convexity) . If for a given c ,  X  sp ( b  X )  X  1 sumptions and notations in Theorem 1, w.p. &gt; 1  X  C As noted earlier, such an RSC result for the special case of squared loss under low X  X ank constraints was shown in Ne-gahban &amp; Wainwright (2012). However, proving the RSC condition for our general setting required a different and more involved proof technique. We prove this theorem in Section 4.3.
 Remaining steps of the proof of Theorem 1: Thus, if  X  and Lemma 2, w.h.p.: From (10) and (11), under assumptions of Theorem 1, we have that for an appropriate constant C , with probability k b  X  k 2 F  X  C max {  X   X  2 , 1 }  X  2 ( M ) max 4.2. Proof of Corollary 1 From the definition of M  X  in (7), we have M = span { u i x  X  ,y v j  X  : x  X  R n , y  X  R m } . Let P U  X  and P V  X   X  R n  X  n , be the projection matrices onto the col-umn and row spaces ( U  X  , V  X  ) of  X   X  , respectively. We Also, rk ( P U  X  ) = rk ( P V  X  ) = rk ( X   X  ) = r . Thus,  X   X   X  M , rk ( X )  X  2 r ; and hence,  X ( M ) = sup Next, we use the following proposition by Negahban &amp; Wainwright (2012), to bound  X  R ( n, |  X  | ) in Theorem 1. Lemma 3 (Lemma 6 of Negahban &amp; Wainwright (2012)) . If  X   X  [ m ]  X  [ n ] is sampled using uniform sampling and |  X  | &gt; n log n , then for a Rademacher sequence { ij ,  X  ( i,j )  X   X  } , Thus, for large enough c 0 &gt; 640 , using  X  R ( n, |  X  | ) = 10 Finally, to prove the corollary, we derive a bound on kP
 X  ( X  X  g ( X   X  )) k 2 using the Ahlswede X  X inter Ma-trix bound (Appendix A. 3 ). Let  X  ( x ) = e x 2  X  1 ; From the equivalence of sub-Gaussian definitions in Lemma 5 . 5 of Vershynin (2010), k X ij  X  g ( X   X  ij ) k  X  g ( X   X  ij )) , we have, k Y ( ij ) k  X   X  c 0 where ( a ) follows from Fubini X  X  Theorem, ( b ) fol-lows as ( X ij  X  g ( X   X  ij )) is sub X  X aussian, and ( c ) follows from the uniform sampling model. Simi-In Lemma A. 3 , using  X  2 ij  X  nb 2 ,  X  2 := P ij  X   X   X  n |  X  | b 2 , M = c 0 P large enough C 1 , s.t. if |  X  | &gt; C 1 n log n , then, C  X  w.h.p.  X  2 = C 4.3. Proof of Theorem 2 This proof uses symmetrization arguments and contrac-tions (Ledoux &amp; Talagrand (1991) Ch.4&amp;6). We observe that,  X  ( ij )  X   X  ,  X  v ij  X  [0 , 1] , s.t.
 where ( a ) holds as | (1  X  v ij ) X   X  ij + v ij b  X  ij | X k  X  k b Lemma 4. Under Theorem 2, consider the subset, The proof is provided in Appendix A. 1 .
 Further, as b  X   X  V , R ( b  X ) = R ( b  X  M ) + R ( b  X  M 4 R ( b  X  M )  X  4 X ( M ) k b  X  k F . Using Lemma 4, and (15), and choosing |  X  | = c  X  2 ( M ) n log n , for large enough c ,  X 
L := e We provide simulated experiments to corroborate our the-oretical guarantees, focusing specifically on Corollary 1, where we consider the special case where the underlying parameter matrix is low X  X ank, but the underlying noise model for the matrix elements could be any of the gen-eral class of exponential family distributions. We look at three well known members of exponential family suitable for different data X  X ypes, namely Gaussian, Bernoulli, and binomial, which are popular choices for modeling continu-ous valued, binary, and count valued data, respectively. 5.1. Experimental Setup We create low X  X ank ground truth parameter matrices,  X   X   X  R m  X  n of sizes n  X  { 50 , 100 , 150 , 200 } (for simplicity we considered square matrices, m = n ); we set the rank to r = 2 log n . The observation matrices, X , are then sam-pled from the different members of exponential family dis-tributions parameterized by  X   X  . For each n , we uniformly sample a subset  X  entries of the observation matrix X , and estimate b  X  from (6).
 Evaluation: For each member of the exponential family of distribu-tions considered, we can measure the performance of our vation space using an appropriate error metric err ( b X,X ) , where b X is the maximum likelihood estimate of the recov-ered distribution, b X = argmax X P ( X | b  X ) (we use RMSE, MAE in our plots). From our corollary, we require |  X  | = O ( rn log n ) samples for consistent recovery, so we plot the error metric against the the  X  X ormalized X  sample size, rn log n . For reasons of space, we only provide results for the error metric in observations space plotted against the the  X  X ormalized X  sample size. The remainder of the results are provided in Appendix B . It can be seen from the plots that the error decays with increasing sample size, corrob-orating our consistency results; indeed |  X  | &gt; 1 . 5 rn log n samples suffice for the errors to decay to a very small value. Further, the aligning of the curves (for different n ) given the  X  X ormalized X  sample size corroborates the convergence rates as well.
 The research was funded by NSF Grants IIS-0713142 and IIS-1017614. Pradeep Ravikumar acknowledges the sup-port of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1320894, DMS-1264033.
 Banerjee, A., Merugu, S., Dhillon, I. S., and Ghosh, J.
Clustering with bregman divergences. JMLR , 6:1705 X  1749, 2005.
 Candes, E. J. and Plan, Y. Matrix completion with noise. Proceedings of the IEEE , 2010.
 Candes, E. J. and Recht, B. Exact matrix completion via convex optimization. Foundations of Computational mathematics , 2009.
 Candes, E. J. and Tao, T. The power of convex relaxation: near-optimal matrix completion. IEEE Transactions on Information Theory , 2010.
 Candes, E. J., Li, X., Ma, Y., and Wright, J. Robust prin-cipal component analysis? Journal of the ACM (JACM) , 58(3):11, 2011.
 Collins, M., Dasgupta, S., and Schapire, R. E. A general-ization of principal components analysis to the exponen-tial family. In NIPS , pp. 617 X 624, 2001.
 Davenport, M. A., Plan, Y., Berg, E., and Wootters, M. 1-bit matrix completion. arXiv preprint arXiv:1209.3672 , 2012.
 Fazel, M., Hindi, H, and Boyd, S. P. A rank minimiza-tion heuristic with application to minimum order sys-tem approximation. In American Control Conference , pp. 4734 X 4739, 2001.
 Forster, J. and Warmuth, M. Relative expected instanta-neous loss bounds. Journal of Computer and System Sciences , 2002.
 Gordon, G. J. Generalized X  2 linear X  2 models. In NIPS , pp. 577 X 584, 2002.
 Jain, P., Netrapalli, P., and Sanghavi, S. Low-rank matrix completion using alternating minimization. In STOC , 2013.
 Kakade, S. M., Shamir, O., Sridharan, K., and Tewari, A. Learning exponential families in high-dimensions:
Strong convexity and sparsity. In AISTATS , JMLR Work-shop and Conference Proceedings, pp. 381 X 388, 2010. Keshavan, R. H., Montanari, A., and Oh, S. Matrix com-pletion from a few entries. IEEE Transactions on Infor-mation Theory , 2010a.
 Keshavan, R. H., Montanari, A., and Oh, S. Matrix com-pletion from noisy entries. JMLR , 2010b.
 Laurent, M. Matrix completion problems. Encyclopedia of Optimization , 3:221 X 229, 2009.
 Ledoux, M. and Talagrand, M. Probability in Ba-nach Spaces: isoperimetry and processes , volume 23. Springer, 1991.
 Mnih, A. and Salakhutdinov, R. Probabilistic matrix fac-torization. In NIPS , pp. 1257 X 1264, 2007.
 Mohamed, S., Ghahramani, Z., and Heller, K. A. Bayesian exponential family pca. In NIPS , pp. 1089 X 1096, 2008. Negahban, S. Structured Estimation in High-Dimensions . PhD thesis, EECS Department, University of California, Berkeley, May 2012.
 Negahban, S. and Wainwright, M. J. Joint support recovery under high-dimensional scaling: Benefits and perils of l1,-regularization. NIPS , 21:1161 X 1168, 2008.
 Negahban, S. and Wainwright, M. J. Restricted strong con-vexity and weighted matrix completion: Optimal bounds with noise. The Journal of Machine Learning Research , 98888:1665 X 1697, 2012.
 Obozinski, G., Wainwright, M. J., and Jordan, M. I. Sup-port union recovery in high-dimensional multivariate re-gression. The Annals of Statistics , 39(1):1 X 47, 2011. Recht, B. A simpler approach to matrix completion. JMLR , 12:3413 X 3430, 2011.
 Salakhutdinov, R. and Mnih, A. Bayesian probabilistic ma-trix factorization using markov chain monte carlo. In ICML , pp. 880 X 887. ACM, 2008.
 Srebro, N., Rennie, J., and Jaakkola, T. S. Maximum-margin matrix factorization. In NIPS , pp. 1329 X 1336, 2004.
 Tipping, M. E. and Bishop, C. M. Probabilistic principal component analysis. Journal of the Royal Statistical So-ciety: Series B (Statistical Methodology) , pp. 611 X 622, 1999.
 Vershynin, R. Introduction to the non-asymptotic analy-sis of random matrices. arXiv preprint arXiv:1011.3027 , 2010.
 Yang, E. and Ravikumar, P. Dirty statistical models. In NIPS , 2013.
 Yang, E., Allen, G., Liu, Z., and Ravikumar, P. Graphical models via generalized linear models. In NIPS , 2012. Yang, E., Ravikumar, P., Allen, G. I., and Liu, Z. Condi-tional random fields via univariate exponential families.
In Advances in Neural Information Processing Systems ,
