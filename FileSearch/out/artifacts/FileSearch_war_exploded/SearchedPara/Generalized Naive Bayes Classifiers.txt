 This paper presents a generalization of the Naive Bayes Clas-sifier. The method is specifically designed for binary clas-sification problems commonly found in credit scoring and marketing applications. The Generalized Naive Bayes Clas-sifier turns out to be a powerful tool for both exploratory and predictive analysis. It can generate accurate predictions through a flexible, non-parametric fitting procedure, while being able to uncover hidden patterns in the data. In this paper, the Generalized Naive Bayes Classifier and the orig-inal Bayes Classifier will be demonstrated. Also, important ties to logistic regression, the Generalized Additive Model (GAM), and Weight Of Evidence will be discussed. The problem considered is that of predicting the class proba-bility of a binary event (or target) given a set of independent variables (predictors). This problem arises frequently in var-ious fields such as credit scoring, marketing and medical studies. One of the oldest and most simple techniques for binary classification is the Naive Bayes Classifier. Although simple in structure and based on unrealistic assumptions, Naive Bayes Classifiers (NBC) often outperform far more sophisticated techniques. In fact, due to its simple structure the Naive Bayes Classifier is an especially appealing choice when the set of independent variables is large. This is re-lated to the classic bias-variance tradeoff; the NBC tends to produce biased estimated class probabilities, but it is able to save considerable variance around these estimates. Despite these appealing features, the NBC is rarely used to-day in credit scoring or marketing applications. In fact, most popular statistical software packages do not have an NBC module. There are at least two reasons for this: First, the biased class probabilities can be a genuine problem for mod-eling applications where the sole focus is not classification or ranking. Second, the NBC is estimated under the assump-tion that predictors are conditionally independent given the target variable. As a result relationships between the depen-dent variable and the predictors are estimated in isolation without paying attention to covariance between the predic-tors. The NBC is therefore not able to approximate the multivariate regression function, and as a data exploration tool it adds no more information than a univariate analysis. This paper presents a generalization of the Naive Bayes Clas-sifier. The method is called the Generalized Naive Bayes Classifier (GNBC) and extends the NBC by relaxing the assumption of conditional independence between the pre-dictors. This reduces the bias in the estimated class prob-abilities and improves the fit, while greatly enhancing the descriptive value of the model. The generalization is done in a fully non-parametric fashion without putting any restric-tions on the relationship between the independent variables and the dependent variable. This makes the GNBC much more flexible than traditional linear and non-linear regres-sion, and allows it to uncover hidden patterns in the data. Moreover, the GNBC retains the additive structure of the NBC which means that it does not suffer from the prob-lems of dimensionality that are common with other non-parametric techniques.
 Previous research has successfully tackled the severe assump-tions of the Naive Bayes Classifier [4] [3]. However, most of these efforts deal with improving the Naive Bayes Classifier for text classification, i.e. the classification of some docu-ment into one of a set pre-defined categories given a vec-tor of words and characters. The generalization presented in this paper is specifically designed for the type of binary classification problems that arise in credit scoring and mar-keting applications. In this type of setting, the vector of attributes is usually comprised of multiple continuous and discrete variables, and hence most text mining techniques do not apply. In fact, the method presented here is closely related to the Generalized additive model and various non-parametric regression techniques. Let Y be a binary random variable where tors are conditionally independent given Y , the conditional joint probabilities can be written as Combining this with Bayes Theorem leads to the Naive Bayes Classifier log P ( Y = 1 | X 1 , . . . , X p ) where f ( x j | Y ) is the conditional density of X j . The conditional densities can be estimated separately using non-parametric univariate density estimation. Joint den-sity estimation is therefore avoided, which is especially de-sirable when the model contains a large number of predic-tors. More advanced non-parametric techniques of the form form poorly under such conditions. Rapidly increasing vari-ance around the estimates as a result of sparseness of data is a typical problem when estimating the multidimensional function f ( x 1 , . . . , x p ), and CPU time is a problem with large data sets. Furthermore, given that densities can be estimated non-parametrically, the NBC can model the re-lationship between X j and Y in a flexible and unrestricted fashion. In this aspect, the NBC is much more flexible than traditional linear and non-linear regression.
 Since the conditional independence assumption is rarely true, the savings in variance and CPU time is not free. The es-timated class probabilities tend to be biased and therefore the probabilities are typically not used directly but rather as rank-scores. This is a problem for certain modeling applica-tions where ranking or classification is not the only purpose of the analysis. Furthermore, the descriptive value of the model is limited: Consider the ratio, which shows how the log-odds changes with X j . This func-tion is called the Naive Effect since covariance between X and other predictors is not taken into consideration. Obvi-ously, as a data exploration tool, each Naive function adds nothing more than a univariate analysis. The idea behind the Generalized Naive Bayes Classifier is to relax the conditional independence assumption by ad-justing the Naive Effects. This is done by adding p func-accounts for the marginal bias attributed to g j ( x j ). Hence the GNBC can be written as log P ( Y = 1 | X 1 , . . . , X p ) where viously if the predictors are conditionally independent, b 0 for all j , and the model reduces to an NBC.
 In short, the GNBC decomposes the total bias into p terms where the j  X  X h term is a function of X j . Treating the bias like this not only allows us to minimize the bias in the esti-mated probabilities, but also enhances the descriptive value of the model. The expression is actually an additive approximation to the multivariate version effect of X j , fully adjusted for the effect of the other vari-ables. The function b j ( x j ) reflects the marginal bias, i.e. how the effect of X j changes in the presence of the other predictors.
 The decomposition of the bias also allows the GNBC to preserve the additive property of the NBC and hence avoid joint density estimation. Sparseness of data and inflated variance is therefore not an issue which makes the GNBC a practical choice for most binary modeling applications. Fitting a Generalized Naive Bayes Classifier is a two-stage process. In the first stage, the Naive Effects are estimated using univariate density estimation. In the second stage the adjustment functions are estimated by iteratively smoothing the partial residuals against the predictors. The Naive Effects can be estimated separately using uni-variate kernel density estimates. Let N ij ( x ij ) denote the symmetric nearest neighborhood around x ij , which contains the k observations to the left of x ij and the k observations to the right of x ij . The kernel density estimate for X ij the form where h is the width of the neighborhood and I ( x ij , x an indicator function The purpose of the weight function K  X  is to reduce the weight given to observations within N k ( x ij ) that are far away from x ij . This reduces jaggedness in the curves and makes estimates more robust when dealing with large num-bers of ties in the predictor variable. Popular choices for K include the minimum variance kernel and the Epanechnikov kernel The estimated conditional densities can then be computed as  X  f ( x ij | Y = 1) =  X  f ( x ij | Y = 0) = which leads to the estimator of for the Naive Effects If X j is discrete, simple histogram estimators can be used which provides a seamless way to mix discrete and contin-uous variables. In the discrete case, the Naive Effects are popularly known as the weight-of-evidence .
 The smoothness of the estimated densities is controlled by the smoothing parameter. As with any smoother there is a bias-variance tradeoff when selecting  X  . A large  X  will produce a smooth curve with low variance and potentially high bias. Conversely, a small  X  will produce a flexible curve with potentially high variance and little bias. The former curve might be too smooth and miss important patterns in the data, while the latter curve might be too jagged and not generalize well to testing data. Although there are iterative methods to find the optimal smoothing parameter, a good choice can usually be found by combining some simple rules with cross-validation results. If an estimated curve fits the training data well but fits the testing data poorly, increasing  X  will most likely improve the validation results. Likewise, If n or the number of events is small, a large  X  may be necessary to avoid sparseness of data. The marginal biases are estimated through an iterative back-fitting algorithm similar to the procedure used with a lo-gistic Generalized Additive Model [1]. It uses the NBC as a starting point and then, for each predictor in turn, estimates the marginal bias using the most current esti-mates for the other predictors. This step is repeated un-process. This reiterates that the goal of the GNBC is sim-ply to extend the NBC by correcting for the bias caused by the conditional independence assumption.
 Let  X   X  i denote the estimated log odds for observation i , i.e. and  X   X  i denote the estimated target probability The odds ratio at y i around the current estimate  X   X  i can be approximated by the first order Taylor series This leads to the partial residual for variable X j e ij = log which isolates the bias caused by X j by removing the effect of all other variables. Hence an estimate for b j ( x j ) can be obtained by smoothing e j against X j with weights using the most current estimate of  X  i . Because  X  i and w are functions of  X  b j ( x j ), the dependent variable changes when  X  b ( x j ) is updated and the smoothing itself must be done it-eratively. Hence the marginal bias for x j , given the most current estimate of  X  i , is estimated by repeatedly smooth-ing e j against x j , updating the weights at each iteration. fashion, a backfitting algorithm is used in conjunction with the iterative smoothing described above. The backfitting algorithm works by cycling through the predictors and fit-ting smoothing functions to the partial residuals, using the most current estimates for the other smoothing functions. Hence we can fit the GNBC through a nested do-loop: The outer loop controls the cycles of the backfitting algorithm. Each time the backfitting algorithm has cycled through all the predictors, the outer loop evaluates a goodness of fit cri-terion as well as the convergence status of the smoothing functions. If the smoothing functions cease to change or the goodness of fit criterion ceases to improve, the outer loop ter-minates. The inner loop is simply the backfitting algorithm itself, and within the backfitting algorithm is a do-until loop that controls the iterative smoothing described above. This is the same type of maximizer used with GAM, and it is also referred to as the Local Scoring Algorithm [1]. Fur-thermore, it can be shown that this routine is a special case of Iteratively Re-weighted Least Squares (IRLS) where the log-likelihood function L ( b 1 ( x 1 ) , . . . , b p ( x p )) = is being maximized. The difference between this procedure and the typical IRLS is that the weighted linear regression has been replaced by the backfitting algorithm in the outer loop. The procedure has several attractive properties: First, tion is an obvious fitting criterion for the GNBC. Second, the log-likelihood for this problem is concave which means that convergence is almost always guaranteed.
 The algorithm is described in more detail in the following: Let S j ( h ( x j )) denote the weighted smooth of some function h ( x j ) against X j using the weights given above. The choice of smoother is not critical, but for consistency the smoother should be based on the same neighborhoods used in the esti-mation of the Naive Effects. This is because the purpose of this stage is to remove bias caused by the conditional inde-pendence assumption, not bias caused by smoothing. Given a smoother, we can write the algorithm as: Initialize:  X  b 0 j ( x j ) = 0, for all j Outer Loop : k = 1 , 2 , . . .
 Inner Loop : j = 1 , . . . , p, j = 1 , . . . , p, . . . where S j has to be applied iteratively until  X  b k j ( x If X j is a categorical variable, a simple weighted average of the partial residuals for each class is used instead of S outer loop is repeated until the ratio is less than some specified threshold, or the likelihood func-tion ceases to improve. The initial estimate of the intercept is At the end of each iteration, the intercept is adjusted to ensure that the expected number of events equals the actual, i.e. which is identical to the way the intercept is estimated in a logistic regression model. The GNBC is not a practical algorithm for variable selec-tion when the number of variables is large. Although the GNBC is computationally efficient compared to other non-parametric techniques it still requires much more CPU time than LDA or logistic regression. Therefore, another tech-nique is recommended for large variable selection tasks. One technique that has proven to be very effective for such prob-lems is a combination of the standard Naive Bayes Classifier and logistic regression. First, Naive Effect functions are es-timated for all the candidate variables. Second, a logistic regression containing all the Naive Effects is fitted, and a stepwise selection is performed to pick the most significant Naive Effects. Once a manageable set of variables have been determined, the GNBC is used to fit the final model. This technique should be preferred over variable selection from linear effects, as the Naive Effects ensure that variables with strong non-linear effects will be picked up during the selec-tion process. Once the marginal bias functions have been estimated by the GNBC, we are often able to reduce the set of variables even further by identifying variables that are highly affected by multicollinearity. In such cases, the mar-ginal bias functions can neutralize the Naive Effects, which is usually a sign that the variables are abundant. The %GNBC SAS macro allows for estimation of both Naive and Generalized Naive Bayes Classifiers, and has a function to output Naive Effects and Marginal bias functions. Hence it can be used in conjunction with PROC LOGISTIC for large variable selection problems. In most cases the ultimate goal is to build a classification system that can score all prospects or existing customers, in order to evaluate their credit risk or propensity to purchase a certain product. This requires the ability to score records that are not in the training data. Implementing the GNBC as a classification system is easy, although more complicated than implementing a simple LDA model or logistic regres-sion model. For every variable, a look-up table linking X to b j ( x j ) + g j ( x j ) must be created. Furthermore, we ex-pect to observe values that are not present in the training data which means that the system should be able to ex-trapolate from the loop-up tables. If we let x (1) j , . . . , x denote the ordered values of X j in the training data, the system should map values in the range ( x ( i ) j , x ( i +1) j b the training data are mapped to b j ( x (1) j ) + g j ( x b This implementation algorithm can be performed with the %SCOREGNBC SAS macro. The data set used in this example is the same data used to demonstrate GAM in the book  X  X lements of Statistical Learning X  by Hastie and Tibshirani [2]. The data was do-nated by George Forman of Hewlett-Packard laboratories, Palo Alto, California, and is publicly available at the site ftp.ics.uci.edu . It contains various information on 4601 email messages, where 1813 are categorized as spam and 2788 are legitimate emails. The goal of this analysis is to build a filter that prevents spam from entering the inbox. One strategy is to estimate the probability of spam for every inbound email using a binary regression model. If the esti-mated probability exceeds a certain level, the email is clas-sified as spam and sent to a junk mail folder.
 The data set contains 57 continuous ordinal predictors as described below: 1536 records were randomly selected for testing, and 3065 were allocated to the training data. The model was then built in two steps: In the first step, a simple NBC containing all 57 predictors was fitted, followed by a stepwise selection of Naive Effects. The following variables were selected: wordFreqGEORGE wordFreqOUR wordFreqOVER wordFreqREMOVE wordFReqINTERNET wordFReqREPORT wordFReqFREE wordFReqBUSINESS wordFReqCREDIT wordFReqMONEY wordFReq1999 wordFReqEDU wordFReqEXCL wordFReqDOLLAR wordFreqHP wordFreqPROJECT capLengthMAX capLengthAVE charFreqDOLLAR charFreqEXCLAMATION In the second step, the final model was obtained by fit-ting a GNBC containing the selected predictors. A smooth-ing parameter of  X  = 0 . 6 was chosen for all the variables, and the minimum variance kernel was chosen for density estimation. With the convergence criterion set at 0 . 001, the %GNBC macro converged in 6 iterations, which took roughly 1 minute of real computing time. The GNBC re-quires roughly O ( n ) operations, so even with a large dataset computation time is manageable. The model fit statistics are summarized below It appears that the GNBC does remove a substantial amount of bias. Note that the GNBC is primarily a bias reduction technique which explains why the improvement is mostly re-flected in the log-likelihood and MSE, and to a lesser extent in the C-statistic.
 The spam cut-off can be derived using Bayes rule. Let L 1 denote the loss associated with classifying legitimate emails as spam, and L 2 denote the loss associated with classifying spam as legitimate email. With an equal loss assumption, the cut-off is then given by In reality, it is more severe to classify legitimate email as spam and hence L 1 should be greater than L 2 , but for this example we assume equal loss. Using the %SCOREGNBC macro to score the testing data the following confusion ma-trix was generated which shows an overall misclassification rate of 5.3%. The spam data illustrates how much information and pre-dictive strength can be lost when assuming linear effects. The spam data is typical in the sense that the correlation between the dependent variable and the independent vari-ables is not linear. Two patterns that are common in the spam data, and in most other real-life modeling problems, is the V-shaped and the  X  X ockeystick-shaped X  correlation. The hockeystick occurs when the log odds increase or de-crease rapidly as the variable increases from the minimum, but then does not change much after that. Both patterns are hard to model with parametric functions, and poorly cap-tured with a linear effect. Moreover, in order to attempt ap-proximating these patterns with parametric functions, one would have to know that these patterns exist prior to fitting the model. With the GNBC, no knowledge about the cor-relation between X j and Y is required prior to fitting the model, and because of the non-parametric density estima-tion, it can adapt to correlation of any shape. The table below demonstrates how much fit would have been lost if we had used a standard linear technique.
 Examples of hockeystick shaped correlation and V-shaped correlation are given in figures 1 &amp; 2. The graphs show the bias adjustment function, the Naive Effect, as well as capLengthMax has the hockeystick effect, whereas the vari-able wordFreqOur , shown in figure 2, has more of a U-shaped effect. Both graphs indicate that most of the bias adjust-ment is needed when the naive function peaks. This suggests that, had we just fitted an NBC, we would have overesti-mated the target probability in certain ranges. In this paper, we have discussed the Generalized Naive Bayes Classifier. The GNBC is a flexible algorithm for predicting probabilities of a binary target given a set of independent variables. It can fit a large model without any prior knowl-edge about the independent variables, which enables us to uncover hidden patterns in the data and spend more time selecting variables and designing the model. This makes the GNBC a powerful tool for prediction as well as data explo-ration.
 Finally, it is clear that there is a strong link between the logistic GAM and the GNBC. In fact, the GNBC can be viewed as a two-stage method that combines GAM and NBC; the first stage fits and NBC to the data, and the second stage uses GAM to minimize the bias. This turns out to be a powerful combination. The NBC is fast regard-less of the size of the model, and we typically only need a couple of additional GAM iterations to adjust the bias and find the optimum. In addition, the two-stage process allows us to measure the severity of the conditional independence assumption, as well as the predictive strength gained from removing the bias.
 [1] T. Hastie and R. Tibshirani. Generalized Additive Mod-[2] T. Hastie, R. Tibshirani, and J. Friedman. Elements of [3] F. Peng, D. Schuurmans, and S. Wang. Augmenting [4] J. D. M. Rennie, L. Shih, J. Teewan, and D. R. Karger.
