 andrewsmith81@gmail.com, huo@gatech.edu Manifold learning (ML) methods have attracted substantial attention due to their demonstrated po-formance of these methods. The main contribution of this paper is to establish some asymptotic properties of a local manifold learning algorithm: LTSA [13], as well as a demonstration of some of subspaces of certain matrices, and then carry out a matrix perturbation analysis. Many efficient ML algorithms have been developed including locally linear embedding (LLE) [6], ISOMAP [9], charting [2], local tangent space alignment (LTSA) [13], Laplacian eigenmaps [1], and Hessian eigenmaps [3]. A common feature of many of these manifold learning algorithms is smallest eigenvalues of a kernel or alignment matrix. The exact form of this matrix, of course, depends on the details of the particular algorithm.
 Second, the solution to each step of the LTSA algorithm is an invariant subspace, which makes analysis of its performance more tractable. Third, the similarity between LTSA and several other ML algorithms (e.g., LLE, Laplacian eigenmaps and Hessian eigenmaps) suggests that our results the application of ML algorithms.
 The rest of the paper is organized as follows. The problem formulation and background information are presented in Section 2. Perturbation analysis is carried out, and the main theorem is proved concluding remarks in Section 6. (possibly multivariate) values x i  X  IR d ,d &lt; D,i = 1 , 2 ,...,n , such that matrix. The central questions of manifold learning are: 1) Can we find a set of low-dimensional vectors such that equation (1) holds? 2) What kind of regularity conditions should be imposed on f ? 3) Is the model well defined? These questions are the main focus of this paper. 2.1 A Pedagogical Example Figure 1: An illustrative example of LTSA in nonparametric dimension reduction. The straight line pattern in (c) indicates that the underlying parametrization has been approximately recovered. An illustrative example of dimension reduction that makes our formulation more concrete is given the noisy observations, using k = 10 nearest neighbors. In subfigure (c), the result from LTSA is compared with the true parametrization. When the underlying parameter is faithfully recovered, one should see a straight line, which is observed to hold approximately in subfigure (c). 2.2 Regularity and Uniqueness of the Mapping f If the conditions on the mapping f are too general, the model in equation (1) is not well defined. where A is an invertible d by d matrix and b is a d -dimensional vector. As being common in the manifold-learning literature, we adopt the following condition on f .
 Condition 2.1 (Local Isometry) The mapping f is locally isometric: For any  X  &gt; 0 and x in the distance. We have following lemma [13].
 Given the previous condition, model (1) is still not uniquely defined. For example, for any d by d discussed.
 Condition 2.3 (Local Linear Independence Condition) Let Y i  X  IR D  X  k , 1  X  i  X  n, denote a matrix whose columns are made by the i th observation y i and its k  X  1 nearest neighbors. We matrix Y i P k is greater than 0 .
 and 1 k is a k -dimensional column vector of ones. The regularity of the manifold can be determined Furthermore, let x = ( x 1 ,...,x d ) T . The Hessian is a D by D matrix, The following condition ensures that f is locally smooth. We impose a bound on all the components of the Hessians.
 is a prescribed constant. 2.3 Solutions as Invariant Subspaces and a Related Metric We now give a more detailed discussion of invariant subspaces. Let R ( X ) denote the subspace square matrix, forms another solution to the dimension reduction problem, we have It is evident that R ( XO T ) = R ( X ) . This justifies the invariance that was mentioned earlier. denote the Euclidean norm of the vector of canonical angles between two invariant subspaces ([8, Section I.5]), and letting X and e X denote the true and estimated parameters, respectively, how do we evaluate k tan( R ( X ) , R ( e X )) k 2 ? 2.4 LTSA: Local Tangent Space Alignment We now review LTSA. There are two main steps in the LTSA algorithm [13]. P Y 2. The solution to LTSA corresponds to the invariant subspace which is spanned and determined by the eigenvectors associated with the 2 nd to the ( d + 1 )st smallest eigenvalues of the matrix for later analysis. We now carry out a perturbation analysis on the reformulated version of LTSA. There are two steps: P space under global alignment. 3.1 Local Coordinates We require a bound on the size of the local neighborhoods defined by the X i  X  X . Condition 3.1 (Universal Bound on the Sizes of Neighborhoods) For all i, 1  X  i  X  n , we have  X  i &lt;  X  , where  X  is a prescribed constant and  X  i is an upper bound on the distance between two In this paper, we are interested in the case when  X   X  0 .
 minimum (respectively, maximum) singular values of X i P k . Let We can bound d max as d min  X  d max  X   X  Condition 3.2 (Local Tangent Space) There exists a constant C 2 &gt; 0 , such that The above can roughly be thought of as requiring that the local dimension of the manifold remain constant (i.e., the manifold has no singularities.) The following condition defines a global bound on the errors (  X  i ).
 Condition 3.3 (Universal Error Bound) There exists  X  &gt; 0 , such that  X  i, 1  X  i  X  n , we have k y i  X  f ( x i ) k  X  &lt;  X  . Moreover, we assume  X  = o (  X  ) ; i.e., we have  X   X   X  0 , as  X   X  0 . which is reflected in the above condition.
 Within each neighborhood, we give a perturbation bound between an invariant subspace spanned by hard to verify that matrix B T i (respectively, e B T i ). Theorem 3.4 Given invariant subspaces R ( B T i ) and R ( e B T i )) as defined above, we have where C 3 is a constant that depends on k , D and C 2 .
 subspace in step 1 of the modified LTSA. It will be used later to prove a global upper bound. 3.2 Global Alignment Condition 3.5 (No Overuse of One Observation) There exists a constant C 4 , such that Note that we must have C 4  X  k . The next condition (Condition 3.6) will implicitly give an upper bound on C 4 .
 to which a single observation belongs.
 We will derive an upper bound on the angle between the invariant subspace spanned by the result of LTSA and the space spanned by the true parameters.
 IR space of the matrix: problem formulation, we made no assumption about the x i  X  X , we can still assume that the columns the columns and multiplying by an orthogonal matrix. Based on the previous paragraph, we have where and Let ` min denote the minimum singular value (i.e., eigenvalue) of L 2 . We will need the following condition on ` min .
 Condition 3.6 (Appropriateness of Global Dimension) ` min &gt; 0 and ` min goes to 0 at a slower rate than  X   X  + 1 2 C 1  X  ; i.e., as  X   X  0 , we have As discussed in [12, 11], this condition is actually related to the amount of overlap between the nearest neighbor sets. Theorem 3.7 (Main Theorem) As mentioned in the Introduction, the above theorem gives a worst-case bound on the performance of LTSA. For proofs as well as a discussion of the requirement that  X   X  0 see [7]. A discussion on investigation. We refer to [5] for some simulation results related to the above analysis. aforementioned framework. We modify the LTSA (mainly on how to choose the size of the nearest neighborhood) for a reason that will become evident later.
 proved for x i being sampled on a uniform grid, using the properties of biharmonic eigenvalues for partial differential equations) holds: context in the future.
 So far, we have assumed that k is constant. However, allowing k to be a function of the sample size convergence of the estimated alignment matrix to the true alignment matrix.
 be careful in disregarding constants, since they may involve k . We have that C 3 = C 2 are fundamental constants not involving k . Further, it is easy to see that k P since each point has k neighbors, the maximum number of neighborhoods to which a point belongs is of the same order as k .
 Now, we can use a simple heuristic to estimate the size of  X  , the neighborhood size. For example, suppose we fix and consider -neighborhoods. For simplicity, assume that the parameter space is and dropping the constants, we get follow a more general distribution rather than only lying in a uniform grid), then we have convergence is as fast as possible. Simplifying the exponents, we get As a function of  X  restricted to the interval [0 , 1) , there is no minimum X  X he exponent decreases with  X  , and we should choose  X  close to 1 . However, in the proof of the convergence of LTSA, it is assumed that the errors in the local step converge to 0 . This error is given by Disregarding constants and writing this as a function of n , we get This quantity converges to 0 as n  X  X  X  if and only if we have restricted further.
 The following table gives the optimal exponents for selected values of d along with the convergence fixed value of k , there seems to be a threshold value of n , above which the performance degrades. This value increases with k , though perhaps at the cost of worse performance for small n . How-ever, we expect from the above analysis that, regardless of the value chosen, the performance will eventually become unacceptable for any fixed k . To the best of our knowledge, the performance analysis that is based on invariant subspaces is new. addressed (Section 5.1). In addition to a discussion on the relation of LTSA to existing dimension reduction methodologies, we will also address relation with known results as well (Section 5.2). 5.1 Open Questions correctness of (8) at all. It turns out the proof of (8) is quite nontrivial and tedious. the rate of convergence of  X   X  0 as a function of the topology of f and the sampling scheme. After doing so, we may be able to decide where our theorem is applicable. 5.2 Relation to Existing Work Zha [13] do not interpret their solutions as invariant subspaces, and hence their analysis does not yield a worst case bound as we have derived here. Reviewing the original papers on LLE [6], Laplacian eigenmaps [1], and Hessian eigenmaps [3] proofs of consistency than those presented in these papers.
 ISOMAP, another popular manifold learning algorithm, is an exception. Its solution cannot im-mediately be rendered as an invariant subspace. However, ISOMAP calls for MDS, which can be associated with an invariant subspace; one may derive an analytical result through this route. We derive an upper bound of the distance between two invariant subspaces that are associated with the numerical output of LTSA and an assumed intrinsic parametrization. Such a bound describes the performance of LTSA with errors in the observations, and thus creates a theoretical foundation Our results can also be used to show other desirable properties, including consistency and rate of convergence. Similar bounds may be derivable for other manifold-based learning algorithms. [1] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data repre-[2] M. Brand. Charting a manifold. In Neural Information Processing Systems , volume 15. Mit-[3] D. L. Donoho and C. E. Grimes. Hessian eigenmaps: New locally linear embedding tech-[4] X. Huo, X. S. Ni, and A. K. Smith. Mining of Enterprise Data , chapter A survey of manifold-[5] X. Huo and A. K. Smith. Performance analysis of a manifold learning algorithm in dimension [6] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. [7] A. K. Smith. New results in dimension reduction and model selection. Ph.D. Thesis. Available [8] G. W. Stewart and J.-G. Sun. Matrix Perturbation Theory . Academic Press, Boston, MA, [9] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear [10] T. Wittman. MANIfold learning Matlab demo. URL: [11] H. Zha and H. Zhang. Spectral properties of the alignment matrices in manifold learning. [12] H. Zha and Z. Zhang. Spectral analysis of alignment in manifold learning. In Proceedings of [13] Z. Zhang and H. Zha. Principal manifolds and nonlinear dimension reduction via local tangent
