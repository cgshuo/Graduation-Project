 Monash University Monash University
This article presents an investigation of corpus-based methods for the automation of help-desk e-mail responses. Specifically, we investigate this problem along two operational dimensions: (1 )information-gathering technique, and (2 )granularity of the information. We consider two information-gathering techniques (retrieval and prediction )applied to information represented at two levels of granularity (document-level and sentence-level). Document-level methods corre-correspond to applying extractive multi-document summarization techniques to collate units of information from more than one e-mail. Evaluation of the performance of the different methods shows that in combination they are able to successfully automate the generation of responses response automation solution. 1. Introduction
E-mail inquiries sent to help desks often  X  X evolve around a small set of common ques-tions and issues. X  1 This means that help-desk operators spend most of their time dealing with problems that have been previously addressed. Further, a significant proportion of help-desk responses contain a low level of technical content, addressing, for example, inquiries sent to the wrong group, or requests containing insufficient detail about the customer X  X  problem. Organizations and clients would benefit if an automated process was employed to deal with the easier problems, and the efforts of human operators were focused on difficult, atypical problems.
 task. Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. It is therefore no surprise that early attempts at response automation were knowledge-driven (Barr and
Tessler 1995; Watson 1997; Delic and Lahaix 1998). These systems were carefully de-signed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).
 been largely superseded by data-intensive, statistical approaches. An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this, to date, there has been little work on corpus-based approaches to help-desk response automation (notable exceptions are Carmel, Shtalhaim, and Soffer 2000; Lapalme and
Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007). A major factor limiting this work is the dearth of corpora X  X elp-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. automation of help-desk responses. Our study is based on a large corpus of request X  response e-mail dialogues between customers and operators at Hewlett-Packard. Ob-servations from this corpus have led us to consider several methods that implement different types of corpus-based strategies. Specifically, we have investigated two types of methods (retrieval and prediction) applied at two levels of granularity (document and sentence). In this article, we present these methods and compare their performance.
A key issue in the generation of help-desk responses is the ability to determine when an automatically generated response for a particular query can be sent to a user, and when the query should be passed to an operator. In Section 3, we employ method-specific, empirically determined, applicability thresholds to make this decision; in Section 6, we propose a meta-level process for selecting a response-generation method, which obviates the need for these thresholds.
 properties of the help-desk domain. In Section 3, we describe our response-generation methods. An automatic evaluation of these methods is presented in Section 4, and a small user-based evaluation in Section 5. In Section 6, we describe the meta-level process which learns to select between the different methods, and evaluate its performance in
Section 7. Related work is discussed in Section 8, and concluding remarks are presented in Section 9. 2. The Help-Desk Domain
The help-desk domain offers interesting challenges to response automation in that, on one hand, responses are generalized to fit standard solutions, and on the other hand, responses are tailored to the initiating request in order to meet specific customer needs. 598
For example, the first sentence of the response in Figure 1(a) is tailored to the user X  X  request, whereas the rest of the response is generic, and may be used when replying to other queries. 3 In addition to responses that contain such a mixture of specific and generic information, there are inquiries that warrant very specific or completely generic responses, as seen in Figures 1(b) and 1(c), respectively.
 contain a high level of repetition and redundancy. This may be attributed to commonal-ities in customer issues combined with the provision of in-house manuals to help-desk operators. These manuals connect particular topics with standard response templates, prescribe a particular presentation style, and even suggest specific responses to certain queries. For example, Figure 2 shows two rather different response e-mails which share a sentence (italicized). Thus, having access to these manuals would enable us to easily identify prescribed sentences. More importantly, it would enable us to determine the context in which these sentences are used, which in turn would allow us to postulate additional response sentences. An interesting avenue of investigation would involve adapting our approach to help-desk situations where such manuals are accessible. raised by different customers imply that the responses sent to these customers contain varying degrees of overlap (rather than being identical). Hence, providing a response for a new request may involve reusing an existing response in its entirety, putting together parts of responses that match individual components of the request, or com-posing a completely new response. This suggests that different response-generation strategies may be suitable, depending on the content of the initiating request and how well it matches previous requests or responses. In our work, we focus on the first two of these situations, where either complete existing responses or parts of responses are reused to address a new request.
 ( docking station and install ) are also mentioned in the response. This situation suggests a response-automation approach that follows the document retrieval paradigm (Salton and McGill 1983), where a new request is matched with existing response documents (e-mails). However, specific words in the request do not always match a response well, and sometimes do not match a response at all, as demonstrated by the examples in Figures 1(a) and 1(c), respectively.
 new request is matched with an old one, and the corresponding response is reused.
However, analysis of our corpus shows that this does not occur very often, because unlike response e-mails, request e-mails exhibit a high language variability: There are many customers who write these e-mails, and they differ in their background, level of expertise, and pattern of language usage. Further, there are many requests that raise multiple issues, hence matching a new request e-mail in its entirety is often not possible. it may be possible instead to find correlations between requests and responses. For example, the generic portion of the response in Figure 1(a), and the entire response in Figure 1(c), may be repeated for many different kinds of requests. If repeated suffi-ciently, entire responses or parts of responses will be strongly correlated with particular combinations of request words, such as send , battery and replace in Figure 1(a), and firewall , ip ,and network in Figure 1(c). 3. Response-Generation Methods
The properties of the help-desk domain outlined in the previous section have motivated us to study the response-automation task along two dimensions. The first dimension pertains to the strategy applied to determine the information in a response, and the 600 second dimension pertains to the granularity of the information. We implemented two alternative strategies. The first is a retrieval strategy that attempts to match a new request with previous requests and responses, and the second is a prediction strategy that looks for correlations between requests and responses in order to predict a response for a new request. For both types of strategies, we considered two levels of granularity for a unit of information: document and sentence.
 sion: Document Retrieval , Document Prediction , Sentence Retrieval ,and Sentence
Prediction . We also implemented a fifth method for addressing situations such as the example shown in Figure 1(a), which warrant a mixed response that has both a generic component and a component tailored to the user X  X  request. This is a hybrid prediction X  retrieval method implemented at the sentence level: Sentence Prediction X  X etrieval Hy-brid . These five methods are summarized in Table 2 (Section 3.3). The implementation of these methods relies on the judicious selection of thresholds for different aspects of the processes in question. In principle, machine learning techniques could be used to determine optimal threshold values. However, owing to practical considerations, we selected these values by trial and error. Table 3 shows the range of values we tried for these thresholds, and the values we selected (Section 3.3). 3.1 Document-Level Methods
A document-level method attempts to reuse an existing response document (e-mail) in its entirety. Unlike sentence-based methods that attempt to put together portions of different responses (Section 3.2), a document-level approach avoids issues pertaining to the coherence and completeness of a response, as a response composed by a help-desk operator is likely to be both coherent and complete. Therefore, if a particular request can be addressed with a single existing response document, then a document reuse approach would be preferred. An important capability of a response-generation system is to be able to determine when such an approach is appropriate, and when there is insufficient evidence to reuse a complete response document.
 and Document Prediction . 3.1.1 Document Retrieval ( Doc-Ret ). This method follows a traditional Information Re-trieval paradigm (Salton and McGill 1983), where a query is represented by the content terms it contains, and the system retrieves from the corpus a set of documents that best match this query. In our case, the query is a new request e-mail to be addressed by the system, and we have considered three views of the documents in the corpus: (1) previous response e-mails, (2) previous request e-mails, or (3) previous request X  response pairs. The first alternative corresponds to the more traditional view of retrieval as applied in question-answering tasks, where the terms in the question are matched to those in the answer documents. We consider the second alternative in order to address situations such as the example in Figure 1(c), where a request might not match a particular response, but it may match another request, yielding the response to that request. The third alternative addresses situations where a new request matches part of another request and part of its response.
 corpus) to determine a retrieval score, and pick the document with the highest score.
The similarity is calculated using a bag-of-lemmas representation with TF . IDF (term-frequency-inverse-document frequency) weightings (Salton and McGill 1983), but in the request-to-response option we use TF = 1, as it yields the best results. We posit that this happens because a response to a request does not necessarily contain multiple instances of request terms. Hence, what is important when matching a request to a response is the number of (significant) terms in common, rather than their frequency. In contrast, when matching a request to a request, or a request to a request X  X esponse pair, term frequency would be more indicative of the goodness of the match, as the document also has a request component.
 an applicability threshold, which is currently set empirically (Table 3). If retrieval is successful, then the response associated with the retrieved document is reused to reply to the user X  X  request.
 of the Doc-Ret method. The evaluation is performed by considering each request e-mail in turn, removing it and its response from the corpus, carrying out the retrieval process, and then comparing the retrieved response with the actual response (if there are several similar responses in the corpus, an appropriate response can still be retrieved). The results of this experiment are shown in Table 1. The first column shows which document retrieval variant is being evaluated. The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold). We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request X  X esponse pairs yields even more retrieved documents. For the cases where retrieval took place, we used F-score (van Rijsbergen 1979; Salton and McGill 1983) to determine the similarity between the response from the top-ranked document and the real response (the formulas for F-score and its contributing factors, recall and precision, appear in
Section 4.2). The third column in Table 1shows the proportion of requests for which this similarity is non-zero. Again, the third variant (matching on request X  X esponse pairs) retrieves the highest proportion of responses that bear some similarity to the real responses. The fourth column shows the average similarity between the top retrieved response and the real response for the cases where retrieval took place. Here too the third variant yields the best similarity score (0.52).
 variant is superior. Hence, we use this variant as the Doc-Ret method in subsequent experiments. 3.1.2 Document Prediction ( Doc-Pred ). The Doc-Ret method may fail in situations where the presence or absence of some terms in the requests triggers a generic template response. For instance, in the example in Figure 1(c), given the terms firewall and CP-2W , 602 we would like to retrieve the generated response. However, the first Doc-Ret alternative would fail, as the response has no common terms with the request. The other two Doc-
Ret alternatives would fail if different requests in the corpus mention different issues about these two terms, and thus no single request or request X  X esponse document in the corpus would yield a good match with a new request that mentions these terms. correlations between terms, rather than matches. In principle we could look for direct cor-relations between request terms and response terms. However, since we have observed strong regularities in the responses at the document level, we decided to reduce the dimensionality of the problem by abstracting the response documents and then looking for correlations at this higher level. This approach did not seem profitable for request e-mails, which unlike responses, have a high language variability. Hence, we keep their representation at a low level of abstraction (bag-of-lemmas).

Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request X  X  features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case, the clustering is performed by the program Snob, which implements mixture model-ing combined with model selection based on the Minimum Message Length (MML) criterion (Wallace and Boulton 1968; Wallace 2005). We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method,
Section 3.2.2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lem-matized) corpus word in the document in question (after removing stop-words and words with very low frequency). 4 The predictive model is a Decision Graph (Oliver 1993), which, like Snob, is based on the MML principle. The Decision Graph is trained on unigram and bigram lemmas in the request as input features, of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given re-quest, and returns the probability that this prediction is correct. This probability is our indicator of whether the Doc-Pred method can address a new request. As for the
Doc-Ret method, an applicability threshold for this parameter is currently determined empirically (Table 3). 3. 2Sentence-Level Methods
The document-level methods presented in the previous section are designed to address situations where requests are sufficiently specific to strongly match a previous request or response e-mail (Doc-Ret), or requests contain terms that are predictive of complete template response e-mails (Doc-Pred). document-level approach, because requests only predict or match portions of responses.
An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response. This task can be cast as extractive multi-document summarization. Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable. In our work, we gather sets of sentences, and assume (but do not employ) existing approaches for their organization (Goldstein et al. 2000; Barzilay, Elhadad, and McKeown 2001; Barzilay and McKeown 2005).
 bination response X  in situations where there is insufficient evidence for a single docu-ment containing a full response, but there is enough evidence for parts of responses.
Although such a combined response is generally less satisfactory than a full response, the information included in it may address a user X  X  problem or point the user in the right direction. As argued in the Introduction, when it comes to obtaining information quickly on-line, this option may be preferable to having to wait for a human-generated response. In contrast, the document-level approach is an all-or-nothing approach: If there is insufficient evidence for a complete response, then no automated response is generated. 3.2.1 Sentence Retrieval ( Sent-Ret ). As we saw in Section 2 (Figure 1(b)), there are situa-tions where the terms in response sentences match well the terms in request sentences.
To address these situations, we consider the Sent-Ret method, which employs retrieval techniques as for the Doc-Ret method, but at the sentence level.
 look for response sentences that match individual request sentences, rather than entire documents; and (2) in Sent-Ret we perform recall-based retrieval, rather than retrieval based on cosine-similarity, where the request sentence is the reference document for the recalled terms. The second difference is a result of the first, as a good candidate for sentence retrieval contains terms that appear in a request sentence, but is also likely to contain additional terms that expand on the matching terms (Figure 1(b)). Thus, recall is calculated for each response sentence with respect to each request sentence as follows (since TF = 1yielded the best result for the request X  X esponse option in Doc-Ret, we also use it for Sent-Ret).
We retain the response sentences whose recall exceeds an empirically determined ap-plicability threshold (Table 3), and produce a response if at least one sentence was retained. 3.2.2 Sentence Prediction ( Sent-Pred ). As for the Doc-Pred method, the Sent-Pred method starts by abstracting the responses. It clusters response sentences using the same 604 clustering program (Snob) and bag-of-lemmas representation as Doc-Pred.
Doc-Pred method, where only a single response cluster is predicted (resulting in a single response document being selected), the Sent-Pred method may predict several promis-ing Sentence Clusters ( SCs ). A response is then composed by extracting sentences from the predicted SCs.
 three small SCs (in practice SCs can have tens and even hundreds of sentences). The thick arrows correspond to high-confidence predictions, while the thin arrows corre-spond to sentence selections. The other components of the diagram demonstrate the workings of the Sent-Hybrid method (Section 3.2.3). In this example, three of the request terms, repair , faulty and monitor , result in a confident prediction of two SCs: SC
The sentences in SC 1 are identical, so we can arbitrarily select a sentence for inclusion in the generated response. The sentences in SC 2 are similar but not identical, hence we are less confident in arbitrarily selecting a sentence from SC one sentence (see the subsequent discussion on removing redundant sentences). predict SCs from users X  requests. 7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request. During the prediction stage, the SVMs predict zero or more SCs for each request, as shown in
Figure 3. We then apply the following steps. 1. Calculate the scores of the sentences in the predicted SCs. 2. Remove redundant sentences from cohesive SCs; these are SCs which 3. Calculate the confidence of the generated response.

Calculating the score of a sentence. The score of each sentence s following formula.
 where m is the number of SCs, Pr( s j | SC i ) is the probability that s (obtained from Snob), and Pr( SC i ) is approximated as follows. Pr( SC i ) = where 606 (precision), let us return to the request in Figure 3. SC ability for this request, because SC 1 includes sentences from responses to many other requests that contain the words repair and faulty . The word monitor also contributes to this high prediction probability, but not as strongly as repair and faulty .Thisis because SC 1 includes sentences from responses whose requests mention different faulty products, for example, monitor , printer ,or notebook . However, if there are more cases of faulty monitors in the corpus than other faulty products, then requests about repairing monitors will have a higher prediction probability than requests about repairing other products. In contrast to prediction probability, SVM reliability reflects its overall per-formance (on the training data), and is independent of particular requests. Thus, the
SVM for SC 1 has a higher reliability than that for SC 3 , because it is easier for an SVM to learn when SC 1 is appropriate (predominantly from the presence of the words faulty and repair ).
 restrictions on prediction probability and cluster cohesion (Table 3), which cause the Sent-Pred method to often return partial responses.

Removing redundant sentences. After calculating the raw score of each sentence, we use a modified version of the Adaptive Greedy Algorithm by Filatova and
Hatzivassiloglou (2004) to penalize redundant sentences in cohesive clusters. This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we re-tain one sentence as a reference sentence X  X .e., its score is not decremented). Specifically, given a sentence s k in cluster SC l which contains a sentence with a higher or equal score, the contribution of SC l to Score( s k )( = Pr( SC l )  X  Pr( s
After applying these penalties, we retain only the sentences whose adjusted score is greater than zero (for a highly cohesive cluster, typically only one sentence remains).
Calculating the confidence of an automated response. The calculation of sentence scores described previously determines which sentences should be included in an auto-matically generated response. In order to decide whether this response should be used, we need an overall measure of the confidence in it. Our confidence measure aggregates into a single number the values of the attributes used to assign a score to the individual sentences in a response, as follows.
 where
This measure combines our confidence in the SCs selected to generate response sen-tences with the completeness of the resultant response. Confidence is represented by the thresholds employed to select suitable SCs; and completeness is represented by the ratio of the number of SCs that were deemed suitable, and the number of SCs that could possibly be used to generate a response. These are SCs whose prediction probability is greater than 0 (i.e., there is some evidence in the corpus for their use in the generation of a response sentence for the current request). We also use a minimal applicability threshold of 0 for the confidence measure (Table 3). This threshold reflects our notion that a partial response, even a response with one sentence, may still be useful. 3.2.3 Sentence Prediction X  X etrieval Hybrid ( Sent-Hybrid ). As seen in cluster SC ure 3, it is possible for an SC to be strongly predicted without it being sufficiently cohesive for a confident selection of a representative sentence. However, sometimes the ambiguity can be resolved through cues in the request. In this example, one of the sentences in SC 2 matches the request terms better than the other sentences, as it contains the word monitor . In order to capture such situations, we combine prediction confidence with retrieval score to guide sentence selection (as for Sent-Pred, we use a recall measure with TF = 1; the values for the thresholds mentioned herein appear in Table 3). 608 recall thresholds, we remove redundant sentences as follows. Redundant sentences are removed from cohesive clusters as described in Section 3.2.2; for SCs with medium cohesion, we retain the sentence with the highest recall; retain all the sentences. The rationale for this policy is that the sentences in an SC with medium cohesion are sufficiently similar to each other, so the selection of more than one sentence may introduce redundancy. In contrast, sentences that belong to uncohesive
SCs are deemed sufficiently dissimilar to each other, so we can select all the sentences that satisfy the recall criterion.
 at least one response sentence. This happens when (a) the confidence in the highly cohesive SCs exceeds an applicability threshold; or (b) the confidence in one of the SCs with medium or low cohesion exceeds an applicability threshold, and the number of sentences retrieved for this SC exceeds an applicability threshold. As for Sent-Pred, confidence is calculated using Equation (6). Both applicability thresholds (confidence and number of retrieved sentences) are set to 0 (Table 3). 3.3 Summary
The focus of our work is on the general applicability of the different response automa-tion methods, rather than on comparing the performance of particular implementa-tion techniques. Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research. Specifically, we used Decision Graphs (Oliver 1993) for Doc-Pred, and
SVMs (Vapnik 1998) for Sent-Pred. 11 Additionally, we used unigrams for clustering documents and sentences, and unigrams and bigrams for predicting document clusters and sentence clusters (Sections 3.1.2 and 3.2.2). Because this variation was uniformly implemented for both approaches, it does not affect their relative performance. These methodological variations are summarized in Table 2.
 ods requires the selection of different thresholds, which are subjective and application dependent. Table 3 summarizes the thresholds required for the different methods, the range of values we considered, and the values we selected. The applicability thresholds are boldfaced, and those learned by the meta learning process (Section 6) are indicated in the rightmost column (Sent-Ret is not considered by this process owing to its poor performance; see Section 4).
 610 4. Automatic Evaluation of Individual Methods
In this section, we offer a comparative evaluation of the response automation methods presented in Section 3, where we measure the ability of the different methods to address the requests in the corpus. We first describe the data used in our experiments, followed by the experimental set-up and results. 4.1 The Corpus
Our initial corpus consisted of 30,000 e-mail dialogues between customers and help-desk operators at Hewlett-Packard. The dialogues deal with a variety of user requests, which include requests for technical assistance, inquiries about products, and queries about how to return faulty products or parts. To focus our work on simple dialogues, we extracted a sub-corpus that satisfies two conditions: 1. The dialogues contain exactly two turns: a request e-mail followed by a 2. The response e-mails are reasonably concise (15 lines at most). This of topics. We were hoping to account for significant differences between groups of dialogues on the basis of their topic. In addition, there was a practical motivation to break up this large sub-corpus into smaller chunks for ease of handling. We therefore applied Snob to automatically cluster the sub-corpus into separate topic-based data sets.
The clustering, which was done using as input the lemmas in the subject line of the users X  e-mails, produced 5 1data sets, some of which were quite small ( 1 1data sets had less than 25 dialogues). Because Snob returns the significant terms in each cluster, we merged the smaller data sets manually according to these terms X  X  process that yielded 15 data sets in total, each data set containing between 135 and 1,236 e-mail dialogues.
Owing to the time limitations of the project, the procedures described in this article were applied to 8 of the 15 data sets, which contain a total of 4,904 dialogues (73.6% of the sub-corpus, and 16.3% of the original corpus). These data sets, which were chosen on the basis of their coverage of the corpus, are described in Table 4, qualitative overview of our results, which are discussed in Section 4.3.
 dialogues, and cause differences in the performance of response generation methods for different types of dialogues. However, these factors are not readily apparent upon initial analysis. Topic-based clustering, which is readily apparent, is a reasonable starting point for distinguishing between different data sets. The analysis presented in Section 4.3 considers other features that characterize the data sets and the behavior of the response generation methods. 4. 2Experimental Set-Up
Our experimental set-up is designed to evaluate the ability of the different response-generation methods to address unseen request e-mails. In particular, we want to deter-mine the applicability of our methods to different situations, namely, whether different requests are addressed only by some methods, or whether there is a significant overlap between the methods.

Quality is a subjective measure, which is best judged by the users of the system (i.e., the help-desk customers or operators). In Section 5, we discuss the difficulties asso-ciated with such user studies, and describe a human-based evaluation we conducted for a small subset of the responses generated by our system (Marom and Zukerman 2007b). However, our more comprehensive evaluation is an automatic one that treats the responses generated by the help-desk operators as model responses, and performs text-based comparisons between the model responses and the automatically generated ones.
 into 10 test sets, each comprising 10% of the e-mail dialogues; the remaining 90% of the dialogues constitute the training set. For each of the cross-validation folds, the responses generated for the requests in the test split are compared against the actual responses generated by help-desk operators for these requests. Although this method of assessment is less informative than human-based evaluations, it enables us to evaluate the performance of our system with substantial amounts of data, and produce represen-tative results for a large corpus such as ours.
 automatically generated response: precision and F-score (van Rijsbergen 1979; Salton and McGill 1983). Precision measures how much of the information in an automati-cally generated response is correct (i.e., appears in the model response), and F-score measures the overall similarity between the automatically generated response and the model response. F-score is the harmonic mean of precision and recall , which measures how much of the information in the model response appears in the gener-ated response. We consider precision separately because it does not penalize missing 612 information, enabling us to better assess our sentence-based methods. Precision, recall, and F-score are calculated as follows using a word-by-word comparison (stop-words are excluded). 13
These measures are applied to responses generated after the thresholds in Table 3 have been used to determine the applicability or coverage of each response-generation method. Recall that the sentence-based methods can generate partial responses, many of which contain only one obvious and non-informative sentence, such as Thank you for contacting HP and Thank You, Mike, HP eServices . We have manually excluded such responses from the calculation of coverage, in order to prevent these responses from artificially improving this metric for the sentence-based methods. This was done by visually inspecting the sentence clusters created by Snob for these methods, and remov-ing clusters composed of non-informative sentences such as the above (between four and six clusters were removed from each data set in this manner). Once a response is deemed to cover a request, then the full response (including these sentences) is used to calculate its quality. This has a small impact on the results of our evaluation, as a typical response includes two such sentences (opening and closing), and the average length of a response is 8.11 sentences. 4.3 Results
Figure 4 shows the coverage, average precision, and average F-score of each response-generation method per data set, where the averages are computed only for requests that are covered by the method in question. For example, the average precision of the
Doc-Ret method for data set no. 2, 0.39, is calculated over the 59% of the data set that was covered by this method. Table 4 shows data set descriptions and sizes, together with an overview of the best retrieval-based and prediction-based method for each data set ( SENTENCE refers to both Sent-Pred and Sent-Hybrid). The best method was selected manually on the basis of the results in Figure 4. To this effect, we considered the methods whose coverage exceeds some minimum (e.g., 10%), and chose the method(s) which could adequately answer the largest number of queries in the data set (based on coverage combined with F-score and precision). Table 5 presents the coverage and unique/best coverage of each method (the percentage of queries covered only by this method or for which this method produces a better reply than other methods), and the average and standard deviation of the precision and F-score obtained by each method (calculated over the requests that are covered). 614 and performance of the different methods for the different data sets (this result was confirmed with an ANOVA statistical test). Our results support the following specific observations.
 methods that perform well.

Doc-Ret. As seen in Table 5, Doc-Ret uniquely addresses 22% of the requests. However, the performance of this method is quite variable (high standard deviation), which may be due to an overly liberal setting of the applicability threshold (which results in both poor and good responses being generated, hence the high variability). Nevertheless, there are some cases where this method uniquely addresses requests quite well. This happens in situations such as that in Figure 1(b), where the initiating request is suffi-ciently similar to other requests with the same response. In contrast, Doc-Ret would not work well for a request such as that in Figure 1(a), which is quite detailed and specific, and hence unlikely to match any other request X  X esponse document.

Doc-Pred. Only about a tenth of the requests covered by Doc-Pred are uniquely ad-dressed by this method, but the generated responses are of a fairly high quality, with an average precision and F-score of 0.82. As indicated previously, the higher F-score and lower precision of Doc-Pred (compared to Sent-Pred) may be explained by the fact that the complete responses produced by the Doc-Pred method sometimes contain specific sentences that are inappropriate for the current situation. The rather large stan-dard deviation for F-score and precision suggests that Doc-Pred exhibits a somewhat inconsistent behavior. This may be explained by Figure 4, which shows that Doc-Pred performs very well on data set no. 3 (product replacement), but not so well on the others (Doc-Pred also has good F-score and precision scores for data set no. 2, but poor coverage).

Sent-Pred. In contrast to Doc-Pred, Sent-Pred can find regularities at the sub-document level, and generate partial responses, which typically omit inappropriate sentences that may be included by Doc-Pred. As a result, the responses generated by Sent-Pred have a consistently high precision (average 0.94, standard deviation 0.13), but this can be at the expense of recall, which explains the lower F-score (compared to Doc-Pred). Overall, the Sent-Pred method outperforms the other methods in 5% of the cases, where it either uniquely addresses requests, or produces responses with a higher F-score than those generated by other methods. As an example of situations where Sent-Pred outperforms all other methods, consider Figure 1(a), where Sent-Pred outputs the response shown 616 in this example, but without the first sentence. In other words, the generic portion of the response is confidently produced, and the specific portion is left out due to insufficient evidence. This example shows the benefit of a partial response: Although the response does not actually answer the user X  X  specific question (which would be difficult to automate due to the complex nature of the request), it can potentially save the user valuable time by referring him or her to the appropriate repair service.
Sent-Hybrid. The Sent-Hybrid method extends the Sent-Pred method by performing sentence retrieval. Sent-Hybrid X  X  higher coverage is achieved by the retrieval compo-nent, which disambiguates between groups of candidate sentences, thus enabling more sentences to be included in a generated response. However, this is at the expense of precision. Although retrieval selects sentences that match closely a given request, these sentences can differ from the  X  X elections X  made by a human operator in the model response. Precision (and hence F-score) penalizes such sentences, even when they are more appropriate than those in the model response. For instance, consider the example at the top of Figure 6. The response is quite generic, and is used almost identically for several other requests. The Sent-Hybrid method produces a very similar response, shown in the text at the bottom of Figure 6. Only the first sentence differs from the first sentence in the model response (the different parts have been italicized). The sentence selected by the Sent-Hybrid method, which matches more request words than the first sentence in the model response, was chosen from a sentence cluster with medium cohesion (Section 3.2.3), which contains sentences that describe different reasons for setting up a repair (the matching word is screen ). The rather high standard deviation in the precision (and hence F-score) for Sent-Hybrid may be due to these kinds of situations. Nonetheless, this method outperforms the other methods in about 10% of the cases, where it either uniquely addresses requests, or produces responses with a higher F-score than those generated by other methods.

All the methods combined. The bottom row of Table 5 shows that all the methods together have a coverage of 72%, which means that at least one of the methods can produce a non-empty and non-trivial response for 72% of the requests. The combined
F-score and precision averages are calculated on the basis of the best-performing method for each request. At first glance, using the best method may appear too lenient, as in practice, we cannot always automatically select this method in advance. However, these averages also suffer from the fact that in many cases only the Doc-Ret method is applicable, but its performance is poor. As mentioned previously, this tension between coverage and performance may be attributed to our empirically determined applicabil-ity thresholds (Section 3). 4.4 Summary
We have investigated the suitability of different response generation methods for the help-desk task. These methods, which vary significantly in their approach to response automation, cover a wide range of situations that arise in a help-desk corpus. that users of our system can select a single best response-generation method on the basis of these features. However, with the exception of data set no. 3, no clear set of features presents itself to support such a selection. Further, as seen in Table 4, superficial features of the data sets, such as topic and size, are not sufficient to characterize the applicability and performance of the different methods. These results indicate that (1) there are deeper features of data sets that must be considered in order to select a single suitable technique; or (2) the selection of a technique does not depend on features of the data set itself, but on the spread of situations in the data set. That is, the applicability and performance of a response-generation method depends on the specifics of the situation, and different data sets contain a different spread of situations. This second conjecture is supported by the results in Figure 4 and Table 5, which indicate that different methods have pockets of unique applicability in each data set. Of course, this still begs the question of why different data sets have different spreads of situations. Unfortunately, we do not have an answer to this question, and circumvent it by using the meta-learning technique described in Section 6, which has the added benefit of obviating the problem of selecting applicability thresholds.
 learning, the following considerations can be applied.
 5. User-Based Evaluation of Individual Methods
The size of our corpus necessitates an automatic evaluation in order to produce mean-ingful results, especially because we are comparing several methods under a number of 618 experimental settings. Although our automatic evaluation has yielded useful insights, it has two main limitations.

These limitations reinforce the notion that automated responses should be assessed on their own merit, rather than with respect to some model response.
 in that they provide answers to queries. These systems addressed the evaluation issue as follows.
 ies needed to produce meaningful results for our system. Firstly, the size of our corpus and the number of parameters and settings that we need to test mean that in order for a user study to be representative, a fairly large sample involving several hundreds of request X  X esponse pairs would have to be used. Further, user-based evaluations of the output produced by our system require the subjects to read relatively long request X  response e-mails, which quickly becomes tedious.
 study where we asked four judges (graduate students from the Faculty of Informa-tion Technology at Monash University) to assess the responses generated by our sys-tem (Marom and Zukerman 2007a). Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator. Our user study assessed the response-generation methods from the following perspectives, which yield information that is beyond the F-score and precision measures obtained in the automatic evaluation.
 5.1 Experimental Set-Up
We had two specific goals for this evaluation. First, we wanted to compare document-level versus sentence-level methods. Second, we wanted to evaluate cases where only the sentence-level methods can produce a response, and establish whether such responses, which are often partial, provide a good alternative to a non-response. We therefore presented two evaluation sets to each judge. 1. The first set contained responses generated by Doc-Pred and Sent-Hybrid. 2. The second set contained responses generated by Sent-Pred and
Each evaluation set contained 80 cases, randomly selected from the corpus in proportion to the size of each data set, where a case contained a request e-mail, the model response, and the responses generated by the two methods being compared. Each judge was given 20 of these cases, and was asked to assess the generated responses on the four criteria listed previously. 14 thus avoiding a situation where a particularly good or bad set of cases is evaluated by all judges. In addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges X  assessments would be comparable.
Because the judges do not evaluate the same cases, we could not employ standard inter-annotator agreement measures (Carletta 1996). However, it is still necessary to 620 have some measure of agreement, and control for bias from specific judges or specific cases. This was done by performing pairwise significance testing, treating the data from two judges as independent samples (we used the Wilcoxon Rank-Sum Test for equal medians). We conducted this significance test separately for each method and each of the four criteria, and eliminated the data from a particular judge if he or she had a significant disagreement with other judges. This happened with one of the judges, who was significantly more lenient than the others on the Sent-Pred method for the first, second, and fourth criteria, and with another judge, who was significantly more stringent on the Sent-Hybrid method for the third criterion. 5. 2Results
Figure 7 shows the results for the four criteria. The left-hand side of the figure pertains to the first evaluation set, and the right-hand side to the second set.
 are applicable, the former is generally preferred, rarely receiving a zero informativeness judgment. Because the two methods are evaluated for the same set of cases, we can perform a paired significance test for differences between them. Using a Wilcoxon signed rank test for a zero median difference, we obtain a p-value 0 . 01, indicating that the differences in judgments between the two methods are statistically significant. The right-hand side of Figure 7(a), which compares the two sentence-based methods, shows that there do not appear to be significant differences between our subjects X  assessment of these methods. This result is confirmed by the paired significance test, which produces a p-value of 0.13.
 information criteria also favor the Doc-Pred method. The responses produced by
Doc-Pred were judged to have significantly less missing information than those gen-erated by Sent-Hybrid (the paired significance test produces a p-value 0 . 01). The responses produced by Doc-Pred were also judged to have less misleading informa-tion than those generated by Sent-Hybrid, but the paired differences between the two methods are not statistically significant (the p-value is 0.125). The second evaluation set was judged to have missing information in approximately 55% of the cases for both sentence-level methods (the p-value is 0.11, indicating an insignificant difference between these methods). This high proportion of missing information is in line with the relatively low F-scores obtained in the automatic evaluation (Table 5), as missing infor-mation results in low recall and hence a lower F-score. The results for the misleading-information criterion also indicate no significant difference between the sentence-level methods (the p-value is 1). The low proportion of misleading information is in line with the high precision values obtained in the automatic evaluation (Table 5) X  X hereas responses with a high precision may be incomplete, they generally contain correct information.
 than  X  X orse X  judgments, although the opposite is true for Sent-Hybrid, and that both
Doc-Pred and Sent-Hybrid receive a small proportion of  X  X etter X  judgments. The paired significance test produces a p-value 0 . 01, confirming that these differences are signif-icant. The right-hand side of Figure 7(c) shows smaller differences between Sent-Pred and Sent-Hybrid, and indeed the p-value for the paired differences is 0.27. Notice that
Sent-Pred does not receive any  X  X etter X  judgments, whereas Sent-Hybrid does. 5.3 Summary
The results of this study show that responses provided by document-level methods were preferred to responses provided by sentence-level methods, but when document-level methods cannot be used, the sentence-level methods provide a good alternative.
Additionally, although our trial subjects showed a slight preference for the output produced by the Sent-Hybrid method compared to Sent-Pred, this preference was not statistically significant.
 tion 4), the result regarding the Sent-Hybrid method is somewhat disappointing. This is because we hoped that our trial subjects would prefer Sent-Hybrid to Sent-Pred, as the former is designed to better tailor a response to a request. However, we cannot determine from this result whether indeed there is no difference between the sentence-based methods, or whether such a difference simply could not be observed from our test sample of at most 80 cases, which constitutes 1.8% of the corpus used in our automatic evaluation (as indicated previously, it would be quite difficult to conduct user studies with a much larger data set).
 ducting meaningful user studies for a large corpus such as ours. These problems are exacerbated by our proportional data-selection policy, which is necessary to make the 622 test set representative of the corpus, but increases the difficulty of drawing specific con-clusions, for example, determining whether a particular method is favored for specific data sets. 6. Meta-Learning
In Section 4, we employed empirically determined applicability thresholds to cir-cumscribe the coverage of the different response-generation methods. However, as shown by our results, these thresholds were sometimes sub-optimal. In this section, we describe a meta-level process which can automatically select a response-generation method to address a new request without using such thresholds.
 most confident regarding its decision (Burke 2002). However, in our case, the individual confidence (applicability) measures employed by our response-generation methods are not comparable (e.g., the retrieval score in Doc-Ret is different in nature from the pre-diction probability in Doc-Pred). Hence, prior to selecting the most confident method, we need to find a way to compare the different measures of confidence. Because the performances of the different methods are comparable, we do this by establishing a link between confidence and performance. In other words, our meta-level process learns to predict the performance of the different methods from their confidence levels on the basis of previous experience. These predictions enable our system to recommend a particular method for handling a new (unseen) request (Marom, Zukerman, and Japkowicz 2007).
 consists of applying supervised learning, where a winning method is selected for each case in the training set, all the training cases are labeled accordingly, and then the system is trained to predict a winner for unseen cases. However, in our situation, there is not always one single winner (two methods can perform similarly well for a given request), and there are different ways to pick winners (for example, based on F-score or precision). Therefore, such an approach would require the utilization of subjective heuristics for creating labels, which would significantly influence what is being learned.
Instead, we adopt an unsupervised approach that finds patterns in the data X  X onfidence values coupled with performance scores (Section 6.1) X  X nd then attempts to fit unseen data to these patterns (Section 6.2). Heuristics are still needed in order to decide which response-generation method to apply to an unseen case, but they are applied only after the learning is complete (Section 6.3). In other words, the subjective process of setting performance criteria (which should be conducted by the organization running the help-desk) does not influence the machine learning process. 6.1 Training
We train the system by clustering the  X  X xperiences X  of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations (7) and (8), respectively). We then use the program
Snob (Wallace and Boulton 1968; Wallace 2005) to cluster these experiences. Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA) X  X hese di-mensions account for 95% of the variation in the data. The bottom part of Figure 8(b) shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15 These clusters were chosen because they illustrate clearly three situations of interest.
 6. 2Prediction
We test the system with an unseen set of requests, which we feed to each response-generation method. Each method then outputs a value for its confidence measure. We do not know in advance how each method will perform X  X his information is missing, and we predict it on the basis of the clusters obtained from the training set. Our prediction of how well the different methods will perform on an unseen case is based on (1) how well the unseen case fits each of the clusters and (2) the average performance values in each cluster as indicated by its centroid. 624 values are most similar to those in the centroid of Cluster 16. In this case, the selection of a method depends on whether we favor recall or precision, as Doc-Pred has a higher recall than Sent-Pred, but Sent-Pred has a higher precision. Now, Cluster 15 (not labeled in Figure 8(a)) contains similar confidence values to those of Cluster 16, but its (precision, recall) values for Doc-Pred and Sent-Pred are (0.76, 0.66) and (0.84, 0.67) respectively. If Cluster 15 had the strongest match with the unseen case, then Sent-Pred would have been chosen, regardless of any preferences for precision or recall.
However, it is not clear that the best policy consists of simply choosing the method suggested by the best-matching cluster. This is particularly the case when an unseen example has a reasonably good match with more than one cluster (e.g., Clusters 15 and 16).
 missing values. For each unseen data point x (with missing performance values), Snob calculates Pr( c i | x ), the posterior probability of each cluster c
These probabilities indicate how well an unseen case matches each of the clusters. For example, for the unseen case in Figure 8(b), Snob may assign posterior probabilities of 0.5 and 0.3 to Clusters 16 and 15, respectively (and lower probabilities to weaker-matching clusters, such as Cluster 8). 16 formance of each response-generation method: Max , which considers only the best-matching cluster (i.e., that with the highest posterior probability); and Weighted , which considers all clusters, weighted by their posterior probabilities. These techniques are used to calculate the estimated precision ( p k ) and estimated recall ( r response-generation method method k  X  { Doc-Ret, Doc-Pred, Sent-Pred, Sent-Hybrid follows.
 6.3 Method Selection
In order to select a method for a given request, we need to combine our estimates of precision and recall into an overall estimate of performance, and then choose the method with the best estimated performance. The standard approach for combining precision and recall is to compute their harmonic mean, F-score, as we have done in our comparative evaluation in Section 4. However, in order to accommodate different levels of preference towards precision or recall, as discussed herein, we use the following weighted F-score calculation (van Rijsbergen 1979).
 where w is a weight between 0 and 1given to precision. When w = 0 . 5 we have the standard usage of F-score (Equation (9)), and for w &gt; 0 . 5, we have a preference for high precision. For example, for w = 0 . 5, the precision and recall values of Cluster 16 (Figure 8(b)) translate to F-scores of 0.895 and 0.865 for Doc-Pred and Sent-Pred, respec-tively, leading to a choice of Doc-Pred. In contrast, for w = 0 . 75, the respective F-scores are 0.897 and 0.914, leading to a choice of Sent-Pred. 7. Evaluation of Meta-Learning
We evaluate the meta-learning system by looking at the quality of the response pro-duced by the method selected by this system, where, as done in Section 4, quality is measured using F-score and precision. However, here we employ 5-fold cross-validation (instead of 10-fold) to ensure that we get a good spread of selected methods in each testing split. This is particularly important when only a few methods dominate for a data set. 7.1 Experimental Set-Up
In our evaluation, we compare the alternative approaches for estimating performance (Equations (10) and (11)), and consider the effect of favoring precision when selecting a method via the weighted F-score calculation (Equation (12)). To perform these compar-isons we employ the following configurations.
 We also devised the following baselines to help ground our results.
 for all the response-generation methods. Therefore, we also test these configurations in 626 a practical setting where the system has the choice of not selecting any method if the estimated performance of all the methods is poor. We envisage that a practical system would behave in this manner, in the sense that a request for which none of the existing methods can produce an appropriate response would be passed to an operator. As mentioned in Section 4.2, we consider precision to be an important practical criterion because it does not penalize partial but correct responses. Therefore, we  X  X mplement X  our practical system by selecting only responses whose estimated precision is above 0.8. For these tests we also report on coverage, that is, the percentage of cases where this condition is met. Note that the baselines do not have an estimated precision because they do not use meta-learning. However, for completeness, we implement the practical system for them as well, with a threshold of 0.8 on actual precision. 7. 2Results
Table 6 shows the results of our tests averaged over all the cases in the corpus (with standard deviations in parentheses). The left-hand side corresponds to the setting where the system always selects a response-generation method, and the right-hand side corre-sponds to the setting where a method is selected only if its precision equals or exceeds 0.8 (this is an estimated precision for the Max and Weighted configurations, and an actual precision for the Gold and Random baselines).
 line has the worst performance. The Gold baselines outperform their corresponding meta-learning counterparts (except for the precision of Weighted50), but the differ-ences in precision are not statistically significant between the Gold and the Weighted configurations (using a t-test with a 1% significance level). Comparing the correspond-ing Weighted and Max configurations, the former is superior, but this is statistically significant only for the difference in precision values between Weighted50 and Max50.
Comparing a standard F-score calculation with a precision-favoring calculation ( w = 0 . 5 versus w = 0 . 75 in Equation (12)), as expected, precision is significantly higher for the of a reduced F-score, but the increase is larger than the reduction. best precision and the second-best F-score, 17 but its coverage is quite low (only 37.6%).
In contrast, the meta-learning configurations cover a proportion of the requests that is comparable to the coverage of the Gold baselines (approximately 57%), and all the results are substantially improved; as expected, all the precision values are high, and also more consistent than before (they have a lower standard deviation). These results are quite impressive for the meta-learning configurations, as their selection between methods is based on estimated precision, as opposed to the baselines, whose selections are based on actual precision, which is not available in practice. Comparing the corre-sponding Weighted and Max configurations, there are no significant differences in F-score, but Weighted outperforms Max on precision (the difference between Weighted75 and Max75 produces a p-value of 0.035). Finally, comparing w = 0 . 5with w = 0 . 75 for both Weighted and Max, as for the All-cases results, the increase in precision is larger than the reduction in F-score (p &lt; 0 . 01).
 response-generation methods, let us inspect what happens for each data set. We saw in Section 4 that the various methods differ in their applicability to the different data sets (Figure 4). Hence, we would expect the meta-level process to select between the methods differently for each data set. Figure 9 shows the method-selection proportions for each data set for the Weighted50 and Weighted75 configurations, using the practical setting where the system can choose not to select any method. What is immediately notable is that no method is selected for two of the data sets (nos. 4 and 6). This follows from the poor performance of all the methods for these data sets (Figures 4(b) and 4(c)).
At first glance, it appears that the selection of the Sent-Pred method for data set no. 1 contradicts the results in Figure 4, which shows a low coverage of Sent-Pred for this data set. However, this selection is justified by the fact that the meta-learning procedure 628 selects methods based on their historical performance (precision and recall), without filtering on applicability threshold (which is the basis for coverage). Figure 9 also highlights the impact of favoring precision on the selection of the Sent-Pred method instead of Doc-Pred. This effect is most dramatic for data set no. 3, but it can also be observed for data sets no. 2 and 5.

Weighted50 configuration, and data set no. 8). Also notable is the fact that Sent-Hybrid is selected only for data set no. 7. We postulate that this happens because data set no. 7 merges several sub-topics, and has different versions of similar responses, where each version has a generic component combined with a request-specific component (this feature is not shared by the other merged data sets no. 6 and 8). Sent-Pred is not confident enough to select one of these specific components, whereas Sent-Hybrid is.
This ability to select a specific response is demonstrated in Figure 10, which shows a request from data set no. 7 and its automated response. This response belongs to a medium-cohesion cluster which contains responses that share the generic segment up to regarding , but the rest of the response refers specifically to terms in the request. 7.3 Summary The meta-learning results may be summarized as follows.
 8. Related Research
The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997). Such technologies require significant human in-put, and are difficult to create and maintain (Delic and Lahaix 1998). In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus.
 responses (Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and
Scheffer 2004; Malik, Subramaniam, and Kaushik 2007). trieves a list of request X  X esponse pairs and presents a ranked list of responses to the user. If the user is unsatisfied with this list, an operator is asked to generate a new response.
The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses. However, there is no attempt to automatically generate a single response.
 document prediction for generating help-desk responses. Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question X  X nswer pairs. Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes. The generated response is the answer that is closest to the centroid of the cluster.
Bickel and Scheffer X  X  results are consistent with ours, in the sense that the performance of the Doc-Ret method is significantly worse than that of Doc-Pred. However, it is worth noting that their corpus is significantly smaller than ours (805 question X  X nswer pairs), their questions seem to be much simpler and shorter than those in our corpus, and the replies shorter and more homogeneous.
 answer pairs from help-center e-mails, and then maps new questions to existing ques-tions in order to retrieve an answer. This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences. In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates. ation of response e-mails: text classification, case-based reasoning, and question answer-ing. Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed. The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches. However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were ap-plied to substitute names of individuals and companies in texts).
 of different kinds of corpus-based approaches (namely, retrieval and prediction) applied at different levels of granularity (namely, document and sentence).
 marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; 630
Berger et al. 2000; Jijkoun and de Rijke 2005; Soricut and Brill 2006). An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information. Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses.
 approach where they recognized named entities and performed anaphora resolution prior to applying ranking metrics to select sentences for inclusion in a thread summary.
However, their approach does not specifically address the question X  X nswer aspect of an e-mail thread, potentially omitting important information. This problem was addressed by Shrestha and McKeown (2004), who performed supervised learning in order to match questions with answers in e-mail threads, as a first step in the summarization of such threads. A significant difference between our approach and theirs is in our use of unsupervised learning, which is necessitated by the size of our data set. Also,
Shrestha and McKeown used high-level features for machine learning, as well as word-based features. As indicated in Section 3.2.2, our Sent-Pred experiments with high-level features (specifically syntactic features) did not improve our results. Finally, Shrestha and McKeown used paragraphs as a unit of information X  X n approach we tried late in our project with encouraging results. This suggests that there are situations where one can generalize a response that is longer than a sentence but shorter than a whole document. Unfortunately, we could not pursue this avenue of research owing to time limitations.
 a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document. They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models. Berger et al. (2000) compared two retrieval approaches ( TF . IDF and query expansion) and two predictive approaches (statistical translation and latent variable models). Jijkoun and de Rijke (2005) compared different variants of retrieval techniques. Soricut and Brill (2006) compared a predictive approach (statistical translation), a retrieval approach based on a language-model, and a hybrid approach which combines statistical chunking and traditional retrieval. Two significant differ-ences between help-desk and FAQs are the following.

These issues also differentiate the help-desk application from other types of question-answering applications, specifically those found in the field of restricted domain ques-tion answering (Moll  X  a and Vicedo 2007). level strategy to combine them. This kind of meta-learning is referred to as stacking by the Data Mining community (Witten and Frank 2000). Lekakos and Giaglis (2007) imple-mented a supervised version of this approach for a recommender system, as opposed to our unsupervised version. They also proposed two major categories of meta-learning approaches for recommender systems, merging and ensemble , each subdivided into the more specific subclasses suggested by Burke (2002) as follows. The merging category corresponds to techniques where the individual methods affect each other in differ-ent ways (this category encompasses Burke X  X  feature combination , cascade , feature augmentation ,and meta-level sub-categories). The ensemble category corresponds to techniques where the predictions of the individual methods are combined to produce a final prediction (this category encompasses Burke X  X  weighted , switching ,and mixed sub-categories).
 the various methods into a single outcome. More specifically, it belongs to Burke X  X  switching sub-category, where a single method is selected on a case-by-case basis. A similar approach is taken in Rotaru and Litman X  X  (2005) reading comprehension system, but their system does not perform any learning. Instead it uses a voting mechanism to select the answer given by the majority of methods. The question answering system developed by Chu-Carroll et al. (2003) belongs to the merging category of approaches, where the output of an individual method can be used as input to a different method (this corresponds to Burke X  X  cascade sub-category). Because the results of all the methods are comparable, no learning is required: At each stage of the  X  X ascade of methods, X  the method that performs best is selected. In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics. Therefore, we need to learn from experience when to use each method. 9. Conclusion
Despite its theoretical importance and commercial impact, the generation of e-mail-based help-desk responses has received scant attention to date. In this article, we have investigated complementary corpus-based, information-gathering methods for automatically addressing help-desk requests. Our results show that a large corpus of request X  X esponse e-mail pairs supports the automation of a significant portion of the help-desk task with data-driven techniques that reuse responses or parts thereof. These techniques are particularly suitable for repetitive, non-technical issues, allowing help-desk operators to focus on more challenging, technical requests.
 that arise in the help-desk domain, and that the performance of different methods varies for different data sets. This suggests that there must be an underlying relationship between methods and features of data sets that needs to be accounted for (Section 4.3). Additionally, our results yield insights regarding the following issues.
 632 evaluation of a large corpus such as ours. Despite its modest size, our user study was useful, as it provided a subjective evaluation of the methods considered, and linked the results of our automatic evaluation with these subjective assessments.
 the meta-learning component. Although our comparative investigation demonstrates the applicability of the different methods, the meta-learning component provides a way to automatically select a method. We have offered an unsupervised approach that learns which is the most promising method based on previous experience. It does so by learning the relationship between the value of the confidence (applicability) measure of each method and its subsequent performance. This eliminates the need to set subjective thresholds on these confidence values, and instead transfers subjective decision-making to a more intuitive part of the system, namely, the actual performance of the methods, measured by the quality of the generated responses. In this way, help-desk managers can decide how strict the system should be, for example, on the precision of responses. formance using only a simple, low-level bag-of-words representation. One avenue for future research is to investigate more sophisticated representations, such as incorpo-rating word-based similarity metrics into the bag-of-words representation, employing query expansion during retrieval, and taking into account syntactic features during retrieval and prediction (recall that we incorporated grammatical and sentence-based syntactic features into the Sent-Pred method without significantly affecting perfor-mance, Section 3).
 granularity, such as paragraphs. Ideally there should be a mechanism that determines dynamically the most suitable level of granularity for capturing the regularities in a collection of e-mails. An information-theoretic approach, such as the MML crite-rion (Wallace and Boulton 1968; Wallace 2005), may be a promising way to address this problem.
 Acknowledgments 634
