 Auer et al., 2002; Even-dar et al., 2006, for example). It can be regarded as a repeated game between two players, with every stage consisting of the following: The world chooses k rewards r rewards, and then observes the reward r i . The contextual bandits setting considered in this paper is the same except for a modification of the first step, in which the player also observes context information x which can be used to determine which arm to pull.
 The contextual bandits problem has many applications and is often more suitable than the standard bandits problem, because settings with no context information are rare in practice. The setting considered in this paper is directly motivated by the problem of matching ads to web-page contents on the internet. In this problem, a number of ads (arms) are available to be placed on a number of web-pages (context information). Each page visit can be regarded as a random draw of the context information (one may also include the visitor X  X  online profile as context information if available) from an underlying distribution that is not controlled by the player. A certain amount of revenue is generated when the visitor clicks on an ad. The goal is to put the most relevant ad on each page to maximize the expected revenue. Although one may potentially put multiple ads on each web-page, we focus on the problem that only one ad is placed on each page (which is like pulling an arm given context information). The more precise definition is given in Section 2.
 Prior Work . The problem of bandits with context has been analyzed previously (Pandey et al., 2007; Wang et al., 2005), typically under additional assumptions such as a correct prior or knowledge of the relationship between the arms. This problem is also known as associative reinforcement learning (Strehl et al., 2006, for example) or bandits with side information. A few results under as weak or weaker assumptions are directly comparable. What we do . We present and analyze the Epoch-Greedy algorithm for multiarmed bandits with solving this problem.
 The paper is broken up into the following sections. We first formally define contextual bandit problems and algorithms to solve them.
 Definition 2.1 (Contextual bandit problem) In a contextual bandits problem, there is a distribu-and r a  X  [0 , 1] is the reward for arm a . The problem is a repeated game: on each round, a sample by the player, its reward r a is revealed.
 Definition 2.2 (Contextual bandit algorithm) A contextual bandits algorithm B determines an arm a  X  { 1 , . . . , k } to pull at each time step t , based on the previous observation sequence ( x Our goal is to maximize the expected total reward P T t =1 E ( x notation r a,t = r a t to improve readability. Similar to supervised learning, we assume that we are x to an arm a . A natural goal is to choose arms to compete with the best hypothesis in H . We introduce the following definition.
 Definition 2.3 (Regret) The expected reward of a hypothesis h is regret of B with respect to a hypothesis h be: The expected regret of B up to time T with respect to hypothesis space H is defined as The main challenge of the contextual bandits problem is that when we pull an arm, rewards of other arms are not observed. Therefore it is necessary to try all arms (explore) in order to form an accurate estimation. In this context, methods we investigate in the paper make explicit distinctions between exploration and exploitation steps. In an exploration step, the goal is to form unbiased samples by randomly pulling all arms to improve the accuracy of learning. Because it does not focus on the best arm, this step leads to large immediate regret but can potentially reduce regret for the future exploitation steps. In an exploitation step, the learning algorithm suggests the best hypothesis learned from the samples formed in the exploration steps, and the arm given by the hypothesis is pulled: the goal is to maximize immediate reward (or minimize immediate regret). Since the samples in the exploitation steps are biased (toward the arm suggested by the learning algorithm using previous exploration samples), we do not use them to learn the hypothesis for the future steps. That is, in methods we consider, exploitation does not help us to improve learning accuracy for the future.
 More specifically, in an exploration step, in order to form unbiased samples, we pull an arm a  X  { 1 , . . . , k } uniformly at random. Therefore the expected regret comparing to the best hypothesis in H can be as large as O (1) . In an exploitation step, the expected regret can be much smaller. Therefore a central theme we examine in this paper is to balance the trade-off between exploration and exploitation, so as to achieve a small overall expected regret up to some time horizon T . Note that if we decide to pull a specific arm a with side information x , we do not observe rewards r samples, where a is picked uniformly at random, can create a standard learning problem without missing observations. This is simply achieved by setting fully observed rewards r 0 such that where I (  X  ) is the indicator function. The basic idea behind this transformation from partially ob-served to fully observed data dates back to the analysis of  X  X ample Selection Bias X  (Heckman, 1979). The above rule is easily generalized to other distribution over actions p ( a ) by replacing k with 1 /p ( a ) .
 The following lemma shows that this method of filling missing reward components is unbiased. Proof We have: Lemma 2.1 implies that we can estimate reward R ( h ) of any hypothesis h ( x ) using expectation estimation can be obtained with uniform convergence learning bounds. The problem of treating contextual bandits as standard bandits is that the information in x is lost. let m be the number of hypotheses, we can get a bound of O ( m ) . However, this solution ignores the fact that many hypotheses can share the same arm so that choosing an arm yields information for many hypotheses. For this reason, with a simple algorithm, we can get a bound that depends on above.
 As discussed earlier, the key issue in the algorithm is to determine when to explore and when to exploit, so as to achieve appropriate balance. If we are given the time horizon T in advance, and would like to optimize performance with the given T , then it is always advantageous to perform a by switching the two steps, we can more accurately pick the optimal hypothesis in the exploitation step due to more samples from exploration. With fixed T , assume that we have taken n steps of exploration, and obtain an average regret bound of n for each exploitation step at the point, then the point n that minimizes the sum.
 Without knowing T in advance, but with the same generalization bound, we can run explo-ration/exploitation in epochs, where at the beginning of each epoch ` , we perform one step of explo-ing T is no more than the optimal regret bound min n [ n + ( T  X  n ) n ] (with known T and optimal stopping point). Therefore the performance of our method (which does not need to know T ) is no worse than three time the optimal bound with known T and optimal stopping point. This motivates 1995).
 Proposition 3.1 Consider a sequence of nonnegative and monotone non-increasing numbers { n } . Let L  X  = min { L : P L ` =1 (1 + d 1 / ` e )  X  T } , then Proof Let n  X  = arg min n  X  [0 ,T ] [ n + ( T  X  n ) n ] . The bound is trivial if n  X   X  L  X  . We only need consider the case n  X   X  L  X   X  1 . By assumption, P L  X   X  1 ` =1 (1 + 1 / ` )  X  T  X  1 . Since P Rearranging, we have L  X   X  n  X  + ( T  X  L  X  ) n  X  .
 eralization bound, yields performance comparable to the optimal bound with known time horizon T .
 Definition 3.1 (Epoch-Greedy Exploitation Cost) Consider a hypothesis space H consisting of estimator step count. Then the per-epoch exploitation cost is defined as: Theorem 3.1 For all T, n ` , L such that: T  X  L + P L ` =1 n ` , the expected regret of Epoch-Greedy in Figure 1 is bounded by This theorem statement is very general, because we want to allow sample dependent bounds to be used. When sample-independent bounds are used the following simple corollary holds: arg min L { L : L + P L ` =1 s `  X  T } . Then the expected regret of Epoch-Greedy in Figure 1 is bounded by Proof (of the main theorem) Let B be the Epoch-Greedy algorithm. One of the following events will occur: Thus the total expected contribution of A to the regret  X  R ( B , H , T ) is at most first L epochs.
 Z This is independent of the number of exploitation steps before epoch ` . Therefore we can treat W ` as ` independent samples. This means that the expected regret associated with exploitation we obtain the desired bound.
 In the theorem, we bound the expected regret of each exploration step by one. Clearly this assumes the worst case scenario and can often be improved. Some consequences of the theorem with specific function classes are given in Section 4. Theorem 3.1 is quite general. In this section, we present a few simple examples to illustrate the potential applications. 4.1 Finite hypothesis space worst case bound Consider the finite hypothesis space situation, with m = |H| &lt;  X  . We apply Theorem 3.1 with a worst-case deviation bound.
 implies that there exists a constant c 0 &gt; 0 such that  X   X   X  (0 , 1) , with probability 1  X   X  : It follows that there exists a universal constant c &gt; 0 such that Therefore in Figure 1, if we choose then  X  ` ( H , s )  X  1 : this is consistent with the choice recommended in Proposition 3.1. In order to obtain a performance bound of this scheme using Theorem 3.1, we can simply take that for any given T , we can take in Theorem 3.1.
 In summary, if we choose s ( Z ` 1 ) = b c p `/ ( k ln m ) c in Figure 1, then Reducing the problem to standard bandits, as discussed at the beginning of Section 3, leads to a bound of O ( m ln T ) (Lai &amp; Robbins, 1985; Auer et al., 2002). Therefore when m is large, the Epoch-Greedy algorithm in Figure 1 can perform significantly better. In this particular situa-tion, Epoch-Greedy does not do as well as Exp4 in (Auer et al., 1995), which implies a regret of O (  X  For many hypothesis classes, the ln m factor can be improved for Epoch-Greedy. In fact, a similar result can be obtained for classes with infinitely many hypotheses but finite VC dimensions. More-over, as we will see next, under additional assumptions, it is possible to obtain much better bounds in terms of T for Epoch-Greedy, such as O ( k ln m + k ln T ) . This extends the classical O (ln T ) bound for standard bandits, and is not possible to achieve using Exp4 or simple variations of it. 4.2 Finite hypothesis space with unknown expected reward gap This example illustrates the importance of allowing sample-dependent s ( Z ` 1 ) . We still assume a finite hypothesis space, with m = |H| &lt;  X  . However, we would like to improve the performance bound by imposing additional assumptions. In particular we note that the standard bandits problem Epoch-Greedy algorithm can have a regret of the form O (ln T ) .
 The main technical reason that the standard bandits problem has regret O (ln T ) is that the expected reward of the best bandit and that of the second best bandit has a gap: the constant hidden in the O (ln T ) bound depends on this gap, and the bound becomes trivial (infinity) when the gap approaches zero. In this example we show that a similar assumption for contextual bandits problems leads to a similar regret bound of O (ln T ) for the Epoch-Greedy algorithm.
 known in advance.
 Although  X  is not known, it can be estimated from the data Z n 1 . Let the empirical reward of h  X  X  be highest empirical reward. We define the empirical gap as h . Again, the standard large deviation bound implies that there exists a universal constant c &gt; 0 such that for all j  X  1 : such that There exists a constant c 00 &gt; 0 such that for any L : Now, consider any time horizon T . If we set n ` = 0 when ` &lt; L , n L = T , and then  X  R ( Epoch-Greedy , H , T )  X  2 L + 1 + c 00 k  X   X  2  X  2 constant depends on the gap  X  which can be small. It is possible to combine the two strategies (that well when the gap  X  is large, but also not much worse than the bound of Section 4.1 when  X  is small. As a special case, we can apply the method in this section to solve the standard bandits problem. The O ( k ln T ) bound of the Epoch-Greedy method matches those more specialized algorithms for the standard bandits problem, although our algorithm has a larger constant. We consider a generalization of the multi-armed bandits problem, where observable context can be used to determine which arm to pull and investigate the sample complexity of the explo-ration/exploitation trade-off for the Epoch-Greedy algorithm.
 The Epoch-Greedy algorithm analysis leaves one important open problem behind. Epoch-Greedy is much better at dealing with large hypothesis spaces or hypothesis spaces with special structures due to its ability to employ any data-dependent sample complexity bound. However, for finite hypothesis that a better designed algorithm can achieve both strengths.
 Auer, P., Cesa-Bianchi, N., &amp; Fischer, P. (2002). Finite time analysis of the multi-armed bandit problem. Machine Learning , 47 , 235 X 256.
 adversarial multi-armed bandit problem. FOCS .
 multi-armed bandit and reinforcement learning problems. JMLR , 7 , 1079 X 1105.
 Heckman, J. (1979). Sample selection bias as a specification error. Econometrica , 47 , 153 X 161. Kearns, M., Mansour, Y., &amp; Ng, A. Y. (2000). Approximate planning in large pomdps via reusable trajectories. NIPS .
 Lai, T., &amp; Robbins, H. (1985). Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics , 6 , 4 X 22.
 Lai, T., &amp; Yakowitz, S. (1995). Machine learning and nonparametric bandit theory. IEEE TAC , 40 , 1199 X 1209.
 based approach. SIAM Data Mining Conference .
 associative bandit problems. ICML .
 Wang, C.-C., Kulkarni, S. R., &amp; Poor, H. V. (2005). Bandit problems with side observations. IEEE
Transactions on Automatic Control , 50 , 338 X 355.
