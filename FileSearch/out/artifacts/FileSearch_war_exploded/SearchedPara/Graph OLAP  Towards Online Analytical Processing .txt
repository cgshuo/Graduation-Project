 } @cs.uiuc.edu
OLAP (On-Line Analytical Processing) is an important notion in data analysis. Recently, more and more graph or networked data sources come into being. There exists a sim-ilar need to deploy graph analysis from different perspec-tives and with multiple granularities. However, tradition al OLAP technology cannot handle such demands because it does not consider the links among individual data tuples. In this paper, we develop a novel graph OLAP framework , which presents a multi-dimensional and multi-level view over graphs.

The contributions of this work are two-fold. First, start-ing from basic definitions, i.e. , what are dimensions and measures in the graph OLAP scenario, we develop a con-ceptual framework for data cubes on graphs. We also look into different semantics of OLAP operations, and clas-sify the framework into two major subcases: informational OLAP and topological OLAP . Then, with more emphasis on informational OLAP (topological OLAP will be covered in a future study due to the lack of space), we show how a graph cube can be materialized by calculating a special kind of measure called aggregated graph and how to imple-ment it efficiently. This includes both full materializatio n and partial materialization where constraints are enforce d to obtain an iceberg cube. We can see that the aggregated graphs, which depend on the graph properties of underly-ing networks, are much harder to compute than their tradi-tional OLAP counterparts, due to the increased structural complexity of data. Empirical studies show insightful re-sults on real datasets and demonstrate the efficiency of our proposed optimizations.
OLAP (On-Line Analytical Processing) [9, 5, 20, 2, 10] is an important notion in data analysis. Given the un-derlying data, a cube can be constructed to provide a multi-dimensional and multi-level view, which allows for effective analysis of the data from different perspectives and with multiple granularities. The key operations in an OLAP framework are slice/dice and roll-up/drill-down, with slice/dice focusing on a particular aspect of the data, roll-up performing generalization if users only want to see a concise overview, and drill-down performing specializa-tion if more details are needed.

In a traditional data cube, a data record is associated with a set of dimensional values, whereas different records are viewed as mutually independent . Multiple records can be summarized by the definition of corresponding aggregate measures such as COUNT, SUM, and AVERAGE. More-over, if a concept hierarchy is associated with each attribu te, multi-level summaries can also be achieved. Users can nav-igate through different dimensions and multiple hierarchi es via roll-up, drill-down and slice/dice operations. Howeve r, in recent years, more and more data sources beyond conven-tional spreadsheets have come into being, such as chemi-cal compounds or protein networks (chem/bio-informatics) , 2D/3D objects (pattern recognition), circuits (computer-aided design), loosely-schemaed data (XML), and social or informational networks (Web), where not only individual entities but also the interacting relationships among them are important and interesting. This demands a new genera-tion of tools that can manage and analyze such data.
Given their great expressive power, graphs have been widely used for modeling a lot of datasets that contain struc -ture information. With the tremendous amount of graph data accumulated in all above applications, the same need to deploy analysis from different perspectives and with mul -tiple granularities exists. To this extent, our main task in this paper is to develop a graph OLAP framework , which presents a multi-dimensional and multi-level view over graphs.

In order to illustrate what we mean by  X  X raph OLAP X  and how the OLAP glossary is interpreted with regard to this new scenario, let us start from a few examples. Example 1 (Collaboration Patterns). There are a set of au-thors working in a given field: For any two persons, if they coauthor w papers in a conference, e.g. , SIGMOD 2004, then a link is added between them, which has an collabo-ration frequency attribute that is weighted as w . For every conference in every year, we may have a coauthor network describing the collaboration patterns among researchers, each of them can be viewed as a snapshot of the overall coauthor network in a bigger context.

It is interesting to analyze the aforementioned graph dataset in an OLAP manner. First, one may want to check the collaboration patterns for a group of conferences, say, all DB conferences in 2004 (including SIGMOD, VLDB, ICDE, etc. ) or all SIGMOD conferences since its inaugu-ration. In the language of data cube, with a venue dimen-sion and a time dimension, one may choose to obtain the (db-conf, 2004) cell and the (sigmod, all-years) cell, where the venue and time dimensions have been generalized to db-conf and all-years , respectively. Second, for the subset of snapshots within each cell, one can summarize them by computing a measure as we did in traditional OLAP. In the graph context, this gives rise to an aggregated graph . For example, a summary network displaying total collaboration frequencies can be achieved by overlaying all snapshots to-gether and summing up the respective edge weights, so that each link now indicates two persons X  collaboration activit ies in the DB conferences of 2004 or during the whole history of SIGMOD.

The above example is simple because the measure is cal-culated by a simple sum over individual pieces of informa-tion. A more complex case is presented next.
 Example 2 (Maximum Flow). Consider a set of cities con-nected by transportation networks. In general, there are many ways to go from one city A to another city B , e.g. , by car, by train, by air, by water., etc. , and each way is op-erated by multiple companies. For example, we can assume that the capacity of company x  X  X  air service from A to B A to B using the planes it owns. Finally, we get a snapshot of capacity network for every service of every company.
Now, consider the transporting capability from a source city S to a destination city T , it is interesting to see how this value can be achieved by sending flows via different paths if 1) we only want to go by air, or 2) we only want to choose services operated by company x . In the OLAP lan-guage, with a transportation-type dimension and a company dimension, the above situations correspond to the (air, all-companies) cell and the (all-types, company x ) cell, while the measure computed for a cell c is a graph displaying the overall maximum flow, which has considered all types and companies that are allowed in c . Unlike Example 1, computing the aggregated graph is now a much harder task; also, the semantics associated with un-aggregated network snapshots and aggregated graphs are different: The for-mer shows capacities on its edges, whereas the latter shows transmitted flows, which must be smaller by its definition. Example 3 (Collaboration Patterns, Revisited). Usually, the whole coauthor network could be too big to comprehend, and thus it is desirable to look at a more compressed view. For example, one may like to see the collaboration activitie s organized by the authors X  associated affiliations, which re -quires the network to be generalized one step up, i.e. , merg-ing all persons in the same institution as one node and con-structing a new summary graph at the institution level. In this  X  X eneralized network X , for example, an edge between Stanford and University of Wisconsin will aggregate all col -laboration frequencies occurred between Stanford authors and Wisconsin authors. Similar to Examples 1 and 2, an ag-gregated graph ( i.e. , the generalized network defined above) is taken as the OLAP measure. However, the difference here is that a roll-up from the individual level to the institutio n level is achieved by consolidating multiple nodes into one, which shrinks the whole graph. Compared to this, in Ex-amples 1 and 2, the graph is not collapsed because we are always examining the relationships among the same set of objects  X  it poses minimum changes with regard to network topology upon generalization.

The above examples demonstrate that OLAP provides a powerful primitive to analyze graph datasets. In this paper , we will give a systematic study on graph OLAP, which is more general than traditional OLAP: In addition to individ-ual entities, the mutual interactions among them are also taken into consideration. Our major contributions are sum-marized as below.  X  For conceptual modeling , a graph OLAP framework  X  For efficient implementation , the computation of ag-The remaining of this paper is organized as follows. In Section 2, we formally introduce the graph OLAP frame-work. Section 3 discusses the general hardness to compute aggregated graphs as graph OLAP measures, which cate-gorizes them into three classes. Section 4 looks into some properties of the measures and proposes a few further opti-mizations. Constraints and partial materialization are st ud-ied in Section 5. We report experiment results and related work in Sections 6 and 7, respectively. Section 8 concludes this study.
In this section, we present the general framework of graph OLAP.
 Definition 1 (Graph Model). We model the data exam-ined by graph OLAP as a collection of network snap-shots G = { G ( I formational attributes describing the snapshot as a whole and G attached with any v  X  V any e  X  E sent different observations, V spond to the same set of objects in real applications.
For instance, with regard to the coauthor network de-scribed in the introduction, venue and time are two infor-mational attributes that mark the status of individual snap -shots, e.g. , SIGMOD 2004 and ICDE 2005, authorID is a node attribute indicating the identification of each node, a nd collaboration frequency is an edge attribute reflecting the connection strength of each edge.

Dimension and measure are two concepts that lay the foundation of OLAP and cubes. As their names imply, first, dimensions are used to construct a cuboid lattice and parti-tion the data into different cells, which act as the basis for multi-dimensional and multi-level analysis; second, mea-sures are calculated to aggregate the data covered, which deliver a summarized view of it. In below, we are going to formally re-define these two concepts for the graph OLAP scenario.

Let us examine dimensions at first. Actually, there are two types of dimensions in graph OLAP. The first one, as exemplified by Example 1, utilizes informational attribute s attached at the whole snapshot level. Suppose the following concept hierarchies are associated with venue and time :  X  venue : conference  X  area  X  all ,  X  time : year  X  decade  X  all ; the role of these two dimensions is to organize snapshots into groups based on different perspectives and granular-ities, e.g. , (db-conf, 2004) and (sigmod, all-years) , where each of these groups corresponds to a  X  X ell X  in OLAP ter-minology. They control what snapshots are to be looked at, but they do not touch the inside of any single snapshot. Definition 2 (Informational Dimensions). With regard to the graph model presented in Definition 1, the set of infor-mational attributes { I tional dimensions of graph OLAP, or Info-Dims in short.
The second type of dimensions are provided to operate on nodes and edges within individual networks. Take Ex-ample 3 for instance, suppose the following concept hierar-chy  X  authorID : individual  X  institution  X  all is associated with the node attribute authorID , then it can be used to group authors from the same institution into a  X  X en-eralized X  node, and a new graph thus resulted will depict interactions among these groups as a whole, which summa-rizes the original network and hides specific details. Definition 3 (Topological Dimensions). The set of dimen-sions coming from the attributes of topological elements ( i.e. , nodes and edges of G the topological dimensions of graph OLAP, or Topo-Dims in short.

The OLAP semantics accomplished through Info-Dims and Topo-Dims are rather different, and in the following we shall refer to them as informational OLAP (abbr. I-OLAP ) and topological OLAP (abbr. T-OLAP ), respectively.
For roll-up in I-OLAP , the characterizing feature is that, snapshots are just different observations of the same under -lying network, and thus when they are all grouped into one cell in the cube, it is like overlaying multiple pieces of in-formation, without changing the objects whose interactions are being looked at.

For roll-up in T-OLAP , we are no longer grouping snap-shots, and the reorganization switches to happen inside in-dividual networks. Here, merging is performed internally which  X  X ooms out X  the user X  X  focus to a  X  X eneralized X  set of objects, and a new graph formed by such shrinking might greatly alter the original network X  X  topological structur e.
Now we move on to measures. Remember that, in tradi-tional OLAP, a measure is calculated by aggregating all the data tuples whose dimensions are of the same values (based on concept hierarchies, such values could range from the finest un-generalized ones to  X  X ll/* X , which form a multi-level cuboid lattice); casting this to our scenario here:
First, in graph OLAP, the aggregation of graphs should also take the form of a graph, i.e. , an aggregated graph . In this sense, graph can be viewed as a special kind of measure, which plays a dual role: as a data source and as a special (aggregated) measure. Of course, other measures that are not graphs, such as node count, average degree, diameter, etc. , can also be calculated; however, we do not explicitly include such non-graph measures in our model, but instead treat them as derived from corresponding graph measures. Second, due to the different semantics of I-OLAP and T-OLAP, aggregating data with identical Info-Dim values groups information among the snapshots, whereas aggregat-ing data with identical Topo-Dim values groups topological elements inside individual networks. As a result, we will give a separate measure definition for each case in below. Definition 4 (I-Aggregated Graph). With regard to Info-Dims { I tributed graph that can be computed based on a set of net-work snapshots G  X  = { G Dims are of identical values; it satisfies: 1) the nodes of M I are as same as any snapshot in G  X  , and 2) the node/edge attributes attached to M I are calculated as ag-gregate functions of the node/edge attributes attached to G
The graph in Figure 1 that describes collaboration fre-quencies among individual authors for a particular group of conferences during a particular period of time is an in-stance of I-aggregated graph, and the interpretation of cla s-sic OLAP operations in graph I-OLAP is summarized as follows.  X  Roll-up: Overlay multiple snapshots to form a higher- X  Drill-down: Return to the set of lower-level snapshots  X  Slice/dice: Select a subset of qualifying snapshots based Definition 5 (T-Aggregated Graph). With regard to Topo-Dims { T tributed graph that can be computed based on an individ-ual network G tical values on their Topo-Dims are grouped, whereas each group corresponds to a node in M T , 2) the attributes at-tached to M T are calculated as aggregate functions of the attributes attached to G
The graph in Figure 2 that describes collaboration fre-quencies among institutions is an instance of T-aggregated graph, and the interpretation of classic OLAP operations in graph T-OLAP is summarized as follows.  X  Roll-up: Shrink the topology and obtain a T-aggregated  X  Drill-down: A reverse operation of roll-up.  X  Slice/dice: Select a subgraph of the network based on
Now, with a clear concept of dimension, measure and possible OLAP operations, we are ready to discuss imple-mentation issues, i.e. , how to compute the aggregated graph in a multi-dimensional and multi-level way.

Recall that in traditional OLAP, measures can be classi-fied into distributive , algebraic and holistic , depending on whether the measures of high level cells can be easily com-puted from their low level counterparts, without accessing base tuples residing at the finest level. For instance, in the classic sale ( time , location ) example, the total sale of [ 2008 , California ] can be calculated by adding up the total sales of [ January 2008 , California ], [ February 2008 , California ], . . . , [ December 2008 , California ], without looking at base data points such as [ 04/12/2008 , Los Angeles ], which means that SUM is a distributive measure. Compared to this, AVG is often cited as an algebraic measure, which is actually a semi-distributive category in that AVG can be derived from two distributive measures: SUM and COUNT, i.e. , alge-braic measures are functions of distributive measures. (Semi-)distributiveness is a nice property for top-down cube computation, where the cuboid lattice can be gradu-ally filled up by making level-by-level aggregations. Mea-sures without this property is put into the holistic categor y, which is intuitively much harder to calculate. Now, consid-ering graph OLAP, based on similar criteria with regard to the aggregated graphs, we can also classify them into three categories.
 Definition 6 (Distributive, Algebraic and Holistic). Con-sider a high level cell c cells it covers: c 1 tributive if M of M d ( c 1 l ) , M d ( c 2 l ) , . . . , i.e. , For a non-distributive aggregated graph M derived from some other distributive aggregated graphs M d , M then we say that it is algebraic . Aggregated graphs that are neither distributive nor algebraic belong to the holistic category.

If we further consider the differences between I-OLAP and T-OLAP, there are actually six classes: I-distributive , I-algebraic, I-holistic and T-distributive, T-algebraic , T-holistic. Due to limited space, in the remaining of this pa-per, we shall put more emphasis on I-OLAP; discussions for T-OLAP will be covered in a future study.
 I-distributive . The I-aggregated graph describing collabo-ration frequency in Example 1 is an I-distributive measure, because the frequency value in a high level I-aggregated graph can be calculated by simply adding those in corre-sponding low level I-aggregated graphs.
 I-algebraic . The graph displaying maximum flow in Ex-ample 2 is an I-algebraic measure based on the following reasoning. Suppose transportation-type and company com-prise the two dimensions of the cube, and we generalize from low level cells (all-types, company 1) , (all-types, com-pany 2) , . . . to (all-types, all-companies) , i.e. , compute the maximum flow based on all types of transportation oper-ated by all companies. Intuitively, this overall maximum flow would not be a simple sum (or other indirect manipula-tions) of these companies X  individual maximum flows. For instance, company 1 may have excessive transporting ca-pability between two cities, whereas the same link happens to be a bottleneck for company 2: Considering both com-panies simultaneously for determination of the maximum flow can enable capacity sharing and thus create a double-win situation. In this sense, maximum flow is not distribu-tive by definition. However, as an obvious fact, maximum flow f is determined by the network c that shows link ca-pacities on its edges, and this capacity graph is distributi ve because it can be directly added upon generalization: When link sharing is enabled, two companies having two separate links from A to B with capacity c 1 from a single link with capacity c 1 flow calculation. Finally, being a function of distributive aggregated graphs, maximum flow is algebraic.
 I-holistic . The I-holistic case involves a more complex ag-gregate graph, where base level details are required to com-pute it. In the coauthor network of Example 1, the median of researchers X  collaboration frequency for all DB confer-ences from 1990 to 1999 is holistic, similar to what we saw in a traditional data cube.
Being (semi-)distributive or holistic tells us whether the aggregated graph computation needs to start from com-pletely un-aggregated data or some intermediate results ca n be leveraged. However, even if the aggregated graph is dis-tributive or algebraic, and thus we can calculate high level measures based on some intermediate level ones, it is far from enough, because the complexity to compute the two functions F Think about the maximum flow example we just mentioned, F the flows, which is by no means an easy transformation.
Based on our analysis, there are mainly two reasons for such potential difficulties. First, due to the interconnect ing nature of graphs, the computation of many graph properties is  X  X lobal X  as it requires us to take the whole network into consideration. In order to make this concept of globalness clear, let us first look at a  X  X ocal X  case: For I-OLAP, ag-gregated graphs are built for the same set of objects as the underlying network snapshots; now, in the aggregated graph of Example 1,  X  X . Agrawal X  and  X  X . Srikant X  X  X  collabora-tion frequency for cell (db-conf, 2004) is locally determined by  X  X . Agrawal X  and  X  X . Srikant X  X  X  collaboration frequency for each of the DB conferences held in 2004; it does not need any information from other authors to fulfill the com-putation. This is an ideal scenario, because the calculatio ns are localized and thus greatly simplified. Unfortunately, n ot all measures provide such local properties. For instance, i n order to calculate a maximum flow from S to T for the cell (air, all-companies) in Example 2, only knowing the trans-porting capability of each company X  X  direct flights between S and T is not enough, because we can always take an indi-rect route via some other cities to reach the destination.
Second, the purpose for us to compute high level aggre-gated graphs based on low level ones is to reuse the inter-mediate calculations that are already performed. However, when computing the aggregated graph of a low level cell , we only had a partial view about the c i work snapshots; now, as multiple pieces of information are overlaid into the high level cell c idation needs to be performed, which is very much like the merge sort procedure, where partial ranked lists are reused but somehow adjusted to form a full ranked list. Still, be-cause of the structural complexity, it is not an easy task to develop such reuse schemes for graphs, and even it is possi-ble, reasonably complicated operations might be involved.
Admitting the above difficulties, let us now investigate the possibility to alleviate them. As we have seen, for some aggregated graphs, the first aspect can be helped by their lo-calization properties with regard to network topology. Con-cerning the second aspect, the key is how to effectively reuse partial results computed for intermediate cells so th at the workload to obtain a full-scale measure is attenuated as much as possible. In the following, we are going to examine these two directions in sequel. 4.1 Localization Definition 7 (Localization). For an I-OLAP aggregated graph M G  X  = { G i a neighborhood of v in G node attributes in M borhood of u, v in G edge attributes in M be localizable .
 Example 4 (Common Friends). With regard to the coauthor network depicted in Example 1, we can also compute the following aggregated graph: Given two authors a the edge between them records the number of their common  X  X riends X , whereas in order to build such  X  X riendship X , the total collaboration frequency between a pass a  X 
The above example provides another instance that lever-ages localization to promote efficient processing. Con-sider a cell, e.g. , (db-conf, 2004) , the determination of the aggregated graph X  X  a neighborhood of these two authors in the un-aggregated snapshots of 2004 X  X  DB conferences, i.e. , we only need to check edges that are directly adjacent to either a in this way a third person a lishes with both a quencies summed from the weights of these adjacent edges are at least  X  definition is based on the number of length-2 paths like a a -a tion; now, if we further increase the path length to k , compu-tations can still be localized in a  X  k authors, i.e. , any relevant author on such paths of  X  X riend-ship X  must be reachable from either a steps. This can be seen as a situation that sits in the middle of Example 1 X  X   X  X bsolute locality X  (0-neighborhood) and maximum flow X  X   X  X bsolute globality X  (  X  -neighborhood).
There is an interesting note we want to put for the ab-solutely local distributive aggregated graph of Example 1. Actually, such a 0-neighborhood localization property de-generates the scenario to a very special case, where it is no longer necessary to assume the underlying data as a graph: For each pair of coauthors, we can construct a traditional cube showing their collaboration frequency  X  X LAPed X  with regard to venue and time , whose computation does not de-pend on anything else in the coauthor network. In this sense, we can treat the coauthor network as a virtual union of pair-wise collaboration activities, whereas Example 1 can indee d be thought as a traditional OLAP scenario disguised under its graph appearances, since the graph cube we defined is nothing different from a collection of pair-wise tradition al cubes. As a desirable side effect, this enables us to leverag e specialized technologies that are developed for tradition al OLAP, which in general could be more efficient. But after all, the case is special anyway, most graph measures would not hold such absolute localization, which is also the rea-son why traditional OLAP proves to be extremely restrictive when handling networked data. 4.2 Attenuation
In below, we are going to explain the idea of attenuation through examples, and the case we pick is maximum flow. In a word, the more partial results from intermediate cal-culations are utilized, the more we can decrease the cost of obtaining a full-scale aggregated graph .

To begin with, let us first review some basic concepts, cf. [6]. Given a directed graph G = ( V, E ) , c : V indicates a capacity for all pairs of vertices and E is pre-cisely the set of vertex pairs for which c &gt; 0 source node s and a destination node t , a flow in G function f : V such that, (i) f ( u, v ) =  X  f ( v, u ) : skew symmetry, (ii) v 6 = s/t, P u  X  V f ( u, v ) = 0 : flow conservation. Since most maximum flow algorithms work incrementally, there is an important lemma as follows.
 Lemma 1 Let f be a flow in G and let G graph, where residual means that the capacity function of G if
Note that, the + /  X  notation here means edge-by-edge addition/subtraction; and in summary, this lemma X  X  core idea is to look for a flow f  X  in G the current flow f in G .

For the graph OLAP context we consider, in order to compute the algebraic aggregated graph displaying max-imum flow, the function F graph c as its input; now, since capacity can be written as the sum of a flow and a residual graph: c = f + c decomposition provide us some hints to pull out the useful part f , instead of blindly taking c and starting from scratch?
Suppose that the capacity graph of cell (all-types, com-pany 1) is c 1 , where f c  X  f 1 denotes the corresponding residual graph. Likewise, we have c 2 , f out loss of generality, assume there are only these two com-panies whose transportation networks are overlaid into (all-types, all-companies) , which has a capacity of c = c 1 + c Claim 1 f if P
ROOF . Since f 1 and f 2 are restricted to the transportation networks of company 1 and company 2, respectively, f must be accommodated by the overall capacity c = c 1 + c 2 , even if link sharing is not enabled. After subtracting f the residual graph thus resulted is: A direct application of Lemma 1 finishes our proof.
As it is generally hard to localize maximum flow compu-tations with regard to network topology, the above property is important because it takes another route, which reuses partial results f from c 1 + c 2 to c 1 closer to the overall maximum flow f big portion of it, f fore we start an augmenting algorithm. Unfortunately, ther e are also cases where such attenuation properties are hard, i f not impossible, to develop.
 Example 5 (Centrality). Centrality is an important con-cept in social network analysis, which reflects how  X  X en-tral X  a particular node X  X  position is in a given network. One definition called betweenness centrality C path to model this: Let n est paths ( i.e. , equally short ones) between two nodes j k between j, k that go through i , with C for all node pairs: C  X  X tar X -shaped network, all shortest paths must pass the net -work center, which makes C ( | V | X  1)( | V | X  2) / 2 .

Only considering shortest paths is inevitably restrictive in many situations; and thus, information centrality C one step further by taking all paths into account. It models any path from j to k as a signal transmission, which has a channel noise proportional to its path length. For more de-tails, we refer the readers to [17], which has derived the fol -lowing formula based on information theoretic analysis: Le t A be a matrix, whose a strength between node i and node j ; define B = D  X  A + J where D is a diagonal matrix with D a matrix having all unit elements; perform an inverse opera-tion to get the centrality matrix C = B  X  1 , write its diagonal sum as T = P | V | the information centrality of node i is then equivalent to Now, with regard to the coauthor network described in Example 1, if we define the interaction strength between two authors as their total collaboration frequency for a set of network snapshots, then an aggregated graph M be defined, whose node i is associated with a node attribute C ( i ) equaling its information centrality.
 Claim 2 The computation of M in a level-by-level aggregation scheme.
 P
ROOF . As we can see, the core component of information centrality computation is a matrix inverse. Now, given two portions of network snapshots that are overlaid, the overal l centrality matrix is: ( D 1 + D 2 )  X  ( A 1 + A 2 ) + J  X  1 = ( B 1 + B 2  X  J ) From intermediate results, we know the centrality matrices C not help much to decrease the computation cost of inverting B 1 + B 2  X  J
When things like this happen, an alternative is to aban-don the exactness requirement and bound the answer within some range instead; as illustrated by the following section , this would become useful if the cube construction is subject to a set of constraints.
In above, we have focused on the computation of a full cube, i.e. , each cell in each cuboid is calculated and stored. In many cases, this is too costly in terms of both space and time, which might even be unnecessary if the users are not interested in obtaining all the information. Usually, user s may stick with an interestingness function I , indicating that only those cells above a particular threshold  X  make sense to them. Considering this, all cells c with I ( c )  X   X  comprise an iceberg cube, which represents a partial materialization of the cube X  X  interesting part.

Optimizations exist as to how such an iceberg cube can be calculated, i.e. , how to efficiently process the constraint of
I ( c )  X   X  during materialization, without generating a full cube at first. Two most important categories of constraints are anti-monotone and monotone . They can be  X  X ushed X  into the computation process as follows: In bottom-up com-putation, i.e. , high level cells are calculated first, followed by low level cells they cover, on one hand, if a high level cell c we know that no low level cell c isfy it, and thus the computation can be immediately termi-nated with c a high level cell c then we no longer need to perform checkings for any low level cells covered by c isfy it. Concerning top-down computation, the roles of anti -monotonicity and monotonicity are reversed accordingly.
It is easy to see that the anti-monotone and monotone properties depend on specific analysis of measures and in-terestingness functions. Here, since we are working with networked data, some graph theoretic studies need to be made. For example, regarding the maximum flow X  X  value amount of transportation a network can carry from S to T , | f |  X   X  | f | is anti-monotone, because the transporting ca-pability of one company must be smaller than that of all companies together (actually, as we showed in Section 4, the flow value of c c  X  X  that are covered by c than the normal anti-monotonicity defined between two in-dividual cells); also, regarding the diameter d of a graph, which is designated as the maximum shortest path length for all node pairs, d  X   X  links can only cut down the distance between two vertices. Certainly, having such properties derived will greatly hel p us in constructing an iceberg cube.
In this section, we present empirical studies evaluat-ing the effectiveness and efficiency of the proposed graph OLAP framework. It includes two kinds of datasets, one real dataset and one synthetic dataset. All experiments are done on a Microsoft Windows XP machine with a 3GHz Pentium IV CPU and 1GB main memory. Programs are compiled by Visual C++. 6.1 Real Dataset
The first dataset we use is the DBLP Bibliogra-phy ( http://www.informatik.uni-trier.de/  X  ley/db/ ) down-loaded in April 2008. Upon parsing the author field of papers, a coauthor network with multiple snapshots can be constructed, where an edge of weight w is added between two persons if they publish w papers together. We pick a few representative conferences for the following three re-search areas:  X  Database (DB): PODS/SIGMOD/VLDB/ICDE/EDBT,  X  Data Mining (DM): ICDM/SDM/KDD/PKDD,  X  Information Retrieval (IR): SIGIR/WWW/CIKM; and also distribute the publications into five-year bins: we obtain two informational dimensions: venue and time , on which I-OLAP operations can be performed.

Figure 3 shows a classic OLAP scenario. Based on the definition in Section 4, we compute the information central-ity of each node in the coauthor network and rank them from high to low. in general, people who not only publish a lot but also publish frequently with a big group of collaborator s will be ranked high. Along the venue dimension, we can see how the  X  X entral X  authors change across different research areas, while along the time dimension, we can see how the  X  X entral X  authors evolve over time. In fact, what Figure 3 gives is a multi-dimensional view of the graph cube X  X  base cuboid; without any difficulty, we can also aggregate DB, DM and IR into a broad Database field, or generalize the time dimension to all-years , and then compute respective cells. The results are omitted here due to the lack of space. 6.2 Synthetic Dataset
The second experiment we perform is used to demon-strate the effectiveness of the optimizations that are pro-posed to efficiently calculate a graph data cube. The test we pick is the computation of maximum flow as a graph OLAP measure, which has been used as an exemplifying application in above.
 Generator Mechanism . Since it is generally hard to get real flow data, we develop a synthetic generator by our-selves. The data is generated as follows: The graph has a source node s and a destination node t , and in between of them, there are L intermediate layers, with each layer con-taining H nodes. There is a link with infinite capacity from s to every node in layer 1, and likewise from every node in layer L to t . Other links are added from layer i to layer on a random basis: For the total number of H H choices between two layers, we pick  X H 2 pair of nodes and add a link with capacity 1 between them.

For the graph cube we construct, there are d dimen-sions, each dimension has card different values ( i.e. , car-dinality), which can be generalized to  X  X ll/* X . For a base generalized level, we generate a snapshot of capacity net-work L 5 H 1K  X  0 . 01 , i.e. , there are 5 layers of intermediate nodes, and 0 . 01 (1 K ) 2 = 10 K links are randomly added between neighboring layers.

The algorithm we use to compute the maximum flow works in an incremental manner. It randomly picks an aug-menting path from s to t until no such paths exist. To ac-commodate top-down computation, where high level cells are computed after low level cells so that intermediate re-sults can be utilized, we integrate our attenuation scheme with the classic Multi-Way aggregation method for cube computation [20].
 The results are depicted in Figure 4 and Figure 5, with Figure 4 fixing the cardinality as 2 and varying d from 2, 3, . . . up to 6, and Figure 5 fixing the number of dimensions seen that, the optimization achieved through attenuation i s obvious; especially, when the dimensionality goes high, on e may reap orders of magnitude savings.
OLAP (On-Line Analytical Processing) is an important notion in data mining, which has drawn a lot of attention from the research communities. Representative studies in-clude [9, 5], and a set of papers on materialized views and data warehouse implementations are collected in [10]. There have been a lot of works that deal with the efficient computation of a data cube, such as [20, 2], whereas the wealth of literature cannot be enumerated. However, all these researches target conventional spreadsheet data, i.e. , OLAP analysis is performed on independent data tuples that mathematically form a set. In contrast, as far as we know, ours is the first that puts graphs in a rigid multi-dimensiona l and multi-level framework, where due to the nature of the underlying data, an OLAP measure in general takes the form of an aggregated graph.

The classification of OLAP measures into distributive, algebraic and holistic was introduced in the traditional OLAP arena, where we can also find related works for ice-berg cubing [7], partial materialization [12] and constrai nt  X  X ushing X  [15]. It is important to see how these basic as-pects are dealt with in the graph OLAP scenario; and as we can see from the discussions, things become much more complicated due to the the increased structural complexity .
In graph OLAP, the aggregated graph can be thought as delivering a summarized view of the underlying networks based on some particular perspective and granularity. In th is sense, concerning the generation of summaries for graphs, there have been quite a few researches that are associated with terminologies like compression, summarization, sim-plification, etc. . For example, [16, 3] study the problem of compressing large graphs, especially Web graphs; however, they only focus on how the Web link information can be efficiently stored and easily manipulated to facilitate com -putations such as PageRank and authority vectors, which do not give any insight into the graph structures. Similarly , [4] develops statistical summaries that analyze simple gra ph characteristics like degree distributions and hop-plots. They are useful but hard to be navigated with regard to the un-derlying networks; also, the multi-dimensional functiona l-ity that can conduct analysis from different angles is miss-ing. Another group of papers [19, 1, 13], which we refer as graph simplification, aim to condense a large network by preserving its skeleton in terms of topological features . In this case, attributes on nodes and edges are not impor-tant, and the network is indeed an unlabeled one in its ab-stract form. Works on graph clustering (to partition simila r nodes together), dense subgraph detection (for community discovery, link spam identification, etc. ) and graph visu-alization include [14], [8] and [11], respectively. They al l provide some kind of summaries, but the objective and re-sult achieved are substantially different from those of thi s paper.

With regard to summarizing attributed networks that in-corporates OLAP-style functionalities, [18] is the closet to ours in spirit. It introduces an operation called SNAP (Summarization by grouping Nodes on Attributes and Pair-wise relationships), which merges nodes with identical la-bels, combines corresponding edges, and aggregates a sum-mary graph that displays relationships for such  X  X eneral-ized X  node groups. Users can choose different resolutions by a k -SNAP operation just like rolling-up and drilling-down in an OLAP environment. Essentially, the above set-ting showcases an instance of topological graph OLAP that is defined here. Though we did not emphasize T-OLAP in this paper, based on our analysis of the aggregated graph describing collaboration frequency, it seems that SNAP can also be categorized as locally distributive, even if the con -text is switched from I-OLAP to T-OLAP.
We examine the possibility to apply multi-dimensional and multi-level analysis on networked data, and develop a graph OLAP framework , which is classified into two ma-jor subcases: informational OLAP and topological OLAP , based on the different OLAP semantics. Due to the na-ture of the underlying data, an OLAP measure now takes the form of an aggregated graph . With regard to efficient implementation, because of the space limit, we focus on I-OLAP in this paper. We categorize aggregated graphs based on the difficulty to compute them in an OLAP context, and suggest two properties: localization and attenuation, whi ch may help speedup the processing. Both full materializa-tion and constrained partial materialization are discusse d. Experiments show insightful results on real datasets and demonstrate the efficiency of our proposed optimizations.
As for future works, there are a lot of directions we want to pursue on this topic, for example, extending the current framework to heterogenous-typed graphs, hyper-graphs, etc. , and our immediate target would be providing a thorough study on topological OLAP.

