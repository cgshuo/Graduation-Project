 1. Introduction
New standards in document representation have caused Information Retrieval (IR) to design and implement models and tools to index, retrieve and present documents according to the given document structure. While standard IR treats docu-ments as if they were atomic entities, modern IR needs to be able to deal with more elaborate document representations, such as for example documents written in SGML, HTML or XML. These document representation formalisms enable struc-tured documents (i.e. documents whose content is organised around a well-defined structure) to be represented and de-scribed. Examples of these documents are books and textbooks, scientific articles, technical manuals, etc. This means that documents should no longer be considered as atomic entities, but as aggregates of interrelated units that need to be indexed, retrieved, and presented both as a whole and separately, in relation to the user X  X  needs. In other words, given a query, an
Information Retrieval System (IRS) must retrieve the set of document components or structural units that are most relevant to this query, not just entire documents.

Within IR, the area of research dealing with structured documents is known as structured document retrieval . A good over-view of structured document retrieval can be found in Chiaramella (2001) and Lalmas (2009) . The inclusion of the structure of a document in the indexing and retrieval process affects the design and implementation of the IR system in many ways.
First of all, the indexing process must consider the structure in an appropriate way so that users can search the collection both by content and structure. Secondly, the retrieval process should use both structure and content when estimating the relevance of documents. Finally, the interface and the whole interaction must enable the user to make full use of the doc-ument structure. In fact, querying by content and structure can only be achieved if the user can specify in the query what he or she is looking for, and where this should be located in the required documents. The  X  X  X hat X  involves the specification of the content, while the  X  X  X here X  is related to the structure of the documents.

There are already many IR systems which are able to deal with structured documents and the series of INEX (INitiative for the Evaluation of XML Retrieval) Workshop proceedings ( Fuhr, Lalmas, &amp; Trotman, 2008; Fuhr, Lalmas, &amp; Trotman, 2007;
Lalmas, 2002 ) is an excellent source of information. It is, however, also true that in many cases these systems take as input non-structured queries involving only content (the so-called content-only , CO, queries), kind of document component.

What we propose in this paper is a general methodology to convert some of these systems into fully structured IR systems which are able to process structured queries involving both content and structure (the so-called CAS queries). Our starting point, therefore, is any information retrieval system which is capable of computing (given a non-structured query) a rele-vance value for each structural unit in a document, representing the posterior probability of relevance of that unit given the query. 2
In Section 2 of this paper, we explain the kind of structured query being dealt with. Section 3 describes our proposal for transforming a partially structured IRS into a fully structured one, by allowing the system to manage structured queries. Sec-tion 4 reviews some of the existing approaches for managing content and structure queries, whereas the experimental eval-uation of our proposal is explained in Section 5 . Finally, Section 6 contains our concluding remarks and various proposals for future research. 2. XML and structured queries
The eXtensible Markup Language , known as XML ( W3C, 2006 ), is a simple language which was initially designed to meet the challenges of electronic publishing. Nowadays, it also plays an increasingly relevant role in the exchange of a wide range of data via Internet. XML is a simplified version of the Standard Generalized Markup Language, SGML , which was originally designed for document definition, but too complex for the Web.

XML is in fact a meta-language as its main objective is to facilitate the creation of markup languages to represent struc-tured data 3 (this is one of the main differences with HTML, as XML is used to describe rather than show data, as HTML does).
The sets of rules which define a new language are usually expressed in a DTD (Document Type Definition) or XML Schema file. These files describe the elements comprising the documents derived from that DTD or Schema as well as their relation-ships, i.e. the internal structure of the documents.

Broadly speaking, an XML document could be viewed as an ordered, labeled tree. Each internal node of the tree is an ele-ment, including the root node, which may contain other element nodes or text nodes. The leaf nodes contains text associated to other elements. Each internal node in the conceptual tree is represented in the XML file with an opening tag plus a closing tag. Fig. 1 shows an e-mail represented in XML and its associated tree (where rectangles are tags and ellipses are texts). This tree-type representation is called Document Object Model, DOM , and offers a way of accessing and processing XML documents.

In addition to being a markup language, XML is also a technological framework where different technologies are devel-oped with the aim of offering useful services for the most frequent user X  X  requests. This is the case of XML Path Language,
XPath ( W3C, 2007 ): a language aimed to access parts of an XML document. XPath allows the selection and reference of texts and elements in an XML file.

For the purpose of this paper, we will review only the features of XPath relevant to our work. Therefore, a name of an ele-that any numberof elements could be included inthe path ( A // B , where B is a descendant,though not necessarily a direct one of i.e. with an arbitrary number of elements in between: email/from/address and email/to/address . A slash at the beginning of the expression means that the path starts at the root element (/ A ). An asterisk Pantuza, &amp; Ziviani, 2005 ) means that the XPath expression engine would select the third B element child of A .
At this point, we must distinguish between the two views of the XML content ( Manning, Raghavan, &amp; Sch X tze, 2008 ): data-and text-centric XML. On the one hand, a data-centric XML document is seen as a container for data, usually non-text data (worker profiles, invoices, flight schedules, etc.). The document meaning depends on the structured data contained, and presents a regular and complex structure and homogeneous content. When the user formulates queries for such a type of XML document, he or she is interested in an exact match, such as a database-type style. On the other hand, text-centric
XML documents usually represent text documents (books, e-mails, etc.), and the structure is more irregular and the data het-erogeneous. The queries, although taking the structure into account, focus on the text and require ranking. The aim of text-centric XML retrieval is therefore to develop methods to find correspondence between the text of the query and the text of the XML documents (but also considering structural restrictions) and this is the context in which this paper is set.
In order to allow queries combining content and structure (Content And Structure Queries, CAS, in INEX terminology) to be specified, the NEXI language ( Trotman &amp; Sigurbj X rnsson, 2005 ) was designed. It is a simplified XPath containing only the descendant operator (//) in a tag path and also an extended XPath containing the about function. NEXI has been used by INEX since 2004.

The kind of structured CAS query considered by NEXI can take two possible forms: // C [ D ]: Returns C units that fulfill the condition D . // A [ B ]// C [ D ]: Returns C descendants of A where A fulfills the condition B and C fulfills the condition D .
A and C are paths (sequences of elements or structural units), specifying structural restrictions, whereas B and D are filters , which specify content restrictions, and // is the descendant operator. C is the target path (the last structural unit in C is the one that we want to retrieve) and path A is the context . Each content restriction will include one or several about clauses, connected by either and or or operators; each about clause contains a text (a sequence of words or terms) together with a relative path, from the structural unit which is the container of the clause to the structural unit contained in it where this text should be located. The about clause is the IR counterpart of the classical contains clause used in XPath (which requires an exact matching between the textual content of the clause and a part of the text in the structural element being evaluated).
However, about does not demand such a strict matching but states, vaguely, that a relevant element should satisfy the infor-mation need expressed by means of the text contained in the clause.
 Example 1. Let us suppose that the hierarchical structure (e.g. the XML tree) of a document collection is the one displayed in
Fig. 2 . An example of a NEXI-structured query is the following:
What we want to retrieve with this query are D units which are contained within A units. The target D units should speak about text 1 and contain an N unit speaking about text 5; the context A units should be about text 2 and also contain F and J units dealing with text 3 and text 4, respectively (see Fig. 3 ). 3. Managing structured queries
In this section, we will explain how a structured CAS query of the type considered in Section 2 can be managed using an IR system able to process only CO queries.

Each about clause which is part of the structured query will give rise to a (non-structured) subquery. This subquery will be used to compute the posterior probabilities of the structural units specified within the clause, by postprocessing the out-put of the base system (removing the units which do not satisfy the structural conditions specified in the about clause and in the path). 4
Fig. 4 shows the modules required for a probabilistic retrieval system in order to deal with structured queries. A first mod-ule, a NEXI Query Processor , is shown and this extracts the content subqueries from the NEXI query (those which occur in the about clauses). These are passed to the XML retrieval system. It then runs a retrieval and obtains a ranking of relevant ele-ments for each subquery. With these rankings plus the original NEXI query, the last module, the Output Processor , filters the results that do not satisfy the structural restrictions for each subquery, selects the objective elements and computes an RSV (relevance status value) for each element, returning a sorted list of units satisfying the initial query.
 Let us consider the following example (simple query of type // A [ B ]): Example 2.

This query, in the context of a book collection, searches for chapter units with a title about text 1 and containing a section about text 2. This is an example of the first, simpler type of query supported by NEXI.

In this example, the posterior probabilities computed for the subquery text1 only focus on the title units which are con-tained in chapter units; the posterior probabilities for the subquery text2 are also restricted to those section units contained within chapter units. In this way, structural units associated to paths such as, for example, /book/chapter/section , /book/chap-ter/section/paragraph or /book/chapter/author would be discarded for the first subquery, whereas structural units associated to paths such as /book/chapter/section/paragraph , /book/chapter/title or /book/chapter/author would also be eliminated for the sec-ond subquery.

Once we have discarded these units (thus keeping only the probabilities p  X  X  == chapter == title  X  clause(s) (in the example this is the probability of the chapter units, p  X  X  == chapter  X  1. computing the aggregated probability of all the structural units of the same type associated to the corresponding unit (in the case of the container unit having more than one of these units) and then; 2. combining these aggregated probabilities for the different structural units specified in the different about clauses.
In the example, we need to compute the aggregated probability of the section units (step 1), p  X  X  == chapter  X  (because, in this case, a chapter may contain more than one section), whereas the probability of the title unit, p  X  X  == chapter  X  l == title j text 1  X  , does not change, because a chapter will only have a single title. tion units and title units will then be associated to their container chapter units (step 2), p  X  X  == chapter  X 
In order to satisfy the about clauses, it is sufficient for one section to be relevant (and also the corresponding title of the chapter), and it is not necessary for many or all of the sections of a chapter to be relevant. Therefore, to aggregate the prob-abilities of the structural units of the same type (when the container unit can have more than one of these units), it is appro-priate to use a combination model representing a disjunction. However, to combine the probabilities of different types of units inside the container unit, this will depend on the type of connective being considered, either conjunction ( X  X  X nd X ) or the probability of the chapter would be obtained as where AND and OR should be understood as operators for aggregating probabilities (instead of the boolean operators).
This process will compute the posterior probabilities of the structural units containing the about clauses specified in the query, for both the context ( A ) and the target ( C ), but these probabilities must also be combined to obtain the final proba-not any context for this unit. Let us, however, examine another, more complex example (query of type // A [ B ]// C [ D ]): Example 3.

This query attempts to retrieve bibliography units containing text 3, which must be in chapter units with a text 1-related title and contain a section about text 2. The bibliography units are the target and the chapter units are the context.
In this case, in addition to computing the posterior probabilities of chapter units, p  X  X  == chapter  X  example, we would similarly compute the posterior probabilities of bibliography units in chapter units given the query
D = text 3, p  X  X  == chapter == bibliography  X  l j D  X  . It is then necessary to combine p  X  X  == chapter  X  query, p  X  X  == chapter == bibliography  X  l j Q  X  . This combination clearly must also be conjunctive. 3.1. Combining probabilities In the general case, let us consider a query Q = //A[B]//C[D], i.e. Q  X  Q Since this query gives rise to n + m subqueries, we use the base IRS to compute the probabilities p  X  : j text p  X  : j text Cj  X  ; i  X  1 ; ... ; n ; j  X  1 ; ... ; m , for all the structural units.

First, we filter the results of these queries to focus on the structural units of interest, namely p  X  X  == A == A about clause in the context and p  X  X  == A == C == C j  X  l
Secondly, these probabilities are used to obtain the aggregated probabilities of the structural units of the same type, p  X  X  == A  X  l == A i j text Ai  X  for the context and p  X  X  X  == A  X 
Thirdly, we combine these aggregated probabilities for the different structural units specified in the different about clauses, in order to obtain the probabilities for the content and the target, p  X  X  == A  X  where the operator OP may be either conjunction (AND) or disjunction (OR).

The final probability of the desired structural units, p  X  X  X  == A  X  3.2. Probabilities of disjunctions and conjunctions
In order to manage disjunctive and conjunctive aggregations in a probabilistic setting, a simple and reasonable option is to use (noisy) OR and (noisy) AND gates. Let U 1 ; ... ; U
OR i  X  1  X  p  X  U i  X  X  and the conjunction AND 3.2.1. Noisy-OR gates A noisy OR gate is defined by means of ( Pearl, 1988 ): where w o  X  U i  X  , with 0 6 w o  X  U i  X  6 1, is a weight representing the probability that the disjunction is true if U the probabilistic sum t -conorm ( Schweizer &amp; Sklar, 1983 ). 3.2.2. Noisy-AND gates A noisy AND gate is defined by means of ( Pearl, 1988 ): where w a  X  U i  X  , with 0 6 w a  X  U i  X  6 1, in this case is the probability that the conjunction is false if U tion may be reduced to the product of the individual probabilities (which is equivalent to using the product t -norm, Schwe-izer &amp; Sklar, 1983 ).
 3.3. Implementation details The previous proposed methodology has been implemented by means of several programs which are external to the base XML information retrieval system being considered.

The first program, the query processor, parses the NEXI query and extracts the different content-only subqueries. These are passed to the XML retrieval system, which runs a retrieval for each subquery and obtains a ranking of relevant elements. These ranked outputs are stored in (text) files, containing the document identifier, the XPath and the RSV of each element.
These files are then processed by another program, the output processor, which in turn calls (according to the structure of the original query) three filters implemented as perl scripts.

The first script removes the results that do not satisfy the structural restrictions for each subquery and computes the aggregated probabilities of the structural units of the same type, using an OR gate in Eq. (1) ; this filter is executed once for each subquery. A second script then takes the output files of the first script and computes the probabilities of the context the context. The third script computes the final probabilities by combining the probabilities of the context and the target by using an AND gate in Eq. (3) . 4. Related work
In this section, we would like to review various techniques for answering CAS queries found in the specific literature, mainly publications derived from the INEX workshops. We do not wish to make an exhaustive analysis but merely to show the main types of CAS resolution methods and, for those techniques more similar to our proposal, to present the main dif-ferences with them. We should also mention that this brief study starts in 2004, the year when INEX made the decision to use NEXI as the language to specify CAS queries. In INEX 2002 and 2003, although this type of query existed in the official tasks, another specification was used.

We have observed two types of approaches in the specialized bibliography for solving structured queries: those in which the retrieval of XML elements given a CAS query is integrated in the XML retrieval model, and those in which CAS queries are managed as an additional, differentiated layer on top of the XML retrieval model. Let us start with the second group as our approach also belongs to it.

Studying several different papers describing methods where CAS queries resolution was made on top of a retrieval model, we can easily appreciate how CAS queries are solved in a similar way: content queries of the about clauses are launched to the retrieval systems. Once the rankings are generated, their elements are filtered and aggregated in some way, according to the structural restrictions. The selection of the final elements is made from those which, while satisfying the target path, are also in the ranking of elements fulfilling the context path. In some cases, the resolution method is also supported by the exis-tence and use of several indexes containing information about different XML elements.
 Some examples following the previously mentioned CAS queries resolution method are the following.

In Vittaut, Piwowarski, and Gallinari (2005) , the authors design a general algebra to express NEXI queries as probabilistic events, which can then be evaluated using a Bayesian network model as the underlying XML retrieval model.

The base of the paper ( Larson, 2005 ) is a Logistic Regression and Okapi BM-25 algorithm implementation for XML retrie-val (plus a combination technique) under several indexes. CAS queries are solved by analyzing the NEXI expression and deciding which indexes to use in the search as well as restricting elements that do not fit the structural restrictions. Each content query contained in the about clauses is run and the results are combined.

Crouch,Mahajan,andBellamkonda(2005) presentanextensionoftheVectorSpaceModel(VSM)forXMLdocuments.Inthis case, each document is seen as a set of subvectors, each corresponding to some specific type of element. The content queries of each about clause are run through the targeted subvector. Later, results are merged and the target elements are returned.
In Geva (2005) , the content of the leaf elements is indexed and additional information is stored (paths from the roots, etc.) in a database. The score of each leaf element is computed with a very simple formula, that takes into account the terms in each about clause, and propagated through the remaining nodes in the XML documents. Elements that satisfy the context and target parts of the NEXI query are selected and their scores added. In this line of relevance propagation, and representing the XML documents as trees, in Sauvagnat and Boughanem (2005) , XML retrieval is seen as a relevance propagation process in the tree starting from leaf nodes. A NEXI query is decomposed into subqueries which are then further decomposed into elementary subqueries, and these are propagated first in the tree. The relevance values obtained are aggregated to obtain scores for the subqueries, and in turn these are aggregated to obtain the score of the original query.

Based on the system presented in Geva (2005) as the underlying XML retrieval engine, in van Zwol (2006) , a CAS evalu-ation method is shown whereby the original NEXI query is divided into the context and target subqueries: rankings are ob-tained for each content query from an about clause; each ranking is filtered to match the path constraints from each about clause; projections of score values to the elements at the end of the paths are computed, and the different rankings within a filter are then combined. At the end of these processes, there are two sets of elements: those satisfying the context query and those satisfying the target query. The output are only those nodes of the target query with a matching ancestor in the context query.

With a multinomial language model as a retrieval model and three indexes (structure, article level, and element level) as the physical representation, in the approach presented in Sigurbj X rnsson, Kamps, and de Rijke (2004), Sigurbj X rnsson,
Kamps, and de Rijke (2005) , the NEXI query is decomposed into pairs (path, content query). Each subquery, constraining dif-ferent parts of a document, is run and a ranking of suitable elements obtained. Finally, the rankings must be mixed in order to obtain a single sorted list of elements fulfilling the target path, with an associated score.

Our approach could be easily classified in this category since it is completely independent of the underlying XML retrieval model. We could say that the overall process of the evaluation of NEXI queries presented in this paper is similar to those presented in Sigurbj X rnsson et al. (2004) and van Zwol (2006) . Broadly speaking, the main differences are the combination technique and the selection of the final elements, which in our case are based on a probabilistic approach based on noisy gates. More precisely, the method considered in van Zwol (2006) uses combinatory logic and min X  X ax normalization to combine the different rankings, and the final elements selected are only those target elements that have a matching ancestor in the set of elements obtained for the context query (whereas in our case all the preselected target elements are returned, although those ones whose ancestors do not match with elements in the context query become penalized). In the method proposed in Sigurbj X rnsson et al. (2004) , the mixture of the results of the subqueries also uses different operators (maximum and sum). Moreover, the context subqueries contribute to the final RSV of a target element in the same way that the target subqueries. Finally, different ways of decomposing the CAS query into a set of CO subqueries are also proposed in Sig-urbj X rnsson et al. (2004) .
 Now, let us focus on the CAS queries resolution approach where the mechanism of resolution is totally integrated in the XML retrieval model, i.e. given a query the score function usually incorporates some XML features to deal with CAS queries. Broadly speaking, in the specialized literature, we observe how modifications of the Vector Space Model and integration of XML in relational databases are the predominant research lines.

With respect to the first line, the main changes in the VSM are the incorporation of XML features to the classic cosine ranking function, the use of different indexes to store the information provided by the different XML elements and the use of additional tree representations, combining scores obtained with the VSM with tree matching. Examples of this cate-gory are the following.

In Azevedo et al. (2005) , the authors modify the VSM including new XML specific features: the nesting nature of the XML elements and the fact that each element could be retrieved. The authors design a new ranking function similar to the cosine measure for flat documents. To deal with CAS queries, a factor is included to measure how the query structural constraints are satisfied by the paths of the elements. The paper ( Mass &amp; Mandelbrod, 2005 ) presents a model based on indexing the different types of elements in separate indexes. Based on the VSM, the query is run on each index and the different rankings are merged. For CAS queries, the content query is run against the article index to locate candidates that fulfill the query con-straints. In a second step, each part of the query is submitted in parallel to each one of the remaining indexes. A relevance value is computed only for those valid elements from the first step. A third extension is found in Weigel, Schulz, and Meuss (2005) . This model is adapted to represent the document structure and enhanced with a tree representation of the queries and documents. NEXI queries are solved by computing content scores using the VSM and making tree matching operations to satisfy the structural restrictions.

Various examples joining XML retrieval with relational databases may also be found. The definition of new query lan-guages adapted to cope with structural restrictions or the mapping of structural queries to SQL are the most common ap-proaches to solve CAS queries by means of DBMSs. A couple of representatives of these two methodologies are ( Mohan &amp;
Sengupta, 2005; Shimizu, Terada, &amp; Yoshikawa, 2007 ). In the former, a DBMS is adapted to work with XML documents, defining a query language, based on SQL, called DSQL to work with structured documents. Structural restrictions are very easy to deal with as they are naturally included in the DSQL language itself. In the latter ( Shimizu et al., 2007 ), CAS queries are directly mapped to SQL queries. In addition, a score function is introduced in order to produce a ranking of relevant elements.

Other methods where relational databases are used are ( Theobald, Broschart, Schenkel, Solomon, &amp; Weikum, 2007 ), where index structures are implemented in a relational database, and query resolution is based on a combination of com-puting content and structural scores; and ( Pehcevski, Thom, Tahaghoghi, &amp; Vercoustre, 2005 ), as a case combining a text search engine and a native XML database.

A third main line to resolve CAS queries is based on representing the XML documents by means of trees: Arvola, Kek X l X i-nen, and Junkkari (2006) show a method based on structural indexes (represented as trees) and element weight computa-tions which takes into account the query and the context of each element. Element weights are calculated according to the content query from each about clause and those with weights greater than 0 are selected. Elements that do not fulfill the corresponding structural restriction are then disregarded. These two steps are repeated until the whole NEXI query is pro-cessed, applying the final target structural restrictions to the remaining elements. Another tree-related approach is that pre-sented in Ali et al. (2007) which uses a combination of structural summaries, labeled trees representing the XML hierarchical structure of documents and queries, and the BM25 retrieval model. Broadly speaking, the former is used to select document components satisfying the structural restrictions of the NEXI query, and the latter to assign a relevance score and therefore to rank them. 5. Experimental evaluation
In this paper, firstly we will use Garnata as our base system in the experiments. This is a structured information retrieval system based on probabilistic graphical models, namely Bayesian networks and influence diagrams, but so far it is only able to process non-structured queries. A description of this system can be found in de Campos, Fern X ndez-Luna, Huete, and
Romero (2006) , whereas its theoretical basis is explained in more detail in de Campos, Fern X ndez-Luna, and Huete (2004), de Campos, Fern X ndez-Luna, and Huete (2005) . Garnata has been tested at three editions of the INEX Workshop ( de Campos, Fern X ndez-Luna, Huete, Mart X n-Dancausa, &amp; Romero, 2008; de Campos, Fern X ndez-Luna, Huete, Mart X n-Dan-causa, &amp; Romero, 2009; de Campos, Fern X ndez-Luna, Huete, &amp; Romero, 2007 ) and has also been applied to build a real IRS for parliamentary documents ( de Campos, Fern X ndez-Luna, Huete, Mart X n, &amp; Romero, 2008; de Campos et al., 2009 ). The first XML document collection considered is the one used in the last three editions of the INEX Workshop, namely
Wikipedia (an XML version of the English Wikipedia), at the beginning of 2006 ( Denoyer &amp; Gallinari, 2006 ) with its 659,388 articles (and around 4,600 megabytes in size). In terms of the queries (and the corresponding relevance judgments) used to test our proposal, we have selected the set of queries developed for INEX X 2006, not, however, employed all the original queries which have relevance judgments used in these editions of INEX, but a subset containing 90 queries (34 from 2006, 36 from 2007 and 20 from 2008). For the sake of completeness, the list of these CAS que-ries is displayed in Tables 8 X 10 in the Appendix.

We have not used either queries which are formally equivalent to content-only queries (i.e. queries of type //*[about(.,-text)]) or the majority of queries that while not strictly equivalent to content-only queries are in fact equivalent if we con-we have used only those queries where the relevance judgments are coherent with the structural restrictions imposed by the
CAS queries. In a few cases, we have modified the original CAS query to reach greater coherence between structural restrictions and the narrative of the query. For example, the query //section[about(.,pyramids of egypt)//image[about(.,pyramids)], looks for image elements contained in section elements, whereas the narrative of this query establishes that section elements containing images are looked for. We have then reformulated the query as //section[about(.,pyramids of egypt) and about(.// (figure j image),pyramids)].

In order to obtain the corresponding CO query for each CAS query, we removed all the structural components of the CAS query and kept only the content words. We then ran the Garnata retrieval system using the CO queries (base-CO) and the augmented system using the CAS queries, and compared the results. The weights of the noisy OR and noisy AND gates were fixed (without previous tuning) to constant values, 11 w o The measures of retrieval effectiveness are those used in the focused task of the INEX X 2007 and 2008 ad hoc track ( Kamps, iP[0.05] and iP[0.10]) and the average interpolated precision (AiP), all of them averaged across the 90 queries. In the focused task the system must return a ranked list of the most focused document components and the resulting document parts should not overlap. The criterion used to decide, when we find two overlapping components in the raw output generated by Garnata, which to preserve in the final output, is to keep the component having the greatest relevance value and, in case of tie, we keep the more general component (the one containing a larger amount of text) ( de Campos et al., 2008 ).
The results of our experiments are summarized in Table 1 , which displays the corresponding performance measures for the base-CO and the augmented system, as well as the percentage of improvement achieved by the proposed system and the (significant or very significant difference) we denote this by using  X  X   X  X r X  X   X , respectively. In Fig. 5 we also display the re-call-precision curves for the 101 recall levels.

The results are quite conclusive: the proposed method for managing CAS queries systematically improves the results of the system using CO queries in relation to all the performance measures. Moreover, the improvements achieved are statis-tically significant, ranging from a minimum of 15% to a maximum of 50%. We can also observe that the results are much better for lower recall levels. In fact, in Fig. 5 we can observe that the precision values of the augmented system are always better than those of the base-CO system for all the recall levels until the value 0.4; next, they become worse from 0.41 to 0.76 and again become better from 0.77 to 1.0. This means that adequately processing structured queries seems to concentrate highly relevant document parts in the first positions in the ranking of the results. Consequently, this technique greatly im-proves the precision and does not significantly deteriorate the recall (which is the initial intuition motivating the use of CAS queries).

In order to see whether the improvement obtained is consistent across the three different sets of queries considered, we display in Table 2 the performance measures broken down by year. We can observe a behaviour more or less similar across the three years, with the exception of the average interpolated precision, where we get important improvements for the years 2007 and 2008 but a small (not significant) worsening for 2006. This confirms that our method is very effective at lower recall levels but more unstable on the average.

In order to study whether our performance improvement is due to simple target restrictions or to the handling of the more complex structural requirements (because the base-CO system can return any element from the collection but our ap-proach can only return elements having the same type as specified by the user), we are going to compare also with another additional baseline (which we will call base-CAS) which uses the same CO queries as the base-CO system, but additionally filters away all element that do not match the target element constraint. So, given a CAS query //A[B]//C[D], whereas the base-CO system transforms this query into //*[about(.,content words in B and D)], the base-CAS system transforms it as //A//C[about(.,content words in B and D)]. Table 3 and Fig. 6 display the results of the comparison between our augmented system and the base-CAS system being considered.

We can see that the augmented system still performs significantly better than the base-CAS system with respect to all the performance measures, although in this case the differences are lesser than in the previous case (except with the average interpolated precision). Therefore, we can obtain two conclusions: first, even a simple processing of the CAS queries can lead to improved performance with respect to using only CO queries; second, the proposed method improves further the results of this baseline treatment of CAS queries.

For the sake of completeness, detailed results for individual queries, for the iP[0.01] and AiP measures (which were the  X  X  X fficial X  measures at INEX, Kamps et al. (2008) ) of the augmented CAS, the base-CO and the base-CAS, are displayed in Ta-bles 11 X 13 in the Appendix. Table 14 shows the number of queries where our approach is either better, worse or equal than base-CO and base-CAS.

We want also to compare our proposal with another, state-of-the-art method for managing CAS queries. To this end, we have selected and implemented on top of the Garnata system the  X  X  X ull propagation X  method proposed by Sigurbj X rnsson puter Society collection. The results of this comparison are displayed in Table 4 and Fig. 7 . We can observe significant differences in performance between the two methods, favouring again our proposal. In fact the
SKR method performs rather poorly in our experimental setting: it behaves worse than the base-CAS except in average inter-polated precision, and scarcely outperforms the base-CO (again with the exception of the average interpolated precision).
In order to gain insight if there is a direct relationship between base and augmented system performance, we have also carried out experiments with another base system different from Garnata. We have selected the freely available PF/Tijah sys-tem 13 ( Hiemstra, Rode, van Os, &amp; Flokstra, 2006; Westerveld et al., 2007 ), part of the MonetDB/XQuery database system.
Although PF/Tijah supports several retrieval models, we used the default language model. We have then repeated all the pre-vious experiments but using PF/Tijah instead of Garnata. The results are summarized in Table 5 . We can see that the augmented base-CO and the SKR systems. Therefore, although the behaviour of the proposed method may be somewhat different depend-ing on the base system being used, the trend is to improve the results of the base system.

Another interesting question to test is whether the proposed method for managing structured queries is also able to show consistent improvement using different document collections. To this end we have selected the IEEE Computer Society col-full-texts, marked up in XML, of 12,107 articles of the IEEE Computer Society X  X  publications from 12 magazines and 6 trans-actions, covering the period of 1995 X 2002, and totalling 494 megabytes in size. In 2005 the collection was extended with new articles from the period 2002 X 2004, giving a total of 16,819 articles and 764 megabytes in size. We have used a subset of 22 CAS queries from INEX X 2003, 14 19 from INEX X 2004 15 Appendix, in Tables 15 X 17 ).

In the first four editions of INEX (previous to 2006), the relevance judgments were done in a different way, using a two-dimensional scale taking into account exhaustivity and specificity, which were mapped to a single relevance scale by employing several quantization functions. We have used the so-called strict quantization , which focuses on components rated as highly exhaustive and highly specific (and whose exact definition varies slightly across years, Kazai &amp; Lalmas (2006),
Malik, Lalmas, &amp; Fuhr (2005) ), as done also in Kamps, Marx, de Rijke, and Sigurbj X rnsson (2006), Sigurbj X rnsson et al. (2004), and van Zwol (2006) .

The measures of retrieval effectiveness are those used in INEX X 2005, namely the (system-oriented) non-interpolated mean average effort-precision (MAep) and the (user-oriented) normalized extended cumulated gain (nxCG) at various fixed ranks (nxCG[10], nxCG[25] and nxCG[50]) ( Kazai &amp; Lalmas, 2006 ), averaged across the 55 queries. These measures have been computed using the EvalJ evaluation package, more specifically using the XCGEval package. with Wikipedia, we still consider a focused task where overlap is not permitted (hence the raw results obtained by the IRS are filtered to remove overlapping components). Therefore, we always use the option ble 6 displays the results of these experiments, where the base IR system used has been Garnata. The augmented system once again gets the best results, although the differences with the other systems are somewhat lesser than in the case of the
Wikipedia collection, specially with respect to the base-CAS system. These differences are not statistically significant in many cases (although in some cases this may be due in part to the less number of queries considered, 55 instead of 90).
Finally, in order to determine the sensitivity of the proposed model to the parameters considered for the AND and OR gates, w a and w o , we have carried out another series of experiments varying these parameters. We have used in this case the Wikipedia collection and the Garnata IR system. The results are displayed in Table 7 .

The first fact that can be observed in Table 7 is that the values of the weights w ence on the results. Performance deteriorates systematically as the weight of the AND gate decreases, excluding the weight w a  X  1 : 0 (pure AND gate), which produces disastrous results. Therefore, a very high weight different from 1.0 for the AND gate, like 0.999 gives the best results. The value of the weight for the OR gate, w a lesser extent. The weight w o  X  1 : 0 (pure OR gate) gives almost the best results, together with w selection of weights ( w o  X  1 : 0 and w a  X  0 : 999) was, fortunately, a good choice.

In general, our results are considerably better than certain previous results reported in the literature by other research-ers ( Sigurbj X rnsson et al., 2004; Trotman &amp; Lalmas, 2006; Woodley, Geva, &amp; Edwards, 2006 ), who did not find any signif-icant improvement (or even no improvement at all) when managing CAS queries instead of CO queries. Our results are more in agreement with those in Kamps et al. (2006) , who reported improvements in early precision, although we have also obtained good results in average measures. There is no simple explanation for this difference. The naive answer would be to say that some of these previous methods were not able to process structured queries effectively, but probably there are other reasons, such as the different document collections: our results are better for Wikipedia than for IEEE, which was the collection used to test these methods. This suggests that the effectiveness in managing CAS queries may depend on the type of collection, queries and performance measures being used. Another difference of our experimental setting with re-spect to previous studies is the type of task considered, focused in our case and thorough in the other cases. Even the base systems considered are also different. Therefore, although the proposed method has demonstrated its effectiveness, we believe that more research efforts should be invested in order to clarify when and how managing CAS queries is worthwhile. 6. Concluding remarks
In this paper, we have proposed a general methodology for managing structured queries within partially structured IRS able to only deal with content-only queries. Our method can be applied to any probabilistic IRS without the need to modify the system or interact with it in a complex way.

We have demonstrated the effectiveness of our approach by using two specific structured IRS and two benchmark XML document collections as the test bed. The experimental results confirm that a proper management of the structure present in CAS queries can significantly improve the retrieval capabilities of the system.

Our view of CAS queries in this work has been rather strict in the sense that the target-path constraints present in the query must be upheld for a result to be relevant. If a user asks (for example) for section units to be returned, these must be returned (although other aspects of the query are interpreted from the IR perspective, i.e. loosely). There is, however, an-other less strict, vague interpretation of a CAS query ( Trotman &amp; Sigurbj X rnsson, 2005 ), where target-path requirements need not be fulfilled, and the path specifications should therefore be considered hints as to where to look. For future work, we plan to modify our method to deal with this vague interpretation of CAS queries. Another interesting future research could be to try to take advantage of the flexibility of the noisy-OR/AND used by our approach to improve further the results, by studying how to assign the weights w  X  U i  X  used in these noisy gates in a non-uniform way, depending for example on the type of query being processed or on the type of element being returned.
 Acknowledgments
This work has been jointly supported by the Spanish  X  X onsejer X   X  a de Innovaci X n, Ciencia y Empresa de la Junta de Anda-luc X   X  a X ,  X  X inisterio de Ciencia e Innovaci X n X  and the research programme Consolider Ingenio 2010, under Projects TIC-04526, TIN2008-06566-C04-01 and CSD2007-00018, respectively.
 Appendix A.
 See Tables 8 X 17 .
 References
