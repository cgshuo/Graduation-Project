 The mismatch of terms between a user X  X  query and the potential relevant documents is a common problem in information retrieval. There are many reasons for vocabu-lary mismatch such as synonymy, polysemy and word inflections due to morpholog-ical processes. Synonymy may result in degradation of recall while polysemy may hurt the retrieval precision. Relevance feedback is a useful technique which involves the user directly in the search process and allows him/her to modify the query until his/her information need is satisfied. However, such a manual process of refining a query is a laborious task and most users prefer less manual intervention during the search process.

Automatic query expansion (or AQE) [Carpineto and Romano 2012] is an effective technique which completely reduces this manual effort by finding potentially useful terms and adding them to the original query. Typically, an AQE method assumes that a fixed number of top ranked documents are relevant to the query and extracts a set of potentially useful terms from those documents and adds them to the query, which is then used to retrieve the final set of documents.

A large number of AQE methods have been developed over the decades. Some AQE X  X  focus on finding good expansion terms, while the other methods focus on finding good quality feedback documents. A wide variety of term selection and weighting methods exist in the literature. Most methods are based on the assumption that the terms with little informative content will have the same (random) distribution in any subset of the collection, whereas the terms that are most closely related to the query will have a com-paratively higher probability of occurrence in the relevant documents. Following this general paradigm, various functions have been proposed that assign high scores to the terms that best discriminate relevant from nonrelevant documents. Chi-square statis-tics [Doszkocs 1982] and Robertson selection value [Robertson 1991] compute weight of the terms by measuring the deviation of the term distribution in the top returned doc-uments from the entire collection. In the same spirit, Carpineto et al. [2001] describe a method that measures the difference in term distribution (using KL divergence) be-tween the pseudo relevant documents and the whole collection, and then the terms having higher scores are added to the query along with KL divergence score as the weight of the term. Binary independence model (BIM) [Robertson and Sparck Jones 1988] uses similar variables, however the actual term weight is computed by taking the ratio between the term X  X  relevance odds and the nonrelevance odds. In the vec-tor space framework, Rocchio X  X  method [Salton 1971] for relevance feedback assigns a score to each term in the top retrieved documents by a weighting function applied to the whole collection. Slightly different from these approaches, some other methods go beyond the distribution based approach and use the association between the query words and a term either in the whole corpus (global) [Xu and Croft 1996] or in the top ranked documents (local) [Xu and Croft 2000]. More recent methods consider the posi-tion of terms from the query terms in order to reduce the noisy terms in the expanded query [Cao et al. 2008; Lv and Zhai 2010; Miao et al. 2012].

A well-known problem in a typical pseudo relevance feedback algorithm is its in-stability with respect to the variation of two major parameters, namely, the number of feedback documents and the number of terms [Carpineto and Romano 2012]. Research has shown that the average performance fluctuates wildly when these parameters are varied [Billerbeck and Zobel 2004]. For a broad topic (that is, when the number of relevant documents is large), a relatively large number of feedback documents is less likely to lead to query drift [Mitra et al. 1998], while for a query having a small num-ber of relevant documents in the collection might be susceptible to query drift due to a large number of feedback documents. However, selecting the number of feedback doc-uments in a query dependent way seems to be a difficult task and therefore, this poses an important challenge to an automatic relevance feedback algorithm. In this article, we propose a novel algorithm based on an aggregated incremental document and term selection policy to address this problem. This article makes the following contributions.  X  It proposes a novel automatic query expansion algorithm which selects and weights the terms by incrementally selecting feedback documents from top retrieved set and then aggregates the weights computed from each feedback set using various types of mean and a modified form of the arithmetic mean.  X  It devises a term selection measure for selecting expansion candidates from top retrieved documents.  X  It presents a number of expansion term weighting schemes based on easily computable term features.
  X  It experimentally demonstrates that the proposed incremental blind feedback algo-rithm significantly outperforms two strong baseline expansion methods for most of the collections, while the proposed weighting schemes significantly outperform the no expansion baseline in a consistent manner and act as an effective input to the proposed incremental blind feedback algorithm. The rest of the article is organized as follows. The following section provides a dis-cussion on some of the related work. Section 3 describes the proposed work. Experi-mental setup and the results are given in Sections 4 and 5 respectively. We conclude in Section 6. Pseudo relevance feedback (PRF) has been extensively studied in the last decades. The two following issues are mainly addressed in a typical pseudo relevance feedback method. (1) Identification of useful terms and their weighting. (2) Selection of good quality feedback documents. In this work we focus on the first issue, namely, term selection and term weighting.

A large number of PRF algorithms exist in the literature and we describe a few of them (for a comprehensive treatment and for references to the extensive literature on the subject one may refer to the survey article by Carpineto et al. [2012]). The idea of top retrieved documents as a source of potential relevant documents for a searcher X  X  intent can be found in the work by Attar and Fraenkel [1977]. In this work they use local dynamic clustering iteratively on the top retrieved documents to identify the related terms and then feed them back to the system to improve the quality of the retrieved documents. In the vector space model a popular approach to pseudo rele-vance feedback (known as Rocchio X  X  method) is to construct a new query that maxi-mizes the similarity with pseudo relevant documents while minimizing the similarity with pseudo nonrelevant documents.

Although Rocchio X  X  method [Salton 1971] is simple and intuitive, its notable short-coming is that the term weights computed in this framework reflect more the useful-ness of a term with respect to the collection rather than its importance in the query relevant part of the corpus. Based on this observation Carpineto et al. [2001] integrate an information theory based term weighting scheme within the Rocchio framework. This approach first computes the KL divergence score of each term between its prob-ability in the pseudo relevant documents and that in the whole collection. The idea is that if a term is more probable in the feedback documents than in the collection, then the KL divergence score is higher. The KLD score is then used to select the terms and this score is used as the weight in the Rocchio framework.

In the language model framework, a massive query expansion method has been pro-posed by Lavrenko and Croft [2001]. The basic idea once again is similar: they esti-mate the probability of joint occurrence of a term with the query terms in the pseudo relevant documents. Unlike the information theory based method this approach ex-plicitly considers the association of a term with every query term. In the follow up development the basic relevance model has been modified by interpolating the proba-bilities of a term from the original query model and the relevance model [Jaleel et al. 2004]. This enhanced model is popularly known as RM3 and is widely used recently as a baseline AQE [Bendersky et al. 2011; Cao et al. 2008; Krikon et al. 2010; Lv and Zhai 2010; Miao et al. 2012]. Metzler and Croft [2005] propose an interesting generalization of the relevance model that takes term dependencies into account. By modeling the relevance distribution by Markov random fields, a set of features (such as proximity, ordered or unordered query coverage within a window) is used including the term occurrence information.

Many researchers have tried to go beyond the selection of expansion terms based on a single criterion. For example, Carpineto et al. [2002] use multiple term ranking function to select the expansion terms based on majority voting. On the other hand, the use of multiple document feedback model has been used by Collins-Thompson and Callan [2007] and Collins-Thompson [2009]. In these methods, a number of feedback models are generated from the top retrieved set employing sampling with replacement and then Dirichlet distribution is fit over the feedback language models. The language model at the mean or mode is then chosen as the final enhanced feedback model. They then perform model combination over several enhanced models, each based on slightly modified query sampled from the original query (no expansion, leaving a single query term, keeping only a single term). The authors argue that in this way it is possible to remove noisy terms as well as focus on expansion terms which are related to multiple query aspects.

Sakai et al. [2005] propose a selective sampling method that skips some top retrieved documents based on selective sampling that aims at collecting a variety of document samples based on the assumption that the initial top P documents are not necessarily representative samples from the whole set of good pseudo relevant documents. The skipping of documents is done based on a clustering criterion. Once the set of pseudo relevant documents are obtained the terms are selected in the usual way. More re-cently, a clustering based re-sampling of top retrieved documents has been used to improve the performance of the PRF method [Lee et al. 2008]. The method creates overlapping clusters of top retrieved documents and then repeatedly feeds dominant documents that appear in multiple highly ranked clusters.

In summary, unlike many of the methods, the proposed method does not focus on document selection for query expansion. Instead, its main focus is to select potential expansion terms from pseudo relevant documents using an incremental approach. The incremental approach distinguishes it from the existing approaches for term selection, where the terms are selected by taking into account all the pseudo relevant documents as a unit. Also, our method is different from the method Incremental Relevance Feed-back proposed by Aalbersberg [1992] which is an interactive technique that involves human in the loop. Like the most query expansion methods, our algorithm consists of two main parts. The first part selects the candidate expansion terms from the top retrieved set of docu-ments. In the second part, the algorithm assigns a weight to each selected term and expands the query with those weights. Then the similarity score between the query q and a document d is computed as where w tq and w td denote the weight of the term t in the query q and the document d respectively. We devise a number of weighting schemes that use various information such as the number of feedback documents that contain the term, the specificity of the term in the collection, the relative average term frequency between the pseudo relevant set and the collection.
 Our term selection scheme employs two key properties of a term. In fact, these two properties are commonly used in a typical pseudo relevance feedback algorithm. (1) The number of feedback documents which contain the term. That is, its generality (2) The rareness/specificity of the term in the ambient document collection. The intuition behind the first property is that if a term is consistently present in a set of feedback documents, the chance is high that the particular term is co-related with the query terms. Therefore, the inclusion of the term in the expanded query is expected to comply with the theme of the original query, and consequently reduces the chance of query intent drift. So, in a nutshell, the importance of a term as an expansion candidate is more or less proportional to its frequency of occurrence in the feedback documents.

The other important clue is the rareness of the term in the entire document collec-tion. Ideally, a blind relevance feedback method should be able to work without the manual discrimination between important and unimportant terms. If the emphasis is given only on the feedback document frequency, it increases the risk of selection of terms which are common in the collection. Since such terms are known to be less use-ful, their inclusion may not provide us the expected benefit which we intend to achieve. Moreover, if a system does not explicitly use manually identified unimportant terms (commonly known as stopwords), then they are an easy target for such a term selection scheme. Therefore, to prevent such less useful terms from being selected we incorpo-rate the rareness of a term in the collection within our selection scheme. Since, inverse document frequency is a well established measure of a term X  X  specificity, we use it in our term selection function in combination with the feedback document frequency of the term. Based on the above observations, we define the following scoring function for term selection.
 Here R q denotes the set of feedback documents for the query q and df ( t , number of documents in R q where t occurs. We use standard idf, log is the number of documents in the collection and df ( t , of term t in the collection C . Note that we have used the logarithm of df ( t , equation 2 to dampen its magnitude. Otherwise, df ( t , R q term selection process and consequently, may select the terms which are general in the collection, contradicting our original goal of choosing terms which are frequent in the feedback document set but rare in the collection. We use Equation 2 to select top scoring n (a pre-specified number) terms. Once the potentially good feedback terms are selected, our next objective is to assign a weight to each term that reflects their goodness as a potential expansion candidate. The general principle is that the query terms are assigned higher weights and the expansion terms are assigned comparatively less weights in order to prevent the searcher X  X  intent drift. We also adopt the same principle and assign a fixed weight 1.0 to each query term, while the weight of an expansion term is determined by a specific weighting scheme designed for this particular purpose which we describe below.

In some methods, the weighting of a selected term is done using the same formula which was used to select them. For example, both the relevance model [Lavrenko and Croft 2001] and the information theory based method [Carpineto et al. 2001] make use of the same score for selection and weighting. There are methods which do the job differently, where the selection scheme and the weighting scheme are different [Cao et al. 2008]. We devise the following three weighting schemes based on both these principles. (1) Using selection function . In this scheme we first determine the highest selection (2) Combining relative average frequency . We now introduce a new factor in our pre-(3) Using feedback document frequency . In the previous two weighting formulae we So far we have taken a familiar approach: first assume a fixed number of top retrieved documents as relevant and then select some terms and estimate their weights. Our primary focus was selecting and weighting the terms for query expansion. Such an approach gives more or less a summary of the appearance of a term in the entire pseudo relevant set of documents and is therefore sensitive to the quality of the feed-back documents. To better understand the phenomenon, consider the TREC query 311 ( industrial espionage ). The initial (no feedback) ranked list for this query has high percentage of relevant documents in top 10, however, the amount of relevant docu-ments falls drastically as we go further down the list (top 20). As a result if we take top 20 documents for feedback, a typical AQE method selects many terms that are unrelated to the query intent, which ultimately degrades the retrieval performance. On the other hand, if the TREC query 304 ( endangered species mammals ) is consid-ered, we observe a different outcome: the top 10 retrieved set for this query contains only 2 relevant documents, while top 20 set contains 8 relevant documents. Thus, it is clear that for the former case a smaller number of documents is going to give better results, while for the later situation, a larger number of documents seems more effec-tive. Therefore, a varying number of pseudo relevant documents (PRDs) seems a better practical choice for enhancing retrieval quality. However, determination of query spe-cific cut off (the number of PRDs) is an extremely difficult task. Indeed, this is perhaps the most compelling reason why many AQE algorithms do not perform well for a sub-stantial number of queries, since they choose a fixed number of top scoring documents for feedback. One potential solution to this problem is to down-weight the terms that are coming from the documents having low similarity score with the query. Indeed, relevance model [Lavrenko and Croft 2001] adopts this strategy. However, it does not address the problem reasonably well, since the distribution of similarity scores for top ranked documents is very smooth, which in turn has marginal effect on term selection and weighting. This shortcoming has been addressed in the refined relevance model (RM3) [Jaleel et al. 2004] by interpolating the relevance model based weight with the original query model weight. However, this approach pays too much attention to the top ranked documents and consequently yields moderate additional benefit (see results section for details).

We propose a different approach which we term Incremental Blind Feedback (IBF in short). The basic idea is simple: first select a sufficiently large number of top ranked documents and then from that set generate multiple sets of feedback documents fol-lowing an incremental approach. Then from each such set a sufficiently large number of terms are selected and their weights are computed using the selection and weighting schemes. As the process comes to an end the resulting weight for a term is computed by aggregating the weights determined from each set of PRDs. The main distinction be-tween the previous approaches and the proposed approach is that instead of selecting terms by considering the pseudo relevant documents as a single evidence, we generate multiple evidence from a reasonably large number of pseudo relevant documents in a systematic way. Then, we combine this evidences to identify useful terms. Since the feedback term selection proceeds in an incremental fashion, the term weight computed from each feedback set gives us the trend of occurrence of a term. The main intuition is that if a term happens to be potentially useful, it would appear among the top scor-ing terms of multiple feedback document set. In sum, IBF consists of two main steps, namely, weight generation and weight aggregation . The details of each of the steps are described as follows. 3.3.1. Weight Generation. We now formally describe the first step, namely, the weight generation methodology. Let d q 1 , d q 2 ... d q n be the set of top n ranked documents for the query q . At first iteration we take the documents d q 1 , d ally around 5, please see the paragraph preceding Section 4 for details) and then select the potential expansion terms and assign a weight to each of them based on a weight-ing scheme. In the next iteration, we double up the number of feedback documents, that is, we take the documents d q 1 , d q 2 , ... d q 2 r in the i -th iteration we choose the documents d q 1 , d q of the potential expansion terms. The iteration continues until 2 ( | R | ). Clearly, if there are m iterations we get the weight vector for each term t . Note that in each iteration the set of selected terms may not be the same and in that case the weight of an absent term is assumed to be zero (i.e., w
Now, the most crucial issue in the incremental selection of feedback documents is the rate at which we increment the size of the PRD as the iteration progresses. The strategy presented in the preceding section doubles up the number of PRDs at every iteration, that is, if the number of PRDs at i -th iteration is k ,thenat ( i tion the number of PRDs is increased to 2  X  k . This seemingly ad-hoc strategy has a reasonable intuitive justification that we detail next.

For example, at i -th iteration we choose k documents for term selection and weight-ing. Now, in the very next iteration ( ( i + 1 ) -th) there are three broad possibilities in setting the number of PRDs. We can increase k to k + c ( c &gt; 0), where c can be much smaller than k , close to k or much larger than k . If we choose c much smaller than k , then the top scoring terms from k + c documents are likely to be the same as that from the previous iteration and consequently does not provide us the trend of change in term score distribution (or selection of potential expansion terms). On the other hand, if c is chosen much larger than k then we are emphasizing on the documents that are further down the rank list, which potentially increases the risk of selection of less useful terms. Therefore, in view of the above, c  X  Now the question is how close should c be to k ? It seems a very difficult task to de-termine theoretically, and therefore in order to avoid pathological intricacies that may otherwise arise if we pay too much attention to the proper choice of c ,weset c We would rather rely on the above intuition supported by experimental validation (see Section 5.2.3). 3.3.2. Weight Aggregation. Once the weights are generated, our next step is to aggre-gate them to produce a resultant score which will be used for final ranking and weight-ing of terms. We employ four simple approaches to combining the weights. The first three approaches are basically the central tendencies measured using arithmetic mean (AM), geometric mean (GM) and harmonic mean (HM), while the fourth approach combines the standard deviation (of weights) with the arithmetic mean. Formally, our three mean based approaches are as follows. Algorithm 1 Incremental Blind Feedback 1: For the query q retrieve a set of n documents R ={ d q 2: Set i = 1 (iteration number). 3: Set k = r (base case, usually 5). 4: for k  X | R | do 5: Take top k documents. 6: Choose a large number of terms (say, 100) using the selection function (Equa-7: Denote the set of terms selected in the i -th iteration as T 8: Compute weights of these terms using some weighting scheme. 9: Denote the weight of a term t in i -th iteration as w i 10: Set k = 2  X  k . (i.e double up the number of feedback document). 11: Set i = i + 1. 12: end for 13: Compute the weight for each term t  X  T j using any one of the formula given in 14: Rank the terms based on descending order of w tq . 15: Select top m terms for expansion and make w tq as the resulting weight for the
It is well known that AM is sensitive to the possible outliers and it moves the central tendency toward the higher elements, whereas in contrast HM does quite the opposite by moving the value toward the smaller elements. Geometric mean takes a somewhat balanced role since AM  X  GM  X  HM. On the other hand, since a vector may contain zero entries, AM can be a safer choice over the other two. The approach to tackle zero entries (for GM and HM) is described in the paragraph named Addressing Zero Entries at the end of this section.

The previous three schemes do not take into account the variance in the data. Our next hypothesis is that if a term gets largely varying weights, it may not be an impor-tant term. Specifically, we anticipate that a term is more important if its mean weight is high and the variance of its weights is small. Based on our assumption, we devise a measure which dampens the mean weight by a factor which is essentially a decreasing function of the standard deviation of its weights computed over the iterations. We call it discounted mean based aggregation (DM in short). The following equation formally defines the weighting formula. The notation SD ( t ) denotes the standard deviation of the weights of a term t . We add (a small number, 0.0001) with SD ( t ) to avoid a zero within logarithm. The denominator  X  log ( + SD ( t )) is used for normalization. Negative of logarithm is introduced since SD ( t )&lt; 1. Algorithm 1 describes the complete procedure.

Addressing Zero Entries. Note that the weight vector may contain zero entries and therefore needs special attention depending upon the particular aggregation scheme that is chosen. For example, if arithmetic mean is used to aggregate the weights, zero entries do not pose a problem. However, zero entries are problematic for both geometric mean and harmonic mean. Therefore, when a zero entry is encountered (for GM and HM) it is replaced by the following value : min ( W ( t )) number of zero entries of W ( t ) , max ( W ( t )) is the maximum element of the vector while min ( W ( t )) is the minimum among the nonzero entries of assert that the above formula for replacement of zero entries is heuristic rather than theoretical. Our main objective is that the scheme should consider the following goals. (1) Value of zero entries should be less than the minimum nonzero value of the (2) The value should be proportional to the ratio of minimum and maximum nonzero (3) The replacement weight should be inversely proportional to the number of zero The replacement formula designed above clearly reflects the above motivations and, therefore, we believe it can be a reasonable strategy to start with.
 For our experiments we set | R | to 50 and the number of terms in an iteration to 100. The intention is to choose a reasonably large number of documents and terms so that we miss as little as possible [Mitra et al. 1998]. To set the value of the number of feed-back documents in the base case ( r ), we optimize the average precision by performing an exhaustive search in the range [ 1  X  10] on the training collections. As it turns out, the best value of r is always in the range [5  X  8], suggesting that a relatively small value is a better choice. We therefore, set it to 5 for all our experiments without further tuning. In order to asses our query expansion method we conduct experiments on a large num-ber of queries from three Indian language, TREC news and TREC web test collections. We also compare the performance of our methods with two strong baselines (described in Section 4.3) as well as against no expansion. In particular, the primary goals of our experiments are as follows. (1) To select the best weighting scheme that will be used in the proposed Incremental (2) Evaluating Incremental Blind Feedback algorithm (Section 5.2). (3) The effect of feedback (no. of docs and no.of terms) parameters on the retrieval We test our proposed methods on five different test sets consisting of three Indian lan-guages (Hindi, Bengali and Marathi) as well as TREC English news and web data. The Indian language collections are from FIRE [Majumder et al. 2010] evaluation forum. The English news collection is from TREC 6, 7 and 8, while TREC 9 and 10 collection (wt10g) is the English web corpus.

All query expansion methods (proposed as well as the baselines) contain a number of parameters. Therefore, for the sake of reliable comparison we use separate collections for training and testing. The training collections are used to find the best parameter values that optimize the mean average precision and then those parameter values are used to carry out the experiments on the test collections. The statistics for the training and test collections are summarized in Table I. We remove the queries having no rele-vant documents in the recall base. Training for TREC 6 and 7 and 8 and TREC 9 and 10 test collections was done on TREC 1 and 2 and 3 collection, while for the three Indian language test collections, the corresponding language X  X  training collections are used.
 All our experiments are carried out using TERRIER 1 retrieval system. In particular, we use a probabilistic divergence from randomness based model called IFB2 [Amati and Van Rijsbergen 2002]. This model is a probabilistic nonparametric model. We re-port our results under various metrics, namely, MAP, P@10 and the recall. We use title field of the topics. For English collections we applied stemming using Porter stem-mer [Porter 1997] and for Indian language collections we use Porter like stemmers developed by Dolamic and Savoy [2009, 2010]. All experiments are done based on uni-gram word assumption. We did not use any phrase or positional information. Statisti-cally significant performance differences are determined using two-sided paired t -test at 95% confidence level ( p &lt; 0.05). We compare the performance of our expansion method against two strong automatic query expansion methods as well as retrieval runs that use no expansion. The baseline methods are described as follows. (1) No Feedback (NoFb). The performance of the divergence from randomness based (2) Relevance Model (RM3) [Jaleel et al. 2004; Lavrenko and Croft 2001]. This model (3) Information Theoretic Method (IT) [Carpineto et al. 2001]. This method was shown The performance of all automatic query expansion methods crucially depends on the choice of the two common parameters, namely, the number of feedback documents and the number of expansion terms. To set the proper parameter values for each of the methods we use the collections different from the test collections. For each of the methods, we run the experiments on training collections by varying the number of feedback documents from 5 to 50 in the step of 5. Likewise we do the same process for the number of feedback terms. Then the parameter combination that gives maximum MAP value is used to carry out experiments on the test collections for that method. Op-timal parameters (document, term) for IT on English data are (30, 40), while on Hindi, Bengali and Marathi they are (25,40), (20,40) and (15,50) respectively. Optimal param-eters for RM3 on English, Hindi, Bengali and Marathi are (40,50), (30,40), (10,40) and (20,30) respectively. W1 and W2 and W3 achieve best performance on English training data when parameters are (20,40), (25,40), (25,40) respectively. On Hindi, the optimal parameters for W1, W2 and W3 are the same (20,40). On Bengali and Marathi W1 is best at (15,40) and (20,50) respectively. On Bengali and Marathi, both W2 and W3 give best performance at (20,40) and (25,50) respectively.

Table II shows the optimal MAP values for each of the methods. The table clearly shows that there are no significant performance differences between IT, W2 and W3.
To determine a suitable value of the parameter  X  in W2 and W3, we carry out similar experiments by varying  X  from 0 to 5 in the increment of 0.1. The best value of  X  lies in [0.8 X 1.1] and the performance monotonically decreases as the value of  X  increases. Therefore, all experiments are conducted by setting  X  = 0.95 (average of the best interval). In the next two subsections we present our main evaluation results. In Section 5.1 we primarily focus on evaluating the effectiveness of our selection function and three weighting schemes. We also compare the performance of our methods with no expan-sion baseline. In Section 5.2 we present our main evaluation results. There we use the relatively better weighting scheme determined on the basis of the experimental out-comes given in 5.1 as a base weighting formula for our Incremental Blind Feedback approach. We start by presenting our results for the three weighting schemes on the test data. We use the selection function given in Equation 2 for all our expansion results. The perfor-mance (measured using MAP, P@10 and relevant returned) of no expansion baseline and three proposed weighting schemes is shown in Table III. The table also shows the relative percentage improvements (within parentheses) of various methods measured against no expansion. In the tables, W1, W2 and W3 denote the weighting formula given in Equations 3, 4 and 6 respectively. We use the parameters learnt from the training data for all the experiments.

Table III clearly shows that the improvements obtained by the proposed schemes are always significantly better than the no feedback. On TREC 6,7,8 W3 achieves the high-est improvement in MAP and is significantly better than no expansion. W3 achieves 16% MAP improvement compared to no expansion. The table also shows that W3 yields the highest precision and recall improvement.

On TREC web collection (TREC 9 and 10) RM3 achieves the highest MAP and P@10, but the improvement is not significant compared to W3. Performance (MAP) of W2 and W3 are comparable, while W1 seems to be the least effective weighting scheme.
We now analyze the results for three Indian language collections. Clearly, the ex-tent of improvement is large for Hindi and Marathi, while it is relatively smaller on Bengali collection. In particular, the MAP improvements on Hindi by W3 is nearly 20% and on Marathi it is above 28%. Unlike the TREC collections, W1 seems to be more effective than W2 on Hindi. However, W2 is more effective than W1 on both Bengali and Marathi collections. On these collections, IT performs better than RM3 and often poorer than W3. Only on Marathi collection, IT is better than all other methods.
So far we have used Equation (2) as the only function for term selection that are then weighted using different weighting functions. We now investigate the influence of term selection function. W1 uses the same selection and weighting scheme and thus we only focus on W2 and W3 weighting functions. Table IV summarizes the performance of W2 and W3 as the selection scheme. As it turns out, the use of Equation (2) as the selection function has little impact on retrieval effectiveness of the weighting schemes. If W2 is used for both selection and weighting it does marginally better than using Equation (2) for selection and W2 for weighting on three out of five collections. On the other hand, W2 as selection and weighting scheme does better on Bengali and Marathi collections. Slightly different trend is exhibited when W3 is used for selection and weighting. Use of Equation (2) with W3 weighting always does better than if W3 is used for both the purposes. However, the performance differences are very small.

Summary. Based on the results presented in Table III we arrive at the conclusion that among the three proposed weighting schemes, W3 seems to give better perfor-mance than the other two (W1 and W2) in most of the cases, although it is slightly poorer on some occasions. Performances of W2 and W3 are comparable, while W1 per-forms consistently poorer than these two. In addition, the comparison with no feedback baseline reveals that on four out of five (except TREC 9,10) collections, W1, W2 and W3 are significantly better than no expansion. We also notice that both W1 and W2 can be used as a potential term selection scheme. The weighting schemes that we design often perform well on a number of test col-lections as measured in terms of various standard metrics such as MAP, P@10 and recall. Nevertheless, they suffer from the familiar problems encountered by a typical PRF algorithm, that is, their sensitivity to the fixed number of feedback documents for every query. This is not unusual, since so far we have not made any specific attempt to address those issues. Our primary goal was to experimentally validate their effective-ness, and to determine a relatively better method. Our results confirm that among the three weighting formulae W2 and W3 provide comparable performance and therefore we use both W2 and W3 as the base weighting functions for Incremental Blind Feed-back. We now present the experimental results of our Incremental Blind Feedback based method. 5.2.1. Performance Comparison of Aggregation Strategies. We reiterate that the proposed Incremental Blind Feedback (IBF) algorithm uses four different approaches to aggre-gating the weights. Therefore, in this section we seek to gain an overall insight about the influence and effectiveness of different aggregation methods. Our primary goal is to choose a relatively better strategy that will be used for the subsequent experiments. Once again, for the sake of unbiased comparison, we carry out the experiments on the training collection.

Table V presents the performance of various aggregation schemes for two weighting schemes W2 and W3 on the training data. It is evident from the table that the per-formance of IBF is often the best when AM is used as the aggregation method, while GM is the closest to AM among the others. At the same time, the results convince us that DM is the least effective, while GM and HM are not very far from AM. The reason behind the better performance of AM is that the presence of zero entry in the weight vector does not create any difficulty since it is based on additive formula. On the other hand, all the other three aggregation schemes are very sensitive to the presence of zero entries in the weight vectors. Since AM provides maximum benefit among the ag-gregation schemes, we present our main results using AM as the aggregation strategy for IBF. 5.2.2. Main Results. The results of Incremental Blind Feedback are given in Table VI. We present results using both W2 and W3 as the base weighting formula and AM as the aggregation strategy. The expression IBF(W2, AM) in Table VI denotes that the base weighting formula is W2 and AM is the aggregation method, IBF(W3,AM) is similarly defined. We use 30 terms for IBF based on the experiments done on the training data (see Section 5.2.3 for the effect of the number of terms).
Table VI clearly shows that the retrieval performance noticeably improves when we use the Incremental Blind Feedback based method. On TREC 6,7,8 data, the combina-tion of W3 and AM gives rise to 21.2% MAP enhancement compared to that of no feed-back. This improvement on all the baselines is statistically significant. IBF(W2,AM) performs almost equally with IBF(W3,GM) and the improvements are significant against NoFb, RM3 and IT.

On TREC 9, 10 collection, IBF(W3,AM) performs better than all the other methods, and the performance is significantly better than all the baselines. On the other hand, IBF(W2,AM) is better than no feedback and IT, but not better than RM3. We note that the performance of IT on TREC 9 &amp; 10 collection is very poor. The reason is that the performance of IT generally drops with a larger number of feedback documents especially for a collection where the average number of relevant documents is small, which is the case in TREC 9 &amp; 10. On the other hand, RM3 gives a stable performance even in such a case since RM3 down-weights the terms that come from the documents having lower similarity with the original query.

Now we shift our focus to analyzing the effectiveness of our Incremental Blind Feed-back based weighting strategies on three Indian language collections. On the basis of the results shown in Table VI, we can clearly claim that the performance of IBF based method is consistently superior to that of the baselines. The only exception is the re-sults on Marathi data where IT does better than both IBF(W2,AM) and IBF(W3,AM). RM3 seems to be the least effective among the methods. Like TREC news and web English results, IBF(W3,AM) achieves consistently better MAP than IBF(W2,AM) but the differences are not statistically significant. Significance tests confirm that the per-formances of IBF(W3,AM) and IBF(W2,AM) are always statistically significantly bet-ter than all the three baselines on Hindi, while on Marathi collection none of the IFB X  X  are better than the baselines. In fact, on Marathi data IT achieves the maximum MAP value. However, the difference is not statistically significant compared to IBF(W3,AM).
Table VII compares the performance of the incremental blind feedback methods with the base weighting schemes on which they operate. It is clearly evident that the perfor-mances of IBF(W3,AM) and IBF(W2,AM) are noticeably better than the base formula. Both IBF methods almost always significantly outperform the base weighting schemes on which IBF operates. On TREC 9,10 and Hindi collections the performances of the two IBF methods are more than 10% better than the base formula, which clearly indicates that the incremental blind feedback method substantially enhances the retrieval quality.

In the preceding paragraph we compare the performance of IBF to the two proposed base weighting formulae. We now extend our experiments to examine the generality of the proposed method (IBF) under other base weighting formula for query expansion. Specifically, we explore the effectiveness of IBF when RM3 and IT are chosen as the base weighting schemes.

Table VIII shows the performance of IBF compared to RM3 and IT on all the five test collections. The number of feedback documents and terms are kept the same as in the previous experiments. Once again, the experimental results suggest that IBF is effective under RM3 and IT as the base formulae. IBF always does better than both RM3 and IT and performance differences are often statistically significant.
So far, we have primarily confined ourselves to evaluating the effectiveness of incre-mental blind feedback method and comparing them with the baselines. We reiterate that the proposed IBF, in each iteration, doubles up the number of feedback docu-ments. Our next set of experiments are designed to compare the proposed IBF with two different baseline settings. The first baseline is the standard blind feedback (non-incremental) where top 50 documents are used as a source of term selection, while the other baseline adopts an incremental approach but with an increment of 5 docu-ments. The later setting is particularly intended to understand the impact of degree of increment of feedback documents as the iteration progresses.

Table IX clearly shows that the proposed IBF that doubles up the number of feedback documents in each iteration, always outperforms the two baselines, and the perfor-mance differences are often statistically significant. Table IX also shows that a larger number of feedback documents in standard nonincremental feedback approach notice-ably hurts the performance. Incremental feedback always gives a better performance. The experimental results also establish that the degree of increment in IBF does have a significant effect on retrieval performance.

The query wise performance of our proposed methods compared to two baselines are given in Table X. The number in each cell gives the percentage of query in which the method in the column achieves poorer/better average precision than the method in the corresponding row. In this analysis also, IBF(W3,AM) is often superior to all other methods, while IBF(W2,AM) performs equally well on TREC 6,7,8 collection.
Summary. Overall, the Incremental Blind Feedback based method provides greater benefit as a query expansion method than the two strong baselines, namely, RM3 and IT, as well as the corresponding base weighting schemes on which IBF operates. The two approaches, namely, IBF(W2,AM) and IBF(W3,AM), almost always significantly outperform the baseline schemes, and sometimes this performance gain is more than 10% better than that achieved by the baseline AQEs.

Among the four weight aggregation approaches AM seems to provide a better per-formance improvement, although there are exceptions on a few occasions where GM, HM and DM do better than AM. Nevertheless, the arithmetic mean based method is relatively consistent among the aggregation strategies. 5.2.3. Effect of Parameters. The performance issues under varying parameter values are shown in Figures 1 X 6. For this study we have chosen three datasets. We have cho-sen TREC 6,7,8 and wt10g (TREC 9,10) as the representatives from news and web collections respectively, while Hindi is chosen as a member of Indian language cate-gory. The main objective is to get an overall idea about the nature of the performance variation with respect to the change in feedback parameter values.

Figures 1 X 3 give the trend of change of MAP (achieved by RM3, IT, W2 and W3) with respect to the variation in the number of documents when the number of feed-back terms is fixed (30 terms). Clearly, among the four methods RM3 is consistently stable with the increase in the number of PRDs, and the change of MAP is negligible. In contrast, the other three methods (IT, W2 and W3) are quite sensitive to the increase in the number of PRDs. All these three methods reduce the average effectiveness noticeably as we choose a larger number of documents. They generally achieve the best performance around 15 to 20 documents. The fall in performance is very rapid for TREC 9,10 collection.

On the other hand, the fluctuation of MAP is not very sharp for varying number of feedback terms. Figures 4 X 6 show the trend of change in MAP graphically when the number of feedback documents is fixed (20 docs) and the number of terms varies. On TREC 6,7,8 collection MAP increases slowly as we take a larger number of terms, while for Hindi collection the number of terms around 40-50 seems to be better. Another important observation that can be made from these figures is that the number of terms fewer than 20 is often a bad choice, while the gain is marginal if we take more and more terms beyond a limit (30).

Let us now concentrate on the stability of our Incremental Blind Feedback algo-rithm. Note that we have incrementally chosen up to 50 documents for feedback, which is large enough for a pseudo feedback method [Mitra et al. 1998]. Now we want to see how the performance varies with a variation in the number of terms. It is evident from Figures 4 X 6 that the incremental methods are a little ineffective for a smaller number of terms (less than 20). The methods achieve good average precision around 30 X 40 terms (except TREC 9,10, where around 15 gives better results), while the performance stabilizes slowly as the number of terms is increased. Two aggregated weighting meth-ods (IBF(W2,AM) and IBF(W3,AM)) show a comparable stability with the change in the number of expansion terms.

Figure 7 shows the effect of the rate of increment on retrieval performance for our three representative collections when the method is IBF(W3,AM) (as a representative from IBF). The x-axis represents the rate of increment, that is, the ratio between c and k given in Section 3.3.1. To study the effect we vary c / k from 0.2 to 2.0. We noticed that the performance of IBF is generally the best when the value of c is close to k which matches our intuition, while a higher rate of increment happens to have a more detrimental effect on retrieval quality. In this article we propose a simple automatic query expansion method. We follow a familiar approach to query expansion by assuming top retrieved documents as rele-vant and selecting and weighting terms from that set. As part of our contribution we propose a simple term selection function and three weighting schemes based on easily computable term features. We then propose a novel automatic query expansion algo-rithm based on incrementally selecting feedback documents from top retrieved set and then computing term weights by combining the weights collected from the feedback sets. We use various types of mean and a discounted mean of weights as an aggregated weight for the final weighting.

Our initial experiments with several test collections reveal that the proposed weight-ing schemes almost always perform significantly better than no expansion. The exper-imental results confirm that the incremental blind feedback based method performs significantly better than the base weighting schemes and all the baselines on most of the collections.

