 Many aspects of human cognition involve the in-teraction of constraints that push a decision-maker toward different options, whether in something so trivial as choosing a movie or so important as a fight-or-flight response. These constraint-driven decisions can be modeled with a log-linear system. In these models, a set of constraints is weighted and their violations are used to determine a prob-ability distribution over outcomes. But where do these constraints come from?
We consider this question by examining the dominant framework in modern phonology, Opti-mality Theory (Prince and Smolensky, 1993, OT), implemented in a log-linear framework, MaxEnt OT (Goldwater and Johnson, 2003), with output forms X  probabilities based on a weighted sum of constraint violations. OT analyses generally as-sume that the constraints are innate and univer-sal, both to obviate the problem of learning con-straints X  identities and to limit the set of possible languages.

We propose a new approach: to learn con-straints with limited innate phonological knowl-edge by identifying sets of constraint violations that explain the observed distributional data, in-stead of selecting constraints from an innate set of constraint definitions. Because the constraints are identified as sets of violations, this also per-mits constraints specific to a given language to be learned. This method, which we call IBPOT, uses an Indian Buffet Process (IBP) prior to define the space of possible constraint violation matri-ces, and uses Bayesian reasoning to identify con-straint matrices likely to have generated the ob-served data. In identifying constraints solely by their extensional violation profiles, this method does not directly identify the intensional defini-tions of the identified constraints, but to the extent that the resulting violation profiles are phonologi-cally interpretable, we may conclude that the data themselves guide constraint identification. We test IBPOT on tongue-root vowel harmony in Wolof, a West African language.

The set of constraints learned by the model sat-isfy two major goals: they explain the data as well as the standard phonological analysis, and their vi-olation structures correspond to the standard con-straints. This suggests an alternative data-driven genesis for constraints, rather than the traditional assumption of fully innate constraints. 2.1 OT structure Optimality Theory has been used for constraint-based analysis of many areas of language, but we focus on its most successful application: phonol-ogy. We consider an OT analysis of the mappings between underlying forms and their phonological manifestations  X  i.e., mappings between forms in the mental lexicon and the actual vocalized forms
Stated generally, an OT system takes some in-put, generates a set of candidate outputs, deter-mines what constraints each output violates, and then selects a candidate output with a relatively unobjectionable violation profile. To do this, an OT system contains four major components: a generator G EN , which generates candidate out-put forms for the input; a set of constraints C ON , which penalize candidates; a evaluation method E VAL , which selects an winning candidate; and H , a language-particular weighting of constraints that E VAL uses to determine the winning candi-date. Previous OT work has focused on identifying the appropriate formulation of E VAL and the val-ues and acquisition of H , while taking G EN and C
ON as given. Here, we expand the learning task by proposing an acquisition method for C ON .
To learn C ON , we propose a data-driven markedness constraint learning system that avoids both innateness and tractability issues. Unlike pre-vious OT learning methods, which assume known constraint definitions and only learn the relative strength of these constraints, the IBPOT learns constraint violation profiles and weights for them simultaneously. The constraints are derived from sets of violations that effectively explain the ob-served data, rather than being selected from a pre-existing set of possible constraints. 2.2 OT as a weighted-constraint method Although all OT systems share the same core structure, different choices of E VAL lead to dif-ferent behaviors. In IBPOT, we use the log-linear E VAL developed by Goldwater and John-son (2003) in their MaxEnt OT system. MEOT extends traditional OT to account for variation (cases in which multiple candidates can be the winner), as well as gradient/probabilistic produc-tions (Anttila, 1997) and other constraint interac-tions (e.g., cumulativity) that traditional OT can-not handle (Keller, 2000). MEOT also is motivated by the general MaxEnt framework, whereas most other OT formulations are ad hoc constructions specific to phonology.

In MEOT, each constraint C i is associated with a weight w i &lt; 0 . (Weights are always negative in OT; a constraint violation can never make a candidate more likely to win.) For a given input-candidate pair ( x,y ) , f i ( y,x ) is the number of vi-olations of constraint C i by the pair. As a maxi-mum entropy model, the probability of y given x is proportional to the exponential of the weighted sum of violations, set of all output candidates for the input x , then the probability of y as the winning output is:
This formulation represents a probabilistic extension of the traditional formulation of OT (Prince and Smolensky, 1993). Traditionally, constraints form a strict hierarchy, where a single violation of a high-ranked constraint is worse than any number of violations of lower-ranked con-straints. Traditional OT is also deterministic, with the optimal candidate always selected. In MEOT, the constraint weights define hierarchies of vary-ing strictness, and some probability is assigned to all candidates. If constraints X  weights are close to-gether, multiple violations of lower-weighted con-straints can reduce a candidate X  X  probability below that of a competitor with a single high-weight vio-lation. As the distance between weights in MEOT increases, the probability of a suboptimal candi-date being chosen approaches zero; thus the tradi-tional formulation is a limit case of MEOT. 2.3 OT in practice Figure 1 shows tableaux , a visualization for OT, applied in Wolof (Archangeli and Pulley-blank, 1994; Boersma, 1999). We are interested in four Wolof constraints that combine to induce vowel harmony: * I , P ARSE [rtr], H ARMONY , and P
ARSE [atr]. The meaning of these constraints will be discussed in Sect. 4.1; for now, we will only consider their violation profiles. Each column rep-resents a constraint, with weights decreasing left-to-right. Each tableau looks at a single input form, noted in the top-left cell: ete , EtE , Ite , or itE .
Each row is a candidate output form. A black cell indicates that the candidate, or input-candidate white cell indicates no violation. Grey stripes are Figure 1: Tableaux for the Wolof input forms ete ,
EtE ,
Ite , and itE . Black indicates violation, white no on the distribution (see Sect. 4.3). overlaid on cells whose value will have a negligi-ble impact on the distribution due to the values of higher-ranked constraint.

Constraints fall into two categories, faithful-ness and markedness, which differ in what infor-mation they use to assign violations. Faithfulness constraints penalize mismatches between the in-put and output, while markedness constraints con-sider only the output. Faithfulness violations in-clude phoneme additions or deletions between the input and output; markedness violations include penalizing specific phonemes in the output form, regardless of whether the phoneme is present in the input.

In MaxEnt OT, each constraint has a weight, and the candidates X  scores are the sums of the weights of violated constraints. In the ete tableau at top left, output ete has no violations, and there-fore a score of zero. Outputs Ete and etE vio-late both H ARMONY (weight 16) and P ARSE [atr] (weight 8), so their scores are 24. Output EtE vi-olates P ARSE [atr], and has score 8. Thus the log-probability of output EtE is 1/8 that of ete , and the log-probability of disharmonious Ete and etE are each 1/24 that of ete . As the ratio between scores increases, the log-probability ratios can become arbitrarily close to zero, approximating the deter-ministic situation of traditional OT. 2.4 Learning Constraints Choosing a winning candidate presumes that a set of constraints C ON is available, but where do these constraints come from? The standard as-sumption within OT is that C ON is innate and universal. But in the absence of direct evidence of innate constraints, we should prefer a method that can derive the constraints from cognitively-general learning over one that assumes they are pre-specified. Learning appropriate model features has been an important idea in the development of constraint-based models (Della Pietra et al., 1997).
The innateness assumption can induce tractabil-ity issues as well. The strictest formulation of in-nateness posits that virtually all constraints are shared across all languages, even when there is no evidence for the constraint in a particular lan-guage (Tesar and Smolensky, 2000). Strict uni-versality is undermined by the extremely large set of constraints it must weight, as well as the possible existence of language-particular con-straints (Smith, 2004).

A looser version of universality supposes that constraints are built compositionally from a set of constraint templates or primitives or phono-logical features (Hayes, 1999; Smith, 2004; Id-sardi, 2006; Riggle, 2009). This version allows language-particular constraints, but it comes with a computational cost, as the learner must be able to generate and evaluate possible constraints while learning the language X  X  phonology. Even with rel-atively simple constraint templates, such as the phonological constraint learner of Hayes and Wil-son (2008), the number of possible constraints ex-pands exponentially. Depending on the specific formulation of the constraints, the constraint iden-tification problem may even be NP-hard (Idsardi, 2006; Heinz et al., 2009). Our approach of casting the learning problem as one of identifying viola-tion profiles is an attempt to determine the amount that can be learned about the active constraints in a paradigm without hypothesizing intensional con-straint definitions. The violation profile informa-tion used by our model could then be used to nar-row the search space for intensional constraints, either by performing post-hoc analysis of the con-straints identified by our model or by combining intensional constraint search into the learning pro-cess. We discuss each of these possibilities in Sec-tion 5.2.

Innateness is less of a concern for faithfulness than markedness constraints. Faithfulness viola-tions are determined by the changes between an input form and a candidate, yielding an indepen-dent motivation for a universal set of faithfulness constraints (McCarthy, 2008). Some markedness constraints can also be motivated in a universal manner (Hayes, 1999), but many markedness con-clear where a universal set of markedness con-straints would come from. 3.1 Structure The IBPOT model defines a generative process for mappings between input and output forms based on three latent variables: the constraint violation matrices F (faithfulness) and M (markedness), and the weight vector w . The cells of the violation matrices correspond to the number of violations of a constraint by a given input-output mapping. F ijk is the number of violations of faithfulness con-straint F k by input-output pair type ( x i ,y j ) ; M jl the number of violations of markedness constraint M  X  l by output candidate y j . Note that M is shared across inputs, as M jl has the same value for all input-output pairs with output y j . The weight vec-tor w provides weight for both F and M . Proba-bilities of output forms are given by a log-linear function: p ( y j | x i ) =
Note that this is the same structure as Eq. 1 but with faithfulness and markedness constraints listed separately. As discussed in Sect. 2.4, we as-sume that F is known as part of the output of G EN (Riggle, 2009). The goal of the IBPOT model is to learn the markedness matrix M and weights w for both the markedness and faithfulness constraints.
As for M , we need a non-parametric prior, as there is no inherent limit to the number of marked-ness constraints a language will use. We use the Indian Buffet Process (Griffiths and Ghahramani, 2005), which defines a proper probability distri-bution over binary feature matrices with an un-bounded number of columns. The IBP can be thought of as representing the set of dishes that diners eat at an infinite buffet table. Each diner (i.e., output form) first draws dishes (i.e., con-straint violations) with probability proportional to the number of previous diners who drew it: p ( M jl = 1 |{ M zl } z&lt;j ) = n l /j . After choosing from the previously taken dishes, the diner can try additional dishes that no previous diner has had. The number of new dishes that the j -th cus-tomer draws follows a Poisson(  X /j ) distribution. The complete specification of the model is then: M  X  IBP (  X  ) ; Y ( x i ) = Gen ( x i ) w  X  X  X   X (1 , 1) ; y | x i  X  LogLin ( M,F,w, Y ( x i )) 3.2 Inference To perform inference in this model, we adopt a common Markov chain Monte Carlo estimation procedure for IBPs (G  X  or  X  ur et al., 2006; Navarro and Griffiths, 2007). We alternate approximate Gibbs sampling over the constraint matrix M , using the IBP prior, with a Metropolis-Hastings method to sample constraint weights w .

We initialize the model with a randomly-drawn markedness violation matrix M and weight vector w . To learn, we iterate through the output forms y ; for each, we split M  X  j  X  into  X  X epresented X  con-straints (those that are violated by at least one output form other than y j ) and  X  X on-represented X  constraints (those violated only by y j ). For each represented constraint M  X  l , we re-sample the value for the cell M jl . All non-represented constraints are removed, and we propose new constraints, vi-olated only by y j , to replace them. After each it-eration through M , we use Metropolis-Hastings to update the weight vector w .
 Represented constraint sampling We begin by resampling M jl for all represented constraints M  X  l , conditioned on the rest of the violations ( M  X  ( jl ) ,F ) and the weights w . This is the sam-pling counterpart of drawing existing features in the IBP generative process. By Bayes X  Rule, the posterior probability of a violation is propor-tional to product of the likelihood p ( Y | M jl = 1 ,M  X  jl ,F,w ) from Eq. 2 and the IBP prior prob-ability p ( M jl = 1 | M  X  jl ) = n  X  jl /n , where n  X  jl is the number of outputs other than y j that violate constraint M  X  l .
 Non-represented constraint sampling After sampling the represented constraints for y j , we consider the addition of new constraints that are violated only by y j . This is the sampling coun-terpart to the Poisson draw for new features in the IBP generative process. Ideally, this would draw new constraints from the infinite feature ma-trix; however, this requires marginalizing the like-lihood over possible weights, and we lack an ap-propriate conjugate prior for doing so. We approx-imate the infinite matrix with a truncated Bernoulli draw over unrepresented constraints (G  X  or  X  ur et al., new constraints, with weights based on the auxil-iary vector w  X  . This approximation retains the un-bounded feature set of the IBP, as repeated sam-pling can add more and more constraints without limit.

The auxiliary vector w  X  contains the weights of all the constraints that have been removed in the previous step. If the number of constraints removed is less than K  X  , w  X  is filled out with draws from the prior distribution over weights. We then consider adding any subset of these new con-straints to M , each of which would be violated only by y j . Let M  X  represent a (possibly empty) set of constraints paired with a subset of w  X  . The posterior probability of drawing M  X  from the trun-cated Bernoulli distribution is the product of the lihood p ( Y | M  X  ,w  X  ,M,w,F ) , including the new constraints M  X  .
 Weight sampling After sampling through all candidates, we use Metropolis-Hastings to estimate new weights for both con-straint matrices. Our proposal distribution is Gamma ( w k 2 / X , X /w k ) , with mean w k and mode w k  X   X  sampling on the constraints, which occurs only on markedness constraints, weights are sampled for both markedness and faithfulness features. 4.1 Wolof vowel harmony We test the model by learning the markedness con-straints driving Wolof vowel harmony (Archangeli and Pulleyblank, 1994). Vowel harmony in gen-eral refers to a phonological phenomenon wherein the vowels of a word share certain features in the output form even if they do not share them in the input. In the case of Wolof, harmony encourages forms that have consistent tongue root positions.
The Wolof vowel system has two relevant fea-tures, tongue root position and vowel height. The tongue root can either be advanced (ATR) or re-tracted (RTR), and the body of the tongue can be in the high, middle, or low part of the mouth. These features define six vowels: We test IBPOT on the harmony system provided in the Praat program (Boersma, 1999), previ-ously used as a test case by Goldwater and John-son (2003) for MEOT learning with known con- X  Markedness:  X  Faithfulness:
These constraints define the phonological stan-dard that we will compare IBPOT to, with a rank-ing from strongest to weakest of * I &gt;&gt; P ARSE [rtr] &gt;&gt; H ARMONY &gt;&gt; P ARSE [atr]. Under this rank-ing, Wolof harmony is achieved by changing a disharmonious ATR to an RTR, unless this cre-ates an I vowel. We see this in Figure 1, where three of the four winners are harmonic, but with input itE , harmony would require violating one of the two higher-ranked constraints. As in previ-ous MEOT work, all Wolof candidates are faithful with respect to vowel height, either because height changes are not considered by G EN , or because of a high-ranked faithfulness constraint blocking
The Wolof constraints provide an interesting testing ground for the model, because it is a small set of constraints to be learned, but contains the H
ARMONY constraint, which can be violated by non-adjacent segments. Non-adjacent constraints are difficult for string-based approaches because of the exponential number of possible relation-ships across non-adjacent segments. However, the Wolof results show that by learning violations di-rectly, IBPOT does not encounter problems with non-adjacent constraints.

The Wolof data has 36 input forms, each of the form V 1 tV 2 , where V 1 and V 2 are vowels that agree in height. Each input form has four candidate out-puts, with one output always winning. The outputs appear for multiple inputs, as shown in Figure 1. The candidate outputs are the four combinations of tongue-roots for the given vowel heights; the inputs and candidates are known to the learner. We generate simulated data by observing 1000 in-model must learn the markedness constraints * I and H ARMONY , as well as the weights for all four constraints.

We make a small modification to the constraints for the test data: all constraints are limited to bi-nary values. For constraints that can be violated multiple times by an output (e.g., * I twice by ItI ), we use only a single violation. This is necessary in the current model definition because the IBP pro-duces a prior over binary matrices. We generate the simulated data using only single violations of each constraint by each output form. Overcoming the binarity restriction is discussed in Sect. 5.2. 4.2 Experiment Design We run the model for 10000 iterations, using de-terministic annealing through the first 2500 it-erations. The model is initialized with a ran-dom markedness matrix drawn from the IBP and weights from the exponential prior. We ran ver-sions of the model with parameter settings be-tween 0 . 01 and 1 for  X  , 0 . 05 and 0 . 5 for  X  , and 2 and 5 for K  X  . All these produced quantitatively similar results; we report values for  X  = 1 ,  X  = 0 . 5 , and K  X  = 5 , which provides the least bias toward small constraint sets.

To establish performance for the phonological standard, we use the IBPOT learner to find con-straint weights but do not update M . The resultant learner is essentially MaxEnt OT with the weights estimated through Metropolis sampling instead of gradient ascent. This is done so that the IBPOT weights and phonological standard weights are learned by the same process and can be compared. We use the same parameters for this baseline as for the IBPOT tests. The results in this section are based on nine runs each of IBPOT and MEOT; ten MEOT runs were performed but one failed to con-verge and was removed from analysis. 4.3 Results A successful set of learned constraints will satisfy two criteria: achieving good data likelihood (no worse than the phonological-standard constraints) and acquiring constraint violation profiles that are phonologically interpretable. We find that both of these criteria are met by IBPOT on Wolof.
 Likelihood comparison First, we calculate the joint probability of the data and model given the priors, p ( Y,M,w | F, X  ) , which is proportional to the product of three terms: the data likelihood p ( Y | M,F,w ) , the markedness matrix probabil-ity p ( M |  X  ) , and the weight probability p ( w ) . We present both the mean and MAP values for these over the final 1000 iterations of each run. Results are shown in Table 1.

All eight differences are significant according to t -tests over the nine runs. In all cases but mean M , the IBPOT method has a better log-probability. The most important differences are those in the data probabilities, as the matrix and weight prob-abilities are reflective primarily of the choice of prior. By both measures, the IBPOT constraints explain the observed data better than the phono-logically standard constraints.

Interestingly, the mean M probability is lower for IBPOT than for the phonological standard. Though the phonologically standard constraints Table 1: Data, markedness matrix, weight vec-tor, and joint log-probabilities for the IBPOT and the phonological standard constraints. MAP and mean estimates over the final 1000 iterations for each run. All IBPOT/PS differences are significant ( p &lt; . 005 for MAP M ; p &lt; . 001 for others). exist independently of the IBP prior, they fit the prior better than the average IBPOT constraints do. This shows that the IBP X  X  prior preferences can be overcome in order to have constraints that better explain the data.
 Constraint comparison Our second criterion is the acquisition of meaningful constraints, that is, ones whose violation profiles have phonologically-grounded explanations. IBPOT learns the same number of markedness constraints as the phonological standard (two); over the final 1000 iterations of the model runs, 99 . 2% of the it-erations had two markedness constraints, and the rest had three.

Turning to the form of these constraints, Figure 2 shows violation profiles from the last iteration heights must be faithful between input and out-put, the Wolof data is divided into nine separate paradigms , each containing the four candidates (ATR/RTR  X  ATR/RTR) for the vowel heights in the input.

The violations on a given output form only affect probabilities within its paradigm. As a result, learned constraints are consistent within paradigms, but across paradigms, the same con-straint may serve different purposes.

For instance, the strongest learned markedness constraint, shown as M1 in Figure 2, has the same violations as the top-ranked constraint that ac-tively distinguishes between candidates in each paradigm. For the five paradigms with at least one high vowel (the top row and left column), M1 has the same violations as * I , as * I penal-izes some but not all of the candidates. In the other four paradigms, * I penalizes none of the candidates, and the IBPOT learner has no rea-son to learn it. Instead, it learns that M1 has the same violations as H ARMONY , which is the highest-weighted constraint that distinguishes be-tween candidates in these paradigms. Thus in the high-vowel paradigms, M1 serves as * I , while in the low/mid-vowel paradigms, it serves as H AR -
The lower-weighted M2 is defined noisily, as the higher-ranked M1 makes some values of M2 inconsequential. Consider the top-left paradigm of Figure 2, the high-high input, in which only one candidate does not violate M1 (* I ). Because M1 has a much higher weight than M2 , a violation of M2 has a negligible effect on a candidate X  X  prob-fluenced more by the prior than by the data. These inconsequential cells are overlaid with grey stripes in Figure 2.

The meaning of M2 , then, depends only on the consequential cells. In the high-vowel paradigms, M2 matches H ARMONY , and the learned and stan-dard constraints agree on all consequential viola-tions, despite being essentially at chance on the in-distinguishable violations (58%). On the non-high paradigms, the meaning of M2 is unclear, as H AR -MONY is handled by M1 and * I is unviolated. In all four paradigms, the model learns that the RTR-RTR candidate violates M2 and the ATR-ATR can-didate does not; this appears to be the model X  X  at-tempt to reinforce a pattern in the lowest-ranked faithfulness constraint (P ARSE [atr]), which the ATR-ATR candidate never violates.

Thus, while the IBPOT constraints are not identical to the phonologically standard ones, they reflect a version of the standard constraints In paradigms where each markedness constraint distinguishes candidates, the learned constraints match the standard constraints. In paradigms where only one constraint distinguishes candi-dates, the top learned constraint matches it and the second learned constraint exhibits a pattern con-sistent with a low-ranked faithfulness constraint. have negligible effects on the probability distribution. 5.1 Relation to phonotactic learning Our primary finding from IBPOT is that it is possi-ble to identify constraints that are both effective at explaining the data and representative of theorized phonologically-grounded constraints, given only input-output mappings and faithfulness violations. Furthermore, these constraints are successfully ac-quired without any knowledge of the phonological structure of the data beyond the faithfulness vio-lation profiles. The model X  X  ability to infer con-straint violation profiles without theoretical con-straint structure provides an alternative solution to the problems of the traditionally innate and univer-sal OT constraint set.

As it jointly learns constraints and weights, the IBPOT model calls to mind Hayes and Wilson X  X  (2008) joint phonotactic learner. Their learner also jointly learns weights and constraints, but directly selects its constraints from a composi-tional grammar of constraint definitions. This lim-its their learner in practice by the rapid explosion in the number of constraints as the maximum con-straint definition size grows. By directly learning violation profiles, the IBPOT model avoids this ex-plosion, and the violation profiles can be automat-ically parsed to identify the constraint definitions that are consistent with the learned profile. The inference method of the two models is different as well; the phonotactic learner selects constraints greedily, whereas the sampling on M in IBPOT asymptotically approaches the posterior.

The two learners also address related but dif-ferent phonological problems. The phonotactic learner considers phonotactic problems, in which only output matters. The constraints learned by Hayes and Wilson X  X  learner are essentially OT markedness constraints, but their learner does not have to account for varied inputs or effects of faith-fulness constraints. 5.2 Extending the learning model IBPOT, as proposed here, learns constraints based on binary violation profiles, defined extensionally. A complete model of constraint acquisition should provide intensional definitions that are phonolog-ically grounded and cover potentially non-binary constraints. We discuss how to extend the model toward these goals.

IBPOT currently learns extensional constraints, defined by which candidates do or do not violate the constraint. Intensional definitions are needed to extend constraints to unseen forms. Post hoc vi-olation profile analysis, as in Sect. 4.3, provides a first step toward this goal. Such analysis can be integrated into the learning process using the Rational Rules model (Goodman et al., 2008) to identify likely constraint definitions composition-ally. Alternately, phonological knowledge could be integrated into a joint constraint learning pro-cess in the form of a naturalness bias on the con-straint weights or a phonologically-motivated re-placement for the IBP prior.

The results presented here use binary con-straints, where each candidate violates each con-straint only once, a result of the IBP X  X  restriction to binary matrices. Non-binarity can be handled by using the binary matrix M to indicate whether a candidate violates a constraint, with a second distribution determining the number of violations. Alternately, a binary matrix can directly capture non-binary constraints; Frank and Satta (1998) converted existing non-binary constraints into a binary OT system by representing non-binary con-straints as a set of equally-weighted overlapping constraints, each accounting for one violation. The non-binary harmony constraint, for instance, be-comes a set { *(at least one disharmony), *(at least two disharmonies), etc. } .

Lastly, the Wolof vowel harmony problem pro-vides a test case with overlaps in the candidate sets for different inputs. This candidate overlap helps the model find appropriate constraint structures. Analyzing other phenomena may require the iden-tification of appropriate abstractions to find this same structural overlap. English regular plurals, for instance, fall into broad categories depending on the features of the stem-final phoneme. IBPOT learning in such settings may require learning an appropriate abstraction as well. A central assumption of Optimality Theory has been the existence of a fixed inventory of uni-versal markedness constraints innately available to the learner, an assumption by arguments regarding the computational complexity of constraint iden-tification. However, our results show for the first time that nonparametric, data-driven learning can identify sparse constraint inventories that both ac-curately predict the data and are phonologically meaningful, providing a serious alternative to the strong nativist view of the OT constraint inventory. We wish to thank Eric Bakovi  X  c, Emily Mor-gan, Mark Mysl  X   X n, the UCSD Computational Psy-cholinguistics Lab, the Phon Company, and the re-viewers for their discussions and feedback on this work. This research was supported by NSF award IIS-0830535 and an Alfred P. Sloan Foundation Research Fellowship to RL.

