 Personalization of web search results as a technique for im-proving user satisfaction has received notable attention in the research community over the past decade. Much of this work focuses on modeling and establishing a profile for each user to aid in personalization. Our work takes a more query-centric approach. In this paper, we present a method for efficient, automatic identification of a class of queries we de-fine as localizable from a web search engine query log. We determine a set of relevant features and use conventional machine learning techniques to classify queries. Our exper-iments find that our technique is able to identify localizable queries with 94% accuracy.
 H.3.3 [ Information Storage And Retrieval ]: Informa-tion Search and Retrieval X  Search process ; I.5.4 [ Pattern Recognition ]: Applications X  Text processing Experimentation, Human Factors, Measurement Localizable query, web search, machine learning
Typical queries submitted to web search engines contain very short keyword phrases [6]. These short text queries are generally insufficient to fully specify a user X  X  informa-tion need, yet users still have an expectation of finding rel-evant content. With the global search market approach-ing 10 billion queries per month 1 and substantial incentives to increase market share, maximizing user satisfaction with query results is in constant focus. Researchers have investi-gated several techniques for personalizing search results by http://searchenginewatch.com incorporating contextual metadata about the user during both document retrieval and result ranking. Others have attempted to classify queries into general categories based on perceived user intent.

We address the problem from a different angle, by study-ing which metadata is most applicable to a particular query . In this paper, we will present a technique for automatically identifying a class of queries we define as localizable from a web search engine query log. Localizable queries are those search strings for which the user would implicitly prefer to see results prioritized by their geographical proximity;  X  X ir-port shuttle X  or  X  X talian restaurant X , for example, are likely submitted by a user with the goal of finding information or services relevant to their current whereabouts. Our analysis suggests a significant fraction of user queries would benefit from such localization. Users, however, explicitly add loca-tion constraints to less than one half of such queries. By automatically localizing the appropriate queries, we can not only improve a user X  X  search experience, but also the spon-sored advertisement matching for locally available goods and services.

In addition to traditional desktop and laptop PCs, mo-bile and handheld devices are rapidly becoming a major source for information retrieval from the web. Many such devices are already equipped with geocoding capabilities though GPS, triangulation, or other techniques, and mar-ket research estimates that nearly all will contain some form of location awareness within the next few years. Currently, providers typically offer location-aware services and mobile-specific content though special portals 234 . This creates a burden on both the user, who must remember the mobile-specific sites, and the content providers, who must maintain parallel versions of their sites.

We aim to eliminate the need for such redundancy and specialization by answering the question: How can  X  X radi-tional X  web search engines know when to incorporate a vari-ety of rich per-user metadata in their results? Our approach is to identify the queries which contain locations as contex-tual modifiers, and extract the base portion of those queries. We define a set of distinguishing features and experiment with a variety of classifiers to automatically identify the set of localizable queries from a search engine query log [20].
Our objective is to ultimately improve overall user satis-faction with web search results by automatically localizing queries when appropriate. It is worth noting that, in this pa-http://www.google.com/mobile/ http://livesearchmobile.com/ http://mobile.yahoo.com/ per, we are focusing on the task of identifying which queries would benefit from localization. The decision to localize is based on a classifier we construct, and thus we feel it is im-portant to prioritize the accuracy of classification. In partic-ular, we wish to avoid  X  X alse-positive X  classifications, which would lead to erroneously localizing queries. In cross valida-tion experiments, our technique correctly identifies approx-imately 46% of the  X  X ocalizable X  queries with 94% accuracy.
The rest of the paper is organized as follows. Section 2 discusses related work in query refinement and personal-ization. We discuss user studies providing the motivation behind our work in section 3 and outline our approach in section 4. We analyze the query log data and discuss lo-calizable query identification in section 5, and describe the relevant features for classification in section 6. Our results with several supervised classifiers are presented in section 7. Section 8 discusses our conclusions and presents some ideas for future work.
Considerable work has been done in the area of person-alized and per-user web search. Researchers have investi-gated several data sources for refining or expanding user queries with relevant contextual terms, including in-link an-chor text [13], co-occurring terms within the result set [25, 2], co-occurring terms in query logs [5], and keywords from the user X  X  desktop environment [12].

Personalization of ranking algorithms was proposed in [19] by adjusting the  X  X andom-surfer X  model based on user pref-erences. Jeh and Widom[7] addressed some of the scalability concerns of this personalized PageRank approach.

Taxonomies and ontologies have been used to filter and rank search results using concept weights learned from user browsing behavior [22, 24]. Liu et. al [17] discuss person-alizing and disambiguating queries by classifying them into per-user category profiles based on past browsing history.
Others have performed query-dependent studies, though the focus is typically not per-user personalization. Lau and Horvitz [14] use Bayesian networks to estimate user goals by classifying query refinement patterns found in search engine logs. Lee et. al [15] present a set of features for automati-cally classifying user intent as navigational or informational for a set of queries.

Many of these personalization techniques are effective when a user X  X  query is ambiguous, such as those containing terms with homonyms or multiple senses. Our work differs from most prior research in personalized web search by addressing personalization from a query-dependent focus, rather than user-dependent. This allows us to personalize results by utilizing applicable metadata whenever it is deemed appro-priate, not only for ambiguous queries.
Before we discuss the technical aspects of our work, it is worth spending a moment to focus on a few important preliminary questions. In particular, we wish to verify that the concept of  X  X ocalizable X  is consistent amongst users, and that automatic localization is worthwhile .
Web search engines typically incorporate a complex mix of factors when ranking query results. Factoring in local-ization increases the complexity of the system, and so it is important to verify that its overall impact justifies that com-plexity. To that end, we conducted a user study to estimate the percentage of query instances which would benefit from localization. We gave 9 survey participants each a different list of 100 randomly sampled entries from an America On-line search query log [20] and asked them to classify each query into one of three categories: 1. Query would likely not benefit from localization 2. Query would likely benefit from automatic localization 3. Query is already localized Our survey participants said that, on average, 70% of queries would not benefit from localization, 16% of queries would benefit from automatic localization, and 14% of queries were already localized. Automatic localization may potentially improve the results for a significant fraction of user queries, as these results suggest that, while approximately 30% of the queries issued to search engines are localizable, users only explicitly localize about one-half of them.
Research involving user satisfaction and search result per-sonalization typically must deal with some level of subjec-tivity. Automatic localization is a form of personalization, and so we pose the question: do users generally agree on which queries should be localized ? To address this issue, we administered a second user survey. As our goal is to now see whether users agree on which queries are localizable, and approximately 15% of queries in the log are localizable, we felt a random sample from the query log would not provide sufficient opportunity for users to disagree. We constructed a list of 102 queries, approximately one half of which we believed to be localizable. This list was presented to 8 par-ticipants who were asked to make a binary judgment for each query about whether it would benefit from localization or not.

The results were tabulated to determine whether users agree on which queries would benefit from localization. Fig-ure 1 shows a plot of user agreement, with the number of users who disagreed with the majority along the X-axis, and the number of queries on the Y-axis. We found that users agreed that queries for goods and services, such as  X  X ood supplies X  and  X  X ome health care providers X  were localizable, while more general queries for information, such as  X  X alories coffee X  and  X  X ye chart X  are not. The intent of other queries, such as  X  X edical license X  and  X  X arathon X  are more vague, and our survey participants were evenly divided.

We note that subjects were asked to make their best inter-pretation of the query intent given only the query text, and so some level of discrepancy is expected. The overall results are encouraging, as we see that users are evenly divided on only 8 of the queries, while at most one person disagreed with the majority for about 50% of the queries.
Without a complete implementation of a X  X ocalizing X  X earch system to perform experiments with, we must find other ways to estimate the overall user satisfaction with query lo-calization. User studies by Joachims et. al. [9] suggest that clickthroughs are a reasonable approximation of relevance feedback. During query feature collection, we measured the clickthrough rates for both the localized and non-localized form of the same  X  X ase X  query, and found that the aver-age clickthrough rate for the localized instances of a local-izable query is approximately 17% higher than for the non-localized instances. While not a perfect interpretation of user preference, it is an auspicious precursory indicator.
For any query q , we wish to efficiently determine whether q is localizable or not. Our basic approach is to build a query classifier using features collected from a web search engine query log. This classifier can then be used by the search engine to make realtime decisions about localization on a per-query basis.

We begin identifying localizable queries by finding pre-viously issued queries which contain an explicit localiza-tion modifier, with the assumption that the  X  X ase X  of these queries may be generally localizable. We identify all entries in the query log which contain  X  X ocations X  and extract the  X  X ase X  of these queries. Once we have the set of all base queries, we select a sample to use for classifier training. For this subset of queries we compute relevant distinguishing features and evaluate multiple well-known supervised classi-fiers to determine which are best suited for our task. Each of these steps are discussed in the following sections. In our analysis we use a search query log from America Online [20], which contains queries from 657,426 distinct users over a three month period from March 1 to May 31, 2006. The log contains approximately 36 million rows of data, covering 10 million textually unique queries from 21 million search  X  X nstances X .

We start construction of our classifier by finding queries in the log which contain location modifiers. The AOL query log contains queries in the English language, and so we have focused our location identification on states, counties, and cities in the United States using a list available from the U.S. Census Bureau 5 . For queries which contain one or more of these locations, we consider the location as a contextual modifier added by the user, and remove it to find the  X  X ase X  query. For example, the base of the query  X  X an francisco public parks X  is  X  X ublic parks X . These base queries are the ones which we would like to automatically localize. http://www.census.gov/
Query ID Base Query Location Tag 10005397 county florida animal shelter city:lee 10005397 |  X  X  county animal shelter city:florida 10005397 |  X  X  county animal shelter state:florida 10005397 florida animal shelter county:lee county 10005397 |  X  X  animal shelter city:florida 10005397 |  X  X  animal shelter state:florida 10005397 lee county animal shelter city:florida 10005397 |  X  X  county animal shelter city:lee 10005397 |  X  X  animal shelter county:lee county 10005397 lee county animal shelter state:florida
In the remainder of this section, we will discuss how base queries are identified from the query log, how queries sharing a  X  X imilar X  base are grouped together, and some considera-tions for matching user queries to entries in the log.
Queries are typically very short, consisting of only 2-3 terms [6] rather than complete, grammatically correct sen-tences. Additionally, all queries in the log have been nor-malized to lower case. As a result it is difficult to employ general linguistic approaches, such as parts-of-speech tag-ging, to aid location tagging. Likewise, techniques based on other indicators, such as capitalization or punctuation, may not reliably label locations within queries.

Instead of using cumbersome grammatical tools to iden-tify locations, we use a simple string matching process, and ensure accuracy using a set of features carefully selected to eliminate false positives. While relatively straightforward, this technique proves quite effective, and the simplicity sup-ports scalability as well as language independence.
To identify localized queries, we inspect the text of each query and compare it to the Census Bureau list of loca-tions. Every match generates a new base query, where the matched portion of text is tagged with the detected location type (state, county, or city). Queries may contain multi-ple localizations, such as a city and state name. Rather than complicate our tagger, we choose to simply remove the tagged tokens and enqueue the remainder of the query for further processing. As a result, a single entry in the query log may produce multiple base queries.

We favored this technique over removing all  X  X ocations X  and generating a single base query from each entry because, in general, we cannot be certain when query terms are spec-ifying a location. Several words in the English language are also used as city names, such as Parks, Arizona. If we choose to remove all terms matching a location in a single step, we would not be able to identify the correct base  X  X ublic parks X  in the example  X  X an francisco public parks X  discussed above.
Table 1 shows all of the base queries generated from the source query  X  X ee county florida animal shelter X . Indentation is used to illustrate how the original query is processed to ultimately result in each of the possible base queries shown. For example, the first row is obtained by removing  X  X ee X  from the original query. The second and third rows are generated by further tagging the resulting base query  X  X ounty florida animal shelter X . For the approximately 10 million distinct entries in the query log, we identify 4.9 million unique base queries.
As Table 1 shows, tagging the query  X  X ee county florida animal shelter X  generates 10 entries comprising 5 textually distinct base queries. After processing the entire query log, we group together queries which share a  X  X imilar X  base query q , and define L ( q b ) as the set of location tags which occur with q b . We explored several alternatives for this similarity mapping, ranging from an exact string match to a bag-of-words model with stopwords eliminated and terms stemmed using Porter X  X  suffix stemming algorithm [21].

The choice of mapping function has implications on the accuracy and coverage of our classifier, as well as how we determine which base query a potentially localizable user query issued to a search engine corresponds to. We will now briefly discuss some of the options considered.
An exact match model produces the largest set of distinct base queries, as we only group together the entries that are textually equivalent. Using the entire text of a query al-lows us to distinguish between semantically different queries whose text may, from an algorithmic point of view, only differ in seemingly insignificant ways.

Exact matching, however, also potentially introduces many unintelligible base queries. With location terms removed from a query, the remaining text may never actually be is-sued to a search engine as a query by itself. Extracting the base of the query  X  X arks in [city] X  would produce  X  X arks in X , which is unlikely to appear as a user query, and in fact does not appear in the query log. The query  X  X arks X , however, occurs 53 times.
Many modern information retrieval systems ignore com-mon words, such as conjunctions and prepositions, frequently referred to as stopwords. By eliminating stopwords from queries, we more easily group together logically equivalent queries, such as  X  X arks in [city] X  and  X  X arks near [city] X  into a single base  X  X arks X .
A bag of words model ignores the ordering of terms in a query. Combined with stopword elimination, this may help consolidate semantically equivalent queries such as  X  X irport shuttle X  and  X  X huttle to airport X  into a single common base. In some cases, however, ignoring the word ordering may ac-tually change the meaning of the query.
Term stemming algorithms, such as the suffix stemming algorithm described by Porter [21], can help normalize term tense and plurality. While this may improve precision for some semantically equivalent queries such as  X  X estaurant X  and  X  X estaurants X , it may also occasionally result in colli-sions between distinct terms which share a common stem, reducing precision. For example,  X  X niverse X  and  X  X niversity X  share the common stem  X  X nivers X .

Other forms of  X  X temming X , such as morphological analysis and lemmatization, may produce more accurate results for related term grouping than algorithmic affix or suffix stem-ming. Lemmatization is frequently discussed in the field of statistical machine translation [16]. Such techniques are significantly more complex, however, typically requiring ad-Stopwords Eliminated Bag-of-words Stemmed Queries ditional data sources such as a lexicon and parts-of-speech tagger.
In our classifier evaluation, we found that stopword elim-ination is the only preprocessing step which has significant impact on the final classification results. Fundamentally, we feel it provides the best combination of normalizing logically equivalent queries with minimal semantic loss. As Table 2 shows, additional processing does not noticeably reduce the size of the base query set, and as a result, calculated feature scores will not change significantly. In the remainder of this paper, when we refer to the base of a localized query, we are referring to the stopword eliminated version.
Our tagging process generates a base query any time it finds text which matches a location. Several city and state names have homonyms, and thus text matching is not suffi-cient. For example,  X  X ansas X  may refer to the state, one of several cities, the rock band, or even a particular movie with that title. When we find a query containing  X  X ansas X , how do we know whether the user was referring to a location or one of the other senses of the word?
In this section we discuss a set of features measurable from a query log which a supervised classifier may use to make that determination, and discriminate localizable queries from the false positives. We investigated several features, both about the individual queries as well as aggregate measures of the grouped queries. Some query features, such as frequency counts, have relatively straightforward interpretations. Oth-ers are more subtle and require additional discussion.
Although our analysis is performed over a window of user queries contained within a log, real-world query logs col-lected by search engines are constantly expanding. This necessitates the ability to adapt our classifications as new examples are collected, and we felt it was important to con-sider this when selecting features. We therefore focused our feature selection on those which are easily calculated incre-mentally as the data expands.
Online information sources such as Wikipedia 6 rely on the collective expertise of their users to ensure the knowl-edge base is accurate. We adapt a similar model for web search queries, where every query instance can be treated as a  X  X ote X . In our case, users vote for the localization of query q by submitting it to the search engine with a location spec-ified.

For every textually distinct query q i we define b i  X  [0 the fraction of users who would benefit from the localization of q i .Everyquery q i has an associated value r i  X  [0 , 1], http://www.wikipedia.org
Table 3: Locations Occurring with  X  X eclaration X  defined as Q i is the count of all instances of q i in the query log, and Q ( L ) is the count of all query instances tagged with some location  X  L , for which q i is the base. This r i value repre-sents the  X  X ocalization ratio X  for q i . Assuming some fraction of the users who issued q i to the search engine implicitly wanted localized results, r i defines an estimated lower bound on b i .

Localization ratio provides some insight into what fraction of users believe that a particular query would benefit from localization. It is, however, susceptible to small sample sizes, as a query issued by a single user may have an r i value of 1. Localization ratio is also unable to identify false positives resulting from incorrectly tagged locations. For example, the base query  X  X arnes X  has an r value of over 0.75, yet a vast majority of its occurrences come from incorrectly tagging  X  X oble X  as a location in the query  X  X arnes and noble X . For these reasons, localization ratio is insufficient as the sole feature for classification.
Our query tagging process is based on string comparison, creating a match for any text which is listed as a US state, county, or city. Some of these locations are homonyms in the English language, introducing false positives to the can-didate list which must be filtered out. As an example,  X  X nde-pendence X  is a city in Missouri, and is tagged as such in the query  X  X eclaration of independence X . The base query  X  X ec-laration X  occurs a total of 176 times with some location, 113 alone of which are due to the query  X  X eclaration of indepen-dence X . Table 3 shows the top 4 most frequently occurring locations for the base query  X  X eclaration X .

To aid in identifying these entries, we start with a basic assumption about any localized query q l : That is, given an instance of any localized query q l with base q b , the probability of q l containing location is approx-imately equal across all possible locations of q b . Base queries which have a highly skewed distribution of location occur-rence counts suggest that either the query is only relevant to those locations, or the tagged  X  X ocation X  is actually part of the query, rather than a localization modifier.
To estimate the distribution, we calculate several mea-sures for the set of locations  X  L ( q b ), including minimum, maximum, mean, median, and standard deviation of their occurrence counts.
Search result clickthrough rates have been used in stud-ies evaluating and improving the effectiveness of web search engines [8]. User studies by Joachims et. al. [9] suggest that clickthroughs are a reasonable approximation of relevance feedback.

These studies focus on improving the document order-ing by treating user clickthroughs as relative relevance judg-ments and adjusting for any bias the presented ordering in-troduces. We are primarily concerned, however, with the relevance of the overall result set, as opposed to the relative ranking of the returned documents. In our experiments, we use clickthrough events recorded in the query log as a comparator of user satisfaction with the localized and non-localized versions of their search results. We define a binary user satisfaction function with the results of query instance q as: We compute the total clickthrough count for each base query q as the sum of S over all instances of q b in the query log. This sum is then divided by the number of instances of q b calculate the clickthrough rate. Likewise, we calculate the clickthrough count and rate for the set of localized queries with base q b . We may then compare the clickthrough rates of both the localized and base forms of a query, where a positive difference is considered as an indicator of increased user satisfaction with the results.
At first glance, frequency count measures may seem like potential red herrings, given that there is no practical bound on their values. For example, the query  X  X arnes and noble X  is tagged as  X  X arnes and [city:noble] X  and  X  X city:barnes] and noble X , due to the cities of Noble, Oklahoma and Barnes, Kansas. The query  X  X arnes and nobel X  occurs in 2679 unique search sessions, significantly higher than the average of just over 2 occurrences. If we judged localizabilty based on a threshold of popularity, both of the base queries X  X arnes X  X nd  X  X oble X  would likely be (incorrectly) identified as localizable. Despite this, frequency counts are still useful measures. In particular, frequency count serves as a normalizing or significance factor for other features, such as r , by taking into account a query X  X  popularity.
In addition to query occurrence counts, we also consider the sources for those occurrences. For every query q i ,we calculate the number of distinct users who have issued the query, in both its localized and non-localized forms. These two measures provide a different form of normalization for the occurrence counts q and q L by adjusting for bias intro-duced from a single user issuing the same query multiple times.
Using the features discussed in the previous section, we now turn our attention to evaluating classification algorithms for learning localizable queries. We manually tagged a train-ing data set and evaluated the effectiveness of several super-vised learning algorithms, including naive Bayesian, decision trees, support vector machines, and neural networks. In ad-dition to these individual classifiers, we evaluated techniques for improving accuracy by combining multiple classifiers, in-cluding AdaBoost [4], Bayesian boosting [11], and indepen-dent majority voting.

Our experiments were conducted using classifiers and boost-ing techniques implemented as part of the RapidMiner [18] machine learning framework.
In order to eliminate the most impertinent data, we per-formed two filtering steps when selecting our training data set. Prior to selection, we removed candidate queries with localization count q L &lt; = 1, which reduced the size of the stopword-eliminated candidate localizable query set to ap-proximately 1.7 million. We selected a random sample of 200 entries from this set and, prior to tagging it, performed an additional filtering step as follows: we removed queries which occurred with only one distinct location modifier ( n L &lt; were only issued by a single user ( u q = 1), or whose base form was never issued to the search engine ( q =0).
After this filtering, we manually tagged the 102 remain-ing entries from our random sample of candidate localizable queries, 48 of which the author deemed to be localizable. This training set consisting of 48 positive (localizable) and 54 negative (non-localizable) examples was used in a series of classification experiments discussed below. This set com-prises the same queries presented to users in our survey dis-cussed in section 3.2, where the author X  X  classification agreed with the majority for 91 of the 102 entries.
We compare the effectiveness of several well-known super-vised classifiers using standard precision and recall measures. As our overall goal is to identify localizable queries and ulti-mately use that knowledge to alter search query results, we feel it is important to emphasize precision over recall, and in particular, the accuracy of positive (localizable) classifi-cations. In terms of user satisfaction, we believe correctly localizing a smaller subset of all localizable queries is prefer-able to localizing a larger subset at the expense of increasing the number of incorrectly localized queries.

The precision and recall measures discussed below are for positive example identification based on 10-fold cross-validation experiments. To compensate for our filtering step on the training data, we consider the queries removed to be classified as non-localizable. While filtering does not affect the computed precision, based on our survey in section 3.1 we approximate 15 of these 98 queries are localizable, and adjust the recall score accordingly.
Using a set of (assumed independent) feature scores, a naive Bayesian classifier estimates the probability a given in-stance belongs to each of the possible discrete output classes. In our case, each instance is a user query, and the output is a boolean variable specifying whether the query is local-izable or not. Despite simplistic independence assumptions, naive Bayes classifiers typically perform comparably to more complex classifiers [23]. For our data set, the naive Bayes classifier achieves 55% precision at 59% recall.

In addition to feature independence, naive Bayes classi-fiers assume continuous variables follow a Gaussian proba-bility distribution. The distribution for some features, such as localization ratio ( r ), follows such a distribution. Other features, such as location frequency count ( n L ) do not follow a Gaussian, as seen in Figures 2 and 3.

As the Gaussian assumption does not hold for all fea-tures, we investigate an alternative. Flexible Naive Bayes classifiers use a kernel-based density estimation function for continuous variables, and have been shown to greatly reduce the error rate of naive Bayes classifiers [10]. A kernel-based naive Bayes classifier improves the classification accuracy to 64% precision, albeit at a reduction in recall to 43%.
Decision trees are widely used in data mining and ma-chine learning applications. When constructing a decision tree, the training example set is recursively divided into sub-groups based on a particular feature. In our experiments, we construct decision trees with three distinct split criteria: information gain, the Gini coefficient, and the normalized in-formation gain ratio. Table 5 shows the precision and recall measurements for each of these criteria.

A significant advantage of decision trees is the transparency of the final classifier. We inspected each of the three sepa-rate decision trees generated to study which features were the most distinguishing. The localization ratio r was used in all three trees, as were some combination of location dis-tribution measures ( n L ,  X  n L ,and  X  n L ). Click-through rates (  X  c q and  X  c q L ) were factors in two of the three trees.
Support Vector Machines (SVMs) [3] are a popular form of supervised learner. SVM is well suited to binary classifi-cation problems, where each instance can be represented by asetof n distinct numeric values.

Like many vector-based techniques, SVM classifiers are relatively opaque, making it more difficult to manually in-spect and determine which features contributed most signif-icantly to the classification. Regardless, the accuracy and recall of SVM for our classification task surpasses decision trees, achieving 75% precision at an 62% recall rate.
Neural Networks are relatively complex systems capable of, among other tasks, supervised learning for classifica-tion [1]. The nodes in a neural network can be separated into input and output layers, and some number of internal  X  X idden X  layers. We evaluated feedforward neural networks comprising of one to three hidden layers, beyond which re-call for positive training examples dropped to zero. Table 6 show the results, which indicate that neural networks are the most accurate of the individual classifiers evaluated.
Boosting algorithms, such as AdaBoost [4], have been shown to improve the accuracy of  X  X eak X  learning classi-fiers. The final  X  X trong X  classifier produced by the boosting algorithm generally consists of a weighted combination of multiple weak classifiers, iteratively trained with a weighted set of examples based on previous classification errors. We evaluated the effectiveness of BayesBoost [11] with naive Bayesian classifiers and AdaBoost with decision trees. The results were mixed, as shown in Table 7. In some cases, precision and recall actually decreased.
While the supervised classifiers discussed above produce relatively high precision results, we noticed that the set of false positives (queries incorrectly classified as localizable) produced by the individual classifiers did not fully overlap. We experimented with another style of aggregate learner, where the final classification is determined by the majority vote from a set of discrete classifiers. Unlike boosting, which builds a final classifier from multiple instances of the same learning algorithm trained on varying example sets, this  X  X n-semble X  style classifier comprises distinct learners trained on the same example set.

We choose to combine the best individual performing clas-sifiers using a simple majority vote scheme, where each com-ponent classifier is given equal weight, we achieved signifi-cantly higher precision than any individual classifier: up to 94% precision at 46% recall. Table 8 shows the results for three such voting classifiers, each consisting of a neural network with two hidden layers, an SVM classifier, and a decision tree with the specified split criteria.
Our evaluations demonstrate that conventional supervised learning algorithms are capable of distinguishing localizable queries with relatively high levels of precision. Neural net-works successfully identify over one half of localizable queries with 85% accuracy. SVMs identify a larger subset of localiz-able queries than neural networks, while precision decreases to 75%. Taking the majority vote of these two independent classifiers along with a decision tree, we are able to achieve over 90% classification accuracy.
In this paper we have presented a scalable technique for determining which query strings submitted to a web search engine would benefit from automatic localization. Using data from a query log, we have shown that straightforward query tagging combined with an appropriate set of features and a standard supervised classifier can achieve up to 85% precision. A meta-classifier comprised of three conventional classifiers performs even better, achieving 94% precision in cross-validation experiments.

Our focus has been on identifying explicit locations within the United States, such as city or state names. This could be expanded, however, to include other locale data, such as specific sites or landmarks (e.g.  X ... near the Eiffel Tower X ) or relative locations (e.g.  X ... by the public library X ).
Our work has focused on determining whether a particular query is localizable or not. Once these queries are identified, a next logical step is to evaluate techniques for integrat-ing the classifier into an information retrieval system. With proper indexing, it should be possible to compute the feature scores and classify a user query in realtime. Once a decision to localize has been made, the system must determine the proper degree of localization. For example, should the query be localized to the state or city level? Our tagging process maintains information about the specific locations which oc-cur with each query, making this data readily available.
The features discussed in section 6 were selected based on the static nature of the available query log data. With a  X  X ive X  system, we may design relevance experiments to collect more dynamic features for use in classification. Incorporat-ing a mix of localized and non-localized results for a user query and measuring user activity (e.g. clickthoughs) may be used to evaluate user preferences, and act as a feedback loop to future iterations of the classification algorithm to further improve precision.
This work is partially supported by NSF grants, IIS-0534784 and IIS-0347993. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the funding institutions. [1] C.M.Bishop. Neural Networks for Pattern [2] C. Buckley, G. Salton, J. Allan, and A. Singhal. [3] C. J. C. Burges. A tutorial on support vector [4] Y. Freund and R. E. Schapire. Experiments with a [5] C.-K. Huang, L.-F. Chien, and Y.-J. Oyang. Relevant [6] B. J. Jansen, A. Spink, and T. Saracevic. Real life, [7] G. Jeh and J. Widom. Scaling personalized web [8] T. Joachims. Optimizing search engines using [9] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [10] G. H. John and P. Langley. Estimating continuous [11] Y.-H. Kim, S.-Y. Hahn, and B.-T. Zhang. Text [12] R. Kraft, C. C. Chang, F. Maghoul, and R. Kumar. [13] R. Kraft and J. Zien. Mining anchor text for query [14] T. Lau and E. Horvitz. Patterns of search: Analyzing [15] U. Lee, Z. Liu, and J. Cho. Automatic identification of [16] Y.-S. Lee. Morphological analysis for statistical [17] F. Liu, C. Yu, and W. Meng. Personalized web search [18] I. Mierswa, M. Wurst, R. Klinkenberg, M. Scholz, and [19] L. Page, S. Brin, R. Motwani, and T. Winograd. The [20] G. Pass, A. Chowdhury, and C. Torgeson. A picture of [21] M. F. Porter. An algorithm for suffix stripping. [22] A. Pretschner and S. Gauch. Ontology based [23] I. Rish. An empirical study of the naive bayes [24] M. Speretta and S. Gauch. Personalized search based [25] J. Xu and W. B. Croft. Query expansion using local
