 In the standard Markov Decision Process (MDP) formal-ization of the reinforcement-learning (RL) problem (Sut-ton &amp; Barto, 1998), a decision maker interacts with an en-vironment consisting of finite state and action spaces. Al-gorithms for RL in MDP environments suffer from what is known as the curse of dimensionality : an exponential ex-plosion in the total number of states as a function of the number of state variables . Learning in environments with extremely large state spaces is challenging if not infeasible without some form of generalization. Exploiting the un-derlying structure of a problem can enable generalization and has long been recognized as important in representing sequential decision tasks (Boutilier et al., 1999). In this paper, we propose an extension to the standard MDP formalism, which we call Object-Oriented MDPs (OO-MDPs) , and present an efficient learning algorithm for de-terministic OO-MDPs. We claim that this object-based ap-proach is a natural way of viewing and describing many real-life domains that enables multiple opportunities for generalization. There are many ways of incorporating ob-jects into models for learning and decision making X  X his paper explores one particular approach as a first attempt to understand the issues that arise.
 Our representation has multiple connections with other for-malisms proposed in the Relational Reinforcement Learn-ing literature (van Otterlo, 2005), but emphasizes simplic-ity and tractability over expressive power. Our representa-tion starts from attributes that can be directly perceived by the agent, rather than predicates or propositions introduced by the designer (although we allow the encoding of prior knowledge in propositional form). A similar formalism, relational MDPs (RMDPs) , was introduced by Guestrin et al. (2003) in the context of planning, and is based on the same insight. While our formalism has similarities to RMDPs , we introduce a number of changes, mainly in the way transition dynamics are described, to enable efficient learning and generalization.
 To present and test our approach, we first provide bench-mark experiments in the well-known Taxi domain (Diet-terich, 2000). We further demonstrate its applicability by designing an agent that can solve an interesting problem in the real-life videogame Pitfall 1 . We use a standard Markov Decision Process (MDP) no-tation throughout this paper (Puterman, 1994). A finite MDP M is a five tuple S , A ,T, R , X  . We use T ( s | s, a to denote the transition probability of state s given state X  action pair ( s, a ) and R ( s, a ) to denote the expected reward value. A deterministic MDP is one in which there is a sin-gle next state s for every given state s and action a ; that is,  X  s  X  X  ,a  X  A ,  X  s  X  X  : T ( s | s, a )=1 . We will use the Taxi domain, defined by Dietterich (2000), as an example to introduce our formalism. Taxi is a grid-world domain (see Figure 1.a), where a taxi has the task of picking up a passenger in one of a pre-designated set of locations (identified in the figure by the letters Y, G, R, B ) and dropping it off at a goal destination , also one of the pre-designed locations. The set of actions for the taxi are North, South, East, West , PICKUP and DROPOFF . Walls in the grid limit the taxi X  X  movements.
 A common factored-state representation for the Taxi prob-lem uses Dynamic Bayesian Networks (DBNs) to indicate how state variables influence each other. For example, the location of the taxi after a North action only depends on its current location and is independent of the passenger or destination variables.
 We depart from this representation and introduce one based on objects and their interactions. Many elements in our representation are similar to those of relational MDPs (Guestrin et al., 2003) with significant differences in the way we represent transition dynamics. Similar to RMDPs , we define a set of classes C = { C 1 ,...,C c } . Each class in-cludes a set of attributes Att ( C )= { C.a 1 ,...,C.a a } each attribute has a domain Dom ( C.a ) . A particular envi-ronment will consist of a set of objects O = { o 1 ,...,o where each object is an instance of one class: o  X  C i . The state of an object o. state is a value assignment to all its at-tributes. The state of the underlying MDP is the union of the states of all its objects: s = o i =1 o i . state . An OO-MDP representation of Taxi has four object classes: Taxi , Passenger , Destination and Wall . Taxi , Passenger and Destination have attributes x and y , which define their lo-cation in the grid. Passenger also has a Boolean attribute in-taxi , which specifies whether the passenger is inside the taxi. Walls have an attribute that indicates their position in the grid. The Taxi domain, in its 5  X  5 version shown in Figure 1.a, has one object of each class Taxi , Passenger , and Destination , and multiple ( 26 ) objects of class Wall . This list of objects points out a significant feature of the OO-MDP representation. Whereas in the classical MDP model, the effect of encountering walls is felt as a prop-erty of specific locations in the grid, the OO-MDP view is that wall interactions are the same regardless of their loca-tion. As such, agents X  experience can transfer gracefully throughout the state space.
 When two objects interact in some way, they define a rela-tion between them. A combination of the relation estab-lished, plus the internal states of the two objects, deter-mines an effect  X  X  change in value of one or multiple at-tributes in either or both interacting objects. This behavior is defined at the class level, meaning that different objects that are instances of the same class behave in the same way when interacting with other objects. Formally, a relation r : C i  X  C j  X  Boolean is a function, defined at the class level, over the combined attributes of objects of classes C and C j . Its value gets defined when instantiated by two ob-jects o 1  X  C i and o 2  X  C j . For our Taxi representation, we will define 5 relations: touch N ( o 1 ,o 2 ) , touch S touch E ( o 1 ,o 2 ) , touch W ( o 1 ,o 2 ) and on ( o 1 ,o fine whether an object o 2  X  C j is exactly one cell North, South, East or West of an object o 1  X  C i , or if both objects are overlapping (same x, y coordinates). Different domains require different relations.
 When the object taxi i  X  Taxi is on the northern edge of the grid and tries to perform a North action, it hits some object wall j  X  Wall and the observed behavior is that it doesn X  X  move. We say that a touch N ( taxi i , wall j ) rela-tion has been established and the effect of an action North under that condition is no-change . On the other hand, if  X  touch N ( taxi i , wall j ) is true and the taxi performs the ac-tion North , the effect will be taxi i .y  X  taxi i .y +1 .As stated before, these behaviors are defined at the class level, so we can refer in general to the relation touch N ( Taxi , Wall as producing the same kind of effects on any instance of taxi i  X  Taxi and wall j  X  Wall .
 We define some properties of these transition dynamics more formally in the next section. 3.1. Transition Dynamics Every state s induces a certain value assignment to all at-tributes of all objects X  X nd therefore all relations X  X n the domain. Transitions are determined by interactions be-tween objects. Every pair of objects o 1  X  C i and o 2  X  C the set of relations r ( o 1 ,o 2 ) that are true X  X r false X  X t the current state, determine an effect X  X  change of value in some of the objects X  attributes.
 Definition 1 An effect is a single operation over a single attribute att in the OO-MDP. We will group effects into types , based on the kind of operation they perform. Ex-amples of types are arithmetic (increment att by 1 , subtract 2 from att), and constant assignment (set att to 0 ). Definition 2 A term t is any Boolean function. In our OO-MDP representation, we will consider terms representing either a relation between two objects, a certain possible value of an attribute of any of the objects or, more gener-ally, any Boolean function defined over the state space that encodes prior knowledge. All transition dynamics in an OO-MDP are determined by the different possible settings of a special set of terms called T .
 Definition 3 A condition is a set T c of terms and negations of terms from T that must be true in order to produce a particular effect e under a given action a .
 We can summarize an OO-MDP transition cycle as follows: 1: while agent is acting do 2: Agent observes current state s and returns action a . 3: From state s , the environment extracts all relations 4: For each (if any) fulfilled condition in T c , there X  X  an 5: If no conditions were fulfilled, no change takes place 6: Otherwise, the environment uses the set of effects to 7: The environment chooses a reward r from R ( s, a ) . 8: Agent is told r . 9: end while We introduce Deterministic Object-Oriented Rmax ( DOORMAX ), an algorithm for learning and solving deterministic OO-MDPs. DOORMAX is correct and, as we will show, provably efficient under the following assumptions.
 Assumption 1 For each action and each attribute, only ef-fects of one type can occur.
 Assumption 2 For every action a , attribute att and effect type t , there is a set CE of condition X  X ffect pairs that deter-mine changes to att given a . No effect can appear twice on this list, and there are at most k different pairs X  |CE| X  Plus, no conditions T i and T j in the set CE contain each other:  X  ( T i  X  T j  X  T j  X  T i ) . The number of terms or negations of terms in any condition is bounded by a known constant M .
 Assumption 3 Effects are invertible , that is, given states s and s , for each attribute att and each effect type we can determine a unique effect that would transform att from its value in s to its value in s . 4.1. Definitions and Data Structures We introduce some definitions, notation, and data struc-tures that will be used to describe DOORMAX :  X  T is the union of all terms t that will be involved in  X  For every state s  X  S , the function cond ( s ) returns the  X  A condition T c  X  T is represented by a string c S of  X  Given two conditions represented as strings c 1 and c 2  X  A condition c 1 matches another condition c 2 , noted  X  For any states s and s and attribute att , the function  X  A prediction p is a pair ( p. model ,p. effect ) , where  X  For each action a , each attribute att and each ef- X  If an action a produces no effect from a given state  X  Two effects are incompatible if, for any initial value 4.2. OO-MDP Representation of Taxi To facilitate understanding of the notation and data struc-tures, we present a full example of our representation in the Taxi domain.
 The set of terms T , which determines the transition dynam-ics of the OO-MDP, includes the four touch N/S/E/W rela-tions between the taxi and the walls; the relevant relations between the taxi and the passenger and destination; the at-tribute value passenger . in-taxi = T ; and all their nega-tions: { touch N/S/E/W ( taxi , wall ) , on ( taxi , passenger ) } Consider the state s where the taxi is in position (2 , 4) in Figure 1.a), the passenger is inside the taxi, and the des-tination is G . For this state, the function cond ( s ) returns: { touch N ( taxi , wall ) ,  X  touch S ( taxi , wall ) , The corresponding 7 -character string representation for this condition is 1001001 , following the prior order for the terms.
 Let X  X  now assume that the agent tries to perform the ac-tion East , which takes it to state s where the taxi is in location (3 , 4) . The corresponding cond ( s ) is similar, ex-cept that now the taxi is not touching a wall to its West (  X  touch W ( taxi , wall ) ). The corresponding string represen-tation of the new condition is: 1000001 . The observed ef-fect is that the taxi moved to location (3 , 4) . In our repre-sentation, two effect types are allowed: arithmetic and con-stant assignment. Therefore, the function eff taxi .x ( s, s will return two values: increment (1) and set-to (3) . Now, the agent takes another East action, and gets to state s , where location is (4 , 4) , it X  X  touching a wall to the East and standing on the destination. cond ( s ) can now be rep-resented as 1010011 . The two observed effects to taxi .x are increment (1) and set-to (4) . Note that the transition model for an OO-MDP need not predict the changes to the condi-tions, only to the attributes. The condition values are then derived separately using the knowledge of the relevant re-lations and their definitions.
 Finally, we X  X l consider separately the actions that produce no effect. Let X  X  assume the agent also attempted an action North from each of the previous states, which resulted in it hitting a wall and staying in the same state. We treat these cases differently: The corresponding conditions 1001001 , 1000001 and 1010011 will be identified as failure condi-tions for action North and incorporated into the set F North Whenever we observe a new condition c i such that any ex-isting condition in F North matches it, we predict that per-forming a North action will have no effect. 4.3. Learning Algorithm The DOORMAX algorithm (Algorithm 1) follows the gen-eral structure of most RL algorithms in the Rmax fam-ily, which work as follows. Using examples of transi-tions ( s, a, s ) , a learning algorithm constructs the tran-sition model T . The learning algorithm must satisfy the KWIK (knows what it knows) conditions (Li et al., 2008), which say: (1) all predictions must be accurate (assuming a valid hypothesis class), and (2) however, the learning algo-rithm may also return  X  , which indicates that it cannot yet predict the output for this input. The sample complexity or KWIK bound of a learning algorithm is the maximum number of times it returns  X  . In the Rmax setting, any transition that cannot yet be predicted is assumed to lead to a fictious s max state from which maximum reward can be obtained.
 Algorithm 1 DOORMAX : main() method 1: // Set up data structures: 2: for all actions a  X  A do 4: for all attributes att  X  c  X  C Att ( c ) do 5: for all effect types type do 6: pred ( a, att , type )  X  X  X  7: Add pred ( a, att , type ) to set of active predic-8: end for 9: end for 10: end for 11: while  X  (Termination criterion) do 12: Observe current state s . 13: Choose action a according to exploration pol-14: Observe new state s . 15: Update learned model using method 16: end while The two main routines of the algorithm are predictTransition (Algorithm 2), which pre-dicts the next state given a current state and action based on the current model, and addExperience (Algorithm 3), which learns a model of the OO-MDP. If predictTransition is not able to predict a next state with accuracy, it returns s max .
 To help understand these routines, we present a couple of intuitions, based on the Taxi examples presented in the pre-vious section. Notice that if we applied the  X  operator to cond ( s ) and cond ( s ) , the two conditions from which an East action produced an increment (1) effect, we would ob-tain: 1001001  X  1000001 = 100 * 001 . The resulting con-dition indicates that the term touch W ( wall , taxi ) is irrele-vant with respect to action East and effect increment (1) we also compare the two pairs of effects obtained, we ob-serve that we consistently observed increment (1) , whereas set-to (3) and set-to (4) are incompatible effects. These ob-servations constitute the central ideas for the learning algo-rithm.
 Algorithm 2 predictTransition(s,a) method 0: Inputs: state s and action a . 0: Output: a predicted state s  X  S  X  X  s max } . 1: if  X  c  X  F a s.t. c | = cond ( s ) then 2: // The current condition is a known failure condition. 3: Return s 4: else 5: for all attributes att  X  c  X  C Att ( c ) do 6: E  X  X  X  7: for all effect types type do 8: if  X  p  X  pred ( a, att , type ) s.t. p. model | = 9: Add p. effect to E 10: end if 11: end for 12: if E =  X  X  X  X  e i ,e j  X  E s.t. e i and e j are incom-14: else 15: // Set E contains all the individual operations 16: s  X  apply E to s 17: Return s 18: end if 19: end for 20: end if Under the current assumptions, the effects of a given action on a given attribute assuming effects of a given type can be learned with a worst-case bound of O ( n M ) , where n = | is the number of terms and M is the maximum number of terms involved in any of the conditions. This worst-case bound can be guaranteed by a variant of SLF-Rmax , an al-gorithm introduced by Strehl et al. (2007).
 The uniqness assumption, Assumption 2, is not needed for SLF-Rmax to achieve this worst-case bound. However, DOORMAX , by taking advantage of this assumption, is able to learn faster in many domains. Some empirical evi-dence to support this claim appears in Section 6. If we assume M is a constant, SLF-Rmax can be used to provide guaranteed efficient results. However, for many domains DOORMAX will result much more efficient in practice. We conjecture that the two approaches can be run in parallel, to achieve the best of both.
 Intuitively, the good empirical results of DOORMAX lie in the way condition-effects are learned each time they are ob-served. The worst-case occurs when the agent observes an exponential amount of failures before observing instances of the set of effects it needs to learn.
 We now show that the problem of learning the transition dynamics of an OO-MDP has polynomial sample complex-ity in the KWIK setting, when by sample we only refer to the cases where an effect is observed (as opposed to failure samples where s = s ).
 We split the proof in two parts. First, we show that learning the right (condition, effect) pairs for a single action and at-tribute is KWIK-learnable, and then we show that learning the right effect type for each action X  X ttribute, given all the possible effect types, is also KWIK learnable.
 Theorem 1 The transition model for a given action a , at-tribute att and effect type type in a deterministic OO-MDP is KWIK-learnable with a bound of O ( nk + k +1) , where n is the number of terms in a condition and k is the maximum number of effects per action X  X ttribute.
 Proof: Given state s and action a , the predictor for effect type type will return  X  if cond ( s ) is not a known failure condition and there is no condition in pred ( a, att , type ) that matches cond ( s ) . In that case, it gets to observe s and updates its model with cond ( s ) and the observed effect e . We show that the number of times the model can be updated until it always has a correct prediction is O ( nk + k +1) :  X  if the effect e has never been observed before for  X  if the effect e has never been observed, but  X  if the effect e is such that there already exists a predic-Therefore, there can be at most nk + k +1 updates to the model for a particular action a , attribute att and effect type type before pred ( a, att , type ) either has a correct prediction or gets eliminated.
 Corollary 1 The transition model for a given action and attribute in a deterministic OO-MDPs is KWIK-learnable with a bound of O ( h ( nk + k +1)+( h  X  1)) , where n is the number of terms in a condition, k is the max number of effects per action X  X ttribute, and h is the number of effect types.
 Proof: Whenever DOORMAX needs to predict s given state s and action a , it will consult its current predictions for each attribute and effect type. It will return  X  if:  X  for any of the h effect types type i , pred ( a, att , type  X  for some attribute att , there are two effect types We have shown that, in total, DOORMAX will only predict  X 
O ( h ( nk + k +1)+( h  X  1)) times before having an accurate model of the transition dynamics for an action and attribute in the OO-MDP. First, we use the Taxi domain to demonstrate how DOOR-MAX makes use of the OO-MDP representation to outper-form Factored-Rmax , an algorithm based on a factored-state MDP representation. Second, we show how DOOR-MAX and Factored-Rmax scale when the size of the state space increases, by comparing them on the 10  X  10 version of Taxi. Finally, we demonstrate how DOORMAX can be Algorithm 3 addExperience(s,a,s X ,k) method 0: Inputs: an observation &lt; s,a,s &gt; ; k , the maximum 1: if s = s then 2: // Found a failure condition for action a , update F a 3: Remove all c  X  F a s.t. cond ( s ) | = c . 4: F a  X  F a  X  X  cond ( s ) } 5: else 6: for all attributes att  X  c  X  C Att ( c ) do 7: for all e  X  eff att ( s, s ) do 8: Find a prediction p  X  pred ( a, att ,e. type ) such 9: if  X  p then 10: // We already have a (condition, effect) pre-11: p. model  X  p. model  X  cond ( s ) S . 12: if  X  c  X  ( pred ( a, att ,e. type ) \ p ) . models s.t. 13: // Conditions overlap, violating an as-14: Remove pred ( a, att ,e. type ) from P 15: end if 16: else 17: // We observed an effect for which we had 18: if  X  c  X  pred ( a, att ,e. type ) . models s.t. 19: Remove pred ( a, att ,e. type ) from P 20: else 21: Add ( cond ( s ) ,e ) to pred ( a, att ,e. type ) . 22: // Verify that there aren X  X  more than k pre-23: if | pred ( a, att ,e. type ) | &gt;k then 24: Remove pred ( a, att ,e. type ) from P 25: end if 26: end if 27: end if 28: end for 29: end for 30: end if applied to succesfuly model and solve a real-life problem, the Pitfall videogame. 6.1. Taxi The first experiments we present are based on the Taxi do-main previously introduced. We run experiments on two versions: the original 5  X  5 -grid version presented by Di-etterich (2000), which consists of 500 states, and an ex-tended 10  X  10 -grid version with 8 passenger locations and destinations, with 7200 states (see Figure 1). The purpose of the extended version is to demonstrate how DOORMAX scales by properly generalizing its knowledge about con-ditions and effects when more objects of the same known classes are introduced.
 We compare DOORMAX against Factored-Rmax , an al-gorithm from the Rmax family that uses a factored-state MDP and models transitions using a DBN provided as in-put. Both algorithms are model based and use Rmax-style exploration, so we hope to be able to truly compare the un-derlying representations.
 The representation used for DOORMAX was described in the previous sections. In the case of Factored-Rmax ,we provide a DBN with some derived features that make learn-ing faster. The state variables used are the Taxi x and y locations, plus two Boolean features: in-taxi , representing whether the passenger is in the taxi , and at-destination , representing whether the taxi is standing at the passenger  X  X  destination .
 The experiments for both algorithms and both versions of the Taxi problem were repeated 100 times, and the re-sults averaged. For each experiment, we run a series of episodes, each starting from a random start state. We eval-uate the agent X  X  learned policy after each episode on a set of six  X  X robe X  combinations of taxi (x,y) location, pas-senger location, passenger destination . The probe states used were: { (2 , 2) ,Y,R } , { (2 , 2) ,Y,G } , { (2 , 2) the number of steps taken before learning an optimal policy for these six start states.
 The results are shown in the following table, with the last column showing the ratio between the results for the 10  X  10 version vs the 5  X  5 one: Number of states 500 7200 14.40
Factored Rmax
OO-Rmax We can see how DOORMAX not only learns with signifi-cantly less sample complexity, but also how well it scales to the larger problem. After increasing the number of states by more than 14 times, DOORMAX only requires 1 . 55 times the experience.
 The main difference between DOORMAX and Factored-Rmax is their internal representation, and the kind of gen-eralization it enables. After just a few examples in which  X  touch N ( taxi , wall ) is true, DOORMAX learns that the action North has the effect of incrementing taxi .y by 1 , whereas under touch N ( taxi , wall ) it fails. This knowledge, as well as its equivalent for touch S/E/W , is generalized to all 25 (or 100 ) different locations. Factored-Rmax only knows that variable taxi .y in state s depends on its value in state s , but still needs to learn the transition dynamics for each possible value of taxi .y ( 5 or 10 different values). In the case of actions East and West , it X  X  even worse, as walls make taxi .x depend on both taxi .x and taxi .y , which are 25 (or 100 ) different values.
 As DOORMAX is based on interactions between objects, it learns that the relation between taxi and wall is indepen-dent of the wall location. Each new wall is therefore the same as any known wall, rather than a new exception in the movement rules, the kind Factored-Rmax needs to learn. 6.2. Pitfall Pitfall is a video game released in 1982 by Activision for the Atari game console. The goal is to have the main char-acter ( Man ) traverse a series of screens while collecting as many points as possible while avoiding obstacles (such as holes, pits, logs, crocodiles and walls) and under the time constraint of 20 minutes. All transitions in Pitfall are deter-ministic. Our goal in this experiment was to have the Man cross the first screen from the left to the right with as few actions as possible. Figure 2 ilustrates this first screen. Our experiments were run using a modified Atari emula-tor that ran the actual game and detected objects from the displayed image. We used a simple heuristic that identified objects by color clusters and sent joystick commands to the emulator to influence the play. For each frame of the game, a list of object locations was sent to an external learning module that analyzed the state of the game and returned an action to be executed before the emulator continued on to the next frame. If we consider that we start from screen pixels, the flat state representation for Pitfall is enormous: 16 640 x 420 . By breaking it down into basic objects, through an object recognition mechanism, the state space is in the order of the number of objects to the number of possible locations of each object: 6 640 x 420 . OO-MDPs allow for a very succint representation of the problem, that can be learned with only a few experience samples.
 The first screen contains six object types: Man, Hole, Lad-der, Log, Wall and Tree . Objects have the attributes x , y , width and height , which define their location on the screen and dimension. The Man also has a Boolean at-tribute of direction that specifies which way he is fac-ing. We extended the touch X relation from Taxi to describe diagonal relations between objects, including: touch NE ( o i ,o j ) , touch NW ( o i ,o j ) , touch SW ( touch SE ( o i ,o j ) . These relations were needed to properly capture the effects of moving on and off of ladders. In our implementation of DOORMAX , we defined seven actions: WalkRight, WalkLeft, JumpLeft, JumpRight, Up, Down and JumpUp . For each of these actions, however, the emulator has to actually execute a set sequence of smaller frame-specific actions. For example, WalkLeft requires four frames: one to tell Pitfall to move the Man to the left, and three frames where no action is taken to allow for the ani-mation of the Man to complete. Effects are represented as arithmetic increments or decrements to the attributes x , y , width , height , plus a constant assignment of either R or L to the attribute direction .
 The starting state of Pitfall is fixed, and given that all transitions are deterministic, only one run of DOORMAX was necessary to learn the dynamics of the environment. DOORMAX learns an optimal policy after 494 actions, or 4810 game frames, exploring the area beneath the ground as well as the objects en route to the goal. Once the transi-tion dynamics are learned, restarting the game results in the Man exiting the first screen through the right, after jumping the hole and the log, in 94 actions ( 905 real game frames), which is what the optimal policy requires.
 A few examples of the (condition, effect) pairs learned by DOORMAX are shown below: We introduced OO-MDPs, an object-oriented representa-tion for reinforcement-learning problems that provides a natural way of modeling a broad set of domains, while en-abling generalization. We presented DOORMAX , a learn-ing algorithm for deterministic OO-MDPs that not only outperforms state-of-the-art algorithms for factored-state representations, but also scales very nicely with respect to the size of the state space, as long as transition dynamics between objects do not change. We presented bounds for learning transition dynamics of determinstic OO-MDPs in the KWIK framework.
 One limitation of our work is that we do not yet have a provably efficient algorithm for stochastic domains, which is part of our future work. While OO-MDPs can model stochastic transitions, a more complex learning algorithm would be needed to learn transitions effectively in the face of noise.
 The second component of our future research is the exten-sion of the object-oriented model to be able to handle in-heritance. We hope to be able to exploit knowledge about objects being part of a common super-class to learn their behaviors faster. Ideally, algorithms could also learn the object definitions and classes automatically, as well.
