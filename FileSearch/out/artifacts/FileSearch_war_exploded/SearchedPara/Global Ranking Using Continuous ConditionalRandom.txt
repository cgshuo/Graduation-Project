 bioinformatics, and speech recognition. In this paper, we take information retrieval as an example. on a single object. In other words, the relations between the objects are not directly represented in the model. In many application tasks this is far from being enough, however. For example, in Pseudo Relevance Feedback [17, 8], we manage to rank documents on the basis of not only relevance of documents to the query, but also similarity between documents. Therefore, the use of a model solely based on individual documents would not be sufficient. (Previously, heuristic methods were developed for Pseudo Relevance Feedback.) Similar things happen in the tasks of Topic Distillation [12, 11] and Subtopic Retrieval [18]. Ideally, in information retrieval we would exploit a ranking model defined as a function on all the documents with respect to the query. In other words, ranking We refer to this setting as  X  X lobal ranking X  and give a formal description on it with information retrieval as an example.
 lows the uses of both relations between objects and contents of objects [16]. However, conventional CRF cannot be directly applied to global ranking because it is a discrete model in the sense that the output variables are discrete [16]. In this work, we propose a Continuous CRF model (C-CRF) to deal with the problem. The C-CRF model is defined as a conditional probability distribution over ranking scores of objects (documents) conditioned on the objects (documents). The specific probability distribution can be represented by an undirected graph, and the output variables (rank-ing scores) can be continuous. To our knowledge, this is the first time such kind of CRF model is proposed.
 We apply C-CRF to two global ranking tasks: Pseudo Relevance Feedback and Topic Distillation. Experimental results on benchmark data show that our method performs better than baseline meth-ods. Document ranking in information retrieval is a problem as follows. When the user submits a query, for each of the documents using the ranking model, and sorts the documents according to the ranking scores. The scores can represent relevance, importance, and/or diversity of documents. for the number of documents retrieved with q . Note that the numbers vary according to queries. We We call the ranking  X  X ocal ranking X , if the ranking model is defined as Furthermore, we call the ranking  X  X lobal ranking X , if the ranking model is defined as The major difference between the two is that F takes on all the documents together as its input, while f takes on an individual document as its input. In other words, in global ranking, we use not only the content information of documents but also the relation information between documents. There are many specific application tasks that can be viewed as examples of global ranking. These include Pseudo Relevance Feedback, Topic Distillation, and Subtopic Retrieval. 3.1 Continuous CRF Continuous Conditional Random Fields is a conditional probability distribution with the following density function, where  X  is a K 1 -dimensional parameter vector and  X  is a K 2 -dimensional parameter vector, and C-CRF is a graphical model, as depicted in Figure 1. In the conditioned undirected graph, a white vertex represents a ranking score, a gray vertex represents a document, an edge between two white and a white vertex represents the dependency of a ranking score on its document (content). (In principle a ranking score can depend on all the documents of the query; here for ease of presenta-tion we only consider the simple case in which it only depends on the corresponding document.) In C-CRF, feature function h k represents the depen-dency between the ranking score of a document and the content of it, and feature function g k represents a relation between the ranking scores of two documents.
 Different retrieval tasks may have different relations (e.g. similarity relation, parent-child relation), as will be explained in Section 4. For ease of reference, we call the feature functions h k vertex features, and the feature functions g k edge features.
 Note that in conventional CRF the output random vari-ables are discrete while in C-CRF the output variables are continuous. This makes the inference of C-CRF largely different from that of conventional CRF, as will be seen in Section 4. 3.2 Learning respect to the C-CRF model, rank the documents of a new query. 4.1 Pseudo Relevance Feedback (PRF) Pseudo Relevance Feedback (PRF) [17, 8] is an example of global ranking, in which similarity be-a round of ranking, assuming that the top ranked documents are relevant; then conducts another round of ranking, using similarity information between the top ranked documents and the other doc-uments to boost some relevant documents dropped in the first round. The underlying assumption is that similar documents are likely to have similar ranking scores . Here we consider a method of using C-CRF for performing the task. 4.1.1 Continuous CRF for Pseudo Relevance Feedback We first introduce vertex feature functions. The relevance of a document to the query depends on Next, we introduce the edge feature function. Recall that there are two rounds in PRF: the first round scores each document, and the second round re-ranks the documents considering similarity between documents. Here the similarities between any two documents are supposed to be given. We incorporate them into the edge feature function. where S i,j is similarity between documents x i and x j , which can be extracted by some operator s the two documents are. Sine only similarity relation is considered in this task, we have only one edge function ( K 2 = 1 ).
 The C-CRF for Pseudo Relevance Feedback then becomes where Z ( x ) is defined as To guarantee that exp must have  X  k &gt; 0 3 and  X  &gt; 0 .
 The item P similar documents have similar ranking scores. We can see that CRF combines the two rounds of ranking of PRF into one.
 To rank the documents of a query, we calculate the ranking scores of documents with respect to this query in the following way.
 with S i,j = s ( x i , x j ) , D is an n  X  n diagonal matrix with D i,i = with X i,k = x i,k . If we ignore the relation between documents and set  X  = 0 , then the ranking ranking.
 For n documents, the time complexity of straightforwardly computing the ranking model (11) is of order O ( n 3 ) and thus the computation is expensive. The main cost of the computation comes from matrix inversion. We employ a fast computation technique to quickly perform the task. First, we make S a sparse matrix, which has at most K non-zero values in each row and each column. We can do so by only considering the similarity between each document and its K 2 nearest neighbors. Next, we use the Gibbs-Poole-Stockmeyer algorithm [9] to convert S to a banded matrix. Finally we solve the following system of linear equation and take the solution as ranking scores. Since S is a banded matrix, the scores F ( x ) in Eq.(12) can be computed with time complexity of O ( n ) when K  X  n [5]. That is to say, the time complexity of testing a new query is comparable with those of existing local ranking methods. Algorithm 1 Learning Algorithm of Continuous CRF for Pseudo Relevance Feedback
Initialize parameter log  X  k and log  X  for t = 1 to T do end for
Output: parameters of CRF model  X  k and  X  . 4.1.2 Learning Ascent cannot be directly applied to such a constrained optimization problem. Here we adopt a instead of  X  k and  X  . As a result, the new optimization issue becomes unconstrained and Gradient Ascent method can be used. Algorithm 1 shows the learning algorithm based on Stochastic Gradient where A =  X  T eI +  X D  X   X S , | A | is determinant of matrix A , b = X X  , c = denotes the long column vector formed by concatenating the columns of matrix X , and X ,k denotes the k -th column of matrix X . 4.2 Topic Distillation (TD) (to be ranked higher) [12, 11]. Here we apply C-CRF to Topic Distillation. 4.2.1 Continuous CRF for Topic Distillation We define the vertex feature function h k ( y i , x ) in the same way as in Eq.(7). Recall that in Topic Distillation, a page is more preferred than its child page if both of them are relevant to a query. Here the parent-child relation between two pages is supposed to be given. We incorporate them into the edge feature function. Specifically, we define the (and the only) edge feature function as of x j , and r ( x i , x j ) = 0 for other cases.
 The C-CRF for Topic Distillation then becomes where Z ( x ) is defined as To guarantee that exp have  X  k &gt; 0 .
 large than that of y j with high probability.
 To rank the documents of a query, we calculate the ranking scores in the following way. where D r and D c are two diagonal matrixes with D ri,i = Similarly to Pseudo Relevance Feedback, if we ignore the relation between documents and set  X  = 0 , the ranking model degenerates to a linear ranking model in conventional local ranking. 4.2.2 Learning In learning, we use Gradient Ascent to maximize the log likelihood. We use the same technique as which is similar to Algorithm 1. c = 4.3 Continuous CRF for Multiple Relations We only consider using one type of relation in the previous two cases. We can also conduct global easily incorporate various types of relation as edge feature functions. For example, we can combine similarity relation and parent-child relation by using the following C-CRF model: Pr( y | x ) = In this case, the ranking scores of documents for a new query is calculated as follows. We empirically tested the performance of C-CRF on both Pseudo Relevance Feedback and Topic Algorithms ndcg1 ndcg2 ndcg5 BM25-PRF 0.3962 0.4277 0.3981 RankSVM 0.4952 0.4755 0.4579 We made use of OHSUMED in LETOR for Pseudo Relevance Feedback and TREC2004 in LETOR for Topic Distillation. As evaluation measure, we utilized NDCG@n (Normalized Discounted Cu-mulative Gain) [6].
 As baseline methods for the two tasks, we used several local ranking algorithms such as BM25, RankSVM [7] and ListNet [2]. BM25 is a widely used non-learning ranking method. RankSVM of-the-art algorithm of the listwise approach. For Pseudo Relevance Feedback, we also compared with a traditional feedback method based on BM25 (BM25-PRF for short). For Topic Distillation, we also compared with two traditional methods, sitemap based term propagation (ST) and sitemap based score propagation (SS) [11], which propagate the relevance along sitemap structure. These algorithms can be regarded as a kind of global ranking methods but they are not based on supervised learning. We conducted 5 fold cross validation for C-CRF and all the baseline methods, using the partition provided in LETOR.
 The left part of Table 1 shows the ranking accuracies of BM25, BM25-PRF, RankSVM, ListNet, and C-CRF, in terms of NDCG averaged over five trials on OHSUMED data. C-CRF X  X  performance is superior to the performances of RankSVM and ListNet. This is particularly true for NDCG@1; accuracy than ListNet. The results indicate that C-CRF based global ranking can indeed improve search relevance. C-CRF also outperforms BM25-PRF, the traditional method of using similarity for the task.
 The right part of Table 1 shows the performances of BM25, SS, ST, RankSVM, ListNet, and C-CRF model in terms of NDCG averaged over 5 trials on TREC data. C-CRF outperforms RankSVM and ListNet at all NDCG positions. This is particularly true for NDCG@1. C-CRF achieves 8 points higher accuracy than RankSVM and ListNet, which is a more than 15% relative improvement. The task. C-CRF also outperforms SS and ST, the traditional method of using parent-child information for Topic Distillation. The result suggests that it is better to employ a learning based approach. ranking, except Relational Ranking SVM (RRSVM) proposed in [14], which is based on a similar motivation as our work.
 There are large differences between RRSVM and C-CRF, however. For RRSVM, it is hard to com-relations in different edge feature functions. There is a hyper parameter  X  in RRSVM representing necessary for C-CRF, however, because the trade-off between them is handled naturally by the fea-ture weights in the model, which can be learnt automatically. Furthermore, in some cases certain approximation must be made on the model in RRSVM (e.g. for Topic Distillation) in order to fit into the learning framework of SVM. Such kind of approximation is unnecessary in C-CRF anyway. Besides, C-CRF achieves better ranking accuracy than that reported for RRSVM [14] on the same benchmark dataset. We studied learning to rank methods for global ranking problem, in which we use both content information of objects and relation information between objects for ranking. A Continuous CRF (C-CRF) model was proposed for performing the learning task. Taking Pseudo Relevance Feedback and Topic Distillation as examples, we showed how to use C-CRF in global ranking. Experimental ranking tasks.
 Maximum A Posteriori Estimation to the problem. (2) We have assumed absolute ranking scores given in training data. We will study how to train C-CRF with relative preference data. (3) We have studied two global ranking tasks: Pseudo Relevance Feedback and Topic Distillation. We plan to look at other tasks in the future.

