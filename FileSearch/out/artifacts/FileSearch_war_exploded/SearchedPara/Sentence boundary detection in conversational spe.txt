 ORIGINAL PAPER Hironori Takeuchi  X  L. Venkata Subramaniam  X  Shourya Roy  X  Diwakar Punjani  X  Tetsuya Nasukawa Abstract This paper presents a technique for adding sentence boundaries to text obtained by Automatic Speech Recognition (ASR) of conversational speech audio. We show that starting with imprecise boundary information, added using only silence information from an ASR system, we can improve boundary detection using Head and Ta i l phrases. We develop our technique and show its effectiveness on two manually transcribed and one automatically transcribed cor-pus. The main purpose of adding sentence boundaries to ASR transcripts is to improve linguistic analysis, namely infor-mation extraction, for text mining systems that handle huge volumes of textual data and analyze trends and features of the concepts. Hence, we also show how the addition of bounda-ries improves two basic natural language processing tasks X  PoS label assignment and adjective-noun extraction. 1 Introduction Given conversational data in the form of audio files, the out-put of an Automatic Speech Recognition (ASR) System is a non-punctuated stream of words. However many applica-tions such as information retrieval and natural language pro-cessing benefit from (or even require) a sentence structure. In this work, we propose a solution to this problem of automa-tic sentence structure identification for conversational data transcripts. Word recognition errors introduced by an ASR system and the presence of noise such as repeats, false starts, and filler words in spontaneous conversational speech makes identifying structural information a more challenging task compared to well-written text.

Many researchers have worked in the area of identifying structure in speech transcripts. Most of this work has been based on supervised techniques which require using prosodic as well as lexical features. These techniques learn sophisti-cated models based on carefully annotated data. However, in many real life settings, generating such a well-labeled trai-ning data set is difficult or even impossible. We draw our motivation from the domain of help desks. Many companies today operate help desks as they allow them to be in direct contact with their customers. The number of calls handled by a help desk can range from a few hundreds to tens of thousands per month depending on the scale of operation. An ASR system deployed in such a help desk produces large volumes of data everyday in the form of speech transcripts. This data are valuable for doing analysis at many levels, e.g. these transcripts can be used to identify the problems and issues associated with different products and services, as well as for training and evaluating agents [ 16 , 17 ]. Such analysis requires many lower level NLP operations like parts-of-speech (PoS) tagging, parsing, information extraction, and summarization.

It is common NLP wisdom that these techniques bene-fit greatly from presence of sentence boundary information in the text. We make a twofold contribution in that context. Firstly, we propose a simple technique to identify sentence boundaries in ASR transcripts using minimal manual super-vision. Most of the existing systems require training data carefully annotated using a number of prosodic features fol-lowing specific guidelines [ 15 ]. We only use the silence infor-mation, which is relatively immune to transcription noise and build a k -gram model. This model is then used for identifying the sentence boundaries in the transcripts. The time requi-red for generating our model and using it to mark sentence boundaries is significantly much less as compared to other models.

We believe our proposed technique will be easier to imple-ment in real-life scenarios like help desks of large organiza-tions. The data sets from different processes in a help desk are different in terms of the vocabulary, style of speaking, and duration of speech by each speaker. For each data set it may not be feasible to create a well-labeled training set with boun-dary information. However, the proposed technique which requires minimum human intervention can be implemented in such scenarios.

The second contribution of this work is in showing the impact of automatic sentence boundary detection for infor-mation extraction (IE) tasks. We have already mentioned that the main purpose for the insertion of sentence boundaries to ASR conversational text is to improve linguistic analysis. Examples of such analysis could be extraction of key pro-blem phrases, typical responses to a problem, and trend data. Such linguistic analysis techniques are dependent on part-of-speech taggers and syntactic parsers which in turn expect sentence segmentation to be present in text. Hence, correct identification of input sentences for such tools is critical for the quality of their outputs.

In order to capture appropriate concepts through informa-tion extraction that is based on linguistic analysis, a sentence boundary should not be inserted in the middle of a phrase or an expression that represents a specific concept as a whole. Thus, insertion errors (typically, too many added boundaries) in sentence boundaries leads to lower precision in information extraction. On the other hand, lack of appropriate sentence boundaries leads to lower recall in information extraction, as it tends to yeild more noise (typically, inappropriate phrases). For example, while extracting noun phrases from the fol-lowing expression  X  X k let me get my password hang on a second ok X  without any sentence boundary in the middle, the parser will likely end up extracting  X  X y password hang X  and  X  X  second ok X , as the noun phrases. Towards this end, capturing typical Noun Phrases (NP) that express objects for analysis, typical Verb Groups (VG) that express actions and changes, and typical adjective noun (AN) pairs, are crucial in IE tasks. Such concepts are usually domain dependent and as such their extraction varies from domain to domain. In addition, when working with ASR conversational text, it is essential to deal with noise caused by incomplete utterances and errors in recognition.

We evaluate our system to show its effectiveness in two ways: 1. By comparing the boundaries added by the system with 2. By comparing the outputs of a PoS tagger on data with 2 Background and related work A large body of previous work exists in the sentence boun-dary detection domain for both broadcast speech transcripts Most techniques use a combination of lexical and prosodic features where a manually marked text collection is used as a training set. Many of them use a Hidden Markov Model (HMM) approach to capture the lexical features using n-gram language models [ 1 , 14 ]. Additional features such as prosody, including silence, are modeled as observation likelihoods attached to the n -gram states of the HMM [ 14 ]. More recently authors have used maximum entropy [ 4 , 8 ] and conditional random fields [ 9 ] to model a combination of lexical and pro-sodic features to obtain good sentence boundary detection on both broadcast speech and conversational speech. An inter-esting combination of decision tree models of prosody with a hidden event language model in an HMM framework for detecting events at each word boundary are proposed by Kahn et al. [ 5 ].

Text analysis, information extraction and knowledge mining require processing of the text by performing syntactic and semantic analysis [ 11 ]. Such systems require well for-med sentences for the natural language processing modules to work efficiently. Researchers have studied the impact of auto-matically predicted commas on the name tagging task using a maximum n -gram probability criterion [ 3 ]. In help desks, the analysis of customer X  X gent conversations is very impor-tant for extracting key actionable insights. While some work appears on using the noisy transcripts directly without sen-tence boundaries [ 13 ], availability of text with sentence boun-daries would greatly enhance information extraction from conversational text. 3 Datasets Before going into the details of the proposed technique, let us look at some of the transcription data sets. The follo-wing section contains details of two manually transcribed and one automatically transcribed data set which we used in all our experiments. The first manually transcribed data set that we use is the Switchboard Cellular Part 1 Transcription , produced by the Linguistic Data Consortium (LDC). 1 This release contains 250 transcripts of 5 X 6 min telephone conver-sations on various topics balanced by gender, under varied environmental conditions. These calls are transcribed using conventions similar to HUB-5 English [ 2 ].

The voices of the two speakers are recorded on two dif-ferent channels: a local channel and a remote channel .A turn has a speaker channel identification, and has a beginning and an ending time stamp. The time stamp has both a start and an end point, and neither point can overlap a previous time stamp of the same speaker. The insertion of breakpoints has the same appearance as a new speaker turn. Breakpoints can be inserted wherever they seem convenient to the transcriber. They should occur at the natural boundaries of speech, such as pauses, breaths, etc.

The following punctuation marks are used in the trans-cripts.  X  periods (.) are added at the end of declarative sentences  X  question marks (?) are added at the end of interrogative  X  commas (,) are added between clauses as is accepted in
The other manually transcribed data set that we used is also from LDC and is known as CallHome English Corpus. It consists of 120 unscripted telephone conversations between native speakers of English. The transcription scheme used was very similar to the scheme used for the Switchboard corpus. Figure 1 shows some example snippets from these data sets.

The automatically transcribed data set is from the tech-nical Helpdesk of a large corporation. The ASR system was trained on approximately 300 h of 6 KHz, mono audio data. The resultant transcriptions have a word error rate of about 40%. The transcripts contain speaker identity , beginning time and duration of each word and actual word . Special symbols indicate silence along with its duration. 2
Table 1 shows the summary of the three data sets. In this table, a Complete Turn is a turn that ends with a sentence boundary and Total Boundaries are the number of  X . X  and  X ? X . We ignore commas  X , X  in this task. 4 Boundary detection using head and tail phrases In this section, first we introduce the concept of imprecisely marked boundaries in automatic as well as manual transcripts. Later we show how such data can be used to learn a model which can be used to mark sentence boundaries in free text. 4.1 Automatic generation of training data In the case of ASR transcribed data, the presence of a pause or silence in conversation is an indication of a sentence boundary. However, owing to the presence of spontaneity, hesitation, repetition, and interruptions in conversation, boundaries marked using only silence information are not accurate enough to be useful. Using only silence results in the marking of false boundaries .Many true boundaries are also missed because people do not pause appropriately bet-ween sentence units. We propose to build a model from text with boundaries noisily marked using silence information and then use this model to remove false boundaries as well as add the boundaries missed by the silence model. This simple method has the advantage of being independent of carefully and manually created training data. We put a boundary when two or more consecutive silence characters occur in the trans-cripts. Single silences are ignored since they represent very small pause duration. The boundaries so marked are then used for generating the model.

For the manually transcribed datasets, the boundaries flag-ged by human transcribers are used for training the model. We observed a distinct lack of consistency in the boundaries marked by the human transcribers. The number of bounda-ries present should depend on the length of transcription (say number of words ). Also, as these conversations are similar in nature, it is reasonable to expect similar number of sen-tence boundaries for transcriptions of similar length. Howe-ver, this is not true with either of the manual dataset; there is a lot of subjectivity among transcribers when setting sen-tence boundaries while transcribing conversations. Figure 2 shows the high variation in average number of boundaries per word across different calls. We can expect a detrimental effect when such inconsistently labelled data is used to train any supervised system. 4.2 Steps for building Head and Tail phrase model In this section we will present our technique for using impre-cisely marked boundaries based on silence information to identify sentence boundaries in test data. It essentially involves learning probable Head and Ta i l phrases and using them to mark sentence boundaries. A phrase which occurs at the beginning (or end) of a sentence is called a Head (or Ta i l ) phrase, respectively. Our technique is based on the obser-vation that there are some phrases which are more likely to be Head (or Tail) phrases than others. For example, phrases such as hello this is , would you like are typically found at the beginning of a sentence whereas phrases such as help you today , clickonok are commonly found at the end of a sentence. We describe our technique as a sequence of the following steps:  X  Data cleansing : Both manual as well as automatic trans- X  Identifying Head X  X ail phrases : In Sect. 4.1 we mentioned  X  Inserting sentence boundaries : The selected Head and  X  Removing false boundaries : As discussed in Sect. 4.1 At this point, a question might come to the reader X  X  mind: is it possible to learn a statistical classifier given a collec-tion of k -grams? This can be seen as a three class classi-fication problem of k -grams: Head Phrase, Tail Phrase and None. If k -grams are represented as f i and these clases as C then according to the Bayesian formulation the problem is to compute the probability P ( C j | f i ) . Naive Bayes algorithm is extensively used for document classification task [ 10 ]. Dra-wing a parallel from document classification, P ( C j | f i ) = The denominator in the RHS does not differ between catego-ries and can be left out. Hence, the above equation becomes P ( C j | f i ) = P ( C j ) P ( f i | C j ) Making a simplifying assumption of equal class priors, 3 P ( C j | f i ) = P ( f i | C j )
In the case of document classification, each document can be treated as composed of independent words or phrases. However, in this case it is not obvious how the k -gr ams can be represented in terms of a collection of independent features. The goodness scores mentioned above are nothing but P ( f i | C j ) and hence a classification technique will learn these probabilities just like the Head and Tail phrases tech-nique mentioned above. It is, in fact, easier to include other heuristics such as removing false boundaries in the proposed approach. 4.3 Boundary detection using bi-directional maximum We compared our technique with a bi-directional maximum entropy based tagger to see how a more traditional approch compares with our method. We used an efficient implemen-tation of a maximum entropy tagger [ 18 , 19 ] for boundary detection. In our references the authors had used the tagger to assign parts-of-speech (PoS) tags to the words based on a PoS tagged corpus. In our case we modeled sentence boun-dary detection problem as a similar tag assignment problem  X  the tags being boundary or no-boundary depending on whether the word is followed by a boundary or not. The tag-ger is trained on training data with every word marked with the tag &lt; nb &gt; if it is not followed by a sentence boundary and &lt; b &gt; otherwise. This model learns the boundaries based on the neighbouring words and their tags (both the left and right context). We will not go into the details of the model learning, interested readers are encouraged to see the relevant references. As with the Head phrase and Tail phrase based boundary detection, training data is created using silence information. We also pass this data through the step of data cleansing described previously. Once the model is learned it can be used to tag words with &lt; nb &gt; or &lt; b &gt; which no sentence boundary information is present. 5 Evaluation In this section, we present the evaluation of our sentence boundary insertion method. As mentioned in Sect. 1 ,wealso look at how PoS tagging and NP extraction are affected by the presence or absence of sentence boundaries. 5.1 Experiments In these experiments, we evaluate the performance of our punctuation insertion method. The experiments are perfor-med using manually transcribed and automatically transcri-bed datasets, as described in Sect. 3 . A major part of these datasets are used to learn the Head and Tail phrases based model and the rest is held out for the purpose of testing our model. These manually created held-out datasets are referred to as gold test data for each corpus. The datasets are imprecisely labeled in terms of sentence boundaries. In the case of manually transcribed datasets X  X he Switchboard corpus and CallHome corpus X  X he noise is due to incon-sistent marking of sentence boundaries by transcribers. Our automatically transcribed dataset, the Helpdesk dataset, has sentence boundaries based on silence information, which is again imprecise. As per the proposed approach, sets of Head and Tail phrases are identified from these datasets. Head and Tail phrases so learned are then used to mark sentence boun-daries on held-out sets. A similar procedure was followed for the maximum entropy based approach. As the training data, we used 210, 100 and 1,600 calls from the Swichboard, Call-Home and Helpdesk corpora, respectively. We used 35 and 20 calls from Swichboard and CallHome corpora, respecti-vely, and 20 calls from help desk corpus as gold test data. We employ several standard accuracy measures to compare the performance of different techniques. In the following table: entry n 11 is the number of times the automatic approach predicted actual sentence boundaries correctly. Recall and precision are defined as R = n 11 /( n 11 + n 10 ) P = n used measure, is also reported. We also define Error Rate (ER) as the ratio of total number of erroneous punctuations inserted to the total number of punctuations in the test data, the lower the better.

Table 2 shows the evaluation results for the manual data-sets. The performance of the proposed technique of lear-ning Head and Tail phrases from manually marked imprecise boundaries is similar on both the datasets, with Switchboard being slightly better. All these accuracy numbers are cal-culated against the manually marked sentence boundaries, which themselves are done in a subjective manner (Sect. 4.1 ). Hence, our actual performance numbers are better than the numbers shown in Table 2 . In fact, removing calls with very few boundaries increases our F-score by almost 10%.
The results for the silence-based approach, Head and Tail (HT) phrases based approach and a combination of the two approaches to identify sentence boundaries on help desk data-set are presented in Table 3 . As we can see in the table the silence based approach has a very low recall which results in a F1 score of only 0.37 and an ER of nearly 1. Our HT approach has significantly better precision and recall; it has 0.65 F1 score and an ER of only 0.60. The combined model has an improved recall with respect to HT, but there is a decrease in precision. This we believe is due to the presence of false boundaries. The best results are achieved when false boundaries are removed from the output of the combined model resulting in a F1 score of 0.70 and 0.58 word error rate. The maximum entropy model, as described in Sect. 4.3 , performs better than the silence-only model, but does not reach the performance of HT method. We believe this is the case because the noisy examples do not form a good enough training set for the maximum entropy model. As mentioned previously, the HT approach allows us to include heuristics that cannot easily be incorporated in other methods.
In this table HT refers to the Head phrase and Tail phrase method of this paper. Sil + HT refers to putting boundaries using both HT and silence information. Sil + HT  X  FB refers to removing False Boundaries as described in Sect. 5 . 5.2 Evaluation in preprocessing tasks for information In many text mining systems, we extract useful information from text using Natural Language Processing (NLP) tech-niques. Since the state of the art linguistic analysis tools such as Part-of-Speech (PoS) taggers and syntactic parsers are based on sentence segmentation, correct identification of input sentences is critical for these tools to perform well. We study the impact of punctuation insertion on the results of some linguistic analysis tools. In particular, we look at PoS tagging and adjective noun (AN) pair extraction.

In our first evaluation, we compare the results of PoS tagging on text with and without punctuation added by our method. We use the manually added PoS tags as the gold standard data and calculate accuracies by comparing with it.
Table 4 shows accuracies of PoS tagging on two text data from Switchboard corpus, data without punctuations (None) and data with manually marked imprecise punctuation by our method of learning Head and Tail phrases (System). From this result, it is found that PoS tagging is improved by adding punctuations. This difference in accuracy is meaning-ful because a t -test [ 12 ] rejects the possibility of an accidental difference between them at a probability level of  X  = 0 . Table 5 shows accuracies of PoS tagging on 20 calls from Helpdesk data. We can see that our Sil + HT  X  FB method performs best, as we saw in the evaluations mentioned in the last section. In these results, the differences between Silence and HT and between Sil + HT and Sil + HT  X  FB are mea-ningful under the t -test at a probability level of  X  = 0
Tables 7 and 8 show the results of the PoS tagging for the most frequent ten keywords in 35 calls from the Switch-board dataset and 121 calls from the Helpdesk dataset. We highlight the effect of different techniques for sentence boun-dary detection on PoS tagging. From these results, it is seen that the identification of non-noun keywords is improved by using the proposed method while maintaining similar per-formance for nouns. In conversational data, pronouns (such as i ) and interjections (such as yeah , oh ) appear frequently. Table 6 compares performance of proposed technique on a few sample pronouns and interjections. It is found that incor-rect PoS labels are assigned to these three keywords much more frequently in the unpunctuated data, and the propo-sed HT based technique outperforms inconsistent manual punctuations. Hence, inserting punctuation information is very important in a text mining system that relies on PoS information for information extraction from text. Based on these results, we believe our punctuation insertion method will improve the performance of a text mining system for conversational data.

Tables 9 and 10 show the most frequent ten adjective X  X oun expressions in each dataset. In these tables, the expression in italic denotes the improper adjective X  X oun expressions. In each case, it can be seen that the extracted expressions are improved on inserting punctuations. In experiments using the switchboard dataset, because of the small size of the data, the number of actual adjective X  X oun expressions is very small. As a result, the extracted expressions for system bounda-ries contain noisy results, but the frequencies of such noisy expressions are dramatically reduced by our sentence boun-dary insertion. In experiments using the help desk data, it can be seen that there are significant improvements using our proposed methods (HT, Sil + HT and Sil + HT  X  FB). In the baseline method (Silence), however, there are still noisy expressions with high frequencies. In the analysis of docu-ment collections using text mining technologies, we usually obtain the distribution of frequent expressions in the selec-ted document collections and derive insights based on these lists. For this purpose, it is necessary that there are few noisy expressions in the high freqency expressions lists. 6 Conclusions Most current boundary detection techniques are based on training a system on a carefully prepared, manually anno-tated, boundaries dataset. In this paper we have shown that we can train a sentence boundary detector based on only the silence information provided by an ASR system. Such a sentence boundary detector based on building a Head and Tail phrase language model is shown to have good accuracy. The language model is also shown to perform competently on manually transcribed data. In addition, we demonstrated that boundary detection results in improved PoS label assign-ment and adjective X  X oun extraction, two key preprocessing steps in information extraction. As a future direction it would be interesting to measure the improvement due to boundary detection in the actual information extraction task. We would also like to see the effect of errors introduced by ASR system on sentence boundary detection task. An idea toward this end would be to build a manual transcript as well as ASR trans-cript of some conversational data and apply the proposed algorithm on both.
 References
