 In this paper, we propose an approach for efficient approx-imative R k NN search in arbitrary metric spaces where the value of k is specified at query time. Our method uses an approximation of the nearest-neighbor-distances in order to prune the search space. In several experiments, our solution scales significantly better than existing non-approximative approaches while producing an approximation of the true query result with a high recall.
 Categories and Subject Descriptors: H.2.2 [Physical Design]: Access methods General Terms: Algorithms, Performance Keywords: Approximative similarity search, reverse near-est neighbor
A reverse k -nearest neighbor (R k NN) query returns the data objects that have the query object in the set of their k -nearest neighbors. A naive solution of the R k NN problem requires O ( n 2 )time,asthe k -nearest neighbors of all of the n objects in the data set have to be found. In general, the R k NN problem appears in many practical situations such as geographic information systems (GIS), traffic networks, adventure games, or molecular biology where the database objects are general metric objects rather than Euclidean vec-tors. In most applications, the parameter k can change from query to query and is not known beforehand. In addition, the efficiency of the query execution is much more important than effectiveness, i.e. users want a fast response even if the results are only approximate (as far as the number of false drops and false hits is not too high). Existing approxima-tive approaches for R k NN search [2, 3] are only designed for Euclidean vector data. All other approaches for the R k NN search are exact methods that usually produce considerably higher runtimes.

In this paper, we propose an efficient approximate solu-tion for the R k NN problem based on the observation that if the distance of an object p to the query q is smaller than the 1-nearest neighbor distance of p , p can be added to the result set. Our solution is extends the work in [1] and is de-signed for general metric objects and allows R k NN queries for arbitrary (unbounded) values of k . The idea is to use a suitable approximation of the k NN distances for each k of every object in order to evaluate database objects as true hits or true drops without requiring a separate k NN search. The proposed concepts can be integrated into any hierarchi-cally organized, tree-like index structure for metric spaces. In addition, it can also be used for Euclidean data by using a hierarchically organized, tree-like index structure for Eu-clidean data. In summary, our solution is the first approach that can answer R k NN queries for any k  X  N in general metric databases. Since our solution provides superior per-formance but approximate results, it is applicable whenever efficiency is more important than complete results. How-ever, we will see in the experimental evaluation that the loss of accuracy is negligible.
The only existing approach to R k NN search that can han-dle arbitrary values of k atquerytimeandcanbeusedfor any metric objects is the MR k NNCoP-Tree [1]. This ap-proach, however, is optimized for exact R k NN search and its flexibility regarding the parameter k is limited by an addi-tional parameter k max . This additional parameter must be specified in advance, and is an upper bound for the value of k at query time. If a query is launched specifying a k&gt;k the MR k NNCoP-Tree cannot guarant ee complete results. In our scenario of answering approximate R k NN queries, this would be no problem but since the MR k NNCoP-Tree con-straints itself to compute exact results for any query with k  X  k max , it generates unnecessary computational overhead.
Our idea is to store one approximation of the k NN dis-tances for any k  X  N . This approximation is represented by a function, i.e. the approximated k NN distance for any value k  X  N can be calculated by applying this function. Similar to existing approaches, we can use an extended tree-like metric index, that aggregates for each node the one approximation of the approximations of all child nodes or data objects con-tained in that node. These approximations are again repre-sented as functions. At runtime, we can estimate the k NN distance for each node using this approximation in order to prune nodes analogously to the way we can prune objects.
A suitable model function for the approximation of our k NN distances for every k  X  N should obviously be as com-pact as possible in order to avoid a high storage overhead and, thus, a high index directory. In our case, we can as-sume that the distances of the neighbors of an object o are given as a (finite) sequence
NNdist ( o )= nndist 1 ( o ) , nndist 2 ( o ) ,...,nndist for any k max  X  N . Our task here is to describe the dis-crete sequence of values by some function f o : N  X  R with f ( k )  X  nndist k ( o ). As discussed above, such a function should allow us to calculate an approximation of the k NN distance for any k ,evenfor k&gt;k max by estimating the corresponding values.

Following the theory of self-similarity it can be assumed that the k NN distances also follow the power law, i.e. k nndist k ( o ) d f ,where d f is the fractal dimension. Transferred into log-log space, we have a linear relationship:
From this observation, it follows that it is generally sen-sible to use a model function which is linear (and thus com-pact) in log-log space, corresponding to a parabola in non-logarithmic space. In the following, we consider the pairs (log( k ) , log( nndist k ( o )) as points of a two-dimensional vec-tor space ( x k ,y k ). Like in most other applications of the theory of self-similarity, we need to determine a classical re-gression line that approximates the true values of nndist with least squared errors. This line is exactly the approxi-mation of the k NN distances we want to aggregate. In other words, for each object o  X  X  , we want to calculate the func-tion f o ( x )= m o  X  x + t o that describes the regression line of the point set { (log k, log nndist k ( o )) | 1  X  k  X  k max
From the theory of linear regression, the parameters m o and t o can be determined as
Using these concepts, an accurate approximation for each object of the database can be generated. When using a hierarchically organized index structure, the approximation canalsobeusedforthenodesoftheindextopruneirrelevant sub-trees. Usually, each node N of the index is associated with a page region representing a set of objects in the subtree which has N as root. In order to prune the subtree of node N , we need to approximate the k NN distances of all objects in this subtree, i.e. page region. If the distance between the query object q and the page region of N , called MINDIST, is larger than this approximation, we can prune N and thus, all objects in the subtree of N .TheMINDISTisalower bound for the distance of q to any of the objects in N .The aggregated approximation should again estimate the k NN distances of all objects in the subtree representing N with least squared error. This can be done in a straight-forward manner.

The concepts presented here can be integrated into any hierarchically organized index for metric objects or into Eu-clidean index structures. Then, the algorithm for approx-imate R k NN queries is similar to the exact R k NN query algorithms of the MR k NNCoP-Tree. However, using our concepts, the index can be used to answer R k NN queries for any k specified at query time. Let us point out that the value of k is not bound by a predefined k max parame-ter, although the approximation of the k NN distances are computed by using only the first k max values, i.e. the k NN distances with 1  X  k  X  k max .The k NN distance for any k&gt;k max can be extrapolated by our approximations in the same way as for any k  X  k max .

Aquery q is processed by traversing the index from the root of the index to the leaf level. A node N needs to be refined if the distance between q and N is smaller than the aggregated k NN distance approximation of N . Those nodes having distance to q larger than their aggregated k NN dis-tance approximation are pruned. The traversal ends up at the data node level. Then, all points p inside the ob-tained nodes are tested using their approximation f p ( x )= m p  X  x + t p .Apoint p is a hit if log( dist ( p, q ))  X  m Otherwise, if log( dist ( p, q )) &gt;m p  X  log k + t p ,point p is a miss and should be discarded.

In contrast to other approaches that are designed for R k NN search for any k , our algorithm directly determines the re-sults. In particular, we do not need to apply an expensive refinement step to a set of candidates. This further avoids a significant amount of execution time.
We integrated our concepts into an M-Tree and compared ourconceptswiththemethodsproposedin[1]usingtwo real-world metric datasets. On both datasets, our approach clearly outperforms the competing MR k NNCoP-Tree. The performance gain of our approach over the existing method also grows with increasing database size.

We also executed R k NN queries on the metric databases with varying k and compared the scalability of both com-peting methods. The parameter k max was set to 100 for both approaches in all experiments. With increasing k ,the performance gain of our method over the competitor rapidly grows.

In summary, in almost all parameter settings, our novel so-lution turned out to be clearly faster than the MR k NNCoP-Tree which computes an exact solution but is limited by the k max parameter. On the other hand, our solution is not limited to any k  X  k max but only designed for approxi-mate answers. However, in all our experiments, we achieved high recall values of clearly above 90%. Furthermore, the recall does not decrease significantly when answering R k NN queries with k&gt;k max . This is important, since it indicates that our solution is very efficient and produces high quality results.
