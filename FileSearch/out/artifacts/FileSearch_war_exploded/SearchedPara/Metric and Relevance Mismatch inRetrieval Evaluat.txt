 Information retrieval (IR) experiments based on the Cranfield methodology mea-sure system performance using a set of qu eries and a test collection. The queries are run over the collection using a searc h system, and for each document that is returned, a human judge decides whether the document is relevant to the query, or not. The overall utility of the search system is then computed using a metric that aggregates the relevance judgements for documents in ranked lists returned by the system. In this batch evaluation approach, different search systems are compared based on how well they scor e on such metrics. For example, many papers report IR system comparisons u sing the TREC document collections, topics and judgements, using Mean Average Precision (MAP) or Precision at 10 documents retrieved (P @10) as the metric [23].

An alternate way to evaluate systems is to take a group of human users and ask them to perform search tasks with different systems, comparing outcome measures such as time to complete a tas k, success or failure on a task, or sub-jective measures like user satisfaction. Previous studies [1,2,8,9,12,18,19] have shown that attempting to transfer results from batch experiments to real users is difficult. That is, the systems rated as superior in the batch experiments may in fact not assist users in performing their tasks more quickly or more accurately than the systems that are rated more poorly in the batch experiments.
In this paper, we explore ways in which these two experimental paradigms may be reconciled. There are many possible ca uses for this seeming mismatch between batch and user-based experimental outcomes. We investigate two reasons using controlled batch and user experiments.
 Mismatching metrics. It is possible that the metric used in a batch experi-ment to show that System A is superio r to System B does not reflect the user task for which these systems will be employed. For example, if a batch experi-ment uses the MAP metric, which contai ns a recall component, but the user task is solely precision based, such as finding a single answer to a simple question, then differences between systems in the ba tch experiment may be meaningless in the user domain. On the other hand, if the batch experiment used a metric such as Precision at one document returned (P@1) or at three documents returned (P@3), then it is perhaps more likely that the batch results would carry over into the user domain. For example, Turpin and Scholer [19] used the MAP metric to choose superior systems, but then em ployed those systems on a precision-based user task and found that they did not outperform the inferior systems. When they re-analysed their data to choose systems based on the P@1 metric, it suggested that users performed better w ith the superior system. However, the analysis of P@1 was inconclusive because of the small number of systems for the non-relevant category of this metric. Motivated by this finding, we explicitly examine possible metric mismatch by using P@1 in our batch experiments, and a precision-based user outc ome measure. We also extend this analysis to incor-porate multiple levels of relevance, factoring in differences between non-relevant, relevant, and highly relevant documents.
 Mismatching relevance profiles. Batch system results are based on rele-vance judgements assigned to documents by human assessors. However, it is possible that relevance judgements used in the batch experiments are made us-ing different criteria, or on a different scale (whether perceptual or actual), than judgements that are made in a user study. For example, in this study we use TREC documents that are judged on a three-point scale: non-relevant (0), rel-evant (1), and highly relevant (2). The TREC judging criteria define level zero as being applicable where no  X  X art of the document contains information which the assessor would include in a report on the topic X ; while the distinction be-tween level one and two was  X  X eft to the individual assessors to determine X  [7]. If subjects in a user study receive identical i nstructions to the judges in the batch experiment, and carry out their evaluation in as similar an environment as pos-sible, there is still scope for individuals to decide their own threshold on what information they would  X  X nclude in a report X , and to distinguish between the two categories of relevance. Even in the highly controlled TREC judging envi-ronment, the overlap between the relevan ce judgements of assessors is on average only about 45% [22], indicating that thresholds between relevance categories can differ even within relatively homogeneous populations where identical judging instructions are given. Therefore relevance mismatch, where users and batch judges have different expectations and preferences for documents of different relevance levels, may lead to conflictin g results between batch and user results. We investigate the impact of relevance mismatch based on the split agreement approach [14], where users are classified into groups based on their responses to documents of different relevance levels.

These two possible explanations for d ifferences between batch and user ex-periments are investigated through a user study. In Section 2 we survey related background work on experimental evaluation in IR. Our experimental methodol-ogy, including details of the user-based searching task, is explained in Section 3. Results are presented and discussed in Section 4, with conclusions and further work being considered in Section 5. The Cranfield paradigm of information retrieval evaluation involves using a search system to run a set of queries on a fixed collection of documents. For each potential answer that the search system returns, a human is required to judge the relevance of the particular document for the current query. This is the dominant framework for experimental IR, and is used, for example, in the on-going series of Text REtrieval Conferences (TREC). TREC provides standard collections, queries, and relevance judg ements so that the performance of differ-ent IR systems can be compared using co mmon testbeds [23]. In TREC, queries are derived from topics that represent use r information needs: topics consists of a title field (a small number of keywords, representative of what a user might type into a web search engine), a description (a longer statement of the topic, usually a single sentence), and a narrative (a short paragraph specifying further requirements) [6].

Based on the system search result lists and relevance judgements, different system performance metrics can be cal culated. Many metr ics that have been proposed in the literature focus on precision, which is the number of relevant documents that the search system has found as a proportion of the total number of documents that the system has returned. Average precision (AP) is calculated as the mean of the precision at each releva nt item that occurs in a result list for a single query. Relevant documents that are not returned by the system contribute a precision of zero; this metric thus has a recall component, since the system is penalised for missing answers. Across a s et of queries, the mean average precision (MAP) provides a single number that summa rizes search perfor mance, reflecting both the precision and the recall of the system [5].

Another widely-used class of performan ce metrics is the precision of a system at a particular cutoff point N in the search results list. For example, P@1 eval-uates a system based on the relevance of the first item in the result list, while P@10 calculates the precision over the fi rst 10 results. These metrics are popular for evaluating web search tasks, since use rs typically focus on results that occur early in the ranked list [17]. Analysis by Buckley and Voorhees has indicated that these P@ N metrics require a relatively larger number of test queries, compared to other metrics such as MAP, in order to give stable results for the evaluation of batch experiments [4].

The most commonly used IR system performance metrics, such as those pre-sented previously, treat relevance as a binary criterion: a document is either relevant, or it is not. Even where documents may have been judged on a multiple-level relevance scale, these levels are ty pically folded together into a binary clas-sification before the metrics are calculated. However, studies of multiple levels of relevance have indicated that the traditional binary relevance assumption may not be appropriate where actual users of search systems are concerned [16,21]. In the TREC evaluation framework, the criterion for relevance states that if the document includes any reference to the topic, it should be counted as being relevant. This includes documents that are only marginally relevant, where the document does not contain information other than that contained in the topic description; in other words, these documents are largely useless from a user X  X  perspective. Investigating the ability of users to judge documents of different relevance levels, Vakkari and Sormunen concluded that the likelihood of iden-tifying highly relevant documents is much higher than for marginally relevant ones [21]. Further, analysis of 38 topics from TREC-7 and 8 by Sormunen showed that around 50% of documents that were judged as relevant under the TREC binary criterion were of this marginal category [16]. We investigate the effect of accounting for different levels of relev ance has on the results of user-based and batch retrieval experiments.

The cumulative gain (CG) family of retrieval metrics are based on the idea that the relevance of documents is not equal: the usefulness to a user will depend on the level of relevance of an item [11]. This allows multiple levels of relevance to be incorporated in system evaluation, unlike the previously discussed metrics which assume a binary relevance scale. The CG values, where more highly rele-vant documents are rewarded by adding more to the overall performance score, can then be discounted (DCG) so that the further a document is from the top of a ranked list, the more heavily its relevance score is adjusted. In this paper, we investigate DCG@1 as a multiple-relevance level alternative to P@1. Since discounting is usually not applied at the first rank of the answer list, CG@1 and DCG@1 are equivalent.

The Cranfield paradigm of IR evaluation makes a number of simplifying as-sumptions about users: essentially, use rs and real search tasks are removed from the evaluation process, with both information needs and relevance being re-duced to static components of the analysis. While this allows for repeatability of experiments, and the controlled evaluation of retrieval algorithms, it is widely acknowledged that these assumptions are significant simplifications of the ac-tual retrieval process [10]. A number of s tudies have therefore investigated the relationship between system-centric retr ieval performance metrics and the per-formance of users engaged in a range of different search tasks, which we briefly survey here. A relationship between the ability of users to find answer facets and high changes in the level of the bpref evaluation metric was found by Al-lan et al. [2]. Investigations by Hersh and Turpin found no relationship between MAP and user performance on an instance recall task [8], or a question an-swering task [18]. The relationship b etween simple web search tasks and MAP was investigated by Turpin and Scholer [19]; no relationship was found with a precision-oriented task, but a weak r elationship was observed with a recall-oriented task.

Other recent studies have considered th e relationship between result rele-vance and user satisfaction. Experiments by Huffman and Hochster showed that system performance measured by DCG@3 was related to user satisfaction for informational searches [9]; user satisfaction was measured by asking subjects to rate their overall search experience on a s even-point scale. Al-Maskari et al. [1] compared the precision and various cumulative-gain metrics of search results with user satisfaction. Here, users rated their satisfaction b ased on the accuracy, coverage and ranking of results. A high correlation was found between satisfac-tion and both the precision and CG metrics, while the correlation with nDCG was low. In a series of carefully controlled experiments, Kelly et al. [12] demon-strate a strong correlation between precision and user satisfaction; ranking also influenced user ratings, but to a lesser extent. In this paper, instead of using self-reported measures of satisfaction, we investigate user performance based on success in completing a simpl e search task, measuring the time taken to find a relevant document.

To construct user relevance profiles, Scholer, Turpin and Wu proposed the split agreement approach [14]. Here, users are analysed based on their rate of agreement when presented with documents at different TREC relevance levels. Users can deviate from TREC-like relevance behaviour in two ways: generous users have lower criteria for relevance than TREC judges, and are often satisfied even with non-relevant (level 0) documents. Conversely, parsimonious users have stricter relevance criteria than TREC judges, and are usually satisfied only with a highly relevant (level 2) document. Users who are TREC-like follow the assumed batch relevance profile, generally disc arding level 0 documents, but liking level 1 and 2 documents.

Relevance profiles are established through repeated presentation of documents with different TREC relevance levels (unknown to the user). For each presented document, the user is asked to indicate whether they find the document to be relevant for a specified information need, or not. Across many presentations of documents, a response proportion can thus be calculated for each TREC rele-vance level. For example, a particular user may judge level 0, 1 and 2 documents to be relevant 6%, 63% and 94% of the time, respectively.

User classes are based on these proportions. Specifically, a generous user is defined as someone who judges level 0 documents to be relevant more than 50% of the time. A parsimonious user, on the other hand, judges level 1 documents to be relevant less than 50% of the time [14]. To investigate relevance mismatch, we attempt to classify users based on their relevance preferences, aiming to establish for each user whether their relevance pr ofile is similar to that of TREC judges. This study investigates the relations hip between the P@1 and DCG@1 system performance metrics and user perform ance on a web search task, and how this is affected by user perceptions of relev ance. We use TREC data for the basis of our batch experiments, and a user study to collect data for a searching task. Users and document collection. 40 experimental subjects were recruited from RMIT University by advertising on newsgroups and notice-boards. All sub-jects were required to complete entry questi onnaires. Participants were university students undertaking undergraduate or postgraduate studies in computer science and information technology, and most were very familiar with online searching (the median response for searching frequency was that searches are conducted  X  X nce or more a day X ). Subjects were from a variety of cultural backgrounds, but all had a reasonable grasp of the English language (a requirement for study-ing at RMIT University). Experiments were carried out in accordance with the guidelines of RMIT University Human R esearch Ethics Committee. Three of the 40 user study participants were unable to carry out all required aspects of the experiments, and are excluded from the analysis below.

The documents used for the searching task are from the TREC GOV2 col-lection, a 426 Gb crawl of the US .gov domain carried out in 2004 [6]. This collection was used for the TREC Terabyte tracks in 2004 X 2006, and has 150 associated search topics and corresponding relevance judgements, made by NIST assessors. The relevance judgements are o n a three-level ordinal scale: not rele-vant (0); relevant (1); and highly relevant (2). According to the standard TREC judging approach, if any part of the document contains information that the assessor would include in a report on the topic, it should be judged relevant [7]. That is, relevant documents will include those that are only of marginal value, containing little or no information beyond what is already included in the topic statement.
 Search systems. To investigate the relationship between user search perfor-mance and system performance as measured by a batch metric, we mimic batch experimental results by constructing ranked lists using the known TREC rel-evance levels of documents to achieve a given level of the performance metric under investigation. A set of ranked lists at a given level can be thought of as being generated by a search system that is engineered to always produce ranked lists that achieve a particular level of the metric, for any topic.

Given that the TREC relevance judgements have three levels, there are thus three possible systems for the DCG@1 metric, namely lists starting with a doc-ument of relevance level 0, 1, or 2. For P@1, a binary metri c, these relevance levels are folded together: either level 0 compared to combined levels 1 and 2; or combined levels 0 and 1 compared to level 2. To reduce variation, all system lists had identical TREC relevance scores assigned after the first position. The document relevance level allocati ons for complete system lists were where X  X  X  0 , 1 , 2 } . Lists were constructed to a depth of 10 documents.
For the search task, 24 topics were chosen from TREC topics 700-850; the constraint for topic selection was that each topic must have the required number of documents at each relevance level to allow the construction of the appropriate lists. Documents were assigned to lists by relevance level, with candidate docu-ments being drawn from the top 50 documents from the two runs with highest MAP scores submitted to the Terabyte track for 2004, 2005 and 2006; that is, they are documents that would feasibly be returned in response to the topic by a modern search system. Only documents of type  X  X ext/html X  were retained, with other content types being discarded. Similarly, documents smaller than 750 bytes or larger than 100,000 bytes were discarded.
 Search task and user interface. Users were asked to carry out a precision-based search task: to quickly find useful information about a topic. This type of search is common on the web, and can be considered to be a simple instance of the informational search categories identified by Rose and Levinson [13]. As a performance outcome, we measure the amount of time that a user needs to complete the task.

Specifically, the search scenario is that of a user being asked to find useful information about a topic: The information needs were framed in a task-based scenario so as to ground them in a practical context; Borlund has d emonstrated that searcher behaviour that is elicited through simulated search tasks may be similar to behaviour that is exhibited when engaged with real information needs [3].

A search session proceeded as follows. First, a subject was presented with an information need, comprised of the narrative and description fields of a TREC topic, at the top of the screen. Under the information need, a search interface was available. This was closely modelled on the search screens of popular Web search engines, and consisted of a text-box for the entry of search terms, together with a  X  X earch X  button. After a user entered a query, they were presented with a results list of the required system level (that is, corresponding to one of the precision variants, as described previous ly). Users were not able to reformulate their queries.

Entries in the search results lists consisted of the document title, together with a short query-biased summary. The document summaries were generated following the approach of Turpin et al. [20], using the title field of the TREC topic as the query words. The document title was a hyperlink which, when clicked, opened the underlying document in a new window.

From the document window, in addition to being able to read the document, subjects were presented with two option buttons:  X  X ave X , to mark the document as relevant to the information need; and  X  X ancel X , to close the document window and return to the search results list. Choosing to save a document brought up a confirmation dialogue box, which asked the subject to enter a brief description of why the document was considered to be relevant. After saving one document, the user is deemed to have complete d that particular search task.

All interactions between users and th e search system were written to a system log, including timestamps of when actions took place. Timings for the precision-based search task were calculated from when the user clicked the search button, until they chose to save their document.

Users were asked to carry out searches o n the 24 topics three times, so that each topic would be completed with every system level. The experimental design ensured that users were presented with t opics and systems in different orders, to account for possible biases and learning effects. Due to fatigue that was apparent in the last half hour of the user study, we only analyse the first 48 (out of 72) total searches for each user below. However, due to rotation in the experimental design, the results are balanced so that, across all searches, topic and system combinations were used an equal number of times. Based on the user study, we investigate whether system differences as shown by batch metrics that focus on the relevance of the top-ranked position in a search result list transfer successfully to an actual search task. We then examine relevance profiles, and whether these can h elp to explain the relationship between the two evaluation paradigms. 4.1 Comparing Batch and User Performance To investigate our first hypothesis, that the P@1 and DCG@1 system perfor-mance metrics are closely matched to a precision-bas ed user search task, we analyse the relationship between search system level and time taken to find a useful document. The mean and median times that a user needed to find a use-ful document with different systems are shown in Table 1. On average, the task time falls as the level of the system performance metric rises. A multifactorial analysis of variance (ANOVA) indicates that the effect of the different system levels is statistically significant ( p&lt; 0 . 0001). However, the time data from the user search task is truncated at zero, and so violates the normality assumption. Although ANOVA is generally robust, we th erefore also analyse system effects using the Kruskal-Wallis test, a non-para metric alternative to ANOVA [15]; this supports the previous results, also showing a statistically significant effect for system ( p&lt; 0 . 0001). Follow-up tests are required to distinguish which specific system levels lead to significant differences in performance.

There are three search systems, corresponding to documents at the first rank position with relevance level 0, 1 or 2. However, for batch system performance to be expressed using the P@1 metric, relevance needs to be folded into a bi-nary scale, giving two ways of grouping relevance levels: folding level 1 and 2 documents together, as is commonly done in TREC; or, folding level 0 and 1 documents together. Differences between these system levels are examined using the Wilcoxon signed-rank test, a non-parametric test of the null hypoth-esis that the median values of two samp les are the same. For both relevance groupings, the differences in search times are statistically significant ( p =0 . 0002 for 0 versus 1 and 2; p&lt; 0 . 0001 for 0 and 1 versus 2) indicating that P@1 batch results transfer to the user task.

For DCG@1, multiple levels of relevance can be accounted for explicitly in the batch metric, so all three systems can be compared directly. User perfor-mance differs significantly between systems 0 and 2 ( p&lt; 0 . 0001), and between systems 1 and 2 ( p =0 . 0046). However, the difference between systems 0 and 1 is only weakly significant ( p =0 . 0989). These results indicate that there is a noticeable difference for the average ti me that users need to find a useful docu-ment using search systems with different DCG@1 levels. However, this effect is most noticeable when comparing non-relevant documents (system 0) and highly relevant documents (system 2). Marginally relevant documents (system 1) are also clearly differentiated from highly relevant documents, but are similar to non-relevant documents.
 These differences between the three releva nce levels strongly suggest that, for P@1, it is preferable to fold level 1 (marginally relevant) documents with level 0 (non-relevant) documents, since the difference between level 1 and 2 is much stronger than the difference between level 0 and 1. 4.2 Relevance Profiling Based on Split Agreement To investigate the effect of relevance mismatch on the relative outcomes of batch and user experiments, we classify users based on their relevance preferences ex-pressed during searching. While working through the search topics, users were able to view documents, and then either choose to save them (a relevance vote), or close them and continue searching (a non-relevance vote). We use these rel-evance decisions to classify users, u sing the split agreement approach outlined in Section 2. Recall that users are classi fied into three groups: TREC-like (their relevance profile matches the TREC judgin g scheme); generous (their threshold for relevance is lower than that for TREC, so they are likely to accept level 0 documents as useful); and parsimonious (their threshold for relevance is higher than that for TREC, so they are unlikely to accept level 1 documents as useful).
If users can be successfully classified acco rding to their relevance behaviour, then we would expect TREC-like users to show a larger difference in the time taken to find a useful document. That is, for users with relevance profiles that more closely match the criteria used i n the batch experiment, the difference between retrieval systems as observed in the user task should be the most pro-nounced. Conversely, for users whose rel evance profiles differ from the batch rel-evance judgements, the difference in ret rieval systems should be less pronounced (generous users would be expected to be somewhat faster, no matter which sys-tem they are using; the opposite expectation holds for parsimonious users, who would be expected to be slower no matter which system they are using).
Note that here we are analysing relevance mismatch compared to the under-lying batch experiment assumptions, based on the TREC relevance judgements: relevance is binary, with level 1 and level 2 documents grouped together into a single  X  X elevant X  category. That is, for the P@1 metric there are two possible outcomes, score of 0 (from level 0 documents), and a score of 1 (from level 1 or 2 documents).

Search times for different systems are shown in Figure 1. Across all users (represented by a triangle), task compl etion time falls when using a system with P@1 of 1, compared to 0. This difference is statistically significant, as shown in Table 2. Generous users are fast, whether they are using a system with a metric level of 0 or 1. As expected, the time taken to find a useful document is similar at both levels for this group, and the difference in batch metric does not lead to significantly different outcomes in the user task. Generous users are slow-est when using the system where P@1 equals 0, and speed up when P@1 equals 1. Users in the TREC-like class exhibit similar behaviour to the generous class. Both of these classes show a substantially larger difference in median time be-tween the two systems than do generous users.

We note that, based on our post hoc grouping of users, the number of subjects in each class differs (for example, only 8 out of 37 users are in the TREC-like category, contributing to a weaker p -value despite the noticeable difference in median time). Nevertheless, it appears that relevance profiles can help to deter-mine whether conclusions about batch P@1 values can be transferred to a user population: for generous users (who are s atisfied with low-relevance documents as measured on the TREC scale, and don X  X  differentiate strongly between any document levels), the differences are un likely to hold. However, when the popu-lation consists of TREC-like or parsimonious users, the batch results are likely to be transferable. Batch evaluation is the dominant paradigm used to compare the performance of information retrieval systems. While a growing body of literature has sug-gested that there are mismatches betw een batch experiments based on widely used performance metrics such as MAP and actual search tasks are carried out by users, our results indicate the a simple performance metric such as P@1 can lead to search scenarios where the exp ected outcomes from batch experiments transfer directly to a precision-based us er search task. This effect is statisti-cally significant when relevance is treated as a binary criterion, as in the TREC framework. When multiple-level rele vance judgements are available, DCG@1 is similarly effective at transferring ex pected batch experiment outcomes to a precision-oriented user search task. The d ifference in user performance is signifi-cant between the level 0 and 2, and level 1 and 2, relevance levels. However, it is only weakly significant between relevance levels 0 and 1. This suggests that the when multiple levels of relevance are folded into a binary scale, marginally rele-vant documents (level 1) should be grouped with non-relevant documents. This is in contrast with current standards used in IR evaluation, where marginally relevant documents are generally bundled with highly relevant documents.
The three system levels described above are intended to reflect possible scenarios of the DCG@1 and P@1 metrics. We note that, given the fixed distribution of relevance levels after rank position 1, our three defined system levels also correspond to particular values o f other batch metrics. This holds for any metric that is only dependent on th e relevance values of items within the top 10 positions of the ranked list (for example P@ N or DCG@ N for N  X  10). However, for these metrics, the system levels defined for this study represent only a small range of the possible values that the metrics can take on. Therefore, the above conclusions from focusing on N =1 metrics should not be extended to the N&gt; 1 alternatives directly. Moreover, metrics such as MAP, which include a recall component, will diffe r from topic to topic, since each topic considered will have a varying number of total relevant d ocuments available. The conclusions from our experiments therefore do not transfer directly to such metrics.
We also investigated relevance mismatch, using split agreement to classify users into different relevance groups. Our analysis demonstrated that the trans-ferability of batch experiment conclusions can differ between user classes; in par-ticular, generous users, who have low thresholds for considering a document to be relevant, do not reflect the batch conclusions obtained form the P@1 metric.
The relevance profile analysis used the entire data obtained from the searching task; however, to be useful from a practi cal point of view, relevance matching should allow us to infer whether batch re sults are likely to successfully trans-fer to users, without requiring a full-scale user-study. In future work, we intend to investigate suitable approaches for estimating user relevance profiles with a minimum of effort. Naturally, there are many other possible causes of mismatch between batch experiments and user-based evaluations, including different lev-els of knowledge about the topics being searched on, age differences, cultural differences, and gender differences. We plan to incorporate these into the user classification approaches in future work.

