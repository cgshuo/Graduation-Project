 Modeling and understanding the interactions between users and items account-ing for what kind of item this specific user like, as well as exploring relationships amongst the items themselves annotating which type of shirts match the pants just purchased, are the core tasks of a recommender system. In other words, user preferences and sequential patterns are captured by the above two forms of interactions respectively.
 ested can be mined from behavioral data, such as browsing and co-purchasing logs. However the latent bundle information reflected by user-level sequential patterns remains unutilized. For example, the fact that X and Y were bough together by users X  subconscious or users bought X also bought Y by unawarely. In order to predict user actions what is the next product to purchase, movie to watch, or place to visit, we strive to predicting personalized sequential behaviors from collaborative data (e.g. purchase histories of users), which is challenging as the combining of user-level sequential patterns and latent bundle relation-ships, which are extracted from semantics associated with products, need to be carefully to account for both personalization and sequential transitions. items and users X  behaviors? in particular the texts associated items and user-s, which are both high-dimensional and semantically complex. Secondly, con-sidering users tendencies to behave with consistent contents within and across bundles, how can we model temporal and latent bundle dynamics? Facing the la-tent bundle dimensions winding among products and users X  inclinations towards and organize these characteristics in the most effective way. Finally, the sparsity of the data involved. We must expend considerable effort to cope with. This issue is exacerbated when modeling semantic simultaneously, due to the high-dimensional representations of the items involved.
 using a Markov Chain, which capture the sequential behaviors with which the next product to purchase. We integrate the semantics and latent bundles to F-PMC to pick out and emphasize the factors influencing users X  behaviors. And we choose an approach based on logistic regression to model the bundle relation-ships because (1) it can be scaled to millions of products by hundreds of millions of links, and (2) it can be adapted to incorporate both latent features and se-mantic features, as well as arbitrary transformations and combinations of these features. By combining the two models by the semantics associated with prod-ucts, we can capture individual users preferences towards particular styles, as well as the tendency to interact with items that are coefficient to users X  records. We also make several extensions of these models, e.g. to handle longer memory than simply the previous action.
 by sparsity issues in many real-world datasets, which makes it hard to estimate parameters accurately from limited training sequences. In the paper, we devel-op the framework Latent Bundles Sequential Behaviors for Personalized Rank-ing (LBS-ranking) combining FPMC and logical regression, incorporates latent bundling features in social networking service environment for the task of person-alized ranking on feedback datasets, as shown in Fig.1. By learning the semantic dimensions people consider when selecting products, the framework is able to alleviate cold start issues, help explain recommendations in terms of semantic signals, and produce personalized rankings that are more consistent with users preferences. In terms of latent bundle relationships intertwining with each prod-uct and the strength of sequential behaviors in the sparse real-world datasets, the framework naturally integrates sequential behaviors and hidden bundle re-lationships between products by learning a personalized weighting scheme over the sequence of items to characterize users. Our main contributions are outlined as follows:  X  we propose a novel framework that explores the latent bundling relationships  X  Conducting experiments on real-world recommendation dataset to demon-Fig. 1: An example of how our method, LBS-Ranking, makes recommendations. Skirt marked by A is recommended to the user because it (1) frequently follows the dress and short skirt brought by user recently and meet the user X  X  interest in combining different clothes; (2) is related with clothes in user X  X  wardrobe, which is good to match clothes according user X  X  style. The former is modeled by Markov Chains for personal sequential patterns and the later explored the latent bundle relationships between items and personal preferences for different bundles.
 Section 3 is the problem formulation. Model learning and inferring is detailed in Section 4. The experiments are presented in Section 5. Section 6 is the conclusion. Sequential Recommendation. Markov chains are a powerful tool to mod-el stochastic transitions between different states. In sequential recommendation domains, Markov chains have been studied by several earlier works, from inves-tigating their strength at uncovering sequential patterns (e.g. [2, 3]), to directly modeling decision processes with Markov transitions [2]. In a more recent pa-per [1], Rendle et al. proposed to combine the strength of Markov chains at modeling the smoothness of subsequent actions and the power of Matrix Fac-torization at modeling personal preferences for sequential recommendation. The resulting model, called FPMC, has shown superior prediction accuracy by bene-fiting from both simultaneously. Our work extends the basic idea mainly by mod-eling complex visual and social dynamics and further considering Markov chains with higher orders. And there is another line of work that employs (personal-ized) probabilistic Markov embeddings to model sequential data like playlists and POIs [4 X 6]. These works differ from ours in terms of both the signals being modeled and the types of models.
 cessfully applied to sentiment analysis and natural language processing. In recent years, several approaches have been developed for learning composition opera-tors that map word vectors to sentence vectors including recursive networks [7], recurrent networks [8], convolutional networks [9,10] and recursive-convolutional methods [11, 12] among others. All of these methods produce sentence represen-tations that are passed to a supervised task and depend on a class label in order to back propagate through the composition weights. [13] proposed a mod-el, called skip-thought vectors, for learning high-quality sentence vectors without a particular supervised task in mind. The model abstracts the skip-gram model of words to the sentence level, and encodes a sentence to predict the sentences around it.
 past user ratings to predict unknown ratings, has attracted more and more attention [14]. Collaborative Filtering can be roughly categorized into memory-based [15] and model-based methods [16].Despite the success of various model-based methods, matrix factorization based model has become one of the most popular methods due to its good performance and efficiency in handling large datasets [16].
 the set of items. Each user u is associated with a sequence of actions S u (e.g. S k  X  I . The action history of all users is denoted by S = (S u 1 , S Additionally, each item i is associated with an explicit feature vector  X  i , e.g. in our case Semantic features extracted from reviews. Using the above data ex-clusively, our objective is to predict the next action of each user and thus make recommendations accordingly. Notation to be used throughout the paper is sum-marized in Tab.1. Next, we will build our model incrementally Details can be found in Section 4. 4.1 Background Factorizing Personalized Markov Chains: Markov Chains are strong at cap-turing short-term dynamics. Nevertheless, such methods are limited in their a-bility to capture user preferences that are both personal and long-term. FPMC is introduced as a recommender method based on personalized Markov chains over sequential set data. The advantage of this approach is that each transition is influenced by transitions of similar users, similar items and similar transitions. Thus the quality of the final transition graph is much higher than that of a full parametrized model. where r u ,r j ,r i the latent feature for the users, the items in the transition (out-going nodes) and the items to predict (ingoing nodes). Bringing together the personalized set Markov chains with the factorized transition cube results in the factorized personalized Markov chain.
 ically learns features from raw data using a general purpose learning procedure, instead of designing features by human engineers [13]. Inspired by the good idea, our key idea is to adopt skip-thought vectors to learn the representation of review content, where review contents with similar semantics and sentiment will have similar vector representations. The Recurrent Neural Networks(RNN) structure can take any length of sentence as input, we thus employ the learnt encoder to represent the review contents, then each review content is represented as a D -dimension vector  X  . The vector  X  typically represents the deep semantics of the review help improve the prediction accuracy. 4.2 Modeling Personalized Markov Chains Incorporating Semantic While the formulation Eq.1 only makes use of the collaborative data, without be-ing aware of the underlying content of the items themselves. Such a formulation may suffer from cold item issues where there arent enough historical observations to learn accurate representations of each item. Modeling the content of the items can provide auxiliary signals in cold-start settings and alleviate such issues. mantic features. The semantic vector  X  i and  X  u for item i and user u adopted by skip-thought vectors can been incorporated into the Collaborative Filtering framework. Given the previous action sequence S u of user u and the semantic features  X  S u corresponding to items in sequence S u of user u , the probability choice u made at time step t and | S u | = t  X  1. Following this intuition, we propose to factorize the personalized Markov chain with the following formulation:  X p u ( i | S u ) =  X  u +  X  i +  X  r u ,r i  X  + where  X  j is the semantic features learned from RNN,  X  i is item i  X  X  transition feature vector, r u and r i are employed to capture user us latent preferences and item is latent properties respectively,  X  u and  X  i are the bias for user u and item i . In this new formulation, each user is associated with a vector  X  u =  X  previous L actions should contribute with different weights to the high-order smoothness. feature of user U is indicated by 1 | resents the user u  X  X  preference for next action. The transition of user u from item S t  X  1 at time step t  X  1 to item i at time step t in Equ.2 capture the long-term preferences of user u and temporary interest of user u . 4.3 Latent Bundle Relationships While most recommender systems focus on analyzing personalized patterns of interest in products to provide personalized recommendations, another impor-tant problem is to understand relationships between products, in order to surface recommendations that are relevant to a given context. In this section, we focus on learning the latent bundling relationships among items arisen by users X  per-sonalized sequential behaviors. In our case, we want to model the relationships that the transition of user u from sequential actions S u to item i (at time step t ) can be explained: there are latent bundling relationships between item i and the items in sequence S u of user u , and users X  behaviors are influenced by the latent bundles. In other words, users will choice items that most relevant to the previous action sequence S u .
 means each of the previous L actions should contribute with different weights to the high-order smoothness. That is, we want the personalized weighting factor as: where  X  u k = D  X  u , X   X  i , X  S u We want the semantics associated with each product to be related for logistic regression in the sense that we are able to learn a logistic regressor parametrized by  X  u that predicts , using the sematic  X  S u is a pairwise feature vector describing the two products. by defining our features to be the element wise product between  X  S u with similar semantics are likely to be linked. The logistic vector  X  u for user u then determines which u  X  X  personal preference for different bundles. 4.4 Model Learning The ultimate goal of the sequential prediction task is to rank observed (or ground-truth) items as high as possible so that the recommender system can make plausible recommendations. This means it is natural to derive a person-alized order &gt; u,t at each step t to minimize a ranking loss such as sequential bayesian personalized ranking. Here i &gt; u,t j means that item i is ranked higher than item j for user u at step t given the action sequence before t .
 (  X  p u,t,  X  is a shorthand for the prediction in Eq.3)is employed to characterize the probability that ground-truth item S u t is ranked higher than a negative item j given the model parameters  X  , p ( S u t &gt; u,t i |  X  ). Assuming independence of users and time steps, model parameters  X  are inferred by optimizing the following Maximum A Posteriori (MAP) estimation: p (  X  ) is a Gaussian prior over the model parameters. Note that due to the additive characteristic, our formulation can allow t to run from 2 (instead of L + 1) to the last item in S u . we adopt Stochastic Gradient Descent (SGD) which has seen wide success for learning models in BPR-like optimization frameworks. The SGD training procedure works as follows. First, it uniformly samples a user u from U Finally, the optimization procedure updates parameters in the following fashion: where  X  is the learning rate and  X   X  is a regularization hyperparameter. 5.1 Dataset and Evaluation Metric To evaluate the ability and applicability of LBR-ranking to handle different real-world scenarios, we include a spectrum of large datasets from different domains in order to predict actions ranging from the next product to purchase, next movie to watch, to next review to write, and next place to check-in. Note that these datasets also vary significantly in terms of user and item density (i.e., number of actions per user/item). introduced by [17]. This is one of the largest datasets available that includes re-view texts and time stamps spanning from May 1996 to July 2014. Each top-level category of products on Amazon.com has been constructed as an independent dataset by [17]. In this paper, we take four large categories including Movies, Books, clothing, and Electronics, shown in Table.2.
 actions of each user to create a validation set G and a test set O : one action for validation and the other for testing. All other actions are used as the training set T . The training set T is used to train all comparison methods, and hyper-parameters are tuned with the validation set G . Finally, the predicted ranking is evaluated on the test set O : with the widely used metric AUC (Area Under the ROC curve): where t is the ground-truth item of user u at the most recent time step t . The indicator function  X  ( b ) returns 1 if the argument b is true, 0 otherwise. The goal here is for the held-out action to calculate how highly the ground truth item has been ranked for each user u according to the learned personalized total order &gt; recall(@k). Given a set of recommended items of a given user rec , and a set of known-relevant products rel (ground-truth brought) the precision is defined as: where the fraction of recommended items that were relevant. The precision@k is then the precision obtained given a fixed budget, i.e., when rec = k. This is relevant when only a small number of recommendations can be surfaced to the user, where it is important that relevant products appear amongst the first few suggestions.
 products we select two type behaviors from Amazon:  X  X sers who bought x also bought y and Users frequently bought x and y together X , shown in Table.2.Since we experiment on the latent bundle relationships and it is impractical to train on all pairs of non-links, we start by building a balanced dataset by sampling as many non-links as there are links. For each experiment, the latent bundle relationships R and a random sample of non-relationships  X  R are pairs of items connecting different subcategories of the category we are experimenting on. Note that | R | =  X  R and they share the same distribution over the items. We split our training data ( Rand  X  R) into 80% training, 10%validation, 10% test, treating report the error on the test set. The iterative fitting process described in Equ.5 continues until no further improvement is gained on the validation set. 5.2 Baselines  X  BPR-MF : [18] is the state-of-the-art method for personalized ranking in  X  FPMC : is a method that uses a Personalized Markov Chain for the sequen- X  FMC(Factorized Markov Chains) : factorizes the item-to-item transition  X  Item-to-Item Collaborative Filtering (CF) : In 2003 Amazon [20] re-Ultimately, our baselines are designed to demonstrate that (1) the strength of state-of-the-art personalized patterns and latent bundle relationships unaware method on our datasets ( BPR-MF); (2)the importance of modeling temporal dynamics (FMC) (3) the effectiveness of the state-of-the-art sequential prediction method by combining BPR-MF and MC (FPMC);(4) Identifying relationships among items is not enough to provide personal sequential patterns(CF); and that (5) the strength of our proposed combination of factoring sequential prediction and latent bundle relationships(LBS-Ranking). 5.3 Performance and Quantitative Analysis Error rates on both of all items and cold start sets, for all experiments are re-ported in Table.3(Lower is better.) and results in terms of the average AUC on Books are shown in Fig.2(results for Womens clothes, Movies and Eletronis are similar and suppressed for brevity). For experiments on also bought relation-ships, LBS-Ranking uses K = 100 dimensions for semantics, D = 30 dimensions for other factors and L = 5 order of Markov Chains, and BPR, FPMC and FMC also use D = 30. We make a few observations to explain and understand our findings as follows: 1) BPR vs CF vs LBS-Ranking: BPR makes the worse than other meth-2) BPR vs FMC vs FPMC: Compared with BPR-MF, FMC focuses on cap-Table 3: Test errors (1  X  AUC ) of the predictions on all items or cold start set using products on datasets of the Amazon. 3) CF vs FPMC vs LBS-Ranking: CF proceed by computing for each pair 4) The Order of Markov Chains: Generally, the performance of LBS-Ranking the quality of the recommendations produced by LBS-Ranking. We want to examine whether the substitutes in ranking for the query are complementary to user X  X  preference and needing. We demonstrate the latent dimensions learned by LBS-Ranking, which kind of characteristics the framework is capturing to explain users X  preferences of complementary. A direct way to exhibit these dimensions extracted by Eq.3 is to rank items highly that achieve maximal values for each user, seen Fig.3(results for movies, eletronis and women X  X  clothes are similar and suppressed for brevity). Fig. 2: Performance comparison of differ-ent methods with number of training it-erations on Books data. LBS-Ranking not only learns the hidden bundles, but also discovers the most relevant underlying semantic dimensions and maps items and users into the uncovered space. Exploring the latent bundle relationships between items with users sequential patterns is challenging especially when it comes to large real-world datasets. In this paper, we tried to resolve the above issues by building sequential pat-terns with semantics to model correlative items and users X  changing behaviors simultaneously. Empirically we evaluated our proposed framework on sequential patterns to test its ability to handle both preferences of users and the hidden bundle relationships. Experimental results demonstrated the advantage of our approach over other baseline models.
