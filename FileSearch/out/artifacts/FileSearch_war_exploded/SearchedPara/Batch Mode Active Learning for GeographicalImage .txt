 Data classification has been an active research topic in the machine learning community for many years. To automatically assign data samples to a set of predefined classes is the goal of data classification tasks. One prerequisite for data classification is to obtain the labeled samples beforehand. So data collec-tion and annotation is a fundamental problem, but actually it is very time and labor consuming to label the samples. In order to reduce the effort in acquiring labeled samples, active learning is a very useful tool in such situations when unlabeled data is cheap to collect but labeling them is expensive. Now a number of active learning methods [5] have been developed for data classification [22]. The key idea of active learning is to identify the samples that are most infor-mative with respect to the current classification model. In current research, two points should be taken into account in the query function adopted for selecting the batch of the most informative samples to design the practical active learn-ing algorithms: the uncertainty and the diversity [3,4,16]. The uncertainty aims to choose the samples the current classifier is most uncertain about [1,2]. For multi-class classification problems, the margin based uncertainty sampling is a popular method which aims to select the samples which are closest to the separat-ing hyperplane[11]. In such methods, the queried samples may have redundancy. The diversity of active learning aims to alleviate this problem by querying the most representative samples for the overall patterns of the unlabeled data[6]. Since using either kind of criterion alone is not sufficient to get the optimal re-sult, there are several works trying to query the unlabeled samples with both high information and high representation[3,16]. Usually they are either heuristic in designing the specific query criterion or ad hoc in measuring the informative-ness and representativeness of the samples. In this paper, the proposed method combines the discriminative information and representative information into one optimal formula as a diversity criterion to remove the redundancy samples in the uncertainty criterion. For the discriminative information, we derive a novel form of upper bound for true risk in the active learning setting, by minimizing this upper bound to measure the discriminative information. For the represen-tative information, we adopt the maximum mean discrepancy (MMD) to match the distribution between the labeled sa mples and queried samples. We test the proposed method on two benchmark hyperspectral images. The rest of this pa-per is organized as follows: Section 2 briefly reviews the related work. Section 3 introduces the general framework of the proposed active learning method in detail. Section 4 pres ents the experiments and inter prets the results. Finally, we conclude the paper and propose in Section 5. In recent research, a number of criterions for active learning have been pro-posed for classification problems. One common criterion is called uncertainty sampling[1,2], which aims to query the samples with the least certainty to the current model. The other common criterion is called diversity sampling[3,4,16], whose goal is to reduce the redundancy of t he uncertain samples. For the uncer-tainty sampling, there has been a large amount of research into the study of the uncertainty criterion, which can be grou ped into three main areas: 1) query by committee [5,6], where the uncertainty is evaluated by measuring the disagree-ment of a committee of classifiers; 2) the posterior probability[8,9], in which the posterior probability is used to measure the uncertainty of the candidates; and 3) the large margin heuristic, where the distance to the margin is calculated to measure the uncertainty, which is well-suited for the margin-based classifiers such as support vector machine (SVM)[10,11]. However, in the current research, less attention is being paid to the diversity criterion. In general, cluster algorithms, such as k-means[12] and its kernel version[13,14], can be used to select the most uncorrelated samples by extracting from each cluster the representative one that is closest to the corresponding cluster center for labeling. However, clustering al-gorithms depend on the correctness of the convergence and are usually influenced by the adequacy of initialization. Meanwhile, in such methods, the queried data are not guaranteed to be i.i.d (identical and independent distribution), and the sampled data could not stay the same with the original data distribution, as they are selectively sampled based on t he active learning criterion[15]. This part mainly describes the active learning method proposed in this paper. Since using either kind of criterion alone is not sufficient to get the optimal re-sult and less attention is paid on the diversity criterion, there are several works trying to query the unlabeled samples with both high information and high representation[16,4], so the proposed method is mainly focused on the diver-sity criterion and combines the discriminative information and representative information into one formulation as the diversity criterion. For uncertainty cri-terion, the proposed method adopts the two advanced the large margin heuristic method, namely MS, MCLU[17,18]. Then the mechanisms of the two uncertainty criterion is briefly summarized. 3.1 Uncertainty Criterion The MS Active Learning Algorithm. The margin sampling (MS) heuristic takes the advantage of the distance to the hyperplane in SVM which builds a linear decision function in the high dimensional feature space H where the samples are more likely to be linearly separable. Consider the decision function of the two-class SVM: Therefore, the candidate included in the training set is the one that respects the condition. The MCLU Active Learning Algorithm. The adopted MCLU technique selects the most uncertainty samples according to a confidence value, c ( x ) ,x  X  U , the n decision boundaries in the binary SVM classifiers included in the OAA architecture[16]. In this technique, the distance of each sample x  X  U to each hyperplane is calculated, and a set of n distance values is obtained. Then, the con-fidence value c ( x ) can be calculated using different strategies[18]. In this paper, the difference function c diff ( x ) strategy is used, which considers the difference between the first and second largest distance values to the hyperplanes. 3.2 The Proposed Diversity Criterion Suppose we are given a dataset with n samples D = { x 1 ,x 2 ,...,x n } of d dimen-( x n  X  l samples form the unlabeled set U = { x date dataset for the active learning. In the following, we will discuss the discrim-inative and informative criterion in detail. As mentioned above, the proposed diversity criterion combines the discriminative information and representative information into one optimal formula. Now we represent discriminative informa-tion and representative information, respectively.
 Discriminative Information Determined by the Minimum Margin. In this paper, we derive a novel form of upper bound for true risk in the active learning setting, by minimizing this upper bound to measure the discriminative information. In formula (4), Y i  X  X  1 ,  X  1 } f is the classifier, f 2 F is used to constrain the complexity of the classifier class. It is known that, generally speaking, the data classification is a multi-class problem. Now we develop it to a multi-class formula, and assume there are classes in a dataset. According to formula (4), binary class problem could be built, so the multi-class optimal formula could be represented as: min label; Q c is query samples set and is get under the binary problem when the class c label is 1 and the other classes label is -1. Since the multi-class optimal formula is composed by N c binary classes, and it is independent for each binary class, so we can solve (5) by solve each binary class independently. Now we use the linear regression model in the kernel space as the classifier, which is in the form of f ( x )= w T  X  ( x ), with the feature mapping  X  ( x ). By minimizing the worst case, the objective function (5) becomes: min Representative Information Determined by MMD. The representative part in proposed method is the MMD term. MMD is used to constrain the distribution between the labeled and que ried samples, and makes it as similar to the overall sample distribution as possible. It captures the representative information of the data structure in the proposed method. According to[25], the MMDcanbeexpressedas
In formula (7), 1 l , 1 u are two vectors of length l and u , respectively, with all entries 1;  X  is the indicator vector with u elements and each element  X  i  X  X  0 , 1 } ;  X 
T 1 u = b ; b is the query number for a binary class problem. K is the kernel its sub-matrix between the samples from set A and B. The objective function (6) can be further simplified as where K 1 = 1 2 K UU ,k = k 3  X  k 2 ,  X  x i  X  U, k 2 ( i )= l + b n The Proposed Discriminative and Representative Diversity Criterion.
 We combine the discriminative and representative information into one optimal formula, which use a weight to balance the discriminative information term and representative information term. Firstly, build the optimal formula for the binary problem is built and developed it to multiclass problem. So we can put (4) and (8) together, and obtain the binary class optimal formula: +  X  (  X  T K 1  X  + k X  )
Like the formula (5), we develop it to multi-class problem. min The formula (9) is the main part in the proposed method. The alternating op-timization strategy is employed to solve it[3]. Since the multiclass is formed by many binary problems, (10) can be solved by a binary model. For a binary prob-lem, if the query index  X  is fixed, the objective is to find the best classifier based on the current labeled and query samples: min The equation (11) can be solved by the alternating direction method of multi-pliers (ADMM)[20]. If w is fixed, the objective becomes which can be rewritten as where a j =( w T  X  ( x j )) 2 +2 w T  X  ( x j ) , (13) is a standard quadratic program-ming. If we relax  X  to a continuous value range [0 , 1] u ,the b samples in the pool U corresponding to the largest b elements are the sample with discriminative and representative information. 3.3 The Proposed Batch Mode Active Learning In the novel discriminative and representative criterion, a QP problem should be solved, so if the unlabeled data set is big, the solution will be slow. In order to overcome this problem, for each iteration, the proposed method firstly uses the uncertainty criter ion to select the N most uncerta in samples from the unlabeled samples pool U as the new unlabeled samples U n . The optimal methods include MS, MCLU. For each class, the training set L and the unlabeled samples U n are used to establish a binary class problem, so when there is N c classes, N c binary class problem can be built. For each binary class the discriminative and representative criterio n are employed to select b batch size samples, so we can get b ( N c ) query samples in the query set G . Since we apply the discriminative and representative criterion on the same unlabeled samples U n ,the same samples may be queried in different binary class, so the b ( N c ) query samples may contain the repetitive samples. Firstly, the repetitive samples in G should be merged to obtain the new dataset S in which the samples are different. In this step the size of S is adaptive. In order to control the size of the querying samples added into the training set in each iteration and reduce the spatial redundancy, the maximum number of the new queried samples is set as h. The last samples we queried according to the size of S. If the size of S is greater than the initial query batch size h, the h samples are chosen corresponding to the h smallest values of formula (9) from the set S. In this way, the size of query samples is always less than or equal to the initial h. The progress of proposed method is shown in Algorithm 1.
 Algorithm 1. Algorithm 2 The Progress of Proposed Method In this section, two benchmark hypersp ectral images have been used for exper-iments. The proposed method is compared with the state-the-art batch mode active learning methods and at last the experimental results are analyzed. We list all the methods we compared in the experiments in Table 1.

Two benchmark hyperspectral image datasets are Indian Pines and Wash-ington DC[23]. Indian Pines was acquired by NASAs Airborne Visible Infrared Imaging Spectrometer (AVIRIS). A total of 10249 samples from 16 classes are used. Meanwhile, Washington DC Mall d ata set is a Hyperspectral Digital Im-agery Collection Experiment airborne Hyperspectral Image. A total of 8424 pix-els from seven classes were used in our experiments on it. We summarize the characteristics of the data sets in Table 2.

In the experiments, for each data set, 15 samples are randomly selected for each class as the initial training set, a nd the rest are regarded as the unlabeled samples. the total samples are took as the testing set. In the uncertainty criterion, 400 uncertain samples are chose to input into our proposed diversity criterion. Because the queried samples of the proposed method is adaptive, so here the maximum value of h which is the number of samples add to the training set should be set in each iteration. For Indian Pines data set which includes 16 classes, in the experiment, we set h=20, and set the batch size b=2 which is the number of samples queried from a binary class. For the Washington DC data set which includes 7 classes, we set h=15 and b=3. But for the other compared method, the number of queried samples is manually fixed, so for Indian Pines the number of queried samples is 20 and for Washington DC the number is 15. There are some parameters in the compared methods, we adopt the values in the original papers. In our proposed method, we set the regularization weight , and the trade-off parameter is chosen from a candidate set by a cross validation. For each data set, we use the same kernel for all methods, which is properly chosen from the linear kernel or RBF kern el with the optimal kernel width. For the fairness, the same SVM classifier is used for all methods to evaluate the informativeness of the selected sample s and set the number of iterations as 60 . The accuracy curve of the SVM classifier are reported after each query. The SVM provided by LIBSVM[21]. 4.1 Results and Analysis For each data set, we run the experiment independently for several runs, and present the results in Fig. 1. Intuitionally, for the two benchmark hyperspectral image datasets experimental results, we can observe that our method outper-forms the competitors in five aspects. Firstly, the proposed methods MS-DR and MCLU-DR show the curves higher learning rate than the other methods. Secondly, when the overall accuracy is same, the queried samples of the pro-posed method are less than the state-the-art methods. Thirdly, when the iter-ation time is same, the proposed method can obtain a higher overall accuracy with less queried samples than the other methods. And the standard deviation of the proposed method is smaller than the compared methods. Finally, the curves of the proposed method are much more smooth than the state-the-art methods, especially for the Washington DC experi ments. These results demonstrate that both discriminative and representative information are critical for active learn-ing, and a proper balance of these two sources of information will boost the active learning performance.

In the proposed method, the important parameter is the balance weight  X  be-tween discriminative information and representative information. In the experi-ment, for each iteration, we run our algorithm from a candidate set { 10  X  5 , 10  X  4 , 10  X  3 ..., 10 5 } , and choose the best value which make the overall accuracy highest. We report the results on the two hyperspectral image data set. Because for each binary class there is a best value, so in each iteration the value should choose N c times, N c is the number of classes. Now we show the value statistics of each class in 60 iterations in figure 2. From fig.2, we can observe the distribution of trade-off parameter  X  on two data sets are similar in the experiments. We can see that when the  X  is greater than 1, these values occupation is less than 30%, this part values is mainly to adjust the training set when the uncertain information is not enough to select the most informative samples. But for the  X  is less than 1, this part values take an occupation more than 70%. For the proposed method, when  X  is greater than 1, the representative information is predominantly, or the discriminative information is major. According to that, all the experiments demonstrates that the discriminative information is predominant. So if the user does not require selecting the best value, he can choose a compromised value which not only pays attention on the discriminative information but also considers the representative information.  X  =10  X  3 can be used in all the iterations. 4.2 Sensitivity Analysis This paper developes a dicriminative and representative information based batch-mode active learning, and applys it to Hyperspectral image classification. Ac-cording to the experients on two benchma rk remote sensing image data sets, the proposed AL method shows a superior performance when compared with the other state-of-the-art AL methods. First ly, the proposed diversity can effectively reduce the number of querying samples with the adaptively number of the new queried samples. Secondly, by considering the uncertainty information in the di-versity, the proposed diversity criterion measures the uncertainty information by the disiciminative information, and combines the representative information into one formula. Finally, maximum mean discrepancy (MMD) is used to measure the representative information, and makes sure the new queried samples are i.i.d between each other. That also makes the new queried samples have consistency of distribution in the original space.
 Acknowledgments. This work was supported in part by the National Ba-sic Research Program of China ( 973 Program) unde r Grant 2012CB719905, the National Natural Science Fo undation of China under Gra nts 61471274 and 41431175the Natural Scien ce Foundation of Hubei Province under Grants 2014CFB193 and the Fundamental Research Funds for the Central Universities under 211  X  274175. National Natural Scien ce Foundation, C hina (No.70901060 and 61471274), Hubei Province Natural S cience Foundation (No. 2011CDB461), Youth Plan Found of Wuhan City (No.201150431101) and Wuhan City Science and Technology Plan Project (No. 2015010101010023). The authors also grate-fully acknowledge the helpful comments and suggests of the reviewers, which have improved the presentation.

