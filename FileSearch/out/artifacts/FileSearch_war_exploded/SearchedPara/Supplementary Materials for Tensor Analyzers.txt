 The objective function Q of the EM algorithm is the expected complete log-likelihood, taken over the posterior distribution of the latent factors, and summed over N training cases: The closed-form M-step update equations are: the 6 random latent factors of two TAs in the figure below. Left panel is from a TA learned on around 20 Gibbs iterations.
 Figure 1: left : MCMC trace plot for TA learning on synthetic data. right :MCMC trace plot for TA learning on high dimensional face images.
 We also looked at how posterior inference converges in a 200 component MTA trained on 8 x 8 natural image patches. Figure 2: Gibbs sampling quickly converges around 20 iterations. 9 components were randomly picked from a MTA with 200 components trained on natural image patches. For each component, we randomly selected 6 latent factors (one for every color). basic Importance Sampling gives: to 1 . 0 . For TAs, the log of the tractable base distribution q ( z ) is: which is simply the prior distribution over the latent factors.
 the distribution of interest, we can write the intermediate distribution as: Algorithm 1 AIS for TA let k = 1 , 2 ,...,K ,  X  k =1 = 0 ,  X  k = K = 1 . for i = 1 to M do end for 100,000 Monte Carlo samples estimate the  X  X rue X  log-likelihood.
 Therefore, Gibbs sampler, which simply performs alternating Gibbs sampling of the TA X  X  posterior where the original diagonal noise  X  is modified to be  X   X  . We denote this operator as T  X  ( z 0  X  z ) . independent AIS chains.
 the estimated log-likelihood by sampling from the prior (Sec. 3.3 of main paper), using 100,000 samples. Since we are sampling from only 2 dimensions, this Monte Carlo estimator has very low variance and its value is taken to be the true data log-likelihood. just as good as MFAs on the simpler ones. Figure 5: MTA vs. MFA on 2D synthetic data. A mixture of two components are used in both MTA and MFA.
 by a randomly initialized MFA model. ture of Factor Analyzers on the 2D synthetic datasets .
