 Nowadays, many factors support a growing production of information. A regular increase of 30% was noted between 1999 and 2002 in the information production [1]. The problem of accessing this mass of information comes under the field of domains like digital libraries and information retrieval but currently few works of these domains have taken into account the size effect in their approaches. The size of large collections (or web) coupled with and the ambiguity of user query make it difficult for search engines to return the most recent and relevant information in real-time. The need to learn more about they way collections size acts on retrieval effectiveness becomes increasingly pressing. In this work, we present works dealing with multigraded relevance and in the last part we present the metrics designed to evaluate the ability of IR systems to rank documents according to their relevance levels. 2.1 The Relevance as a Complex Cognitive and Multidimensional Relevance is the central concept for IR evaluation, usually considered as a binary notion. However, some research works showed that the relevance is a complex cognitive concept, that has many dimensions ([2], [3] , [4], [5]. Many different aspects of relevance have also been discussed by proposed definitions and clas-sifications ([4], [6], [7], [8]). It is not an easy job to judge documents and give them a relevance level regarding a topic as many variables affect the relevance ( Rees et al. [9]: about 40 variables, Cuadra et al. [10]: 38 variables). All these works and many others suggest that there is no single relevance (there are many relevances) and that relevance is a complex cognitive problem. 2.2 Multigraded Relevance in IR In the user X  X  point of view, it is desirable to have IRS that retrieve documents according to their relevance level [11]. IR evaluation methods should then credit (or at least recognize) IRS for their ability to retrieve highly relevant docu-ments at the top of their results list, by taking into account various relevance levels of a document for a given query; they have been studied in some pre-vious IR works ( Tang et al. [12]: a seven-point scale, Spink et al. [13] used a three-point scale). Some test collections provide multigraded relevance assess-ments (TREC Web Track collection: three point scale [14], INEX collections: a multilevel scale, NTCIR evaluation campaign [15]). Kek X l X inen et al. [11] used a four-points scale for relevance level : highly relevant, fairly relevant, mar-ginally relevant, not relevant . Each of these relevance level has to be expressed by a numerical value for computing measures. One of the remaining question is the choice of these values and the semantic they should have. Their work also proposed generalized non binary recall and precision , that are extensions of standard binary recall and precision taking into account multiple relevance levels [11]. The Discounted Cumulated Gain and the Cumulated Gain are also proposed by the same authors in [16]. We present them using our formalism in section 3.2. Sakai [17] also proposed a measure based on of the Cumulated Gain .

Our conceptions meet those of Kek X l X inen et al. [16] concerning the fact that multiple relevance levels should be taken into account when evaluating IRS. While information grows continuously, for the users lambda, one of the main issue for IRS will become to retrieve documents with highly relevance level at the top of the results list. We design metrics to allow the evaluation of this ability in IRS as collections size increase. Let C 1 and C 2 be two collections of different sizes such as C 1  X  C 2 and S an IRS. The aim is to analyze how S behaves on each collection to determine if its effectiveness improves, remains the same or decreases when the collection size increases. Our measures are based on the comparisons of the relevance levels of the first documents in the results lists for the two collections.
 3.1 Relevance Level Importance For a given topic, we assume that a relevance level is given to every document regarding this topic. Let { rel i } ,i =1 ,..., n be the set of possible relevance lev-els; two documents are equivalent if they have the same relevance level regarding this topic.

We define a total order relation on the set of the relevance levels noted : rel i rel j when i&gt;j . This total order relation gives the preference wished on retrieved documents but it gives no indication about the importance of a particular relevance level regarding the other relevance levels. However, it is the importance of a relevance level that characterizes the quality/quantity of information expected from a document of this relevance level. One may need to highly credit (resp penali ze) retrieval systems that return the documents with the highest relevance level at the top (resp not at the top) of their results list: in this case, the highest relevance level must have a high importance (compared to the importance of the other relevance levels) when evaluating retrieval results. On the other side, some applications need to retain many documents of good relevance levels, the difference between a document of good relevance level and a document of high relevance level is not important. Thus, a function I that models the importance of relevance levels depends of the types of applications the IRS is designed for and is characterized by the following properties (a positive and increasing function):  X  I ( rel i ) &gt; 0  X  I ( rel i ) &gt;I ( rel j ) if rel i rel j i.e. i&gt;j The choice of the number of relevance levels and the attribution of numerical values to relevance levels is still an open problem in IR. Kek X l X inen [18] used different empirical weighting schemes (see figure 1). Giving a numerical values of importance to relevance levels means nothing in the absolute; but in the relation with others relevance levels, these values can be associated to a semantics as we show it through the gain function (section 3.3).
 3.2 Cumulated Gain at a Given Rank The Cumulated Gain, CG as proposed by Kek X l X inen et al. [16], is computed at rank r by the sum of relevance levels of documents retrieved at any rank k  X  r : The Discounted Cumulated Gain(DCG) also computes relevance gains with a discount factor which is a decreasing function of the rank: the greater the rank, the smaller share of the document relevance level is added to the cumulated gain. This factor is needed to reduce progressively the impact of the gain of relevant information according to the rank (steep reduction with a function like the inverse of the rank disf ( k )=1 /k if the first documents are those we want to focalize on during the evaluation or less steeply with a function like the inverse of the log of the rank disf ( k )=1 /log b ( k ) as in [16]). By averaging over a set of queries, the average performance of a particular IR method can be analyzed. Averaged vectors have the same length as the individual ones and each component i gives the average of the i th component in the individual vectors. The averaged vectors can directly be visualized as gain-by-rank graphs. The actual CG and DCG vectors are also compared to the best theoretically possible. We described the building of the best theoretically results list in section 3.4, as we re-use it for our metrics. 3.3 Information Gain Between Two Relevance Levels For a given topic, in front of two documents with two different relevance levels, the same amount of relevant information is not expected from the two doc-uments. It is interesting to quantify the relevant information gained (or lost) when moving from a relevance level to another, that is a function of the rele-vance levels: Gain ( rel i ,rel j )= g ( rel i ,rel j ) , with these characteristics:  X  g ( rel i ,rel j ) &gt;g ( rel i ,rel k ) if rel j rel k i.e. if j&gt;k  X  g ( rel i ,rel j ) &lt;g ( rel k ,rel j ) if rel i rel k i.e. if i&gt;k  X  g ( rel i ,rel i )=0 . There is neither a gain nor a loss of information when one By deduction, we have: g ( rel i ,rel j ) &lt; 0 if rel i rel j i.e. if i&gt;j .
Indeed if we have rel i rel j , then this means that the quantity of relevant information contained in the document of relevance level rel i is higher than the quantity of relevant information contained in a document of relevance level rel j . Thus, when moving from a document of relevance level rel i toadocumentof relevance level rel j , one loses relevant information and so g ( rel i ,rel j ) &lt; 0 . In the same way, g ( rel i ,rel j ) &gt; 0 if rel j rel i i.e. if i&lt;j
It is obvious that the gain function between two relevance levels depends on the importance associated to each of the relevance levels.
 elled by the mathematical distance (we can build a distance between different relevant levels, using their associated numerical value of importance ). We respect all the properties of the function g . 3.4 Information Gain at a Rank When Collection Size Increases We assume in this study that the measures proposed will be used to evaluate the effectiveness of a system on a collection that grows (from a first collection C 1 to a second collection C 2 with C 1 the addition of new documents and becomes a collection C 2 , our assumption is that the effectiveness of a good retrieval system should at the worst case stay the same (from C 1 to C 2 ), whatever be (the relevance level of) the documents added. This effectiveness should not decrease, whatever be the documents added, as all the documents in C 2 were already in the collection C 1 . And when new relevant documents are added, a good retrieval system effectiveness should stay the same or increase from C 1 to C 2 .

For a given topic t , d t k ( C ) is the document retrieved at rank k when the collection C is queried using the topic t . the information gain at rank k between the results lists of both collections is computed using the gain function as follows: Move t k ( C 1 ,C 2 )= g ( RelLevel ( d t k ( C 1 )) ,RelLevel ( d t k ( C 2 )))
This Move expresses the relevant information gain (resp loss) at rank k when moving from C 1 results list for the topic t to C 2 results list for the topic t .We obtain a vector of weighted Moves by applying a discount factor &lt;disf (1)  X  Move t 1 ( C 1 ,C 2 ) , ..., disf ( N )  X  Move t N ( C 1 ,C 2 ) &gt; Measure Type 1. There are two possibilities for using these vectors:  X  For a given cut-off level N , either we sum the vectors X  elements topic by  X  either we sum the weighted Moves rank by rank for all the topics and we Thus, by querying an IRS on a set of collection { C i } such as C i  X  C i +1 ,weobtain information gains realized when collection size increases, and we can analyze the impact of collection size on the information gain. According to our assumptions, the measure Measure 1 t N ( C 1 ,C 2 ) should not be negative for a good retrieval system, as C 1  X  C 2 .
 Measure Type 2 . For a given collection C ,theIRS S provide a result list Retri -so-called ideal for this topic in the same way as [16].
 Example: Consider HR FR MR NR the relevance levels of [16] (see table 1 and a topic t with 7 documents HR, 10 documents FR, 20 documents MR. We choose N =30 .The ideal results list of size 30 for topic t is as follows: We can now build the weighted vectors of Moves between the results list for the collection C and the ideal results list: &lt;disf (1)  X  Move t 1 ( C, Ideal _ C ) , ..., disf ( N )  X  Move t N ( C, Ideal _ C ) .
As for the previous case, we have two possibilities of using these vectors for evaluation:  X  At the cut-off level N and for the topic t we compute :  X  we sum the weighted vectors elements rank by rank for all the topics and we In this work, we propose some metrics based on the notion of multigraded rel-evance levels for evaluating the way IRS scale. Their goal is to provide some information on the coherence between the ranking of documents retrieved by an IRS and the relevance levels of these documents as collection size increases. Some recent metrics in IR used a notion of relevance with multiple levels, e.g. the Discounted Cumulated Gain or the Cumulated Gain . For a given collection and an IRS, these metrics compute the relevant information gain obtained as one goes through the results list returned by an IRS on a given collection. Our met-rics compute the relevant information gain obtained when a single IRS is used on a collection which grows. Thus we evaluate the ability of the IRS to rank the documents according to their relevance levels when collection size grows. All the metrics that use multigraded relevance need to associate a numerical value to each relevance level and this is still not well studied in IR: in this study, we formalize the (obvious) constraints linked to the attribution of numerical values to relevance levels through the importance function and the gain function .
We are now working on the relation between our metrics and the existing metrics that used multigraded relevance levels through some experiments.
