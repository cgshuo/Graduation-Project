 This paper introduces a new method for estimating the local neigh-borhood and scale of data points to improve the robustness of spec-tral clustering algorithms. We employ a subset of empty region graphs  X  the  X  -skeleton  X  and non-linear diffusion to define a locally-adapted affinity matrix, which, as we demonstrate, provides higher quality clustering than conventional approaches based on k near-est neighbors or global scale parameters. Moreover, we show that the clustering quality is far less sensitive to the choice of other algorithm parameters, and to transformations such as geo-metric distortion and random perturbation. We summarize the re-sults of an empirical study that applies our method to a number of 2D synthetic data sets, consisting of clusters of arbitrary shape and scale, and to real multi-dimensional classification examples from benchmarks, including image segmentation.
 I.5.3 [ Clustering ]: Algorithms; H.2.8 [ Database applications ]: Data mining Spectral Clustering, Proximity Graphs
Clustering is at the core of modern data mining tools. Com-mon techniques, such as those based on K -means or explicit den-sity models, are being replaced by spectral methods for clustering, where points are clustered based on a spectral analysis of a matrix of pairwise similarities or affinities, instead of relying on a particu-lar cluster model.

Spectral clustering has been applied successfully in a number of fields, including image segmentation, text mining, and data analysis in general. However, there remain a number of open questions: (1) How to define the neighborhood around data points to estimate a  X  X ood X  affinity matrix, (2) how to adapt the algorithm to account for variations in local scale or density of the data, and (3) how to automatically select the number of clusters. This paper concerns the former two questions.

The most common approaches to date rely on simple neighbor-hood definitions, such as k -nearest neighbor (kNN) graphs or graphs, both which involve parameters that govern the graph den-sity. However, clustering results may change dramatically for dif-ferent values of k or  X  . In particular, when the neighborhood graph is too sparse, clusters break up into individual components that can-not be aggregated by spectral clustering, while the spectral method can be unreliable for identifying clusters in an overconnected, dense graph. Alternatively, one can simply connect all points in a fully connected graph and rely on a single scale parameter  X  to define the affinity between pairs of points in a weighted graph, which can be thought of as a fuzzy instance of  X  -graphs. In either case, the optimal choice of these parameters varies with the dimensionality and across data sets, and more importantly often within a data set, as the resulting selection of neighbors or affinities does not adapt to the local density or distribution of points.

In this paper, we exploit empty region graphs (ERGs) to con-struct neighborhoods without requiring a particular choice of the neighborhood extents. In contrast to kNN graphs, which prescribe the number of neighbors without regard to their relative locations, empty region graphs account for the spatial distribution of points to define neighborhoods of varying extent and cardinality, and do in general not suffer from missing or redundant connections. We show that these graphs generally improve the accuracy of spectral clustering algorithms. We also introduce a diffusion-based mecha-nism that estimates the density based on the average neighborhood size around a point to define affinities. We show that this local scal-ing algorithm, when combined with empty region neighborhoods, results in a better classification that is robust to noise and geometric transformations of the data points.

We present results on a number of synthetic benchmark data sets, as well as real multi-dimensional classification problems, including image segmentation.
Spectral Clustering . Spectral clustering is becoming a suc-cessful alternative to techniques based on K -means [21] or den-sity models [10], and dates back to Donath and Hoffman [8] and Fiedler [11]. Recently, spectral clustering has found a niche in im-age segmentation [28], text mining [7] and as a data mining tool in general [26, 17]. Since then, there has been a trend in improv-ing spectral clustering through a detailed analysis of the underlying graph structure [22], the scale and density parameters [34, 1], and the stability [15] and consistency [33] of the algorithm. Most re-lated to our work are the techniques that attempt to estimate the local scale or density to improve spectral clustering of data with varying densities, shapes and levels of noise. Among the first to address this problem for data mining were Zelnik-Manor and Per-ona [34], who improve the general algorithm by Ng et al. [26] with local scaling. This approach, although effective even in high di-mensions, was shown to be suboptimal for noisy data sets, or for data with clusters of different densities [25]. To alleviate this prob-lem, Nadler and Galun [25] introduce a coherence measure of a set of points belonging to the same cluster. Although not exclu-sive of spectral methods, the authors show that it alleviates some of the intrinsic limitations of spectral clustering. To deal with noise, Li et al. [19] propose a warping model that maps the data into a new space more suitable for clustering and more resilient to noise. Other methods are able to cluster data consisting of regions of arbi-trary shapes, such as density based clustering [27], and, in a similar spirit to Zelnik-Manor and Perona X  X  method, locally scaled density based clustering [1].

In this paper, we address the problem of locally-scaled and noise robust spectral clustering. We take a different approach and iden-tify the problem as early as the selection of the neighborhood graph. Maier et al. suggest that the construction of the graph has a measur-able effect on the results of spectral clustering [22]. Inspired by this paper, we turned to alternative neighborhood graphs, namely empty region graphs [3], in an effort to obtain better neighborhoods.
Empty Region Graphs . Neighborhood or proximity graphs cre-ate a geometric structure that connects two points if they are close in some sense. These graphs have been well studied and include the relative neighborhood graph [16], the Gabriel graph [14], skeletons [18],  X  -local graphs [2] and Delaunay triangulations [12]. A subset of these, called the empty region graphs, define a neigh-borhood graph where two points are connected if a geometric re-gion parameterized by those points does not contain any other point [3]. These graphs have been well studied in terms of their geomet-ric properties [6, 3], and have been applied in geographic analy-sis, pattern recognition and machine learning. Proximity graphs have been applied to clustering as well. Urquhart et al. [32] use the Gabriel graph and the Relative Neighbor graph to improve hierar-chical clustering, noting that these graphs result in natural clusters that can be separated depending on the local graph density [32]. Carreira and Zemel apply an ensemble of minimum spanning trees to form neighborhood graphs that are more resilient to noise and varying densities [4]. Choo et al. propose an agglomerate method for hierarchical clustering that merges candidate clusters that be-long to the same connected component in the Gabriel graph [5].
In this paper, we propose the use of the one-parameter  X  -skeleton empty region graph to construct locally-scaled affinity matrices that improve the accuracy of spectral clustering. We show that this ap-proach is more effective when combined with a diffusion step that enhances the block structure of the affinity matrix and, thus, the separability of clusters.
Our approach combines neighborhoods defined by empty region graphs with density estimation techniques and spectral clustering.
Spectral clustering refers to a general algorithm where data are clustered into groups based on spectral analysis of a matrix of pair-wise affinities or similarities between data points. The intuition is that, based on a similarity graph between points, a good clustering should partition the graph such that points in the same group are similar and points in different groups are dissimilar to each other. The spectral properties of the graph Laplacian helps us partition the graph in that manner, based on one of the properties of the graph Laplacian, which states the graph Laplacian has eigenvalue 0 with multiplicity equal to the number of connected components of the affinity matrix.

A general algorithm for spectral clustering can be implemented as follows [26]: Given a set of n points S = { s 1 , s 2 to be partitioned into K clusters,
Clearly, the accuracy of the clustering depends, among other fac-tors, on the graph density k and the scale parameter  X  . Fig. 1 shows how k = 3and k = 7 nearest neighbor graphs with a fixed scale parameter are used to cluster groups of 2D points, two of which define small dense clusters, while the third forms a sparse back-ground. When k is small (Fig. 1(a)), important connections are missing within the background cluster, which is separated into two components. Increasing k helps connect the background cluster (Fig. 1(b)), but adds many redundant edges between clusters that diffuse them and cause the algorithm to misclassify background points. Varying k and  X  together may improve the results, though when the point density varies significantly, it may be that no com-bination of k and  X  yields the correct clustering.

To deal with disparate densities, Zelnik-Manor and Perona define a more general affinity that incorporates local scaling [34]. Instead of a single scale parameter, they define the affinity between two points as: where  X  i and  X  j are the local scale parameters estimated for points s and s j , respectively. In the original paper, this parameter is de-fined as  X  i = d ( s i , s ( i ) J ) ,where s ( i ) J is the J tice, it was found that a single setting, J = 7, gave acceptable re-sults.

Although local scaling tends to improve the clustering, we found that the quality of the results using this approach still depends on finding the right combination of k and J , and that these choices are dependent on the dimensionality of the domain. Furthermore, as we will discuss in Section 5, a single value of J may not correctly cluster data in the presence of noise or under nonlinear geometric transformations. Figure 1: Proximity graphs and clusters for kNN and the  X 
In addition to graphs based solely on absolute or relative dis-tances between points, a number of alternative proximity graphs have been proposed, such as the relative neighbor graph (RNG) and the Gabriel graph (GG), as surveyed by Jaromczyk et al. [16]. A family of these, known collectively as the empty region graphs , are more representative of the neighborhood of a point and less redundant than kNN, and are more efficient to compute than sim-plicial tessellations such as the Delaunay triangulation, particularly in high dimensions.

Definition 1. Agraph G ( S , R )=( S , E ) is an empty region graph if for every edge ( p , q )  X  E , a canonical region R ( p not contain any other point in S : where R defines the neighborhood and is called the empty region . Some common ERGs are: Nearest Neighbor Graph (NNG) . This is the directed graph that results from the empty region R ( p , q ) formed by the open d -ball centered on p with radius d ( p , q ) . Relative Neighborhood Graph (RNG) . This graph is defined by a lune-shaped region consisting of the intersection of two d -balls of radius d ( p , q ) , one centered on p and the other centered on q , i.e., Gabriel Graph (GG) . This is the graph defined by a d -ball cen-tered at 1 2 ( p + q ) with diameter d ( p , q ) , i.e.,  X  -Skeleton . The so-called lune-based  X  -skeleton is a one-parameter generalization of the RNG and GG, defined as follows: It follows that  X  = 2 gives the RNG, while  X  = 1 is the GG. Fig. 2 depicts the geometric regions associated with different values of  X  . Finally, we note that geometric inclusion of one region within another also implies a partial order of the resulting neighborhood graphs (in terms of their edges), so that: This observation is key, since it allows us to explore neighborhood graphs of varying density using a single parameter,  X  , without the problems associated with kNN or  X  -graphs. Section 5 provides em-pirical results that suggest that the clustering is far more stable for different values of  X  than they are for variations in k and
We now motivate the use of empty region graphs for representing the neighborhood around a point, and consequently, for shaping the affinity matrix, and describe a general method for its application in spectral clustering.
As a first step, we construct the  X  -skeleton of the data points for a given value  X   X  ( 0 , 2 ] . As described above,  X  -skeletons parame-terize the neighborhood graph of a collection of points in a different Figure 3: Two proximity graphs and their corresponding affin-ity matrices (a) 6NN, (b) 1-Skeleton, or Gabriel graph (GG). The bridging node between clusters destroys the block struc-ture of the affinity matrix. While this is pronounced for the 6NN, it only affects two nodes (B and C) for the GG. way than kNN does. In this paper we show that those graphs lead to better clustering than kNN graphs.

Graphs such as the k -nearest neighbor graph are susceptible to short circuiting nearby clusters when extra points lie between the clusters. As shown in Fig. 3(a), separating the two clusters is diffi-cult due to the connecting node. In fact, the corresponding affinity matrix is formed by two blocks that overlap, and a large number of off-diagonal elements.
 A  X  -skeleton alleviates this problem, as illustrated in Fig. 3(b). In this case, the connecting node (which may be due to noise or the presence of a smaller cluster) is joined to each cluster via a single edge. In the resulting affinity matrix, there are three blocks, but the off-diagonal elements are confined to the vicinity of point A .
Nowweshowthat  X  -skeletons provide better estimates than kNN of the local scale of a point. This local value defines the scale parameter for the computation of the affinity matrix. Using local scaling allows a pair of points within a high-density cluster to be assigned the same affinity as a pair of points in a low-density clus-ter when their separation in relation to the local scale is the same.
A natural measure of the local scale around a point include func-tions of the distance to its neighbors, such as the mean or the me-dian distance. These measures can be brittle in k NN graphs, as suggested by Zelnik and Perona [34], and as studied by Maier et al. [22]. Picking the distance to an arbitrary nearest neighbor may prove more effective, but is sensitive to density changes and noise.
Here, we provide an initial estimate of the local scale using the mean or the median distance to a point X  X  neighbors in an empty re-gion graph. These measures make sense for these graphs since they approximate locally the spatial extents associated with each point. To illustrate this, consider the points in Fig. 4(a), connected in a 6NN graph. The dashed circle associated with each point has a ra-dius equal to the average distance of the point to its neighbors, and can be understood as a representation of the local scale. Naturally, over-connecting the points results in artificially large local scales. The larger the local scale, the higher the affinity is of a point with a neighbor enclosed in its respective dashed circle. In Fig. 4(a), there is a high affinity between the points near the boundary of both clus-ters. The 3NN graph, in contrast, produces smaller local scales and clearly separates the two clusters (Fig. 4(b)). However, finding (c) 1-skeleton (no diffusion) (d) 1-skeleton (diffusion) Figure 4: Average distance to neighbors as a measure of local scale. (a) Poor scale estimate due to an overconnected graph. (b) Better scale estimate using a sparser graph. (c) GG further improves the local scale, except for cluster boundary nodes. (d) Diffusion corrects the scale of boundary nodes. the appropriate k parameter for a kNN graph proves difficult. In contrast, the mean distance to the neighbors in the 1-Skeleton is a good estimator of the local scales required to cluster the points ac-curately, as shown in Fig. 4(c). Note that the local scales for each cluster is roughly the same as in the 3NN, except for the extremes of the edge connecting the two clusters, where the local scale is larger than expected, and one runs the risk of clustering them together.
This behavior, the estimation of larger scales for nodes connected via intercluster edges, is the result of using a  X  -skeleton, which is well connected . One might consider the use of the median distance to exclude outliers and solve the problem, but this technique may fail when the neighborhood graph is sparse A more fundamental problem are small  X  X liques X  of a few close points that are embed-ded in a larger cluster. Their median neighbor distance may be far smaller than the scale suggested by the surrounding density, result-ing in artificially small local scales and poor affinity with the rest of the cluster. To avoid this, one must analyze the local scale by look-ing not only at the immediate neighbors of a point, but at a possibly larger set.

To address this, we introduce a propagation mechanism based on non-linear diffusion, which improves the estimate of the local scale of a point by querying the scale of its neighbors, similar to reachability in density based clustering [27].
To deal with the local scale of boundary points, one must en-sure that a boundary point has a local scale so that points outside the cluster exhibit less affinity than those within the same cluster. In turn, those neighbors in the same cluster should exhibit affinity with their neighbors, and so on. Thus, the local scale of a point is affected by points that may not be immediate neighbors.

To determine the local scale, we use non-linear diffusion, such that the local density of a point (inverse of the local scale) is it-eratively blended with the local densities of its neighboring points. Because shorter edges are likely to correspond to points in the same cluster, we use inverse distance weighted kernels, such as Gaussian or inverse polynomials.

The method works as follows: we start from an estimate of the local scale  X  0 i of a point s i as the mean or median distance to its neighbors in the neighborhood graph. We iteratively refine the local scale for some iterations t  X  X  1 ,..., T } , where N ( s i ) is the neighborhood of point s i (using a and the weights are normalized kernels, defined as the product of two Gaussians G ( d ,  X  )= exp (  X  d 2 /  X  ) , The first kernel blends the scales of nearby points, and ensures that intracluster scales are made more uniform than intercluster scales. The parameter  X  D , called diffusivity , controls the speed at which diffusion propagates the local scales in terms of the distance be-tween points. Diffusivity alone, however, makes the scales con-verge to a uniform value for the entire graph. We then introduce an additional kernel that penalizes the weight when the difference in scale is high, similar to bilateral filtering in image denoising [30]. The parameter  X  C ,or conductivity , controls how fast the diffusion propagates along scale discontinuities.

Note that we diffuse the local density instead of the local scales, similar to equivalent kernel density estimators [9], where the recip-rocal of the local scale approximates the density of a point. We found this approach more accurate than blending the local scales.
Our approach, although designed to eliminate the selection of a global parameter k for the number of neighbors or global scale, requires the selection of different parameters, namely  X  , which controls the density of the neighborhood graph, and the diffusion parameters, T ,  X  D and  X  C . Unlike the exploration of k and  X  , we have found that parameterizing the graph density via  X  allows for more resilience to changes in point density, e.g. by limiting the number of intercluster edges. On the other hand, the diffusion parameters are inter-dependent. A large  X  D may result in convergence to the same solution as a small  X  D with only a fraction of the number of iterations T . Moreover, these diffusion parameters are similar to those used in kernel density estimation and diffusion in general, and have been well studied [20].

To illustrate the sensitivity of our algorithm to these parame-ters, we conducted an experiment to find the pair of parameters that yields the best clustering of the data set depicted in Fig. 5(a). We compared five scenarios: (1) Global scaling [26], defined in terms of number of neighbors k and global scale  X  , (2) local scaling [34], in terms of number of neighbors k and local scale neighbor J ,(3) our local scaling using graphs of different density via the parameter  X  , (4) our diffusion approach for a fixed  X  = 1and  X  C = 1, in terms of diffusivity  X  D and number of iterations T , and (5) our non-linear diffusion approach, in terms of the diffusivity and conductivity pa-rameters, while keeping a constant number of iterations T Fig. 5 shows the results for this experiment, depicted as surfaces of the normalized mutual information (NMI) X  X igher is better X  X or a discretization of their respective parameters. Global scaling is, not surprisingly, the most sensitive approach, and the optimal clus-tering is only achieved within a narrow interval of values. Local scaling in this case exhibits optimality for a fixed interval of the lo-cal scale. However, overshooting this interval produces drastically sub-optimal results. In contrast, exploring the graph density us-ing  X  instead of k exhibits little sensitivity for  X  &gt; can be said about the diffusion parameters. Typically, we set (e) Diffusion,(  X  C = 1 ,  X  = 1 ) (f) Conductivity ( T = 10 Figure 5: Sensitivity of clustering to various sets of parameters. and  X  C to 1. We believe bandwidth selection algorithms for ker-nel density estimation can be used towards improving these initial estimates [9].
Finally, once we determine the local scale associated with each point, we construct the affinity matrix as where R (  X  ) is an empty region template, parameterized by gorithm 1 summarizes the full algorithm.
We now describe the computational cost of the key steps in our algorithm.

Construction of ERG. Constructing an ERG can be expen-sive. A brute-force implementation requires O ( n 3 ) is prohibitively expensive for most practical applications. How-
Algorithm 1: New algorithm for spectral clustering using empty region graphs. input : Data points S = { s 1 ,..., s n } , number of clusters K , output : Classification C
G  X  ERG ( S ) Construct ERG for i  X  1 to n do for t  X  1 to T do A  X  A FFINITY ( { s i } , {  X  i } ) Compute affinity matrix ever, there are known algorithms that compute the RNG and GG in O ( n 2 ) time [31, 24].

We further reduce the computation cost of obtaining an ERG by restricting the neighbor search to the k max nearest neighbors of a point, where k max is usually a constant factor larger than the k value selected for k nearest neighbor graphs, but k max n . The overall complexity of computing such a kNN graph is and the additional cost of computing an ERG becomes
Diffusion. The diffusion process involves a sequential walk on the neighborhood graph and requires O ( | E | T ) time, where T is the number of iterations and | E | X  nk max is the number of edges in the graph.

Solution of Eigenvector Problem. In general, computing the eigenvectors of the Laplacian matrix takes O ( n 3 ) time, but, as de-scribed by Song et al. [29], sparse eigensolvers, such as the variants of Lanczos/Arnoldi factorization (e.g., ARPACK ), have a cost of O ( m 3 )+( O ( nm )+ O ( nk max )+ O ( m  X  K ))  X  (# restarted Arnoldi) where m &gt; K is the Arnoldi length used to compute the first K eigenvectors of the affinity matrix.

Clustering the spectrum. Finally, using K -means to cluster the eigenvectors has a cost of O ( nK )  X  (# K -means iterations).
Overall, as suggested by Song et al. [29], as the data size grows larger, the cost of the clustering becomes dependent on the con-struction of the affinity matrix. Nonetheless, the additional cost of computing an ERG instead of a kNN is relatively small. According to our results, this marginal increase is well worth the benefits of using the  X  -skeleton instead of k -nearest neighbors.
We have validated our algorithm with a number of low dimen-sional synthetic data sets and a few (higher-dimensional) real clas-sification problems. To compare the quality of these datasets, we measure the normalized mutual information (NMI) between the clustering X and the ground truth classification Y : where I ( X ; Y ) is the mutual information between X and Y and H and H ( Y ) are their entropies, respectively.
To illustrate the benefits of using the  X  -skeleton, we analyzed the results of our algorithm compared to traditional approaches us-ing k -Nearest Neighbors (kNN) when applied to transformations of a synthetic data set. We compare three types of transformations: geometric distortion (shear transformation), decimation (where we remove 0 . 05 Dn points, for a decimation factor D  X  X  1 ,..., noise (where we perturb the data with Gaussian noise of increasing standard deviation). Fig. 6(a) compares the result of clustering a 2D data set consisting of three concentric rings under these three transformations. Fig. 6(b) shows quality surfaces for the different transformations and different values of the main parameters for our method (  X  and T ) vs. the number of neighbors J used to define the local scale in kNN-based methods (as a reference, Zelnik and Per-ona X  X  method uses J = 7). The quality of the clustering increases as the color of the surface approaches yellow. Notice how the optimal value for kNN methods (bottom) varies depending on the degree of distortion, decimation or noise. For our method, the quality con-verges as we increase the number of iterations and is less sensitive to the transformation itself.
 A similar analysis was performed for other data sets, as shown in Fig. 7. Here, we show the mean, minimum and maximum quality for different values of  X  using our approach (blue), and for differ-ent values of k (global scale X  X reen) and J (local scale X  X ed) for kNN methods. In general, we observed a higher quality for empty region graphs and a lower variance when compared to globally-scaled clustering. Particularly for the first two rows, our approach is also better than locally-scaled clustering based on kNN. Note an important exception, the fourth row, consisting of small clus-ters surrounded by a random noisy background.  X  -skeletons do not cluster data as well on this data set, and in other similar cases. Locally-scaled kNN based methods are able to segment these be-cause a small number of neighbors is able to keep them discon-nected, while ERGs are always well-connected.
We validated our approach on multi-dimensional data sets from synthetic generators and the UCI Machine Learning repository [13]. The results are summarized in Table 1. We compare our approach using the  X  -skeleton (the parameter  X  chosen and number of iter-ations are reported next to each, for  X  D = 0 . 1and  X  C conventional clustering using kNN and (1) global scaling, (2) local scaling [34], and (3) DBSCAN, a density-based approach [27]. In all these cases, we report the best results obtained after exhaustively exploring their key parameters ( k and  X  for global-kNN and DB-SCAN, k for local-kNN and  X  and T for ours). As seen in the table, it was possible to find a suitable neighborhood graph using the parameter that produces better results than exhaustive search of the parameters of other conventional algorithms. Although the density of the graph increases with the number of dimensions, the explo-ration of the  X  parameter allows us to get the necessary density to cluster the higher-dimensional data sets. Similar to the results in Fig. 7, we also observed that quality is less sensitive to values of K . NMI=0.944 NMI=0.761 NMI=0.987 NMI=0.752
NMI=1.000 NMI=0.766 local scale given by J in [34].
Finally, we applied our method to image segmentation, where each pixel is defines a five dimensional point of its pixel coordinates x and y , and the color intensities in the L  X  u  X  v  X  color space. In all these results, the number of clusters is picked manually. Fig. 8 shows three examples from the Berkeley Segmentation Dataset and Benchmark [23]. Note how ERG based methods produce visually better clusters, while methods such as local scaling result in over-segmentation. Notably, Fig. 8(c) demonstrates the importance of good neighborhood graphs for local scaling. Attempting to separate this image into two segments proves difficult with a global scale using kNN, and users must resort to over-segmentation (4 clusters instead of 2). In our case (leftmost panels), the Gabriel graph is able to segment the airplane using both 2 and 4 clusters.
A fundamental problem in clustering is defining a neighborhood graph that defines how similar two data points are. This paper in-troduces a more natural parameterization of the neighborhood den-sity based on the  X  -skeleton. We showed that using  X  to define the affinity between points provides higher quality than the preva-lent approach of picking k nearest neighbors or finding a global scale parameter. We have also shown that our clustering results are far less sensitive to the choice of  X  and the diffusion parameters (diffusivity and conductivity), and to data transformations such as perturbations and geometric distortion.

Our diffusion-based local scaling approach proves effective for clustering irregular data sets and estimating the correct local scale at each point, despite the fact that the empty region graphs we ex-plore (supersets of the relative neighbor graph), are well-connected. We must point out that we introduce additional parameters to tune the diffusion mechanism, but these are well known in other do-mains where diffusion is applied, and there are methods for choos-ing them. From a practical standpoint, one can define heuristics to explore the parameter space efficiently. As a rule of thumb, one defines the diffusivity parameter as the smallest scale one wants to preserve in the data set, if that information is known, for example, minimum NMI while varying  X   X  [ 0 . 8 , 2 . 0 ] (blue), k when interested in the presence (or absence) of data features of a given size (e.g., in medical image segmentation).

In our experiments, we often apply the 1-Skeleton as an initial candidate for generating the affinity matrix. Over-segmentation may be an indication that the graph is too sparse and one might explore other graphs with  X  &lt; 1. Conversely, when one suspects that the result is under-segmented, one may explore sparse graphs, with 1 &lt;  X   X  2. Our approach can be extended in a number of ways to retrieve the number of clusters automatically, as suggested by approaches like [34, 1]. We believe our approach, proving to be resilient to noise and other types of data perturbations, is a step forward towards robust spectral clustering, and may have broader applications where a neighborhood graph is required.
 Prepared by LLNL under Contract DE-AC52-07NA27344.
 LLNL-CONF-513768. , T ) Global (kNN) Local (kNN) DBSCAN
