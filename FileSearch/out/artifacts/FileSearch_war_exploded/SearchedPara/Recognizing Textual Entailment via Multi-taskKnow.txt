 For the natural language, a common phenomenon is that there exist a lot of ways to express the same or similar meaning. To discover such different expressions, the Recognizing Textual Entailment (RTE) task is proposed to judge whether the meaning of one text (denoted as H) can be inferred (entailed) from the other one (T)[3]. For many natural language processing applications like question answering, information retrieval which need to deal with the diversity of natural language, recognizing textual entailments is a critical step.
 features and lexical similarity. Among them, [8] break the T -H pair apart into discourse commitments and then the RTE task is reduced to the identification of the commitments from T which are most likely to support the inference of the commitments from H . However, the discourse commitments are still derived from the original text and can not present the implicit meaning latent behind the text. [18] proposed a probabilistic inference framework which transfer the discourse commitments of T -H pairs into predicate-argument structure and use distant supervision as well as Markov Logic Network (MLN) to infer the correctness of H given T . However, the performance of MLN is limited by data sparsity problem, namely, the MLN X  X  inference rules can only cover a small portion of predicates. [17] proposed an attentive LSTM method, which use word-by-word attention to model the entailment between words. However, it did not use the KB information to help the model to infer the truth.
 task Knowledge Assisted LSTM (MKAL) for recognizing textual entailment, which use an embedded knowledge base (KB) to help the deep neural network for implicit reasoning. We use PTransE [10], a state-of-the-art KB embedding system which can model the inference rules in KB, to calculate the embeddings of each entities and relations in the KB. We transfer the sentences into predicate-argument triples as [18] did using a convolutional neural network. And then, we input the predicates X  embeddings into LSTM for implicit inference. Moreover, we add predicate-to-predicate attention into our model to detect the inference relationship between predicates. In addition, we take relation classification task as an auxiliary task to facilitate our RTE task.
  X  We propose a deep neural network architecture called Multi-task Knowledge  X  We propose an easy-generalized KB-assistant method, which can make full  X  We apply predicate-to-predicate attention to detect the inference relation- X  We propose a multi-task architecture, which use relation classification task Textual Entailment Recognizing (RTE) task has been widely studied by many previous work. Firstly, the methods use statistical classifiers which leverage a wide variety of features, including hand-engineered features derived from com-plex NLP pipelines and similarity between sentences ( T and H ) and sentence are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sen-tences.
 commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H . Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize.
 [17] uses the attention-based technique to improve the performance of LSTM-based recurrent neural network. However, they did not take advantage of the knowledge base information for inference, so that they cannot handle the cases where the facts in H are implicitly contained by T (the facts in H cannot be directly got from the facts in T . Instead, they should be inferred by the facts in T ). Fig 2 shows the architecture of our model. The input is the T -H sentences and the pre-identified entities. The entitiy X  X  semantic embedding is just the word embed-ding of its head word. The relation X  X  semantic embedding is calculated by convo-lutional neural network (CNN). For conducting implicit reasoning, KB inference information was brought into our model, namely, the entity X  X /relation X  X  semantic embeddings were concatenated with their corresponding PTransE embeddings (which contains the KB information). After that, predicate-by-predicate atten-tion based LSTM is applied to generate the representation of T -H pair. Finally, a logistic regression classifier is used to judge the entailment class ( Entailment , Neutral , Contradiction ) of the T -H pair. In addition, we use an auxiliary task to facilitate our main task (RTE). 3.1 Relation Embedding Calculation In order to better represent the implicit information of the sentence, we decide to transfer the sentence into a series of predicate-argument triples  X (entity A, relation, entity B) X  as [18] did. We decide to calculate the relation embedding instead of extracting the relation string directly for the following two reasons: (1) The relation between two entities can be easily figured out if the origin sentence and the two entities are given; (2) The process of extracting relation string may suffer from great information loss and lacks generalization.
 in Fig 1, in the Window Processing component, each token is further represented as Word Features (WF)(which is composed of the embeddings of each word) and Position Features (PF)(of the two entities). Then, the vector goes through a convolutional component. Finally, we obtain the relation embedding through a non-linear transformation.
 word of the two entities. For example, in the T sentence in Fig 2, the relative distances of  X  X arried X  to  X  X enna X  and  X  X octor X  are 2 and  X  3, respectively. And then, the relative distances are mapped to a vector of dimension d p (a hyperparameter) which is randomly initialized. Then we obtain the distance vectors d 1 and d 2 with respect to the relative distance of the current word to the where X  X  R n 0  X  t is the output of the window processing, n 0 = w  X  n , w is the window size, n is the dimension of feature vector, and t is the token number of the input sentence. W 1  X  R n 1  X  n 0 , where n 1 is the size of max-pooling layer. To determine the most useful feature in each dimension of the feature vectors, we perform max-pooling over time on the convolution result Z  X  R n 1  X  t to get the max-pooling layer P . Finally, the relation embedding is calculated as Eq 2. embedding. 3.2 KB Reasoning In many RTE cases, the facts in H cannot be directly got from the facts in T . Instead, they should be inferred by the facts in T . For example, as is shown in the T -H pair of Fig 2, the fact in H should be inferred from the facts in T . So we need inference rules derived by KB to help us conduct reasoning. form as modeled by PTransE [10], which is a KB embedding method modeling relation paths like r 1 + r 2 = r 3 . Therefore, the representations of entities and relations learned by PTransE contain the information of implicit inference rules. So we use PTransE to model the inference rules in KB. We take the entities X  and relations X  PTransE embeddings as well as the semantic embeddings together as the input of the LSTM-RNN to conduct inference as is shown in Fig 2. and relations.
 the entity can be found in KB, we will use the corresponding PTransE embedding as this entity X  X  KB information. If the entity cannot be found in KB, then KB cannot bring it any information, so we just set its X  KB information as zero. perfect textual relation is too hard to extract), but we do hope to find a match for the textual relations from KB relations. So we use a syntactic constraint method [5] to extract raw relation string (RRS) from two adjacent entities. The syntactic constraint requires relation phrase to match the POS tag pattern shown in Figure 3. The pattern limits relation phrases to be either a simple verb phrase (e.g., invented), a verb phrase followed immediately by a preposition or particle (e.g., located in), or a verb phrase followed by a simple noun phrase and ending in a preposition or particle (e.g., has atomic weight of). Different from [5], we also allow the case of only one preposition (e.g. from, at), because this kind of relation usually states for location and affiliation.
 semantic embedding in prior by averaging the embedding of each word. We choose the KB relation which has the minimal cosine distance to the extracted RRS as its corresponding KB relation.
 and then use MLN to conduct inference, our method can cover all entities and relations in KB, which may alleviate the data sparsity problem.
 3.3 LSTM-based Attentive Neural Reasoner LSTM Long short-term memory (LSTM) based recurrent neural networks (RNNs) have long been tried to apply to a wide range of NLP tasks. includ-ing RTE [2]. LSTMs use memory cells to store information for a long period. In our model, the better LSTM remembers the predicates, the more accurate recog-nizing result it can give. Given an input gate i t , a forget gate f t , an output gate o , a memory cell c t , candidate memory cell state e C t and a hidden state h t . The LSTM transition equations are listed in Eq 3,  X  is element-wise multiplication. Attentive LSTM for Predicate Reasoning As is shown in Fig 2, we take the combination of semantic embedding and PTransE embedding of predicate-argument triples as input to LSTM.
 to-predicate attention in our model, which can model the inference relationship between the predicates in T and H . Following [17], we attend over the first LSTM X  X  output vectors of T , while the second LSTM processes H one predicate at a time. Denote Y  X  R d  X  L as a matrix consisting of the T sentence X  X  LSTM output vector [ h 1  X  X  X  h L ], this can be modeled as follows: where M t  X  R d  X  L , t  X  R d is the attention weight vector over all output vectors of T for every predicate x t with t  X  ( L + 1 ; N ) in H and r t  X  R d is dependent on the previous attention representation r t  X  1 to inform the model about what was attended over in the previous step. The outer product  X  is to repeat the previous operand as many times as the word number in T ( L times).
 linear combination of the last attention-weighted representation r N and the last LSTM output vector h N . where h  X   X  R d . Then h  X  is fed into a classifier: where O  X  R 3 is the final output of this T -H pair, each entry contains the score of an entailment class (entailment, neutral, contradiction). 3.4 Multi-task Training for Relation Representation Multi-task learning (MTL) is a kind of machine learning approach, which trains both the main task and auxiliary tasks simultaneously with a shared represen-tation learning the commonality among the tasks. In our work, we use auxiliary training to compensate the lack of supervision in the main task. Intuitively, the CNN which is used to calculate relation embeddings can be improved by rela-tion classification task (RC). Therefore, we use the relation classification task to assist our main task (RTE).
 relation semantic embeddings calculated by CNN as well as PTransE embeddings were used for LSTM, the relation semantic embedding were also taken as the feature vector of the RC task as shown in Eq 7. Given an input example x , the component is calculated by softmax operation.
 where a = { W 3 } represents for the RC-task-only parameters, s = { W 1 ; W 2 } relation classification, the loss function of this auxiliary task is shown as Eq 8. 3.5 Model Training We define the ground-truth label vector y for each T -H pair as a binary vector. dimensions are set to 0. In our model, the RTE task is classification problem and we adopt cross entropy loss as the objective function. Given the LSTM neural network parameters RT E , the objective function for a T -H pair can be written as, We use mini-batch AdaDelta [24] to train the parameters RT E ; s ; a . Referring to the training procedure in [11], we select one task in each epoch and update the model according to its task-specific objective ( J or J a ).
 time. Therefore, we assign different regulative ratio and a to different tasks to adjust the learning rate of AdaDelta. 4.1 Datasets and Model Configuration We conduct experiments on the Stanford Natural Language Inference corpus (SNLI) [2]. Since our main task is RTE, Table 1 summarizes the statistics of the three entailment classes in SNLI.
 [7], which contains 10717 annotated examples, including 8000 training instances and 2717 test instances.
 extracting each NP node in the sentence X  X  dependency parse tree. We use the pre-trained word embeddings provided by GloVe [15], and the dimension of the embeddings d is 50. The hyperparameters of our model are set as in Table 2. 4.2 Overall Performance  X  LSTM[2] is a LSTM-based method which encodes T and H independently.  X  Classifier[2] is a feature-engineered classifier which use a large set of elab- X  Attention[17] is a LSTM method which processes H sentence conditioned split our result into four parts. MKAL(CNN+LSTM) is the most basic model, which only use CNN to calculate the relation embedding and then input all the triples X (entity A, relation, entity B) embeddings into LSTM encoder. Finally, the output of LSTM is taken as the feature representation of the T -H pair. We can see that MKAL(CNN+LSTM) has achieved an accuracy of 78.9%, which cannot outperform the state-of-art result of [17]. One possible reason is that the process of using CNN to extract the relations X  representation may lead to severe informa-tion loss. After we add an auxiliary task (RC task), the multi-task architecture makes the RTE accuracy improved to 82.5%, which is a great improvement. Then we add PTransE embedding (+ KB) to bring inference information to our model, which improves the accuracy to 83.8%. Finally, the attention technique (+ Attention) has improved the accuracy to 84.2%, which has outperformed the state-of-the-art results (Wilcoxon signed-rank test, p &lt; 0 : 05). 4.3 Effect of Multi-Task auxiliary task. The relation classification task X  X  performance is shown in Table 4. Although our auxiliary task did not outperform the origin CNN model in [25], the auxiliary task is comparable to the origin CNN model. The reasons why we did not achieve a better result in the auxiliary task are as follows: First, we did not use the lexical features described in [25] for simplicity. Second, the supervise information of RTE may be not enough for the relation classification task. 4.4 Effect of KB Of all the extracted RRS in the SNLI test set, about 19.6% can be matched with KB relations in YAGO. For the entities in the SNLI test set, about 5.4% can be matched with YAGO X  X  entities. Although the ratio of matched entities/relations is not very high, the KB information is still effective to the overall performance (1.3 percentage point according to Table 3). Some examples of matched entities and relations are shown in Table 5.
 4.5 Effect of Attention cate in H gains another 0.4 percentage point improvement. We argue that this is due to the model being able to check for the entailment between the predicates in T and H .
 original sentence of Fig 6 X  X  left figure is as follows: According to the figure, the predicate  X  X ingto (church choir,mass) X ,  X  X ing (mass,song) X  and  X  X t (book,church) X  are important for entailing the H predicate  X  X illwith (church,song) X , which is reasonable for human. The original sentence of Fig 6 X  X  right figure in Fig 6 is the example in Fig 2. In this attention matrix, we can see that all of the three predicates in T are important to entail the H  X  X  predicate. pared to other works [17]. One possible reason is, the size of our attention matrix is much smaller than other works since we only use predicate-by-predicate at-tention. There may not be too much predicates in a sentence, but a sentence usually contains many words. In our model, we define each two adjacent enti-ties and their relation make a predicate. Statistically, in Fig 5, we illustrate the distribution of the number of entities in one sentence. We can see that most of the T sentences have less than 10 entities, which means less than 9 predicates; most of H sentences have less than 5 entities, which means less than 4 pred-icates. Smaller size may weaken the role of attention. For example, in Fig 6 X  X  right figure, all of the three predicates are attended, so that it may not make too much difference than there is no attention technique at all. Therefore the effect of attention matrix is weaker than the effect in previous works.
 In this paper, we use a multi-task architecture to recognize textual entailment. We use relation classification task to facilitate the recognizing textual entail-ment (RTE) task (main task). The main task use CNN to calculate the relation-s X  semantic embeddings and concatenate the adjacent entities X  embedding and their relation X  X  embedding as the predicate X  X  semantic embedding. At the same time, we give each entity and relation a PTransE embedding (KB information), and put all of them into LSTM-based recurrent neural network so that implicit reasoning can be conducted. With the help of predicate-by-predicate attention technique, we detect the entailment between predicates. The experiments show that the KB information, the attention technique, especially the multi-task ar-chitecture are effective to the RTE task.

