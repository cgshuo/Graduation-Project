 Recently, with the explosion of social networks, microblogs, instant mes-sages, Question &amp; Answer forums, the number of texts that users generate is extremely massive. For example, according to the statistics from http://www. internetlivestats.com/twitter-statistics/ , the number of tweets posted per day is about 500 millions. With the increase in the popularization of internet, this number will be even bigger in the future. These sources of information are really attractive to data scientists because analyzing them would provide a lot of insights into users. From this, companies can come up with many strategies in doing business. Unfortunately, unlike formal or official documents (academic papers, news articles, etc.), data from these sources are often short texts and most are origi-nated from social sources. Those kinds of data feature three challenges:  X  Short : Short texts have extremely short length. In fact, in some popular social networks such as Twitter, users are restricted to write a status (tweet) with no more than 140 characters.  X  Massive : The number of short texts is big and increases tremendously. For example, the number of tweets generated per day is more than 500 milions.  X  Dynamic : The topics of short texts are highly dynamic and reflect the trends of the society. A topic might emerge and disappear over time.
 The limited length of short texts poses various difficulties which have been Dirichlet Allocation (LDA) [ 3 ] and Hierarchical Dirichlet Processes (HDP) [ 15 ] occurrence of terms. However, a short text often contains few co-occurrences and does not provide a clear context. In statistical perspectives [ 14 ], we can never recover/learn correctly a model from very short texts even though we may have arbitrarily large collections.
 In this paper, we show a very simple approach to dealing with short texts. Instead of using bag-of-words to represent documents, we propose to use both terms (words) and biterms which lead to bag-of-biterms (BoB). By this way, BoB brings us many significant benefits: (1) BoB naturally lengthens documents and thus helps us reduce bad effects of shortness. Therefore, a statistical model might be recovered better [ 14 ]. (2) BoB enables the posterior inference in a large class of probabilistic models including HDP to be less intractable [ 16 ]. This suggests that BoB helps learn a model better. (3) No modification of existing models/methods is necessary, and thus BoB can be easily employed in a wide class of statistical models.
 To evaluate the benefits of BoB, we take HDP into account as it is a popular model and is the base for many other models. HDP [ 15 ] is a nonparametric model which can automatically grow as more data come in. Therefore, HDP can deal with the dynamic of short texts. In combination with an online learning algorithm in [ 17 ], HDP can deal with two challenges (massive and dynamic) mentioned above. Data for evaluation are three big short text collections which have been crawled from Twitter, Yahoo Q&amp;A, and New York Times.
 From extensive experiments we find that in most cases BoB helps Online HDP [ 17 ] work significantly better than bag-of-words. Both predictiveness and quality of the learned models are significantly improved. We also find that BoB helps Online HDP less sensitive to learning rate paramaters, which is an important property in pratice. This suggests that BoB really helps us recover HDP better. The rest of paper is organzied as follows. In Sect. 2 , we present a short review of previous approaches to dealing with short texts. In Sect. 3 , we present general idea about Hierarchical Dirichlet Processes and the online learning algorithm [ 17 ]. In Sect. 4 , we present BoB and related definitions. In Sect. 5 , we describe our experiments and the evaluation of the new representation. In Sect. 6 ,we draw some conclusions for the paper. Besides simply applying traditional models such as LDA, HDP to short texts directly, there have been many other approaches to dealing with analyzing short texts. Here we summarize some of them to have a general view about previous approaches.
 from searching keywords in each short document are used to evaluate its semantic meaning. One disadvantage of this method is the dependence on external tools, we cannot guarantee the quality of search engine for any keyword, especially those from social networks. Therefore it is hard to use this approach for a general short text corpus.
 Wikipedia as a source of additional information for short texts. For a short doc-ument, the authors search the closest articles from Wikipedia and take advantage of this extra information. On the other hand, in [ 11 ], the authors use Wikipedia as a universal knowledge to build a model by applying LDA. From built model, the authors do inference for each document and integrate the result into its vec-tor. The weak point of this approach is there would be nothing sure about the correlation between the universal corpus and the short text corpora as they are almost generated from social networks with informal content and noises. hashtags or by the authors of tweets, the result is surely better when applying LDA to grouped texts. However, by grouping, one must know the metadata of the corpus, not all corpora of short texts have metadata as tweets. Therefore this technique is not a general approach to dealing with any corpus of short texts. a single word. More specifically, they generate all pairs of words (called biterm) for each document and aggregate all the biterms in all documents into one col-lection, and next they model the generative process for this collection. The good point of this approach is that it requires no additional information or any sources of knowledge. However, this approach does not model the generative process for each document, the authors use a heuristics to infer the topic proportions for each document. This does not guarantee the consistency between training phase and inferring phase.
 metadata or external sources of knowledge and able to take advantage of the flexibility of HDP. More concretely, we deal with the problem of analyzing short texts by coming up with a new representation of document and applying it to Online HDP. By this way, we can fully deal with three challenges of short texts mentioned above. In topic modeling, HDP [ 15 ] is considered as the nonparametric model of LDA [ 3 ], the generative process is
G  X  DP (  X , Dir (  X  )) ,G where Dir is Dirichlet distribution, DP is Dirichlet process and Mult is multino-mial distribution.  X ,  X ,  X  0 are hyperparameters.  X  k (for k level atoms. z di is topic index of w di and w di is the i We follow [ 17 ] to use stick-breaking construction [ 15 ] to have a closed-form coordinate ascent variational inference: G =  X   X 
Beta (1 , X  ) , X  k =  X  k where c dt are document-level topic indices. Beta is Beta distribution. Approximating the posterior distribution of its latent variables bases on the idea of mean-field variational inference [ 8 ]. The general idea of this technique is considering a family of distribution over latent variables, which is defined by free parameters and then finding the closest member of this family to the true posterior. Following [ 17 ], the variational distribution has the following form: q ( c , z ,  X  ,  X  ,  X  )= where  X  dt ,  X  dn are the variational parameters of multinomial distribution,  X  is the variational parameter of Dirichlet distribution, ( u variational parameters of Beta distribution, K is topic-level truncation, T is document-level truncation. Using Jensen inequality for the log likelihood of observed data we obtain a lower bound called ELBO. Taking derivatives with respect to each variational parameter, we have the coordinate ascent updates, more details at [ 17 ].
 Online learning for HDP bases on stochastic optimization [ 8 ]. The idea of the technique is based on computing noisy estimates of the gradient of ELBO instead of the true gradient which requires iterating through the whole dataset. At time t the fast noisy estimates of gradient is computed by subsampling a small set of document (called minibatch). Based on noisy estimates, the intermediate global parameters (  X   X  ,  X  u ,  X  v ) are calculated and then update global parameters with a decreasing learning rate schedule:  X  ( t )  X  (1  X   X  (1  X   X  4.1 Bag-of-Words (BoW) Bag-of-words (BoW) is the conventional representation of documents. In this representation, a document is considered as a container and words as items regardless of the information about the order of words. BoW has been widely used in common models for topic modeling such as LDA and HDP. Though being widely used in practice, BoW exhibits many limitations in modeling short texts [ 6 ]. The reason is that statistical models such as HDP often base on the statistical information about the co-occurence of words; when the document is short, the co-occurence is not enough to provide a clear context. This fact inspired us to try to find out a new representation that is applicable to HDP and more suitable than BoW for modeling short documents. 4.2 Bag-of-biterms (BoB) First, we define a  X  X iterm X  is created by a pair of words that co-occur in some document. For example, given a document containing two words  X  X haracter X  and  X  X tory X , (character, story) is a created biterm and (story, character) is also a created biterm.
 set of created biterms { b ij } from words ( w i ,w j ) where i =1 ,...n and j =1 ...n and associated frequencies defined by:  X  Frequency of b ii from ( w i ,w i )is f i  X  Frequency of b ij from ( w i ,w j ) where i = j is 1 cies { 2 , 2 , 4 } . In BoB, this document is represented by set of biterms b the frequency of biterms, we have the observation that short documents only have several distinct words, and each often appears 1 time, therefore, to guar-antee that the influence of additional biterms would not surpass original words, we set 1 for the frequency of each biterm.
 biterm in our perspective can be created from two identical words while biterm in [ 18 ] is only created by two different ones. Moreover, the purpose of creating biterms in our approach is to find a new representation of documents and apply it to a wide class of statistical models, while in [ 18 ], the authors aggregate all biterms derived from all documents to form a collection of biterms and model this collection as a mixture of topics, there is no concern about documents in the course of modeling process, only in inferring step, the authors use a heuristic to infer the topic proportion for each document. This does not guarantee the consistency between modeling phase and infering phase.
 Based on the definition presented above, we see that BoB has the same form like BoW, therefore applying BoB in HDP is the same as BoW. Moreover, we can see several properties of BoB:  X  The document representations in BoB are improved thanks to additional biterms b ij where i = j .  X  Size of vocabulary in BoB (the number of distinct biterms) is higher than that of BoW. The fact is that each biterm is created from a pair of words.  X  BoB is an expansion of BoW: BoB can be viewed as BoW adding additional  X  X ngredients X . In fact, in BoB, biterms b ii can be considered as original words w i and biterms b ij ,i = j are additional ingredients.
 These properties are the foundations for the explanation for the superiority of BoB in comparison with BoW which is presented in Sect. 4.3 .
 The Size of Vocabulary and Biterm Threshold in BoB. Note that in BoB, because of the symmetry of biterms b ij and b ji ( i = j ) in every document, in practice, we can reduce the memory for storage by merging b only use b ij ( i&lt;j ) with frequency 2 instead of 1.
 The biggest challenge of BoB is the dilation of vocabulary set. In theory, the number of possible biterms might be up to V b = V ( V  X  1) / 2+ V = V ( V +1) / 2 where V is the number of distinct words in corpus. This number is quite large that requires a lot of memory to store and the training time might last so long that makes the algorithm become infeasible. Fortunately, owing to the shortness of short texts, the practical number of distinct biterms is often not so large (see the description of datasets in Sect. 5.1 ) that storing and running are possible in a regular personal computer.
 However, besides the purpose of improving the speed of learning the model and reducing the memory needed during training process, removing low-frequency biterms is also considered as the preprocessing step like removing words with low frequency (noises) in preprocessing text. Therefore, we need to have a threshold called  X  X iterm threshold X . The biterms with document fre-quency lower than this threshold are eliminated from the representation of docu-ment. The size of vocabulary in BoB can be adjusted by changing biterm thresh-old. In our experiments presented at Sect. 5 , we also evaluate the influence of V on the quality of the model. Note that when biterm threshold is are removed, therefore BoB becomes BoW.
 4.3 Explanation for the Potential Superiority of BoB for Short There are several reasons for believing in a better performance of BoB. of a document is much more longer than that in BoW. Namely, in BoB each document is d ( d  X  1) longer than document in BoW, where d is the number of distinct words in this document. In [ 14 ], the authors showed that document length plays an important role in topic modeling, when documents are extremely short, the learned model is expected to have poor performance. The idea of lengthening short documents has also been carried out in previous researches [ 6 , 7 , 9 , 10 ]. In these researches, the authors aggregate short documents in a rational way to make them longer, consequently bring about better results.
 in topic modeling has indicated that the MAP inference for topic mixtures of each document can reach a global optimum when the length of document and the size of vocabulary are large enough [ 16 ]. Therefore, BoB can help us do posterior inference in topic models better than BoW. As a result, the learned model from BoB is expectedly better than that in BoW.
 ument in a rational way. In fact, biterms b ii =( w i ,w i original word w i and biterms b ij =( w i ,w j ) with i = j are the additional  X  X ngre-dients X . In a short document, due to the lack of information, the context becomes ambiguous and unclear. The ingredients being added would reinforce the context, make it clearer. In this section, we conduct extensive experiments on three different large short text collections to evaluate the effectiveness of our approach for dealing with short texts. We run Online HDP (as described in Sect. 3 ) in both BoW and BoB and compare their two performance measures and their sensitivity to learning rate parameters. We also investigate the influence of the size of vocabulary in BoB on the quality of the model. We use the source code of Online HDP from: http://www.cs.cmu.edu/  X  chongw/software/onlinehdp.tar.gz . 5.1 Datasets To prepare for our experiments, we had crawled three large collections of short texts, as described below:  X  Yahoo Questions : This dataset is crawled from https://answers.yahoo.com/ ,  X  Tweets : This dataset is a set of tweets crawled from Twitter ( http://twitter.  X  Nytimes Titles : This dataset is a set of titles of articles from The New York Times ( http://www.nytimes.com/ ) from 01/01/1980 to 29/11/2014. Each document is the title of an article.
 These datasets went through a preprocessing procedure including tokenizing, stemming, removing stopwords, removing low-frequency words (appear in less than 3 documents) and removing extremely short documents (less than 3 words). About BoB, for each corpus we set a different biterm threshold (definition of biterm threshold in Sect. 4.2 ) to make sure that the number of created biterms is not so large. The detailed description for each dataset is on Table 1 . 5.2 Performance Measures To compare the two representations, we use two very common performance mea-sures in topic modeling: the LPP and NPMI.
 Log Predictive Probability (LPP). LPP measures the predictiveness and generalization of a learned model to new data. The procedure for this measure is similar to [ 8 ]. For each dataset, we randomly separate into training set and test set. In test set, we only pick out documents with length greater than 4. For each chosen document, we divide it randomly into two part ( w 4:1, we did inference for w 1 and estimated the distribution of w BoB, when estimating the distribution of w 2 given w 1 , we need to convert the topic-over-biterms (distribution over biterms) to topic-over-words (distribution over words), the conversion fomula is described in Appendix.
 Normalized Pointwise Mutual Information (NPMI). NPMI [ 1 ] measures the coherence of topics of a model. This measure indicates the quality of the topics learned from dataset by evaluating words with highest probability of each topic (called top words). Here we compute this measure by considering 10 top words for each topic. In case of BoB, we first need to convert the topic-over-biterms (distribution over biterms) to topic-over-words (distribution over words) the conversion fomula is described in Appendix. 5.3 Result Performance Comparison. To evaluate thoroughly the performance of the new representation, we run Online HDP in various settings for each representa-tion in each dataset. More concretely, The settings of learning rate parameters (  X ,  X  ) form a grid:  X   X  X  1 , 20 , 40 , 60 , 80 , 100 } ,  X  setting of (  X ,  X  ), we fix the minibatch size = 5000, truncation for corpus K = 100, truncation for document T = 20 and  X  0 =1 . 0,  X  =1 . 0.
 shows the median of 24 LPPs associated with 24 settings of (  X ,  X  ) on three datasets in both representations. As we can see, When the number of documents seen increases, both Bow and BoB attain a better predictive capability regardless of learning rate parameters. However, in all three datasets, BoB outperforms BoW significantly. Moreover, BoB has good starting points while BoW always has poor ones and needs much time to be stable. This superiority of model learned from BoB can be explained by better inference for each document. As mentioned in Sect. 4.3 , the goodness of this inference may originate from the longer length in each document, larger vocabulary size and clearer context by additional ingredients. As the inference for each document is more precise, the model is recovered better.
 NPMIs associated with 24 settings of (  X ,  X  ) in both BoB and BoW , there is also a huge distinction between BoB and BoW like LPP, especially in Tweets . Apart from outperforming BoW significantly, BoB also has much better starting points. All of these observations indicates the superiority of BoB to BoW in the quality of learned topics in Online HDP. Besides the reasons explained above, the goodness of BoB in terms of NPMI can also be explained by the conversion from topic-over-biterms to topic-over-words (presented in appendix). Biterm b in BoB can be considered as w i in BoW. However in BoB there are additional ingredients b ij ,i = j , these items would contribute their probability to w BoB. Therefore, a word with more biterms containing it would be stressed more than a word with less biterms containing. By this way, the probability of words in each topic would converge to the true one more quickly and more precisely. Sensitivity of Learning Rate Parameters. To compare the sensitivity of  X  in BoB and BoW, we fix  X  =0 . 9 and explore different values of  X  { sensitivity of  X  in BoB and BoW, we fix  X  = 20 and explore different values of  X  see that BoB is much less sensitive to both  X  and  X  than BoW. Less dependence on learning rate parameters is a good property of BoB in practice, one does not have to consider exhaustive model selection for learning rate parameters. The Influence of Size of Vocabulary in BoB ( V b ). To evaluate the influence of learning rate in BoB with dataset Tweets ) and carry out experiments with varying V b by changing biterm threshold in set { 10 , 15 , 20 , 25 , 30 , that as biterm threshold is  X  , BoB becomes BoW. The experimental result is described in Fig. 5 . We observe that when V b = V (BoB becomes BoW), the model has the poorest predictive capability. The model attains best predictive performance when V b is not the smallest or the biggest, it seems that there is a tradeoff in V b . Namely when biterm threshold is large, there are many biterms being removed from document in BoB, as a result, the documents become shorter and we return to the problem of short text. On the other hand, if biterm threshold is small, there are many biterms being added to documents even they only appear in a small number of documents. These biterms can be considered as noises that might degrade the model quality. We investigated a new representation which is called bag-of-biterms (BoB) for documents. We found that BoB has many interesting properties, which overcome some severe limitations of bag-of-words representation. It can help us learn sig-nificantly better statistical models such as HDP from short texts. It can be easily employed in a large class of statistical models and methods without any need of modification. It also compliments to existing approaches to deal with short texts. Therefore, we believe that BoB is a potential approach, and can be used in a wide context.
 In BoB, after we finish training the model, we obtain topics, each is the multino-mial distribution over biterms and we want to convert to the topics with distri-bution over words. Assume that  X  k is the distribution over biterms of topic k . According to probability: p ( w i | z = k )= V j =1 p ( w i ,w j | z = k )= V j =1 p ( b As discussed in Sect. 4.2 , in implementing BoB, we can merge b b ij with i&lt;j . Because of identical occurence in every document, after finish-ing training process, the value of p ( b ij | z = k ) will be expectedly the same as p ( b ji | z = k ). Therefore, in grouping these biterms into one, the conver-sion version of this implementation is: p ( w i | z = k )=
