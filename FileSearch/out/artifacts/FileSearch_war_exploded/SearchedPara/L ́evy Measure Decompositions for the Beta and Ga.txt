 Yingjian Wang yw65@duke.edu Lawrence Carin lcarin@duke.edu A prominent distinction of nonparametric methods rel-ative to parametric approaches is the utilization of stochastic processes rather than probability distribu-tions . For example, a Gaussian process ( Rasmussen &amp; Williams , 2006 ) may be employed to nonparamet-rically represent general smooth functions on a con-tinuous space of covariates ( e.g. , time). Recently the idea of nonparametric methods has extended to fea-ture learning and data clustering, with interest respec-tively in the beta-Bernoulli process ( Thibaux &amp; Jor-dan , 2007 ) and the Dirichlet process ( Ferguson , 1973 ). In such processes the nonparametric aspect concerns the number of features/clusters, which are allowed to be unbounded ( X  X nfinite X ), permitting the model to adapt the number of these entities as the given and fu-ture data indicate. The increasing importance of these models in machine learning warrants a detailed theo-retical analysis of their properties, as well as simple constructions for their implementation. In this paper we focus on L  X evy processes ( Sato , 1999 ), which are of increasing interest in machine learning.
 A family of L  X evy processes, the pure-jump nonde-creasing L  X evy processes, also fit into the category of the completely random measure proposed by King-man ( Kingman , 1967 ). The beta process ( Hjort , 1990 ) is an example of such a process, which is applied in nonparametric feature learning. The gamma process falls in this family as well, with its normalization the well-known Dirichlet process. Hierarchical forms of such models have become increasingly popular in ma-chine learning ( Teh et al. , 2006 ; Teh , 2006 ; Thibaux &amp; Jordan , 2007 ), as have nested models ( Blei et al. , 2010 ), and models that introduce covariate depen-dence ( MacEachern , 1999 ; Williamson et al. , 2010 ; Lin et al. , 2010 ).
 As a consequence of the important role these mod-els are playing in machine learning, there is a need for the study of the properties of pure-jump nondecreasing L  X evy processes. As examples of such work, ( Thibaux &amp; Jordan , 2007 ) and ( Paisley et al. , 2010 ) present ex-plicit constructions for generating the beta process, ( Teh et al. , 2007 ) derives a construction for the Indian buffet process parallel to the stick-breaking construc-tion of the Dirichlet process ( Sethuraman , 1994 ), and ( Thibaux , 2008 ) obtains a construction for the gamma process under the gamma-Poisson context. Apart from these specialized construction methods, in ( Kingman , 1967 ) a general construction method for completely random measures is proposed, by first decomposing it into a sum of a countable number of  X  -finite measures, and then superposing the Poisson processes according to these sub-measures. By regarding the completely random measure as a L  X evy process, this method corre-sponds to decomposing the L  X evy measure, which pro-vides clarity of theoretical properties and simplicity in practical implementation. However this L  X evy measure decomposition method has not yet come into wide use in machine learning and statistics, probably due to the nonexistence of a universal construction of the measure decomposition.
 In this paper we develop explicit and simple decom-positions by following the conjugacy principle for two widely used L  X evy processes, the beta and gamma pro-cesses. The conjugacy means that the decompositions are manifested by leveraging the forms of conjugate likelihoods to the L  X evy measures. The decompositions bring new perspectives on the beta and gamma pro-cesses, with associated properties analyzed here in de-tail. The decompositions are constituted in terms of an infinite set of sub-processes of form convenient for computation. Since the number of sub-processes is infinite, a truncation analysis is also presented, of in-terest for practical use. We show some posterior prop-erties of such decompositions, with the beta process as an example. We also extend the decomposition to the symmetric gamma process (positive and negative jumps), suggesting that the L  X evy measure decomposi-tion is applicable for other pure-jump L  X evy processes represented by their L  X evy measures. Summarizing the main contributions of the paper:  X  We constitute L  X evy measure decompositions for  X  The decomposition of the beta process unifies the  X  Truncation analyses and posterior properties for L  X evy processes ( Sato , 1999 ) and completely random measures ( Kingman , 1967 ) are two closely related con-cepts. Specifically, some L  X evy processes can be re-garded as completely random measures. In this section brief reviews and connections are presented for these two important concepts. 2.1. L  X evy process A L  X evy process X (  X  ) is a stochastic process with in-dependent increments on a measure space ( X  , F ).  X  is usually taken to be one-dimensional, such as the real line, to represent a stochastic process with varia-tion over time. By the L  X evy-It X o decomposition ( Sato , 1999 ), a L  X evy process can be decomposed into a con-tinuous Brownian motion with drift, and a discrete part of a pure-jump process. When a L  X evy process X (  X  ) only has the discrete part and its jumps are pos-itive, then for  X A  X  F the characteristic function of the random variable X ( A ) is given by: with  X  satisfying the integrability condition ( Sato , 1999 ). The expression in ( 1 ) defines a category of pure-jump nondecreasing L  X evy processes, including most of the L  X evy processes currently used in nonpara-metric Bayesian methods, such as the beta, gamma, Bernoulli, and negative binomial processes. With ( 1 ), such a L  X evy process can be regarded as a Poisson point process on the product space R +  X   X  with the mean measure  X  , called the L  X evy measure. On the other hand, if the increments of X (  X  ) on any measurable set A X  X  are regarded as a random measure assigned on the set, then X (  X  ) is also a completely random measure. Due to this equivalence, in the following dis-cussion we will not discriminate the pure-jump nonde-creasing L  X evy process X with its corresponding com-pletely random measure  X . 2.2. Completely random measure A random measure  X  on a measure space ( X  , F ) is termed  X  X ompletely random X  if for any disjoint sets A 1 , A 2  X  F the random variables  X ( A 1 ) and  X ( A 2 ) are independent. A completely random measure  X  can be split into three independent components: where  X  f = P  X   X  X   X  (  X  )  X   X  is the fixed component, with the atoms in I fixed and the jump  X  (  X  ) ran-dom; I is a countable set in F . The deterministic component  X  d is a deterministic measure on ( X  , F ).  X  f and  X  d are relatively less interesting compared to the third component  X  o , which is called the ordinary component of  X . According to ( Kingman , 1967 ),  X  o is discrete with both random atoms and jumps. In ( Kingman , 1967 ), it is noted that  X  o can be further split into a countable number of independent parts: Denote  X  as the L  X evy measure of (the L  X evy process corresponding to)  X  o ,  X  k as the L  X evy measure of  X  k ,  X  a Poisson process with  X  its mean measure, and  X  k a Poisson process with  X  k its mean measure; ( 3 ) further yields: which provides a constructive method for  X  o : first con-struct the Poisson process  X  k underlying  X  k , and then with the superposition theorem ( Kingman , 1993 ) the union of  X  k will be a realization of  X  o . In the following sections we show how this general construction method of ( 4 ) can be applied on pure-jump nondecreasing L  X evy processes of increasing interest in machine learning, with an emphasis on the beta and gamma processes, and their generalizations. A beta process ( Hjort , 1990 ) is a L  X evy process with beta-distributed increments; B  X  BP( c (  X  ) ,  X  ) is a beta process if where  X  is the base measure on measure space ( X  , F ) and a positive function c (  X  ) the concentration func-tion. Expression ( 5 ) indicates that the increments of the beta process are independent, which makes it a special case of the L  X evy process family. The L  X evy mea-sure of the beta process is where Beta(0 , c (  X  )) = c (  X  )  X   X  1 (1  X   X  ) c (  X  )  X  1 proper beta distribution since its integral over (0 , 1) is infinite. As a result, its underlying Poisson process , i.e. , the Poisson process with  X  as its mean measure on the product space  X   X  (0 , 1), denoted  X , has an infinite number of points drawn from  X  , yielding where  X  i is the jump (increment) which happens at the atom  X  i . Real variable  X  =  X  ( X ) is termed the mass parameter of B , and we assume  X  &lt;  X  . 3.1. Beta process L  X evy measure decomposition The infinite integral of the improper beta distribution inspires a decomposition of the improper distribution with an infinite number of proper distributions. The singularity in the improper beta distribution is mani-fested from  X   X  1 . Since  X   X  (0 , 1), the geometric series expansion yields and substituting ( 8 ) in ( 6 ), with manipulation detailed in the Supplementary Material, we have the L  X evy mea-sure decomposition theorem of the beta process: Theorem 1 For a beta process B  X  BP( c (  X  ) ,  X  ) with base measure  X  and concentration c (  X  ) , denote  X  as its underlying Poisson process and  X  the L  X evy measure, then B and  X  can be expressed as where B k is a L  X evy process with  X  k its underlying Poisson process. The L  X evy measure  X  k of B k is a de-composition of  X  : where Beta(1 , c (  X  )+ k ) is the PDF of beta distribution with parameters 1 and c (  X  ) + k .
 Theorem 1 is the beta process instantiation of the com-pletely random measure decomposing in ( 4 ), which in-dicates that the underlying Poisson process  X  of the beta process B is the superposition of an infinite num-ber of independent Poisson processes {  X  k }  X  k =0 , with  X  k the mean measure of  X  k and  X  k the mean measure of the restriction of  X  k on  X . As a result, the beta pro-cess B can be expressed as a sum of an infinite number of independent L  X evy processes { B k }  X  k =0 with {  X  k the underlying Poisson process. The independence of fact that both  X  and c (  X  ) are fixed parameters. 3.2. The L  X evy process B k It is interesting to study the properties of B k , such as the expectation and variance. Denoting B k ( d X  ) = c (  X  )+ k +1  X  k ( d X  ) as the base measure of B k , for  X A  X  F : It is noteworthy that the L  X evy process B k is no longer a beta process, since ( 5 ) is not satisfied. By Theorem 1, the jumps of B k follow a proper beta distribution parameterized by the concentration function c (  X  ) and the index k , and  X  k determines the locations where the jumps happen. Since { B k }  X  k =0 are independent w.r.t. the index k , with Theorem 1: The detailed procedure to derive ( 11 ) and ( 12 ) is given in the Supplementary Material. 3.3. Simulating the beta process 3.3.1. Poisson superposition simulation Theorem 1 reveals that the underlying Poisson pro-cess of a beta process is a superposition of an infinite number of Poisson processes, each of which has a finite set of atoms. This perspective also provides a simula-tion procedure for the beta process: first, the Poisson process  X  k is sampled for all k = 0 , 1 , 2 ,  X  X  X  , (here we term the index k as the  X  X ound X  of the simulation); then take the union of the samples of each  X  k as a realization of the Poisson process  X . With the mark-ing theorem ( Kingman , 1993 ) implicitly applied, the simulation procedure of the beta process is as follows: Simulation procedure : For round k : 1: Sample the number of points for  X  k : n k  X  tion of  X  (and equivalently of B ).
 We refer to the above simulation procedure as the Poisson superposition simulation , for the central role of the Poisson superposition. The especially conve-nient case is when the beta process is homogeneous, i.e. , c (  X  ) = c is a constant. In this case {  X  ki } n all rounds k are drawn from the same distribution  X / X  ; and n k is drawn from Poisson( c X  c + k ). For round k , both the number of points and the jumps statistically di-minish as k increases, suggesting that the infinite sum in ( 9 ) may be truncated as B = P K k =0 B k for large K , with minimal impact. Such truncation effects are investigated in detail in Section 3.4 . 3.3.2. Related work In ( Thibaux &amp; Jordan , 2007 ) the authors derived the above simulation procedure for the homogeneous case within the beta-Bernoulli process context, which is shown here a necessary result of the L  X evy measure de-composition. The same decomposing manipulation of Theorem 1 can be also applied to the stable beta pro-cess ( Teh &amp; G  X or  X ur , 2009 ) which yields: It is noteworthy that the decomposition procedure de-scribed in Theorem 1 is not the only L  X evy measure de-composing method for the beta process. The work of ( Paisley &amp; Jordan , 2012 ) and ( Broderick et al. , 2011 ) show that the stick-breaking construction of the beta process in ( Paisley et al. , 2010 ) is indeed a result of another way of decomposing the L  X evy measure of the beta process. We next analyze the truncation prop-erty of the construction described in Section 3.3.1 and make comparison with the construction of beta process in ( Paisley et al. , 2010 ). 3.4. Truncation analysis Since the Poisson superposition simulation operates in rounds, it is natural to analyze the distance between the true beta process B and its truncation P K k =0 B k , with truncation at round K . A metric for such dis-tance is the L 1 norm: || B  X  The expectation in ( 14 ) is w.r.t. the normalized mea-sure  X / X  , which yields k B k 1 = 1. When B is homo-geneous, ( 14 ) reduces to c c + K +1 , which indicates that the L 1 distance decreases at a rate of O ( 1 K ). For the stick-breaking construction of beta process described in ( Paisley et al. , 2010 ), the L 1 distance is: ( c c +1 Another metric is the L 1 distance between the marginal likelihood of a set of data b = b 1: M , with m  X  ( b ) denotes the marginal likelihood (here the like-lihood is a Bernoulli process) with prior B , and m K ( b ) for P K k =0 B k . This metric was applied on the trun-cated Indian buffet process ( Doshi et al. , 2009 ) and truncated stick-breaking construction of the beta pro-cess ( Paisley &amp; Jordan , 2012 ), which indicates 1 4
Z Pr(  X  k &gt; K, 1  X  i  X  n k , 1  X  m  X  M, s.t. b m ki = 1) where b 1: M i.i.d.  X  BeP( B ) are drawn from a Bernoulli process with base measure B ; b m ki = b m (  X  ki ) is the m th realization of the Bernoulli process at atom  X  ki . For the truncation P K k =0 B k it can be shown that the RHS of ( 15 ) is bounded by: For the homogeneous case, the bound of ( 16 ) is 1  X  exp(  X  M  X  c c + K +1 ). For the stick-breaking con-struction of beta process, the bound is given by: 1  X  exp(  X  M  X  ( c c +1 ) K +1 ) ( Paisley &amp; Jordan , 2012 ). In order to analyze the bound w.r.t. the truncation level by number of atoms, denote I K = P K k =0 n k as the total number of atoms in P K k =0 B k . Since in ( 16 ) decreases at a faster rate w.r.t. I than the stick-breaking construction of beta process. This indi-cates that the simulation procedure described in Sec-tion 3.3.1 follows a steeper statistically-decreasing or-der. The proof is presented in the Supplementary Ma-terial. 3.5. Posterior estimation The goal of the inference is to estimate the beta process B from a set of observed data b with prior BP( c,  X  ). The data b = b 1: M is the same as in Section 3.4 , which can be expressed as: where each b i,m  X  X  0 , 1 } . 3.5.1. Posterior of B k Jordan , 2007 ), the base measure of B | b is a measure with positive masses assigned on single atoms. Theo-rem 1 is still applicable to this beta process with mixed type of base measure, which yields where the B 0 , B 0 k ,  X  0 k , and  X  0 k are the posterior coun-terparts of B , B k ,  X  k , and  X  k . 3.5.2. Posterior estimation of  X  i : the jumps following the distribution Beta(1 , c + M + k ) at the atom  X  i , whose sum is the  X  i . Thus the posterior estimation of  X  i is given by from which it can be verified that E (  X  i | b ) = P M m =1 the same as the posterior of  X  i without decomposition: For the  X  i with no observations, i.e. , P M m =1 b i,m = 0, only a particular B k will contribute to  X  i . In this case, first the round k to which  X  i belongs is drawn, then  X  i is drawn from the beta distribution of that round: where MP(  X  ) is a multinomial process with proba-bility vector  X  , and  X  is proportional to the average number of points in each round. Since in practical pro-cessing  X  is always to be truncated with a truncation level K , by the analysis in Section 3.4 , ( 20 ) provides a way to estimate the  X  i within the first K rounds. And  X  i in each round are of statistically different impor-tance, contrasted to the evenly assigned mass in the Indian buffet process. 3.6. Relating the IBP and beta process The study of the beta process through its L  X evy mea-sure, as discussed in this paper, also uncovers a connec-tion between the Indian buffet process (IBP) ( Griffiths &amp; Ghahramani , 2005 ) and the beta process, by their L  X evy measures. The IBP with prior  X  i  X  Beta( c  X  N , c ) can be regarded as a L  X evy process with the L  X evy mea-sure given as: here N is the same as the K in ( Griffiths &amp; Ghahra-mani , 2005 ). It can be proved that: which indicates that the beta process is the limit of the IBP with N  X   X  . The detailed proof of ( 22 ) is presented in the Supplementary Material. Thus the IBP is like a  X  X osaic X  approximation of beta process, which becomes finer with N increases.
 A gamma process ( Applebaum , 2009 ) is a L  X evy process with independent gamma increments. The gamma process is traditionally parameterized with a shape measure and a scale function: G  X   X P(  X ,  X  (  X  )) where  X  is the shape measure on a measure space ( X  , F ), and the scale  X  (  X  ) a positive function. A gamma pro-cess can be intuitively defined by its increments on infinitesimal sets: When  X  (  X  ) =  X  is a scalar, the gamma process is called homogeneous. The gamma process can also be ex-pressed in the form with a base measure G 0 and a concentration c (  X  ), with c = 1 / X  and G 0 =  X  X  ( Jor-dan. , 2009 ), to conform with other stochastic processes widely used in machine learning, such as the Dirich-let process. However, the discussion in this paper will stick to the traditional form given by ( 23 ). As a pure-jump L  X evy process, the gamma process can be regarded as a Poisson process on the product space  X   X  R + with mean measure  X  : gamma distribution with an infinite integral on R + , which yields the expression of G : 4.1. L  X evy measure decomposition Like the beta process, the L  X evy measure of the gamma process is characterized by an improper distribution. However, unlike the beta process, the decomposition of the L  X evy measure of the gamma process comes from the exponential part. With the details shown in the Supplementary Material, the gamma process G can be decomposed into two parts: The second term in ( 26 ) is a gamma process with the same shape measure, and half the scale of the gamma process G ; the first term  X  1 is a L  X evy process with the L  X evy measure P  X  h =1 Gamma( h,  X  (  X  ) 2 ) dp Here Gamma( h,  X  (  X  ) 2 ) is the PDF of the gamma distri-bution, with shape parameter h and scale parameter Further decomposing the exponential part of the gamma process  X P(  X ,  X  (  X  ) / 2) in ( 26 ) yields G =  X  + X  2 + X P(  X ,  X  (  X  ) / 3), bearing a gamma process with the same shape and with the scale parameter further decreased. Repeating this manipulation, we obtain the Theorem 2: Theorem 2 A gamma process G  X   X P(  X ,  X  (  X  )) with shape measure  X  and scale  X  (  X  ) can be decomposed as: with  X  k ,  X  kh L  X evy processes with  X  k ,  X  kh their L  X evy measures.
 Theorem 2 is the gamma process instantiation of ( 4 ), which indicates that G can be expressed as the sum of an infinite number of L  X evy processes  X  k , k = 1 , 2 ,  X  X  X  , where  X  k is also the sum of an infinite number of L  X evy processes  X  kh , h = 1 , 2 ,  X  X  X  . 4.2. L  X evy processes  X  k and  X  kh In order to obtain further insights into the gamma pro-cess G in Theorem 2, the expectations and variances of  X  k and  X  kh on any measurable set A X  X  are given: For the variances of  X  k and  X  kh :
Var( X  kh ( A )) =
Var( X  k ( A )) = [ Since the L  X evy processes  X  k are independent w.r.t. k , with analogy to ( 12 ) it can be verified that the ex-pectation and variance of  X  k sum to the expectation of variance of G . The derivations in this section are presented in the Supplementary Material. 4.3. Simulation of gamma process Parallel to the simulation of beta process in Section 3.3.1 , a simulation procedure of the gamma process is presented: Simulation procedure : Sample the L  X evy process  X  1: Sample the number of points for  X  kh : n kh  X  2: Sample n kh points from  X  :  X  khi i.i.d.  X   X   X  , for i = where  X  = R  X   X  ( d X  ) is the mass of the shape mea-is a realization of the gamma process G . An advan-tage of the above simulation procedure compared to the simulation procedure of the beta process in Sec-tion 3.3.1 is that independent of whether the gamma process is homogeneous or inhomogeneous,  X  khi is al-ways drawn from a fixed distribution  X / X  . Like with the beta process construction in Section 3.3.1 , for the gamma process simulation procedure, as k increases the expected number of new points and the expected jumps decrease, again suggesting accurate truncation. 4.4. Truncation analysis Since in the simulation procedure in Section 4.3 the index k and h both go to infinity, it is practical to analyze the distance between the true gamma process and the truncated one. To measure such a distance, we apply the L 1 norm described in Section 3.4 : where the expectation in ( 30 ) is w.r.t. the normalized measure  X / R  X   X  (  X  )  X  ( d X  ) with || G || 1 = 1; and K and H are the truncation level of k and h . Then for the situation with H =  X  : which indicates a O ( 1 K ) decreasing rate as same as the truncated beta process shown in ( 14 ). It is note-worthy that  X  1 alone accounts for on average half the mass of G . When H is finite, a remaining distance P 4.5. Generalized gamma process and Theorem 2 can be easily extended to some variations of the gamma process. Here we give the examples of the generalized gamma process ( Brix , 1999 ) and sym-metric gamma process ( C  X inlar , 2010 ).
 The generalized gamma process extends the ordinary gamma process by adding a parameter 0 &lt;  X  &lt; 1, Then with the same decomposition procedure, it is straightforward that the L  X evy measure for  X  kh of the generalized gamma process will change to  X  kh = The symmetric gamma process is a L  X evy process whose increments are the differences of two gamma-distributed variables with the same law, whose L  X evy negative increments, the symmetric gamma process is not a completely random measure. However, the same decomposition procedure is still applicable, yielding  X  kh = Gamma( | p | h, Gamma( h,  X  (  X  ) k +1 ), then decide the sign of p through a symmetric Bernoulli distribution. The L  X evy measure decomposition of the beta and gamma processes provides new perspectives on the two widely used stochastic processes, by casting insights on the sub-processes constituting them, here the B k and  X  . And the decomposition prescriptions described here are far from the only ways of such decomposi-tion. Theoretically elegant construction methods are derived from the proposed decompositions, which are directly implementable in practice.
 We have applied the proposed beta and gamma rep-resentations in numerical experiments, the details of which are omitted, as this paper focuses on founda-tional properties. However, to briefly summarize ex-perience with such representations, consider for exam-ple the image inpainting problem considered in ( Zhou et al. , 2009 ), based upon a beta process factor analy-sis model ( Paisley &amp; Carin , 2009 ). In experiments we performed with such a model, using a Gibbs sampler, the beta process prior was implemented using the pro-cedure discussed in Section 3.3.1 , with the posterior estimation in Section 3.5 applied for inference. The proposed representation infers a dictionary with the  X  X mportant X  dictionary elements captured by the low-index members (see the discussion in Section 3.3.1 ). The model prioritized the first three dictionary el-ements as being pure colors, specifically red, green, and blue, with the important structured dictionary el-ements following (and no other pure-color dictionary elements, while in ( Zhou et al. , 2009 ) many  X  seem-ingly redundant  X  pure-color dictionary elements are inferred). This  X  X lean X  inference of prioritized dictio-nary elements may be responsible for our also higher observed PSNR in signal recovery, compared to the re-sult given in ( Zhou et al. , 2009 ). The new gamma pro-cess construction in Section 4.3 may be implemented in a similar manner, and may be employed within re-cent models in machine learning in which the gamma process has been utilized ( e.g. , ( Paisley et al. , 2011 )). The research reported here was supported by ARO, NGA, ONR and DARPA (MSEE program).
 Applebaum, D. Levy Processes and Stochastic Calcu-lus . Cambridge University Press, 2009.
 Blei, D.M., Griffiths, T.L., and Jordan, M.I. The nested chinese restaurant process and bayesian non-parametric inference of topic hierarchies. J. ACM , 57(2), 2010.
 Brix, A. Generalized gamma measures and shot-noise
Cox processes. Advances in Applied Probability , 31: 929 X 953, 1999.
 Broderick, T., Jordan, M., and Pitman, J. Beta pro-cesses, stick-breaking, and power laws. Bayesian analysis , 2011.
 C  X inlar, E. Probability and Stochastics . Graduate Texts in Mathematics. Springer, 2010.
 Doshi, F., Miller, K.T., Van Gael, J., and Teh, Y.W. Variational inference for the Indian buffet process. In AISTATS , volume 12, 2009.
 Ferguson, T. A Bayesian analysis of some nonpara-metric problems. The Annals of Statistics , 1973. Griffiths, T. and Ghahramani, Z. Infinite latent feature models and the Indian buffet process. In NIPS , 2005. Hjort, N.L. Nonparametric Bayes estimators based on beta processes in models for life history data. Annals of Statistics , 1990.
 Jordan., M.I. Hierarchical models, nested models and completely random measures. In Frontiers of Sta-tistical Decision Making and Bayesian Analysis: In
Honor of James O. Berger . New York: Springer, 2009.
 Kingman, J.F.C. Completely random measure. In
Pacific Journal of Mathematics , volume 21(1):59-78, 1967.
 Kingman, J.F.C. Poisson Processes . Oxford Univer-sity Press, Oxford, 1993.
 Lin, D., Grimson, E., and Fisher, J. Construction of dependent dirichlet processes based on poisson processes. In NIPS , pp. 1396 X 1404. 2010.
 MacEachern, S.N. Dependent Nonparametric Pro-cesses. In In Proceedings of the Section on Bayesian Statistical Science , 1999.
 Paisley, J., Blei D.M. and Jordan, M.I. Stick-breaking beta processes and the poisson process. AISTATS , 2012.
 Paisley, J. and Carin, L. Nonparametric factor analysis with beta process priors. In ICML , 2009.
 Paisley, J., Zaas, K., Woods, C., Ginsburg, G., and
Carin, L. A stick-breaking construction of the beta process. In ICML , pp. 847 X 854, 2010.
 Paisley, J., Wang, C., and Blei, D. The discrete infinite logistic normal distribution for mixed-membership modeling. In AISTATS , 2011.
 Rasmussen, C. and Williams, C. Gaussian Processes for Machine Learning . MIT Press, 2006.
 Sato, K. L  X evy processes and infinitely divisible distri-butions . Cambridge University Press, 1999.
 Sethuraman, J. A constructive definition of Dirichlet priors. Statistica Sinica , 1994.
 Teh, Y.W. A hierarchical Bayesian language model based on Pitman-Yor processes. In Coling/ACL , pp. 985 X 992, 2006.
 Teh, Y.W. and G  X or  X ur, D. Indian buffet processes with power-law behavior. In NIPS , 2009.
 Teh, Y.W., Jordan, M.I., Beal, M.J., and Blei,
D.M. Hierarchical dirichlet processes. JASA , pp. 101:1566 X 1581, 2006.
 Teh, Y.W., G  X or  X ur, D., and Ghahramani, Z. Stick-breaking construction for the Indian buffet process. In AISTATS , 2007.
 Thibaux, R. Nonparametric Bayesian Models for Ma-chine Learning . PhD thesis, EECS Dept., University of California, Berkeley, Oct 2008.
 Thibaux, R. and Jordan, M.I. Hierarchical beta pro-cesses and the Indian buffet process. In AISTATS , 2007.
 Williamson, S., Orbanz, P., and Ghahramani, Z. De-pendent Indian buffet processes. In AISTATS , 2010. Zhou, M., Chen, H., Paisley, J., Ren, L., Sapiro, G., and Carin, L. Non-parametric Bayesian dictionary learning for sparse image representations. In NIPS ,
