 University of Ottawa University of Ottawa Polish Academy of Sciences
We present an approach to the automatic creation of extractive summaries of literary short stories. The summaries are produced with a specific objective in mind: to help a reader decide whether she would be interested in reading the complete story. To this end, the summaries give evaluated the summaries on a number of extrinsic and intrinsic measures. The outcome of this evaluation suggests that the summaries are helpful in achieving the original objective. 1. Introduction
In the last decade, automatic text summarization has become a popular research topic with a curiously restricted scope of applications .A few innovative research directions have emerged, including headline generation (Soricut and Marcu 2007), summarization of books (Mihalcea and Ceylan 2007), personalized summarization (D  X   X az and Gerv  X  as 2007), generation of tables-of-contents (Branavan, Deshpande, and Barzilay 2007), sum-marization of speech (Fuentes et al .2005), dialogues (Zechner 2002), evaluative text (Carenini, Ng, and Pauls 2006), and biomedical documents (Reeve, Han, and Brooks 2007) .In addition, more researchers have been venturing past purely extractive sum-marization (Krahmer, Marsi, and van Pelt 2008; Nomoto 2007; McDonald 2006) .By and large, however, most research in text summarization still revolves around texts characterized by rigid structure .The better explored among such texts are news articles (Barzilay and McKeown 2005), medical documents (Elhadad et al .2005), legal docu-ments (Moens 2007), and papers in the area of computer science (Teufel and Moens 2002; Mei and Zhai 2008) .Although summarizing these genres is a formidable challenge in itself, it excludes a continually increasing number of informal documents available electronically .Such documents, ranging from novels to personal Web pages, offer a wealth of information that merits the attention of the text summarization community.
We attempt to make a step in this direction by devising an approach to summarizing a relatively unexplored genre: literary short stories.
 istics that help identify some of the important passages without performing in-depth semantic analysis .These characteristics include predictable location of typical items in a document and in its well-delineated parts, cue words, and template-like structure that often characterizes a genre (e.g., scientific papers). This is not the case in literature.
Quite the contrary X  X o write fiction in accordance with a template is a sure way to write poor prose .One also cannot expect to find portions of text that summarize the main idea behind a story, and even less so to find them in the same location .In addition, the variety of literary devices (the widespread use of metaphor and figurative language, leaving things unsaid and relying on the reader X  X  skill of reading between the lines, a contribution of this work to demonstrate that summarizing short fiction is feasible using state-of-the-art tools in natural language technology .In the case of our corpus, this is also done without deep semantic resources or knowledge bases, although such resources would be of great help .We leverage syntactic information and shallow se-mantics (provided by a gazetteer) to produce indicative summaries of short stories that people find helpful and that outperform naive baselines and two state-of-the-art generic summarizers.
 the course of this work we concentrate on producing summaries of short stories suit-able for a particular purpose: to help a reader form adequate expectations about the complete story and decide whether she would be interested in reading it .To this end, the summary includes important elements of the setting of a story, such as the place and the main characters, presented as excerpts from the complete story .The assump-tion behind this definition is this: If a reader knows when and where the story takes place and who its main characters are, she should be able to make informed decisions about it.
 introduced this limitation for two reasons .There is an  X  X deological X  side of the decision:
Not many people want to know what happens in a story before reading it, even if this may help them decide that the story is worth reading .There also is a practical side, namely the complexity of the problem: Summarizing the plot would be considerably issue in the future .For now, creating indicative summaries of short stories is challenge enough.
 lead baseline and a ceiling .Figure 1 shows an example of an automatically produced summary that meets the aforementioned criteria .A reader can see that the story is set in a restaurant where the customers are tended to by two waitresses: the fair Aileen who  X  X ins hearts X  and  X  X he-bag-o X -meal X  plain-faced Tildy .If the reader chooses to pursue the story, she will find the description of an accident of paramount importance to Tildy: usually under-appreciated Tildy .It causes a complete change in how she views herself .
The story then unfolds to reveal that the customer was drunk on the day in question and that he returned to apologize several days later .This apology is a severe blow to
Tildy and an abrupt end of many a dream that the incident had spurred in her head .The story ends with Aileen trying to comfort her crying friend by saying  X  X e ain X  X  anything 72 of a gentleman or he wouldn X  X  ever of apologized. X  Yet, the summary in Figure 1 does not reveal these facts .For comparison, Figure 2 shows a summary obtained by taking the same number of sentences from the beginning of the story .As the reader can see, such a trivial approach is not sufficient to create a useful summary .Figure 3 shows a manually created  X  X deal X  summary.
 written by renowned writers, including O .Henry, Jerome K .Jerome, Anton Chekhov, and Guy de Maupassant .The stories, with the exception of a few fairy tales, are clas-sical examples of short social fiction .The corpus was collected from Project Gutenberg ( www.gutenberg.org ) and only contains stories in English .The average length of a story is 3,333 tokens and the target compression rate expressed in the number of sentences is 94%.
 forth indicative summaries ), the system searches each story for sentences that focus on important entities and relate the background of the story (as opposed to events).
Correspondingly, processing has two stages .Initially, the summarizer identifies two types of important entities: main characters and locations .This is achieved using a gazetteer, resolving anaphoric expressions and then identifying frequently mentioned and focus on one of the important entities .In order to separate the background of a story from the plot (i.e., events), we rely on the notion of aspect. aspectual type of a clause using either machine learning or manually produced rules.
This is achieved by relying on an array of verb-related features, such as tense, lexical aspect of the main verb, presence of temporal expressions, and so on .Finally, the system composes a summary out of the selected sentences.
 indicative summaries successfully, one needs to consider many facets of the problem.
An informative data representation, computational complexity, and usability of the final product are only some of them .Because the project is at the stage of an advanced
Therefore, we concentrated on several specific issues and left many more to future work and to fellow researchers.
 creating summaries .We devised an informative and practical data representation that could be reproduced without too much cost or effort .Secondly, we restricted ourselves to identifying the most informative portions of the stories and paid much less attention to readability and coherence of the resulting summaries .Even though readability is an important property, we hypothesized that informativeness is yet more important .Once the task of identifying informative passages has been accomplished, one can work on achieving coherence and readability .In the end, the emphasis was on the creation of extractive summaries using established tools and methods and on the identification of genre-specific properties that can help summarization.
 summaries of literary prose call for a thorough evaluation using a variety of metrics.
That is why we conduct three distinct evaluation experiments .The summaries are 74 evaluated both extrinsically and intrinsically .They are also compared with two naive baselines (lead and random) and two state-of-the-art summarization systems designed for summarizing newswire (henceforth baseline summarizers ). summaries, and answer questions about them .Some questions are factual in nature of the summary ) .The results show that the machine-made summaries are significantly summaries.
 pared against extracts created by people using sentence co-selection measures (pre-cision, recall, and F-score) .By sentence co-selection we mean measuring how many sentences found in manually created extracts are selected for inclusion in automatically produced summaries .The results suggest that our system outperforms all baselines, including state-of-the-art summarizers.
 the machine-made and the baseline summaries with the model abstracts .The results suggest that these measures are not well suited for evaluating extractive indicative summaries of short stories.
 the body of research in automatic story comprehension .Section 3 describes the process aspect, gives an overview of the system X  X  design, and discusses the linguistic motivation behind it .Section 5 describes the classification procedures (the use of machine learning and manual rule creation) that distinguish between the descriptive elements of a story and the passages that describe events .Section 6 reports on the evaluation of summaries which our system produces .Section 7 draws conclusions and outlines directions for future work. 2. Related Work
Summarization of literary prose is a relatively unexplored topic .There exists, however, a substantial body of research tackling the problem of story comprehension .During the 1970s and 1980s, a number of researchers in artificial intelligence built story-understanding systems that relied in one way or another on contemporary research in psychology and discourse processing.
 cognitive structure (known as macrostructure [van Dijk 1980] or schema [Bartlett 1932]) and that they can be decomposed into a finite number of cognitive units .According infinite number of combinations of a (relatively) small number of cognitive units .This direction was pioneered in 1928 by Vladimir Propp (1968) with his detailed analysis of 100 Russian folk tales .After a period of oblivion, these ideas regained popularity: van
Dijk and Kintsch (1978) demonstrated the existence of macrostructures and their role in story comprehension and recall; Rumelhart (1975), Thorndyke (1975), and Mandler (1987) developed sets of cognitive schemas (story grammars) that could be applied to certain types of stories; and Lehnert (1982) proposed to represent action-based stories in terms of a finite number of plot units X  X onfigurations of affect (or emotional) states of its characters.
 with those in artificial intelligence and resulted in a score of story-understanding and question-answering systems .Many of these relied on a set of manually encoded schemas and chose the most appropriate one for a given story (Cullingford 1978; Dyer 1983; Leake 1989) .For example, a system called BORIS (Dyer 1983) processed stories word-by-word to create a very rich semantic representation of them using Memory
Organization Packets (MOPs) and Thematic Affect Units (TAUs) .These knowledge structures were activated by means of a very detailed lexicon where each lexeme was associated with MOPs and TAUs it could invoke.
 tempted to circumvent this problem by learning to recognize more general structures.
FAUSTUS (Norvig 1989) recognized six general classes of inferences by finding patterns of connectivity in a semantic network .It could be adapted to new kinds of documents by extending its knowledge base and not the underlying algorithm or patterns .Research in automatic story comprehension offered a number of important solutions for subsequent developments in artificial intelligence .No less important, it pointed out a number of challenges .All these systems required a formidable amount of semantic knowledge and a robust and efficient way of building a semantic representation of texts .In addition, systems such as BORIS or SAM (Cullingford 1978) also needed a set of schemas or schema-like scenarios .With such labor intensity, these requirements prohibit using schema-based approaches for real-life stories (e.g., fiction) and only allow the processing of artificially created examples.
 appears rather modest: Our system does not  X  X nderstand X  stories, nor does it retell their plot .Instead, it offers the reader hints X  X mportant information about the story X  X  setting X  X hich should help her guess what type of story is to come .This assumption appears reasonable because it has been shown that comprehension and recall of dis-course are strongly influenced by the reader X  X  familiarity with the type of schema (van
Dijk and Kintsch 1978) .Because our system is tailored to work with classics of the genre, it was our expectation that the gist of the story X  X  setting offered to the reader in the wording of the original would give her an idea about the story X  X  themes and likely plot developments .The results of our experiments appear to back this assumption . than the plot would have a strong influence on the reader X  X  decision to read or not to read the story .The setting of the story, its characters, and style are some of the important factors .Many outstanding literary works differ not so much in plot as in their setting or moral. 3 Our system attempts to capture important elements of the setting explicitly and we expect that some elements of style may be captured implicitly due to the extractive nature of the summaries. 76 3. Identifying Important Entities
During the first stage of summary production the system identifies important entities in stories .Initially, we planned to identify three types of entities: people, locations, and time stamps .During a preliminary exploration of the corpus, we analyzed 14 stories for the presence of surface indicators of characters, locations, and temporal anchors. employed the GATE Gazetteer (Cunningham et al .2002), and only considered entities it recognized automatically.
 of characters (on average, 64 mentions per story, excluding pronouns) .On the other hand, the 14 stories contained only 22 location markers, mostly street names .Four stories had no identifiable location markers .Finally, merely four temporal anchors
Christmas) .These findings support the intuitive idea that short stories revolve around their characters, even if the ultimate goal is to show a larger social phenomenon .They also suggest that looking for time stamps in short stories is unlikely to prove productive, because such information is not included in these texts explicitly .That is why our system does not attempt to identify them.
 to maximize the amount of information available about them .It contains an anaphora resolution module that resolves pronominal and noun phrase anaphoric references to animate entities .The term anaphora , as used in this work, can be explained as a way of mentioning a previously encountered entity without naming it explicitly .Consider
Examples 1a, 1b, and 1c from  X  X  Matter of Mean Elevation X  by O .Henry .The noun phrase Mlle. Giraud from Example 1a is an antecedent and the pronouns her and she from
Example 1c are anaphoric expressions or referents .Example 1c illustrates pronominal anaphora , and Example 1b illustrates noun phrase anaphora .Here the noun phrase the woman is the anaphoric expression which refers to the antecedent Mlle. Giraud from
Example 1a. (1a) John Armstrong ent 1 and Mlle. Giraud ent 2 rode among the Andean peaks, enveloped (1b) To Armstrong ent 1 the woman ent 2 seemed almost a holy thing. (1c) Never yet since her ent 2 rescue had she ent 2 smiled.
 sonal pronouns ( I,me,my,he,his ...) and singular definite noun phrases that denote animate entities (e.g., the man ,butnot men ) .It is implemented in Java, within the GATE framework, using the Connexor Machinese Syntax parser (Tapanainen and J  X  arvinen 1997).
 documents are parsed with the Connexor Machinese Syntax parser .The parsed data are then forwarded to the Gazetteer in GATE, which recognizes nouns denoting locations and persons .The original version of the Gazetteer only recognizes named entities and professions, but we extended it to include 137 common animate nouns such as man, woman, soldier ,or baby .During the next stage, pronominal anaphoric expressions are resolved using an implementation of the algorithm proposed by Lappin and Leass (1994). 5 Subsequently, anaphoric noun phrases are identified using the rules outlined by
Vieira and Poesio (2000) .Finally, anaphoric noun phrases are resolved using a modified version of the Lappin and Leass algorithm, adjusted to finding antecedents of nouns. The implementation is described in detail in Kazantseva (2006).
 labor-intensive .We estimated the performance of the module by manually verifying the results it achieved on two short stories of the training set (Table 1) .The error rates for pronominal anaphora resolution are significantly lower than those for noun phrase anaphora resolution (15.07% vs. 36.84%). This is not unexpected because resolving noun phrase anaphora is known to be a very challenging task (Vieira and Poesio 2000) .The results also reveal that referring to characters by pronouns is much more frequent than by noun phrases X  X n our case, the ratio of pronominal to nominal expressions is almost 4:1 .This suggests that resolving pronominal anaphoric expressions is crucial to summarizing short stories.
 ence of expressions denoting locations .After resolving anaphoric expressions, char-acters central to each story are selected based on normalized frequency counts taking anaphoric expressions into account .The output of this module consists of short stories annotated for the presence of location markers and main character mentions. 4. Selecting Descriptive Sentences Using Aspectual Information 4.1 Linguistic Definition of Aspect
We rely on aspect to select salient sentences that set out the background of a story .In this paper, the term aspect denotes the same concept as what Huddleston and Pullum (2002, page 118) call the situation type .The term refers to  X  X ifferent ways of viewing the internal temporal consistency of a situation X  (Comrie 1976, page 3) .Informally, the aspect of a clause suggests the temporal flow of an event or a state and the speaker X  X  position with respect to it.
 in Figure 4, with examples for each type. 78 time and consist of successive phases (Vendler 1967, page 99) .For instance, an event of writing an essay consists of writing separate words, correcting, pausing between words, and so on .A state of understanding each other, on the other hand, does not imply such compositionality: It remains unchanged throughout the whole period when it is true.
In other words, the meaning of events exhibits a dynamic component, whereas that of states does not.
 or occurs momentarily .The latter type of events are referred to as achievements ,and events that imply duration are known as processes .For example, the nature of events such as dropping, stumbling, or recognizing is that they occur instantaneously and, therefore, are achievements .On the other hand, events such as playing golf or writing an essay last for some time, so they are processes.
 a situation implies an ending (Vendler 1967, page 100) .This property is known as telicity .Reading a book in the context of Example 2a implies that the person finished reading it: the overall situation is telic .We cannot say that she has read the book in the first 15 minutes of doing so because the implied ending was not achieved (i.e., the book has not been read) .Such situations are referred to as accomplishments .On the other hand, playing golf or talking on the phone does not imply that the process must end with a specific conclusion and the situation is atelic .Such situations are called activities . ple, repetitions .Consider Examples 2a and 2b . (2a) She read a book. (2b) She read a book a day.
 Example 2b is referred to as a serial situation (Huddleston and Pullum 2002, page 123).
It is considered to be a state, even though a single act of reading a book would constitute an event. ated with descriptions, that is to say, with things that are, or things that were happening for an extended period (consider He was a tall man vs. He opened the window ) .The remainder of Section 4 describes how we identify single and serial stative clauses and use them to construct summaries. 4.2 Overall System Design Several system components are responsible for selecting salient background sentences.
A story, annotated for the presence of important entities (as outlined in Section 3), is parsed with the Connexor Machinese Syntax parser .The sentences are then recursively clause is defined as a main verb as identified by the parser (whether finite or non-finite) with all its complements, including subject, modifiers, and their constituents.
The system offers a choice: a fine-grained or coarse-grained representation .The main difference between the two is in the level of detail at which each clause is represented.
For instance, a fine-grained feature vector has three different features with seven possi-ble values to carry tense-related information: tense , is progressive ,and is perfect , whereas a coarse-grained vector carries only one binary feature, is simple past or present . ity of the representation, one may choose between two different procedures for sentence selection .The first procedure employs machine learning techniques, namely the C5 .0 decision tree induction (Quinlan 1992) .The second procedure applies a set of manually created rules that guide the classification process .Section 4 .3 gives a motivation for fea-tures used in each data set. Sections 5.1 X 5.3 describe the experimental setting. Section 6 presents the results.
 4.3 Feature Selection: Description and Motivation
There are two criteria for the selection of features for both representations: (Criterion 1) a clause should  X  X alk X  about important things, such as characters or (Criterion 2) a clause should contain background descriptions rather then events
We hypothesize that sentences which satisfy both criteria are good candidates for inclu-sion in indicative summaries .In other words, a summary that consists of such sentences would familiarize the reader with important elements of the setting of the story, but would not reveal the plot.
 related and location-related .We have designed character-related features to help iden-tify sentences that focus on characters, not just mention them in passing .These features important character with a salient grammatical function (e.g., subject). Location-related 80 features are intended to help identify sentences where named entities tagged as loca-tions by the Gazetteer indeed refer to location names.
 ground sentences (as opposed to those relating events) and are therefore suitable for inclusion in indicative summaries .To this end, the features that contribute towards
Criterion 2 are designed to identify stative clauses and clauses that describe serial situa-tions .A single unambiguous indicator of aspectual type does not exist, but a number of verb-related characteristics of the clause may signal or limit its possible aspectual type.
These characteristics include the lexical aspect of the main verb, tense, the presence of temporal expressions, voice, and certain properties of the direct object .The verb-related features capture this information in our representation. 7 features and motivates their inclusion .Table 2 shows how many features contribute to each criterion, and how many discrete values they have .Appendix A contains a complete list of features used in both representations, explains how they are computed, and shows the cardinality of the sets of possible values.
 tences are about one of the important characters in the story .So, this group of features describes whether a clause contains a character mention and what its grammatical function is (subject, object, indirect object, or other) .Mentions of characters early in the text tend to contain more salient background information .That is why character-related features reflect the position of a parent sentence character is introduced .In addition, these features capture the presence of a character mention that is premodified by a noun phrase .The interest in such mentions is inspired by the fact that these constructions X  X ppositions X  X ften introduce new entities into the discourse (Vieira and Poesio 2000) .For the same reasons, the system also establishes whether a character mention is nominal or pronominal (e.g., Jack vs. he ), whether it is used in the genitive case (e.g., Jack X  X  ) and, for common nouns, whether the mention is accompanied by an indefinite article.
Gazetteer recognizes as markers of location denote locations .Location-related features help identify mentions of locations in each clause and verify that these mentions indeed denote a place .These features describe whether a clause contains a mention of a location and whether it is embedded in a prepositional phrase .The rationale for these features is that true location mentions are more likely to occur inside prepositional phrases, such as from Chicago or to China .
 that help determine its aspectual type.
 in isolation, without regard to the context provided by a particular clause .Just as for clauses, a verb may be a state (or stative) verb (e.g., believe ), or an event verb (e.g., run ).
Event verbs are further subdivided into verbs of activity (e.g., read ), accomplishment (e.g., take a test ), and achievement (e.g., drop ).
 discussed by Vendler (1967), Dorr and Olsen (1997), and Huddleston and Pullum (2002, pages 118 X 123) .Dorr and Olsen have proposed a privative model of this relation X  X ee
Table 3 .The model states that verbs are categorized into aspectual classes based on whether they exhibit one or more of the following properties: dynamicity, durativity, and telicity .Dorr and Olsen speculate that, depending on the context of usage, verbs may form clauses that have more of these properties than the main verb viewed in isolation, but that it is impossible for a verb to  X  X hed X  one of its properties .We illustrate this in Examples 3a and 3b .In Example 3a the state verb know participates in an accomplishment clause; the clause is telic, although the verb by itself is not .On the other hand, an attempt to deprive the accomplishment verb destroy of its telic meaning when constructing a clause of type activity fails to create an acceptable clause (Example 3b). (3a) He knew it that very moment .(accomplishment) (3b) *He was destroying it for an hour .(activity) 9 system capture this information .The fine-grained data set contains three features with six possible values that show whether the main verb of a clause is durative, dynamic, or telic .The coarse-grained data set contains a single feature with four possible values (the lexical aspect of a verb according to the model in Table 3) .We derive this information from a manually compiled database of Lexical Conceptual Structures (Dorr and Olsen 1997), which contains these properties for 4,432 English verbs. 82 number of constraints on its aspectual type .For instance, simple tenses are more
It is also commonly accepted (Dowty 1979; Huddleston and Pullum 2002, page 119) that stative clauses cannot be realized using progressive tenses (see Examples 4a and 4b) .Huddleston and Pullum (2002, page 121) stipulate that it is also the case with achievement clauses (see Example 4c). (4a) John is running .(event, activity) (4b) *John is knowing the answer .(state) (4c) *John was recognizing her .(event, accomplishment)
Among the constraints that grammatical tense imposes there is the special relation between simple present tense and event clauses .As a rule, clauses realized in simple present tense cannot denote events, but only states (Huddleston and Pullum 2002, page 119) .The matter is illustrated in Examples 5a through 7b . (5a) She knew history well .(state) (5b) She knows history well .(state) (6a) She fell off a chair .(event) (6b) *She falls off a chair .(event) (7a) She danced (last night) .(event) (7b) She dances .(state)
In the fine-grained data set the information related to tense is expressed using three features with seven possible values (whether a clause is in present, past, or future tense; whether it is progressive; and whether it is perfective) .In the coarse-grained data set, this information is expressed using one binary feature: whether a clause is in simple, past, or present tense.
 such as usually, never, suddenly, at that moment , and many others, are widely employed to mark the aspectual type of a sentence (Dowty 1979; Harkness 1987; By 2002) .Such markers provide a wealth of information and often unambiguously signal aspectual type .For example: (8a) She read a lot tonight. (8b) She always read a lot .(or She used to read a lot .)
Such expressions are not easy to capture automatically, however .In order to use the information expressed in temporal adverbials, we analyzed the training part of the corpus for the presence of such expressions .There were 295 occurrences in 10 stories .
It turns out that this set can be reduced to 95 templates .For example, the expressions this year, next year, that long year can all be reduced to a template some expression year .
Possible values of time expression are further restricted to allow only valid modifiers (e.g., last , next ,butnot yellow ) .The system captures temporal expressions using a cas-cade of regular expression .It first identifies the least ambiguous unit (in this example year ) and then attempts to find the boundaries of the expression .The complete list of regular expressions used appears in Kazantseva (2006).
 (location, duration, frequency, or enactment) (Harkness 1987); magnitude (year, day, etc.); and plurality (year vs. years). The fine-grained data set contains three such fea-tures with 14 possible values (type of expression, its magnitude, and plurality) .The coarse-grained data set contains one binary feature (whether a clause contains an ex-pression that denotes a long period of time).
 page 51) .Both data sets contain one binary feature that describes this information . determine whether a given clause is stative or dynamic. (9a) She wrote a book .(event) (9b) She wrote books .(state) determiner and whether it is used in a singular or plural form .Two binary features that describe this information are included in the fine-grained data set.
 a clause and its parent sentence, such as whether these were affirmative statements, exclamations, or questions; their index in the text; and a few others .The fine-grained data set contains three such features with seven possible values .The coarse-grained data set contains three features with four values. 4.4 Handling Clauses with the Verb Have
The preceding section notes that the same verb may form clauses of different aspectual types depending on its context .A verb with a particularly ambiguous aspect is the verb have (when used as the main verb and not an auxiliary) .Its meaning is strongly influenced by what kind of direct object it takes .That is why determining its aspectual type is a very challenging task .This issue is illustrated in Examples 10a X 10c . (10a) She had lunch .(event) . (10b) She had a friend .(state) . (10c) She had an accident .(event) .

Due to the high degree of ambiguity, our system handles clauses with have as the main verb in a manner different from all other clauses .This machinery remains the same regardless of what options are used for the granularity of representation and for sentence selection procedures.
 proach proposed by Siegel (1998a) .The solution relies on WordNet (Fellbaum 1998) and contains a set of rules that determine the aspectual type of a have -clause based on the top WordNet category of the direct object .For instance, the direct object lunch from
Example 11a belongs to the category food and, according to rules from Siegel (1998a), the aspectual type of a clause is event .The direct object friend from Example 11b belongs to the category person , so the aspectual type of the clause is state .Siegel (1998a) used
WordNet 1.6, whereas we work with a newer version, WordNet 2.0. The structure of 84 this newer ontology is different from that of version 1.6. For this reason, we consider all parent categories in the hypernym tree, not only the top category .For the sake of completeness, Figure 5 shows the pseudo-code for this procedure .The system judges a have -clause to be summary-worthy if two conditions are fulfilled: the clause contains a mention of one or more important characters and it is a state clause. 5. Experiments 5.1 Experimental Setting parsed with the Connexor parser and named entities are recognized using the GATE
Gazetteer .Then the system resolves anaphoric references and identifies important char-acters and locations .During the next stage, the summarizer splits all source sentences into clauses and creates coarse-and fine-grained representations for each clause .A clause is modeled as a vector of character-, location-and verb-related features .Finally, the system employs two alternative procedures to select summary-worthy sentences: manually designed rules and machine learning.
 in creating summaries of short stories .The experimental corpus consisted of 47 short stories split into a training set of 27 stories and a test set of 20 stories .The average length of a story in the corpus was 3,333 tokens, 244 sentences, or approximately 4.5 U.S.-letter-manually so that its training and test portions contained approximately an equal num-ber of stories by the same writer .The first author of this paper annotated each clause of every story for summary-worthiness and achieved the compression rate of 6%, counted in sentences .This rate was the target compression rate in all further experiments . summary-worthy and all others as not summary-worthy .The test data set contained 7,890 clauses, 406 of them summary-worthy.
 tify the best settings .Then we ran two sets of experiments on the test portion .In the first set of experiments, we applied a manually designed set of rules that select sen-tences for possible inclusion in summaries .These experiments are described in Sec-tion 5.2. The second set of experiments relied on using machine-learning techniques to create summaries .It is described in Section 5 .3 .After the completion of the experi-ments, the summaries were evaluated by six judges .They were also compared against extractive summaries produced by three people .Section 6 discusses the evaluation procedures in detail and reports the results. 5.2 Experiments with Manually Designed Rules
The first classification procedure applies manually designed rules to a clause-level representation of the original stories to produce descriptive summaries .The rules are designed using the same features as those used for machine learning and described in Section 4.3 and in Appendix A.
 one for the coarse-grained and another for the fine-grained representation .The rules operate at clause level .If a clause is deemed summary-worthy, the complete parent sentence is included in the summary .Figure 6 displays a few examples of rules for the fine-grained data set (a clause is considered to be summary-worthy if a rule returns
True ) .The first rule attempts to select clauses that talk about one of the main characters and contain temporal expressions of type enactment .Therationaleforthisruleisthat is signaled by the main stative verb .The third rule rejects clauses in progressive tense because such clauses are unlikely to contain background information.
 coarse-grained representation function differently .Each clause is assigned a score based on the values of its features .The system then selects 6% of sentences that contain clauses with the highest scores .The scores attributed to the particular feature values were assigned and fine-tuned manually using linguistic knowledge described in Section 4.3.
The reasons why the procedures for the two data sets differ are as follows .Assigning and fine-tuning the scores is a more flexible process and it is easier to perform manu-ally .Ideally, we would apply score-based rules to both representations, but assigning and fine-tuning the scores manually for the fine-grained data set is excessively labor-intensive: there are too many features with too many values .For instance, one may want to reward clauses in simple past or present tenses, reflecting the fact that such
This information is expressed in the coarse-grained data set using one binary feature simple past present and fine-tuning the score is trivial .On the other hand, the same 86 information in the fine-grained data set is distributed over three features with a total Distributing the  X  X eward X  among three independent features is far less obvious. representation, were selected and fine-tuned empirically using the training portion of the corpus as a guide .Once the parameters had been adjusted, the system produced two sets of summaries for the test portion of the corpus (one for each representation).
Figures 7 and 8 show the rationale for the algorithms .The interested reader is referred to Kazantseva (2006) for pseudo-code. 5.3 Experiments with Machine Learning
As an alternative to rule construction, in the second set of experiments we performed decision tree induction with C5.0 (Quinlan 1992) to select salient descriptive sentences. C5.0 was our choice mainly because of the readability of its output.
 of all annotated clauses belonged to the positive class) .Because the corpus was rather small, we applied a number of techniques to correct class imbalance in the training data set .These techniques included classification costs, undersampling (randomly removing instances of the majority class), oversampling (randomly duplicating instances of the minority class), and synthetic example generation (Chawlar et al .2002) .Using tenfold cross-validation on the training data set and original annotations by the first author, we selected the best class-imbalance correction techniques for each representation and also fine-tuned the learning parameters available in C5.0. These experiments brought the best results when using classification costs for the coarse-grained data set and undersampling for the fine-grained data set.
 a small experiment .We removed one feature at a time from the training set and used the decrease in F-score as a measure of informativeness .The experiment showed that in the coarse-grained data set the following features were the most informative: the presence of a character in a clause, the difference between the index of the current sentence and the sentence where the character was first mentioned, syntactic function of a character mention, index of the sentence, and tense .In the fine-grained data set the findings are similar: the index of the sentence, whether a character mention is a subject, the presence of a character mention in the clause, and whether the character mention is a pronoun are more important than the other features.
 validation, the system produced two sets of summaries for the test data set. 6. Evaluation 6.1 Overview We are not aware of any agreed-upon metrics for evaluating summaries of short fiction.
In fact, it is not wholly clear what makes one summary better than another even for manual ones .That is why we evaluate our summaries using a variety of metrics and baselines, hoping to obtain a stereoscopic view of their quality.
 fulness of the summaries .It is designed so as to compare the machine-made summaries with a random baseline and also with a  X  X eiling X  X  manual abstracts (henceforth model summaries ) .To achieve this, we engaged 15 evaluators to read the summaries and
Such experimental design allowed us to evaluate extrinsically the informativeness of the summaries and intrinsically their usefulness .Both types of questions were asked first after reading the summary alone and then after reading the complete story .The summaries are an anonymous mix of random, machine-made, and model ones (i.e., the evaluators did not know whether the summaries were produced by programs or by people) .Section 6 .2 describes the experiment in detail .
 sentence co-selection with the manually created extracts .It was designed to allow the comparison of machine-made summaries with two naive baselines and with two state-of-the-art generic summarizers (baseline summarizers) .Section 6 .3 contains the description of this experiment.
 summaries with the manually created abstracts using ROUGE (Lin 2004) X  X  package for automatically evaluating summaries .This experiment is described in Section 6 .4 . 6.2 Evaluating Informativeness and Usefulness of the Summaries
We define the objectives of this experiment as measuring the informativeness, the usefulness and, to some extent, the linguistic quality of the summaries that our system 88 produces .The informativeness is measured indirectly X  X y asking people factual questions about the story .The linguistic quality and the usefulness are evaluated intrinsically X  X y asking people to rank specific characteristics of the summaries.
Fifteen unbiased evaluators answer both types of questions twice, first after reading the summary alone and then again after reading the complete story X  X epeating the procedure for all 20 test stories .Asking the questions after reading the summary alone measures the informativeness and the usefulness of the summaries in a realistic situation: To an evaluator, the summary is the only source of information about the original story .Repeating the procedure after reading the complete story evaluates the summaries in a situation where the evaluator has the complete information about the source .Each evaluator works with a mix of machine-made, random, and model summaries with six or seven summaries of each kind .This allows comparing the performance of our summarizer with a baseline and a ceiling.
 of the process prohibits asking the subjects to evaluate all four summary types .That is also why it is not possible to use more than one baseline or the summaries created by the baseline systems. 11 Restricted to evaluating only one type of the machine-made summaries, we opt for the coarse-grained rule-based ones, mainly because the coarse-grained representation and the rules make it easier to trace why the system selects specific sentences.
 of the summaries.

Evaluating these facets of the summaries reveals whether we achieve the objective of producing informative summaries .The focus of the system was not on readability .Still, we evaluate how readable the summaries are, because severe lack of coherence may prevent people from correctly interpreting the available information .We have provided and 5.

The baseline consists of randomly selected sentences .Both the machine-made and the random summaries contain the same number of sentences .The ceiling consists of the summaries written by two human subjects .The summary writers were instructed to write 20 summaries of short stories in a way that does not reveal all of the plot .They received one example summary and were allowed to reuse sentences from the stories, to employ metaphor and any other literary devices they found useful.
 and the location of the story in their own words .The first author rated the answers on the scale of  X  1 to 3 .A score of 3 means that the answer is complete and correct, 2 = slightly incomplete, 1 = very incomplete, 0 = the subject cannot find the answer in the text, and  X  1 = the answer is incorrect .The question asking to identify the time frame of the story is a multiple-choice one: select the century when the story takes place. 0 if it is not or if the subject cannot infer time from the text) .We calculate the mean answers for each question and compare them across summary types using the Kruskal X  Wallis test and the Mann X  X hitney test (also known as the Wilcoxon Rank X  X um test).
The tests are appropriate when the response variable is ordinal and the dependent points.
 samples come from the same population .It is based on calculating the K statistic which follows  X  2 distribution for sample sizes of five or larger .Given i samples containing t data points each with R i being the sum of ranks of all data points in sample t calculated as follows (Leach 1979, page 150): of the difference, we rely on the Mann X  X hitney test .The test is based on calculating the
S statistic .For large sample sizes the distribution of S can be approximated using the normal distribution.
 where t 2 is the size of the smaller sample, n is the size of both samples together, and R is the sum of ranks in the smaller sample .We use the Kruskal X  X allis test with 0 .01 con-fidence level .In order to avoid increasing the chance of Type I error when performing pairwise comparisons, we set per-comparison confidence level for the Mann X  X hitney test at  X  =  X / c where  X  is the desired per-experiment confidence level and c is the number of comparisons (Leach 1979, page 161) .In our case  X  = 0 . 0033.
 score of 1 to 6, with 1 indicating a strong negative property and 6 indicating a strong to avoid the evaluators X  giving excessive preference to the middle rank .We measure the mean ranks for each question and compare them across summary types using the
Kruskal X  X allis and Mann X  X hitney tests .The inter-annotator agreement is computed 90 using Krippendorff X  X   X  (Krippendorff 2004, pp .221 X 236) (henceforth  X  ).  X  measures disagreement between annotators corrected for chance disagreement.
Unlike other coefficients of inter-coder agreement,  X  allows taking into account the magnitude of disagreement by specifying a distance metric  X  in our case: a situation when raters disagree whether to give a summary a rank of 1 or 6 should be penalized more heavily than a situation when they do not agree between the ranks of 5 and 6 .When computing  X  , we use the distance metric suggested by
Krippendorff for ordinal data (Krippendorff 2004, page 223): where c and k , c &lt; k , are the two ranks.
 300, n = 15, and c = 6, where N is the total number of items (i.e., summary X  X tory pairs ranked), n is the number of raters, and c is the number of available categories. tising at the Department of Linguistics at the University of Ottawa .Most of them are third-and fourth-year undergraduate students of linguistics .The only requirement for participation was to be a native speaker of English .We hired two people to create model summaries for the 20 stories of the test set .The summary writers worked approximately 15 X 20 hours each. Fifteen people were hired to evaluate the summaries (i.e., to read the summary X  X tory pairs and answer the questions) .The task of evaluating a sum-mary required approximately 12 X 15 hours of labor per person .All participants were paid .The instructions for summary writers are available at www.site.uottawa.ca/ ankazant/instructions-writers.zip .The instructions for evaluators can be found at www.site.uottawa.ca/  X  ankazant/instructions eval.zip .
 sought to measure the informativeness of the summary, every evaluator worked on 20 distinct stories of the test set and no one worked with the same story more than once.
The summaries were a randomly selected mix of random, machine-made, and model summaries.
 the test set of stories and the instructions and had seven working days to submit their abstracts .A week later, we sent randomly generated packages of summary X  X tory pairs to the evaluators .The packages contained between six and seven summaries of each kind (random, machine-made, and model) .Each evaluator worked with exactly one summary for each story, reading a total of 20 pairs .Every summary was evaluated by five subjects .The evaluators had seven working days to complete the task .
 swers between the machine-made, the baseline, and the model summaries using the
Kruskal X  X allis and Mann X  X hitney tests .The column Groups shows homogeneous groups, identified using the Mann X  X hitney test with 99.67% confidence (recall that per-comparison confidence level  X  = 0 . 0033) .The groups are denoted using distinct literals (e.g., A, B, C ). shows that in these respects the machine-made summaries are X  X ather predictably X  consistently more informative than the random ones .The difference between the machine-made and the random summaries is not statistically significant for the question asking to name the time of the story .Keeping in mind how rare absolute temporal anchors are in short stories, this is not surprising .The manual summaries, however, are ranked higher with statistical significance .This may suggest that the machine-made summaries are not as coherent as the model ones, which prevents the reader from finding implicit cues about timeframe available in the summaries.
 cant for the questions about the time and the place of the story, but not for the questions about the main characters .This suggests that the machine-made summaries are almost as informative as the model ones when it comes to informing the reader whom the story is about .They cannot, however, give the reader as good an idea about the time and the place of the story as the model summaries can.
 ences between answers obtained after reading the summary alone and after reading the complete story are significant in all cases.
 types, along with the homogeneous groups identified using the Mann X  X hitney test, with 99.67% confidence. The request to rank readability was made only once X  X fter reading the summary; the request to evaluate the completeness was made only after reading the complete story .(The corresponding cells in Table 5 are empty .) best, the machine-made summaries as second-best, and the random ones as worst .The differences between summary types are significant in all cases.
 average (3.28 on the scale of 1 to 6). For all other questions (relevance, completeness, and 92 usefulness), the machine-made summaries are ranked as slightly worse than average (around 2.81). This shows that even though the summaries are somewhat useful and consistently outperform the random baseline, they fall short of the quality of the manual abstracts .This is hardly surprising given the inherent difficulty of summarizing fiction and the exploratory nature of this work .It is worth remarking that even the model summaries do not appear to be perfect: The evaluators ranked them around 5.20, even though they had significantly worse summaries to compare against .This may suggest that the task is not easy even for people, let alone for a computer program. which the evaluators agree when answering the subjective questions. is measured using Krippendorff X  X   X  .The results show substantial agreement but fall reach such high agreement is hardly surprising: the task of ranking the quality of the summaries is highly subjective .Instead of asking the subjects to bin items into continuous in nature: the quality of a summary .That is why we interpret the level of agreement as sufficient for the purpose of evaluating the quality of the summaries. 6.3 Comparing the Machine-Made Summaries and the Manually Created Extracts
Measuring sentence co-selection between extractive summaries created by humans and those created by automatic summarizers has a long tradition in the text summarization community (Lin and Hovy 2000; Marcu 2000), but this family of measures has a number of well-known shortcomings .As many have remarked on previous occasions (Mani 2001; Radev et al .2003), co-selection measures do not provide a complete assessment of the quality of a summary .First of all, when a summary in question contains sentences that do not appear in any of the model extracts, one may not be sure that those sentences are uninformative or inappropriate for inclusion in a summary .In addition, documents have internal discourse structure and sentences are often inter-dependent .Therefore, even if a summary contains sentences found in one or more reference summaries, it does not always mean that it is advisable to include those sentences in the summary in question.
 however, measure a quality that is objective and easy to pin down: how many sentences that humans judge summary-worthy are included in the machine-made summary.
Such a metric is a useful complement to the results reported in Section 6.2. It has the advantage of being easy to interpret and comprehend .It also has a long tradition of usage which allows us to compare our summarizer X  X n a familiar scale X  X ith other summarization systems .That is why we chose co-selection as the basis for comparing the summaries that our system produces with manually created extracts.
Each annotator is asked to read 10 short stories and to select 6% of sentences that, in their opinion, constitute a good indicative summary .In this manner three people annotate each story of the test set for summary-worthy sentences .We used their annotations as a gold standard and compared the machine-made summaries against them .In addition, we used the same gold standard and metrics to evaluate the quality of two baseline summaries and of two summaries produced by state-of-the art summarization systems. many sentences found in our system X  X  summaries and the baseline summaries occur in the extractive summaries created by the human annotators .We are also interested in finding out whether our summarizer outperforms the trivial baseline algorithms and the existing state-of-the-art summarizers fine-tuned to summarizing newswire. baselines and a ceiling .Intuitively, when a person wishes to decide whether to read a book, she opens it and flips through several pages at the beginning .Imitating this process, we computed a simple lead baseline consisting of the first 6% of the sentences in a story .The second baseline consists of 6% of sentences of the story selected at random .
The ceiling consists of all sentences deemed summary-worthy by one of the human annotators. 94 ments over the existing generic state-of-the-art systems put to work on fiction .To this end, we compared our summarizer with two systems that were top performers in the
Document Understanding Conference (henceforth DUC) 2007, the annual  X  X ompeti-tion X  for automatic summarizers .In DUC competitions the summarization systems are evaluated on a variety of metrics: manually assigned scores (ranking readability, grammaticality, non-redundancy, referential clarity, focus, and coherence), the pyramid method (Nenkova and Passonneau 2004), and ROUGE scores (Lin 2004) .There is no unified ranking of the systems X  performance, and selecting the best summarizer is not straightforward .We chose two systems among the top performers in DUC 2007 X 
GISTexter (Harabagiu, Hickl, and Lacatusu 2007) and CLASSY (Schlesinger, O X  X eary, and Conroy 2008; Conroy, Schlesinger, and O X  X eary 2007) .GISTexter appears to be the best summarizer according to the scores assigned by the human judges .Apart from baselines, it is consistently ranked as the best or the second-best system on most characteristics evaluated by the judges (the only exception is non-redundancy where
GISTexter is ranked eighth) .CLASSY, on the other hand, is one of the four top systems according to ROUGE scores .The scores it received from the human judges are also quite good.
 tion of newswire articles on a specific topic .Each input collection was accompanied by a topic statement that briefly explained what the summaries should cover .Therefore, both CLASSY and GISTexter are geared towards multi-document query-based sum-marization of newswire X  X  task dramatically different from that of summarizing short fiction. 14 No adjustments were made to either system to make them more suitable for summarizing stories .Therefore, the comparison is not wholly fair, but X  X n the absence of systems similar to ours X  X t was the only possibility to compare our summarizer with the state-of-the-art in the community and to verify whether genre-specific methods are useful in summarizing fiction.
 ways we create three distinct gold-standard summaries .The majority gold-standard summary contains all sentences selected by at least two judges .It is best suited to give an overall picture of how similar the machine-made summaries are to the man-made ones .The union gold standard is obtained by considering all sentences that are judged summary-worthy by at least one judge .Union summaries provide a more relaxed measurement .Precision computed on the basis of the union gold standard gives an idea of how many irrelevant sentences a given summary contains (sentences not selected by any of the three judges are more likely to prove irrelevant) .The intersection summaries are obtained by combining sentences that all three judges deemed to be important.
Recall measured on the basis of the intersection gold standard says how many of the most important sentences are included in summaries produced by the system (sentences selected by all three judges are likely to be the most important ones) .All summaries are compared against each gold-standard summary using precision (P), recall (R), and equally-weighted F-score (F).

TP denotes true positives, FP = false positives, TN = true negatives, and FN = false negatives.
 is established using the one-way Analysis Of Variance test (ANOVA) and the Tukey Honestly Significant Differences test (henceforth Tukey HSD ).
 comparing variance between samples with total variance within samples: where p is the number of samples, n is the total number of observations, S sum of squared deviations between sample means and the total mean, and S the total sum of squared deviations within samples .The f statistic is distributed as F when the null hypothesis is true (i.e., the differences between sample means are not significant).
 sample means are significant overall, but the test does not say anything about differ-ences between particular pairs of sample means .Tukey HSD is a test that does just that .
It measures q , the studentized range statistic: where M l is the larger of the sample means, M s is the smaller one, and SE is the standard error of the data in question .In accordance with our interpretation of the three types of gold standards, we use the most meaningful measurement for each standard: F-score for the majority, precision for the union, and recall for the intersection .We also use the same measurement to set the ceiling for each standard (i.e., by choosing the manually created extract that compares best on that scale).
 maries, we measure how well these people agree among themselves .We estimate the agreement using Scott X  X   X  (Scott 1955). 15 This coefficient measures the observed agreement between judges corrected for chance agreement. 96 categories in set K , c is the number of coders, n ik is the number of coders who assign item i to category k ,and n k is the total number of items assigned to category k by all annotators (Artstein and Poesio 2008, pp .562 X 563) .
 presence of summary-worthy sentences .These people are colleagues and acquaintances of the first author .At the time of the experiment none of them was familiar with the design of the system .Four annotators are native speakers of English and the remaining two have a very good command of the language.

Three annotators created extractive summaries for each story .In addition, there were eight distinct automatically produced summaries per story: four summaries produced by our system, two baseline summaries, and two summaries created by the baseline systems from DUC.
 stories and had two weeks to annotate them .The participants reported having taken 10 X 20 hours to complete the task.
 group and with the first author of this article .The agreement with the first author is reported because she created the initial training and test data for experiments .The num-bers 3 and 4 state whether the statistic is computed only for three subjects participating in the evaluation or for four subjects (including the first author) .As can be seen from
Table 6, the agreement statistics are computed for each group separately .This is because the sets of stories that they annotated are disjoint .The  X  X verage X  column shows an average of these figures, to give a better overall idea.
 point specified by Krippendorff (2004) .On a less demanding scale, Landis and Koch (1977) interpret values in the range of 0.21 X 0.4 as fair agreement and in the range of 0.41 X 0.6 as moderate agreement. 16 Weak agreement is not surprising: Many researchers report that people do not agree well on what sentences constitute a good summary of a document (Rath, Resnick, and Savage 1961; Salton et al .1997; Lin and Hovy 2003) .In most cases the agreement corresponding to  X  of 0.42 would not be sufficient for creating a resource, but we interpret this level of agreement as acceptable for evaluating a single facet of the summaries that are also evaluated in other ways.
 computer-made summaries against the gold-standard summaries produced by people.
In each table, the entry HUMAN corresponds to the summaries created by the annotator who achieves the highest scores for the corresponding standard .The  X  X roups ( metric ) X  column reports homogeneous groups identified using Tukey HSD with 95% confidence for the specified metric.
 but it always falls short of the performance of the best human summary .The improve-ment margins between the random and the baseline systems X  summaries and those produced by our system are rather wide .The weaker performance of the baseline summarizers strongly suggests the need for genre-specific methods when summarizing short fiction.
 statistically significant, yet they are much narrower .We interpret this as an indication that the lead baseline is more demanding than the random one when creating indicative summaries of short fiction.
 98 blance to manual ones .There is no straightforward way to interpret these results as good or bad in the context of other summarization systems .Firstly, the task is new and no comparable results exist .Secondly, even though sentence co-selection metrics have been widely used for evaluating summaries of other genres, different compression rates, different gold standards, and availability of naturally occurring competitive baselines (e.g., lead baseline in newswire summarization) make fair comparison difficult. For example, Marcu (2000, page 214) reports achieving F-score of 76.04 when creating sum-maries of newswire articles at 10% of their original length .The lead baseline achieves
F-score of 71.89. When summarizing dialogues, Zechner (2002, page 479) reports weighted accuracy of 0.614 compared to the lead baseline X  X  performance of 0.438 (the numbers are averages over five different summary sizes of 5%, 10%, 15%, 20%, and 25%) .In this context we interpret the results in Tables 7 X 9 as suggesting that our genre-specific system outperforms the naive baselines and two generic summarizers. 6.4 Evaluating Summaries using Lexical Overlap
ROUGE (Lin 2004) is a package for automatically evaluating summaries .Given one or more gold-standard summaries (usually written by people), ROUGE offers several metrics for evaluating the summary in question .The metrics reward lexical overlap between the model summaries and the candidate one .Depending on the metric, the lexical units taken into consideration are n -grams, word sequences, and word pairs. tomatic summarizers at DUC .Following this tradition, we ran ROUGE to evaluate our summaries and to compare them to the baselines (including CLASSY and GISTexter). much lexical overlap exists between the machine-made and the model summaries .We achieved this by computing ROUGE-2 and ROUGE-SU4 scores. 17 by our summarizer, the lead and the random baselines, and the summaries created by
GISTexter and CLASSY .In addition, we included a ceiling by computing ROUGE scores for the model summaries.
 and the candidate one .It is computed according to the following formula: where S is the set of reference summaries, b is a bigram in the reference summary s ,
Count match ( b ) is the number of bigrams that both summaries share, and Count ( b )isthe total number of bigrams in the reference summary s .
 skip-bigrams they have in common .A skip-bigram is any pair of words in a sentence, allowing for arbitrary gaps.
 where X is the reference summary of length m , Y is the candidate summary, SKIP2(X,Y) is the number of skip-bigram matches between X and Y ,and C is the combination function.
 The maximum gap allowed by skip-bigrams is 4 (hence SU4).
 humans we implemented the following leave-one-out procedure .At first, we computed
ROUGE scores by comparing all automatically produced summaries (i.e., those created by our system and the baseline ones) and one of the model summaries against the second available model summary .Next, the procedure was repeated but the model summaries were switched .The significance of the differences was tested using ANOVA and Tukey HSD for 95% confidence level .When calculating ANOVA and Tukey HSD, we used the scores obtained from both runs.
 cally produced and model summaries .The results are inconclusive .
 the rest with 95% confidence are the randomly generated ones .The scores of all other summaries are too close to reject the hypothesis that the differences are due to chance.
This is the case even with the differences between the model and the automatically produced summaries .A possible interpretation could be that all summaries are of very high quality that is indistinguishable from that of the model summaries .This hypothesis, however, can be easily dismissed: The results reported in Sections 6.2 and 6.3 clearly show that the quality of the summaries produced by our system is well below the ceiling.
 distinct groups of summaries .Group A includes the rule-based fine-grained summaries and those produced by CLASSY .The second group includes the lead baseline, three types of summaries created by our summarizer, the model summaries, and those cre-ated by GISTexter .The last group contains the random and the lead baselines .Even though ROUGE-SU4 measurement seems to have more discriminative power, it is at least puzzling that it cannot distinguish between the model and the automatically 100 produced summaries .In particular, placing the rule-based coarse-grained summaries and the model ones in the same group directly contradicts the results reported in
Section 6.2 X  X hat people find the model summaries far superior to this particular type of summary produced by our summarizer.
 are not well suited for evaluating indicative summaries of short stories .An explana-tion could be that when people summarize fiction X  X ather than newswire or scientific papers X  X hey seem to use fewer sentences and clauses verbatim and, by and large, in-troduce more generalization and abstraction .(We have made this informal observation when processing the model summaries used in this experiment.) This results in little lexical overlap with the source text and hence with extractive summaries of any flavor.
This hypothesis, however, is only preliminary and requires further investigation. 7. Conclusions
We presented an approach to summarizing literary short stories .The text sum-marization community has not yet seriously explored this genre, except for early sem-inal work on story understanding .In contrast with the story-understanding systems proposed in the 1970s and 1980s, our system does not require labor-intensive semantic resources X  X nowledge-bases and schemas X  X nd it works on real-life stories, namely, short fiction.
 readers form adequate expectations about the original story .We have demonstrated that such summaries can be produced without deep semantic resources, only relying on syntax and the information about important entities in the story .According to the judges who evaluated the summaries, our summaries are somewhat useful for their original purpose, even if their quality falls far short of the quality of manual abstracts.
Our summaries appear better than the naive baselines and than two state-of-the-art summarizers fine-tuned for working with newswire.
 summarization of short stories .First of all, we confirmed informally that characters tend to be a central element of short fiction .Character mentions provide a wealth of information that can be leveraged in automatic summarization .This finding was also reflected in the approach proposed by Lehnert (1982) .In addition, it appears that features in Section 5.3. Besides, relatively high performance of the lead baselines also lesserrolethaninmorestructureddocuments.
 rizing, and otherwise processing fiction available electronically .The current system accomplishes with some success a limited task of producing indicative summaries or short stories, but much more work is needed to create high-quality flexible summaries of literary works suitable for more than one purpose .Perhaps the most obvious extension to the current system would be summarizing the plot of short stories .Although this is not useful given our original criterion (forming adequate expectations about the story, without  X  X poilers X ), the ability to handle plot would allow the creation of different types of summaries .We also hope to explore the possibility of establishing structure within stories: Knowing that certain portions of a story lay out the setting while others describe events or the culmination would be a significant step towards better summarization.
 needs to be considered .We have concentrated thus far on summary production rather than on establishing the criteria that define the quality of the summary .Evaluation of summaries remains an issue even where well-structured factual documents are summary: The facts, for instance, are likely to be less important than in scientific papers or news items .Other candidate qualities may include closeness to the language or the tone of the original story, the information about the author, the time period, or ideology behind a certain work of fiction .This remains an open question, the answer to which may well lie outside the field of computational linguistics. 102 Appendix A: Features Used in the Coarse-and the Fine-Grained Clause Representations
The appendix lists features computed to represent a clause in the fine-grained data set (Table 12) and in the coarse-grained data set (Table 13) .Prior to constructing feature vectors, the stories are parsed with the Connexor Machinese Parser .All syntactic infor-mation is computed on the basis of the parser output .The  X  X ategory X  column shows other ( O ) .LCS refers to the database of Lexical Conceptual Structures (Dorr and Olsen 1997).
 104 Acknowledgments 106 108
