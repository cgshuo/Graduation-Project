
Emre Neftci 1 , Elisabetta Chicca 1 , Giacomo Indiveri 1 , Jean-Jacques Slotine 2 , Rodney Douglas 1 Cortical neural networks are characterized by a large degree of recurrent excitatory connectivity, and local inhibitory connections. This type of connectivity among neurons is remarkably similar, across all areas in the cortex [1]. It has been argued that a good candidate model for a canonical micro-circuit, potentially used as a general purpose cortical computational unit in the cortices, is the soft Winner-Take-All (WTA) circuit [1], or the more general class of Cooperative Competitive Networks [2] (CCN). A CCN is a set of interacting neurons, in which cooperation is achieved by lo-cal recurrent excitatory connections and competition is achieved via a group of inhibitory neurons, driven by the excitatory neurons and inhibiting them (see Figure 1). As a result, CCNs perform both common linear operations as well as complex non X  X inear operations. The linear operations include analog gain (linear amplification of the feed X  X orward input, mediated by the recurrent ex-citation and/or common mode input), and locus invariance [3]. The non X  X inear operations include non X  X inear selection or soft winner X  X ake X  X ll (WTA) behavior [2, 4, 5], signal restoration [4, 6], and multi X  X tability [2, 5]. CCN networks can be modeled using linear threshold units, as well as re-current networks of spiking neurons. The latter can be efficiently implemented in silicon using Integrate X  X nd X  X ire (I&amp;F) neurons and dynamic synapses [7]. In this work we use a prototype VLSI CCN device, comprising 128 low power I&amp;F neurons [8] and 4096 dynamic synapses [9] that op-erate in real-time, in a massively parallel fashion. The main goal of this paper is to address the open question of how to determine network parameters, such as the strength of recurrent excitatory couplings or global inhibitory couplings, to create well X  X ehaving complex networks composed of combinations of neural computational modules (such as CCNs) as depicted in Figure 1. The theoret-ical foundations used to address these problems are based on contraction theory [10]. By applying this theory to CCN models of linear threshold units and to combinations of them we find upper bounds to contraction conditions. We then test the theoretical results on the VLSI CCN of spiking neurons, and on a combination of two mutually coupled CCNs. We show how the experimental data presented are consistent with the theoretical predictions. Figure 1: CCNs and combinations of CCNs . (a) A CCN consisting of a population of nearest neigh-bor connected excitatory neurons (blue) receiving external input and an inhibitory neuron which receives input from all the excitatory neurons and inhibits them back (red). (b) Photo of the VLSI CCN Chip comprising I&amp;F neurons. (c) Three coupled CCNs, showing examples of connectivity patterns between them. Neural network models of linear threshold units (LTUs) ignore many of the non X  X inear processes that occur at the synaptic level and contain, by definition, no information about spike timing. How-ever networks of LTUs can functionally behave as networks of I&amp;F neurons in a wide variety of cases [11]. Similarly boundary conditions found for LTU networks can be often applied also to their I&amp;F neuron network counterparts. For this reason, we start by analyzing a network of LTUs whose structure is analogous to the one of the VLSI CCN of I&amp;F neurons, and derive sufficient boundary conditions for contraction.
 If we consider a CCN of recurrently connected LTUs according to a weight matrix W , as shown on Figure 1, we can express the network dynamics as: Where N is the total number of neurons in the system, the function g ( x ) = max ( x , 0 ) is a half X  X ave applied to the neurons and  X  i are the time constants of the neurons. We assume that neurons of each type (i.e. excitatory or inhibitory) have identical dynamics: we denote the time constant of excitatory neurons with  X  ex and the one of inhibitory neurons with  X  ih . Throughout the paper, we will use the excitation respectively, and w ie , w ei for inhibitory to excitatory neuron and vice versa. The W matrix has the following shape: A CCN can be used to implement a WTA computation. Depending on the strength of the connec-tions, a CCN can implement a Hard (HWTA) or Soft (SWTA) WTA. A HWTA implements a max operation or selection mechanism: only the neuron receiving the strongest input can be active and all other neurons are suppressed by global inhibition. A SWTA implements more complex operation such as non X  X inear selection, signal restoration, and multi X  X tability: one or several groups of neu-rons can be active at the same time, neurons belonging to the same group cooperate through local excitation, different groups compete through global inhibition. The activity of the  X  X inning X  group of neurons can be amplified while other groups are suppressed. Depending on the strength of in-hibitory and excitatory couplings different regimes are observed. Specifically, in Sec. 4 we compare a weakly coupled configuration, which guarantees contraction, with a strongly coupled configura-tion in which the output of the network depends on the input and the history, showing hysteretic (non X  X ontracting) behaviors in which the selected  X  X inning X  group has advantages over other group of neurons because of the recurrent excitation. 3.1 Contraction of a single network A formal analysis of contraction theory applied to non X  X inear systems has been described in [10,12]. Here we present an overview of the theory applied to the system of Eq. (1).
 In a contracting system, all the trajectories converge to a single trajectory exponentially fast inde-pendent of the initial conditions. In particular, if the system has a steady state solution then, by definition, the state will contract and converge to that solution exponentially fast. Formally, the sys-tem is contracting if d d t k  X  x k is uniformally negative (i.e. negative in the entire state space) where  X  x corresponds to the distance between two neighboring trajectories at a given time. In fact, by path integration, we have d d t R P 1 P neighboring). This leads to the following theorem: said to be contracting if all its trajectories converge exponentially to a single trajectory. A sufficient condition is that the symmetric part of the Jacobian J =  X   X  x f is uniformly negative definite . This condition can be written more explicitly as where I is the identity matrix and J s is the symmetric part of J It is equivalent to J s having all its eigenvalues uniformly negative [13].
 We can define more generally a local coordinate transformation  X  z =  X   X  x , where  X  ( x , t ) is a square matrix, such that M ( x , t ) =  X  T  X  is a uniformly positive definite, symmetric and continuously dif-ferentiable metric. Note that the coordinate system z ( x , t ) does not need to exist, and will not in the general case, but  X  z and  X  z  X   X  z can always be defined [14]. Then, in this metric one can compute the generalized Jacobian F = ( d d t  X  +  X  J )  X   X  1 . If the symmetric part of the generalized Jacobian, F , is negative definite then the system is contracting. In a suitable metric it has been shown that this condition becomes sufficient and necessary [10]. In particular, if  X  is constant, F s is negative 0  X   X   X  1 v  X  R N . Consequently, we can always choose a constant M to simplify our equations. Let us now see under which conditions the system defined by Eq. (1) is contracting. Except for the rectification non X  X inearity, the full system is a linear time X  X nvariant (LTI) system, and it has a fixed point [15]. A common alternative to the half-wave rectification function is the sigmoid, in which case the Jacobian becomes differentiable. If we define f i ( x , t ) as then the Jacobian matrix is given by J i j =  X   X  x and  X  i is the time constant of neuron i , with  X  i =  X  ex for the excitatory neurons and  X  i =  X  ih for the inhibitory ones. We assume that the w ei and w ie weights are not zero so we can use the constant metric: which is positive definite. With this metric, MJ can be written MJ =  X  I + DK , where D ij = g derivative are both bounded), we can then use the method proposed in [16] to determine a sufficient condition for contraction. This leads to a condition of the form  X  max &lt; 0, where A graphical representation of the boundaries defined by this contraction condition is provided in Figure 2. The term |  X  max | is called the contraction rate with respect to metric M . It is of particular interest because it is a lower bound for the rate at which the system converges to its solution in that metric. Figure 2: Qualitative phase diagram for a single CCN of LTUs . We show here the possible regimes of the given in Eq. (1) as a function of excitation and inhibition. In the region D the rates would grow without bounds if there were no refractory period for the neurons. We see that a system which is unstable without inhibition cannot be in region A ( i.e. within the boundaries of Eq. (5)). Note, however, that we do not quantitatively know the boundaries between B and C and between C and D 3.2 Contraction of feed X  X ack combined CCNs One of the powerful features of contraction theory is the following: if a complex system is composed of coupled (feed X  X orward and feed X  X ack) subsystems that are individually contracting, then it is possible to find a sufficient condition for contraction without computing the system X  X  full Jacobian. In addition it is possible to compute a lower bound for the full system X  X  contraction rate. Let F s be the symmetric part of the Jacobian of two bi X  X irectionally coupled subsystems, with symmetric feed X  X ack couplings. Then F s can be written with four blocks of matrices: where F 1 s and F 2 s refer to the Jacobian of the individual, decoupled subsystems, while G and G  X  are the feed X  X ack coupling components. If we assume both subsystems are contracting, then a sufficient condition for contraction of the overall system is given by [17]: value of G  X  G . By the eigenvalue interlacing theorem [13] we have that the contraction rate of the combined system is given by  X  max ( F s )  X  min i  X  ( F is ) i = 1 , 2.
 For the specific example of a combined system comprising two identical subsystems coupled by a uniform coupling matrix G = w fb I we have  X  2 ( G ) = w 2 fb . The combined system is contracting if: The results obtained with this analysis can be generalized to more than two combined subsystems, and with different types of coupling matrices [17]. Note that in a feed X  X orward or a negative X  feedback case (i.e. at least one of the  X  G  X  X locks X  in the non X  X ymmetric form is negative semidefi-nite), the system is automatically contracting provided that both subsystems are contracting. Given this, the condition for contraction of the combined system described by Eq. (8) becomes: w fb &lt;  X  max . Note that the contraction rate is an observable quantity, therefore one can build a contracting system consisting of an arbitrary number of CCNs as follows: 1. Determine the contraction rate of two CCNs by using Eq. (5) or by measuring it. 2. Use Eq. (7) to set the weight of the relation. Compute the upper bound to the contraction rate of the combined system as explained above. 3. Repeat the procedure for a new CCN and the combined one. The VLSI device used in this work implements a CCN of spiking neurons using an array of low X  power I&amp;F neurons with dynamic synapses [8, 18]. The chip has been fabricated using a standard AMS 0 . 35  X  m CMOS process, and covers an area of about 10 mm 2 . It contains 124 excitatory neurons with self, 1 st , 2 nd , 3 rd nearest X  X eighbor recurrent excitatory connections and 4 inhibitory neurons (all X  X o X  X ll bi X  X irectionally connected to the excitatory neurons). Each neuron receives input currents from a row of 32 afferent plastic synapses that use the Address Event Representation (AER) to receive spikes. The spiking activity of the neurons is also encoded using the AER. In this representation input and output spikes are real X  X ime asynchronous digital events that carry analog information in their temporal structure. We can interface the chip to a workstation, for prototyping Figure 3: Contraction of a single VLSI CCN . (a) A raster plot of the input stimulus(left) and the mean firing rates(right): the membrane potential of the I&amp;F neurons are set to a random initial state by stimulating them with uncorrelated Poisson spike trains of constant mean frequency (up to the dashed line). Then the network is stimulated with 2 Gaussian bumps of different amplitude centered at Neuron 30 and Neuron 80, while, all the neurons received a constant level of uncorrelated input during the whole trial. (b) The response of the CCN to the stimulus presented in (a). (c) Mean responses of 100 trials, calculated after the red dashed line with error bars. The shaded area represents the mean input stimulus presented throughout the experiment. The system selects the largest input and suppresses the noise and the smaller bump, irrespective of initial conditions and noise. Neurons 124 to 128 are inhibitory neurons and do not receive external input. experiments using a dedicated PCI X  X ER board [19]. This board allows us to stimulate the synapses on the chip ( e.g. with synthetic trains of spikes), monitor the activity of the I&amp;F neurons, and map events from one neuron to a synapse belonging to a neuron on the same chip and/or on a different chip. An analysis of the dynamics of our VLSI I&amp;F neurons can be found in [20] and although the leakage term in our implemented neurons is constant, it has been shown that such neurons exhibit responses qualitatively similar to standard linear I&amp;F neurons [20].
 A steady state solution is easily computable for a network of linear threshold units [5, 21]: it is a fixed point in state space, i.e. a set of activities for the neurons. In a VLSI network of I&amp;F neurons the steady state will be modified by mismatch and the activities will fluctuate due to external and microscopic perturbations (but remain in its vicinity if the system is contracting). To prove contraction experimentally in these types of networks, one would have to apply an input and test with all possible initial conditions. This is clearly not possible, but we can verify under which conditions the system is compatible with contraction by repeating the same experiment with different initial conditions (see Sec. 4.1) and under which conditions the system is not compatible with contraction by observing if system settles to different solutions when stimulated with different initial conditions (see Sec. 4.3). 4.1 Convergence to a steady state with a static stimulus The VLSI CCN is stimulated by uncorrelated Poisson spike trains whose mean rates form two Gaussian X  X haped bumps along the array of neurons, one with a smaller amplitude than the other superimposed to background noise (see Figure 3a). In a SWTA configuration, our CCNs should select and amplify the largest bump while suppressing the smaller one and the noise. We set the neurons into random initial conditions by stimulating them with uncorrelated Poisson spike trains with a spatially uniform and constant mean rate, before applying the real input stimulus (before the dashed line in Figure 3a ). Figure 3b shows the response of the CCN to this spike train, and Figure 3c is the response averaged over 100 trials. This experiment shows that regardless of the initial conditions, the final response of the CCN in an SWTA configuration is always the same (see the small error bars on Figure 3c), as we would expect from a contracting system. 4.2 Convergence with non X  X tatic stimulus and contraction rate As the condition for contraction does not depend on the external input, it will also hold for time X  varying inputs. For example an interesting input stimulus is a bump of activity moving along the array of neurons at a constant speed. In this case, the firing rates produced by the chip carry informa-Figure 4: Contraction rate in VLSI CCNs using non X  X tatic stimulation . The input changed from an initial stage, where all the neurons were randomly stimulated with constant mean frequencies (up to 3 s), to a second stage in which the moving stimulus (freshly generated from trial to trial) is applied. This stimulus consists of a bump of activity that is shifted from one neuron to the next. Panels (a) and (b) show trials for two different configurations (weak and strong) and the colors indicate the firing rates calculated with a 300 ms sliding time window. The panel (c) compares the mean rates of neuron #25 in the weakly coupled CCN (green), the strong CCN (blue) and the input (red), all normalized to their peak of activity and calculated over 50 trials. We see how the blue line is delayed compared to the red and green line: the stronger recurrent couplings reduces the contraction rate. tion about the system X  X  contraction rate. We measured the response of the chip to such a stimulus, for both strong and weak recurrent couplings (see Figure 4). The strong coupling case produces slower responses to the input than the weak coupling case, as expected from a system having a lower contraction rate (see Figure 4c). The system X  X  condition for contraction does not depend on the individual neuron X  X  time constants, although the contraction rate in the original metric does. This also applies to the non X  X tatic input case, where the system will converge to the expected solu-tion, independently of the neurons time constants. Local mismatch effects in the VLSI chip lead to an effective weight matrix whose elements w sel f , w 1 , w 2 , w ie are not identical throughout the array. This combined with the high gain of the strong coupling, and the variance produced by the input Poisson spike trains during the initial phase, explains the emergence of  X  X seudo-random X  winners around neuron 30,60 and 80 in Figure 4b. 4.3 A non X  X ontracting example We expect a CCN to be non X  X ontracting when the coupling is strong: in this condition the CCN exhibits a hysteretic behavior [22], so the position of the winner strongly depends on the network X  X  initial conditions. Figure 5 illustrates this behavior with a CCN with very strong recurrent weights. 4.4 Contraction of combined systems By using a multi-chip AER communication infrastructure [19] we can connect multiple chips together with arbitrary connectivity matrices (e.g. G in Sec. 3.2), and repeat experiments analogous to the ones of Sec. 4.1. Figure 6 shows the response of two CCNs, combined via a connectivity matrix as shown in Figure 6b, to three input bumps of activity in a contracting configuration. We applied contraction theory to combined Cooperative Competitive Networks (CCN) of Linear Threshold Units (LTU) and determined sufficient conditions for contraction. We then tested the the-oretical predictions on neuromorphic VLSI implementations of CCNs, by measuring their response to different types of stimuli with different random initial conditions. We used these results to deter-mine parameter settings of single and combined networks of spiking neurons which make the system behave as a contracting one. Similarly, we verified experimentally that CCNs with strong recurrent couplings are not contracting as predicted by the theory. Figure 5: VLSI CCN in a non-contracting configuration . We compare the CCN with very strong lateral recurrent excitation and low inhibition to a weakly coupled CCN. The figures present the raster plot and mean rates of the CCNs response (calculated after the dashed line) to the same stimuli starting from two different initial conditions. Panels (b) and (e) show the response of a contracting CCN, whereas panels (c) and (f) show that the system response depends on the initial conditions of (a) and (d). Therefore the the "Strong CCN" is non X  X ontracting. Figure 6: Contraction in combined CCNs . (a) and (d) Single trial responses of CCN1 and CCN2 to the input stimulus shown in (c); (b) Connectivity matrix that couples the two CCNs (inverted identity matrix); (e) Mean response of CCNs, averaged over 20 trials (data points) superimposed to average input frequencies (shaded area). The response of the coupled CCNs converged to the same mean solution, consistent with the hypothesis that the combined system is contracting. This work was supported by the DAISY (FP6-2005-015803) EU grant, and by the Swiss National Science Foundation under Grant PMPD2-110298/1. We thank P. Del Giudice and V. Dante (ISS), for original design of the PCI-AER board and A. Whatley for help with the software of the PCI-AER board.

