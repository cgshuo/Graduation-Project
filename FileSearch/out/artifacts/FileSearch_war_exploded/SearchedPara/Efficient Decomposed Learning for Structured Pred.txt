 Rajhans Samdani rsamdan2@illinois.edu Dan Roth danr@illinois.edu Structured output spaces occur in many machine learning applications which aim to label certain sets of interdependent variables where the dependencies be-tween variables dictate what assignments are possible. Several techniques have been proposed for learning in structured prediction (Collins, 2002; Tsochantaridis et al., 2004; Taskar et al., 2004). Typical discrimina-tive structural learning algorithms (e.g. Collins (2002); Tsochantaridis et al. (2004)) perform a global MAP in-ference over the entire (hence  X  X lobal X ) output space as an intermediate step. We refer to such learning techniques as global learning (GL). Global inference, and hence GL, can be slow for models with high-order, expressive relations between the output variables. GL algorithms perform exact MAP inference as a black box which may be an overkill for several prob-lems, making learning slow. To alleviate this, we pro-pose a novel algorithm which restricts the MAP in-ference to a smaller part of the output space by us-ing additional information about a) the actual gold labels, b) the constraints on the output space, and c) the underlying parameters, which we want to learn. Consequently, our algorithm is much more efficient than GL. We call our approach Decomposed Learning (DecL) as we decompose the inference into smaller in-ference procedures over more tractable output spaces. We prove that in some settings, DecL is guaranteed to be equivalent to GL. We present experiments in real-world settings (where our theoretical assumptions may not hold) and show that DecL, with small-sized and problem-specific decompositions, perform as well as GL, while being significantly faster.
 Several existing works perform approximate inference during supervised structured learning to the same end. Such approaches can broadly be divided into those that relax the expressive interactions between out-put variables (Roth &amp; Yih, 2005; Punyakanok et al., 2005; Sutton &amp; Mccallum, 2009) during learning and those that relax the integrality constraints on assign-ments (Kulesza &amp; Pereira, 2008; Finley &amp; Joachims, 2008; Martins et al., 2009; Meshi et al., 2010). Some of the MCMC-based contrastive techniques (Hinton, 2002; Wick et al., 2011) are conceptually similar to DecL in that they use approximate gradient steps for learning. Our work is also related in spirit to Meshi et al. (2010) who consider a Linear Programming re-laxation of the entire inference and perform parameter updates after small message-passing inference steps. However, unlike these techniques, we don X  X  replace ex-act inference by approximate inference; instead, we perform exact inference on a smaller output space. The closest works to DecL are Pseudolikelihood-based techniques (Besag, 1974; Sontag et al., 2010) to learn-ing; however, while Pseudolikelihood is consistent asymptotically, DecL aims to achieve equivalence to GL with a finite amount of data.
 The outline of this paper is as follows. Sec. 2 intro-duces the problem and notation and Sec. 3 assays two extreme styles of structured prediction. We introduce our approach in Sec. 4 and provide theoretical results in Sec. 5. We finally present empirical results in Sec. 6. Consider a structured prediction setting where a d -dimensional input x is drawn from a space X and the output variable y is, w.l.o.g., a vector of binary labels { y 1 ,...,y n } drawn from Y  X  { 0 , 1 } n . The space Y may be specified by a set of declarative con-straints which can be viewed as a form of specifying some domain knowledge over y .
 Inference: The labels in y are correlated and so it is advantageous to predict them simultaneously. As is typical, we express the prediction over all variables in y using a scoring function f ( x , y ; w ) = w  X   X  ( x , y ) as where  X  ( x , y )  X  R d are feature expressed over both x and y , and w  X  R d are weight parameters. We refer to the arg max inference above as MAP inference 1 . Structural Learning and evaluation: The focus of this work is on learning the weight parameter, w , from a given collection of labeled training instances D = ( x 1 , y 1 ) ,..., ( x m , y m ). As is standard, the quality of a learned hypothesis is measured using a loss function  X  : { 0 , 1 } n  X  { 0 , 1 } n  X  R  X  0 , satisfying  X ( y , y ) = 0 ,  X  y  X  X  0 , 1 } n .
 We focus on two popular classes of scoring functions f ( x , y ; w ):  X  Singleton with constraints: f ( x , y ; w ) is a  X  Pairwise Markov Networks: For a Pairwise This section discusses two styles of learning the pa-rameter w from the training data D : global learning and local learning, with their shortcomings.
 Global Learning Given the inference procedure in (1) and training data D , a popular discriminative learning approach (Tsochantaridis et al., 2004; Taskar et al., 2004) is to minimize an SVM-style convex upper bound on the loss 2 over the training data: The inference step in (2), involving max, is performed globally over all the labels of y and hence we call this style, Global Learning (GL). GL tends to be slow which hinders applications with a large output space or a large number of training examples.
 Local Learning For faster learning, several approx-imations to GL have been used which ignore certain structural interactions so that the rest of the structure is easier to learn. We call this general paradigm of learning by relaxing to a more local or easy-to-learn structure, Local Learning (LL). For instance, when highly expressive constraints are used over the struc-ture, then dropping such constraints makes the struc-ture more  X  X ocal X  and faster to learn: for singleton functions (Punyakanok et al., 2005; Barzilay &amp; Lap-ata, 2006), ignoring constraints reduces the problem to learning n independent binary classifiers w i ; in case of sequential or tree-structured problems, the task re-duces to learning with dynamic programming infer-ence (Koo et al., 2010; Roth &amp; Yih, 2005). In case of multi-label classification, ignoring interactions be-tween labels reduces the problem to learning a binary classifier for each label. In most LL scenarios, the ignored constraints, if any, are injected back during inference. Refer to Punyakanok et al. (2005) for a detailed analysis and comparison of GL and LL for singleton scoring functions with constraints.
 LL schemes are much faster than GL; in general, how-ever, LL fails to take advantage of the structure of Y which is where Decomposed Learning comes in. For a training instance ( x j , y j )  X  D , let nbr ( y j be a subset of the output space defining a  X  X eighbor-hood X  around y j , which is referred to as the ground truth or the gold output. The key idea behind decom-posed learning (DecL) is to learn w by discriminating the supervised label y j from only all y 0  X  nbr ( y j ) in-stead of all y 0  X  Y . nbr ( y j ) can use additional infor-mation about the structure ( Y ) and parameters which we intend to learn ( w  X  ) such that it captures the struc-ture of Y while being much smaller. Fig. 1 shows the general schema for both GL and DecL, showing the similarities and the differences.
 Let N = { nbr ( y j ) | j = 1 ,...,m } be the collection of neighborhoods for all training instances. To pursue the general idea behind our approach, we use a max-margin formulation (Taskar et al., 2004) for learning over given data D = { ( x j , y j ) } m j =1 . Specifically we minimize a loss function DecL ( w ; D,N ) given by
X The idea of looking at a smaller output space is natu-ral; the key question is how do we create these neigh-borhoods so that the resulting learning algorithm is correct or at least gives a good approximation to GL. To motivate our technique for doing so, we use a simple example of multi-class classification with la-bels 1 ,...,r . This problem can be expressed as a structured prediction problem over r binary variables y ,...,y r such that an instance with label q is repre-sented as a binary vector y [ q ] obeying the constraint P i y [ q ] i = 1 and with y [ q ] i = 1. For a training in-stance ( x , y [ q ]), GL aims to learn a scoring function which gives a score less than y [ q ] to all other pos-sible outputs y [ i ] ,  X  i 6 = q . Since the outputs are constrained such that any two outputs y [ q ] and y [ i ] differ on just the bits y q and y i , this is achieved by merely comparing assignments to pairs of bits y q and y ,  X  i 6 = q . This is exactly what techniques like multi-class SVM (Crammer &amp; Singer, 2002) and constrained classification (Har-Peled et al., 2003) do 3 . Overall, while multi-class classification is indeed a simple case as the space Y contains just n outputs, we generalize the idea of creating neighborhoods over a large number of variables via smaller and more local comparisons. We generate nbr ( y j ), by fixing a subset of the output variables to their gold labels in y j , while allowing the rest of them to vary, and repeating the same for differ-ent subsets of output variables. We formalize this idea through what we define as decompositions (hence, decomposed learning.) We give theoretically desirable properties of these neighborhoods in Sec. 5.
 Definition 1. Given a set of n binary output vari-ables indexed by { 1 ,...,n } , a decomposition S is a set containing distinct and non-inclusive (possibly overlapping) index sets which are subsets of { 1 ,...,n } : S = { s 1 ,...,s l |  X  i, s i  X  X  1 ,...,n } ;  X  i,k, s i 6 X  s Before explaining learning with decompositions, we give some notation. Given two output instances y , y 0  X  Y , let s ( y , y 0 ) be the set indexing the differ-ences between y and y 0 i.e. s ( y , y 0 ) = { i : y i 6 = y Given a set s  X  X  1 ,...,n } , denote  X  s = { 1 ,...,n }\ s . Let y s  X  { 0 , 1 } | s | denote an assignment to the vari-ables indexed by set s . Let ( y s , y j  X  s ) be the output formed by replacing variables in y j indexed by s by corresponding variables in y s .
 We associate one decomposition S j with each training instance ( x j , y j ) and do inference during learning as follows. Given a gold output variable y j pick a set s  X  S j , fix variables in y j  X  s and look at all assignments to y s such that ( y s , y highest scoring assignment over all feasible selections of y s and over all s  X  S j and return the structure. Given a decomposition S j for y j , let the corresponding neighborhood be nbr ( y j , S j ) given by nbr ( y j , S Algorithm 1 Subgradient-descent Alg. for DecL { y  X  Y| X  s  X  S j , s ( y j , y )  X  s } . Using the above style of inference results in minimizing the following convex function for learning To minimize Eq. 4, we use a subgradient descent scheme shown in Alg. 1 4 .
 Let DecL-k be the special case where all subsets of { 1 ,...,n } of size k ( k  X  1) are considered in the decomposition. For multi-class classification, DecL-2 with  X  as the Hamming loss is the same as multi-class SVM (Crammer &amp; Singer, 2002) and Alg. 1 with DecL-2 and  X  = 0 (perceptron loss) yields constrained classification (Har-Peled et al., 2003) thus closing our loop on multi-class classification. Note that, in Step 5 of Alg. 1, going over all sets in S j to find arg max can be slow if the number of sets inside each decom-position is large (e.g. in DecL-k for large k .) To get around this, we compute max over a few sets selected uniformly at random from the decomposition. One can also use more complicated convex optimization tech-niques which require evaluating the max over just one set at a time (Gaudioso et al., 2006).
 In practice, decompositions can be guided by domain knowledge  X  highly coupled output variables should be put in the same set while somewhat unrelated vari-ables should be kept separate. The complexity of learning is small if the sizes of the sets considered in the decomposition are small. Sec. 5 provides theoret-ical results on decompositions for certain cases. Our theoretical anaylsis carries a different flavor than standard generalization bounds. We present theoreti-cal results to show some conditions under which DecL is equivalent to GL. We start with the trivial observa-tion that when each neighborhood is equal to Y , then DecL is the same as GL. Due to the lack of space, we have moved all the proofs to the supplement.
 We assume that the data is separable. Our interest is in all parameters w  X  which satisfy the following margin-separation condition  X  ( x j , y j )  X  D : the set of which can be written (omitting regulariza-tion and using (2)) as W  X  = { w | l ( w ) = 0 }  X  R Let W dec = { w | DecL ( w ; D,N ) = 0 }  X  R d be the set of weights obtained by DecL 5 (we leave the neighbor-hoods selected for DecL implicit here.) Throughout this section, we assume that there exists at least one separating weight vector in W  X  .
 Assumption 1: W  X  is non-empty.
 We use the following property to express our results. Exactness: DecL is said to be exact if W dec = W  X  for the given data D .
 Our goal is to find small neighborhoods for DecL for which exactness holds. Note that the Pseudolikelihood-based approaches (Besag, 1974; Dil-lon &amp; Lebanon, 2010; Sontag et al., 2010) to structured prediction are asymptotically consistent; that is, they are equivalent to GL only in the limit of infinite data. In practice, one uses a finite amount of data to obtain a weight vector by minimizing a convex regularizer on w (e.g. min k w k p for p  X  1) while requiring separation (Cond. (5).) In this case, exactness, i.e. W dec = W  X  , implies that DecL and GL minimize the same regular-ization function over two equal sets  X  if the regularizer is strictly convex, they will output the same weight. Thus exactness is clearly a stronger and more useful property than asymptotic consistency. Our goal is to determine families of decompositions that will result in the exactness of DecL.
 To analyze exactness of DecL, we use the following property to characterize the loss function  X .
 Subadditivity:  X ( y , y 0 ) is subadditive if  X  y , y 0 , y 1 , y 2  X  Y , with s ( y , y 1 )  X  s ( y , y we have  X ( y , y 0 )  X   X ( y , y 1 ) +  X ( y , y 2 ). Several common loss functions like Perceptron loss i.e. no margin requirement, Hamming loss, and zero-one loss are subadditive. We now make the following sim-ple observations.
 Observation 1 ( Closed and Convex ) . W  X  is an intersection of closed half spaces  X  one for each sepa-ration constraint given by (5) . Thus W  X  is closed and convex. Similarly, W dec is closed and convex. Observation 2 ( Outer bound ) . For all decomposi-tions, the set of separating weights for DecL give an outer-bound on the set of separating weights for GL, i.e. W  X   X  W dec as DecL seeks to separate the gold output from only a subset of the output space.
 Due to observation 2, to show that DecL is exact for some decompositions, we need only show that for any w 0 /  X  W  X  , we also have w 0 /  X  W dec  X  since both W  X  and W dec are closed and convex, we need to show this only for w 0 immediately outside the boundary of W  X  (see the proof in the supplement.) To this end, we define B ( w , ) = { w 0 | k w 0  X  w k X  } as a closed ball of radius centered around w .
 Theorem 1. DecL is exact if  X  w  X  W  X  ,  X  &gt; 0 , such that  X  w 0  X  B ( w , ) ,  X  ( x j , y j )  X  D the fol-lowing condition holds for nbr ( y j ) : if  X  y  X  Y with f ( x j , y ; w 0 ) +  X ( y j , y ) &gt; f ( x j , y j ; w nbr ( y j ) with f ( x j , y 0 ; w 0 ) +  X ( y j , y 0 ) &gt; f ( x This theorem essentially requires that a w 0 which does not globally separate examples in D , also does not sep-arate the decomposed learning examples. We note that this theorem is very general and applies to any struc-tured prediction problem (and any  X .) We use this theorem to prove exactness for certain decompositions based on some easy to determine characterizations of a) the structure ( Y ), b) the correct parameters ( W  X  ), and c) the data D . The following corollary is an immediate consequence of Theorem 1. Roughly, this corollary requires that the difference between the score of the gold output and that of any other output is bounded by the sum of score differences between the gold output and that of outputs in the neighborhood. Corollary 1. DecL is exact if  X  is subadditive and  X  w  X  W  X  ,  X  &gt; 0 such that  X  w 0  X  B ( w , ) ,  X  ( x j , y j )  X  D ,  X  y  X  Y , s ( y , y j ) can be partitioned into sets s ,...,s l such that  X  k  X  { 1 ,...,l } , ( y s nbr ( y j , S j ) and Using these general results, we now examine two dif-ferent classes of scoring functions mentioned in Sec. 2. 5.1. Exactness of DecL for Singleton Scoring In this section, we present exactness results for DecL with singleton scoring function f ( x , y ; w ) = P specified by constraints. For instance, Y can be spec-ified by a collection of l logical constraints: Y = { y  X  { 0 , 1 } n | C k ( y ) = 1 ,k = 1 ,...,l } where C k is a logi-cal function (e.g. OR ) over binary variables in y . Y can also be specified by linear constraints over y as Y = { y  X  X  0 , 1 } n | A y  X  b } .
 In several practical applications, the constraint struc-ture has some symmetry to it and we can invoke Cor. 1 to provide exactness guarantees for decompositions with set sizes independent of the number of variables. The following corollaries apply to two such cases with set sizes only dependent on the number of constraints. Corollary 2. If Y is specified by k OR constraints, then Decl-( k + 1) is exact for subadditive  X  . Corollary 3. If Y is specified by k ( k  X  1 ) linear constraints: A y  X  b (or  X   X   X ,  X  =  X ), where A is a binary matrix such that any two variables in y participate in at most one constraint, Decl-3 k is exact for subadditive  X  .
 As a consequence of Cor. 2, if the space Y is speci-fied by k horn clauses (Srikumar &amp; Roth, 2011), then DecL-( k + 1) is exact regardless of the number of vari-ables in each clause.
 We also note that the results in this section are based on worst-case analyses. In practice, much smaller-sized decompositions work well in most cases. 5.2. Exactness for Pairwise Markov Networks While in the last section, we presented exactness re-sults solely based on constraints, in this section, we present decompositions for binary PMNs using some knowledge about the true parameters W  X  .
 Recall that, for PMNs, f ( x , y ; w ) = P is the set of edges for the underlying graph. Inference over such functions is NP hard in general.
 A pairwise potential function,  X  i,k is called submodular if (  X  i,k (1 , 1) +  X  i,k (0 , 0))  X  (  X  i,k (1 , 0) +  X  i.e. it prefers similar labels; it is called supermodular if (  X  i,k (1 , 1) +  X  i,k (0 , 0))  X  (  X  i,k (1 , 0) +  X  Assumption 2: Assume that  X  ( i,k )  X  E,  X  x j  X  D,  X  w  X   X  W  X  ,  X  ik (  X  ,  X  , x j ; w  X  ) is either submodular or supermodular; also, we know if any given  X  ik is sub-modular or supermodular.
 Such knowledge about pairwise potential functions is often available in practice, especially for submodular potentials. For instance, in several computer vision tasks, neighboring pixels are more likely to carry the same label (Besag, 1986; Boykov et al., 1998); in infor-mation extraction tasks, consecutive words are likely to belong to the same field. We can also approximately determine this information by computing mutual in-formation over labeled data. With Assumption 2, we present a decomposition, which leads to exactness. For each instance ( x , y j = { y j 1 ,...,y j n } ), define E j = { ( u,v )  X  E | (  X  uv is submodular and y j y ) or (  X  uv is supermodular and y j u 6 = y j v ) } i.e. E removes those edges from E where the labels on nodes  X  X isagree X  with the corresponding pairwise po-tentials. With ( x j , y j ), we associate a decomposition S pair ( y j ) = { c 1 ,...,c l } where c 1 ,...,c l are indices of the maximal connected components in E j .
 Theorem 2. For PMNs where Assumption 2 is sat-isfied, DecL with S pair is exact with subadditive  X  . As a simple illustration, consider a sequential HMM where it is known that the same-state transition prob-abilities are higher than those for different states i.e. all  X  ik are submodular. Then for y j = 1110011, the corresponding decomposition is S pair = connected components with the same label.
 Notably, graph cuts can be used for efficient learning over binary PMNs with submodular potentials (Szum-mer et al., 2008). We note that with submodular potentials, we can augment decomposed inference in DecL with graph cuts in a similar fashion to make it even more efficient.
 Finally, DecL can be used when certain additional global constraints are added to PMNs (Roth &amp; Yih, 2005). The exactness guarantees hold for S pair ( y j ) if  X  y  X  Y ,  X  s  X  S pair ( y j ), ( y s , y j  X  s )  X  Y . Exact global inference can replace DecL inference for those training examples where this condition does not hold. In prac-tice, we find (see Sec. 6.3) that S pair works very well for non-binary PMNs in the presence of constraints, where some of our assumptions do not hold. This section presents experimental results on non-ideal real world settings showing that DecL is effective and robust. We show results on synthetic data as well as two real-world tasks of multi-label classification and information extraction. We perform exact inference using ILP, wherever needed. We show that DecL per-forms favorably relative to GL on these tasks while greatly reducing the training time. We use appropri-ate LL approaches as competing baselines. In settings with constraints, we consider another baseline, LL+C, that uses constraints  X  which were ignored during learning  X  for inference during testing. 6.1. Experiments on Synthetic Data We first analyze DecL with simple decompositions  X  DecL-1,2,3  X  with singleton scoring functions in a controlled synthetic setting by measuring the perfor-mance and efficiency. We generate data with 10 binary output variables, which are constrained by randomly generated linear constraints. We ensure that the re-sulting Y contains at least 50 outputs. The features x are sampled randomly from a 20-dimensional space. We randomly generate singleton scoring functions and determine gold labelings for each instance as per Eq. 1 (thus we know that the data is separable.) For learning, we use SVM-Struct (Tsochantaridis et al., 2004) to implement our algorithms. Our LL baseline ignores the constraints during learning thereby reducing it to learning 10 independent binary classifiers. We test on 200 instances and tune C , the regularization penalty parameter, on 100 validation in-stances; we average over 10 random trials. Fig. 2 plots the loss for each algorithm against the size of the train-ing data and Tab. 1 shows the training time on 320 examples. Note that the training time for DecL could be much lower with preprocessing of data.
 We observe that although LL and LL+C exhibit rel-atively low error even with a small amount of data, they fail to converge to a near-perfect classifier like GL, with a large amount of data. On the other hand, DecL-2 , 3 exhibit performance close to global learning while taking much less time to train. 6.2. Multi Label Document Classification We test various algorithms on a multi-label document classification task over the Reuters dataset (Lewis et al., 2004). We use one section of the data with 6,000 instances and reduce it to the 30 most frequent labels. We keep 3600 instances for training, 1200 for testing, and 1200 for validation.
 We model the scoring function as a PMN over a com-plete graph over all the labels to capture interactions between all pairs of labels. We compare DecL-1,2,3 with GL and a local learning (LL) approach which drops the pairwise components reducing the prob-lem to 30 independent binary classifiers. We again use SVM-Struct for learning the parameters for GL and DecL. We measure the performance using a per-instance F1 measure given by F 1 = 2 c t + p , where t is the number of gold labels for this instance, p is the num-ber of predicted labels, and c is the number of correct predictions; we report averages over all test instances. Tab. 2 presents results with 10-fold cross validation 6 . DecL-2,3 perform as well as GL and much better than LL. Notably, DecL-2 is 6 times faster than GL. As in the synthetic data experiments, DecL-1, a.k.a. Pseu-domax (Sontag et al., 2010), performs badly. 6.3. Information Extraction: Sequence We test the efficacy of our approach on two informa-tion extraction tasks inspired by our analysis of PMNs in Sec. 5.2. Our task is to identify the functional fields (e.g.  X  X uthor X ,  X  X itle X ,  X  X acilties X ,  X  X oommates X ) from citations (McCallum et al., 2000) and advertise-ments (Grenager et al., 2005) datasets. We model this setting as an HMM (a special case of PMN) with dif-ferent functional fields as hidden states and words as emissions. We add certain global constraints borrowed from Chang et al. (2007) to the HMM, which necessi-tate ILP-based inference. For the given tasks, words corresponding to a field, e.g.  X  X itle X , occur in long contiguous blocks; thus we assume that the correct HMM transition matrix has a high same-state transition probability which is a gen-eralization of the submodular potentials we assumed in Sec. 5.2 and hence a natural testing ground for our theory. We use the decomposition S pair presented in Sec. 5.2 to perform DecL; intuitively, these decompo-sitions enable DecL to capture the  X  X iagonal-heavy X  nature of the HMM transition matrix while allowing it to learn the transitions between different fields. We perform discriminative learning using averaged structured perceptron (Collins, 2002). We use HMM without constraints as an LL baseline. We obtain an LL+C baseline by adding constraints during test. Table 3 presents the results for the two domains. LL-based approaches perform very well for small data sizes because with a less expressive model, they need less data to generalize. However, with large amounts of training data, GL and DecL easily outperform HMM and LL+C. DecL does slightly, although not significantly, better than GL while being 2-8 times faster. Our results compare favorably with the state-of-the-art supervised results reported on these datasets by Chang et al. (2007) (CRR07.) Overall, we gather that by utilizing very simple knowledge of the task at hand (submodular potentials), we can perform near-global learning while being very efficient. We presented Decomposed Learning (DecL)  X  a technique for efficient structural learning. DecL learns efficiently by performing inference over a small part of the output space. We provided theoretical results, which use characterizations of the structure, target parameters, and ground truth labels to decompose the output space such that the resulting DecL is efficient and equivalent to exact learning. While the common approximation practice in structural learning tasks is to use approximate MAP inference without guar-antees, our approach may provide a way to achieve significant improvements in these cases and can be augmented with existing approximation techniques like LP-relaxation. Indeed, our experimental results indicate that our algorithms are robust and perform very well on real world data.
 Acknowledgments: This research is sponsored
