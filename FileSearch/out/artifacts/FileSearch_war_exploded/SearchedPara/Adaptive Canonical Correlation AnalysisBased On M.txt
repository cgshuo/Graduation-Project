 Florian Yger florian.yger@litislab.eu Maxime Berar maxime.berar@litislab.eu Gilles Gasso gilles.gasso@insa-rouen.fr Alain Rakotomamonjy alain.rakoto@insa-rouen.fr Canonical Correlation Analysis (CCA) is a well-known dimensionality reduction method. Given two views (or representations) of the same set of objects, it aims at finding projections for each representation such that their correlation is maximized in the projection space. As every popular method in machine learning, since its first formulation (Hotelling, 1936) CCA has been ex-tended to a kernel version (Lai &amp; Fyfe, 2000; Akaho, 2001), to online and recursive versions (V  X  X a et al., 2007) and quite recently to a sparse version (Hardoon &amp; Shawe-Taylor, 2011).
 CCA is usually formulated as the Generalized Sin-gular Value Decomposition (Generalized SVD) of the cross-covariance matrix (Sun et al., 2009). Besides, it aims at finding projections that are orthogonal with respect to the auto-covariance matrices of each view. As CCA belongs to the class of Latent Variables meth-ods, it shares close connections with those methods. Indeed, according to Rosipal &amp; Kr  X amer (2006); Sun et al. (2009), CCA is a generalization of Orthonormal-ized Partial Least Squares.
 A wide variety of applications from multimodal deep learning (Ngiam et al., 2011) to Brain Computer Inter-face (BCI) (Hakvoort et al., 2011) used CCA for fea-ture extraction. However, those applications involve either non-stationary or big datasets. In such con-texts, an adaptive algorithm for solving CCA would be useful. Incremental algorithms have been proposed for CCA and are based on a Recursive Least Squares algorithm (V  X  X a et al., 2007), but they solve a sequence of rank-one CCA problems and cope with the orthog-onality constraints using a deflation scheme.
 Differential geometry of matrix manifolds provides an elegant way for dealing with matrix constraints. This framework has proved competitive for matrix con-strained optimization problems (Meyer et al., 2011) and more specifically for eigenproblems (Absil et al., 2008). In this contribution, we cast the CCA prob-lem as an optimization problem on matrix manifolds, and solve it using classical gradient algorithms on less studied manifolds. As an application, our adaptive CCA is used to track principal correlations subspaces of EEG signals over time, in order to detect the exact time step when abrupt changes occur in those sub-spaces. In our BCI context, such a change means that the patient switches from a mental task to another. For a real-time BCI system, a fast, reliable, low rank and adaptive algorithm is needed to handle noisy and non-stationary signals (Millan et al., 2004). Keeping those needs in mind, we developed an adaptive version of CCA based on matrix manifolds.
 The next section of this paper is devoted to the CCA algorithm and to its adaptive formulation. Then, we introduce the basic tools of Riemannian geometry used in our algorithm and we describe the steps of our CCA algorithm. Finally, before discussing some extensions of this work, we present numerical results on a toy dataset and on BCI data.
 2.1. Background Assume given two full-rank data matrices X  X  R n  X  T and Y  X  R m  X  T . The matrices C xy = X &gt; Y , C x = X &gt; X , C y = Y &gt; Y are estimates of the cross-covariance and auto-covariance matrices. We assume that both views on the data are centered.
 Usually, CCA is formulated in terms of Rayleigh Quo-tient (Hardoon et al., 2004; Bie et al., 2005) involving the covariance matrices of both views C x and C y and between the views C xy : By extending the Rayleigh Quotient using the trace operator, multiple projections can be obtained simul-taneously by solving the following fixed rank p opti-mization problem (Sun et al., 2009) Several alternative formulations exist and involve Gen-eralized Eigenvalue Decompositions of symmetric defi-nite positive matrix pencils (Golub &amp; Van Loan, 1996): However, if the data are non-stationary, the previous formulations are hard to update, making them unsuit-able for online or adaptive applications.
 Concatenating the two views, the following eigenprob-lem is often used for the CCA problem (Bie et al., 2005; V  X  X a et al., 2007): This formulation, true for rank-one eigenproblem, can-not be extended to a higher rank without a costly and numerically unstable deflation procedure (unsuitable for an adaptive formulation). Indeed, for p &gt; 1 the constraint U &gt; C x U + V &gt; C y V = I p implied by the for-mulation is not equivalent to the constraints of (2). As the trace operator is invariant to multiplication of both U and V by elements of O ( p ), the group of or-thonormal matrices in R p  X  p , we must modify Prob-lem (2) to enforce uniqueness of the solution. Intro-ducing the matrix N as a diagonal matrix in R p  X  p with strictly decreasing positive elements (Absil et al., 2008, p.11), we propose to replace the objective func-tion of Problem (2) by the Brockett cost function tr ( U &gt; C xy V N ). Hence the obtained solution will cor-respond to the solutions of the previous eigenproblems. 2.2. Adaptive formulation At each time step t , new samples x t in R n and y t in R are acquired and all covariance matrices are updated using a forgetting factor 0 &lt;  X  &lt; 1:
This forgetting update is a common tool in subspace tracking (dos Santos Teixeira &amp; Milidi  X u, 2010), assim-ilable to estimation over an exponential window. The case  X  = 1 leads to an incremental problem.
 The adaptive CCA problem is formulated as In our adaptive setting, knowing the solution ( U t  X  1 ,V t  X  1 ), we wish to update the solution at a cheap cost. However, we need our solution ( U t , V t ) to sat-isfy the C t x -and C t y -orthogonality conditions. In the sequel, we present a gradient algorithm, but, in order to be able to apply this algorithm, we need the initial point ( U t  X  1 ,V t  X  1 ) to satisfy the updated constraints. U first task is to find a feasible starting point referred to as ( U t 0 ,V t 0 ). We call this the metric update subprob-lem. From then, our second task is to maximize the cost function. 2.2.1. Metric update subproblem The metric update consists in finding a subspace satis-fying the new orthogonality constraints while conserv-ing the span of the previous subspace. The product by matrices in the general linear group GL p (the set of all invertible p  X  p matrices) conserves the span. The metric problem then aims at finding matrices in GL p such that: pressed cross-covariance and G x =  X  I p + z x z &gt; x G y =  X  I p + z y z &gt; y be the compressed auto-covariance matrices.
 The only elements of GL p respecting the constraints can be written as elements of two instances of the Gen-eralized orthogonal group defined for a given matrix G 0 (positive definite matrix G ) as: The metric update problem is now a full SVD problem on the Generalized orthogonal groups: From the solution of (4), the new subspace matrices are obtained as along with a new compressed covariance matrix 2.2.2. Cost function update subproblem This subproblem consists in finding new subspaces maximizing the cost while satisfying the updated met-ric constraints, (eventually) resulting in a change of span and it is equivalent to Problem 3: At the end of the cost function phase, the new sub-space matrices U t and V t are obtained directly as solu-tions of the subproblem, and so is the new compressed covariance matrix L t = U &gt; t C t xy V t .
 In each subproblem, the matrix constraints define Rie-mannian matrix manifolds. Hence, each subproblem corresponds to the maximization of a Brockett cost function on matrix manifolds. In the case of the metric update, the manifold is a product of two Generalized orthogonal groups and in the case of the cost func-tion update, the manifold is a product of Generalized Stiefel manifolds.
 Optimization on Riemannian manifold is currently a very active research field in machine learning com-munity (Meyer et al., 2011) and more broadly in the numerical optimization discipline (Absil et al., 2008). In a complete Riemannian view, we should express our optimization problem as a search along a geodesic curve in the manifold. Such a search being in practice intractable, we chose to approximate it by a search along another smooth curve on the manifold. This smooth curve is defined by a function that transforms any displacement in the tangent space to a point on the manifold. Such a function (that also obeys to some technical conditions, see (Absil et al., 2008)) is called a retraction. Figure 1 depicts a Riemannian manifold M and a tangent space at a point X on this mani-fold. The tangent space is a vector space that locally approximates the manifold. Then the retraction is a mapping that locally transforms a search in the man-ifold into a search in the tangent space.
 In this section, we briefly recall our Tangent space and retraction formulae. The Stiefel manifold St ( p,n ) is defined as The orthogonal group O ( p ) is the Stiefel manifold St ( q,p ) with q = p : O ( p ) = { X  X  R p  X  p : X &gt; X = For a given matrix G 0, we define the Generalized Stiefel manifold as: This manifold contains the p -dimensional G -orthonormal subspaces of R n  X  n . The Generalized orthogonal group O G ( p ) is the Generalized Stiefel manifold St ( q,p ) with q = p : The former CCA optimization problems can be now seen as optimization problems on matrix manifolds St
C x ( p,n ) and St C y ( p,m ): and on manifolds O G x ( p ) and O G y ( p ) as in Eq. 4. 3.1. Tangent space The definitions of the tangent space and of the retrac-tion can be found in (Absil et al., 2008) for the Stiefel manifold and for the orthogonal group. Deriving the same calculus for the generalized case, we obtain the following definitions for the tangent space at a point X on a Generalized Stiefel manifold: T X St G ( p,n ) = { Z  X  R n  X  p : X &gt; GZ + Z &gt; GX = 0 } . An alternative characterization of T X St G ( p,n ) decom-poses any element of T X St G ( p,n ) as the sum of a term G -orthogonal to X , written X  X  K , and of the product of X with a skew-symmetric matrix  X  in S skew ( p ): T where S skew ( p ) denotes the set of all skew-symmetric p  X  p matrices. In the case of the Generalized orthogo-nal group, the alternative characterization of the tan-gent tangent space is simplified: 3.2. Retraction One example of retraction for the (Generalized) Stiefel manifold is the polar retraction: There exists several other retractions that can be ap-plied for (Generalized) Stiefel manifold. Among them, the retraction based on QR-decomposition (adapted to the metric) could also be applied.
 Until now, we derived equations and formulae char-acterizing tangent space and retraction on St G ( n,p ) and O G ( p ). However, in our optimization problems, the manifold of interest is a product manifold of two generalized Stiefel manifolds.
 In (Ma et al., 2001), the authors proved that the geodesics in the product manifold are the products of the geodesics in the factor manifolds. This helpful property enables us to compute the gradients and the retractions on each of the factor manifolds separately. This section presents a manifold gradient algorithm adapted to product of Generalized Stiefel manifolds and product of Generalized orthogonal groups. First, we present the formulae for the cost function subprob-lem. The Generalized orthogonal group being a partic-ular case of Generalized Stiefel manifold, we shortened the description of its gradient. Finally, we sum up the approach in Algorithm 1. 4.1. Gradient ascend on Generalized Stiefel At time t 0 , after the metric update phase, for each subspace, we can compute compressed samples and residuals: using Sherman-Morrison-Woodbury formula to get ef-ficiently the inverse of matrices. For instance, we have The gradient of the Brockett cost function on the Gen-eralized Stiefel manifold St C x at U is defined as : with L = U &gt; C xy V . Note that the Brockett matrix N insures that if the gradient is null then the compressed covariance matrix L is diagonal and U is solution of the eigenproblem. In the adaptive case, the gradient becomes Defined as the sum of a C t x -orthogonal element and of a skew-symmetric element,  X  U belongs to the tangent space T U St ( p,n ) and thus a retraction can be used. Choosing a polar retraction, the updated matrix U t +1 for a step length  X  U in R is : The right term of the retraction product includes two rank-one matrices orthogonal to each other, multiple of the (orthogonal) projectors on Nz y and  X  z x , where  X  z z the updated matrix can be computed efficiently as: with the coefficients  X  x and  X   X  x defined as: where  X  x ,  X   X  x are positive coefficients taking into ac-count the norm of the residuals, the compressed sam-ples and the squared step length: The same formula holds for the gradient of the right subspace exchanging the respective roles of each view. The step lengths  X  U and  X  V are jointly determined using standard line-search techniques. 4.2. Gradient ascend on Generalized pressed auto-covariance matrices. Initial points on the manifold are computed using the following formulae: and at these points, the gradient is written:  X  A polar retraction being too costly in this case, we apply an oblique version of QR decomposition, where the projections are made according to G x . Again, the same formula holds for the gradient of the right sub-space exchanging the respective roles of each view. Algorithm 1 Adaptive CCA algorithm input: subspace matrices ( U,V ), forgetting factor  X  , output: updated subspace matrices ( U,V ) compute compressed samples z x , z y (Eq. 6) compute initial matrix O u , O v (Eq.9-10) perform Gradient ascend (Eq. 11) update U,V update auto-covariance matrices C x , C y compute z x , z y and residuals f x , f y (Eq. 6) perform Gradient ascend (Eq. 7) update U,V (Eq. 8) update cross-covariance matrix C xy In the begining of this paper, in preamble to the expla-nation on CCA, we assumed both views of the data to be centered. This theoretical requirement being how-ever rarely met in practice, we used the following for-mula (dos Santos Teixeira &amp; Milidi  X u, 2010) in order to estimate the mean of a given view w (used to center 5.1. Toy dataset We compare the performances of our algorithm to those of the adaptive RLS-CCA algorithm (V  X  X a et al., 2007) with a corrected orthogonalization scheme. The solutions given by both methods are compared to the exact solution given by a batch algorithm (solving the Generalized SVD at every step). The evaluation crite-ria are the orthonormality errors w.r.t. the metric on both views e o ( t ) = k U the distance between the oblique projectors associated to each subspace normalized by the rank of the prob-lem: and the ratio e c between the cost obtained using the competing algorithms and the cost of the exact method.
 Our simulation protocol generates samples x of size n = 36 and y of m = 34 living in two different full-rank subspaces. We are interested in tracking the first p = 30 principal directions of the correlated subspaces with a forgetting factor  X  = 0 . 99 during 2000 time steps. The results are averaged over 50 trials. Figure 2 shows the cost and accuracy errors with ran-dom feasible initializations. The error on the obtained cost stabilizes at 62% of the batch cost for our algo-rithm, whereas the LRR-CCA stabilizes at 27% of the batch cost algorithm. In both cases, the accuracy er-rors are in the same range and the orthogonality errors are acceptable (  X  10  X  13 ). As our adaptive algorithm only performs one gradient step for every sample, com-parison with the updated batch is far from being op-timal. But it achieves correct results, compatible with our change detection application for BCI. 5.2. BCI Competition III dataset V We tested our method on the dataset V of the BCI Competition III (Millan et al., 2004; Blankertz et al., 2004). It contains data from three subjects during 4 non-feedback sessions. The subjects were asked to per-form a mental task during about 15 seconds and then switch to a randomly chosen task. There are three tasks : imagination of repetitive left hand movements, or imagination of repetitive right hand movements or generation of words beginning with the same random letter. The goal is to predict the current mental task of the subject every 0.5 seconds.
 In this study, we apply our adaptive CCA algorithm for change detection and we focus on detecting the change in mental task of the subjects. Some pre-computed features were provided for the competition. Those 96 features consists of power spectral density (PSD) in the band [8  X  30] Hz of the 8 centro-parietal channels, spatially filtered by a surface Laplacian. We will calculate the correlation between the features of left and right electrodes, respectively x t and y t , letting unused the features of the central electrodes. The three mental tasks performed by the subjects in-volve different regions of the brain. Hence, the corre-lation of the signals measured over those regions will be different for two different mental tasks. So, the cor-relation matrix between the right and left electrodes should give us information about the current mental state of the brain. By tracking the changes in the principal subspaces of this matrix, we should be able to detect if the subject switched from one mental task to another.
 The cross-covariance and auto-covariance matrices were initialized on the 100 first samples (  X  6s) of each session. This choice is a compromise between an accu-rate estimation of the covariance matrices and the need for the criterion to be stable before the first change. The forgetting factor  X  was set to 0 . 98 such that it avoids numerical problem and gives a sufficient adapt-ability to the algorithm. Finally, the rank of the de-composition was set to p = 4 subspaces. This choice is a trade-off between summing up most of the views correlation and keeping a low rank representation for a fast optimization.
 In (dos Santos Teixeira &amp; Milidi  X u, 2010), a criterion in-volving the reconstruction error on the current sample has been used for anomaly detection. We adapted this criterion to our context and used it in order to detect the change in mental task of the subjects. Let n x = n y = 36, the number of features in both views x t and y . For a given sample, we calculate the residuals r t x and r t y of the projection of the current views on the pre-Finally, we average the norms of the residuals on both views (with respect to their metric) leading to the per-For the test session, we need to determine a threshold  X  above which a change is detected. For a given subject,  X  is defined as the minimum of the criterion c evaluated over the change points in the three training sessions. The course of the reconstruction criterion c over time is shown for each subject during each session in Figure 3. The vertical red dashed lines show the time step when the subject is asked to switch from a mental task to another. For every subject, the horizontal black dotted lines show the threshold  X  .
 We observe in Figure 3 that the criterion c t rapidly rises at the exact moment of the change and that the criterion oscillates in the next few samples after a change. So we decided to adopt the following decision rule for the test data : if the criterion is greater than the threshold  X  and if it was not previously (within 5 samples = 312ms) greater than the threshold  X  , a change is detected. Obviously, this is a naive rule and it could be improved by using more complex hypoth-esis test on the distribution of the criterion. However, as we want to highlight the effect of the CCA, this sim-ple thresholding heuristic is clearly sufficient as made clear by Table 1.
 In order to evaluate the performance of our adap-tive CCA method, we compare the AUC of the adaptive CCA to a state-of-the-art change detection method (Desobry et al., 2005). This method referred to as KCD in Table 1 is applied on the 96 precom-puted PSD features with 100 samples sliding windows. The parameters (  X  of the Gaussian kernel,  X  of the One-class SVM) algorithm were validated on the three training sessions in order to give the best AUC. As a comparison, we show the evolution of the KCD cri-terion in Figure 4. The CCA algorithm surpris-ingly outperforms the KCD algorithm. However, the AUC results may be explained by the strong a priori knowledge included in this approach by studying the correlation EEG signals between left and right hemi-spheres. Indeed this simple algorithm applied to fea-tures adapted to the problem outperforms a complex method applied on out of the box features. Our simple heuristic on the residuals of the adaptive CCA gives interesting results. It is notably able to detect changes without any delay and may be a good feature extraction for more robust change detection al-gorithm. Moreover, sensor and feature selection could also enhance the obtained results. Finally, using some stronger a priori on BCI data, this adaptive CCA ap-proach could be extended to classify the task after the change is detected. We proposed an adaptive formulation of the classi-cal CCA algorithm based on matrix manifolds. The Riemannian framework enabled us to build a fast and adaptive two-steps gradient algorithm. Moreover, we proposed an approach for change detection in the cor-relation of two time series. This simple approach was tested on a BCI dataset and gave promising results for True Positives 7 10 8 False Positives 3 0 1 False Negatives 1 1 3
True Negatives 3493 3461 3476
AUC (KCD) 81 . 12 68 . 63 58 . 35 detecting changes in mental tasks.
 We are currently exploring several extensions of this work. Our algorithm is based on a simple gradient and using the same manifold framework, a Newton method could be derived and should improve the dis-tance to the batch solution. This paper focused on an adaptive formulation of the problem, its extension in an online learning framework (Warmuth &amp; Kuzmin, 2006) should provide regret bounds for both views. Finally, Approximate Joint Singular Value Decompo-sition (AJSVD) (Congedo et al., 2011) is an exten-sion of CCA handling the correlations between multi-ple views. To our knowledge, it has not been studied in an adaptive settings which would be of interest to BCI application.

