 Customer satisfaction is a very important indicator of how successful a contact center is at providing services to the cus-tomers. Contact centers typically conduct a manual survey with a randomly selected group of customers to measure cus-tomer satisfaction. Manual customer satisfaction surveys, however, provide limited values due to high cost and the time lapse between the service and the survey.

In this paper, we demonstrate that it is possible to au-tomatically measure customer satisfaction by analyzing call transcripts enabling companies to measure customer satis-faction for every call in near real-time. We have identified various features from multiple knowledge sources indicating prosodic, linguistic and behavioral aspects of the speakers, and built machine learning models that predict the degree of customer satisfaction with high accuracy. The machine learning algorithms used in this work include Decision Tree, Naive Bayes, Logistic Regression and Support Vector Ma-chines (SVMs).

Experiments were conducted for a 5-point satisfaction mea-surement and a 2-point satisfaction measurement using cus-tomer calls to an automotive company. The experimental re-sults show that customer satisfaction can be measured quite accurately both at the end of calls and in the middle of calls. The best performing 5-point satisfaction classification yields an accuracy of 66.09% outperforming the DominantClass baseline by 15.16%. The best performing 2-point classifi-cation shows an accuracy of 89.42% and outperforms both the DominantClass baseline and the CSRJudgment baseline by 17.7% and 3.3% respectively. Furthermore, Decision Tree and SVMs achieve higher F-measure than the CSRJudgment baseline in identifying both satisfied customers and dissat-isfied customers.
 H.4 [ Knowledge Management ]: Mining and representing text, Classification Algorithms, Design, Experimentation, Measurement Customer Satisfaction, Contact Center Calls, Speech Ana-lytics, Natural Language Processing, Text Mining, Classifi-cation, Machine Learning
Contact centers are critical interfaces between companies and their customers. The top two goals of contact centers are reducing operational costs and improving customer satis-faction, i.e., providing the best quality services at the lowest possible cost. The two goals have been perceived not com-patible and having tradeoffs [1]. Companies have mostly focused on achieving the first goal by automating critical processes or outsourcing customer service to other countries with lower labor cost. Most research for contact centers have also been drawn to developing tools for improving agent pro-ductivity and saving the costs. Those tools range from real-time agent assistance [22] to automatic call monitoring [38] and semi-automated call logging [4].

Customer satisfaction (C-SAT) is a very important in-dicator of how successful a contact center is at providing services to the customers, and has been widely used in eval-uating the performance of a contact center. Research has shown that customer satisfaction has a strong correlation with profitability [11] and also has strong positive effects on customer retention [27]. A study by Bain &amp; Company found that, for many companies, an increase of 5% in customer re-tention can increase profits by 25% to 95% [28]. However, unlike productivity enhancement and cost saving, it is very hard to objectively measure customer satisfaction.
Most contact centers conduct a manual survey with a small group of customers to measure customer satisfaction. A manual customer satisfaction survey is typically conducted via a telephone interview or a mail-in form, in which cus-tomers are asked to evaluate each statement in the ques-tionnaire using a 5-point Likert scale [17]. A typical 5-point question on customer satisfaction is answered as X  X om-pletely Dissatisfied X , X  X omewhat Dissatisfied X , X  X eutral X , X  X ome-what Satisfied X , or  X  X ompletely Satisfied X .

Manual customer satisfaction surveys pose three major limitations. First, they are very expensive since most com-panies hire an external market research firm to conduct a survey. Second, because of the cost, the survey size is typ-ically very small, and, thus, the conclusions drawn from the survey are not very reliable. Typically, only 1 X 5% of callers are surveyed, and of these, only a small fraction re-sponds to the survey. A recent study finds that response rates have been falling across all forms of survey research for decades [2]. Third, a manual survey is typically con-ducted a couple of weeks after a case is finally closed, and, therefore, it is often too late to take an action to prevent customer defection.

Therefore, a tool that can automatically measure cus-tomer satisfaction for every call would be highly valuable. Such a tool enables companies to measure customer satisfac-tion for each and every call. Furthermore, with a real-time speech transcription system, customer satisfaction can be measured in real-time allowing supervisors to take over a call when a customer becomes unhappy and to resolve the customer X  X  issue.

In this work, we present a fully automated method for measuring customer satisfaction by analyzing automatically transcribed calls. The main technical contributions of the work are two folds. First, we identified various features which are highly correlated with C-SAT scores. The fea-tures indicate prosodic, linguistic and behavioral aspects of the speakers, and are automatically extracted from call tran-scripts and information stored in contact centers X  database. Second, we developed machine learning models that predict, with high accuracy, customer satisfaction based on the au-tomatically extracted feature set.

Experiments are carried out with 115 customer calls to an automotive company for a 5-point satisfaction measurement (i.e., from X 1 X  X o X 5 X ) and a 2-point satisfaction measurement (i.e., X  X atisfied X  X s.  X  X issatisfied X ) using four widely used ma-chine learning algorithms: Decision Tree, Naive Bayes, Lo-gistic Regression and Support Vector Machines (SVM). Two sets of customer calls are used in the experiments; one com-prising the entire conversations, and the other comprising only the first half of conversations.

The performance of automated systems are measured via 10-fold cross validation and are compared with two baseline methods. The first baseline method is an artificial classifier which assigns the dominant class to all calls (a.k.a, Dom-inantClass ). The second baseline is the customer service representative (CSR)s X  judgment on customer satisfaction (a.k.a., CSRJudgment ).

The experimental results show that customer satisfaction can be measured quite accurately both at the end of calls and in the middle of calls. The best performing 5-point satisfaction classification yields an accuracy of 66.09% out-performing the DominantClass baseline by 15.16%. The best performing 2-point classification shows an accuracy of 89.42% and outperforms both the DominantClass baseline and the CSRJudgment baseline by 17.7% and 3.3% respec-tively. Furthermore, Decision Tree and SVMs perform bet-ter than the CSRJudgment baseline in identifying dissatis-fied customers achieving 11.5% and 3.2% higher F-measure respectively.
Customer satisfaction has been said to be one of the most widely studied areas in marketing [3], but there has been little attempt to automatic customer satisfaction measure-ment. Recently, Godbole and Roy proposed a tool that help contact center Quality Analysts analyze customer feedback text by providing text classification and interactive docu-ment labeling [10]. To the best of our knowledge, however, there has been no previous research on customer satisfac-tion measurement by analyzing automatically generated call transcripts.

Some related bodies of work has been done in the text min-ing and natural language understanding areas. They include emotion detection in spoken dialogue [18, 8, 32], sentiment analysis and classification [24, 34, 37, 12, 9, 23, 36, 13] and opinion mining [14, 15, 16] for customer review or feedback documents. However, emotion or sentiment detection alone is not sufficient for measuring customer satisfaction. We an-alyzed contact center calls to study the relationship between customer satisfaction and the use of sentiment words by the customers. Figure 1 depicts the composition of  X  X atisfied X  calls and  X  X issatisfied X  calls in terms of the differences in the number of positive sentiment words and the number of negative sentiment words spoken by the customers. Figure 1: Relationship between customer satisfac-tion and sentiment words. The chart shows the comparison of X  X issatisfied X  X alls and X  X atisfied X  X alls with respect to the relative use of positive sentiment words and negative sentiment words. diff is com-puted by subtracting the number of negative senti-ment words from the number of positive sentiment words (i.e.,  X  X ositive X  - X  X egative X ) spoken by the customers.

As we can see from the figure, both satisfied customers and dissatisfied customers use more positive sentiment words re-gardless of their satisfaction level. Only 8% of dissatisfied customers use same number or more negative words than positive words, while 6.4% of satisfied customers also used same or more negative words than positive words. The anal-ysis result indicates that the difference between the positive sentiment words and the negative sentiment words spoken by customers in  X  X atisfied X  calls and  X  X issatisfied X  calls is negligible. This analysis results motivate us to look beyond customers X  sentiment for measuring customer satisfaction.
The main differences of our work from the related work are the following. Firstly, customer satisfaction is an overall judgment based on cumulative experience with the service and is influenced by multiple factors, including the customer service quality, the time duration spent to have the issue resolved, whether a compensation (or other goodwill token, e.g., discount or reimbursement) was offered, to name a few. Therefore, to capture the influence of these multiple factors, various knowledge sources need to be exploited to estimate the level of customer satisfaction. In this work, we identified both structured and unstructured features which are highly correlated with C-SAT scores.

Secondly, unlike review or feedback text which are in-tended to express the authors X  opinions, customer calls often contain no explicit emotional expressions or multiple emo-tional states. Some customers do not express their sentiment or satisfaction level explicitly during a call. Some customers change their sentiment as the call progresses and the issue gets resolved. Some customers expresses different sentiments toward different objects in a call. In the automotive com-pany X  X  case, many customers express their dissatisfaction with the dealership, but they are generally satisfied with the contact center service.

Thirdly, automatic call transcripts are highly noisy and fragmentary due to word recognition errors of the automatic speech recognition (ASR) system and high rate of interrup-tions and repeats during conversations. Therefore, applying text mining on automatic call transcripts is much more chal-lenging than on review-type text.

Lastly, most of the related work focused on the binary distinction of positive vs. negative for an opinionated text. Pang et al. attempted to generalize the problem of catego-rizing opinionated text into a finer-grained classification task (three or four classes) [23]. In this work, we conduct experi-ments for both a binary and a 5-ary distinction of customer satisfaction.
Customer satisfaction has traditionally been measured by interviewing a small set of selected customers. C-SAT sur-veys often measure customer satisfaction level from  X 1 X  to  X 5 X  using a 5-point Likert scale. However, the differences among the scores are very hard to distinguish even for hu-mans. Especially, the distinctions between  X 1 X  ( X  X ompletely dissatisfied X ) and X 2 X ( X  X omewhat dissatisfied X ), and between  X 4 X  ( X  X omewhat satisfied X ) and  X 5 X  ( X  X ompletely satisfied X ) are very vague.

The main goal of conducting customer satisfaction sur-vey is in identifying satisfied customers and dissatisfied cus-tomers to evaluate the performance of their contact cen-ter and to identify areas for service quality enhancement. Therefore, in most cases, a binary classification of customers into satisfied customers and dissatisfied customers might be sufficient.

In this work, we investigate the feasibilities of real-time measurement of customer satisfaction for both classification scenarios. 1. 5-point satisfaction classification assigning contact cen-2. 2-point satisfaction classification assigning contact cen-
The main goals for this study are two-fold. First, we aim to identify feature combinations that are highly cor-related with customer satisfaction scores and can be au-tomatically extracted from data sources available in most contact centers. Second, we aim to identify machine learn-ing approaches which can measure the degree of customer satisfaction with reasonably high accuracy.
In this section, we describe the four machine learning al-gorithms used in this work, and explain the features in great detail.
To our knowledge, this is the first attempt for applying natural language processing (NLP) and machine learning technologies to automatically measure customer satisfaction by analyzing call transcripts. Therefore, we explore several machine learning algorithms which have been successfully used for many other NLP tasks and compare the models to find a best model for customer satisfaction classification. Specifically, we apply the following four classification meth-ods: Decision Tree, Naive Bayes, Logistic Regression (a.k.a., maximum entropy classifier), and Support Vector Machines (SVMs).

Decision Tree : A decision tree is a predictive model, which creates a tree providing a mapping from observa-tions about an item (i.e., attributes) to its target value (i.e.. class). In this work, we use C4.5 which builds decision trees using the concept of information entropy [26].

Naive Bayes : Naive Bayes algorithm is a simple prob-abilistic classifier based on applying Bayes X  theorem. The method assumes that all features are mutually independent, and parameter estimation for the naive Bayes models uses the method of maximum likelihood [20]. Given features x  X  X  and the class variable y , naive Bayes assigns a test example x = ( x 1 , ..., x k ) to the class y with the highest P ( y | x 1 , ..., x k ) = P ( y )
Logistic Regression : Logistic Regression models pre-dict the probability of an event (i.e., class) by fitting data to a logistic curve (a.k.a sigmoid curve). Logistic function is and  X  i  X  X  are regression coefficients. The success of a logistic regression method is dependent on the appropriateness of  X  X igmoid X  to match the known distribution.

SVMs: The main idea of SVMs is to find a hyperplane which splits the positive examples from negative examples with the largest distance in between the two example sets [35]. In this work, we use C-support vector classification (C-SVC) with a radial basis function (RBF) kernel.
Customer satisfaction survey results typically include ver-batim comments in which customers provide detailed expla-nations on why they are satisfied or dissatisfied (i.e., voice of the customer). Some sample customer comments which contain the reasons of  X  X ompletely satisfied X  and  X  X ess than satisfied X  are listed in Table 1.

To learn what factors influence customer satisfaction with a contact center service, we analyzed the verbatim comments in 16,500 C-SAT survey results of the automotive company to identify potential features for C-SAT prediction that can be used to build a C-SAT model. Table 2 lists the most frequently mentioned reasons for being completely satisfied. Interestingly, the reasons for being less than satisfied are essentially the opposites of the reasons for being completely satisfied.

We also analyzed sample call transcripts to identify good determinants of customer satisfaction. Example aspects we investigated include the call duration, the number of on-holds during a call, sentiment words, competitor mentions,
They were very helpful. Very instructive in explain-ing how to handle the problem I had. Very, very friendly and explaining it to me. No frustration.
They were very friendly. Very knowledgable of what the problem was. They directly guided me through the problem I had.

Well I called and they responded. Although they said within an hour they came in ten minutes in-stead, it made me very happy.
 Customer Assistance Center.
 and talk speed of the speakers, etc. Based on the analysis of the verbatim comments and the call transcripts, we se-lect the following 20 features which show high correlation with C-SAT scores. The features are categorized into struc-tured, prosodic, lexical and contextual features based on the knowledge sources.
Structured features include features that are not usually available in call transcripts, but can be extracted from the contact center X  X  database. Our analysis show that the fol-lowing two structured features are highly correlated with C-SAT scores.

Goodwill: This feature provides information on whether a goodwill token was offered to the customer, and the type of goodwill offered.

Previous Inbound Interactions: Inbound interactions include any customer-initiated contacts to the contact cen-ter. Examples of inbound interactions are calls, emails or in-stant messaging which the customer initiated. This feature is the number of previous inbound interactions the customer has made before the telephone conversation.
Prosodic attributes of a conversation provide valuable in-formation about the nature of call, and have widely been used in speech act and dialogue understanding [8, 33]. These attributes can imply the emotional status of the speakers. In this work, we extract the following six prosodic features which can indicate a customer X  X  satisfaction level, and are available in call transcripts. Please note that the presented system is not integrated with an ASR system, and, thus, prosodic features that can only be extracted from acoustic signals such as energy, pitch and F0 are not used.
Long Pause: Long pauses during a call can influence the flow of conversation. For instance, many long pauses by the agent can annoy the customer. In this work, we define a long pause as a pause between two adjacent words lasting more than 5 seconds. The number of all long pauses during a call is used as a feature for classification.

Call Dominance: This feature represents who domi-nated the conversation in terms of the talking time. Our study found that dissatisfied customers tend to dominate the calls more than satisfied customers.

The call dominance rate is computed based on the relative talking time between the speakers. The talking time of each speaker ( TalkingTime ( S i )) during a call is computed using the following equation.
 where U ij denotes the j -th utterance spoken by speaker S
The call dominance rate of a speaker S i , D ( S i ), is com-puted as the percentage of the speaker X  X  talking time over the talking time of all speakers.
 In this work, we use the call dominance rate of the customer as a feature.

Talking Speed: This feature measures the average talk-ing speed of a speaker. The average talking speed of a speaker is computed by the number of words spoken by the speaker divided by the speaker X  X  talking time in the call.
Our analysis on the speakers X  average talking speed reveals interesting insights. Agents tend to talk faster in calls that were reported to be  X  X atisfied X  calls than in calls reported to be  X  X issatisfied X  calls (average speed 1.9 in  X  X atisfied X  calls vs. 1.5 in  X  X issatisfied X  calls). On the other hand, customers tend to speak faster during  X  X issatisfied X  calls (2.5 in  X  X at-isfied X  calls vs. 2.8 in  X  X issatisfied X  calls). In this work, the talking speed of both the customer and the CSR are included in the feature set.

Barge-in: Interrupting during the other person X  X  speech may indicate that the person is loosing patience. When an utterance starts before the previous utterance ends, we regard the utterance as a  X  X arge-in X . The numbers of barge-ins initiated by both the CSR and the customer are included in the feature set.
Previous work on spoken dialogue analysis mostly include word n-grams as lexical features [8, 32]. In this work, lexical features consist of words which may indicate the customer X  X  emotional state and class-specific words which can reliably distinguish one class from the others. We extract the fol-lowing eight lexical features.

Product Name: This feature specifies the product fam-ily name (in this work, the make of the vehicle) for which the customer is seeking a solution. Typically, customers reveal the product name when they describe the problem they are experiencing.

In this work, we apply a heuristic method using a product taxonomy to identify the product name in call transcripts. We select the first product name mention in the customer X  X  utterances as the product of interest. If no product name is found in the customer X  X  utterances, the first product name mentioned by the CSR is selected. In the case of the au-tomotive company, customers often mention the vehicle X  X  model name, but not the make. We infer the make name using a product taxonomy that provides the relationships between the models and the makes. When no product name is present in the call transcript, the product with most cus-tomers is used as the default value.

Filler: Fillers are words or sounds that people often say unconsciously that add no meaning to the communication. Examples of fillers in English include X  X h X ,  X  X h X ,  X  X mm X , etc. The frequency of fillers in a conversation is often reflective of a speaker X  X  emotional state. Most contact centers encourage their CSRs to minimize the use of fillers. In this work, the numbers of fillers spoken by the customer and the CSR are counted separately, and both numbers are used as features.
Competitor Name: Mentions of competitors or a com-petitor X  X  product are a good indicator of the customer dis-satisfaction with the product. For instance, an unhappy customer might say  X  X  will buy a XXX 1 next time X . This sentence does not contain any explicit sentiment, but it cer-tainly expresses a negative sentiment. In this work, we use a manually compiled lexicon of all automotive companies and their product names to recognize competitor mentions. a competitor X  X  name Only the number of competitors X  names mentioned by the customer is used.

Sentiment Word: Call center conversations also con-tain many words showing the speaker X  X  emotion or affect. To identify words with sentiment polarity, we use the sub-jectivity lexicon described in [36]. The lexicon contains a list of words with a priori prior polarity ( positive , negative , neutral and both ) and the strength of the polarity ( strong-subj vs. weaksubj ). In this work, we use only words of which prior polarity is either positive or negative , and the strength of the polarity is strongsubj . A few words which are frequently used non-subjectively in conversational text such as  X  X kay X ,  X  X ind X ,  X  X ight X , and  X  X es X  are removed from the sentiment word list.

We perform a local context analysis to decide the polarity of a sentiment word (see [36] for more complete contextual polarity analysis) in a context. If a sentiment word has a polarity shifter within a two word window in the left, the po-larity of the word is changed based on the shifter [25]. For instance, if a positive sentiment word appears with a nega-tion word, the polarity of word in the context is negative. The number of positive sentiment words and the number of negative sentiment words spoken by the customer are in-cluded in the feature set.

Category-specific Word: Some set of words tend to ap-pear more frequently in a certain category than other cat-egories and, thus, can reliably identify the category. We call these words category-specific words. Category-specific words are automatically extracted based on Shannon X  X  en-tropy, which is a measure of the degree of randomness or un-certainty [30]. More specifically, we define category-specific words as words that appear frequently in the corpus and have low entropy.

The entropy of a word is computed as follows. We first created a corpus of call transcripts, which comprises only the last calls of service requests with manual customer sat-isfaction survey results. 2 We then calculate the probability of a word, w , appearing in the  X  X atisfied X  category (i.e., C-SAT score  X 4 X  or  X 5 X ) and the probability of w appearing in the  X  X issatisfied X  category (i.e., C-SAT score  X 1 X  or  X 2 X ). where f s ( w ) and f d ( w ) denote the counts of word w in the  X  X atisfied X  call set and in the  X  X issatisfied X  call set respec-tively, and f ( w ) = f s ( w ) + f d ( w ).
 The entropy of w , H ( w ), is defined as in Equation 1.
In this work, we select words that appear 20 times or more in the corpus, and the entropy is equal to or less than 0.9 (i.e., words appearing in a category 68% or more of the time) as category-specific. Furthermore, if p s ( w ) is bigger than p d ( w ), the word w is regarded as a X  X atisfied X  X ord , and otherwise as a X  X issatisfied X  X ord. The numbers of X  X atisfied X  words and  X  X issatisfied X  words spoken by the customer are used as features. we hypothesize that customer satisfaction is more influ-enced by the last call than earlier calls category contains more legal terms such as  X  X awyer X  and  X  X ourt X .
Table 3 lists the most  X  X atisfied X  words and  X  X issatisfied X  words.
Contextual features are phrases or expressions used in cer-tain contexts which can affect the customer X  X  satisfaction level. Based on our analysis of customers X  X omments and sample call transcripts, we identified the following four con-textual features.

CSR X  X  Positive Attitude: These features intend to re-flect the CSR X  X  positive attitude toward the customer. We manually collected a list of phrases which CSRs often use to express courteousness or to rephrase the customer X  X  prob-lem. For instance,  X  X et me see if I understood... X  and  X  X s I understand, ... X  can hint that the CSR is trying to under-stand the customer X  X  question correctly. Also, expressions like  X  X  am happy to assist/resolve/address .. X  and  X  X  am sorry to hear .. X  in the beginning of a call can indicate that the CSR was sympathetic and willing to help the customer. In this work, we count the number of such expressions in the first ten utterances spoken by the CSR.

CSR X  X  Contact Information: As noted in Table 2, cus-tomers consider a CSR responsible when the CSR provided her contact information for the customer to be able to reach the CSR directly in a later time. Example of the expressions are  X  X urther question X ,  X  X y number X ,  X  X ontact information X ,  X  X xtension X , and  X  X all me back X . We recognize these expres-sions in the last ten utterances spoken by the CSR. Follow-up Schedule: A follow-up is a call made by the CSR to the customer after the current call is ended. We can not know from the transcript of the current call if there was a follow-up. Instead, we check if the CSR scheduled a follow-up during the conversation.

A follow-up schedule can be an attribute for a responsi-ble CSR, but also can indicate that the customer X  X  problem was not resolved during the call. CSRs usually schedule a follow-up at the end of the call, and obtain the customer X  X  contact information. We recognize the existence of a follow-up schedule by identifying cue words such as  X  X all you back X  and  X  X ouch base X  and expressions for a telephone number, day and hour information in the last 20 utterances.
Gratitude: Finally, we look at the customer X  X  response at the end of the call. When the customer uses many ex-pressions showing gratitude such as X  X ppreciate X  X nd X  X reat X , that can indicate that the customer is satisfied. We count the number of such expressions in the last ten utterances spoken by the customer.
As described in Section 3, we conduct experiments for 5-point satisfaction classification and 2-point satisfaction clas-sification using two sets of customer calls; a call transcript set comprising entire conversations and a call transcript set comprising only the first half of conversations. Especially, the fist half conversations are used to investigate the feasi-bility of measuring customer satisfaction in real-time when the conversation is still in progress.

The experiments were conducted with RapidMiner, a ma-chine learning toolkit offering a wide range of methods for data pre-processing, machine learning and validation [19]. We used the default settings in RapidMiner for Decision Tree and Naive Bayes. For Support Vector Machines, we use the C-support vector classification (C-SVC) with a radial basis function (RBF) kernel as implemented in LIBSVM [5]. Lo-gistic Regression uses maximum likelihood, which is an iter-ative procedure. We set the maximum number of iterations to 300. The standard Logistic Regression in RapidMiner ap-plies only to binary classification, and it was extended to a multiclass classifier for the 5-point satisfaction classification using  X  X ne-against-all X  strategy [29].
We acquired customer calls to a contact center of the au-tomotive company which were recorded during a two month period time in 2007. The call set constitutes the base source of our experimental data. The calls were transcribed using the IBM Attila Speech recognition toolkit [31]. The ASR system was retrained with sample customer calls from the same contact center as well as general conversational tele-phony speech data and broadcast news, and shows an overall word error rate of 26%.

To develop supervised machine learning systems, we need annotated ground truth data. Hand annotation of customer satisfaction is not only time consuming but also very diffi-cult. Customer satisfaction is very subjective and, thus, is hard to achieve high inter-annotator agreement as experi-enced in previous work on sentiment analysis [8, 32, 6]. To avoid the need of costly and inconsistent human annotation, we use manual C-SAT survey results as the ground truth. We argue that the satisfaction ratings in the surveys are in fact hand annotation done by the customer themselves and, thus, most accurate.

We obtained the manual C-SAT survey results conducted for the calls used in this work by matching the surveys with the customer calls. Note that a C-SAT survey is conducted for a service request not for an individual call. A service request typically consists of multiple interactions between a customer and one or more agents via multi-modal media including telephone conversations, emails and postal mail. In many cases, a service request comprises more than one telephone conversations resulting in a 1-to-n relationship be-tween a C-SAT score and customer calls. A C-SAT score for a service request reflects the customer X  X  cumulative experi-ence across multiple interactions with the contact center.
To mitigate this problem, we selected the service requests which involved only one incoming call from the respective customers, resulting in 115 service requests. The cumulative call length of the 115 calls is 27 hours 34 minutes 55 seconds, and the call transcripts contain 171,860 tokens and 16,323 speaker turns. Among 16,323 utterances, 8,139 utterances were spoken by the CSRs, and 8,184 utterances were spoken by customers showing almost same talk distribution by the CSRs and the customers.
In this work, we use the following two baseline systems for performance evaluation purpose. The first baseline system is an artificial classifier which assigns all calls to the most frequent class (i.e., C-SAT score  X 5 X  for 5-point classifica-tion, and  X  X atisfied X  for 2-point classification). This baseline is called DominantClass hereafter. Table 4 shows the distri-bution of the 115 calls across the five C-SAT scores and the three categories.
 Table 4: The number of calls across numerical C-SAT scores and three categories. The accuracy of the DominantClass baseline is 57.39% and 75.96% for 5-point classification and 2-point classification respectively.

The second baseline comes from the CSRs who handled the customer calls. In the contact center, the CRSs are re-quired to judge if the customer is satisfied or dissatisfied when they close a service request, and to record their judg-ment in the database. We use the CSRs X  judgment as the second baseline system, and call it CSRJudgment . Note that the CSRJudgment baseline can only be used for the 2-point satisfaction classification.

Table 5 shows the contingency table of the CSRs X  judg-ment on customer satisfaction. As we can see in the table, CSRs identified satisfied customers with high precision and recall, but recall for dissatisfied customers is very low.
In this section, we discuss the experimental results of the 5-point satisfaction classification and the 2-point satisfac-tion classification at the end of calls. The performance of the automatic systems are compared with the two baseline systems based on average classification accuracy, precision, recall and F 1 -measure of 10-fold cross validation. Table 5: The contingency table of the customer ser-vice representatives X  judgment on customer satis-faction. The accuracy of the CSRJudgment base-line is 86.54%, and F 1 measures for  X  X atisfied X  calls and  X  X issatisfied X  X alls are 91.36% and 69.57% re-spectively.
The accuracy of the baseline system and the four classi-fication systems for 5-point C-SAT classification are sum-marized in Table 6. The second column ( All ) displays the best performance of each algorithm when all features were used. The accuracy reported here is the average accuracy of 10-fold cross validation.
 Table 6: Accuracy for 5-point C-SAT classification. All numbers are in percentage.

As we can see from the table, all four automatic methods outperform the DominantClass baseline. The SVM-based approach achieves the best accuracy (66.09%) which out-performs the baseline method by over 15%. There is no substantial performance difference among the other three approaches.
The experimental results of 2-point satisfaction classifica-tion are described in Table 7 in detail. For 2-point satisfac-tion classification, we compare the four automatic systems with both the DominantClass baseline and the CSRJudg-ment baseline in terms of classification accuracy, precision, recall and F-measure.

The highest classification accuracy for 2-point satisfaction classification (89.42%) was achieved by the decision tree-based approach and the SVM-based approach. The methods outperform the DominantClass baseline and the CSRJudg-ment baseline by 17.7% and 3.3% respectively. Further-more, both systems produce higher F-measure values than the CSRJudgment baseline in identifying both satisfied calls and dissatisfied calls. Specially note that the decision tree-based system achieves 11.5% higher F-measure than the hu-man judgment for identifying dissatisfied calls.
In this section, we discuss the relative contributions of the different feature types to automatic C-SAT measurement. We ran the experiments with one feature type removed at a time (i.e., leave-one-out), and compare the results. All in-dicates that all features were used. All-Str , All-Pro , All-Lex and All-Con indicate the cases where the structured, prosodic, lexical and contextual features were removed re-spectively.

The comparison of classification accuracy for 5-point and 2-point classification with the different feature sets are de-picted in Figure 2 and Figure 3 respectively. As we can see from the charts, the All model outperforms all other models except Decision Tree X  X  All-Pro model for 2-point satisfac-tion. Also note that structured and lexical features have bigger impact on C-SAT measurement than the other two feature types.
 Figure 2: Effects of the different feature sets on 5-point satisfaction classification
In the previous section, we showed that customer satis-faction can be automatically measured with high accuracy by analyzing by analyzing the conversation between a cus-tomer and a CSR. Another interesting question is that if we can X  X redict X  X -SAT in real-time, i.e., when the conversation is still in progress. With a real time transcription system, such tools can enable supervisors take over a call when a customer becomes unhappy to resolve the customer X  X  issue.
To answer this question, we conduct experiments with only the first half of the conversations and measure how accurately we can predict C-SAT in the middle of a call. Since SVMs and Decision Tree methods were proven to be best performing approaches, we carried out the experiments only with the two approaches. Furthermore, it is worth not-ing that  X  X oodwill X  information is typically available at the Figure 3: Effects of the different feature sets on 2-point satisfaction classification end of calls, and thus we removed  X  X oodwill X  feature from the feature set. All other feature values were extracted from the automatically transcribed transcripts of the first half of the calls.

Figure 4 depicts the accuracy comparison of customer satisfaction measurement using the first half of calls with the results obtained from using the entire calls, for both 5-point satisfaction classification and 2-point satisfaction clas-sification. Both Decision Tree and SVM perform better when the entire conversations were available. However, both methods significantly outperform the DominantClass base-line even with only the half of calls. Also note that the SVM method produces the accuracy comparable to the CSRJudg-ment baseline for the 2-point satisfaction classification.
Figure 5 shows the comparison of F-measure of 2-point satisfaction classification. The results confirm that analyz-ing the entire conversations provides more accurate predic-tion of customer satisfaction than analyzing only partial con-versations. As we can see from the figure, the degree of per-formance degradation is larger for  X  X issatisfied X  calls than for  X  X atisfied X  calls. Also, SVMs are shown to be less prone to the information loss than Decision Tree for identifying both  X  X atisfied X  calls and  X  X issatisfied X  calls.

The main reasons of the performance degradation might be the following. First, the  X  X oodwill X  feature is not used at all for the experiment with the first half of calls. Second, contextual features including  X  X SR X  X  Contact Information X ,  X  X ollow-up Schedule X  and  X  X ratitude X  typically appear at the end of calls. Therefore, these features are mostly absent for C-SAT prediction in real-time. It is worth noting that the presence of X  X SR X  X  Contact Information X  X nd X  X ollow-up Figure 4: Accuracy comparison of customer satis-faction measurement with two different sets of cus-tomer calls.  X  X ntireCalls X  denotes the classifica-tion results from using the entire conversations, and  X  X alfCalls X  shows the classification results from an-alyzing only the first half of the conversations. The dashed line denotes the accuracy of the Dominant-Class baseline, and the solid line denotes the accu-racy of the CSRJudgment baseline.
 Schedule X  often indicate that the customer X  X  issue was not resolved during the call. The results seem to support the findings by other research on the strong correlation of first call resolution and customer satisfaction [7, 21].
Customer satisfaction is one of the key performance indi-cators of contact centers. However, due to high cost, contact centers conduct a manual survey with a very small number of customers limiting the value of the survey results. The primary goal of this work is to investigate if customer sat-isfaction can be automatically measured by analyzing auto-matically generated call transcripts using NLP and machine learning (ML) technologies. Such tools can enable compa-nies to measure customer satisfaction for each and every call in near real-time, and, thus, to obtain more reliable knowl-edge about customer satisfaction.

We analyzed manual customer satisfaction survey results and sample call transcripts to identify features that are highly correlated with customer satisfaction scores. Analysis of such survey results can provide features for C-SAT predic-tion that can be used to build a C-SAT model for predicting C-SAT with high accuracy. Our experiments show that au-tomatic C-SAT measurement using machine-generated call transcripts is feasible. Automatic C-SAT measurement at the end of calls outperform human judgment in terms of both overall classification accuracy and F-measure. Experi-ments for measuring customer satisfaction in real-time, i.e., while the conversation is still in progress, also produce classi-fication accuracy comparable to human judgment with much less information. The results imply that, with a real time transcription system, such tools can allow supervisors take over a call when a customer becomes unhappy and to resolve the customer X  X  issue directly preventing customer defection.
To further improve the accuracy of automatic C-SAT mea-surement, we plan to extend the feature set to include acous-Figure 5: Comparison of F-measure for  X  X atisfied X  calls and  X  X issatisfied X  calls in the cases where the entire calls were used and where only half of the calls were used.  X  X T X  stands for  X  X ecisionTree X  in the legend.  X  Entire X  and  X  Half X  denotes the cases where the entire conversation set and where the half conversations were used respectively. tic features such as F0, pitch and energy level of the voices, call information such as the call waiting time, and call his-tory information such as if a promised follow-up call was actually made. [1] Anderson, E. W., C. Fornell, and R. T. Rust: 1997, [2] Brennan, M., S. Benson, and Z. Kearns: 2005,  X  X he [3] Burton, S. M.: 1997,  X  X odelling the Determinants of [4] Byrd, R. J. N. M. S., W. Teiken, Y. Park, K. F. [5] Chang, C. and C. Lin: 2001,  X  X IBSVM: a library for [6] Devitt, A. and K. Ahmad: 2008,  X  X entiment Polarity [7] Feinberg, R., I.-S. Kim, L. Hokama, K. de Ruyter, and [8] Forbes-Riley, K. and D. Litman: 2004,  X  X redicting [9] Gamon, M.: 2004,  X  X entiment classification on [10] Godbole, S. and S. Roy: 2008,  X  X ext Classification, [11] Hallowell, R.: 1996,  X  X he relationships of customer [12] Hu, M. and B. Liu: 2004,  X  X ining and Summarizing [13] Kanayama, H. and T. Nasukawa: 2006,  X  X ully [14] Kim, S.-M. and E. Hovy: 2006a,  X  X xtracting Opinions, [15] Kim, S.-M. and E. Hovy: 2006b,  X  X dentifying and [16] Kobayashi, N., K. Inui, and Y. Matsumoto: 2007, [17] Likert, R.: 1932,  X  X  Technique for the Measurement of [18] Liscombe, J., J. Venditti, and J. Hirschberg: 2003, [19] Mierswa, I., M. Wurst, R. Klinkenberg, M. Scholz, and [20] Mitchell, T.: 1997, Machine Learning . McGraw Hill. [21] Monger, J., M. Rudick, and L. O X  X lahavan: 2004, [22] Nambiar, U., H. Gupta, and M. Mohania: 2007, [23] Pang, B. and L. Lee: 2005,  X  X eeing Stars: Exploiting [24] Pang, B., L. Lee, and S. Vaithyanathan: 2002, [25] Polanyi, L. and A. Zaenen: 2004,  X  X ontextual Valence [26] Quinlan, J. R.: 1993, Programs for Machine Learning . [27] Ranaweera, C. and J. Prabhu: 2003,  X  X he influence of [28] Reichheld, F. F.: 2001, Loyalty Rules: How Today X  X  [29] Rifkin, R. M. and A. Klautau: 1998,  X  X n defense of [30] Shannon, C. E.: 1948,  X  X  Mathematical Theory of [31] Soltau, H., B. Kingsbury, L. Mangu, D. Povey, G. [32] Somasundaran, S., J. Ruppenhofer, and J. Wiebe: [33] Takeuchi, H., L. Subramaniam, T. Nasukawa, , and S. [34] Turney, P. D.: 2002,  X  X humbs Up or Thumbs Down? [35] Vapnik, V.: 1995, The Nature of Statistical Learning [36] Wilson, T., J. Wiebe, and P. Hoffmann: 2005b, [37] Yi, J., T. Nasukawa, R. Bunescu, and W. Niblack: [38] Zweig, G., O. Siohan, B. Ramabhadran, D. Povey, L.
