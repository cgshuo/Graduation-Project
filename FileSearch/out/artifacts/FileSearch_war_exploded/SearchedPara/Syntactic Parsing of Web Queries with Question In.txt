 As the World Wide Web grows in volume, it encompasses ever-increasing amounts of text. A major gateway to this invaluable resource is Web queries which users compose to guide a search engine in retrieving the information they desire to inspect. Automatic processing of Web queries is therefore of utmost importance.
Previous research (Bergsma and Wang, 2007; Barr et al., 2008) suggested that many Web queries are trivial in structure, usually embodying entity lookup, e.g.  X  frozen  X  or  X  condos in NY  X . However, with the increasing popular-ity of Community Question Answering (CQA) sites, such as Yahoo Answers, StackOverflow and social QA forums, more Web queries encompass information needs related to questions that these sites can answer. We found that this subcategory of queries, which we call CQA queries (following Liu et al. (2011; Carmel et al. (2014)), exhibits a wide range of structures. This suggests that the process-ing of such queries, which constitute  X  10% of all queries issued to search engines (White et al., 2015), may benefit from syntactic analysis (Tsur et al., 2016).

Recent progress in statistical parsing (Choi et al., 2015) has resulted in models that are both fast, pars-ing several hundred sentences per second, and accurate. These parsers, however, still suffer from the problem of domain adaptation (McClosky et al., 2010), excelling mostly when their training and test domains are similar. This problem is of particular importance in the hetero-geneous Web (Petrov and McDonald, 2012) and is ex-pected to worsen when addressing queries, due to their non-standard grammatical conventions.
 Some recent work addresses the syntactic analysis of User Generated Content (UGC) (Petrov and McDonald, 2012; Eisenstein, 2013; Kong et al., 2014). Yet, these efforts generally focus on UGC aspects related to gram-matical mistakes made by users (Foster et al., 2008) and to the unique writing conventions of specific Web plat-forms, such as Twitter (Foster et al., 2011; Kong et al., 2014). Our analysis of thousands of CQA queries, how-ever, reveals that regardless of such issues, CQA queries are generated by a well-defined grammar that sometimes deviates from the one used to generate the standard writ-ten language of edited resources such as newspapers. Consequently, this work has two main contributions. First, we extend the standard dependency grammar to de-scribe the syntax of queries with question intent. The ex-tended grammar is driven by the concept of a syntactic segment : an independent syntactic unit within a poten-tially larger syntactic structure. A query may include several segments, potentially related to each other se-mantically but lacking an explicit syntactic connection. Hence, query analysis consists of the query X  X  segments and their internal dependency structure, and may be com-plemented by the inter-segment semantic relationships. Therefore, we constructed a new query treebank consist-ing of 5,000 CQA queries, manually annotated accord-ing to our extended grammar. A comparison of direct application of an off-the-shelf parser (Clear (Choi and McCallum, 2013)) trained on edited text (OntoNotes 5 (Weischedel et al., 2013)) to a raw query with the appli-cation of the same parser to the gold-standard segments of that query is given in Fig. 1.

Second, we develop two CQA query parsing algo-rithms that can adapt any given off-the-shelf dependency parser trained on standard edited text to produce syntactic structures that conform to the extended grammar. Both our algorithms employ distant supervision in the form of a training set consisting of millions of (query, title) pairs. The title is the title of the Yahoo Answers ques-tion page that was clicked by the user who initiated the query . The alignment between the query and the title provides a valuable training signal for query segmenta-tion and parsing, since the title is usually a grammatical question. Both algorithms employ an off-the-shelf parser, but differ on whether segmentation and parsing are per-formed in a pipeline or jointly.

We compared our algorithms to several alternatives: (a) Direct application of an off-the-shelf parser to queries; (b) A supervised variant of our pipeline algorithm where thousands of manually segmented queries replace the dis-tant supervision source; and (c) A pipeline algorithm sim-ilar to ours where segmentation is based on the predic-tions of a query language model. We report results on two query treebank tasks: (a) Dependency parsing, reporting Unlabeled Attachment Score (UAS); and (b) Query seg-mentation, which reflects the core aspect of the extended grammar compared to the standard one.

In experiments on our new treebank, our joint model outperformed the alternatives on UAS for the full test set and for the subset of single-segment queries. Our pipeline model excelled both on UAS and on segmentation F1 for two large subsets that are automatically identifiable at test time: (a) Queries that consist mostly of content words (42.4% of the test set); and (b) Queries for which the confidence score of the off-the-shelf parser is at most 0.8 (30% of the test set). It also beat all other models on the subset of multi-segment queries. The Web attracts considerable NLP research attention (e.g. Eisenstein (2013)). Here we focus on grammar and parsing of Web data in general and queries in particular. Syntactic Query Analysis Web queries differ from standard sentences in a number of aspects: they tend to be shorter, not to follow standard grammatical conven-tions, and to convey more information than can be di-rectly inferred from their words. Consequently, a number of works addressed their syntactic analysis. Allan and Raghavan (2002) use part-of-speech (POS) tag patterns in order to manually map very short queries into clarifi-cation questions, which are then presented to the user to help them clarify their intent. Barr et al. (2008) trained POS taggers for Web queries and used a set of rules to map the resulting tagged queries into one of seven syntac-tic categories, whose merit is tested in the context of in-formation retrieval tasks. Manshadi and Li (2009) and Li (2010) addressed the task of semantic tagging and struc-tural analysis of Web queries, focusing on noun phrase queries. Bendersky et al. (2010) used the POS tags of the top-retrieved documents to enhance the initial POS tag-ging of query terms. Bendersky et al. (2011) proposed a joint framework for annotating queries with POS tags and phrase chunks. Ganchev et al. (2012) trained a POS tagger on automatically tagged queries. The POS tags of the training queries are projected from sentences contain-ing the query terms within Web pages retrieved for them. The retrieved sentences were POS tagged using an off-the-shelf tagger. These works, as opposed to ours, do not aim to produce a complete syntactic analysis of queries. Syntactic Parsing of Web Data To the best of our knowledge, only a handful of works have aimed at build-ing syntactic parsers for Web data. Petrov and McDonald (2012) conducted a shared task on parsing Web data from the Google Web Treebank, consisting of texts from the email, weblog, CQA, newsgroup, and review domains. The participating systems relied mostly on existing do-main adaptation techniques to adapt parsers trained on existing treebanks of edited text to the Web. Foster et al. (2011) took a similar approach for tweet parsing. Con-trary to our approach, these works rely on existing gram-matical frameworks, particularly phrase-structure and de-pendency grammars, and do not aim at adapting them to domains such as Web queries, where standard grammar does not properly describe the language. This may be the reason Web queries were not included in the shared task.
A work that is more related to ours is Kong et al. (2014), who addressed the task of tweet parsing. Like us, they adapt the grammatical annotation scheme to the target linguistic domain and produce a multi-rooted syn-tactic structure. However, CQA queries and tweets ex-hibit different syntactic properties: (1) tweets often con-sist of multiple sentences, while CQA queries are con-cise in nature and usually correspond to a phrase, a frag-ment of a sentence, or several of these concatenated; and (2) queries are generated in order to retrieve information from the Web. Tweets, on the other hand, usually aim to convey a short message. These differences lead us to take approaches substantially different from theirs. 3.1 Motivating Analysis In this section we define the class of CQA queries and analyze their properties in comparison with other writing genres. This analysis will establish the motivation for the extension of standard dependency grammar so that it ac-counts for CQA queries (  X  3.2).

The data we analyzed consists of queries randomly sampled from the Yahoo Answers log. In cases where a searcher, after issuing a Web query on a search engine, viewed a question page on Yahoo Answers, a popular CQA site, this query is logged. From this log we sampled 100K queries for our analysis, as well as 5,000 additional queries for constructing a query treebank (see  X  3.3).
According to Barr et al. (2008), who analyzed queries from Yahoo X  X  search engine, 69.8% of general Web queries are composed of a single noun phrase, leaving lit-tle room for a meaningful taxonomy. Focusing on CQA queries removes this bias and allows exploration of ad-ditional syntactic categories of queries. Especially, we would like to delve into the categories Barr et al. label as word salad (e.g.  X  mp3s free  X ) and other-query (e.g.  X  florida reading conference 2006  X ), cited as composing 8.1% and 6.8% of all queries respectively and for which no analysis is offered.

To characterize the domain of CQA queries, we com-pare its properties to those of other Web domain sam-ples: (a) 100K general Web queries; (b) 120K titles of questions posted on Yahoo Answers; and (c) 100K story bodies from Yahoo News. In our analysis, individual sen-tences were identified using the OpenNLP sentence split-
Table 1 presents four measures of syntactic complex-ity (four leftmost measure columns). The first column re-ports the average number of word tokens per parsed item. The second column contains the median and mean de-pendency tree depths, defined as the number of edges in the longest path from the root node to a leaf in the tree. The third and fourth columns present the fraction of de-pendency tree root edges that go to words POS-tagged as nouns or as verbs, respectively. We use these last two measures as proxies of the syntactic category of the in-put text, with noun roots often indicating simple noun phrases and verb roots often indicating more complex Finally, the rightmost column of the table presents the average parser confidence per item provided by Clear, which we use as a proxy for parsing difficulty.
As the table shows, both CQA and general Web queries are harder to parse (according to parser confidence) com-pared to news article sentences and CQA question ti-tles. Yet, CQA and general Web queries strongly differ with respect to their syntactic complexity. Indeed, CQA queries have substantially more tokens and deeper trees. Moreover, while 62.9% of the root nodes in general query parse trees govern a noun and 30.3% govern a verb, in CQA queries the respective figures are flipped: 32.2% and 62.7%, respectively. 3.2 Dependency Grammar Extension for Queries Our analysis above reveals the special status of CQA queries. Like general Web queries, CQA queries are hard to parse. However, while the difficulty of parsing general Web queries may result from their short length and shal-low syntactic structure, CQA queries are longer and seem to have a deeper syntactic structure.

Based on our manual inspection of thousands of the queries used in the above analysis, we propose an exten-sion of the standard dependency grammar so that it ac-counts for CQA queries. Our reasoning is that the gram-matical structure of CQA queries is a syntactic forest. The query X  X  tokens are partitioned into one or more con-tiguous syntactic segments , each representing a maximal constituent unit syntactically independent of the other units. The final syntactic representation of the query con-sists of a set of trees produced according to the Stanford Dependency schema (De Marneffe and Manning, 2008), one tree per segment. Based on this observation, CQA queries may consist of several syntactically independent segments, as opposed to grammatical sentences which consist of exactly one segment. Importantly, query seg-ments are syntactically independent, although they tend to be semantically related.

Figure 2 provides examples of parsed queries. Query (a) consists of a single segment, a verb phrase rooted by the word make . Query (b) is composed of two segments that are syntactically independent, but semantically con-nected. Particularly, the first segment is an interrogative sentence rooted in the word done and the second is a noun phrase which specifies the pronoun it from the first seg-ment. Finally, query (c) consists of two segments, each a noun phrase, presumably connected by an is-made-of semantic relation. As in query (b), the segments of this query are syntactically independent, but unlike query (b), their semantic connection is more loose. The existence of such loose connections motivates us to exclude semantic subcategorization from the syntactic layer.

We note that the notion of segment has another mean-ing, within the task of query segmentation (Bergsma and Wang, 2007; Guo et al., 2008; Tan and Peng, 2008; Mishra et al., 2011; Hagen et al., 2012). This task X  X  goal is to identify words in the query that together form compound concepts or phrases, like  X  Chicago Bulls  X . As such, this task differs from ours as it defines segmentation in semantic rather than syntactic terms.

We also note that query segments are distinct from the concept of fragments in constituency parsing. Marcus et al. (1993) introduce fragments in order to overcome problems involving the attachment point of various mod-ifying phrases, e.g. in the sentence  X  In Asia, as [FRAG in Europe], a new order is taking shape  X . While proper treatment of such phrases often requires extra syntactic information, their syntactic connection to other parts of the sentence is present, unlike between query segments. 3.3 Query Treebank Following our proposed grammar, we constructed a tree-bank by manually annotating 5,000 queries that landed on Yahoo Answers (see  X  3.1). These queries were randomly split into a 4,000-query test set and a 1,000-query devel-opment set. Four human annotators segmented both sets, and parsed the test set with unlabeled dependency trees (including POS tags). The annotators X  sets did not over-lap, yet cross-reviews were made in cases deemed diffi-cult by them. On a validation set of 100 queries tagged by all annotators, the agreement scores measured were 0.97 for segmentation and 0.96 for dependency edges.
To evaluate how well out-of-the-box parsers conform with our grammar, we applied seven dependency parsers to the 4,000-query test set. We trained all parsers on the gold-standard segment of each query; and (b) to full, un-segmented, queries. For comparison we also recap the performance of the parsers on the OntoNotes 5 test set as reported in Choi et al. (2015). The same non-gold POS tags were provided to all parsers (see  X  6.2).

The results, presented in Table 2, establish the diffi-culty of our task: UAS differences between OntoNotes and full queries range from 13.4% to 18.6%. Moreover, injecting gold segmentation knowledge increases the per-formance of the parsers by 5.3-5.9%, highlighting the value of accurate segmentation in syntactic query anal-ysis. Finally, the ranking of the parsers with respect to their accuracy on queries differs from their ranking with respect to accuracy on OntoNotes, raising Redshift to first place and dropping RBG to last place.

Since knowledge of segment boundaries consistently improves parsing quality in our analysis, we next present two distant supervision approaches that attempt to dis-cover the segments of an input query and return a parse forest that adheres to the segment boundaries. As our first approach, we present a query segmentation algorithm which can be combined with an off-the-shelf parser in two ways in order to form a pipeline query pars-ing system: (a) first segmenting the query and then apply-ing the parser to each of the segments; or (b) first parsing the query and then fixing the resulting dependency tree so that it conforms with the segmentation. In the sec-ond setup the parser X  X  output is aligned against the seg-mentation such that dependency edges which cross seg-ment boundaries are re-assigned to become root edges. In our experiments we found that, for all our tested pipeline models, the first setup performed slightly better. We therefore report results only for this setup.

Our segmentation algorithm is based on the observa-tion that CQA queries are generated in order to express a searcher X  X  need which is likely to be formulated as a question on the web page that they then visit, and that this paraphrasing can serve as a source for query seg-mentation cues. The algorithm therefore inspects at train-ing time (query,title) pairs where title is the title of a Ya-hoo Answers question page that was clicked by the user who initiated query . A query for which a full word-wise alignment to the clicked question can be found is an-notated with segmentation markers according to several cues, then added to the training set.
 Segmentation Cues The following cues are used for detection of query segment boundaries:  X  Reordering : a part of the query which appears in  X  Intruding word classes : if between two words Training Set Generation We started with a query log for 60 million Yahoo Answers pages. We filtered out ti-tles of more than 20 words, titles that do not start with a question word, and titles that do not have an associated query which after lowercasing and punctuation removal contains only words from the title (as well as possibly  X  site:  X  terms or the word  X  to  X ). The remaining 7.5 mil-lion queries were automatically segmented according to the above cues. A sample of 100 queries was found to have a segmentation F1 score (defined in  X  6.2) of 64.5. Model and Training We trained a linear chain CRF set of 7.5M automatically segmented queries. The model employs the following standard features: (a) unigram and bigram word features (  X  2 and  X  1 windows around the represented word respectively); (b) unigram and bi-gram POS features (in a  X  2 window); (c) unigram word+POS features (  X  1 window); and (d) distance of word from start and end of query, as well as each distance combined with word and/or POS. Algorithm 1 Projection-based query parsing An alternative distant supervision approach is to employ the (query,title) pair set in the training of a model that maps queries to natural language questions. These in-ferred questions are supposedly easier to parse as they follow standard grammatical conventions. A query pars-ing algorithm that employs such a mapping component has three steps: first, a grammatical question with a sim-ilar information need to that of the query is automati-cally inferred. Then, the inferred question is parsed by an off-the-shelf dependency parser, trained on a gram-matical corpus. Finally, the question parse is projected onto the query, inducing the query X  X  multi-rooted syn-tactic forest. Algorithm 1 presents pseudo-code for the projection-based query parsing algorithm.

The first step maps a given user query to a syn-thetic natural-language question. For this step we imple-mented the Query-to-Question (Q2Q) algorithm of Dror et al. (2013). Q2Q maps queries into valid CQA ques-tions by instantiating templates that were extracted out of (query,title) pairs taken from the page view log of a CQA site. We trained the Q2Q algorithm on millions of (query,title) pairs taken from the Yahoo Answers log, where each title starts with a question word and query length  X  3 .

Our algorithm obtains the top inferred Q2Q ques-tion and parses it using an off-the-shelf parser, trained on grammatical English sentences (lines 2-3 in Algo-rithm 1). It then projects the question parse tree onto the original query to generate its syntactic structure, in ac-cordance with the extended dependency grammar (  X  3.2), as follows. First (lines 4-7), the algorithm traverses the question parse, removing all question tokens that do not appear in the query and reassigning the dependents of each removed token to be headed by its parent. In ad-dition, as prepositions and conjunctions inserted by the Q2Q templates are strong signals of segmentation, parse subtrees governed by such nodes are also treated as sepa-rate segments. Following this phase, all remaining ques-tion tokens appear in the query. The algorithm is then left with a syntactic forest, since the root node may have mul-tiple children, each one defining a query segment. Lines 8-10 extract these segments and their parse trees.
Figure 3 shows the projection process for two queries, the second of which demonstrates how an added prepo-sition ( X  on  X ) invokes segmentation. We emphasize that the only manually annotated data required for the train-ing of our joint model is a treebank of standard edited text (OntoNotes 5), used for training the off-the-shelf parser.
Since we expect the projected query parse to produce trees for contiguous segments, we only accept the top Q2Q result where the query word order is maintained in the question, and use an off-the-shelf parser which pro-duces projective trees. In addition, the projection algo-rithm may collapse several edges from the question tree into a single edge within a query segment tree, leaving the resulting label unspecified. In this work we evaluate unlabeled parse trees and therefore defer the treatment of this issue to future work (leaving GenerateLabel () in line 4 of Algorithm 1 unspecified). 6.1 Models We evaluated the following models (summarized in Ta-ble 4) on our new query treebank (  X  3.3): Distant Supervision Models Our proposed pipeline (  X  4) and joint (  X  5) models. Our pipeline model, de-noted by DistPipe , performs query segmentation and then parses each segment. Our projection-based joint model, denoted by Q2QProj , parses an inferred question for the query and projects the parse tree on the query. Both mod-els use the Clear parser for parsing. Importantly, both our models do not require any type of manually annotated queries (parsed or segmented) for training.

The Q2Q algorithm employed by our joint model re-turns a question for only 2954 of the 4000 test set queries. We thus reverted to the full-query parsing baseline when it returned no result.
 Baselines The first natural baseline is the Clear parser, which is employed by our models. We note that while in practice it generated a multi-segment structure for  X  2% of the development queries, compared to  X  25% of the queries in the gold standard annotation, making it ill-qualified for multi-segment queries.

As a second baseline, denoted by Lm , we constructed a pipeline model identical to ours expect that segmen-tation is performed with a Language model (LM) based approach. For this aim we applied a Neural-Network language model (NNLM) (Mikolov, 2012) to each input query. For each word the language model computes its likelihood given the current model state, which is based on previous words. We then used the 1,000-query devel-opment set to find the optimal probability threshold for which words with estimated probability under the thresh-old are considered  X  X urprising X  and therefore mark the beginning of a new segment. We learned a language pled queries of length  X  3 .
 Supervised Models We further compare our distant su-pervision approach to an algorithm that does use manu-ally annotated queries for training, denoted by Sup . For this aim we implemented a pipeline model identical to ours, except that the pairwise linear-chain CRF is trained on manually-annotated queries. The algorithm is trained and tested following a 5-fold cross-validation protocol over the 4,000-query test set.
 Ensemble Model We also tested the complementary aspects of distant and manual supervision by construct-ing the same pipeline model, except that segmentation is based on both types of supervision sources. We ex-perimented with various ensemble generation techniques, and the one that has shown to work best was a method that unifies the segmentation decisions of the distant-supervised and supervised CRFs: the model, denoted by Ens , considers a token to be a segment boundary if it is considered to be so by at least one of the CRFs. Gold Segmentation An upper-bound benchmark to our pipeline approach. This is a pipeline model, denoted by Gold , that is identical to ours, except that the segmenta-tion is taken from the gold standard. 6.2 Evaluation Tasks and Data Pre-Processing We consider two evaluation tasks: (a) Dependency pars-ing, reporting Unlabeled Attachment Score (UAS); and (b) Query Segmentation, reporting the F1 score, where each segment is represented by its boundaries: in order for an observed segment to be considered correct, both of its ends must match those of a gold segment.

All tested algorithms, except for our joint model, di-rectly segment and parse queries and hence require these queries to be POS-tagged. Thus, we POS-tagged the test-adapted to queries using the self-training algorithm of Ganchev et al. (2012). Our self-training set consisted of 14M (query,title) pairs from the Yahoo Answers log. This tagger reached 88.2% accuracy on our query treebank,
Analyzing our development set we noticed that a very strong indicator that a query is a grammatical, and thus consists of a single segment, is when it starts with a WH-word or an auxiliary verb. Hence, in all pipeline models, except Gold , we do not segment such queries but rather directly apply the Clear parser to them. In postmortem analysis of the test set we found that this indicator was correct for 93.2% of the 1600 detected queries with 40% recall with respect to the single-segment queries subset. 6.3 Results Tables 5 and 6 present the results of our experiments. The All Queries column in Table 5 reports the performance of the tested models on the full test set. Overall, methods based on distant and manual supervision are superior to the baseline methods in both measures. Interestingly, our Q2QProj model performs best in terms of UAS (77.1%) although its segmentation F1 is mediocre (63.5). In terms of UAS, the distant-supervised and supervised models ap-proach the performance of the upper bound gold standard segmentation. For example, Q2QProj is outperformed by Gold by only 3.6%. The Clear parser baseline, which is not trained to identify multiple segments, lags 5.7-5.8 F1 points behind the models that employ CRF segmentation. This is translated to a difference in UAS of up to 1.7%. The Lm baseline, on the other hand, scores substantially lower than the other models in both measures. This may be an indication of the syntactic, rather than lexical, na-ture of our task.

In development set experiments we were able to char-acterize two subsets of queries on which our distant su-pervised pipeline model performs particularly well, out-performing even the supervised pipeline algorithm which requires thousands of manually annotated queries for training. One such set is the subset of queries that con-tain at most one word not tagged with what we define to be a content word POS , namely: noun, verb, adjective or adverb ( X   X  1 ncw  X  column in Table 5). Intuitively, this subset, which accounts for 42.4% of the test set, consists of queries that convey larger amounts of semantic con-tent and are structured less coherently. On this subset, our DistPipe model outperforms the supervised Sup model by 4.1 segmentation F1 points, and by 1.7% in UAS.
The other subset, which accounts for 30% of the test set, includes those queries for which the confidence score of the Clear parser, when applied to the whole query, is at most 0.8 ( LowConf column in Table 5). This subset singles out cases deemed difficult by the parser, indicat-ing queries with non-standard syntax. Indeed, the perfor-mance of all models substantially drops compared to the full set or the  X   X  1 ncw  X  set. Here again, DistPipe im-proves over Sup by 5.8 segmentation F1 points and 1.9% in UAS, with additional gain by the ensemble model Ens .
Q2QProj performs lower than the pipeline models on both subsets, suggesting that this approach does not work for difficult queries as well as it does for more simple queries. The decreased performance of the Clear baseline on both these subsets compared to the entire test set is not surprising, given their challenging syntactic properties.
Altogether, queries belonging to either of the two sub-sets (or both) account for 50.2% of the full set, empha-sizing the benefit of developing more sophisticated en-semble approaches, based on the above characteristics of queries and the individual tested models.

Next, we turn to Table 6, which compares the per-formance of the various models on the test subsets that consist of single-segment or multi-segment queries only 12 . Our pipeline model, DistPipe , excels on the multi-segment subset, achieving a segmentation F1 of 42.2 and UAS of 67.5% compared to only 23.5 and 64.3% respectively of the fully supervised Sup model. Our joint model Q2QProj achieves a UAS score similar to the su-pervised model, though its segmentation performance is lower. The ensemble model Ens provides additional im-provement, hinting that the Sup and DistPipe models may have learned somewhat different segmentation cues. The segmentation F1 of Clear is as low as 2.2, as its training set contains only single-segment sentences.

We note that the intersection between the multi-segment subset and the  X   X  1 ncw  X  subset is only 622 queries (63.4% of the multi-seg set, 36.7% of  X   X  1 ncw  X ), and with LowConf it is only 524 queries (53.3% of the multi-seg set, 43.7% of LowConf ). This demonstrates that our Dist model is of merit for a variety of query types.
On Single-segment queries, our joint model, Q2QProj , achieves the best UAS. This may be because it pro-vides the parser with more context for telegraphic single-segment queries, so much so, that it even outperforms the Gold benchmark. Both the Q2QProj and DistPipe mod-els over-segment (86.5 and 86.8 F1), compared to the near perfect single-segment detection of the supervised Sup model (96.2). Still, these differences are only mildly reflected in the UAS scores for single-segment queries. 6.4 Error Analysis To better understand our distant-supervised signal, we applied the CRF tagger introduced in  X  4 (without addi-tional filtering) to the test set and analyzed two cases: 100 false positives  X  single-segment queries which were incorrectly tagged with multiple segments, and 100 false negatives  X  multi-segment queries the tagger was wrong to tag as having a single segment. Two types of queries cause most false positives cases. The first type, making up 65% of the errors, is a full (or nearly-full) question or sentence. In half of these cases, a word in the middle of the query, which often marks the beginning of a grammatical question, is incorrectly marked as a segment start. Such an example is  X  [sher-lock is the best show ever]  X , where the underlined word is the incorrectly tagged segment start. The other main error is the segmentation of a query consisting of a single noun phrase (19% of the cases), for example  X  [clothing product testing]  X .

Our false negative analysis discovered four main types of errors, where a multi-segment query is not segmented. The most frequent one (35%) is cases where the tagger did not detect a syntactic cue for a segment start, e.g. in  X  [grilling pork chops] [seasoning]  X , where  X  and  X  is po-tentially missing. Another common mistake (24%) is a named entity which is added as its own segment for con-text (usually in the beginning or the end of the query). Such a query is  X  [biotin] [mcg vs mg]  X . The third type of errors (17%) is queries for which segmentation de-tection requires the understanding of the semantics be-hind the query. One example is  X  [movies on youtube] [list]  X , where  X  youtube list  X  was construed as a single noun phrase. The forth type (11%) contains a reference for a preferred content provider by the searcher on its own segment, such as  X  [is illuminati good] [yahoo]  X .
This analysis shows that the more frequent errors in-volve semantics and require either a different segmenta-tion approach, or more semantic-oriented features. We studied the syntactic properties of Web queries with question intent. We motivated the need to extend the de-pendency grammar framework so that it accounts for such queries, and constructed a new Query Treebank, anno-tated according to the extended grammar. We then devel-oped distant-supervised algorithms that can parse queries according to our grammar. Our algorithms outperform strong baselines, including a supervised model trained on thousands of manually segmented queries.

In future work we would like to improve the query analysis performance of our algorithms. In addition, we plan to assess the contribution of query parsing to IR tasks such as document retrieval and query reformulation. We thank: Bettina Bolla and Shir Givoni, for their an-notation work on Query Treebank; Avihai Mejer, for im-plementing the Q2Q element of the projection algorithm; and Joel Tetreault, for providing the infrastructure for the experiments in Section 3.3.

