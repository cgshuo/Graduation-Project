 Displaying sponsored ads alongside the search results is a key mon-etization strategy for search engine companies. Since users are more likely to click ads that are relevant to their query, it is cru-cial for search engine to deliver the right ads for the query and the order in which they are displayed. There are several works investi-gating on how to learn a ranking function to maximize the number of ad clicks. In this paper, we address a new revenue optimization problem and aim to answer the question: how to construct a rank-ing model that can deliver high quality ads to the user as well as maximize search engine revenue? We introduce two novel methods from di ff erent machine learning perspectives, and both of them take the revenue component into careful considerations. The algorithms are built upon the click-through log data with real ad clicks and impressions. The extensively experimental results verify the pro-posed algorithm that can produce more revenue than other methods as well as avoid losing relevance accuracy. To provide deep insight into the importance of each feature to search engine revenue, we extract twelve basic features from four categories. The experimen-tal study provides a feature ranking list according to the revenue benefit of each feature.
 I.2.6 [ Artificial Intelligence ]: Learning; H.3.3 [ Information Stor-age and Retrieval ]: Information Search and Retrieval Algorithms, Design, Performance, Economics, Experimentation. Sponsored search, revenue optimization, ranking, machine learn-ing.
The practice of sponsored search advertising, where the paid ad-vertisements appear alongside web search results, is now one of the largest sources of revenue for search engine companies. When a user types a query, search engine delivers a list of ads that are rele-vant to the query adjacent to or above the search results pages. Such ads are called sponsored ads. When a user clicks on a ad, he taken to the landing page of the advertisement. Under the Pay per click (PPC) advertising model, such click generates a fixed amount of revenue to search engine, where the fixed amount is the bidding price of that ad. Thus, the total revenue generated by a particular ad to search engine is the number of clicks multiplied by the cost per click (CPC). In this paper, we investigate a simplified case where the generalized second price auction is not considered in ad rank-ing. We assume that the ad ranking is not only determined by the CPC, but also depends on the relevence between query and ad and on the pairwise relevance between ad and ad. Moreover, search en-gine receives the amount of CPC every time when the ad is clicked by a user, and the charged amount is fixed and not changed by the ranked order of this ad.

When a user types a query, the search engine delivers several ads that have high possibilities of being clicked by that user. The search engine will receive more clicks if better matched ads are delivered. Thus, high accuracy of delivering the most preferred ads to each user will help search engine maximize the number of clicks. Previous works addressing this problem mainly focused on improving the ad matching relevance in sponsored search [12, 5, 2]. However, learning such a function to maximize the number of clicks is not exactly the search engines wanted.

The ultimate goal of search engine is to find an optimal ranking scheme which can maximize the total revenue. Specifically, be-sides the number of clicks, an extremely important factor, which substantially influences the total revenue, is the bidding price of each ad. We list two situations which indicate the ad bidding price should be taken more careful considerations: Since users are more likely to click on the items on the top of a ranked list than the items lower in the ranking [8, 11], if two ads both have comparable possibilities of being clicked for a particu-lar user, then search engine should generate higher rank to the one with higher bidding. Another reason to include the revenue into the analysis is that we can not always expect the correctness of predict-ing the user X  X  preference. Through sacrificing the accuracy on those clicked ads with lower bidding, it could intensify the attention on those clicked ads with higher bidding after incorporating the bid-ding into the ranking model. With these thoughts in mind, search engine would place the ads in sponsored results that not only are relevant to the user X  X  query, but also have potential to increase more revenue. In this paper, we take the revenue into careful consider-ation and propose two novel algorithms that optimize the overall revenue from di ff erent machine learning perspectives. One of the proposed algorithms is appealing since it not only produces much more revenue income but also avoid losing relevance accuracy.
The remainder of the paper is organized as follows. In section 2 we formally define the revenue optimization problem and intro-duce the click-through log. Section 3 introduces three di proaches to learning from click-through data for sponsored search. Section 4 describes twelve basic features extracted for investiga-tion. Section 5 displays the the experimental results and the last section conclude the paper.
In recent years, it attracts more attentions to predict if an ad is likely to be clicked and how search engine produces an optimal list of ads. The related works on sponsored search advertisement can generally be categorized into three approaches. The first approach applied learning to rank algorithms to optimize the ad relevance [12, 14, 4], and their objective is to maximize the number of clicks. If the ranking function can accurately model the user click decision and reach to the relevance accuracy close to 100%, this approach also leads to the revenue maximization. However, as two situations mentioned above, the real case is more complicated. In order to take the revenue into consideration, the second approach combines revenue and relevance through either a form of linear combination [14] or multiplying the relevance score by the bidding price [2]. The methods under this approach are heuristic so that it is not easy to find the optimal parameters. Especially when the number of tun-ing parameters is many, the expense of heuristic parameter tuning becomes infeasible since the computational complexity increases exponentially as the number of parameters increases. The third ap-proach decomposes the expected revenue we aim to maximize into two parts, i.e., CTR (click-through rate) and the bidding price. Sev-eral algorithms are proposed to estimate the CTR value for an ad with high precision (low variance)[6, 16, 15, 7]. The CTR value is an absolute measurements between a query and an ad. The as-sumption of this approach, as indicated in [7], is that there is an intrinsic relevance of an ad, which independent of the context of the user X  X  query. Thus, the ads with high absolute CTR value many not be relevant to a particular user X  X  query. In summary, the first approach only produces the results with high relevance accuracy. The second and third approaches use the two steps procedure to optimize revenue instead of relevance. Although they can generate better revenue, they always lose much relevance accuracy.
As discussed in Joachims [10, 8], there are two important factors a ff ecting the click decision from users. One is that the positional bias which leads to more clicks on links ranked on the top posi-tions. Another is that the user X  click decision is not only influenced by the absolute relevance of the clicked link, but also by the overall quality of the other ads in the ranking list. These two factors show that clicks have to be interpreted relative to the order of presenta-tion and relative to the other ads in the same list. Many learning to rank algorithms are based on these assumptions and have produced some promising results [9, 10, 13]. As a result, in this paper, our algorithm are based on similar assumptions: (1) A click can only serve as an indication that an ad is more relevant than the ads that are not clicked in the same ranking list, but not as an absolute indi-cation of the ad relevance. (2) To avoid the positional bias, we only choose the first three ads in the ad list alongside the search results from the click-through data.
Each record in the ad click-through data is the quadruple  X  q , Ad c ( p ) , r q ( p )  X  , consisting of the query q , the ad Ad p , the binary variable c q ( p ) encoding the click information, and the bidding price r q ( p ) for this ad. More specifically, when a user types in a particular query q , p  X  N is the position of ad within the dis-playing ads list. Ad q ( p ) refers to the p th ad in the ads list, c if and only if Ad q ( p ) has been clicked by that user. A typical ex-ample is shown in Table 1. The column  X  revenue  X  indicates the bidding price of the corresponding ad. We use the click-through data as in this format throughout the paper. Several ad pairs are constructed for each ranking list. Table 1 displays the result of such ad pairs constructed from click-through data in Table 1. In the column  X  label  X , + 1 indicates that the user prefers the first ad to the second one in the ad pair.
 queries ad s pos click revenue ad s pair label query 1 ad 2 2 3 query 2 ad 2 2 X 5 query 3 ad 2 2 7 ( ad 2 , ad 3 )  X  1 query 4 ad 2 2 X 9 Our aim is to maximize the expected revenue for search engine. Suppose Q is the set of total queries collected from search engine, then the available data is represented as: Based on a fixed ranking function R (  X  ), defined as where n q is the number of ads in the ads list corresponding to the query q , we can rank the ads lists for all queries in Q . Here we assume that there are no ties in the rank mapping, i.e., the ranking function R (  X  ) is bijective. Thus for each query q , the ranking func-( q  X  X  ) in the way that
R  X  q , Ad q ( p )  X  = i  X  X  X   X  q ( p ) = i , ( p , i = 1 ,..., n
According to the above denotations, we define two important scores: the revenue score and the relevance score . Figure 1: Two examples of the ranking function (a) The rank-ing with high relevance but low revenue; (b) The ranking with lower relevance but higher revenue.

Definition 1. Given the evaluation set { X  q , Ad q ( p ) , c and a fixed learned function R (  X  ), the revenue score # Revenue is: q (1) returns the original position in the ads list whose ad is ranked on the first position by the ranking model. Thus, c q (  X  r (  X   X  1 q (1)) = r q (  X   X  1 q (1)) is the received revenue for search engine. From this definition, if the ad ranked on the first position is actu-ally clicked by the user, then the bidding from the clicked ad is assumed to be received. Otherwise, if a clicked ad is not ranked on the first position, we lose the revenue that ad generates (i.e., c (  X   X  1 q (1)) r q (  X   X  1 q (1)) = 0, no revenue received). The revenue score is the revenue we actually receive divided by the largest possible fore, in order to maximize the search engine revenue, the revenue score acts as the criteria of both optimizing and evaluating the rank-ing model.

Definition 2. Given the evaluation set { X  q , Ad q ( p ) , c and a fixed learned function R ( . ), the relevance score # Relevance is: # Relevance = P clicked by users. This number divided by the total number of queries is the relevance score, which is similar to the ad ranking accuracy used in [5].

We use the data in Table 1 as an example, and it is clear that the largest possible revenue we could receive will be 4 + 5 + a ranking model produces the first position ad as ad3, ad2, ad1 and and2 respectively, then the totally received revenue is 1 15 and the final revenue score is 15 22 . Accordingly, the relevance
After introducing these definitions, the revenue optimization can be formulated as two major problems: (1) What ranking strategy we develop in order to maximize the revenue score as well as pre-serve the relevance score; (2) Which features that best captures the user X  click behavior and achieves high revenue score.

The revenue score reflects the purpose of maximizing search en-gine revenue, while the revenue score only reflects the users X  rel-ative preference. A learned ranking function may have higher rel-evance score but still lead to lower revenue. We expect that an appropriate ranking function which takes revenue into account can produce higher revenue even if it sacrifices the correctness of pre-diction. Figure 1 illustrates an example of this case. In Figure (b), the ranking function reaches to 25% relevance score, but the re-ceived revenue is 9. However, in Figure 1(a), the ranking function reaches to 50% relevance score, but the revenue is only 5 or 8. It seems that the revenue optimization contradicts the relevance opti-mization. However, it is not always true. In Figure 1(a), the ranking function reaches to 50% accuracy if either Ad 1 1 or Ad 1 as the clicked ad. If Ad 1 1 is ranked first, the received revenue is 5. On the other hand, if Ad 1 3 is on the first, better revenue is received, i.e., 8. Thus, we can receive 3 more bidding revenue but not lose the relevance. In this paper, one important issue in developing new ranking model is to maximize the revenue score and simultaneously avoid losing the relevance score.
We investigate three approaches of learning the ranking func-tion. We first introduce a learning to rank algorithm similar to [10]. We then propose two new ranking methods, both of which aim to maximize the revenue score. Throughout this section, we adopt the pairwise learning paradigm. Suppose for each query-ad pair ( q , Ad q ( p )), it corresponds to a fea-code the users X  preference information (the click information in this paper) through a function f (  X  , ~ w ) : R k  X  X  X  R as: f ( ~ x q ( p (1) ) , ~ w ) &gt; f ( ~ x q ( p (2) ) , ~ w )  X  X  X  ( c on
R k , i.e., f ( ~ x , ~ w ) =  X  ~ w , ~ x  X  , then (6) is equivalent to:  X  ~ w , ~ x q ( p (1) )  X  ~ x q ( p (2) )  X  &gt; 0  X  X  X  ( c Based on (7), pairwise learning casts the ranking problem as a clas-sification task, where the samples are from:
S = { ~ x q ( p (1) )  X  ~ x q ( p (2) ) , c q ( p (1) )  X  c where c q ( p (1) )  X  c q ( p (2) ) = 1 or  X  1 corresponding to labels. To simplify the notation, we use the following set  X  S to represent S where c i  X  { + 1 ,  X  1 } is the class label and N is the number of pairs in (8). We introduce two pairwise learning methods which use the same procedure to optimize a solution ~ w  X  . However, two ap-proaches of deriving the final ranking function from the learned function f ( ~ w  X  , ~ x ) are di ff erent.
Learning to rank algorithms have been investigated extensively, and we only give a brief description. In order to be consistent to the second algorithm in 4.1.2, we use the logistic regression algo-rithm to learn the optimal ~ w . The learning problem is to solve the following convex optimization: where  X  is the regularization parameter whose value is specified in advance. We call this method RankLogistic. (Compared to RankSVM [9], the only di ff erence is that it uses the logistic re-gression algorithm). Suppose that ~ w  X  log is the optimal weights. Ac-cordingly, we arrange the samples ~ x q ( p )  X  p in decreasing order as f ( ~ x q ( l (1) q ) , ~ w  X  log ) &gt; f ( ~ x q ( l (2) Our ranking functions  X  q are which determines our final ranking function R (  X  ) according to (3).
The revenue post-optimization method is a direct combination of relevance and revenue. However, unlike [2, 14], this approach is inspired by the following optimal solution: Given a set of ads { ad 1 , ad 2 , ad 3 } and their corresponding bidding price { r compute the estimated probability of an ad ad i being clicked by this user conditioning on being displayed along the same query, denoted by p i = p ( ad i | query ). Accordingly, we could predict the i th ad when its expected revenue r i p i is the largest among { r 1 p
The advantage we choose logistic regression in Section 4.1.1 is that the resulting solution gives us probability interpretations. Sup-pose the solution of the logistic regression is ~ w  X  log three ads set { ad 1 , ad 2 , ad 3 } , the learned ~ w  X  log the preference relation  X  for any ad pair: As suggested in [3], we can further construct the desired probabili-ties { p 1 , p 2 , p 3 } , which is consistent to (13)-(15), as follows where p i = p ( ad i | query ) is the probability of user clicking ad ter seeing the query ( i = 1 , 2 , 3). Finally, based on these derived probabilities, we can rank each three ads according to the rank of { r
The reasons that the revenue post-optimization takes the bias to-wards those high bidding ads is straightforward. This approach is also a two step procedure similar to the idea of combining rev-enue and relevance[6, 7, 15, 16]. However, unlike estimating CTR or other absolute relevance evaluation, these possibilities p p reflect users X  relative preference rather than an absolute metric. The proposed relevance measurement is a relative relevance score and is query-specific, which is proved to be more accurate in the experiment.
To maximize the revenue score, besides learning users X  prefer-ence, it is crucial to develop a method that biases to those ads with higher bidding price. Compared to the revenue post-optimization approach, we propose a novel approach revenue direct-optimization from a di ff erent perspective. The revenue direct-optimization achieves the revenue maximization globally through defining a new loss function which intensifies the training on those ads with higher bid-ding price and consequently forces a positive change to the weight value of the bidding price feature. This approach aims to globally optimize the revenue and can find the ranking function in one step. In contrast, the revenue post-optimization approach is a two step procedure where the revenue is locally taken into account. After combining the relevance and revenue directly, it often harms the relevance score too much.

The revenue optimization problem can be formulated as maxi-mizing the empirical revenue Rev ( ~ w ) However, since this function is not continuous, it is infeasible to optimize a solution ~ w to maximize Rev ( ~ w ). Hence we construct a new loss function to approximate the function Rev ( ~ w ). We first replace by the sum and obtain the approximate revenue This approximation is very concise especially when n Further, we replace the 0  X  1 loss in (20) by the Bernoulli negative log-likelihood loss, and the final empirical loss function L ( ~ w ) we aim to optimize is since c q ( p ) = 0 when c q ( p )  X  c q ( i ) =  X  1.

We assume f ( ~ w , ~ x ) is linear in ~ x . For each q  X  X  , for each p and c ( p ) = 1, we construct r q ( p )  X  1 more ads pairs: where ad q ( p ) is the clicked ad. After including these new ads pairs into the training data, performing RankLogistic is equivalent to minimizing (21). Thus, the solution can be easily optimized through using the logistic regression method after properly pro-cessing the training data. The optimal solution which minimizes the loss in (21) will thus maximize the approximate empirical rev-enue.

From the loss function L ( ~ w ) in (21), we notice that the rev-enue direct-optimization approach also tries to avoid losing the rel-evance score. If we have two ranking models with the same rel-evance score, this approach will choose the model with high rev-enue. Moreover, if a model best captures the users X  click decision process and reaches to 100% relevance score, the solution which minimizes L ( ~ w ) can also optimize a model to achieve this. How-ever, the two-step procedure for revenue optimization cannot guar-antee this.

We give an intuitive discussion on why the minimization of L ( ~ w ) biases to high bidding ads. The ads pair ( ad q ( i ) , ad tributes to the loss in (21) has to satisfy two conditions: (1) two ads in the pair have the same query index; (2) at least one of the two ads has been clicked. We assume P = { ( ad (1) i , ad (2) ads pairs satisfying these two conditions. Without loss of general-ity, we assume the first ad ad (1) i in each ads pair is the clicked ad. Suppose their feature vectors are { ( ~ x (1) i , ~ x (2) rewritten as
L 0 ( ~ w ) =  X  || ~ w || 2 + Compared to the logistic loss function, the above loss function has a set of weights r (1) i , which corresponds to the bidding of those clicked ads. Thus if all of them equal to 1, then the loss in (22) is exactly the same as the loss in (10). In this case, the loss (10) treats all the potential errors of training samples equally important. How-ever, the new loss in (22) incorporates the bidding as the weight into the loss function thus intensifies those ads with larger bidding. To investigate further on this point, we split the loss L components: L 0 ( ~ w ) =  X  || ~ w || 2 + + X where x i = ( x i 1 ,..., x ik ) is an ad feature with the k dimensions. We assume the first feature in the feature representation is the bidding i.i.d samples from an distribution, then in average r (1) = ( w  X  which minimize (10) and (23) respectively. The optimal solution ~ w  X  of (10) cannot minimize the new approximate empirical loss in (23). An increment in ~ w  X  1 leads to an increase in (10) but leads to a w , which implies that ~ w  X  X  X  makes the bidding feature play a more significant role in the ranking function than ~ w  X  does. As a result, the output of the ranking function for an ad increases substantially if its bidding is large. This is exactly the bias we aim to emphasize for those ads with higher bidding price. Table 2: A list of extracted features in the representation of each query and ad pair.

We totally extract 12 features to represent each query and ad pair (see Table 2). These features can be separated into four categories: the low-level relevance features, the high-level relevance features, the CT R features and some other features such as the bidding price and the match type between the input query and the advertiser X  X  bidding keywords. There are four match type values indicating ex-act match, broad match, phrase match and smart match.

The low-level relevance features include TF(term frequency), TF * IDF(inverse document frequency) [1], and edit distance. The term frequency is the summation of the number of times each query term appears in the ad title and description. Inverse document frequency is a balance factor to evaluate how important a term is to an ad in the advertisement corpus. We compute the edit distances between each query term and the displayed URL and take the largest as the edit distance value. The high-level relevance features include the outputs from the Okapi BM25 [17] and LMIR (language models for information retrieval) [19] ranking algorithms which measure the relevance between query and either ad title or ad description. In particular for LMIR, there are several smoothing methods such as Dirichlet, Jelinek-Mercer, absolute discounting and etc. We adopt the Dirichlet smoothing in this paper.

Ad CTR is a statistical measurement whose value is computed by the number of ad clicks divided by the total number of ad impres-sions. If one ad X  X  CTR is larger than another ad X  X  CTR, this ad has higher probability to be clicked. The ad CTR value is an absolute indication of the ad popularity, which is independent to the input query. The estimation of ad CTR often has high variance, espe-cially for new ads whose number of ad impressions is small. Since new ads have little chance to be seen by users, their CTR values are likely to zero. However, these new ads cannot be judged as unpop-ular ads. As a result, we cannot treat new ads with little number of ad impressions the same as old ads whose number of ad impres-sions is large. The details on how to handle the old ads and new ads separately are described in the experimental part. Similarly, we calculate the CTR value for advertiser account and advertising campaign. Each advertiser account corresponds to multiple adver-tising campaigns and each advertising campaign includes multiple ads.
We collect three months click-through log data with ad clicks and impressions. After randomly sampling across the overall log data and filtering out the query whose returned ads number is less than 3, we totally have 10 million queries and each query corresponds to a list of first three ads in the ad list alongside the search results. We use 80% records in the training and the left 20% records in the testing. Since the collected data samples are adequate enough in terms of 12 features, cross validation is not applied.

The estimation of the CTR features is only reliable for  X  X ld X  ads with large impression number, hence, we split the experimental evaluation into two parts. In the first part, we exclude the CTR fea-tures and use other 9 features as the feature representation. In this case, all of the training data are used to learn a ranking model and all of the testing data are used to evaluate the revenue and relevance score. In the second part, in order to make use of the CTR features, we only select those ads with at least 100 impressions, where the impression number is counted in the training data. We denote these ads set as S CT R . For a query one of whose returned ad is not in S
CT R , this query and all its returned ads will not be included. After choosing the  X  X ld X  ads from the training and testing data, we can build and evaluate a ranking model based on 12 features. Figure 2: Revenue direct-optimization results from di regularization parameter values in 9 features Figure 3: Revenue direct-optimization results from di regularization parameter values in 12 features
In the experiments, we consider two baseline methods. The first baseline is learning to rank introduced in Section 4.1.1. The second baseline is the CTR estimation approach whose ranking function is based on CTR*Bidding. Unlike the pairwise learning method, the CTR estimation is a regression problem and we use the support vector regression algorithm[18] to solve this regression problem. Since CTR value plays as the regression value, only  X  X ld X  ads with ad impression numbers larger than 100 can be used to train the re-gression model. Hence, in the baseline 2, we use the same ads to train the ranking model in both 9 feature and 12 feature representa-tions. Some previous works on the CTR prediction have developed new features which can boost the estimation performance. How-ever, the CTR prediction is still a challenge work and this paper does not explore advanced features on this point. We only use 12 basic features in order to be consistent to other methods. All the algorithms used in the experiments have a regularization parameter whose value is specified by user in advance. We tune this param-eter from the range [0 . 0001 , 0 . 001 ,..., 100000]. For the baseline 1, we choose the optimal parameter according to the relevance score. The optimal parameter for other methods is according to the rev-enue score. However, we notice from the experimental results that the optimal solution is not sensitive to the parameter tuning due to the large scale of training data.

Table 3 illustrates the revenue and relevance score comparisons among four methods in two di ff erent feature representations. Since learning to rank directly optimizes the relevance, it reaches highest relevance scores in both 9 features and 12 features. However, the revenue scores from learning to rank are lowest among four meth-ods. On the contrary, the revenue direct-optimization (RDO) ap-proach always achieves the highest revenue scores, which has rel-ative 19.7% revenue improvement in 9 features and relative 7.3% Table 4: The weight value of the bidding feature extracted from the optimal solution. revenue improvement in 12 features compared with the baseline 1. As shown in Table 4, in learning to rank, the weight w  X  1 the bidding feature are -0.116 and -0.057 in 9 features and 13 fea-tures respectively, while the weight w  X  X  X  1 values in RDO are 0.038 and 0.090. As indicated in (23), RDO directly takes more empha-sis on the bidding price in the ranking function.

Although the revenue post-optimization (RPO) approach pro-duces high relevance score comparable to RDO, its relevance score are much lower. This is really undesirable for search engines, since very low relevance score has a negative influence to the users X  sat-isfaction, thus leading to the decrease in total clicks especially in the long term. In contrast, we note that RDO not only achieves the highest revenue score, but also maintains relative high relevance score. This is exactly the appropriate balance that search engines hope to achieve. In the baseline 2, the revenue score produced from 9 feature representation is lower than the revenue score from RPO, another two step procedure proposed in this paper. It indicates only depending on these 9 features cannot train a good CTR estimation model. However, in 12 feature representation where the CTR fea-tures are included, the baseline 2 can produce higher revenue score. Both the two step approaches, i.e., the baseline 2 and RPO, lose many relevance score. Therefore, a unified revenue optimization approach like RDO is more appealing in the revenue optimization. Figure 4: Revenue direct-optimization results from di percentage of training data in 9 features
In the revenue direct-optimization approach, the regularization parameter  X  we choose to optimize the revenue score is 0 . 001. However, the learnt ranking function is not sensitive to the choice of the  X  value, which is shown in Figure 2 and Figure 3. Both the revenue score and the relevance score change slightly when  X   X  [0 . 0001 , 100]. As long as  X  becomes larger than 100, the revenue score and relevance score start to drop. As a result, the optimal solution can be easy to choose without careful parameter tuning.

Figure 4 and Figure 5 demonstrate the revenue scores and the rel-evance scores in terms of di ff erent percentage of training data using the revenue direct-optimization approach. We notice that there are little variations if di ff erent percentages of training data are involved into the training. When taking only 20% percentage of training into account, the revenue score and relevance score remains the similar as the results from 100% data. Figure 5: Revenue direct-optimization results from di percentage of training data in 12 features
We notice from Table 3 that there is a systematic improvement of both relevance score and revenue score when comparing 9 fea-ture experiment to 12 feature experiment, which means including three CTR features are very useful if the CTR estimations are con-fident. The learning algorithm and the extracted features are two most important issues to develop stat-of-the-art techniques. In this subsection, we investigate the importance of each feature to search engine revenue and provide a feature ranking list.

Recursive feature elimination (RFE) is a feature selection method which greedily eliminates the most useless feature at each step and provides us a rough idea of a feature ranking list. We apply the recursive feature elimination procedure on both 9 features and 12 features where the base ranking model is the revenue direct-optimization algorithm. Our goal is to rank the features according to their predictiveness to the final revenue score. The RFE results on 9 features are summarized in Figure 6. For example, in the first step of RFE, we discard each feature from the whole feature set and train 9 ranking functions on the left 8 features. Their scores are filled into the R1 column. The score 47.073 at the revenue po-sition is from the ranking function trained on other 8 features after deleting the revenue feature. Thus, for the feature after deleting which we have highest revenue score, this feature is considered as the most useless feature compared with others. We notice that the score produced from tf is highest in the first step, thus RFE will delete this feature and continue the next step. The RFE procedure greedily eliminates the useless features and finally can produce a ranking list of features. As shown in Figure 6, we highlight the highest score in red color at each step of RFE and eliminate the corresponding feature. The order in the most left column gives a ranking list of features. As a result, in 9 feature representation,  X  X ev-enue X  is the most important feature and  X  X M25 of title X  and  X  X atch Type X  are ranked the 2nd and the 3rd important features in revenue direct-optimization. The importance of the  X  X M25 of title X  feature coincides with the intuition in ad click. After user types a query, if the ad title mostly matches this query, this ad has high probability to be clicked by the user. However, user is unlikely to look into the Figure 6: Recursive feature elimination results on 9 features where the revenue direct-optimization algorithms is used as the base ranking model. ad description thus the relevance between query and ad description is not that important.

Figure 7 demonstrates the RFE results on 12 features. In this case,  X  X evenue X ,  X  X ds CTR X ,  X  X atch type X  and  X  X M25 of title X  are four most important features of the whole feature set. Besides the  X  X ds CTR X  features, these important features are consistent to the results on 9 features. From these results, the feature extracted from the BM25 method shows more important than those extracted from LMIR. Moreover, we notice that after eliminating most of fea-tures and only keeping the two important features,  X  X evenue X ,  X  X ds CTR X , the revenue score remains very high. Only these two features can produce 0.52779 revenue score, which is only a slightly score decrease compared with taking all features into account. This score is also much higher than the highest score obtained from 9 feature representation. These facts indicate that  X  X ds CTR X  is a crucial and extremely useful feature in determining the users X  click behavior. This observation suggests us that it has much potential to discover the confident CTR related features for  X  X ew X  ads even though their impression numbers are small. In contrast, the  X  X evenue X  feature is like an augmented feature whose functionality is to incur bias to those ads with high bidding. Therefore, adding a  X  X evenue X  feature leads to a significant increase in the revenue score. This issue is what we expect when we develop the revenue direct-optimization ranking model.
In this paper we investigate two novel approaches to learning and evaluating sponsored search ranking systems using real click-through log data. By optimizing two novel objective functions, we reach an appropriate balance between achieving highest rev-enue and maintaining high relevance. Previous works investigate on how to estimate the relevance scores for each sponsored ads. Giving high ranking to those ads with high relevance score will fulfill search engine user X  X  satisfaction, thus has much potential to Figure 7: Recursive feature elimination results on 12 features where the revenue direct-optimization algorithms is used as the base ranking model. attract more user clicks which may ultimately generate business transactions. However, under Pay Per Click system, optimizing the sponsored search by delivering the most relevant ads is not equiv-alent to maximizing search engine X  X  total revenue. In this paper, our proposed novel method, revenue post-optimization , aims to maximize the total revenue. This approach is similar to previous works [6, 7, 15, 16], since they all try to directly combine some relevance score and the revenue score in a two way procedure that might maximize the revenue. However, previous works using some heuristic combination cannot guarantee the solution is optimal in any sense. Instead, through estimating a set of probabilities to the ads in the same list, the revenue post-optimization method is opti-mal in decision-making when the aim is to maximize the revenue.
However, all such direct relevance and revenue combinations ap-proaches have the problem of losing the ads quality in high ranking positions. We then propose another novel ranking scheme which not only achieves the highest revenue but also avoid losing the qual-ity of returned ads. Instead of trying to achieve a simple two-step procedure, the novel ranking function, revenue direct-optimization, aims to directly maximize the approximate empirical revenue. Through utilizing such ranking function, the recursive feature elimination procedure on the feature set provides more insights into the impor-tance of each feature to the total revenue. A feature ranking list is then generated according to its contribution in training the op-timal ranking function. In our future works, we will continue the revenue optimization works from the direction of exploring more features. As suggested in this paper, more CTR and relevance fea-tures should be investigated to further improve the revenue score. [1] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information [2] A.Z. Broder, P. Ciccolo, M. Fontoura, E. Gabrilovich, [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, [4] D. Chakrabarti, D. Agarwal, and Vanja Josifovski.
 [5] M. Ciaramita, V. Murdock, and V. Plachouras. Online [6] K. Dembczynski, W. Kotlowski, and D. Weiss. Predicting [7] J. Feng, H. Bhargava, and D. Pennock. Implementing [8] L. Granka, T. Joachims, and G. Gay. Eye-tracking analysis of [9] R. Herbrich, T. Graepel, and K. Obermayer. Large margin [10] T. Joachims. Optimizing search engines using clickthrough [11] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and [12] A. Lacerda, M. Cristo, M.A. Goncalves, W. Fan, N. Ziviani, [13] T. Liu, T. Qin, J. Xu, W. Xiong, and Hang Li. Letor: [14] F. Radlinski, A. Broder, P. Ciccolo, E. Gabrilovich, [15] M. Regelson and D.C. Fain. Predicting click-through rate [16] M. Richardson, E. Dominowska, and R. Ragno. Predicting [17] S. E. Robertson. Overview of the okapi projects. Journal of [18] V. N. Vapnik. Statistical Learning Theory . John Wiley &amp; [19] C. Zhai and J. La ff erty. A study of smoothing methods for
