 There is a fundamental tradeoff between effectiveness and ef-ficiency when designing retrieval models for large-scale docu-ment collections. Effectiveness tends to derive from sophis-ticated ranking functions, such as those constructed using learning to rank, while efficiency gains tend to arise from improvements in query evaluation and caching strategies. Given their inherently disjoint nature, it is difficult to jointly optimize effectiveness and efficiency in end-to-end systems. To address this problem, we formulate and develop a novel cascade ranking model, which unlike previous approaches, can simultaneously improve both top k ranked effectiveness and retrieval efficiency. The model constructs a cascade of increasingly complex ranking functions that progressively prunes and refines the set of candidate documents to mini-mize retrieval latency and maximize result set quality. We present a novel boosting algorithm for learning such cas-cades to directly optimize the tradeoff between effectiveness and efficiency. Experimental results show that our cascades are faster and return higher quality results than comparable ranking models.
 Categories and Subject Descriptors : H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms : Algorithms, Performance Keywords : learning to rank, effectiveness, efficiency
There is often a tension between effectiveness and effi-ciency when building information retrieval systems. To a-chieve greater effectiveness (i.e., to deliver higher quality results), system designers are driven towards complex rank-ing functions that may combine evidence from dozens, hun-dreds, or even thousands of relevance signals, typically using sophisticated machine learning techniques [16]. This fre-quently comes at a cost in efficiency (i.e., a slower system), since complex ranking functions are computationally expen-sive, thus requiring more resources to achieve the same level of service. On the other hand, efficiency can be enhanced through a variety of approaches such as index pruning, fea-ture pruning, approximate query evaluation, and systems engineering. However, most of these approaches degrade ef-fectiveness, typically in ways that are difficult to control.
With the goal of achieving a better balance between re-trieval effectiveness and efficiency, recent work has explored approaches to ranking that exhibit good tradeoff character-istics [27, 28]. In general, they work by eliminating features that are costly to compute and not predicted to contribute much to the quality of the results. While efficiency-minded feature selection is a natural way to make existing models faster, such pruning may negatively affect retrieval effective-ness. This problem is exacerbated for very large collections under tight efficiency constraints. In this setting, only a small handful of cheap features can be used for ranking, which can result in poor retrieval effectiveness.

We introduce a novel cascade ranking model for efficient ranked retrieval. Unlike previous approaches, the cascade uses a sequence of increasingly complex ranking functions to progressively prune documents and refine the rank order of non-pruned documents. Thus, the cascade model views retrieval as a multi-stage progressive refinement problem, where each stage considers successively richer and more com-plex ranking models, but over successively smaller candidate document sets. The intuition is that although complex fea-tures are generally more time-consuming to compute, addi-tional overhead is offset by the fact that fewer documents are examined. This type of ranking paradigm is well-suited for large document collections, because the number of rel-evant documents is very small compared to the collection size. Hence, the ability to quickly hone in on a small set of candidate documents, via the cascade, can yield higher quality results and faster query execution times.

To achieve a desired efficiency-effectiveness tradeoff, we describe a novel boosting algorithm, a generalization of Ada-Rank [30], that jointly learns the model structure (i.e., op-timal sequence of ranking stages) and the set of documents to prune at each stage. Experiments show that our cas-cade model can simultaneously improve effectiveness and ef-ficiency compared to non-cascade feature-based models.
This paper has three major contributions. The first is the cascade model itself. Although similar coarse-to-fine-grained models have been used in other disciplines, to our knowledge this is the first application of this principle for learning an efficient ranking model. Second, we introduce a novel boosting algorithm for learning ranking cascades, complete with a theoretical analysis. Finally, we carry out an extensive evaluation of our proposed model on several web collections. Results show that the cascade model con-sistently outperforms current state-of-the-art models, both in terms of efficiency and effectiveness.

The remainder of the paper is organized as follows: Sec-tion 2 describes related work. Section 3 details our proposed cascade ranking model, and Section 4 presents how cascade models can be learned. Next, Section 5 describes experi-mental results. Finally, Section 6 concludes the paper and outlines possible directions for future work.
In recent years, we have witnessed the success of machine learning approaches to the document ranking problem, as a whole known as learning to rank (e.g., [6, 17, 9, 30, 16], just to name a few). For the most part, however, these approaches have focused exclusively on effectiveness, some-times leading to ranking functions that deliver high qual-ity results, but are unbearably slow. Recently, however, a thread of work has emerged, dubbed learning to efficiently rank, that adopts an efficiency-minded approach. For ex-ample, regularization is useful for  X  X ncouraging X  models to have few non-zero parameters, thereby serving as a crude form of feature pruning [24, 11]. More recently, Wang et al. [27, 28], in two separate studies, proposed linear ranking models that explicitly account for feature costs: the first is able to discover a more optimal point in the tradeoff space between efficiency and effectiveness, while the second is able to perform feature pruning to meet an externally-imposed time constraint. With feature pruning, the retrieval engine still implements a single monolithic ranking function X  X nd thus it remains necessary to compute complex features for many documents (which is especially problematic for large web collections). In addition, since the number of non-relevant documents is significantly larger than the number of relevant documents in web-scale collections [8], applying a monolithic ranking model (even if used with a fast query evaluation engine) may waste computations, because a large number of the documents examined are non-relevant. In contrast, the cascade model considers increasingly complex features/ranking models on progressively fewer candidate documents. This approach, as we will show in the exper-iments, results in ranking functions that are simultaneously effective and efficient.

Our work is complementary to query evaluation [5, 25, 23]. Query evaluation focuses on how to efficiently evaluate a given ranking model, where we aim to learn an efficient ranking model in the first place. Along a similar direction, Cambazoglu et al. [8] explored early-exit strategies for addi-tive ensembles. Although this approach bears some super-ficial resemblance to our cascade model, it is in fact com-pletely different. They focus on optimizing the query eval-uation process given a particular additive ensemble X  X nd not about learning the ensemble. In contrast, our proposed model is accompanied by a boosting algorithm for learning optimal cascades according to a tradeoff metric.

A complementary approach to fast query evaluation is in-dex pruning [10, 20]. Since query evaluation time monotoni-cally increases with length of postings lists, shorter postings lists translate into faster queries. While discarding postings help reduce query latency, it does not directly optimize the underlying effectiveness and efficiency tradeoffs.

Finally, a variety of system engineering strategies exist to increase search efficiency, especially in operational settings. Caching [2, 21] reduces query latency significantly. Parti-tioning the document collection reduces latency, while repli-cation of services increases throughout [12, 3]. These strate-gies are not IR-specific, but represent general principles for building large-scale distributed systems. In particular, we are not concerned with the system engineering aspects of ef-ficiency (caching, partitioning, and replication), since these techniques can be applied to the cascade model just as they can be applied to any retrieval algorithm. These system en-gineering aspects can be viewed as orthogonal to our learn-ing framework; their effects are captured by our analytical model of query execution time, which simply serves as an input to our task of learning an efficient ranking model.
The idea of progressive refinement in the cascade model is related to the general class of coarse-to-fine models that have been successfully applied in computer vision and ma-chine learning [26, 29]. Most notably, Viola and Jones [26] tackled the problem of real-time face detection in images by a sequence of binary classifiers of increasing complexity, such that the most unlikely images are rejected early by simple classifiers, and the more promising object-like regions are passed to more complex classifiers for further consideration. However, these models are for high recall/precision appli-cations and cannot be used for ranked retrieval, since they can only filter, but not rank items. As we will show, our proposed cascade is specifically built for achieving high top k ranked effectiveness in a very efficient manner.
In web search, there are significantly more non-relevant documents than relevant documents and most users only browse the top few results. Applying a monolithic ranking model for each query, even if used in conjunction with fast query evaluation techniques (e.g., [5, 25, 23]), may not be very efficient because a large number of scored documents are likely to be non-relevant and/or will not appear in the top k . Our proposed cascade model leverages these facts to achieve high top k ranked effectiveness in a highly efficient manner by constructing ranking models from simple to com-plex, applying the simple ones first, and pruning documents at each subsequent stage so that more complex (and better) ranking models are computed over fewer documents.

The cascade model consists of an additive sequence of stages { S 1 ...S T } , where each stage S t is associated with a pruning function J t and a local ranking function H t . Each stage receives as input the set of ranked documents from the previous stage and applies two sequential operations: first, the pruning function J t is used to remove a number of docu-ments from the input set (thus reducing the amount of effort involved in document scoring); then, the score contribution of the local ranking function H t is added to the candidate documents still under consideration (to improve top k qual-ity of the remaining documents). The results are forwarded to the next cascade stage for further pruning and re-ranking.
By construction, the cascade is arranged so that the local ranking functions increase in cost (and thus, complexity). Early stages take advantage of  X  X heap X  ranking models to rank documents; the pruning functions discard documents that are unlikely to appear in the final top k . As a re-sult, each successive stage is presented with a smaller candi-date set, which enables the cascade to exploit more complex Figure 1: An example cascade. After an initial ranking function H and costly ranking models X  X ence improving effectiveness X  without sacrificing efficiency.

As an example, Figure 1 presents a cascade model. The input to the cascade is a set of documents for a given query, and the final output is a ranked list of k documents (where k is specified in advance). An initial ranking function H applied to obtain an initial ranked list, R { H 0 } , which is then passed as input to the first stage. At the first cascade stage, the pruning function J 1 prunes documents in R { H 0 } based on features of these documents (details in Section 3.1). The output from the pruning operation, denoted by R { H 0 ,J 1 re-ranked by adding the contribution of H 1 to the document scores. This process repeats for the next cascade stage.
The overall score of a non-pruned document d i at the end of a cascade model with T stages has the following form: where  X  t denotes the importance of the local model H Following previous work in learning to rank, each H  X  X eak ranker X . We postpone discussing the actual ranking functions and our feature set until Section 4.

The iterative pruning and scoring mechanisms of the cas-cade provide a way to explicitly control the tradeoff between retrieval efficiency and ranked effectiveness. In terms of effi-ciency, the cascade aims to reduce the number of candidate documents at each stage: where each | R { X  ,J t } | denotes the resulting size of the docu-ments after pruning at stage t , and the  X  abbreviates the pre-vious sequence of pruning and re-ranking actions that have been applied to the input ranked documents R . In terms of effectiveness, the cascade aims to achieve the following: where E ( R { X  ,H t } ) denotes the resulting top k effectiveness from applying H t to refine the overall scores of the non-pruned documents that have reached S t .

We can trivially obtain the most desirable outcome for either Equation (2) or Equation (3) at the expense of the other. If we set the pruning functions to never discard any documents, then the final ranked effectiveness E ( R will be as high as possible since there will be no  X  X oss X  due to pruning. However, the cascade will likely be inefficient. Alternatively, if we prune every document, the result will almost certainly be fast, but ineffective. Thus, the objective is to design a well-balanced cascade by jointly learning the local ranking and pruning functions, guided by a tradeoff metric. We describe exactly such an algorithm in Section 4.
Before proceeding, a comment about the order in which pruning and re-ranking is performed at each stage: the prun-ing function is applied on the ranked documents produced from the previous stage, i.e., J t reduces the size of the output documents from stage t  X  1. The reason behind this order-ing is that while the local ranking function H t  X  1 used at the previous stage helps to refine the top k ranked effectiveness, pruning its re-ranked documents has a direct impact on the efficiency of stages t,t +1 ,...,T . If the pruning is aggressive, then fewer documents will reach t,t + 1 ,...,T , thereby im-proving efficiency. Therefore, when learning the cascade, the pruning function defined for output documents from t  X  1 should (ideally) be jointly selected with the ranking func-tions at t,t + 1 ,...,T . For example, if H t is complex, then pruning documents from t  X  1 must be aggressive to make it feasible to apply H t ; on the other hand, if H t is simple, then more documents from t  X  1 may be kept and scored. While it would be ideal to jointly consider the pruning func-tion with all subsequent ranking functions, this significantly complicates learning. Instead, we only consider the pruning function for documents produced from t  X  1 with the current ranking model H t at stage t . To instantiate a cascade, we need to define the pruning functions, discussed next.
At the input of each cascade stage t , we receive a set of ranked documents R { X  ,H t  X  1 } passed from the previous stage, which are then filtered by the pruning function J t . Since we have complete rank and score information for these input documents (up to stage t  X  1), the pruning function J t can utilize their global rank and score. There are many ways to prune documents based on such global information: both rank-based [8] and score-based pruning methods [8, 1] have been proposed in the past. A key benefit of our model is that it is highly modular and flexible X  X he cascade is not restricted to a single pruning technique, but different stages can use different pruning functions J t , which may be better suited to work with its corresponding local model H t . This allows us to simply treat the different pruning methods as  X  X runing features X , which can be selected at each stage. Our goal is not to develop novel pruning methods, but rather to use existing methods as building blocks within our model.
In this section, we present three pruning methods ( J that we have found to work well in our experiments. Each of these methods is parameterized by a pruning threshold  X  The first two use document rank and score information to prune, while the third also considers the score distribution. Rank-based. This pruning method uses document rank to eliminate a desired proportion of the input documents at each stage. The rank-based cutoff is defined as follows: A document is pruned if it ranks below this cutoff value, where  X  t here is the pruning parameter, and | R { X  ,H t  X  1 the size of input documents at stage t . Large values of  X  lead to more aggressive pruning, i.e.,  X  t = 1 means all doc-uments are discarded.
 Score-based. Document scores provide another signal for pruning. Document scores for different queries are different, so enforcing a common score threshold is unlikely to work well. Instead, the score threshold is defined relative to the score range in each input document set: Where ScoreRange t  X  1 is defined to be the difference be-tween the maximum and minimum scores in input R { X  ,H t  X  1 equivalent to normalizing each score into [0, 1] by using the maximum and minimum scores in the candidate set. A doc-ument is pruned if it scores less than this threshold, where  X  t is the pruning parameter. As before, large value of  X  leads to more aggressive pruning.
 Mean-Max threshold. Often it is useful to consider the document score distribution for pruning. Several previous studies [14, 1] have considered the problem of inferring the score distributions of relevant and non-relevant documents, which are then used to help identify the best cutoff threshold for the top k documents in the ranked list to optimize a given evaluation metric. However, these methods only work for set-based measures such as F-measure and precision/recall, and do not work for top k ranked effectiveness measures.
We instead adopt a variant of this approach, and use a mean-max threshold function to capture characteristics of the score distribution, defined as a combination of the mean and the max of the input document scores: Where MaxS t  X  1 and MeanS t  X  1 are the maximum and mean scores in input R { X  ,H t  X  1 } , respectively. A document is pruned if it scores less than this mean-max threshold. Similar ap-proaches for using a mean-max threshold to control runtime scoring/prediction complexity have been used in the NLP task of structured prediction [29]. This formulation has the advantage that the pruning function can be better suited for each individual ranked list of documents.
We now turn to the problem of learning a well-balanced cascade that optimizes a desired tradeoff between retrieval efficiency and ranked effectiveness. The entire cascade is defined by { &lt;J t (  X  t ) ,H t , X  t &gt; } , for t = 1 ,...,T : J the pruning function and associated parameter (Section 3.1), and H t is the local ranking model (described below) with its associated weight  X  t .

Before we can learn a cascade, we must define how we measure top k ranked effectiveness and retrieval efficiency. For effectiveness, our primary measure is NDCG at k , al-though other metrics defined over top k rankings can easily be used instead. For retrieval efficiency, we use a cost model to estimate the execution cost of a given cascade. Retrieval engine details, such as query evaluation and caching strate-gies, are orthogonal to our general framework since their effects on query execution are captured by our cost model and simply serve as input to our learning algorithm.
The total cost of cascade S = { S t } , t = 1 ,...,T for query q , denoted by C ( S ,q i ), is the sum of individual stage costs: The cost of each stage is determined by the complexity of H and how many documents will be evaluated by H t . We let U denote the unit cost of evaluating H t over each document. The total cost of H t at stage S t is given by: where | R i { X  ,J t } | denotes the size of the non-pruned docu-ments after applying J t . Intuitively, this cost model captures the fact that evaluating a more complex model over a large number of documents will result in greater time complexity.
The exact value of U t depends on the implementation de-tails of the search engine. Several previous studies have pro-posed strategies for estimating retrieval costs [7, 25]. The most common approach is directly fitting U t to the actual query execution time of the ranking model [7]. We use this common approach for estimating U t , where we run each H t on the set of training queries, record its time, and set U to be the total time taken divided by the number of docu-ments evaluated by H t . For convenience, we normalize the unit costs so that the cheapest feature has a cost of one.
Finally, the query execution costs are unbounded, which makes them difficult to work with when learning a model. Therefore, we need to map the costs into the range [0 , 1]. This is accomplished by using an exponential decay func-tion exp(  X   X  C ( S ,q i )) to transform the cost into the [0 , 1] interval (  X  = 0 . 01 in our experiments). Other normalization techniques, such as computing the maximum cost (e.g., cost of applying the most expensive feature to every document in the collection) and then using it as a normalization factor, are also possible. However, this particular alternative may not differentiate costs very well since the cost distribution is likely to be skewed.
The cascade learning problem is a multi-objective opti-mization problem [22]. The final objective metric is obtained by linearly combining the multiple objectives, which, in our case are the top k ranked effectiveness and the cost of the cascade model S . For a given query q i , the tradeoff is defined as follows: where E ( S ,q i ) represents ranked effectiveness, C ( S ,q the computational cost (Equation 4), and  X   X  [0 , 1] is a parameter that controls the relative importance between ef-fectiveness and efficiency. Note that E ( S ,q i ) and E ( f mean the same thing, i.e., the effectiveness achieved by a cascade S with T stages (by Equation 1); in cases where we wish to draw attention to the ranking of the cascade, we use f
T for convenience. From the tradeoff definition, it should be clear that as we add more stages to a cascade, the total cost will increase. Therefore, in order to improve the trade-off metric, the effectiveness gain from adding extra stages must counteract their costs. Algorithm 1 : Boosting algorithm for cascade learning
Initialize distribution P 1 ( q i ) = 1 /N , where N is the number of queries and q i denotes a query;
Initialize cascade model S = {} ; for t = 1 ,...,T do end return cascade model S
We now turn to the problem of learning the best cascade model. The general setup is that given a set of ranking features (described later in Section 4.4), pruning functions (Section 3.1), and training queries with their associated rel-evance judgments, we want to learn a cascade to optimize a given tradeoff metric, where the cascade model is character-ized by { &lt;J t (  X  t ) ,H t , X  t &gt; } , t = 1 ,...,T .
We propose a novel boosting algorithm, a generalization of AdaRank [30], that jointly learns the cascade structure and parameters. It is important to note that we can not simply use AdaRank to optimize our tradeoff metric because AdaRank assumes linearity of its optimization metric O , i.e., denotes the additive model up to stage t  X  1, S t is a stage and  X  t is the local weight [30, 16]. Note that each AdaRank stage consists of only H t (a weak learner), since it does not perform document pruning. The tradeoff metric T does not satisfy the assumption because  X  t in our case is not defined over the entire stage S t , since in addition to H t , the stage has a pruning function J t for document reduction as well.
Our boosting algorithm for jointly optimizing top k ranked effectiveness and retrieval efficiency in a unified framework is shown in Algorithm 1. The algorithm proceeds in rounds to sequentially learn a set of cascade stages to optimize over weighted training instances. Each training instance (a query q ) has an associated importance weight, denoted by P t ( q Initially, the weight distribution is set to uniform, and is updated at the end of each iteration. At each iteration, the parameterized pruning function J t (  X  t ) and the weak ranker H t are first constructed based on the weighted training data. We describe this construction in more detail in Section 4.4.
Once J t (  X  t ) and H t are chosen, the algorithm selects the local weight  X  t &gt; 0 for the ranker H t , where E ( S t C ( S t ,q i ) in the formula denote the effectiveness and cost, respectively, from evaluating H t on the reduced set of doc-uments (after J t ). Intuitively,  X  t captures the effectiveness of H t over weighted training instances.

Once  X  t is selected, we add the fully constructed stage to the current cascade model. The weight distribution P t +1 then updated using the cost and effectiveness from the over-all cascade (as defined in the previous section). The weights on the underperforming queries (i.e., queries that have poor ranked effectiveness, yet are expensive to compute) are in-creased, so the subsequent iteration can focus more on im-proving those hard queries. Note that H 0 , the first stage in the cascade, is not associated with any pruning. Stage H simply scores and passes a set of top hits R { H 0 } to the first cascade stage (in our experiments, | R { H 0 } | = 20 , 000).
In this paper, we use single features as weak rankers H t as in AdaRank [30]. Table 2 provides a summary of the fea-tures, which are similar to those in previous work (e.g., [18]). We use two families of scoring functions, based on the Dirich-let score from language modeling and BM25. Each family consists of a unigram feature, a bigram proximity feature that takes term order into account (parameterized with a window S  X  { 1 , 2 , 4 } ), and a bigram feature score for un-ordered terms (parameterized with a window S 0  X  X  2 , 4 , 8 } ).
Typically, bigram features are computed over the entire query, which is problematic, as pointed out in Wang et al. [28]. Consider the query  X  X hite house rose garden X : in-tuitively, the bigram  X  X hite house X  is more important than  X  X ouse rose X . Computing features for all bigrams would be wasteful, so we need a mechanism to capture the importance of different query bigrams. It is accomplished by parameter-izing bigram features with an  X  X mportance bin X . Each query bigram occupies a bin, sorted by concept importance as measured by the weighted sequential dependence model [4]. Therefore, selecting the first bin amounts to selecting the most important query bigram. The cross of the feature and the bin is available to the learner to independently select from, thus allowing the cascade to selectively add query bi-grams. Note that unigram features are not binned.

We note that our cascade model and learning algorithm can work with other ranking features beyond those defined here. Our approach can easily handle hundred or even thou-sands of features, the scale at which commercial search en-gines operate. The contribution of this work is not feature engineering, but rather the novel cascade ranking model and learning algorithm.

Let S t denote the pair &lt; J t (  X  t ) ,H t &gt; , where J ing function as defined in Section 3.1 and H t is a weak ranker drawn from one of the features described above. At each boosting iteration, we select a stage S t according to the fol-lowing formula: where E ( S t ,q i ) and C ( S t ,q i ) are effectiveness and cost, re-spectively, from computing H t over the reduced set of docu-ments (after pruning). 1 The goal of this optimization is to find the optimal combination of J t (  X  t ) and H t that best bal-ances cost and effectiveness over the weighted training data. Several methods can be used for this optimization, and in
Note that for the purposes of the above arg max computa-tion,  X  t is irrelevant. features,  X  is a smoothing parameter; for BM25, K = k [(1  X  b )  X  b  X  ( | D | / | D | 0 )] , and k 1 , b are free parameters. this work, we employ grid search [19] to find the set of H J and  X  t that maximizes the equation.

A formal analysis of the boosting algorithm is presented in Section 4.5 and the proof is provided in the Appendix. For now, we note that when the tradeoff parameter  X  = 0 (i.e., efficiency is ignored), the model simplifies to AdaRank. However, our algorithm can produce both effective and effi-cient ranking models and can be viewed as a generalization of AdaRank X  X  effectiveness-only approach.
In this section, we show how our boosting algorithm can continuously improve the tradeoff metric over the training data. We want to maximize the tradeoff metric T over the training queries: which is equivalent to: Because 1  X  x  X  e  X  x for any real value x , we minimize an exponential upper-bound of above expression:
In our case, a linear combination of weak rankers is used to score the documents, with pruning performed at each stage. The optimization in Equation 9 is the same as: where S t  X  1 denotes the cascade up to stage t  X  1. For de-termining a single stage S t , our boosting algorithm takes the approach of  X  X orward stage-wise selection X  [13], i.e., suc-cessively adding each cascade stage to improve the overall tradeoff metric. It can be proved that there exists a lower bound on the tradeoff metric over the training data: T  X  1  X  where  X  t is given in Equation 6, and let where f t  X  1 denotes the linear combination of weak rankers up to t  X  1 (applied to the non -pruned documents only) queries. This means that the tradeoff metric can be contin-uously improved as long as the following holds: That is, this condition is satisfied as long as the gain in effectiveness from additional stages is not outweighed by its cost. A detailed proof can be found in the Appendix.
This section presents experimental results: we first de-scribe the experimental setup and implementation details, and then present an evaluation using TREC data.
Our cascade model was evaluated on three TREC web test collections: Wt10g, Gov2, and Clue (first English segment of ClueWeb09). Details for these collections are provided in Table 1. The topic titles were used as queries, split equally into a training and a test set. Model parameters were tuned on the training set; reported results are from the test set.
We compare our cascade model against a set of strong baselines in terms of top k ranked effectiveness and retrieval efficiency. Effectiveness is measured in terms of NDCG20 and precision at 20 (P20), while retrieval efficiency is mea-sured in terms of average query execution time. Our cas-cade model is compared against three others: AdaRank [30], Table 1: Summary of TREC collections and topics used in our experiments. which can be seen as a special case of the cascade model (i.e., optimized for effectiveness only with no efficiency con-siderations); a previously best-known model that jointly op-timizes for both ranked effectiveness and efficiency by reduc-ing the number of features computed at query time (which we call  X  X eaturePrune X ) [27]; and the basic query-likelihood model (QL). For fairness of comparison,  X  X eaturePrune X  re-implements the approach and training method (greedy line search) described by Wang et al. [27], using the exact same feature set and objective function as our cascade model. As previously noted, since Cambazoglu et al. [8] focuses on early-exit strategies given a particular additive ensemble, it is difficult to meaningfully compare with our approach.
For training, we used NDCG20 as the effectiveness mea-sure ( E ) in our tradeoff metric T , with  X  set to 0.1. Both the cascade structure and the cascade parameters are auto-matically learned by directly optimizing the tradeoff metric over the training set. All results are reported over the test set. The Wilcoxon signed rank test with p &lt; 0 . 05 was used to test for statistical significance.
 Experiments were performed on a server running Red Hat Linux, with dual Intel Xeon quad-core processors (E5620 2.4GHz), 64GB RAM, and six 2TB 7.2K RPM SATA drives in RAID-6 configuration.
All models were implemented in the Ivory open-source retrieval toolkit [15]. Baseline QL, AdaRank, and Feature-Prune work exactly as one might expect: by traversing post-ings in an inverted index and performing document-at-a-time scoring with max-score optimization [25]. The first stage of our cascade H 0 also works in the same way, using the weak learner that was selected by our boosting algo-rithm (retaining the top 20,000 hits). However, the remain-ing stages in the cascade adopt a different architecture. For stage H 1 and subsequent stages, we construct a forward in-dex, which is essentially a list of pairs consisting of a doc-ument and a query term, grouped by the document. This structure can be efficiently built on the fly as we traverse postings in the initial cascade stage, by retaining the top documents as determined by the pruning function used in the first cascade stage. The forward index is small enough to be stored in memory and query evaluation for the subsequent stages is performed by iterating over the forward index. The reported retrieval efficiency of our cascade model accounts for the overall time taken by the cascade to return results, including the first stage. Other than this architectural dif-ference, all models share exactly the same code, which makes for a fair comparison. Note that in all cases we used a single monolithic inverted index (i.e., no document partitioning). Based on the method described in Section 4.1, we computed U
T to be 1 for unigram and 20 for bigram features. This empirically matches actual retrieval times well.

One final implementation detail: to speed up pruning, our cascade allows pruning J t to be performed  X  X n-the-fly X  within the computation of H t , and so it incurs no additional cost. To see this, we observe that all three pruning meth-ods prune input documents based on their rank order, i.e., a document with low score will be pruned before a document with high score. Thus, we simply iterate over the input doc-uments in rank order, checking if each document d i passes J and if so, H t is evaluated; else, the pruning/scoring process at stage t terminates (because if d i does not pass pruning, any document ranked below it will not either). Note that descriptive statistics such as minimum, maximum, mean, etc. can be computed at the previous stage and passed to the pruning function. Coupling pruning J t with H t makes pruning extremely efficient.
Table 2 reports NDCG20, P20, and average query eval-uation time for our cascade model, QL, AdaRank, and the FeaturePrune method. For all three datasets, percentage im-provements for both NDCG20 and P20 are shown in paren-theses: over QL for AdaRank, and over QL/AdaRank for FeaturePrune and the cascade model. Statistical signifi-cance is denoted by special symbols in the table.

In all datasets, the cascade model achieves similar (and many times slightly better) effectiveness compared to Ada-Rank in both NDCG20 and P20, while being much faster. For instance, the cascade is 32.7%, 48.7%, and 34.7% faster than AdaRank on Wt10g, Gov2, and Clue, respectively. This means that our cascade model can equal or beat an effectiveness-only boosting model, while also being much faster. This illustrates that using a monolithic ranking func-tion, as has been common practice for ad hoc retrieval, trades a great deal of efficiency for effectiveness. Such costly mono-lithic models are not more effective either since most of the documents they score are not relevant anyway. This also highlights the advantage of the cascade: by progressively re-ducing the size of candidate documents, it allows for the use of more complex ranking functions for high effectiveness without sacrificing efficiency.

Furthermore, we observe that the efficiency improvement of the cascade over AdaRank is greater for the two larger datasets (Gov2 and Clue) than Wt10g. This makes sense: compared to smaller document collections, larger collections contain more non-relevant documents. Thus, by filtering out these documents early in the ranking process, the cascade drastically improves efficiency and avoids evaluating docu-ments that have little chance of appearing in the top k .
The cascade model also outperforms the feature prun-ing method in all evaluation measures across all datasets. In terms of retrieval time, the cascade is 12.9%, 44.5%, and 24.9% faster than the feature prune method on Wt10g, Gov2, and Clue, respectively. In terms of ranked effective-ness, the feature pruning method is slightly worse than Ada-Rank, likely due to removing ranking features for efficiency considerations. The FeaturePrune method behaves exactly as described in Wang et al. [27], discovering a better trade-off point by giving up a bit of effectiveness for a gain in efficiency. However, the cascade model is able to obtain the best of both worlds: it can achieve better top k effectiveness and return results in a shorter amount of time.

Finally, compared to QL, which only uses simple term-based features for ranking and hence is very efficient, we observe that the cascade model is only slightly slower, but achieves much better top k effectiveness. Our cascade model NDCG20 P20 Time NDCG20 P20 QL 0.080 34.07 32.40 1.15 44.57 50.93 2.60 27.50 34.20
AdaRank 0.260 35.49 33.50 3.90 47.37 * 53.60 6.55 30.94 * 37.40
FeaturePrune 0.201 34.86 33.10 3.61 47.16 51.87 5.70 29.66 36.20
Cascade 0.175 35.60 33.80 2.00 47.44 * 54.47 * 4.28 30.60 Filtered Filter loss NDCG20 Filtered Filter loss for rank-based pruning, S for score-based pruning, and
M for the mean-max threshold. outperforms QL by 4.5%, 6.4% and 11.3% in NDCG20 on Wt10g, Gov2, and Clue, respectively, with the improve-ments on Gov2 and Clue statistically significant. Similar gains are also observed for P20.
For the cascades learned in the previous section, we exam-ine their behavior on a stage-by-stage basis in terms of effec-tiveness and efficiency. A detailed analysis is shown in Ta-ble 3: for each cascade stage, we present NDCG20 achieved up to that stage and the percentage of documents filtered from the previous stage. The values are annotated with the pruning function J learned by our boosting algorithm at each stage. We also compute filter loss, defined as the percentage of documents incorrectly filtered (i.e., relevant documents which are pruned) out of all documents passed from the previous stage. For all three collections, a tradeoff parameter of  X  = 0 . 1 yields four stages. This is because the cost from adding additional stages outweighs the marginal gain in effectiveness, even with document pruning.

We see from Table 3 that in most cases, each cascade stage processes a substantially smaller set of documents than the previous stage, but always improves ranked effectiveness. As an example, by Stage 1, the cascade reduces the document set size by more than 90% in all three test collections, how-ever, NDCG20 continues to improve in subsequent stages, due to using high quality/expensive ranking features over the small number of retained documents. For instance, our boosting algorithm learns to use simple term-based features in the initial stage for all three datasets, and uses term-proximity features (which are more costly) in subsequent stages to further improve the model X  X  retrieval effectiveness. It is also interesting to see in all three datasets, the first stage prunes much more aggressively than subsequent stages. Be-cause the input document set size is the largest at the first stage, the efficiency of the cascade can be significantly im-proved by eliminating a large number of documents early.
Also interesting is that in comparison to the NDCG20 achieved by FeaturePrune in Table 2 (which optimizes the same tradeoff), the cascade quickly achieves comparable ef-fectiveness, and then surpasses it in subsequent stages. For instance, in comparison to Table 2, by stage 1, the cas-cade begins to surpass the final NDCG20 score achieved by FeaturePrune (Wt10g and Clue). By the final stage, the cascade NDCG20 scores surpass that achieved by AdaRank (Wt10g and Gov2). This illustrates that document pruning performed by the cascade improves efficiency while having minimal impact on effectiveness, compared to the monolithic ranking models. This is confirmed by the very low filter loss reported in the table (nearly zero for all stages).
We also observe that at stage 2 for Wt10g and stage 3 for Gov2, the cascade does not prune any input documents. This behavior can be explained by the tradeoff metric X  X he effectiveness gain from applying the ranking feature on all input documents outweighs its cost, and therefore the op-timal pruning parameters at these stages are zero (i.e., no pruning). However, interestingly, the learned cascade for Clue, the largest collection, always prunes at all stages, and much more aggressively (e.g., at 97 . 7% , 68 . 3% and 10 . 7%) than the same stages for the two smaller collections. This is because web-scale collections contain a greater proportion of non-relevant documents; to combat this, the model learns that more aggressive pruning is necessary.

Finally, further analysis reveals that relevant documents that are filtered by our cascade are not ranked in the top k documents by AdaRank either, i.e., these documents have no chance of entering the top k even if an effectiveness-centric model is used. The cascade model is able to obtain the best of both worlds: it can achieve better top k effectiveness and return results in a shorter amount of time, compared to both AdaRank and the feature-pruning approach.
Our final set of experiments explores the effects of vary-ing  X  , the tradeoff parameter that balances effectiveness E and cost C in our objective function T . The setting of  X  af-fects the cascade model and the feature pruning method, but not AdaRank (since it does not take into account efficiency) or the QL baseline (since no training is involved). Fig-ure 3 shows NDCG20 of our cascade model and the feature-pruning method as function of average query evaluation time for each of the three collections, where each point represents Different values of  X  produce different effectiveness/efficiency tradeoffs: a high value penalizes costly ranking functions, thus yielding faster models, whereas a smaller value yields more effective models. In each graph, the effectiveness lower bound, defined as the minimum effectiveness achieved under any condition, is plotted as the lower solid line, and the effec-tiveness upper bound, defined as the maximum effectiveness achieved, is plotted as the upper dotted line.

While in general effectiveness improves for both the cas-cade and the feature-pruning method when given more time for ranking, the cascade model consistently achieves equal or better NDCG20 across all conditions. It also approaches the upper bound more rapidly as time increases. Although both the cascade model and the feature-pruning method are able to realize different effectiveness/efficiency tradeoffs, these re-sults show that the cascade model is superior in being able to return higher quality results much faster.

We also note that for Gov2 and Clue, the cascade tradeoff curve rises more steeply than for Wt10g. This bolsters our argument that the cascade model works particularly well for large collections. From the point of view of top k ranked effectiveness and efficiency, applying ranking features on a small number of documents is considerably more efficient but can also be more effective (by eliminating many non-relevant documents from consideration).
In this paper, we introduce a cascade ranking model for efficient top k retrieval, where a sequence of increasingly complex ranking functions is used to progressively refine a shrinking set of candidate documents. We propose a novel boosting-based algorithm that jointly learns the model struc-ture (i.e., the optimal sequence of ranking stages) and the pruning criteria at each stage. Experiments show that our cascade model is able to simultaneously achieve high effec-tiveness and fast retrieval.

There are several future directions. We have only begun to explore the design space of machine learning algorithms for multi-objective optimization in the context of learning to rank: possibilities include other types of objectives (beyond linear combinations), other pruning functions (that take into account more than document score and rank), and different types of weak rankers (for example, decision trees). More work along these lines can further contribute to this emerg-ing thread of learning to efficiently rank.
This work has been supported by NSF under awards IIS-0836560, IIS-0916043, and CCF-1018625. Any opinions, findings, conclusions, or recommendations expressed are the authors X  and do not necessarily reflect those of the sponsors. The second author is grateful to Esther and Kiri for their loving support and dedicates this work to Joshua and Jacob. [1] A. Arampatzis, J. Kamps, and S. Robertson. Where [2] R. Baeza-Yates, A. Gionis, F. Junqueira, V. Murdock, [3] L. Barroso and U. H  X  olzle. The Datacenter as a [4] M. Bendersky, D. Metzler, and W. Croft. Learning [5] E. Brown. Fast evaluation of structured queries for [6] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [7] F. Cacheda, V. Plachouras, and I. Ounis. A case study [8] B. Cambazoglu, H. Zaragoza, O. Chapelle, J. Chen, [9] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. [10] D. Carmel, D. Cohen, R. Fagin, E. Farchi, [11] K. Collins-Thompson. Reducing the risk of query [12] J. Hamilton. On designing and deploying [13] T. Hastie, R. Tibshirani, and J. Friedman. The [14] E. Kanoulas, V. Pavlu, K. Dai, and J. Aslam.
 [15] J. Lin, D. Metzler, T. Elsayed, and L. Wang. Of Ivory [16] T.-Y. Liu. Learning to rank for information retrieval. [17] I. Matveeva, C. Burges, T. Burkard, A. Laucius, and [18] D. Metzler. Automatic feature selection in the Markov [19] J. Nocedal and S. Wright. Numerical optimization . [20] A. Ntoulas and J. Cho. Pruning policies for two-tiered [21] G. Skobeltsyn, F. Junqueira, V. Plachouras, and [22] R. Steuer. Multiple Criteria Optimization: Theory, [23] T. Strohman, H. Turtle, and W. Croft. Optimization [24] R. Tibshirani. Regression shrinkage and selection via [25] H. Turtle and J. Flood. Query evaluation: Strategies [26] P. Viola and M. Jones. Robust real-time object [27] L. Wang, J. Lin, and D. Metzler. Learning to [28] L. Wang, D. Metzler, and J. Lin. Ranking under [29] D. Weiss and B. Taskar. Structured prediction [30] J. Xu and H. Li. AdaRank: A boosting algorithm for
