 Neural networks have exhibited considerable po-tential in various fields (Krizhevsky et al., 2012; Graves et al., 2013). In early years on neural NLP research, neural networks were used in lan-guage modeling (Bengio et al., 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc. In the field of NLP, neural networks are typically combined with word embeddings, which are usually first pre-trained by unsupervised algorithms like Mikolov et al. (2013); then they are fed forward to standard neural models, fine-tuned during supervised learn-ing. However, embedding-based neural networks usually suffer from severe overfitting because of the high dimensionality of parameters.
A curious question is whether we can regular-ize embedding-based NLP neural models to im-prove generalization. Although existing and newly proposed regularization methods might alleviate the problem, their inherent performance in neural NLP models is not clear: the use of embeddings is sparse; the behaviors may be different from those in other scenarios like image recognition. Further, selecting hyperparameters to pursue the best performance by validation is extremely time-consuming, as suggested in Collobert et al. (2011). Therefore, new studies are needed to provide a more complete picture regarding regularization for neural natural language processing. Specifically, we focus on the following research questions in this paper.
 RQ 1: How do different regularization strategies RQ 2: Can regularization coefficients be tuned in-RQ 3: What is the effect of combining different
In this paper, we systematically and quan-titatively compared four different regularization strategies, namely penalizing weights, penalizing embeddings, newly proposed word re-embedding (Labutov and Lipson, 2013), and dropout (Srivas-tava et al., 2014). We analyzed these regulariza-tion methods by two widely studied models and tasks. We also emphasized on incremental hyper-parameter tuning and the combination of different regularization methods.

Our experiments provide some interesting re-sults: (1) Regularizations do help generalization, but their effect depends largely on the datasets X  size. (2) Penalizing ` 2 -norm of embeddings helps optimization as well, improving training accu-racy unexpectedly. (3) Incremental hyperparam-eter tuning achieves similar performance, indicat-ing that regularizations mainly serve as a  X  X ocal X  effect. (4) Dropout performs slightly worse than ` 2 penalty in our experiments; however, provided very small ` 2 penalty, dropping out hidden units and penalizing ` 2 -norm are generally complemen-tary. (5) The newly proposed re-embedding words method is not effective in our experiments. Experiment I: Relation extraction . The dataset in this experiment comes from SemEval-2010 between two marked entities in each sentence. We refer interested readers to recent advances, e.g., Hashimoto et al. (2013), Zeng et al. (2014), and Xu et al. (2015). To make our task and model general, however, we do not consider entity tag-ging information; we do not distinguish the order of two entities either. In total, there are 10 labels, i.e., 9 different relations plus a default other .
Regarding the neural model, we applied Col-lobert X  X  convolutional neural network (CNN) (Collobert and Weston, 2008) with minor modi-fications. The model comprises a fixed-window convolutional layer with size equal to 5, 0 padded at the end of each sentence; a max pooling layer; a tanh hidden layer; and a softmax output layer.
Experiment II: Sentiment analysis . This is another testbed for neural NLP, aiming to pre-dict the sentiment of a sentence. The dataset is the Stanford sentiment treebank (Socher et al., 2011) 2 ; target labels are strongly/weakly positive/negative , or neutral .

We used the recursive neural network (RNN), which is proposed in Socher et al. (2011), and fur-ther developed in Socher et al. (2012); Irsoy and Cardie (2014). RNNs make use of binarized con-stituency trees, and recursively encode children X  X  information to their parent X  X ; the root vector is fi-nally used for sentiment classification.

Experimental Setup . To setup a fair compari-son, we set all layers to be 50-dimensional in ad-vance (rather than by validation). Such setting has been used in previous work like Zhao et al. (2015). Our embeddings are pretrained on the Wikipedia corpus using Collobert and Weston (2008). The learning rate is 0.1 and fixed in Experiment I; for RNN, however, we found learning rate decay helps to prevent parameter blowup (probably due to the recursive, and thus chaotic nature). There-fore, we applied power decay (Senior et al., 2013) with power equal to  X  1 . For each strategy, we tried a large range of regularization coefficients, no effect with granularity 10x. We ran the model 5 times with different initializations. We used mini-batch stochastic gradient descent; gradients are computed by standard backpropagation. For
It needs to be noticed that, the goal of this paper is not to outperform or reproduce state-of-the-art results. Instead, we would like to have a fair com-parison. The testbed of our work is two widely studied models and tasks, which were not chosen on purpose. During the experiments, we tried to make the comparison as fair as possible. There-fore, we think that the results of this work can be generalized to similar scenarios. In this section, we describe four regularization strategies used in our experiment.  X  Penalizing ` 2 -norm of weights. Let E be the  X  Penalizing ` 2 -norm of embeddings. Some  X  Re-embedding words (Labutov and Lipson,  X  Dropout (Srivastava et al., 2014). In this This section compares the behavior of each strat-egy. We first conducted both experiments with-out regularization, achieving accuracies of 54 . 02  X  0 . 84% , 41 . 47  X  2 . 85% , respectively. Then we plot in Figure 1 learning curves when each regulariza-tion strategy is applied individually. We report training and validation accuracies through out this paper. The main findings are as follows.  X  Penalizing ` 2 -norm of weights helps gener- X  Penalizing ` 2 -norm of embeddings unexpect- X  Re-embedding words does not improve gen- X  Dropout helps generalization. Under the best The above experiments show that regularization generally helps prevent overfitting. To pursue the best performance, we need to try out different hy-perparameters through validation. Unfortunately, training deep neural networks is time-consuming, preventing full grid search from being a practical technique. Things will get easier if we can incre-mentally tune hyperparameters, that is, to train the model without regularization first, and then add penalty.

In this section, we study whether ` 2 penalty of weights and embeddings can be tuned incremen-tally. We exclude the dropout strategy because its does not make much sense to incrementally drop out hidden units. Besides, from this section, we only focus on Experiment I due to time and space limit.

Before continuing, we may envision several possibilities on how regularization works.  X  (On initial effects) As ` 2 -norm prevents pa- X  (On eventual effects) ` 2 penalty lifts er-
To verify the above conjectures, we design four settings: adding penalty (1) at the beginning, (2) before overfitting at epoch 2, (3) at peak perfor-mance (epoch 5), and (4) after overfitting (valida-tion accuracy drops) at epoch 10.

Figure 2 plots the learning curves regarding pe-nalizing weights and embeddings, respectively; baseline (without regularization) is also included.
For both weights and embeddings, all settings yield similar ultimate validation accuracies. This shows ` 2 regularization mainly serves as a  X  X ocal X  effect X  X t changes the error surface, but parame-ters tend to settle down to a same catchment basin. We notice a recent report also shows local optima Table 1: Accuracy in percentage when we com-bine ` 2 -norm of weights and embeddings (Exper-iment I). Bold numbers are among highest accu-racies (greater than peak performance minus 1.5 times standard deviation, i.e., 1.26 in percentage). Table 2: Combining ` 2 regularization and dropout. Left: connectional weights. Right: embeddings. ( p refers to the dropout rate.) may not play an important role in training neural networks, if the effect of parameter symmetry is ruled out (Breuel, 2015).

We also observe that regularization helps gener-alization as soon as it is added (Figure 2a), and that regularizing embeddings helps optimization also right after the penalty is applied (Figure 2b). We are further curious about the behaviors when different regularization methods are combined.
Table 1 shows that combining ` 2 -norm of weights and embeddings results in a further accu-racy improvement of 3 X 4 percents from applying either single one of them. In a certain range of coefficients, weights and embeddings are comple-mentary: given one hyperparameter, we can tune the other to achieve a result among highest ones.
Such compensation is also observed in penal-izing ` 2 -norm versus dropout (Table 2) X  X lthough the peak performance is obtained by pure ` 2 regu-larization, applying dropout with small ` 2 penalty also achieves a similar accuracy. The dropout rate is not very sensitive, provided it is small. In this paper, we systematically compared four regularization strategies for embedding-based neural networks in NLP. Based on the experimen-tal results, we answer our research questions as follows. (1) Regularization methods (except re-embedding words) basically help generalization. Penalizing ` 2 -norm of embeddings unexpectedly helps optimization as well. Regularization perfor-mance depends largely on the dataset X  X  size. (2) ` 2 penalty mainly acts as a local effect; hyperpa-rameters can be tuned incrementally. (3) Combin-ing ` 2 -norm of weights and biases (dropout and ` 2 penalty) further improves generalization; their co-efficients are mostly complementary within a cer-tain range. These empirical results of regulariza-tion strategies shed some light on tuning neural models for NLP.
 This research is supported by the National Basic Research Program of China (the 973 Program) un-der Grant No. 2015CB352201 and the National Natural Science Foundation of China under Grant No. 61232015. We would also like to thank Hao Jia and Ran Jia.
