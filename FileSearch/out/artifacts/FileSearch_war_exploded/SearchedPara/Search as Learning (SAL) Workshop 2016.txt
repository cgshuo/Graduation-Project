 The  X  X earch as Learning X  (SAL) workshop is focused on an area within the information retrieval field that is only beginning to emerge: supporting users in their learning whilst interacting with informa-tion content.
 Search, learning, human information interaction
Search systems to date are viewed more as tools for the retrieval of content to satisfy immediate information needs, than as part of larger complex information environments in which humans learn while interacting with information content. As users increasingly learn informally while searching as well as use search systems as tools for self-study, there is a growing recognition of the impor-tance to address the challenges of designing , developing , and eval-uating search systems that foster discovery and enhance learning outside of formal educational settings.

The research agenda of  X  X earch as Learning X  aims to bring to-gether these challenges and opportunities by reaching out to re-searchers with backgrounds in information science (IS), human com-puter interaction (HCI), and information retrieval (IR), with the goal of integrating conceptual, experimental, and simulation-based approaches and methodologies from within these different fields. This will allow the transformation of search systems as isolated in-formation access tools into systems that provide support for learn-ing directly and that consider the broader outcomes of searching beyond a set of search results.

Studies in IS have always focused on the broader context of search with the aim to understand, conceptualise, and form theo-ries of the relations between user behavior and the users X  informa-tion environment. Earlier work identified search as not to be an isolated activity but part of a larger information seeking process, i.e.,  X  X  process, in which humans purposefully engage in order to change their state of knowledge X  [12]. Further work proposed that the information seeking processes should be thought of as driven by higher-level human needs or the user environment, e.g, in the context of a work task [1]. The importance of learning has resur-faced as noted by Jansen et al. [10]:  X  X  learning theory may better describe the information searching process than more commonly used paradigms of decision making or problem solving X . Con-necting learning theory and IS perspectives [13] suggests that  X  X he use of information as the fundamental building block for learning X . In the first edition of this workshop (SAL X 14) Freund et al. [7] presented a conceptual framework for the role of search in infor-mal learning and Bystr X m [2] discussed the social, individual and techno-material dimensions of search as a learning activity in the context of real-life work tasks and how this leads to future research topics and methodological implications for interactive information retrieval. These conceptual models and frameworks provide theo-retical foundations for exploratory analyses of log data as well as the development of empirical models to validate the proposed the-ories. Recently, Vakkari [17] and Rieh et al. [15] reviewed relevant research so far and discussed future directions from both theoretical and empirical perspective [8].

Within the IR and HCI community studies addressing different aspects of learning during search are gaining traction, while follow-ing a more empirical, data-driven methodology, e.g., studies that analyse user learning behaviour from commercial search logs [5, 18], and that investigate behavioural indicators of the evolving sta-tus of users X  knowledge during search [4, 6, 19]. Further, the need to develop evaluation paradigms that go beyond individual query interaction has been noted by many IR researchers. A number of directions have been proposed, e.g., towards whole session based evaluation, user-centric evaluation methods and metrics [3, 9, 14, 16], as well as evaluation of search outcome in broader contexts such as work tasks [11]. The effort of integrating whole-session and task based evaluation of IR systems with IR X  X  test-collection driven simulation based evaluation paradigm is reflected by the re-cent editions of the TREC Session Track and Tasks Track. The in-teractive and context-rich nature of learning as both a process and outcome of information seeking is the next frontier in the evaluation of information systems and is a natural fit for the strong evaluation focussed IR community.

Together, the theory from IS and empirical models from IR stud-ies have the necessary prerequisites to shed light on how local eval-uation of ranking and system design within a particular part of the search process can be combined with more global assessments of system-user performance. On the one hand, conceptual models and frameworks of the cognitive process of searching and learning would provide guidance to use the empirical results in appropriate contexts; and on the other hand, empirical predictive models and simulation based evaluation would provide means to translate the conceptual models and frameworks into operations at an algorith-mic level. However, it is not yet well understood how the results of these different lines of research should be put together and opera-tionalised for the design and evaluation of IR systems that support learning activities.
 SIGIR is a key venue that brings together researchers from IS, HCI, and IR, and the SAL workshop provides the opportunity where theory meets empirical studies. The discussion in the workshop will not only identify and prioritise the problems and solutions on the topic of search as learning, but also contribute to the future of IR research in seeking integrated cognitive and empirical approaches to the theory, modelling, and evaluation of information seeking pro-cesses and information systems.
The Search as Learning Workshop aims to flesh out research di-rections and methodologies and survey state-of-the-art approaches in this important emerging research area. We are particularly inter-ested in engaging researchers across the IR, IS, HCI and learning science fields.

Topics of interest include the following but not limited to: The workshop includes two keynotes presentations (Kevyn Collins-Thompson from University of Michigan and Andreas Nuernberger from University of Magdeburg), presentations of selected papers, and interactive sessions in the format of breakout groups.
The focus of the workshop is to encourage interaction and collab-oration among attendees. With the interactive sessions, we expect to identify and define the major challenges from the perspective of different research areas, and to explore and discover interdisci-plinary challenges. Overall, we expect the workshop to foster new research directions for search and learning and future collabora-tions such as joint projects and publications. [1] P. Borlund. The IIR evaluation model: a framework for [2] K. Bystr X m. Searching as a learning activity in real life [3] M. Cole, J. Liu, N. Belkin, R. Bierig, J. Gwizdka, C. Liu, [4] M. J. Cole, J. Gwizdka, C. Liu, N. J. Belkin, and X. Zhang. [5] C. Eickhoff, J. Teevan, R. White, and S. Dumais. Lessons [6] C. Eickhoff, S. Dungs, and V. Tran. An eye-tracking study of [7] L. Freund, H. O X  X rien, and R. Kopak. Getting the big [8] P. Hansen and S. Y. Rieh. Editorial: Recent advances on [9] J. He, M. Bron, A. de Vries, L. Azzopardi, and M. de Rijke. [10] B. J. Jansen, D. Booth, and B. Smith. Using the taxonomy of [11] K. J X rvelin and P. Ingwersen. Information seeking research [12] G. Marchionini. Information seeking in electronic [13] D. Neuman. Learning in information-rich environments. In [14] K. Raman, P. N. Bennett, and K. Collins-Thompson. Toward [15] S. Y. Rieh, K. Collins-Thompson, P. Hansen, and H.-J. Lee. [16] M. D. Smucker and C. L. Clarke. Time-based calibration of [17] P. Vakkari. Searching as learning: A systematization based [18] R. W. White, S. T. Dumais, and J. Teevan. Characterizing the [19] X. Zhang, M. Cole, and N. Belkin. Predicting users X  domain
