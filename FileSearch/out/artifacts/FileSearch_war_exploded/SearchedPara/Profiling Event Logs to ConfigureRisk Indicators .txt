 Managing risks is one of the top priorities in corporate and government organ-isations 1 . According to ISO Guide 73:2009, risk is the  X  X ffect of uncertainty on objectives X  where an effect is  X  X  deviat ion from the expected  X  positive and/or negative X  [6]. Risk identification is an essential starting point for risk manage-ment. It is defined as a  X  X rocess of finding, recognizing and describing risks X  [6]. Although many risk management approaches provide high-level guidance about risk management strategy, they do not provide any tools to operationalize this strategy [12,15]. Standard ISO 31000 specifies that  X  X isk identification can in-volve historical data X  [15], however it do es not provide any further guidelines on how to use historical data.
 Managing business processes is another important concern of an organisation. Business processes are exposed to differ ent risks. For instance, a process may not be finished within the time-frame defined by a service level agreement, it may produce low-quality results, or it may exceed its budget. We refer to risks that threaten the achievement of process goals as process-related. Most organisations use information systems supporting their operational business processes. Often these systems also record information a bout process executions in event logs. Our belief is that this information can be exploited for the identification of process-related risks.
 In our preliminary work [14] we introduced the idea of using Process Risk Indicators (PRIs) to predict whether a deadline transgression is likely to hap-pen or not. For example, if an activity is repeated multiple times for a case, then the likelihood of delay is significantly higher. We also introduced a method for instantiating these indicators from event logs based on statistical techniques for outlier detection. However, our initial experiments showed that further work is required to properly calibrate the indicators to reduce the number of  X  X alse positives X , i.e., cases that are predicted to be late but in the end are not. In this paper we present a novel method for configuration of PRIs that uses informa-tion about outcomes from cases execute d in the past. The method aligns these indicators with the specifics of a particular process to minimize the number of false positives. We demonstrate the feasibility of the proposed approach using case studies with data sets from an Australian insurance company (Suncorp).
The remainder of the paper is organized as follows. Section 2 discusses re-lated work. The general approach to proces s risk identification is presented in Section 3 followed by a description of eight PRIs. We then show how to configure these PRIs using information about the o utcomes of cases in the past. Section 4 discusses our implementation in ProM and reports on our experimental results. Section 5 concludes the paper. Few approaches exist that aim to identify and/or assess process risks [7,8,21]. Wickboldt et al. proposed a framework that uses process execution data for risk assessment [21]. Risk assessment modules of the framework use information about risk events reported during past activity executions. Our approach also predicts future risks based on past behaviours, but it does not require risk-related information to be explicitly stored. Jallow et al. [7] proposed a framework for identification of operational process risks. However, estimation of the probabili-ties and impacts associated with risk-related activities was assumed to be done by experts. Our approach avoids subjective opinions and learns such values from historic event data. Jans et al. [8] proposed using process mining for the identi-fication of one particular type of risk (transactional fraud risk) and showed that available process mining tools can help a uditors detect fraud. By contrast, our approach focuses on quantifiable values such as delays or product quality and it emphasises automatable techniques for risk identification that can be used for run-time operational support [16].

Van Dongen et al. proposed an approach for predicting the remaining cycle time of a case by applying non-parametric regression and using case data as predictor variables [20]. The approach for predicting remaining process time proposed by van der Aalst et al. [18] is based on building an annotated transition system and estimating the average remaining time of cases that visited the same state previously. In contrast, our approach predicts the likelihood of case delay rather than the remaining execution time.

Grigori et al. presented a set of integra ted tools that help manage process ex-ecution quality supporting such features as analysis and prediction [3]. In other work they propose a method for exception analysis, prediction, and preven-tion [4]. A common feature of these approaches is that it is the responsibility of users to specify what process properties ( conditions, exceptions etc.) they would like to analyse. Our approach does not require such input and is based on a set of risk indicators.

In our own earlier work we introduced th e idea of using Process Risk Indicators for predicting case delays and proposed a method for instantiation of the indica-tors from event logs [14]. The method is based on statistical techniques for outlier detection. It used a simple analysis which assumed that process behaviours have normal distributions with fixed thresholds being sufficient to identify  X  X isky X  behaviours. Our initial experiments reve aled that risk indicators can be used to predict case delays [14], but further work is required to properly calibrate the indicators to reduce the number of false positives. In this paper we present a method for configuration of risk indicators for process delays that significantly improves precision of case delays predictions. 3.1 Approach Our goal is to develop a method that can identify the risk of delay for running cases with a high degree of precision. Our method analyses characteristics of a current case, compares them with chara cteristics of similar cases executed in the past and predicts a case delay if a  X  X isky X  behaviour is detected. Our overall approach consists of three major steps: (1) define Process Risk Indicators; (2) configure PRIs; (3) identify the presence of PRI instances in a current case.
First, we need to identify which behaviour of a process can be considered  X  X isky X . In our initial work we introduced the use of Process Risk Indicators (PRIs) for predicting case delays. We defined a PRI as  X  X  pattern observable in an event log whose presence indicates a higher likelihood of some process-related risk X  [14]. For example, an unusually large number of activity repetitions per case may indicate a likely case delay or low-quality output because there seems to be a problem processing this case.

In our preliminary work we also introduced a method for identifying the pres-ence of a PRI based on the  X  X ample standard deviations X  approach for outlier detection [14]. For each PRI we defined cut-off thresholds as x 2 s .Observations whose values are higher than this value were considered outliers. A limitation of the method is the assumption that some particular process behaviour follows a normal distribution (e.g., activity repetitions in a case) which may not be valid in many cases. We also assumed that atypical behaviour of a process can be considered  X  X isky X , e.g. when some activity in a case has an atypically long du-ration it signals a higher likelihood of the case delay. However, while conducting initial experiments we learned that though atypical behaviour is often associated with case delays it is not always  X  X isky X . For example, if a process contains an automated activity which typically takes a very small amount of time compared to the total time that cases take, then variations to the execution time of such an activity, even relatively large ones, do not affect the case duration.
To overcome these weaknesses of our in itial work we present here a method for configuration of indicators so that the specific characteristics of a particular process are taken into account. We again use cut-off thresholds to identify  X  X isky X  behaviours, however we introduce a way of learning the threshold values by using information about outcomes of cases in the past. The method allows us to identify atypical process behaviour that has been often associated with case delays in the past rather than assuming any outlier indicates a risk. 3.2 Process Risk Indicators (PRIs) A PRI is a pattern that signals an increased likelihood of some process-related risk and which can be identified by analysing an event log. In our previous work [14] we introduced the idea of using Process Risk Indicators to identify the risk of case delay. For the purpose of this paper we use several indicators that can be discovered using basic event logs, information about case outcomes and process models, all of which were available to us in our industrial case study. Below we define eight Process Risk Indicators for process delays.
 PRI 1: Atypical activity execution time. The duration of an activity sig-nificantly exceeds its typical duratio n. An activity may take more time than usual due to human factors: an employee executing the activity may be inex-perienced or occupied with many other tasks. Fatigue is a common factor that may cause a delay. Another reason can b e a complex or exceptional case that requires additional investigation/learning. Activity delay is also often caused by a third party X  X  involvement X  X educing the number of contacts with third parties is one of Business Process Re-engi neering X  X  best practices [11].
 PRI 2: Atypical waiting time. An activity has not been started for an atypically long period of time. One possible explanation for long waiting times is a lack of available resources. Another possible reason is the  X  X oo hard basket X  syndrome, i.e., the situation where no one is willing to start an activity as it is perceived to be too challenging or time consuming. Also, some employees tend to process certain tasks in batches, which may increase a particular task X  X  waiting time. A typical example is an approval task. Removing batch-processing is another of the BPR best practices [11] , as is reducing waiting times because these often occupy 95% of the throughput time of a case [9].
 PRI 3: Multiple activity repetitions. The number of times an activity is repeated in a case significantly exceeds its usual value. It may be necessary to repeat an activity if previous attempts fail. This can happen due to third party involvement, e.g., not receiving an expected service from subcontractors or failure to provide required information by a client. Employees may also need to repeat a task because of inexperie nce or complex case requirements. PRI 4: Presence of a  X  X isky X  activity. A case contains a  X  X isky X  activity. An activity is considered  X  X isky X  if the majority of the cases that contained this activity in the past have been delayed. Execution of a  X  X isky X  activity may be related to a case X  X  specifics. For example, consultation with an expert or a manager may be required for an exceptionally complex case.
 PRI 5: Multiple resource involvement. More resources are involved in a case than usually. One possible reason for such a situation is the so-called  X  X ot potato X  phenomenon where a case is forwarded between different resources be-cause nobody is willing to take charge of it. Atypically high resource involvement can also be needed for a very complex case. Reducing the number of parties in-volved in a case is another of the BPR best practices [11]. Balasubramanian et al. name frequent hand-overs of work between people in a process as one of the factors that can lead to time overruns [2].
 PRI 6: Atypical sub-process duration. The sum of activity duration and its waiting time in a case (referred to here a s a sub-process) is significantly higher than its typical value. We introduce this indicator to be able to work with event logs that only record  X  X omplete X  events fo r activities, as is often the case for real event logs. This indicator tackles the same issues as PRIs 1 and 2.
 PRI 7: High resource workload. An activity has been assigned to or started by a resource with a high workload. The workload of a resource at a point in time is the number of items that were started by or assigned to the resource but not yet completed. High resource workl oad is often mentioned in the literature as a reason for such risks as time overruns or low-quality outputs [5,13]. PRI 8: Use of a  X  X isky X  resource. An activity has been assigned to or started by a X  X isky X  resource. A  X  X isky X  resource for some activity is the one that was often involved in execution of this activity in delayed cases. Some human resources may be incompetent or inexperienced when it comes to the execution of some activities in a process. It is import ant to use recent data for identification of this PRI as the qualification levels and experience of resources will change over time. Another reason for a resource to be considered risky is a tendency to postpone execution of certain acti vities, e.g., approval tasks. 3.3 Configuring Process Risk Indicators Our method for configuration of indicators requires information about known outcomes from cases that happened in the past, i.e., whether they were delayed or completed in time. We aim to find for the PRIs the values of parameters that could predict delays with a required deg ree of precision in the past. If we cannot detect such values for an indicator then it is not used for a particular process.
An input parameter to our method is a desired precision level. Precision is the fraction of cases predicted to be delayed that are actually delayed. Increasing precision is usually done at the expense of decreasing recall, which is defined as the fraction of delayed cases that can be successfully predicted against the actually delayed cases. If a user deals with a critical process, he may prefer monitoring alerts with lower precision levels in order to increase recall, while for a non-critical process he may want to check only those alerts that indicate a very high likelihood of a case delay.

For each relevant process behaviour (e.g., the number of activity repetitions in a case) we look for the smallest value that allows distinguishing between delayed and in time cases with a required degree of precision. This value is used as a cut-off threshold. In order to define this threshold we need to check the effectiveness of various ca ndidate values. However, there could be a wide range of these. Analysing past logs can be time consuming, so in order to reduce the search space we learn cut-off thresholds for the PRIs by checking only those values from a pool of selected candidates. We use the following heuristic to define candidate values. First, we discard those values lower than the mean x (which gives us a measure of central tendency). We then include those values calculated as x n s ,where s is the standard deviation (as a measure of statistical dispersion), and n is in the range of 0 to 10 with an increment of 0.25 (these values were used for the experiments, they are input parameters). We do not necessarily assume a normal distribution. Nevertheless, these conventional statistical measures provide a natural starting point for searching for thresholds. We then check all values from the defined pool of candidates.

We are interested in indicators that ca n predict delays during a case X  X  exe-cution. Therefore, while learning parameters of PRIs from past execution data, our method considers only those events that happened before a deadline, i.e., we discard activities that have been star ted after the deadline has been missed.
As an example of the calculation, consider PRI 5  X  Multiple resource involve-ment X . PRI 5 is a case-based PRI, i.e., it can have only one value per case and we define one cut-off threshold. In order to identify and use PRI 5 the following steps are performed: 1. Define candidate values T for the cut-off threshold t : 2. Define the cut-off threshold t : 3. Check the number of resources involved in the current case and alert a like-For activity-based PRIs such as PRI 1 ( X  X typical activity execution time X ), PRI 2 ( X  X typical waiting time X ), PRI 3 ( X  X ultiple activity repetitions X ) and PRI 6 ( X  X typical sub-process duration X ) a similar procedure is repeated for each activity to learn proper thresholds. A case can have multiple instances of an activity-based PRI, e.g., several activities may be delayed or repeated. We consider that there is a chance of a case delay if the case contains at least one instance of an activity-based PRI. For resource-based PRI 7  X  X igh resource workload X  we learn appropriate values for cut-off thresholds for each resource. If in a current case an activity is assigned to or started by a resource with a high workload (defined by the learned threshold), a case delay is more likely.
PRIs 4 and 8 do not follow the general procedure described above. These are examples of indicators that can only be identified using information about the out-comesofcasesinthepast.ToidentifyPRI4 X  X resenceofariskyactivity X  X echeckif there exists an activity that is executed mainly in delayed cases. For PRI 8 we check for each pair  X  X ctivity-resource X  if some resource X  X  involvement in the execution of an activity mainly occurs in delayed cases. Then we check if a current case contains a  X  X isky X  activity or if an activity is assigned to a  X  X isky X  resource. Identification of such behaviour signals increased likelihood of case delay. 4.1 Experimental Setup To estimate the quality of case delay predictions by our method we use hold-out cross-validation [10]. This is a commonly used statistical practice that implies partitioning of data into two subsets, where one subset is used for initial learn-ing (a training set), and the results are validated using the other subset (a test set). To facilitate validation of our approach we have implemented a plug-in of the process mining framework ProM 6 2 . The plug-in takes as an input two event logs. It uses one log as a training set to configure the PRIs, then it anal-yses cases in the other log (a test set) to identify occurrences of these PRIs. An input parameter is the expected case duration. Cases that take longer than this value are considered to be delayed. If any of the indicators is found in a case it is predicted to be delayed. We c ompare predicted case delays with the actual case durations and evaluate the performance of the process risk identifica-tion method by estimating the values of  X  X recision X  and  X  X ecall X . These metrics are often used in different machine learn ing areas to estimate performance of prediction techniques. Precision is calc ulated as the fractio n of cases correctly predicted to be delayed against the total number of cases predicted to be de-layed. Recall is calculated as the fract ion of delayed cases that are successfully predicted against the number of cases that are actually delayed. These values are calculated separately for each indicator to evaluate their individual performance. We also calculate the values of precision and recall for all indicators combined to evaluate their cumulative performance.

We used two different approaches to splitting data into a training set and a test set. In one approach, we split event logs randomly, such that 75% of cases were put into a training set and 25% of cases in a test set (referred to later as a  X  X andom X  split). In the other approach, cases that were completed during one period of time (four months) were put into a training set while cases that were started within the next period (two months) were put into the test set (referred to later as a  X  X ime X  split). As our approach is based on learning from past execution data it is important to use large data sets for training, therefore we decided to put more data in the training set while still having enough data in the test set for meaningful validation.

Before applying our method for risk identification it is important to perform data pre-processing. Processes tend to e volve over time. To avoid learning from outdated information recen t data should be used. For our experiments we picked cases that were active over the same period of six months. The algorithm should use only completed cases to properly co nfigure PRIs, therefore partial traces representing running process instances should be filtered out. The results of any process mining algorithm depend on input data, therefore the quality of event log data is crucial [1]. For example, if event l og data contains mislabelled activities, the performance of the algorithm may be affected, therefore it is important to clean event log first (e.g., filtering out mislabelled events). It is also important to separately analyse cases that are execute d in different contexts that affect their durations. For example, the expected case duration may depend on the type of customer ( X  X old X  versus  X  X ilver X ) or type of service ( X  X remium X  versus  X  X or-mal X ). If such execution co ntexts are known, event log data should be first split and cases that are executed in different contexts should be analysed separately. 4.2 Data Properties and Preprocessing We evaluated our approach using two data sets from Suncorp , a large Australian insurance company. Both data sets repre sent insurance claim processes from different organisational units, referred to here as data set A and data set B. Both event logs provided by Suncorp contained only completed cases. Data set B contains cases from five departments and was split into five sets (referred to here as B1 X  X 5) which were used in separate experiments. Each data set (A, B1 X  X 5) was split into a training set and a test set. The training set was used by the algorithm for learning the cut-off th resholds. Cases in the test set were used for evaluating the performance of the PRIs.

We first cleaned up the data sets by filtering out cases with activities that appear only once in the whole log. In most cases, such activities were not really unique though their label was. Typically this was a consequence of combining an activity X  X  name with the name of the employee who executed that activity. We used original unfiltered data sets to more accurately estimate resource workloads (required for PRI 7).

To more accurately estimate waiting t imes (for PRIs 2 and 6) we used process models. We first identified the pre-set of an activity, i.e. the set of activities that can directly precede a given activity. We th en calculated the waiting time for the activity as the difference between its  X  X tart X  time and the  X  X omplete X  time of the last activity from its pre-set preced ing it in the case. Since we did not have process models, we instead used process mining to discover them from the event logs. First we filtered the logs so that they contained only cases representing mainstream process behaviour and used these filtered logs to discover process models represented by Petri nets with one of the ProM process mining plug-ins [19]. For data set A we used 95% of the cases representing the most frequent process variants. Data sets B1 X  X 5 proved to have a large variety of process variants. For these data sets only those cases were used for process discovery that share the same process variant with at least four other cases. These filtered logs were only used for process discovery and not in the experiments.
Suncorp X  X  business analysts provided us with indications about what they feel should be the usual case durations for d ifferent departments. However, while analysing the event logs we realized that these expectations are not realistic as more than 50% of cases have durations higher than expected in four out of six data sets. For these data sets we therefore learned the values for typical case durations such that at least 50% of cases in a set are completed in time. These values were used in the experiments. Figure 1 shows as an example the distribu-tion of case throughput times for data set B4. Only cases highlighted in blue are completed in time if we consider the value provided by the company X  X  business analysts to be the typical case duration. It is very likely that the behaviour of a process is different when an explicit due date exists and is communicated to workers. However, this should not aff ect the performance of our method since process behaviour is still consistent across training and test data sets.
Data set A has some nice properties which make it suitable for our experi-ments: a significant number of cases, steady case arrival rates and similar case duration distributions over time (low variability). Figure 2 shows some basic properties of data set A.
 For data sets B1 X  X 5 additional filtering was required. We were informed by Suncorp that cases with claim amounts higher than a certain value are considered  X  X omplex X  and that it is normal for them to have long durations. We filtered the event logs for our experiments and only used  X  X imple X  cases that are expected to be completed within a certain time peri od. We found a large number of process variants in these sets. High variability of the processes can be explained by the fact that process models used by Suncorp are not prescriptive and are only used as guidance. High process variability may decrease precision of delay predictions for two PRIs that use information about the order of activities (PRI 2  X  X typical waiting time X  and PRI 6  X  X typical sub-process duration X ). The performance of other PRIs is not expected to be affect ed since they do not rely on the order of activity executions. Also case arrival rates, case durations, and mean active and waiting times were found to change over time. All these characteristics of the process may have influenced the results of the experiments. Figure 3 depicts basic characteristics of these five data sets.
 4.3 Performance of the PRIs We first conducted our experiments with data set A. Figure 4 depicts the results of the experiments conducted with event log A using a random split and Figure 5 depicts results of the experiments using a time split. An input parameter for the algorithm is the  X  X esired precision level X . When we learn a cut-off threshold for an indicator we pick the minimum value of the threshold that allowed predicting case delays in a training set with a given precision level. We conducted exper-iments for three precision levels: 95%, 90% and 80%. The columns represent results for individual PRIs. The last column represents the cumulative result for all indicators: a case is predicted to be delayed if any of the indicators is found in the case. For a desired precision level the first two rows represent the number of True Positives ( TP ) and the number of False Positives ( FP ) produced. These predictions are produced before expiry of the deadline. The next two rows are the number of False Negatives ( FN ) and the number of True Negatives ( TN ). TP FP is the number of cases predicted to be delayed. The precision is calcu-lated as the fraction TP TP FP . TP FN is the number of cases actually delayed and can be used to compute the recall which is the fraction of delayed cases that are successfully predicted and the actually delayed cases, i.e., TP TP FN . Figures 4 and 5 show both precision and recall values for the test sets.

The results of the experiments for the two different types of event log split were comparable in terms of the indicators X  per formance. Most predictions in both cases came from PRIs 1, 2 and 6. Some delays were indicated by PRIs 4 and 8. Poorly performing indicators for this data set were PRIs 3, 5 and 7. In the vast majority of cases it was only possible to identify PRIs 3 ( X  X ultiple activity repetitions X ) and 5 ( X  X ultiple resource involvement X ) after the deadline was missed. One of the reasons for the poor performance of PRI 7 ( X  X igh resource workload X ) for this log may be the fact that we do not have all data for the process (incomplete cases were filtered out). We also assumed that resources are involved full-time in this one par-ticular process which may not be true. Figures 4and 5 also demonstrate the number of delays that can be predicted with these i ndicators for different precision levels. In the  X  X andom X  split experiment it can be observed that lowering the desired pre-cision level leads to a decrease in precisi on and an increase in recall. While this can also be observed in the  X  X ime X  split expe riment the decrease of precision is more pronounced while the increase in recall is less.

We have also applied to data set A the risk identification algorithm without con-figuring the PRIs using a  X  X andom X  75/25% split. The results are depicted in Fig-ure 6. For PRIs 1, 2, 3, 5, 6 and 7 the cut-off thresholds were defined as x 2 s , i.e., we assume normal distributions and use a 95% confidence interval. We did not use PRIs 4 and 8 in this experiment as they can only be learned using information about the outcomes of past cases. Precision levels for all indicators were signifi-cantly lower than the corresponding values from our previous experiment where we configured the PRIs (depicted in Figure 4). This confirms that proper configu-ration of indicators is an essential step in the risk identification method.
Then we conducted the experiments with data sets B1 X  X 5. Figure 7 depicts the results of the experiments for five departments in data sets B1-B5. We have used a random 75/25% split and 90% as the value for the desired precision level. PRIs 1, 2, 6 and 8 demonstrated a good performance for all departments, and a few delays were predicted with PRIs 3, 5 and 7. PRI 4 ( X  X resence of a risky activity X ) did not predict any delays for these data sets because no single activity was a strong predictor of delays in these logs. 4.4 Moment of Delay Prediction We also evaluated the ability to predict delays early during a case X  X  execution which is obviously a highly desirable capability. In order to do so we checked how many true positive and false positive predictions (coming from any of the indicators) were generated before a given point in time during a case X  X  execution, to find the earliest point when we can identify risks. Since the event logs available to us do not have  X  X ssign X  events recorded, we consider the time of the  X  X tart X  event for an activity to be the discovery time for PRIs 3, 4, 5, 7 and 8, e.g., when an activity has been started by a  X  X isky X  resource (PRI 8), or by a resource with a high workload (PRI 7). The earliest time when we can observe PRI 1 ( X  X typical activity duration X ) is the time of the  X  X tart X  event of an activity plus the value of PRI 1 X  X  threshold for this activity. For example, if an activity is not completed within three days (the threshold value) after it has been started there is a higher likelihood of the case delay, i.e., at this point we can already predict delay. The earliest time when PRI 2 ( X  X typical waiting time X ) can be observed is either the time of the  X  X omplete X  event of an activity plus the maximum of its successors X  PRI 2 thresholds or the time of the  X  X tart X  event of the next activity if it has been started earlier and its wait duration is higher than its PRI 2 threshold. For example, if an activity is completed and none of its successors have been started within two days (maximum of their PRI 2 thresholds), we can say at this point that a case delay is likely due to PRI 2. A similar approach for calculating the discovery time is used for PRI 6.

Figure 8(a) depicts the discovery tim es for data set A. Recall that the dis-covery time is the time at which a true positive or false positive predictions are generated. Figure 8(b) presents the discovery times for data set B5. The horizon-tal axes in both diagrams represent the number of days since the beginning of a case when the risk of the case delay was discovered. Cases from data set A should be completed within 14 days while the typical case duration for data set B5 is 120 days. The vertical axes depict the cumulative number of delay predictions at a certain point in time. For example, Figure 8(a) shows that more than 1000 correct delay predictions have been generated within the first twelve days. For data set A early predictions (below seven days) are coming mainly from PRI 4 ( X  X resence of a risky activity X ) and PRI 8 ( X  X se of a risky resource X ). Early predictions for data set B5 (below 30 days) were generated mainly by PRI 8 ( X  X se of a risky resource X ) and PRI 7 ( X  X igh resource workload X ). 4.5 Discussion Some of the limitations of the experiments described above are related to the data available to us. One of the two data sets provided by Suncorp displayed high process variability. Multiple process variants may have influenced the per-formance of PRIs that rely on the order of activities (PRIs 2 and 6), however the performance of other indicators sh ould not be affected. The other concern is related to estimating the performance of PRI 7  X  X igh resource workload X . This is due to two reasons. The first one is that the event logs available to us contained only completed cases, i.e., traces corresponding to running process instances were filtered out. We also assumed that all resources are involved in one process. Hence, the workload of resources may have been underestimated. In order to more accurately estimate th e performance of this PRI complete information about all processes in a company is required. This limitation should not affect the performance of other indicators.

A limitation of the approach is our assumption that a process is in a steady state, i.e. it is not changing over time. To deal with this limitation in this paper we used data from a relatively short period (six months). However, if a process X  X  behaviour is constantly changing, the amount of available up-to-date data may be insufficient for proper configuration of PRIs.

We considered instance and process contexts, however we did not consider social and external contexts using the terminology of [17], that may also influence case durations. This is a direction for possible future research. Another direction for future work is to investigate the relation between PRIs and the extent of the expected delay. In this paper, we presented a method for configuration of risk indicators for process delays. The method learns param eters of indicators by analysing event logs and exploiting information about the outcomes of cases completed in the past. Such configuration of indicators takes the specifics of a particular process into account thus improving the accuracy of the predictions. We conducted a number of experiments with different d ata sets from an Australian insurance company that confirmed that this approach decreases the level of false positive alerts and thus significantly improves the precision of case delay predictions.
The experiments demonstrated the ability to predict case delays with eight selected PRIs. Some of the indicators sh owed a consistently good performance in all data sets (e.g., PRIs 1, 2 and 6), others are good predictors of delays for some processes but did not predict delays for others (e.g., PRIs 4, 7 and 8). PRIs 3 and 5 produced few predictions for this particular data set due to the fact that it was typically possible to discover these indicators after the deadline was missed. As is often the case in the data retrieval field, there is a trade-off between precision and recall. It is hard to predict more than 50% of case delays with a high degree of precision using our indicators, while many delays can be predicted with a degree of precision of 80%. We expect that our approach can be applied for configuration of indicators for other types of process risks such as cost overruns or low-quality outputs, but this should be explored in future work. Acknowledgements. This research is funded by the ARC Discovery Project  X  X isk-aware Business Process Management X  (DP110100091). We would like to thank Suncorp for providing the data sets for analysis.
