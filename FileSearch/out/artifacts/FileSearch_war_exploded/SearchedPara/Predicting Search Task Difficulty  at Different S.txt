 Knowing, in real time, whether a current searcher in an information retrieval system finds the search task difficult can be valuable for tailoring the system's support for that searcher. This study investigated searcher's beha viors at different stages of the search process; they are: 1) first-round point at the beginning of the search, right before searchers issued their second query; 2) middle point, when searchers proceeded to the middle of the search process, and 3) end point, when searchers finished the whole task. We compared how the behavioral features calculated at these three points were different between difficult and easy search tasks, and identified behavioral features during search sessions that can be used in real -time to predict perceived task difficulty. In addition, we compared the prediction performance at different stages of search proce ss. Our results show that a number of user behavioral measures at all three points differed between easy and difficult tasks. Query interval time, dwell time on viewed documents, and number of viewed documents per query were important predictors of task difficulty. The results also indicate that it is possible to make relatively accurate prediction of task difficulty at the first query round of a search. Our findings can help search systems predict task difficulty which is necessary in personalizing support for the individual searcher. Categories and Subject Descriptors H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  relevance feedback, search process General Terms Performance, Measurement, Experimentation, Human Factors. Keywords Search task difficulty; search task difficulty prediction; user searching behavior ; personalization Search engines do a decent job with simple search tasks, but are less successful for complex and/or difficult tasks. Being able to infer and predict, in real-time, when searchers are having difficulty through observation of us ers X  interaction behaviors in search sessions can help the system determine if it is necessary to intervene and assist users. Accordingly, this could prevent user frustration and/or system switch. Researchers have examined search difficulty in terms of users X  behavioral differences in easy and difficult tasks (e.g., [4], [10], [14], [22]), and have also attempted to build task difficulty predictive models (e.g., [3], [21], [22]). Most of these approaches have not addressed the problem of building real-time predictive models. To our knowledge, the onl y attempts to use real-time behaviors to build difficulty predictive models have been [22] and [3]. Liu et al. [22] generated pr edictive models of task difficulty based on behavioral measures at th ree levels: the first-round level, the accumulated level, and the w hole-session level, and found that the predictive model at the accumulated level could achieve comparable prediction performance with the whole-session level model. A limitation of [22] is that even though the behavioral measures on the accumulated level could be calculated after each query, that study only examined these behaviors at the end of the search session. Arguello [3] evaluated behavioral features for predicting task difficulty at two points in the search session: in the first-round and whole-session, and found different behavioral features were selected in the model at different points. Our work is similar to [3] and [22], but we examine at what point during the search session the predictive models can achieve the best prediction performance. Specifica lly, we employ a tree-structure modeling method, recursive partitioning, to generate predictive models of task difficulty at three different search stages: at the beginning of search, in the middle of search, and at the end of the search. We had four research questions in this study: 1) What are the differences in behavioral variables between 2) What behavioral variables are important predictors of task 3) Which search stage could achieve the best prediction 4) Are there any differences in prediction performance at To explore these questions, a la boratory user experiment was conducted to collect client-side us er-system interaction log data, and user ratings of task difficulty elicited after they finished the tasks. Several highlights of our study are: first, we made an extensive examination of user search behaviors between easy and difficult tasks, at three search stages: the first-round (early in a search, from the beginning of search until issuing the second query), middle-point (calculated in the middle of the search process), and end-point (calculated when the search task is finished) (cf., [22]). Second, a recursive partitioning method was used to construct a number of tree models that can predict task difficulty in real-time. The predictive models showed that using only a limited number of signifi cant behavioral measures could yield quite good prediction pe rformance. Third, our results demonstrate that users X  search be haviors at the very beginning had the best prediction accuracy of task difficulty. These all shed light on personalization of in formation retrieval. 2. RELATED WORK In this section we review prior work on search task difficulty, and its relationship with user search behaviors, and the use of behavioral measures to generate predictive models of task difficulty. 2.1 Search task difficulty and user behaviors Recent years have seen an increasing research interest in search task difficulty. Li &amp; Belkin [18] noted that task difficulty could only be subjectively establishe d, as assessed by task doers. Kim [14] suggests that difficulty is the task doer X  X  perception of task complexity, which could be both pr e-and post-task perceptions. A recent study by Liu, Kim &amp; Creel [24] analyzed specific reasons why information searchers found some search tasks difficult. They developed a coding scheme for these reasons and demonstrated that task features (e.g. time limitation, complexity), user aspect (e.g. low knowledge, little interest), and user-task interaction (e.g. system features, document features) could all lead to users X  feeling of task difficulty. Previous studies have defined task difficulty from different aspects. Aula, Khan, &amp; Guan [4] defined task difficulty by users X  success or failure in finding the an swers to their tasks, which were closed information tasks that ha ve a single, unambiguous answer. Through a lab experiment with 23 participants and a large-scale study with 179 participants, they found that in difficult tasks, users started by formulating more diverse queries, used advanced operators more, and spent longer time on the SERPs (search engine result pages). Some other studies highlight the importance of task type in the relationship between task difficulty and search behaviors. Kim [14] examined the effect of task difficulty on user behaviors in three types of tasks: factual, in terpretive, and exploratory, and found that in factual tasks, post-task difficulty was significantly associated with task completion time and the numbers of queries and documents viewed; in explorat ory tasks, user behaviors were significantly correlated with pre-task difficulty; and in interpretive tasks, there were rarely significa nt correlations between behaviors and task difficulty. Liu, Liu, Gwizdka &amp; Belkin [23] examined user behaviors in tasks with differ ent difficulty levels as well as of different types: single-item closed tasks, multiple-item closed tasks, and open-ended tasks. They found that in difficult tasks, users had longer task completi on time, issued more queries, viewed more content pages, and had longer dwell time on content pages. Using the medical doma in TREC genomic track task topics, in a laboratory setting, Liu et al. [22] found that in difficult tasks, users had, inter alia , longer dwell time on SERPs, fewer document saved, and more queries issued. 2.2 Task difficulty prediction Researchers in the IR field have looked at query performance and difficulty prediction from the langua ge model perspective (e.g., [7], [8], [9]). As defined above, the current paper focuses on search task difficulty from th e whole task level, which may include multiple queries to accomplish the search task. Along this line, Gwizdka &amp; Spence [10] examined how users X  behaviors could indicate the difficulty of a factual information-seeking task. Task difficulty was self-assessed by users after each task. Their results indicated that higher s earch effort, lower navigational speed, and lower search efficiency were good predictors of task difficulty tested by regression models. One limitation of their study is that the predictive factors are not easily captured in real-time, and variables such as search efficiency can be obtained only after the search session is complete. Liu, Gwizdka, Liu &amp; Belkin [21] attempted to do real-time prediction of task difficulty from user behaviors. In their lab study, the tasks were categorized as easy and difficult based on users X  post-task judgment of task s X  difficulty levels. Users X  behaviors were grouped at the w hole-task-session level and the within-task-session level. Their study found that both whole-session level and within-session level user behaviors can serve as task difficulty predictors in Logistic Regression models. Whole-session level variables showed higher prediction accuracy, but do not enable real-time prediction. On the other hand, while within-session level factors can ensure real-time prediction, prediction accuracy in general was mediocre, especially in some types of tasks, possibly because of the limited number of within-session factors that were considered and used in their model. Previous research has demonstrated that the behavioral measures captured and calculated at the end of search (the so called whole-session level behaviors) had higher prediction accuracy; however they were not helpful for real-tim e prediction. Recent studies ([22] and [3]) explored predicting task difficulty using the first-round level of behavioral measures, and compared such models with the measures collected at the end of search. The results of [22] demonstrated that the predictive model incorporating first-round and accumulated levels of beha viors had fairly good prediction performance, and was comparable with the whole-session level model. Arguello's [3] study show ed that whole-session prediction was more effective than first -round prediction, and different behavioral features were selected by the predictive models at the different stages. Hassan, et al. [13] analyzed struggling and exploring behaviors in Web search sessions that consist of many queries, and built classifiers to distinguish exploring and struggling sessions using behavioral and topical features. The predictive model was based on beha vioral measures calculated at the end of search sessions, and they also compared such prediction with predictions at three points in the session (i.e. after 1 /2 nd /3 rd queries). Their results showed that the best performance was achieved when the classifier has access to the entire session. Our current work extends prior wo rk in three directions: (1) we explicitly investigate predicting task difficulty at three search stages in the search session: the first-round, the middle point, and the end-point; (2) we employ a tree model generation method, recursive partitioning to genera te predictive models of task difficulty; (3) we examine whether task features also affect the effectiveness of predictive models of task difficulty at different search stages. 2.3 Search stage in information retrieval Search stage has been found to be a significant factor affecting search behaviors in previous studies. Kulthau [17] proposed the ISP (Information Seeking Process) model, in which of the information search process has six stages: initiation, selection, exploration, formulation, collecti on, and presentation. The user X  X  feelings, thoughts, and actions va ry along the different stages. Vakkari and colleagues (e.g.,[27] [ 28]) found that as the task stage progressed, the users' vocabul aries changed from broader to narrower terms, the users were less likely to start their initial queries by introducing all the search terms, were more likely to enter only a fraction of the terms, and tended to use more synonyms and parallel terms. To mbros, Ruthven, &amp; Jose [26] identified the stages in the users X  task progress by identifying the first and last sets of Web docume nts that the users visited. Their study found that the users X  criteria of assessing the relevance of Web pages for information seeking tasks through the duration of a task displayed a certain degree of variation, especially for tasks for which the users had a higher perception of task completion. White, Ruthven &amp; Jose [31] studied how the use and effectiveness of implicit relevance feedback (IRF) and explicit relevance feedback (ERF) is affected by three factors: search task complexity, the search experience of the user and the stage in the search. With respect to the search stage, their results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end. Liu &amp; Belkin [20] explored whether dwell time can be a reliable indicator of document usefulness in multi-session tasks. They found that task stage helped interpret document usefulness from the first dwell time, i.e., the first duration that a document was viewed. 2.4 User modeling methods in IR In addition to task difficulty prediction, researchers in the IR community have modeled users ba sed on their search behaviors with respect to other aspects, including modeling of search interests ([2],[11],[14],[19],[30]), search success ([1]), frustration ([10]), satisfaction ([13]), and s earch engine switching behaviors ([29]). Different types of mode ling techniques have been used. The modeling methods most ofte n employed include logistic regression (e.g. [10], [22], [29]), machine-learning methods including Bayesian modeling, k-nearest neighbor, Support vector machines (SVM), and Classification and Regression Trees (CART) (e.g. [1], [10], [16]). Some research has compared several methods to see which had the best prediction. For example, [32] considered several modeling met hods to predict the estimated citation of a particular publicat ion after a given time period and found that recursive partitioning achieved the best prediction performance. Recently, recursive partitioning has attracted increasing attention in generating predictive models in IR (e.g. [19]). In our study, we employ th is method to generate predictive models of task difficulty. Recursive partitioning is a tree-str uctured classification technique [6], which is a nonparametric type of analysis that repeatedly subdivides data into smaller and smaller subgroups based on characteristics that predict the value of a target variable. Recursive partitioning, unlike logistic regression, is nonlinear in its parameters, and has an advantage if the true relationship between the variables and the outcome of interest is nonlinear. The model generation process using the recursive partitioning method repeatedly asks two questions at each step: 1) which independent variable (behavioral measures in our study) should be selected to obtain maximum reduction in the he terogeneity of the dependent variable (task difficulty level in our study); 2) what is the best cutoff value of the selected inde pendent variable? These two steps are repeated until splitting no longer adds value to the predictions. We decided to use recursive partitioning because of its several attractive technical pr operties: nonparametric ; stepwise method to select variables; can handle datasets with complex structure and is not affected by multi-collinearity issues. 3. USER EXPERIMENT 3.1 Tasks 3.1.1 Task Design In our study, we chose the jour nalism domain to design search tasks for reasons of both validi ty and convenience. Although journalism can be associated with any topic, it has a relatively small number of work task types. Th is means that we were able to have a range of topics for our tasks, while maintaining a good measure of control over realistic tasks, thus enhancing validity. We also had ready access to a un iversity journalism department, which meant both that we had experts to help us define the work tasks, and access to participants trained for such professional journa lism tasks. Task descrip tions were formalized from interviews with experts. We then constructed a set of four work/search tasks which varied on several of the task classifica tion facets proposed by [18], which attempts to identify and integrate the various facets of task in a single scheme. The facets that we manipulated included Product, Goal (quality) and Objective Complexity, because these are the facets that we believed could affect users X  search behaviors. We added one facet,  X  X evel X , which was a significant aspect of tasks in the work environment we studied, to the facets if the scheme of [19]. It is important to note that the goal of this study is not to test the performance on these four tasks, but rather to investigate the combinations, and behaviors and document usefulness prediction. These task facets are general to many search tasks and topics; thus controlling task facets can increas e the generality of our study. We acknowledge that it would be im portant in the future to test whether the same facets, with di fferent search tasks and topics, would lead to the same behaviors and predictions in other domains. The four work tasks and associated search tasks that we identified are presented below. The descriptions of these tasks follow the simulated task environment proposed by Borlund [5], and are couched in journalism terms: jour nalists are typically given an assignment, and an associated task to complete. Background Information Collection (BIC): Your assignment: You are a journalist at the New Yo rk Times, working with several others on a story about  X  X hether a nd how changes in US visa laws after 9/11 have reduced enrollment of international students at universities in the US X . You ar e supposed to gather background information on the topic, specific ally, to find what has already been written on this topic. Your Task: Please find and save all the stories and related materials that have alread y been published in the last two years in the New York Times on this topic, and also in five other important news papers, either US or foreign. Copy Editing (CPE): Your assignment: You are a copy editor at a newspaper and you have only 20 minutes to check the accuracy of the three underlined statements in the excerpt of a piece of news story below. Title: New South Korean Presid ent Lee Myung-bak takes office Lee Myung-bak is the 10th man to serve as South Korea X  X  president and the first to come from a business background. He won a landslide victory in last December X  X  election. He pledged to make economy his top priority during the campaign. Lee promised to achieve 7% annu al economic growth, double the country X  X  per capita income to US$4,000 over a decade and lift the country to one of the topic se ven economies in the world. Lee, 66, also called for a stronger al liance with top ally Washington and implored North Korea to forg o its nuclear ambitions and open up to the outside world, promising a better future for the impoverished nation. Lee said he would launch massive investment and aid projects in the North to increase its per capita income to US$3,000 within a decade  X  X nce North Korea abandons its nuclear program and chooses the path to openness. X  Your Task: Please find and save an authoritative page that either confirms or disconfirms each statement. Interview Preparation (INT): Your assignment: Your assignment editor asks you to write a news story about  X  X hether state budget cuts in a state in US are affecting financial aid for college and university students. Your Task: Please find the names of two people with appropriat e expertise that you are going to interview for this story and save just the pages or sources that describe their expertise and how to contact them. Advance Obituary (OBI): Your assignment: Many newspapers commonly write obituaries of import ant people years in advance, before they die, and in this assi gnment, you are asked to write an advance obituary for a famous person. Your Task: Please collect and save all the information you will need to write an advance obituary of the artist Trevor Malcolm Weeks. 3.1.2 Classification of the Four Tasks Table 1 shows the values of the va ried facets for each of the four search tasks that we gave to the participants. Task Product Level Goal (Quality) Objective complexity BIC Mixed Document Specific High CPE Factual Segment Specific Low INT Mixed Document Mixed Low OBI Factual Document Amorphous High BIC produced a Mixed Product, because identifying  X  X mportant X  newspapers is Intell ectual, and finding documents on the topic is Factual. It is at the Document Level because whole stories are judged; it has the Specific Goal of finding documents on a well-defined topic; it has High Obje ctive Complexity because of the number of sources and ac tivities that need to be consulted/done. CPE produced a Factual Product, be cause only facts have to be identified; it is at the Segment Level, because items within a document need to be found; it has the Specific Goal of confirming facts; it has Low Objective Complexity because only three facts need to be confirmed. INT produced a Mixed Product, because defining expertise is Document Level, because expertise is determined by a whole page; Goal Quality is Mixed, because determining expertise is amorphous but contact information is specific; it has Low Objective Complexity because only two people need to be found. OBI produced a Factual Product, because only facts about the person are needed; it is at the Document Level because entire documents need to be examin ed; Goal Quality is Amorphous because  X  X ll the information X  is undefined; it has High Objective Complexity because many facts need to be found. 3.2 Participants We used a convenience sampli ng method, recruiting students from a U.S. university undergra duate Journalism/Media Studies program to mimic journalists. They were recruited from relevant writing and reporting classes personally, with distributed flyers, and via email listservs of the u ndergraduate students. To ensure that the participants had appropriate journalism skills, only upper-division undergraduates who had co mpleted either one journalism writing or reporting class were selected. There were a total of 32 participants, 26 female and 6 male, aged between 18 and 27 years old. Most students were native English speakers (73%), with the remainder of the population stating a high degree of English knowledge. Participants rated their searching skills high with an average search experience of 8.5 years. They were generally positive about their average success during online search. They were informed in advance that th eir payment for participation in the experiment would be $20, and that the 8 who have saved the best set of pages for all four tasks, as judged by an external expert, would receive an additional $20. The rationale for the extra payment was to encourage participan ts to treat their assigned tasks seriously. 3.3 Search system The search interface in the experiment system has two frames: on the left side is a panel that allows the users to save desired pages and also to delete them in case they change their mind; on the right side is the regular Internet Explorer (IE) window, with a blank starting page. Figure 1 depicts the search interface with two saved web pages. 3.4 Procedure Each participant was invited individually to a usability lab. After signing a consent form and doing a warm-up task, the participants then performed the four web search tasks described above. The order of the four tasks was sy stematically rotated for each participant following a Latin Square design. Before each task, the participants were given a pre-task questionnaire which asked about their self-assessed familiarity with the search task and topic, and the estimated difficulty of the task. After each task, they were given a post-task questionnaire asking about their experienced difficulty of the task and satisfa ction with performance of the task. Although the experiment setting was controlled, participants were allowed to search freely on the Web using IE 6.0 as the starting point for access to any search engines or web sites they desired. All the participants X  interactions with the computer system, including keyboard and mouse activities, webpages visited, and screen capture were recorded by Morae Recorder 3.0 (http://www.techsmith.com). In each task, participants finished the task when they decided they had found and saved enough information objects for the purp oses of the task. If they had searched for at least 20 minutes, they were told this was the case, and asked to finish soon. After completing the post-search questionnaire, they were asked to evaluate the usefulness of the information objects they saved, or saved and then deleted, through replaying the search using the screen capture program. 4. VARIABLES 4.1 Difficulty measurements Task difficulty is the independent variable in this study. We followed Li &amp; Belkin X  X  [18] definition of task difficulty, which takes it as a subjective measure. We elicited task difficulty by asking users to self-rate, based on based on a 7-point scale, how difficult they thought the tasks were after completing the search tasks. Details are described in Section 5. 4.2 Behavioral variables We considered a comprehensive list of behavioral variables, extracted from the logged user-s ystem interaction data, as dependent variables. Specifically, the behavioral variables we examined in this study were all on the  X  X ccumulated level X , meaning these behavioral measures can be calculated in real-time during the search sessions. These behavioral measures include: 1) Average query interval time: the average length in seconds 2) Mean dwell time of all documents: the average dwell time 3) Mean dwell time of all SERPs: the average dwell time of 4) Average total dwell time of unique documents: the average 5) Average total dwell time of unique SERPs: the average 6) Number of viewed documents per query: average number 7) Number of unique viewed documents per query: average 8) Number of viewed SERPs per query: average number of 9) Number of unique viewed SERPs per query: average Based on previous studies, we co njectured that users' search behaviors at different search st ages may be different, or have different predictive power of search task difficulty. To test this, this study investigated whether di fferent behavioral variables are identified as predictors of task difficulty, or the prediction performance of the same behaviors are affected by the search stages. We operationalized search stages by calculating the above behavioral measures at three po ints during search stages: first-round, middle point, and end point. The detailed descriptions are as follows:  X  First-round means the behavioral measures were calculated  X  Middle point means the behavioral measures were  X  End point means the behavioral measures were calculated Since three different points of search stage were considered in this study, search sessions that contained fewer than three queries were excluded from the current examination (9 sessions in total). In fact, search sessions that contained fewer than 3 queries were all rated as easy tasks by users, and therefore no need to predict the task difficulty for such sessions. 5. USER BEHAVIOR COMPARISON BY TASK DIFFICULTY Participants X  ratings on task diffic ulty were elicited on a 7-point scale in the post-search questionnaires, as subjective ratings of task difficulty. As mentioned in section 3.1, the four tasks we assigned to participants had different objective complexity levels. As shown in Table 2, users X  subjective difficulty ratings were correlated with objective complexity level, but objective complexity did not always mean difficulty. Participants spent more time, visited more pages, and issued relatively more queries to accomplish the search tasks in the BIC and OBI than in the CPE and INT tasks. The former two were rated more difficult than the latter. Table 2. Task features and the number of queries by tasks Objective Complexity level High Low Low High Average rating of difficulty 4.53 2.31 3.31 5.25 Average task completion time (minutes) Average number of unique pages visited Average number of queries 20.79 7.86 12.71 17.48 In the data analysis, we collap sed the scores based on both the distribution of difficulty scores and the meaning of the different rating points: Difficult for user ratings of 5-7; Neutral for a rating of 4; Easy for ratings of 1-3. Altogether, there were 56 easy tasks and 52 difficult tasks, and the remaining 20 were neutral tasks, which were not included in the analysis. As mentioned before, we excluded search sessions that contained fewer than three queries (9 sessions). Therefore, among al l 128 sessions, we analyzed 99 search sessions (128-20-9=99). We explored the distribution of each behavioral variable in both the difficult and the easy tasks, and found that the majority of th em were not normal. Therefore, the Mann-Whitney U test was used to compare these variables between the easy and difficult groups at three different time points during the search process, as is reported in Table 3 below. 5.1 First-round It can be seen from the first fo ur columns in Table 3 that five behavioral measures were signifi cantly different between difficult and easy tasks in the first-round. Specifically, the mean dwell time of all documents (8.67 seconds), and the average total dwell time of unique documents (9.79 seconds) were shorter in difficult tasks than in easy tasks (14.68 second s &amp; 15.36 seconds respectively). In addition, searchers visited fewer documents (0.9) and fewer unique documents (0.75) after issu ing the first query in difficult tasks than in easy tasks (1.51 &amp; 1.28). The query interval time between the first and second query was shorter in difficult tasks (32.13 seconds) than in easy tasks (46.39 seconds).
Behavioral measures on the first-round level Difficult (Mean) all documents 17.03 8.67 1950 &lt;0.01 5.2 Middle and End point The center four columns in Table 2 show that only one variable, average query dwell time, was significantly different between easy and difficult tasks. The right four columns in Table 3 show that the same four of nine behavioral me asures had significant differences between difficult and easy tasks. Specifically, compared with easy tasks, in difficult ones, searchers had a much shorter average total dwell time of unique documents (16.58 seconds vs. 20.92 seconds); searchers visited fewer documents per query (2.06 vs. 2.64), and visited fewer unique documents per query (1.53 vs. 1.84). Similar to the first and middle time points, searchers spent a significantly shorter average query interval time in difficult tasks (48.09 seconds) than in easy tasks (63.81 seconds). 6. DIFFICULTY PREDICTION 6.1 Recursive partitioning method In this section, we explain how we used recursive partitioning to generate predictive models of task difficulty based on the behavioral measures we presen ted above, and its results. When implementing recursive partitioning, one issue is to be careful to prevent over-fitting. A pruning procedure is used in this study to cut off redundant nodes. In addition, we also used 5-fold cross-validation [25] to balance the needs of validating the results and retaining as many observations as possible. In the evaluation, the performance of each model is the average across all 5-test sets. 6.2 Predictive models This section presents the pred ictive models using recursive partitioning based on variables from different levels, as shown in Figures 2, 3 and 4. We use description of Figure 2 to explain how to read the tree models. First, the top node represents one of the training datasets, containing 42 sessions that were rated as  X  X ifficult X  tasks by users, and 37 sessions rated as  X  X asy X  tasks by users. The top node is marked as  X  X asy X  because there were more  X  X asy X  tasks in that dataset. Using the tree model, the top node is split into two daughter nodes, on the basis of the value of the variable  X  X ean_dwell_content X , and the cutoff value is 13.23. The 19 sessions on the right were all predicted as  X  X asy X , which contained 14 easy sessions and 5 di fficult sessions. The left node, containing 37 difficult tasks and 23 easy tasks, was further split into two nodes, on the basis of the value of the variable  X  X r_query_interval X , and the cutoff value was 12.65. On the left node, all 24 sessions were predic ted as  X  X ifficult X , which contained 18 difficul t sessions and 6 easy sessions. On the right node, all 14 sessions were predicte d as  X  X asy X , which contained 3 difficult sessions and 11 easy sessions. 1) First round (FR) model (Figure 2): if the average dwell 2) Middle point (MI) model (Figure 3): if the average query 3) End point (ED) model (Figure 4): At this point, the model 6.3 Behavioral measures iden tified in three stages The behavioral measures identified as predictors of task difficulty at three stages are summarized in Table 4. At all stages, the average query interval time and the average total dwell time of unique documents were selected as predictors of task difficulty. Stage Behavioral variable s as predictors FR model MI model ED model 6.4 Model Evaluation We evaluated the prediction performance by three measures: overall accuracy, and precision and F measure of difficult tasks. The overall accuracy was calculated as the number of correctly predicted easy and difficult tasks divided by the total number of search sessions; the precision of difficult tasks was calculated by the number of correctly predicted difficult tasks divided by the number of all tasks that were predicted as difficult. The measure of precision of difficult tasks was selected because the goal of the predictive models was to correctly predict difficult tasks by observing users X  search interactions. The F measure (  X  =0.5) was calculated using the following equation: In order to compare the improvements by predictive models in the prediction performance, we selected a baseline situation: when all the sessions were predicted as difficult tasks. Therefore, the overall accuracy and precision of difficult tasks in the baseline situation were both 52.5%, and the F (  X  =0.5) is 58.0%. Table 5. Prediction performance at different search stages. 
Numbers in bold indicate the best prediction performance Baseline 52.5% 52.5% 58.0% FR model 60.7% (+15.6%) 61.0% (16.2%) 62.6% (+7.9%) MI model 54.5% (+3.8%) 59.9% (+14.1%) 55.8% (-3.8%) ED model 54.6% (+4.0%) 61.6% (+17.3%) 55.9% (-3.6%) Table 5 shows the prediction performance of the models at the three search stages. It demonstrates that in general, all the predictive models outperformed the baseline. Comparatively, the FR model was the best among the three models, in overall accuracy (60.7%), precision of difficult tasks (61%) and F measure (62.6%). The ED model achieved higher precision of difficult tasks (61.6%) than the FR model (61.0%), but the overall accuracy (54.6%) was not as good as the FR model. The MI model also outperformed the baseline in accuracy and prediction of difficult tasks, but not in the F measure. Compared with the FR and the ED models, the MI model was the worst among the three stage models in all three measures. 6.5 Model evaluation by task type We further investigated whether prediction accuracy at three search stages was different by different search tasks. As shown in Table 6, in both BIC and OBI tasks, the best prediction accuracy was achieved in the FR model; th e MI and ED models had similar prediction accuracy, but were much worse than the FR model. In the CPE task, the best prediction accuracy was achieved in the ED model, and the MI model was the worst. However, the INT task shows another pattern, the MI mode l providing the best prediction accuracy, with the FR and ED models quite low. We should note that th e objective complexity and subjective task difficulty of the four search tasks were different. Specifically, the BIC and OBI tasks were designed to have high complexity, and users issued relatively more queries to accomplish the search tasks, and rated them relatively more difficult, compared with the CPE and INT tasks. Therefore, it seems that for difficult and complex tasks, the first-round predictive model had the best prediction performance, while for search sessions that were relatively less complex and difficult, the predictive model at later search stages (middle or end point) worked bett er. This indicates that first-round prediction was more importa nt for difficult tasks. 
Table 6. Prediction performance by task type (Numbers in bold indicate the best prediction performance among three Task Num. of sessions FR model MI model ED model BIC 28 64% 46% 50% CPE 22 68% 59% 82% INT 24 38% 63% 42% OBI 25 72% 52% 48% 6.6 Model evaluation by the number of queries in the session In a real-time situation, the system needs to determine when the searcher is in the middle of the search and when it is the end of search, in order to apply the predictive models. The selection of the middle and end points during search varied according to the total number of queries in the sear ch sessions. In this section, we compare the prediction accuracy by the number of queries in the whole search session, to examine at which stage, or at how many queries the searcher issued, would be the best time to make prediction of task difficulty fo r different search tasks. 
Table 7. Prediction performance by the number of queries (Numbers in bold indicate the best prediction accuracy among Num. of queries Num. of sessions FR model MI model 10&lt;number_query&lt;=15 21 43% 52% 43% Table 7 shows that when the sear ch sessions contained fewer than or equal to 5 queries, or the session contained more than 15 queries, the FR model had the highest prediction accuracy. Therefore the first-round point, after searchers issued one query and right before they issued the second query, is the best time to make the task difficulty predicti on. For sessions containing more than 5 queries but fewer than or equal to 10 queries, the ED model had the highest prediction accuracy. This indicates that for these sessions, the best time to make the prediction was after searchers issued 5 to 10 queries. For sess ions containing more than 10 queries but fewer than 15 queries, the MI model had the highest prediction accuracy. This indicates that the best time to make the prediction for sessions with 10 to 15 queries was after searchers issued 5 to 8 queries. In sum, our data show that the best time point to make the prediction of task difficulty is the first-round point, i.e. after the searcher issued the first query and right before he/she issued the second query; a nd then the next time point is after searchers had issued 5 to 8 queries. The latter conclusion needs further examination to be confirmed. 7. DISCUSSION 7.1 Behavioral differences and predictors of task difficulty A number of behavioral variables at three different points of search stages were examined in this study: first-round, middle point, and end point. The results showed significant differences between easy and difficult tasks (Table 3). In general, most of the significant patterns at these three different stages of the search session i nvolved the same variables: 1) average query interval time, 2) average dwell time on documents, 3) average number of viewed documents per query. This shows that patterns of these variables were consistent at different stages during search process, indicating that variables in these aspects were good and stable indicato rs of task difficulty. We found that if the query interval time was quite short, then it was likely that the task was difficult. We can imagine when a user quickly reformulates his/her initia l query, it often means the first query does not lead to any useful information, and then it is likely the task is difficult. Even though the average query interval time became longer at later stages than the beginning of search, the differences in this measure between easy and difficult tasks were consistently significant at all three stages and were identified as a predictor of task difficulty in all three predictive models (FR, MI and ED). In addition, it was found that if the average total dwell time on content pages was quite short, it wa s likely the task was difficult. This may be explained by the re lationship between dwell time and document usefulness. If a user spe nds a very short time on all the documents they clicked, these do cuments were not useful to the user, and it is therefore very likely the user has some difficulty with the search task. The pattern of this measure between difficult and easy tasks was consistent through the different search stages. Even though it was not significantly different in the middle of the search, this measure was also selected as a predictor of task difficulty in all three predictive models (FR, MI and ED). The average number of viewed documents per query was also shown to be significantly diff erent between difficult and easy tasks. If a searcher viewed very few documents after issuing any query, it was likely that he/she did not find many useful documents during the search process and then the search session would often be rated as difficult. It should be noted that the vari ous types of dwell time spent on SERPs did not show significant differences between easy and difficult tasks at any of the search stages. This seems to be different from what has been found in the literature, that users spend a longer time on SERPs in di fficult tasks than in easy ones ([4], [21], [22], [23]). However, a closer look at the descriptive data shows that the tendency was the same, i.e., dwell time on SERPs in difficult tasks was descriptively longer than that in easy tasks. The non-significance finding may be due to, we think, the specific task design in this experi ment, for example, the CPE task which was rated as easy, and th e OBI task which was rated as difficult, yet in both tasks, user s could spend much longer time on the documents themselves just to confirm the fact was right in the CPE task, and the person was the requested one in the OBI task. Therefore the dwell time on content pages was shown to be more important indicators of task difficulty in our experiment. However, the measure of dwell time on SERPs was selected as a predictor of task difficulty in the predictiv e models at the middle and end points (MI and ED models); theref ore, this measure was also a reliable predictor of task difficulty, especially when combined with other predictors. 7.2 Predictive models at different search stages Several points are worth discussing with regard to the models that were built. First, our results dem onstrate that it is possible that using a limited number of variables in the model can obtain relatively good prediction performan ce. Even though a number of behavioral measures were signifi cantly different between easy and difficult tasks, the predictive models using recursive partitioning only identified a few behavioral variables as good predictors of task difficulty (Table 4). For example, the first-round model, which was later found to achieve the best prediction performance, only identified two behavioral m easures as predictors. Some previous studies have found that using a combination of implicit measures could lead to better prediction performance (e.g. [22]); however, in practice, it may not be easy to record and calculate a large number of behavioral measures during the search process, and it is very important to have predictive models with a small number of behavioral measures. Second, it is promising to find that the FR model had the best prediction accuracy and the highest F measure among the predictive models at the three search stages; the ED model had only the best precision. This re sult seems to be different from prior work, like [3] and [22], which found the predictive model using whole-session variables had better prediction performance than the first-round model. But a close examination reveals that the whole-session variables they employed were different from the behavioral measures we calculated in the ED model. When calculating whole-session variables in their studies, they used behavioral measures that describe d the amount of effort that users have devoted in searching, e.g. the number of queries, the number of SERPs visited, the number of unique terms in queries, etc.; and it is understandable that the more effort a user devotes in searching, the more likely the task is difficult. In our study, all the behavioral measures we calculated at the three search stages were on the accumulated level, which described the users X  search process at different time points of search, like the dwell time features and Web page interaction features. The results also demonstrate that the prediction performance at the middle point was not as good as the first-round or the end point of search. A possible explan ation for this is that, when searchers found a search task difficult, it is very likely that he/she encountered difficulties during the first-round of search. Then after the searcher overcomes the difficulties faced at the beginning, he/she experiences a steady period in the middle of search when more useful information can be collected. However, if the search task is difficult, then the searcher is very likely to have difficulties in finding more useful informa tion at the end of search and decides to stop searching; that the task is as finished as it's going to get. Third, our results also demonstrate that the prediction performance by different predictive models could vary by the type of search task. Among the four s earch tasks, the CPE task got the best prediction accuracy (82%), followed by the OBI task (72%). The prediction performance in BIC and INT tasks was relatively low (64% and 63%). Referring to Table 1, the CPE and OBI tasks are both factual tasks, in which only factual information was needed for the tasks; while the BIC and INT tasks are both tasks that involve intellectua l an alysis of the factual information found. It seems that the prediction performance for tasks that involve intellectual work was not as good as for tasks that only involve factual information. This result is consistent with previous work ([14]). In that study, Kim suggests that task type is an important variable in task difficulty, and she found that in factual tasks, task difficulty was significantly associated with task completion time and the numbers of queries a nd documents viewed; and in interpretive tasks, there were rarely significant correlations between behaviors and task difficul ty. This is reasonable since for tasks involving intellectual work, searchers rating the task difficulty may refer to the difficulty in task features, rather than their interactions with search systems. However, further investigation is still needed to explore whether other behavioral measures that were not examined in the current study would be predictors of task difficulty for ta sks involving intellectual work. With respect to the predictive models at the three stages, the results demonstrate that the FR model was best for BIC and OBI tasks, and for CPE task, the ED model had the best accuracy, while for INT task, the MI model was the best. This result seems to be related to the objective complexity facet of the tasks: BIC and OBI tasks were designed to be high-complexity tasks, and searchers issued more queries to accomplish the tasks. We then further examined the relationship between the number of queries in one search session and the prediction performance at three stages during search. We found sim ilar results: for search sessions that required many queries (more than 15) and very few queries (fewer than 5), the model in the first-round was more indicative of task difficulty; for sessions that contained 5 to 10 queries, the end-point model worked best; for sess ions containing 10 to 15 queries, the middle-point model worked best. It is possible that searchers had some difficulties at the beginning of the search for sessions with not many queries, but after they overcame the initial difficulty, they found the task was not so difficult. Regarding the application of predictive models of task difficulty, our results demonstrate that th e first-round of search is an important point to make predictions of task difficulty, because it could achieve best prediction performance, on both accuracy and F measures. Even though the prediction at the end also performed well, it would be too late for the system to provide assistance for searchers at the end of their searching. We also explored predictive models at th e middle point of search, and found it did not perform as well as the first-round and the end point; considering it is difficult for the system to decide when is the middle point of search session to apply the MI model, it is more important for the system to monito r users X  behavioral measures at the beginning of a search session, to make predictions of whether the search is difficult or not, and thus to decide whether to provide certain assistance or personalization to help them accomplish searching. 8. CONCLUSIONS Using data collected in a controlled lab experiment, we examined the differences in users X  search behavior measures on the accumulated level in easy and difficult search sessions at three search stages. User behavioral measures were calculated at three search stages during search: 1) first-round level at the beginning of the search, 2) middle-point, and 3) end-point by the end of the search. Our results show that a number of user behavioral measures at all three stages show differences between easy and difficult tasks. The predictive mo dels we generated confirmed the results from the behavioral compar ison by task difficulty level. The predictive models identified average query interval time, the average dwell time on viewed documents, and the average number of viewed documents per query as important predictors of task difficulty. Our predictive models demonstrate that if a user feels a search task is difficult, then it is very like that his/her search interactions could reveal the di fficulties in the first-round of search, especially for complex tasks that require more queries. Therefore, it is possible to make accurate predictions of task difficulty in the beginning search round. In addition, the results also indicate that there might exist different types of task difficulty in different task types. The prediction performance and predictive model selection may vary according to task types. Despite being based on a controlle d lab experiment, our findings can help in the design of search systems which predict task difficulty and thereby provide pe rsonalized support to specific users. 9. ACKNOWLEDGMENTS Our thanks to IMLS for sponsoring the research experiment under grant number LG#06-07-0105-05. We thank all PooDLE members for their contribution on collecting and preparing the data used in this paper. The first author also thanks to National Nature Science Foundation of China (NSFC) #71303015 for further support of this research. 10. REFERENCES [1] Ageev, M., Guo, Q., Lagun, D. , &amp;Agichtein, E. (2011). Find [2] Agichtein, E., Brill, E., Du mais, S., &amp; Ragno, R. (2006). [3] Arguello, J. (2014). Predicting Search Task Difficulty. In [4] Aula, A., Khan, R. &amp; Guan, Z. (2010). How does search [5] Borlund, P. (2003). The IIR ev aluation model: A framework [6] Breiman, L. (1984). Classification and Regression Trees . [7] Carmel, D., Yom-Tov, E., Darlow, A., &amp;Pelleg, D. (2006). [8] Collins-Thompson, K., &amp; Benne tt, P. N. (2010). Predicting [9] Cronen-Townsend, S., Zhou , Y., &amp; Croft, W. B. (2002). [10] Feild, H., Allan, J., &amp; Jones, R. (2010). Predicting searcher [11] Fox, S., Karnawat, K., Mydland, M., Dumais, S., &amp; White, [12] Guo, Q., Yuan, S., &amp; Agichtein, E. (2011). Detecting success [13] Gwizdka, J., Spence, I. (2006). What can searching behavior [14] Kelly, D. &amp; Belkin, N.J. (2004). Display time as implicit [15] Kim, J. (2006). Task difficulty as a predictor and indicator of [16] Kotov, A., Bennett, P. N., White, R. W., Sumais, S. T., [17] Kuhlthau,C.C. (1991). Inside the search process: Information [18] Li, Y. &amp; Belkin, N.J. (2008). A faceted approach to [19] Liu, C., Belkin, N.J. &amp; Cole , M. (2012). Personalization of [20] Liu, J. &amp; Belkin, N. J. (2010). Personalizing information [21] Liu, J., Gwizdka, J., Liu, C. , &amp;Belkin, N. J. (2010). [22] Liu, J., Liu, C., Cole, M., Belkin, N. J., &amp; Zhang, X. (2012). [23] Liu, J., Liu, C., Gwi zdka, J., &amp;B elkin, N. (2010). Can search [24] Liu, J., Kim, C.S., Creel, C. (2013). Why Do Users Feel [25] Picard, R., &amp; Cook, R. (1984). Cross-validation of regression [26] Tombros, A., Ruthven, I., &amp; Jose, J. M. (2004). How users [27] Vakkari, P. (2001). A theory of the task-based information [28] Vakkari, P., &amp; Haka la, N. (2000). Cha nges in relevance [29] White, R. W., &amp; Dumais, S.T. (2009). Characterizing and [30] White, R. W., &amp; Kelly, D. (2006 ). A study of the effects of [31] White, R. W., Ruthven, I., &amp; Jose, J. M. (2005). A study of [32] Yan, R., Tang, J., Liu, X., Shan, D., &amp; Li, X. (2011). Citation 
