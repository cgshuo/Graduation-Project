 In a network such as hippocampal area CA3 that shows prominent oscillations during memory re-trieval and other functions [1], there are apparently three, somewhat separate, ways in which neurons might represent information within a single cycle: they must choose how many spikes to fire; what the mean phase of those spikes is; and how concentrated those spikes are about that mean. Most groups working on the theory of spiking oscillatory networks have considered only the second of these  X  this is true, for instance, of Hopfield X  X  work on olfactory representations [2] and Yoshioka X  X  [3] and Lengyel &amp; Dayan X  X  work [4] on analog associative memories in CA3. Since neurons do re-ally fire more or less than one spike per cycle, and furthermore in a way that can be informationally rich [5, 6], this poses a key question as to what the other dimensions convey.
 The number of spikes per cycle is an obvious analog of a conventional firing rate. Recent sophis-ticated models of firing rates of single neurons and neural populations treat them as representing uncertainty about the quantities coded, partly driven by the strong psychophysical and computa-tional evidence that uncertainty plays a key role in many aspects of neural processing [7, 8, 9]. Single neurons can convey the certainty of a binary proposition by firing more or less strongly [10, 11]; a whole population can use firing rates to convey uncertainty about a collectively-coded analog quantity [12].
 However, if neurons can fire multiple spikes per cycle, then the degree to which the spikes are concentrated around a mean phase is an additional channel for representing information. Concen-tration is not merely an abstract quantity; rather we can expect that the effect of the neuron on its postsynaptic partners will be strongly influenced by the burstiness of the spikes, an effect apparent, for instance, in the complex time-courses of short term synaptic dynamics. Here, we suggest that concentration codes for the uncertainty about phase  X  highly concentrated spiking represents high certainty about the mean phase in the cycle.
 One might wonder whether uncertainty is actually important for the cases of oscillatory processing that have been identified. One key computation for spiking oscillatory networks is memory retrieval [3, 4]. Although it is not often viewed this way, memory retrieval is a genuinely probabilistic task [13, 14], with the complete answer to a retrieval query not being a single memory pattern, but rather a distribution over memory patterns. This is because at the time of the query the memory device only has access to incomplete information regarding the memory trace that needs to be recalled. Most importantly, the way memory traces are stored in the synaptic weight matrix implies a data lossy compression algorithm, and therefore the original patterns cannot be decompressed at retrieval with absolute certainty.
 In this paper, we first describe how oscillatory structures can use all three activity characteristics at their disposal to represent two pieces of information and two forms of uncertainty (Section 2). We then suggest that this representational scheme is appropriate as a model of uncertainty-aware probabilistic recall in CA3. We derive the recurrent neural network dynamics that manipulate these firing characteristics such that by the end of the retrieval process neurons represent a good approx-imation of the posterior distribution over memory patterns given the information in the recall cue and in the synaptic weights between neurons (Section 3). We show in numerical simulations that the derived dynamics lead to competent memory retrieval, supplemented by uncertainty signals that are predictive of retrieval errors (Section 4). Single cell The heart of our proposal is a suggestion for how to interpret the activity of a single neuron in a single oscillatory cycle (such as a theta-cycle in the hippocampus) as representing a probability distribution. This is a significant extension of standard work on single-neuron represen-tations of probability [12]. We consider a distribution over two random variables, z  X  X  0 , 1 } , a Bernoulli variable (for the case of memory, representing the participation of the neuron in the mem-ory pattern), and x  X  [0 , T ) , where T is the period of the underlying oscillation, a real valued phase variable (representing an analog quantity associated with that neuron if it participates in that pattern). This distribution is based on three quantities associated with the neuron X  X  activity (figure 1A): r the number of spikes in a cycle,  X  the circular mean phase of those spikes, under the assumption that there is at least one spike, c the concentration of the spikes (mean resultant length of their phases, [15]), which measures how In keeping with conventional single-neuron models, we treat r , via a (monotonically increasing) probabilistic activation function 0  X   X  ( r )  X  1 , as describing the probability that z = 1 (figure 1B), phase x as being conditional on z . If z = 0 , then the phase is undefined. However, if z = 1 , then the distribution q  X  ( x ;  X  ) (of width T ) around the mean firing phase (  X  ) of the spikes. The mixing proportion in this case is determined by a (monotonically increasing) function 0  X   X  ( c )  X  1 of the concentration of the spikes. In total: as shown in figure 1C. The marginal confidence in  X  being correct is thus  X  ( c, r ) =  X  ( c )  X   X  ( r ) , which we call  X  X urst strength X . We can rewrite equation 1 in a more convenient form: Population In the case of a population of neurons, the complexity of representing a full joint dis-tribution P[ x , z ] over random variables x = { x i } , z = { z i } associated with each neuron i grows exponentially with the number of neurons N . The natural alternative is to consider an approxima-tion in which neurons make independent contributions, with marginals as in equation 2. The joint Figure 1: Representing uncertainty. A) A neuron X  X  firing times during a period [0 , T ) are described by three parameters: r , the number of spikes;  X  the mean phase of those spikes; and c , the phase concentration. B) The firing rate r determines the probability  X  ( r ) that a Bernoulli variable associ-ated with the unit takes the value z = 1 . C) If z = 1 , then  X  and c jointly define a distribution over phase which is a mixture (weighted by  X  ( c ) ) of a distribution peaked at  X  and a uniform distribution. distribution is then whose complexity scales linearly with N .
 Dynamics When the actual distribution P the population has to represent lies outside the class of representable distributions Q in equation 3 with independent marginals, a key computational step is to find activity parameters  X  , c , r for the neurons that make Q as close to P as possible. One way to formalize the discrepancy between the two distributions is the KL-divergence Minimizing this by gradient descent defines dynamics for the evolution of the parameters. In general, this couples the activities of neu-rons, defining recurrent interactions within the network. 1 We have thus suggested a general representational framework, in which the specification of a com-putational task amounts to defining a P distribution which the network should represent as best as possible. Equation 5 then defines the dynamics of the interaction between the neurons that optimizes the network X  X  approximation. One of the most widely considered tasks that recurrent neural networks need to solve is that of autoassociative memory storage and retrieval. Moreover, hippocampal area CA3, which is thought to play a key role in memory processing, exhibits oscillatory dynamics in which firing phases are known to play an important functional role. It is therefore an ideal testbed for our theory. We characterize the activity in CA3 neurons during recall as representing the probability distribution over memories being recalled. Treating storage from a statistical perspective, we use Bayes rule to define a posterior distribution over the memory pattern implied by a noisy and impartial cue. This distribution is represented approximately by the activities  X  i , r i , c i of the neurons in the network as in equation 3. Recurrent dynamics among the neurons as in equation 5 find appropriate values of these parameters, and model network interactions during recall in CA3.
 Storage We consider CA3 as storing patterns in which some neurons are quiet ( z m i = 0 , for the i th neuron in the m th pattern); and other neurons are active ( z m i = 1 ); their activity then defining a firing phase ( x i m  X  [0 , T ) , where T is the period of the population oscillation. M such memory traces, each drawn from an ( iid ) prior distribution, (where p z is the prior probability of firing in a memory pattern; P ( x ) is the prior distribution for firing phases) are stored locally and additively in the recurrent synaptic weight matrix of a network of N neurons, W , according to learning rule  X  : We assume that  X  is T  X  oplitz and periodic in T , and either symmetric or anti-symmetric:  X  ( x 1 , x 2 ) =  X  ( x 1  X  x 2 ) =  X  ( x 1  X  x 2 mod T ) =  X   X  ( x 2  X  x 1 ) .
 Posterior for memory recall Following [14, 4], we characterize retrieval in terms of the posterior trix, and the prior over the memories. Under some basic independence assumptions, this factorizes into three terms The first term is the prior (equation 6). The second term is the likelihood of receiving noisy or partial recall cue (  X x ,  X z ) if the true pattern to be recalled was ( x , z ) :
P [  X x ,  X z | x , z ] = Y where  X  1 = P [  X  z = 1 | z = 1] and  X  0 = P [  X  z = 0 | z = 0] are the probabilities of the presence or absence of a spike in the input given the presence or absence of a spike in the memory to be recalled,  X  P 1 (  X  x | x ) and in the memory to be recalled.
 The last term in equation 8 is the likelihood that weight matrix W arose from M patterns includ-Since the learning rule is additive and memory traces are drawn iid , the likelihood of a synaptic weight is approximately Gaussian for large M , with a quadratic log-likelihood [4]: where  X  W and  X  2 W are the mean and variance of the distribution of synaptic weights after storing M  X  1 random memory traces (  X  W = 0 for antisymmetric  X  ).
 Dynamics for memory recall Plugging the posterior from equation 8 to the general dynamics equation 5 yields the neuronal update rules that will be appropriate for uncertainty-aware memory recall, and which we treat as a model of recurrent dynamics in CA3.
 We give the exact formul X  for the dynamics in the supplementary material. They can be shown to couple together the various activity parameters of the neurons in appropriate ways, for instance weighting changes to  X  i for neuron i according to the burst strength of its presynaptic inputs, and increasing the concentration when the log posterior of the firing phase of the neuron, given that it should fire, log P[  X  i | z i = 1 ,  X  x ,  X  z , W ] , is greater than the average of the log posterior. These dynamics generalize, and thus inherit, some of the characteristics of the purely phase-based network suggested in [4]. This means that they also inherit the match with physiologically-measured phase response curves (PRCs) from in vitro CA3 neurons that were measured to test this suggestion [16]. The key difference here is that we expect the magnitude (though not the shape) of the influence of a presynaptic neuron on the phase of a postsynaptic one to scale with its rate, for high concentra-tion. Preliminary in vitro results show that PRCs recorded in response to burst stimulation are not qualitatively different from PRCs induced by single spikes; however, it remains to be seen if their magnitude scales in the way implied by the dynamics here. Figure 2: A single retrieval trial in the network. Time evolution of firing phases (left panels) , concentrations (middle panels) , and rates (right panels) of neurons that should (top row) or should not (bottom row) participate in the memory pattern being retrieved. Note that firing phases in the top row are plotted as a difference from the stored firing phases so that  X  = 0 means perfect retrieval. Color code shows precision (blue: low, yellow: high) of the phase of the input to neurons, with red lines showing cells receving incorrect input rate. Figure 2 shows the course of recall in the full network (with N = 100 neurons, and 10 stored patterns with p z = 0 . 5 ). For didactic convenience, we consider the case that the noise in the phase input was varied systematically for different neurons within a recall cue (a fact known to the network, ie incorporated into its dynamics), so that it is possible to see how the differential certainty evolves over the course of the network X  X  dynamics. The top left panel shows that neurons that should fire in the memory trace ( ie for which z = 1 ) quickly converge on their correct phase, and that this convergence usually takes a longer time for neurons receiving more uncertain input. This is paralleled by the way their firing concentrations change (top middle panel) : neurons with reliable input immediately increase their concentrations from the initial  X  ( c ) = 0 . 5 value to  X  ( c ) = 1 , while for those having more unreliable input it takes a longer time to build up confidence about their firing phases (and by the time they become confident their phases are indeed correct). Neurons that should not fire ( z = 0 ) build up their confidence even more slowly, more often remain fairly uncertain or only moderately certain about their firing phases, as expressed by their concentrations (middle bottom panel)  X  quite righteously. Finally, since the firing rate input to the network is correct 90%, most neurons that should or should not fire do or do not fire, respectively, with maximal certainty about their rate (top and bottom right panels) .
 Various other metrics are important for providing insight into the operation of the network. In particular, we may expect there to be a relationship between the actual error in the phase of firing of the neurons recalled by the memory, and the firing rates and concentrations (in the form of burst strengths) of the associated neurons themselves. Neurons which are erring should whisper rather than shout. Figure 3A shows just this for the network. Here, we have sorted the neurons according to their burst strengths  X  , and plotted histograms of errors in firing phase for each group. The lower the burst strength, the more likely are large errors  X  at least to an approximation. A similar relationship exists between recalled (analogue) and stored (binary) firing rates, where extreme values of the recalled firing rate indicate that the stored firing rate was 0 or 1 with higher certainty (Figure 3B). Figure 3C shows the results of a related analysis of experimental data kindly donated by Francesco Battaglia. He recorded neurons in hippocampal area CA1 (not CA3, although we may hope for some similar properties) whilst rats were shuttling on a linear track for food reward. CA1 neurons have place fields  X  locations in the environment where they respond with spikes  X  and the phases of these spikes relative to the ongoing theta oscillation in the hippocampus are also known to convey information about location in space [5]. To create the plot, we first selected epochs with high-quality and high power theta activity in the hippocampus (to ensure that phase is well estimated). We then computed the mean firing phase within the theta cycle,  X  , of each neuron as a function of the location of the rat, separately for each visit to the same location. We assumed that the  X  X rue X  phase x a neuron should recall at a given location is the average of these phases across different visits. We Figure 3: Uncertainty signals are predictive of the error a cell is making both in simulation (A,B) , and as recorded from behaving animals (C) . Burst strength signals overall uncertainty about and thus predicts error in mean firing phase (A,C) , while graded firing rates signal certainty about whether to fire or not (B) . then evaluated the error a neuron was making at a given location on a given visit as the difference between its  X  in that trial at that location and the  X  X rue X  phase x associated with that location. This allowed us to compute statistics of the error in phase as a function of the burst strength. The curves in the figure show that, as for the simulation, burst strength is at least partly inversely correlated with actual phase error, defined in terms of the overall activity in the population. Of course, this does not constitute a proof of our representational theory.
 One further way to evaluate the memory is to compare it to two existing associative memories that have previously been studied, and can be seen as special cases. On one hand, our memory adds the dimension of phase to the uncertainty-aware rate-based memory that Sommer &amp; Dayan [14] studied. This memory made a somewhat similar variational approximation, but, as for the mean-field Boltzmann machine [17], only involving r and  X  ( r ) and no phases.
 On the other hand, the memory device can be seen as adding the dimension of rate to the phase-based memory that Lengyel &amp; Dayan [4] treated. Note, however, that although this phase-based network used superficially similar probabilistic principles to the one we have developed here, in fact it did not operate according to uncertainty, since it made the key simplification that all neurons participate in all memories, and that they also fire exactly one spike on every cycle during recall. This restricted the dynamics of that network to perform maximum a posteriori (MAP) inference to find the single recalled pattern of activity that best accommodated the probabilistic constraints of the cue, the prior and the synaptic weights, rather than being able to work in the richer space of probabilistic recall of the dynamics we are suggesting here.
 Given these roots, we can follow the logic in figure 4 and compare the performance of our memory with these precursors in the cases for which they are designed. For instance, to compare with the rate-based network, we construct memories which include phase information. During recall, we present cues with relatively accurate rates, but relatively inaccurate phases, and evaluate the extent to which the network is perturbed by the presence of the phases (which, of course, it has to store in the single set of synaptic weights). Figure 4A shows exactly this comparison. Here, a relatively small network ( N = 100 ) was used, with memories that are dense ( p z = 0 . 5 ), and it is therefore a stringent test of the storage capacity. Performance is evaluated by calculating the average error made in recalled firing rates).
 In the figure, the two blue curves are for the full model (with the phase information in the input being relatively unreliable, its circular concentration parameter distributed uniformly between 0.1 and 10 across cells); the two yellow curves are for a network with only rates (which is similar to that described, but not simulated, by Sommer &amp; Dayan [14]). Exactly the same rate information is provided to all networks, and is 10% inaccurate (a degree known to the dynamics in the form of  X  0 and  X  1 ). The two flat dashed lines show the performance in the case that there are no recurrent synaptic weights at all. This is an important control, since we are potentially presenting substantial information in the cues themselves. The two solid curves show that the full model tracks the reduced, rate-based, model almost perfectly until the performance totally breaks down. This shows that the phase information, and the existence of phase uncertainty and processing during recall, does not Figure 4: Recall performance compared with a rate-only network (A) and a phase-only network (B) . The full model (blue lines) performs just as well as the reduced  X  X pecialist X  models (yellow lines) in comparable circumstances (when the information provided to the networks in the dimension they shared is exactly the same). All models ( solid lines ) outperform the standard control of using the input and the prior alone (dashed lines) . corrupt the network X  X  capacity to recall rates. Given its small size, the network is quite competent as an auto-associator.
 Figure 4B shows a similar comparison between this network and a network that only has to deal with uncertainty in firing phases but not in rates. Again, its performance at recalling phase, given uncertain and noisy phase cues, but good rate-cues, is exactly on a par with the pure, phase-based network. Further, the average errors are only modest, so the capacity of the network for storing analog phases is also impressive. We have considered an interpretation of the activities of neurons in oscillating structures such as area CA3 of the hippocampus as representing distributions over two underlying quantities, one binary and one analogue. We also showed how this representational capacity can be used to excellent effect in the key, uncertainty-sensitive computation of memory recall, an operation in which CA3 is known to be involved. The resulting network model of CA3 encompasses critical aspects of its physiological properties, notably information-bearing firing rates and phases. Further, since it generalizes earlier theories of purely phase-based memories, this model is also consistent with the measured phase response curves of CA3 neurons, which characterize their actual dynamical interactions.
 Various aspects of this new theory are amenable to experimental investigation. First, the full dy-namics (see the supplementary material) imply that firing rate and firing phase should be coupled together both pre-synpatically, in terms of the influence of timed input spikes, and post-synaptically, in terms of how changes in the activity of a neuron should depend on its own activity. In vitro ex-periments along the lines of those carried out before [16], in which we have precise experimental control over pre-and post-synaptic activity can be used to test these predictions. Further, making the sort of assumptions that underlie figure 3C, we can use data from awake behaving rats to see if the gross statistics of the changes in the activity of the neurons fit the expectations licensed by the theory.
 From a computational perspective, we have demonstrated that the network is a highly competent associative memory, correctly recalling both binary and analog information, along with certainty about it, and degrading gracefully in the face of overload. In fact, compared with the representation of other analogue quantities (such as the orientation of a visually preseted bar), analogue memory actually poses a particularly tough problem for the representation of uncertainty. This is because for variables like orientation, a whole population is treated as being devoted to the representation of the distribution of a single scalar value. By contrast, for analogue memory, each neuron has an independent analogue value, and so the dimensionality of the distribution scales with the number of neurons involved. This extra representational power comes from the ability of neurons to distribute their spikes within a cycle to indicate their uncertainty about phase (using the dimension of time in just the same way that distributional population codes [12] used the dimension of neural space). This dimension for representing analogue uncertainty is coupled to that of the firing rate for repre-senting binary uncertainty, since neurons have to fire multiple times in a cycle to have a measurable lack of concentration. However, this coupling is exactly appropriate given the form of the distribu-tion assumed in equation 2, since weakly firing neurons express only weak certainty about phase in any case. In fact, it is conceivable that we could combine a different model for the firing rate uncertainty with this model for analogue uncertainty, if, for instance, it is found that neuronal firing rates covary in ways that are not anticipated from equation 2.
 Finally, the most important direction for future work is understanding the uncertainty-sensitive cou-pling between multiple oscillating memories, where the oscillations, though dynamically coordi-nated, need not have the same frequencies. Exactly this seems to characterize the interaction be-tween the hippocampus and the necortex during both consolidation and retrieval [18, 19]. Acknowledgments Funding from the Gatsby Charitable Foundation. We are very grateful to Francesco Battaglia for allowing us to use his data to produce figure 3C, and to him, and Ole Paulsen and Jeehyun Kwag for very helpful discussions.
 References
