 Conditional maximum entropy models [1, 3], conditional max ent models for short, also known as multinomial logistic regression models, are widely used in applications, most prominently for multi-computer vision [12] over the last decade or more.
 These models are based on the maximum entropy principle of Ja ynes [11], which consists of se-lecting among the models approximately consistent with the constraints, the one with the greatest entropy. They benefit from a theoretical foundation similar to that of standard maxent probabilistic shows that these models belong to the exponential family. As shown by Lebanon and Lafferty [13], solving precisely the same optimization problem with the sa me constraints, modulo a normalization constraint needed in the conditional maxent case to derive p robability distributions. While the theoretical foundation of conditional maxent mod els makes them attractive, the computa-A number of algorithms have been described for batch trainin g of conditional maxent models using a single processor. These include generalized iterative sc aling [7], improved iterative scaling [8], gradient descent, conjugate gradient methods, and second-order methods [15, 18].
 This paper examines distributed methods for training condi tional maxent models that can scale to very large samples of up to 1B instances. Both batch algorith ms and on-line training algorithms such here on batch distributed methods.
 We examine three common distributed training methods: a dis tributed gradient computation method [4], a majority vote method, and a mixture weight method. We a nalyze and compare the CPU and network time complexity of each of these methods (Section 2) and present a theoretical analysis of conditional maxent models (Section 3), including a study of the convergence of the mixture weight method, the most resource-efficient technique. We also repo rt the results of large-scale experiments comparing these three methods which demonstrate the benefit s of the mixture weight method (Sec-tion 4): this method consumes less resources, while achievi ng a performance comparable to that of standard approaches such as the distributed gradient compu tation method. 1 In this section, we first briefly describe the optimization pr oblem for conditional maximum entropy models, then discuss three common methods for distributed t raining of these models and compare their CPU and network time complexity. 2.1 Conditional Maxent Optimization problem Let X be the input space, Y the output space, and  X  : X  X  Y  X  H a (feature) mapping to a Hilbert space H , which in many practical settings coincides with R N , N = dim( H ) &lt;  X  . We denote by $  X  $ the norm induced by the inner product associated to H .
 ! optimization problem: Here,  X   X  0 is a regularization parameter typically selected via cross -validation. The optimization problem just described corresponds to an L 2 regularization. Many other types of regularization have tions based on other norms. This paper will focus on conditio nal maximum entropy models with L 2 regularization.
 These models have been extensively used and studied in natur al language processing [1, 3] and predicted by the model for an input x is: Since the function F S is convex and differentiable, gradient-based methods can b e used to find a global minimizer w of F S . Standard training methods such as iterative scaling, grad ient descent, conjugate gradient, and limited-memory quasi-Newton all h ave the general form of Figure 1, where the update function  X  : H  X  H for the gradient  X  F S ( w ) depends on the optimization method selected. T is the number of iterations needed for the algorithm to conve rge to a global minimum. of the loop. 2.2 Distributed Gradient Computation Method across p machines. Consider a sample S =( S 1 ,...,S p ) of pm points formed by p subsamples of m points drawn i.i.d., S 1 ,...,S p . At each iteration, the gradients  X  F S p machines in parallel. These separate gradients are then sum med up to compute the exact global gradient on a single machine, which also performs the optimi zation step and updates the weight vector received by all other machines (Figure 2). Chu et al. [ 4] describe a map-reduce formulation for this computation, where each training epoch consists of one map (compute each  X  F S and one reduce (update w ). However, the update method they present is that of Newton-Raphson, which requires the computation of the Hessian. We do not cons ider such strategies, since Hessian computations are often infeasible for large data sets. 2.3 Majority Vote Method The ensemble methods described in the next two paragraphs ar e based on mixture weights  X   X  R p . Let  X  p = {  X   X  R p :  X   X  0  X  experiments.
 Instead of computing the gradient of the global function in p arallel, a (weighted) majority vote method can be used. Each machine receives one subsample S k , k  X  [1 ,p ] , and computes w k = the majority vote method for an input x is where I is an indicator function of the predicate it takes as argumen t. Alternatively, the con-ditional class probabilities could be used to take into acco unt the uncertainty of each classifier: y =argmax y 2.4 Mixture Weight Method The cost of storing p weight vectors can make the majority vote method unappealin g. Instead, a single mixture weight w  X  can be defined form the weight vectors w k , k  X  [1 ,p ] : The mixture weight w  X  can be used directly for classification. 2.5 Comparison of CPU and Network Times This section compares the CPU and network time complexity of the three training methods just described. Table 1 summarizes these results. Here, we denot e by N the dimension of H . User CPU represents the CPU time experienced by the user, cumulative CPU the total amount of CPU time for the machines participating in the computation, and latency the experienced runtime effects due to network activity. The cumulative network usage is the amount of data transferred across the network during a distributed computation.
 For a training sample of pm points, both the user and cumulative CPU times are in O cpu ( TpmN ) when training on a single machine (Figure 1) since at each of t he T iterations, the gradient compu-tation must iterate over all pm training points and update all the components of w . For the distributed gradient method (Section 2.2), the wors t-case user CPU of the gradient and parameter update computations (lines 3-4 of Figure 2) is O cpu ( mN + pN + N ) since each parallel gradient calculation takes mN to compute the gradient for m instances, p gradients of size N need to be summed, and the parameters updated. We assume here that the time to compute  X  is negligible. If we assume that p * m , then, the user CPU is in O cpu ( mNT ) . Note that the number of iterations identical.
 In terms of network usage, a distributed gradient strategy w ill incur a cost of O net ( pNT ) and a latency proportional to O lat ( NT ) , since at each iteration w must be transmitted to each of the p machines (in parallel) and each  X  F S improved through better data partitioning of S when  X  ( x,y ) is sparse. The exact runtime cost of latency is complicated as it depends on factors such as the ph ysical distance between the master and each machine, connectivity, the switch fabric in the networ k, and CPU costs required to manage messages. For parallelization on massively multi-core mac hines [4], communication latency might be negligible. However, in large data centers running commo dity machines, a more common case, network latency cost can be significant.
 The training times are identical for the majority vote and mi xture weight techniques. Let T k be the method. However, in practice, T max is typically less than T since convergence is often faster with smaller data sets. A crucial advantage of these methods over the distributed gradient method is that their network usage is significantly less than that of the dis tributed gradient computation. While parameters and gradients are exchanged at each iteration fo r this method, majority vote and mixture Thus, the overall network usage is O net ( pN ) with a latency in O lat ( NT ) . The main difference between the majority vote and mixture weight methods is the u ser CPU (and memory usage) for be distributed over p machines for the majority vote method, but that would incur a dditional machine and network bandwidth costs. This section presents a theoretical analysis of conditiona l maxent models, including a study of the convergence of the mixture weight method, the most resource -efficient technique, as suggested in the previous section.
 The results we obtain are quite general and include the proof of several fundamental properties of the weight vector w obtained when training a conditional maxent model. We first p rove the stability of w in response to a change in one of the training points. We then g ive a convergence bound for w as a function of the sample size in terms of the norm of the feat ure space and also show a similar result for the mixture weight w  X  . These results are used to compare the weight vector w pm obtained by training on a sample of size pm with the mixture weight vector w  X  .
 by conditional maximum entropy when trained on sample S , w # the vector returned when trained on S # , and let  X  w denote w #  X  w . We shall assume that the feature vectors are bounded, that i s techniques similar to those used by Bousquet and Elisseeff [ 2], or other authors, e.g., [6], in the analysis of stability. In what follows, for any w  X  H and z =( x,y )  X  X  X  Y , we denote by L z ( w ) the negative log-likelihood -log p w [ y | x ] .
 Theorem 1. Let S # and S be two arbitrary samples of size m differing only by one point. Then, the following stability bound holds for the weight vector retur ned by a conditional maxent model: Proof. We denote by B F the Bregman divergence associated to a convex and different iable function functions. Since the Bregman divergence is non-negative, B G Similarly, B F where we used the convexity of L z ! yields The gradient of w , X  L z  X  L z Thus, we obtain $ X  L z Let D denote the distribution according to which training and tes t points are drawn and let F ! be the objective function associated to the optimization defin ed with respect to the true log loss: F w ! = argmin Theorem 2. Let w  X  H be the weight vector returned by conditional maximum entrop y when inequality holds: Proof. Let S and S # be as before samples of size m differing by a single point. To derive this bound, we apply McDiarmid X  X  inequality [17] to  X  ( S )= $ w  X  w ! $ . By the triangle inequality and Theorem 1, the following Lipschitz property holds: Thus, by McDiarmid X  X  inequality, Pr[  X   X  E[  X  ]  X  " ]  X  exp shown for the expectation of  X  (see longer version of this paper): E[  X  ]  X  2 R and setting the right-hand side of McDiarmid X  X  inequality t o  X  show that the following holds with probability at least 1  X   X  .
 Note that, remarkably, the bound of Theorem 2 does not depend on the dimension of the feature space but only on the radius R of the sphere containing the feature vectors.
 Consider now a sample S =( S 1 ,...,S p ) of pm points formed by p subsamples of m points drawn a learning bound for w  X  .
 Theorem 3. For any  X   X   X  p , let w  X   X  H denote the mixture weight vector obtained from a sample of size pm by combining the p weight vectors w k , k  X  [1 ,p ] , each returned by conditional maximum the following inequality holds: For the uniform mixture  X  0 =(1 /p,..., 1 /p ) , the bound becomes Proof. The result follows by application of McDiarmid X  X  inequalit y to  X  ( S )= $ w  X   X  w ! $ . Let S denote the weight vector obtained by training on subsample S # k and w # holds: |  X  ( S # )  X   X  ( S ) | = Thus, by McDiarmid X  X  inequality, which proves the first statement and the uniform mixture case since $  X  0 $ =1 / Theorems 2 and 3 help us compare the mixture weight w pm obtained by training on a sample of size pm versus the mixture weight vector w  X  the sample size. To simplify the analysis, we shall assume th at  X  = O (1 /m 1 / 4 ) for a sample of size m . A similar discussion holds for other comparable asymptoti c behaviors. By Theorem 2, $ w pm  X  w ! $ converges to zero in O (1 / (  X  that case. But, by Theorem 3, the slack term bounding $ w  X  rate O (1 / (  X  in the bound on $ w  X  however. E[ $ w  X  holds: By the proof of Theorem 2, E[ $ w 1  X  w ! $ ]  X  R/ (  X  O (1 /m 1 / 4 ) . In summary, w  X  bound for w  X  terpart term in the bound for w pm . Table 2: Description of data sets. The column named sparsity reports the frequency of non-zero feature values for each data set. We ran a number of experiments on data sets ranging in size fro m 1M to 1B labeled instances (see Table 2) to compare the three distributed training methods d escribed in Section 2. Our experiments were carried out using a large cluster of commodity machines with a local shared disk space and a high rate of connectivity between each machine and between m achines and disk. Thus, while the processes did not run on one multi-core supercomputer, the n etwork latency between machines was minimized.
 We report accuracy, wall clock, cumulative CPU usage, and cu mulative network usage for all of our experiments. Wall clock measures the combined effects of th e user CPU and latency costs (column 1 of Table 1), and includes the total time for training, inclu ding all summations. Network usage measures the amount of data transferred across the network. Due to the set-up of our cluster, this includes both machine-to-machine traffic and machine-to-d isk traffic. The resource estimates were calculated by point-sampling and integrating over the samp ling time. For all three methods, we used the same base implementation of conditional maximum entrop y, modified only in whether or not the gradient was computed in a distributed fashion.
 Our first set of experiments were carried out with  X  X edium X  sc ale data sets containing 1M-300M in-stances. These included: English part-of-speech tagging, generated from the Penn Treebank put word and the words in a window of size two; Sentiment analysis , generated from a set of with a bag of words feature representation; RCV1-v2 as described by [14], where documents having multiple labels were included multiple times, once for each label; Acoustic Speech Data , a 39-outputs (43 phones  X  3 acoustic states); and the Deja News Archive , a text topic classification problem generated from a collection of Usenet discussion fo rums from the years 1995-2000. For all text experiments, we used random feature mixing [9, 20] to co ntrol the size of the feature space. The results reported in Table 3 show that the accuracy of the m ixture weight method consistently matches or exceeds that of the majority vote method. As expec ted, the resource costs here are similar, with slight differences due to the point-sampling methods and the overhead associated with storing p models in memory and writing them to disk. For some data sets, we could not report majority vote results as all models could not fit into memory o n a single machine.
 The comparison shows that in some cases the mixture weight me thod takes longer and achieves somewhat better performance than the distributed gradient method while for other data sets it ter-minates faster, at a slight loss in accuracy. These differen ces may be due to the performance of the optimization with respect to the regularization parameter  X  . However, the results clearly demon-strate that the mixture weight method achieves comparable a ccuracies at a much decreased cost in network bandwidth  X  upwards of 1000x. Depending on the cost m odel assessed for the underlying network and CPU resources, this may make mixture weight a sig nificantly more appealing strategy. mental set-up of high rates of connectivity, then the mixtur e weight method could be substantially faster to train. The outlier appears to be the acoustic speec h data, where both mixture weight and distributed gradient have comparable network usage, 158GB and 200GB, respectively. However, the bulk of this comes from the fact that the data set itself is 157 GB in size, which makes the network usage closer to 1GB for the mixture weight and 40GB for distri buted gradient method when we discard machine-to-disk traffic.
 For the largest experiment, we examined the task of predicti ng the next character in a sequence of text [19], which has implications for many natural langua ge processing tasks. As a training and evaluation corpus we used the English Gigaword corpus [1 0] and used the full ASCII output space of that corpus of around 100 output classes (uppercase and lowercase alphabet characters variants, digits, punctuation, and whitespace). For each c haracter s , we designed a set of observed hashed each into a 10k-dimensional space in an effort to impr ove speed. Since there were around 100 output classes, this led to roughly 1M parameters. We the n sub-sampled 1B characters from the corpus as well as 10k testing characters and established a training set of 1000 subsets, of 1M instances each. For the experiments described above, the re gularization parameter  X  was kept fixed across the different methods. Here, we decreased the parame ter  X  for the distributed gradient method since less regularization was needed when more data was avai lable, and since there were three orders of magnitude difference between the training size for each i ndependent model and the distributed gradient. We compared only the distributed gradient and mix ture weight methods since the majority vote method exceeded memory capacity. On this data set, the n etwork usage is on a different scale than most of the previous experiments, though comparable to Deja 250, with the distributed gradient method transferring 13TB across the network. Overall, the m ixture weight method consumes less resources: less bandwidth and less time (both wall clock and CPU). With respect to accuracy, the mixture weight method does only slightly worse than the dist ributed gradient method. The individual models in the mixture weight method ranged between 49.73% to 50.26%, with a mean accuracy of 50.07%, so a mixture weight model improves slightly over a random subsample models and decreases the overall variance. Our analysis and experiments give significant support for th e mixture weight method for training very large-scale conditional maximum entropy models with L 2 regularization. Empirical results suggest that this method achieves similar or better accurac ies while reducing network usage by about three orders of magnitude and modestly reducing the wa ll clock time, typically by about 15% or more. In distributed environments without a high rate of c onnectivity, the decreased network usage of the mixture weight method should lead to substantia l gains in wall clock as well. Acknowledgments [1] A. Berger, V. Della Pietra, and S. Della Pietra. A maximum entropy approach to natural [2] O. Bousquet and A. Elisseeff. Stability and generalizat ion. Journal of Machine Learning [3] S. F. Chen and R. Rosenfeld. A survey of smoothing techniq ues for ME models. IEEE Trans-[4] C. Chu, S. Kim, Y. Lin, Y. Yu, G. Bradski, A. Ng, and K. Oluko tun. Map-Reduce for machine [5] M. Collins, R. Schapire, and Y. Singer. Logistic regress ion, AdaBoost and Bregman distances. [6] C. Cortes, M. Mohri, M. Riley, and A. Rostamizadeh. Sampl e selection bias correction theory. [7] J. Darroch and D. Ratcliff. Generalized iterative scali ng for log-linear models. The Annals of [8] S. Della Pietra, V. Della Pietra, J. Lafferty, R. Technol , and S. Brook. Inducing features of [9] K. Ganchev and M. Dredze. Small statistical models by ran dom feature mixing. In Workshop [10] D. Graff, J. Kong, K. Chen, and K. Maeda. English gigawor d third edition, linguistic data [11] E. T. Jaynes. Information theory and statistical mecha nics. Physical Review , 106(4):620630, [12] J. Jeon and R. Manmatha. Using maximum entropy for autom atic image annotation. In Inter-[13] G. Lebanon and J. Lafferty. Boosting and maximum likeli hood for exponential models. In [14] D. Lewis, Y. Yang, T. Rose, and F. Li. RCV1: A new benchmar k collection for text catego-[15] R. Malouf. A comparison of algorithms for maximum entro py parameter estimation. In Inter-[16] M. Marcus, M. Marcinkiewicz, and B. Santorini. Buildin g a large annotated corpus of English: [17] C. McDiarmid. On the method of bounded differences. In Surveys in Combinatorics , pages [18] J. Nocedal and S. Wright. Numerical optimization . Springer, 1999. [19] C. E. Shannon. Prediction and entropy of printed Englis h. Bell Systems Technical Journal , [20] K. Weinberger, A. Dasgupta, J. Langford, A. Smola, and J . Attenberg. Feature hashing for [21] T. Zhang. Solving large scale linear prediction proble ms using stochastic gradient descent
