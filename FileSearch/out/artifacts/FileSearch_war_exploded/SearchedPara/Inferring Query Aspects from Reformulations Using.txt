 When the information need is not clear from the user query, a good strategy would be to return documents that cover as many aspects of the query as possible. To do this, the possible aspects of the query need to be automatically iden-tified. In this paper, we propose to do this by clustering reformulated queries generated from publicly available re-sources and using each cluster to represent an aspect of the query. Our results show that the automatically generated reformulations for the TREC Web Track queries match up quite well with actual sub-topics of these queries identified by TREC experts. Moreover, agglomerative clustering us-ing query-to-query similarity based on co-occurrence in text passages can provide clusters of high quality that potentially can be used to identify aspects.
 H.3.3 [ Information Search and Retrieval ]: Query For-mulation Algorithms, Measurement, Performance, Experimentation. Query diversity, query reformulation, clustering, anchor text.
User queries do not always clearly represent the actual in-formation need. They can be ambiguous or underspecified. Ambiguous queries are those that have different interpreta-tions such as  X  X REC X , which might refer to the home page of Texas Real Estate Commission or Text Retrieval Con-ference. Underspecified queries are queries with a known interpretation that have different aspects or sub-topics that may be relevant. For example, the user submitting the query  X  X pple inc. X  might be looking for general information about the company or products of the company.

In order to deal with these queries, retrieval systems should retrieve documents that are relevant to different aspects of the query rather than a single, dominating aspect. Conven-tional information retrieval (IR) systems can do little for these types of queries since they rank documents without regard to possible meanings the query might have.
Dealing with these types of queries is the motivation for introducing diversity into search results, to make the set of returned documents cover as many aspects of the queries as possible. Existing work in this area generally relies on an intial retrieval and then selecting documents from the re-trieved set according to some criteria. These methods can be categorized as implicit and explicit . The implicit approach [2, 13] chooses documents different to those that have been previously selected without modeling the actual sub-topics of the query. The explicit approach, on the other hand, explicitly models aspects of a query using a taxonomy [1], relevant documents [3] or reformulations [10, 12]. Many of the explicit models, however, assume the availablility of the optimal aspect representation for a query, leaving their ef-fectiveness with automatically generated aspects unclear.
In this paper, we propose a simple method to generate re-formulations that represent possible aspects of a query from of which are publicly available. We first generate reformula-tions by using these resources, and then cluster them using different clustering algorithms with different similarity mea-sures. Our experiments show that many of the reformula-tions we generate for TREC queries in fact correspond well with their sub-topics as identified by TREC experts.
Even though many techniques have been proposed for query reformulation, most of them aim to generate queries that are more effective than the original query [7, 5]. Their effectiveness for providing reformulations that cover differ-ent intents is thus unclear. Therefore, instead of using these models, we use a rather simple technique to generate refor-mulations from publicly available resources including anchor text extracted from a web collection and the Microsoft N-gram Services.
A nchor text is known to be an effective feature for web search [9]. Previous researchers have observed the similarity between anchor text and queries [6, 5]. Therefore, in this paper, we treat each anchor text as a reformulation that can potentially represent one aspect of a query.

The web collection from which we extract the anchor text contains 500 million pages in English that were crawled from the web during early 2009. We extracted all pairs of anchor text and associated urls from the web pages in this collection.
Web pages are connected to one another via links, each of which is associated with some anchor text. A link is called internal if two connected pages are from the same domain and external if they come from different domains. Since most of the internal links are for navigation purposes, their associated anchor text is not very helpful. Typical examples of such anchor text are  X  X ome X  and  X  X ndex X . As a result, we only consider external links.

In order to reduce noise, we discarded anchors that con-tain non-English words and those that contain navigation-triggered words such as  X  X lick X ,  X  X ownload X  and  X  X ubscribe X . We also removed anchors that contain only numbers and stop words. Among the resulting anchors, we keep only those with frequency greater than 1 and are connected to at least two urls. The resulting anchor text collection contains 8 , 215 , 751 unique anchors.

For any given query, we use the top-M most frequent an-chor texts that contain all of its terms as its reformulations.
The Microsoft Web N-gram Services provide smoothed n-gram models built from document body, document title, anchor text and queries in the Bing query log separately. Each model gives the probability of seeing an unigram u coming after an n-gram n , or P ( u | n ).

For each query q , we obtain the top-M unigrams u with largest P ( u | q ). Each reformulation is formed by adding u to q .

We put all reformulations generated from the two sources above into a list L . Since we aim to use each reformulation as a query aspect, we keep only those with reasonably high frequency. Ideally, we can obtain this frequency from query logs. Because we rely only on publicly available resources, we approximate this frequency by the number of times the query appears in a web collection. Finally, we order the reformulations in L by their frequency and keep only the top-M, which will be the candidates for clustering.
The list of reformulated queries generated above are then clustered into groups, each of which is considered a coarse representation of a query aspect. The clustering is based on a measure of query similarity.
Since the queries are short, computing their similarity based only on the query words is not likely to be effective. Instead, we expand a query with documents that are likely to be relevant to it. Specifically, we represent each query q by the relevance model P q ( w | R ) [8] estimated from the top-10 documents returned by the query likelihood retrieval model for q .
 The similarity of two reformulations r 1 and r 2 is then the KL-divergence between their relevance models P r 1 ( w | R ) and P 2 ( w | R ). We also try the cosine similarity measure as an alternative to KL-divergence.
Since estimating relevance models for every reformulation is computationally expensive, we also examine a more effi-cient method based on passage analysis. The idea is that two queries are more similar if they co-occur often in the same text passages. Therefore, for every pair of reformu-lations r i and r j , we compute N i and N j  X  the number of passages in which each of them occurs, and N  X  the number of passages in which they co-occur. The similarity between r and r j is given by the Jaccard score:
We applied two standard clustering algorithms: K-Means and Agglomerative Clustering.
The algorithm initializes each of the K clusters with a random reformulation. It then iteratively partitions all re-formulations into K clusters in which each reformulation is assigned to the cluster that is most similar to it. The simi-larity between a query and a cluster is the average similarity between this query and all the other queries in the cluster. The algorithm terminates when the cluster assignment for reformulations does not change.
Agglomerative clustering has an advantage over K-Means in that we do not have to specify the number of clusters be-forehand. The standard algorithm treats each reformulation as a singleton cluster. It sucessively merges pairs of clusters that are most similar to each other until some creteria is achieved. In our experiments, the algorithm stops when the intra-cluster similarity drops below a certain threshold  X  . We use complete-link to compute the similarity between two clusters, which is the minimum pair-wise similarity between the two clusters.

This procedure generally produces a deep binary tree, which is unnessary in our case since we are not interested in the tree structure. Therefore, we collapse all leave nodes, which correspond to our generated reformulations, starting from the third level (excluding the root node) to their parent and use these parent nodes as our resulting clusters.
In our experiments, we use queries from TREC Web Track 2009 and 2010. This query set contain 100 queries. Each query comes with associated sub-topics identified by TREC experts. On average, there are 4 . 6 subtopics per query. For each query, we generate reformulations as described in Section 2. We evaluate the quality of these reformulations by judging how many of them correspond to the actual sub-t opics identified by TREC experts. Then, we evaluate the clusters provided by different combinations of clustering al-gorithms and similarity measures.
We used ClueWeb-09 category B as the web collection both for estimating the frequency of reformulations and es-timating the co-occurrence statistics. For frequency esti-mation, we found it too strict to require the exact query to appear in the document. Therefore, we relaxed this by counting the number of times all of the query X  X  terms co-occur within a windows of size 10 and used this as its fre-quency. For passage analysis, two reformulations are con-sidered co-occurring in the same text passage if all of their terms co-occur within a window of size 20.

We set the number of reformulations M = 100 in all of our experiments. As for K-Means, we empirically set K = 10.
As mentioned in Section 2, we put all reformulations gen-erated from different sources together for each query and keep only top-100 most frequent ones. Among these refor-mulations, 15% is exclusively from the anchor text, 76% is exclusively from the Web N-gram service and 9% is from both sources.

In this experiment, two graduate students independently judge each of those 100 reformulations to see if it corresponds to any actual sub-topics of the query. A reformulation is then labeled by the corresponding sub-topic, or  X  X one X  if it does not match with any of the sub-topics. The inter-agreement between our two judges is 94%.

An actual sub-topic of the query is considered covered if at least one of the reformulations corresponds to it. Fig. 1 shows the percentage of subtopics (averaged across all queries) covered by the top-N of the 100 reformulations with N vary-ing from 10 to 100. In general, the reformulations covers on average about 60% of the actual sub-topics, which is promis-ing considering these reformulations are acquired in a very simple way. This suggests that publicly available resources are very useful at identifying aspects of queries.
It is worth noting that the reformulations that do not correspond to any of the aspects are not necessarily bad. In fact, many of them represent valid intents that were not identified by TREC experts. We leave the evaluation of these reformulations for future work.
In this section, we tried different combinations of cluster-ing algorithms and similarity measures to cluster all refor-mulations we have generated. We expect the techniques to be able to put reformulations with the same label into the same cluster.

To evaluate the quality of the generated clusters, we use the Rand index (RI), a well-known cluster quality measure. It computes the percentage of decisions that are correct and is calculated as follows: where T P (true positive) is the number of pairs of reformula-tions with the same labels that are put into the same cluster, T N (true negative) is the number of pairs with different la-Figure 1: Quality of the generated reformulations in t erms of how many of the actual sub-topics of the queries they cover.
 Table 1: Quality of the automatically generated re-formulations.
 Judge-1 Agglo. 0.64 0.67 0.76 K-Means 0.5 0.55 0.59
Judge-2 Agglo. 0.63 0.7 0.73 K-Means 0.48 0.55 0.57 bels that are put into the same cluster, F N (false negative) is the number of pairs with the same labels that are put into different clusters, and F P (false positive) is the number of pairs with different labels that are put into the same clus-ters. Reformulations with the label  X  X one X  are ignored in this computation since  X  X one X  is not a topic. Table 1 shows the RI score that different combinations achieve.
The first thing we observe from Table 1 is that agglom-erative clustering consistently outperforms K-Means. The reason seems to be due to the fact that K-Means forces ev-ery reformulations to be in some cluster. This can result in unrelated reformulations being put into the same cluster. Once clusters are filled with unrelated reformulations, the centroids of those clusters are not very different from each other, making the cluster assignment in the next iteration unreliable. Agglomerative clustering only merges two clus-ters if they are very similar to each other, and has a lower chance of putting reformulations into unrelated clusters.
Secondly, Table 1 shows that the similarity measure based on co-occurrence is consistently better than those based on relevance models. It should be noted that most of the refor-mulations, especially those generated from the Microsoft N-Gram Services, are different to each other by only one word. The longer the original query, the less impact the augmented word has on the relevance model. As a result, the relevance models for these reformulations are more similar than they should be. The similarity measure based on co-occurence, on the other hand, is not affected as much by the length of the original query. Two reformulations are similar as long as their augmented words co-occur with each other and with Table 2: Example of clusters generated by agglom-e rative clustering for the query  X  X atellite X  { s atellite radio; sirius satellite radio; xm satellite radio } the original query. This gives the co-occurrence-based mea-s ure superiority over the other two.

Tables 2 presents an example of clusters generated by agglomerative clustering with the co-occurrence similarity measure for the query  X  X atellite X .
Existing work in query diversity focuses mainly on select-ing a diverse subset of documents returned by an initial re-trieval. Maximal Marginal Relevance (MMR) [2] is a well-known example of this approach. It sequentially selects doc-uments that are most different to those previously selected. Probabilistic versions of MMR have also been investigated [13, 4]. While these approaches only aim to select docu-ments that cover different topics, others model the query X  X  aspects explicitly [3, 1, 12].

Our approach is different to this existing research in that it works on the query side. It aims to generate clusters of reformulations, each of which represents one aspect of the query. In this preliminary study, even though we have not explicity diversified the set of reformulations, the cluster-ing has this effect. Since clustering aims to put similar re-formulations together, reformulations representing different aspects should be put in different clusters.

Query-side diversification has been investigated by Radlin-ski and Dumais [10]. However they use a proprietary query log whereas we examine the usefulness of publicly available resources. The most similar work to ours is that done by Radlinski et al. [11] in which they also infer queries X  intents. However, they also rely on proprietary query logs.
Our approach does have a relationship to document-side diversification. The aspects our method generates for queries can be used by models such as [1, 12]. In addition, these models search for candidates from a set of documents re-trieved for the original query. Our aspects are reformula-tions of the original query, and can be combined with the query to retrieve more potential documents for document-side models to work with.
In this paper, we have shown that reformulations for queries obtained from publicly available resources such as anchor text and Microsoft Web N-Gram Services match up well with the actual sub-topics of the queries. We then tested whether clusters of reformulations represent aspects, using different clustering algorithms and query similarity mea-sures. We found that agglomerative clustering consistently outperforms K-Means and the similarity measure based on co-occurence is not only more effecient but also works better than similarity based on relevance models.

We observe that many of the clusters we obtained have very high intra-cluster consistency. We plan to build aspect representation from each cluster and use these in retrieval experiments.

Since clustering puts similar reformulations together, it has the effect of diversification because diverse reformula-tions should go to different clusters. Therefore, it would be interesting to see how clustering performs compared to applying existing techniques such as MMR on the set of re-formulations.

As noted earlier, our method identified many interesting aspects that were not present in the TREC judgment. In the future, we will evaluate these reformulations, and identify how many of them represent valid aspects.
This work was supported in part by the Center for In-telligent Information Retrieval and in part by ARRA NSF IIS-9014442. Any opinions, findings and conclusions or rec-ommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsor. [1] R. Agrawal, S. Gollapudi, A. Halverson and S. Leong. [2] J. Carbonell and J. Goldstein. The Use of MMR, [3] B. Carterette and P. Chandar. Probabilistic Models of [4] H. Chen and D.R. Karger. Less is More: Probabilistic [5] V. Dang and W.B. Croft. Query Reformulation Using [6] N. Eiron and K.S. McCurley. Analysis of Anchor Text [7] R. Jones, B. Rey and O. Madani. Generating Query [8] V. Lavrenko and W.B. Croft. Relevance-based [9] D. Metzler, J. Novak, H. Cui, and S. Reddy. Building [10] F. Radlinski and S. Dumais. Improving Personalized [11] F. Radlinski, M. Szummer, and N. Craswell. Inferring [12] R. Santos, C. Macdonald and I. Ounis. Exploiting [13] C. Zhai, W. Cohen and J. Lafferty. Beyond
