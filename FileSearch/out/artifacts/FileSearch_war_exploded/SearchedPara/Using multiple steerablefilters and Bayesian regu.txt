 1. Introduction
According to numerous studies in psychology and psychiatry, faces are a very special source of inspiration. In particular, considerable attention has been paid to facial expressions, seen as an important way of information revealing the emotional state, intentions and beliefs in communication between persons.
In this context, Mehrabian said in Mehrabian (1968) that the impact of facial expressions on the listener in a  X  face to face conversation is much more important than the textual content of the message delivered. He shows that for a verbal message, 7% of the content is given by the meaning of words, 38% by the way according to which the words are spoken and 55% by facial expressions. This shows that fac ial expressions play a major role in human communications.

Facial expressions are generated by contractions of facial muscles, which results in temporally deformed features such as eye lids, eye brows, nose, lips and skin texture, often revealed by wrinkles and bulges. In the majority of research analyzing facial expressions, six universal emotions are considered (happiness, disgust, surprise, sadness, anger, and fear), to which is added the neutral expression ( Ekman and Friesen, 1987 )( Fig. 1 ).
Although facial expressions recognition has been studied by many researchers in the recent few years, the state of the art in this fi eld is not very rich compared to facial recognition. A survey of the research made regarding facial expressions recognition can be found in Pantic and Rothkrantz (2000) and Fasela and Luettin (2003) . Once a face is detected in the image, the corresponding region is extracted, and is usually normalized to have the same size. The next step is to extract facial features that describe the most appropriate representation of the face images for recogni-tion. Mainly, two approaches can be distinguished: the template-based approach and the feature-based one.

The template-based approach aims to establish a model for each expression from the geometrical facial features and distances between them (including mouth, eyes, eyebrows and nose). The most commonly used methods are based on line detection ( Gao et al., 2003 ), motion analysis from templates ( Kotsia and Pitas, 2007 ), active appearance models ( Abboud et al., 2004; Cheon and Kim, 2009 ) and facial action units (FAU) ( Ekman and Friesen, 1987; Panatic and Pitas, 2006 ). The detected points can be extracted in 2D or 3D ( Pantic and Rothkrantz, 2000; Dornaika and Raducanu, 2007; Venkatesh et al., 2009 ).

The feature-based methods use textural information as fea-tures for expression information extraction. Among the existing methods, we quote mainly the LBP ( Zhao and Pietikainen, 2007; Shan et al., 2009 ), SIFT ( Berretti et al., 2011 ), and Gabor ( Lyons and Akamatsu, 1998; Lyons et al., 1999; Liu and Wang, 2006 ).

This paper addresses these problems by proposing a new invariant textural descriptor based on steerable pyramids. A steer-able pyramid is a method in which images are decomposed into a set of multi-scale, and multi-orientation image sub-bands, where steerable fi lters introduced by Freeman and Adelson (1991) have attracted attention because they can analyze the contents of the image in many directions and at different levels of resolution. These fi 1995; Mahersia et al., 2007; Mahersia, 2010 ).

The remainder of this paper is organized as follows. We fi 3 .In Section 4 , we conduct some experiments in order to demonstrate the effectiveness of the prop osedmethod.Finally,in Section 5 we draw the conclusions and outline future work to be carried out. 2. Steerable pyramid representation
In signal processing, a signal can be decomposed into sub-bands, such as by wavelet transform. The wavelet transform is widely used in many applications since the pyramid structure of wavelets responds well to a human visual system. However, the two major drawbacks of wavelets are the lack of translation invariance, especially in two-dimensional (2D) signals ( Cheon and Kim, 2009 )andthepoor selectivity in orientation. To overcome this problem, the  X  linear combination of a set of basis fi lters, has been proposed ( Simoncelli et al., 1992, 1995 ).

The system diagram of a steerable pyramid for a single stage is shown in Fig. 2 .
 The pyramid is divided into two parts: analysis and synthesis.
On the analysis part, the image is decomposed into low-pass and high-pass sub-bands, using steerable fi lters L 0 and H 0 pass band continues to break down into a set of band-pass sub-pass sub-band is sub-sampled by 2 along the two directions x and y . Repeating the shaded area provides the recursive structure.
Due to its invariant properties, the pyramid structure of the ste-erable wavelet was used in texture analysis ( Montoya et al., 2007;
Mahersia, 2010 ). It captures textures in both structural and random aspects ( Portilla and Simoncelli, 2000; Mahersia et al., 2007 ). derivatives of an image in any direction can be interpolated by several basis derivative functions. As shown in Ref. Montoya et al. (2007) ,any K th-order directional derivative is a linear combination the fi rst-order steered fi lter in cartesian coordinates f arbitrary direction  X  can be easily interpolated and synthesized by taking a linear combination of the basis fi lters f 0 1 1 by the following equation: f  X  x ; y  X  X  cos  X   X   X  f 0 1 1  X  x ; y  X  X  sin  X   X   X  f 90 1 1  X  x ; respectively by Eqs. (2) and (3) f  X  x ; y  X  X  f  X  x ; y  X  X   X   X 
Fig. 3 shows the convolution result of the disk image with the f  X  x ; y  X  steered to the direction 30 1 .

As mentioned in the works of Freeman and Adelson (1991) , the steerable pyramid is best de fi ned in the Fourier domain where it provides a polar-separable decomposition, thus allowing indepen-dent representation of scale and orientation. Considering the polar-separability of the fi lters in the Fourier domain, the 2000 ): L  X  r ;  X   X  X 
H  X  r ;  X   X  X  where r ,  X  are the polar frequency coordinates.

The Fourier magnitude of the K th oriented band-pass fi lter is given in polar-separable form by the following equation:
B  X  r ;  X   X  X  Ang k  X   X   X  Rad  X  r  X  X  6  X  where k  X f 0 ; ... ; K 1 g .
 iterative stages, with radial and angular parts, de fi ned as
Rad  X  r  X  X 
Ang k  X   X   X  X   X  k cos  X  k where
Fig. 4 illustrates a three-level steerable pyramid decomposition of the Happiness expression, with k  X  3. Shown are the four orientated band-pass images at three scales and the fi nal low-pass image.
Finally, the steerable decomposition of the image I is given f 0 ; ... ; K 1 g , by the following equation ( Montoya et al., 2007 ):
O  X  x ; y  X  X   X   X 1 3. Facial expressions recognition with steerable pyramids
The proposed facial expression recognition system is constructed in fi 3.1. The pre-processing step
This step aims to prepare the face image to be decomposed with the steerable transform described in Section 2 by locating the face and removing all unnecessary details (i.e. background in fl do that, we fi rstly crop the images of size 256 256 into 168 120 as done in Buciu et al. (2003) , Dubuisson et al. (2002) , Shih and
Chuang (2008) ,and Shinohara and Otsu (2004) with respect to the upper left corner. Then, we reduce lighting variations by normalizing  X  i ; j  X  , M and STD denote the estimated mean and standard deviation of I at pixel ( i , j ). The normalized image is de fi ned as follows:
N  X  i ; j  X  X   X  I cr M  X  n C 1 STD  X  C
C and C 2 are two constants set experimentally to 50 and 100, image to obtain an enhanced facial image. 3.2. Feature extraction
To be able to correctly decide about the expression of the preprocessed face, we ensure two steps: a training process and a testing process. These steps are summarized as below: Training step : In this stage, known expression images are trained.
These images are preprocessed as described above and then decomposed by using a three-level steerable decomposition.
Then, norms like entropy, energy, K urtosis and standard deviation arecalculatedasfeaturesbyusingtheequationsgivenin Table 1 . These features are stored in the features' library.

Testing step : This stage is a testing stage. Here, the unknown facial image is preprocessed and then decomposed using the same steerable transform. Then, a set of features are extracted and compared with the corresponding feature values stored in the features' library. To achieve this comparison, we use multi-layer perception Bayesian regularization neural networks in order to train data set and classify features that are extracted using the steerable coef fi cients. 3.3. Facial expression classi fi cation
After computing the feature vector, we use it as an input to an arti fi cial neural network to classify facial images according to the expression they contain.

According to many researches made from 2005 till now, neural networks are considered as a powerful tool to recognize facial expressions ( Xiao et al., 2006; Tanchotsrinon et al., 2011; Kazmi layer, one single hidden layer, and one output layer. However, the us to use a 4-layer feed-forward neural network trained by a Bayesian regularization learning algorithm. Bayesian regularization has been proven to have good generalization properties when used in the training of the neural network. In our work, the networks are and targets Rudovic et al. (2013) .

The classical back-propagation algorithm looks for the mini-mum of an error function (given in Eq. (12) ) using the method of gradient descent  X  n  X  X  1 m  X  m the number of training data. Obviously, t j ( n ) is the desired of the neurone j at iteration n . More details about the back-propagation algorithm can be found in Rojas (1996) and Karlik et al. (2003) .

The Bayesian regularization has modi fi ed the given cost func-tion in order to improve the model's generalization capability. Hence, the objective function in Eq. (12) is expanded with the addition of a term,  X  w which penalizes large weights J  X  n  X  X   X  X  D  X  n  X  X   X  X   X   X  n  X  X  13  X  Thus, the objective function can be written as J  X  n  X  X  where N is the neurons' number and  X  and  X  are parameters to be optimized in Bayesian framework of MacKay ( Foresee and Hagan, 1997; MacKay, 1992 ) and called adaptive regularization hyper-parameters. The main problem with implementing regularization is setting the correct values for the objective function parameters. MacKay (1992) has done extensive work on the application of Bayes' rules to neural network training and to optimizing regulariza-our problem. However, we show later that the optimal regulariza-tion's technique requires costly computation of the Hessian matrix.
To overcome this drawback, approximations with the Leven-berg Marquardt optimization's algorithm are used in this work to locate the minimum point. The smaller  X   X  is, better will be the generalization capability of the network. So, if  X  c  X  , training overemphasizes weight size reduction while tolerating higher errors, but, if  X  c  X  ,over fi tting occurs.

Nevertheless, the common choice for  X   X   X  n  X  is the sum of squares of the network weights, as given by the following equation:  X  n  X  X  1 N  X  N
Here are the steps required for Bayesian optimization of the regularization parameters.

Bayesian algorithm for back-propagation networks. 1. Initialization : Set all the weights to random values 2. Forward computation : for each neuron j in layer l , 4. Weight updates : Calculate the posterior-probability 5. Iterations Repeat steps 2 through 4 until convergence
In addition, we use in our work, a 10-fold cross-validation procedure is applied to combat more and more over fi tting and improve the generalization ability of the back-propagation trained network. Thus, our input data are partitioned into three sets: training set, validation set, test set.

Each time, our model is trained with the training set and then veri fi ed with the generated validation set the until the validation error starts increasing. Hence, we stopped the training procedure and select the network with minimum validation error as the best model. Finally, the selected neural network, including best weights, is saved and then, will be used to classify the test set and achieve the generalization error.

On the other hand, the partitioning of the input data was performed randomly: we randomly divided the data set into ten folders. Each time, we use one folder as validation set, we train eight out of the 10 folders and test the remaining folder. Then, we averaged all the 10 recognition rates to obtain the fi nal perfor-mance of the proposed system. In order to create distinct data sets for cross-validation, none of the sets in the training folder appear in any of the remaining folders. Finally, every network was trained to give the maximum value of 1 for exact facial expression and 0 for all other expressions. 3.4. Databases used for experiments on two databases: Jaffe and Cohn  X  Kanade. These are the most commonly used databases to evaluate and compare the various recognition algorithms. Before going into experimental details and method. 3.4.1. Japanese female facial expression (JAFFE) database including surprise, fear, disgust, anger, happiness, sadness and neutral, for 10 Japanese female models ( Lyons et al., 1999 ). Each female has two to four examples for each facial expression. The size of each image is 256 256 pixels. An example of these categories is presented in Fig. 7 . 3.4.2. The Cohn  X  Kanade database frames ( Kanade et al., 2000 ) showing the evolution of each emotional expression from neutral expression in the fi rst frame to the highest intensity of the given expression in the last frame.
Fig. 8 shows some examples of this data set. However, in this database, few subjects have the seven expressions, thus, we have selected 16 students who have all the seven different facial expression. In this way, we have collected 457 images to be used in the cross validation process.
 and to provide at the same time a large amount of training data in terms of different expressions and poses, we have generated from these two databases, 426 Jaffe images and 910 Cohn  X  Kanade images by fl ipping each image as shown in Fig. 9 . All the experi-ments are carried out in MATLAB R2012a environment running on a desktop with CPU Intel Core i3 2.7 GHz and 4 GB RAM.
 4. Experimental results
To test the algorithm's generalization performance, we used 10-fold cross-validation. For that, the two databases were randomly split into 10 subsets of approximately equal size. Each subset is trained and tested 10 times; each time, one subset is reserved for the validation scheme, and one fold from the remaining ones is used for the test while the train procedure is performed on the data set minus the two previous folds.

The fi nal calculated cross-validation error is the average of the estimated errors from the 10 folds.

As we said before, the proposed model will be trained with the training set and then veri fi ed with the generated validation set until the validation error starts increasing. Then, the training procedure will be stopped, and we will test the testing set with the best neural network obtained with the best validation perfor-mances. An example of the validation check can be shown in Fig. 10 .

The average recognition rate is about 93.33% for the Jaffe database and 98.13% for the Cohn  X  Kanade database ( Table 2 and
Fig. 13 ). These rates are obtained with four levels of decomposi-tion, six directions (0 1 ,30 1 ,60 1 ,90 1 ,120 1 and 150 vector calculated in each steerable sub-band by the mean of energy, variance and kurtosis, which leads to a feature vector that contains 6 4 3  X  72 attributes. These features were classi a neural network classi fi er with seven outputs coding the different emotional categories. We used a 4-layer feed-forward neural network, including 72 neurons in the input layer(  X  the length of the feature vector), two hidden layers with 14 hidden neurons in each one, and seven output neurons in the output layer (  X  the number of expression categories). We used the Bayesian back-propagation algorithm to train the network and 10-fold cross-validation to determine the true test performance.
 (Columns represent the emotion selected by our method for samples belonging to the emotion of each row). 4.1. Comparison with different norms and different hidden neurons meters such as norms used for the attribute extraction (energy, variance, kurtosis and entropy) and the optimal number of the hidden neurons. Optimum values of these parameters are deter-mined via experiments during the validation process. The testing range for the hidden neurons is chosen to be from 7 to 28, whereas the features used in our experiments are L 1  X  [ e 1 e 2 ], L Kanade databases are shown in Figs. 11  X  13 .
 most dif fi cult expressions to classify, whereas happiness and surprise are the easiest to recognize. 4.2. Comparison with different activation functions play an important role in the convergence of the learning algo-rithm. In the literature, the most used functions are the logistic sigmoid function and hyperbolic tangent function. Nevertheless, none of the mentioned studies make any kind of comparison regarding facial expressions recognition.
 performances of neural networks tested with three activation functions: logistic sigmoid function ( logsig ), hyperbolic tangent about these functions can be found in Rojas (1996) .
 layered network structures, were evaluated to compare their considered 14 then 28 hidden neurons in each hidden layer.
Table 5 and 6 present respectively the classi fi cation results and the computational time of the training process for the two databases Jaffe and Cohn  X  Kanade.
 tion functions, adapted to one-and two-hidden layered network structures, were evaluated to compare their effectiveness for classi fi cation. As can be observed, the tangent hyperbolic activation function reached better results, caused faster conver-gence of the learning algorithm and gave the highest overall accuracy in 2-hidden layered network. 4.3. In fl uence of the pre-processing step
Image pre-processing is an important step before applying any other technique on images. To prove the importance of this step, we tested the effectiveness of our proposed algorithm with and without the pre-processing phase. The results are shown in Table 7 .

Table 7 shows that the pre-processing operations help us to improve the recognition rate without increasing computational complexity. As can be observed, almost 98% of the total run time is devoted to the learning step whereas the time consumed by the pre-processing step does not exceed 0.68% of the overall run time. The total run time is computed for all the images in the database (213 images in Jaffe database and 457 images in Cohn  X  Kanade database).
 4.4. Comparison with other works
To show statistical robustness of the proposed method, we compare our facial expressions' recognition rate with the other recognition's rates obtained with methods listed in Buciu et al. (2003) , Dubuisson et al. (2002) , Lyons et al. (1999) , Shih and
Chuang (2008) , Shinohara and Otsu (2004) , Tai and Chung (2007) , and Zhang et al. (1998) that used the Jaffe database. The perfor-mance comparisons are given in Table 8 .

In order to give the reader a concrete feeling of the proposed results, we present some examples in Fig. 14 and 15 where the original labeling expressions and our neural system outputs are both shown. These two fi gures show that our system has correctly labeled the given expressions for the two databases Jaffe and Cohn  X  Kanade.

Fig. 16 compares our results and those presented in Zhang et al. (1998) . In this work, the authors mentioned some cases in which, their system does not agree with the labeling given in the database because the person may have posed an incorrect expression.
Despite this, we can notice that our system succeeds to recognize these ambiguous expressions.

As shown in Fig. 13 , disgust is not easily identi fi ed as happiness and surprise. Fig. 17 shows two cases, where our system failed to recognize the emotion of disgust. Nevertheless, we cannot com-pare these cases because authors who have used them do not mention the obtained labels. 5. Discussion and conclusion
In this paper, a novel method is proposed for facial expression recognition, which combines steerable pyramid transform to extract expressions' features and neural networks to recognize these expressions. Experimental results demonstrate that our proposed method can provide robust features, and achieve good accuracy. From the experimental results, we make the following observations: neurons and two hidden layers, is 93.33% with the Jaffe database and 98.13% with the Cohn  X  Kanade database.
 References
