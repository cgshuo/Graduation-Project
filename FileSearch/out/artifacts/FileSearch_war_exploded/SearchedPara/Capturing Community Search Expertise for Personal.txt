 We describe and evaluate an approach to capturing and re-using search expertise within a community of like minded searchers, such as the employees of a company or organisa-tion. Within knowledge based industries, search expertise X  the ability to quickly and accurately locate information ac-cording to a specific information need X  X s an important cor-porate asset and in our approach we attempt to capture this knowledge by mining the title and snippet texts of results that have been selected by community members in response to their queries. Our assumption is that the snippet text of a result must play a role in helping users to judge the initial relevance of that result and that the snippet terms of selected results must contain especially informative terms about the goals and preferences of the searchers. In other words, results are selected be cause the user recognises cer-tain combinations of terms in their snippets which are re-lated to their information needs. Our approach seeks to build a community-based snippet index that reflects the evolving interests of a group of searchers. This index is then used to re-rank the results returned by some underly-ing search engine by boosting the ranking of key results that have been frequently selected for similar queries by commu-nity members in the past.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Design, Human Factors Personalization, Community, Web Search, Snippet
This material is based on works supported by Science Foun-dation Ireland under G rant No. 03/IN.3/I361.
 Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00.
The internet has become a vital tool for locating infor-mation in the modern world, not just as a global platform for business but also as a tool for companies and organ-isations that rely on fast access to vital information and services. Knowledge workers, in particular, have come to rely on the Internet as a key source of information. In 2001, 66.8% of Americans aged 25 years and over who were em-ployed in a managerial or professional speciality regularly used the internet and email as part of their work 1 ,andwith the phenomenal growth of the internet in recent years we can only assume that this proportion has since grown. In fact, in many scenarios a person X  X  ability to use the Internet effectively in their job is an important skill and the most productive workers are those able to locate relevant infor-mation quickly and efficiently. As always, some people are more skillful than others in this regard. Obviously Web search engines play a key role when it comes to locating rel-evant information and so understanding how to search effec-tively is a vital skill for today X  X  knowledge workers. For this reason, employees who have developed superior Web search expertise can be extremely valuable in an organisation and their search expertise can be a important source of corpo-rate knowledge. Traditionally this type of expertise remains hidden within the organisation as it is obviously bound to the individual, but if this information can be captured and shared within the organisation then it can help others to search more effectively and become more productive. This can help to accelerate the training and development of new employees and novices within an organisation and also helps the to protect and maintain this vital source of corporate knowledge if individuals leave the organisation.

In this paper we describe a novel technique for enhanc-ing traditional Web search engines in two important ways. First, we demonstrate how our technique allows a traditional search engine to adapt to the search preferences of a com-munity of like-minded searchers and thus deliver results that are more relevant to their precise needs, even in the face of vague and ambiguous queries. Second, we demonstrate how this technique works by capturing and leveraging the search history of a community of searchers and so maintains a powerful repository of search expertise. In this paper we propose that this approach to search  X  which we will refer to as collaborative Web search (CWS), although it is actually
Source: NTIA and ESA, U.S. Department of Commerce, using U.S. Census Bureau Current Population Survey Sup-plements a significant extension of the original CWS research [24]  X  is especially well suited to enterprise search tasks. In many organisations employees naturally serve as a community of like-minded individuals since during the normal course of their work they will often search for broadly related infor-mation. In addition, improving search productivity within an organisation can offer significant benefits, especially in relation to employee productivity. Moreover, providing or-ganisations with a facility to capture, share and leverage search expertise can help to provide access to a vital source of corporate knowledge.

Before describing the details of our approach to collabo-rative Web search (Section 4 ) we first discuss some of the motivations behind our research and highlight some of the conceptual differences between our technique and the orig-inal CWS research. Next, in Section 3 we describe recent research in a number of areas closely related to our own re-search. In Section 5 we demonstrate our approach in prac-tice through an example search session and following this we describe the results of a recent evaluation study involving a live-user trial in a corporate setting.
The fact that typical Web queries contain an average of only 2-3 terms [11] and that queries can be vague or badly formed often leads to Web search engines performing poorly in comparison to searchers expectations. For example, a query like  X  X ordan pictures X  offers no clues about whether the searcher is likely to be looking for images of the rac-ing team, the country, the basketball star, or the model. A searcher has a context within which they submit such a query, and without this context being explicitly expressed in the query a search engine will re turn many results which are not relevant for the searcher. Approaches which attempt to personalize the selection and ranking of search results offer one possible solution [16]. By learning about the the context of their search it may be possible to prioritise certain results that are more likely to be relevant. In the case of a commu-nity of searchers that share similar information needs, the search knowledge expressed through previous search behav-iour within the community may help to define the context of the community. In the above example, if the the searcher is an employee of a Formula One magazin e publisher then similar searches carried out by other employees will likely fo-cus on Formula One related results, and so by capturing this information it should be possible to prioritise those results related to motor racing, e.g. images of the Jordan Formula One team. In addition, results which have been selected by members of the community in the past can be promoted in new searches for similar queries, reducing search time and re-using the expertise involved in previous searches. So go-ing back to our example, if a particularly good quality image of the Jordan Formula One team was previously found for a similar query in the past, by another employee of the For-mula One publication, then this useful and relevant result can be promoted in the list of results returned to the current searcher.

Personalizing search results introduces a number of chal-lenges and issues that need to be addressed if it is to prove useful in practice. Certainly there are key challenges when it comes to understanding how personal information and search context can be captured and reused for result se-lection and/or re-ranking. In addition, there are of course significant privacy issues, especially if a given solution in-volves profiling individual users. In our approach to person-alized Web search we leverage the past search behaviours of a community of like-minded users (e.g., the employees of a company) in order to post-process the results returned by a traditional search engine with a view to identifying certain results for promotion. It avoids the need to store individ-ual user profiles and so has the added benefit of protecting the privacy of individual searchers; the search behaviours of a community of searchers are anonymously combined to produce a single community-based profile.

The work described in this paper has been inspired by previous research on Collaborative Web Search (CWS) [24] which highlighted the high degree of query repetition and result selection regularity that naturally exists within com-munity based search scenarios. Briefly, CWS maintains a community search profile by recording the queries submit-ted and the results selected by community members. When faced with a new target query, CWS promotes results that have consistently been selected for this and similar queries by the community in the past. The work presented here pro-poses a more elaborate community model by maintaining a community-based snippet index as a way to drive promo-tions. Thus, instead of simply storing the queries and the URLs of the pages selected, we produce a local index based on the terms that are contained in the (query-sensitive) snip-pets of selected results; we assume that these snippets are produced in a sensible fashion by the underlying search en-gine. In this way we can capture more than just the fact that the searcher is interested in a particular result for her query. The snippet text of the selected result must contain certain terms that proved to be especially informative to the searcher and re-indexing the result using these terms is a way to emphasise those aspects of a result that are likely to be most interesting to the searcher X  X  community at large.
When our Formula One publication employee is searching for  X  X ordan pictures X  she is likely to select results related to the racing team. These results will contain terms such as F1, Formula One, Eddie Jordan, Grand Prix etc. in their snippets, and by re-indexing the result using these special terms we can make the result available for promotion in the future for a whole host of Formula One related queries, including queries where there is no term overlap with the source (  X  X ordan pictures X  ) query. Incidentally, the version of CWS described by [24] would not be able to recognise this page as relevant to a new query unless there is a direct term overlap between the new query and a query that resulted in the selection of this page. This inevitably limits the situ-ations when community-based promotions can be made by CWS and one of the contributions of this work is to address this limitation while ensuring that promotions continue to be relevant and informative.
In this paper we have identified two important areas of related work. First, there have been a number of research efforts devoted to making Web search more context-sensitive or personalized so that search engines may respond in more intelligent ways to the type of vague, context-free queries that are commonplace. We briefly summarise a number of such approaches and highlight the collaborative Web search approach to personalization that serves as the starting point for this work. In addition, given that our approach to per-sonalizing Web search relies on the local re-indexing of re-sults that appear to be relevant to a community of searchers, we also look at related work from the indexing literature, fo-cusing on a number of recent efforts to use query terms and snippet texts as the basis for re-indexing.
Many searches fail because the queries lack vital informa-tion. For example, most queries fail to include terms that usefully describe the search context or the preferences of the searcher. Consequently, researchers have recently focused on ways to exploit context during search, either by explicitly es-tablishing context up-front or by implicitly inferring it. For example, the Inquirus 2 meta-search engine [7] supplements keyword-based queries with a context category; users explic-itly select from a set of categories such as  X  X esearch paper X ,  X  X omepage X  etc. Alternatively, implicit context can be auto-matically inferred. For example, systems such as Watson [4] take advantage of user activity prior to the search to judge context; Watson monitors a user X  X  word processing activi-ties and uses document text as the basis for query terms. In contrast, relevance feedback techniques attempt to use actual search results to inform context. For example, [13] extracts correlated terms from top-ranking search results to focus context on the most relevant search results as opposed to the entire set.

Information about the local context of the query is just one way to supplement vague query terms. Another ap-proach involves collecting and using information about a searcher X  X  personal preferences, as they develop over time, in order to provide a more focused set of results that are likely to address the searchers long-term and short-term interests. A wide range of implicit user activities have been proposed as sources of information, including the users X  query history [22, 25] and browsing history [14, 26]. The major commer-cial Web search engines are also now offering personalized search based on user profiles learned from individuals X  search history [8, 27].

As discussed above, the collaborative Web search [24] ap-proach is especially relevant to the work presented in this paper. Collaborative Web search is actually a form of meta-search [5] that focuses on personalizing results for the pref-erences of a community of like-minded users rather than individual searchers. As already mentioned, it does this by maintaining a community profile consisting of the queries that community members use and the results that they se-lect. When presented with a new target query, collabora-tive Web search retrieves results from its community history which have previously been selected for the target query or for queries that are similar (by virtue of term-overlap) to the target query. These results are added to the result-list produced by the meta-search component so that they ap-pear as community promotions; results that are judged to be especially relevant to the community. The work of [23] has shown that this form of personalized Web search can prove to be very effective in many search scenarios where natural communities (formal or ad-hoc) of searchers can be identified. However, as mentioned in the previous section the technique is restricted in the promotions it can make which is a limitation that we will attempt to overcome in the work presented in this paper.
The use of snippets for document indexing in IR was sug-gested as early as 1958 [12], and more recently work by [20] has looked at generating an alternative index using generic document summaries which can then be queried in parallel to a full content index or used as a source for pseudo rele-vance feedback. The key insight is that these generic sum-maries (which are query independent) express the content of a document in a more concise form than the original text and this can be used to improve retrieval effectiveness. Our approach is different in that we use query-sensitive docu-ment snippets which summarise a document for a particular searcher according to their search query. The alternative index we generate from these is based on past search behav-iour and is specific to a searcher, or in the context of CWS, a community of searchers and we can use this personalized approach to improve retrieval effectiveness.

The work of [10] on document transformation suggests modifying indexed document content according to previous selection behaviour in order to bring the documents closer to the queries that led to their selection. The query terms for which a document is selected are added to the indexed term vector for the document, essentially weighting those terms in the document. Over time, this allows the document to drift towards the query terms for which it was successfully selected in the past. Again our approach is different in that we create a new personalized index for a specific community without altering the existing full text index and this enables our approach to be applied as a personalized meta search engine on top of existing Web search engines. We also use query-sensitive snippets which provide a far richer set of terms than the query terms alone, and instead of just adding these terms to the existing full text document representation we create a whole new document representation using these terms.
In this work we believe that the key to understanding how to personalize search results for the needs of a user or group of users depends critic ally on our ab ility to under-stand their preferences. In what follows we will describe our snippet-based approach to indexing the search patterns of a community of like-minded searchers. Apart from for-mally defined search communities such as the employees of a company or organisation that we mentioned previously, communities may be defined by more ad hoc associations such as the visitors to a Web site specialising in wildlife and endangered species for example. The interested reader is referred to the work of [24] for a more complete treatment of the origins of search communities.
Intuitively, we can assume that when a rational user se-lects a result from a result-list it is usually an indication of their interest in the result in question; result selections indicate relevance. There must be something about this re-sult that has led the searcher to believe that it will answer their information need. As a consequence, in the past re-searchers have often made the reasonable assumption that by analysing the content of a selected result it will be pos-sible to learn something about the user X  X  information need. This has led to various approaches to relevance feedback for re-ranking based on analysis of the term distributions in the content of selected results [18].

But surely an analysis of the entire result content is jump-ing the gun? After all, at selection time the searcher has not had any opportunity to review the result as a whole. Typ-ically, they will only have had an opportunity to review a condensed summary of the result in the form of a result snip-pet. This suggests an alternative feedback strategy, one that involves a more limited content analysis that is focused on the snippet terms only [21]. After all, snippet texts need not be representative of the result document as a whole. Snip-pet texts are usually query-sensitive summaries that often emphasise just one aspect or region of the result, an aspect that may not feature prominently in full-content analysis of the document as a whole.

In this work we focus on the snippets of selected results as a key source of relevance information for a community of searchers. However, unlike traditional approaches to rele-vance feedback, which usually only accommodate a local re-ranking of the current result-list, we are interested in min-ing community selection behaviours in order to produce a persistent record of searcher interests, so that we may influ-ence the ranking of future searches by community members, where appropriate. To do this we construct a secondary document index that is personalized for the interests of a community of searchers based on an indexing of the snip-pets of their selected results.
Our approach to community-based, personalized search borrows heavily from the origina l collaborative Web search work of [24] and our system architecture is presented in Fig-ure 1. Consider some user u , a member of some community C . A new target query q T from u is initially answered by a traditional meta-search engine to produce a result-list, R In parallel, q T is used to query a local document index, I that has been constructed from the snippet texts of results by the community in the past. This produces a new list of results, R C , that are more closely aligned with community interests, R M and R C are combined and returned to the user as R T . Typically this combination involves promoting prominent results from R C ahead of those in R M .Inthe following sections we will discuss the construction and main-tenance of the local snippet index from community search behaviour and how new results are selected and ranked to produce R C .
We use ( C, u, q T ) to denote a search for query q T by user u in community C . Consider a result r selected in response to such a search; we say, selected ( C,q T ,r )= true .Wecan reasonably assume that the snippet for this result s ( r, q must contain terms t 1 ,...,t n which are of special interest to the user in relation to their query. Therefore, s ( r, q can be used to represent the document corresponding to r for ( C, u, q T ). In this sense s ( r, q T )isa surrogate for r in the context of ( C, u, q T ) and thus we propose that r can be indexed by using the terms contained within s ( r, q T ).
Accordingly, our approach to collaborative Web search involves constructing a community-based index by indexing Figure 1: The basic architecture for our version of collaborative Web search overlays a community based re-ranking on top of a traditional form of meta-search. Thus, for a user u in some commu-nity C the results of an initial meta-search, R M ,are revised with reference to the community X  X  snippet index, I C to produce a new result-list, R C ,thatis adapted to community preferences. R M and R C are combined and returned to the user as R T . each selected result by its snippet terms. In general then, given that a result r might actually be selected for a number of different queries, q 1 ,...,q n , it will come to be indexed under a number of different snippets, s ( r, q 1 ) ,...,s ( r, q Thus, for a given community of searchers each document will come to be represented by its surrogate, S C ( r )asshown in Equation 1
This is illustrated in Figure 2, which shows an example result list in response to ( C, u, q i ). Results r 1 and r selected and their surrogates are updated accordingly. In this case r 1 has only ever been selected for q i and so its sur-rogate S C ( r 1 ) is made up of the single snippet s ( r contrast, r 3 has been selected previously for another query, q and so its surrogate, S C ( r 3 ), is made up of the combi-nation of s ( r 3 ,q i )and s ( r 3 ,q j ). Both of these results are indexed under the terms that are contained in their surro-gates in the local community index. Result r 2 has never been selected by this community and so it does not appear in the local index I C .

The essential point of all of this is that it allows each document or result page to be represented differently for different communities of users. In turn the representation that is used for a given document and a particular commu-nity is designed to reflect the various parts of the document that have led to selections by this community X  X  members; these parts are the snippets that have been used to display the result during searches. Figure 2: Documents that have been selected by some community of searchers are represented by surrogates that are indexed locally by the snippet terms for these documents.

Documents that are broadly relevant to a community X  X  in-terests are likely to be retrieved for a wide variety of queries and are likely to be selected for many of these queries. As a result their document surrogates will cover a significant portion of the document X  X  contents and the snippet index will reflect this by associating the document with a broad set of index terms. In contrast, we might consider other documents that are only relevant to a community through some small part of their contents. These are more likely to be retrieved for a much more restricted set of query terms and their snippets will also be drawn from a limited subset of their content so that their index terms will also be very limited.
As mentioned above, when a new query q T is submitted by some member of a community, it is dispatched in paral-lel to the meta-search component and the community-based relevance engine. The former combines the results of a num-ber of underlying search engines to produce a meta-search result-list R M whereas the latter looks to produce a comple-mentary result-list, R C , made up of those results that have previously been selected by the community for some related queries.

We discussed briefly in Section 3.1 how the traditional ap-proach to CWS [23] produces R C by simply combining all results that have previously been selected for a set of related queries q 1 ,...,q k such that these queries share some mini-mal overlapping terms with q T . If this minimum is set too low then unrelated queries will be included and irrelevant results may be promoted. Increasing the minimum overlap threshold, however, will limit CWS to focus on a potentially restricted set of queries, missing some queries that are likely to be relevant. Sometimes two queries will be related and yet will contain no common terms; e.g.  X  X aptain Kirk X  and  X  X tarship Enterprise X  ; see also related work by [2, 19] for related work on alternative approaches to evaluating query relatedness and similarity.

Our new approach of maintaining a local snippet index has the potential to provide a solution to these problems. The results that make up R C are now those results which are indexed under the terms that occur in q T . This means, for example, that a result r , which was previously selected for the query  X  X aptain Kirk X  , can potentially be returned in response to the query  X  X tarship Enterprise X  if the terms  X  X aptain Kirk X  occurred in the snippet of a result previously selected in response to the query  X  X tarship Enterprise X  .
In the current implementation of our snippet indexing technique we use Lucene 2 to perform the basic indexing and retrieval over the local snippet index, I C .Thus,each community-selected result is indexed as a vector of its snip-pet terms weighted using a standard TF-IDF weighting model [17]. To keep the IDF values up to date, the snippet index has to be re-indexed after every new community-selected re-sult is added, however in order to improve efficiency this re-indexing could be performed less frequently. At retrieval time, Lucene queries I C ,using q T to produce a ranked list of results that will ultimately make up R C . The ranking function we used in Lucene is based on the TF-IDF scores of the index terms matched against q T so that results which match q T on more discriminating terms are preferred. However, this is not the only information that is available when it comes to ranking the results in R C .Our approach also records frequency information about the selec-tion patterns of a community, including the number of times that a given result has been selected for a given query. This is equivalent to the hit-matrix information used in standard CWS [24]. The hit-matrix is simply a matrix of hit counts H such that the entry in cell H ij represents the number of times that result r j has been selected by the community in response to query q i . This allows us to compute an ad-ditional relevance score for each result in R C according to Equation 2. Thus, the relevance of result r j to query q the proportion of times that r j has been selected for q i ative to the total number of selections that have been made for results retrieved for q i . So results that are regularly se-lected for a given query are preferred to results that are only sometimes selected.

The relevance of a result to the current target query q T can be computed directly in this way, but of course a result may have been previously selected for a range of different queries, q 1 ,...,q n so we need a way to combine these differ-ent relevance scores. The simplest way to do this is to use the similarity between the target query and each q i ,which the result has been selected for, as a way to discount each relevance score. Query simila rity can be calculated using the standard term overlap met ric based on Jaccard X  X  coef-ficient (see Equation 3), which has previously shown to be relatively effective as a query similarity metric for CWS in [3].

Equation 4 then shows how we combine the TF-IDF and relevance scores of a result r j from R C to produce its fi-nal combined relevance score ; q T is the current query and q ,...,q n are the related queries for which promoted result r was previously selected. In short, the overall relevance of a result is based on its TF-IDF score from the local snippet http://lucene.apache.org index boosted by its selection relevance over those queries that are similar to the target query.
 In our current implementation R T is the union of R C and R M with the R C results returned before the R M results. Thus the precision of R C is critical and for this reason we use two techniques to filter R C to enhance precision; that is, in addition to the usual stop-word removal and stemming during indexing and retrieval.

First, we threshold the proportion of query terms that must be present in the document surrogate for that docu-ment to be retrieved as part of R C . This effectively elimi-nates results that match on only a few of the query terms and can help to eliminate superficial results from being re-trieved. By default we set this threshold to allow for the re-trieval of results that match at least 50% of the query terms. Second, we limit the total number of results returned in R to ensure that community promotions do not over-power the traditional meta-search results. Normally we set this limit at 5-10 promotions.
In our current implementation we use Google and Ya-hoo as the underlying search engines for the meta-search. By way of an example, Figure 3 shows the top results of a search for the query  X  X un information X  by an employee of a local software company when the system was newly in-stalled; that is, prior to the population of the community index. The top results shown are a mixture of results re-lating to the solar system and various products and services from Sun Microsystems as returned by the standard Google and Yahoo search engines, and many are unlikely to be rel-evant to the searcher.

Figure 4 shows the same search carried out some time later, after the community of company employees has been exposed to a more significant amount of search activity. This time there are 2 promoted results returned both of which are clearly related to Sun Microsystems system administration. These results have been promoted because the community X  X  local index suggest that they are related to the target query; both have previously been selected when their snippets con-tained the query terms.
So far we have described a technique for personalizing search results relative to a community of like-minded searchers based on a combination of local snippet indexing and selec-tion based relevance. We have suggested that this technique canbeusedto promote certain results in a result-list as a way to compliment the  X  X ne size fits all X  results returned by some underlying search engine or combination of search en-gines. In this section we describe the results of a preliminary evaluation designed to test whether or not the promoted re-sults add value to the standard result list.
 Evaluating this work has proven to be extremely difficult. The standard IR test collections, such as those provided Figure 3: Result list without promoted results.
 as part of TREC [1], are not adequate. In particular, our technique relies on the availability of behavioural search data for a community of searchers, including the queries they have used, the results that they have selected, and the snippets associated with these results. To the best of our knowledge such a data collection is not readily available. Even if this type of data set was available it would still not meet our evaluation requirements because we also need to evaluate the response of live-users to our search promotions.
The evaluation approach we have adopted goes some way to addressing these issues. We have run a live-user trial of our search engine (snippet-based indexing on top of a Google plus Yahoo meta-search engine) for a community of related users, the 60 or so employees of a local software firm aged between 20 and 60 and with responsibilities ranging from software engineering to administration; the assumption be-ing that much of their search activity will relate to the core business of the firm and as such would exhibit the repeti-tion and regularity expected in a community-based search scenario.
To begin with we populated our local snippet index by replaying the search sessions of the community over a 14 month period directly preceding the trial. This information had been captured previously and while this sounds like a significant source of search data it is worth noting that the interests of the community are likely to have shifted over this period of time and thus not all of the data is likely to prove as relevant as it might once have been. More importantly perhaps it is also worth highlighting this data actually re-flects only about 10% of the search activity during this time, which was due to limitations in the data that was available to us. Nevertheless we felt that this 10% would serve to provide a representative sample of the real search behaviour of the community members.

It has been shown previously that searchers exhibit a nat-ural bias towards selecting the top ranking results over lower ranked results when presented with an ordered list of search results [9]. This means that, for the purposes of this trial, it is not ideal to exclusively present the promoted results ( R ahead of the meta-results ( R M ) in the final result-list ( R because it may bias the selections of searchers during the trial in favour of the promoted results. To remove this bias we chose to interleave the promoted and meta-results in the final result-list, alternating whether or not the first result was a promoted or meta-result from session to session. So, in a session where there were n promoted results ( | R C | we interleaved these results with the top n meta-results to effectively display 2 n promoted results to the searchers fol-lowed by the remaining meta-results.
During the trial period, the users were asked to use our search engine in the normal way and their selection patterns were logged. The results presented below were gathered at the end of a 2-week trial period and accounted for a total of 430 search sessions. The trial was divided into two weeks to test different promotion thresholds. During the first week we set the query overlap threshold to &gt; 50% (thereby allowing results which shared over 50% of the target query terms to be retrieved from the snippet index) and limited the number of promotions to 20 ( max ( | R C | ) = 20). During the second week we increased the query overlap threshold to 100% X  X o that only results that included all of the query terms could be retrieved from the snippet index X  X nd we set max ( | R C to be 5. Our expectation for week two was to see a fewer number of (hopefully) higher quality promotions.

How then can we assess the performance of our search en-gine during the trial? Obviously the IR community has a very mature view on evaluating IR performance with rela-tively robust metrics such as precision and recall considered best practice when it comes to summarising result-list rel-evance. However, in our trial it is difficult to assess result relevance directly, short of asking searchers to provide ex-plicit relevance feedback, which we chose not to do. Instead, we adopted a simpler version of relevance by proposing that the selection of at least one result in a given search session acts as a crude, but nevertheless useful indicator of result-list relevance; see also [23] for a similar evaluation method-ology. We refer to a search session where at least one result has been selected as a successful session .Ifnoresultsare selected (a failed session ) then we can be relatively confi-dent that the search engine has not retrieved a result that is obviously relevant to the searcher. Note that we do not distinguish here between sessions with different numbers of selected results, mainly because it is not possible to conclude much from the frequency of result selections. For example, one might be tempted to conclude that users selecting more results is a sign of increasing result relevance, except that a similar argument can be made in support of decreasing result relevance, on the basis that the initial selections must not have satisfied the users.
The results of the trial, and the difference between its two stages are summarised in a number of key metrics, presented in Table 1. First of all we notice that during week one, 48% of the search sessions led to community-based result promo-tions. In other words, for the 246 queries submitted by the participants in the trial in week one, 118 of these queries led to result-lists which contained at least one promoted result; we call such a session a promoted session . Indeed, these 118 promoted sessions contained on average 5 promotions. As expected the frequency of promotions drops sharply for week two of the trial because of the more stringent pro-motion thresholds used during this week. The percentage of promoted sessions drops by just over 37%, from 48% to 30%, and the average number of promoted results per pro-moted session drops by 42% from 5 to 2.9. We also notice different overall success rates between the two weeks. Dur-ing week one, 41% of all sessions are successful, where as during week two this rises to 60%. Most likely part of this increase can be explained by the availability of additional search history (the week one search data) during week two, but as we will see there is strong evidence to suggest that the more stringent promo tion thresholds also contribute to this increase by eliminating potentially low-quality promotions from being made.
These results show that our snippet-based approach to promotion is capable of generating a significant number of community-oriented promotions, but they remain silent on the critical issue of result quality. Using our simple no-tion of successful sessions we notice that week one generated Metric Week 1 Week 2 Total Sessions 246 184 Overall Success Rate 41% 60% Promoted Sessions 118 (48%) 55 (30%) Avg Promotions/Session 5 2.9 Successful Promoted Sessions 63% 69%
Successful Non-Promoted Ses-sions Successful Sessions with Only
Promoted Sessions Selected a 63% successful session rate among its promoted sessions compared to a 45% success rate among the non-promoted sessions. In other words, during week one, when promotions were made, users found at least one result worth selecting in 63% of these sessions. But when no promotions could be made, and only meta-results could be returned, then the success rate decreased to only 45%. Thus the presence of promotions led to a relative increase of 40% in terms of successful sessions. Similar results were noted for week two. This time 69% of the promoted session proved to be success-ful, compared to only 57% of the non-promoted sessions, a relative increase of 21%. Moreover, this 69% represents a relative increase of about 10% compared to the promoted session success rate of week one, suggesting that even though less promotions were generated during week two, the more stringent thresholds did result in higher quality promotions.
In the above section we compared the success rates of the promoted sessions to those of the non-promoted sessions. This is perhaps not such a fair comparison, however. Af-ter all, by definition promoted sessions are different types of sessions than non-promoted sessions. Promoted sessions contain promotions, non-promoted sessions do not. More importantly, the fact that promoted sessions contain promo-tions suggests that the queries used in promoted sessions are more central to the community  X  X  interests. We can say this because we know that our local snippet index contains his-torical selection information related to promoted sessions X  that is where the promotions come from X  X ut not for the non-promoted sessions.

Perhaps a more reasonable approach to our analysis then, is to look at the promoted sessions in more detail in order to discern whether there is any value being added by the pro-moted results themselves. Specifically, are there any sessions for which promoted results only have been selected. These sessions are important because it is reasonable to conclude that without the promotions they would not have been suc-cessful; after all, by definition, only promoted results have been selected. The results in Table 6.3 demonstrate that such sessions do exist. In fact during week 1, 22% of the pro-moted sessions contain selections for promoted results only. In other words, during week one there were 118 promoted sessions (48% of 246 total sessions) and we know that 63% (74 sessions) of these were successful. However, only 78% of these 74 sessions were successful as a consequence of the meta-results returned. Promoted results contributed an ad-ditional 16 successful sessions (22% of 74 sessions), a relative increase over standard meta-search of 36%.

A similar difference is noted in the week two results, al-though this time the difference between the success rates of meta and promoted results is even greater. 29% of the 38 successful promoted sessions in week two were successful be-cause of the presence of promoted results only, whereas the remaining 71% of the successful promoted sessions (27 ses-sions) were successful by virtue of at least the meta-results. This means that the promoted results contribute an extra 11 successful sessions (29% of 38 sessions), a relative increase over the standard meta-search of more than 40%. This is further evidence that the more stringent promotion thresh-olds have succeeded in significantly improving the quality of the promoted results.
This evaluation has helped to clarify the potential benefits of our snippet-based approach to collaborative Web search, at least within the limitations of our trial setup: 1. Promotions accompany meta-search results in a signif-2. The success rate of promoted sessions exceeds that of 3. As many as 29% of successful promoted sessions at-4. The availability of additional search experience during 5. Strengthening of the promotion thresholds during week
Of course there are limitations with this evaluation. Its negatives include the relatively short period of time cov-ered and its limited number of searchers and searches, not to mention a weak notion of result quality and relevance. These issues notwithstanding we believe that the trial still provides a reasonable starting point for an evaluation of our approach. Although the trial itself took place over a 2 week period, it was based on search experiences recorded over a 14 month time-frame. In addition, the results gathered over the trial weeks appear to be consistent and significant. While the number of searchers was limited to approximately 60 people, at least these searchers were assessed in a realistic search scenario; instead of conducting laboratory trials or assessing performance in closed-world search tasks we chose to assess search performance in the field by monitoring the natural searches of real users as they carried out their nor-mal duties of work.
We believe that ultimately, when it comes to evaluating the performance of personalized search techniques, it is cru-cially important to work with live-users on real search tasks. The effects of personalization are difficult to judge reliably in an off-line setting as we must be able to judge how the behaviour of users changes as a result of being presented with new personalized results, for example. Off-line eval-uations that rely on some  X  X lack-box X  notion of relevance, even those that are guided by real user preference data, may tell us whether or not a particular personalization technique is capable of delivering more or less relevant results, but it will not be able to tell us how searchers will respond to these results. Thus we suggest that one of the major ad-vantages of our evaluation is that it assesses performance in terms of real user behaviour. And while we would wel-come the availability of standard data-sets to support the evaluation of personalized search techniques as part of off-line evaluations X  X ata-sets that include detailed historical search behaviors such as the queries submitted, their ori-gins, the results selected, and some information about the types of users carrying out these searches X  X e believe that researchers should complement such studies with realistic live-user trials.
Knowledge workers spend a significant portion of their work time using the Internet and corporate intranets to locate the information they need to perform their duties. Different employees will inevitably have different levels of search expertise. Expert searches will likely be more effi-cient and productive than novice searchers, finding relevant information more reliably and more quickly. We believe that this expertise can and should be captured by organisation in order to help them help their employees search more effec-tively. And with this in mind we have described an approach to Web search that focuses on capturing and reusing the search expertise of a community of like-minded searchers, such as groups of employees within an organisation.
We have demonstrated how our approach to Web search can improve upon existing search engines such as Google or Yahoo by supplementing their standard result-lists with results have have proved relevant to the search community in the past. The results of a live-user trial in a realistic search context show that relevant promotions can be made frequently and that these promotions are capable of signif-icantly enhancing the quality of the standard meta-search results. In addition it is also possible to manipulate the quality of promoted results as a way to balance promotion frequency and quality.

Finally, it is worth mentioning that there are many in-teresting issues related to community-based search that we have not had the space to discuss in this paper, including community origins and profile drift ; how a community X  X  in-terests may shift over time and how these shifts affect the use of community models. These issues constitute ongoing research, and the interested reader is referred to the work of [6] and [15]. [1] The Text REtrieval Conference (TREC). [2] E. Balfe and B. Smyth. Case-Based Collaborative [3] E. Balfe and B. Smyth. An analysis of query similarity [4] J. Budzik and K. Hammond. User Interactions with [5] D. Dreilinger and A. E. Howe. Experiences with [6] D. Gibson, J. M. Kleinberg, and P. Raghavan. [7] E.Glover,S.Lawrence,M.D.Gordon,W.P.
 [8] Google Inc. Google Personal. [9] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [10] C. Kemp and K. Ramamohanarao. Long-term [11] S. Lawrence and C. L. Giles. Context and Page [12] H. P. Luhn. The automatic creation of literature [13] M. Mitra, A. Singhal, and C. Buckley. Improving [14] M. Morita and Y. Shinoda. Information filtering based [15] G. Paliouras, C. Papatheodorou, V. Karkaletsis, C. D. [16] J. Pitkow, H. Schutze, T. Cass, R. Cooley, [17] C. J. V. Rijsbergen. Information Retrieval, 2nd [18] J. Rocchio. Relevance Feedback in Information [19] M. Sahami and T. D. Heilman. A web-based kernel [20] T. Sakai and K. Sparck-Jones. Generic summaries for [21] X. Shen, B. Tan, and C. Zhai. Implicit user modeling [22] X. Shen and C. X. Zhai. Exploiting query history for [23] B. Smyth, E. Balfe, O. Boydell, K. Bradley, P. Briggs, [24] B. Smyth, E. Balfe, J. Freyne, P. Briggs, M. Coyle, [25] M. Speretta and S. Gauch. Personalized search based [26] K. Sugiyama, K. Hatano, and M. Yoshikawa. Adaptive [27] Yahoo Inc. Yahoo My Web.
