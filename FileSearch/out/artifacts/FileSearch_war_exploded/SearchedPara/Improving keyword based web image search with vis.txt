 Zhiguo Gong  X  Qian Liu Abstract This paper discusses techniques for improving the performance of keyword-based web image queries. Firstly, a web page is segmented into several text blocks based on semantic cohesion. The text blocks which contain web images are taken as the associated texts of corresponding images and TF*IDF model is initially used to index those web images. Then, for each keyword, both relevant web image set and irrelevant web image set are selected according to their TF*IDF values. And visual feature distributions of both positive image and negative image are modeled using Gaussian Mixture Model. An image X  X  relevance to the keyword with respect to visual feature is thus defined as the ratio of positive distribution density over negative distribution density. We combine the text-based relevance model with visual feature relevance model to improve the performance. Thirdly, a query expansion model is used to improve the performance further. Expansion terms are selected according to their cooccurrences with the query terms in the top-relevant set of the original query. Our exper-iments show that our approach yield significant improvement over the traditional keyword based query model.
 Keywords We b i m a g e  X  Web search  X  Associated text  X  Visual feature  X  Query expansion 1 Introduction With the explosive growth of the web, millions of images, on almost any subject, are avail-able on the web. How to effectively and efficiently reuse this comprehensive and huge image resource is drawing more and more attention from both academic and industrial communi-ties. Many web image search engines, either as commercial systems such as Google image search ( http://images.google.com/ ), Yahoo Image Search ( http://images.search.yahoo.com ), Lycos ( http://lycos.com/ ), AltaVista photo finder ( http://www.altavista.com/ ), Ditto ( http://www.ditto.com/ ), art.com ( http://www.art.com ), Amico ( http://www.amio.org ), and exalead image search engine ( http://www.exalead.com/image ) or as research prototypes such ASHOW [ 17 ], and PageRank for Product Image Search [ 12 ] have been successfully devel-oped. Comparing with the techniques used in text-oriented web search engines, however, web image search systems still lack mature and agreeable techniques to support effective image queries. 1.1 Previous work on web image retrieval Most of commercial web image search systems, such as Google, Lycos, and AltaVista, only support keyword based searches. Those systems use surrounding blocks of text to index the corresponding images. Then users can search web images just in the same ways as for general web search engines.

In academic community, Shen and his colleagues use Weight ChainNet to represent the semantics of web images [ 27 ]. Their system allows users to query the web images with a sentence. Then the sentence is evaluated against weighted lexical chains of ChainNet to determine the relevance of web images.

Since being created from various application domains, web images, even with close visual features, may have great differences in their semantics. Thus, few systems only use visual features to index and search web images. And PicToSeek is an example of pure content-based web image retrieval system [ 9 ].

PicASHOW is a web image search system based on Web links [ 17 ]. In this system, two web images are supposed to be similar if they are co-cited by many web pages. This model is actually extended from some well known link-based web page schemes to the context of web images retrieval. Though the system implemented by Harmandas et al. [ 11 ] is also based on links, it is actually a text based web image search system. In their work, links are used to track back source pages of the image container page. Then, container page and its source pages are used together to index the corresponding image.

Many web image search systems make use of some combined models to support web image searches. ImageRover combined textual and visual cues to support web image retrieval indexing (LSI) and visual features of web images are represented as color histogram and dominant orientation histogram. To get the search going, the user needs to initially key in keywords to evaluate in the LSI space, then select several sample images for refining the query results. WebSeer is another prototype system to support combined web image search [ 28 , 29 ]. Different to ImageRover, WebSeer makes use of image attributes, such as dimen-sions , file sizes , grayscale / colors ,and image origin , to refine keyword-based queries. Mu-kherjea et al. [ 19 ] developed an image search engine AMORE which combined keywords and visual features to refine the searches for images. In their system, associated texts and visual features of web images are considered as two independent factors and combined lin-early. The user has to provide a sample image in order to make use of visual feature ranking factors. Another similar prototype was completed by Lu and Williams [ 18 ]. Natsev and his colleagues [ 20 ] proposed a concept-based query expansion method for multimedia retrieval. In their work, multimedia-oriented queries are expanded with using lexical rule-based ontol-ogy mapping and statistical correlation analysis. In order for their proposed approach to work, a concept lexicon called LSCOM-lite is created from a training collection, and each concept is described with a set of words and phrases manually selected. Therefore, the performance of the proposed method is closely related to the manually created concept lexicon.
In general, traditional solutions often employ linear models to combine texts and visual features for search of web images. Such methods can not make use of the co-enforcement works, such as [ 8 , 12 , 34 , 36 ], are proposed recently.

Feng and Chua [ 8 ] developed a bootstrapping method which uses a text classifier and a visual feature classifier to successively co-train the relationships between web images and text concepts. But the system needs to start its work with a small set of manually labeled sample images. Another work was proposed by Yanai and Barnard [ 36 ]. In their work, Google Image Engine was used as the first process to gather web images for some predefined concepts. A Gaussian mixture model was employed as the visual feature classifier model. Their system refined word-image relationships in a way similar to the work by Feng and Chua [ 8 ]. The main difference of these two systems is that [ 8 ] needs a small set of manually labeled sample images. Actually both works are designed to refine the results of Google Image Search results rather than image search from scratch. The system proposed by Wang et al. [ 34 ] is designed in the purpose of improving web image retrievals with reinforcement between text similarity model and visual feature similarity model. In their work, image sim-ilarity is reinforced by propagation of similarities between text data set and image data set. In their system, text data refer to text blocks which contain web images, and image data refer to web images themselves. By iteratively propagating similarities between these two sets, the intra similarities are co-enforced. However, a text query by a user is often not as a text block. There is no information provided in the paper as how to use it to evaluate keyword based query. Jing and Balujia [ 12 ] introduced a simple algorithm to re-rank the image results returned by Google image search engine. In their approach, images are taken as vertexes of a graph and visual similarities are assigned as the initial weight of the edges between images. Through an iterative procedure based on PageRank algorithm computation, a numerical weight is assigned to each image and it can be used to re-rank web images in the results. 1.2 Contributions of this work In this paper, we introduce the techniques used in our keyword-based web image search system. We improve the performance of the system with three techniques: (1) web page segmentation, (2) visual feature enhancement, and (3) query expansion enhancement.
Any web page can be represented as a DOM tree, with nodes as HTML tags. We suppose that text tb which is contained in the same HTML tag element with image i is more relevant to i . However, what should be the relevance of tb if it is not in the same tag element with the image? In our approach, we expand relevant text block of web image i recursively to include all the siblings of tb if their semantic cohesion is higher than a given threshold.
In order to measure the semantic cohesion of text blocks, we need to define the semantic relevance between terms. We define semantic relevance of two terms as their co-occurrence within the web collection. Two measures, Support and Confidence between them, are used to reach this objective. Then, semantic cohesion of two text blocks is measured using CHAME-LEON algorithm [ 14 ]. Sibling elements under the same parents are merged if the semantic cohesion among them is over a pre-defined threshold. The merging starts bottom up from leave nodes recursively until semantic cohesion of the block is below a given threshold. Thus a web page can be segmented into several text blocks tb 1 , tb 2 ,..., tb n . Now, we suppose each tb i is a semantic independent text block on the page. For web image i , let tb j ( i ) be the text block containing i , then image i is indexed using the terms of tb j ( i ).
For each query q , the results for q can be a long list. And large amount of noise or irrelevant images may exist in the results. To reduce the influence of those noise images, we improve TF*IDF based relevance by using visual feature distributions.

We suppose important images for the same concept are similar in visual feature, or can be clustered into several groups according to their visual features. We use Gaussian Mixture Model to describe the visual feature distribution for a concept. Then, we define an image X  X  relevance to the concept in visual feature space as the ratio of positive web image distribution density over negative web image distribution density. Thus, a combined relevance model is defined by using both text-based relevance and feature-based relevance.

To improve the performance further, a query expansion model is used in our system. For any original query, terms are selected from the top-relevant results using Local Context Anal-ysis [ 35 ]. Those selected terms have higher co-occurrences with all the terms of the original query in the top-relevant results. Our experiments show that our enhancement approach can produce significant improvement over the original query model for the keyword based web image retrieval.

In the remainder of the paper, Sect. 2 gives our segmentation method based on text cohe-sion. In Sect. 3 , we address our enhancement approach in detail. Section 4 shows our exper-imental results, and in Sect. 5 , we conclude this paper. 2 Associated texts of web images Many past works use several text blocks of the web page to index the corresponding web image. However, no standard work exists as how to correlate text blocks to web images. Most works agree upon that the image name, ALT tag of the image, title of the Web page, and the close text are important to index the corresponding Web image [ 4 , 5 , 10 , 27 , 30 , 34 ]. But the scopes of the close texts of the web images are different from work to work. For example, [ 4 ] take the surrounding text from 20 terms before the image as well as 20 terms after the image as the close text, works [ 5 , 27 , 30 ] take the paragraph containing the image as the close text, and [ 10 ] use the whole page as the close text. In [ 34 ], a web page is segmented using a method called VIPS, which partitions web pages according the presentation homogeneity of the web page elements. In this paper, we segment web pages with semantic cohesion of the text blocks in order to get the proper surrounding text for a web image. The detail tech-nologies of our segmentation approach will be in another paper, and only the summarized algorithm is given in this paper. 2.1 Text cohesion In our approach, the semantic similarities between terms are based on confidence and support of the two terms. For any two terms t 1 and t 2 , confidence ( Conf )and support ( Sup )aredefined as follows: of documents containing both t i and t j , and || D || is the number of total documents in the collection.
Above definitions for Confidence and Support between two words reflect their co-occur-t , and Sup ( t i , t j ) indicates their absolute co-occurrence in the collection. In other words, these two functions can get higher values if t i and t j are often used together in web docu-ments. Therefore, in this paper, we suppose that two terms are likely to be features for the same topic if they have higher values of Confidence and Support . To reduce the computa-tional complexity, we only keep the confidence and support values which are over given thresholds respectively. Currently in this paper, we empirically set the minimum Support to 0.05 and minimum Confidence to 0.1. With this filtering, the number of edges is reduced to 400,000 (originally, with Sup = 0 . 00001 and Conf = 0 . 05, total number of edges is around 4,000,000). Then the semantic similarity between terms t i and t j is defined as Then, Sim ( t i , t j ) is regarded as the values of edges in a term graph. The text cohesion metric is thus defined on the term graph.

For any text block tb , we take it as a linear organization of terms, and simply cut tb in the middle into two equal-sized parts b 1 and b 2 . Then the cohesion of tb is measured by the semantic relevance between b 1 and b 2 . The simple way to measure the semantic relevance is to use value of cosi ne ( b 1 , b 2 ). However, cosine only takes into account of term repetitions, disregarding semantic similarities among terms. In our approach, CHAMELEON metric is used for this purpose [ 14 ]. With CHAMELEON algorithm, cohesion of tb is defined as: connectivity between b 1 and b 2 . They are defined respectively as: is the sum of edges across b 1 and b 2 , EC ( b i ) is the sum of edges within block b i . RC and RI indicate relative closeness and relative interconnectivity respectively between these two blocks. 2.2 Semantic based page segmentation A web page p can be represented as a DOM tree, with tags as the nodes. Our semantic-based page segmentation algorithm ( SBPS ) is performed to merge all the child nodes under a parent node bottom up recursively until the cohesion of among those child nodes is below a given threshold or another web image is found under this node. As in Fig. 1 , for example, two images i 1 and i 2 under the same node N 010 of the DOM tree. The initial caption text of i 1 is N 01002 , and the algorithm will merge all the sub-nodes under N 0100 if the cohesion of N 0100 is over the given threshold, or else N 01002 will be the final caption text for i 1 . But the algorithm will not try to merge N 0100 with N 0101 because N 0101 has image i 2 embedded. Therefore, this segmentation algorithm takes into account of not only text cohesions, but also text separations from other images in the same page.

Let { tb 1 , tb 2 ,..., tb Jk } be all the partitioned blocks for p , then the caption text of web image i j is defined as the text block which contains i j . 2.3 Index web images For any web image i j , let T 1 , T 2 , T 3 and T 4 represent the block types of image X  X  owner page title , image ALT , image name ,and image caption respectively. For any term w k , the weight of term w k associated to image i j is defined as: inverse document frequency of w k in all the text blocks of type T l , X  l is the weight of block type T l and its value is determined with using a greedy algorithm called Two-Way-Merging [ 10 ]. Then, the conditional probability of term w k , given a web image i j is defined as: term w k is defined as: where I is the set of all web images. Here, w(w k , i j ) can be interpreted as the significance degree of word w k in the associated text of web image i j . Therefore, P T (w k | i j ) is the nor-the normalized value of it with respect to all web images whose associated text contain w k .
Most of text based web image search systems are based on above formula or their variants to rank web images for a query [ 10 , 18 , 19 ]. However, the above basic model shows two obvi-ous limitations: (1) many irrelevant images may be ranked earlier in the results of a query, (2) some really relevant images may not be ranked earlier in the results. The problems are mainly due to the fact that web images are loosely associated with the texts. 3 Relevance enhancement The basic relevance formula P T (Eqs. 8 and 9 ) does not take into account of the influences from visual features of web images. As a matter of fact, the visual feature distributions of web images can be used to improve their relevance to the given concept. 3.1 Visual feature enhancement In our system, we use color and texture to represent visual contents of web images. Color fea-ture is described using color histogram in HSL color space [ 31 ], texture feature is constructed using Daubechies wavelet transform [ 6 ].

We concatenate color histogram and texture features to represent the visual contents of web images. For any web image i , let i c and i t be its color histogram and texture vector respectively, then i is represented as a concatenation of these two feature i = i c , i t .
Let w k stand for any of word, concept or term in this paper. For any w k , let I (w k ) = { i | P T ( i j | w k )&gt; th  X  x } be the set of web images with high probabilities for w k in term of P ( i j | w k ). Then we suppose all web images in I (w k ) are relevant to concept w k . However, though some images may be with higher conditional probability values, they may be irrele-vant or even noise images with respect to w k . Therefore, the problem now is how to reduce or alleviate the influences of those irrelevant images in the list. To do so, we suppose real relevant images in I (w k ) tend to be dense in their visual features and irrelevant images or noise images tend to be outliers in the set. Figure 2 gives an example for word  X  X icrosoft X . In this example, all the seven web images are clustered into three clusters, cluster 1, cluster 2 and cluster 3. It is clear that cluster 1 and cluster 2 represent two different spectrums of  X  X icrosoft X  and cluster 3 only contains noise images.

In this paper, we use Gaussian Mixture Model to model the distribution of the relevant images for word w k . To improve the distribution further, negative set of web images for word w k is also used. Let I be the set of web image collection in our system, the set of negative general, NI (w k ) is much larger in size, comparing with positive image set I (w k ). It will cause expensive computation cost if all data in NI (w k ) are used as the negative sample images in constructing the model. On the other hand, if less data are selected as the sample data for the modeling, ill sampling may produce poor model for the image distribution. To overcome those problems mentioned, in our approach, Quadtree index is used to select sample images from NI (w k ).

In fact, Quadtree is an effective approach to index data in multiple dimensional spaces [ 24 ]. It is based on recursive subdivision of multiple dimensional spaces. Let n be the num-ber of dimensions, the first level subdivision of the cube, which contains all the web images, produces 2 n sub-cubes by cutting the original cube at the middle point of each dimension. Additionally, a number, called capacity of the sub-cube, is associated with each of those sub-cubes to indicate the total number of web images located within the sub-cube. This pro-cess can be recursively performed on each sub-cube if it contains images more than a given threshold. It is obvious that Quadtree can provide an efficient search for similar images to a given image. In this paper, we use it to select sample images from NI (w k ).

The sampling process for negative images includes two phases. Firstly, each web image associated capacity indicator of the sub-cube, which contains i j , is decreased by 1. After all such web images are passed through the Quadtree , the capacity number of each sub-cube indicates the number of negative web images contained within this sub-cube. Therefore, after the first phase, associated capacity numbers of sub-cubes show the distribution of negative web images with respect to w k . In the second phase, negative images are selected with using the Quadtree. For each sub-cube in the same level of Quadtree , the number of images selected is proportional to its capacity size of negative images. This method can guarantee selected images well represent the overall distribution of NI (w k ). To simplify our notations, NI (w k ) is still used to denote the set of selected negative images. In our implementation, the size of NI (w k ) is kept the same as positive set I (w k ).
 We perform EM algorithm on I (w k ) and NI (w k ) respectively, then construct Gaussian Mixture Models accordingly as [ 23 ]: where P 1 and P 2 are density functions for conditional probability given w k and non -w k respectively, i represents any image, N is the dimension of the image feature, m 1 is the number of positive components  X  1 , j , X  1 , j , 1 , j represent the weight, the mean vector, and the covariant matrix of the j th positive component, respectively, on condition  X  1 , j = 1 . The parameters for P 2 have the similar explanations. Therefore P 1 and P 2 give visual feature distributions of w k images and non -w k images respectively. To balance the influences of positive distribution and negative distribution, we use the same number m of components for both situations, that is m = m 1 = m 2 . 3.2 Estimating parameters using EM algorithm In this section, we discuss how to determine parameters for Gaussian Distribution Model in this paper. With m components, for simplicity of notations, the Gaussian Mixture Distribution is represented as where i refers to the image feature for a web image, w k refers to the concept,  X  = (  X  ,  X  , ) denotes all those parameters. That is,  X  = ( X  1 , X  2 ,..., X  m ),  X  l = P ( C = l ;  X  ) are the weights of components, P ( i | C = l ,w k ;  X  ) are Gaussian distribution density function for the l th component and it is determined by the mean  X  l and the covariance l of each compo-nent C = l .  X  = ( X  1 , X  2 ,..., X  m ), = ( 1 , 2 ,..., m ). Therefore, to get P ( i | w k ;  X  ), we need to know the values of parameter  X  .

One of the effective methods for solving this problem is called expectation-maximization or EM algorithm [ 22 , 23 ]. The basic idea of EM is to assume that all of those parameters are known to us in the initial phase, then to infer the probability that each sample data belongs to each component. With the inferred probability of the data as the weight, each component is fitted to the entire data. This process need to be performed iteratively until convergence. In this paper, the initial value for parameter  X  = (  X  ,  X  , ) is arbitrarily selected, and denoted as  X  0 = ( X  0 , X  0 , 0 ). Then, the algorithm iterates the following three steps [ 23 ]: 1. E -step: Compute probabilities p lj = P ( C = l | i j ,w k ;  X  0 ), which is the probability 2. M -step: compute the new mean, covariance, and component weights as: 3. S -step: Let S be the sample set of web images, then log likelihood change is calculated The above loop stops when | t | &lt; X , where  X  is a given threshold to determine the conver-gence of the iteration. 3.3 Combining textual and visual feature relevance For any web image i j , we define the relevance of i j , given concept w k , as:
From the definitions, it is clear that R F gives the relevance degree of an image to word w may contain irrelevant web images, we use two techniques to alleviate the influences of those irrelevant training images. Firstly, clustering is performed with using EM algorithm, therefore influences of outlier images are alleviated. Secondly, non -w k web images are used positive distribution over negative distribution. Therefore R F is more significant than P 1 alone. Intuitively, R F can be applied to all web images to calculate their relevance to concept w k with respect to feature distribution. However, because of the limitations of current visual feature representations, two images may be rather different in their semantics even though they are much close in feature. Thus, we combine text based relevance model with feature based relevance model. For any web image i j , we normalize Eq. ( 19 )as: Then a conjunction relevance model from both text space and visual feature space is given as:
Therefore, this combined model indicates the relevance of image i j to word w k through both associated text and image feature distribution. In other words, if web image i j has word w k in its associated text with higher weight, and together, many other similar images have w k in their associated texts too, the overall relevance of i j to w k is enhanced. With our observa-tions, in the web image collection, there are also many web images which have less similar images associated to the same word w k , but they are really relevant to w k . Actually, this problem is caused by reasons as: (1) images may show different views of the same object, and web images on one view of the object are not enough; (2) current web image collection do not have enough images on those objects. To handle such limitations, we define the combined relevance model as: where  X  in [0,1] is the parameter to balance the weights of P T and ( P T  X  P F ) 1 / 2 . R TF = P T when  X  = 0 , and R TF = ( P T  X  P F ) 1 / 2 when  X  = 1 . In fact, the optimal value of  X  may be different from case to case. If the image type is visual significant, such as  X  X nimal X  images, the  X  value will be larger, otherwise it will be smaller or even 0. The optimal values for different domains can be determined by training techniques.

This equation indicates the degree of relevance given word w k based on both of the associ-q = (w 1 ,w 2 ,...,w L ), we define the relevance of query q to image i j as:
In the web environment, however, most of the queries only include one or two words, espe-cially for web image query. To raise the performance further, in this paper, query expansion is used. In the following subsection, we address our methodology for the query expansion in our system. 3.4 Query expansion enhancement Query expansion techniques are often used to improve query performances in many infor-mation retrieval systems. There are two kinds of query expansions in literature, global query expansion and local query expansion [ 35 ]. Global expansions construct semantic relevance of terms based on the corpus-wide statistics. Then, terms are selected according to their global relevance to query terms. Typical works include [ 7 , 13 , 21 ]. Local expansions select terms from the top-relevant documents of the query result, then, expand the query using those selected terms. Such techniques are extensively addressed in [ 1 , 3 ]. Voorhees and Harman have shown that local expansion techniques can often outperform global techniques in gen-eral, but traditional local techniques are not robust and can seriously hurt retrieval [ 33 ]. The problem is mainly caused by the situations when few relevant documents in the top-relevant results. To solve the problem, Xu and Croft [ 35 ] have proposed Local Context Analysis , which selects expansion terms based on cooccurrences with the query terms with respect to the top-relevant documents. Their experiment shows the method can yield more effective and consistent retrieval results. For this sake, in this paper, we employ the idea of Local Context Analysis in our web image retrieval system, but with some modifications. Differently from the for the same objective. In our definition, because inversed document frequencies of id f (w x ) and id f (w l ) are already leverage their influences in conditional probability P T , we do not need to use them explicitly in our metric.

Let q = (w 1 ,w 2 ,...,w L ) be a query, S ={ i 1 , i 2 ,..., i n } be the set of top-n relevant web images. For any word w x , we define the relevance between w x and query term w l with respect to set S as:
It is clear from the definition that: (1) if w x does not appear in the associated texts of the then co (w x ,w l ) = 1 , otherwise co (w x ,w l ) is between 0 and 1. Finally, we define word w x s relevance to the whole query q as: where L is the number of terms in q , co (w x ,w l ) is modified from co (w x ,w l ) as: where  X  is a small positive number for smoothing technique. In general, a small  X  value is good for precision, and a large  X  value is good for recall [ 35 ]. For web image query, precision is more important, so we empirically set  X  = 0 . 001 in this paper. We use co (w x ,w l ) instead one of the query term of q . In general, f (w x , q ) is between  X  and 1, and f (w x , q ) = 1 only if term w x has the same relevance degree with all the query terms with respect to any web images in the top-n relevant set.

It is clear from the definition that f (w x , q ) is the geometric average value of co-relevant degrees of w x with all terms of q , and it emphasizes on the co-relevant degrees with all terms. in the local collection. In our approach, however, we suppose that those expanded terms in q have some decays in ranking the results of the query, comparing with terms in the original query q . For this sake, we define the relevance function of expanded query ( q , q e ) as: That is, the relevance of expanded term to web image i j is the original relevance degree R TF multiplied by its relevance degree to the original query q . 4 Experimental results The crawler of our system gathered about 850,000 web pages with a given set of seeds which are randomly selected from dot-com, dot-edu, and dot-gov domains.

Around 210,000 web images are left after noise images, such as icons, banners, logos and any image with size less than 5 k, are removed automatically by the system. About 23,000 web images are randomly selected to annotate semantic labels. Then, Quadtree , with respect created to support fast access of web images either using visual features or terms. We further select 100 queries (30 1-term queries, 30 2-term queries, 20 3-term queries, 10 4-term queries and 10 5-term queries) from the sample queries used by the students of our department in recent years, one half for training those parameters and other half used for testing the perfor-mance. The collection of web images is divided into three parts, and three pairs of experts are assigned to go through each part of the web image collection in order to define the relevance of web images to those testing queries using the tool as in Fig. 3 . Any difference in labeling the images within each pair is resolved with discussions among those experts. Sometimes more than one subject is defined for the same images. For example,  X  X aptop X ,  X  X otebook X , and  X  X omputer X  may be annotated to the same web images. Then, our experiments are carried out in this context. 4.1 Objective metric for determining parameters In the area of information retrieval, precision/recall is well accepted evaluation method for the performance of the models. An ideal information retrieval algorithm is trying to raise the values for both of the two objectives. Since the result of a query is usually a long list in size, especially in the World Wide Web environment, a figure of precision versus recall changing is commonly used as a performance measurement for a retrieval model. However, this metric can not be used as an objective function in determining parameters. Instead, in this study, we employ a single value summary of retrieval precisions (Average Precision) as our objective function in order to determine the parameters of our models [ 2 , 30 ].

Let Q ={ q 1 , q 2 ,..., q K } be the set of sample queries. Suppose that R q is the total number of relevant web images in the collection for a query q . Then, the average precision ( AP )of the model for query q is defined as: where R q is the total number of all relevant results of query q in the collection and N k is the number of results up to the k th relevant result in the result list of q by the model. As a matter of fact, AP q is the single value metric which indicates the performance of the model for q . Then, the performance of a search model over the query set Q is given as: where || Q || is the number of queries in Q . We u s e AP Q as the objective function to learn the parameters for our models. 4.2 Performance of semantic-based segmentation of web images To know the effectiveness of the proposed semantic-based segmentation algorithm ( SBPS ) for web images, we compare it with other two segmentation methods, First-Paragraph-Based Page Segmentation ( FPPS ) and Vision-Based Page Segmentation ( VIPS ), which, we think, can represent the state of art of web image segmentations.

Most previous works for web image searches make use of the first paragraph which con-tains web images as the associated text to index the corresponding web images [ 5 , 18 , 19 , 27 , 30 ]. In many cases, the first paragraphs containing web images may not have enough text to represent the semantics of web images, thus cause the text-based index with lower performance to support text-based query. However, if more texts surrounding web images are used to index corresponding web images, additional irrelevant words may reduce the per-formance too. VIPS is proposed as the improvement for the segmentation of web texts [ 34 ]. It segments web pages based on perception homogeneities of text blocks of web pages. In other words, several text blocks are merged as one block if their vision perceptions are much similar. A metric called DoC (Degree of Coherence) is used to indicate how coherence of the content presentation in the block with respect to visual perception. We set DoC = 5, as our experiments show this value is optimal in segmenting web pages. For our SBPS algorithm, we set the threshold of coh ( tb ) as 4.5 since our experiment states the segmentation of web pages produce the best result with this threshold. After segmentations of web pages with these three methods, text-based indexes are created respectively. Average Precision ( AP )is used to measure the performances of different segmented texts to index web images. Table 1 shows the differences of their AP values.
 4.3 Parameter determination for gaussian mixture model In general, the more training data is used, the more reliable the model is. However, in our th -x (threshold). With less value of th-x , the percentage of noise images in the training set is increased too, thus make the model less reliable. On the other hand, with bigger value of th-x , the number of training images is decreased though less noise images are included. To complicate the problem, the number of components of Gaussian Mixture Model also has influences to the effectiveness of the model. To simplify the problem, we assign th-x with several values, and for each of the values we study the optimal number of components for Gaussian Mixture Model with cross-validation. We believe that, if the value of th-x is too small (large percent of noise images in the training set) the learned model can not be reliable anyway. With our preliminary experiments, we study the cases when th-x = 0 . 35 , 0 . 25, and 0.15. For each of those values, the total number of images gathered and its corresponding percentage of correct images in pair is (57, 0.61), (195, 0.54), and (346, 0.32) respectively. For each threshold, we use different number of components to construct Gaussian Mixture Model, and use AP ( Average Precision ) as the objective function to measure the performance of the model. Figure 4 shows our experiments.
 It is clear from the figure, out of those three situations for selecting sample images, the Gaussian Mixture Model reach its best performance when th-x = 0 . 25 and number of com-ponents m = 10 . In this case, training web images gathered is around 200, with a little more than half are accurate images to the concept. Using the ratio of positive Gaussian Mixture distribution over negative Gaussian Mixture Distribution as the relevant metric for the query, AP value can reach 0.12 when m = 10 . Then, AP slowly drops down. That means around 10 views for each concept exist on average. In fact, the number of real views for each concept is less than 10 from our observation. In other words, noise images bring more clusters of the web images selected. For the situation when th-x = 0 . 35, model reaches its best performance when m = 5 . The reason is due to the fact that the percentage of noise images for the concept is smaller comparing with the case with th-x = 0 . 25. In the last situation, the performance of the model can not change significantly. With above analysis, we use th-x = 0 . 25 together with m = 10 as our parameters for the Gaussian Mixture Model to describe image X  X  relevance to a concept.
 4.4 Parameter determination for combined model In Sect. 3 , we provide our combined model for ranking web images of a query. We still use AP ( Average Precision ) as the objective for the performance of R TF . Figure 5 shows the performance with different values of  X . The combined model is P T when  X  = 0. And it is equivalent to the conjunction model P T  X  P F when  X  = 1. The combined model reach its optimal performance when  X  = 0 . 8. It is clear from the result that, though the conjunction model outperforms text-alone mode P T , there are still a lot of relevant web images which can not be benefited from the visual feature distributions. 4.5 Parameter determination for query expansion In order to use our expansion model, the number of top-relevant results and the number of expanded words need to be determined. Xu and Croft [ 35 ] have carried out extensively studies on several data collections with different properties in document size, type, content, query length, and so on. On average, they set the default value for the number of top-relevant documents as 100, and the default value for the number of expanded concepts as 70, and the default value for the size of passages as 300 words. According to their research, the optimal number of top-relevant documents is based on the percentage of actually relevant documents in the set. If the percentage of correct documents in the top-relevant set is small, the expansion performance will be poor. In the context of our web image search system, the sizes of associ-ated texts of web images are generally small and semantically cohesive after performing the segmentation method. Expansion words are selected using function f (w x , q ). To distinguish the original words and those expanded words in their contributions in ranking the results, the contribution of expanded terms to rank a relevant image is scaled down with their relevance degree to the original query q (Eq. 27 ). By doing so, more expanded terms may not degrade the performance of the expansion. With our experiments, expanded terms more than 60 could not increase the retrieval performance any more. The change of performance when number of terms from 30 to 60 is not significant. Figure 6 shows the system performances with different number of top-relevant set and number of terms. 4.6 Performances of different retrieval models In this subsection, we show the performances of the original text-based model P T , visual feature enhanced model R TF , and expansion model R TFE for term-based web image queries. We suppose most of text-based retrieval models for web image query are variants of P T . Specially in this paper, our P T is computed using associated texts of web images after per-forming web page segmentation. For visual feature enhanced model R TF , we set th-x = 0 . 25, combination factor  X  = 0 . 8 , number of top-relevant set = 100, number of expanded terms = 60. Figure 7 give the retrieval performances of those models. It is clear from the results that (1) visual feature enhancement may not significantly raise the precisions in the very beginning of the retrieval results, however, with increase of result number it can pro-duce much better and consistent improvement over the result precisions; (2) visual feature enhancement plus term expansion can significantly raise the retrieval precisions especially for the first several results; (3) visual feature enhancement model and visual feature enhance-ment plus expansion have less improvements after recall = 0.35. In fact, according to our experiment, associated text of web images can provide maximum recall of 55% on average. In other words, around 55% relevant web images can be indexed with using their associated texts. Our proposed approach can rank relevant web images much earlier (more relevant results occur when recall value in [0, 0.35]). With our test, the average precision improve-ment of R TFE (Combined Model) over the original model is statistically significant with p -value = 0.04.

In the web search context, large amount of results can be returned for a query, and the user may not go through all the results. Therefore, ranking relevant result earlier is definitely expected. Another effective metric to measure web search performance is to calculate pre-cisions of results with respect to positions. Because such metric does not need to know the overall relevant result in the collection for each query, we apply our method to all web images in our current database (around 240,000 web images). Table 2 shows the precision values with respect to positions of results. The precision values are the percentages of relevant web images in the result lists up to corresponding positions. From the experiment results, it is clear that the improvement of the Combined Model over the original model is statistically significant for poison 10 and 20 with p -values &lt;0.05 (confidence = 0.95). Though feature based enhancement ( R TF ) alone can also improve the retrieval performance comparing with original model, however, it fails in p -value test for all the positions addressed. 5Conclusion In this paper, we have addressed our techniques for keyword based web image retrieval. We firstly partition web pages into several text blocks based on their semantic cohesions, we take the blocks which contain web images as the associated texts for the corresponding web images. A probability model is created to define the terms X  relevant degrees to the images. Then, for each word, a positive web image set and negative web image set are created accord-ingly. We use Gaussian Mixture Distribution Model to enhance the relevant degrees from the aspect of visual feature distributions. By doing so, a term X  X  relevant degree to a web image is enhanced if more similar images associated to this term. This visual feature based enhance-ment is pre-computed. For web search, a user often use few terms as a query for his search. We are sure that if more other terms which are semantically relevant to query terms are in the same associated text of a web image, this web image has more confidence to the query. We e m p l oy local context analysis for this sake. Our experiments have supported our assump-tion. The combination of visual feature enhancement and term expansion enhancement can significantly improve the retrieval performance of web image searches.

As we know from the introduction section, web image search systems often make use three kinds of techniques to support their searches, including keyword-based, content-based, and link-based. And keyword-based model is the most fundamental techniques. Thus, most of commercial web image search systems only support keyword-based searches. In such model, the user does not need to provide a sample images in his original query for search. As a matter of fact, content-based techniques and link-based techniques are complementary to our text-based model.

To create web image search systems with the proposed method, inverted index based on associated texts ( P T ) and Quadtree index based on visual features of web images are firstly created respectively. For each term w k in the collection, positive web image set and nega-tive image set are selected with using inverted index and Quadtree together. Then, Gaussian Mixture Model, thus combined relevance degree R TF ( i j | w k ) is obtained accordingly. In our inverted index. Above procedures are pre-calculated on the collection. As a matter of fact, feature enhancements are more sensitive to noun terms other than other kinds of terms. In order to reduce the computations, some online dictionary such as Wo rd N e t can be used to enhance only the noun terms in the collection. For other terms, the system can remain P T as their relevance model. We suppose this solution is effective because most query terms used by the user are nouns for web image query.

For new inserted web images to the database, text-based relevance degree P T and visual features are extracted, and inverted index and Qaudtree index are maintained in the tradi-tional way. Its visual feature-based relevance degree is computed with using the previous model. With more new web imaged inserted to the collection, previous selected sample web image for training the model may not represent the new distributions. Re-training the model whenever a new image is inserted, however, is obviously unnecessary in the viewpoint of statistics. In our system, for any term w k , the corresponding Gaussian Model is re-trained only if the number of web images with P T ( i j | w k )&gt; th-x is increased significantly (say one quarter in our current implementation). By this way, the system can incrementally update the index with reasonable computation cost.
 References Author Biographies
