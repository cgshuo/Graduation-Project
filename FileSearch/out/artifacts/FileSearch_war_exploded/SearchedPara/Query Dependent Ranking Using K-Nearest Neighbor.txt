
Many ranking models have been proposed in information re-trieval, and recently machine learning techniques have also been applied to ranking model construction. Most of the existing meth-ods do not take into consideration the fact that significant di ff er-ences exist between queries, and only resort to a single function in ranking of documents. In this paper, we argue that it is nec-essary to employ di ff erent ranking models for di ff erent queries and conduct what we call query-dependent ranking. As the first such at-tempt, we propose a K-Nearest Neighbor (KNN) method for query-dependent ranking. We first consider an online method which cre-ates a ranking model for a given query by using the labeled neigh-bors of the query in the query feature space and then rank the docu-ments with respect to the query using the created model. Next, we give two o ffl ine approximations of the method, which create the ranking models in advance to enhance the e ffi ciency of ranking. And we prove a theory which indicates that the approximations are accurate in terms of di ff erence in loss of prediction, if the learning algorithm used is stable with respect to minor changes in training examples. Our experimental results show that the proposed online and o ffl ine methods both outperform the baseline method of using a single ranking function.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models
Algorithms, Performance, Experimentation  X 
The work was done when the first, the third, and fourth authors were interns at Microsoft Research Asia.

Query dependent ranking, k -nearest neighbor, stability
Ranking will continually be an important research topic, as long as search and other information retrieval applications keep devel-oping and growing. When used in search, ranking becomes a task as follows. Given a query, the documents related to the query in the document repository are sorted according to their relevance to the query using a ranking model, and a list of top ranked documents is presented to the user. The key problem for the related research is to develop a ranking model that best represents relevance.
Many models have been proposed for ranking, such as the Boolean model [2], the vector space model [23, 24], BM25 [21] and lan-guage model for IR [13, 18]. Recently, machine learning tech-niques called learning to rank have also been applied to automatic ranking model construction [5, 6, 7, 11, 17]. By leveraging labeled training data and machine learning algorithms, this approach is able to make the tuning of ranking model theoretically sounder and prac-tically more e ff ective. The training data consists of queries, their associated documents and labels representing relevance of docu-ments. In this paper, we also base our work on learning to rank.
In most of the previous work, a single ranking function is used to handle all queries. This may not be appropriate, particularly for web search, as explained below. Instead, it would be better to exploit di ff erent ranking models for di ff erent queries. In this paper, we refer to this approach as query-dependent ranking. (We note that some authors use the term  X  X uery dependent ranking X  to refer to the document ranking process of using both query information and document information, in contrast to the process of using document information alone [20]. We use the term di ff erently in this paper.)
Queries in web search may vary largely in semantics and the users X  intensions they represent, in forms they appear, and in num-bers of relevant documents they have in the document repository. For example, queries can be navigational, informational, or transac-tional [22]. Queries can be personal names, product names, or ter-minology. Queries can be phrases, combinations of phrases, or nat-ural language sentences. Queries can be short or long. Queries can be popular (which have many relevant documents) or rare (which only have a few relevant documents). Using a single model alone would make compromises among the cases and result in lower ac-curacy in relevance ranking.
The IR community has realized the necessity of conducting query-dependent ranking. However, e ff orts were mainly made on query classification [3, 4, 12, 14, 22, 25, 26], but not ranking model con-struction or ranking model learning. The only exception is Kang and Kim X  X  work [12], to our knowledge, in which queries were classified into two categories based on search intension and two dif-ferent ranking models were tuned and used for the two categories. Therefore, more investigations on the approach are needed, which is also the motivation of this work.

Inspired by previous work [8], we propose a query-dependent method for ranking model construction, on the basis of K-Nearest Neighbor (KNN). We position the training queries into the query feature space in which each query is represented by a point. In ranking, given a test query we retrieve its k nearest training queries, learn a ranking model with these training queries, and then rank the documents associated with the test query using that model. The accuracy of ranking can be enhanced by employing the proposed method, due to the following reasons. First, in the method ranking for a query is conducted by leveraging the useful information of the similar queries and avoiding the negative e ff ects from the dissimi-lar ones. Second,  X  X oft X  classification of queries is carried out and similar queries are selected dynamically. Our experimental study has verified the superiority of the KNN method to both the single model approach and the query classification based approach.
Since KNN needs to conduct online training of the ranking model for each test query, and this would not be a ff ordable in practice, we further propose two approximations of the method, which move the training o ffl ine. We give both theoretical justification and empiri-cal verification for the two o ffl ine methods. Specifically, we prove that the approximations are accurate in terms of di ff erence in loss of prediction, if the learning algorithm used is stable with respect to minor changes in training examples. The contributions of this paper include the following points: (1) Proposal of the approach of query-dependent ranking, (2) Development of KNN methods for query-dependent ranking, (3) Theoretical and empirical investigations of the methods.
The rest of this paper is organized as follows. In Section 2, re-lated work is presented. In Section 3, the KNN method for query-dependent ranking is proposed. In Section 4, experimental results are reported. In the last section, conclusions are made and future work is discussed.
There has not been much previous work on query dependent ranking, except [12], as explained above. The most relevant re-search topics are query classification and learning to rank.
Many e ff orts have been made on query classification. In [3, 4, 25], queries were classified according to topics, for instance, Com-puters, Entertainment, Information, etc. as used in KDD Cup 2005. In [12, 14, 22, 26], queries were classified according to users X  search needs, for instance, topic distillation, named page finding, and homepage finding. Machine learning methods such as sup-port vector machines were usually employed in the classification. However, query classification was not extensively applied to query dependent ranking, probably due to the di ffi culty of the query clas-sification problem.

Recently, a large number of studies have been conducted on learning to rank and its application to information retrieval. Exist-ing methods for learning to rank fall into three categories: the point-wise approach [17], which transforms ranking to classification or regression on single documents; the pair-wise approach [5, 7, 11], which formalizes ranking as classification on document pairs; and the list-wise approach [6, 28, 29], which directly minimizes a loss function defined on document lists. The KNN based method pro-posed in this paper can also be viewed as a new learning to rank method.
Queries can vary greatly between each other in several perspec-tives, as explained above. For example, popular queries like  X  X oc-cer X  can have many relevant documents and thus features measuring document popularity (e.g. PageRank) will be important for rank-ing the documents related to this query. In contrast, rare queries like  X  X IGIR 2007 workshop on learning to rank X  may only have a few relevant documents, and thus the use of document popularity in ranking is not even necessary. Using a single ranking model alone would not be able to deal with these cases properly. Naturally we think about employing the  X  X uery dependent approach X , in which we train and use di ff erent ranking models for di ff erent queries.
A straightforward approach to query dependent ranking would be to employ a  X  X ard X  classification approach in which we classify queries into categories and train a ranking model for each category. We think, however, that it could be very di ffi cult to achieve high performance with this approach .

When looking at the data, we often observe that it is hard to draw clear boundaries between the queries in di ff erent categories. Let us take the TREC 2004 web track data as an example. There are in to-tal 225 queries in the dataset, which have been manually classified into three categories: topic distillation, named page finding, and homepage finding. The queries are also associated with documents and relevance labels of these documents.

We define features of queries, following the proposal in [26], and represent the queries in a 27-dimensional query feature space. We next reduce the space to 2-dimensions by using Principal Compo-nent Analysis (PCA). We then plot the queries in this reduced space and obtain the graph in Figure 1.

One can see from the figure that queries in di ff erent categories are mixed together and cannot be separated using hard classifica-tion boundaries. At the same time, one can observe that with high probability a query belongs to the same category as those of its neighbors. We refer to this observation as the  X  X ocality property of queries X . This locality property motivates us to tackle the problem of query-dependent ranking using a K-Nearest Neighbor (KNN) approach. In some sense, we can view KNN as an algorithm per-forming  X  X oft X  classification in the query feature space.
We employ a K-Nearest Neighbor method for query dependent ranking. For each training query q i (with corresponding training data as S q i , i = 1 ,..., m ) 1 , we define a feature vector and represent it in the query feature space (a Euclidean space). Given a new test query q , we try to find its k nearest training queries in terms of Euclidean distance. We then train a local ranking model online us-ing the neighboring training queries (denoted as N k ( q )) and rank the documents of the test query using the trained local model. For local model training, we can in principle employ any existing learning to rank algorithm. In this paper, we choose Ranking SVM [11].
We call the corresponding algorithm  X  X NN Online X . Figure 2 illustrates the workings of the algorithm where the square denotes test query q , triangles denote training queries, and the large circle denotes the neighborhood of query q . The details of the algorithm are presented in Figure 3.

Needless to say, the query features used in the method are critical to its accuracy. In this paper, we simply use the following heuristic method to derive query features and leave further investigation of the issue to future work.

For each query q , we use a reference model (in this paper BM25) to find its top T ranked documents, and take the mean of the feature values of the T documents as a feature of the query. For example, if one feature of the document is tf-idf , then the corresponding query feature becomes average tf-idf of top T ranked documents of the query. If there are many relevant documents, then it is very likely that the value of average tf-idf would be high.
The time complexity of KNN Online is high, and most of the computation comes from step (c) (online training) and step (b) (finding k nearest neighbors).
 At step (c), it trains the ranking model online from S  X  q 0  X  N k ( q ) S q 0 . Usually model training is time consuming. For in-q i contains query q i , the training instances derived from its as-sociated documents and the relevance judgments. When we use Ranking SVM as the learning algorithm, S q i contains all the docu-ment pairs associated with training query q i .
 stance, the time complexity of training a Ranking SVM model is of polynomial order in number of document pairs [10].
 At step (b), it finds k nearest neighbors in the query feature space. If we use a straightforward search algorithm, the time complex-ity will be of order O ( m log m ), where m is the number of training queries.

To reduce the aforementioned complexity, we further propose two algorithms, which move the time-consuming steps o ffl ine.
To improve the e ffi ciency of KNN Online, we consider a new algorithm in which we move the model training o ffl ine. We refer to this new algorithm as KNN O ffl ine-1. KNN O ffl ine-1 is a method as follows.

In training, for each training query q i , we find its k nearest neigh-bors N k ( q i ) in the query feature space. Then, we train a model h from S N k ( q i ) o ffl ine and in advance.
 In testing, for a new query q , we also find its k nearest neighbors N ( q ). Then, we compare S N k ( q ) with every S N k ( q i to find the one sharing the largest number of instances with S where | . | denotes the number of instances in a set. Next, we use h the model of the selected training set (it has been created o ffl ine and in advance), to rank the documents of query q .

Figure 4 illustrates the workings of KNN O ffl ine-1. Here the square represents the test query q and triangles represent the train-ing queries. The triangles in the solid-line circle are the nearest neighbors of q , the solid triangle represents the selected training query q i  X  according to Eq.(1), and the triangles in the dotted-line circle are the nearest neighbors of q i  X  . In KNN O ffl ine-1, the model learned from the triangles in the dotted-line circle is used to test query q . Figure 5 shows the details of the algorithm.
 As will be discussed in Section 3.3.4, the model used in KNN Online and the model used in KNN O ffl ine-1 are similar to each other, in terms of di ff erence in loss of prediction.
In KNN O ffl ine-1, we can avoid online training. However, we introduce additional computation for searching for the most  X  X im-ilar X  training set. Furthermore, we still need to find the k nearest neighbors of the test query online which is also time-consuming.
Considering that online response time is critical for search en-gines, we propose a new algorithm, which we call KNN O ffl ine-2, to further reduce the time complexity in applying KNN O ffl ine-1. Find k nearest neighbors O ( m log k ) O ( m )
The idea of KNN O ffl ine-2 is as follows.Instead of seeking the k nearest neighbors for the test query q , we only find its single nearest neighbor in the query feature space. Supposing that the nearest neighbor is q i  X  , we directly apply the model h q i  X  trained from S (o ffl ine and in advance) to test query q . In this way, we simplify the search of k nearest neighbors to that of a single nearest neighbor, and we no longer need to use Eq.(1) to find the most similar training set. As a result, the time complexity can be significantly reduced.
The basic idea of KNN O ffl ine-2 is illustrated in Figure 6. As for the algorithm description, the only di ff erence between KNN O ffl ine-2 and KNN O ffl ine-1 is that in the former steps (b) and (c) become  X  X ind the nearest neighbor of q , denoted as q i  X  the algorithm details. In Table 1, we list the time complexities of testing for KNN O ffl ine-1 and KNN O ffl ine-2. Here, n denotes the number of docu-ments to be ranked for the test query, k denotes the number of near-est neighbors, and m denotes the number of queries in the training From Table 1, we can see that the time complexities for testing KNN O ffl ine methods mainly lie in the ranking part, which are of order O ( n log n ). In this regard, the time complexities of online testing are comparable with that of the single model approach (one that trains a model using all the training data), which is also of order O ( n log n ).

As for training, it is clear that the KNN O ffl ine methods have much higher time complexity than the single model approach. Sup-pose Ranking SVM is used as the learning algorithm. As we know, the time complexity of training Ranking SVM is of polynomial or-2 Note that we treat k , m and n as variables, and leave other parame-ters like T as constant in our complexity analysis. Furthermore, the complexities we list in Table 1 are in accordance with our imple-mentations, which are not necessarily the lowest complexities that one could get. Usually m and n can range from thousands to tens of thousands [5]. In our experiments, k can be in the hundreds. der in number of document pairs [10]. We use c to represent the polynomial coe ffi cient. According to [16], c  X  1 . 2 to 2, depend-ing on the dataset and feature set used. Suppose that p is the av-erage number of document pairs per training query, then the time complexity of training for the single model approach is of order O (( mp ) c ), while those for the KNN O ffl ine methods are of order KNN O ffl ine methods are about k ( k / m ) ( c  X  1) times higher than that of the single model approach. (Note that we can further improve the e ffi ciency of training by running multiple training processes in parallel .) However, considering the fact that for a search engine the e ffi ciency requirement on o ffl ine processing is usually not as high as that on online processing, we can still say that KNN O ffl ine-1 and KNN O ffl ine-2 are feasible in practice.
Next, we conduct theoretical analysis to see whether and on which condition KNN O ffi ne-1 and KNN O ffl ine-2 are accurate ap-proximations of KNN Online, on the basis of stability theory. This is very important for running the algorithms in practice. Definition 1. Uniform leave-one-instance-out stability
Let X , Y denote the input and label spaces respectively. S = { z 1 = ( x 1 , y 1 ) ,..., z p = ( x p , y p ) }  X  ( X  X  Y ) p denotes the training set derived by leaving one instance z i ( i = 1 ,..., p ) out of S . L is the learning algorithm which can learn a model h S by minimizing the loss function we say that L has uniform leave-one-instance-out stability  X  , if for  X  z  X  X  X  Y ,
T  X  X  X  X  X  X  X  X  1. Let S 1 , S 2 denote two training sets with p1 and p2 instances respectively, h S 1 , h S 2 be two models learned from them by using a learning algorithm L. If L has uniform leave-one-instance-out stability  X  , then we have  X  z  X  X  X  Y , where 4 is the number of shared instances in S 1 and S 2 .
It is easy to verify that Theorem 1 holds, and we omit the proof here.

Theorem 1 states that when the training sets of two models are similar, the models will also be similar in terms of the di ff erence in loss, if the learning algorithm is stable with respect to minor changes in the training examples.
 In our case, Ranking SVM is used as the learning algorithm. Accordingly, we have (1) Ranking SVM is proven to have uniform leave-one-instance-(2) Suppose that for a test query q , the training set S
In the experiment we used data obtained from a commercial search engine. There are two datasets, one containing 1,500 train-ing queries and 400 test queries (denoted as Dataset 1) and the other containing 3,000 training queries and 800 test queries (denoted as Dataset 2). Each query is associated with its retrieved documents along with labels representing the relevance of those documents. There are five levels of relevance: perfect, excellent, good, fair, and bad. For a query-document pair, a feature vector is defined. There are in total 200 features including term frequency, PageRank, etc. All the methods tested in this section are based on the same feature set.

Note that we did not use the public LETOR data [15], because the numbers of queries in the datasets are too small to conduct meaningful experiments on query dependent ranking.
In the experiments, we adopted Ranking SVM [11] as the basic learning algorithm. Ranking SVM has only one parameter  X  repre-senting the trade-o ff between empirical loss and model complexity. We set  X  = 0 . 01 for all methods.

In KNN, we used BM25 as the reference model to rank docu-ments, chose the top T = 50 documents, and then created query features.

As we know, the parameter k in KNN is crucial to the perfor-mance of the algorithm. In practice, this parameter is tuned auto-matically based on a validation set. In order to clearly illustrate the influence of this parameter on the ranking performance, however, we present the testing results with respect to di ff erent values of k instead of automatically determining its most appropriate value.
We used NDCG [9] as our evaluation measure. NDCG at posi-tion n is calculated as: where j is the position in the document list, r ( j ) is the score of the j -th document in the document list (we represent scores of perfect, excellent, good, fair, and bad as 4, 3, 2, 1, 0, respectively), and Z n is a normalizing factor. Z n is chosen so that for the perfect list NDCG at each position equals one.
We compared the proposed KNN methods with the baselines of the single model approach (denoted as Single) and the query clas-sification based approach (denoted as QC).

For the second baseline, we implemented the query type classi-fier proposed in [26] to classify queries into three categories (topic distillation, name page finding, and home page finding). Then we trained one ranking model for each category. For a test query, we first applied the classifier to determine its type, and then used the corresponding ranking model to rank its associated documents.
The experimental results are shown in Figure 7. Here we set k = 400 for Dataset 1, and k = 800 for Dataset 2 for all the Figure 7: Comparisons between KNN Methods and Baselines.
 NDCG@1 through NDCG@10 (See also the experiments in [5]). From Figure 7, we can see that the proposed three methods (KNN Online, KNN O ffl ine-1 and KNN O ffl ine-2) perform comparably well with each other, and all of them almost always outperform the baselines (Single and QC). We conducted t-tests on the improve-ments in terms of mean NDCG. The results indicate that for both Dataset 1 and Dataset 2, the improvements of the KNN methods over both Single and QC are statistically significant (p-value &lt; 0.05). We make the following observations from these results: (1) The better results of KNN over Single indicate that query (2) The superior results of KNN to QC indicate that an approach (3) QC cannot work better than Single, mainly due to the rela-with respect to a large range of k values. Here we simply picked one point from that range and used it in the experiments. Figure 8: Performances of KNN Methods with Di ff erent k Values on Dataset 1.
We tested the performances of the KNN methods with di ff erent values of parameter k , the number of nearest neighbors selected. Notice that when k = m , KNN becomes equivalent to Single, where m denotes the number of training queries.
 Figure 8 shows the performances of the proposed methods on Dataset 1 with di ff erent k values in terms of NDCG@5 and NDCG@10. From this figure, we can see that as k increases, the performances first increase and then decrease. More specifically, (1) When only a small number of neighbors are used, the perfor-(2) When the numbers of neighbors used increase, the perfor-(3) However, when too many neighbors are used (approaching
Figure 9: Change Ratio between KNN Online and KNN O ffl ine. (4) The best performances are achieved when k takes values from We obtain similar results on Dataset 2. Due to space limitations, we omit them here.
From the theoretical analysis in Section 3.3.4 we know that if the training query sets in KNN Online and KNN O ffl ine have a large overlap (i.e.  X  is very small), then the performances of KNN Online and KNN O ffl ine will be very close to each other. We call  X  the change ratio.

We tried to verify that this low change ratio assumption holds for the datasets. Since Ranking SVM is used, document pairs become the instances in this case. Figure 9 shows the relationship between k and change ratio on Dataset 1 and Dataset 2. The x -axis denotes k and the y -axis denotes change ratio.
 From the figure we can see that the change ratio is always small. Furthermore, when k approaches m , change ratio approaches zero. Given the results,the theoretical discussions in Section 3.3.4 indi-cate that KNN Online and KNN O ffl ine can have very similar per-formances. This seems to be verified by the experimental results in Section 4.2 as well (i.e. their test performances are similar on both Dataset 1 and Dataset 2). idea whether the same conclusion holds on larger datasets. Besides, the change ratio of KNN O ffl ine-1 is smaller than KNN O ffl ine-2. This is reasonable since more simplifications have been introduced in KNN O ffl ine-2. Furthermore, we see that the change ratio is smaller on Dataset 2 than on Dataset 1. This seems to be reasonable: as data size increases, the density in the query feature space will become higher, and therefore it will become easier to find similar neighbors. Therefore, the change ratio may be smaller in a larger dataset and the performances of KNN Online and KNN O ffl ine will tend to be more similar.
In this paper, we have pointed out that owing to the great va-riety of queries, ranking of documents in search (particularly web search) should be conducted by using di ff erent models based on dif-ferent properties of queries. We have proposed a K-Nearest Neigh-bor (KNN) approach to learning ranking functions along this direc-tion. To improve the e ffi ciency of the method we have also devised two methods which conduct training o ffl ine. Experimental results show that the proposed approach outperforms both the single model approach and the query classification based approach.

As future work, we plan to investigate the following issues: (1) We have focused on reducing the complexity of online pro-(2) The complexity of online processing in the KNN methods (3) As query features, we utilized one approach. We will try to (4) We have used Euclidean distance as the metric in our KNN (5) We have tried the KNN methods with fixed numbers of neigh-(6) We have studied one approach to query dependent ranking. [1] S. Agarwal and P. Niyogi. Stability and generalization of [2] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information [3] S. M. Beitzel, E. C. Jensen, A. Chowdhury, and O. Frieder. [4] S. M. Beitzel, E. C. Jensen, O. Frieder, D. Grossman, D. D. [5] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, [6] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. Learning to [7] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An e ffi cient [8] D. S. Guru and H. S. Nagendraswamy. Clustering of [9] K. J X rvelin and J. Kek X l X inen. Cumulated gain-based [10] T. Joachims. Making large-scale support vector machine [11] T. Joachims. Optimizing search engines using clickthrough [12] I. Kang and G. Kim. Query type classification for web [13] J. La ff erty and C. Zhai. Document language models, query [14] U. Lee, Z. Liu, and J. Cho. Automatic identification of user [15] T. Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. LETOR: [16] T.-Y. Liu, Y. Yang, H. Wan, H.-J. Zeng, Z. Chen, and W.-Y. [17] R. Nallapati. Discriminative models for information retrieval. [18] J. M. Ponte and W. B. Croft. A language modeling approach [19] F. P. Preparata and M. I. Shamos. Computational [20] M. Richardson, A. Prakash, and E. Brill. Beyond PageRank: [21] S. Robertson. Overview of the okapi projects. In Journal of [22] D. E. Rose and D. Levinson. Understanding user goals in [23] G. Salton. The SMART Retrieval System-Experiments in [24] G. Salton and M. E. Lesk. Computer evaluation of indexing [25] D. Shen, J.-T. Sun, Q. Yang, and Z. Chen. Building bridges [26] R. Song, J.-R. Wen, S. Shi, G. Xin, T.-Y. Liu, T. Qin, [27] E. Xing, A. Ng, M. Jordan, and S. Russell. Distance metric [28] J. Xu and H. Li. Adarank: a boosting algorithm for [29] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support
