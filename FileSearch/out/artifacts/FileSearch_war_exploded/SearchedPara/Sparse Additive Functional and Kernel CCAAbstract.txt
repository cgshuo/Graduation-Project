 Sivaraman Balakrishnan sbalakri@cs.cmu.edu Kriti Puniyani kpuniyan@cs.cmu.edu John Lafferty lafferty@galton.uchicago.edu Canonical correlation analysis ( Hotelling , 1936 ), is a classical method for finding correlations between the components of two random vectors X  X  R p 1 and Y  X  R p 2 . Given a set of n paired observations ( X 1 ,Y 1 ) ,..., ( X n ,Y n ), we form the design matrices and v  X  R p 2 that are solutions to the optimization where the columns of X and Y have been standardized to have mean zero and standard deviation one. This is the sample version of the problem of maximizing the correlation between the linear combinations u T X and v Y , assuming the random variables have mean zero. CCA can serve as a valuable dimension reduction tool, allowing one to quickly zoom in on interesting phe-nomena shared by multiple data sets. This tool is in-creasingly attractive in genomic data analysis, where researchers perform multiple assays per item. For in-stance, data including DNA copy number (or compar-ative genomic hybridization, CGH), gene expression, and single nucleotide polymorphism (SNP) informa-tion can be collected on a common set of patients. Witten et al. ( 2009 ) present examples of recent stud-ies involving such data.
 When the data are high dimensional, as is often the case for genomic data, the classical formulation of CCA is not meaningful, since the sample covariance matrices X T X and Y T Y are singular. This has mo-tivated different approaches to sparse CCA, which regularizes ( 1 ) by suitable sparsity-inducing  X  1 penal-ties ( Witten et al. , 2009 ; Witten &amp; Tibshirani , 2009 ; Parkhomenko et al. , 2007 ; Chen &amp; Liu , 2012 ). Spar-sity can lead to more interpretable models, reduced computational cost, and favorable statistical proper-ties for high dimensional data. Existing methods for CCA are, however, restricted in that they attempt to find linear combinations of the variables X  X nteresting correlations need not be linear. The need for this flexi-bility motivates the nonparametric approaches we con-sider in this paper.
 The general nonparametric analogue of ( 1 ) is where f and g are restricted to belong to an appropri-ate class of smooth functions. Bach &amp; Jordan ( 2003 ) introduce a version of this called kernel CCA by ap-plying the  X  X ernel trick X  to the CCA problem. Kernel CCA allows flexible nonparametric modeling of corre-lations, solving ( 2 ) with additional regularization to enforce smoothness of the functions f and g in ap-propriate reproducing kernel Hilbert spaces. How-ever, this general nonparametric model suffers from the curse of dimensionality, as the number of samples required for consistency grows exponentially with the dimension. It is thus necessary to further restrict the complexity of possible functions. We consider the class of additive models which can be written as in terms of univariate component functions ( Hastie &amp; Tibshirani , 1986 ). In the regression setting, such models no longer require the sample size to be exponential in the dimension; however, they only have strong statistical properties in low dimensions. Recently, several authors have shown how sparse additive models for regression can be effi-ciently estimated even when p &gt; n ( Ravikumar et al. , 2009 ; Koltchinskii &amp; Yuan , 2010 ; Meier et al. , 2009 ; Raskutti et al. , 2010 ).
 In this paper we propose two additive nonparamet-ric formulations of CCA, one over a family of RKHSs and another over Sobolev spaces without a reproduc-ing kernel. In the low-dimensional setting where we do not enforce sparsity, the formulation over Sobolev spaces is closely related to the Alternating Conditional Expectations (ACE) formulation of nonparametric re-gression due to Breiman &amp; Friedman ( 1985 ). In addi-tion to formulating algorithms for the optimizations, we provide risk consistency guarantees for the global risk minimizer in the high dimensional regime where min( p 1 ,p 2 ) &gt; n .
 An important consideration is that sparse nonpara-metric CCA is biconvex, but not jointly convex in f and g . This is true even for the linear CCA model, which is a special case of the model we propose. In the absence of the sparsity constraints the linear problem reduces to a generalized eigenvalue problem which can be efficiently solved. This remains true in the nonpara-metric case as well. Over an RKHS, the problem with-out sparsity is a generalized eigenvalue problem where Gram matrices replace the data covariance matrices. In the population setting over the Sobolev spaces we consider, Breiman &amp; Friedman ( 1985 ) show that the problem reduces to an eigenvalue problem with respect to conditional expectation operators.
 Returning to the nonconvex sparse CCA problem, Witten et al. ( 2009 ) and Parkhomenko et al. ( 2007 ) suggest using the solution to the nonsparse version of the problem to initialize sparse CCA; Chen &amp; Liu ( 2012 ) use several random initializations. As we show in simulations, both approaches can lead to poor re-sults, even in the linear case. To address this issue, we propose and study a simple marginal thresholding step to reduce the dimensionality, in the spirit of the diag-onal thresholding of Johnstone &amp; Lu ( 2009 ) and the SURE screening of Fan &amp; Song ( 2010 ). This results in a three step procedure where after preprocessing we use the nonsparse version of our problem to determine a good initialization for the sparse formulation. In Sections 2 and 3 we briefly describe the additive Sobolev and RKHS function spaces over which we work, introduce our two nonparametric CCA formu-lations, and discuss their optimization. In Section 4 we address the non-convexity of the formulations and initialization strategies. In Section 5 we summarize the theoretical guarantees of these procedures when p ,p 2 &gt; n and in Section 6 we describe some simula-tions and real data experiments. Recall the linear CCA problem ( 1 ). We will now derive its additive generalization over RKHSs. Let F j  X  L 2 (  X  ( x j )) be a reproducing kernel Hilbert space of univariate functions on the domain of X j , and let G k  X  L 2 (  X  ( y k )) be a reproducing kernel Hilbert space of univariate functions on the domain Y k , for each j = 1 ,...,p 1 and k = 1 ,...,p 2 . We assume that E [ f j ( X j )] = 0 and E [ g k ( Y k )] = 0 for all f j g k  X  G k for each j and k . This is necessary to en-force model identifiability. In practice, we will always work with centered Gram matrices to enforce this (see Bach &amp; Jordan ( 2003 )).
 Denote by F = { f = P p 1 j =1 f j ( x j ) | f j  X  F j } and functions of x and y , respectively.
 We are given n independent tuples of the form { Y i 1 ,...,Y ip 2 } , and positive definite kernel functions on each covariate of X and Y . We denote the Gram matrix for the j th X covariate by K xj and for the k th Y covariate by K yk .
 We will need to regularize the CCA problem to enforce smoothness and sparsity of the functions. The two norms k f play an important role in our approach. We can now formulate the sparse additive kernel CCA ( SA-KCCA ) problem as 1 n 1 n for given regularization parameters  X  f , X  g ,C f and C . As with the group LASSO, constraining P j k f j k 2 encourages sparsity amongst the functions f j Ravikumar et al. ( 2009 ). As stated, this is an in-finite dimensional optimization problem over Hilbert spaces. However, a straightforward application of the representer theorem shows that it is equivalent to the following finite dimensional optimization problem: max 1 n  X   X  1 n X .
 Here  X  is an ( n  X  p 1 ) matrix,  X  j is its j th column,  X  is an ( n  X  p 2 ) matrix and  X  k is its k th column. The problem ( 6 ) is not convex. However, if we fix the function g (or equivalently the coefficients  X  ) the problem is convex in f (equivalently  X  ), and vice-versa. This biconvexity leads to a natural optimization strat-egy for ( 6 ) which we describe below. However, this procedure only guarantees convergence to a local op-timum and in practice we still need to be able to find a good initialization.
 In the absence of the sparsity penalty the prob-lem becomes an additive form of kernel CCA ( Bach &amp; Jordan , 2003 ). One could also consider al-ternative formulations that, for instance, separate the smoothness and variance constraints. One attractive feature of our formulation is that without the sparsity constraint the problem can be reduced to a generalized eigenvalue computation which can be solved optimally. This leads us to a strategy of biconvex optimiza-tion that mirrors the linear algorithm of Witten et al. ( 2009 ); specifically, initialize by solving the problem without the sparsity constraints, fix  X  and optimize for  X  and vice-versa until convergence. As our exper-iments will show this is indeed a good strategy when p ,p 2 &lt; n . However, new ideas, to be described in Section 4 , are necessary to scale this to the high di-mensional setting where p 1 ,p 2 &gt; n . We now formulate an optimization problem for sparse additive functional CCA ( SA-FCCA ), and derive a scalable backfitting procedure for this problem. Here we work directly over the Hilbert spaces L 2 (  X  ( x )) and L (  X  ( y )). We will denote by S j the subspace of  X  ( x j measurable functions with mean 0, with the usual in-T k for the functions of y .
 To enforce smoothness we consider functions lying in a ball in a second order Sobolev space. We further assume the functions are uniformly bounded, and the measures  X  are supported on a compact subset of a Euclidean space with Lebesgue measure  X  . For a fixed uniformly bounded, orthonormal basis  X  jk with re-spect to  X  we have
F j = f j  X  X  j : f j = and similarly for G k . We will call these the smooth functions, and denote by F and G the set of smooth additive functions over the respective Hilbert spaces. Our formulation of sparse additive functional CCA is the optimization s.t. where the k . k 2 norm is defined as in additive kernel CCA. This problem is superficially similar to ( 2 ); how-ever, there are three important differences. First, we don X  X  regularize for smoothness but instead work di-rectly over a Sobolev space of smooth functions. Sec-ondly, we do not constrain the variance of the function f . Instead, in the spirit of  X  X iagonal penalized CCA X  of Witten et al. ( 2009 ) we constrain the sum of the variances of the individual f j s. This choice is made primarily because it leads to backfitting updates that have a particularly simple and intuitive form. Per-haps most importantly, we can no longer appeal to the representer theorem since we are not working over RKHSs.
 We study the population version of this problem to derive a biconvex backfitting procedure to directly op-timize this criterion. The sample version of the al-gorithm is described in Algorithm 1, and a complete derivation is part of the supplementary material. To gain some intuition for this procedure we describe one special case of the population algorithm, where g is fixed and both constraints on f are tight. Consider the Lagrangian problem max The norms are defined as k f k 1 = P p 1 j =1 the case when  X , X  &gt; 0, and denote a  X  g ( Y ). We now can derive a coordinate ascent style procedure where we optimize over f j holding the other functions fixed. The Fr  X echet derivative w.r.t. f j in the direc-tion  X  gives one of the KKT conditions E [( a  X  2  X f j  X   X  X  j )  X  ] = 0 for all  X  in the Hilbert space H j , where the subdifferential is  X  j = f j  X  is the set { u j  X  X  j | E ( u 2 j )  X  1 } if Using iterated expectations the KKT condition can be written as E [( E ( a | X j )  X  2  X f j  X   X  X  j )  X  ] = 0. De-note E ( a | X j )  X  P j . In particular, if we consider  X  = E [( E ( a | X j )  X  2  X f j  X   X  X  j ], we can see that E [( E ( a | X j )  X  2  X f j  X   X  X  j )] = 0, i.e., E ( a | X  X  X  j = 0 almost everywhere.
 Then if at the following soft thresholding update: Now, going back to the constrained version, we need to select  X  and  X  so that the two constraints are tight. To get the sample version of this update we replace the conditional expectation P j by an estimate S j a , where S j is a locally linear smoother.
 Algorithm 1 Biconvex backfitting for SA-FCCA input { ( X i ,Y i ) } , parameters C f , C g , initial g ( Y output Final functions f , g The formulations of SA-KCCA and SA-FCCA above are not jointly convex, but are biconvex. Hence, iter-ative optimization algorithms may not be guaranteed to reach the globally optimal solution. To address this issue, we first run the algorithms without any sparsity constraint. The resulting nonsparse collections of func-tions are then used as initializations for the algorithm that incorporates the sparsity penalties. While such initialization works well for low dimensional problems, as p increases, the performance of the estimator goes down (Figure 1 ). To extend the algorithms to the high dimensional scenario, we propose marginal threshold-ing as a screening method to reject irrelevant variables and run the SA-FCCA and SA-KCCA models on the reduced dimensionality problem. For each pair of variables X i and Y j , we fit marginal functions to that pair by optimizing the criteria in either Equation ( 6 ) or Equation ( 7 ) without the sparsity constraints since we only consider one X and one Y covariate at a time. We then compute the correlation on held out data. This constructs a matrix M of size p 1  X  p 2 with ( i,j ) entry of the matrix representing an estimate of the marginal correlation between f i ( X i ) and g j ( Y j We then threshold the entries of M to obtain a sub-set of variables on which to run SA-FCCA and SA-KCCA . Theorem 5.3 discusses the theoretical proper-ties of marginal thresholding as a screening procedure, and Section 6.2 presents results on marginal thresh-olding for high dimensional problems. In this section we will characterize both the func-tional and kernel marginal thresholding procedures and study the theoretical properties of the estimators ( 6 ) and ( 7 ). We will state the main theorems and defer all proofs to the supplementary material.
 The theoretical characterization of these procedures relies on uniform large deviation inequalities for the covariance between functions. For simplicity in this section we will assume all the univariate spaces are identical. In the RKHS case we restrict our attention to functions in a ball of a constant radius in the Hilbert space associated with a reproducing kernel K . In the functional case the univariate space is a second order Sobolev space where the integral of the square of the second derivative is bounded by a constant . With some abuse of notation we will denote these spaces C . We are interested in controlling the quantity
 X  n = sup where f j ,g k  X  X  ,j  X  X  1 ,...,p 1 } ,k  X  X  1 ,...,p 2 } . All results extend to the case when each covariate is endowed with a possibly distinct function space. Lemma 5.1 (Uniform bound over RKHS) Assume sup x | K ( x,x ) |  X  M &lt;  X  , for func-where C is a constant depending only on M , and  X  = Note that  X  is independent of the dimensions p 1 and p 2 and that under the assumption that K is bounded,  X  = O (1 / much smaller. The second term depends only logarith-mically on p 1 and p 2 and this weak dependence is the main reason our proposed procedures are consistent even when p 1 ,p 2 &gt; n .
 Lemma 5.2 (Uniform bound for Sobolev spaces) Assume k f k  X   X  M  X  X  X  , then where C 1 and C 2 depend only on M .
 Lemma 5.1 is proved via a Rademacher symmetriza-tion argument of Bartlett &amp; Mendelson ( 2002 ) (see also Gretton et al. ( 2004 )) while Lemma 5.2 is based on a bound on the bracketing integral of the Sobolev space (see Ravikumar et al. ( 2009 )). The Rademacher bound gives a distribution dependent bound which can in some cases lead to faster rates.
 We are now ready to characterize the marginal thresh-olding procedure described in Section 4 . To study marginal thresholding we need to define relevant and irrelevant covariates. For each covariate X j , denote sidered irrelevant if  X  j = 0 and relevant if  X  j &gt; 0. Similarly, for each Y k we associate  X  k defined analo-gously.
 Now, assume that for every pair of covariates, we find the maximizer of the SA-FCCA or SA-KCCA objec-tive over the given sample, over the appropriate class C and with E ( f 2 j )  X  1 , E ( g 2 k )  X  1. Recall that for marginal thresholding we do not enforce sparsity. The global maximization of the SA-KCCA objective can be efficiently carried out since it is equivalent to a gen-eralized eigenvalue problem. For SA-FCCA however, the backfitting procedure is only guaranteed to find the global maximizer in the population setting. Theorem 5.3 Given P ( X  n  X   X  )  X   X  . 1. With probability at least 1  X   X  , marginal threshold-2. Further, if we have that  X  j or  X  k  X  2  X  then under The importance of Lemmas 5.1 and 5.2 is that they provide values at which to threshold the marginal co-variances. In particular, notice that the minimum sample covariance that can be reliably detected, with no false inclusions, falls rapidly with n and approaches zero even when p 1 ,p 2 &gt; n .
 In the spirit of early results on the LASSO of Juditsky &amp; Nemirovski ( 2000 ); Greenshtein &amp; Ritov ( 2004 ) we will establish the risk consistency or per-sistence of the empirical maximizers of the two ob-jectives. Although we cannot guarantee that we find these empirical maximizers due to the non-convexity this result shows that with good initialization the for-mulations ( 6 ) and ( 7 ) can lead to solutions which have good statistical properties in high dimensions. For SA-KCCA we will assume that our algorithm maximizes over the classes
F = and for SA-FCCA we will assume that our algorithm maximizes the same objective over the same class with-out the RKHS constraint but which are instead in a Sobolev ball of constant radius. Denote these solutions (  X  f,  X  g ).
 We will compare to an oracle which maximizes the population covariance Denote this maximizer by ( f  X  ,g  X  ). Our main result will show that these procedures are persistent , i.e., cov( f  X  ,g  X  )  X  cov(  X  f,  X  g )  X  0 even if p 1 ,p 2 &gt; n . Theorem 5.4 (Persistence) If p 1 p 2  X  e n  X  for some  X  &lt; 1 and C f C g = o ( n (1  X   X  ) / 2 ) , then SA-FCCA and SA-KCCA are persistent over their respective func-tion classes. 6.1. Non-linear correlations We compare SA-FCCA and SA-KCCA with two models, sparse additive linear CCA ( SCCA ) ( Witten et al. , 2009 ) and kernel CCA ( KCCA ) ( Bach &amp; Jordan , 2003 ). Figure 1 shows the perfor-mance of each model, when run on data with n = 150 samples in p 1 = 15, p 2 = 15 dimensions, where only one relevant variable is present in X and Y (the re-maining dimensions are Gaussian random noise). We report two metrics to measure whether the correct cor-relations are being captured by the different methods -(a) test correlation on 200 samples, using the esti-mated functions, and (b) precision and recall in iden-tifying the correct variables involved in the correlation estimation. Each result is averaged over 10 repeats of the experiment. Since KCCA uses all data dimensions in finding correlations, its precision and recall are not reported.
 When the relationship between the relevant variables is linear, all methods identify the correct variables and have high test correlation. While KCCA should be able to identify non-linear correlations, since it is strongly affected by the curse of dimensionality, it has poor test correlation even in p = 15 dimensions. Both SA-FCCA and SA-KCCA correctly identify the relevant variables in all cases, and have high test cor-relation. 6.2. Marginal thresholding We now test the efficiency of marginal thresholding by running an experiment for n = 150, p 1 = 150, p 2 = 150. We generate multiple relevant variables as: f ( X i ) = cos Thus, there are four relevant variables in each data set. X and Y are sampled from a uniform distribution, and standardized before computing f i ( X i ). Each f i ( X i also standardized before computing Y j . We repeat the experiment by generating data 10 times, and report results in Table 2 . Bandwidth in the different methods was selected using a plug-in estimator of the median distance between points in a single dimension. The sparsity and smoothness parameters for all methods were tuned using permutation tests, as described in Witten et al. ( 2009 ), assuming that C f = C g = C , and  X  f =  X  g =  X  .
 We ran marginal thresholding by splitting the data into equal sized train and held out data, fitting marginal functions on the train data, computing func-tional correlation on the held out data, and picking a threshold so that n/ 5 elements of the thresholded correlation matrix are non-zero. We found that in all experiments, marginal thresholding always selected the relevant variables for the subsampled data. Table 2 shows the precision, recall and test correlations for the different methods. As can be expected, SA-FCCA and SA-KCCA are able to correctly identify the rele-vant variables, and the estimated functions have high correlation on test data.
 We visualize the effect of the parameter tuning by plot-ting regularization paths, as the sparsity parameter is varied ( n =100, p 1 = p 2 =12). For SA-FCCA and SA-KCCA , the norm of each function is plotted, and for sparse linear CCA, the absolute values of the entries of u and v are shown. Figure 3 shows how, unlike SCCA , SA-FCCA and SA-KCCA are able to sepa-rate the relevant and non-relevant variables over the entire range of the sparsity parameter. 6.3. Application to DLBCL data We apply our non-linear CCA models to a data set of comparative genomic hybridization (CGH) and gene expression measurements from 203 diffuse large B-cell lymphoma (DLBCL) biopsy samples ( Lenz , 2008 ). We obtained 1500 CGH measurements from chromosome SA-FCCA 0.94 1 0.785 SA-KCCA 0.98 0.95 0.8 1 of the data, and 1500 gene expression measurements from genes on chromosome 1 and 2 of the data. The data was standardized,and Winsorized so that the data lies within two times the mean absolute deviation. We used marginal thresholding to reduce the dimen-sionality of the problem, and then ran SA-FCCA . Permutation tests were used to pick an appropriate bandwidth and sparsity parameter, as described in Witten et al. ( 2009 ). We found that the model picked interesting non-linear relationships between CGH and gene expression data. Figure 2 shows the functions ex-tracted by the SA-FCCA model from this data. Even though this data has been previously analyzed using linear models, we do not necessarily expect gene ex-pression measurements from Affymetrix chips to be linearly correlated with array CGH measurements, even if the specific CGH mutation is truly affecting the gene expression. Further, the extracted functions in Figure 2 suggest that the changes in gene expres-sion are dependent on the CGH measurements via a saturation function -as the copy number increases, the gene expression increases, until it saturates to a fixed level, beyond which increasing the copy numbers does not lead to an increase in expression. From a systems biology view point, such a prediction seems reason-able since single CGH mutations will not affect other pathways that are required to be activated for large changes in gene expression.
 Non-linear In this paper we introduced two proposals for nonpara-metric CCA and demonstrated their effectiveness both in theory and practice. Several interesting questions and extensions remain. CCA is often run on more than two data sets, and one is often interested in more than just the principal canonical direction. Chen &amp; Liu ( 2012 ) have proposed group sparse linear CCA for sit-uations when a grouping of the covariates is known. These extensions all have natural nonparametric ana-logues which would be interesting to explore. As in the case of regression ( Koltchinskii &amp; Yuan , 2010 ), the KCCA formulation considered in this paper can also be generalized to involve multiple kernels and kernels over groups of variables in a straightforward way. While thresholding marginal correlations one can imagine exploiting the structure in the correlations. In particular, in the ( p 1  X  p 2 ) marginal correlations ma-trix we are looking for a bicluster of high entries in the matrix. Leveraging this structure could potentially al-low us to detect weaker marginal correlations. Finally, an important application of kernel CCA is as a con-trast function in independence testing. The additive formulations we have proposed allow for independence testing over more restricted alternatives but can be used to construct interpretable tests of independence. Research supported in part by NSF grant IIS-1116730, AFOSR contract FA9550-09-1-0373, and NIH grant R01-GM093156-03.

