 Ranking algorithms, whose goal is to appropriately order a set of objects/documents, are an important component of in-formation retrieval systems. Previous work on ranking algo-rithms has focused on cases where only labeled data is avail-able for training (i.e. supervised learning). In this paper, we consider the question whether unlabeled (test) data can be exploited to improve ranking performance. We present a framework for transductive learning of ranking functions and show that the answer is affirmative. Our framework is based on generating better features from the test data (via Ker-nelPCA) and incorporating such features via Boosting, thus learning different ranking functions adapted to the individ-ual test queries. We evaluate this method on the LETOR (TREC, OHSUMED) dataset and demonstrate significant improvements.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval; H.4.m [ Information Systems Applications ]: Miscellaneous X  machine learning Algorithms, Experimentation Information retrieval, Learning to rank, Transductive learn-ing, Boosting, Kernel principal components analysis
Ranking algorithms, whose goal is to appropriately order a set of objects/documents, are an important component of information retrieval (IR) systems. In applications such as web search, accurately presenting the most relevant docu-ments to satisfy an information need is of utmost impor-tance: a suboptimal ranking of search results may frustrate the entire user experience. Work supported by an NSF Graduate Research Fellowship
While many methods have been proposed to tackle the (document) ranking problem, a recent and promising re-search direction is to apply techniques in machine learning. In this approach, a training set consisting of user queries and the corresponding list of retrieved documents and relevance jugdements is provided to the machine learning algorithm. The relevance judgments may be provided by a human an-notator or an implicit feedback mechanism (e.g. query logs) [13]. The algorithm then learns a  X  X anking function X  that predicts relevance judgments close to that of the training set. Thus far, much research has focused on (a) different learning algorithms [8, 7, 5, 11, 35, 17], and (b) the in-terplay between optimization objectives and IR evaluation measures [20, 28, 32, 4, 33].

We explore an orthogonal research direction here: We ask the question,  X  X an additional unlabeled data (i.e. query-document pairs without relevance judgments) be exploited to improve ranking performance? X  In particular, we con-sider the case known as transductive learning, where such unlabeled data is the test data to be evaluated.
To be precise, let q = query, d = list of retrieved doc-uments, and y = list of relevance judgments. Let S = query-doc-labels. The traditional task of  X  X upervised learn-ing X  is to learn a ranking function using S ; the ranker is then evaluated on a previously unseen and unlabeled test set E = { ( q u , d u ) } u =1 ..U . In transductive learning, both S and E are available when building the ranking function, which is also then evaluated on E . This has the potential to outperform supervised learning since (1) it has more data, and (2) it can adapt to the test set.

Due to its promise, transductive learning (and more gener-ally, semi-supervised learning 1 ) is an active area of research in machine learning. Previous work mostly focused on clas-sification problems; work on semi-supervised ranking is be-ginning to emerge.

In this paper, we demonstrate that  X  X earning to rank with both labeled and unlabeled data X  is a research direction worth exploring. Our contribution is a flexible transductive framework that plugs in and improves upon existing super-vised rankers. We demonstrate promising results on TREC and OHSUMED and discuss a variety of future research di-rections.

The paper is divided as follows: Section 2 outlines the
Semi-supervised (inductive) learning is more general in that the unlabeled data E need not to be the test set; it can give predictions and be evaluated on another previously unseen and unlabeled data. See [37] for a good review. proposed transductive framework. Section 3 presents the main experimental results. Then, Section 4 raises several questions and delves into more detailed analyses of results. Finally, Section 5 draws connections to various related work and Section 6 ends with a discussion on future research.
When designing a machine learning solution to a problem, there are several commonly chosen strategies: (a) Engineering better features to characterize the data. (b) Designing an objective function that more closely ap-proximates the application-specific evaluation metric. (c) Developing a more effective algorithm for optimizing the objective.

In this work, we explore option (a). The key idea is to au-tomatically derive better features using the unlabeled test data. In particular, an unsupervised learning method (i.e. a learning algorithm that does not require labels, e.g. clus-tering, principal components analysis) is applied to discover salient patterns ( P u ) in each list of retrieved test documents d . The training data is projected on the directions of these patterns and the resulting numerical values are added as new features. The main assumption is that this new training set better characterizes the test data, and thus should outper-form the original training set when learning rank functions. Algorithm 1 Transductive meta-algorithm Input: Train set S = { ( q l , d l , y l } l =1 ..L Input: Test set E = { ( q u , d u ) } u =1 ..U Input: DISCOVER(), unsupervised algorithm for discover-Input: LEARN(), a supervised ranking algorithm Output: Predicted rankings for test: { y u } u =1 ..U 1: for u = 1 to U do 2: P u = DISCOVER( d u ) # find patterns on test data 3:  X  d u = P u ( d u ) # project test data along P u 4: for l = 1 to L do 5:  X  d l = P u ( d l ) # project train data along P u 6: end for 7: F u (  X  ) = LEARN( { ( q l ,  X  d l , y l ) } l =1 ..L ) 8: y u = F u (  X  d u ) # predict test ranking 9: end for Algorithm 1 shows the pseudocode for this meta-algorithm. DISCOVER() is the generic unsupervised method. It is impor-tant to note that it is applied to each test document list d separately (line 2). This is because queries are formulated independently and that different d u exhibit different salient patterns. 2 LEARN() is the generic supervised method for learning rank functions. Since the feature-based represen-tations of the training documents ( { d l } l =1 ..L ) are enriched with additional test-specific features (line 5), we learn a dif-ferent ranking function F u (  X  ) for each test query (line 7).
The usefulness of test-specific features and test-specific ranking functions is illustrated in Figures 1(a) and 1(b).
One can imagine extensions where successive queries are dependent. See Section 6. Figure 1: Plots of documents for 2 different queries in TREC X 04 (y-axis = BM25, x-axis = HITS score). Relevant documents are dots, irrelevant ones are crosses. Note that (a) varies on the y-axis whereas (b) varies on the x-axis, implying that query-specific rankers would be beneficial.
 These are plots of documents from two TREC X 04 queries. The x-axis shows the (normalized) HITS Hub score of a document, while the y-axis shows the (normalized) BM25 score of the extracted title (both are important features for learning). Irrelevant documents are plotted as small crosses whereas relevant documents are large dots. For the first query (Fig. 1(a)), we see that the data varies mostly along the y-axis (BM25); for the second query (Fig 1(b)), the vari-ation is on the x-axis (HITS). These 2 document lists would be better ranked by 2 different rankers, e.g. one which ranks documents with BM25 &gt; 2 . 5 as relevant, and the second which ranks documents with HITS &gt; 1 . 25 as relevant. A single ranker would find it difficult to simultaneosly rank both lists with high accuracy.

In this paper, we use kernel principal components analysis (Kernel PCA)[24] as the unsupervised method and Rank-Boost[8] as the supervised ranker. In theory, any algorithm can be plugged in for DISCOVER() and LEARN() . In practice, it is important to consider the interaction between feature and learning, and to ensure that DISCOVER() generates fea-tures that LEARN() is able to exploit. We will argue here that Kernel PCA and RankBoost is one such good pair, but there may well be others.
Principal components analysis (PCA) is a classical tech-nique for extracting patterns and performing dimensionality reduction from unlabeled data. It computes a linear combi-nation of features, which forms the direction that captures the most variance in the data set. This direction is called the principal axis, and projection of a data point on it is called the principal component. The magnitude of the prin-cipal component values indicates how close a data point is to the main directions of variation.

Kernel PCA [24] is a powerful extension to PCA that computes arbitrary non-linear combinations of features. As such, it is able to discover patterns arising from higher-order correlations between features. We can imagine Kernel PCA as a procedure that first maps each data point into a (possi-bly) non-linear and higher-dimensional space, then performs PCA in that space. More precisely, let d be a list of m docu-ments and d j be the original feature vector of document j . Then Kernel PCA can be seen as the following procedure: 1. Map each doc d j to a new space d j 7 X   X ( d j ), where 2. Compute covariance matrix in this new space: 3. Solve the eigen-problem:  X  v = Cv . 4. The eigenvectors v with the largest eigenvalues  X  form
In practice, Kernel PCA uses the dual formulation to avoid solving the above eigen-problem in high dimensional space (this is known as the kernel trick). See [24] for the derivation; here we only present the steps needed for this paper: 1. Define a kernel function k (  X  ,  X  ) : ( d j , d j 0 )  X  2. There exist kernels of the form 3. Let the m  X  m matrix K be the kernel values of all 4. Kernel PCA reduces to solving the eigen-problem m X   X  = 5. For a new document d n , its principal component is
The kernel function defines the type of non-linear patterns to be extracted. In this work, we use the following kernels:
In the context of Kernel PCA, we drop the subscript in d u to avoid clutter. d u or d is a document list; d j is one document vector within the list.
Kernel PCA scales as O ( m 3 ), due to solving the eigen-problem on the m  X  m kernel matrix K . Nevertheless, extremely fast versions have been proposed; for instance, Sparse kernel feature analysis [26] is based on sparsity con-straints and can extract patterns in O ( m ).
RankBoost[8] is an extension of the boosting philosophy [23] for ranking. In each iteration, RankBoost searches for a weak learner that maximizes the (weighted) pairwise rank-ing accuracy (defined as the number of document pairs that receive the correct ranking). A weight distribution is main-tained for all pairs of documents. If a document pair receives an incorrect ranking, its weight is increased, so that next it-eration X  X  weak learner will focus on correcting the mistake.
It is common to define the weak learner as a non-linear threshold function on the features (decision stump). For ex-ample, a weak learner h (  X  ) may be h ( d j ) = 1 if X  X M25 score &gt; 1 X  and h ( d j ) = 0 otherwise. The final ranking function of RankBoost is a weighted combination of T weak learners: where T is the total number of iterations.  X  t is computed during the RankBoost algorithm and its magnitude indicates the relative importance of a given weak learner (feature). Finally, a ranking over a document list d is obtained by calculating y j = F ( d j ) for each document and sorting the list by the value of y j .
 There are several advantages to using RankBoost with Kernel PCA in our transductive framework: 1. Inherent feature selection: RankBoost selects T fea-2. Non-linear thresholding in weak learners h (  X  ): One 3.  X  X nytime X  training: Boosting can be seen as gradient
Finally, for clarity, we present a concrete walk-through of a search scenario using our transductive framework: 0. A training set is prepared and stored in memory/disk. 1. User submits a query q u 2. Initial IR engine returns possibly relevant documents d 3. Run Algorithm 1, lines 2 to 8. Output y u . 4. Rank the documents d u by y u . Return results to user. In contrast to the supervised approach, the practical chal-lenges here are: (a) scalable storage of the training set, (b) fast computation of Algorithm 1 (during user wait). We leave these computational issues as future work.
We perform experiments on the LETOR dataset [18], which contains three sets of document retrieval data: TREC X 03, TREC X 04, and OHSUMED. This is a re-ranking (subset ranking) problem, where an initial set of documents have been retrieved and the goal is sort the set in an order most relevant to the query. The TREC data is a Web Track Topic Distillation task. The goal is to find webpages that are good entry points to the query topic in the .gov domain. The OHSUMED data consists of medical publications and the queries represent medical search needs. For TREC, doc-uments are labeled { relevant , irrelevant } ; an additional label { partially relevant } is provided for OHSUMED.
The LETOR dataset conveniently extracts many state-of-the-art features from documents, including BM25 [22], HITS [14], and Language Model [34]. It is a good datasest for our experiments since we will be discovering patterns from features that have already been proven to work. Table 1 summarizes the data (e.g. in TREC X 03, the ranker needs to sort on average 983 documents per query, with only 1 document in the set being relevant); see [18] for details.
Our experimental setup compares three rankers. The base-line is a supervised RankBoost, trained on the original training data. This is compared with transductive , which is Algorithm 1 with LEARN() being RankBoost and DIS-COVER() being Kernel PCA run 5 times with different ker-nels: -Polynomial kernel ( poly ), order=2 -Radial basis function kernel ( rbf ), bandwidth=1 -Diffusion kernel ( diff ), time constant=1, 10-nn graph -Diffusion kernel ( diff ), time constant=10, 10-nn graph -Linear kernel ( linear ) (corresponds to linear PCA) We create 25 additional features representing the first 5 prin-cipal components from each of the above 5 kernels. #query 50 75 106 #document 49k 74k 16k avg #doc/query 983.4 988.9 152.3 #relevant doc 516 1600 4.8k avg #relevant/query 1 0.6 28 avg #doc pairs 302k 262k 369k #feature (original) 44 44 25 #feature (transductive) 54 69 50
The third ranker ( combined ) performs a simple system combination of baseline and transductive . For each test query, it first normalizes the outputs y u of baseline and transductive , then returns the averages of these normalized outputs. While more sophisticated combination methods have been proposed (c.f. [16]), here we investigate whether a simple unweighted average is sufficient to give improve-ments.

Following LETOR convention, each dataset is divided into 5 folds with a 3:1:1 ratio for training, validation, and test set. We use the validation set to decide which kernels to use in the transductive system. The transductive sys-tem employs all 5 kernels (25 features) in TREC X 04 and OHSUMED, but only poly and rbf kernels (10 features) in TREC X 03. We decided not to use the validation set to pick features in a more fine-grained fashion (e.g. how many principal components, what kernel parameters) since we ex-pect the optimal settings to vary for each test query, and relying on RankBoost X  X  inherent feature selection ability is a more efficient and effective solution. For both transduc-tive and baseline , RankBoost is run for a total of 150 iterations, determined from initial experiments. We conjec-ture that transductive may be improved by automatically tuning for the best number of boosting iterations for each test query, but we leave that to future work.

The evaluation metrics are mean averaged precision (MAP) and normalized discount cumulative gain (NDCG@n) [12]. We report results from the average of 5-fold cross-validation and judge statistical significance using the dependent t-test.
Figure 2 compares the 3 systems on the 3 datasets. Table 2 shows the same results in numbers (boldfaced MAP/NDCG numbers indicate a statistically significant improvement (p &lt; 0.05) over baseline .) 4 The main observations are: 1. Both transductive and combined outperform base-2. For TREC X 03, all improvements are statistically signif-Our RankBoost baseline is comparable but different from LETOR[18], mainly due to different feature normalization (mean-variance vs. [0 , 1] scaling). For reference, we report the baseline score comparison in the following format: MAP(ours/LETOR) NDCG@1(ours/LETOR) TREC X 03: MAP(.1880/.2125) N@1(.3200/.2600) TREC X 04: MAP(.3524/.3835) N@1(.4400/.4800) OHSUMED: MAP(.4427/.4403) N@1(.5252/.4977) All results here are compared to our version of the baseline. Table 2: Main result (Figure 2 in table form). All transductive/combined improve on baseline. Statis-tically significant improvements are bold-fonted.
TREC X 03 baseline .1880 .3200 .2619 .2504 .2498 transductive .3226 .5000 .4512 .4140 .4092 combined .2627 .3600 .3568 .3670 .3444
TREC X 04 baseline .3524 .4400 .4060 .4037 .4294 transductive .3703 .4667 .4152 .4286 .4507 combined .3829 .4933 .4544 .4397 .4638
OHSUMED baseline .4427 .5252 .4707 .4479 .4274 transductive .4455 .5503 .4890 .4567 .4457 combined .4509 .5692 .4829 .4735 .4503 3. In TREC X 04 and OHSUMED, combined improves on
We believe these are very promising results which demon-strate that transductive learning (and the framework we pro-pose in Algorithm 1) has the potential to improve ranking.
We ask several questions in order to examine the prop-erties of the transductive framework in more detail. These questions seek to answer what is beneficial and not beneficial in the proposed framework. Are we seeing gains in transductive simply because Kernel PCA extracts good features per se, or particulary because the features are extracted on the test set (i.e. the trans-ductive aspect)? To answer this, we built a new system, KPCA on train , which runs Kernel PCA on each training list (as opposed to projecting the training lists to principal directions of the test lists). We then train RankBoost on this data, which is the same training data as baseline except for the additional Kernel PCA features. This new RankBoost is then evaluated on the test set. The results (Table 3) show that KPCA on train is worse than transductive (e.g. .2534 vs. .3226 MAP for TREC X 03), implying that transductive aspect of adapting to each test query is essential. Table 3: KPCA on test (transductive) vs. KPCA on train (inductive) . KPCA on train underperforms; adapting to test queries is a useful strategy.
 Kernel PCA features are in general difficult to interpret be-cause they involve non-linear combinations and the  X  gener-ated from the eigen-problem represents weights on samples, not features. We can only get a rough answer by computing the correlation between the values of a Kernel PCA feature and an original feature. Table 4 lists some features that cor-relate with particular kernel PCA features (e.g. in TREC X 04 query10, the Diffusion feature correlated highly with HITS). It is important to note, however, that this kind of analysis only serves as extra reference to help us understand par-ticular test queries: most Kernel PCA features have little correlation to original features. The average correlation on TREC X 04 is less than 0.15. What weak learners h (  X  ) in transductive  X  X  multiple rank-ing functions ( F u (  X  ) = P T t =1  X  t h t (  X  )) achieve large |  X  instance, how often are Kernel PCA features chosen com-pared to the original features? To analyze this, we look at the 25 transductive ranking functions in TREC X 04 that im-prove more than 20% over the baseline . For each ranking function, we look at the top 5 features and note their type: { original, polynomial, rbf, diffusion, linear } . 24 of 25 functions have both original and Kernel PCA features in Table 4: Some examples of original features that correlate highly with Kernel PCA features (coeff. of determination in parentheses). However, most features (not listed) have low correlation due to their non-linear relationship.
 Figure 3: Top 5 feature combinations employed in RankBoost, by count. There is a diversity of feature combinations that lead to good performance, indi-cating that different test queries require different rankers. the top 5, indicating that Kernel PCA features are quite useful. It is even more interesting to note the distribution of top 5 feature combinations (Figure 3): no single combina-tion is more prevalent than others. This again supports the intuition that test-specific rankers are better than a single general ranker. How important is the non-linear aspect of Kernel PCA? Would we have achieved similar gains if we restrict trans-ductive to perform only linear PCA? To test this, we trained new systems consisting of original features plus 5 linear PCA features, vs. original features + 5 polynomial, rbf, or dif-fusion kernel features. On TREC X 04, we observe the MAP scores, in order: .3701 (rbf), .3670 (poly), .3627 (diff), .3614 (linear). However, on TREC X 03, linear is not the worst: .3032 (diff), .2996 (linear), .2895 (poly), .2754 (rbf). Thus, non-linearity is important in most cases, but one should not expect non-linear kernels to always outperform linear ones. The best strategy is to employ multiple kernels. In Section 3, we present overall results averaged over all test queries. A more detailed analysis would include per-query MAP and NDCG. Figure 4 reports a histogram of queries that are improved vs. degraded by transductive . Figure 5: Scatterplot of TREC X 03 MAP results for transductive (x-axis) vs. baseline (y-axis).
 For each plot, the bars on the right side indicates the num-ber of queries that improved more than 1%, 20%, and 50% over baseline . Bars on the left side indicate the number of queries that become more than 1%, 20%, and 50% worse than baseline .

One observes that our transductive approach does not give improvements across all queries. We are seeing gains in Sec-tion 3 because the proportion of improved queries is greater than that of degraded queries (especially for TREC X 03).
It would be helpful to understand exactly the conditions where the transductive approach is beneficial vs. harm-ful. On TREC X 03, there is slight evidence showing that transductive seems to benefit queries with poorer base-lines (See Figure 5, scatterplot of baseline and transductive MAP scores). One hypothesis is that the original features of more difficult queries are not sufficiently discriminative, so Kernel PCA has more opportunity to show improvements. However, this trend is not observed in TREC X 04.

We also attempted to see if differences at the query level correlates with e.g. (a) number of documents, (b) number of relevant documents, (c) pairwise ranking accuracy in train-ing, but no single factor reliably predicts whether a query will be benefited by the transductive ranker. Ideally, both DISCOVER() and LEARN() need to operate at real-time since they are called for each test query. In our ex-periments on a Intel x86-32 (3GHz CPU), KernelPCA (im-plemented in Matlab/C-MEX) took on average 23sec/query for TREC and 4.3sec/query for OHSUMED; Rankboost (im-plemented in C++) took 1.4sec/iteration for TREC and 0.7sec/iteration for OHSUMED. The total compute time per query (assuming 150 iterations) is around 233sec/query for TREC and 109sec/query for OHSUMED. It remains to be seen whether real-time computation can be achieved by bet-ter code optimization or novel distributed algorithms.
There are generally two types of problems in  X  X earning to rank with partially-labeled data. X  In the problem we con-sider here, the document lists in our dataset are either fully labeled { d l } l =1 ..L , or not labeled whatsoever { d u The second type of problem is concerned with the case when a document list d is only partially-labeled, i.e. some docu-ments in d have relevance judgments, while other documents in the same list d do not. This second problem can arise when, e.g. (a) the document list retrieved by one query is too long and the annotator can only label a few documents, (b) one uses a implicit feedback mechanism [13] to generate la-bels and some documents simply cannot acquire labels with high confidence. So far the research community does not yet have precise terminology to differentiate the two prob-lems. Here we will call Problem One  X  X emi-supervised Rank Learning X  and Problem Two  X  X earning to Rank with Missing Labels X  .

Several methods have been proposed for the Missing La-bels problem, e.g. [36, 30, 10, 29]: the main idea there is to build a manifold/graph over documents and propagate the rank labels to unlabeled documents. One can use the propagated labels as the final values for ranking [36] (trans-ductive), or one can train a ranking function using these values as true labels [10, 29] (inductive). One important point about these label propagation methods is that they do not explicitly model the relationship that document d j is ranked above, say, d k . Instead it simply assumes that the label value for d j is higher than that of d k , and that this information will be preserved during propagation.
An alternative approach that explicitly includes pairwise ranking accuracy in the objective is proposed in [1]. It also builds a graph over the unlabeled documents, which acts as a regularizer to ensure that the predicted values are sim-ilar for closely-connected documents. [6] also proposes a graph-based regularization term, but in contrast to [1], it defines the graph nodes not as documents, but as document pairs . Just as the pairwise formulation allows one to ex-tend Boosting to RankBoost, this formulation allows one to adopt any graph-based semi-supervised classification tech-nique to ranking. However, generating all possible pairs of documents in a large unlabeled dataset quickly leads to in-tractable graphs.
 Most prior work consist of graph-based approaches for the Missing Labels problem. However, they may be extended to address the Semi-supervised Rank Learning problem if one defines the graph across both d l and d u . For instance, [29] investigates label propagation across queries, but concluded that it is computationally prohibitive. Beyond the compu-tational issue, however, how to construct a graph across dif-ferent queries (whose features may be at different scales and not directly comparable) is an open research question.
To the best of our knowledge, [27] 5 is the only work that tractably addresses the Semi-supervised Rank Learning prob-lem. First, it uses a supervised ranker to label the doc-uments in an unlabeled document list; next, it takes the most confident labels as seeds for label propagaton. A new supervised ranker is then trained to maximize accuracy on the labeled set while minimizing ranking difference to label propagation results. Thus this is a bootstrapping approach that relies on the initial ranker producing relatively accurate seeds.

Our feature-based approach differs from the above graph-based and bootstrapping approaches, and is more similar to work in feature extraction for domain adaptation [3, 21] or multi-task learning [2]. In fact, one can consider trans-ductive learning as an extreme form of domain adaptation, where one adapts only to the given test set.

Finally, we wish to note that various approaches incor-porating similar semi-supervised ideas have been explored in the IR community. For instance, query expansion by pseudo-relevance feedback [31] can be thought as a query-specific transductive techniques that uses the bootstrapping assumption (i.e. top retrieved documents are relevant and can be exploited).
We present a flexible transductive framework for learning ranking functions. The main intuition is based on extract-ing features from test documents and learning query-specific rankers. Our experiments using Kernel PCA with Rank-Boost demonstrate significant improvements on TREC and OHSUMED, and point to a promising area of further re-search. Possible extensions and future work include:
Although this paper has the same title as ours, the ap-proach is entirely unrelated. [1] S. Agarwal. Ranking on graph data. In ICML , 2006. [2] R. K. Ando and T. Zhang. A framework for learning [3] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira. [4] C. Burges, R. Ragno, and Q. Le. Learning to rank [5] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [6] W. Chu and Z. Ghahramani. Extensions of gaussian [7] K. Crammer and Y. Singer. Pranking with ranking. In [8] Y. Freund, R. Iyer, R. Schapire, and Y. Singer. An [9] X. Geng, T.-Y. Liu, T. Qin, and H. Li. Feature [10] J. He, M. Li, H.-J. Zhang, H. Tong, and C. Zhang. [11] R. Herbrich, T. Graepel, and K. Obermayer. Advances [12] K. J  X  arvelin and J. Kek  X  al  X  ainen. IR evaluation methods [13] T. Joachims. Optimizing search engines using [14] J. Kleinberg. Authoritative sources in a hyperlinked [15] I. Kondor and J. Lafferty. Diffusion kernels on graphs [16] J. H. Lee. Analysis of multiple evidence combination. [17] P. Li, C. Burges, and Q. Wu. McRank: Learning to [18] T.-Y. Liu, T. Qin, J. Xu, W. Xiong, and H. Li. [19] L. Mason, J. Baxter, P. Bartless, and M. Frean. [20] D. Metzler. Direct maximization of rank-based [21] R. Raina, A. Battle, H. Lee, B. Packer, and A. Ng. [22] S. Robertson. Overview of the Okapi projects. Journal [23] R. E. Schapire and Y. Singer. Improved boosting [24] B. Sch  X  olkopf, A. Smola, and K.-R. M  X  uller. Nonlinear [25] J. Shawe-Taylor and N. Cristianini. Kernel methods [26] A. Smola, O. Mangasarian, and B. Sch  X  olkopf. Sparse [27] T. Truong, M.-R. Amini, and P. Gallinari. Learning to [28] M.-F. Tsai, T.-Y. Liu, T. Qin, H.-H. Chen, and W.-Y. [29] J. Wang, M. Li, Z. Li, and W.-Y. Ma. Learning [30] J. Weston, R. Kuang, C. Leslie, and W. S. Noble. [31] J. Xu and W. B. Croft. Query expansion using local [32] J. Xu and H. Li. AdaRank: A boosting algorithm for [33] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A [34] C. Zhai and J. Lafferty. A study of smoothing [35] Z. Zheng, H. Zha, K. Chen, and G. Sun. A regression [36] D. Zhou, J. Weston, A. Gretton, O. Bousquet, and [37] X. Zhu. Semi-supervised learning literature survey.
