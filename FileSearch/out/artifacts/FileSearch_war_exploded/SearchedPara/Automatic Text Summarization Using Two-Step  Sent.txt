 and present the most important content to a user in a condensed form and in a man-and convey it in less space than original text. The most widespread summarization But some problems have occurred in both methods for text summarization. Despite they are very useful resources for summarization systems but a weak point of them approaches are easy to understand and implement, but generally they show low accuracy. ciently combine statistical approaches. By combining several statistical methods in two guistic method (DOCUSUM) [7]. Moreover, our method can also a have low cost and robust system architecture because it does not require any linguistic resources. combining two adjacent sentences for solving feature sparseness problem; it occurs in text summarization because of using only one sentence as the linguistic unit. And then gram pseudo sentences for calculating the importance of them. The goal of the first step is not to extract salient sentences but to remove noisy sentences. Thereafter, we can get more useful pseudo sentences through removing noisy pseudo sentences. Aggregation Similarity method [5] to the linear combination of the first step. Because the Aggregation Similarity method estimates the importance of sentences by calculat-system after removing the noisy sentences. Since our system carries out a summariza-higher performance than other statistical methods and DOCUSUM as a linguistic method. 
The rest of this paper is organized as follows. Section 2 describes related works in tion, we draw conclusions and present future works. The summarization system has two main categories: Linguistic approaches and Statis-clause structural information in a sentence by using the linguistic resources while the latter uses title, frequency, location, and cue words and so on. 2.1 Linguistic Approaches Bazilay and Elhadad constructed Lexical Chain by calculating semantic distance tences related to these strong chains are chosen as a summary. The methods which use semantic relations between words depend heavily on manually constructed resources such as WordNet [12]. WordNet is not available in several languages such as Korean and this kind of linguistic resources are hard to maintain. To overcome the limitation of this problem , Ko, Kim, and Seo constructed Lexical more loosely connected than Lexical Chains. They call their system DOCUSUM. 
Marcu used discourse structural information [11]. This is based on contextual struc-ture through analyzing sentence relations and sentence semantics. time and expansion manner, they leave much room for improvement. 2.2 Statistical Approaches table. Therefore, this method ignores the semantic content of words and their potential membership in multi-word phrases. the presence of words such as significant, hardly, impossible signals topic sentences. 
A query-based summarization makes a summary by extracting relevant sentences from a document [3]. The criterion for ex traction is given as a query. The probability occurred in the query and a sentence. 3.1 Overall Architecture several weak points: feature sparseness and low performance. The former problem is pending on the particular format and the style of writing. 
In order to successfully deal with the feature sparseness problems, we made an as-sumption. The salient sentences are grouped at a definite position without regarding to location of subjective sentences. To apply it to our system, we combine two adjacent the feature sparseness problem in part. 
In order to improve the performance of statistical methods regardless of the particu-lar formats, we make an efficient combination of them by means of estimating various statistical methods. 
In the first step, the system estimates the importance score of bi-gram pseudo sen-valuable bi-gram pseudo sentences which are called by noisy data. In the second step, separated from remaining bi-gram pseudo sentences. Here, we add Aggregation Simi-larity method to our combination method to achieve better performance. 
The Fig. 1 shows the architecture of our summarization system. 3.2 General Statistical Methods Our system uses several statistical methods. We here present several typical statistical methods. from title in Boolean weighted vector space model. The inner product method is ex-ploited for similarity between a sentence and a query. in i -th sentence and w qk is the weight of k -th word in the query. Location Method. It has been said that the leading several sentences of an article are important and a good summary [15]. Therefore, the leading sentences in compression rate are extracted as a summary by the location method. where S i is an i -th sentence and N is the total number of sentences in the text. Aggregation Similarity Method. The score of a sentence is calculated as the sum of similarity with other all sentence vectors in document vector space model. Each score is computed as follows [5]. 
Equation 4 calculates the similarity with other sentence and w ik is the weight of k -th word in i -th sentence. Frequency Method. The frequency of term occurrences within a document has often been used for calculating the importance of sentences [14]. In this method, the score of a sentence can be calculated as the sum of the score of words in the sentence. The as follows [13]. texts, and df i is the document frequency of word i in the whole data set. 3.3 The TF-Based Query Method As described above, the title has usually been used for a query and the Title method has shown higher performance than other approaches in general. However, in special cases, it can be hard to extract a title from documents or any style of documents has no-title. For these cases, we propose a method to extract topic words for a query. The frequency in a document. This method considers words with high frequency as impor-tant concepts such as [8]. 
Like the Title method, the inner product metric is used as the similarity measure be-common nouns are used after eliminating stop words. For sentence vectorization, the Boolean weighting is used as follows: However, binary representation has generally showed better performance in summari-zation [4]. By the following Equation 7, we calculate the similarity between sentences and the TF-based query. 
To verify our TF-based query method, we did experiments according to the number when using one topic word, the most frequent word . 3.4 The Combination of Statistical Methods in Two Steps Before introducing our method in detail, we observed the performance of each statis-performance of general statistical methods are shown in Table 2. ments, the TF-based method is used instead of the Title method. 3.4.1 Removing the Noisy Sentences in the First Step In the first step, our goal is to reduce noisy sentences. First of all, our system gener-Then, since Title and Location methods show high performances, our system linearly combines these methods in the first step as follows: where notations in this Equation follow those of Equation (1) and (3). After all the bi-because they are regarded as noisy sentences. 3.4.2 Extracting Summary in the Second Step After the first step, the system can get more salient bi-gram pseudo sentences. There-sentences as the important score of a sentence. The final Equation is as follows: method. method as shown in the following Equations (11) and (12). 4.1 Data Sets and Experimental Settings The articles consist of several genres such as politics, culture, economics, and sports. Each test document has title, content, 30 % summary, and 10 % summary. The 30 % and 10 % summaries of the test document are made by manually extracting sentences from content. In our experiments, we used 841 document-summary pairs after elimi-nating duplicate articles and inadequate summary pairs although the size of summari-zation test set was reported as 1,000 documents [6]. documents as a validation set which are selected at random. Hence, the test set in our experiments is composed of the rest of data set (561 documents). Equation (13). where P is precision and R is recall. 4.2 Experimental Results 4.2.1 Comparing with One-Step Combination the performance of Two-step combination method with that of One-step combination combination) as shown in Fig. 2. Next, to verify the addition of Aggregation Similar-the 10% summary and 1.5% advanced score on the 30% summary. Similarity combination) showed 1.1% and 2.3 % advanced scores higher than TfQ&amp;L (One-step TF-based query and Location combination) in 10% and 30% summary respectively as shown in Table 4. 
Note that we chose T&amp;L method for One-step because it showed the best perform-ance in above both two cases (especially better than T&amp;L&amp;A ). 4.2.2 Comparing with Other Summarization Methods In this section, we make a comparison between our Two-step method and other sys-tems such as Title, Location, and DOCUSUM. Especially, DOCUSUM is used for comparison with linguistic approaches. than title, location and even DOCUSUM. Even though DOCUSUM used knowledge resource such as context vector space for lexical clustering, it rather showed 1.2% and 5% lower performance than our method in 10% summary and 30% summary respec-tively. 
Moreover, we conducted experiments in no-title case. DOCUSUM* used only a topic keywords query without a title query extracted by its lexical clustering method. any other method (even DOCUSUM). pseudo sentences to solve feature sparseness problem and Two-step combination than other statistical methods and DOCUSUM. Even though our system does not use any knowledge resource, it reported much better performance than DOCUSUM. In this paper, we implemented the high performance summarization system only to statistical methods. Two-step sentence extraction method to multi-document summarization. We will plement new multi-document summarization system by using Two-step sentence extraction method. This work was supported by grant No. R01-2003-000-11588-0 from the basic Re-
