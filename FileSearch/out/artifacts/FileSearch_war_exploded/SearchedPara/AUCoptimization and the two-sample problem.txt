 The statistical problem of testing homogeneity of two samples arises in a wide variety of appli-cations, ranging from bioinformatics to psychometrics through database attribute matching for in-stance. Practitioners may rely upon a wide range of nonparametric tests for detecting differences in distribution (or location) between two one-dimensional samples, among which tests based on lin-ear rank statistics , such as the celebrated Mann-Whitney Wilcoxon test. Being a (locally) optimal procedure, the latter is the most widely used in homogeneity testing. Such rank statistics were orig-inally introduced because they are distribution-free under the null hypothesis, thus permitting to set critical values in a non asymptotic fashion for any given level. Beyond this simple fact, the cru-cial advantage of rank-based tests relies in their asymptotic efficiency in a variety of nonparametric situations. We refer for instance to [15] for an account of asymptotically (locally) uniformly most powerful tests and a comprehensive treatment of asymptotic optimality of R -statistics. In a different context, consider data sampled from a feature space X  X  R d of high dimension with binary label information in { X  1 , +1 } . The problem of ranking such data, also known as the bipartite ranking problem, has recently gained an increasing attention in the machine-learning literature, see [5, 10, 19]. Here, the goal is to learn, based on a pooled set of labeled examples, how to rank novel data with unknown labels, by means of a scoring function s : X  X  R , in order that positive ones appear on top of the list. Over the last few years, this global learning problem has been the subject of intensive research, involving issues related to the design of appropriate criteria reflecting ranking performance or valid extensions of the Empirical Risk Minimization approach (ERM) to this framework [2, 6, 11]. In most applications, the gold standard for measuring the capacity of a scoring function s to discriminate between the class populations however remains the area under the ROC curve criterion ( AUC ) and most ranking/scoring methods boil down to maximizing its empirical counterpart. The empirical AUC may be viewed as the Mann-Whitney statistic based on the images of the multivariate samples by s , see [13, 9, 12, 18].
 The purpose of this paper is to investigate how ranking methods for multivariate data with binary labels may be exploited in order to extend the rank-based test approach for testing homogeneity between two samples to a multidimensional setting. Precisely, the testing principle promoted in this paper is described through an extension of the Mann-Whitney Wilcoxon test, based on a preliminary ranking of the data through empirical AUC maximization. The consistency of the test is proved to hold, as soon as the learning procedure is consistent in the AUC sense and its capacity to detect  X  X mall X  deviations from the homogeneity assumption is illustrated by a simulation example. The rest of the paper is organized as follows. In Section 2, the homogeneity testing problem is formulated and standard approaches are recalled, with focus on the one-dimensional case. Section 3 highlights the connection of the two-sample problem with optimal ROC curves and gives some insight to our appproach. In Section 4, we describe the testing procedure proposed and set prelimi-nary grounds for its theoretical validity. Simulation results are presented in Section 5 and technical details are deferred to the Appendix. We start off by setting out the notations needed throughout the paper and formulate the two-sample problem precisely. We recall standard approaches to homogeneity testing. In particular, special attention is paid to the one-dimensional case, for which two-sample linear rank statistics allow for constructing locally optimal tests in a variety of situations.
 Probabilistic setup. The problem considered in this paper is to test the hypothesis that two inde-pendent i.i.d. random samples, valued in R d with d  X  1 , X + 1 , ..., X + n and X  X  1 , ..., X  X  m are identical in distributions. We denote by G ( dx ) the distribution function of the X + i  X  X , while the one of the X  X  j  X  X  is denoted by H ( dx ) . We also denote by P ( G,H ) the probability distribution on the underlying space. The testing problem is tackled here from a nonparametric perspective, meaning that the distributions G ( dx ) and H ( dx ) are assumed to be unknown. We suppose in addition that G ( dx ) and H ( dx ) are continuous distributions and the asymptotics are described as follows: we set N = m + n and suppose that n/N  X  p  X  (0 , 1) as n, m tend to infinity. Formally, the problem is to test the null hypothesis H 0 : G = H against the alternative H 1 : G 6 = H , based on the two data sets. In this paper, we place ourselves in the difficult case where G and H have same support, X  X  R d say.
 Measuring dissimilarity. A possible approach is to consider a probability (pseudo)-metric D on the space of probability distributions on R d . Based on the simple observation that D ( G,H ) = 0 under the null hypothesis, possible testing procedures consist of computing estimates b G n and b H m of the instance. Beyond computational difficulties and the necessity of identifying a proper standardization in order to make the statistic asymptotically pivotal ( i.e. its limit distribution is parameter free), the major issue one faces when trying to implement such plug-in procedures is related to the curse of dimensionality. Indeed, plug-in procedures involve the consistent estimation of distributions on a feature space of possibly very large dimension d  X  N  X  .
 Various metrics or pseudo-metrics can be considered for measuring dissimilarity between two proba-bility distributions. We refer to [17] for an excellent account of metrics in spaces of probability mea-sures and their applications. Typical examples include the chi-square distance, the Kullback-Leibler divergence, the Hellinger distance, the Kolmogorov-Smirnov distance and its generalizations of the following type where F denotes a supposedly rich enough class of functions f : X  X  R d  X  R , so that MMD(G , H) = 0 if and only if G = H . The quantity (1) is called the Maximum Mean Dis-crepancy in [1], where a unit ball of a reproducing kernel Hilbert space H is chosen for F in order to allow for efficient computation of the supremum (1), see also [23]. The view promoted in the present paper for the two-sample problem is very different in nature and is inspired from traditional procedures in the particular one-dimensional case.
 The one-dimensional case. A classical approach to the two-sample problem in the one-dimensional setup lies in ordering the observed data using the natural order on the real line R and then basing the decision depending on the ranks of the positive instances among the pooled sample: functions G and H respectively. This approach is grounded in invariance considerations, practical simplicity and optimality of tests based on R -estimates for this problem, depending on the class of alternative hypotheses considered. Assuming the distributions G and H continuous, the idea underlying such tests lies in the simple fact that, under the null hypothesis, the ranks of positive instances are uniformly distributed over { 1 , ..., N } . A popular choice is to consider the sum of  X  X ositive ranks X , leading to the well-known rank-sum Wilcoxon statistic [22] which is distribution-free under H 0 , see Section 6.9 in [15] for further details. We also recall that, the validity framework of the rank-sum test classically extends to the case where some observations are tied ( i.e. when G and/or H may be degenerate at some points), by assigning the mean rank to ties [4]. We shall denote by W n,m the distribution of the (average rank version of the) Wilcoxon statistic c W n,m under the homogeneity hypothesis. Since tables for the distributions W n,m are available, no asymptotic approximation result is thus needed for building a test of appropriate level. As it will be recalled below, the test based on the R -statistic c W n,m has appealing optimality properties for certain classes of alternatives. Although R -estimates ( i.e. functions of the R i  X  X ) form a very rich collection of statistics, but, for lack of space, we restrict our attention to the two-sample Wilcoxon statistic in this paper.
 Heuristics. We may now give a first insight into the way we shall tackle the problem in the multi-dimensional case. Suppose that we are able to  X  X roject X  the multivariate sampling data onto the real line through a certain scoring function s : R d  X  R in order to preserve the possible dissimilarity (considered in a certain specific sense, which we shall discuss below) between the two populations, leading then to  X  X arge X  values of the score s ( x ) for the positive instances and  X  X mall X  values for the negative ones with high probability. Now that the dimension of the problem has been brought down to 1 , observations can be ranked and one may perform for instance a basic two-sample Wilcoxon test based on the data sets s ( X + 1 ) , ..., s ( X + n ) and s ( X  X  1 ) , ..., s ( X  X  m ) . Remark 1 (L EARNING A S TUDENT t TEST .) We point out that it is precisely the task Linear Discriminant Analysis (LDA) tries to performs, in a restrictive Gaussian framework however (when G and H are normal distributions with same covariance structure namely). In order to test deviations from the homogeneity hypothesis on the basis of the original samples, one may consider applying a i  X  m } , where b  X  denotes the empirical discriminant function, this may be shown as an appealing alternative to multivariate extensions of the standard t test [14].
 The goal of this paper is to show how to exploit recent advances in ROC / AUC optimization for extending this heuristics to more general situations than the parametric one mentioned above. ROC curves are among the most widely used graphical tools for visualizing the dissimilarity be-tween two one-dimensional distributions in a large variety of applications such as anomaly detection in signal analysis, medical diagnosis, information retrieval, etc. As this concept is at the heart of the ranking issue in the binary setting, which forms the first stage of the testing procedure sketched above, we recall its definition precisely.
 Definition 1 ( ROC curve) Let g and h be two cumulative distribution functions on R . The ROC curve related to the distributions g ( dt ) and h ( dt ) is the graph of the mapping: f : R  X  R . When the distributions g ( dt ) and h ( t ) are continuous, it can alternatively be defined as the parametric curve t  X  R 7 X  (1  X  h ( t ) , 1  X  g ( t )) .
 One may show that ROC ((g , h) ,  X  ) is above the diagonal  X  :  X   X  [0 , 1] 7 X   X  of the ROC space if and only if the distribution g is stochastically larger than h and it is concave as soon as the likelihood ratio dg/dh is increasing. When g ( dt ) and h ( dt ) are both continuous, the curves ROC((g , h) ,. ) and ROC((h , g) ,. ) are symmetric with respect to the diagonal of the ROC space with slope equal to one. Refer to [9] for a detailed list of properties of ROC curves.
 The notion of ROC curve provides a functional measure of dissimilarity between distributions on R : the closer to the corners of the unit square the curve ROC ((g , h) ,  X  ) is, the more dissimilar the distributions g and h are. For instance, it exactly coincides with the upper left-hand corner of the unit square, namely the curve  X   X  [0 , 1] 7 X  I {  X   X  ]0 , 1] } , when there exists l  X  R such that the support merges with the diagonal  X  when g = h . Hence, distance of ROC ((g , h) ,  X  ) to the diagonal may be naturally used to quantify departure from the homogeneous situation. The L 1 -norm provides a convenient way of measuring such a distance, leading to the classical AUC criterion ( AUC standing for area under the ROC curve ): The popularity of this summary quantity arises from the fact that it can be interpreted in a proba-bilistic fashion, and may be viewed as a distance between the locations of the two distributions. In this respect, we recall the following result.
 Proposition 1 Let g and h be two distributions on R . We have: where Z and Z 0 denote independent random variables, drawn from g ( dt ) and h ( dt ) respectively. We recall that the homogeneous situation corresponds to the case where AUC(g , h) = 1 / 2 and the Mann-Withney statistic [16] is exactly the empirical counterpart of AUC(g , h) . It yields exactly the same statistical decisions as the two-sample Wilcoxon statistic, insofar they are related as follows: For this reason, the related test of hypotheses is called Mann-Whitney Wilcoxon test ( MWW ). Multidimensional extension. In the multivariate setup, the notion of ROC curve can be extended the following way. Let H ( dx ) and G ( dx ) be two given distributions on R d and S = { s : X  X  R | s Borel measurable } . For any scoring function s  X  S , we denote by H s ( dt ) and G s ( t ) the images of H ( dx ) and G ( x ) by the mapping s ( x ) . In addition, we set for all s  X  X  : multivariate probability measures G and H . One may thus consider evaluating the dissimilarity between H ( dx ) and G ( dx ) on R d through the family of curves { ROC(s ,. ) } s  X  X  or through the collection of scalar values { AUC(s) } s  X  X  . Going back to the homogeneity testing problem, the null assumption may be reformulated as The next result, following from standard Neyman-Pearson type arguments, shows that the supremum sup s  X  X  AUC(s) is attained by increasing transforms of the likelihood ratio  X  ( x ) = dG/dH ( x ) , x  X  X  . Scoring functions with largest AUC are natural candidates for detecting the alternative H 1 . Theorem 1 (O PTIMAL ROC CURVE .) The set of S  X  = { T  X   X  | T : R  X  R strictly increasing } defines the collection of optimal scoring functions in the sense that:  X  s  X  X  , with the notations ROC  X  ( . ) = ROC(s  X  ,. ) and AUC  X  = AUC(s  X  ) for s  X   X  X   X  .
 Refer to Proposition 4 X  X  proof in [9] for a detailed argument. Notice that, as dG/dH ( X ) = dG  X  ( X ) /dH  X  (  X  ( X )) , replacing X by s  X  ( X ) with s  X   X  S  X  leaves the optimal ROC curve un-touched. The following corollary is straightforward.
 Corollary 1 For any s  X  X   X  , we have: sup s  X  X  | AUC(s)  X  1 / 2 | = AUC(s  X  )  X  1 / 2 . Consequently, the homogeneity testing problem may be seen as closely related to the problem of estimating the optimal AUC  X  , since it may be re-formulated as follows: Knowing how a single optimal scoring function s  X   X  S  X  ranks observations drawn from a mixture of G and H is sufficient for detecting departure from the homogeneity hypothesis in an optimal fash-estimate of AUC  X  and thus yields an asymptotically (locally) uniformly most powerful test. Let F ( dx ) = pG ( dx ) + (1  X  p ) H ( dx ) and denote by F s ( dt ) the image of the distribution F by s  X  S . Notice that, for any s  X   X  S  X  , the scoring function S  X  = F s  X   X  s  X  is still optimal and the score variable S  X  ( X ) is uniformly distributed on [0 , 1] under the mixture distribution F (in addition, it may be easily shown to be independent from s  X   X  S  X  ). Observe in addition that AUC  X   X  1 / 2 may be viewed as the Earth Mover X  X  distance between the class distributions H S  X  and G S  X  for this  X  X ormalization X : Empirical AUC maximization. A natural way of inferring the value of AUC  X  and/or selecting a scoring function  X  s with AUC nearly as large as AUC  X  is to maximize an empirical version of the AUC criterion over a set S 0 of scoring function candidates. We assume that the class S 0 is sufficiently rich in order to guarantee that the bias AUC  X   X  sup s  X  X  plexity is controlled (when measured for instance by the VC dimension of the collection of sets {{ x  X  X : s ( x )  X  t } , ( s,t )  X  S 0  X  R } as in [7] or by the order of magnitude of conditional Rademacher averages as in [6]). We recall that, under such assumptions, universal consistency results have been established for empirical AUC maximizers, together with distribution-free gener-alization bounds, see [2, 6] for instance. We point out that this approach can be extended to other relevant ranking criteria. The contours of a theory guaranteeing the statistical performance of the ERM approach for empirical risk functionals defined by R -estimates have been sketched in [8]. Assume that data have been split into two subsamples: the first data set D n 0 ,m 0 = { X + 1 , ..., X + n the second data set D 0 n compute a pseudo-two-sample Wilcoxon test statistic from the ranked data. We set N 0 = n 0 + m 0 and N 1 = n 1 + m 1 and suppose that n i /N i  X  p as n i and m i tend to infinity for i  X  X  0 , 1 } . Let  X   X  (0 , 1) . The testing procedure at level  X  is then performed in two steps, as follows. The next result shows that the learning step does not affect the consistency property, provided it outputs a universally consistent scoring rule.
 Theorem 2 Let  X   X  (0 , 1 / 2) and suppose that the ranking/scoring method involved at step 1 yields a universally consistent scoring rule  X  s in the AUC sense. The score-based rank-sum Wilcoxon test is universally consistent as n i and m i tend to  X  for i  X  X  0 , 1 } at level  X  , in the following sense. Remark 2 (C ONVERGENCE RATES .) Under adequate complexity assumptions on the set S 0 over which empirical AUC maximization or one of its variants is performed, distribution-free rate bounds for the generalization ability of scoring rules may be established in terms of AUC , see Corollary 6 in [2] or Corollary 3 in [6]. As shown by a careful examination of Theorem 2, this permits to derive a convergence rate for the decay of the score-based type II error of MWW under any given alternative ( G,H ) , when combined with the Berry-Esseen theorem for two-sample U -statistics. For instance, if a typical 1 / rate of order O P Remark 3 (I NFINITE -DIMENSIONAL FEATURE SPACE .) We point out that the method presented here is by no means restricted to the case where X is of finite dimension, but may be applied to functional input data, provided an AUC -consistent ranking procedure can be applied in this context. The procedure proposed above is extremely simple once the delicate AUC maximization stage is performed. A stunning property is the fact that critical thresholds are set automatically, with no ref-erence to the data. We firts consider a low-dimensional toy experiment and display some numerical results. Two independent i.i.d. samples of equal size m = n = N/ 2 have been generated from two conditional 4 -dimensional gaussian distributions on the hypercube [  X  2 , 2] 4 . Their parameters are denoted by  X  + and  X   X  for the means and  X  is their common covariance matrix. Three cases have been considered. The first example corresponds to a homogeneous situation:  X  + =  X   X  =  X  1 Eventually, the third example corresponds to a much more difficult problem,  X  X lose X  to H 0 , where  X  each of these examples is illustrated by Fig. 2 in terms of (optimal) ROC curve. The table in Fig. 2 gives Monte-Carlo estimates of the power of three testing procedures when  X  = 0 . 05 (averaged over B = 150 replications): 1) the score-based MWW test, where ranking is performed using the scoring function output by a run of the T REE R ANK algorithm [9] on a training sample D n 0 ,m 0 , 2) the LDA-based Student test sketched in Remark 1 and 3) a bootstrap version of the MMD-test with a Gaussian RBF Kernel proposed in [1].
 Figure 1: Powers and ROC curves describing the  X  X istance X  to H 0 for each situation: example 1 (red), example 2 (black) and example 3 (blue).
 on R d are generated, with larger values for the input space dimension d  X  { 10 , 30 } . We considered several problems at given toughness. The increasing difficulty of the testing problems considered is controlled through the euclidian distance between the means  X   X  = ||  X  +  X   X   X  || and is described by Fig. 2, which depicts the related ROC curves, corresponding to situations where level  X  = 0 . 05 : the score-based MWW test, where ranking is again performed using the scoring function output by a run of the T REE R ANK algorithm on a training sample D n 0 ,m 0 , the KFDA test proposed in [23], a bootstrap version of the MMD-test with a Gaussian RBF Kernel ( MMD ) and another version, with moment matching to Pearson curves ( MMD mom ), using also with a Gaussian RBF kernel (see [1]). Monte-Carlo estimates of the corresponding powers are given in the Table displayed in Fig. 2. We have provided a sound strategy, involving a preliminary bipartite ranking stage, to extend clas-sical approaches for testing homogeneity based on ranks to a multidimensional setup. Consistency of the extended version of the popular MWW test has been established, under the assumption of universal consistency of the ranking method in the AUC sense. This principle can be applied to other R -statistics, standing as natural criteria for the bipartite ranking problem [8]. Beyond the illus-trative preliminary simulation example displayed in this paper, we intend to investigate the relative efficiency of such tests with respect to other tests standing as natural candidates in this setup. to W n 1 ,m 1 under the null hypothesis. For any distribution H , we thus have:  X   X   X  (0 , 1 / 2) , Taking the expectation, we obtain that the test is of level  X  for all n , m . Figure 2: Power estimates and ROC curves describing the  X  X istance X  to H 0 for each situation: case 1 (black), case 2 (blue), case 3 (green) and case 4 (red).
 For any s  X  S , denote by U n 1 ,m 1 ( s ) the empirical AUC of s evaluated on the sample D 0 n Recall first that it follows from the two-sample U -statistic theorem (see [20]) that:  X  as n , m tend to infinity. In particular, for any pair of distributions ( G,H ) , the centered random variable p )) for any s  X  X  such that the distribution H s ( dt ) is continuous. Refer to Theorem 12.4 in [21] for further details.
 We now place ourselves under an alternative hypothesis described by a pair of distinct distribution evaluated on the sample D 0 n be bounded by: where Observe that, by virtue of the CLT recalled above, converges to z  X  / p 12 p (1  X  p ) . Now, the fact that type II error of  X  converges to zero as n i and m i tend to  X  for i  X  { 0 , 1 } immediately follows from the assumption in regards to the AUC of  X  s ( x ) universal consistency and the CLT for two-sample U -statistics combined with the theorem of dominated convergence. Due to space limitations, details are omitted.
