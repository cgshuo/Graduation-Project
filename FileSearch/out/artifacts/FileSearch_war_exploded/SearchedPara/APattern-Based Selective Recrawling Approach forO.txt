 Traditional recrawling methods learn navigation patterns in order to crawl related web pages. However, they cannot re-move the redundancy found on the web, especially at the object level. To deal with this problem, we propose a new hypertext resource discovery method, called  X  X elective re-crawling X  for object-level vertical search applications. The goal of selective recrawling is to automatically generate URL patterns, then select those pages that have the widest cov-erage, and least irrelevance and redundancy relative to a pre-defined vertical domain. This method only requires a few seed objects and can select the set of URL patterns that covers the greatest number of objects. The selected set can continue to be used for some time to recrawl web pages and can be renewed periodically. This leads to significant savings in hardware and network resources.

In this paper we present a detailed framework of selective recrawling for object-level vertical search. The selective re-crawling method automatically extends the set of candidate websites from initial seed objects. Based on the objects ex-tracted from these websites it learns a set of URL patterns which covers the greatest number of target objects with lit-tle redundancy. Finally, the navigation patterns generated from the selected URL pattern set are used to guide future crawling. Experiments on local event data show that our method can greatly reduce downloading of web pages while maintaining comparative object coverage.
 H.3.3 [ Information Search and Retrieval ]: Search pro-cess Algorithms, Design, Experimentation.
 crawling, vertical search, object page
The rapid growth and change in the information needs of the World Wide Web poses unprecedented scaling challenges for general-purpose crawlers and search engines. Searching for information is a primary activity on the web. Most exist-ing search engines index and query web resources at the level of the web page. In recent years, object-level vertical search (OLVS, X  X ertical search X  X or products, locations, people, spe-cific services, or academic relationships) has become one of the main goals of the web search community. There are already several successful systems, such as Google scholar, Yahoo! and Bing shopping. In general terms, objects are real-world entities. OLVS engines discover the target web resources, extract structural data objects and then enable more precise queries to be carried out upon them [16]. In this procedure, data extraction [12] and web database schema matching [19] have been research hotspots. However, the task of crawling relevant web pages has not received much attention.

The goal of the OLVS crawler is to find object pages (web pages that contain target objects). The crawled web pages are passed to the corresponding object extractor to extract structured object information and build the object ware-house. A number of web resources (such as E-Commerce portals, home pages, the websites of special interest groups, etc.) focus on specific content categories and only provide information for objects related to a specific vertical domain. However, information on these websites tends to be frag-mented and incomplete. Moreover, as many sites contain similar content, there is great redundancy among them.
For example, a local entertainment event search engine may contain information about concerts. In this system, an object is defined as an event tuple consisting of three elements, (name, location, time). The system will extract the title of the concert as name, where it is held as loca-tion, and when it takes place as time. Figure 1 shows three object pages with concert information. Page 1 and page 2 are screenshots from www.culture.sh.cn , and page 3 is a screenshot from en.piao.com.cn . They all contain the same information about  X  X he Philharmonia Orchestra 2010 Shanghai Concert X . It is clear that redundancy not only exists between websites but also within the same site.
One straightforward way to obtain such information is to filter the target web pages. However, the filtering method requires a priori acquisition of a large amount of web pages. This is time consuming and therefore impractical for many application scenarios [14]. Nie et al. [16] describe a focused crawler [5] that uses the heuristic page classifier and the par-P age 1: Event information about  X  X he Philharmonia Orchestra 2010 Shanghai Concert X  from www.culture.sh.cn .
 tial object relationship graph to guide the crawling process. However, as discussed by Lin et al. [14], this link-based search strategy is inefficient as the target resources of OLVS often do not link to each other (for example E-Commerce portals). Lin et al. [14] present a meta-search based method which used general-purpose search engines to find relevant websites by submitting queries composed of representative data instances. However, this method only finds the web-sites, which may only contain a small percentage of relevant pages. None of these methods take redundancy into account.
To deal with the problem, we propose a new hypertext resource discovery method called  X  X elective recrawling X . Se-lective recrawling is based on the recrawling method and navigation pattern. It only requires several initial seeds as input, and uses a URL pattern set to represent the tar-get resource. Our proposed selective recrawling method is divided into five stages. The first stage uses a two-phase strategy for website extension. Phase one uses seed objects to retrieve seed websites, in the second phase more objects are extracted from the seed websites and used to retrieve a set of candidate websites. In the second stage, it crawls all pages from each website, extracts all objects and builds distinct object clusters. Thirdly, it inducts URL patterns for object pages. Fourthly, it selects a URL pattern set that encompasses the greatest amount of objects with the fewest irrelevant and redundant pages. Finally, navigation patterns are generated based on the selected URL pattern set. These navigation patterns can be used to recrawl the web pages for some time. This leads to significant savings in hardware and network resources.

This paper focuses on the detailed framework of the se-lective recrawling method. The second section describes the representation of the key concepts including object, URL pattern and navigation pattern. The third section intro-duces the procedures for navigation pattern learning for se-lective recrawling. The experimental results are shown in the fourth section and section five reviews related work. Fi-nally, in section six we draw some conclusions and discuss future work.
Objects are real world entities. Vertical search engines in different domains collect different types of objects. An object usually has several elements (attributes), and can be represented as a tuple. For example, event objects may be represented by a tuple of three elements (name, location, time) and book objects may be represented by a tuple of four elements (title, author, publisher, year). The informa-tion extraction subsystem of OLVS engines should extract all possible tuples from web pages with properly assigned elements, where the i -th tuple, T i , is formally defined as: where t k i is the k -th element of the i -th tuple, which iden-tifies a specific attribute of the object.

The web pages illustrated in page 1 and page 3 of Figure 1 contain one local event object and page 2 contains several. Most of the events are concerts, plays, sporting events, etc. The object extraction subsystem will extract the title or brief description of an event as name, where it is held as location, and when it takes place as time. Table 1 lists the object tuples that have been extracted from the three web pages in Figure 1.

Once the objects are extracted from the web pages, a cor-responding clustering procedure detects and merges dupli-cate objects. Table 1 shows that the second tuple extracted from page 2 refers to the same object found in pages 1 and 3.
A URL pattern is represented as a regular expression and describes a group of pages with similar URLs. In the cur-rent example, many of the object pages X  URLs can only be differentiated by their parameters; they are obviously gen-erated from a same database. Because the URL parameters are always at the end of the string, one simple method to generate URL patterns is to identify the prefix of these sim-4th rows). il ar URLs. In this paper, we use the longest common URL prefix as the URL pattern.

For example, the URLs in the first four rows of Table 2 are similar and their corresponding pages have the same layout. These URLs all use the same page template, but the data displayed on the page depends upon the id parameter in the URL. Using the id parameter these pages draw specific data from the background database. Their longest common prefix, as illustrated in the 5th row of Table 2, can easily be inducted to represent this cluster of URLs.
A traditional crawler follows every links it detects, result-ing in the download of many irrelevant and redundant pages. An OLVS system only downloads object pages and does not require duplication at the object-level. As object extraction is a time and resource consuming task, reducing download-ing leads to savings in both bandwidth and computational resources. In order to reduce unnecessary downloading, the crawler is guided by the navigator to only crawl targeted pages. In our system, we use the navigation pattern tech-nique to generate the navigator.

The concept of Navigation Pattern ( NP ) was first in-troduced by Lage et al.[13] to guide the navigation of a crawler through a target website in order to retrieve relevant pages. Basically, NP-based crawling adopts a breadth-first traversal of the target website. The NPs are represented as a level-based URL pattern set and each level contains a list of URL patterns. These represent links in the level that the crawler should follow to reach the target pages.

The NPs are generated for each website, which is repre-sented as a tree structure with pages as nodes and edges as the links that the crawler follows. URL patterns are sum-marized for each level of the tree. Table 3 shows a simple example of a three-level navigation pattern for a local event search engine.
 Table 3: A simple example of a three-level naviga-tion pattern for www.culture.sh.cn .

T able 3 shows that the first level is the homepage of the website. The second level has three URL patterns, one of which ( www.culture.sh.cn/alltickets en. htm ) is the URL of the page which lists current events. The third level has two URL patterns, which are the same as the last two patterns of the second level. A possible reason is that the breadth-first traversal cannot visit all the pages that match the same URL patterns within the limited levels.

Next, the crawler is guided by the NPs to only travel and crawl those pages that match the URL patterns of each level.
In this section we propose a framework for selective re-crawling (illustrated in Figure 2). There are two phases: the pattern learning phase (left of the dotted line) and the recrawling phase (right of the dotted line). The recrawling phase basically uses the navigation pattern based method described in [20]. The learning phase simply uses the classic breadth-first crawler for general purpose crawling, and the object extraction and clustering systems are domain related. Therefore, we will focus on the learning phase, specifically the four items shown in bold in Figure 2.

Figure 2 shows that in the learning phase, the system URL Pattern Selection Navigation Pattern Generation first finds websites relevant to seed objects. Secondly, it crawls all the pages of these websites, extracts all the objects and builds distinct object clusters. Thirdly, it inducts URL patterns for object pages. Fourthly, it selects a URL pattern set that encompasses the greatest number of objects with the fewest irrelevant and redundant pages. Finally, navigation patterns are generated based on the selected URL pattern set. In the recrawling phase, the navigation patterns are used to guide future crawling.
The proposed system takes seed objects as input and uses general-purpose search engines (e.g. Google, Yahoo!) to find websites relevant to the target objects. In order to make the seeds less sensitive and to crawl the most relevant web pages, a two-phase website expansion method is used. It consists of two similar steps; it first uses seed objects as queries to retrieve seed websites, then extracts more objects from these seed websites in order to retrieve other websites.
For each query, the top-ranked URLs are retrieved, and their corresponding websites are extracted. The websites are counted, sorted by their retrieval times and finally the top (e.g. 5) sites are chosen as the seed websites.

Next, all the pages are downloaded from the seed websites and all the objects are extracted from the pages in order to extend the set of seed objects. Then the same process is repeated with the extracted objects in order to find more websites. At the end of the process, the top N (e.g. 300) websites are used as candidate sites.

Queries are automatically generated by combining the ele-ments of each seed object. For example, suppose the first ob-ject in Table 1 is one of the seed objects, namely  X  X he Phil-harmonia Orchestra 2010 Shanghai Concert will take place in Shanghai Oriental Art Center-Concert Hall at 19:30:00 on May 1st, 2010 X . Since the date and time are not useful infor-mation in this domain, only the combination of the concert name X  X he Philharmonia Orchestra 2010 Shanghai Concert X  and the venue  X  X hanghai Oriental Art Center-Concert Hall X  is used as query. The object can be used to locate many other websites (such as www.culture.sh.cn , en.piao.com.cn , etc. ) that provide information about this concert and other local events.
This section describes URL pattern generation , which is the foundation for navigation pattern generation. URL pattern generation summarizes the URL patterns of the ob-ject pages.
 Object pages are those pages that contain target objects. A close look at object pages reveals that (as they have been generated from a same database) many of the URLs can only be differentiated by their parameters.
 Therefore, we take the prefixes that are common to the URLs as patterns (as the parameters that differentiate them are usually at the end of the URL). Any URL not matching a pattern is regarded as a single pattern. Finally, the pat-terns are pruned. If one pattern is covered by another, it is deleted. For example, if pattern A is the prefix of pattern B , then keep A , and delete B . Repeat the delete procedure, until there are no more pattern pairs.

URL pattern generation is performed on each folder of the file structure. This means that URL pattern must include the last slash  X / X  of the corresponding URL. If two URLs differ in content before the last slash, they probably originate from different kinds of web pages and should not be included in the same URL pattern.
URL pattern generation can reduce the number of pages traveled within a site. Furthermore, URL pattern selec-tion can reduce the number of pages traveled in all the candidate sites. URL pattern selection selects the subset of URL patterns that covers the greatest amount of distinct object clusters and disregards irrelevant or redundant pages. This section proposes an incremental greedy algorithm for URL pattern selection.

The same real world entity may have different representa-tions in different information contexts. Once the objects are extracted from the websites, they are grouped into distinct clusters. If an object can be extracted from a web page whose URL matches a pattern, we say the pattern covers the object. Following object extraction and URL pattern generation, we get several URL patterns and each pattern covers several objects. For OLVS, it is not necessary to cover d uplicate objects 1 , although many pages may contain the same objects.

URL patterns are summarized based on the URLs of the object pages, but they may still include irrelevant pages. Sometimes a URL pattern covers many objects but matches too many irrelevant pages. This situation leads to a huge unnecessary downloading cost. Therefore, instead of looking for a pattern that can cover the maximum number of new distinct objects[23], we look for a pattern that has a maxi-mum ratio of new distinct objects to the number of matched pages. We therefore propose a new, cost-based URL pattern selection method, described by Algorithm 1.

In order to take into account both the coverage and the cost of a pattern, a new scoring method for a pattern p is proposed in equation 1:
In this equation, p  X  o means that URL patten p covers the object o ;  X { o | p  X  o; o  X  O } X  represents the number of objects in set O that can be covered by pattern p ; p  X  d means that URL pattern p matches web page d ;  X { d | p  X  d; d  X  D } X  represents the number of pages in set D that are matched by pattern p .
 A lgorithm 1 An Incremental Greedy Algorithm for URL Pattern Selection.
 Inp ut: URL patterns P , objects O , web pages D the mapping list M = { p  X  o | p  X  P; o  X  O } ; and the threshold t ; Output: A subset of URL patterns P s . 1: Initialize: P s =  X  ; 2: while true do 3: Greedy URL Pattern Selection: 4: if Score ( X  p ) &lt; t then 5: break ; 6: end if 7: Add URL Pattern: 8: Update: 9: D = D  X  X  d |  X  p  X  d } 10: end while 11: return P s
In the algorithm, t indicates the percentage of the object coverage of the selected pattern set. t = 100% selects a set of URL patterns that covers all objects. This algorithm uses the scoring method defined in equation 1 to calculate the pattern scores.
The concept of the Navigation Pattern (NP) was first in-troduced by Lage et al. [13]. They used fixed and man-ually generated NPs to guide the navigation of a crawler
Oth er applications may be interested in the different meth-ods of object representation and aim to show all objects in tabular or other forms. through a target website to retrieve pages for data extrac-tion tasks. To reduce the human effort, Vidal et al. [20] proposed a structure-driven approach to automatically gen-erate the NPs.

In this section, we describe our object-based approach, which is based on the method proposed by Vidal et al. (ibid). The approach introduces a new definition of target pages.
In the method proposed by Vidal et al. (ibid), the creation of NPs is accomplished in two phases: site mapping and nav-igation pattern generation. In the site mapping phase, all paths (starting from the entry point) are traversed to look for target pages. In the structure-driven approach [20], the target pages are defined as those web pages that are struc-turally similar to the given sample page. In our approach, target pages are defined as those object pages that can be covered by any selected URL pattern. Then, each path from the entry point to a target page is recorded, in order to gen-erate a Target Pages Map (TPM). The TPM is the minimum spanning tree of a website where all leaves are target pages. It should be noted that traversing is limited to the same website.

The goal of the second phase, navigation pattern genera-tion, is to create a generic representation of the TPM. This generic representation is called a Navigation Pattern (NP) and is based on regular expressions, as described in section 2.3. The navigation pattern is generated from the TPM by summarizing URL patterns, level by level. By using regular expressions to identify links that lead to target pages, the NP can account for new links in the website even after the mapping phase has been completed. Fi gure 3: Example website structure. The bold bor-der nodes ( c 1 , d 1 , d 2 , e 1 ) represent object pages and the dotted border node ( d 3 ) represents a page cov-ered by a URL pattern for object pages.

The website is represented as a tree structure and an ex-ample is shown in Figure 3. Each node of the tree repre-sents a web page, and the edges represent the links that the crawler follows. The tree is generated as the web pages are downloaded. In this example, thirteen pages ( index , a 1 a a website, and only four pages ( c 1 , d 1 , d 2 , e 1 ) are object pages. There are three selected URL patterns ( C , D , E ) that correspond to the object pages (e.g. C corresponding to c 1 , D corresponding to d 1 , d 2 , d 3 , and E corresponding to e 1). In this example, no object can be extracted from d 3 though its URL is matched by the URL pattern D . The fact may be that d 3 contains some objects however they cannot be extracted due to the limitations of the object extraction Figure 4: The pruned tree for the website in Figure 3, which only keeps the paths linking to the object pages. Figure 5: The navigation pattern generated based on the pruned tree in Figure 4. tools.

Next, the paths and nodes that are not in the path to the object pages are pruned. In this way, we obtain the ob-ject pages tree illustrated in Figure 4. For each level of the pruned tree, URL patterns are summarized to generate nav-igation pattern. For the second level, pattern A is obtained from a 1 and a 2 , and pattern C from c 1 . Patterns D and E are found at the third level. The navigation pattern for the above example are illustrated in Figure 5. Each level of the navigation pattern contains the URL patterns of pages that the crawler should visit to retrieve the object pages.
For future crawls, the breadth-first crawler can use the navigation pattern to decide which links should be followed. If the crawler finds a new link when it scans a page from the i -th level and the new link is matched by a pattern in the i + 1-th level of the navigation pattern, then it is useful and the crawler will download it. In Figure 5, if the crawler is processing a page that matches pattern A or C in the second level and finds a new link that matches pattern D , then this new link is useful and will be downloaded in the next level. On the other hand, if the link does not match pattern D or E , it will be omitted. The tree structure of recrawling, guided by the navigation pattern in Figure 5, is illustrated in Figure 6. It is interesting to see that the pruned potential object found in page d 3 can be retrieved using the path index -&gt;a 3 -&gt;d 3 .

As Baeza-Yates and Castillo [2] have demonstrated that the breadth-first crawler can cover 90% of pages by only traversing three to five levels, our breadth-first NP-based crawler and general crawler both travel limited levels during crawling. Figure 6: The tree structure of recrawling guided by the navigation pattern in Figure 5.

Experiments are based on a Chinese entertainment event extraction system. An object is defined as an event tuple consisting of three elements (name, location, time). Most of the events are concerts, plays, or sporting events, etc. Throughout all experiments, an object extraction and clus-tering tool is used to extract and merge event information from the structure, semi-structure and free text of web pages.
As specified by the system requirement, only a few seed objects are provided. The system uses these seed objects to retrieve several seed websites. Initial crawling and page extraction from the seed websites generates more objects; these extended objects are used to create the set of candi-date websites. URL patterns and navigation patterns are generated from the web pages downloaded from the candi-dates sites, and are used to guide future crawling.
In the various experiments carried out, the crawling phase should ideally take place in parallel. However, this is impos-sible because of variations in the conditions found on the web and the unstable network environment. In order to achieve simultaneous crawling, all pages of the candidate websites are first downloaded by traversing all possible links within the sites. Their links and pages are saved as tree structures. Consequently, all crawling-based experiments can be com-pared by running the simulation on the same subset of the Web.

In all experiments (unless otherwise specified) there are five seed objects, five seed websites, and not more than 300 candidate websites. Although it is possible to work with much more than 300 candidate websites, in reality the num-ber of relevant websites containing target objects in a spe-cific domain is limited. Therefore, we considered that 300 candidate sites were enough for our experiments.
Table 4 shows to what extent our system can reduce the downloading of web pages. It compares the number of URL patterns that are used to guide crawling, the size and the number of web pages that are downloaded, the number of distinct events that can be extracted from them and the number of correct events extracted.

The first row shows the results of a full download of pages; the second row lists the results of non-selective recrawling; the third to fifth rows show the results of our selective re-crawling.

RC stands for the non-selective ReCrawling method and SR C is the Selective ReCrawling method. The bracketed numbers of the SRC method indicate the different thresholds t used in the algorithm. Three coverage percentages, 100%, 95%, 90%, were used. These thresholds control the trade-off between object coverage and web page downloading.
Table 4 shows that there are a large number of irrele-vant and redundant pages. Nearly half of the downloaded pages do not contain objects. The NP-based recrawling method therefore largely reduces the downloading of irrele-vant pages. Furthermore, more than 20% of the pages from which objects can be extracted are redundant, as the selec-tive recrawling method can select a subset of URL patterns that cover a certain percentage of objects. Our proposed selective recrawling method can further reduce the down-loading of redundant web pages by 23.0-68.7% compared to the non-selective recrawling method. Our method is able to retrieve more than 95% of the event information found using the non-selective recrawling method, from less than one-third the number of downloaded pages.

This experiment also shows that the number of extracted objects decreases as the coverage threshold decreases. At the same time the decrease in the percentage of correct ob-jects is smaller than the decrease in the percentage of dis-tinct objects, as illustrated in the 5th and 6th columns of Table 4. In other words, the precision of the extracted ob-jects increases as the coverage threshold decreases. This is because the selected URL patterns can guide the crawler to crawl pages from which objects can be easily extracted. Therefore, the coverage of correct objects is usually larger than the coverage of distinct objects. Furthermore, with im-provements in the performance of the object extraction and object de-duplication tool, this coverage can be increased even more.

Table 4 shows that the number of extracted events does not reduce by much even when the number of URL patterns greatly decreases. This observation provides a strong guar-antee that most objects can be collected from the collective resources.
In order to test how  X  X ld X  navigation patterns (generated from an old URL pattern set) can be used for future crawl-ing, two-week old navigation patterns were used to crawl web pages. The results are shown in Figure 7. RC and SRC have the same meanings as in Table 4. In the figure, the vertical axis represents the percentage of pages that are downloaded and the percentage of correct events extracted. The word  X  X ew X  in the legend means the crawler is guided by the current learned navigation patterns while X  X ld X  X eans the crawler is guided by two-week old navigation patterns. Fi gure 8: Two-phase vs. one-phase: A compari-son of the number of events extracted from the web pages crawled.
 For example, the second pillar in the third group represents the percentage of pages to be download if the crawler is guided by  X  X ld X  patterns learned with the algorithm SRC (95%).

Figure 7 shows that SRC(100%) can extract more than 90% of correct events using two-week old NPs, while only half as many pages are downloaded. As the threshold de-creases the number of downloaded pages continues to de-crease. SRC(95%) and SRC(90%) can both extract about 90% of correct events by downloading only about 40% and 30% of object pages.

Figure 7 also shows that compared with non-selective recrawling, selective recrawling performs better in reducing web page downloading, although the former retrieves a few more events. This shows that old navigation patterns can work well for future crawling, while decreasing the cost of downloading and covering most objects.
In order to make the seed less sensitive and to crawl more relevant web pages, a two-phase website extension method is used. It involves two similar steps: it first uses seed objects as queries to retrieve seed websites, then more objects are extracted from these seed websites and used to retrieve a set of candidate websites. In this experiment, our proposed two-phase system is compared with a one-phase system which uses seed objects to retrieve candidate websites directly.
Figure 8 shows that the number of objects collected by the one-phase system is less than half of that collected by the two-phase system. Therefore, the websites extension step is of correct events extracted. #events Fi gure 9: Varying the number of seed objects: A comparison of the number of events extracted when n o (from 1 to 25) events are selected as initial seed objects. necessary.

A potential reason why the two-phase method outper-forms the one-phase method is that it is difficult to re-trieve many candidate websites with only a few seed objects. Furthermore, the retrieval rate of many target websites is around one or two objects and it is difficult to select them from the list of sites retrieved. By adding one more step into the system, the intermediate seed objects created can generate more object queries and make it more likely that relevant sites are retrieved, thereby increasing the chance that the candidate websites are trustworthy.
Popular objects are good seed objects because they are usually widely distributed across relevant websites. They are well-known and can be easily collected by humans. How-ever, how many objects should be used as seeds? More seed objects require more human intervention, while fewer seed objects may lead to low coverage. To find the optimal num-ber of seed objects, n o (from 1 to 25) seed objects were tested as initial input.

In order to randomly select initial seed objects, 50 popu-lar seed objects were collected to form the initial pool. The experiment used n o seed objects from the pool. The ex-perimental results are shown in Figure 9. The horizontal coordinate is the number of seed objects and the vertical coordinate is the number of extracted events. The results of ten runs with randomly selected seed objects are illustrated for each n o ; the line shown in the graph joins the average number of events for each n o .

The figure shows little variation in the number of ex-tracted objects between different runs using the same num-ber of seed objects. This may due to the fact that the 50 candidate seeds in the pool are all popular objects.
The figure also shows little variation in the number of extracted objects as the number of the seed objects varies. Our system performs well with only a limited number of initial seed objects.
In order to crawl more relevant web pages, seed objects are used as queries to retrieve seed websites. Then objects are extracted from the seed websites and used to retrieve more websites. However, how many websites should be selected as seed websites? More seed websites require greater down-loading costs, while fewer sites may lead to low coverage. To find the optimal number of seed websites, an experiment was run using n s (from 1 to 20) top-ranked websites as seed websites.

The results are shown in Figure 10. The number of seed websites is shown on the horizontal axis, and the vertical axis shows the number of extracted events. The figure shows that there is little variation in the number of objects as the number of seed websites varies. It is interesting to note that only one seed website is enough for our system, this minimizes the cost of the intermediate extension step. All experiments were run on a computer with a 3.00GHz CPU. The running time for both download and extraction #events Fi gure 10: Varying the number of seed websites: A comparison of the number of events extracted when n s (from 1 to 20) top-ranked sites are selected as seed websites. is related to the number of downloaded pages. As both phases can be processed in distributed programs or different threads, all the reported running time results are converted using the assumption of a single CPU and a single thread.
For a full download the total crawling time is about 38 hours and the total object extraction time is about 11 hours. The total time for URL pattern generation and selection is 94.7 seconds and the running time for navigation pattern generation is 15.0 seconds.

Therefore, even if the percentage of downloaded web pages is reduced to 20%, the bottleneck remains the downloading and the extraction phase (especially the downloading phase). In the system, cost reduction can be achieved by further reducing the number of downloaded web pages.
To the best of our knowledge, there is little existing work that systemically investigates the problem of object page crawling. However, our approach is motivated by previous work, which is reviewed here.

One early related paper looks at focused crawling [5, 7, 9]. The goal of a focused crawler is to only retrieve web pages that are relevant to a predefined topic. Rather than collecting and indexing all accessible web documents, a fo-cused crawler only crawls content related to a specific topic or domain.

URL pattern based non-selective recrawling [1] is most closely related to our work. This system crawls web pages starting from a given set of seed URLs. Crawled web pages are partitioned into sets of relevant and irrelevant pages. A set of exclusion and/or inclusion patterns are discovered from the sets of relevant and irrelevant pages and subse-quent crawling of the web is restricted to the set of exclusion and/or inclusion patterns. However, both the focused crawl-ing and the non-selective recrawling technique proposed by Agrawal et al. (ibid) only consider the relationship between pages and topic, and do not take page redundancy into ac-count.

Deep Web (or hidden Web) crawling [4, 18, 21] is another related research topic. Deep Web content is usually stored in databases, and dynamically presented to users. Most object-related websites are a kind of deep web as they consist of dynamic pages generated from database records. However, selective recrawling and a Deep Web crawler differ in their focus. A Deep Web crawler focuses on how to prepare ap-propriate queries to retrieve hidden pages, while selective recrawling attempts to discover a set of useful URL patterns in order to selectively download object pages.

Another recent study discusses the problem of web page de-duplication. The objective of web page de-duplication [3, 10, 15, 22, 11] is to remove redundant web documents, in order to facilitate indexing and archiving. This is also one of the requirements of object-level vertical search. Most current work focuses on content-based duplicate detection [10, 15]. In this technique a web document is characterized by a  X  X ingerprint X , and any pair of documents with a high degree of similarity are considered as duplicates. However, content-based duplicate detection can only be carried out after web pages have been downloaded, and the technique cannot help reduce bandwidth waste in crawling. Koppula et al.[11] describe a method for learning URL patterns for web page de-duplication. Their method builds regular ex-pressions that match web pages of interest. However they did not consider object-level redundancy.

The incremental web crawler [6, 8, 17] is another approach to recrawling. The crawler selectively and incrementally up-dates its index and/or local collection of web pages, rather than periodically refreshing the collection in batch mode. However, the method only describes how to download new pages and how to determine which previously downloaded pages are still fresh.

Furthermore, all of the above techniques consider rela-tions at page level, while selective recrawling examines such relations at the object level.
In this paper we describe a new hypertext resource dis-covery method called selective recrawling. It only requires a few seed objects and can select a set of URL patterns to cover the greatest amount of target objects with least irrel-evance and redundancy. The selected URL pattern set can be used to recrawl web pages for some time and only needs to be renewed periodically.

The selective recrawling method uses a two-phase strategy for website extension. It first uses seed objects to retrieve seed websites. It then extracts more objects from these web-sites and uses them to retrieve more websites. This strategy minimizes the need for seed objects and makes the system less sensitive to them. The selective recrawling method uses a new, cost-based URL pattern selection algorithm to select a set of URL patterns that represent the target resource. It is worth noting that both URL pattern generation and selection can reduce the number of pages that the crawler visits. URL pattern generation reduces the number of pages to be traveled within one site and decreases downloading of irrelevant pages, while URL selection further reduces the number of pages to be traveled in all the candidate sites and decreases redundant page downloading.

The generated navigation patterns can continue to be used for some time. However, as web pages change every day old navigation patterns are not applicable forever and should be renewed periodically. On the other hand, on a large scale, the web changes slowly, which means that old patterns a re useful for a limited period of time. Since the system can be started using different initial objects, the simplest method is to restart the system when the performance of the old navigation patterns begins to drop. However, gen-erating new navigation patterns by restarting the system is costly. Therefore, further work is needed to investigate potential strategies for incrementally renewing navigation patterns with least cost.
 The authors would like to thank Mengjing Jiang and Yiming He for coding the entertainment event extraction system and thank the anonymous reviewers for their helpful comments. This work was partially supported by National Science Foundation of China (60503070), the foundation of Huawei, Shanghai Leading Academic Discipline Project (B114), 973 Program (2010CB327900) and National Major Science and Technology Special Project of China (2014ZX03006005). [1] N. Agrawal, S. V. Balakrishnan, and S. Joshi. System [2] R. Baeza-Yates and C. Castillo. Crawling the infinite [3] Z. Bar-Yossef, I. Keidar, and U. Schonfeld. Do not [4] L. Barbosa. An adaptive crawler for locating [5] S. Chakrabarti, M. van den Berg, and B. Dom.
 [6] J. Cho and H. Garcia-Molina. The evolution of the [7] M. Diligenti, F. Coetzee, S. Lawrence, C. L. Giles, and [8] J. Edwards, K. McCurley, and J. Tomlin. An adaptive [9] T. Fu, A. Abbasi, D. Zeng, and H. Chen. Sentimental [10] M. Henzinger. Finding near-duplicate web pages: a [11] H. S. Koppula, K. P. Leela, A. Agarwal, K. P. [12] A. H. F. Laender, B. A. Ribeiro-Neto, A. S. da Silva, [13] J. P. Lage, A. S. da Silva, P. B. Golgher, and A. H. F. [14] L. Lin, G. Li, and L. Zhou. Meta-search based web [15] G. S. Manku, A. Jain, and A. D. Sarma. Detecting [16] Z. Nie, J. Wen, and W. Ma. Object-level vertical [17] C. Olston and S. Pandey. Recrawl scheduling based on [18] S. Raghavan and H. Garcia-molina. Crawling the [19] E. Rahm and P. A. Bernstein. A survey of approaches [20] M. L. A. Vidal, A. S. Silva, E. S. Moura, and J. M. B. [21] K. Vieira, L. Barbosa, and J. Freire. Siphon++: A [22] Q. Zhang, Y. Zhang, H. Yu, and X. Huang. Efficient [23] Y. Zhou, M. Jiang, Q. Zhang, X. Huang, and L. Wu.
