 Abstract Web 2.0 provides user-friendly tools that allow persons to create and publish content online. User generated content often takes the form of short texts (e.g., blog posts, news feeds, snippets, etc). This has motivated an increasing interest gorisation is the task of classifying documents into a certain number of predefined categories. Traditional text classification techniques are mainly based on word frequency statistical analysis and have been proved inadequate for the classification of short texts where word occurrence is too small. On the other hand, the classic approach to text categorization is based on a learning process that requires a large number of labeled training texts to achieve an accurate performance. However labeled documents might not be available, when unlabeled documents can be easily collected. This paper presents an approach to text categorisation which does not need a pre-classified set of training documents. The proposed method only requires the category names as user input. Each one of these categories is defined by means of an ontology of terms modelled by a set of what we call proximity equations . Hence, our method is not category occurrence frequency based, but highly depends on the definition of that category and how the text fits that definition. Therefore, the proposed approach is an appropriate method for short text classification where the frequency of occurrence of a category is very small or even zero. Another feature of our method is that the classification process is based on the ability of an extension of the standard Prolog language, named Bousi * Prolog , for flexible matching and knowledge representation. This declarative approach provides a text classifier which is quick and easy to build, and a classification process which is easy for the user to understand. The results of experiments showed that the proposed method achieved a reasonably useful performance.
 Keywords Text categorization Ontologies Thesauri Unlabeled short texts 1 Introduction User generated content has been a major aspect of Web 2.0 era. Often these contents are formed by short texts which are created on daily basis as on-line evaluations of commercial products, posts of blogs or comments in social networks, news feeds, web pages titles, snippets, etc. In March 2011, Blogpulse Stats 1 shown that the total amount of identified blogs was greater than 158 million and more than one million blog posts were indexed a day. This proliferation of contents has motivated that in recent years, the computational linguistics community shown an increasing interest on the efficient analysis of short texts. Moreover, classification of short text messages is one of the most useful method to avoid becoming overwhelmed by the raw data.

Classification or categorisation is the task of assigning objects to one of several predefined categories. Text categorisation (also known as text classification) is the task of automatically sorting a set of documents into categories from a predefined set (Sebastiani 2002 ). In automatic text categorisation, the decision criterion of the text classifier is usually learned from a set of training documents, labelled for each class (Meretakis et al. 2000 ). This type of learning is called supervised learning because a supervisor, the human who defines the classes and labels training documents, serves as a teacher directing the learning process (Manning et al. 2008 ).
The text categorisation process is usually split into several steps. In the first place, a set of previously classified documents is assumed to be available. Those documents, which are labelled for each class, constitute the training document set. Using a learning algorithm, the decision criterion of the text classifier is learned automatically from the training document set by an induction process. A set of rules which describe the different categories is obtained after this step. This description will be used later to classify new documents, not included in the training set. Therefore, the degree of precision of the classifying method heavily depends on the decision criterion; that is the set of rules previously mentioned.

Traditional text classification techniques, mainly based in word frequency statistical analysis, work well when the word frequency is high enough to capture the semantics of the document. However, when dealing with short text messages, traditional techniques will not perform as well as they would have performed on larger texts (Sriram et al. 2010 ). This behavior conforms with our initial intuition, since the word occurrence is too small. These word frequency based techniques do not provide sufficient knowledge about the text itself preventing a correct classification. Thus, short text categorization cannot be carried out only relying in statistical methods; it is also necessary to exploit the semantic relationships between words.

Another problem with these methods is related with the exponential growth of the number of labeled document training sets required as the desired precision degree of the method increases. In this way, the time and effort required for collecting and preparing an adequate training set could be a restriction, and probably, prohibitive. This is an important issue in order to classify short texts, because there are a lot of available short texts, but the majority of them are unlabeled.

There are several approaches to address the classification problems induced by short texts. Faguo et al. ( 2010 ) proposed a novel method for short text classification based on statistics and rules. Their proposal achieves a high performance in terms of precision and recall, but requires the participation of a human agent. So it is not an automatic classification method. Liu et al. ( 2008 ) use short snippets of blogger X  X  posts to user modeling. The proposal is based on a two-layer classification model, one for the probability of a snippet belonging to each category and another for feature selection. That approach needs a huge volume of blog posts to train the first layer classifiers for blog snippets. Zelikovitz and Hirsh ( 2000 ) presents a method that combine labelled and unlabelled documents in order to reduce the user participation. That method for classifying short texts uses a combination of labeled training data plus a secondary corpus of unlabeled but related longer documents.
Another relevant approach is the one proposed by Boutari et al. ( 2010 ), a deep study about the use of five term concept association measures to drive text expansion prior to performing classification and clustering of short texts. That work investigates a term expansion approach based on analyzing the relationships between the term concepts present in the concept lattice associated with a document corpus.

On the other hand, the approach of categorising texts based on lists of categories and unlabelled documents has been attempted previously in the literature: A generalised bootstrapping algorithm for text categorisation is proposed by Gliozzo et al. ( 2005 ). In that paper, the categories are described by relevant seed features. Its main contributions are the introduction of two unsupervised steps in order to improve the initial categorisation step of the bootstrapping scheme. Gliozzo X  X  approach ( 2005 ) has been improved in recent papers. In the work of Barak et al. ( 2009 ) the words that are likely to refer specifically to the meaning of the category name are extracted from WordNet (Fellbaum 1998 ) and Wikipedia. 2 The final definition of each category is obtained through a disambiguation process of the extracted words using a Latent Semantic Analysis (LSA) model. The results obtained by this approach increase the classification precision of the previously related works.
 Another approach to unsupervised text categorisation is the one proposed by Ko and Seo ( 2009 ), which is an extension of a previously related work developed by the same authors (Ko and Seo 2004 ). In that paper, the text classifier is built by using only unlabelled documents and the label of each category. The learning method is based on a bootstrapping algorithm and feature projection techniques. The proposed method can also be used as an assistant tool for easily creating training data for supervised methods. The achieved results are reasonably useful compared to supervised methods.

The work of Ha-Thuc and Renders ( 2011 ) proposed a large-scale hierarchical text classification which does not require any labelled data. In this case, each category is defined by its description and relationships with other categories. The classification process is based on ontological knowledge and it is divided in the following steps: retrieving pseudo-labelled documents, iteratively training the cat-egory models and categorizing test documents.

On the other hand, Cambria et al. ( 2011a ) have presented a multi-disciplinary approach (Sentic Computing) based on computational intelligence and semantic web techniques for categorizing text at semantic-level. This approach has been applied to social media marketing (Grassi et al. 2011 ) and opinion mining (Cambria et al. 2011b , c ).

Previously mentioned papers use an unsupervised learning approach for training the document classifier. Although they do not need previously classified documents, all of them require a training phase. During this phase, they apply different methods of analysis, extraction, and knowledge representation to the document collection which should be classified. The approach introduced in this paper for text categorisation does not require any previously classified collection or training phase. Knowledge required for text categorisation is obtained from thesauri and ontologies like WordNet (Fellbaum 1998 ) or ConceptNet (Liu and Singh 2004 ; Havasi et al. 2007 ) by measuring the semantic closeness between concepts. As a first stage, a proximity relation is generated. Afterwards, the classifying process uses it in combination with a flexible search method implemented by an extension of the Prolog programming language. Moreover, our method is not directly based on an analysis of the frequency of occurrence of a certain category, but depends highly on the definition of that class (through an ontology) and how the text fits that definition. Therefore, the proposed approach is an appropriate method for short text classification where the frequency of occurrence of a category is very small or even zero.
 Under this novel approach, the only input required is the list of category names. These names are used to retrieve the semantic descriptions of the concepts involved with each one of the categories from the previously mentioned thesauri and ontologies. In this way, the category names are transformed into a set of concept descriptions. More precisely, each one of these categories is defined by means of an ontology or thesaurus of terms modelled by a set of proximity equations. These category descriptions are the main input of the classification process. At this point, it is worth noting that text categorisation based on concepts is an approach to overcome the main difficulties inherent to classification based only on lexical aspects (Garce  X  s et al. 2006 ), as long as accurate and explicit concept definitions are available
As it was mentioned before, in our method the classification process is based on the abilities of an extension of the standard Prolog language, called Bousi * Prolog for flexible matching and knowledge representation. This extension offers a fuzzy unification mechanism, based on proximity relations, which allows the flexible search of concepts in documents (Julia  X  n-Iranzo et al. 2009 ). Hence, our method implements a clean separation between knowledge (refined by an ontology), logic (expressed by rules) and control supported by the underlying programming language. The combination of these components provides a declarative approach to text classification which inherits some of their features directly from the logic programming language used for its implementation. Hence, it admits a declarative interpretation which explains what is being computed without a special concern on how the computation takes place. This makes our text classifier easier to understand and to build than usually it is. Our system has been developed entirely using Bousi * Prolog through a software application called inspect , with only 64 predicates, 103 clauses and 697 lines, which can be downloaded at the URL: http://dectau.uclm.es/bousi . Contrary to the behavior of other text classifiers implemented with more conventional programming languages, our system and its constituent programs allow multiple uses and an incremental development strategy. Also it supports interactive querying , that is, the user can launch various queries of interest, in order to interact with the developed system.

At the same time, the classification process became more understandable for the user, since it mainly relies on an ontology description, which is extracted from the knowledge contained in standard sources with a minimal manipulation While most of the work in classification nowadays is founded on statistical methods, this paper takes a Semantic Web and Soft-Computing approach using thesauri as a source of domain knowledge.

In order to evaluate the performance of the classification method, four distinct text categorization tasks have been carried out. In each case, the examples are short texts that have been from the World Wide Web (snippets, newswires, web titles, RSS feeds). The experimental results show that different types of proximity relations, used as input for the classification process, produce diverse results. Although some difficulties exist with some of the input relations, accurate results have been generally obtained by our method, equivalent to the ones referenced in literature (as will be shown in Sect. 4 ).

The paper is organised as follows: Sect. 2 includes a concise description of proximity relations between concepts and the Bousi * Prolog language which offers the required mechanisms to implement a text categorisation method using a declarative approach. Section 3 describes our method in detail including an explanatory example. Section 4 explains the experiments and the results obtained in order to verify how good the solution is. Finally, our conclusions and future work are outlined in Sect. 5 . 2 Background In this section, the fundamental concepts supporting our approach to text classification are explained beginning with the notion of proximity relation. These relations are used to express the semantic proximity between concepts included in ontologies and thesauri. Four relevant relations appropriate for the approach are introduced. Later, the main features of the Bousi * Prolog programming language are explained. Bousi * Prolog can be considered a Prolog extension which implements proximity-based fuzzy unification. Thus, it is a declarative programming language, well suited to flexible query answering. 2.1 Proximity relations between concepts Although there are several declarative formalisms for the representation of semantic relations, such as the one introduced by the Cyc project 3 (Lenat 1995 ), we adopted the use of binary fuzzy relations to specify them. Binary fuzzy relations were introduced by Zadeh in ( 1965 ). Formally, a binary fuzzy relation on a set U is a fuzzy subset on U 9 U (that is, a mapping U U !  X  0 ; 1 ). Given a and b two elements in U ,an entry of a fuzzy relation will be denoted as R X  a ; b  X  X  a , being a its relationship degree. A binary fuzzy relation R is said to be a proximity relation if it fulfills the reflexive property (i.e. R X  x ; x  X  X  1 for any x 2 U ) and the symmetric property (i.e. R X  x ; y  X  X R X  y ; x  X  for any x ; y 2 U ). A proximity relation which in x ; y ; z 2 U ) is said to be a similarity relation . The operator  X  D  X  is an arbitrary t-norm. The notion of transitivity above is D -transitive. If the operator D = ^ (that is, it is the minimum of two elements), we speak of min -transitive or ^ -transitive. This is the standard notion of transitivity used in this paper.

According to the approach introduced in this paper, concepts with a positive closeness relation to a category name should be identified, including the degree of the relationship, and formalised into a fuzzy relation. For this purpose, knowledge bases, ontologies and thesauri are used in order to estimate the relationship degree between two concepts, i.e. how semantically similar or close they are. Several methods have been proposed in the literature to compute semantic closeness. Le and Goh ( 2007 ) presented a survey of these methods. More specifically, they explored the existing techniques to calculate semantic resemblance and highlight the advantages and disadvantages of each one. A taxonomy of methods to measure concept resemblance is shown in Fig. 1 .

Specifically, in order to calculate the semantic closeness between concepts, we use different conceptual relations included in ontologies, thesauri and dictionaries like Concept Net and WordNet. In the following paragraphs we summarise the main conceptual relations used in this paper: 2.1.1 Structural analogy ConceptNet (Liu and Singh 2004 ; Havasi et al. 2007 ), is a freely available common sense knowledge base and natural language processing toolkit. 4 ConceptNet is constructed as a network of semi-structured natural language fragments. The ConceptNet Java API has a GetAnalogousConcepts() function that returns a list of structurally analogous concepts, given a source concept. The degree of structural analogy between these terms and the source concept is also provided by ConceptNet. Then, for each element b in the list of structurally analogous concepts to a source concept a and their degree of relationship a , we build an entry R X  a ; b  X  X  a of a fuzzy relation.
 Example 1 The set of entries shown below is a partial view of the original output obtained by the GetAnalogousConcepts() function and the source concept  X  X  X heat X  X . It is important to note that structural analogy is not strictly a semantic measure but a resemblance degree according to the characteristics of the represented concepts. In ConceptNet, two nodes are analogous if their sets of incoming edges overlap. Two concepts with a positive degree of structural analogy share similar properties and have similar functions. For example,  X  X  X cissors X  X ,  X  X  X azor X  X ,  X  X  X ail clipper X  X , and  X  X  X word X  X  are perhaps like a  X  X  X nife X  X  because they are all  X  X  X harp X  X , and can be used to  X  X  X ut something X  X . 2.1.2 Contextual neighborhood In ConceptNet (Liu and Singh 2004 ) the contextual neighbourhood around a concept is found by performing spreading activation from that source concept, radiating outwardly to include other concepts. The relatedness of any particular concept with some other concept is a function of the number of links and the number of paths between them, and the directionality of the edges. In addition, pairwise resemblance of concepts indicates the mutual information shared between two concepts, allowing similar nodes to be aggregated, leading to a more accurate estimation of contextual neighbourhood. For example, concepts like  X  X  X enu X  X ,  X  X  X rder food X  X  or  X  X  X aiter X  X  are in the contextual neighborhood of the source concept  X  X  X estaurant X  X . Nowadays, there are several advances in ConceptNet specification. For example, Speer et al. ( 2008 ) have proposed AnalogySpace, a new technique to try to solve the problem of reasoning over very large common sense knowledge bases by forming the analogical closure of a semantic network through dimensionality reduction. 2.1.3 WordNet similarity WordNet (Fellbaum 1998 ) is another possible source of knowledge to be used. WordNetSimilarity 5 is a freely available software package that offers an implementation of six measures of semantic resemblance and three measures of relatedness between pairs of concepts (or word senses), all of which are based on the WordNet lexical database. Three resemblance measures are based on path lengths between concepts, and the three remaining resemblance measures are based on information content, which is a corpus-based measure of the specificity of a concept. Finally, one of the three measures of relatedness is path based, and classifies relations in WordNet as having a direction, and the last two measures incorporate information from WordNet glosses as a unique representation for the underlying concept. An option to define closeness relations could be the combined use of a dictionary that provides a definition of a word and the WordNetSimilarity API that provides some relationship degrees between two words.
 Example 2 The definition of  X  X  X heat X  X  can be extracted from WordNet ( X  X  X nnual or biennial grass having erect flower spikes and light brown grains ...  X  X ) and used to compute the closeness of the terms expressed in the definition using the WUP measure (Wu and Palmer 1994 ) provided by WordNetSimilarity.
 2.1.4 Synonymy-based similarity Wordnet is a thesaurus but is also an ontology. It groups English words into sets of synonyms called synsets. It also provides short, general definitions, and records the various semantic relations between these synonym sets. Thus, it is possible to know the meaning of a word and, at the same time, to associate it with other words using ontological relations like synonymy, antonymy, hyperonymy, hyponymy, meronymy, etc. The semantic relationships and the synsets can be used to obtain a degree of closeness between two words and thus, the set of words related to another word.

Soto et al. ( 2008 ) introduce a formula in order to calculate concept resemblance based on the synonymy degree between those concepts. The degree of relationship between two concepts depends on the number of WordNet meanings that they share. More formally, we can define a semantic relation based on the meaning shared by two words. Let Mt  X  X  be the set of different meanings associated with a certain term t and Mt  X  X  jj the number of meanings of the term t , then the fuzzy relation R between two terms t 1 , t 2 , defined in WordNet, expressing the degree of relationship between both terms, is defined as:
Before ending this subsection, it is important to mention that all of the above detailed methods build a partial view of a fuzzy relation (certainly, only the entries connecting a category with a set of related terms are produced). Therefore, some post-processing of that partial relation may be needed depending on the features of the semantic relationship that we wish to establish. If we need to work with a proximity relation, it is necessary to build the reflexive, symmetrical closure of the partial relation. On the other hand, if the desired relation is a similarity, we need to build the reflexive, symmetrical and transitive closure of the partial relation. For-tunately, Bousi * Prolog gives automatic support for the generation of these kinds of closures, as will be commented in the next subsection. 2.2 Bousi * Prolog and flexible search Bousi * Prolog (BPL for short) (Julia  X  n-Iranzo et al. 2009 ; Julia  X  n-Iranzo and Rubio-Manzano 2009a , b ) is a fuzzy logic programming language whose main objective is to make the query answering process flexible and to manage the vagueness occurring in the real world by using declarative techniques. Its design has been conceived to make a clean separation between Logic , Vague Knowledge and Control . In a BPL program Logic is specified by a set of Prolog facts and rules, Vague Knowledge is mainly specified by a set of, what we call, proximity equations , defining a fuzzy binary relation (expressing how close two concepts are), and Control is let automatic to the system, through an enhancement of the S election-rule driven L inear resolution strategy for D efinite clauses that we call  X  X  X eak X  X  SLD resolution. Weak SLD resolution replaces the classical syntactic unification procedure of the classical SLD resolution principle with a fuzzy unification algorithm based on proximity relations defined on a syntactic domain.

Informally, this weak unification algorithm states that two terms f  X  t 1 ; ... ; t n  X  and degree, and each of their arguments t i and s i weakly unify. Therefore, the weak unification algorithm does not produce a failure if there is a clash of two syntactical distinct symbols, whenever they are approximate, but a success with a certain approximation degree. Hence, Bousi * Prolog computes substitutions as well as approximation degrees.

Bousi * Prolog is implemented as an extension of the standard Prolog language. It is publicly available and can be executed online via Java Web Start 6 or downloaded at the URL: http://dectau.uclm.es/bousi . Currently it is delivered in two imple-mentation formats: a high level and a low level implementation. The high level implementation (Julia  X  n-Iranzo et al. 2009 ) is written in Prolog through a meta-interpreter. One step further, Julia  X  n-Iranzo and Rubio-Manzano ( 2009b ) have pre-sented the structure and main features of a low level implementation for Bousi * Prolog , consisting in a compiler and an enlargement of the Warren Abstract Machine able to incorporate fuzzy unification and to execute BPL programs efficiently.

The Bousi * Prolog syntax is mainly the Prolog syntax but enriched with a built-in symbol  X  X  *  X  X  used for describing proximity relations 7 by means of what we call a  X  X  X roximity equation X  X . Proximity equations are expressions of the form:
Although, a proximity equation represents an entry of an arbitrary fuzzy binary relation, its intuitive reading is that two constants, n -ary function symbols or n -ary predicate symbols are approximate or similar with a certain degree. That is, a proximity equation a * b = a can be understood in both directions: a is approxi-mate/similar to b and b is approximate/similar to a with degree a . Therefore, a Bousi * Prolog program is a sequence of Prolog facts and rules followed by a sequence of proximity equations. The following example illustrates both the syntax and some features of the weak resolution semantics.
 Example 3 Assume a fragment of a deductive database that stores information about people and their preferences on teaching.

In a standard Prolog system, if we ask about who can teach mathematics, launching the goal  X  X  ?-can_teach(X,math) . X  X , the system do not produce any answer. However the Bousi * Prolog system answers  X  X  X=john with 0.8  X  X  and  X  X 
X=mary with 0.6  X  X . In order to understand this behavior, it is interesting to reproduce the different steps that the Bousi * Prolog system follows to obtain these answers: 1. At compiling time, the proximity equations (jointly with the rest of the program 2. At running time, the goal is solved by weak SLD resolution. The operational
The nondeterministic operational mechanism of the language also computes a second successfully derivation leading to the answer: ({X=mary}, 0.6) . In this case, the clue is the existence of the entry  X  X  R X  math ; chemistry  X  X  0 : 6 X  X  in the considered proximity relation.

On the other hand, Bousi * Prolog implements a number of other features, such as Manzano 2010 ) or the automatic support for generating some standard closures of a fuzzy relation (Julia  X  n-Iranzo 2008 ). The last one is intensively used in our proposal of categorization through the internal operational mechanism of Bousi * Prolog . Due to the importance of this last feature for the present work , ending this subsection, we light up its fundamentals and some implementation details.
 Given a finite set A of cardinality n and assuming that we list the elements of A on an arbitrary sequence f a 1 ; a 2 ; ... ; a n g , then a fuzzy binary relation R on A can be represented by a matrix M = [ m ij ] such that m ij  X R X  a i ; a j  X  . Sometimes we say that m ij is the entry h i ; j i of M , which is called the adjacency matrix of R . Note that, because we work with finite alphabets, fuzzy binary relations on a syntactic domain can be represented by adjacency matrices. In order to build the reflexive, symmetric and transitive closures of a fuzzy relation we proceed as follows: Building the reflexive closure of R : for each entry h i ; i i in M do m ii := 1;
Building the symmetric closure of R : for each entry h i ; j i in M , such that m ij = 0,
Building the transitive closure of R : for each column k and entry h i ; j i in M do m ij : = m ij _ ( m ik ^ m kj ); where  X  X  _  X  X  a n d  X  X  ^  X  X  are, respectively, the maximum and the minimum operators;
Note that, for computing the transitive closure of a relation, we use a direct extension of the wellknown Warshall X  X  algorithm (Warshall 1962 ), where the classical meet and joint operators on the set {0, 1} have been changed by the maximum and the minimum operators on the real interval [0, 1] respectively. A fact that makes this Warshall-like X  X  algorithm attractive is that it computes the transitive closure in only one pass over M (in the sense that each element is tested once), a fact that is not obvious (Warshall 1962 ). Another interesting property is that it preserves the approximation degrees provided with the original relation R . 10
Corresponding to any fuzzy binary relation R on A and its adjacency matrix representation M , there is a labeled directed graph (or digraph ) G whose nodes (or vertices ) are the members of the domain of R and whose labeled arcs are the triples a ! a ij a j for which R ( a i , a j ) = a ij . Hence we can see these procedures as processes that complete the original relation with new (direct) labeled arcs. In the case of the transitive closure, these new (direct) labeled arcs are storing information on the existence of a path between two elements. Moreover, the path stored is the one with the minimum approximation degree, being a lower bound of the existing relationship of those connected elements.

At this point, it is important to underline that closure construction is done at compiling time, so it has not a harmful effect on the execution efficiency of a program. Quite the opposite, we think it contributes to its efficiency (e.g. avoiding the search of path connexions among elements in order to establish their closeness). Also, closure construction provides the programmer with great freedom to define the fuzzy binary relation he/she wants to work. Certainly, he/she can supply to the system a partial specification of the relation, given an initial subset of relation entries represented by proximity equations. Then, by default, the system automat-ically generates a reflexive, symmetric closure in order to build a proximity relation, completing the partially specified relation. On the other hand, if the Bousi * Prolog directive  X  X  :-transitivity(yes)  X  X  is included in a BPL program, the transitive closure is also computed, leading to a similarity relation. Note however that, it is not easy (for the programmer itself) to define a similarity relation on a set of entries due to the transitivity constrains, which may contradict the initial approximation degrees. Therefore, this is a valuable feature also by this reason. 3 Text classification proposal Bousi * Prolog allows us to implement a declarative approach to text categorisation using flexible matching and knowledge representation by means of an ontology of terms modelled by a set of proximity equations. The following sections show how proximity equations can be used as a fuzzy model for text categorisation where the knowledge base is selected from an ontology; that is, a structured collection of terms that formally defines the relations among them (Gruber 1995 ). This is an useful application for the Semantic Web (Shadbolt et al. 2006 ), where people are exposed to great amounts of (textual) information.

The objective of any process of classification of documents is to assign one or more predetermined categories to classify each one of the documents. In our approach, the availability of a set of labelled documents or a training process is not necessary; only background knowledge is used to classify the documents. The proposed method consists of the following steps or phases: 1. Knowledge Base Building : It is necessary to build the definition of the 2. Document Processing : The input documents are processed using classical 3. Flexible Search and Computing Occurrence Degrees : Bousi * Prolog is used 4. Computing Document Compatibility Degrees : The compatibility degrees of 5. Classification Process : Each document is classified as pertaining to the
In order to describe the proposed classification method effectively and to detail the phases above enumerated, let us consider a running example that will be developed throughout this section. We are going to consider the problem of classifying a short text with regard to a set of categories, and to describe the results produced when they are processed by the proposed method.
 Example 4 Consider a set with four categories ( air , agriculture , water and transportation ) jointly with the following document extracted from the English version of EnviWeb Portal 11  X  X nd stored in a file named  X  X  runningEX  X  X  X .
First, we want to link one or several categories with the document, since this is the essence of a classification process. On the other hand, note that the  X  X  X nviweb expert X  X  classified the document inside the water category. Categories like air or agriculture are also possible but the expert did not choose them. Therefore, we would like to identify what is the knowledge that the expert used to classify the document in this category disregarding the others. 3.1 Knowledge base building The first step in classifying a document, with regard to a set of categories without any prior training process, is to define each of these categories to use them as accurately as possible. The starting point of this definition is the concept related to the category name. Therefore, concepts like  X  X  X ater X  X  or  X  X  X ir X  X , that are examples of categories in the EnviWeb Portal, will be the source of the background knowledge.

The definition of a concept is built from the set of concepts that are semantically close to it. These semantic relationships are extracted from some kind of controlled vocabulary or thesaurus which is relevant to a certain domain of knowledge, like economics. More precisely, we follow the techniques described at the end of Sect. 2.1 to construct a fuzzy relation. In this context, it is important to choose the correct meaning of a word for the classification process. If the work domain is known, it is necessary to use a process of disambiguation in order to realise the definition of polysemous words. For example, in the economic and financial domain of the Reuters collection, the category  X  X  X nterest X  X  is related to  X  X  X oan X  X  or  X  X  X ebt X  X  but not to  X  X  X uriosity X  X . On the other hand, it is also important to include in the category definition only those concepts that sharply define the category and occasionally other categories of the domain.

For our running example (Example 4), the knowledge is extracted from a thesaurus. More precisely, the sources are Wordnet related terms and WordNet-Similarity. A fragment of the generated proximity relation is as follows: Afterwards, these entries are represented as a set of proximity equations: Once the proximity equations are established, they are loaded into the Bousi * Prolog system in order to serve as a knowledge base for the flexible search and classification process. We recall that, by default, Bousi * Prolog compiles proximity equations into a proximity relation. That is, it automatically generates the reflexive and symmetric closures of the original (partial) relation. Additionally, if the transitivity flag is enabled (by means of the transitivity/1 directive), the transitive closure is also generated, producing a similarity relation. 3.2 Document processing The first stage in processing the document is a linguistic pre-process that consists of removing stop words, performing a stemming process based on WordNet and grouping meaningful couples of words. Also, when it is necessary, acronyms and abbreviations are expanded into full words. For our running example, the text obtained after this process is the following:
This pre-processed text acts as one of the inputs for the next step, mainly consisting of a flexible search of terms which are close to one of the considered categories.

At this point, it is important to note that the following phases of the classification process are managed by the Bousi * Prolog system. An application program, named inspect.bpl , drives the rest of the process. The program inspect.bpl inspects a sequence of documents stored in a file whose internal structure is consistent with the SMART standard format (see Fig. 2 ). It takes advantage from the features of Bousi * Prolog for flexible search of solutions. This program includes an ontology of terms, modeled by proximity equations, and a set of more than 64 predicates, 103 clauses and 697 lines of code. 3.3 Flexible search and computing occurrence degrees As it was noted, the essence of this phase is searching for the terms which are close to one of the considered categories and computing the occurrence degrees that will provide the necessary results for selecting the category or categories that must be assigned to a document.

The content of a file is read, word by word, by a predicate called inspect/3 whose input arguments are the Filename being inspected and a term, named Keyword , which is one of the pre-established categories that may be assigned to a document. This predicate looks for those words that are close (according to the proximity equations) to the term Keyword . As a result of the inspection, the predicate inspect/3 returns as output a record, called DataAccount , with statistical data and the following structure: There is a sublist for each document i stored in the file Filename . Each sublist Li stores a sequence of triples t(X, N, D) , where X is a term close or similar to the term Keyword , with degree D , which occurs N times in the text texNumber(i) .In order to search for words close or similar to a given one, this predicate relies on the fuzzy unification mechanism implemented in the core of the Bousi * Prolog language. More specifically, it uses a weak unification operator , also denoted by * , which is the fuzzy counterpart of the syntactic unification operator present in the standard Prolog language.

Coming back to our running example, after inspecting the text runningEX for the category  X  X  water  X  X , by using the predicate inspect/3 , the system offers the following output:
The result shows the number of times that the words  X  X  aquatic  X  X  ,  X  X  wastewater  X  X  and  X  X  river  X  X  occur in the text (only once) and the degree of relation between these words and the category  X  X  X ater X  X  ( 0.5 , 0.35 and 0.45 , respectively). 12 3.4 Computing document compatibility degrees In order to estimate the degree of compatibility between a category and the document contents, it is necessary to execute the predicate compDegree/4 which is a higher order predicate based on inspect/3 . It takes a file, named File ,a category, Category , a compatibility measure operation, named Operator , and returns a document compatibility degree account list, named DCD_Account .
 The predicate compDegree/4 , after calling the predicate inspect/3 , compresses the Data-Account list into a document compatibility degree, using the operation Operator . In more detail, the predicate applyTo/2 constructs the expression  X  X 
Operator(DataAccount, DCD_Account)  X  X  and launches it as a goal. Then, for each sublist [texNumber(i), t(Ti1,Ni1,Di1) , ... , t(Tin, Nin,Din)] of the DataAccount list, the former expression computes a new sublist [texNumber(i), CDi] , where CDi is the compatibility degree of the category Category for the document i . It is possible the use of several formulae to obtain these compatibility degrees. Table 1 summarises a set of sensible options.

For our running example, using the compatibility measure operator sum ( weighted sum of the occurrence degrees ), defined in Table 1 , we obtain a 1.3 compatibility degree of the category water for the considered text.

The last step in the computation of the compatibility degrees is driven by the predicate seqInspect/4 . This predicate takes as input a file, File , a list of categories, CategoryList , to be inspected and a compatibility measure operator, Operator , returning a document compatibility list, named CompList . Roughly speaking, it consists of the sequential execution of compDegree/4 , looking for words that are related to each one of the categories which exist in CategoryList and computing a document compatibility degree for these categories. Each category reaches a compatibility degree within each document in the text file. More precisely, the list CompList , returned by seqInspect/4 with statistical data, has the following structure: where every DCD _ Account _ ij is a document compatibility degree list computed by the predicate compDegree for a Category _ i and a document j . For our running example, the execution of this predicate produces the following output: 3.5 Classification process The procedure for the classification of documents is very simple: the categories with the higher compatibility degree are selected as the  X  X  X inners X  X . A predicate classify/2 gets the document compatibility list, CompList , obtained in the previous phase and produces a list with the following structure: where WinTopics _ k is the list of categories assigned to the document k in the text file. The list WinTopics _ k may contain one or several categories or it may be unclassified with regard to the list CategoryList of input categories.

For our running example, the category water is the winner with a compatibility degree of 1.3. It is clear that the category water should be selected as a winner because the semantic closeness relations maintained with water and the words aquatic , river and wasterwater . The word agriculture does not occur in the text but the semantic closeness between agriculture , biocide and pesticide provides a high compatibility degree between the text and this category. This reasoning scheme would be, more or less, the procedure that the expert could have followed to classify the document based on the ontological/semantic knowledge represented by the proximity equations. 4 Experiments In this section, the performance of the proposed classification method is evaluated in terms of the classification accuracy. 4.1 Test data collections The proposed classification approach has been tested on four distinct text-categorization tasks that we have taken from the World Wide Web. Table 2 shows the name of the data set, number of samples, total number of categories, and average length of the samples. 1. News Snippets : 1160 news has been extracted from the English Version of the 2. Web Snippets : ODP-239 (Carpineto and Romano 2009 ) is a collection of web 3. NewsWires-1 : A set of short texts (news limited up to 160 characters long) 4. NewsWires-2 : Another set of texts selected from Reuters-21578, as just 4.2 Performance measures As performance measures, we followed the standard definition of recall, precision, and F measure (the harmonic mean between precision and recall) (Van Rijsbergen 1979 ). For the evaluation of performance average across categories, we used the micro-averaging method (Yang and Liu 1999 ).
 Before formalizing these concepts we need to introduce the following notations. Given a set of categories D  X f f 1 ; ... ; f n g and a set of text documents D X f d 1 ; ... ; d m g to be classified, we denote the set formed by the documents d 2D classified as pertaining to a category f 2 D by C f . Also, we denote the set of documents d i 2D assigned by an expert to the category f 2 D as E f . Then, the by the following formulas.
 Observe that for a set S , as usual, | S | denotes the cardinality of S . Therefore, j C f j denotes the number of documents which are classified as corresponding to category f by the text classifier; analogously, j E f j denotes the number of documents whose assigned category is f , according to an expert pronouncement, and j C f number of documents which are correctly classified as pertaining to a category f by the classification method.

These preceding concepts can be generalized to a set of categories D . In this case, the precision ( P ), recall ( R ) and their F measure ( F ) w.r.t. D are calculated by the following formulas.
 4.3 Proximity relations As previously explained, several proximity equations are needed in order to develop the classification process. These equations are extracted from the previously mentioned knowledge bases (see Sect. 2.1 ).

Different types of closeness relations have been used in order to develop the knowledge base required for the classification process. They can be grouped as:  X  Concept Net : The structural analogy and the contextual neighbourhood between  X  WordNet : The construction of the proximity relation has been done expanding  X  Synonymy-based Similarity . The degree of proximity between category names is  X  Wikipedia : Since ConceptNet contains common sense knowledge and WordNet  X  YAGO (Suchanek et al. 2008 ): The use of YAGO for text categorization try to
Only the synonymy-based similarity , the structural analogy and the contextual neighbourhood are considered like similarity relations and then, the reflexive, symmetric and transitive closure of these fuzzy relations are generated. The remainder closeness relations are non-transitive proximity relations and only the reflexive, symmetric closure are generated.

Finally, note that the baseline is represented by the use of the syntatic equality (i.e., a category is represented only by its name).
 4.4 Experiment process In order to classify a document collection using the conceptual proximity relations previously described and to measure the performance of the classification method, the following sets are constructed: 1. The set D  X f f 1 ; ... ; f n g containing the labels which represent each one of the 2. The set D X f d 1 ; ... ; d m g which contains the documents to be classified. 3. Each one of the documents d i 2D is associated with one (or more) labels,
Once those sets are built, the classification process is launched for each one of the proximity relations defined in the previous section. The steps of the process were informally described in Sect. 3 . Now, we summarize them, in a more formal manner, as follows: 1. According to the selected knowledge base and method for constructing a 2. The obtained proximity equations are loaded into the Bousi * Prolog system and
All the experiments are carried out by using the predicate experiment/3 that is defined in the following way: FileName is the name of the file in which the documents to be classified are stored, CategoryList is list of topics, in this case the predefined ones were used (see Table 2 ). Finally, Process gives the aggregation function to compute the compatibility degree between a category and a concept. In our case, the sum of occurence degress ( sum ) is specified.

Essentially, as explained in Sect. 3 , each category is sequentially searched by seqInspect/4 in each one of the documents and the compatibility degree between each pair category-document is obtained. Then, using classify/2 , the resulting categories are classified according to their compatibility degree and those ones with the higher degree are selected by obtaining a list of selected categories by document. Finally, using compareW/2 , the selected categories are compared with the reference categories chosen by the expert in order to estimate the degree of fitness of the classification process. 4.5 Experiment results As we have just commented, we consider that a document is classified correctly if at least in one of its assigned categories coincides with one of the reference categories (assigned by the expert). During the analysis of the experiment results, while comparing the percentage of correct classifications with the  X  X  X ncorrect X  X  ones, it is important to distinguish between those produced by positive wrong classifications and those produced by a lack of classification of the documents. The first case, wrong classifications, implies a contradiction with the knowledge used by the expert for classifying the documents. In the second case, unclassified documents, means that one or more definitions are absent from the knowledge base, which should (or could) be completed in a subsequent phase.

The classification process was carried out with each one of the semantic relations previously defined. Classification results are shown in Table 3 where we display the percentages of correct classifications ( C ), wrong classifications ( W ), unclassified categories in different experiments.

The results obtained by using the similarity relations based on Concept Net are not acceptable. In many cases, the precision is poor when there is a high percentage of wrong classification. On the other hand, the recall is poor when there are many short texts not assigned to any category, increasing the number of unclassified documents. Comparing the proximity relations used, it is clear that the results are greatly improved when the definition of each category is complete enough. For example, in the News and Web snippets experiments the best results were obtained by using the combination of Wikipedia and WordnetSimilarity jointly with the WUP measure (Wu and Palmer 1994 ) , which brings a more complete concept definition of the selected categories. On the other hand, the best results in the experiments NewsWires X 1 and NewsWires X 2 based on the Reuters News Collection are provided by WordNet and the use of the WUP measure. The addition of the factual knowledge provided by YAGO (for example, places or organizations) gives the best results in the NewsWires X 2 (Reuters-10) experiment.

At this point, it is important to note that a good result obtained by the use of an specific ontology means that it incorporates a definition of the concepts with a higher quality and completeness than the rest. However, the effectiveness of the method greatly depends on a good pairing of the problem with the background knowledge, and WordNet and Concept Net are not specific sources of domain knowledge. The selection of an appropriate specific ontology in a certain knowledge domain can provide better results, as we stated when we complete them with factual information given by YAGO. Another relevant result is the considerable improve-ment achieved on the effectiveness of the classification process by using semantic relations. The number of documents correctly classified is significantly better than the one obtained when the syntactic equality is used exclusively.

Despite the existence of some different characteristics w.r.t. our approximation and others referenced in the literature, a comparison between them is possible and it is shown in Table 4 . This table reproduce the results obtained when the Reuters-10 experiment was done. Although results, according to the F measure, given by Bousi * Prolog using Wikipedia or WordNet are not always the best, those results could be considered acceptable, specially taking into account that the comparable results obtained by Barak et al. ( 2009 ) were produced using a transformed combination of knowledge bases. The approach proposed here is a classification method with limited complexity and a high dependency on the knowledge base used. Therefore, the obtained results promise great possibilities because they are better than those results obtained by Barak et al. ( 2009 ) using solely WordNet or Wikipedia or those obtained by Gliozzo et al. ( 2005 ) using context information. Indeed, if we incorporate factual information, such as the one coming from YAGO, in the definition of the categories, we are able to obtain even better results than the just mentioned before. 5 Conclusions and future work One key difficulty with current text classification learning algorithms is that they require a large, often prohibitive, number of labeled training examples to learn accurately. Labeling must often be done by a person, a tedious and time-consuming process. In this paper, a declarative text categorization approach, which does not employ a training process, has been presented. The method proposed is based on semantic relations (in particular, proximity relations) between concepts which describe each one of the categories.

One of the main strengths of this approach is the possibility of classifying documents without having some pre-classified training set of documents and even without a training process. In this way, starting from a list of category names, a classification mechanism could be set out without requiring additional treatments. Since text classification is a task based on the use of pre-defined categories, then such definition of the required categories, for classifying documents, should be known. Hence, there is no need for training the software w.r.t. a specific document collection but to increase the knowledge about the tag collection, which is supposed to be known a priori. So, it should be possible to apply the method with different document collections while keeping the same tag set without training the software once more. Within this approach, the category names are defined by means of an ontology of terms modeled by a set of proximity equations. The definition of a concept is built from the set of concepts that are semantically close to it. These semantic relationships are extracted from controlled vocabularies and thesauri which are relevant to a certain domain of knowledge.

Using these descriptions, the Bousi * Prolog logic programming language allows to perform a flexible search of the concepts represented by those categories inside the documents. Once the proximity equations are established, they are loaded into the Bousi * Prolog system as a knowledge base for the flexible search and classification process. By default, proximity equations are compiled into a proximity relation, generating the reflexive and symmetric closures of the original relation. Optionally, also it is possible to generate the transitive closure, leading to a similarity relation. This last ability has been used in this paper to model ontologies which are structural analogies.

The logic of the proposed classification mechanism is independent of the knowledge base used, providing a declarative approach to text classification where these main components are treated separately. The knowledge used for the classification process could be obtained from generic thesauri and expressed in a way which is understandable for any non-expert user. Thus the classification process is more comprehensible to the user than other approaches like Bayesian classifiers. Moreover, the knowledge to be used could be general or domain specific in order to classify the document according to certain pre-established categories.

The main problem of our method is that its performance depends on the quality of the category definitions (represented by proximity equations). If a category name is not well defined, the classification process performance achieved will be comparatively poor. There are several options for improving our approach in the near future: 1. First, to improve the knowledge bases used in the classification process and/or 2. Second, to consider the use of more complex t-norms (like Einstein product) in 3. Third, to apply more complex aggregation formulae in order to determine the References
