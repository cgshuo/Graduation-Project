 Max-margin is a powerful principle to construct machine learning algorithms which has been applied to a wide spectrum of areas ranging from kernel method VC-dimension or Flat-dimension, a simple or sparse classifier often induces a concurs with Occam X  X  Razor principle.
 learning problem becomes convex and can be solved analytically. Nonetheless, cost of the learning problem becoming non-convex and only a convergence to a local optimum is guaranteed. This impedes the usage of the aforementioned method to real dataset. In [ 12 ], the burden in computation is addressed by Adaptive Multi-hyperplane Machine (AMM) was proposed for efficiently han-dling large-scale datasets. AMM has some advantageous features: (1) it has a run online, and (3) the number of hyperplanes associated with each class can be automatically discovered. However, AMM has no principle to tune the com-plexity and sparsity levels of the solution. The redundant hyperplanes are cut off by an exhaustive pruning weight procedure which heuristically prunes the hyperplanes whose lengths are less than a predefined threshold. can run online and also operate well under the memory budget requirement. In this paper, we leverage the stochastic gradient descent framework with the max-margin principle to propose Sparse Adaptive Multi-hyperplane Machine (SAMM). Besides inheriting several advantages of AMM, with SAMM we can underfitting. Our experiment on several large benchmark datasets demonstrates computational speedup. out the paper. The dot product of two vectors w, x is denoted by w, x w For any positive number N , the set including the first N positive numbers is defined as [ N ] { 1 , 2 , ..., N } . Given a logical statement A , and is 0 otherwise. A norm of vector x is denoted by x and the dual norm is defined as x  X  sup It is known that the dual norm for x 2 x, x 1 / 2 is itself, the dual norm norm for x 1 d i =1 | x i | is x  X  max is defined as W p,q = W 1 p , W 2 p , ..., W n p q . The dual norm of a group norm W p,q is the group norm W r,s where 1 p + 1 r =1and 1 for any two vectors x, y  X  S and number  X   X  [0; 1], we have  X x +(1 A function f : S  X  R is convex if f (  X x +(1  X   X y ))  X   X f ( x )+(1 any x, y  X  S and  X   X  [0; 1].
  X  such that f ( u )  X  f ( v )  X   X , u  X  v  X  0 for all u  X  S .
 sup max-margin principle embodies as maximizing the discrepancy between to dis-criminative values, one for the correct label and the other for the runner up. We depart from the original work [ 4 ] where the model representation ability is restricted since only a single hyperplane is associated with each class and ends with Adaptive Multi-hyperplane Machine (AMM) [ 12 ] where simultane-tation ability and SGD is applied for speedup. 3.1 Multi-class SVM Given the training set D = { ( x n ,y n ) } N n =1 , where instance x D -dimensional feature vector and y n  X  X  = { 1 ,...,M } is the corresponding function f : R D  X  X  that can accurately predict label of a new instance. discriminative function as f ( x ) = argmax w now translated into the following optimization problem of the maximal and runner-up discriminative values  X  ( x n instance ( x n ,y n ) is defined as l ( W ;( x n ,y n )) = max 0 , 1 + max g ( y ,x n )) .
 In this model, each class is associated with only a single hyperplane and distributions inside. 3.2 Multi-hyperplane Machine The work of [ 1 ] proposed an extended version of multi-class SVM that allows inative function is redefined as g ( i, x ) = max hyperplanes associating with the i -th class. The weight matrix W now becomes hyperplane in y n -th class being used by the instance ( x discriminative value. The task of finding the optimal matrix W variables z =[ z n ] N n =1 is addressed by solving the optimization problem current optimal matrix W  X  where the loss function at ( x n ,y n ) is defined as 2. Given the matrix W  X  , we find the current optimal assignment as z = argmin evaluated as z n = argmin 3.3 Adaptive Multi-hyperplane Machine An improvement of Multi-Hyperplane Machine, namely Adaptive Multi-matrix. At each t -th iteration, a random instance ( x t ,y from the training set D . The instantaneous objective function associated with t -th instance ( x t ,y t ) is defined as P ( t ) ( W | z and the sub-gradient matrix  X  ( t ) is computed as model predicts correct label of instance x t , i.e., the loss value l ( W ;( x where i t = argmax are less than a predefined threshold. In our viewpoint, subtracting the small-length weights though helps reducing the model size and training time but can impact to the prediction accuracy. In this section, to encourage the sparsity of the solution, we reformulate the optimization problem of AMM by incorporating the group norm W in terms of a lower number of hyperplanes per class and non-zero components per hyperplane. Because minimizing W 2 , 1 also inspires as many as possible performed in our proposed model without any heuristic or predefined threshold. 4.1 Optimization Problem In SAMM model, the group norm W 2 , 2 is replaced by the elastic group norm  X ( W )  X  2 W 2 , 2 +  X  W 2 , 1 . The optimization problem of SAMM becomes rated. Mathematically, minimizing W 2 , 1 encourages the lengths of the compo-nent weights w i,j 2 going to 0 or decreasing to a small amount. It means that the parameter  X  in SAMM is used to control the sparsity level of the solution. Furthermore, it can be seen that AMM is a special case of SAMM when  X  =0. 4.2 Optimization Solution To find the solution of SAMM, we employ two-step alternative approach. In the solved to find the current optimal W  X  In the second step, the latent variables z =[ z n ] N n =1 argmax To develop a stochastic gradient descent solution for the optimization prob-using our notation.
 Theorem 1. Let f be  X  -strongly convex w.r.t . over a set l ,l (2) ,...,l ( T ) be a sequence of convex functions, and L then, for any u  X  S , we have: 1 t =1 f ( u )+ l We define l ( t ) ( W ) = max 0 , 1 + max i  X  X \ y n g ( i, x
W 2 , 2 whose duality is W vector and equal to vector 0 if l ( t ) ( W ) = 0, otherwise it becomes where i t = argmax when i = i t ,j = j t or i = y t ,j = z t . Therefore, we have bounded in a hypersphere, i.e., x 2  X  R,  X  x  X  R D .
 formulation Algorithm 1. Sparse Adaptive Multi-hyperplane Machine 4.3 Generalization Error of SAMM Let b i define the number of using hyperplanes for the i -th class. We have the following generalization error bound.
 Theorem 2. Suppose we are able to correctly classify an i.i.d sampled training set D using the AMM model then we can upper bound the generalization error with probability greater than 1  X   X  as where B = N min { b The above theorem reveals that the generalization error is proportional to hyperplanes for each class and the average number of non-zero components in each hyperplane. This reasons why tuning the sparsity level in SAMM can reduce the generalization error. 5.1 Experimental Settings We establish the experiments on 9 benchmark datasets 1 . We make comparison our proposed method SAMM with AMM [ 12 ], Pegasos [ 9 ] including both lin-ear and kernelized versions (LPegasos and KPegasos), and kernelized LIBSVM (KSVM) [ 3 ]. All codes are implemented in C/C++ and the codes of baseline methods are achieved from the corresponding authors. All experiments are per-formed on the computer with the configuration of core I5 3 . 2GHz and 16GB in RAM. 5.2 Evaluation on Accuracy and Time of the Proposed Method AMM, SAMM, and KSVM and  X  for SAMM. The considered ranges are  X   X  10  X  7 , 10  X  2 and  X   X  10  X  7 , 10  X  2 . For KSVM and KPegasos, RBF kernel, and record the corresponding mean and standard deviation.
 to the optimal parameter set. As observed from Table 1 , comparing with the linear methods, our proposed SAMM gains superiority accuracies to others on all experimental datasets. The reason is that as compared with AMM, our pro-posed SAMM can tune the sparsity level of the solution which help boost the Comparing with LPegasos, the fact that SAMM outperforms it is explainable with the kernelized methods, SAMM also produces the comparable classification accuracies on the experimental datasets.
 the training time of SAMM is always less than AMM and longer than LPega-sos. Although the computational complexities of SAMM and AMM are simi-lar, the computational cost in each iteration of SAMM is less than AMM due to the smaller number of using hyperplanes and the sparser non-zero compo-SAMM always exceeds that of LPegasos because of LPegasos X  X  model simplicity. arable data in nature. Comparing with the kernelized versions, because of the Especially for the large scale datasets namely mnist, webspam, and url, SAMM are much faster than both KSVM and KPegasos. Regarding prediction time, our proposed SAMM always takes less time than AMM (cf. Table 3 ). This can be per class and the average number of non-zero components per hyperplane. By the same reason of the cheaper kernel computation cost, SAMM also provides the shorter prediction times as compared with the kerneilized methods. percentage of non-zero components per hyperplane and report them in Table 5 . As shown in this table, the average numbers of hyperplanes per class of SAMM and AMM are comparable. This implies that AMM X  X  prune weight procedure operates somehow exhaustively and this may compromise the prediction accu-racy of AMM itself. Nonetheless, the average percentage of non-zero compo-nents per hyperplane of SAMM is always lower than AMM for all experimental datasets. This observation again confirms our expectation about sparsity level of SAMM. 5.3 Tuning the Sparsity and Its Influence on Performance To investigate the influence of  X  to accuracy, we conduct the experiment where  X  is varied and  X  is kept fixed. As observed from Table 4 , when  X  is varied in ascending order, the accuracy is increased at first to its peak and then is gradually decreased. This fact may be partially explained as increasing  X  ,the overfitting and brings out an optimal model. However, when  X  is increased to a bigger value, the estimated model becomes too simple, and underfitting may consequently happen.
 2  X  D datasets as displayed in Figs. 1 and 2 . In Fig. 1 , AMM requires 3 hyper-planes to classify the data and the learning seems to be overfitted while SAMM meanwhile, SAMM with a tuning of  X  =0 . 04 requires only 5 hyperplanes which is intuitively a better solution. In this paper, we leverage stochastic gradient descent framework with max-margin principle to propose Sparse Adaptive Multi-hyperplane Machine (SAMM). By incorporating the group norm L 2 , 1 to its model, with SAMM we to avoid both overfitting and underfitting. We validate the proposed method on large benchmark datasets. The experimental results show that SAMM can actu-racy while simultaneously achieving shorter training time compared with the baselines. Lemma 1. Given that g =  X f + h where f is  X  -strongly convex w.r.t norm and h is convex, g is  X  X  -strongly convex w.r.t norm . .
 w.r.t the norm . Lemma 4. Let v =  X  1  X 
 X 
