 Pseudo-relevance feedback, or query expansion, has been shown to improve retrieval performance in the adhoc re-trieval task. In such a scenario, a few top-ranked documents are assumed to be relevant, and these are then used to ex-pand and refine the initial user query, such that it retrieves a higher quality ranking of documents. However, there has been little work in applying query expansion in the expert search task. In this setting, query expansion is applied by as-suming a few top-ranked candidates have relevant expertise, and using these to expand the query. Nevertheless, retrieval is not improved as expected using such an approach. We show that the success of the application of query expansion is hindered by the presence of topic drift within the profiles of experts that the system considers. In this work, we demon-strate how topic drift occurs in the expert profiles, and more-over, we propose three measu res to predict the amount of drift occurring in an expert X  X  profile. Finally, we suggest and evaluate ways of enhancing query expansion in expert search using our new insights. Our results show that, once topic drift has been anticipated, query expansion can be success-fully applied in a general manner in the expert search task. Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval; H.3.4 [Systems and software]: User profiles and alert services General Terms: Performance, Experimentation Keywords: Expert Finding, Expertise Modelling, Expert Search Information Retrieval, Query Expansion, Topic Drift
In [16], Rocchio introduced the classical Information Re-trieval (IR) concept of relevance feedback to improve a rank-ing of documents. An application of this is pseudo-relevance feedback (PRF), which has been used in adhoc search tasks to automatically improve the retrieval performance of doc-ument IR systems. The basic idea of PRF is to assume that a number of top-ranked documents are relevant, and learn from these documents to improve retrieval accuracy [19]. In Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00. Query Expansion 1 (QE), information from these top-ranked documents, known as the pseudo-relevant set, is used to ex-pand the initial query and re-weight the query terms.
Unlike classical IR systems that generate rankings of doc-uments, an expert search system aids a user in their  X  X x-pertise need X  by identifying people with relevant expertise to the topic of interest. Such a system can be useful in large Enterprise settings with vast amounts of digitised in-formation, where people are a c ritical source of information because they can explain and provide arguments about why specific decisions were made [8]. Typically, an expert search system associates a set of documents to each candidate ex-pert, known as profiles, to represent their expertise in the system. Candidates are then ranked in response to a query using the expertise evidence in their profiles.

In this paper, we aim to have a general application of query expansion (QE) to the expert search task, to enhance the retrieval accuracy of an expert search system. This aim is important, as while QE has been shown to be useful in adhoc document IR tasks [1, 15], the application of QE is not as useful for Web IR tasks, such as topic distillation and known-item finding [6]. In finding a general application of QE to the expert search task, we will show that it can indeed be successfully applied to increase the retrieval accuracy of an expert search system. Specifically, from an initial ranking of candidates with respect to a query, an application of QE in an expert search system would select several top-ranked candidate experts as the pseudo-relevant set, then expand the query using terms from their interests. Then re-running using the expanded query terms would generate a higher quality candidate ranking.

It is known that the effectiveness of QE in an adhoc doc-ument search system is affected by the quality of the initial top-ranked documents used for pseudo relevance feedback (known as the pseudo relevant set) [20]. However, we hy-pothesise that the presence of topic drift within candidate profiles can reduce the effectiveness of QE in the expert search task. What do we mean by this? Well a candidate expert can have several or many unrelated areas of expertise, which are reflected in the contents of their profile. Now we believe that when using the entire profile for query expansion for a query about a topic, these other unrelated expertise ar-eas can wrongly influence the outcome of QE. Consider an IR example: W. B. Croft is generally considered an expert in language modelling, and an expert search system for IR should rank him highly in response to the query  X  X anguage
In this work, we use the terms pseudo-relevance feedback and query expansion interchangeably. modelling for IR X . However Croft and other highly ranked candidates might share expertise in clustering. If QE is then applied, the expanded query terms might be more orientated towards clustering than language modelling, causing a top-ical drift in the new ranking of candidates.

This work aims to provide a framework for the general and successful application of QE in an expert search task. In particular, this work investigates the extent to which topic drift affects QE in expert search, and secondly, to investigate how to account for this expertise drift while applying QE in an expert search system.

The contributions of this paper are as follows: Firstly, we show how QE can be appropriately applied in an expert search task, when the pseudo-relevance objects are only can-didate names. While this is useful for the successful auto-matic application of QE, this will also facilitate several other related tasks, for instance, the interactive application of QE in an expert search setting, or perhaps finding similar ex-perts. Secondly, we propose and evaluate several measures for  X  X ohesiveness X . While these measures are evaluated in the contextoftheexpertsearchtask,itisofnotethatthesemay have applications in other areas of IR. For instance, cohe-siveness measures may be applied on a ranking of documents to facilitate diversifying the top-ranked results in order to satisfy more types of users [17]. Lastly, we present several fine-grained QE approaches for expert search that reduce the occurrence of topic drift during QE, leading to a more effective QE that works on a list of candidates. Moreover, these approaches for QE are general and do not depend on the retrieval approach used for ranking the candidates. The remainder of this paper is structured as follows: In Section 2, we introduce the expert search task and the Vot-ing Model for expert search that we use in this work. Sec-tion 3 introduces how QE can be applied in this task, and presents the experimental setting and the baseline retrieval performances applied in this paper. Moreover, in Section 4, we investigate the extent to which topic drift is occurring during QE. In Section 5, we present three measures which we use to predict the amount of expertise drift within a can-didate profile. Section 6 proposes and evaluates approaches for considering expertise drift when applying QE. We show that these successfully reduce topic drift and enhance the application of QE in the expert search task. In Section 7, we provide concluding remarks and ideas for future work.
Modern expert search systems for Enterprise settings work by using documents to form the profile of textual evidence for each candidate expert [5]. The candidate X  X  profile rep-resents the expertise of the candidate expert in the expert search system. This documentary evidence can take many forms, such as intranet documents, documents, emails au-thored by the candidates, or web pages visited by the candi-date (see [11] for an overview). In this work, the profile of a candidate is considered to be a set of documents associated with the candidate. These candidate profiles can then be used to rank candidates in response to a query.

Among the first models for expert search, is that proposed by Craswell et al [7], where all documents in each candi-date X  X  profile are combined into  X  X irtual documents X , which are then directly ranked in response to a user query. How-ever, because the contribution of each document in a profile is not measured individually, this approach is less effective than other subsequent approaches.

The advent of the expert search task in the recent TREC 2005 and 2006 Enterprise tracks has stimulated research in-terest in expert search [5, 18]. From this forum, there have been three main approaches for expert search: Balog et al. proposed the use of language models in expert search [3] based on two formal models. Their first model is based on Craswell et al X  X  virtual document approach described above. For their second model, evidence from distinct documents in the candidate profiles are combined. Their experimental results showed that the second model improved over the sim-pler first model. Later, the probabilistic approach proposed by Cao et al. in [4] and the hierarchical language models proposed by Petkova &amp; Croft [14] use a more fine-grained approach with windowing of documents around candidate name occurrences. However, in all three approaches, the relevance score of each candidate is determined utilising the relevance score of documents, as calculated using a language modelling approach.

In contrast, the Voting Model for expert search proposed by Macdonald &amp; Ounis in [11] considers the problem of ex-pert search as a voting process. The ranking of documents , with respect to the query Q , denoted by R ( Q ), is assumed to provide inherent evidence about a possible ranking of can-didates. The ranking of candidates can then be modelled as a voting process, from the retrieved documents in R ( Q ) to the profiles of candidates: every time a document is re-trieved and is associated with a candidate, then this is a vote for that candidate to have relevant expertise to Q .The ranking of the candidate profiles can then be determined by applying a voting technique that aggregates the votes of the documents. Eleven voting techniques for ranking experts were defined in [11]. Each of these voting techniques em-ploy various sources of eviden ce derived from the ranking of documents, such as counting the number of documents associated with each candidate that are retrieved (number of votes), or the scores or ranks of the associated documents of each candidate (strength of votes).

In this work we choose to use the Voting Model for expert search proposed by Macdonald &amp; Ounis , because it also takes the relevance of documents in each candidate X  X  profile into account. Moreover, in contrast to [3], which can only use the language modelling approach, the Voting Model is general and flexible, and not limited to any document re-trieval approach. In particular, we apply the expCombMNZ technique, which ranks candidates by considering the sum of the relevance scores of the documents associated with each candidate X  X  profile, combined with the count of the number of documents from the profile that are ranked in the docu-ment ranking R ( Q ). In expCombMNZ, the relevance score of a candidate C  X  X  expertise to a query Q is given by: score cand expCombMNZ ( C, Q )= R ( Q )  X  profile ( C ) where profile ( C ) is the set of documents associated with candidate C ,and score ( d, Q ) is the relevance score of the document in the document ranking R ( Q ). The number of documents from the profile of candidate C that are in the ranking R ( Q ) is denoted by R ( Q )  X  profile ( C ) ,and exp () is the exponential function. Note that this approach is gen-eral, as any retrieval model can be used to generate the ini-tial ranking of document R ( Q ). The exp () function serves to bias the retrieved candidates towards those associated to higher-ranked documents.
Section 3 introduces the two ways that QE is applied in this work. The first of these is suitable for any expert search technique that uses candidate profiles as sets of documents, while the latter is only applicable using the Voting Model.
The application of query expansion in adhoc search tasks is known to improve retrieval performance [1, 15]. To have a general application of QE to the expert search task, we desire to have a QE mechanism that works on the ranking of candidates (which we call Candidate Centric QE).
In [12], a candidate centric approach for QE was proposed that considers the entire profiles of the top-ranked candi-dates as the pseudo-relevant set. Note that this approach is not limited to the Voting Model, and can be applied to any expert search model that uses profiles to rank candidates. Moreover, an alternative application of QE, called Docu-ment Centric QE, was also proposed in the setting of the Voting Model, where the initial ranking R ( Q )isimproved by the application of QE, before the voting technique gen-erates the final ranking of candidates.

As mentioned above, to have a general application of QE for expert search, we need to consider the candidate rank-ing as the pseudo-relevant set. However, the experimental results from [12] show that the candidate centric approach to QE did not perform as well as the document centric ap-proach. In this paper, we hypothesise that the failure ex-hibited by candidate centric QE is due to the occurrence of topic drift. Therefore, we aim to investigate and measure the topic drift problem in candidate centric QE, and then pro-pose how candidate centric QE can be markedly improved by reducing its susceptibility to topic drift. If this topic drift is anticipated in candidate centric QE, and can enhance re-trieval performance over the baselines, then we can conclude that QE can be successfully applied in the expert search task. In the remainder of this section, we introduce the core expert search experimental setting applied in this work, and the document centric and candidate centric baselines.
In this section, we define our experimental setup. Our ex-periments are carried out in the setting of the Expert Search tasks of the TREC Enterprise track, 2005 and 2006. The TREC W3C collection is indexed using Terrier [13], remov-ing standard stopwords and applying the first two steps of Porter X  X  stemming algorithm. Our initial experimental re-sults have shown that applying only this weaker form of stemming results in increased high precision without degra-dation in mean average precision (MAP) for this task.
Next, we generate the profiles of documentary evidence of expertise for the candidates: for each candidate, doc-uments which contain an exact match of the candidates full name are used as the candidate X  X  profile. Using exact name matches, instead of say the candidates X  last names only, ensures that only documents the candidates are defi-nitely related to are associated with them, hence reducing the amount of mismatched evidence and ensuring good re-trieval performance [3].

From the two TREC expert search tasks, we have a to-tal of 99 topics with relevance assessments. For the TREC 2006 topics, where there are several topic fields, we only use the title field (ie short queries) -the TREC 2005 task only had one topic field where all terms formed the query. Documents in the initial ranking R ( Q ) are ranked using the DLH13 document weighting model [10] from the Divergence from Randomness (DFR) framework [1]. We chose to exper-iment using DLH13 because it performs robustly on many collections and tasks (including expert search) without any need for parameter tuning [10, 11]. Indeed, DLH13 has no term frequency normalisation parameter that requires tun-ing, as this is assumed to be inherent to the model. Hence, by applying DLH13, we remove the presence of any term frequency normalisation parameter in our experiments.
In QE, terms found in the pseudo-relevant set are weighted, and the best of these are added to the initial query. In this work, we use the query expansion mechanism from the DFR framework [1]. In particular DFR deploys several term weighting models that measure the informativeness of each term in the pseudo relevant set. In our investigation into query expansion in expert search, we need to determine if the term weighting model employed has any effect on the conclusions concerning our two approaches for query expan-sion. DFR term weighting models measure the informative-ness of a term by considering the divergence of the term occurrence in the pseudo-relevant set from a random distri-bution. One term weighting model, known as Bo1, is based on Bose-Einstein statistics and is similar to Rocchio [1]. The other is based on the Kullback Leibler (KL) divergence be-tween the pseudo-relevant set sample and the collection. In Bo1, the informativeness w ( t )ofaterm t is given by: where tf x is the frequency of the term in the pseudo-relevant set, and P n is given by F N ; F is the term frequency of the query term in the whole collection and N is the number of objects in the collection.

Alternatively, w ( t ) can be calculated using a term weight-ing model based on Kullback Leibler divergence [1]: in tokens of the pseudo-relevant set, and token c denotes the total number of tokens in the collection. Using either Bo1 or KL to define w ( t ), the top exp term informative terms are identified from the top exp item ranked items (these must exist in at least 2 items), and these are added to the query ( exp term  X  1, exp item  X  2). Finally, for both the Bo1 and KL term weighting models, the query term frequency qtw of an expanded query term is given by [1] qtw = qtw + w ( t ) w max ( t ) ,where w max ( t )isthemaximum w ( t ) of the expanded query terms. qtw = 0 if the query term was not in the original query. We use the default setting for the QE parameters, ie exp item =3and exp term = 10, suggested by Amati in [1] after extensive experiments with several adhoc document test collections. While adjusting these parameters may enhance the retrieval performance of both document centric and candidate centric QE, we choose to leave these at their default settings, as initial experiments have shown that these do not alter the conclusions [12].
Table 1 presents the retrieval performance achieved by the baseline expert search system, and by applying the doc-ument centric (DocQE) and candidate centric (CandQE) forms of QE, using both the Bo1 and KL term weight-ing models. The retrieval performance is reported on the Table 1: Results for QE using the Bo1 and KL term weighting models. Results are shown for the base-line runs, with document centric query expansion (DocQE) and candidate centric query expansion (CandQE). The best results for each measure and term weighting model combination are emphasised.
 Statistically significant improvements at ( p  X  0 . 05 ) and ( p  X  0 . 01 ) over the corresponding baseline are denoted by * and **, respectively.
 TREC 2005 and 2006 Enterprise track, expert search tasks. Statistically significant improvements from the baselines are shown using the Wilcoxon signed rank test.

Firstly, the performances from TREC 2005 and TREC 2006 are widely different. This follows the normal pattern: TREC 2005 was widely seen as a pilot task, where the can-didate expertise relevance assessments were derived from an out-of-corpus ground truth -the membership of the W3C working groups. In contrast, for the TREC 2006 expert search task assessments were made for each pooled candi-date, and hence the scale of the evaluation results is very dif-ferent. The baseline voting technique, expCombMNZ, com-bined with the DLH13 document weighting model performs well above the median run for TREC 2005 (MAP 0.1402), and these results are similar to those of the 3rd top group participating that year. For TREC 2006, the median run of (MAP 0.3412), and these results are similar to those of the 2nd top group participating that year. Moreover, these settings are very competitive baselines on which to base our experiments.

With regards to the application of QE, at first inspec-tion, it appears that document centric QE outperforms the candidate centric QE on both MAP and P@10, in all set-tings. In particular, it can be seen that applying docu-ment centric QE results in an increase over the baseline for both the TREC 2005 and TREC 2006 topics, on both MAP and P@10. These improvements are statistically significant ( p&lt; 0 . 05) in 4 out of 8 cases.

Finally, compared to the baselines, applying candidate centric QE almost always results in a degradation from the baselines, the exception being MAP for KL on the TREC 2006 topics, but this increase is not significant. In particu-lar, Figure 1 shows the breakdown by topic of delta MAP for CandQE and DocQE (TREC 2006, Bo1). These show that while CandQE can enhance performance for some topics, it can seriously damage performance on many more topics. In contrast, DocQE improves many more topics. Moreover, across these topics, there is only a weak correlation (Spear-man X  X   X  =0 . 267, not statistically significant) between the topics that CandQE improves and those that DocQE im-proves.

The use of the Voting Model allows inference with respect to the document ranking R ( Q ). In particular, it is intuitive Figure 1: By topic breakdown of changes in MAP for CandQE and DocQE (with Bo1) on the TREC 2006 topics. that a higher quality R ( Q ) should improve the quality of the generated candidate ranking. These results infer that while DocQE improves R ( Q ), there appears to be no ben-efit in applying QE directly in the expert search task, as concluded in [12]. However, this is counter-intuitive: the application of pseudo-relevance feedback has been shown to improve retrieval effectiveness in other adhoc tasks, so it seems there is a problem in the application of candidate centric QE that needs addressed. We hypothesise that the problem is that of topic drift occurring during candidate centric QE. In Section 4, we illustrate the extent to which topic drift is occurring. Moreo ver, in Section 5, we introduce and evaluate several  X  X ohesiveness measures X  that attempt to predict when topic drift is occurring in a candidate profile. Our assumption which is evaluated in Section 6, is that if topic drift is accounted for in the candidate centric QE, then retrieval performance will be markedly improved.
We suggest that the less promising performance of candi-date centric QE is due to  X  X opic drift X . A candidate profile contains many documents that represent the various inter-ests of a candidate. As illustrated in Section 1 by the W. B. Croft example, when candidate centric QE is performed, the expanded query terms may describe other common, but not relevant, interests of the candidates in the pseudo-relevant set, causing more candidates with these incorrect interests to be retrieved erroneously. Topic drift is more likely to occur with candidate centric QE than with document centric QE as candidate profiles contain many documents, likely to be about several topics, while, comparatively, single documents are likely to remain related to one or two topics.
We develop two methods to measure the extent that topic drift is occurring during candidate centric QE. The first of these analyses the candidates that were used in the pseudo-relevant set. The second method investigates the quality of the expanded query terms.

By examining the relevance assessments for the expert search task, it is possible to observe that some candidates can have relevant expertise to multiple topics. Figure 2 shows the distribution of the number of topics candidates have relevant expertise in, for the TREC 2005 and TREC 2006 topics. Note that for TREC 2006, assessors were asked to judge for each topic the pooled candidates for relevance, using supporting documents to make those judgements. This was a substantially more complete judgement than for TREC Figure 2: The distribution of the number of topics candidates have relevant expertise in, for the TREC 2005 and TREC 2006 relevance assessments. 2005, where relevance assessments were emulated using an out-of-corpus ground truth (W3C working ground member-ship). Hence, for the TREC 2005 set, we believe that the emulated judgements are incomplete from the viewpoint of the candidate -ie they do not reflect accurately the number of areas of expertise that many candidate have. Moreover, this can be observed in that there are a higher number of candidates that are only expert in one topic for the TREC 2005 set than the TREC 2006 set (see Figure 2).

To assess the extent that the candidates being used for rel-evance feedback in candidate centric QE had many areas of expertise, we count how many times they have been judged as relevant in the relevance assessments. The ideal scenario is that the candidates used in the pseudo-relevant set are not just expert in the current topic, but are not expert in any other topics, to prevent topic drift occurring during QE. For the reasons mentioned above regarding the number of exper-tise judgements in the TREC 2005 relevance assessments, we use only the TREC 2006 judgements for this analysis.
In fact over all 99 topics, for the candidate centric QE, the candidates used in the pseudo-relevant set were, in average, expert in 9.62 topics of interest. This is strikingly different from the average expertise of 1.27 topics for each candidate in the collection. This infers that the candidates used in pseudo-relevant set were expert in more topics than the cur-rent topic, and hence the QE mechanism was more likely to be affected by topic drift by identifying off-topic terms to ex-pand the query with. Furthermore, by correlating the delta Average Precision in applying candidate centric QE over the no QE baseline with the average number of topics that pseudo-relevant candidates had interests in, we can indeed relate the problem of topic drift in the candidate profiles to poor QE performance. For instance, when using the Bo1 term weighting model on the 49 TREC 2006 queries, the cor-relation (Spearman X  X   X  ) exhibited is  X  =  X  0 . 357, which is a statistically significant correlation. The negative correlation shows that when the candidates used in the pseudo-relevant set are expert in only few topics, QE is likely to do better, while if they are expert in many topics, it is likely that it is detrimental to apply QE to that query.

Our second measure examines the quality of the query terms added to the initial query by the QE approach. We compare the expanded query terms brought by the docu-ment centric and candidate centric QE approaches, by mea-suring the probability of an expansion term occurring in the relevance assessments. As the judgements for TREC 2006 were performed by identifying supporting documents for each relevant candidate [18], we use the set of supporting documents for all relevant candidates as our ground truth. From the results in Table 1, we expect that the expanded terms identified by candidate centric QE will not occur as much in the relevance judgements as those identified by doc-ument centric QE.
 Formally, for a query Q which has expanded query terms Table 2: For the TREC 2006 setting, the mean probability of an expanded query Q e being gener-ated by the relevant supporting documents (Mean P ( Q e | Rel ) ).
 Q , the probability of the expanded query terms occurring in the set of relevance assessments for query Q , Rel ,is: where tf Rel is the term frequency of term t in the set of doc-ument Rel ,and token Rel is the number of tokens in the set Rel . exp term is the number of expanded query terms. qtw is the weight given to the expanded query term t in the re-fined query. It is used to prevent query terms that were given little weight in the expanded query biasing the measure.
Table 2 presents the Mean P ( Q e | Rel ) for each QE setting on the TREC 2006 topics. From this we can see that the likelihood of the expanded terms being in the relevant doc-ument is lower for both candidate centric QE settings. This demonstrates that indeed the query terms being identified in candidate centric QE are less useful than those identified by document centric QE. Because of the nature of the ap-plied term weighting models, we reject the idea that they are identifying noise as informative terms, and instead hypothe-sise that that topic drift is indeed occurring in the candidate centric QE, compared to the document centric QE.

In the following section, we investigate how we can au-tomatically predict the extent to which a candidate profile is about one central area of expertise. Following Amitay et al [2], who measured the  X  X ohesiveness X  of a ranking of doc-uments, we denote a candidate profile in which the expert has one sole interest as cohesive. In the following section, we present three ways of measuring cohesiveness, of which two are from the vector-space and language modelling frame-works. Our aim is that if we can show that un-cohesive candidate profiles can be identified, then we can possibly take this into account for an enhanced QE approach.
In the previous experiments, we hypothesise that the ex-pertise drift within a candidate profile are responsible for the poor performance of the candidate centric QE. To this end, we investigate how cohesive a candidate X  X  profile is: we measure the extent to which a candidate X  X  expertise profile is around a central topic. For this we use three measures: firstly, simply counting the number of documents associated with each candidate ( profile ( C ) ), secondly the Cosine measure, and lastly Kullback-Leibler (KL) divergence [9].
For the first of these measures, profile ( C ) ,ourintu-ition is simply that the more expertise evidence found for a candidate, the more likely it is that the candidate X  X  expertise varies across more than one topic. Moreover, as any expert search system must have knowledge of the documents in each candidate X  X  profile, this measure is simple to calculate.
Our second and third cohesiveness measures are based on the intuition that the more the language model of a candi-date X  X  profile differs from its constituent documents, the less cohesive the profile is. We use Cosine and KL divergence to measure this. The cohesiveness of a candidate profile can be measured using the Cosine measure from the vector-space framework as follows: where tf d is the term frequency of term t in document d ,and tf
C is the total term frequency of term t in all documents in profile ( C ) (denoted t  X  profile ( C )). Cohesiveness measures the mean divergence between every document in the profile and the profile itself. Note that Cohesiveness is bounded between 0 and 1, where 1 means that the docu-ments represent the profile completely.

Alternatively, we measure the cohesiveness of a candidate profile by measuring the mean KL divergence between the language model of every documents in the profile and the language model model of the profile itself. Formally, the KL divergence between two probability distributions 1 , 2 is: We use maximum likelihood to estimate the probability of a term t occurring in the document model d , and the prob-ability of a term in the profile model C .Tomeasurethe cohesiveness of a candidate profile, we use the mean KL di-vergence between the language model of every document in the profile and the language model of the profile itself:
Note that  X  C, Cohesiveness KL ( C )  X  0, and the larger the value, the less cohesive the profile of candidate C is. We now evaluate the three defined measures of cohesiveness.
To evaluate our measures of cohesiveness, we use the rel-evance assessments of the TREC 2006 expert search task, as described in Section 4, as the ground truth to evaluate how effective we are at measuring the cohesiveness of candi-dates. Our hypothesis is that candidates with less cohesive candidate profiles (i.e. more expertise drift) will be expert in more topics, according to the relevance assessments, and will be more likely to cause topic drift in candidate centric QE. To perform the evaluation, we rank all the candidates which are expert in one or more topics, and correlate these with the cohesive measures defined above.

Table 3 shows the Spearman X  X  rank correlation (  X  )be-tween the cohesiveness measures and the ground truth from the TREC 2006 judgements. From the results, we can see that there are moderately stro ng correlations between all three cohesiveness measures and the ground truth, the high-est of which is exhibited by profile ( C ) . Note that the cor-relation for Cohesiveness Cos is negative because this mea-sure gives lowest values for the most cohesive profiles.
Furthermore, there are seve ral possible reasons that an even higher correlation is not observed: Firstly, with only 49 topics from TREC 2006, it is entirely possible that some candidates expertise areas were not covered by the topics. This could mean that candidates predicted to have many Table 3: Correlations between various predictors of cohesiveness and the ground truth based on the TREC 2006 expertise relevance assessments. areas of expertise are ranked low in the ground truth be-cause the topics did not cover many of their expertise areas. Secondly, expertise assessment at TREC is performed by pooling the suggested candidates by submitted retrieval sys-tems. This infers that not all possible candidates will have been judged for each topic, meaning that there may exist relevant candidates not judged. Thirdly, before an assessor can judge a candidate expert as having relevant expertise to the topic, they must have seen at least one supporting document. Supporting documents for each candidate are provided by systems, and are pooled for each candidate. A candidate who has relevant expertise in  X  X eal life X  may not be marked as relevant as a supporting document was not present in the collection, or not pooled and judged.
Despite the caveats in this evaluation described above, the correlations exhibited in Table 3 demonstrate that these measures are sufficiently accurate with respect to the ground truth, and moreover, they are are equally comparable.
Other methods of measuring cohesiveness exist: For in-stance, in TREC 2003, Amitay et al. [2] filtered a ranking of documents for cohesive documents using the combination of IDF and Entropy. Alternatively, taking the mean diver-gence between every pair of documents in a candidate profile would have required the use of symmetric divergence opera-tors, e.g. J-Divergence [9], however as some candidate pro-files are extremely large (around 5000 documents), the time taken to compute such measures for all candidates would have been unfeasible. Indeed, some preliminary experiments suggest that 587,436,281 document-document comparisons would be required to measure the cohesiveness of all candi-dates in the profile set applied in this work 2 .
Similarly, and analogous to [17], cohesiveness could be measured by clustering candidate profiles: the number of distinct clusters in a profile gives an indication of the num-ber of topics the candidate showed expertise in. However, the simple measures proposed a bove give good correlations to our ground truth, and the most effective, profile ( C ) , is extremely cheap to compute, as an expert search system will already know the associations between documents and candidates. Now that we have reasonably good predictors of cohesiveness, we show in Section 6 how candidate centric QE can be improved to account for topic drift.
In the previous section, we proposed three measures which can predict how many topics a candidate has relevant exper-tise in. Moreover, when a candidate has many areas of ex-pertise represented in their candidate profile, then this may be responsible for the occurrence of topic drift during can-didate centric QE: if any additional non-relevant topic areas
However, while 11% of these comparisons are duplicates and could be skipped, the time taken to compare this many document pairs would still be unfeasible for any real world applications or experimental settings. were shared in the profiles of any candidates in the pseudo-relevant set, then terms from these topics areas might be added to the expanded query, causing candidates who only have expertise in the non-relevant topic areas to be retrieved.
In this section, we propose three approaches that enhance candidate centric QE, based on hypotheses concerning how topic drift can be reduced. The approaches are designed to reduce the topic drift that has been identified and discussed in this paper, and could be applied using other expert search techniques rather than the Voting Model.
 Hypothesis 1 : Query expansion can be enhanced by not considering candidates with non-cohesive profiles during ps-eudo-relevance feedback.
 Hypothesis 2 : Query expansion can be enhanced by only considering the on-topic parts of candidate profiles. Lastly we combine Hypotheses 1 &amp; 2 to form a third: Hypothesis 3 : Query expansion can be enhanced by only considering the on-topic parts of the non-cohesive profiles.
The remainder of this section defines three approaches for query expansion in the expert search task based on the three hypotheses respectively. The first of these approaches, Selective Candidate Centric QE , makes use of a measure of cohesiveness, such as those defined in Section 5 above, to prevent non-cohesive candidate profiles being considered for the pseudo-relevant set. We assume that by remov-ing non-cohesive candidate profiles from the pseudo-relevant set, only candidates with relevant expertise mostly about the topic will remain. Expanding the query using this refined pseudo-relevant set would exhibit less topic drift than the candidate centric QE defined in Section 3. However, a pos-sible disadvantage is that this approach is too harsh, and removes useful candidates from the pseudo-relevant set. In contrast, the second approach (based on Hypothesis 2), Candidate Topic Centric QE , does not make use of the co-hesiveness measures, but instead considers only the subset of documents in the candidate profiles which are about the initial user topic for inclusion in the pseudo-relevant set, similar to [19]. We can use the relevance score of the docu-ment to the query as an indicator for the topicality of each document in a candidate profile. By only considering the highest scored documents in the pseudo-relevant set of can-didate profiles, the expanded query terms are more likely to be about the topic of interest. However, it is possible that the removed portion of the profile was a good source of expanded query terms.

Lastly, in the third approach, which we call Selective Can-didate Topic Centric QE (Hypothesis 3), for the pseudo-relevant set we consider all the documents of the profiles of cohesive candidates, while for non-cohesive candidates, only documents from the profiles which are on-topics are consid-ered. Similar to Selective Candidate Centric QE, we use a cohesiveness measure to predict the cohesiveness of the candidate profiles of the pseudo relevant set.

To show that we have successfully taken into account the topic drift, we compare to the CandQE results in Table 1. Moreover, to assess whether QE is actually useful in expert search, we compare also to the baseline (no QE) and to the stronger DocQE results from Table 1.
In Hypothesis 1, we desire to reduce the amount of topic drift occurring during query expansion, which occurs be-cause the candidate profiles used as the pseudo-relevant set are not cohesive. In this approach, which we denoted Selec-tive Candidate Centric QE, we take into account a cohesive-ness measure, such as one of these we defined in Section 5, to predict candidates that do not have a cohesive profile and hence should not be considered during QE.

We use the profile ( C ) cohesiveness measure because this shows the highest correlation with our ground truth. For this approach, we set a threshold sel profile docs .When a candidate X  X  profile contains more documents than the thresh-old sel profile docs , the candidate will not be considered for pseudo-relevance feedback.

Table 4 shows the results when applying Selective Candi-date Centric QE while varying the sel profile docs thresh-old. From the results, we can see that this approach for QE produces marked increases in both MAP and P@10 over the candidate centric QE baselines, some of these increases being statistically significant. Compared to the document centric baseline, improvements are exhibited on the TREC 2005 topics only (significant only in one case). With rela-tion to the threshold sel profile docs , a value around 200 to 500 document appears to be a good setting for this col-lection. In particular, at a threshold of 500 on the TREC 2006 queries, the average number of TREC 2006 topics that the candidates in the pseudo-relevant set are expert in is 3.6. This is a marked contrast from the 9.62 topics observed in Section 4 for CandQE, and shows that the profiles used in this approach are much more cohesive. Contrasting the performance of the approach on the TREC 2005 and 2006 tasks, we see that more statistically significant increases are exhibited for the 2005 task, while the easier 2006 task shows a lesser benefit in applying this approach. This mirrors the results of DocQE and CandQE in Table 1. Finally, the un-derlined values in Table 4 show when selective candidate centric QE improves over all other settings for that task and term weighting model (baseline, DocQE, CandQE). We can see that the proposed simple approach is comparable to doc-ument centric QE for the TREC 2006 task, and outperforms it for certain threshold values on the TREC 2005 data.
In Hypothesis 2, we desire to reduce the occurrence of topic drift when applying QE, by reducing the amount of irrelevant information in the candidate profiles considered for pseudo-relevance feedback. This is similar to how the language modelling [3] and voting approaches for expert search [11] improve over the virtual document approach of [7] -instead of focusing on the entire candidate profiles, the em-phasis is placed on the on-topic documents within each can-didate profile. When QE is being applied, it is unlikely that documents in the profiles which were not at least on-topic will bring any terms related to the user X  X  topic of interest. Hence, they should not be considered for the pseudo-relevant set. In this case, the pseudo-relevant set for QE becomes the set of documents that are associated with the first exp item ranked candidates, but are predicted to be relevant to the topic. We call this approach Candidate Topic Centric QE.
In this approach, we remove off-topic material from the pseudo-relevant set of candidate profiles before QE takes place. Detecting whether a document is on-topic is mea-sured simply by using the relevance score of the document to the query, score ( d, Q ). However, as most document weight-ing models do not compute bounded retrieval scores, we simply select the exp cand doc top scored documents from each of the candidate profiles in the pseudo-relevant set. The special value ALL designates when all documents with 0.3520 *  X  0.5041 0.5980 0.3780 **  X  0.5240 ** 0.6163 0.3780 **  X  0.5077 0.6102 0.3560 **  X  0.4857 0.5510 0.3200*  X  0.4894 0.6000 0.3500  X  0.5415 0.6429 0.3640  X  0.5549  X  0.6592 0.3580  X  0.5540  X  0.6571 0.3220  X  0.5616  X  0.6592 Table 4: Selective Candidate Centric QE: Candidates with profile ( C ) Statistically significant improvements at ( p  X  0 . 05 )and( p improvements over the DocQE baseline are denoted  X  . score ( d, Q ) &gt; 0 in the candidate profile are considered. Note also, that this approach is not specific to the Voting Model, as any expert search approach would be able to compute a relevance score for each document in a candidate X  X  profile.
Table 5 presents the experimental results when applying candidate topic centric QE. We vary exp cand doc across a selection of values while the exp item and exp term QE parameters remain unchanged as in Section 2. Firstly, it is apparent that this approach for QE generates substantial in-creases on both TREC expert search tasks, for all measures and QE term weighting models. There are statistically sig-nificant increases in each setting for certain values of the threshold, and marked increases over the document-centric QE baseline, except for the KL term weighting model on the TREC 2006 task. In particular, the value 500 is very close to the ALL setting, and produces no difference in performance. The setting range 5-20 documents produce the best results on both tasks. Moreover, performance is enhanced more for MAP than P@10. In this approach, similar to Selective Candidate Centric QE, a selective technique based on a cohesiveness measure. The aim here is to identify the uncohesive candidates in the pseudo relevant set, and reduce the topic drift that they induce, by applying Candidate Topic Centric QE only for those candidates. For the candidates with cohesive profiles, this filtering of the profile is unnecessary and is not applied.
Table 6 presents the experimental results when applying selective candidate topic centric QE across a range of set-tings of the sel profile docs threshold of the cohesiveness measure. In these experiments, we leave exp cand doc = 10, as this value gave good performance with the Candidate Topic Centric QE approach for both the TREC 2005 and 2006 tasks. Examining Table 6, we draw the following ob-servations: firstly, this approach is also successful at im-proving over the CandQE baseline. Moreover, most set-tings on both TREC tasks can outperform the DocQE ap-proach defined earlier, for both MAP and P@10 measures. With respect to parameter sel profile docs , the approach seems to be stable, with this having only some impact on retrieval performance, however the values 200 &amp; 500 exhibit the best retrieval performance. Finally, the performance of the Selective Candidate Topic Centric QE approach would be improved by an appropriate setting of both parameters sel profile docs and exp cand doc . More generally, it would be useful to understand the connection between exp item , exp cand doc &amp; exp term in a similar manner to the param-eter scans presented in [12].
The approaches for query expansion described are general models for applying QE in expert search. Any of them could easily be applied using other term weighting models, or from candidate rankings generated using other expert search ap-proaches. Comparing to the no QE baseline system defined in Section 3, the proposed QE approaches markedly outper-form the baseline 3 , suggesting that it is helpful to appro-priately apply QE in expert search. In particular, MAP is generally improved, by applying the proposed approach for QE, however P@10 is less improved. This suggests that ap-plying QE increases the recall of relevant candidates at lower ranks more than perfecting the top-ranked candidates. It is also of note that the proposed approaches are at least com-parable to document centric QE, and in some cases exhibit a marked increase in performance.

Comparing the two first approaches, we can see that the approach based on Hypothesis 1 could be too harsh as it may remove the only useful expertise evidence for relevance feed-
Denoted by underline in Tables 4, 5 &amp; 6  X  0.5445**  X  0.6506**  X   X  0.5381**  X  0.6531**  X   X  0.5522**  X  0.6265 **  X   X  0.5567 **  X  0.6551**  X   X  0.5355**  X  0.6265**  X   X  0.5436**  X  0.6347**  X   X  0.5452**  X  0.6367**  X   X  0.5445**  X  0.6306**  X   X  0.5672  X  0.6776  X   X  0.5582  X  0.6831  X   X  0.5702  X  0.6776  X   X  0.5749  X  0.6857  X   X  0.5693  X  0.6857  X   X  0.5675  X  0.6673  X   X  0.5677  X  0.6755  X   X  0.5672  X  0.6776  X   X  0.5681  X  0.6959  X   X  0.5739  X  0.6918  X   X  0.5696  X  0.6776  X   X  0.5721  X  0.6755  X  back. The approach based on Hypothesis 2 relies heavily on the quality of the document relevance scores. According to the results in Tables 4 &amp; 5, there is no clear winner over both years of the TREC tasks: for TREC 2005, both approaches are equivalent; while for the TREC 2006 task, Candidate Topic Centric QE performs best overall.

The Selective Candidate Topic Centric QE approach pre-sented in Table 6 is a stable approach that consistently out-performs the CandQE baseline (except for P@10 using Bo1 on the TREC 2006 queries) 4 , and outperforms the other ap-proaches. Moreover, we wish to check that the approach does not favour longer or shorter candidates than the origi-nal baseline expert search system. To this end, we devise a measure that sees how prolific the candidates in the ranking are:
Denoted by  X  in Tables 4, 5 &amp; 6 Figure 3: By topic breakdown of changes in MAP for Selective Candidate Topic Centric QE (with Bo1 and sel profile docs = 500 ) on the TREC 2006 topics. where C  X  Q is all the candidate retrieved for query Q , Topics ( C ) is the number of topics from the TREC 2006 task that the candidate C was expert in, and rank ( C, Q ) is the rank of candidate C in the ranking for query Q . In particular, for the TREC 2006 task, using the setting sel profile docs = 500 for the Bo1 term weighting model on the Selective Candidate Topic Centric QE approach, the mean P rolif icness Ranking across all queries is 37.3, which is very similar with the baselines: no QE (37.5); CandQE (37.2); and DocQE (36.6). Hence, we conclude that no addi-tional bias towards candidates with a few or many interests has been introduced by this approach.

To investigate how similar it is to the two query expan-sion baselines (CandQE and DocQE), we calculate the delta MAP from each approach to the baseline, and then correlate. In particular, for the TREC 2006 task, using the setting sel profile docs = 500 for the Bo1 term weighting model on the Selective Candidate Topic Centric QE approach, we find that the delta MAP is more closely correlated with the CandQE baseline than the DocQE baseline (  X  =0 . 518 vs  X  =0 . 305). This is surprising given that the new approach outperforms the CandQE baseline by +24.9%. By compar-ing Figure 3, which shows a histogram of the delta MAP given by the new approach, to Figure 1, it can be seen that the new approach improves on CandQE by eliminating many of the negative queries of CandQE, whilst keeping the better queries.
In this paper, we showed that dealing with the topic drift problem is necessary for a succesfull application of query ex-pansion in expert search. In particular, when topic drift is appropriately dealt with, applying QE can improve on a no QE baseline, and on a simple QE applied on the retrieved documents (DocQE). Moreover, with appropriate settings of the parameters in the proposed approaches, further enhance-ment of retrieval performance is likely. Lastly, the proposed approaches can be easily implemented on top of an existing document search engine, without the need for any additional index structures.

The cohesiveness measures proposed in this work have ap-plications other than in the expert search task. For instance, in a normal search engine, it may be desirable to produce a diverse ranking of documents for ambiguous queries, to satisfy more possible distinct user needs [17].
 Our future research directions from this work are two-fold: Firstly, it is clear that the problem of topic drift does occur, particularly within the expert search task. Further mea-sures that can show when and h ow topic drift is occurring during QE would be beneficial. Secondly, the successfull application of QE to expert search introduces other poten-tial applications, such as finding similar experts, creating a diverse ranking of candidates for ambiguous queries, and even the automatic creation of a  X  X oadmap of expertise X  in an organisation.
