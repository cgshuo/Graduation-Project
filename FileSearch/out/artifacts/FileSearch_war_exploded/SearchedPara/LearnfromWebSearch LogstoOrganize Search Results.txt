 E ectiv e organization of searc h results is critical for impro v-ing the utilit y of any searc h engine. Clustering searc h results is an e ectiv e way to organize searc h results, whic h allo ws a user to navigate into relev ant documen ts quic kly. How-ever, two de ciencies of this approac h mak e it not alw ays work well: (1) the clusters disco vered do not necessarily corresp ond to the interesting asp ects of a topic from the user's persp ectiv e; and (2) the cluster lab els generated are not informativ e enough to allo w a user to iden tify the righ t cluster. In this pap er, we prop ose to address these two de -ciencies by (1) learning \interesting asp ects" of a topic from Web searc h logs and organizing searc h results accordingly; and (2) generating more meaningful cluster lab els using past query words entered by users. We evaluate our prop osed metho d on a commercial searc h engine log data. Compared with the traditional metho ds of clustering searc h results, our metho d can give better result organization and more mean-ingful lab els.
 Categories and Sub ject Descriptors: H.3.3 [Informa-tion Searc h and Retriev al]: Clustering, Searc h pro cess General Terms: Algorithm, Exp erimen tation Keyw ords: Searc h result organization, searc h engine logs, interesting asp ects
The utilit y of a searc h engine is a ected by multiple fac-tors. While the primary factor is the soundness of the under-lying retriev al mo del and ranking function, how to organize and presen t searc h results is also a very imp ortan t factor that can a ect the utilit y of a searc h engine signi can tly. Compared with the vast amoun t of literature on retriev al mo dels, however, there is relativ ely little researc h on how to impro ve the e ectiv eness of searc h result organization.
The most common strategy of presen ting searc h results is a simple rank ed list. Intuitiv ely, suc h a presen tation strat-egy is reasonable for non-am biguous, homogeneous searc h Cop yright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. results; in general, it would work well when the searc h re-sults are good and a user can easily nd man y relev ant doc-umen ts in the top rank ed results.

However, when the searc h results are div erse (e.g., due to ambiguit y or multiple asp ects of a topic) as is often the case in Web searc h, the rank ed list presen tation would not be e ectiv e; in suc h a case, it would be better to group the searc h results into clusters so that a user can easily navigate into a particular interesting group. For example, the results in the rst page returned from Google for the ambiguous query \jaguar" (as of Dec. 2nd, 2006) con tain at least four di eren t senses of \jaguar" (i.e., car, animal, soft ware, and a sports team); even for a more re ned query suc h as \jaguar team picture", the results are still quite ambiguous, includ-ing at least four di eren t jaguar teams { a wrestling team, a jaguar car team, South western College Jaguar softball team, and Jac kson ville Jaguar football team. Moreo ver, if a user wants to nd a place to download a jaguar soft ware, a query suc h as \do wnload jaguar" is also not very e ectiv e as the dominating results are about downloading jaguar bro chure, jaguar wallpap er, and jaguar DVD. In these examples, a clustering view of the searc h results would be much more useful to a user than a simple rank ed list. Clustering is also useful when the searc h results are poor, in whic h case, a user would otherwise have to go through a long list sequen tially to reac h the very rst relev ant documen t.

As a primary alternativ e strategy for presen ting searc h results, clustering searc h results has been studied relativ ely extensiv ely [9, 15, 26, 27, 28]. The general idea in virtually all the existing work is to perform clustering on a set of top-rank ed searc h results to partition the results into natur al clusters, whic h often corresp ond to di eren t subtopics of the general query topic. A lab el will be generated to indicate what eac h cluster is about. A user can then view the lab els to decide whic h cluster to look into. Suc h a strategy has been sho wn to be more useful than the simple rank ed list presen tation in sev eral studies [8, 9, 26].

However, this clustering strategy has two de ciencies whic h mak e it not alw ays work well:
First, the clusters disco vered in this way do not necessarily corresp ond to the interesting asp ects of a topic from the user's persp ectiv e. For example, users are often interested in nding either \phone codes" or \zip codes" when entering the query \area codes." But the clusters disco vered by the curren t metho ds may partition the results into \local codes" and \international codes." Suc h clusters would not be very useful for users; even the best cluster would still have a low precision.

Second, the cluster lab els generated are not informativ e enough to allo w a user to iden tify the righ t cluster. There are two reasons for this problem: (1) The clusters are not corresp onding to a user's interests, so their lab els would not be very meaningful or useful. (2) Even if a cluster really corresp onds to an interesting asp ect of the topic, the lab el may not be informativ e because it is usually generated based on the con ten ts in a cluster, and it is possible that the user is not very familiar with some of the terms. For example, the ambiguous query \jaguar" may mean an animal or a car. A cluster may be lab eled as \pan thera onca." Although this is an accurate lab el for a cluster with the \animal" sense of \jaguar", if a user is not familiar with the phrase, the lab el would not be helpful.

In this pap er, we prop ose a di eren t strategy for parti-tioning searc h results, whic h addresses these two de ciencies through imp osing a user-orien ted partitioning of the searc h results. That is, we try to gure out what asp ects of a searc h topic are likely interesting to a user and organize the results accordingly . Speci cally , we prop ose to do the follo wing:
First, we will learn \interesting asp ects" of similar topics from searc h logs and organize searc h results based on these \interesting asp ects". For example, if the curren t query has occurred man y times in the searc h logs, we can look at what kinds of pages view ed by the users in the results and what kind of words are used together with suc h a query . In case when the query is ambiguous suc h as \jaguar" we can exp ect to see some clear clusters corresp onding di eren t senses of \jaguar". More imp ortan tly, even if a word is not ambiguous (e.g., \car"), we may still disco ver interesting asp ects suc h as \car ren tal" and \car pricing" (whic h happ ened to be the two primary asp ects disco vered in our searc h log data). Suc h asp ects can be very useful for organizing future searc h results about \car". Note that in the case of \car", clus-ters generated using regular clustering may not necessarily re ect suc h interesting asp ects about \car" from a user's persp ectiv e, even though the generated clusters are coher-ent and meaningful in other ways.

Second, we will generate more meaningful cluster lab els using past query words entered by users. Assuming that the past searc h logs can help us learn what speci c asp ects are interesting to users given the curren t query topic, we could also exp ect that those query words entered by users in the past that are asso ciated with the curren t query can pro vide meaningful descriptions of the distinct asp ects. Thus they can be better lab els than those extracted from the ordinary con ten ts of searc h results.

To implemen t the ideas presen ted above, we rely on searc h engine logs and build a history collection con taining the past queries and the asso ciated clic kthroughs. Giv en a new query , we nd its related past queries from the history collection and learn asp ects through applying the star clustering al-gorithm [2] to these past queries and clic kthroughs. We can then organize the searc h results into these asp ects using categorization techniques and lab el eac h asp ect by the most represen tativ e past query in the query cluster.

We evaluate our metho d for result organization using logs of a commercial searc h engine. We compare our metho d with the default searc h engine ranking and the traditional clustering of searc h results. The results sho w that our metho d is e ectiv e for impro ving searc h utilit y and the lab els gen-erated using past query words are more readable than those generated using traditional clustering approac hes.
The rest of the pap er is organized as follo ws. We rst review the related work in Section 2. In Section 3, we de-scrib e searc h engine log data and our pro cedure of building a history collection. In Section 4, we presen t our approac h in details. We describ e the data set in Section 5 and the exp erimen tal results are discussed in Section 6. Finally , we conclude our pap er and discuss future work in Section 7.
Our work is closely related to the study of clustering searc h results. In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documen ts returned from a tra-ditional information retriev al system. Their results validate the cluster hypothesis [20] that relev ant documen ts tend to form clusters. The system \Group er" was describ ed in [26, 27]. In these pap ers, the authors prop osed to cluster the results of a real searc h engine based on the snipp ets or the con ten ts of returned documen ts. Sev eral clustering algo-rithms are compared and the Sux Tree Clustering algo-rithm (STC) was sho wn to be the most e ectiv e one. They also sho wed that using snipp ets is as e ectiv e as using whole documen ts. However, an imp ortan t challenge of documen t clustering is to generate meaningful lab els for clusters. To overcome this dicult y, in [28], sup ervised learning algo-rithms were studied to extract meaningful phrases from the searc h result snipp ets and these phrases were then used to group searc h results. In [13], the authors prop osed to use a monothetic clustering algorithm, in whic h a documen t is assigned to a cluster based on a single feature, to organize searc h results, and the single feature is used to lab el the corresp onding cluster. Clustering searc h results has also at-tracted a lot of atten tion in industry and commercial Web services suc h as Vivisimo [22]. However, in all these works, the clusters are generated solely based on the searc h results. Thus the obtained clusters do not necessarily re ect users' preferences and the generated lab els may not be informativ e from a user's viewp oint.

Metho ds of organizing searc h results based on text cate-gorization are studied in [6, 8]. In this work, a text classi-er is trained using a Web directory and searc h results are then classi ed into the prede ned categories. The authors designed and studied di eren t category interfaces and they found that category interfaces are more e ectiv e than list interfaces. However prede ned categories are often too gen-eral to re ect the ner gran ularit y asp ects of a query .
Searc h logs have been exploited for sev eral di eren t pur-poses in the past. For example, clustering searc h queries to nd those Frequen t Ask ed Questions (FAQ) is studied in [24, 4]. Recen tly, searc h logs have been used for suggesting query substitutes [12], personalized searc h [19], Web site design [3], Laten t Seman tic Analysis [23], and learning retriev al rank-ing functions [16, 10, 1]. In our work, we explore past query history in order to better organize the searc h results for fu-ture queries. We use the star clustering algorithm [2], whic h is a graph partition based approac h, to learn interesting as-pects from searc h logs given a new query . Thus past queries are clustered in a query speci c manner and this is another di erence from previous works suc h as [24, 4] in whic h all queries in logs are clustered in an oine batc h manner.
Searc h engine logs record the activities of Web users, whic h re ect the actual users' needs or interests when conducting Table 1: Sample entries of searc h engine logs. Dif-feren t ID's mean di eren t sessions.
 Web searc h. They generally have the follo wing informa-tion: text queries that users submitted, the URLs that they clic ked after submitting the queries, and the time when they clic ked. Searc h engine logs are separated by sessions . A session includes a single query and all the URLs that a user clic ked after issuing the query [24]. A small sample of searc h log data is sho wn in Table 1.

Our idea of using searc h engine logs is to treat these logs as past history , learn users' interests using this history data automatically , and represen t their interests by represen ta-tive queries. For example, in the searc h logs, a lot of queries are related to \car" and this re ects that a large num ber of users are interested in information about \car." Di eren t users are probably interested in di eren t asp ects of \car." Some are looking for ren ting a car, thus may submit a query like \car ren tal"; some are more interested in buying a used car, and may submit a query like \used car"; and others may care more about buying a car accessory , so they may use a query like \car audio." By mining all the queries whic h are related to the concept of \car", we can learn the asp ects that are likely interesting from a user's persp ectiv e. As an example, the follo wing is some asp ects about \car" learned from our searc h log data (see Section 5). 1. car rental, hertz car rental, enterprise car 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...

In order to learn asp ects from searc h engine logs, we pre-pro cess the raw logs to build a history data collection. As sho wn above, searc h engine logs consist of sessions. Eac h session con tains the information of the text query and the clic ked Web page URLs, together with the time that the user did the clic ks. However, this information is limited since URLs alone are not informativ e enough to tell the in-tended meaning of a submitted query accurately . To gather rich information, we enric h eac h URL with additional text con ten t. Speci cally , given the query in a session, we obtain its top-rank ed results using the searc h engine from whic h we obtained our log data, and extract the snipp ets of the URLs that are clic ked on according to the log information in the corresp onding session. All the titles, snipp ets, and URLs of the clic ked Web pages of that query are used to represen t the session.

Di eren t sessions may con tain the same queries. Thus the num ber of sessions could be quite huge and the informa-tion in the sessions with the same queries could be redun-dan t. In order to impro ve the scalabilit y and reduce data sparseness, we aggregate all the sessions whic h con tain ex-actly the same queries together. That is, for eac h unique query , we build a \pseudo-do cumen t" whic h consists of all the descriptions of its clic ks in all the sessions aggregated. The keyw ords con tained in the queries themselv es can be regarded as brief summaries of the pseudo-do cumen ts. All these pseudo-do cumen ts form our history data collection, whic h is used to learn interesting asp ects in the follo wing section.
Our approac h is to organize searc h results by asp ects learned from searc h engine logs. Giv en an input query , the general pro cedure of our approac h is: 1. Get its related information from searc h engine logs. 2. Learn asp ects from the information in the working set. 3. Categorize and organize the searc h results of the input
We now give a detailed presen tation of eac h step.
Giv en a query q , a searc h engine will return a rank ed list of Web pages. To kno w what the users are really interested in given this query , we rst retriev e its past similar queries in our prepro cessed history data collection.

Formally , assume we have N pseudo-do cumen ts in our history data set: H = f Q 1 ; Q 2 ; :::; Q N g . Eac h Q sponds to a unique query and is enric hed with clic kthrough information as discussed in Section 3. To nd q 's related queries in H , a natural way is to use a text retriev al al-gorithm. Here we use the OKAPI metho d [17], one of the state-of-the-art retriev al metho ds. Speci cally , we use the follo wing form ula to calculate the similarit y between query q and pseudo-do cumen t Q i : where k 1 and b are OKAPI parameters set empirically , c ( w; Q and c ( w; q ) are the coun t of word w in Q i and q resp ectiv ely, IDF ( w ) is the inverse documen t frequency of word w , and avdl is the average documen t length in our history collec-tion.

Based on the similarit y scores, we rank all the documen ts in H . The top rank ed documen ts pro vide us a working set to learn the asp ects that users are usually interested in. Eac h documen t in H corresp onds to a past query , and thus the top rank ed documen ts corresp ond to q 's related past queries.
Giv en a query q , we use H q = f d 1 ; :::; d n g to represen t the top rank ed pseudo-do cumen ts from the history collection H . These pseudo-do cumen ts con tain the asp ects that users are interested in. In this subsection, we prop ose to use a clustering metho d to disco ver these asp ects.

Any clustering algorithm could be applied here. In this pap er, we use an algorithm based on graph partition: the star clustering algorithm [2]. A good prop erty of the star clustering in our setting is that it can suggest a good lab el for eac h cluster naturally . We describ e the star clustering algorithm below.

Giv en H q , star clustering starts with constructing a pair-wise similarit y graph on this collection based on the vector space mo del in information retriev al [18]. Then the clusters are formed by dense subgraphs that are star-shap ed. These clusters form a cover of the similarit y graph. Formally , for eac h of the n pseudo-do cumen ts f d 1 ; :::; d n g in the collection H , we compute a TF-IDF vector. Then, for eac h pair of documen ts d i and d j ( i 6 = j ), their similarit y is computed as the cosine score of their corresp onding vectors v i and v that is
A similarit y graph G can then be constructed as follo ws using a similarit y threshold parameter . Eac h documen t d is a vertex of G . If sim ( d i ; d j ) &gt; , there would be an edge connecting the corresp onding two vertices. After the similarit y graph G is built, the star clustering algorithm clusters the documen ts using a greedy algorithm as follo ws: 1. Asso ciate every vertex in G with a ag, initialized as 2. From those unmarke d vertices, nd the one whic h has 3. Mark the ag of u as center . 4. Form a cluster C con taining u and all its neigh bors 5. Rep eat from step 2 until all the vertices in G are
Eac h cluster is star-shap ed , whic h consists a single center and sev eral satel lites . There is only one parameter in the star clustering algorithm. A big enforces that the connected documen ts have high similarities, and thus the clusters tend to be small. On the other hand, a small will mak e the clusters big and less coheren t. We will study the impact of this parameter in our exp erimen ts.

A good feature of the star clustering algorithm is that it outputs a center for eac h cluster. In the past query collec-tion H q , eac h documen t corresp onds to a query . This center query can be regarded as the most represen tativ e one for the whole cluster, and thus pro vides a label for the cluster naturally . All the clusters obtained are related to the input query q from di eren t persp ectiv es, and they represen t the possible asp ects of interests about query q of users.
In order to organize the searc h results according to users' interests, we use the learned asp ects from the related past queries to categorize the searc h results. Giv en the top m Web pages returned by a searc h engine for q : f s 1 ; :::; s we group them into di eren t asp ects using a categorization algorithm.

In principle, any categorization algorithm can be used here. Here we use a simple cen troid-based metho d for cat-egorization. Naturally , more sophisticated metho ds suc h as SVM [21] may be exp ected to achiev e even better perfor-mance.
 Based on the pseudo-do cumen ts in eac h disco vered asp ect C , we build a cen troid protot ype p i by taking the average of all the vectors of the documen ts in C i : All these p i 's are used to categorize the searc h results. Specif-ically , for any searc h result s j , we build a TF-IDF vector. The cen troid-based metho d computes the cosine similarit y between the vector represen tation of s j and eac h cen troid protot ype p i . We then assign s j to the asp ect with whic h it has the highest cosine similarit y score.

All the asp ects are nally rank ed according to the num ber of searc h results they have. Within eac h asp ect, the searc h results are rank ed according to their original searc h engine ranking.
We construct our data set based on the MSN searc h log data set released by the Microsoft Liv e Labs in 2006 [14]. In total, this log data spans 31 days from 05/01/2006 to 05/31/2006. There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.
To test our algorithm, we separate the whole data set into two parts according to the time: the rst 2/3 data is used to sim ulate the historical data that a searc h engine accum u-lated, and we use the last 1/3 to sim ulate future queries. In the history collection, we clean the data by only keep-ing those frequen t, well-formatted, English queries (queries whic h only con tain characters `a', `b', ..., `z', and space, and app ear more than 5 times). After cleaning, we get 169,057 unique queries in our history data collection totally . On average, eac h query has 3.5 distinct clic ks. We build the \pseudo-do cumen ts" for all these queries as describ ed in Section 3. The average length of these pseudo-do cumen ts is 68 words and the total data size of our history collection is 129MB.

We construct our test data from the last 1/3 data. Ac-cording to the time, we separate this data into two test sets equally for cross-v alidation to set parameters. For eac h test set, we use every session as a test case. Eac h session con-tains a single query and sev eral clic ks. (Note that we do not aggregate sessions for test cases. Di eren t test cases may have the same queries but possibly di eren t clic ks.) Since it is infeasible to ask the original user who submitted a query to judge the results for the query , we follo w the work [11] and opt to use the clic ks asso ciated with the query in a session to appr oximate relev ant documen ts. Using clic ks as judgmen ts, we can then compare di eren t algorithms for or-ganizing searc h results to see how well these algorithms can help users reac h the clic ked URLs.

Organizing searc h results into di eren t asp ects is exp ected to help informational queries. It thus mak es sense to focus on the informational queries in our evaluation. For eac h test case, i.e., eac h session, we coun t the num ber of di eren t clic ks and lter out those test cases with few er than 4 clic ks under the assumption that a query with more clic ks is more likely to be an informational query . Since we want to test whether our algorithm can learn from the past queries, we also lter out those test cases whose queries can not retriev e at least 100 pseudo-do cumen ts from our history collection. Finally , we obtain 172 and 177 test cases in the rst and second test sets resp ectiv ely. On average, we have 6.23 and 5.89 clic ks for eac h test case in the two test sets resp ectiv ely.
In the section, we describ e our exp erimen ts on the searc h result organization based past searc h engine logs.
We use two baseline metho ds to evaluate the prop osed metho d for organizing searc h results. For eac h test case, the rst metho d is the default rank ed list from a searc h engine (baseline). The second metho d is to organize the searc h results by clustering them (cluster-based). For fair comparison, we use the same clustering algorithm as our log-based metho d (i.e., star clustering). That is, we treat eac h searc h result as a documen t, construct the similarit y graph, and nd the star-shap ed clusters. We compare our metho d (log-based) with the two baseline metho ds in the follo wing exp erimen ts. For both cluster-based and log-based metho ds, the searc h results within eac h cluster is rank ed based on their original ranking given by the searc h engine.

To compare di eren t result organization metho ds, we adopt a similar metho d as in the pap er [9]. That is, we compare the qualit y (e.g., precision) of the best cluster, whic h is de ned as the one with the largest num ber of relev ant documen ts. Organizing searc h results into clusters is to help users navi-gate into relev ant documen ts quic kly. The above metric is to sim ulate a scenario when users alw ays choose the righ t clus-ter and look into it. Speci cally , we download and organize the top 100 searc h results into asp ects for eac h test case. We use Precision at 5 documen ts (P@5) in the best cluster as the primary measure to compare di eren t metho ds. P@5 is a very meaningful measure as it tells us the perceive d preci-sion when the user opens a cluster and looks at the rst 5 documen ts. We also use Mean Recipro cal Rank (MRR) as another metric. MRR is calculated as where T is a set of test queries, r q is the rank of the rst relev ant documen t for q .

To give a fair comparison across di eren t organization al-gorithms, we force both cluster-based and log-based meth-ods to output the same num ber of asp ects and force eac h searc h result to be in one and only one asp ect. The num-ber of asp ects is xed at 10 in all the follo wing exp erimen ts. The star clustering algorithm can output di eren t num ber of clusters for di eren t input. To constrain the num ber of clusters to 10, we order all the clusters by their sizes, select the top 10 as asp ect candidates. We then re-assign eac h searc h result to one of these selected 10 asp ects that has the highest similarit y score with the corresp onding asp ect cen troid. In our exp erimen ts, we observ e that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric.
Our main hypothesis is that organizing searc h results based on the users' interests learned from a searc h log data set is more bene cial than to organize results using a simple list or cluster searc h results. In the follo wing, we test our hy-pothesis from two persp ectiv es { organization and lab eling. Table 2: Comparison of di eren t metho ds by MMR and P@5. We also sho w the percen tage of relativ e impro vemen t in the lower part.
 Table 3: Pairwise comparison w.r.t the num ber of test cases whose P@5's are impro ved versus de-creased w.r.t the baseline.
We compare three metho ds, basic searc h engine rank-ing (baseline), traditional clustering based metho d (cluster-based), and our log based metho d (log-based), in Table 2 us-ing MRR and P@5. We optimize the parameter 's for eac h collection individually based on P@5 values. This sho ws the best performance that eac h metho d can achiev e. In this ta-ble, we can see that in both test collections, our metho d is better than both the \baseline" and the \cluster-based" metho ds. For example, in the rst test collection, the base-line metho d of MMR is 0.734, the cluster-based metho d is 0.773 and our metho d is 0.783. We achiev e higher accu-racy than both cluster-based metho d (1.27% impro vemen t) and the baseline metho d (6.62% impro vemen t). The P@5 values are 0.332 for the baseline, 0.316 for cluster-based metho d, but 0.353 for our metho d. Our metho d impro ves over the baseline by 6.31%, while the cluster-based metho d even decreases the accuracy . This is because cluster-based metho d organizes the searc h results only based on the con-ten ts. Thus it could organize the results di eren tly from users' preferences. This con rms our hypothesis of the bias of the cluster-based metho d. Comparing our metho d with the cluster-based metho d, we achiev e signi can t impro ve-men t on both test collections. The p-v alues of the signi -cance tests based on P@5 on both collections are 0.01 and 0.02 resp ectiv ely. This sho ws that our log-based metho d is e ectiv e to learn users' preferences from the past query his-tory , and thus it can organize the searc h results in a more useful way to users.

We sho wed the optimal results above. To test the sensi-tivit y of the parameter of our log-based metho d, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set. We compare this result (log tuned outside) with the optimal re-sults of both cluster-based (cluster optimized) and log-based metho ds (log optimized) in Figure 1. We can see that, as exp ected, the performance using the parameter tuned on a separate set is worse than the optimal performance. How-ever, our metho d still performs much better than the optimal results of cluster-based metho d on both test collections. Figure 1: Results using parameters tuned from the other test collection. We compare it with the opti-mal performance of the cluster-based and our log-based metho ds. Figure 2: The correlation between performance change and result div ersit y.

In Table 3, we sho w pairwise comparisons of the three metho ds in terms of the num bers of test cases for whic h P@5 is increased versus decreased. We can see that our metho d impro ves more test cases compared with the other two metho ds. In the next section, we sho w more detailed analysis to see what types of test cases can be impro ved by our metho d.
To better understand the cases where our log-based metho d can impro ve the accuracy , we test two prop erties: result di-versit y and query dicult y. All the analysis below is based on test set 1.

Div ersit y Analysis: Intuitiv ely, organizing searc h re-sults into di eren t asp ects is more bene cial to those queries whose results are more div erse, as for suc h queries, the re-sults tend to form two or more big clusters. In order to test the hypothesis that log-based metho d help more those queries with div erse results, we compute the size ratios of the biggest and second biggest clusters in our log-based re-sults and use this ratio as an indicator of div ersit y. If the ratio is small, it means that the rst two clusters have a small di erence thus the results are more div erse. In this case, we would exp ect our metho d to help more. The re-sults are sho wn in Figure 2. In this gure, we partition the ratios into 4 bins. The 4 bins corresp ond to the ratio ranges [1 ; 2), [2 ; 3), [3 ; 4), and [4 ; + 1 ) resp ectiv ely. ([ i; j ) means that i ratio &lt; j .) In eac h bin, we coun t the num bers of test cases whose P@5's are impro ved versus decreased with resp ect to the ranking baseline, and plot the num bers in this gure. We can observ e that when the ratio is smaller, the log-based metho d can impro ve more test cases. But when Figure 3: The correlation between performance change and query dicult y. the ratio is large, the log-based metho d can not impro ve over the baseline. For example, in bin 1, 48 test cases are impro ved and 34 are decreased. But in bin 4, all the 4 test cases are decreased. This con rms our hypothesis that our metho d can help more if the query has more div erse results. This also suggests that we should \turn o " the option of re-organizing searc h results if the results are not very div erse (e.g., as indicated by the cluster size ratio).

Dicult y Analysis: Dicult queries have been studied in recen t years [7, 25, 5]. Here we analyze the e ectiv eness of our metho d in helping dicult queries. We quan tify the query dicult y by the Mean Average Precision (MAP) of the original searc h engine ranking for eac h test case. We then order the 172 test cases in test set 1 in an increasing order of MAP values. We partition the test cases into 4 bins with eac h having a roughly equal num ber of test cases. A small MAP means that the utilit y of the original ranking is low. Bin 1 con tains those test cases with the lowest MAP's and bin 4 con tains those test cases with the highest MAP's. For eac h bin, we compute the num bers of test cases whose P@5's are impro ved versus decreased. Figure 3 sho ws the results. Clearly , in bin 1, most of the test cases are impro ved (24 vs 3), while in bin 4, log-based metho d may decrease the performance (3 vs 20). This sho ws that our metho d is more bene cial to dicult queries, whic h is as exp ected since clustering searc h results is intended to help dicult queries. This also sho ws that our metho d does not really help easy queries, thus we should turn o our organization option for easy queries.
We examine parameter sensitivit y in this section. For the star clustering algorithm, we study the similarit y threshold parameter . For the OKAPI retriev al function, we study the parameters k 1 and b . We also study the impact of the num ber of past queries retriev ed in our log-based metho d.
Figure 4 sho ws the impact of the parameter for both cluster-based and log-based metho ds on both test sets. We vary from 0.05 to 0.3 with step 0.05. Figure 4 sho ws that the performance is not very sensitiv e to the parameter . We can alw ays obtain the best result in range 0 : 1 0 : 25. In Table 4, we sho w the impact of OKAPI parameters. We vary k 1 from 1.0 to 2.0 with step 0 : 2 and b from 0 to 1 with step 0 : 2. From this table, it is clear that P@5 is also not very sensitiv e to the parameter setting. Most of the values are larger than 0.35. The default values k 1 = 1 : 2 and b = 0 : 8 give appro ximately optimal results.
 We further study the impact of the amoun t of history Figure 4: The impact of similarit y threshold on both cluster-based and log-based metho ds. We sho w the result on both test collections.
Table 4: Impact of OKAPI parameters k 1 and b . information to learn from by varying the num ber of past queries to be retriev ed for learning asp ects. The results on both test collections are sho wn in Figure 5. We can see that the performance gradually increases as we enlarge the num ber of past queries retriev ed. Thus our metho d could poten tially learn more as we accum ulate more history . More imp ortan tly, as time goes, more and more queries will have sucien t history , so we can impro ve more and more queries.
We use the query \area codes" to sho w the di erence in the results of the log-based metho d and the cluster-based metho d. This query may mean \phone codes" or \zip codes". Table 5 sho ws the represen tativ e keyw ords extracted from the three biggest clusters of both metho ds. In the cluster-based metho d, the results are partitioned based on locations: local or international. In the log-based metho d, the results are disam biguated into two senses: \phone codes" or \zip codes". While both are reasonable partitions, our evalua-tion indicates that most users using suc h a query are often interested in either \phone codes" or \zip codes." since the P@5 values of cluster-based and log-based metho ds are 0.2 and 0.6, resp ectiv ely. Therefore our log-based metho d is more e ectiv e in helping users to navigate into their desired results.
 Table 5: An example sho wing the di erence between the cluster-based metho d and our log-based metho d Figure 5: The impact of the num ber of past queries retriev ed.
We now compare the lab els between the cluster-based metho d and log-based metho d. The cluster-based metho d has to rely on the keyw ords extracted from the snipp ets to construct the lab el for eac h cluster. Our log-based metho d can avoid this dicult y by taking adv antage of queries. Specif-ically , for the cluster-based metho d, we coun t the frequency of a keyw ord app earing in a cluster and use the most fre-quen t keyw ords as the cluster lab el. For log-based metho d, we use the center of eac h star cluster as the lab el for the corresp onding cluster.

In general, it is not easy to quan tify the readabilit y of a cluster lab el automatically . We use examples to sho w the di erence between the cluster-based and the log-based meth-ods. In Table 6, we list the lab els of the top 5 clusters for two examples \jaguar" and \apple". For the cluster-based metho d, we separate keyw ords by commas since they do not form a phrase. From this table, we can see that our log-based metho d gives more readable lab els because it generates la-bels based on users' queries. This is another adv antage of our way of organizing searc h results over the clustering ap-proac h.

In this pap er, we studied the problem of organizing searc h results in a user-orien ted manner. To attain this goal, we rely on searc h engine logs to learn interesting asp ects from users' persp ectiv e. Giv en a query , we retriev e its related queries from past query history , learn the asp ects by clus-tering the past queries and the asso ciated clic kthrough in-formation, and categorize the searc h results into the asp ects learned. We compared our log-based metho d with the tra-ditional cluster-based metho d and the baseline of searc h en-gine ranking. The exp erimen ts sho w that our log-based metho d can consisten tly outp erform cluster-based metho d and impro ve over the ranking baseline, esp ecially when the queries are dicult or the searc h results are div erse. Fur-thermore, our log-based metho d can generate more mean-ingful asp ect lab els than the cluster lab els generated based on searc h results when we cluster searc h results.
There are sev eral interesting directions for further extend-ing our work: First, although our exp erimen t results have clearly sho wn promise of the idea of learning from searc h logs to organize searc h results, the metho ds we have exp er-imen ted with are relativ ely simple. It would be interesting to explore other poten tially more e ectiv e metho ds. In par-ticular, we hop e to dev elop probabilistic mo dels for learning asp ects and organizing results sim ultaneously . Second, with the prop osed way of organizing searc h results, we can ex-pect to obtain informativ e feedbac k information from a user (e.g., the asp ect chosen by a user to view). It would thus be interesting to study how to further impro ve the organi-zation of the results based on suc h feedbac k information. Finally , we can com bine a general searc h log with any per-sonal searc h log to customize and optimize the organization of searc h results for eac h individual user.
We thank the anon ymous review ers for their valuable com-men ts. This work is in part supp orted by a Microsoft Liv e Labs Researc h Gran t, a Google Researc h Gran t, and an NSF CAREER gran t IIS-0347933. [1] E. Agic htein, E. Brill, and S. T. Dumais. Impro ving [2] J. A. Aslam, E. Pelek ov, and D. Rus. The star [3] R. A. Baeza-Y ates. Applications of web query mining. [4] D. Beeferman and A. L. Berger. Agglomerativ e [5] D. Carmel, E. Yom-T ov, A. Darlo w, and D. Pelleg. [6] H. Chen and S. T. Dumais. Bringing order to the web: [7] S. Cronen-T ownsend, Y. Zhou, and W. B. Croft. [8] S. T. Dumais, E. Cutrell, and H. Chen. Optimizing [9] M. A. Hearst and J. O. Pedersen. Reexamining the [10] T. Joac hims. Optimizing searc h engines using [11] T. Joac hims. Evaluating Retrieval Performanc e Using [12] R. Jones, B. Rey , O. Madani, and W. Greiner. [13] K. Kummam uru, R. Lotlik ar, S. Roy, K. Singal, and [14] Microsoft Liv e Labs. Accelerating searc h in academic [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl. [16] F. Radlinski and T. Joac hims. Query chains: learning [17] S. E. Rob ertson and S. Walker. Some simple e ectiv e [18] G. Salton, A. Wong, and C. S. Yang. A vector space [19] X. Shen, B. Tan, and C. Zhai. Con text-sensitiv e [20] C. J. van Rijsb ergen. Information Retrieval, second [21] V. N. Vapnik. The Natur e of Statistic al Learning [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai. Laten t [24] J.-R. Wen, J.-Y. Nie, and H. Zhang. Clustering user [25] E. Yom-T ov, S. Fine, D. Carmel, and A. Darlo w. [26] O. Zamir and O. Etzioni. Web documen t clustering: A [27] O. Zamir and O. Etzioni. Group er: A dynamic [28] H.-J. Zeng, Q.-C. He, Z. Chen, W.-Y. Ma, and J. Ma.
