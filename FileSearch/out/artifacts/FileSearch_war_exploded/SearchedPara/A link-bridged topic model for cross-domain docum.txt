 1. Introduction
Traditional machine learning approaches make a basic assumption that the training and test data should be drawn from the same feature space and follow the same distribution. In many real-world applications, however, this independent and identically distributed (i.i.d.) assumption does not hold. It has been extensively demonstrated in the literatures that tradi-tional leaning models perform drastically worse when the i.i.d. assumption no longer holds ( Dai, Yang, Xue, &amp; Yu, 2007; different from those in testing. It utilizes labeled data available from some related (or source) domain in order to achieve effective knowledge transformation from it to the target domain, which plays an important role in the areas of machine learning and data mining. If done successfully, knowledge transfer would greatly improve the performance of learning by  X  avoiding tremendously expensive data annotation effort. Many examples in knowledge engineering justified that transfer learning can be generally beneficial for different applications, such as document classification ( Sarinnapakorn &amp; Kubat, &amp; Liu Nathan, 2010 ), and Web search ranking ( Gao, Cai, Wong, &amp; Zhou, 2010 ).
 tual contents that are interconnected via complex hyperlinks or citations, such as encyclopedia websites (e.g., Wikipedia), research paper archives (e.g., CiteSeer), and user-generated media (e.g., blogs and microblogs). Such kind of data is charac-terized as analogous structures where each article describes a topic (or concept), which contains a title, abstract, content and some references. A typical example is the Wikipedia page on Support Vector Machine. 1 Compared to the documents in tra-ditional information management, these types of data contain links in addition to content. The hyperlinks (or citations) among articles capture their semantic relations and provide additional insights about their relationships.
 the structure of links (or citations). However, link structures provide important information regarding the properties of doc-uments and their relationships. Since the links imply the inter-dependence among the documents, the usual i.i.d. (i.e., inde-pendent and identically distributed) assumption of documents does not hold any more ( Zhu, Yu, Chi, &amp; Gong, 2007 ). From this point of view, the existing cross-domain document classification methods that ignore the link structure may fail to cap-ture the dependency and would be unable to fully mine the common knowledge between different domains. It turns out that the transfer of cross-domain information could be seriously hindered due to the incompleteness of the mined common knowledge.
 document classification. The basic idea is that documents in different domains may share some common topics from the point of view of both content information and link structure, which could be used to mutually reinforce the identification of common topics, thus to enhance the classification knowledge across related but distinct domains. However, there are two essential problems that challenge the procedure of integrating link structures with the shared topics. First, the link data are usually very sparse and the common parts indirectly connected between domains cannot be fully discovered and uti-lized. For this reason, we utilize an auxiliary link network to strengthen the co-citation relationship among documents by embedding the background knowledge into a graph kernel. Our method cannot only enrich the document representation by reducing the data sparseness, but also enlarge the dimensional feature space by introducing new features shared by the documents, which would help fill the gap across domains. Secondly, it is difficult to come up with a unified model that combines the two types of information simultaneously because the learned decompositions of topics must be consistent with content and link statistics as well as the training and test data from different domains, following the basic principle of multi-view transfer learning regarding how to model the domain commonality and difference from the perspective of multiple views. To deal with this problem, we propose a probabilistic Link-Bridged Topic model (LBT) based on Probabilistic
Latent Semantic Analysis (PLSA) ( Hofmann, 1999 ) for cross-domain knowledge transfer using a multi-view approach. LBT correlates the domain-specific features and encodes the domain commonality and distinction, as well as view consistency and difference, into the shared topics. Then the shared topics act as a bridge which helps knowledge transfer from the source to target domain. We derive the log-likelihood objective function for LBT and use EM algorithm for its optimization. Exper-imental results based on two types of datasets demonstrate that our method outperforms state-of-the-art baselines includ-ing the semi-supervised learning algorithm Transductive SVM ( Joachims, 1999 ), the traditional multi-view algorithm Co-rence, 2011 ) and the content-based transfer learning algorithm TPLSA ( Xue, Dai, Yang, &amp; Yu, 2008 ).
 model for cross-domain document classification; Section 4 discusses the experiments and analyzes the results; finally, we conclude in Section 5 with discussions on future work. 2. Related work 2.1. Transfer learning from single view robust classifier by leveraging a large amount of unlabeled data. Some popular semi-supervised learning models include self-graph-based methods ( Joachims, 2003 ), transductive support vector machines ( Joachims, 1999 ), and collective classification following the same distribution. In contrast, transfer learning allows the domains, distributions, and feature spaces used in training and testing to be different ( Pan &amp; Yang, 2010 ). Transfer learning was extensively studied in the machine learning community over the last decade. Its underlying assumption is that multiple tasks share certain structures, and therefore, the tasks can mutually benefit from these shared structures.
Most existing transfer learning methods can be classified into three categories: instance-transfer, feature-transfer and parameter-transfer. Instance-level approach ( Dai et al., 2007; Xiang, Cao, Hu, &amp; Yang, 2010 ) assumes that some training examples in the source domain are similar to the data in target domain which can be used to train the model for the target domain. Re-weighting and importance sampling are two major techniques in instance-transfer learning. The basic idea of domains. The major techniques in feature-transfer approach ( Pan et al., 2010; Raina, Ng, &amp; Koller, 2006; Wang, Domeniconi, that reduces the domain distance. Parameter-transfer approach assumes that the source and target domains share some parameters or priors of their models. The objective of parameter-transfer approach ( Dayanik, Lewis, Madigan, Menkov, &amp; Genkin, 2006; Fujino, Ueda, &amp; Nagata, 2010 ) is to discover the shared hyper-parameters or priors between domains. Our method is essentially a feature-based transfer approach by transforming the original feature space to a latent space, which captures the domain commonality as well as discrepancy.

Several approaches to transfer learning such as ( Xue et al., 2008; Yang, Chen, Xue, Dai, &amp; Yu, 2009; Zhuang et al., 2010 ) make use of PLSA ( Hofmann, 1999 ). PLSA is a widely used probabilistic model, which provides solid statistical foundation. PLSA could be considered as a probabilistic implementation of latent semantic analysis (LSA) ( Deerwester, Dumais, Furnas, Thomas, &amp; Harshman, 1990 ). In this model, each document is considered as the convex combination of several topics, where these topics or latent semantic variables are obtained using the maximum-likelihood principle. An extension to PLSA was proposed in ( Cohn &amp; Hofmann, 2000 ) for identifying principal topics of document collection as well as authoritative docu-ments within those topics, which incorporates the hyperlink connectivity in the PLSA model by using a joint probabilistic model for connectivity and content. The model treats all the documents as being from the same domain and the distribution difference is not taken into consideration. Likewise, Erosheva, Fienberg, and Lafferty (2004) adopted a mixed membership model for words and references in journal publications but treated membership scores as random Dirichlet realizations. Un-like Erosheva et al. (2004) which only uses the references information among the publications themselves, we utilize an aux-iliary link network to mine the indirect co-citation relationship among the documents, which could help bridge the domains gap. Yang et al. (2009) present a new learning scenario, heterogeneous transfer learning, which improves learning perfor-mance when the data can be represented in different feature spaces and where no correspondence between data instances in these spaces is provided. They extend PLSA to help transfer the knowledge from social Web data, which have mixed fea-ture representations. Xue et al. (2008) propose TPLSA to incorporate both labeled and unlabeled data. The hidden variables are used to bridge the documents in training and test domains, and learned under a joint probabilistic model. TPLSA is based on a simultaneous decomposition of the contingency tables associated with term occurrence knowledge in documents from both training and test domains, which identifies the principal topics of the training data as well as documents in the test data that support those topics. Zhuang et al. (2010) propose the Collaborative Dual-PLSA model to simultaneously capture both the domain distinction and commonality among multiple domains. The proposed model has two latent factors, i.e. word con-cept and document class.

Our work is closely related to TPLSA ( Xue et al., 2008 ). Unlike TPLSA which only uses content as bridge, in our work, we focus on combining the content and link information to enhance the view consistency, which is a key issue for the success of multi-view transfer learning as discussed below. Furthermore, we make use of auxiliary link network to alleviate the data sparseness and help knowledge transfer across domains. These two key points make our proposed model distinctive from TPLSA ( Xue et al., 2008 ). To the best of our knowledge, there is no existing study that focused on incorporating auxiliary link network for cross-domain document classification. 2.2. Transfer learning from multiple views
Our work is also related to multi-view learning, where observations are represented by multiple independent sets of fea-tures ( Ruping &amp; Scheffer, 2005 ). Blum and Mitchell (1998) introduced Co-Training. The idea is to train one learner on each view of the labeled examples and then to iteratively have each learner to label the unlabeled examples that receive the high-est confidence. They proved that two independent yet compatible views can be used to learn a concept in the PAC ( Valiant, 1984 ) framework based on few labeled and many unlabeled examples. Following the idea, many people extended the ori-ginal Co-Training approach ( Brefeld, 2004; Ghani, 2002; Nigam, McCallum, Thrun, &amp; Mitchell, 2000 ).

Most research on multi-view learning is within a single domain, and multi-view transferring learning is not common. Our work can be considered as a case of multi-view transfer. Tur (2009) proposed a co-adaptation algorithm, which extends the Co-Training algorithm with model adaptation techniques. Co-adaptation makes the existing model adaptive using machine-labeled data with some weight tuned using a held-out set. Co-adaptation is designed for inductive transfer learning which assumes the target domain has a small amount of labeled data. Zhang et al. (2011) proposed a framework for multi-view transfer learning with a large margin approach. The labeled data from the source domain are weighted and used to construct a large margin classifier for target domain, and data from both domains are used to ensure the classification consistency be-tween different views. The instance-level approach assumes that some similar source training examples can be identified and reused to train the target model. However, the performance of instance-based approach is generally poor since new tar-get features lack support from source data ( Blitzer et al., 2011 ). We focus on feature-level multi-view adaptation or transfer, where knowledge transformation takes place in the multiple transformed feature spaces simultaneously and complementarily.
 3. Link-bridged topic model domains for knowledge transformation. However, if cross-domain document classification methods only focus on the data on the source and target domains themselves, they may fail to capture the common parts among the domains that are indi-rectly connected. We observe that the indirect common co-citation relationship can be enhanced and mined with the help of an auxiliary link network. Furthermore, we combine the content information and co-citation relations using a unified prob-abilistic model based on PLSA ( Hofmann, 1999 ) which maps the data from both domains to the latent topic space. Such map-ping could correlate the domain-specific features via the shared topics. Then the domain commonality and difference are characterized by the shared but differential topics. In other words, the documents from both domains share some similar topics, while their associated probability distributions with the shared topics are to some extent discrepant. On the other hand, since the documents are described with multiple views (i.e., content and links), it can be expected that the general-ization capacity of the model will be further enhanced by leveraging the complementary interactions between different views. 3.1. Problem statement from the source domain, D t be the unlabeled documents set from the target domain. Define D = D s [ D t . The source and target data are assumed to draw from different feature spaces where the i.i.d. assumption no longer holds. Some features are de-fined in source or target domain only while some others are defined in both domains. For the easy of cross-domain feature transformation in the later stage, we technically expand the feature space in pre-processing to include all features from both domains into a unified space, where the missing features in either domain are replenished as 0.
 of feature-vector to represent the document, i.e. bag-of-words and bag-of-links, in order to incorporate the link structures.
Let W be the vocabulary of the document collection. Each document d is represented by a bag-of-words set dataset. 3.2. Bridging domain gaps using auxiliary link network number of references in a paper is normally just about 20 X 30. Thus, how to alleviate the sparseness of link data is a key issue for building a robust cross-domain classifier using link structures among the documents.
 the link information in the concerned document collection itself. We observe that such kind of indirect co-citation relation-ship can be enhanced and mined with the help of an auxiliary link network. For example, given the research papers in the areas of classification and clustering, we may turn to ACM paper citation network to find more indirect co-citation relation-ship among the articles from these two different domains, which would help alleviate the sparseness of link data. In this regard, the ACM paper network can be regarded as an example of  X  X  X igger world X  X  which could provide extra useful back-ground knowledge. A chain of,  X  X  X  friend of a friend X  X  statements can be made in an auxiliary network, to connect any two documents, which can bring two indirectly related documents closer together. The mined common citations can enrich the original link set and act as a bridge, which can be used to further fill the gap across domains and help the transfer. relationship among the documents in V 0 , we can construct the graph G 0 =( V 0 , E 0 ) where the vertex set V 0 represents docu-ments and the edge set E 0 represents the hyperlinks (or citations) between documents. Note that the training and test data-link frequency by using a fragment of the adjacency matrix A 0 . As mentioned above, the data may be very sparse. knowledge transfer. Suppose graph G =( V , E ) is an auxiliary network where the vertex set V represents documents in the auxiliary network and the edge set E represents the hyperlinks (or citations) between documents, and G 0 is a sub-graph of G . The advantage of incorporating the auxiliary network is that it can introduce more nodes and edges which are not in-cluded in the graph G 0 and provide more background knowledge among the documents. Let A denote the adjacency matrix of G . Then we define a base similarity matrix as follows: where A T is the transpose of A . Note that B is symmetric. We then use an exponential diffusion graph kernel ( John &amp; Nello, 2004 ) to mine the co-citation relationship as follows: where k refers to the setting of the length of the reachable path between two nodes. Note that B reflects the direct citing and cited relationship between documents and B k reflects the k -length-path indirect citing and cited relationship. Here we treat trol the contribution of longer paths.

The computation of Eq. (2) is intractable due to the infinite progression. In order to simplify the computation of S , we can rewrite it as another form. Since matrix B is symmetric, there exists an orthogonal matrix U =[ n 1 , ... , n l ], where n i  X  1 the eigenvector of B , to diagonalize B as follows: where D  X  diag  X  k 1 ; ... k l  X  is a diagonal matrix and k i  X  1 6 i 6 l  X  is the eigenvalue of B . Then we can obtain As a result, the calculation of Eq. (4) becomes much simplified since D is a diagonal matrix.

With the background knowledge introduced by the auxiliary network, the estimation of the link  X  X  X requency X  X  m ( c , d ) will become more accurate, which can be formulated as the following: Intuitively, several reasons may account for why the co-citation mined from the auxiliary network would help to knowledge transfer between domains. Firstly, the indirectly related documents become correlated when the indirect co-citation rela-tionships are taken into consideration by the graph kernel, and these common co-citation relationships can be enhanced and mined with the help of an auxiliary link network. It can be expected that the sparseness of link data can be significantly alleviated using the common co-citation relationships. Secondly, auxiliary network can introduce a number of new  X  X  X om-mon friends X  X  shared by the documents which can be viewed as extra features. Then these extra features can be appended into the original feature space. Therefore, the gap between domains would be narrowed by mapping the documents from the original feature space to a higher dimensional feature space.

Fig. 1 shows an illustrated example. All the nodes in the graph refer to documents and the edges refer to the citation rela-tionship between nodes. In order to clarify the different roles they play in the model, we use rectangle node to denote doc-ument and round node to link feature which is also a document. Here we have three document sets, i.e., D ={ v 1 , v 2 } where v and v 2 are from the source and target domain, respectively, V 0 ={ v 1 , v 2 , v 3 , v 4 , v 5 } and V ={ v 1 , v 2 , v 3 , v 4 ellipse to represent the document set V . Note that D # V 0 # V . Let us firstly consider the small world. Given the graph, we can represent the documents as bag-of-links where the feature values are link frequencies, which are a fragment of adjacent matrix A 0 , as shown in Fig. 2 a. The two documents may not share any features and the document-link matrix is very sparse. In other word, there exists a gap between the source and target domains. However, the big world may provide more com-plete background knowledge that helps to bridge the domain gaps. Here the background knowledge is the direct or indirect and v 2 are cited by v 6 . Hence, v 6 and v 7 can be viewed as the bridge that brings v 1 or v 2 closer. This indicates that v 1 As a toy example, we compute S using Equation (2) by setting the maximum length of path to be 2. Fig. 2 b shows an enriched 3.3. The LBT model commonality as well as domain discrepancy. In this regard, LBT is a feature-transfer approach. Our model is based on PLSA ( Hofmann, 1999 ). Intuitively, we can apply PLSA on the source and target data separately. However, since the source and target data are from related domains, they would share some similar topics. Therefore, it is advantageous to merge the two separate models into a joint probabilistic model which allows capturing the domain commonality, i.e., the shared topics by both domains. Meanwhile, since the domains are differential even though they are related, it is reasonable to associate the documents from both domains with the differential probabilities to the shared topics rather than with the equivalent prob-abilities. The rationale is that documents from both domains share some similar topics, while their associated probability distributions with the shared topics are to some extent discrepant. Such a merge-and-differentiate strategy enables the mod-el to capture the domain commonality as well as domain difference.
 words occurring in this document, and the second is from the other documents with direct or indirect citation relationship with this document. The complementary interaction between the two views would help to discover more precise shared top-ics. Likewise, rather than applying two PLSA model on text and link data separately, it is beneficial to integrate them into a joint model.
 process based on PLSA as follows, where the analogous notations from Hofmann (1999) are used: regard to different topics: The graphical representation of our Link-Bridge Topic (LBT) model is shown in Fig. 3 . Note that in Eqs. (6) and (7) both and (9) both decompositions of p ( w | d t ) and p ( c | d t ) share the same document-specific mixing component p ( z | d t ). Specifically, the advantages of the proposed joint model are threefold:
In our joint model, the expanded feature space of both domains are mapped into the latent topic space Z , which could correlate the seemingly unrelated source-and target-specific features by the topics if they have similar conditional prob-ability p ( w | z ) (or p ( c | z )). Such correlation will then help bridge domain gap via the shared topics.
On one hand, integrating the two separate PLSA models on source and target data into the joint model allows to capture the domain commonality, i.e., the shared topics by both domains. On the other hand, associating a topic z e Z with differ-ciated distributions with the topics are differential.

Likewise, integrating the two separate PLSA models on text and link data into the joint model allows us to capture the view consistency, as well as their difference.

In summary, the power of our unified model is that it integrates all these kinds of correlations in a principled manner. The shared topics correlate the domain-specific features and encode the domain commonality and difference, as well as view consistency and difference. Since the mixing topics are shared, the learned decompositions must be consistent with the source and target data, as well as content and link statistics. As such, the shared topics act as the bridge to facilitate knowl-edge transfer from the source to the target domain.

Note that the description of each document can be generalized into multiple distinct views other than simply two. There-fore, our proposed model is further extensible by integrating multiple views of documents into this unified framework. Gen-erally, we propose a novel method of combining multiple views of data in a latent topic model in order to transfer labels from one domain with labeled data to a different but related domain with unlabeled data via the shared latent topics. In this pa-per, we particularly focus on the case of documents with associated link network, but the proposed model is generally appli-cable to data with multiple views other than documents and links. In this regard, LBT can be viewed as a way of combining transfer learning and multi-view learning, which to the best of our knowledge has not received much attention in the literature. 3.3.1. How to transfer Based on Fig. 3 , we derive the log-likelihood objective function as follows: regardless of the number of observations associated with it, k  X  0 6 k 6 1  X  acts as a tradeoff of weight between the training and test data, and larger k indicates more reliance on the source training data set, b (0 6 b 6 1) is the tradeoff between con-tent and link, and larger b indicates content information is weighted more. When b = 1, the objective function ignores all the biases from link structure, and in this case, LBT model is equivalent to TPLSA ( Xue et al., 2008 ); when b = 0, the objective function relies on link structure only, which is referred to as Link-bridged PLSA (LPLSA) in the rest of the paper. It is inter-esting to investigate how LPLSA using link structure only performs compared to TPLSA, which will be done in Section 4 .Our goal is to maximize the log-likelihood L of the LBT model in Equation (10) . Expectation X  X aximization (EM) algorithm is used to find a local optimal solution of L .
 E-Step:
Given the term t and documents d s and d t , calculate the posterior probability of each topic z based on the old estimate of p ( w | z ), p ( z | d s ) and p ( z | d t ): estimate of p ( c | z ), p ( z | d s ) and p ( z | d t ): the below conditional probability is a mixture component of posterior probability of latent topics. 3.3.2. Algorithm for LBT tialize the conditional probability p ( z | d s ) for each labeled document d s e D s as follows: tive function L in Equation (10) . When it is converged, a unique topic label is assigned to the target documents according to f  X  d t  X  X  arg max 4. Experiments parison with the state-of-the-art algorithms as baselines. Two types of datasets, i.e., scientific research papers dataset and web pages dataset, are used for the evaluation. 4.1. Datasets and setup
Cora ( McCallum, Nigam, Rennie, &amp; Seymore, 2000 ) is an online archive of computer science research papers which con-tains approximately 37,000 papers, and over 1 million links among roughly 200,000 distinct documents. The documents in the dataset are categorized into a hierarchical structure. We select a subset of Cora papers for our model training and test, which contained five top-categories and 10 corresponding sub-categories (the numbers are in the parenthesis): DA_1= X  X /data_structures__algorithms_and_theory/computational_complexity/ X  X  (711); DA_2= X  X /data_structures__algorithms_and_theory/computational_geometry/ X  X  (459); EC_1= X  X /encryption_and_compression/encryption/ X  X  (534); EC_2= X  X /encryption_and_compression/compression/ X  X  (530); NT_1= X  X /networking/protocols/ X  X  (743); NT_2= X  X /networking/routing/ X  X  (477); OS_1= X  X /operating_systems/realtime/ X  X  (595); OS_2= X  X /operating_systems/memory_management/ X  X  (1102); ML_1= X  X /machine_learning/probabilistic_methods/ X  X  (687); ML_2= X  X /machine_learning/genetic_algorithms/ X  X  (670).

Note that each top-category contains several sub-categories, while we only select two sub-categories from each top-cat-egory to generate our datasets. Based on this data, we used a way similar to Pan and Yang (2010) to construct our training and test sets. For each set, we chose two top categories, one as positive class and the other as the negative. Different sub-categories were regarded as different domains. The task is defined as top category classification. For example, the dataset denoted as DA-EC consists of source domain: DA_1(+), EC_1( ); and target domain: DA_2(+), EC_2( ). The method ensures the domains of labeled and unlabeled data are related due to same top categories, but the domain distributions are different because they are drawn from different sub-categories. Such a preprocessing is a common practice for data preparation for adaptation purpose. The domain difference can be justified like some previous works ( Pan &amp; Yang, 2010 ) where it was found that SVM classifier trained on in-domain data performed much worse out of domain, which implies large domain gap.
The second dataset we use is the Industry Sectors dataset 2 which is a collection of about ten thousand Web pages belonging to companies from various economic sectors. The corporate Web pages are classified into a hierarchical structure. We chose a subset of Web pages from the five top sectors, i.e., energy, financial, healthcare, transportation and consumer and 10 corre-sponding sub-categories. Based on these five top-categories, we generated 10 datasets in a similar way to what we had done for the Cora datasets to ensure the domain relatedness as well as difference.

We preprocessed the data for both text and link information. For the texts, we removed stop words and low-frequency words with count less than 5. For the links, we removed the links with less than three citation counts. Then the standard TF-IDF ( Salton &amp; Buckley, 1988 ) technique was applied to both the text and link datasets. 4.2. Effectiveness of auxiliary network
Here we examine whether the embedding of the co-citation relationships mined from the auxiliary network into a graph kernel would lead to a better representation of the documents. For each original link dataset, the co-citation relationships among the documents from the two corresponding top-categories are used to construct the auxiliary link network. Then we employed such an auxiliary link network to generate the enriched link dataset (see Section 3.1 ). Finally, since the aux-iliary network would introduce noisy link features, here we employ a simple feature selection mechanism which removes the features whose document frequencies are less than 3 in the dataset. We found this mechanism, though simple, worked well on the Cora datasets.

Since SVM ( Joachims, 1999 ) has shown state-of-the-art performance compared to most of other supervised machine learning methods, we fed two kinds of link datasets to the SVM classifier for performance comparison: SVM-OL: SVM is applied on the original link dataset.
 SVM-L: SVM is applied on the enriched link dataset.

The classification error rate is used to evaluate the classification performance, which is defined as the number ratio be-tween the misclassified test instances and the total test instances.

Table 1 shows the error rate on the Cora and the Sectors datasets. The bold items in Table 1 indicate the best results achieved by the algorithms for each dataset. For the Cora datasets, SVM-L significantly outperformed SVM-OL on most data-sets. On average, the error rate of SVM-L is 23.0% lower than that of SVM-OL. It verifies that the auxiliary network can provide more complete background knowledge about the correlation among the documents which would help to reduce the domain gap. For the Sectors datasets, the performance superiority of SVM-L over SVM-OL is not significant. In comparison with the research papers, the Web pages would contain much more noisy links. In this case, the import of auxiliary network not only provides background knowledge, but also brings with more noisy data, such as advertisement links. Next we will explore a more effective feature selection mechanism to filter out the noisy data in the Web pages. 4.3. Algorithms comparison and analysis
SVM ( Joachims, 1999 ), the traditional multi-view algorithm Co-Training ( Blum &amp; Mitchell, 1998 ), the large-margin-based multi-view transfer learner MVTL-LM ( Zhang et al., 2011 ) and the content-based transfer learning algorithm TPLSA ( Xue et al., 2008 ). We also compared our LPLSA (see Section 3.2 ) with TPLSA. Note that LPLSA is a special case of our proposed model.
 set, respectively. All the link-based datasets used in the following experiments refer to the enriched link datasets. Both the text and link datasets were fed to the multi-view classifiers Co-Training, MVTL-LM and LBT. TPLSA relies on text information only, thus we applied TPLSA on content-based datasets. LPLSA is fed with the link-based datasets. The comparison results are shown in Tables 2 and 3, where the bold items indicate the best results achieved by the algorithms for each dataset. link features. Simply merging the two sets of features make some improvements, implying that text and link can be com-plementary, but it may degrade the confidence of classifier on some instances whose features become conflict because of merge. Co-Training can avoid this problem by boosting the confidence of classifiers built on the distinct views in a comple-mentary way, thus performing better than TSVMs. Since both TSVM and Co-Training don X  X  consider the distribution gap, they performed clearly worse than the transfer learning based approaches including MVTL-LM, TPLSA, LPLSA, and LBT. compared to TPLSA. This demonstrates that link structure reflecting the inter-dependence relationship among the docu-ments is of great value for cross-domain document classification. However, we find that TPLSA significantly outperforms
LPLSA on the dataset EC-ML. One possible reason for this is that the papers under the three sub-categories, i.e., EC_1 (/ encryption_and_compression/encryption/), EC_2 (encryption_and_compression/compression/) and ML_1 (machine_learn-ing/probabilistic_methods/), may share co-citation relationship with those papers of some common topics, such as informa-tion theory, rendering it difficult for LPLSA to distinguish ML_1 from EC_1 and EC_2.

LBT over TPLSA and LPLSA. Firstly, LBT employs the auxiliary link network to discover more shared co-citation between do-mains which helps alleviate the data sparseness and leads to a better representation of documents. Link structure provides importance information about the relationship among documents. Secondly, LBT exploits both the content information and link structure that are incorporated into a unified link-bridged topic model. The shared topics play a key role in the knowl-edge transfer from source domain to target domain. Moreover, the LBT classifier can be regarded as a tradeoff between the content-based classifier and the link-based classifier. The complementary cooperation between two types of classifiers would help discover more precise shared topics and thus commonly yield better prediction performance.

It is shown that multi-view adaptation using MVTL-LM performed worse than LBT on most datasets. Generally, it suggests that instance-based approach relying on instance weighting is not effective when the data of different domains are drawn from different feature spaces. Although MVTL-LM regulates view consistency on both domains X  instances, it cannot identify the useful association between target-specific and source-specific features, which is the key to the success of adaptation especially when domain gap is large and less commonality could be found. In contrast, LBT uses a unified probabilistic model to find such correlations. Such common knowledge smoothly bridges the gap between different domains and enhances the generalization ability of the cross-domain classifiers.

Table 3 shows the error rate on the Sectors datasets. We can see that TSVM-L and LPLSA performed significantly worse than TSVM-C and TPLSA, respectively. This is because the link data contain much noise which seriously deteriorates the pre-diction performance. On average, TSVM-CL performed a little better than TSVM-C. Likewise, LBT outperform TPLSA on most datasets. It indicated that though the link data is noisy and sparse, the two views are still complementary, which helps im-prove the prediction performance.
 On most datasets, LBT significantly outperformed Co-Training and MVTL-LM. However, the advantage of LBT over TSVM-CL on the Sectors datasets is not comparable as it is on the Cora datasets. It suggested that our proposed approach is more effective on the research paper datasets than on the Web page datasets. Intuitively, unlike the scientific papers, the Web pages contain much noisy data, which likely lead to topic drift. Since our proposed method is based on the probabilistic topic model, it would suffer from the topic drift problem. 4.4. Parameter sensitivity
Here we aim to study how the parameters, such as k and b , affect the performance of the proposed algorithm. The results are shown in Figs. 4 and 5 . Fig. 4 shows the error rate curves for different b across different Cora data sets. The parameter b (0 6 b 6 1) acts as a tradeoff of weight between content and link. Note that larger b indicates more reliance on the content information of documents. In this figure, the X -axis shows the change of parameter b which varies from 0.0 to 1.0. The Y -axis then increase when b increases. The algorithm performs worse nearly on all of the data sets when heavily relying on either to 0.8 will achieve the best performances for LBT across most of the datasets. It suggests that the two views of document are complementary and beneficial to the classifier.

Fig. 5 shows the error rate curve for different k across different Cora data sets. The parameter k (0 6 k 6 1) acts as a trade-X -axis shows the change of parameter k which is varied from 0.1 to 1.0. It is shown that setting k at the interval from 0.4 to 0.7 will achieve higher performance for LBT across most of the datasets. But there are some exceptions. For example, the algorithm performs better on the EC-NT dataset when setting k at the interval from 0.7 to 0.9. In most cases, the algorithm performs worse when relying only on the source training set ( k  X  1). It suggests that the distribution between the training and test data from different domains are different, and the algorithm can perform better by taking such kind of distribution difference into consideration than by treating them indiscriminately.

As a result, we tuned the parameters b and k , by using cross-validation on the training dataset for all the experiments. 4.5. Convergence
We tested the convergence property of LBT as well. Fig. 6 shows the experimental results. The X-axis represents the num-ber of iterations. The Y -axis, D L , represents the change of likelihood L in Eq. (7) between two sequential iterations across different datasets. LBT uses EM algorithm to find a local optimal point. The EM algorithm performs the E-step and M-step iteratively, and the convergence is guaranteed. As shown in Fig. 6 , the change of likelihood L decreases very fast during the first 15 iterations and becomes stable after 30 iterations. Thus, we terminated the algorithm after a maximum of 30 iterations.
 5. Conclusion
We introduce a novel method called Link-Bridged Topic (LBT) model for cross-domain document classification. Firstly, we employ the auxiliary link network to discover the shared co-citation relationship between documents in different domains. Then we combine the content information and link structures among documents using a unified probabilistic model to mine the hidden common topics. Based on the sharing structure, the LBT model achieves effective knowledge transformation be-tween different domains. The experimental results demonstrate that compared to the state-of-the-art baseline algorithms our algorithm significantly improves the prediction accuracy of cross-domain document classification.

Embedding the background knowledge mined from the auxiliary link network into a graph kernel can enhance the co-citation relationships among documents and thus help knowledge transfer between domains. However, auxiliary network would also introduce the noisy data. Next we will design a more refined feature selection mechanism to filter out noise data.
Also, transfer learning would hurt the performance when the domains or multi-views are too dissimilar. As part of ongo-ing work we are exploring the boundary between positive transfer and negative transfer and learning how to measure the extent of relatedness between domains and multi-views.
 References
