 The web has invaded our lives. In some sense, the web is a sensor of the real world. Specifically, it has been observed that events and objects are often rep-resented by sets of web pages but not individual web pages [4, 8]. Consequently, a large body of literature has focused on extracting real world events or objects from web data [4, 5, 8, 11, 12]. These approaches can be classified into two groups: structure-based extraction and content-based extraction. In the structure-based approaches, the website structures, hyperlink structures, and URLs are used to extract sets of web pages corresponding to events and objects [4, 8]. In the content-based extractio n, content of web pages are segmented and categorized into subgroups that correspond to different t opics, events, and stories using tech-niques such as natural language processing and probability models [1, 11, 12]. At the same time, such extraction results h ave been proved useful in many applica-tions such as organizing the website structure [8], restructuring the web search results [4], terrorism event detection [9], and Photo Story and Chronicle [5].
Data associated with a set of web pages in a web site can be classified into two types: author-centric and visitor-centric . Author-centric data refers to a set of hyperlinked web pages that descr ibes certain object or event, while visitor-centric data refers to web access sequences of these pages and describes how the web pages are accessed in the history. Observe that author-centric data describes authors X  point of view while visitor-centric data reflects the web visitors X  point of view.

We observed that existing event and object extraction approaches only ana-lyzed the author-centric data. These techniques ignore visitor-centric data .How-ever, often it may not be possible to distinguish different events related to the same topic by using the author-centric data alone. This is because events be-longing to the same topic often share a set of keywords and the pages containing these different events are often connect ed by hyperlinks. For example, web pages talking about different car accidents tend to share keywords like car , accidents , and crash . Also, these pages may be connected as they belong to the same topic (car accident). Hence, it is difficult to distinguish one car accident from another based on only keywords and hyperlink structure.

In this paper, we consider visitor-centric data along with author-centric data to detect real-world events. In other words, we integrate visitor-centric and author-centric data to distinguish different events under the same topic. The major differ-ences between our event detection a pproach and the related research [1, 11, 12, 5, 4, 2] are twofold. First, all the above works focus on either the author-centric or the visitor-centric data, while our approach incorporates the visitor-centric data along with the author-centric data. Second, the temporal property of the visitor-centric data is utilized in our approach to improve the event detection accuracy.
 For example, suppose Figure 1(a) shows a subset of hyperlinked web pages; Figure 1(b) shows the implicit links extracted from the corresponding usage data; and Figure 1(c) shows the evolution pattern of web usage data (the y -axis shows the frequency of a web page being accessed over the time intervals shown in the x -axis ). Here, there is an implicit link between two web pages if and only if they were accessed consecutive ly in the web access sequences [10]. The evolution pattern of web usage data refers to how the web pages changed in the history in terms of their supports [13].

It can be observed that from only Figure 1(a), it is difficult to distinguish sibling pages such as e and f even if they correspond to different events. How-ever, with the evolution of web usage da ta as shown in Figure 1(c), connected web pages with similar content but corresponding to different events can be dis-tinguished. For example, in Figure 1(c), pages e and g have similar evolution pattern while pages e and f have different evolution pattern. At the same, web pages that are not connected by hyperlinks but corresponding to the same event can be identified using implicit links in Figure 1(b), since they are expected to be accessed together. As shown in Figure 1(b), the implicit link between web pages b and g , which are not connected by hyperlink in Figure 1(a), implies that b and g have a possibility to represent the same event. In this paper, we focus on detecting events in a specific website as it is extremely difficult to gather web usage data of the entire web. The contributions of this paper are as follows.  X  To the best of our knowledge, this is the first approach that detects website  X  A multigraph is proposed to model website rela ted data in terms of structure,  X  We present two variants of i Wed algorithm, called fusion-based graph cut In this section, we first discuss how to re present web structure, web content, and web usage data of a web site using structure graph , content graph ,and usage graph , respectively. Then, we present how these three types of graphs are integrated using a multigraph , followed by the problem statement of website-based event detection. 2.1 Structure Graph The web structure data here refers to the set of web pages and hyperlinks between them. It can be modelled as a structure graph , G s = V s ,E s , where each vertex in
V s is a web page and each edge in E s represents the structure similarity (will be defined later) between the two pages t hat are connected by this edge. Note that the structure similarity is defined to reflect the similarity between web pages in terms of structure. The intuition is  X  X wo web pages are structurally similar if they are linked with similar web pages X  [3]. As the base case, we consider a web page maximally similar to itself, to which we can assign a structure similarity score of 1 . With this intuition, given two web pages i and j in V s ,the structure similarity is defined as: Here C is a constant between 0 and 1, | D ( i ) | is the degree of vertex i in the graph and D m ( i )isthe m th neighbor of vertex i . It is obvious that this similarity is an iterative function where similarities between web pages are propagated through recursions. That is, the value of S s ( i, j )inthe t th iteration, denoted by S s t ,is based on the values of the t-1 th iteration. More over it has been proved that this recursive function is nondecreasing and it will converge eventually [3]. We 2.2 Content Graph The web content data refers to the content of each web page. The web content data is modelled as a content graph , G c = V c ,E c , where each vertex in V c is a web page and each edge in E c represents the semantic similarity between two pages. It has been experimentally proven that cosine measure is one of the best measures for web content clustering [ 7]. Hence, we use the cosine measure to quantify semantic similarity between two pages. Given a web page i , using some stemming algorithm, it will be represented as a vector, to the TF.IDF of the terms after stemming [7]. Then, the semantic similarity between two web pages i and j , denoted as S c ( i, j ), is defined as: where ( of vector 2.3 Usage Graph The usage data refers to the access log of the web pages. It also can be modelled as a graph, called usage graph , G u = V u ,E u , where each vertex in V u is a web page and each edge in E u represents the usage pattern-based similarity between two pages. Firstly, we review some of the literature in web usage mining.
In general, web usage data records the interactions between web users and the web server. A web access sequence ( WAS ) is an ordered list of pages accessed t is the time when p i was accessed and t i  X  t i +1  X  i =1 , 2 , 3 ,...,n  X  1. Similar to [13], the WAS s can be represented as a sequences of WAS group based on the user-defined time interval. A WAS group (denoted by G )isabagof WAS s that occurred during a specific time period. Let t s and t e be the start and end was visited between t e and t s . As a result, the historical web log data is divided into a sequence of WAS groups. Let H G = G 1 , G 2 , G 3 , ... , G k be a sequence of k WAS groups generated from the historical web log data. Given a web page of i in H G .Notethat,for1  X  t  X  k ,  X  t ( i )= N | G WAS sthatcontain i .

Given two web pages, i and j , with the corresponding web usage data, the usage pattern-based similarity , denoted by S u ( i, j ), is defined as: where D = k t =1 |  X  t ( i )  X   X  t ( j ) | 2 . Note that, the usage pattern-based simi-larity is a linear combination of the evolution pattern-based similarity and the implicit link-based similarity . The evolution pattern-based similarity is denoted by e  X  D ,where D is the Euclidian distance between the support sequences H(i) and H(j) . The implicit link-based similarity is represented as the percentage of WAS sthatcontain i and j consecutively against the total number of WAS s that contain at least one of i and j . Here,  X  and 1  X   X  are the weights of evo-lution pattern-based similarity and implicit link-based similarity, respectively. It is obvious that both the evolution pattern-based similarity and implicit link-based similarity are within the range of 0 to 1. Similarly, the usage pattern-based similarity is between 0 and 1. 2.4 Multigraph We merge the above three graphs into a multigraph , which includes web struc-ture, web content, and web usage data in a website. A multigraph is a graph whose edges are unordered pairs of vertices, and the s ame pair of vertices can be connected by multiple edges. In this ca se, there are three edges for each pair of vertices. These three edges represen t the edges of structure graph, content graph, and usage graph, respectively.
 Definition 1 [Multigraph] . A multigraph is represented as a 3-tuple M =
V, E, f , where V is a set vertices , E a set of edges, and f is a function f ( e i ) = {{ u,v }| u, v  X  V ;u = v } that takes an edge e i  X  E and returns the set of web pages u and v that are connected by e i . Two edges e i and e j are called parallel or multiple edges if f ( e i )=f( e j ).
 An example of the multigraph representation of website data is shown in Figure 2 with the corresponding structure graph, content graph, and usage graph. Note that, the similarities betw een disconnected web pages are 0 and the weights of the edges represent the corresponding similarity values.
 Website-based Event Detection Problem: Based on the multigraph rep-resentation of the website related data , each real world event corresponds to a strongly connected subgraph in the multigraph. That is, a real world event can be represented as a set of structurally an d semantically strongly connected web pages with similar usage patterns in the multigraph. The website based event de-tection problem is to extract such subgraphs from the multigraph representation. In this section, we present the i Wed event detection algor ithms based on the multigraph representation of the website data. To extract the strongly connected subgraphs from a graph, different graph cut algorithms have been proposed. In this paper, we adopt the normalized graph cut algorithm, which is widely used in object extraction from image data and frame segmentation of video data [6].
The three similarity measures, S s , S c ,and S u , introduced in Section 2 can be classified into two categories: topic similarity and evolution similarity .Topic similarity is the combination of the structure similarity ( S s ) and the semantic similarity ( S c ), while evolution similarity is the usage pattern-based similarity (
S u ). Based on those two categories, we propose two variants of i Wed algorithm for cutting the multigraph. The first approach, called the fusion approach ,fuses the two types of similarity measures together and cuts the graph by treating the multiedges between two vertices as a sin gle edge. The second approach, called the level-wise approach , cuts the graph with the two similarity measures sepa-rately. We now elaborate on these two approaches.
 Fusion Approach: The fusion approach, denoted by FUS , integrates the three similarity measures together using linear combination with different weights. Such kind of fusion has been extensively used in combining different types of similarity measures in web content analysis [3]. In the fusion approach, a new similarity S is proposed as: S =  X S s +  X S c +  X S u ,where  X  ,  X  ,  X  are the weights for the corresponding similarity measures, and  X  +  X  +  X  =1. Then, the multigraph is transformed to a normal graph, where the weight of each edge is represented by S . The graph is then cut using the normalized graph cut algorithm. Level-wise Approach: In the level-wise approach, the topic similarity and the evolution similarity are used to cut the multigraph separately. Note that, the topic similarity, denoted as S T , defined as the fusion of structure similarity and semantic similarity. There are two alternative level-wise approaches. In the first approach, denoted by LT F (Level-wise Topic First), the multigraph is cut based on the topic similarity, which corresponds to only two types of edges in the multi-graph, and the result, C T , is returned. Then, each subgraph in C T is cut again based on the evolution similarity and the final result, C F , is returned. In the second approach, denoted by LEF (Level-wise Evolution First), the multigraph is first cut based on the evolution similarity and the result, C E , is returned. Then, each subgraph in C E is cut again using the topic similarity and the result C
F is returned. The underlying intuition is that, in the first approach, web pages are clustered into semantic topics befo re they are clustered into events as each event is expected to be a set of semantica lly similar web pages that have similar usage patterns. In the second approach, firstly web pages that correspond to similar types of events are gathered together and then clustered based on their semantic relationships.

For both the fusion approach and the level-wise approach, we present the clustering results with a hierarchical structure. That is, at the first recursion of the 2-way graph cut algorithm, there are two partitions. After that each partition is further cut into two child partitions and so on. However, not all the subgraphs correspond to real world even ts. To identify real world events and exclude outliers, we propose an intra-cluster similarity measure , S intra ( G ), for any subgraph G : where i = j and i, j  X  G . Based on this similarity measure, a threshold  X  in the range of [0, 1], is proposed to distinguish the event-based subgraph and the non-event-based subgraph. A subgraph, G in the cut results corresponds to a real world event if and only if S intra ( G )  X   X  . In this section, the experim ental results are presented to show the performance of our proposed event detection approaches. The three approaches, FUS , LT F ,and LEF , are implemented and compared to the baseline approach, B L ,whichonly takes the structure and content of web pages using the corresponding similarity measures proposed in Section 2.

In our experiments, a synthetic e-commerce website dataset is used. Even though there are some real web usage datasets available, but due to privacy issues the original URLs and web pages are not available and they cannot be used in our experiments. The synthetic dataset we generated consists of 300 products and 2000 unique web pages. The 300 products belong to 5 cat-egories, where the content of the web pages are generated according the at-tributes of products in different categories (we use the schema extracted from http://www.bargaincity.com.sg, which is one of the biggest e-commerce web-sites in Singapore). The usage data are generated in three steps. Firstly, the web access sequences are generated usin g uniform random generation. Then, we synthesize a list of 100 events (20 burst events such as one day only promotion and release of new products, 40 periodic events such as weekend promotion and new semester promotion, 20 increasing events such as price of a popular product keeps decreasing, 20 decrea sing events such as some products are fading out of the market). Lastly, some noise access seq uences are randoml y inserted into the web usage data to mimic the real life usage data. In total, there are 10,000,000 unique page requests in the synthetic web usage data, which are partitioned into 100 access groups. 4.1 Evaluation Measures As the event detection results are set of ev ents, which consist of sets of web pages, it is different from existing classification algorithms. Although, we have the set of labelled events with corresponding web pages, the precision and recall mea-sures in our event detection approach are different from the ones commonly used in classification tasks for the following reasons. Since an event consists of many web pages, the event may be detected but the corresponding web pages may not be ac-curate. That is, some pages may be mi ssed and/or some non-related pages may be included. For example, given a real world event E = { P 1 ,P 2 ,P 3 ,P 4 ,P 5 } ,there may be one corresponding event E = { P 1 ,P 3 ,P 4 ,P 7 ,P 8 } in the detection results. Moreover, for one real world event, th ere may be more than two corresponding P } in the detection results. We propose ex tended precision/recall measure for event detection based on the commonl y-used precision/recall from IR.

Let E = { E 1 ,E 2 ,  X  X  X  ,E n } be the set of detected events based on our proposed approach and E = { E 1 ,E 2 ,  X  X  X  ,E m } be the set of labelled events in the dataset, where each event E i consists of a set of web pages { P i 1 ,P i 2 ,  X  X  X  ,P ik } .Foreach E , the corresponding real event E j with the largest value of | E i  X  E j | is selected, | E i | is the number of pages included in that event while | E i  X  E j | is the number of common pages included in both E i and E j . Also, for each real world event E j , the corresponding event E i with the largest value of | E i  X  E j | is selected from the results. Moreover, for different events in the real world, their corresponding events in the results should be different and vise versa. Then, the precision and recall are defined as: 4.2 Experimental Results Two sets of experiments have been conducted to evaluate our proposed event detection approaches. Firstly, comparison of our proposed event detection ap-proaches with the baseline approach is presented. Secondly, we show the effects of intra-similarity threshold  X  on the quality of the detected events. Within each set of results, both the overall performance and the performance for each type of events are presented. Lastly, we discuss about how to set the fusion param-eters in the FUS approach. Note that, the  X  value in the usage pattern-based similarity is set to 0.5 for the following experiments.

Table 1(a) shows the performance of the four approaches with the precision, recall, and F 1 measure 1 . It can be observed that the LEF , FUS ,and LT F ap-proaches outperform the baseline approach, B L , which shows the improvement of integrating the usage data and their evolution patterns. Among our proposed approaches, the LEF and FUS archive better performances than the LT F ap-proach. This is because some of the synthetic events in our dataset usually cover more than one semantic topic. Tables 1(b), (c), and (d) show the performance of our approaches with respect to different types of events.

In the above experiments, weights of the three similarity measures are set to 0.31 , 0.20 ,and 0.49 , which are experimentally proved to be the optimal values for our dataset. The threshold for intra-cluster similarity is set to 0.6 .Tables1(e) and (f) show the quality of the event detection results of the FUS and LEF approaches by varying the corresponding  X  values. The results are for all types of events. Observe that the effects of threshold  X  are similar for the three types of events. When the value of  X  increases from 0.3 to 0.7 , the quality of the event detection results becomes better; when the value of  X  increases from 0.7 to 0.9 , the quality of the event detection resul ts becomes worse. This is because when the threshold for intra-cluster similarity is too small/large, the number of events detected may be too many/few. While the number of real world event is fixed, the performance of the approaches decreases when the threshold is close to the two extremes.

From the results shown in Table 1, it is evident that the FUS approach performs relatively better than other approaches in most cases. This is because, in the FUS approach, the weights of different types of similarities can be tuned. In our experiments, we show the average results of the FUS approach. It can be observed that the usage pattern-based similarity significantly improves the clustering results. Moreover, we observed that the structure similarity is less important than the usage pattern-based similarity but more important than the content similarity. This work is motivated by the fact that existing event and object detection approaches only analyze the content and structure data of a website. In this pa-per, we integrate the author-centric and visitor-centric data to detect real-world events. Experimental results show that our proposed approaches can produce promising results.

