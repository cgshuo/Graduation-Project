 Graph structures constitute a prevalent representation form for modeling con-nections between different entities. In particular, analysis of bi-partite graphs is the focus on a wide spectrum of studies that span from social-network to busi-ness analytics and decision making. In business intelligence, bi-partite graphs may capture the connection between se ts of customers and sets of products. Analysis of such data holds great importance for companies that collect large amounts of customer interaction data in their data warehouses.

One common analytic process for business intelligence is the identification of groups of customers that buy (or do not buy) a subset of products. The avail-ability of such information is advantageous to both sales and marketing teams, as follows: sales people can use these insights for offering more accurate person-alized product suggestions to customers by examining what  X  X imilar X  customers buy. In a similar manner, identification of buying/not-buying preferences can assist marketing people to determine gr oups of customers interested in a set of package products. This can help organize more focused marketing campaigns, hence leading to a more appropriate allocation of the company X  X  marketing funds.
The described problem can be mapped t o a co-clustering instance [1,4,11]. A similar task is  X  X atrix reordering X  which discovers a permutation of matrix rows and columns such that the resulting matrix is as  X  X ompact X  as possible. We view co-clustering as a matrix reordering with a subsequent clustering step for dense area identification. For this work we provide examples from a particular setting, where the rows represent customers and the columns identify the products that a customer has bought; but our approach is applicable in different settings, too. An example of a matrix reorganization is shown in Figure 1 a) and b). Existence of a  X  X ne X  (black dot) signifies that a customer has bought a product, otherwise the value is  X  X ero X  (white dot). It is evident that the reordered matrix view provides strong evidence on the exist ence of patterns in the data.

Existing co-clustering methods can fac e several practical issues which limit both their efficiency and the interpretability of the results. For example, the majority of co-clustering algorithms explicitly require as input the number of clusters in the data. In most business scenarios, such an assumption is unrealis-tic. We cannot assume prior knowledge on the data, but we require a technique that allows data exploration. There exist some methodologies that attempt to perform automatic co-clustering [3], i.e., determine the number of co-clusters. They address the problem by evaluating different number of configurations and retaining the solution that provides the best value of the given objective func-tion (e.g., entropy minimization). Such a trial-based approach can significantly affect the performance of the algorithm. Therefore, these techniques are bet-ter suited for off-line data processing, rather than for interactive data analysis, which constitutes a key requ irement for our setting.

Another shortcoming of many spectral-based approaches is that they typically assume a perfect block-diagonal form for the reordered matrix;  X  X ff-diagonal X  clusters are usually not detected. Thi s is something that we accommodate in our solution, where existence of possible  X  X ff-diagonal X  dense clusters is resolved through a Gaussian-based density estimator algorithm. Because we are interested in finding rectangular clusters, the algorithm discovers the parameters of those rectangles (center, width, height) that best cover the highest density of  X  X ff-diagonal X  areas. Note, that using such an approach we can also support discovery of overlapping co-clust ers with no extra effort.

We use the discovered co-clusters for providing product recommendations to customers. The customers in our setting are not individuals but large companies, for whom we have extensive information such as company turnover, number of employees, etc. We use this information to prioritize recommendations. Recall that a co-cluster corresponds to a set of customers with similar buying patterns. To this end, existence of  X  X hite spots X  wit hin a discovered co-cluster represents potential product recommendations. However, not all white areas within a cluster are equally important. Th ese recommendations need to be ranked .Werankthe quality and importance of each recommendation based on:  X  The quality of the discovered co-clust er. For example, a single white spot in  X  Firmographic and financial characteristics of the customer; recommendations With the combination of the above two characteristics, the recommendations exploit both global patterns as discovered by the co-clustering, and personalized metrics.

In summary, our main contribution is providing a robust, unsupervised and fast solution for co-clustering which can be used for interactive business intel-ligence scenarios. We also demonstrate how to use our solution for providing product recommendations. We perform a comprehensive empirical study using real and synthetic data-sets to validate our solutions. To the best of our knowl-edge, this is one of the few works that evaluate the performance of co-clustering algorithms on real-world, business intelligence data. The principle of co-clustering was introduced first by Hartigan with the goal of  X  X lustering cases and variables simultaneously X  [11]. Initial applications were for the analysis of voting data. Hartigan X  X  method is heuristic in nature and may fail to find existing dense co-clusters under certain cases. In [4] the authors present an iterative algorithm that convergences to a local minimum of the same objec-tive function as in [11]. [1] describes an algorithm which provides constant factor approximations to the optimum co-clustering solution using the same objective function. A spectral co-clustering meth od based on the Fiedler vector appeared in [6]. Our approach uses a similar analytical toolbox as [6], but in addition is automatic (number of clusters need not be given), and does not assume perfect block-diagonal form for the matrix. A di fferent approach, views the input matrix as an empirical joint probability distribution of two discrete random variables and poses the co-clustering problem as an optimization problem from an infor-mation theoretic perspective [7]. So, th e optimal co-clustering maximizes the mutual information between the cluster ed random variables. A method employ-ing a similar metric as the one of [7] appeared in [20]; the latter approach also returns the co-clusters in a hierarchical format. Finally, [16] provides a parallel implementation of the method of [20] using the map-reduce framework. More detailed reviews on the topic can be found in [14] and [21].

Our approach is equally rigorous with the above approaches; more impor-tantly, it lifts important shortcomings of the spectral-based approaches and in addition focuses on the recommendation as pect of co-clustering, something that previous efforts do not consider. 3.1 Preliminaries We denote by I , 0 , 1 the identity matrix, all-zero, and all-one vector, respec-tively, and the dimensions will beco me clear from the context. We define [ m ]:= { 1 , 2 ,...,m } .Let C be an m  X  n matrix. For a subset of its rows R and a sub-set of its columns T we denote by C R,T the sub-matrix formed by rows R and columns T .
 Given an undirected graph G =( V,E )on n vertices with adjacency matrix A , its Laplacian matrix is defined L := D  X  A ,where D is a diagonal matrix of size n with D ii = j A ij . Moreover, the normalized Laplacian matrix of G is that corresponds to the second smallest eigenvalue of 0 L is known as the Fiedler the sum of weights of the edges between the sets S and  X  S . 3.2 Graph Partitioning Partitioning a graph into two balanced vertex sets (i.e., two sets such that nei-ther is much larger than the other one) while minimizing the number of edges between them is a fundamental combinatorial problem with various applica-tions [19]. Choosing a particular balancing condition gives rise to different related measures of the quality of the cut including conductance, expansion, normalized and sparsest cut. The two most commonly used balancing objective functions are the ratio cut [10] and the normalized cut [17]. In ratio cut, the size of a subset S  X  V of a graph is measured by its number of vertices | S | ,wherein normalized cut the size is measured by th e total weights of its edges, denoted by vol( S ). Here we will use the normalized cut objective function Note that the above objective function typically takes a small value if the bi-partition ( S,  X  S ) is not balanced, hence it favors balanced partitions. The goal of graph partitioning is to solve the optimization problem This problem is NP-hard. Many approximation algorithms for this problem have been developed over the last years [2,12], however they turn out not to be per-forming well in practice. In our approac h, we employ a heuristic that is based on spectral techniques and works well in practice [13,9]. Following the notation of [13], for any S  X  V , define the vector q  X  R n as where  X  1 =vol( S )and  X  2 =vol(  X  S ). The objective function in (1) can be written (see [6] for details) as follows where the extra constraint q D 1 = 0 excludes the trivial solution where S = V or S =  X  . Even though the above problem is also NP-hard, we adopt a spectral 2-clustering heuristic: first, we drop the constraint that q is as in Eqn. (2); this relaxation is equivalent to finding the second largest eigenvalue and eigenvector of the generalized eigensystem Lz =  X  Dz . Then, we round the resulting eigen-vector z (a.k.a. Fiedler vector ) to obtain a bipartition of G . The rounding is performed by applying 2-means clustering separately on the coordinates of z , which can be solved exactly and efficiently. The proposed algorithm consists of two steps: in the first step, we compute a per-mutation of the row set and column set using a recursive spectral bi-partitioning algorithm; in the second step we use the permuted input matrix to identify any remaining clusters by means of a Gaussian-based density estimator. 4.1 Recursive Spectral Bi-partitioning In this section, we recall a graph theoretic approach to the co-clustering prob-lem [6]. The input is an m  X  n matrix C . For illustration purposes, we assume that C is a binary matrix, i.e., its elements are in { 0 , 1 } , but our approach is applicable in general. Given C , we can uniquely define a bipartite graph G =( L  X  R, E ) , | L | = m, | R | = n as follows: Each element of the left set of vertices, L , corresponds to a row of C and each element of the right vertices R to a column of C . We connect an edge between i  X  L and j  X  R if and only if C ij = 1. Now given the bipartite graph G corresponding to C , we find a balanced cut in G with few edges crossing the cut. Algorithm 1. Recursive Spectral Bipartition 4.2 Eigengap-Based Termination A basic fact in spectral graph theory is th at the number of connected components in an undirected graph equals to the multiplicity of the zero eigenvalue of its normalized Laplacian matrix. Cheeger X  X  inequality provides an  X  X pproximate X  version of the latter fact [5]. That is, a graph has a sparse (normalized) cut if and only if there are at least two eigenvalues that are close to zero. Let the conductance of a graph G =( V,E ) defined as follows ond smallest eigenvalue of the normalized Laplacian of G . The first inequality of the above equation implies that if  X  2 is large, then G does not have sufficiently small conductance. The latter implication supports our choice of the termination criterion of the recursion of Algorithm 1 (see Step 11 of the SplitCluster proce-dure). Roughly speaking, we want to stop the recursion when the matrix can not be reduced (after permutation of rows and columns) to an approximately block diagonal matrix, equivalently when the bipartite graph associated to the cur-rent matrix does not contain any sparse cut. Using Cheeger X  X  inequality, we can efficiently check if the bipartite graph has a sufficiently good cut or not. An illus-tration of the algorithm X  X  recursion is explored in Fig.2. Our approach has many similarities with Newman X  X  modularity partitioning but it is not identical [15]. 4.3 Discovering Off-Diagonal Clusters After termination of Algorithm 1, we expect to have produced a fairly good reordering of the rows and columns of the input matrix C and moreover to have discovered a set of co-clusters that hav e disjoint row and column sets. However, in almost all instances of practical i nterest, we cannot expect the set of co-clusters to have disjoint row and column sets; several  X  X ff-diagonal X  co-clusters may have appeared after the course of our reordering algorithm. In order not to discard this potentially useful information, we apply as a post-processing step a Gaussian-based density estimator on the matrix after removing the set of co-clusters already discovered by Algorithm 1. This process is depicted in Figure 3. In the first step, we remove all the clusters that have been already extracted by Algorithm 1. In the second step we apply a density estimator to discover any (possibly) remaining co-clusters. That i s, we convolute a Gaussian mask over all positions of the binary matrix to detect the most dense areas. Initially, we set a sufficiently large size on the Gaussian mask 1 . We then progressively reduce the size of the mask by a fixed constant. At each step, we record all sufficiently dense areas (those for which the convolution e xceeds some threshold) and remove the co-cluster from further consideration.
 Complexity: We briefly discuss the time complexity of Algorithm 1. For ease of presentation, assume that the input is a square matrix of size n and let T ( n )be thetimecomplexity.Moreover,wemayassume 2 that in every recursion step the partitioning is balanced. Since the input matrix is sparse, the following recursion holds T ( n )  X  2 T ( n/ 2) + O ( n log n ), where the extra additive factor is due to sorting (Lanczos method is used for co mputing the eigenvector). Solving the recursion we get that T ( n )= O ( n log 2 n ) which implies that our method is only more expensive than a single bi-partitioning by a poly-logarithmic factor. 4.4 Recommendations Algorithm 1 together with the density estimation step output a list of co-clusters. In the business scenarios that we consider co-clusters will represent strongly correlated customer and products subsets. We illustrate how this information can be used to drive meaningful product recommendations.

Many discovered co-clusters are exp ected to contain  X  X hite-spots X . These represent customers that exhibit similar buying pattern with a number of other customers, they still have not bought a product within the co-cluster. These are products that constitute good recommendations. Essentially, we exploit the existence of globally-observable patterns for making individual recommendations.
Not all  X  X hite-spots X  are equally important. We rank them by considering firmographic and financial characteristics of the customers. The intuition is that  X  X ealthy X  customers/companies that have bought many products in the past are better-suited candidates. They are at financial position to buy a product and they have already established a buying relationship. In our formula we consider three factors:  X  Turnover T is the revenue of the company as provided in its financial  X  Past Revenue R is the revenue amount that our company made in its  X  Industry Growth IG represents that predicted growth for the industry in Therefore the rank r of a given white-spot that captures a customer-product recommendation is given by: In our scenario, the weights w 1 , w 2 , w 3 are assumed to be equal but in general they can be tuned appropriately.

We have described how to rank the  X  X hite-spots X  within a particular co-cluster. In order to give a total ordering on the set of the recommendations, we should normalize these ranking value with the importance of each co-cluster. We define the importance of each co-cluster as the product of its area and density normalized by the sum of the importance o f all co-clusters. Hence, we normalize all recommendations by the importance of the corresponding co-cluster. 5.1 Comparison with other Techniques We compare the proposed approach with two other techniques. The first one (SPECTRAL) is described in [6] and is similar with the proposed approach. The main difference is that the approach of [6] performs k -partition of the in-put matrix using the eigenvectors that correspond to the smallest eigenvalues. Moreover, in order to compute the clustering it utilizes k -means clustering which makes the approach randomized, compared to our approach which is determinis-tic. The second one (DOUBLE -KMEANS) is described in [1]. First, it performs k -means clustering using as input vectors the columns of the input matrix and then permutes the columns by grouping together columns that belong in the same cluster. In the second step, it performs the same procedure on the rows of the input matrix using a possible different number of clusters, say l .This approach outputs k  X  l clusters. Typical values for k and l that we use, are be-tween 3 and 5. We run all the above algorithms on synthetic data which we produced by creating several block-diagonal and off-diagonal clusters. We intro-duced  X  X alt-and-pepper X  noise in the p roduced matrix, in an effort to examine the accuracy of the compared algorithms even in when diluting the strength of the original patterns. The results are summarized in Figure 4. We observe that our algorithm can detect with high efficien cy the original patterns, whereas the original spectral and k -Means algorithms present results of lower quality. 5.2 Compression-Based Evaluation It is common to judge the effectiveness of a particular algorithm based on the value of an objective function. However, it is not clear how to evaluate the effectiveness of various co-c lustering algorithms that are designed to optimize different objective functions. It is even harder to fairly compare the quality of two algorithms that output a different number of co-clusters. Therefore, we make a comparison using compressibility metrics: we measure how many bytes the re-ordered array will require when stored using Run-Length-Encoding (RLE) [18]. Recall that RLE replaces long blocks of repetitive values with just two numbers: the value and the length of the  X  X un X . For example, RLE encodes the sequence appropriately each algorithm packed together zeros and ones. Understandably, larger number of bytes for the reordered matrix under RLE compression, indi-cates worse performance in placing zeros and ones in adjacent positions.
The results using are depicted in Table 1. For this we extract matrices that rep-resent buying patterns within our company for various industries of customers, because different industries exhibit different patterns. We notice that the pro-posed recursive algorithm results in compressed matrix sizes significantly smaller than the competitive approaches, suggesting a more effective co-clustering pro-cess.
 5.3 Business Intelligence on Real Datasets For this example we use real-world data provided by our sales department re-lating to approximately 30,000 Swiss customers. The dataset contains all firmo-graphic information pertaining to the customers, such as: industry categorization (electronic, automotive, etc), expected industry growth, customer X  X  turnover for last, past revenue. We perform co-clust ering on the customer -product matrix.. We apply our algorithm on each industry separately, because sales people only have access to their industry of specialization. Figure 5 shows the outcome of the algorithm and the detected diagonal and off-diagonal clusters. The highest ranked recommendations are detected wi thin the blue cluster, and they suggest that  X  X hite-spot X  customers within this cluster can be approached with an offer for the product  X  X ystem-I X . These customers were ranked higher based on their financial characteristics.
 Focus of this work was to explicitly show how co-clustering techniques can be coupled with recommender systems for busin ess intelligence applications. Con-tributions of our approach include:  X  An unsupervised spectral-based technique for detection of large  X  X iagonal X  co- X  A Gaussian-based density estimator for identification of smaller  X  X ff-diagonal X   X  A comprehensive comparison of our approach with prevalent co-clustering  X  A direct application of our methodology in business recommender systems.
