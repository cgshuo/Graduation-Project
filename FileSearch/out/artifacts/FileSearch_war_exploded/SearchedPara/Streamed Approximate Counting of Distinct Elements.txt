 Counting the number of distinct elements in a large dataset is a common task in web applications and databases. This problem is difficult in limited memory settings where storing a large hash table table is intractable. This paper advances the state of the art in probabilistic methods for estimating the number of distinct elements in a streaming setting New streaming algorithms are given that provably beat the  X  X p-timal X  errors for Min-count and HyperLogLog while using thesamesketch.

This paper also contributes to the understanding and the-ory of probabilistic cardinality estimation introducing the concept of an area cutting process and the martingale es-timator. These ideas lead to theoretical analyses of both old and new sketches and estimators and show the new esti-mators are optimal for several streaming settings while also providing accurate error bounds that match those obtained via simulation. Furthermore, the area cutting process pro-vides a geometric intuition behind all methods for counting distinct elements which are not affected by duplicates. This intuition leads to a new sketch, Discrete Max-count, and the analysis of a class of sketches, self-similar area cutting de-compositions that have attractive properties and unbiased estimators for both streaming and non-streaming settings.
Together, these contributions lead to multi-faceted ad-vances in sketch construction, cardinality and error estima-tion, the theory, and intuition for the problem of approxi-mate counting of distinct elements for both the streaming and non-streaming cases.
 G.3 [ Probability and Statistics ]: Probabilistic algorithms, Stochastic processes; H.2.8 [ Database Applications ]: Data mining distinct elements, martingale, cardinality estimation, ran-domized algorithms
Approximating the number of distinct elements in a dataset while using a small amount of memory is an important prob-lem with broad industrial applications. A web company may wish to count the number of distinct users accessing a ser-vice broken down by country of origin [12]. Other areas of application include counting network flows to detect denial of service attacks [6] and database warehousing [1].
A number of algorithms exist for approximately estimat-ing these counts. Most tackle the problem by first construct-ingasummarizationofthedataor sketch and then estimat-ing the number of distinct elements, the cardinality, from the sketch. In practice, Flajolet and Martin X  X  HyperLogLog [8] and Chen et al X  X  Self-learning bitmap [4] (S-bitmap) con-stitute the state of the art in the field when there is no prior estimate of the cardinality. Several theoretical results ex-ist as well. Kane et al [11] give an algorithm with optimal space complexity but sub-optimal statistical efficiency while Giroire X  X  Min-count algorithm [9] has optimal statistical ef-ficiency but poor space requirements. Although these algo-rithms have been described as streaming algorithms, only the S-bitmap exploits the streaming property of data to achieve better accuracy.

This paper develops a general recipe for constructing stream-ing cardinality estimation algorithms by introducing the area cutting process and an estimator for the cardinality using martingale methods. The martingale methods extend the estimation methods used by the S-bitmap [4]. This recipe covers almost all sketching methods. Any sketch which is generated by hashing of each element in the data stream and is not affected by duplicates elements has a representation as an area cutting process. The associated martingale esti-mator is provably unbiased and provably optimal if queried immediately after a sketch update.

Streaming versions of HyperLogLog and Min-count are shown to have superior performance to their non-streaming counterparts. Regular HyperLogLog is shown to require approximately 1 . 5 times more bits than streaming Hyper-LogLog to achieve the same error, and Min-count requires twice the size of streaming Min-count. These streaming es-timators are able to beat optimal estimators which use only the final sketch is because the streaming estimators are able to exploit the values of intermediate sketches as well. Fur-thermore, unlike the S-bitmap which can only be used in a streaming setting, these sketching methods have the ad-vantage that they may also be used in distributed settings where results may be need to be aggregated over multiple servers or computed within a map-reduce framework.
A special class of decomposable self-similar area cutting processes is introduced and shown to have attractive prop-erties. They are shown to have an unbiased estimator for non-streaming data as well as analytic formulas for the vari-ance for both the streaming and non-streaming case. A new sketch construction, Discrete Max-count, yields a decompos-able self-similar area cutting process which has good perfor-mance for small sketches. For a non-optimal encoding of the Discrete Max-count sketch, the space-complexity is derived and shown to have no term which jointly depends on both the error and cardinality n .

The empirical performance of each method is evaluated as well using both simulations and an anonymized Facebook test dataset. Unlike HyperLogLog which requires special corrections for small and medium sized cardinalities [10], the performance of the estimators is consistently good through-out the entire range of cardinalities tested.

Several ideas of note are introduced in this paper. To in-troduce these ideas the paper is structured as follows. In section 2, we formulate the cardinality estimation problem by describing the probabilistic data generating process for the sketches. This leads to the martingale estimator and its properties, including its optimality properties. Next, several existing sketches are described and shown to yield streaming estimates. The area cutting process and a geometric inter-pretation of the sketches are give in section 5. This paper then focuses on self-similar area cutting processes and their properties. Discrete Max-count is introduced as a special case and its space complexity is analyzed, and the optimal and near optimal estimators for the non-streaming versions of Min-count and HyperLogLog are shown to be non-optimal in streaming settings. The streaming methods are evaluated via simulation in section 7 and future research directions are discussed.
To fix the notation in this paper, let X =( X 1 ,X 2 , ... )be a stream of elements and n denote the cardinality, the num-ber of distinct elements, of X . Cardinality estimation meth-ods involve constructing a sequence of sketches S i which are functions of the data seen up to some point, and then esti-mating the cardinality  X  N ( S i ) from the sketches. To ensure that only distinct elements are counted, the sketches should not be affected by duplicates. In other words, they should have the property S i = S i  X  1 if X i = X j for some j&lt;i . In most probabilistic methods, the sketches are based on a transformed stream of data which has been converted using a strong universal hash function h so that ( h ( X i 1 ) ,h ( X forms a sequence of independent Uniform (0 , 1) random vari-ables where the subsequence ( i j ) picks out the first occur-rence of each distinct element. The hash function ensures that duplicates remain duplicates after transformation. The sketches S i are then random elements from some distribu-tion parameterized by n i , the number of distinct elements encountered up to the i th element in the stream. Thus, the problem of estimating the cardinality is reduced to the statistical problem of estimating a parameter for a known family of distributions given the data S i .

Our insight for exploiting the streaming nature of the data is that using just the last sketch S last to estimate the car-dinality throws away information contained in the entire se-quence of sketches. We augment the sketch by including the current estimate into the sketch  X  S i =( S i ,  X  N i Algorithm 1 Algorithm for updating an existing estimate  X  n prev andsketchwithanewitem. function UpdateEstimate ( X  n prev , sketch , item new ) end function augmentation allows the construction of a provably unbi-ased martingale estimator with just an additional O p (lg n ) bits. Here, the notation O p (lg n ) is the probabilistic ana-log of O (lg n ) where the bound holds for arbitrarily small probabilities.
The key idea behind all streaming algorithms described in this paper is that a Markov chain is formed by the sequences of sketches generated from a data stream. This Markov chain is obtained by taking the original sequence of sketches S i indexed by the number of items i encountered and considering only the subsequence S t indexed by the number of distinct items t encountered.

Theorem 1. The sequence of sketches ( S t ) forms a Markov chain when h is a strong universal hash.

Proof. The next sketch S t +1 = f ( S t ,h ( X t )) is some function of the current sketch and the next hashed value. Since h is a strong universal hash, the h ( X t ) are indepen-dent and identically distributed, and the probability of tran-sitioning to some value depends only on the previous value of the sketch S t
For ease of exposition, this paper will refer to the index t interchangeably as both time and the number of distinct elements encountered.

Define the martingale estimator to be the Markov chain where q ( S t ) is the probability of modifying the current sketch and transitioning out of the current state. The initial state of the chain is  X  N 0 = 0. The idea behind this estimator is that with one time step the expected increase in the estimator is exactly 1. Note that although the underlying sequence ( S is not observed since the number of distinct items t is not known at an arbitrary location in the stream, the estimator depends only on observable events, namely on modifications to the sketch.

The resulting algorithm is simple and elegant and can be applied to any sketch which is unaffected by duplicates. Given a sketch, augment it with a counter c and add 1 /q ( S every time there is a change in the sketch. Note that the esti-mate is never updated when a duplicate item is encountered since duplicates do not modify the sketch. The algorithm is summarized in algorithm 1.

The name of the estimator comes from the fact that  X  N t  X  is a martingale. A martingale M t is a Markov process such that E ( M t +1 | M t )= M t . The martingale property aids in
Symbol Description q ( S t ) Probability that a new item modifies the sketch S ( X  t ,Y t ) offset and samples of an offset decomposition deriving properties of the estimator. The most important property is given by the following theorem.
 Theorem 2. The process M t =  X  N t  X  t is a martingale. Hence,  X  N n is an unbiased estimate of the cardinality n .
Proof. The martingale property is trivial to check. E M t EE ( M t | M t  X  1 )= E M t  X  1 . By induction, E M t = E M 0 In other words, E  X  N n = n ,and  X  N n is unbiased.
Another way to view the estimator comes from decompos-ing it in terms of holding times for the Markov chain. Let T ,T 2 ,... be the jump times of the chain, so that T i be the time of the i th modification to the sketch. The holding time T  X  T i  X  1 is the time the chain remains in state S T i  X  1 T
K be the sketch X  X  last modification time before time n . Since the holding time T i  X  T i  X  1 of a discrete Markov chain has a Geometric ( q ( S T i  X  1 )) distribution, the estimator is in-cremented by precisely the expected time it takes before the sketch is modified.

For convenience, we provide a table of commonly used symbols in table 1.
We also consider a Bayesian approach to estimation. This leads to subtlely different estimators which are biased but have slightly stronger optimality guarantees when the car-dinality is drawn from a Geometric (  X  ) prior. The Bayesian formulation is also of additional interest when estimating the error in section 2.2 since the posterior is tractable. The Bayesian estimate with lowest squared error given the se-quence of observed sketches is the conditional expectation given the sketches Interestingly, this gives a slightly different decomposition than the one for the martingale estimator described by 3. The additional term E ( N  X  T K | S T K ,K ) means that the es-timator is biased even under a uniform prior on N .
Under a Geometric (  X  ) prior on the cardinality N ,itis easy to derive
N  X  T K | S T K ,K  X  Geometric (1  X  (1  X   X  )(1  X  q T K )) (7) where q t = q ( S T t ) and the holding times are conditionally independent given the sketches. Note that as  X   X  0the prior approaches the improper uniform prior and the con-ditional distributions simplify to Geometric ( q ( S T )) for ap-propriate T . In this case, the estimator is identical to the martingale estimator except that the Bayesian estimator has one additional term for the last holding time. The Bayesian conditional mean estimator is given by
It is possible to maintain an estimate of the error in addi-tion to keeping an estimate of the cardinality. We estimate the error using both a martingale approach and a Bayesian approach and describe the properties of each.

The martingale estimate of the variance gives an unbiased estimate of the variance, but only at jump times when the sketch is modified. The quadratic variation M t of a process ( M t ) is defined to the the sum of its squared increments: It follows from Doob X  X  decomposition [5] that if M t is a martingale then the quadratic variation process M t is the unique predictable process such that M 2 t  X  M t is a martin-gale. In other words, since E M 2 t =Var( M t ), the quadratic variation process is an unbiased streaming estimate of the variance.

For the martingale estimator, the quadratic variation con-ditional on the observed sketches yields an unbiased estimate of the martingale estimator X  X  variance at jump times: where the second line uses the fact that the increments are geometrically distributed and that the variance of a Geometric ( q ) random variable is 1  X  q q 2 ,
The reason this estimator is inadequate for estimating the variance at a fixed cardinality n , is that the increments af-ter the last jump time involve the unobservable quantity ing a Bayesian argument. Given a uniform prior on n  X  T k the posterior is given by n  X  T K  X  Geometric ( q K ). This gives us a slightly modified estimate A fully Bayesian approach is simple to analyze. Under a Bayesian model with a Geometric (  X  ) prior, all of the hold-ing times, including the last difference, are independent and geometrically distributed according to equations 6 and 7. The variance estimate for the Bayesian estimator at a fixed cardinality is Note that if the estimate is at a stopping time, the Bayesian estimate for the variance also drops the last term in the sum. If  X  = 0 and the estimate is at a jump time, the Bayesian estimate exactly matches the martingale estima-tor. However, if the estimate is at a fixed time n , the con-tribution of the last difference N  X  T K is 1  X  q K q 2 of a Geometric ( q K ) random variable, rather than 1 /q K mean of a Geometric ( q K ) random variable and the contri-bution of the last term for the martingale estimator.
The optimality results in this paper primarily address the notion of statistical efficiency. Given a sketch construc-tion which yields a data generating distribution, what is the estimator with the lowest variance or risk? Much of the computer science theory literature focuses on optimal space-complexity. The primary differences between these two no-tions of optimality are three-fold. First, space-complexity is concerned with an asymptotic rate while statistical efficiency is concerned with the exact constant governing the rate. Sec-ond, statistical efficiency is always analyzed with respect to a given data generating distribution or the sketch construc-tion in this case, while optimal space-complexity does not assume a particular construction. Third, space-complexity depends on both the estimation method from data and the encoding of the data while statistical efficiency only deals with estimation.

Although the method given by [11] has optimal space com-plexity, it has non-optimal statistical efficiency and a non-trivial probability of not returning any result whatsoever. Empirical results [13] show that HyperLogLog has lower er-ror for the same sketch size. Because of these reasons, we do not consider finding an optimal algorithm for cardinality estimation to be a closed problem.

Given the current state-of-the-art in cardinality estima-tion, statistical efficiency has several advantages as a no-tion of optimality for practical algorithms. Since any de-cent statistical estimator for a parametric distribution has standard deviation on the order of O (1 / the number of observations, the asymptotic rate of the stan-dard deviation is typically meaningless. Statistical efficiency appropriately addresses the estimation problem while leav-ing the efficient encoding of the sketch as a separate issue. Furthermore, methods like HyperLogLog achieve a space-complexity of O (  X  2 log log n +log n ) compared to the opti-mal space-complexity O (  X  2 +log n ). The log log n term is effectively constant as 6 bits allows estimation of cardinali-ties approaching 2 64 .Iftheloglog n is ignored, the space-complexity matches the optimal one.
We establish that both the martingale estimator and its error estimate are optimal in the sense that they are mini-mum variance unbiased estimator at jump times. In other words, no other algorithm can also use the same sketch con-struction and have lower squared error if it gives an unbiased estimate. In Bayesian settings, the Bayesian cardinality es-timators are optimal for both fixed cardinalities and at jump times when the cardinality is drawn from the prior distribu-tion.

First, we consider the optimality of the martingale esti-mators at jump times for an infinite stream of data. Con-ditional on the sequence of sketches ( S i ), the increments T +1  X  T t | ( S i ) are independent random variables. This im-plies that the estimator  X  n T at a jump time T is the true conditional mean of the jump time given the sequence of sketches, and the martingale estimator of the variance is the true conditional variance. Hence, they are the optimal unbiased estimate under mean squared error. By the same reasoning, the variance estimate for the martingale estima-tor is also optimal at jump times. In the fixed n case, we are not able to prove that the estimator is a minimum variance unbiased estimator for fixed n .

In the Bayesian case, when the cardinality is a draw from the prior distribution, the Bayesian posterior mean is always optimal since it is the minimizer of squared error. However, it is biased, and an appropriate prior distribution is gener-ally not known. For the same reason, the variance estimate for the Bayesian estimator is also optimal under these con-ditions.
The previous section presented universal estimators of the cardinality and error for any Markov chain of sketches. In this section, these estimators are applied to existing sketch constructions. HyperLogLog [8] and Min-count [9] are de-scribed and their streaming counterparts are derived using the martingale estimator. We also describe the S-bitmap streaming algorithm which already uses a martingale esti-mator and show that the martingale estimator for Linear Probabilistic Counting is nearly identical to the original non-streaming version. Furthermore, we show that the martin-gale estimator is optimal in non-streaming settings as well.
The HyperLogLog algorithm [8] estimates the cardinality basedontheFlajolet-Martin(FM)sketch.Thesketchisan integer valued m -vector S i =( S i 1 , ..., S im ). The sketch is highly compact since each vector entry may be represented in O p (lg lg( n/m )) bits. For the FM-sketch, each element X is translated into a random pair The sketch updates bin M i by taking the max of the current value in the bin and the new value Y i
Sketch S i is updated in one time step with probability The HyperLogLog estimator is  X  N hyperloglog =  X /q ( S n ) where S n is the final sketch and  X  is an appropriate constant to cor-rect for bias. Section 6.4 show that this modification yields a reduction in the variance by a factor of  X  1 . 56.
Giroire X  X  Min-count algorithm [9] maintains the m th -order statistics of the observed sample. In other words, it keeps the m smallest elements in the sample. Let S ( m ) denote the m th smallest element in S . Without loss of generality, as-sume the hashed values are distributed Uniform (0 , 1). The probability a sketch S i is updated by a new element is Like HyperLogLog, the non-streaming Min-count estimator of the cardinality is only a function of the final update prob-ability q ( S n ) of the final sketch. The estimator is given by  X  N [3] when only the final sketch is observed. However, section 6.2 shows that the streaming estimator provably beats this optimal estimator.
The Linear Probabilistic Counting algorithm (LPCA) [15] uses a simple bitmap as a sketch. Each item in the data stream is hashed to a bit, and that bit is set if it is not al-ready. This algorithm is poor for large counts, as it requires space roughly linear in n .If B t is the total number of bits set at time t , then the probability of transitioning out of the current state is q ( S t )=1  X  B t /m . In this case, the martin-gale estimator has a simple analytic expression. This may be compared to the usual LPCA estimator. where H i denotes the i th harmonic number. Since log i  X  H , the LPCA estimator is a very good approximation to the martingale estimator.

The martingale estimator is, in fact, provably optimal for both streaming and non-streaming cases. Since the estima-tor is a function of the number of bits set, B n , in the final sketch and the sketch is not affected by the order in which elements are encountered, it is applicable in non-streaming settings. Furthermore, B n uniquely determines the likeli-hood function, and hence, is a minimal sufficient statistic. Since the martingale estimator is an unbiased estimator and a function of a minimal sufficient statistic, the Rao-Blackwell theorem states that it is a minimum variance unbiased esti-mator [2].
The S-bitmap algorithm also uses a simple bitmap as a sketch. The special property of the S-bitmap is that the relative error of the cardinality estimate defined to be
Var(  X  N ) /N is constant over a chosen range. Like LPCA, the S-bitmap is highly efficient for smaller cardinalities. Ta-ble 2 in [4] shows that it often requires less space to achieve a desired error for cardinalities up to 10 7 . One downside of the S-bitmap is that it is not applicable to distributed set-tings because the value of sketch S i depends on the order in which the distinct elements are observed.

The derivation of the S-bitmap directly chooses transition probabilities q ( S i )thatyieldaconstantrelativerootmean squared error.
 The sketch is updated by setting bit M i if that bit is empty and U i &gt;c ( B i  X  1 ) for a carefully chosen cutoff function c that is monotonically increasing. The cutoff function is chosen in a manner that ensures that the relative error is constant up to the capacity of the sketch. This update process is further illustrated in section 5 and figure 2.

The S-bitmap estimator is identical to the martingale es-timator. Like LPCA, the estimator is purely a function of thenumberofpositivebits B n in the final sketch, and B n is a minimal sufficient statistic. Thus, the S-bitmap estimator is an optimal streaming estimate.
So far, this paper has shown how the theory of Markov chains and martingales lead to cardinality estimators and their properties. We now give a geometric interpretation of sketch constructions via the area cutting process . This pro-cess is universally applicable as it can describe any sketch for counting distinct elements which is not affected by du-plicates. It is useful as it can be used to create new sketches as well as to analyze existing ones.

Consider a Markov process that cuts out sections of a unit square. At each time step, a point is uniformly drawn from the unit square. Rather than hashing to a single random variable, hash each element to a pair of independent uni-forms, X i  X  ( U i ,U i ). If that point falls on a portion of the square that has already been cut off, then do nothing. Otherwise, apply a deterministic procedure to cut off some section of the square that contains that point. It is easy to see that a duplicated point cannot induce more than one cut, so the sketch is not affected by duplicates and approximate for counting distinct items. The probability of an update at step i is equal to the remaining uncut area A i .
Theorem 3. All sketches that are not affected by dupli-cates have a representation as an area cutting process when-ever the universe from which elements are drawn is measure-able.

Proof. The uncut area is simply the measure of the re-maining elements which can affect the sketch.

Theorem 4. The area process forms a sufficient statistic for the unknown cardinality n .

Proof. Write down the log-likelihood of the process. The only terms that depend on the holding times T i  X  T j are functions of the area.

Together with the likelihood and sufficiency principles [14], theorems 3 and 4 say that for any sketch construction, all information that is useful for estimating the cardinality is contained in the area process. More specifically, the Rao-Blackwell theorem [2] states that if  X  n is an unbiased esti-mator and the sequence of areas ( A T i ) is a sufficient statis-tic, then E ( X  n | A T 1 ,A T 2 ,...,A n ) has variance no worse than  X  n . Formally, this means that any estimator which is not a function of the area process can be improved by turning it into a function of the area process by taking the conditional mean.
To illustrate the area cutting process, we describe several existing sketch constructions as area cutting processes. Figure 1: Pictoral representation of the Discretized Max-count (top) and FM (bottom) sketch. The + signs represent distinct items that are hashed uniformly to the rectangle. As each point is placed on the rectangle, shade an area covering the point if it is not already shaded. The shaded area rep-resent the areas that are cut out of the sketch. New points that land in a shaded region do not affect the sketch. For the Discrete Max-count sketch, everything to the left of the cutoff is shaded regardless of whether or not a point falls in aparticularbox. First, consider a continuous process corresponding to the FM-sketchusedinHyperLogLog. Dividetheunitsquare into m horizontal strips of equal size. The point ( U i ,U picks out a strip which contains the point and then cuts off everything to the left of U i within that strip. At any given time, the remaining area in strip j is equal to the one minus the maximum of the U i that fall into that strip. A discretization of this continuous process leads to the FM-sketch. For each horizontal strip, divide it along the vertical axis into intervals [2 v , 2 v  X  1 ) indexed by the expo-nent v . Rather than store the exact location Z j of the maximum element in the strip, store the exponent of the interval it belongs to. Equivalently, store  X  lg Z j .Since  X  lg U i  X  Geometric (1 / 2) when U i  X  Uniform (0 , 1), this is equivalent to taking the maximum of Geometric (1 / 2) ran-dom variables. Figure 1 illustrates this sketch as an area cutting process.
The Min-count sketch is even simpler to visualize as an area cutting process. Instead of a unit square, the process cuts a unit interval. Everything greater than the m th sample is cut off. Note that each of the m samples that are stored also cut off the exact location of the unit interval on which it falls. That prevents duplicates from affecting the sketch. However, those locations form a set of measure 0 since two independent Uniform (0 , 1) variables are equal with proba-bility 0. This also highlights the primary weakness of the Min-count algorithm. In order to have enough precision to treat each sample as occupying an area of 0 measure, each sample requires many bits to store.
Divide the unit square into m uniform vertical strips. Let each item in the stream hash to a uniformly distributed point on the square. Cutting out the vertical strip that an item falls into is equivalent to setting that bit. Figure 2 illustrates the difference between the S-bitmap and LPCA sketch. Figure 2: Pictoral representation of the (top) S-bitmap, (bottom) LPCA. Both share the same set of items that are hashed uniformly on the rectangle, but each cuts out a dif-ferent area. Each vertical cut or equivalently, each bit that is set, results in a horizontal cut as well. Due to this, the S-bitmap sketch has fewer bits set and more remaining ca-pacity.
The area cutting process for the S-bitmap sketch is similar to the LPCA sketch. In addition to cutting a vertical strip out, every update to the square also cuts out an additional horizontal strip. Figure 2 illustrates this process. By cutting out a larger area with each bit, the capacity of the sketch is increased by extending the time until the next bit is set. The virtual bitmap counting algorithm is the same as LPCA except that with probability 1  X   X  an element is hashed to a null bucket and does not update the sketch. This is equivalent to starting with 1  X   X  of the unit square already cut out and dividing the remaining area into m ver-tical strips like linear probabilistic counting. This allows the sketch to work for larger cardinalities at the cost of worse estimation for smaller cardinalities. The unbiased martin-gale estimator is simply 1 / X  times the martingale LPCA estimator.

The multiresolution bitmap [6], covers a range of cardinal-ities effectively by using multiple virtual bitmaps. Similarly to all the other bitmap based sketches, it cuts out vertical strips corresponding to the bin each element hashes to. The difference between the multiresolution bitmap and virtual bitmap is that for the multiresolution the strips have vary-ing widths.
Adaptive sampling is a method similar to Min-count pro-posed by Wegman and analyzed by Flajolet [7]. Rather than always keeping a fixed number of samples m , adaptive sam-pling specifies a maximum number of samples. Whenever the maximum is exceeded, half of the remaining area is cut off, and the samples that fall below the cutoff are kept.
The adaptive sampling estimator is illustrative of the dif-ference between traditional estimators and the martingale estimator. The usual estimator for adaptive sampling is k/p where p is the remaining area in the sketch. Each item stored in the sketch equally adds 1 /p to the estimate. For the mar-tingale estimator, the estimator is x 1 p x where the sum is over all points that updated the sketch, including ones not stored in the final sketch, and p x is the sampling probability or area at the time of adding x .
In addition to better understanding of existing sketches, the area cutting process also leads to other sketches that may be used in approximate counting. The primary weak-ness of the Min-count algorithm is the number of bits re-quired per sample stored, since each sample is a real num-ber. We present the Discrete Max-count sketch which is a simple modification of the Min-count sketch that allows the sketch to be stored more efficiently. The Max-count sketch is obtained by hashing each distinct element to a discrete distribution F and taking the largest m values. The distri-bution F may be used as a tuning parameter which yields improves accuracy for targeted ranges of cardinalities. Note that when the distribution F is continuous, there is effec-tively no difference from the usual Min-count algorithm since F is monotonically increasing and F ( X )  X  Uniform (0 , 1) when X  X  F . Thus, the Min-count sketch is obtained by a simple variable transformation. We consider the case when the discrete distribution is Geometric (1 /m ).

Under this distribution, Discrete Max-count has several interesting properties. First, it has infinite capacity since it can estimate arbitrarily large cardinalities. Second, the sketch may be decomposed into an offset which grows as O (log n ) with the number of distinct elements encountered andasetofsampleswhichdoesnotgrowwith n . Third, the relative error of the algorithm is O (1 / gives the sketch a space complexity of O (log n +  X  2 g ( )) where g is some function and is the relative error. If g is bounded by a constant, the sketch would have optimal space-complexity. Section 6.3 shows that g ( ) may be upper bounded by log using a naive encoding of the samples. We also note that there is no interaction between the relative error and n in the space complexity bound. Kane et al [11] give an extensive list of space complexities for distinct counting algorithms. Only their theoretically optimal but empirically impractical [13] algorithm has this property.
We present the main tool to analyze this process, the no-tion of self-similarity, and give a dense set of proofs justifying these claims. An analysis of the method X  X  running time is deferred to section 7.
We introduce the notions of an offset decomposition and self-similarity for area cutting process. Consider a sequence of sketches S t with areas q ( S t ) and jump times T i . An area process is defined to be self-similar if q ( S T i ) /q ( S q ( S 0 ) /q ( S T j )forall i&gt;j . In other words, it is self-similar when the proportion of the area cut out by j jumps re-mains the same in distribution over time. An area process is strongly self-similar if, in addition, the relative area cut off by a jump q ( S T ) /q ( S T  X  1 ) is independent of q ( S an area cutting process can be written as an offset  X  t and a set of samples Y t so that there is a bijection between S ( X  t ,Y t ) and there exist functions q 0 ,q 1 such that for all t then ( X  t ,Y t ) is an offset decomposition of the area cutting process. An offset decomposition is (strongly) self-similar if the offset area q 0 ( X  t ) is (strongly) self-similar and a process is a decomposable area cutting process if a non-trivial self-similar offset decomposition exists. When 0  X  q 0 ,q 1  X  1, the decomposition conditions state that the area may be obtained by first cutting off an area specified by the offset  X  t and then independently cutting off part of the remaining area specified the samples Y t . The condition also implies that the samples Y t do not carry information about the car-dinality since the distribution of Y t does not depend on t . However, the samples do contain information that prevents double counting of duplicates.

The Min-count sketch is an example of a strongly self-similar area cutting process once at least m items have been encountered. The offset is equal to the m th smallest item S t , and the samples are the remaining items in the sketch scaled by S ( m ) . These rescaled samples are independent Uniform (0 , 1) and independent of the offset. The propor-tion of area remaining after another cut is the maximum of these samples and a new independent Uniform (0 , 1) vari-able. This maximum is always Beta ( m, 1) distributed. Hence, the remaining proportion is independent of the offset, and its distribution does not change over time.

These self-similarity conditions lead to several attractive properties. Strong self-similarity ensures that a sketch has infinite capacity if the offset cannot cut off all the area, and it allows the variance of the martingale estimator to be com-puted exactly. An offset decomposition ensures that the size of the sketch is well-controlled. These properties are proved in the following theorems.

Theorem 5. The size required to store the samples Y t of any offset decomposition does not depend on the cardinality n . Furthermore if the remaining area q ( X  t ,Y t ) &gt; 0 for all t almost surely, then the sketch can give unbiased estimates for arbitrarily large cardinalities almost surely.
Proof. The size required to store the samples Y t is purely a function of the conditional distribution Y t |  X  t Since Y independent of  X  t and the marginal distribution of Y t is the same for all t , the storage requirement for Y t does not depend on n or  X  t . If the remaining area is always greater than 0 then the martingale estimator will give an unbiased estimate for any cardinality.

Theorem 6. For any area cutting process with an offset decomposition ( X  t ,Y t ) , the variance of the martingale esti-mator is
Var(  X  N n )=
Proof. Consider the increments of the quadratic vari-ation process for the martingale estimator. The expected squared increment for the t th distinct item is 1 q ( S t q ( S t )) = 1 /q ( S t )  X  1. The rest follows from the independence of  X  t and Y t and the product form for the area.
Theorem 7. For any strongly self-similar offset decom-position, 1 /q 0 ( X  t )  X  ct is a martingale for some constant c . The variance of the martingale estimator has an analytic form given by the two terms on the right hand side are independent con-ditional on t + 1 being a jump time. Applying the law of total probability on t + 1 being a jump time with probabil-ity q 0 ( X  t ) q 1 ( Y t ), it is easy to show that 1 /q 0 martingale with c = r E q 1 ( Y 1 ). The variance follows from equation 17.

This theorem shows how the two components of an off-set decomposition affects the variance of the estimate. The estimates are more accurate if the expected proportion r that is cut off is small and if the expected area cut off by the sample has low variance. This may be seen from apply-ing the delta method to obtain E (1 /q 1 ( Y 1 ))  X  1 / E method to compute a closed form estimate of the error. ulation and plug those values into equation 19.
Another consequence of theorem 7 is that a strongly self-similar decomposition leads to an unbiased estimator in non-streaming settings. Since 1 /q 0 ( X  t )  X  ct is a martingale, is an unbiased estimator that only depends on the final sketch S n and a known initialization condition  X  0 .Note that the samples Y n do not appear in the estimator. Since the distribution of Y t does not depend on t and is indepen-dent of  X  t , it is an ancillary statistic.

The variance of this estimator may be analyzed in the same manner as showing 1 /q 0 ( X  t )  X  ct is a martingale. Let
E 1 Recursing and using the non-streaming estimator to com-pute E 1 /q 0 ( X  t ) yields that the variance of the non-streaming estimator is approximately
For Min-count, the optimal non-streaming estimator as well as its variance are known. Since it yields a strongly self-similar area cutting process, the variance of the martingale estimator is also known. The optimal non-streaming estima-tor [3] is m  X  1  X  t .Since X  t  X  Beta ( m, t  X  m +1), the estimator has variance ( t  X  m +1) t m  X  2  X  t 2 / ( m  X  2). By equation 19, the martingale estimator has asymptotic variance t 2 2( m  X  1) streaming estimator easily beats the optimal non-streaming estimator and achieves the same error with half the num-ber of samples. Furthermore, the non-streaming estimator in equation 20 is the same as the optimal non-streaming estimator, and the variance estimate converges to the true variance as m  X  X  X  .
Theorem 8. Discrete Max-count converges almost surely to an area cutting process with a strongly self-similar decom-position.

Proof. Let  X  t be the m th largest element S ( m ) t in the sketch and Y ti = S ( i ) t  X  S ( m ) t . By the memoryless property of the geometric distribution,  X  t and Y t are independent, and this is an offset decomposition once the sketch contains at least m items. The proportion of area cut off by the offset q ( S T ) /q ( S T  X  1 ) is the minimum of the samples Y and the newly observed value h ( X T )  X   X  T  X  1 which is also independent of  X  T  X  1 by the memoryless property. Hence, the offset area process q 0 ( X  t ) is strongly self-similar.
The self-similarity property aids in upper bounding the variance and space-complexity of the algorithm. Since Dis-crete Max-count converges to a process with a strongly self-similar decomposition, the asymptotic variance is O ( r ) where r is defined in theorem 7. The number of strips cut off by the offset at jump time T is obtained by taking the minimum of the samples Y T  X  1 along with the newly observed value X
T  X   X  T  X  1 . It may be upper bounded by taking the min-imum of m independent Geometric (1 /m ) variables rather than m unique Geometric (1 /m ) variables. The probabil-ity that the minimum of m independent Geometric (1 /m ) random variables is at most i is 1  X  (1  X  1 /m ) im . This is the distribution function of a Geometric (1  X  (1  X  1 /m ) variable. Using the moment generating function for the geo-metric distribution gives that r&lt; 1 m 1 (1  X  (1  X  1 /m satisfied the inequality 1 &lt; X &lt; 1 / (1  X  1 /m ) m +1  X  pirically, we find that  X   X  1 for a range of m  X  [100 , 10000]. This provides an both an approximate upper bound for the variance of 1 . 58 n 2 /m and proves that the relative error is order O (1 /
The space complexity of the sketch may be analyzed by considering the size required for the offset  X  t and for the samples Y t . In a naive encoding of the samples, each sample is encoded as a difference from the offset, and the number of bits required for each sample is on the order of log( m ) / log(1 1 /m )  X  m log m = O (  X  2 log ) where is the relative error. Naively encoding the offset as an integer requires O ( m log n ) space due to the fine granularity of the discretization. How-ever, the non-streaming estimator in equation 20 demon-strates that the offset is close to a function of the cur-rent estimate. The offset may be encoded as a difference from this expectation. Equations 19 and 23 establish that Note that q 0 ( X  n )=(1  X  1 /m )  X  n is smooth. A simple Chebyshev bound and Taylor expansion yields that  X  0  X  m log  X  N n = O p ( n stored in O p (log n +log ) bits. This gives the space com-plexity of Discrete Max-count to be O (log n +  X  2 log ).
A trivial decomposition for any area cutting process al-ways exists where the samples Y t = {} . Theorem 6 gives that the variance that is approximately the sum of expected reciprocal areas. Since HyperLogLog estimates the cardinal-ity in terms of the reciprocal area,  X  N hll =  X  m m/q ( S Figure 3: Relative error plot with size number of bins. Both streaming methods beat HyperLogLog over the entire range of cardinalities.  X  m =0 . 7213 / (1 + 1 . 079 /m )for m  X  128, substituting n for  X  N hll gives the plug-in estimate By comparison, the estimated variance for HyperLogLog is use approximately 1 . 4426  X  1 . 04 2  X  1 . 56 times the space of Streaming HyperLogLog to achieve the same error. This matches extremely closely to our empirical evaluation where we found that Streaming HyperLogLog X  X  variance bested HyperLogLog X  X  by a factor of 1 . 57.
The streaming cardinality and error estimates are empir-ically validated using both simulations and a real dataset. For all the simulations a 64 bit hash is used. A stream of 10 7 distinct elements is passed through each method and both cardinality and error estimates are obtained at regu-lar intervals on a log scale. This is repeated 5  X  10 5 times for each method, and the results are averaged. The meth-ods considered are HyperLogLog, Streaming HyperLogLog, and Streaming Discrete Max-count with a Geometric (1 /m ) discretization.

To evaluate each method, we compare the relative error and effective bits per sample which are defined as Thedefinitionoftheeffectivesamplesize m effective is moti-vated by the fact that for many approximate counting meth-ods including HyperLogLog, the relative error is approxi-mately 1 /
Note that the effective bits per sample is tied to the en-coding of the sketch. For HyperLogLog, a typical imple-mentation of the sketch which allocates five bits per bin is used. For Discrete Max-count, the sketch is encoded in three parts: the offset which is stored as an integer, a bitmap with Figure 4: Effective bits per sample with size number of bins. For small sketches, Discrete Max-count outperforms both HyperLogLog based methods and requires half the size of HyperLogLog. For larger sketches, the encoding used by Discrete Max-count requires O (log m ) bits per bin which results in worse performance for larger sketches. 2 m bits, and a sorted array of values above the offset plus 2 m . Each value in the array uses log 2 max v  X  2 m bits where the max is taken over values in the array. We note this is not an optimal encoding of Discrete Max-Count, but it provides an upper bound on the optimal bits required per sample. In this case, the amortized running time of the method is O p ( n log m + m log m log n ). Each distinct ele-ment requires O (log m ) time to search through the sorted array, and each update requires O ( m log m ) time. There are O (log n/ log(1  X  1 /m )) = O p ( m log n )updates.
In addition to simulated data, the methods are evaluated on an anonymized Facebook test dataset. The sample con-tains 50 million rows containing approximately 1 million dis-tinct users. To obtain accurate estimates of the relative er-ror, each algorithm is run 4000 times with a randomly salted hash for each run. Figure 5 shows that the results on the dataset match the simulation. There remain several questions of interest in this paper. In particular, questions about the space-complexity of these methods and the encoding used for each sketch remain. The ideas behind the self-similar area cutting processes and the decomposition into an offset  X  t and set of samples Y t leads to the Discrete Max-count sketch and space-complexities that only depend on the cardinality n through a counter. However, the encoding of the samples Y t used in this paper has a space-complexity of O (  X  2 log 1 / ) rather than a rate of O (  X  2 ) to obtain the optimal space-complexity. Like-wise, the FM-sketch yields a self-similar area cutting pro-cess. However, in the formulation of the process in section 6.4 the offset requires storage that grows at a rate greater than the optimal rate. We also conjecture that there are alternatedecompositionoftheFM-sketchintoanoffsetand samples along with an encoding that yields optimal space-complexity. One possible offset is to take the median value of the bins. Figure 5: The left figure shows results on real data match the simulations. The right figure shows that the error esti-mates yield confidence intervals with the correct asymptotic coverage. HyperLogLog has worse coverage in an interme-diate region due to bias. Bias corrected HyperLogLog [10] may improve on the observed coverage; however, the bias corrected method does include any improved estimates of the variance.

Another problem that remains is how to exploit the in-formation in individual sketches that are merged. This may occur because the sketches are computed in a distributed fashion or because the cardinality of interest is the union of several sets with precomputed sketches. Although both Hy-perLogLog and Discrete Max-count have sketches that can be merged. Their estimators either only use the information contained in the final merged sketch or require access to the complete stream and cannot use precomputed sketches.
Another unexplored area is the effect of discretization on the underlying continuous area cutting process. Hyper-LogLog uses the Geometric (1 / 2) distribution for discretiza-tion while Discrete Max-count uses the Geometric (1  X  1 /m ) distribution. The discretizations lead can lead to different operating ranges as well as a different numbers of bits re-quired per stored sample. The tradeoff between sketch size and accuracy due to discretization is not well-understood.
This paper presents a recipe for constructing practical al-gorithms that probabilistically estimate the number of dis-tinct elements in a stream of data as well as the error of the estimate. These streaming algorithms provably outper-form optimal non-streaming methods and yield substantial improvements in the storage requirements to obtain a de-sired relative error. This is born out in both theoretical and empirical results which show that HyperLogLog requires 50 percent more space than Streaming HyperLogLog to achieve the same error and Min-count requires twice as much space as Streaming Min-count.

We also provide a geometric interpretation to all sketches for distinct counting via the area-cutting process. Existing methods such as HyperLogLog, LPCA, and the S-bitmap are described via this process. This interpretation provides a link between different discretizations yielding different sketches and a common continuous area cutting process. Further-more, we introduce the notion of decomposable self-similar area cutting process. Such processes have notable proper-ties including infinite capacity and an encoding in which the sketch may be separated into two components, an off-set which depends on the cardinality n andasetofsamples whose the size depends only on the accuracy . This has in-teresting implications for the space-complexity of methods.
The proposed algorithms are analyzed under both fre-quentist and Bayesian settings. The cardinality estimators are proven to be optimal in various settings. In particu-lar, the proposed martingale estimator is proven to be unbi-ased and optimal when queried immediately after a sketch is modified by an element in the stream. As a simple con-sequence of the analysis, we also give a provably optimal, unbiased estimator for the LPCA sketch which applies in both streaming and non-streaming settings.

These contributions lead to multi-faceted advances in sketch construction, cardinality and error estimation, theory, and intuition for the problem of approximate counting of distinct elements for both the streaming and non-streaming cases. [1] K. Aouiche and D. Lemire. A comparison of five [2] G.CasellaandR.L.Berger. Statistical inference . [3] P. Chassaing and L. Gerin. Efficient estimation of the [4] A. Chen, J. Cao, L. Shepp, and T. Nguyen. Distinct [5] R. Durrett. Probability: theory and examples , [6] C. Estan, G. Varghese, and M. Fisk. Bitmap [7] P. Flajolet. On adaptive sampling. Computing , [8] P. Flajolet,  X  E. Fusy, O. Gandouet, F. Meunier, et al. [9] F. Giroire. Order statistics and estimating [10] S. Heule, M. Nunkesser, and A. Hall. Hyperloglog in [11] D. M. Kane, J. Nelson, and D. P. Woodruff. An [12] A. Metwally, D. Agrawal, and A. E. Abbadi. Why go [13] J. Rae. Data stream cardinality: An empirical study [14] C. Robert. The Bayesian Choice: From [15] K.-Y. Whang, B. T. Vander-Zanden, and H. M.

