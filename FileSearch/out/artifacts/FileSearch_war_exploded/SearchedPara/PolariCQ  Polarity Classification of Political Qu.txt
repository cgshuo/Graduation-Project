 We consider the problem of automatically classifying quotations about political debates into both topic and polarity. These quo-tations typically appear in news media and online forums. Our approach maps quotations onto one or more topics in a category system of political debates, containing more than a thousand fine-grained topics. To overcome the difficulty that pro/con classifica-tion faces due to the brevity of quotations and sparseness of fea-tures, we have devised a model of quotation expansion that har-nesses antonyms from thesauri like WordNet. We developed a suite of statistical language models, judiciously customized to our set-tings, and use these to define similarity measures for unsupervised or supervised classifications. Experiments show the effectiveness of our method.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Linguistic Processing Algorithms, Experimentation Web Information Extraction, Political Opinion Mining Political controversial topics such as  X  X reece bailout X , or  X  X rab Spring revolts X  are discussed in great depth in political debates, newspapers, and other forms of social media. In contrast to stan-dard approaches for sentiment analysis on products (cameras, movies, etc.) [10, 7], the sophisticated nature of political opinions calls for more advanced linguistic techniques. A typical task considers a number of controversial topics such as  X  X mmigration X  or  X  X bortion X  and a set of stakeholders like politicians, and aims to classify the stakeholders X  opinions into pro or con categories for the respec-tive topics [2]. Such analyses are typically based on aggregating many statements and work well for coarse-grained topics. How-ever, political analysts are often interested in individual and brief statements, as quoted in news media, and their pro/con polarity with regard to fine-grained debates such as  X  X eporting illegal im-migrants X  or  X  X mmigration amnesty X . Quotations are a prominent form of highlighting opinions in newspapers, and all kinds of social media.

The problem addressed in this paper is to automatically classify quotations onto fine-grained topics of controversial nature, and to assign a pro/con polarity for each quotation-topic pair. Note that a quotation, given as a short text and the quoted person or party, can refer to several topics. Table 1 shows an example with quota-tions on immigration. We are interested in identifying the pro/con stances in the inputs, not just on the broad issue of immigration but on fine-grained topics (e.g. con Mexican border fence, con deport-ing illegal immigrants), for quotation 1). The problem is much more demanding than traditional forms of sentiment mining for various reasons: (1) fine granularity: the number of debated top-ics that the classifier must consider is potentially very high, in the thousands rather than the usual tens; (2) brevity of statements: quo-tations are usually very short texts (often only a single sentence), so that whatever linguistic features are used tend to be sparse; (3) topic-dependent polarity: the same quotation can have different polarities for different fine-grained topics.

Our approach maps quotations onto one or more topics in a cat-egory system of political debates. We use the fine-grained cate-gories (called debates ) of debatepedia.org as our target. For each pair of quotation and relevant debate, our classifier targets, the debates in debatepedia, come with articles and user discus-sions, and computes a pro/con polarity. For example, for the input texts in Table 1, the output of our method is the topic/polarity pairs shown at the bottom of each quotation. We define language mod-els (LMs), with judiciously chosen features (including bigrams) for each debate, and then use a scoring function based on the query likelihood as a similarity measure that is fed into different kinds of unsupervised or supervised classifiers (kNN, SVM, LDA). The high sparsity in the debates themselves is addressed by smoothing the debates X  LMs via thematically related debates. To overcome the difficulty caused by the brevity of quotations and sparseness of features, we have devised a method of quotation expansion that harnesses thesauri like WordNet. We use synonyms and antonyms (i.e., words for opposite senses, e.g.,  X  X ensorship X  or  X  X egulation X  as antonyms of words like  X  X eutrality X  or  X  X reedom X ) to conceptu-ally expand the text of a quotation. This approach leads to a novel form of enriched feature space: a quotation is then represented by an expanded entailment/contradiction language model.

In Section 2, different features models are introduced. The quo-tation expansion is described in Section 3. Details about the pro-posed LM based models for topic and pro/con classification are g iven in Section 4. Section 5 presents our experimental evaluation. Section 6 positions our contributions with regard to related work. We conclude with Section 7. Topic and Sentiment Terms. A topic term is a term which de-scribes a topic, while a sentiment term is one which describes an opinion. We assume nouns to be topic terms, while verbs, adjec-tives and adverbs are sentiment terms. For example, quotation (1) in Table 1 has the topic terms  X  X all X ,  X  X ampaign X ,  X  X conomy X ,  X  X e-portation X , etc., while its sentiment terms are  X  X estroy X ,  X  X eaken X ,  X  X enseless X , etc.
 Unary and Binary Features. We define unary features and binary features as follows.

D EFINITION 2.1. A unary feature is denoted as h u i where u is either a topic term or a sentiment term.

For example, quotation (1) in Table 1 has the unary features: h wall i , h economy i , h deportation i , h destroy i , h weaken i , etc.
D EFINITION 2.2. For a given quotation Q , let Q T and Q S note the set of its topic terms and sentiment terms respectively. A bi-nary feature , denoted h t, s i , consists of t  X  Q T and s  X  Q that, t and s are connected by a dependency relation. The depen-dency relation is determined by parsing the sentence in Q in which they co-occur using a dependency parser nlp.stanford.edu .
For example, quotation (1) has as binary features: h wall, weaken i , h deportation, destroy i , h economy,weaken i , etc.
Our approach of pro and con classification is built on the intu-ition that opinions which are in agreement with each other have expressions which are in agreement to each other, while opinions which disagree have expressions which are in disagreement. For example in Table 1 quotation (1) which has the expression  X . . . and weaken the economy. X  is in disagreement with the expression in quotation (2)  X . . . could help with the economy X , while the expres-sion  X . . . mass deportation is not the answer X  in quotation (3) is in agreement with the expression  X  X ngaging in a campaign of mass deportation. . . would destroy families X  in quotation (1).
In order to capture the notion of agreement and disagreement for a given quotation, we focus specifically on the binary features of the quotation. That is, the topic and sentiment term pair h t, s i which are in a dependency relationship with each other. The key idea that we propose is to expand the topic term and the sentiment term with both their synonyms as well as their antonyms (see Ta-ble 2). For this expansion, we use the WordNet thesaurus, which gives synonyms and antonyms for many concepts. In order to map a word observed in a quotation onto its proper word sense, that is, the WordNet concept denoted by the potentially ambiguous word, we use the most-common-sense heuristics which has been used ef-fectively in many applications [11].

Let t + , t  X  denote a synonym and antonym of a topic term re-spectively. Analogously, s + and s  X  denote a synonym and antonym of a sentiment term. The possible expansions of a binary feature h t, s i are the pairs h t + , s + i , h t  X  , s  X  i , h t first two are in agreement with the original feature h t, s i , while the last two are in disagreement. As an example, consider the quo-tation in Table 2 and the binary feature h economy, help i . A syn-onym for the topic term  X  X conomy X  is  X  X aving X  while its antonyms could include  X  X pending X  and  X  X xpend X . Similarly, for the senti-ment term  X  X elp X , synonyms include  X  X upport X  and  X  X ssist X . while it X  X  antonyms are  X  X estroy X ,  X  X eaken X . Therefore, the expanded binary features include h economy, weaken i ( h t + , s  X  disagreement with the original feature, as well as h saving, assist i ( h t + , s + i ), which is in agreement with the original feature. Table 2 shows more examples of these binary features.

D EFINITION 3.1. For a topic term t i , the set of topic term syn-onyms is denoted as T i , and the set of topic term antonyms is de-noted as  X  T i . Analogously, S i and  X  S i denote the sentiment term synonyms and antonyms of a sentiment term s i , respectively.
D EFINITION 3.2. For a given binary feature h t i , s i i present in the quotation, we define the set of agreement features as AF = {h t
D EFINITION 3.3. For a given binary feature h t i , s i i present in the quotation, we define the set of disagreement features as DF = {h t
We first use topic features to map a quotation onto one or more debates. Then, for each of the identified debates, we use the joint topic-sentiment unary and binary features, optionally with expan-sion, for inferring the pro/con polarity of the quotation.
We estimate an LM for each debate with unary topic features as terms and then compute the query likelihood. Let P D denote the language model of a debate D . Then, where D is a debate on a fine-grained topic, w is a topic term, and C D is the set of debates belonging to the same category as D in Debatepedia. Let score ( D ) = P ( Q|D ) denote the probability that D generates quotation Q . Then, where Q T i s the set of topic terms in Q . This is the the quota-tion likelihood of Q with respect to D . The set of topics for Q is now Q D = {D| score ( D ) &gt;  X  } , where  X  is a threshold (in our experiments,  X  = 0 . 01 , with an average of 3 topics per quotation).
Once we have a set of topics Q D for the given quotation Q , our task is to classify the polarity of Q on each D  X  Q D . For every de-bate in Debatepedia, there is a set of pro documents and a set of con document. For a debate D  X  Q D , we define D + as the concatena-tion of all pro documents, and D  X  as the concatenation of all con documents for that debate. Given a pro and a con document for D , we compute the quotation likelihoods: P ( Q|D + ) and P ( Q|D If ( P ( Q|D + ) &gt; P ( Q|D  X  )) , the quotation is classified as pro, oth-erwise, we classify it as con. In effect, this is a k NN classifier (k nearest neighbors) with k = 1 in our 2-class settings. We estimate the quotation likelihood with respect to D + as follows. Analogously, we also estimate the quotation likelihood with respect to D  X  . The query is represented as a set of features where each fea-ture is denoted as w i . While we can use the terms in the quotation and debates as is, this is unlikely to give us good results (as we show in our experiments) because of the sparsity of terms in the quotation. To overcome this problem, we make use of our features model for the estimation of the language models of the debates. grams as features to represent the queries and the documents. For a given debate D , we estimate the language model of D + over all possible n -grams, where n  X  3 as follows.
 where w is an n -gram and C D is the background corpus consist-ing of all debates in the same branch of Debatepedia. We esti-lar manner. The LM for D  X  is estimated analogously. Finally, the quotation likelihoods given D + or D  X  are computed.
 UNA . We estimate the LM of a document as a mixture model of two LMs, one considers topic terms and the other considers sentiment terms. The topic terms and sentiment terms together form the unary features. The language model of D + is estimated as: where w is a unary feature, P D + the topic LM of D + , and P D + sentiment LM of D + , and  X  is a parameter which determines the importance of each. The topic LM and sentiment LM are estimated as before, with the universe of terms consisting of topic terms and sentiment terms, respectively. Analogously, we estimate D binary features are h t, s i pairs, where t is a topic term and s is a sentiment term and the two are in a parse dependency relationship. We now estimate the LM of D + and D  X  over both unary and binary features.
 where w is a unary or binary feature, P D + P the previous section.  X  is a weighting factor. The LM for D timated in an analogous manner. In this LM denoted as LM-BIN-I , we assume that the features are independent analogous to assuming independence among unigrams in the standard LM techniques.
In order to increase the accuracy of the LM, we propose the modeling of limited dependence among features denoted as LM-BIN-D . We consider a universe of terms consisting of pairs of fea-tures h f i , f j i where f i and f j could be the unary or the binary features. For example the quotation in Table 2 has as features pairs This is similar to modeling bi-grams in the standard LM setting, but with a crucial difference. While bi-grams are naturally two consec-utive unigrams, we cannot insist that our feature pair are consecu-tive. Instead, we make the default assumption that the feature pair occur in the same sentence. That is, the frequency of a feature pair is the number of sentences in which they co-occur. With this in mind, we estimate our new LM as the interpolation of two LMs, where w is now either a unary feature, or a feature pair. We can now compute the likelihood of generating the quotation from either the pro document or the con document, in a straightforward way. However, since our binary features are confined to the scope of the same sentence, we can alternatively compute the likelihood of generating a sentence from the two polarities X  documents. As a quotation typically consists of few sentences, we can subsequently aggregate over these sentence likelihoods. This is, D EFINITION 4.1. The quotation score given the pro document sen i is a sentence in the quotation, M AX denotes the maximum over the likelihoods of the quotation X  X  sentences, and P ( sen is computed as, Analogously, we define the quotation score given the con docu-ment score ( Q|D  X  ) .
 variant, we make use of the agreement and disagreement features of Section 3. The intuition here is that, not only should a pro doc-ument (respectively, con) agree with the agreement expressions, but the con document (respectively, pro) should agree with the dis-agreement expressions.

D EFINITION 4.2. Given the agreement features Q + and dis-agreement features Q  X  of a quotation Q , in the entailment and contradiction model (EC) , the probability of generating Q given the pro or the con document:  X  is a weight parameter to determine the importance of the agree-ment features of the quotation versus the disagreement features. If  X  = 0 which means we consider only the agreement features, we denote the model in this case as E . We estimate the above prob-abilities (e.g. P ( Q|D + ) ) using the models described above (e.g. LM-UNA, and LM-BIN).
W e evaluated the effectiveness of the proposed features mod-els and the quotation expansion model in combination with several classifiers: LM-based kNN (k nearest neighbors, here with the spe-cial case k = 1 ), LDA, and SVM. We report the classification of quotations into pro/con polarities. The metrics of interest are pre-cision and recall, both micro-averaged over all quotations in a test set and macro-averaged over the classes of pro/con. We created our own training and test datasets as explained next. is focused on political controversies. It consists of ca. 1,700 top-ics called debates such as  X  X eporting illegal immigrants X . Each debate has two types of short documents (quotations) debating it, pro documents and con documents. The quotation belongs to one or more debates in Debatepedia. We extracted 142,253 quotations from Debatepedia, and created our experimental sets.
 Test dataset. We compiled, by random sampling, a held-out set of 250 pro and 250 con quotations from various different topics as a test set . These 500 quotations, each belonging to one or more topics, covered a total of 73 different fine-grained topics from De-batepedia. Since the topics as well as the polarities are given in advance, the ground truth for the classifiers is known. Development dataset. Hyper-parameter tuning is performed on separate development set of 200 pro and con quotations sampled from Debatepedia for 73 topics that occur in the test dataset. Training dataset. For training supervised classifiers, we sampled 4,400 quotations from Debatepedia for 73 topics that occur in the test dataset.
 ing pro and con documents.
 DNone with original features only. Only the unary and binary features extracte from the debate documents are used as features to represent the debates.
 DExp with expansion. In addition to the unary and binary features, their synonyms and antonyms are added to the features set. There-fore, the final set of features for a debate includes its unary and binary features and their expansions with synonyms and antonyms. of LM-based methods against each other and against two baselines. LDA (Latent Dirichlet Allocation). A state-of-the-art latent-topic clustering method (implemented using mallet.cs.umass.edu ). For pro/con classification, LDA is configured with two latent di-mensions; it is run separately for each Debatepedia topic. SVM (Support Vector Machine). A supervised discriminative classifier (implemented using svmlight.joachims.org ). For each Debatepedia topic, we train a binary classifier with a linear ker-nel. Equipped with various feature models (n-grams, unary with expansions and binary with expansions), both LDA and SVM were trained with the quotations in the training dataset described in Sec-tion 5, and tuned with the separate development set.
 LM-based classification. We studied the LM-based methods de-scribed in Section 4 on different test sets, using different feature models, and tuned with the separate development set: (1) the n-grams model: LM-NG , (2) the entailment model given the pro LM and the con LM, in combination with the unary model only denoted as LM-E-UNA , or the binary and the unary models assuming ei-ther binary features independence (BIN-I), denoted as LM-E-BIN-I , or binary features dependence (BIN-D), denoted as LM-E-BIN-D , and finally (3) the entailment and contradiction model given the pro LM and the con LM, in combination with the unary model only LM-EC-UNA , or the binary and the unary models assuming either Table 3: Micro-averaged precision/recall for LM-based pro/con classification on Debatepedia test set Table 4: Micro-and macro-averaged precision/recall for LM-b ased pro/con classifications on the ProCon test set binary features independence, denoted as L M-EC-BIN-I , or binary features dependence, denoted as LM-EC-BIN-D . experiments with family of LM-based methods on classifying quo-tations into pro/con polarities for each topic: (1) the Debatepedia test dataset , and (2) ProCon test dataset : www.procon.org is a political website which provides pro and con quotations by politi-cians on specific topics. Each quotation is tagged by both the topic and the stance (pro or con). We collected 400 quotations on various topics as our second test set.
 Results of Debatepedia dataset. Results of the LM-based ap-proaches with the different topic documents variants are shown in Table 3. These are the results found at  X  = 0 . 64 for the LMs over the unary features ( LM-UNA ),  X  = 0 . 18 for the LMs over the unary and the binary features ( LM-BIN ), and  X  = 0 . 24 for the entailment and contradiction model ( EC ). The hyper-parameter values are automatically determined from the development set. At test level  X  = 0 . 05 using the paired two sample t-test, we found that the results of the different techniques on DNone and DExp are not statistically different. This means that the expansion of the de-bates did not significantly improve the results. On the other hand, the difference in the results between the E models which use only the agreement features of the quotations and the EC models which use both the agreement and the disagreement features is statistically significant. So quotation expansion using both the synonyms and the antonyms improves the results. In addition, the difference in the results of the BIN-I model, and the BIN-D model were statistically significant, too, which means that considering the dependency of the binary features in each sentence improves the results. Results of ProCon dataset. We evaluated a set of 235 quotations from the set of quotations from the ProCon test set assigned to the topics in Debatepedia test set. We considered the classifica-tion models ( LM-EC-BIN-D , LM-EC-UNA , and LM-NG ). For this experiment, we used the hyper-parameter values of the LM de-termined from the development set (e.g.  X  = 0 . 64 ,  X  = 0 . 18 , and  X  = 0 . 24 ). Table 4 shows the results of the three different mod-els. These results are statistically significant at test level  X  = 0 . 05 using the paired two sample t-test.
 LDA &amp; SVM. We conducted experiments on the Debatepe-dia test set in order to evaluate the effectiveness of the proposed feature models and quotation expansion model on the pro/con clas-sification task with two different classifiers SVM and LDA, in com-parison to our LM-based methods. Table 5 shows the micro-and macro-averaged precision and the micro-and macro-averaged re-call of LDA and SVM compared to the LM-based approaches in Table 5: LM pro/con classifiers micro-and macro-averaged p recision (P) and recall (R) compared to SVM and LDA combination with different feature models (e.g. L M-EC-BIN-D, LM-EC-UNA, and LM-NG ). The results are statistically signifi-cant at test level  X  = 0 . 05 using the paired two sample t-test. We notice that the binary features model with expansion improved the results of both the SVM and the LDA classifiers. Moreover, our LM-based methods outperformed SVM and LDA by a significant margin for both precision and recall.
For the pro/con assignment, our best method LM-EC-BIN-D achieves almost 74% precision. It uses the richest features, the de-pendent pairs of binary features and the entailment-contradiction expansions. While one may have hoped for even higher precision, this is actually a decent result given the sophisticated nature and stylistic subtleties of political quotations. The gains over the sim-pler alternatives are statistically significant. This shows that the novel elements in our features model and quotation expansion are indeed decisive for achieving good precision on this difficult clas-sification task. The experimental results also show that our features model are decisive for achieving good pro/con classification preci-sion using classification methods such as SVM and LDA. The over-all winner, however, in this comparison is the LM-based method with rich features and quotation expansion.
Many previous works on sentiment analysis rely on training clas-sifiers with annotated training data [10, 7]). In our work, we follow an alternative approach of using language models (LMs) to clas-sify opinions, thus reducing the dependence on annotated training data. Other studies of sentiment analysis focus on detecting text polarity given that the classified documents are part of social media like online debates, blogs and twitters [4, 1]. These studies rely on the sentiment orientations of the features of the text (positive and negative), which are rich in these types of media. Some other studies further consider the linkage among documents to detect po-larities [4]. However positive/negative features and hyperlinks are very sparse in news media. Prior works addressed the problem of detecting general perspectives (e.g. ideologies or political parties) of given texts, e.g., Republicans versus Democrates, or Palestinian versus Israeli [12, 9]. These works use statistical methods and train classifiers on a set of perspective-annotated documents in order to learn a set of discriminative n-grams of each perspective. This re-quires manually annotated documents, which is not practical if we move to a finer level of granularity.

Coarse-grained classification is taken further by the work of [6], which aims to annotate political speeches and parliament debates, but does not deal with fine-grained topics. In [8] semantic tax-onomies are used to identify aspects of topics, and analyzes opin-ions on these aspects rather than topics as a whole. This applies to opinion mining on politicians (with aspects such as Vietnam war, Watergate affair, etc.), but it does not address the polarity issue of these opinions. In [5] opinions at the collection level are ex-amined with each collection on a topic coming from a different perspective. A latent topic model is devised to discover the com-mon topics across all the perspectives. For each topic, the opinions from each perspective are summarized. A related task is addressed in [3]. They focus on predicting the sentiment polarity of com-ments on blog postings. Their approach models mixed-community responses, to identify the topics and the responses they evoke in different sub-communities. In contrast to all of the above, our work finds the underlying fine grained topics and the opinion on each topic at the quotation-level.
We addressed the problem of automatically classifying quota-tions about political debates which appear in news media and online forums, by politicians or other opinion makers, into fine-grained controversial topics and a pro/con polarity for each topic. We pro-posed a topic/polarity classification approach that maps quotations onto one or more topics in a category system of political debates on fine-grained topics. Our method builds on the estimation of statisti-cal language models on a variety of advanced features designed to overcome the brevity of quotations. We showed the effectiveness of our techniques through systematic experiments on more than 1000 quotations on a variety of topics. Our best method achieved a pre-cision of about 74%. [1] L. Adamic and N. Glance. The political blogosphere and the [2] R. Awadallah, M. Ramanath, and G. Weikum. Harmony and [3] R. Balasubramanyan, W. Cohen, D. Pierce, and [4] C. Burfoot, S. Bird, and T. Baldwin. Collective classification [5] Y. Fang, L. Si, N. Somasundaram, and Z. Yu. Mining [6] R. Kaptein, M. Marx, and J. Kamps. Who said what to [7] B. Liu. Sentiment analysis and subjectivity. In Handbook of [8] Y. Lu, H. Duan, H. Wang, and C. Zhai. Exploiting structured [9] D. Nguyen, E. Mayfield, and C. Ros X . An analysis of [10] B. Pang and L. Lee. Opinion mining and sentiment analysis. [11] S. Ponzetto and R. Navigli. Knowledge-rich word sense [12] X. Zhou, P. Resnick, and Q. Mei. Classifying the political
