  X  X hat other people think X  has always been an important piece of information for most of us during the decision-making process [15]. The goal of opinion mining is to extract and summarize opinionated contents from news, blogs, comments and re-tions, such as marketing intelligence, government policy making and so on. 
Opinion holder and target extraction are fundamental task of opinion mining, and a lot of literatures have been published in this area [1,2,10]. However, the accuracy of cause Chinese sentences are usually very long and often connects two or more self-complete sentences together without any in dicating word or punctuation. Therefore, the parsing approach will bring in more er rors and noisy for the extraction models and methods. 
Sentence compression is a recent framework that aims to select the shortest subse-quence of words that yields an informative and grammatical sentence [12]. Most previ-shows a compressed sentence addressing the conventional content-oriented compression task (CS) and an example of opinion-oriented compressed sentence (OOCS). 
From Table 1 we can see that the conventional compression task can get the core sentence compression (OOSC) is to eliminate non-opinionated sub-sentence and re-tain the opinionate part of the sentence. The  X  X athered X  part of the sentence is deleted OOCS yield a shortest opinionated and grammatical sentence, which paves the way for the further opinion mining steps. 
The traditional sentence compression method could not meet the goal of opinion-sentence. Moreover, it sometimes ignores the opinion words which are very important for OOSC task. On the other hand, an ideal opinion-oriented compressed sentence also eliminate non-opinionate part of the se ntence. It provides a brief summary of the opinion expressed in the sentence, and the shorten sentence brings in a higher parsing accuracy, which will facilitate opinion extraction task in the next mining steps. 
Until recently, many papers have been published for sentence compression using both supervised and unsupervised method. However, there are still important chal-lenges to be tackled for opinion-oriented sentence compression: (1) The Chinese sentences in news articles are usually very long, which brings (2) The opinion-oriented sentence compression is lack of parallel corpus; (3) The compression approach should not only consider the term information In this paper, we propose a scoring based opinion-oriented compression method for Chinese news sentences. To best of our knowledge, this is the first paper that seeks to ized as follows. Section 2 introduces the related work on opinion mining and sentence compression. Section 3 presents the proposed dynamic programming approach for opinion-oriented Chinese sentence compress ion. Section 4 provides experimental results on Chinese news datasets, including automatic evaluations and human subjec-tive evaluations. Finally we present concluding remarks and future work in Section 5. Recently, opinion mining has become a hot topic in research area. For sentiment clas-overall sentiment expressed in them. Turney et al. [18] measured the strength of sen-timent by the difference of the Pointwise Mutual Information (PMI) between the given phrase and the seed words. In [14], Pang et al. employed three machine learning approaches (Naive Bayes, Maximum Entropy, and Support Vector Machine) to label the polarity of IMDB movie reviews. 
Extracting opinion holders, targets and expressions from documents have attracted many researchers X  attentions [3,4,10]. Choi et al. presented an integer linear pro-Performance of the system could be further improved when a semantic role labeling algorithm is incorporated [2]. Wu et al. proposed a novel phrase dependency parsing approach for mining opinions from product reviews, where it converted opinion min-them [20]. Although these methods have achieved relatively high extraction accuracy, they are still sensitive to the parsing errors, which set up obstacles for extracting opin-ions from long Chinese news sentences. Sentence compression could be usefully employed in wide range of applications. For example, it can be used to automatically generate headline of an article [6]; Other mobile phones or PDAs, and producing audio scanning devices for the blind [7]. However, there is no study on how sentence compression can improve the perform-ance of opinion mining in the previous work. 
Most existing studies relied on a parallel corpus to learn the correspondences be-tween original and compressed sentences. Typically sentences are represented by estimate the parameters in the score function of a possible compression. A variety of models have been developed, including but not limited to the noisy-channel model [11], support vector machines [13] and large-margin learning [4]. However, for opin-ion-oriented Chinese sentence compression, no existing parallel corpus can be directly used to training these models. 
An algorithm making limited use of training data was proposed by Clarke and La-pata [3] for English text. Their model searched for the compression with highest score according to the significance of each word, the existence of Subject-Verb-Object structures and the language model probability of the resulting word combination. The weight factors to balance the three measurements were experimentally optimized by a parallel corpus or estimated by experience. 3.1 Problem Definition Sentence compression is defined as follows: given a sequence of words W = w 1 w 2 ... w N M &lt; N ), that is a compressed version of W . To take the sentences in Table 1 as an ex-ample, we have the following explanations in Figure 1. 
We can see from Figure 1 that the original sentence can be divided into two self-completed sub-sentence. The conventional compression method can eliminate some modifier words and location words which are less important information for the origi-nal sentence. However, since the Chinese sentence are usually very long, not all parts  X  X eople gathered in front of theological school X , which does not include any opinion of  X  X eople X . For opinion-oriented sentence compression, we intend to get the reduced sentence as shown in Figure 2. 
In Figure 2, the factual part of the sentence is eliminated, and the opinionated part is retained. We can see that the opinion-oriented compression also preserves the modifier  X   X  X  X   X (strongly), which is eliminated in the conventional compression method. From the compressed version of the sentence, the opinion is expressed more related information from the compressed sentence. Here we give the formal definition of opinion-oriented sentence compression: Definition 1. (Opinion-Oriented Sentence Compression). Given a sequence of words and grammatical sentence but also eliminates the factual part and preserves the opin-ionated part as much as possible. In the Section 3.2, we introduce a score function for opinion-oriented Chinese sen-tence compression. 3.2 Score Function for Opinion-Or iented Chinese Sentence Compression Notice that there is no existing parallel corpus for opinion-oriented Chinese sentence compression. To alleviate this problem, we employ a weakly supervised word dele-tion algorithm based on score function to compress a sentence. 
Inspired by the work of Clark and Lapata [3], the score function in our approach is defined as a measure indicating the appropriateness of a compressed sentence. Based on the definition of opinion-oriented sentence compression, a set of words maximiz-gramming technique. The score function is defined as the sum of word significance opinion score O of each word in the original sentence. 
The score function of the sentence is given by: O , where the value can be set manually or optimized using a small amount of training data. Word Significance Score. The word significance score I measures the relative im-portance of a word in the original sentence. Given a word w i in the original sentence, the function I is defined as: in all the documents and F A is the sum of F i in all the documents ( i
In Clark and Lapata X  X  work, the authors only focused on nouns and verbs as poten-verbs equally for calculating the word significance score. ensuring that the compression results remain grammatical. We apply n-gram probabil-ity estimate the linguistic score of each word in the sentence. Opinion Score. Based on the assumption that the opinion words should have more opportunity to be preserved during the compression approach, we assign an opinion score to each word which belong s to a predefined dictionary. all other words. 3.3 Compression Generation and Selection Based on the score function, we employ a dynamic programming algorithm to find sentence, the dynamic programming algorithm searches different M value to maxi-mize the score function, which generates a list of candidate compressed sentence with different length. Then the final selection of the best candidate compression is a trade-D ( V M ) is used to measure the quality of the candidate compression: The best compression V is selected with the highest density score: where  X  and  X  are the minimum and maximum compressed ratio of the original sen-tence, which restrict the final length of the compressed sentence. 4.1 Experiment Setup tence compression. Since there is no existing parallel corpus available, we build a new evaluation dataset for opinion-oriented Chinese sentence compression. The original data come from NTCIR Multilingual Opinion Mining Task, which consist of about 30,000 documents and totally 3,120,000 sentences. From these documents, we ran-domly pick up 100 opinionate sentences on the topic of  X  Iraq War  X  for the compres-sion task. One annotator majoring in opinion mining was asked to manually delete the preserved words should not only contain the most important information, but also compressed sentence should remain grammatical. Finally, this annotation result is set to be the gold standard for the opinion-oriented Chinese sentence compression. 
Recall that the score function for sentence compression has three components: the significance score, linguistic score and opinion score. The significance score was trained using the NTCIR MOAT corpus which total contain 30,000 documents. The linguistic score was calculated using a trigram language model. In this paper, we use language model [21]. 
The sentiment lexicon is the fundamental tools for estimating the opinion score of each word in the sentence. Our sentiment lexicon is built based on following re-sources: (a) The Lexicon of Chinese Positive Words and the Lexicon of Chinese Negative Words; (b) The opinion word lexicon provided by National Taiwan Univer-sity (NTU); (c) Sentiment word lexicon and comment word lexicon from HowNet [9]. The lexicon is manually verified. Totally, 14,201 positive words, 17,372 negative words and 478 neutral words are obtained. 4.2 Evaluation Methods Both automatic evaluation and human subjective evaluation are employed to measure the correctness of the compressed sentences generated by the proposed approach. The parsing based automatic evaluation methods are not appropriate for the long Chinese sentence, because errors may be brought in during the parsing approach for the origi-nal and compressed sentence. For automatic evaluation, we measure each sentence compression method using BLEU scores [16]. BLEU scores are firstly proposed for evaluating machine transla-sentences [8,19]. A BLEU score is defined as the weighted geometric average of n-gram precisions with length penalties. 4-gram precision and uniform weights are used for the BLEU scores in this paper. 
For human subjective evaluation, one native Chinese speakers majoring in opinion scale. Notice that the student has placed more emphasis on whether the opinion in-formation is preserved in the final compressed result. At the same time, the semantic and grammatical correctness is also considered during the evaluation. 4.3 Experiment Results Our goal of the experiments is to answer this question: whether the proposed algo-rithm can meet the need of the opinion-oriented sentence compression task. 
We check the compression rate of the proposed method and the gold standard, as shown in Table 2. In Table 2, gold standard means the human generated compressed sentence; OOSC denotes the opinion-oriented sentence compression method; TSC denotes the conventional sentence compression method. In this paper, for conven-opinion score. The compression rate is the ratio of the number of Chinese characters in a compressed sentence to that in its original sentence. For OOSC method, there is no compression rate limitation parameter except  X  and  X  , which restrict the length of the compressed sentence. In this paper, we set  X  =0.2 and  X  =0.5 for the proposed algo-rithm. In this way, we can limit the range of the compressed sentence range from 20% to 50% of the original sentence. From Table 2, we can see that the human generated results are about 60% size of the original sentences. The compression rate of proposed method OOSC is under fifty percent of the original sentence. 
To check the effectiveness of the proposed algorithm in compressing Chinese sen-tence, we evaluate our method using BLUE scores, as shown in Table 3. And the human O const =0.01. We can see from Table 3 and Table 4 that the proposed OOSC method outperforms the TSC method which does not consider the opinion information. Compared to Web comments and reviews, the Chinese news articles usually have Conventional sentence compression methods can compress a sentence without chang-ing its meaning. However, these methods do not distinguished opinionated informa-tion from factual information. In this paper, an opinion-oriented Chinese sentence compression method is proposed. The weight of each word in original sentence is measured by its information significance, linguistic consistence and opinion score. A dynamic programming algorithm is employed to find the best combination of the words in the original sentence. Automatic evaluations and human subjective evalua-from the long Chinese sentences. 
Future work will consider more linguistic features and constraints to improve the grammatical consistence of the compressed sentence. We also intend to further evaluate the compressed results by applying opinion holder and target extraction algorithms. Acknowledgments. This work is partially supported by National Natural Science Foundation of China (No.60973019, 60973021), National 863 High Technology De-velopment Program of China (2009AA01Z131, 2009AA01Z150) and HKSAR ITF (No. GHP/036/09SZ). The authors would like to thank Donghao Fang for developing parts of algorithms in this paper. 
