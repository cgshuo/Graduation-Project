 The binary classification of examples x is usually performed with recourse to the mapping  X  y = y minimizes the expected value of the loss, known as minimum co nditional risk. Although tremen-dously successful, these methods have been known to suffer f rom some limitations, such as slow can be attributed to the loss functions  X  ( ) on which the algorithms are based. These are convex too difficult to handle from a computational point of view.
 reduce to the problem of minimizing a Bregman divergence. We derive equivalence results, which both the optimal f  X  , and the functional form of the minimum risk are immediately pined down. of the Bregman divergence to be optimized.
 boosting algorithm, denoted SavageBoost, by combination o f the new loss and the procedure used by Friedman to derive RealBoost [3]. Experimental results s how that the new boosting algorithm is indeed more outlier resistant than classical methods, such as AdaBoost, RealBoost, and LogitBoost. P
X ( x ) the classification risk is R ( f ) = E minimized by the Bayes decision rule. Denoting by  X  ( x ) = P mapping from X to R . The minimization of the 0-1 loss requires that 0-1 loss can be written as a function of this quantity However, this minimization is usually difficult. Many algor ithms have been proposed to minimize alternative risks, based on convex upper-bounds of the 0-1 loss . These risks are of the form where  X  ( ) is a convex upper bound of  X  conditional risk E where we have omitted the dependence of  X  and f on x for notational convenience. Various authors have shown that, for the  X  ( ) of Table 1, the function f  X  holds for any f  X  has derivative  X   X  (0) = 0 [5].
 While learning algorithms based on the minimization of (4), s uch as SVMs, boosting, or logistic Table 1: Machine learning algorithms progress from loss  X  , to inverse link function f  X  tiable, then C  X  where is the Bregman divergence of the convex function F . The second property provides an interesting form of (4) is the best way to elicit the posterior probabilit y  X  ( x ) . The problem is formulated as follows. The expected reward is Savage asked the question of which functions I  X   X  =  X ,  X   X  . These are the functions such that then showed that (10) holds if and only if Defining the loss of the prediction of  X  by  X   X  as the difference to the maximum reward Least squares  X  4(1  X   X  ) 2  X  4  X  2  X  4  X  (1  X   X  ) Log. Regression log  X  log(1  X   X  )  X  log  X  + (1  X   X  ) log(1  X   X  ) it follows that one of minimum Bregman divergence with  X  . Savage went on to investigate which functions J (  X  ) the loss only depends on the difference  X   X   X   X  , and the admissible J are and the admissible J are of the form to Savage X  X  procedure for probability elicitation. Both pr ocedures reduce to the search for fication of F (  X  ) = J (  X  ) , from which the conditional rewards I learning algorithms start from the loss  X  ( ) . The conditional risk C respect to f , so as to obtain the minimum conditional risk C  X  is identical to solving (16) with F (  X  ) =  X  C  X  by deriving the conditional reward functions associated wi th each of the C  X  done with (11) and (12) and the results are shown in Table 2. In all cases I I  X  1 (  X  ) =  X   X  (  X  f  X   X  (  X  )) as the minimization of (4), is more difficult. It requires tha t the I helps to think of the latter as an inverse link function. Or, a ssuming that f  X  it is natural to consider link functions which exhibit the fo llowing symmetry functions as symmetric, and show that they impose a special s ymmetry on J (  X  ) . Theorem 1. Let I Then (17) and (18) hold if and only if In this case, the symmetry of (19), the expected reward of (9) can be writte n in the  X  X achine learning form X  result to the case where J (  X  ) =  X  C  X  Corollary 2. Let I ously differentiable J (  X  ) =  X  C  X  and f with Note that there could be many pairs  X , f  X  X ins down X   X  , according to (25). This is the case of the algorithms in Tabl e 1, for which C  X  and f  X  in the table. view X : start from the loss  X  ( v ) , and find 1) the inverse link function f  X  tional risk, and 2) the value of this risk C  X  tion ( f  X  equivalent, since they both reduce to the search for the prob ability estimate  X   X   X  of (16). Figure 1: Loss function  X  ( v ) (left) and minimum conditional risk C  X  to the algorithms themselves) has been to identify new J functions, namely those associated with perspective is interesting because it enables the derivati on of new  X  functions. The main observation is that, under the customary specificat ion of  X  , both C  X  indirect selection of a link function ( f  X  approximation to the minimum conditional risk of the 0-1 loss , C  X  approximations associated with the existing algorithms ar e shown in Figure 1. The approximation C  X  (  X  ) controlling the link function f maintaining its performance constant, in terms of the expec ted risk.
 We demonstrate this point, by proposing a new loss function  X  . We start by selecting the minimum conditional risk of least squares (using Savage X  X  version w ith k =  X  l = 1 , m = 0 ) C  X  logistic link function (classically used with logistic reg ression) f  X  is small (the margin), but quickly becomes constant as v  X  X  X  X  . This is unlike all other previous  X  of (9) only depends on the convexity of the functions I the symmetries of (22) and (19) hold, and  X  is selected according to (25), the selection of C  X  Algorithm 1 SavageBoost
Input: Training set D = { ( x example x , and number M of weak learners in the final decision rule.

Initialization: Select uniform weights w (1) for m = { 1 , . . . , M } do end for
Output: decision rule h ( x ) = sgn [ P M not matter. We have hypothesized that classifiers designed with (26) sho uld be more robust than those derived using the procedure proposed by Friedman to derive RealBoos t [3]. At each iteration the algorithm searches for the weak learner G ( x ) which further reduces the conditional risk E G ( x ))) | X = x ] of the current f ( x ) , for every x  X  X  . The optimal weak learner is where and where P We refer to the algorithm as SavageBoost , and summarize it in the inset. We compared SavageBoost to AdaBoost [9], RealBoost [3], and LogitBoost [3]. The latter is gen-erally considered more robust to outliers [8] and thus a good candidate for comparison. Ten binary Disorder set. Table 4 shows the number of times each method pr oduced the smallest error (#wins) over the ten data sets at a given contamination level, as well as the average error% over all data sensitivity to outliers [1]. Among the previous methods Ada Boost indeed performed the worst, fol-lowed by RealBoost, with LogistBoost producing the best res ults. This confirms previous reports Ada and RealBoost at all contamination levels, including 0% contamination. LogitBoost achieves comparable results at low contamination levels ( 0% , 5% ) but has higher error when contamination is significant. With 40% contamination SavageBoost has 6 wins, compared to 3 for LogitBoost and, on average, about 6% less error. Although, in all experiments, each algorithm wa s allowed RealBoost. We attribute fast convergence to the bounded nat ure of the new loss, that prevents so future.

