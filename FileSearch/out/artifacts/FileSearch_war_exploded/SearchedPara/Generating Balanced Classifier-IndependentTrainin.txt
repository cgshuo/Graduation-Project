 Supervised learning algorithms can provide promising solutions to many real-world problems such as text classification, anomaly detection and information security. A major limitation of supervised learning is the difficulty in obtaining labeled data to train predictive models. Ideally, one would like to train classifiers on diverse labeled data representative of all classes. In many domains, such as text classification or security, there is an abundant amount of unlabeled data, but obtaining a representative subset is challenging: data is typically highly skewed and sparse.

There are two widely used approaches for selecting data to label X  X andom sampling and active learning. Random sampling, a low-cost approach, produces a subset of the data with a similar distribution to the original data set, producing skewed training data for unbalanced data. Training with unbalanced labeled data yields poor results as reported in recent w ork on the effect of class distribution on learning and performance degradation [1 X 3]. Active learning produces training data incrementally by identifying the most informative data to label at each phase [4 X 6]. However, active learning requires knowing the classifier in advance, which is not feasible in many real applications, and requires costly re-training at each step.

In this paper, we present new strategies to generate training samples from unlabeled data to overcome limitations in random and existing active sampling methods. Our core algorithm is an iterative method, in which we generate a small fraction (e.g., 10%) of the desired training set each iteration, indepen-dently of both the original data distribution as well as the target classifier. More specifically, we first label a small nu mber of randomly selected samples and subsequently apply semi-supervised clustering to embed prior knowledge (i.e., labeled samples) to produce clusters approximating the true classes [7 X 9]. We then estimate the class distribution of the clusters, and increase the balancedness of the training sample via biased sampling.

A simplistic strategy for biased sampling would be to assume that the class distribution of a cluster is the same as the distribution of labeled samples in the cluster, and to draw samples proportionally to the estimated class distri-butions. However, this assumption does not hold in early iterations when the number of labeled samples is small, and there is high uncertainty about the class distributions. We present two hybrid approaches to address this issue that perform well in practice. The first appr oach is to combine the estimated class distribution-based sampling and random sampling. As the number of labeled samples increases, we decrease the influence of random sampling favoring the es-timation based on previously labeled samples. The second approach is for cases where additional domain knowledge is available. We use the domain knowledge to estimate class distributions. Domain knowledge may come in many forms, such as conditional probabilities and correlation, e.g., there is a heavy skew in the geographical location of servers hosting malware[10]. We perform a similar transition between the domain knowledge-based density estimation and previ-ously labeled sample-based estimation.

We have validated these strategies on 1 4 data sets from the UCI data reposi-tory [11] as well as a private data set authorizing users to systems (i.e., labeled grant and deny ). These data sets reflect a range of parameters: some are bal-anced and others highly skewed; and some have binary classes while others have multiple classes. We compare our strategies to random sampling as well as un-certainty based active sampling based on three classifiers: Naive Bayes, Logistic Regression, and SVM. The experiments show that, for highly skewed data sets, our sampling algorithm produces substantially more balanced samples than ran-dom sampling. For mildly skewed data sets, our method results in about 25% more minority samples. Similarly, our algorithm performs better than uncer-tainty sampling based methods for highly skewed samples, producing more than 20% more minority samples on average. For mildly skewed data sets, our algo-rithm X  X  results are not statistically different from uncertainty sampling based on logistic regression. Given that uncertainty sampling requires one to fix the classifier to be trained and is much slower, we conclude that our algorithm is always preferable to both random and uncertainty based sampling. We test the domain knowledge based strategy on the a ccess control permission datasets. Our result show that, in most cases, the addition of domain knowledge significantly improves the convergence of the sampling so we can produce balanced sample sets more quickly.

The quality of training data can best be evaluated by the performance of classifiers trained on this data. We have compared various sampling strategies by training and testing a range of classifiers. Our tests show that the classifiers built with our training data outperform other classifiers in most of the experimental scenarios and produce more consistent performance. Further, our classifiers often outperform uncertainty sampling on AUC and F1 measures, even when sampling and classification used the same classifier. The experimental results confirm that our sampling methods are very generic and can produce highly balanced training data irrespective of the underlying data distribution and the target classifier. There is an extensive body of work on generating  X  X ood X  training data sets. A common approach is active learning, which iteratively selects informative sam-ples, e.g., near the classification border, for human labeling [6, 12 X 14]. The sam-pling schemes most widely used in active learning are uncertainty sampling and Query-By-Committee sampling [13, 15, 16]. Uncertainty sampling selects the most informative sample determined by one classification model, while QBC sampling determines informative samples by a majority vote. A major problem with active learning is that the update process is very expensive as it requires classification of all data samples and retraining of the model at each iteration. This cost is prohibitive for large scale problems. Techniques such as batch mode active learning [17, 18] have been prop osed to improve the efficiency of uncer-tainty learning. However, as the batch s ize grows, the effectiveness of active learning decreases [18 X 20].

Another approach is re-sampling, i.e., over-and under-sampling classes [21, 22], however this requires labeled data. Recent work combines active learning and re-sampling to address class imbalance in unlabeled data. Tomanek and Hahn [23] propose incorporating a class-specific cost in the framework of QBC-based active learning for named entity recognition. By setting a higher cost for the minority class, this method boosts the committee X  X  disagreement value on the minority class resulting in more minority samples in the training set. Zhu and Hovy [24] incorporate over-and under-sampling in active learning for word sense disambiguation. Their algorithm uses active learning to select samples for human experts to label, and then re-samples this subset. In their experiments, under-sampling caused negative effects but over-sampling helps increase bal-ancedness. However, both [24] and [23] are primarily designed and applied to binary classification problems for text and are hard to generalize to multi-class problems and non-text domains.

Our approach is iterative like active learning, but it differs crucially in that it relies on semi-supervised clustering instead of classification and selects target samples based on estimated class distribution in each cluster. This makes it more general where the best classifier is not known in advance. Ours is the first attempt at using active learning with semi-supervised clustering instead of classification and thus does not suffer from over-fitting. Furthermore, since most classification methods require the presence of at least two different classes in the training set, there is a challenge in providing the initial labeling sample for active learning; an arbitrary insertion of instances from at least two classes is required. Our method does not have this limitation, and, although not shown in the experiments, performs as well w ith a random initial sample. Our work provides a general framework which is domain independent and can be easily customized to specific domains. Our strategy for generating balanced trai ning sets are describ ed in this section. Section 3.1 presents a hig h level overview of the algorithm, and later sections provide a more formal description and specific instantiations of the key steps and discuss various tradeoffs. 3.1 Overview Given an unlabeled dataset with unknown class distribution, potentially skewed, our goal is to produce balanced labeled samples for training predictive models. Formally, we can define the balanced training set problem as follows: Definition 1. Let D be an unlabeled data set containing classes from which we wish to select T , a subset of D of size N .Let L ( T ) be the labels of the training data set T , then the balancedness of T is defined as the distance between the label distribution of L ( T ) and the discrete uniform distribution with classes, i.e., D ( Uniform ( ) Multi ( L ( T ))). The balanced training set problem is the problem of finding a training data set that minimizes this distance. If we know the class labels in a given set, then we can use over-and under-sampling to draw balanced sample set [21, 22, 25]. However, the class labels are not known, so instead we must use a series of approximations to approach the results of this ideal solution. We apply an i terative semi-supervised clustering algorithm to estimate the class distribution in the unlabeled data set and guide the sample selection to produce a balanced set. In each iteration, the algorithm draws a batch of samples ( B ), and domain experts provide the labels of the selected samples. The labeled sample s are used in subsequent iterations.
Algorithm 1 is a high level description of our strategy. It takes three inputs: D , an unlabeled data set; , the number of target classes in D ;and N ,thenumber of training samples to generate. We note that the value of the input parameters are previously determined in most applic ations. The number of samples to select in each iteration, B , can be determined automatically based on D and N ,or users can optionally set the batch size as an input parameter.

To start, we select B samples randomly and obtain the labels. Then, a semi-supervised clustering algorithm is applied to embed the labels obtained from the prior steps into the clustering process (Section 3.2), which can be used to approximate the class distributions in the clusters. The key intuition behind the process is that we want to extract more sa mples from clusters which are likely to increase the balancedness of the overall training set. Our algorithm tries to infer the class distribution of each cluster and use this to over-or under-sample. Section 3.3 describes key details of the class density estimation process, the various tradeoffs and their influence on the ultimate results. Once we determine how much to sample from each cluster, we obtain diverse samples using maximum entropy sampling (Section 3.5). We note that there is an implicit secondary optimization of maximizing the entropy of the sampled points, H ( T ), which is the byproduct of the real objective, maxi mizing the performance of a classifier trained on T . 3.2 Semi-supervised Clustering Semi-supervised clustering is a technique which incorporates existing informa-tion into clustering. A number of approaches have been proposed to embed constraints into existing clustering algorithms [9, 26]. We explore two differ-ent strategies: a distance metric techni que for multi-variate numeric data and a heuristic to add class labels in the feature set for categorical data. We use Rele-vant Component Analysis (RCA) [7] for distance metric-based semi-supervised clustering, This is a Mahalanobis metric technique which finds a new space with the most relevant features in the side information. It learns a global distance met-ric parameterized by a transformation matrix  X  C to capture relevant features in the labeled sample set. It maximizes the s imilarity between the original data set X and the new representation Y constrained by the mutual information I ( X, Y ). By projecting X into the new space through transformation Y =  X  C  X  1 2 X ,two projected data objects, Y i ,Y j , in the same connected component have a smaller distance.

Here, we sketch the steps to compute the  X  X ithin-chunklet X  covariance matrix (transformation matrix),  X  C .Givenadataset X = { x i } N i =1 and a labeled sample set L  X  X , suppose u connected components (i.e., chunklets) M = { M j } u j =1 are obtained based on L , which satisfies X = acomponent M j be denoted as { x ji } | M j | i =1 for 1  X  j  X  u . Then, the covariance matrix  X  C is defined by Equation 1, where m j is the centroid of M j . After projecting the data set into a new space using RCA, the data set is re-cursively partitioned until all the clu sters are smaller tha n a predetermined threshold, max cluster . Algorithm 2 summarizes our s emi-supervised clustering algorithm using RCA.

Algorithm 2. Semi-supervised clustering algorithm to divide a data set into balanced clusters
The RCA algorithm makes several assumptions regarding the distribution of the data. Primarily, it assumes the data is multivariate normally distributed, and, if so, produces the optimal result. It has also been shown to perform well on datasets when the normally distributed assumption fails [7], including many of the UCI datasets used in this work. However, it is not known to work well for Bernoulli or categorical distributed da ta, such as the access c ontrol datasets, where it produces a marginal improvement , at best. Instead, we choose a simple method by augmenting the feature set with labels of known samples, i.e., F L , and assigning a default feature value, or holding out feature values, for unlabeled samples. For example, if we have class labels, we will add new binary features. If the sample has class j , we will assign feature j a value of 1, and all other label features a zero. Unlabeled samples are a ssigned a feature corresponding to the prior, the fraction of labeled samples with that class label. As before, we use the recursive binary clustering technique des cribed previously to cluster the data. We find that this simple heuristic produces good clusters and yields balanced samples more quickly for categorical data. 3.3 Determine the Optimal Number to Samples from Each Cluster Once we have clustered the data, the key step is to estimate the class density in the clusters and use this information to perform biased sampling to increase the overall balancedness in the sample set. W e assume that the semi-supervised clus-tering step has produced biased clusters allowing us to approximate a solution of drawing samples with known classes.

The first approach is to assume that the class distribution of the cluster is exactly the same as the class distribution of the labeled samples in this cluster. This is based on the optimistic assumption that the semi-supervised clustering works perfectly and groups together el ements similar to the labeled sample. First, we determine how many samples we wish to draw from each class in this iteration from the total B samples to draw. Let j i be the number of instances of class j sampled after iteration i ,and  X  j i be the normalized proportion of samples with class label j , i.e.,  X  j i = j i where is the number of classes. Next, we use the estimated class distribution in each cluster to determine the appropriate number of samples to draw from each class. Let  X  j i be the probability of drawing a sample with class label j from the previously labeled subset of cluster i . By our assumption, this is exactly the probability of drawing a sample with class label j from the entire cluster. To sample n j samples with label j ,wedraw n j  X  j i k k is the number of clusters. Another strategy is to draw all n j samples from the cluster with the maximum probability of drawing class j , however our method selects a more representative subset, a nd we can obtain good results even if our estimation of cluster densities is incorrect and reduces later classifier over-fitting.
In early stages of the iteration process, where there are few labeled samples, this approach does not work well. We use a hybrid approach where we select a certain percentage of B samples based on the estimates of class distribution and select remaining samples randomly from all clusters. We increase the influence of labeled samples over time as we obtain more labeled samples and thus better estimates on class distribution. Let B L be the number of samples to select based on labeled samples, and B r be the number of samples to select randomly. Then, we compute B L =  X   X B and B r =(1  X   X  )  X B using a sigmoid function  X  = 1 1+ e  X   X t , where t is the iteration number and  X  a parameter that controls the rate of mixing. As t increases (i.e., number of labele d samples increases), we decay the influence of random sampling favoring empirical estimates.
 3.4 Leveraging Domain Knowledge In Section 3.3, we presented a hybrid sampling method that combines sampling based on the class distribution of each cluster and random sampling. In many settings, domain experts may have additional knowledge regarding the distri-bution of class labels and correlations with given features or feature values. For instance, in the problem of detecting malicious web sites, there is a heavy skew in geographical location of the web servers [10]. In access control datasets, one can expect correlations between the depar tment of the employee and granted per-missions. This section outlines a method where we can incorporate such domain knowledge to estimate the class distribution within a cluster. We use domain knowledge in the form of a correlation value between a feature and a class label. For example, corr ( Department =20 , class = grant )=+0 . 1. These correlations may be noisy and incomplete, pertaining to only a small number of features or feature values. Without loss of generality, we will only consider binary labels; the technique can readily be extended to non-binary labels.

Given a small number of feature-class and feature-value-class correlations and the feature distribution within a cluster, we can estimate the class density. We leverage some of the ideas from the MYCIN model of inexact reasoning [27]. They note that domain knowledge is often logically inconsistent and non-Bayesian. For example, given expert knowledge that p ( class = grant | Department = 20) = 0 . 6, we cannot conclude that p ( class = grant | Department = 20) = 0 . 4. Further, a na  X   X ve Bayesian approach requires an estimation of the global class distribution, which we assume is not known a priori. Instead, our approach is based on inde-pendently aggregative suggestive evidence and leverages properties from fuzzy logic. The correlations correspond to inference rules (e.g., Department =20  X  class = grant ), where the correlation coefficients are the confidence weights of the inference rules, and the feature den sity within each class is the degree that the given inference rule is fired. We evaluate each inference rule in support (pos-itive correlation) and refuting (negative correlation) the class assignments, and aggregate the results using the Product T-Conorm, norm ( x, y )= x + y  X  x  X  y . We combine evidence supporting and refuting a class assignment using the rule  X  X lass 1 and not class 2, X  and T-Norm for conjunction, f ( x, y )= x  X  (1  X  y ). Finally, we use domain knowledge-based estimates to supplement the empiri-cal estimates B L .Let B d be the number of samples to select based on domain knowledge, then we select B = B L + B d samples in each iteration. As domain knowledge is inexact and noisy, we decay the influence of its estimates over time, favoring the empirical estimates using the sigmoid  X  described in Section 3.3, i.e., B d =(1  X   X  )  X B . 3.5 Maximum Entropy Sampling Finally, given the set of clusters { C i } k i =1 , and the number of samples to select from each cluster, we sample to maximize the entropy of the sample L ( T ). We assume that the data in each cluster follows a Gaussian distribution. For a continuous variable x  X  C i , let the mean be  X  , and the standard deviation be  X  , then the normal distribution N (  X ,  X  2 ) has maximum entropy among all real-valued distributions. The entropy for a multivariate Gaussian distribution [28] is defined as: where d is the dimension,  X  the covariance matrix, and |  X  | the determinant of  X  . Thus, more variation the covariance matrix has along the principal directions, the more information it embeds.

Note that the number of possible subsets of r elements from a cluster C can grow very large (i.e., | C | r ), so finding a subset with the global maximum entropy can be computationally very intensive. We use a greedy method that selects the next sample which adds the most entropy to the existing labeled set. Our algorithm performs the covariance calculation O ( rn ) times, while the exhaustive search approach requires O ( n r ). If there are no previously labeled samples, we start the selection with the two samples that have the longest dis-tance in the cluster. The maximum entropy-based sampling method is presented in Algorithm 3.
 This section presents a performance comparison of our sampling strategies with random sampling and uncertainty based sampling on a diverse collection of data sets. Our results show that our algorithm produces significantly more balanced sets than random sampling in almost all datasets. Our technique also performs much better than uncertainty based sampling for highly skewed sets, and our training samples can be used to train any classifier. We also describe results which confirm the benefits of domain knowledge. 4.1 Evaluation Setup To evaluate the sampling strategies, we selected 14 data sets from the UCI repository [11] and a private data set co ntaining the assignment of access control permissions to users. The data sets span a wide range of parameters and are summarized in Table 1: some are highly skewed while others are balanced, some are multi-class while others are binary.

All UCI data sets are used unmodified except the KDD Cup X 99 set which contains a  X  X ormal X  class and 20 differ ent classes of network attacks. In this experiment, we selected only  X  X ormal X  class and  X  X uess password X  class to create a highly skewed data set. When a data set is provided with a training set and a test set separately (e.g.,  X  X tatlog X ), we combined the two sets. The features in the access control data set are typica lly organization attributes of a user: department name, job roles, whether the employee is a manager, etc. These categorical features are converted to bi nary features. Since, such access control permissions are assigned based on a comb ination of attributes, these data sets are also useful to assess the benefits of domain knowledge.

For each data set, we randomly select 80% of the data to be used as un-labeled data, from which training samples are generated. The remaining 20% of the samples is used to test classifiers trained with the training samples. For uncertainty-based active learning, we use three widely used classification al-gorithms, Naive Bayes, Logistic Regression, and SVM, and these variants are labeled Un Naive , Un LR ,and Un SVM respectively. We used the C-support vector classification (C-SVC) SVM with a radial basis function (RBF) kernel, and Logisitc Regression with RBF kernel. All classification experiments were conducted using RapidMiner , an open source machine learning tool kit [29]. Lo-gistic Regression in RapidMiner only supports binary classification, and thus it was extended to a multi-class classifier using  X  X ne-against-all X  strategy for multi-class data sets [30]. All experimen tal results reported here are the average of 10 runs of the experiments. 4.2 Comparison of Class Distribution in Training Samples We first evaluate the five sampling methods by comparing the balancedness of the generated training sets. For each r un, we continue sampling till the selected training sample contains 50% of the unlabeled samples or we have 2,000 samples, whichever is smaller. The evaluation metrics we use are the balancedness of the training data and the recall of the minority class. As noted above, each run is done with a random 80% of the underlying data set and the results are averaged over 10 runs. We measure the balancedness of a data set as the distance of the sampled class distribution from the uniform class distribution as defined in Definition 2.
 Definition 2. Let X be a data set with k different classes. Then the uniform distribution over X is the probability density function (PDF), U ( X ) ,where U i = , for all i  X  k .Let P ( X ) be a PDF over the classes produced by a sampling method. Then the balancedness of the sample is defined as the Euclidean distance between the distributions U ( X ) and P ( X ) i.e. d = Table 2 summarizes the results of balancedness comparison, and Table 3 shows the recall of minority class for all the da ta sets respectively. Our method produces significantly better results compared to pure random sampling. On KDD X 99 ,our sampling algorithm yields 10x more minority samples on average than random. Similarly for Page Blocks and the access permission data set, our method pro-duces about 2x more balanced samples. For mildly skewed data sets, our method also produces about 25% more minority samples on the average. For the data sets which are almost balanced, random i s the best strategy as expected. Even in this case, our method produces results which are statistically very close to random. Thus, our method is always preferable to random sampling.

Since uncertainty based sampling methods are targeted to cases where the classifier to be trained is known, the rig ht comparison with these methods should include the performance of the resulting classifiers. Further, these algorithms are not very efficient due to re-training at each step. With these caveats, we can directly compare the ba lancedness of the results. For highly skewed data sets, our method performs bette r especially when compared to Un SVM and Un Naive methods. On KDD X 99 , we produce 20x and 2x more minority samples compared to Un Naive and Un SVM respectively, while Un LR performs almost as well as our algorithm. Similarly for Page Blocks , we perform about 20% better than these methods. We note that our method found all minority samples for all 10 split sets for the Page Blocks set. For other data sets, our algorithm shows no significant statistical difference co mpared to these methods on almost all cases and sometimes we do better. Based on these results, we also conclude that our method is preferable to the uncertainty-based methods based on broader applicability and efficiency.

Figure 1 pictorially depicts the performance of our sampling algorithm as well as the uncertainty based sampling for a few data sets to highlight cases where our method performs better. These figures show the distance from uniform against the percentage of sampled data over iterations. The results show that our sam-pling technique consistently converges towards balancedness while there is some variation with uncertainty techniques, which remains true for other data sets as well. Note that the distance increases in Page Blocks and Access Permission data sets after 20% point is because our method exhausted all minority samples. 4.3 Comparison of Classification Performance In this section, we evaluate the quality of the training samples by comparing the performance of classifiers trained on them. We apply the training samples from the five strategies to train the same type of classifiers (Naive, LR, and SVM), resulting in 15 different  X  X raining-evaluation X  scenarios. Due to space limitations,wepresentinTable4theAUCandF1-measureforbinaryclassdatasets.
We expect the performance of the uncer tainty sampling methods paired with their respective classifier, e.g., Un SVM with SVM and Un LR with Logistic Regression, to perform best. We observe this behavior on KDD and PIMA, but the off-diagonal entries for uncertainty based sampling show poor results. However, on other datasets such as Breast Cancer and SPECT data sets, our approach outperforms the competing uncertainty sampling. Furthermore, our method performs well consistently across all classifiers without being biased to a single classifier and at reduced computation cost. 4.4 Impact of Domain Knowledge The access control permission data sets are used to evaluate the benefit of ad-ditional domain knowledge given as a correlation of a user X  X  attributes (e.g., department number, whether she is a ma nager, etc.) and the granted permis-sion. Our evaluation of sampling with domain knowledge shows that domain knowledge (almost) always helps. There are a few cases where adding domain knowledge negatively impacts performance as shown in Fig 2(b). However, in most cases, domain knowledge substantially improves the convergence of the al-gorithm. The example depicted in Figur e 2(a) is typical of the access control datasets. Since such domain knowledge is mostly used in the early iterations, it significantly helps speed up the convergence.
 In this paper, we considered the problem of generating a training set that can optimize the classification accuracy and also is robust to classifier change. We confirmed through experiments that our method produces very balanced train-ing data for highly skewed data sets and outperforms other methods in correctly classifying the minority class. For a balanced multi-class problem, our algorithm outperforms active learning by a large margin and works slightly better than random sampling. Furthermore, our algorithm is much faster compared to ac-tive sampling. Therefore, the propose d method can be successfully applied to many real-world applications with highly unbalanced class distribution such as malware detection or fraud detection. In future work, we plan to apply kernel methods for semi-supervised clustering which can discover clusters with non-linear boundaries in the original space to better fit nonlinearly separable data. Acknowledgement. This research is continuing through participation in the Anomaly Detection at Multiple Scales (ADAMS) program sponsored by the U.S. Defense Advanced Research Projects Agency (DARPA) under Agreement Number W911NF-11-C-0200.

