 Privacy-preserving data mining (PPDM) is an emergent re-search area that addresses the incorporation of privacy pre -serving concerns to data mining techniques. In this paper we propose a privacy-preserving (PP) Cox model for sur-vival analysis, and consider a real clinical setting where t he data is horizontally distributed among different instituti ons. The proposed model is based on linearly projecting the data to a lower dimensional space through an optimal mapping obtained by solving a linear programming problem. Our ap-proach differs from the commonly used random projection approach since it instead finds a projection that is optimal at preserving the properties of the data that are important for the specific problem at hand. Since our proposed ap-proach produces an sparse mapping, it also generates a PP mapping that not only projects the data to a lower dimen-sional space but it also depends on a smaller subset of the original features (it provides explicit feature selection ). Real data from several European healthcare institutions are use d to test our model for survival prediction of non-small-cell lung cancer patients. These results are also confirmed us-ing publicly available benchmark datasets. Our experimen-tal results show that we are able to achieve a near-optimal performance without directly sharing the data across differ -ent data sources. This model makes it possible to conduct large-scale multi-centric survival analysis without viol ating privacy-preserving requirements.
 I.2 [ Artificial Intelligence ]: Miscellaneous Algorithms, Theory, Performance Privacy-Preserving Data Mining, Cox Regression
Privacy-preserving data mining (PPDM) is a research area that focuses on the incorporation of privacy preservation methods into data mining techniques (e.g., [1]). We are par-ticularly interested in a scenario where the data is horizon -tally distributed among different entities or parties. In th e medical domain this means that there exist several medical institutions (such as hospitals, clinics, etc.) and each on e provides a database containing a complete (or almost com-plete) subset of item sets (patients). An efficient PPDM algorithm should be able to process the data from all the sources and learn data mining/machine learning models that take into account all the information available without sha r-ing private information among the sources. The ultimate goal of a PPDM model is to perform similarly or identically to a model constructed by having access to all the original data (and in distributed scenarios) at the same time/locati on.
There has been recent interest on the incorporation of elec-tronic health records (EHR) in medical institutions world-wide. There is a general belief that the availability of EHR will have several significant benefits for health systems acr oss the world, including improvements in the quality of care (e.g., by tracking performance-based clinical measures), in-creases in the accuracy of insurance reimbursement systems , development of more advanced clinical, computer assisted diagnosis (CAD) tools, etc.

As a consequence, the number of hospitals storing large amounts of data has been increasing. This data can be used to build predictive models to assist doctors in the medical d e-cision process for treatment, diagnosis, prognosis among o th-ers. Ideally, the data from multiple institutions can be ag-gregated with the purpose of creating models with a higher statistical significance. However, sharing the data across in-stitutions becomes a difficult and tedious process that also involves considerable legal and economic burden on the in-stitutions sharing the medical data.

In this paper we propose a novel privacy-preserving tech-nique based on affine projections applied to learn survival predictive models. The model is based on the Cox regres-sion for survival analysis, and we apply it for non-small-ce ll lung cancer (NSCLC) patients treated with (chemo) radio-therapy. The real data is collected from patients treated on three European institutions in two different countries (the Netherlands and Belgium). The framework we are describ-ing in this paper allows to design/learn improved predictiv e models that perform better than the individual models ob-tained by using data from only one institution at a time. Next, we highlight the main contributions of this paper:
The criterion is based on our method for constructing sparse projection matrices [16]. In this paper, this method is related and adapted to the privacy-preserving problem. One of its key advantages is that it builds a data repre-sentation that maintains the properties that are important for the problem at hand (while being privacy-preserving). We are not aware of any other privacy-preserving approach for learning Cox regression or any survival analysis model. While our motivation is on radiation therapy, we also demon-strate our approach on publicly available datasets to ease future comparisons.

The rest of the paper is organized as follows: In Section 2 we present an overview of the related work. Then in Sec-tion 3 we present the overview of the Cox model and the privacy-preserving Cox model. Section 4 develops the op-timal projection method, and Section 5 and 6 describe the experimental results using benchmark datasets and in a real clinical setting. Finally we conclude the paper in Section 7 with a brief discussion.
Privacy-preserving data mining has received a lot of atten-tion recently due to the increasing need to share and analyze data that have previously been stored in a passive state due to its private nature. An example is the healthcare setting, where it is becoming clearer that the market forces point in a direction that require making the full patient electron ic health record (EHR) available to the patients themselves, t o trusted parties, but also in a private manner to third party entities. The challenge is not just that of guaranteeing sec ure transmission of the data to trusted entities. A more critica l challenge is instead to be able to analyze large amount of data available using several entities/locations that do no t wish to share the actual data records with any of the other entities or even with a centralized entity. A clear exam-ple, which is the motivation for this paper, is the case when data from multiple healthcare institutions need to be used to build data mining or machine learning models without actually sharing the original data content.

More generally, the focus of attention in the privacy pre-serving field has been: how to develop accurate models with-out access to the original data in individual records [1]. An excellent overview of privacy-preserving data mining meth -ods can be found in [18]. Privacy preservation is analyzed from various viewpoints based on:
This paper is concerned primarily with the distributed scenario. We focus on horizontal data partition, the case when different entities hold the same input features for dif-ferent groups of data points (individuals). For example, th e horizontal case has been addressed recently in special scen ar-ios [20, 19] by privacy-preserving SVMs and induction tree classifiers. We concentrate on horizontal privacy-preserv ing data mining. In the vertical data distribution scenario, th e entities have some subset of features for the same individ-uals. Likewise, several techniques have been proposed to address this case in special setting including adding rando m perturbations to the data [2, 6]
The privacy-preserving methodology employed in this pa-per consists of a form of data modification based on ag-gregation that allows for it to be easily exchanged and still preserve the privacy of the original data. Thus, avoiding th e need for a more complex cryptographic protocol.

This form of data aggregation is related to a data transfor-mation referred to as random projections which was recently proposed for privacy-preserving data mining [12] (includi ng distributed). The basic idea consist of using an approx-imate random projection method to improve the level of privacy protection but still preserving some statistical c har-acteristics of the data. This form of data modification was called randomized multiplicative data perturbation and basi-cally consists of building a new representation of the data by projecting the data using a randomly build matrix as projection operator. The theoretical underpinnings of thi s form of transformation are based on the celebrated Johnson-Lindenstrauss lemma [8] which indicates that a set of n d -dimensional points can be embedded into a k -dimensional space with k = O (log n ), such that the Euclidean distance between any two points can be maintained within an arbi-trarily small factor. Thus, according to this the data can be made private and still preserve some statistics.

The present approach also applies a multiplicative pertur-bation via a linear (and non-linear) projection. However, i t is in sharp contrast with the randomized approach because it attempts to find a projection that is optimal for the prob-lem at hand. The basic motivation for this is the question about why preserve the overall data structure when often by preserving only the relevant structure we can benefit both in terms of efficiency and accuracy. The efficiency gains are derived from the fact that by preserving only the relevant structure, it is possible to obtain a simpler and more com-pact representation. The accuracy gains are derived from the fact that for a fixed representation size, concentrating on the relevant structure for the problem at hand must im-prove its accuracy.

We can summarize our basic idea as that of finding an optimal perturbation of the data that maintains (primarily ) the relevant, important properties of the data and that at the same time promotes a compact representation.

For completeness, we also note that various approaches are geared toward providing privacy to specific data min-ing techniques. For example, privacy preservation for clus -tering tasks [15]. Several other recently proposed privacy -preserving classification techniques specific to the data mi n-ing model include cryptographically private SVMs [10], and wavelet-based distortion [13]. We are not aware of any privacy-preserving approaches specific to Cox regression o r survival analysis. While our approach was designed with thi s setting in mind, we remark that it is more general and could be applied in a considerably general data mining scenario.
We begin with a brief introduction of the general Cox regression model, and then present the privacy-preserving Cox model. We discuss both the linear Cox model and the non-linear Cox model. In survival analysis, we are interested in the survival time T of each individual from a certain population [4]. This can be characterized by the survival function S ( t ) = Pr( T &gt; t ) for t &gt; 0, which is the probability that the individual is still alive at time t . A related function is the hazard function , which assesses the instantaneous risk of demise at time t , conditional on survival to that time: with p ( t ) being the density function of S ( t ). It is easily seen that the hazard function fully determines the survival function as S ( t ) = exp(  X  R t 0  X  ( u ) du ).

It is of practical interest to relate the hazard function not only to the time t , but also to a set of covariates (explanatory variables), x i  X  R d , of each individual i . In clinical studies, the covariates typically include demographic variables su ch as age and gender, and diagnosis information like the tumor size. One of the first and the most popular survival models is the Cox model, in which the hazard function takes, in its most general form, the following proportional-hazard form: where  X  0 ( t ) is the baseline hazard function , and f ( x function of the covariates x i [3]. Note that the Cox haz-ard function depends on the covariates only via the time-independent function f . It is commonly assumed that f is linear, i.e., f ( x i ) = w &gt; x i with some weight vector w  X  R but we use the general form such that our privacy-preserving models can be applied to possibly non-linear f (cf. Sec-tion 3.2).

In the original paper, Cox also developed the partial like-lihood for parameter estimation. For this we need to dis-tinguish individuals who have the actual survival time ob-served (e.g., observed death or cancer relapse after treat-ment), from individuals who are (right-)censored (e.g., still alive or cancer-free at the end of the study). Let  X  i be an indicator variable which takes 1 if the individual i is from the former group, and 0 otherwise. Then the observed time t is the survival time when  X  i = 1, and the censoring time when  X  i = 0. For a group of n individuals with outcome { t ,  X  i } n i =1 , the Cox X  X  partial likelihood L is defined as where R i = { j : t j  X  t i } is the risk set containing the individuals who are at risk (of failing) at time t i . The key idea here is to compare at each failure time, the risk for the failed individual to the risk for all the other individuals a t risk at that time. This completely eliminated the baseline hazard  X  0 ( t ) from parameter estimation, indicating that the actual times of failure are not important. Censoring times are not important as well, so long as we keep track of the risk sets.

When f is linear, i.e., f ( x i ) = w &gt; x i , it is straightfor-ward to maximize the partial likelihood w.r.t. vector w us-ing, e.g., gradient methods. When f is non-linear, one can formulate a regularization framework and optimize functio n f in the reproducing kernel Hilbert space (RKHS) [17]. See [11] for the details and some empirical studies.
In the case of horizontally partitioned data, suppose we have k different data sources, or parties (e.g., hospitals or medical centers), and each party j has a subset of n j in-dividuals (e.g., patients). A same set of d predictors are shared among all parties, and survival outcomes are avail-able within each party. The task is to develop a horizontal privacy-preserving Cox model (HPPCox) which is able to use all the data across different parties.

We propose a HPPCox model based on lower-dimensional projection methods such as random projection [12], or ran-dom kernel mapping in the general (non-linear) case [14]. For simplicity we focus on the former which only applies for linear survival models, but the whole machinery also ap-plies to non-linear models. The non-linear extension will b e briefly discussed in Section 3.3.

Our basic idea is based on the simple fact that lower-dimensional projections are in general not reversible , which means that with a fixed mapping matrix, we can certainly project a high-dimensional data point uniquely into a low-dimensional space, but it is not possible to uniquely re-cover the exact high-dimensional point with only its low-dimensional projection. 1 Our HPPCox model thus has the following setup (assuming we use all the d features): 1. Choose a lower dimensionality m &lt; d ; 2. Locate a mapping matrix B of size d  X  m ; 3. Project each individual x i  X  R d into z i  X  R m via map-Since there is an information loss when applying the map-ping B , it is not possible to recover the exact x i given z even when B is known. Therefore in the privacy-preserving setting, we can use this technique to  X  X ide X  the sensitive data x i , and only share with others the projected data z along with the survival outcome { t i ,  X  i } . All the data from different parties are then combined, and a standard Cox model can be learned using z i as the (reduced) predictors
Technically one must require that there does not exist a deterministic relationship between any two dimensions, bu t in general it is not difficult to verify. for survival analysis. The whole algorithm is summarized in Algorithm 1.

This HPPCox model actually assumes the following haz-ard function (in the linear case): where the weight vector w is only of length m (instead of d ). When an optimal  X  w and baseline hazard  X   X  0 ( t ) are found using the HPPCox model, for a test individual x  X  the hazard function is calculated as
Two important questions have been left out so far: A significant number of approaches choose the matrix B randomly, and refer to this as random projection privacy-preserving data mining. Apart from the simplicity and the nice properties as shown in [12], random projection in gen-eral yields inferior performance compared to the (non-priv acy-preserving) methods which share the data explicitly (also see Section 5 for an empirical comparison). And so far there has been no approach addressing the feature selection prob-lem in a privacy-preserving setting. In Section 4, we ad-dress this problem by finding an optimal projection matrix B  X  R d 0  X  m , with d 0  X  d , which is designed to have the following properties: (i) Relative distance preservation: by explicitly en-(ii) Lower dimensionality in the projected space: (iii) Lower dimensionality in the input space (fea-
To the best of our knowledge, there are no other methods that attempt to find a projection for privacy-preserving Cox regression that is optimal in the sense described.
The same lower-dimensional projection idea can be ex-tended for kernel mapping [14], which makes it possible to derive non-linear privacy-preserving Cox models. Let  X  ( x ) be a mapping from the input space x  X  R d into a RKHS space H . Applying the lower-dimensional projection in H , we need to choose a matrix B and calculate B &gt;  X  ( x ), which contains the inner-product of  X  ( x ) with every column of B . Let B be such that every column B (: , ` ) =  X  ( b ` ) for some b `  X  R d , we can calculate the inner-product as Algorithm 1 Horizontal Privacy-Preserving Cox Model Require: k different parties (data sources), each holding a 1: Choose m &lt; d , and locate a matrix B of size d  X  m . All 2: Every party calculates z i = B &gt; x i for every individual 3: All the data are combined, and a standard Cox model is Ensure: The learned Cox model is shared among all par-with  X  (  X  ,  X  ) being the reproducing kernel function . This re-producing property allows us to calculate the inner-produc t without an explicit form for  X  (  X  ), and motivates us to con-sider privacy-preserving methods for non-linear Cox model s. For instance in non-linear HPPCox model, we need to take the following steps to get the projections: 1. Specify a (non-linear) kernel function  X  (  X  ,  X  ); 2. Choose a dimension m &lt; n , the number of individuals; 3. Locate m  X  X ake individuals X  { b ` } , with each b `  X  R 4. Project each individual x i  X  R d into z i  X  R m via ker-It is easily realized that when m &lt; n , it is not possible to reconstruct x i from z i and  X  (  X  ,  X  ), so privacy is preserved.
As described in Section 3.1, in order to preserve privacy we follow the standard methodology of applying a lossy trans-formation to the data. In this section we concentrate specif -ically on finding an optimal matrix that defines a linear transformation (projection). We represent the projection as a rank-deficient matrix B . Instead of employing a random matrix B , we focus on identifying a lossy transformation that is optimal at maintaining certain properties of the dat a that (importantly) depend on the task at hand while still preserving data privacy.

Our approach for finding optimal projections is based on our approach for finding sparse matrices introduced in [16], where the optimal projection is found so that certain rela-tionships among data points are preserved, while at the same time the dimensionality of the resulting data is reduced.
In order to formally define what is meant by optimal pro-jection, we require one additional ingredient. We define op-timality in terms of how well the transformation preserves relationships among data points. The type of relationships we consider are quite general. They are of the type: data point i is more like data point j than data point k . Thus, in order to measure the goodness of our projection, we use a set called T that is formed by T elements. Each element is represented by a triplet ( i, j, k ) where i is an index for a data points (similarly for j and k ), such that x i , x j satisfy the above relationship.
The set T can be defined in multiple ways. A user can provide this as a special form of supervision or it can be give n by an algorithm. Note that no specific class label or distance measure is required, but could be used. As an example, an algorithm can simply use an attribute (dimension) of the data points and a simple partial order relation to define it. In the HPPCox case T is naturally defined by the order suggested by the patient X  X  survival time. More specifically , given three data points indexed i, j, k respectively, the goal is to preserve relationships of the form: for a set T of triplets for which this property must hold.
The optimal projection matrix B can formally be defined as the solution to the following optimization problem: s.t.  X  ( i, j, k )  X  T , || B &gt; ( x i  X  x j ) || 2 2  X  || B where 1 ( E ) is the indicator function which returns the value 1 if the logical expression E evaluates to true and zero oth-erwise. The formulation is useful at formalizing the desire d concept of an optimal projection. However, it is not prac-tical since an efficient algorithm for finding B given the set T is not likely to exist. 2 Thus, we provide a different for-mulation that can be seen as an approximation based on a convex relaxation of the problem.

Let us define the d  X  d matrix A = BB &gt; . The following formulation now focuses on finding an optimal A : Note that in this problem we are attempting to make the norm of the columns/rows of A to be zero (thus making it a zero vector) through the use of an L 1 norm regularization, and at the same time enforcing constraints that depend on the user/automatically obtained set of triplets. The param -eter  X  controls the balance between sparseness of A and inequality satisfaction. This can be obtained by tuning (de -pending on the problem at hand).

The above formulation is convex, and can be solved via semi-definite programming (SDP). However, since our fo-cus are data mining applications, we concentrate on large datasets (in terms both of the number of data points and their dimensionality) and thus: 1) the cardinality of T could be potentially large and similarly 2) the size of A increases quadratically with the input space dimensionality. It is we ll-known that they SDP not scale well with the problem size. By contrast linear program (LP) solvers have much better scaling properties. In the following, we further modify the formulation to create an approximation that can be solved via LP.

Using the definitions  X  x ij = vect ( x i x &gt; j ) and a = vect ( A ), where vect () means (column-wise) alignment of all of the
This would lead to a 0-1 Mixed Integer Programming (MIP) problem, known to be NP-hard. matrix elements in a column vector, it can be shown that the above optimization problem can be reformulated into: where the SDP constraint in formulation (5) was tighten into a diagonal dominance constraint using the auxiliary vari-ables S lc  X  R , where S is a matrix of the same dimensional-ity as A . In this problem the last constraint is equivalent to diagonal dominance which implies positive semidefinitenes s (cf. [16]-Theorem 4.1.). Let us denote the data term in the first set of constraints as: for ( i, j, k )  X  T . Note that for any ( i, j ), in order to compute  X  x ij we must know both x i and x j .

This formulation does not take into account the distributed nature of the privacy-preserving problem in this paper sinc e these constraints require all the data to be known and avail-able in one location. For horizontal privacy preserving, we propose a protocol for computing the matrix B using data from all parties as follows. We assume that every party contributes with a set of relative relationships that must b e preserved. Similar to the general case, this set is denoted T ( p ) where p indexes the party.

Thus, each party p makes the following information avail-able for ( i, j, k )  X  T ( p ) : Note that C ( p ) ijk does not reveal the original records x because it combines data from these three records in a way that it is not possible to recover them back since the user does not know at any moment which three records are lin-early combined.

The resulting set of constrains provided by party p is incorporated into a large (combined) problem by any un-trusted party given all the vectors C ( p ) ijk as follows: that is, the combined set of constraints in the final problem (formulation 6) is made of a combination of all sets T ( p )
Note that while this allows imposing constraints for any triplet formed by records from the same party, it does not consider constraints that involve records across parties. This limitation is in general not critical since in most instance s determining the relationship between records may require knowledge of the relevant records by the same entity (which is not the case for private information scenarios).
It is important to note that formulation (6) provides a sparse solution (with zero columns/rows) for the symmetric d  X  d matrix A . Since our main interest is in B and A = BB &gt; , we can find the optimal d 0  X  m matrix B as follows. We first remove the zero rows/columns of A , which results in a d 0  X  d 0 matrix  X  A (i.e., the rest d  X  d 0 features are irrelevant features). We then perform an eigenvalue decomposition for Table 1: Summary of the benchmark datasets. n and d are the number of patients and predictive variables, respectively.
  X  A , i.e.,  X  A = VDV &gt; , with V an orthogonal matrix and D a diagonal matrix with non-negative diagonal entries sorte d in a non-increasing order. The diagonal entries of D are all non-negative because  X  A is positive semidefinite. Then we yield B =  X  V  X  D 1 2 , where  X  V contains the first m columns of V , and  X  D contains the top-left m  X  m submatrix of D . Note that we should have m &lt; d 0 to ensure privacy is preserved, which means the feature selection step should obtain more than m features.
Before going into the details of non-small-cell lung can-cer survival prediction, we first show some empirical result s on some benchmark survival data sets. Table 1 summarizes these five data sets. All of them are related to medical out-comes and are publicly available. A substantial amount of data is censored and also missing. The SUPPORT data set is a random sample from Phases I and II of the SUPPORT [9] (Study to Understand Prognoses Preferences Outcomes and Risks of Treatment) study. As suggested in [7] we split the data set into four different subsets, each corresponding to a different cause of death (SUPPORT-1: ARF/MOSF, SUPPORT-2: COPD/CHF/Cirrhosis, SUPPORT-3: Coma, SUPPORT-4: Cancer). The MELANOMA data is from a clinical study of skin cancer.

For these experiments we randomly split each data set into 4 subsets as 4 parties. Then we randomly pick up 70% of the patients from each party as training patients, and tes t on the rest 30% patients. For HPPCox we combine all the training patients together using Algorithm 1, in which we consider both the learned mapping matrix (HPPCox learn) and random projection (HPPCox rand). These experiments are repeated 20 times with different splits, and the predicti ve Area Under the ROC Curve (AUC) are reported in Figure 1, 2 and 3 for these five data sets. Note that to be able to calculate the ROC curve, we select the point of interest to be the median survival time of the data set ( i.e., the patient get an output +1 if he/she survived longer than the median, or -1 otherwise).

It is clear from these figures that HPPCox yields much better predictions than the Cox model trained on each indi-vidual subset. This indicates that by sharing the data in the privacy-preserving way, PPCox is able to better predict the survival. From these figures we can also see that HPPCox using the learned mapping matrix is in general better than HPPCox using random projection, and with smaller error bars. This means our strategy is very promising in further improving the performance. In Figure 3 we also compare the  X  X PPCox learn X  with the non-HPPCox which explicitly combines the training data from different parties without mapping. The HPPCox one achieves almost the same per-formance as non-HPPCox, indicating that HPPCox can not only preserve privacy, but also achieve almost-optimal per -formance.

We believe the high performance achieved using the rep-resentation provided by our learned (optimized) projectio n matrix is in part due to the feature selection properties of our model. Feature selection can often prevent overfitting and is specially useful in scenarios with limited training d ata relative to the number of dimensions. However, we remark that in this paper we do not concentrate on analyzing the effects of feature selection on this datasets.
Radiotherapy, combined with chemotherapy, is treatment of choice for a large group of non-small cell lung cancer (NSCLC) patients. The marginal role of radiotherapy and chemotherapy for the survival of NSCLC patients has been changed into one of significant importance. Improved radio-therapy treatment techniques allow an increase of the radia -tion dose, while at the same time more effective chemoradi-ation schemes are being applied. These developments have lead to an improved outcome in terms of survival. In sum-mary, an increasing number of patients is being treated suc-cessfully with (chemo) radiation, but an accurate estimati on of the survival probability for an individual patient, taki ng into account patient, tumor as well as treatment charac-teristics and offering the possibility for treatment decisi on-making, is currently not available.

At present, generally accepted prognostic factors for inop -erable patients are performance status, weight loss, prese nce of comorbidity, use of chemotherapy in addition to radio-therapy, radiation dose and tumor size. For other factors such as gender and age the literature shows inconsistent re-sults. In a recent study it was shown that number of in-volved nodal areas quantified by PET-CT was an impor-tant prognostic factor [5]. We performed this retrospectiv e study to develop a prediction model for 2-year survival of NSCLC patients, treated with (chemo) radiotherapy, taking into account all known prognostic factors. To the best of our knowledge, this is the first study of prediction models for NSCLC patients treated with (chemo)radiotherapy.
Between May 2002 and January 2007, a total number of 455 inoperable NSCLC patients, stage I-IIIB, were referred to MAASTRO clinic to be treated with curative intent. Clinical data of all these patients were collected retrospe c-tively by reviewing the clinical charts. If PET was not used as a staging tool, patients were excluded from the study. This resulted in the inclusion of 399 patients. The primary gross tumor volume (GTV primary ) and nodal gross tumor volume (GTV nodal ) were calculated, and the sum of them resulted in the GTV. Radiotherapy planning was performed with a Focus (CMS) system, taking into account lung den-sity and according to ICRU 50 guidelines. There were four different radiotherapy treatment regimes applied for these patients in this retrospective study, therefore to account for the different treatment time and number of fractions per day, the equivalent dose in 2 Gy fractions, corrected for overall treatment time (EQD2T), was used as a measure for the intensity of chest radiotherapy. The final list of clinical v ari-ables are summarized in Table 2. Additionally, a smaller number of patients treated at the other two centers, the Gent hospital and the Leuven hospital, were also collected for this study. There are respectively 86 and 40 patients fro m the Gent and Leuven hospitals, and the same set of clinical variables as the MAASTRO patients were measured.
In this paper we focus on 2-year survival prediction for these NSCLC patients, which is the most interesting pre-diction from clinical perspective. The survival status was evaluated in December 2007. The mean values across pa-tients are used to impute the missing entries if some of these predictors are missing for certain patients. To account for Variable Description Gender Male or Female WHO WHO performance score FEV Lung function T-Stage T stage information N-Stage N stage information NPLN Number of positive lymph nodes GTV Gross tumor volume Chemo With chemo-therapy or not EQD2T Equivalent dose corrected by time
OTT Overall treatment time the very different number of patients from the three sites, a subset of MAASTRO patients were selected for the follow-ing study. In the following we use the names  X  X AASTRO X ,  X  X ent X  and  X  X euven X  to denote the data from the three dif-ferent centers. We finally end up with 80, 85 and 40 patients for MAASTRO, Gent and Leuven, respectively.

Under the privacy-preserving setting, we are interested in assessing the predictive performance of a model combining the patient data from the three centers together, compared to the models trained based on each of these centers. The data combination needs to be done in a way that sensitive information is not uncovered. Therefore for our experiment s we trained the following 5 models under each configuration: For each of the configurations, we vary the percentage of training patients in each of the centers, and report the AUC for the test patients. Note that the testing was performed using all the test patients from all centers.
Figure 4 shows the comparison results of  X  X PPCox learn X  with other Cox model settings. It X  X  clear that  X  X PPCox learn X  again outperforms all the other settings. In Figure 4 middle and right, we show the random projection versus the optimal non-HPPCox training which explicitly combines the data from different centers, and as can be seen the difference is not big. The  X  X PPCox learn X  is slightly better than  X  X P-PCox rand X , so for clarity we didn X  X  draw that curve on the figure. As expected, with higher percentages of training dat a the predictive performance is better. The big error bars in-dicate that when we randomly select 90% of the data for training, the test performance is largely influenced by the quality of the (combined) left-out data.

Our proposed mapping learning algorithm has the nice property that we can automatically do feature selection si-multaneously as the PP Cox modeling. In this study we start from 10 features as listed in Table 2, and finally our algorithm successfully selected, in average, 6.35 feature s. Figure 5: AUC comparison for HPPCox with differ-ent dimensions m , where the optimal non-HPPCox line is using the identified 6 best features.

Finally based on these selected 6 features, we varied the mapping dimensions m for the B matrix we used in PPCox models (see Figure 5), and as expected, bigger m yield better predictive performance. Therefore, in practice we normall y choose m = d  X  1 to maximize the performance of the PP models (which still perfectly satisfies the privacy-preser ving requirements).
We have designed an approach for privacy-preserving data mining based on a linear (lossy) projection of the original data onto a lower-dimensional space. This projection is op-timal in the sense that it preserves the relevant attributes of the data that are important for the application of interest. It can therefore be contrasted with the random projection ap-proach for privacy preserving. We have described our adap-tation of this concept to a real clinical setting where data i s shared across three healthcare institutions. Our approach is able to build more accurate predictive models than what was possible by using only the data from each institution alone and using the random projection approach. These results were also obtained using benchmark datasets. We believe this is the first approach for privacy-preserving data minin g for Cox regression survival analysis.

There are a few interesting challenges related to adapting this approach to the scenario of vertical data distribution . For vertical privacy-preserving, formulation (6) is not ap -plicable in a similar way as for the horizontal case, since it assumes full records are available at once. The outer prod-uct needed to calculate C ( p ) ijk cannot be computed indepen-dently by each party (at each institution). This can be cir-cumvented by letting C ( p ) ijk be an incomplete matrix/vector, where the element corresponding to the above entry is not utilized (equivalentely can be made equal to zero); this ap-proach is not included in this paper due to space consid-erations. However, while this approach makes it possible to obtain a direct vertical privacy-preserving formulatio n, we believe it is sub-optimal and can open the door to new alternative formulations. [1] R. Agrawal and R. Srikant. Privacy-preserving data [2] K. Chen and L. Liu. Privacy preserving data [3] D. R. Cox. Regression models and life tables (with [4] D. R. Cox and D. Oakes. Analysis of Survival Data . [5] C. Dehing-Oberije, D. D. Ruysscher, H. van der [6] W. Du, Y. Han, and S. Chen. Privacy-preserving [7] F. E. Harrell Jr. Regression Modeling Strategies, With [8] W. Johnson and J. Lindenstrauss. Extensions of [9] W. Knaus, F. E. Harrell, J. Lynn, et al. The [10] S. Laur, H. Lipmaa, and T. Mielik  X  ainen.
 [11] H. Li and Y. Luan. Kernel Cox Regression Models for [12] K. Liu, H. Kargupta, and J. Ryan. Random [13] L. Liu, J. Wang, Z. Lin, and J. Zhang. Wavelet-based [14] O. L. Mangasarian and T. Wild. Privacy-preserving [15] S. R. M. Oliveira and O. R. Za  X   X ane. Privacy [16] R. Rosales and G. Fung. Learning sparse metrics via [17] B. Sch  X  olkopf and A. J. Smola. Learning with Kernels . [18] V. Verykios, E. Bertino, I. Fovino, L. Provenza, [19] M.-J. Xiao, L.-S. Huang, Y.-L. Luo, and H. Shen. [20] H. Yu, X. Jiang, and J. Vaidya. Privacy-preserving
