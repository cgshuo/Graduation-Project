 In many research areas of Natural Language Pro-cessing (NLP) lexical knowledge is exploited to perform tasks effectively. These include, among others, text summarization (Nastase, 2008), Named Entity Recognition (Bunescu and Pas  X ca, 2006), Question Answering (Harabagiu et al., 2000) and text categorization (Gabrilovich and Markovitch, 2006). Recent studies in the diffi-cult task of Word Sense Disambiguation (Nav-igli, 2009b, WSD) have shown the impact of the amount and quality of lexical knowledge (Cuadros and Rigau, 2006): richer knowledge sources can be of great benefit to both knowledge-lean systems (Navigli and Lapata, 2010) and supervised classi-fiers (Ng and Lee, 1996; Yarowsky and Florian, 2002).

Various projects have been undertaken to make lexical knowledge available in a machine read-able format. A pioneering endeavor was Word-Net (Fellbaum, 1998), a computational lexicon of English based on psycholinguistic theories. Sub-sequent projects have also tackled the significant problem of multilinguality. These include Eu-roWordNet (Vossen, 1998), MultiWordNet (Pianta et al., 2002), the Multilingual Central Repository (Atserias et al., 2004), and many others. How-ever, manual construction methods inherently suf-fer from a number of drawbacks. First, maintain-ing and updating lexical knowledge resources is expensive and time-consuming. Second, such re-sources are typically lexicographic, and thus con-tain mainly concepts and only a few named enti-ties. Third, resources for non-English languages often have a much poorer coverage since the con-struction effort must be repeated for every lan-guage of interest. As a result, an obvious bias ex-ists towards conducting research in resource-rich languages, such as English.

A solution to these issues is to draw upon a large-scale collaborative resource, namely Wikipedia 1 . Wikipedia represents the perfect com-plement to WordNet, as it provides multilingual lexical knowledge of a mostly encyclopedic na-ture. While the contribution of any individual user might be imprecise or inaccurate, the continual in-tervention of expert contributors in all domains re-sults in a resource of the highest quality (Giles, 2005). But while a great deal of work has been re-cently devoted to the automatic extraction of struc-tured information from Wikipedia (Wu and Weld, 2007; Ponzetto and Strube, 2007; Suchanek et al., 2008; Medelyan et al., 2009, inter alia ), the knowledge extracted is organized in a looser way than in a computational lexicon such as WordNet.
In this paper, we make a major step towards the vision of a wide-coverage multilingual knowledge resource. We present a novel methodology that produces a very large multilingual semantic net-work: BabelNet. This resource is created by link-ing Wikipedia to WordNet via an automatic map-ping and by integrating lexical gaps in resource-poor languages with the aid of Machine Transla-tion. The result is an  X  X ncyclopedic dictionary X , that provides concepts and named entities lexical-ized in many languages and connected with large amounts of semantic relations. We encode knowledge as a labeled directed graph G = ( V,E ) where V is the set of vertices  X  i.e. concepts 2 such as balloon  X  and E  X  V  X  R  X  V is the set of edges connecting pairs of concepts. Each edge is labeled with a semantic relation from R , e.g. { is-a , part-of , . . . , } , where denotes an un-specified semantic relation. Importantly, each ver-tex v  X  V contains a set of lexicalizations of the concept for different languages, e.g. { balloon EN , Ballon DE , aerostato ES , . . . , montgolfi ` ere FR } .
Concepts and relations in BabelNet are har-vested from the largest available semantic lexi-con of English, WordNet, and a wide-coverage collaboratively edited encyclopedia, the English Wikipedia (Section 3.1). We collect (a) from WordNet, all available word senses (as concepts ) and all the semantic pointers between synsets (as relations ); (b) from Wikipedia, all encyclopedic entries (i.e. pages, as concepts ) and semantically unspecified relations from hyperlinked text.
In order to provide a unified resource , we merge the intersection of these two knowledge sources (i.e. their concepts in common) by establishing a mapping between Wikipedia pages and WordNet senses (Section 3.2). This avoids duplicate con-cepts and allows their inventories of concepts to complement each other. Finally, to enable mul-tilinguality, we collect the lexical realizations of the available concepts in different languages by using (a) the human-generated translations pro-vided in Wikipedia (the so-called inter-language links), as well as (b) a machine translation sys-tem to translate occurrences of the concepts within sense-tagged corpora, namely SemCor (Miller et al., 1993)  X  a corpus annotated with WordNet senses  X  and Wikipedia itself (Section 3.3). We call the resulting set of multilingual lexicalizations of a given concept a babel synset . An overview of BabelNet is given in Figure 1 (we label vertices with English lexicalizations): unlabeled edges are obtained from links in the Wikipedia pages (e.g. B labeled ones from WordNet 3 (e.g. balloon 1 n has-part gasbag 1 n ). In this paper we restrict ourselves to concepts lexicalized as nouns. Nonetheless, our methodology can be applied to all parts of speech, but in that case Wikipedia cannot be exploited, since it mainly contains nominal entities. 3.1 Knowledge Resources WordNet. The most popular lexical knowledge resource in the field of NLP is certainly WordNet, a computational lexicon of the English language. A concept in WordNet is represented as a synonym set (called synset ), i.e. the set of words that share the same meaning. For instance, the concept wind is expressed by the following synset: where each word X  X  subscripts and superscripts in-dicate their parts of speech (e.g. n stands for noun) and sense number, respectively. For each synset, WordNet provides a textual definition, or gloss. For example, the gloss of the above synset is:  X  X ir moving from an area of high pressure to an area of low pressure X .
 Wikipedia. Our second resource, Wikipedia, is a Web-based collaborative encyclopedia. A Wikipedia page (henceforth, Wikipage) presents the knowledge about a specific concept (e.g. B AL -LOON ( AIRCRAFT )) or named entity (e.g. M ONT -GOLFIER BROTHERS ). The page typically con-tains hypertext linked to other relevant Wikipages. For instance, B ALLOON ( AIRCRAFT ) is linked to W
IND , G AS , and so on. The title of a Wikipage (e.g. B ALLOON ( AIRCRAFT )) is composed of the lemma of the concept defined (e.g. balloon ) plus an optional label in parentheses which speci-fies its meaning if the lemma is ambiguous (e.g. AIRCRAFT vs. TOY ). Wikipages also provide inter-language links to their counterparts in other languages (e.g. B ALLOON ( AIRCRAFT ) links to the Spanish page A EROSTATO ). Finally, some Wikipages are redirections to other pages, e.g. the Spanish B AL  X  ON AEROST  X  ATICO redirects to A 3.2 Mapping Wikipedia to WordNet The first phase of our methodology aims to estab-lish links between Wikipages and WordNet senses. We aim to acquire a mapping  X  such that, for each Wikipage w , we have:  X  ( w ) = where Senses WN ( w ) is the set of senses of the lemma of w in WordNet. For example, if our map-ping methodology linked B ALLOON ( AIRCRAFT ) to the corresponding WordNet sense balloon 1 n , we would have  X  (B ALLOON ( AIRCRAFT )) = bal-loon 1 n .

In order to establish a mapping between the two resources, we first identify the disambigua-tion contexts for Wikipages (Section 3.2.1) and WordNet senses (Section 3.2.2). Next, we inter-sect these contexts to perform the mapping (see Section 3.2.3). 3.2.1 Disambiguation Context of a Wikipage Given a Wikipage w , we use the following infor-mation as disambiguation context:  X  Sense labels : e.g. given the page B ALLOON ( AIRCRAFT ), the word aircraft is added to the disambiguation context.  X  Links : the titles X  lemmas of the pages linked from the target Wikipage (i.e., outgoing links). For instance, the links in the Wikipage B AL -
LOON ( AIRCRAFT ) include wind , gas , etc.  X  Categories : Wikipages are typically classi-fied according to one or more categories. For example, the Wikipage B ALLOON ( AIR -CRAFT ) is categorized as BALLOONS, BAL-
LOONING, etc. While many categories are very specific and do not appear in Word-Net (e.g., SWEDISH WRITERS or SCIEN-
TISTS WHO COMMITTED SUICIDE), we use their syntactic heads as disambiguation con-text (i.e. writer and scientist , respectively). Given a Wikipage w , we define its disambiguation context Ctx ( w ) as the set of words obtained from all of the three sources above. 3.2.2 Disambiguation Context of a WordNet Given a WordNet sense s and its synset S , we col-lect the following information:  X  Synonymy : all synonyms of s in S . For in-stance, given the sense airplane 1 n and its cor-responding synset { airplane 1 n , aeroplane 1 n , plane 1 n } , the words contained therein are in-cluded in the context.  X  Hypernymy/Hyponymy : all synonyms in the synsets H such that H is either a hypernym (i.e., a generalization) or a hyponym (i.e., a specialization) of S . For example, given bal-loon 1 n , we include the words from its hypernym { lighter-than-air craft 1 n } and all its hyponyms (e.g. { hot-air balloon 1 n } ).  X  Sisterhood : words from the sisters of S . A sis-ter synset S 0 is such that S and S 0 have a com-mon direct hypernym. For example, given bal-loon 1 n , it can be found that { balloon 1 n } and { airship 1 n , dirigible 1 n } are sisters. Thus air-ship and dirigible are included in the disam-biguation context of s .  X  Gloss : the set of lemmas of the content words occurring within the WordNet gloss of S .
 We thus define the disambiguation context Ctx ( s ) of sense s as the set of words obtained from all of the four sources above. 3.2.3 Mapping Algorithm In order to link each Wikipedia page to a WordNet sense, we perform the following steps:  X  Initially, our mapping  X  is empty, i.e. it links each Wikipage w to .  X  For each Wikipage w whose lemma is monose-mous both in Wikipedia and WordNet we map w to its only WordNet sense.  X  For each remaining Wikipage w for which no mapping was previously found (i.e.,  X  ( w ) = ), we assign the most likely sense to w based on the maximization of the conditional probabili-ties p ( s | w ) over the senses s  X  Senses WN ( w ) (no mapping is established if a tie occurs).
To find the mapping of a Wikipage w , we need to compute the conditional probability p ( s | w ) of selecting the WordNet sense s given w . The sense s which maximizes this probability is determined as follows:
The latter formula is obtained by observing that p ( w ) does not influence our maximization, as it is a constant independent of s . As a result, determin-ing the most appropriate sense s consists of find-ing the sense s that maximizes the joint probability p ( s,w ) . We estimate p ( s,w ) as: where score ( s,w ) = | Ctx ( s )  X  Ctx ( w ) | + 1 (we add 1 as a smoothing factor). Thus, in our al-gorithm we determine the best sense s by com-puting the intersection of the disambiguation con-texts of s and w , and normalizing by the scores summed over all senses of w in Wikipedia and WordNet. More details on the mapping algorithm can be found in Ponzetto and Navigli (2010). 3.3 Translating Babel Synsets So far we have linked English Wikipages to Word-Net senses. Given a Wikipage w , and provided it is mapped to a sense s (i.e.,  X  ( w ) = s ), we cre-ate a babel synset S  X  W , where S is the WordNet synset to which sense s belongs, and W includes: (i) w ; (ii) all its inter-language links (that is, trans-lations of the Wikipage to other languages); (iii) the redirections to the inter-language links found in the Wikipedia of the target language. For in-stance, given that  X  (B ALLOON ) = balloon 1 n , the corresponding babel synset is { balloon EN , Bal-lon DE , aerostato ES , bal  X  on aerost  X  atico ES , . . . , pallone aerostatico IT } . However, two issues arise: first, a concept might be covered only in one of the two resources (either WordNet or Wikipedia), meaning that no link can be estab-lished (e.g., F ERMI GAS or gasbag 1 n in Figure 1); second, even if covered in both resources, the Wikipage for the concept might not provide any translation for the language of interest (e.g., the Catalan for B ALLOON is missing in Wikipedia).
In order to address the above issues and thus guarantee high coverage for all languages we de-veloped a methodology for translating senses in the babel synset to missing languages. Given a WordNet word sense in our babel synset of interest (e.g. balloon 1 n ) we collect its occurrences in Sem-Cor (Miller et al., 1993), a corpus of more than 200,000 words annotated with WordNet senses. We do the same for Wikipages by retrieving sen-tences in Wikipedia with links to the Wikipage of interest. By repeating this step for each English lexicalization in a babel synset, we obtain a col-lection of sentences for the babel synset (see left part of Figure 1). Next, we apply state-of-the-art Machine Translation 4 and translate the set of sen-tences in all the languages of interest. Given a spe-cific term in the initial babel synset, we collect the set of its translations. We then identify the most frequent translation in each language and add it to the babel synset. Note that translations are sense-specific, as the context in which a term occurs is provided to the translation system. 3.4 Example We now illustrate the execution of our method-ology by way of an example. Let us focus on the Wikipage B ALLOON ( AIRCRAFT ). The word is polysemous both in Wikipedia and WordNet. In the first phase of our methodology we aim to find a mapping  X  (B ALLOON ( AIRCRAFT )) to an appropriate WordNet sense of the word. To this end we construct the disambiguation context for the Wikipage by including words from its la-bel, links and categories (cf. Section 3.2.1). The context thus includes, among others, the follow-ing words: aircraft , wind , airship , lighter-than-air . We now construct the disambiguation context for the two WordNet senses of balloon (cf. Sec-tion 3.2.2), namely the aircraft (#1) and the toy (#2) senses. To do so, we include words from their synsets, hypernyms, hyponyms, sisters, and glosses. The context for balloon 1 n includes: air-craft , craft , airship , lighter-than-air . The con-text for balloon 2 n contains: toy , doll , hobby . The sense with the largest intersection is #1, so the following mapping is established:  X  (B ALLOON (
AIRCRAFT )) = balloon 1 n . After the first phase, our babel synset includes the following English words from WordNet plus the Wikipedia inter-language links to other languages (we report Ger-man, Spanish and Italian): { balloon EN , Ballon DE , aerostato ES , bal  X  on aerost  X  atico ES , pallone aero-statico IT } .

In the second phase (see Section 3.3), we col-lect all the sentences in SemCor and Wikipedia in which the above English word sense occurs. We translate these sentences with the Google Trans-late API and select the most frequent transla-tion in each language. As a result, we can en-rich the initial babel synset with the following words: mongolfi ` ere FR , globus CA , globo ES , mon-golfiera IT . Note that we had no translation for Catalan and French in the first phase, because the inter-language link was not available, and we also obtain new lexicalizations for the Spanish and Ital-ian languages. Experimental setting. We first performed an evaluation of the quality of our mapping from Wikipedia to WordNet. To create a gold stan-dard for evaluation we considered all lemmas whose senses are contained both in WordNet and Wikipedia: the intersection between the two re-sources contains 80,295 lemmas which corre-spond to 105,797 WordNet senses and 199,735 Wikipedia pages. The average polysemy is 1.3 and 2.5 for WordNet senses and Wikipages, re-spectively (2.8 and 4.7 when excluding monose-mous words). We then selected a random sam-ple of 1,000 Wikipages and asked an annotator with previous experience in lexicographic annota-Mapping algorithm 81.9 77.5 79.6 84.4 MFS BL 24.3 47.8 32.2 24.3 Random BL 23.8 46.8 31.6 23.9
Table 1: Performance of the mapping algorithm. tion to provide the correct WordNet sense for each page (an empty sense label was given, if no correct mapping was possible). The gold-standard dataset includes 505 non-empty mappings, i.e. Wikipages with a corresponding WordNet sense. In order to quantify the quality of the annotations and the dif-ficulty of the task, a second annotator sense tagged a subset of 200 pages from the original sample. Our annotators achieved a  X  inter-annotator agree-ment (Carletta, 1996) of 0.9, indicating almost perfect agreement.
 Results and discussion. Table 1 summarizes the performance of our mapping algorithm against the manually annotated dataset. Evaluation is per-formed in terms of standard measures of preci-sion, recall, and F 1 -measure. In addition we calcu-late accuracy, which also takes into account empty sense labels. As baselines we use the most fre-quent WordNet sense (MFS), and a random sense assignment.

The results show that our method achieves al-most 80% F 1 and it improves over the baselines by a large margin. The final mapping contains 81,533 pairs of Wikipages and word senses they map to, covering 55.7% of the noun senses in WordNet. As for the baselines, the most frequent sense is just 0.6% and 0.4% above the random baseline in terms of F 1 and accuracy, respectively. A  X  2 test reveals in fact no statistical significant difference at p &lt; 0 . 05 . This is related to the random distri-bution of senses in our dataset and the Wikipedia unbiased coverage of WordNet senses. So select-ing the first WordNet sense rather than any other sense for each target page represents a choice as arbitrary as picking a sense at random. We perform a second set of experiments concern-ing the quality of the acquired concepts. This is as-sessed in terms of coverage against gold-standard resources (Section 5.1) and against a manually-validated dataset of translations (Section 5.2). 5.1 Automatic Evaluation Datasets. We compare BabelNet against gold-standard resources for 5 languages, namely: the subset of GermaNet (Lemnitzer and Kunze, 2002) included in EuroWordNet for German, Multi-WordNet (Pianta et al., 2002) for Italian, the Mul-tilingual Central Repository for Spanish and Cata-lan (Atserias et al., 2004), and WOrdnet Libre du Franc  X ais (Beno  X   X t and Fi  X  ser, 2008, WOLF) for French. In Table 2 we report the number of synsets and word senses available in the gold-standard re-sources for the 5 languages.
 Measures. Let B be BabelNet, F our gold-standard non-English wordnet (e.g. GermaNet), and let E be the English WordNet. All the gold-standard non-English resources, as well as Babel-Net, are linked to the English WordNet: given a synset S F  X  X  , we denote its corresponding babel synset as S B and its synset in the English Word-Net as S E . We assess the coverage of BabelNet against our gold-standard wordnets both in terms of synsets and word senses. For synsets, we calcu-late coverage as follows: where  X  ( S B ,S F ) = 1 if the two synsets S B and S
F have a synonym in common, 0 otherwise. That is, synset coverage is determined as the percentage of synsets of F that share a term with the corre-sponding babel synsets. For word senses we cal-culate a similar measure of coverage: WordCov ( B , F ) = where s F is a word sense in synset S F and  X  ( s F ,S B ) = 1 if s F  X  S B , 0 otherwise. That is we calculate the ratio of word senses in our gold-standard resource F that also occur in the corresponding synset S B to the overall number of senses in F .

However, our gold-standard resources cover only a portion of the English WordNet, whereas the overall coverage of BabelNet is much higher. We calculate extra coverage for synsets as follows: SynsetExtraCov ( B , F ) =
Similarly, we calculate extra coverage for word senses in BabelNet corresponding to WordNet synsets not covered by the reference resource F . Results and discussion. We evaluate the cov-erage and extra coverage of word senses and synsets at different stages: (a) using only the inter-language links from Wikipedia (W IKI Links); (b) and (c) using only the automatic translations of the sentences from Wikipedia (W IKI Transl.) or Sem-Cor (WN Transl.); (d) using all available transla-tions, i.e. B ABEL N ET .

Coverage results are reported in Table 3. The percentage of word senses covered by BabelNet ranges from 52.9% (Italian) to 66.4 (Spanish) and 86.0% (French). Synset coverage ranges from 73.3% (Catalan) to 76.6% (Spanish) and 92.9% (French). As expected, synset coverage is higher, because a synset in the reference resource is con-sidered to be covered if it shares at least one word with the corresponding synset in BabelNet.

Numbers for the extra coverage, which pro-vides information about the percentage of word senses and synsets in BabelNet but not in the gold-standard resources, are given in Figure 2. The re-sults show that we provide for all languages a high extra coverage for both word senses  X  between 340.1% (Catalan) and 2,298% (German)  X  and synsets  X  between 102.8% (Spanish) and 902.6% (German).

Table 3 and Figure 2 show that the best results are obtained when combining all available trans-lations, i.e. both from Wikipedia and the machine translation system. The performance figures suf-fer from the errors of the mapping phase (see Sec-tion 4). Nonetheless, the results are generally high, with a peak for French, since WOLF has been cre-ated semi-automatically by combining several re-sources, including Wikipedia. The relatively low word sense coverage for Italian (55.4%) is, in-stead, due to the lack of many common words in the Italian gold-standard synsets. Examples in-clude whip EN translated as staffile IT but not as the more common frusta IT , playboy EN translated as vitaiolo IT but not gigol ` o IT , etc. German Spanish Catalan Italian French Table 3: Coverage against gold-standard wordnets (we report percentages). 5.2 Manual Evaluation Experimental setup. The automatic evaluation quantifies how much of the gold-standard re-sources is covered by BabelNet. However, it does not say anything about the precision of the additional lexicalizations provided by BabelNet. Given that our resource has displayed a remark-ably high extra coverage  X  ranging from 340% to 2,298% of the national wordnets (see Figure 2)  X  we performed a second evaluation to assess its precision. For each of our 5 languages, we selected a random set of 600 babel synsets com-posed as follows: 200 synsets whose senses ex-ist in WordNet only, 200 synsets in the intersec-tion between WordNet and Wikipedia (i.e. those mapped with our method illustrated in Section 3.2), 200 synsets whose lexicalizations exist in Wikipedia only. Therefore, our dataset included 600  X  5 = 3,000 babel synsets. None of the synsets was covered by any of the five reference wordnets. The babel synsets were manually validated by ex-pert annotators who decided which senses (i.e. lexicalizations) were appropriate given the corre-sponding WordNet gloss and/or Wikipage.
 Results and discussion. We report the results in Table 4. For each language (rows) and for each of the three regions of BabelNet (columns), we report precision (i.e. the percentage of synonyms deemed correct) and, in parentheses, the over-all number of synonyms evaluated. The results show that the different regions of BabelNet con-tain translations of different quality: while on av-erage translations for WordNet-only synsets have a precision around 72%, when Wikipedia comes into play the performance increases considerably (around 80% in the intersection and 95% with Wikipedia-only translations). As can be seen from the figures in parentheses, the number of trans-lations available in the presence of Wikipedia is higher. This quantitative difference is due to our method collecting many translations from the redi-rections in the Wikipedia of the target language (Section 3.3), as well as to the paucity of examples in SemCor for many synsets. In addition, some of the synsets in WordNet with no Wikipedia coun-terpart are very difficult to translate. Examples include terms like stammel , crape fern , base-ball clinic , and many others for which we could Language WN WN  X  Wiki Wiki German 73.76 (282) 78.37 (777) 97.74 (709) Spanish 69.45 (275) 78.53 (643) 92.46 (703) Catalan 75.58 (258) 82.98 (517) 92.71 (398) Italian 72.32 (271) 80.83 (574) 99.09 (552) French 67.16 (268) 77.43 (709) 96.44 (758) Table 4: Precision of BabelNet on synonyms in WordNet (WN), Wikipedia (Wiki) and their inter-section (WN  X  Wiki): percentage and total num-ber of words (in parentheses) are reported. not find translations in major editions of bilingual dictionaries. In contrast, good translations were produced using our machine translation method when enough sentences were available. Examples are: chaudr  X  ee de poisson FR for fish chowder EN , grano de caf  X  e ES for coffee bean EN , etc. Previous attempts to manually build multilingual resources have led to the creation of a multi-tude of wordnets such as EuroWordNet (Vossen, 1998), MultiWordNet (Pianta et al., 2002), Balka-Net (Tufis  X  et al., 2004), Arabic WordNet (Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dic-tionaries such as EDR (Yokoi, 1995), and fully-fledged frameworks for the development of multi-lingual lexicons (Lenci et al., 2000). As it is of-ten the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient cov-erage. This barrier has led to proposals that ac-quire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia ) or monolingual corpora (Sammer and Soder-land, 2007; Haghighi et al., 2008). The disam-biguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic net-work from a machine readable dictionary (Nav-igli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to pro-duce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations between concepts denoted by their lexical entries.
 The research closest to ours is presented by de Melo and Weikum (2009), who developed a Uni-versal WordNet (UWN) by automatically acquir-ing a semantic network for languages other than English. UWN is bootstrapped from WordNet and is built by collecting evidence extracted from ex-isting wordnets, translation dictionaries, and par-allel corpora. The result is a graph containing 800,000 words from over 200 languages in a hier-archically structured semantic network with over 1.5 million links from words to word senses. Our work goes one step further by (1) developing an even larger multilingual resource including both lexical semantic and encyclopedic knowledge, (2) enriching the structure of the  X  X ore X  semantic net-work (i.e. the semantic pointers from WordNet) with topical, semantically unspecified relations from the link structure of Wikipedia. This result is essentially achieved by complementing Word-Net with Wikipedia, as well as by leveraging the multilingual structure of the latter. Previous at-tempts at linking the two resources have been pro-posed. These include associating Wikipedia pages with the most frequent WordNet sense (Suchanek et al., 2008), extracting domain information from Wikipedia and providing a manual mapping to WordNet concepts (Auer et al., 2007), a model based on vector spaces (Ruiz-Casado et al., 2005), a supervised approach using keyword extraction (Reiter et al., 2008), as well as automatically linking Wikipedia categories to WordNet based on structural information (Ponzetto and Navigli, 2009). In contrast to previous work, BabelNet is the first proposal that integrates the relational structure of WordNet with the semi-structured in-formation from Wikipedia into a unified, wide-coverage, multilingual semantic network. In this paper we have presented a novel methodol-ogy for the automatic construction of a large multi-lingual lexical knowledge resource. Key to our ap-proach is the establishment of a mapping between a multilingual encyclopedic knowledge repository (Wikipedia) and a computational lexicon of En-glish (WordNet). This integration process has several advantages. Firstly, the two resources contribute different kinds of lexical knowledge, one is concerned mostly with named entities, the other with concepts. Secondly, while Wikipedia is less structured than WordNet, it provides large amounts of semantic relations and can be lever-aged to enable multilinguality. Thus, even when they overlap, the two resources provide comple-mentary information about the same named enti-ties or concepts. Further, we contribute a large set of sense occurrences harvested from Wikipedia and SemCor, a corpus that we input to a state-of-the-art machine translation system to fill in the gap between resource-rich languages  X  such as English  X  and resource-poorer ones. Our hope is that the availability of such a language-rich resource 5 will enable many non-English and multilingual NLP applications to be developed.

Our experiments show that our fully-automated approach produces a large-scale lexical resource with high accuracy. The resource includes millions of semantic relations, mainly from Wikipedia (however, WordNet relations are labeled), and contains almost 3 million concepts (6.7 labels per concept on average). As pointed out in Section 5, such coverage is much wider than that of ex-isting wordnets in non-English languages. While BabelNet currently includes 6 languages, links to freely-available wordnets 6 can immediately be es-tablished by utilizing the English WordNet as an interlanguage index. Indeed, BabelNet can be ex-tended to virtually any language of interest. In fact, our translation method allows it to cope with any resource-poor language.

As future work, we plan to apply our method to other languages, including Eastern European, Arabic, and Asian languages. We also intend to link missing concepts in WordNet, by establish-ing their most likely hypernyms  X  e.g., ` a la Snow et al. (2006). We will perform a semi-automatic validation of BabelNet, e.g. by exploiting Ama-zon X  X  Mechanical Turk (Callison-Burch, 2009) or designing a collaborative game (von Ahn, 2006) to validate low-ranking mappings and translations. Finally, we aim to apply BabelNet to a variety of applications which are known to benefit from a wide-coverage knowledge resource. We have al-ready shown that the English-only subset of Ba-belNet allows simple knowledge-based algorithms to compete with supervised systems in standard coarse-grained and domain-specific WSD settings (Ponzetto and Navigli, 2010). We plan in the near future to apply BabelNet to the challenging task of cross-lingual WSD (Lefever and Hoste, 2009).
