 Large enterprise IT (Information Technology) infrastructure components generate large volumes of alerts and incident tickets. These are manually screened, but it is otherwise difficult to extract information automatically from them to gain insights in order to improve operational efficiency. We propose a framework to cluster alerts and incident tick-ets based on the text in them, using unsupervised machine learning. This would be a step towards eliminating man-ual classification of the alerts and incidents, which is very labor intense and costly. Our framework can handle the semi-structured text in alerts generated by IT infrastruc-ture components such as storage devices, network devices, servers etc., as well as the unstructured text in incident tick-ets created manually by operations support personnel. Af-ter text pre-processing and application of appropriate dis-tance metrics, we apply different graph-theoretic approaches to cluster the alerts and incident tickets, based on their semi-structured and unstructured text respectively. For au-tomated interpretation and read-ability on semi-structured text clusters, we propose a method to visualize clusters that preserves the structure and human-readability of the text data as compared to traditional word clouds where the text structure is not preserved; for unstructured text clusters, we find a simple way to define prototypes of clusters for easy interpretation. This framework for clustering and visualiza-tion will enable enterprises to prioritize the issues in their IT infrastructure and improve the reliability and availability of their services.
 H.2.8 [ Database Applications ]: [Data Mining]; I.5.3 [ Pattern Recognition ]: [Clustering] Hierarchical clustering; Connected Components; Graph cut; Complete Linkage; kd-tree; Non-Negative Matrix Factoriza-tion; Tickets Analysis; Alerts and Incidents management
Organizations of all sizes struggle with a common problem in Infrastructure and Operational Management. Thousands of automated alerts with semi-structured text are generated every day from hundreds of infrastructure tools. Similarly, thousands of incident tickets (or incidents) with manually entered unstructured text are created daily by support per-sonnel. These alerts and incidents are centrally collected (eg. IBM Netcool [5]); however, maintenance of these alerts and incidents is costly and labor intensive, and requires de-tailed analysis from subject matter experts. The classic problem enterprises repeatedly encounter is the identifica-tion of important alerts and incidents, on which attention can be focussed. We desire an alert and incident discov-ery and classification process that can be automated and is data agnostic. It benefits the downstream processing steps by helping to reduce false positive alerts [16] or improving operational efficiency to answer questions such as What are the topics of alerts and incidents? what are the topics or clusters that generate the most volumes of alerts? What is the mean-time-to-repair for a cluster?
Toward addressing this need, we propose two distinct frame-works: one framework to cluster alerts with semi-structured text, and another framework to cluster incidents with un-structured text. The text within alert and incident data are each preprocessed using token normalization and stop-word removal -two common approaches used in text mining. The text within each alert or incident is now represented as a bag of words.

To cluster alerts comprised of semi-structured text, we first choose a distance metric, then apply a top-down clus-tering approach using connected components and graph-cut. Related work in [6] used Levenshtein edit distance and DB-SCAN for text clustering; similarly [14] used Levenshtein distance for simple clustering based on dissimilarity. Re-sulting clusters are then visualized with a novel visualization technique for automated interpretation.

Compared to the clustering of alerts, clustering of inci-dents is more computationally demanding. Alerts are gen-erated by technology components (a database, or an appli-cation) and are already broadly tagged by their component names. Alerts generated from a technology component are clustered independently from those from another technol-ogy component. Clustering of all alerts within a technology component is manageable, especially after the token normal-ization step which reduces the space of alerts substantially. Clustering of incidents, however, is more computationally challenging as there is no high-level grouping of incidents. To cluster incidents, we develop an approach based on ma-trix factorization and KD-tree to create a preliminary group-ing of incidents before applying complete linkage clustering per group and later performing cluster merging in a post-processing step.

Based on the clustering output, we compute several in-teresting business intelligence metrics or key performance indicators per cluster and visualize them. In this paper, we also address the need for automated interpretation of alert clusters. For alert cluster interpretation, we propose a novel visualization that preserves the structure and human-readability of the text data as compared to traditional word clouds where the text structure is not preserved. For in-cident cluster interpretation, we simply find prototypes of clusters. KPI visualization and cluster interpretation would help enterprises prioritize the issues in their IT infrastruc-ture and improve the reliability and availability of their ser-vices.

The rest of the paper is organized as follows. In the fol-lowing two sections, we will describe the details of the frame-work. Then, we present the results of these clustering ap-proaches that provide insights into the top issues in the IT infrastructure.
Figure 1 illustrates the 5 high level steps in the proposed framework to cluster semi-structured alerts text.
In the following sub-sections we will describe these steps in detail.

Tokens are individual words within alerts. For the purpose of clustering, we are only interested in the textual structural information in alerts. Tokens exhibiting high variability such as IP address, date and time are either removed or replaced by another common string. For instance, two alerts A and B may report the same issue at specific IP address (e.g Alert A: service is not running at 10 . 2 . 34 . 56 and Alert B: service is not running at 10 . 12 . 2 . 54). For the purpose of clustering these two alerts based on the issue they refer to (a particular service not running), the specific IP address (e.g 10 . 12 . 2 . 54) in the alert string is not important. By replacing the IP address with a string  X  ipaddr  X , both of these alerts end up containing the same  X  X ormalized X  text. Which particular to-kens to normalize, depends on the problem domain. In our enterprise data set, we performed the following token nor-malizations to bring together alerts that originally appeared to be very different.
Token normalization substantially reduced the alert space and eliminated a lot of noise in the data. It was now a lot easier to detect structural patterns within the normalized alerts and to apply suitable distance metrics for clustering.
Stopwords are words that commonly occur in all of the alert groups. These words do not help in clustering alerts as they are present in a large proportion of the alerts. For instance, every alert originating from a certain service com-ponent contains the word:  X  X ervice X . In order to cluster alerts coming out of this component based on its content, this word ( X  X ervice X ) is not helpful and hence can be consid-ered as a stopword and removed.

In order to identify the stopwords, we perform the follow-ing steps:
Before we can perform any clustering, we need to be able to compare any two alerts where each alert is represented as a bag of words . We propose to use Jaccard distance as a metric to compare any two alerts. In order to compute Jaccard distance, each alert is represented as a bag of words. Let us denote one such set from alert A as A and another set of words from alert B as B . Then, Jaccard distance is defined as below:
Jaccard distance is a number between 0 and 1 and is not sensitive to the position of the matching word in the two alerts that are being compared.
 Given N alerts that need to be clustered, we create at NxN distance matrix using the Jaccard distance metric to compare any two alerts.
 It is possible to apply other distance metrics such as Rank-Biased Overlap [17], which takes into account the position of the match when comparing two alerts. In the previous section, we described how we create a NxN distance matrix that summarizes how the alerts re-late to each other. Given the NxN distance matrix, various clustering methods potentially apply, but each with differ-ent assumptions and advantages. For example, hierarchical clustering with minimax linkage [1] tended to produce over-fragmented clusters in our experiments by assuming that similar events within a cluster are within a certain radius distance from the centroid. Not assuming any cluster shape is a criterion in our clustering method choice. We propose using a graph-theoretic approach that uses connected com-ponent detection to generate initial clusters before applying the graph-cut algorithm to further refine the clusters. Figure 2: Size of Connected Components from a Graph (Graph A) Figure 3: Size of Connected Components from a Graph (Graph B)
Given the NxN distance matrix, we can create a graph where the vertices of the graph correspond to alerts. We can establish an edge between any two vertices (alerts) if the Jaccard distance between the two alerts is less than a thresh-old (e.g 0.5). After creating this graph, we run a connected components detection algorithm on it to identify parts of the graph that are not connected to other parts.

A connected component [4] is a sub-graph in which there exists at least one path that connects any two vertices, and which is connected to no additional vertices in the complete graph. One advantage of using connected components is that it doesn X  X  assume any cluster shape.

Figure 2 below shows the number of connected compo-nents detected from a graph (say Graph A) representing few hundred alerts. There are 5 to 6 significant clusters or connected components detected from this group of alerts.
Similarly, we detected 196 connected components from the another graph (say Graph B) representing few thousand alerts. The size of top 3 or 4 connected components in the case of Graph B tended to be much larger than in the case of Graph A. The sizes of detected connected components from Graph B are shown below in Figure 3.

For instance, the top connected component in Graph B was of size 1717. It was desirable to break this large con-nected component (cluster) into smaller clusters. Toward this end, we employed normalized cut, which is a hierarchi-cal graph partitioning algorithm. In the following subsec-tion, we describe the details of this graph-cut algorithm.
Given a NxN distance matrix (C) of a connected com-ponent, we perform the following steps to derive smaller Figure 4: Affinity matrices of large connected com-ponents clusters from this connected component:
Figure 4 below shows affinity matrices of two such large connected components (clusters). Note that the block struc-ture along the main diagonal of the affinity matrices, indi-cating the presence smaller clusters within the connected component.

After recursive partitioning of Graph B, it is broken down to 4 significant smaller clusters as shown in Figure 5.The Figure 5: Graph cut based recursive partitioning of Graph B into 4 smaller clusters connected component of size 1717 is partitioned into 4 clus-ters of sizes 296,246,174 and 166 respectively and other smaller clusters. Note the cluster radii for the smaller clusters from the graph-cut partitions are smaller than the radius of the large connected component.

If the largest connected component is greater than say 10000 alerts, then computing the full distance matrix and performing Graph cut may not scale. We propose to use the Nystrom method to perform graph cut, in which you randomly sample say 5% of the distance matrix and compute an approximation to the dense solution of the eigen system using the Nystrom approximation as suggested in [2]
To account for the complexity arising from the unstruc-tured nature of the incident tickets, we extend the frame-work for clustering semi-structured Netcool alerts by incor-porating two new elements: non-negative matrix factoriza-tion and KD-tree data structure. The resulting framework consists of the following high-level steps: 1. Token normalization and stopword removal. 2. High-level grouping of tickets based on non-negative 3. Hierarchical clustering within each group of incidents. 4. A refinement step to merge similar clusters.
We take the same token normalization approach as ap-plied to Netcool alerts except that timestamps, file paths, URLs, punctuations, and digits are removed from incident summaries, instead of being replaced by common strings. This is based on the observation that unlike semi-structured alerts, ticket descriptions generally do not conform to cer-tain formats, hence semantically non-meaningful strings like timestamps and file paths are unlikely to contribute to the formation of clusters.

In addition, we also remove common English stopwords such as  X  X he X  and  X  X s X  as well as a custom list of words such as  X  X lease X  and  X  X ollowing X  to further clean the data.
For Netcool data, clustering is performed within each sub-group of alerts. However, sub-group information is not avail-able for incident tickets. This presents a potential compu-tational challenge, due to the need to compute pairwise dis-tances between each pair of tickets. In what follows we intro-duce a novel approach to the creation of high-level grouping of incident tickets. Underlying the approach are two tech-niques: non-negative matrix factorization and KD-tree.
As the first step towards generating a preliminary group-ing of incident tickets, we construct feature vectors to rep-resent tickets. Using bag-of-words representation, we collect the entire set of tickets in a binary matrix. Each row repre-sents a term (word), and each column represents an incident ticket, with the value 1 indicating the presence of a term; and 0 otherwise. As shown in Figure 6, the term-incident matrix is highly sparse. This calls for a dimensionality re-duction step to obtain a more compact representation of the data. Towards this end, we factorize the term-incident ma-trix via non-negative matrix factorization [9, 12].
Non-negative matrix factorization (NMF) is a popular technique for dimensionality reduction and underlies a wide range of machine learning applications such as text min-ing [13, 15, 18], image analysis [10], recommender systems [19], etc. It refers to the factorization of a non-negative ma-trix A m  X  n into the product of two low-rank matrices W m  X  k and H k  X  n , whose elements are also non-negative. The rank parameter k is typically chosen to be much smaller than min( m, n ). Given the input matrix A and the rank k ,NMF can be formulated as the following constrained optimization problem: where  X  F denotes Frobenius norm, and the constraints: W, H  X  0, enforce W and H to take only non-negative ele-ments. This problem is non-convex in both W and H , but becomes a convex problem when either W or H is fixed.
There are many different approaches to solving (2). Alter-nating Least Squares (ALS) [12] is among the most popular ones, due to its simplicity and good practical performance; see Alg. 1 for an outline of the algorithm. Starting with an initial estimation of W , ALS first solves the least squares problem min H A  X  WH 2 F for H , which is equivalent to solving the matrix equation at Step 5 of Alg. 1. The subse-quent Step 6 is a truncation step to ensure non-negativity of the solution. It is clear that the closed-form solution to the matrix equation at Step 5 is H =( W W )  X  1 W A . Although matrix inversion is computationally expensive in general, here the size of the positive-semidefinite matrix ( W W )isonly k  X  k . Hence, the cost of inverting this small matrix is almost negligible. The rest of the operation to ob-tain H involves sparse matrix multiplication, which can be Algorithm 1 Alternating Least Squares for NMF 1: input a matrix A m  X  n  X  0 and an integer 0 &lt;k 2: output two low-rank factors: W m  X  k  X  0and H k  X  n  X  0. 3: Initialize W , e.g. using methods introduced in [8]. 4: repeat 5: fix W ,solve W WH = W A for H . 6: set all negative elements in H to 0. 7: fix H ,solve HH W = HA for W . 8: set all negative elements in W to 0. 9: until a convergence criterion is met. 10: optional standardize the solution: normalize each col-efficiently implemented in an MPP database [3]. Similarly, given the current H , W is updated via a two-step procedure (Steps 7 and 8). The algorithm proceeds in an alternating fashion until a convergence criterion is met. In our imple-mentation, we also standardize the solution as shown at Step 10 of Alg. 1 by normalizing H .

A known limitation of ALS is that its convergence is dif-ficult to analyze, due to the ad-hoc enforcement of the non-negativity constraints. Nevertheless, ALS and many of its variants (see e.g. [19]) have seen great success in real-world applications. In practical implementations, it is a common strategy to monitor changes in the elements of W and H , and stop the algorithm when the changes fall below a pre-specified threshold, or the maximum number of allowed iter-ations is reached. Another practical aspect of ALS is initial-ization. Due to the non-convex nature of the NMF problem, ALS (and in fact any NMF algorithm) is bound to be sen-sitive to the initial estimate of W . We use an initialization strategy suggested in [8], which is to initialize each column of W by averaging a certain number of random columns in A . This simple and intuitive approach has been shown [8] to strike a good balance between performance and compu-tational cost, and worked well in our experiments.
The resulting low-rank factors effectively encode the orig-inal matrix A (term-incident matrix) in a k -dimensional la-tent space. The columns of W are the basis vectors of the latent space. Let a i be a column (an incident ticket) in A , and h i the corresponding column in H . Then, an a i can be approximated by a i  X  W h i , meaning that the approxima-tion is an additive linear combination of the basis vectors, weighted by the entries in h i . Essentially, an h i specifies how an incident ticket is expressed in the latent space, where tickets that share similar descriptions still remain  X  X lose X . Given the low-rank factor H , we organize its columns in a KD-tree [11], a commonly used data structure for searching nearest  X  X eighbors X  in a multi-dimensional space. KD-tree partitions multi-dimensional data into non-overlapping sub-sets (also called subtrees) such that nearest neighbors are more likely to be within rather than outside of a subtree. A KD-tree is constructed in a recursive manner (see Fig. 7): 1. Pick a dimension. 2. Find the median along that dimension. 3. Split the data at the median. 4. Repeat on subtrees until they reach a desired size. As illustrated in Fig. 7, a KD-tree creates a natural grouping of the data. This motivates our use of KD-tree to produce a high-level grouping of incident tickets
Note that at each iteration KD-tree splits the data along only one dimension. The implication is that the order of di-mensions to partition the data can influence the final group-ing. Our implementation gives priority to dimensions that contain more non-zero values. The assumption is that denser dimensions are more discriminative. It is also well known that  X  X he curse of dimensionality X  is a problem of KD-tree. Data points in a high-dimensional space tend to be far away from each other, hence are unlikely to form large groups. Re-call that our KD-tree is constructed in the low-dimensional latent space produced by NMF. This effectively avoids the curse of dimensionality issue. Empirically, we found it suf-ficient to set the dimensionality (i.e., the rank parameter in NMF) to 15 to obtain a satisfactory grouping result.
Having constructed a KD-tree, agglomerative hierarchi-cal clustering is run within each subtree to obtain a re-fined grouping of incident tickets. Agglomerative hierarchi-cal clustering builds a clustering tree from bottom up by merging closest clusters. In contrast to the familiar K-means clustering method, hierarchical clustering does not require the number of clusters to be specified apriori . Instead, a user decides at what height to cut a clustering tree, which indirectly determines the number of clusters. We find that it is often much easier to empirically find an appropriate height than the number of clusters.

Hierarchical clustering operates on pairwise distances. To measure pairwise distances between ticket descriptions, we design a distance measure as below: where X i ,A, and B are sets of distinct words in a ticket description, and | X | denotes the cardinality of a set. The first term: max i | X i | is a constant. It is introduced to en-sure positivity of the distance measure. Note that this dis-tance measure is based solely on the intersection of texts but not length. This is motivated by the observation that descriptions of the same issue can vary widely in length; for instance, the two examples shown in Figure 8 both describe a password related issue, but one is brief, while the other contains much more details.

For clustering, we used the complete-linkage method. It is an agglomerative hierarchical clustering method that defines the distance between two clusters by the farthest pair of data Figure 8: Tickets of the same issue can vary widely in length.
 Figure 9: Complete linkage defines the distance between two clusters by the farthest pair of data points. points (Figure 9). Combined with the intersection-based dis-tance measure (3), complete-linkage clustering ensures that each pair of tickets in a cluster share at least some common words. The resulting clustering tree can be cut at a height to ensure a certain degree of overlap between each pair of tickets within a cluster. Suppose that the maximum length of a ticket description is 30 (i.e., max i | X i | = 30), then set-ting the height to 28 guarantees an overlap of at least 2 words between any two tickets of the same cluster. More-over, an exemplar is selected for each cluster to show the typical  X  X ook and feel X  of tickets in a cluster. It is defined as the ticket that has the shortest distance to its farthest  X  X eighbor X  (Figure 10): Due to the variability of ticket descriptions, the same type Figure 10: The exemplar (center of the encompass-ing circle) has the shortest distance to its farthest neighbor. of tickets can be assigned to different subtrees, or to different clusters even within the same subtree. A refinement step was therefore developed to merge those similar clusters.
First, frequently used words (e.g. top-10 words) are ex-tracted from each cluster to form a feature vector. The intersection-based distance measure (3) is then applied to feature vectors to quantify the dissimilarity between two clusters. Given pairwise distances of clusters, the complete-linkage clustering method is used again to create groups of clusters. For each merged cluster, a representative inci-dent is selected as the exemplar (Figure 10). In our experi-ments this refinement procedure joins about 66% of clusters produced by the initial run of clustering (as described in Sec. 3.3).
We had collected data from a large enterprise X  X  IT infras-tructure that supports several software services that are up all the time over a period of 3 months. In all, we had 5 mil-lion rows of alerts over a period of 3 months. Some of these alerts text were identical as they repeated at a certain time interval (say 1 min). Therefore, by focusing our attention only on those unique alerts we were able to reduce the 5 Mil-lion rows to about 590 , 000. After token normalization and stopword removal, the number of unique alerts came down to 22000 from the original count of 590000. Given these 22000 unique alerts and labels for 4 sub-groups within this set, we wanted to find clusters within 4 sub-groups. The 4 sub-groups are already known to the enterprise and hence we wanted to find clusters within each sub-group.
We loaded all the data onto Pivotal X  X  Greenplum Database (GPDB) running on a Data Computing Appliance (DCA) and implemented the clustering frameworks described above using a combination of psql scripts, MADlib (an opensource big data machine learning toolkit that runs on GPDB), PL/R code and PL/python code. GPDB has the MPP (Massively Parallel Processing) architecture with one master and several segment hosts sharing the computational load and is built for scalable big data analytics.
Figure 11 presents a visualization of the clusters from each of 4 Sub-Groups of alerts. The size of the bubbles in the Fig-ure 11 corresponds to the number of alerts belonging to a particular cluster. The intensity of the coloring of the bub-bles corresponds to the number of incident tickets created from the alerts of a particular cluster. For instance, the darker the green color in the clusters corresponding to Sub-Group 4, the higher the number of incidents that got created from that cluster of alerts. Furthermore, the sizes of clusters in the 4 alert groups tend to have a long-tail distribution. That is, there are few big to medium sized clusters whereas there are a lot smaller clusters in each of the alert groups. Figure 11: Visualization of clusters with the 4 Sub-Groups of Alerts Figure 12: Key Performance Indicator (MTTR) Computed for top k clusters
In order for IT operations to focus on their top critical issues, especially events and incidents that generate major failures, it is sufficient to look at a few big-to-medium sized clusters in each of the alert groups. Furthermore, for each of incidents created we can compute a quantity called time-to-resolve which is the time taken for that particular issue to be resolved. For each of the clusters, one can then look at the mean-time-to-resolve (MTTR) from the incidents that got created from that cluster.

Figure 12 below summarizes this MTTR measure in terms of minutes for each of the top clusters across the 4 Sub-Groups. It also indicates how many of the alerts became incidents in each cluster. For instance, the first cluster has 4248 alerts of which only 1407 became incidents. This im-plies that there is room for improvement by paying attention to the rest of alerts before they become bigger issues.
It is challenging to visualize clustered or grouped semi-structured text data such as alerts. Techniques such as word clouds are often used to visualize text data. While word clouds can show the relative frequency of different words across all alerts in a cluster they do not reveal any structure that is common across alerts.

Consider the hypothetical example of alerts shown in Ta-ble 1. Word clouds would only pick out the fact that the words  X  X erver X ,  X  X ot X  X nd  X  X esponding X  X ccur very often in the set of alerts shown but would not be able to pick out the order in which those words occur. For automated inter-pretation and readability of semi-structured text clusters or groups, we propose a method to visualize clusters that pre-serves the structure and human-readability of the text data as compared to traditional word clouds where the text struc-ture is not preserved. This is especially necessary for large clusters and/or longer sequences of text. The proposed vi-sualization method is also applicable to other types of semi-structured clustered/grouped text or categorical data. Table 1: Example of semi-structured text data
The steps in the visualization method are as follows: 1. For each alert in a given cluster generate (word, posi-2. For each cluster record all (word, position) tuples and 3. Visualize the frequency of (word, position) tuples from Table 2: Occurrence of (word, position) tuples
Figure 14 shows an example visualization of three clus-ters generated from real world semi-structured alerts data discussed in the previous sections. Reading the alerts with high percent occurrence off the figure we see the following elements are common within each respective cluster: 1.  X  X ra#: exception encountered: core dump  X  address 2.  X  X ime gmt positive alert harmless  X  3.  X  X ix hardware error:  X 
The automatic readability of the most common theme in each cluster that this type of visualization provides is key to quick interpretation of the results.
For unstructured incident tickets, we focus on discovering the  X  X opics X  of top clusters. We find that top-25 clusters already accounts for 20% of the total tickets. (There are around 67  X  10 3 tickets in total.) Figure 15 provides some examples of top clusters. The keywords of each cluster are the words shared by all the ticket descriptions in a cluster, while the exemplar provides a more detailed view into what a typical ticket looks like.
This paper presents a framework to automatically cluster alerts with semi-structured text and incidents with unstruc-tured text, generated from large enterprise IT infrastructure. Such clustering information allows IT operation staff to gain insights on issues they face everyday, based on the top clus-ters of alerts and incidents, allowing them to prioritize areas for problem-solving. We proposed two distinct frameworks: one framework to cluster alerts with semi-structured text, and another framework to cluster incidents with unstruc-tured text. The text within alert and incident data are each preprocessed using token normalization and stop-word re-moval. The text within each alert or incident is then rep-resented as a bag of words. To cluster alerts comprised of semi-structured text, we first choose a distance metric, then apply a top-down clustering approach using connected com-ponents and graph-cut. To cluster incidents, we proposed an approach based on matrix factorization and KD-tree to create a preliminary grouping of incidents before applying complete linkage clustering per group and later performing cluster merging in a post-processing step.

Based on the clustering output, we computed several key performance indicators per cluster such as the MTTR (Mean-Time-To-Resolve) and visualize them. For alert cluster in-terpretation, we propose a novel visualization that preserves the structure and human-readability of the text data as com-pared to traditional word clouds where the text structure is not preserved. One possible future research direction would involve incorporation of temporal information. in the alerts is clearly seen [1] J. Bien and R. Tibshirani. Hierarchical clustering with [2] S. F. C.Fowlkes and J.Malik. Spectral grouping using [3] J. Cohen, B. Dolan, M. Dunlap, J. M. Hellerstein, and [4] P. ERDdS and A. R&amp;WI. On random graphs i. Publ. [5] IBM Netcool. http://tinyurl.com/m5v8nh2. [6] S. Jain, I. Singh, A. Chandra, Z.-L. Zhang, and [7] J.Shi and J.Malik. Normalized cuts and image [8] A. N. Langville, C. D. Meyer, R. Albright, J. Cox, and [9] D. D. Lee and H. S. Seung. Learning the parts of [10] S. Z. Li, X. Hou, H. Zhang, and Q. Cheng. Learning [11] A. W. Moore. An introductory tutorial on KD-trees, [12] P. Paatero and U. Tapper. Positive matrix [13] V. P. Pauca, F. Shahnaz, M. W. Berry, and R. J. [14] F. Salfner and S. Tschirpke. Error log processing for [15] F. Shahnaz, M. W. Berry, V. P. Pauca, and R. J. [16] L. Tang, T. Li, L. Shwartz, F. Pinel, and G. Y. [17] W. Webber, A. Moffat, and J. Zobel. A similarity [18] W. Xu, X. Liu, and Y. Gong. Document clustering [19] Y. Zhou, D. Wilkinson, R. Schreiber, and R. Pan.
