 This paper aims to solve the problem of improving the ranking of answer candidates for factoid based questions in a state-of-the-art Question Answering system. We first provide an extensive com-parison of 5 ranking algorithms on two datasets  X  from the Jeop-ardy quiz show and a medical domain. We then show the effective-ness of a cascading approach, where the ranking produced by one ranker is used as input to the next stage. The cascading approach shows sizeable gains on both datasets. We finally evaluate several rank aggregation techniques to combine these algorithms, and find that Supervised Kemeny aggregation is a robust technique that al-ways beats the baseline ranking approach used by Watson for the Jeopardy competition. We further corroborate our results on TREC Question Answering datasets.
 H.3.3 [ Information Search and Retrieval ]: Information filtering, Retrieval models Algorithms Ranking, Question-answering, Rank-aggregation  X  The part of this work was done during the internship at IBM T. J. Watson Research Center, Yorktown Heights, NY  X  The work was done during the employment at IBM T. J. Watson Research Center, Yorktown Heights, NY.

Question answering (QA) is the task of finding an exact answer to a user X  X  natural language question. In February, 2011, the IBM Watson QA system beat human grand champions in the game of Jeopardy!, marking a major milestone in QA research. This was made possible by great advances in all stages of the QA system pipeline, including question analysis, search, hypothesis genera-tion, and hypothesis scoring. One of the key steps at the end of this pipeline is ranking the candidate answers produced by weighing all the evidence extracted and scored in support of each answer. In Watson, this task is solved by using a Machine Learning approach, where a model is built based on a set of training questions with known answers, and then used to score new question-answer pairs. This task can be viewed as a special case of rank-learning, and as such recent advances in learning to rank [6] may be effectively ap-plied here. In this paper, we provide an extensive study of different approaches to ranking candidate answers as applied to QA.
Most recent advances in learning to rank have been driven by web search [8]. While the task of ranking candidate answers in QA is similar to document-retrieval, there are some key differences: (1) Instead of relevance, which can be multi-graded in search, we just have binary relevance judgments, corresponding to the right or wrong answer. (2) The data has high class imbalance. While search datasets also tend to have a high ratio of irrelevant to relevant docu-ments, in the case of QA, there is typically only one correct answer. (3) Search systems are typically evaluated on an entire ranked list or the top 10 results produced, using metrics such as ERR, NDCG, and MAP. However, in most QA systems the desired goal is to get the top-ranked answer right [15, 36]. As such Precision@1 be-comes the primary metric to optimize.

The above distinctions make answer-ranking in QA a special case of the learning to rank problem, and observations made in the context of document retrieval may not hold. This paper makes the following contributions: (1) To the best of our knowledge this is the first work that provides an extensive comparison of several ap-proaches to learning to rank, applied specifically to answer-ranking in open-domain QA (Jeopardy), as well as in the highly special-ized domain of medicine. (2) We show how results can be im-proved using a cascading approach, where the rankings produced by one ranker are provided as inputs to another. Such a cascading approach not only gives better performance, but also speeds up run-time prediction. (3) We demonstrate how multiple rankers can be effectively combined using rank aggregation techniques stemming from the Social Choice Theory literature. As will be seen later in our experiments, in one dataset, logistic regression performs best, while in other, LambdaRank perform best; and a combined sys-tem performs better than any individual ranker in both cases. (4) In practice, since it is hard to find a single ranker that performs best on all datasets, our proposed system is a practical solution to this prob-lem of having to rely on one ranker. Our system does not rely on any single best ranker, rather combines different rankers and gives the best of all. (5) The final system proposed in this paper is both efficient and robust, giving improvements of 3.4 and 2.2% points in Precision@1, over the algorithm used by Watson on the Jeopardy and Medical task respectively. We show similar improvements in performance on publicly available TREC QA datasets.

We believe our work furthers the state-of-the-art in Open Do-main QA by showing how learning to rank (LETOR) can be ef-fective for the task. At the time of running the TREC QA tasks which ended in 2007, pointwise classification methods like logis-tic regression and Maximum Entropy Models were state of the art for the final answer selection component [19, 27, 32]. Many ad-vances in Machine Learning for Information Retrieval tasks have been made since then. We provide an extensive comparison of sev-eral approaches of LETOR and Rank Aggregation for QA and im-prove over the baseline Watson system built for Jeopardy. Given the renewed interest in QA with Jeopardy, Siri and the DARPA BOLT Program 1 , this work establishes a baseline for future QA research. We emphasize that improvements on Jeopardy are significant be-cause the system was worked on for several years and obtaining improvements has not been trivial. We begin by providing an overview of the Watson QA system. Watson answers questions by first analyzing the question, gener-ating candidate answers, and then collecting evidence over its text and knowledge base resources supporting or refuting those answers. For each answer, individual pieces of evidence are scored by an en-semble of answer scorers, yielding features capturing the degree to which evidence justifies or refutes an answer. Most features pro-vide some measure of how justified the answer is for the question http://bit.ly/vViNIG according to evidence in text passages or knowledge base infor-mation. However, there are also scores representing question-level features which indicate information about the question (for instance the answer type specified by the question) as well as answer-level features (for instance the popularity of an answer). An example of a high recall score is a measure of the weighted overlap between terms in a question and a passage. In contrast, an example of a high precision score is a measure of the alignment of grammatical and semantic relationships between the question and passage using a graphical abstraction.

Finally, these features are used to rank candidate answers, based on the likelihood the answer is correct. Crafting successful strate-gies for resolving thousands of answer scores into a final ranking would be difficult, if not impossible, to optimize by hand; so Wat-son was designed to learn over existing questions and their correct answers. From a Machine Learning perspective, a training/test in-stance here, is a feature vector presenting evidence for or against a question-answer pair being correct. Fig. 1 shows an overview of the Watson system pipeline. In this paper, we only focus on the final stage of ranking candidate answers. For a longer description of the machine learning component of Watson the reader is referred to [17].

Architecturally, Watson provides a confidence estimation frame-work which uses a common data model for registration of answer scores and performs machine learning over large sets of training data in order to produce models for answer ranking and confidence estimation. While the ranking component is critical for deploying a full question answering system, it is also essential for developing the system. Developers working on any aspect of the system, from candidate generation to answer scoring, must evaluate the system-wide impact of their component on end-to-end question answering performance. This development methodology requires performing thousands of experiments requiring retraining of the system to as-sess the impact of every potential change. This drives the need for machine learning which is computationally efficient, to allow for rapid experimentation, and which is highly automatic, to achieve valid results without requiring manual parameter tuning or special-ist knowledge of machine learning algorithms to perform each ex-periment. This approach allows developers to focus on optimizing performance of their component and entrust to the framework the assessment and proper combination of the component X  X  scores for the end-to-end question answering task.
We now describe our datasets and evaluation approach.
We evaluate our system on two datasets  X  Jeopardy and Medi-cal. The questions and correct answers for the Jeopardy dataset are obtained from historical records of the American TV quiz show, Jeopardy. In Watson, answer candidates are generated by searching for hypotheses through several sources like Wikipedia, the Bible, IMDB, ebooks on Project Gutenberg, using the INDRI[29] search engine. NLP techniques are used to understand the focus and cate-gory of a question, find the lexical answer type, and finally narrow down on an answer-candidate in the retrieved passages. Details of the techniques used for answer-candidate generation can be found elsewhere [15, 13].

The Medical dataset was obtained from a Jeopardy-like compe-tition held by the American College of Physicians called the Doc-tor X  X  Dilemma . The underlying content being searched in this case includes several medical content sources such as: ACP Medicine, Merck Manual of Diagnosis and Therapy and MKSAP (a study guide from ACP). Several candidate answers are generated per ques-tion using a system similar to Watson but which had been adapted for the medical domain [14].

Although the original datasets contain questions which do not have a correct answer in the candidate set, in our experiments, as in [31], we only consider those questions that have at least one correct answer. Table 1 shows a summary of the data.

For both datasets several hundred features are extracted as de-scribed in the previous section. There are 547 features in the Jeop-ardy dataset and 323 in the Medical dataset. We refer the reader to the references for a detailed description of the NLP components. The focus of this paper is to study the effectiveness of various al-gorithms on ranking the candidate answers. An example question in the Medical dataset is PULMONOLOGY: Diagnosis associated with  X  X gg shell X  calcification of intrathoracic lymph nodes . Wat-son produces candidate answers such as Sarcoidosis , Silicosis and Lymphadenopathy . Our task is to correctly rank these answers, so that the correct answer(s) ( Silicosis ) is placed at the top of the list. In all experiments in this work, the feature set is kept constant and the goal is to evaluate algorithms for ranking.
The output of our system is a ranked list of candidate answers for each question. However, for most QA tasks we only get credit for selecting one correct response  X  for instance, in the Jeopardy! challenge a contestant is only allowed to give one answer. As such, our primary evaluation metric is Precision@1 (P@1), which mea-sures the percentage of questions for which the top-ranked answer is correct.

In some domains, such as Medical QA, there can be more than one correct answer  X  for instance, two valid diagnoses for the same symptoms. In order to evaluate this setting, we also measure Nor-malized Discounted Cumulative Gain [20] at rank K ( K = 5 NDCG@K looks at the top-K candidate answers, and assigns a higher weight to a correct answer that is ranked higher than one that is ranked lower. NDCG@K is computed as: where Q is the total number of queries, and y is the relevance level of candidate answer i for query q . IdealDCG is simply DCG with the ideal ranking. While P@1 only looks at the top-ranked response, NDCG provides a finer granularity for evalu-ation, rewarding algorithms for having a higher rank for a correct answer, even if it is not at the top of the list. All metric numbers in this paper are reported in %.
Over the last decade, learning to rank methods have been effec-tively applied to information retrieval tasks. These methods aim at learning a model that given a query and a set of relevant doc-uments, finds the appropriate ranking of documents according to their relevancy. A question answering task is similar to the infor-mation retrieval task i.e., a question is simply a query and a set of candidate answers is analogous to a set of relevant documents. The goal is now to find the appropriate ranking of these candidate answers according to their correctness. There have been numer-ous learning to rank methods developed, which can be divided into three main categories: pointwise methods, pairwise methods, and listwise methods.

Pointwise methods treat the ranking problem as a standard clas-sification or a regression task. These methods assume that each question-answer pair has either (a) a numerical or ordinal (rank) score associated with it or (b) a relevance label in one of two or more classes. The objective in the former formulation is to find a model that predicts this score correctly through ordinal regression methods [10]. In the latter case, the problem is reduced to classi-fication and can be solved by methods such as SVMs or logistic regression [25].

Pairwise methods like FRank [34], SVMRank [21], RankNet [5], RankBoost [16] aim to learn the pairwise preference of candi-date answers rather than their absolute rank. The intuition behind these approaches is that in information retrieval one cares about metrics like NDCG and MAP which reward a system for a ranking of results as opposed to an absolute prediction of relevance, and modeling preferences is closer to that final objective. In these rank-ing methods, given a ranked set of candidate answers for a query, preferences expressing that one answer is preferred over the other are constructed from each pair of answers. More specifically, each pair of candidate answers is given a binary label { + 1 ,  X  on if the first answer in the pair has a higher rank than the second answer. This construction transforms the original ranking prob-lem into a binary classification problem of predicting these pair-wise preferences. The goal is now to learn a binary classifier that minimizes the number of incorrectly ordered pairs. A total ordering of candidates is inferred from the predicted pairwise preferences. Listwise methods operate on the entire list of candidate answers. Unlike pointwise and pairwise methods where a loss based on the rank of the individual candidate answer or of a pair is minimized; in listwise methods, a direct loss (an appropriate evaluation mea-sure defined by the user) is minimized between the true ranks of the list and the estimated ranks of the list. These methods are the most sophisticated and have been shown to outperform the other two types of methods for information retrieval tasks [28]. Many of these methods allow for the optimization of the final metric relevant to the application. Examples of listwise methods are LambdaRank [28], Coordinate-Ascent [24], AdaRank [37], ListNet [7] and [40].
In this work, we compare the following representative learn-ing to rank methods for each of the 3 categories, applied to the task of question-answering. We emphasize here that these different rankers were selected based their ease of availability and imple-mentation, and also based on their ability to run on a large dataset.
Logistic Regression (LR) is a binary classification method giv-ing a score that is a probability of relevance, and is therefore a natu-ral as well as effective choice for question-answering tasks [27, 19] including for Watson for the Jeopardy challenge [13, 35]. It is a pointwise method, and simply minimizes the logistic loss between the true label of an answer and the estimated label (represented as a real value). At the test time, it assigns a probability of relevance to each candidate answer which can be sorted to get the appropriate Table 2: Performance of different learning to rank methods. Top entry ranking. Although logistic regression has not received much atten-tion in the information retrieval task because of the other sophisti-cated learning to rank methods, it has been successfully used in the question answering task e.g. Watson system, to rank answers. The team working on the Watson system has consistently found logistic regression to beat other point-wise methods and logistic regression was used in the final Jeopardy game[17].
 RankBoost [16] is a pairwise ranking method based on boosting. RankBoost first constructs the pairs (as would be done for any pair-wise method) based on the preference order, and assigns each pair a label +1 if the first answer is ranked higher than the second, and -1 if the second answer is ranked higher that the first; and transforms the ranking task into a binary classification task. RankBoost then applies the boosting algorithm on this classification task. It starts with equal weights assigned to all pairs, and in each round, uses weak rankers to reassign the weights. The pairs that are correctly ranked are given lower weights while those incorrectly ranked are given higher weights. In the end, a linear combination of these weak rankers is used for the final ranking.

AdaRank [37] is similar to RankBoost except that it is a list-wise approach. The weak rankers in AdaRank are learned by di-rectly optimizing ranking measures such as NDCG or MAP, unlike in RankBoost, where a pairwise loss is minimized.

Coordinate-Ascent is a listwise method proposed by [24]. It is a linear feature-based method that directly optimizes the ranking measure using the well-known optimization method i.e., coordi-nate ascent. Optimization is performed cyclically, optimizing one parameter at-a-time while keeping others fixed.

LambdaRank [28] is a listwise method based on the pairwise ranking method, RankNet [5]. LambdaRank is based on the idea that in order to learn a model the actual value of the loss function is not needed, in fact only the gradient of the loss function is suffi-cient. Once a gradient is known, it can be used with standard op-timization methods e.g. gradient descent to minimize the original cost function. An intuitive technique is used to compute the gradi-ent for different evaluation measures like NDCG and MAP and is used to learn the model.

Note that our datasets are relatively large (  X  4GB, both train and test) and some methods, such as SVMRank [21] took more than a day to train on the smaller (Medical) dataset, and were therefore discarded for this study.

As a sanity check of our implementations, we experimented and found that all rankers (except logistic regression, as it is unsuitable for multivalued data) were quite competitive when evaluated on the Yahoo! dataset provided as a part of a recent learning to rank challenge [8].
We evaluated the performance of the different learning to rank methods on the Jeopardy and Medical datasets. For logistic regres-sion, we used the implementation in WEKA [18]. For RankBoost, Coordinate-Ascent, and AdaRank, we used RankLib [11]; where AdaRank and Coordinate-Ascent optimized P@1. For LambdaRank, we used our own implementation, written to optimize NDCG (as it cannot directly optimize P@1). For all rankers, we used the default hyperparameters suggested by authors, without any tuning.
Results are presented in Table 2 for two metrics P@1 (top en-try in each cell), and NDCG@10 (bottom). Given its computa-tional complexity, Coordinate-Ascent did not terminate on Jeop-ardy (after 2 days), and as such we only present its results on the smaller dataset (Medical). We see that despite being the sim-plest, the pointwise method logistic regression performs best on both datasets and on both metrics, followed by LambdaRank and RankBoost. This behavior is in contrast to the one noted by the information retrieval community, where listwise methods usually outperform pairwise methods, and pairwise methods outperform pointwise methods [28]. This could be the case for the following reasons: (1) Pointwise methods may be desirable in this degener-ate case of learning to rank, where we only have binary relevance and mostly just 1 correct answer amongst many incorrect answers. We will see in the next section, that the other rankers often outper-form logistic regression when there is less of a class imbalance. (2) The features generated over the years of Watson development were tested with logistic regression. So there might be an inherent bias in feature generation and selection in favor of logistic regression. This ordering in performance is not what we expect in general  X  as we have independently verified by applying the above rankers on web search data.

In the section, we will provide a mechanism that will not only improve on each of these results, but also speed up the training time, allowing us to fill in the missing entries corresponding to Coordinate-Ascent in Table 2.
As discussed previously Question Answering for factoid ques-tions is a high accuracy task and one is typically interested in pro-viding one (or very few) precise answers. Matveeva et al. ([23]) showed that training a learning to rank algorithm (Rank-Net in their case) using the top N results from an initial ranking helped improve precision at the top of the ranked list for web-search. In this section we study the impact of restricting the training set for the learning to rank algorithms to the top N candidates determined by the baseline Logistic Regression trained on all the training data. The idea be-hind such a pruning technique is that if the first stage is reasonably accurate, the re-ranking phase can focus on improving the precision at high ranks. The motivation is different from active learning or boosting where the most difficult examples are added to the training set in subsequent training phases.

In the experiments in this section we train a base logistic regres-sion using all training data. A new model is trained using the top N answers for each query. For each such model trained we test its performance on the top N candidates as ranked by the base logistic regression on the test data. We repeat this procedure for all rankers including logistic regression and the results are shown in Figure 2.
From the figure, all rankers including logistic regression show significant improvements due to pruning on Jeopardy. When N logistic regression achieves a P@1 of 72.2% which is a statisti-cally significant improvement compared to the baseline model X  X  P@1 of 69.0%. When N = 5, the performance of LambdaRank (P@1=71.8%) is on par with that of logistic regression and their performances are statistically indistinguishable.

On the medical data all rankers show improvements in going from using all the data to pruning at 50. Unlike Jeopardy however, all the rankers achieve their peak performance at different prun-ing thresholds. LambdaRank peaks at N = 20 and further prun-ing seems to hurt its performance. To understand this effect, we list the Success@K metric for the two datasets for the base Logis-tic Regression model. The Success@K metric is the percentage of queries for which a relevant result exists in the top-K results. While on the Jeopardy data, even for K = 5 the base Logistic Regression model achieves 90% success, on the Medical data this value sharply drops after K = 20. We suspect that the quality of the initial ranker is a factor that determines the extent of pruning that can be done for a corpus. The impact of pruning on other metrics like NDCG was similar to P@1.
 We also experimented with using the models trained on the top N candidates to re-score the entire test data but found that our ap-proach of pruning the test data was slightly better. Such pruning of data for training and testing the second pass rankers has the advan-tage of speeding both train and test times allowing us to use more complex rankers that might not be feasible on a large dataset. For example, while we were unable to run Co-ordinate Ascent on the entire Jeopardy data, pruning the results to N = 50 allowed us to evaluate and use this ranker. An additional advantage of pruning is that it reduces the class bias and therefore allows one to benefit from more sophisticated learning to rank methods, an observation also made in [35]. In our experiments, we have observed (see Fig-ure 2) that in case of full data, pointwise methods such as logistic regression outperform pairwise and listwise methods, but when the data is pruned, listwise methods e.g., LambdaRank start to take over.
 Table 3: Success@K for the baseline ranker, on the two corpora
In the next section, we will improve on these results even further by combining these different rankers which will be the output of our final ranking system.
The performance of individual rankers vary significantly across different datasets and training set sizes, as seen in Fig. 2. This suggests that instead of relying on the single best ranker, it may be better to combine all rankers to produce a more robust and ac-curate ranking. Since individual rankers produce an ordering of elements, and not a pointwise score that can be meaningfully ag-gregated, we can leverage approaches to aggregating rankings that have been studied in Social Choice Theory, and successfully ap-plied to meta-search [12].

We begin by formally defining the rank aggregation task. Given a set of entities S , let V be a subset of S ; and assume that there is a total ordering in V . We are given r individual rankers who specify their order preferences of the m candidates, where m is size of V , i.e.,  X  i =[ d 1 ,..., d m ] , i = 1 ,..., V , j = 1 ,..., m .If d i is preferred over d j we denote that by d Rank aggregation function  X  takes input orderings from r rankers and gives  X  , which is an aggregated ranking order. If V equals S , then  X  is called a full list (total ordering), otherwise it is called a partial list (partial ordering).

All commonly-used rank aggregation methods, satisfy one or more of the following desirable goodness properties: Unanimity, Non-dictatorial Criterion, Neutrality, Consistency, Condorcet Cri-terion and Extended Condorcet Criterion (ECC) [1]. We will pri-Algorithm 1 Supervised Kemeny Ranking (SKR)
Input:  X  i =[  X  i 1 ,...,  X  im ] ,  X  i = 1 ,..., r , ordered arrangement of m candidates for r rankers. w =[ w 1 ,..., w r ]  X  where w i is the weight of ranker i  X  =[  X  1 ,...,  X  m ]  X  initial ordered arrangement of m candidates k  X  the number candidates to consider in each ranker X  X  preference list ( k  X  m )
Output:  X   X  rank aggregated arrangement of candidates in de-creasing order of relevance marily focus on ECC, defined below: D
EFINITION 1. The Extended Condorcet Criterion [33] requires that if there is any partition { C , R } of S , such that for any d and d j  X  R a majority of rankers prefer d i to d j , then the aggregate ranking  X  should prefer d i to d j .

The ECC property is highly desirable in our domain, as it elimi-nates the possibility of inferior candidates in a ranking to affect the choice between superior candidates. In other words, it offers the property of Independence of Irrelevant Alternatives. Additionally, ECC is a relaxed form of Kemeny optimal aggregation (defined be-low), where the partition C and R are arranged in the  X  X rue X  order, but not necessarily the elements within partitions C and R . In addi-tion to the desirable theoretical properties, ECC proves to be very valuable in ranking in practice [12, 30], as is further corroborated in our experiments.

We will focus on two classical rank aggregation techniques in this paper: Borda and Kemeny, described below: Borda Aggregation: In Borda aggregation [4] each candidate is assigned a score by each ranker. The score for a candidate is the number of candidates below it in each ranker X  X  preferences. The Borda aggregation is the descending order arrangement of the av-erage Borda score for each candidate averaged across all ranker preferences. Though Borda aggregation satisfies neutrality, mono-tonicity, and consistency, it does not satisfy the Condorcet Crite-rion [38] and ECC. In fact, it has been shown that no method that assigns weights to each position and then sorts the results by ap-plying a function to the weights associated with each candidate sat-isfies the Extended Condorcet Criterion [12]. This includes point-wise classifiers like logistic regression. This motivates the use of order-based methods for rank aggregation that do satisfy ECC. Kemeny Aggregation: A Kemeny optimal aggregation [22] is an aggregation that has the minimum number of pairwise disagree-ments with all rankers, i.e., a choice of  X  that minimizes: Precision@1 . 1 72 . 2 72 . 3 72 . 0 72 . 0 72.4 72.4 . 3 82 . 3 82 . 3 82 . 1 82 . 3 82.4 82.4 . 4 43 . 3 44 . 0 43 . 8 44 . 0 45.5 45.5 (pruned@20) 54 . 7 53 . 6 54 . 6 48 . 0 55.7 53 . 3 55 . 0 54 . 9 54 . 9 55 . 3 55.7 55.7 where the function k (  X  ,  X  ) is the Kendall tau distance measured as |{ ( i , j ) | i &lt; j ,  X  ( i ) &gt;  X  ( j ) , but  X  ( i ) &lt;  X  ( denote the position of i in ranking  X  .

Kemeny aggregation satisfies neutrality, consistency, and the Ex-tended Condorcet Criterion. Kemeny optimal aggregation also has a good maximum likelihood interpretation [39]. Suppose there is an underlying  X  X orrect X  ordering  X  of S , and each order  X  is obtained from  X  by swapping pairs of elements with some prob-ability less than 1 / 2. That is, the  X   X  X  are  X  X oisy X  versions of A Kemeny optimal aggregation of  X  1 ,...,  X  r is one (not necessarily unique) that is maximally likely to have produced the  X   X  X .
While Kemeny aggregation is optimal in the sense described above, computing a Kemeny aggregation is NP-Hard for r  X  4 [12]. So in practice, we use an approach that produces a 2-approximation of Kemeny optimal aggregation, referred to as Approximate Ke-meny in [30]. The Approximate Kemeny can be described simply as a Quick Sort on elements using the majority precedence relation as a comparator, where d i d j if the majority of input rankings has ranked d i before d j . In [30], Approximate Kemeny is shown to satisfy the ECC property. Unless otherwise specified, we will use Kemeny to refer to this approximation in the rest of the paper.
Instead of using total orderings provided by each ranker, we can use partial orderings (for a subset of candidates). Since identify-ing relevant candidates at the top of the list is more important, we use partial orderings corresponding to the top k candidates for each ranker. In our experiments, we use the top-ranked 50% of candi-dates for each ranker, and refer to this variant as Kemeny Top-k . Supervised Rank Aggregation: Kemeny and Borda aggregation, being motivated from Social Choice Theory, strive for fairness and hence treat all rankers as equally important. However, fairness is not a desirable property in our setting, since we know that some individual rankers perform better than others in answer-ranking. If we knew a priori which rankers are better, we could leverage this information to produce a better aggregate ranking. In fact, given the ordering of a (validation) set of candidates, we can estimate the performance of individual rankers and use this to produce a better ranking on a new set of candidates.
 In order to accommodate such supervision, Supervised Kemeny Ranking [30] extends Approximate Kemeny aggregation to incor-porate weights associated with each input ranking. The weights correspond to the relative utility of each ranker; so for our exper-iments we use weights proportional to the P@1 computed on the training set. The pseudo-code for Supervised Kemeny Ranking is presented in Algo. 1. Analogously to the unsupervised setting, we use Supervised Kemeny Top-k to refer to SKR with k set to 50% of m .

As in SKR, for Supervised Borda (See Algo. 2), we incorpo-rate performance-based (Precision@1) weights in Borda aggrega-tion by taking weighted averages of Borda scores instead of simple averages. A similar approach to supervised Borda was used in [2].
Given the 5 base rankers as inputs, we compare the 6 rank aggre-gation methods described above. Since we are trying to establish if rank aggregation can further improve results, we pick the pruning setting with the best individual ranker performance, i.e. pruning at 20 for Medical and 5 for Jeopardy. The results of these experi-ments are presented in Table 4. Following Matveeva et al. ([23]), any metric@K for datasets pruned@N when K &gt; N , was computed by appending the datasets with the next K  X  N examples from the unpruned but ranked data. For example, we appended Jeopardy pruned@5 with the 5 next examples from the logistic regression Algorithm 2 Supervised Borda
Input:  X  i =[  X  i 1 ,...,  X  im ] ,  X  i = 1 ,..., r , ordered arrangement of m candidates for r rankers. w =[ w 1 ,..., w r ]  X  where w i is the weight for ranker i
Output:  X   X  rank aggregated arrangement of candidates in de-creasing order of importance pruning step to compute NDCG@10. This is reasonable for an ap-plication where K results must be shown. In addition to individual ranker performance, we also present the mean performance across rankers. It is clear from the results that all rank aggregation tech-niques perform better than the mean performance of rankers, both in terms of P@1 and NDCG@10. While better than the mean, the unsupervised aggregation techniques are outperformed by the best single ranker. Additionally, note that the best individual ranker is different for each corpus; while it is logistic regression for Jeop-ardy, it is LambdaRank for Medical. So in the absence of hindsight, it is always beneficial to use rank aggregation for more robust re-sults, with lower variance.

The unsupervised aggregation techniques implicitly assume all rankers are equally valuable; as such the performance of the best ranker may be pulled down by other rankers. However, by incor-porating performance-based weights for each ranker, Supervised Kemeny Ranking is able to do even better than the best individual rankers in terms of P@1 without loss on NDCG@10. Since we are already aggressively pruning to the top 20 and top 5 in these datasets, further focusing on the top 50% of these candidates, as done in Kemeny Top-k does not improve results. Also Borda, the weaker aggregation technique, is not competitive even with super-vision. See [30] for a deeper analysis of Kemeny versus Borda.
Based on these results, we recommend always using multiple base rankers and supervised rank aggregation, rather than relying on the best individual ranker. Figure 3 shows our proposed system with its different stages. The initial set of candidates are ranked by logistic regression and then the top N candidates are re-ranked by different rankers. The output of the rankers is aggregated and the top K results as seen fit for the application are shown to the user. Since logistic regression and the different re-rankers use the same feature-set in our setup and only the top N (20 or less) candidates are re-ranked, the over-head due to the additional stages is minimal.

Table 5 summarizes the performance of our proposed system in comparison with the baseline logistic regression. We also report two new metrics in this table: RR@5 and RR@10. The reciprocal rank (RR) is the average of the reciprocal of the rank at which the first correct answer is found. RR@K limits the ranked list per ques-tion to K . All metrics show improvement on both datasets. P@1 NDCG@5 79 . 3 80 . 8 50 . 5 51 . 3 NDCG@10 80 . 9 82 . 4 54 . 6 55 . 7 Table 5: Supervised Kemeny vs baseline. An underline indi-cates significance with a paired t-test at the 0.05 level. on Medical is significant at the 0.1 level (p=0.09) and NDCG@10 and RR@10 are significant at the 0.05 level. For our Medical ap-plication, the P@1 improvement of 2.2% points is substantial in practice. However, it appears that our test set was not large enough to establish statistical significance here. On the larger Jeopardy test set, all metrics show significant improvements even at the 0.01 level. Our final system therefore always shows improvement re-gardless of the performance of the individual rankers, and thus an application developer can use it as a black box without evaluat-ing individual rankers. Rank aggregation therefore provides for a robust system combination strategy that we hope to test on other domains that Watson can be applied to.
In addition to our application domains of Jeopardy! and Medical, we also ran experiments on the publicly-available TREC Question Answering datasets. The TREC tracks have largely focused on fac-toid questions e.g.  X  X hat is David Lee Roth X  X  birthday? X  We used data from the TREC 8-10 and TREC 12 evaluations [36] available at the NIST website (http://trec.nist.gov/data/qamain.html). We only considered questions that had at least one candidate answer from the first pass retrieval stage, giving a total of 1,128 questions of which 794 were randomly partitioned into the training set and the remaining 334 made the test set. On average both the training and test sets contain 2 correct answers per question from about 93 can-didate answers retrieved from the initial retrieval.

In addition to the features used for the Jeopardy! dataset, 3 domain adaptation changes were made: first, the question anal-ysis component was modified to address the differences in ques-tion formats between Jeopardy! and TREC. Second, we included Ephyra [26] and PIQUANT [9] question answer systems X  static type based candidate generation components. Third, we included the AQUAINT corpus, the 3 GB news wire corpus used in TREC evaluation, in our searches. The domain adaptation resulted in 6 new features over those that were used on Jeopardy. Previous work [15] has shown the competitiveness of these additional features on top of the Jeopardy features for the TREC task.

Table 6 shows the performance of individual rankers with a prun-ing setting of 50. The learning to rank methods are quite competi-tive on both P@1 and NDCG@10 with LambdaRank equaling the performance of logistic regression on P@1 and Coordinate-Ascent being slightly better. All aggregation methods show improvement over the mean performance of individual rankers. As before, we see that the best performance is produced by Supervised Kemeny Ranking, both in terms of P@1 and NDCG@10. Table 7 compares Supervised Kemeny with a pruning threshold of 50 to the baseline logistic regression trained on all candidates. The results show that we achieve consistent improvements over the baseline on 5 differ-ent metrics. In additional experiments on testing different prun-ing settings we find that, while Supervised Kemeny improves over the baseline, it is not always the approach with the highest perfor-mance. However, the best performance, in general, is still achieved by one of the rank aggregation approaches.
Related work in the area of QA and learning to rank has been already introduced at various points. In this section we compare our work with relevant work in learning to rank for QA. Although learning to rank methods have extensively been applied to web search, the application to QA has been limited. In fact, to the best of our knowledge, Verberne et al. ([35]) is the only work where learning to rank methods have actually been applied to the QA task with the focus on the learning component  X  unlike others [3, 31] where the focus has been on feature generation. Although similar in spirit, Verberne et al. only study the behavior of various learning to rank methods on the QA task; while in this work, we not only study this behavior, but also provide a robust multi-stage system that is able to improve over the results obtained by simply applying learning to rank methods. Additionally, their work was restricted to why questions while we work on factoid questions. We note that our findings from the ranking stage of this multi-stage system corroborate the observations of Verberne et al., that pairwise and listwise methods are not always superior to pointwise methods.
We have presented a multistage approach to learning to rank can-didate answers in a QA system. The final system proposed is ef-ficient and robust, giving significant improvements over the state-of-the-art Watson baseline, both in open-domain QA as well as in the specialized discipline of Medicine. Our approach is also eas-ily extensible, in that, many more base rankers may be added to our aggregation, which may lead to further improvements. While we evaluated our methods on candidate answers generated by Wat-son, the results are applicable to any QA system, and may be more broadly applicable to other rank-learning tasks.
Additional authors: James Fan(IBM T. J. Watson Research Cen-ter, email: fanj@us.ibm.com ). [1] K. Arrow. Social choice and individual values. New Haven: [2] J. A. Aslam and M. Montague. Models for metasearch. In [3] M. Bilotti, J. Elsas, J. Carbonell, and E. Nyberg. Rank [4] J. Borda. Memoire sur les elections au scrutin. In Histoire de [5] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, . 9 65 . 3 65 . 9 66 . 5 65 . 6 66.8 66.8 (pruned@50) 70 . 1 66 . 4 70 . 6 70 . 4 69 . 9 69 . 5 70 . 4 70 . 6 70 . 7 70 . 6 71.5 71 . 1 [6] C. J. C. Burges, K. M. Svore, P. N. Bennett, A. Pastusiak, [7] Z. Cao, T. Qin, T. Liu, M. Tsai, and H. Li. Learning to rank: [8] O. Chapelle and Y. Chang. Yahoo! learning to rank challenge [9] J. Chu-Carroll, P. A. Dubou X , J. M. Prager, and K. Czuba. [10] K. Crammer and Y. Singer. Pranking with ranking. In [11] V. Dang. Ranklib -a library of ranking algorithms. http: [12] C. Dwork, R. Kumar, R. Naor, and D. Sivakumar. Rank [13] D. Ferrucci. Building watson: An overview of the deepQA [14] D. Ferrucci, A. Levas, S. Bagchi, D. Gondek, and E. Mueller. [15] D. A. Ferrucci, E. W. Brown, J. Chu-Carroll, J. Fan, [16] Y. Freund, R. Iyer, R. Schapire, and Y. Singer. An efficient [17] D. C. Gondek, A. Lally, A. Kalyanpur, J. W. Murdock, P. A. [18] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, [19] A. Ittycheriah and S. Roukos. IBM X  X  statistical question [20] K. J X rvelin and J. Kek X l X inen. Cumulated gain-based [21] T. Joachims. Optimizing search engines using clickthrough [22] J. Kemeny. Mathematics without numbers. In Daedalus , [23] I. Matveeva, C. Burges, T. Burkard, A. Laucius, and [24] D. Metzler and B. Croft. Linear feature-based models for [25] R. Nallapati. Discriminative models for information retrieval. [26] S. Nico, J. Ko, J. Betteridge, M. Pathak, E. Nyberg, E, and [27] J. Prager, E. Brown, A. Coden, and D. Radev.
 [28] C. Quoc and V. Le. Learning to rank with nonsmooth cost [29] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. Indri: a [30] K. Subbian and P. Melville. Supervised rank aggregation for [31] M. Surdeanu, M. Ciaramita, and H. Zaragoza. Learning to [32] J. Suzuki, Y. Sasaki, and E. Maeda. Svm answer selection for [33] M. Truchon. An extension of the condorcet criterion and [34] M. Tsai, T. Liu, T. Qin, H. Chen, and W. Ma. Frank: a [35] S. Verberne, H. van Halteren, D. Theijssen, S. Raaijmakers, [36] E. M. Voorhees. Overview of the TREC 2003 question [37] J. Xu and H. Li. Adarank: a boosting algorithm for [38] H. Young and A. Levenglick. A consistent extension of [39] H. P. Young. Condorcet X  X  theory of voting. In American [40] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support
