 Department of Computer Science and Technology, Xi X  X n Jiaotong University, Xi X  X n, Shaanxi, China 1. Introduction important research topic in machine learning and data mining. It can not only reduce the dimensionality of the data, but also improve a learner in terms of learning performance, generalization capacity and model simplicity.

Generally, feature subset selection aims at removing irrelevant and redundant features as many as and redundant features do not contribute to getting a better predictor for that the most information they carried is already present in other feature(s) [3]. Thus, a number of feature subset selection algorithms have been proposed to handle the irrelevant features or/and redundant features. Of these algorithms, some of them can effectively remove irrelevant features but fail to identify the redundant [4 X 6], yet some of others can remove the irrelevant features while taking the redundant ones into account [7 X 10].
However, feature interaction is a nonnegligible issue in practice. It has been noticed in feature subset F 1 and F 2 are two boolean variables, C denotes the target concept, and  X  represents the xor operation. F 1 or F 2 is irrelevant with C when each is individually evaluated, but they become very relevant when we combine them together. Therefore, removing the interactive features will lead to poor predictive accuracy. The feature subset selection algorithms should eliminate the irrelevant and redundant features while considering the feature interaction. Unfortunately, only a few of them can deal with not only irrelevant and redundant features but also interactive ones [11,12].

Association rule mining can discover the interesting associations among data items [13], hence it has been used to build classifiers with better classification accuracy compared with other types of classi-However, they only evaluated the relevance between feature(s) and target concept without considering redundant and interactive features.

An association rule is an expression of A  X  C ,where A (Antecedent) and C (Consequent) are both of itemset. If we view A as the feature(s) and C as the feature(s)/the target concept, association rules
In this article, we propose a F eature subset sE lection A lgorithm based on aS sociaT ion rule mining into consideration. FEAST uses association as the measure to evaluate the relativity between feature(s) and the target concept, which is quite different from the traditional measures, such as the consistency measure [9,12,20 X 23], dependence measure [8,10,24], distance measure [5,6,25,26] and information theory measure [27 X 30]. The association measure evaluates irrelevant, redundant and interactive features in a uniform way, and it is at least a potential alternative for feature subset selection.
The proposed feature subset selection algorithm FEAST is tested upon both five synthetic data sets and 35 real world data sets. The experimental results show that, FEAST can effectively identify the relevant features while taking the redundant and interactive features into account. Compared with other seven representative feature subset selection algorithms, FEAST not only reduces the feature number, but also improves the performance of the five well-known different types of classifiers.
The rest of the article is organized as follows: In Section 2, we describe the related work. In Sec-tion 3, we present the new feature subset selection algorithm FEAST. In Section 4, we introduce the proposed support and confidence threshold prediction method for FEAST. In Section 5, we provide the experimental results. Finally, we summarize our work and draw some conclusions in Section 6. 2. Related work
Feature subset selection has been an active research topic since 1970 X  X , and a great deal of research work has been published.

Of the existing feature selection algorithms, most of them can effectively identify the irrelevant fea-tures based on different evaluation functions. But not all of them can eliminate the redundant features while taking the feature interaction into consideration [31,32]. Thus, the existing feature subset selec-tion algorithms can be generally grouped into three categories: i) the algorithms only handle irrelevant irrelevant and redundant features while taking feature interaction into consideration.
Traditionally, feature subset selection research has focused on searching for relevant features. Feature weighting/ranking algorithms [33 X 37] weigh features individually and rank them based on their rele-vance to the target concept. A well-known example is Relief [6], which weighs each feature according the redundant features. Liu and Setiono [35] proposed to rank features using Chi-Square statistic. This proposed to select feature subset from trained neural network using the Genetic Algorithm (GA). The GA is used to find the optimal relevant features, which maximize the output function of trained neural network. Unfortunately, it is also incapable of removing redundant features.

Besides irrelevant features, redundant features also affect the speed and accuracy of learning algo-rithms and thus should be eliminated as well [27,39]. CFS [8], FCBF [10], MOD-Tree [40], MIFS [7] and CMIM [41] are examples that take the redundant features into consideration. CFS [8] is achieved uncorrelated with each other. FCBF [10] gives the definition of redundant peer between features based on the symmetric uncertainty measure, and presents a framework of efficient feature selection via rele-vance and redundancy analysis. MOD-Tree [40] identifies the redundant features based on the oblivious decision tree. MIFS [7] uses the mutual information as the evaluation function to identify the relevant features and eliminate the redundant features. CMIM [41] iteratively picks features by maximizing their mutual information with the class to predict. It does not select any feature being similar to the already picked ones, as it does not provide additional information about the class to predict. All these algo-rithms can effectively remove irrelevant and redundant features but not take into the feature interaction consideration.

Moreover, feature interaction is drawing more attention in recent years. That is, features which ap-pears irrelevant singly may become highly relevant when combined with others. There can be two-way, three-way or complex multi-way int eractions among featur es [42]. Thus, Jakulin and Bratko [11] suggest feature and the class) and 3-way (two features and the class) interactions. Zhao and Liu [12] propose an algorithm INTERACT where the feature interactions are implicitly handled by a carefully designed fea-ture evaluation metric and a search strategy with a specially designed data structure. Chanda et al. [30] believe statistical interactions can capture the multivariate inter-dependencies among features, and em-ploy these statistical interactions to improve feature subset selection.

Recently, Xie et al. [19] have utilized the association rules for feature subset selection. They calcu-late the antecedent union set of the corresponding association rules whose consequences are the same our algorithm aims to eliminate the irrelevant and redundant features, and takes the multi-way feature interactions into consideration meanwhile, hence is quite different from the front algorithms. 3. Feature subset selection algorithm proposed feature subset selection algorithm. 3.1. Strong, classification and atomic association rules
Association rule mining searches for interesting relationships among items in a data set D .Let I = { i 1 ,i 2 ,  X  X  X  ,i k } be a set of items, an association rule is an implication of form A  X  B ,where A  X  I , B  X  I ,and A  X  B =  X  .

The support, confidence and lift are three important measures of a rule X  X  interestingness. 1. The support of rule A  X  B is the percentage of instances in D that contain both A and B , denoted 2. The confidence of rule A  X  B is the percentage value that shows how frequently B occurs among 3. The lift of rule A  X  B is denoted as Lift ( A  X  B )= P ( B | A ) /P ( B ) ; this measure reflects the Typically, association rules are considered interesting if they satisfy minimum support threshold ( min-Supp ), minimum confidence threshold ( minConf ) and minimum lift threshold ( minLift ). Usually minLift is set to 1, minSupp and minConf can be set by users or domain experts. Based on these three thresholds, strong association rule (SAR) can be defined as follow.

For the sake of introducing classification association rule (CAR) and atomic association rule (AAR), we first give the concepts of feature value itemset (FVIS) and target value itemset (TVIS).
Takethedataset D xor in Table 1 for example, the feature space F = { F 1 ,F 2 ,F 3 ,F 4 } ,thetarget TVIS can be obtained as follows: FVIS = { F 1 =0 ,F 1 =1 ,F 2 =0 ,F 2 =1 ,F 3 =0 ,F 3 =1 ,F 4 = 1 ,F 4 =0 } ,TVIS = { Y =0 ,Y =1 } .

With the definitions of FVIS and TVIS, classification association rule (CAR) and atomic association rule (AAR) are defined as follows.
 if and only if: Here, the | Z | denotes the cardinality of set Z .
 Definition 3. Atomic association rule (AAR). A rule r : A  X  C is an atomic association rule if and only if:
All CARs constitute classification association rule set (CARset). All AARs constitute atomic as-sociation rule set (AARset). In order to better understand the definitions based on CAR and AAR in the following section, the CARset and AARset of the data set D xor (see Table 1) are generated under minSupp = 20%, minConf = 70% and minLift = 1 as examples (see details in Appendix A). 3.2. Definitions of relevant, redundant and interactive features on association rules.
 concept Y if and only if: Otherwise, f ij is said to be an irrelevant feature value (iRelFV).
 From Definition 4 we can know that the feature values appearing in the antecedent of a rule r  X  CARset are relevant feature values; otherwise, the feature values that never appear in the antecedent of any rule r  X  CARset are irrelevant feature values.

As we know, classification association rules have been extensively used in classification [14 X 18], and these classification algorithms usually possess relatively better classification accuracy. This indicates The feature values appearing in the antecedents of CARs are necessary and related to the target concept. So it is reasonable to identify the relevant feature values by Definition 4.

For example, in data set D xor , F 1 = 0 is relevant since it appears in the antecedent of CAR: { F 1 = 0  X  F 3 =0 } X  X  Y =0 } ,and F 4 = 0 is irrelevant because there does not exist any CAR  X  CARset whose antecedent contains F 4 = 0 (see Appendix A). What X  X  more, F 1 is relevant to the target concept Y of D xor as Y = F 1  X  F 2 ,and F 4 is irrelevant to Y since its values are randomly preassigned. This indicates the Definition 4 can be used to detect irrelevant values from the relevant ones.
However, the feature value appearing in a CAR X  X  antecedent maybe redundant. i.e., two closely-correlated feature values will be simultaneously appearing in the CAR X  X  antecedent. This is because that the association rules are mined based on frequent itemset mining (FIM) [43], but FIM cannot detect the redundant items (i.e., feature values). Because that for a given feature value, if it is frequent and into the frequent itemset as well. To overcome this problem, the redundant feature value is defined as follow. Definition 5. Redundant feature value (RedFV). A specific value f of a feature value set (FVset) is redundant if and only if:
From Definition 5 we can know that, of a given feature value set, a feature value is redundant when it appears in the consequent of a rule in AARset and the rule X  X  antecedent is in the given feature value set as well.

As we know, for a redundant feature value, the information it carries is already present in other feature value. This indicates that it is strongly related to and can be replaced by another feature value. What X  X  more, AAR can be used to explore the correlation between two feature values. Thus, Definition 5 based on AAR can be used to detect redundant values.
 For example, in the data set D xor , of a given feature value set { F 2 =1 ,F 3 =0 } , according to Definition 5, F 3 = 0 is redundant as it appears in the consequent of the AAR { F 2 =1 } X  X  F 3 =0 } of F 2 . This denotes it is reasonable to identify the redundant feature values by Definition 5.
It is noticed that Definition 5 only detects the two-way value redundancy (the redundancy between two values). Of course, there might exist multi-way feature value redundancy (the redundancy among multiple feature values). However, detecting all th e multi-way value redundancy is a combination explo-is of a medium size. Therefore, we mainly focus on the two-way redundancy in this paper. and r F , r A and r B be the CARs of FVset  X  X  Y = y } , A  X  X  Y = y } and B  X  X  Y = y } , respectively. Then, the interactive feature value can be defined as follow.
 Definition 6. k th interactive feature value. The values in FVset are said to interact with each other if and only if:
The confidence of an association rule reflects the description ability of the rule X  X  antecedent to its consequent. The higher confidence means the stronger description ability. In Definition 6, the confidence set A or B is not helpful enough in describing the target concept, FVset = A  X  B is more useful. In this case, feature value sets A and B are said to interact with each other.

According to Definition 4, the CARs usually have h igh confidence since their confidence should be at lest greater than minConf . This implies that all the rules with high confidence are included in CARset. value interactions. That is, the feature value interactions can be reserved by the rules in CARset.
For example, in the association rules generated from data set D xor , Conf ( { F 1 =0  X  F 2 =1 } X  X  Y = 1 } )= 100% is greater than both Conf ( { F 1 =0 } X  X  Y =1 } )= 50% and Conf ( { F 2 =1 } X  X  Y = 1 } )= 50%, so there is a 2 th interactive feature value in feature value set { F 1 =0 ,F 2 =1 } .Actually, means the Definition 6 works well in identifying the interactive feature values of D xor .
According to the definitions of relevant feature value (RelFV), irrelevant feature value (iRelFV), redundant feature value (RedFV) and interactive feature value ,wedefine relevant feature , redundant feature and interactive feature in the following.
 Definition 7. Relevant feature (RelFea). Feature F i is relevant to the target concept Y if and only if: Otherwise, F i is an irrelevant feature (iRelFea).

Definition 7 shows that a feature is relevant when at least one of its values is relevant, and all the when it appears in the antecedent of a CAR. Its absence would affect the confidence of the CAR, i.e., the prediction power of the CAR. On the other hand, under the classical definition of relevant feature [2], a relevant features.

For example, the relevant feature F 1 in the example data set D xor would be picked up as a relevant one under this definition since F 1 = 0 is a relevant value.
 Definition 8. Redundant feature (RedFea). Feature F i is redundant if and only if: redundant; ii) some values of this feature are redundant and some are irrelevant. As irrelevant values provide no information about the target concept and redundant values provide the information which is with the property of the classical definition of redundant feature in [3,32].

For example, in data set D xor (see Table 1), according to Definition 8, the feature F 3 is redundant for that F 3 = 0and F 3 = 1 are both redundant identified by Definition 5; the feature F 4 is also redundant only if:
As we known, there is an intrinsic relationship between a feature and its values. The properties of a feature subset can be learned by its value-assignment. Thus, for a given feature subset, it is reasonable that the interaction among this feature subset could be implied and further studied by that among its feature interaction.
 { F 1 =0 ,F 2 =1 } detected by Definition 6. It is consistent with the target concept Y = F 1  X  F 2 of D 3.3. Feature subset selection algorithm
With the definitions of relevant, redundant and interactive features based on association rules, we propose a novel feature subset selection algorithm FEAST, which searches for the relevant features while taking the redundant features and feature interaction into consideration.

Figure 1 shows the framework of FEAST, which consists of four steps: i) Association rule mining , identification . 1. Association rule mining 2. Relevant feature value set discovery 3. Redundant feature value elimination 4. Feature subset identification Algorithm 1 FEAST
Algorithm 1 provides the pseudo-code description of FEAST. Of the input parameters, support thresh-old minSupp and confidence threshold minConf are used as the constraint condition to achieve strong association rules SARs (see Definition 1). 2
The pseudo-code consists of four parts. In part 1 (lines 1 X 2), CARset and set AARset are mined by function FP_growth() [43] on the given data set D according to minSupp and minConf .Inpart2(lines 3 X 5), the union of the antecedents of the association rules in CARset constitutes the relevant feature value set RFVset. Part 3 (lines 6 X 18) eliminates the redundant feature values in RFVset, where function and removed from AARset. Then if its antecedent is a subset of the current RFVset, the value in r  X  X  consequent is eliminated from RFVset; meanwhile, the rules whose antecedents are identical with r  X  X  consequent are also removed from AARset. This process repeats until that AARset is empty. Part 4 (lines 19 X 24) achieves the selected feature subset S according to the feature values in RFVset.
Time complexity of FEAST. In part 1, the CARset and AARset are mined by function FP_growth () whose time consumption is closely related to the value of minSupp [43]. The time complexity of this part can be represented as O(f(minSupp, D)) ,where f(minSupp, D) is a function of minSupp and D ,which increases with the decrease of minSupp or increase of the size of D . For part 2, once a CAR is generated by FP-growth, its antecedent could be merged into RFVset meanwhile, so the consumed time of this part can be ignored. For part 3, the main time consummation is the process of sorting the rules in AARset, so the time complexity of this part is O ( V  X  log V ) (by quick sort), where V is the number of rules in AARset. The time complexity of part 4 is O ( K ) ,where K is the number of feature values in the final RFVset whose maximum value is the number of all possible feature values in D .

In summary, the time complexity of FEAST is O(f(minSupp, D) + O ( V  X  log V )+ O ( K ) .Since part 1 is the major time consumer in the worst case, the efficiency of FEAST largely depends on that of association rule mining, and it is closely related to the setting of the support threshold minSupp . 4. Support and confidence threshold prediction method
In this section, we first present the general view of the proposed support and confidence threshold prediction method, then introduce the data metrics used to predict the support and confidence thresholds for FEAST. Afterward, we provide an approach for identifying the most suitable support and confidence thresholds. Finally, we give the threshold prediction model construction method. 4.1. General view of the method
For the proposed feature subset selection algorithm FEAST, there are two parameters known as sup-port and confidence thresholds, which should be determined beforehand. This is because different sup-performance for the further learning (such as classification). Moreover, these two parameters also affect the executing efficiency of FEAST. Therefore, the predetermination of the most suitable support and confidence thresholds is a critical problem.

In machine learning, a great deal of research has explored the relationships between classification algorithm performance and data set characteristics [44 X 48]. These research firstly uses some kind of tionships between the measured data set characteristics and the classification algorithm performance by decision tree, instance based rule induction methods or regression models. Afterward, these relationships are used to recommend appropriate classification algorithms for a given new data set.

On top of these research, we believe that there is some kind of relationship between data set metrics and the most suitable support and confidence thresholds for FEAST, and therefore propose a support and confidence threshold prediction method.

In the proposed threshold prediction method, data set metrics for a new data set is first extracted and then used to predict the most suitable support and confidence thresholds for FEAST according to the prediction model constructed from a number of existing data sets. Figure 2 shows the framework of this method.
 The proposed method consists of the Prediction model construction with historical data sets and the Threshold prediction for a new data set. The brief description of these two parts is as follows. 1. Prediction model construction 2. Threshold prediction
As the threshold prediction is straightforward, we just focus on the prediction model construction in the following subsections. 4.2. Data set metrics
Data set metrics is something like feature portraying an instance, as it characterizes a data set. Data data sets, and iii) easy to acquire and be related to the feature selection process in FEAST.
As we know, the support and confidence of association rules are calculated according to the frequen-cies of itemsets. Moreover, Tatti [49] used frequency of itemset to characterize data sets and further to compute distances between them. However, searching all possible itemsets in a data set is difficult. Therefore, in our method, the frequencies of the two kinds of item sets, which are 1-and 2-itemsets, 3 are extracted and used to summarize a data set.

Meanwhile, the features identified by a feature subset selection algorithm should be related to the target concept. Thus, measures that are used to describe the target concept should have some kind of relationships with these features. There are two measures which can be employed to represent the target concept: frequencies of the target concept values and mutual entropies [50] between the target concept and features.

To summarize, for a data set, the four sequences extracted to describe it are as follows: i) a frequency sequence of 1-itemsets; ii) a frequency sequence of 2-itemsets; iii) a frequency sequence of target con-cept values; iv) a mutual entropy sequence.
Although the above introduced sequences can be used to describe a data set, unfortunately for the with different number of features, the lengthes of their mutual entropy sequences are also different. It is unable to compare different data sets with these sequences directly. In order to address this problem, the 11 statistical measures listed in Table 2 are used to substitute each sequence. At the same time, the Finally, a total of 46 (4  X  11 + 2) metrics are used to describe data sets. 4.3. Identifying the most suitabl e support and confidence thresholds
In order to obtain the most suitable support and confidence thresholds for FEAST, brute-force search is employed. It is a trivial but very general problem-solving technique that consists of systematically enumerating all possible candidates for the solution and checking whether each candidate satisfies the problem X  X  statement.

In the context of identifying the most suitable support and confidence thresholds for FEAST, firstly for a given data set, different pairs of support and confidence thresholds are systematically enumerated. Then, the FEAST algorithm with each of these candidate pairs and a number of different types of clas-sification algorithms are performed on the given data set with cross-validation. Note that a number of ity of the selected threshold pair and avoid the selected threshold pair being only suitable for certain specific classification algorithm.
 To compare two threshold pairs of support and confidence across all classification algorithms, the Win/Draw/Loss record [51] and the mean accuracy are employed.

A Win/Draw/Loss record presents three values, corresponding to the numbers of classification algo-rithms for which support and confidence threshold pair scPair 1 obtains better, equal, or worse perfor-mance than pair scPair 2 on classification accuracy.

After obtaining the Win/Draw/Loss records and the mean accuracies across all classification algo-as follows: 1. Compare the Win/Draw/Loss counts between support and confidence threshold pair scPair 1 and 3. Move the selected support and confidence threshold pair from the candidate list CL into the rank-The time complexity of the brute-force search for appropriate threshold pair(s) consists of two parts. Firstly, for each pair of thresholds scPair i (1 i n ) , we run all the classifiers with FEAST, and obtain the corresponding mean classification accuracies. The complexity of this part is O ( n  X  represent the complexities of FEAST and the i th classifier, respectively. Afterward, we rank all these threshold pairs according to the sort strategy introduced above and identify the most appropriate pairs. didate pairs, but also to the complexities of FEAST and the training classifiers. Although this procedure the reliability of the further constructed threshold prediction model. 4.4. Threshold prediction model construction
To explore the relationship between the data set metrics and the support and confidence thresholds used in FEAST, a regression analysis method is employed to build the prediction models of support and confidence thresholds.
 for each of the clusters.
 For each cluster, the prediction model is built with instances collected from a number of data sets. Each instance is composed of two parts: the data set metrics, which can be viewed as the independent variables, and the support and confidence thresholds, which can be regarded as the dependent variables.
Since there are 46 metrics and two thresholds, it is a multiple independent variables and multiple dependent variables regression problem. Moreover, as the support and confidence of association rules are independent with each other, the two-dependent-variable regression problem is divided into two single dependent variable regression problems. Therefore, the prediction models of support and confidence thresholds are constructed separately.

However, according to Section 4.3, it is possible that multiple suitable support and confidence thresh-value. Unfortunately, existing regression methods can not handle the case with multiple dependent val-variables: the minimum value, the mean value and the maximum value of the value set. Then, the predic-tion models are built with the three variables and the metrics separately. The model with the minimum square mean error is picked up as the final prediction model.

Generally, standard multiple regression requires ( n 30 or n&gt; 3 k ) 5 [52,53] to guarantee the sta-tistical validity. Moreover, the precision and robustness of the standard multiple regression model are decreased when there exists multi-collinearity 6 among independent variables [52]. Unfortunately, there should exist multi-collinearity among the 46 metrics, and the condition of n 30 or n&gt; 3 k is not always satisfied either. Therefore, standard regression methods will fail to handle these issues. Partial least square regression (PLSR) [54] is a recent technique that generalizes and combines features from principal component analysis and multiple regression. PLSR extracts a set of latent factors (using prin-cipal components analysis) that maximize explanation of the covariance between the independent and dependent variables, then uses these factors to build multiple regression model to predicts the dependent variable(s). This method is particularly suited when there are more independent variables than the in-stances and there is multi-collinearity among independent variables. Thus, PLSR is employed to build support and confidence threshold prediction models. 5. Experimental results and analysis
In this section, firstly, we empirically evaluate the performance of FEAST, and present the experimen-tal results compared with the other seven representative feature selection algorithms upon both synthetic and real world data sets.

Afterward, we also provide the support and confidence threshold prediction results of the proposed threshold prediction method. 5.1. Benchmark data sets 5.1.1. Synthetic data sets
In order to directly evaluate how well FEAST deals with irrelevant, redundant features and feature are employed.

The first two data sets synData1 and synData2 are generated by the data generation tool RDG1 of the data mining toolkit WEKA. 7 The other three data sets about MONK X  X  problems are available from UCI Machine Learning Repository [55]. The five data sets are described as follows. 2. synData2 . There are 100 instances, 11 boolean features denoted as a 0 ,a 1 ,  X  X  X  ,a 9 and a redundant imply feature interactions. 5.1.2. Real world data sets
To study the performance of feature subset selection algorithms, we have employed 35 benchmark data sets which are available from UC Irvine Machine Learning Repository [55] and website of feature marized in Table 3. From this table we can see that the sizes of data sets vary between 57 and 20,000 instances, and the total number of original features is up to 22283.

Note that for data sets containing features with continuous values, the MDL discretization method [56] is applied to discretize the continuous features. 5.2. Experimental results of feature subset selection
In this section, we firstly introduce the experiment setup, then present the results of feature selection on synthetic and real world data sets, respectively. Finally, we give the sensitive analysis of the two important thresholds minSupp and minConf used in FEAST. 5.2.1. Experiment setup 1. Seven representative feature selection algorithms are chosen to be compared with FEAST. 2. Classification accuracy over the selected feature subset is extensively used as a measure to evaluate 3. To evaluate the performance of the proposed algorithm FEAST, we compare FEAST with seven 5.2.2. Results and analysis 1. Results on synthetic data sets data sets.

From Table 4, we observe that: i) Only the new proposed algorithm FEAST selects the relevant fea-rithms, CFS, FRanker and MIFS can not detect the redundant feature r of the data set  X  X ynData2 X . iii) By comparing the features selected by different algorithms with the target concept on each synthetic data set, only FEAST reserves all the interactive features for all the five data sets. INTERACT works well on all the data sets except for the data set  X  X ynData2 X . The other algorithms detect partial interactive features of some data sets, or identify all interactive features on some but not all the data sets. 2. Results on real world data sets
In this part, we present the comparison results of FEAST with the other seven feature subset selection subset selection; and iii) the runtime. 1). Number of selected features
The reduction on the number of features is an important metric used to evaluate feature subset selection algorithms and the number of original features. From this table we observe that i) all feature subset selection algorithms could significantly reduce the number of features on most data sets. ii) INTERACT obtains the best reduction rate, while FRanker ranks last with the average number of selected features of 308.8. iii) The average number of selected features of FEAST is 19.66, which is less than that obtained by CFS and FRanker. Although the numbers of selected features of MIFS and FSBAR are less than that of FEAST, these two algorithms are not available over the latter five high dimensional data sets due to their high consumption. 2). Classification accuracy comparison
For each of the 35 data sets, we obtain the accuracies of each classifier before and after the feature selection. The results of the five different types of classifiers, namely, Naive Bayes, IB1, C4.5, PART and SVM are shown in Tables 6, 7, 8, 9 and 10, respectively.

Each table contains the accuracies of the corresponding classifier on the 35 data sets with different feature subset selection algorithms, along with the Win/Draw/Loss record which represents the number of data sets where the classification accuracy of the given classifier obtained through FEAST is greater than/equal to/smaller than that with the compared feature selection algorithm.

From Table 6 we observe that i) compared to the original data set, the average accuracy of Naive Bayes is improved by only three out of eight algorithms, i.e., FEAST, CFS and FCBF; ii) FEAST outperforms all the other algorithms in terms of average accuracy. It outperforms CFS by 0.82%, Consist by 2.96%, FCBF by 1.13%, FRanker by 2.02%, MIFS by 4. 75%, INTERACT by 3.16% and FSBAR by 5.98%; iii) FEAST outperforms the other algorithms in terms of Win/Draw/Loss record, as it wins the other algorithms for 21.57 out of 35 data sets on average.

From Table 7 we observe that i) compared to the original data set, the average accuracy of IB1 is improved by FEAST, CFS and FRanker; ii) FEAST is the best in terms of the average accuracy im-provement on IB1. It outperforms CFS by 1.15%, Consist by 2.61%, FCBF by 2.71%, FRanker by 0.94%, MIFS by 6.14%, INTERACT by 4.69% and FSBAR by 6.4%; iii) FEAST outperforms the other algorithms in terms of Win/Draw/Loss record, as it wins the other algorithms for 19.63 out of 35 data sets on average.

From Table 8 we observe that i) compared to the original data set, the average accuracy of C4.5 is improved by FEAST, Consist and FRanker; ii) FEAST outperforms all the other algorithms in terms of average accuracy. It outperforms CFS by 2.83%, Consist by 2.25%, FCBF by 3.41%, FRanker by 2.18%, MIFS by 5.78%, INTERACT by 2.6% and FSBAR by 3.57%; iii) FEAST outperforms the other algorithms in terms of Win/Draw/Loss record, as it wins the other algorithms for 21.43 out of 35 data sets on average.

From Table 9 we observe that i) compared to the original data set, the average accuracy of PART is improved by all the algorithms except for MIFS and FSBAR; ii) FEAST is the best in terms of the average accuracy improvement on PART. It outperforms CFS by 2.89%, Consist by 3.15%, FCBF by 3.73%, FRanker by 3.14%, MIFS by 6.88%, IN TERACT by 3.77 and FSBAR by 4.35%; iii) FEAST outperforms the other algorithms in terms of Win/Draw/Loss record, as it wins the other algorithms for 24 out of 35 data sets on average.

From Table 10 we observe that i) compared to the original data set, the average accuracy of SVM is im-proved by only our algorithm FEAST; ii) FEAST outperforms CFS by 2.12%, Consist by 8.69%, FCBF by 3.4%, FRanker by 1%, MIFS by 5.87%, IN TERACT by 4.18 and FSBAR by 4.62%; iii) FEAST outperforms the other algorithms in terms of Win/Draw/Loss record, as it wins the other algorithms for 24.71 out of 35 data sets on average.

To summarize, for all the five classifiers, FEAST outperforms the other feature subset selection algo-rithms not only in terms of average accuracy but also in terms of Win/Draw/Loss record. Hence, it is improvement.

We also noticed that, compared with CFS, our algorithm FEAST is slightly better in improving the classification accuracies of Naive Bayes. This is due to independent variables are assumed by Naive Bayes, thus interactive features pose negative impact on it [63]. 3). Runtime comparison
Table 11 records the runtime of each feature subset selection algorithm upon the 35 data sets. From it we observe that i) The average runtime of different algorithms varies greatly, FRanker ranks 1 with 379 ms, and INTERACT ranks last with 105882 ms. ii) FEAST is faster than CFS, Consist, INTERACT and FSBAR. Although the algorithms MIFS and FSBAR have relatively shorter runtime, they are not available for high dimensional data sets because of their high consumption. Meanwhile, compared with associative-based algorithm FSBAR, FEAST is much more efficient since it generates association rules by FP-growth algorithm which is more efficient than the Apriori algorithm used in FSBAR. 5.2.3. Sensitive analysis of th e support and confidence thresholds 1. Sensitive analysis upon number of selected features
Figure 3 shows sensitivity analysis results of the support and confidence thresholds on the number of the selected features for the proposed FEAST algorithm.

From Fig. 3(a) we observe that for all the 35 data sets, with the increment of the support threshold, the the number of the frequent itemsets decreases. At the same time, FEAST chooses feature subset from although the number of the selected features deceases with the increment of the support threshold, for the different data sets, the decrement extents are varying. Therefore, we should choose different support thresholds for different data sets.

From Fig. 3(b) we observe that with the increment of the confidence threshold, the number of selected support thresholds with varying values. Further, for the different confidence thresholds, the varying ranges of the support thresholds are different. This means the corresponding numbers of the frequent itemsets and further the numbers of selected features are different as well. This reveals that both the support and confidence thresholds are affected by data set characteristics and we should select different thresholds for different data sets. 2. Sensitive analysis upon classification accuracy
Figure 4 shows sensitivity analysis results of the support and confidence thresholds on the classifica-tion accuracies of the five classifiers with the FEAST algorithm.

From Figs 4(a) and 4(b) we observe that i) for a given data set, the classification accuracy varying given confidence thresholds. This reveals that the proposed feature subset selection algorithm FEAST has no bias for a special classifier, i.e., the results obtained by FEAST is generally suitable. ii) The classification accuracy varies with both the support and confidence thresholds, and the thresholds cor-responding to the highest classification accuracy are different for different data sets. For example, in Fig. 4(a), the support threshold corresponding to the highest classification accuracy is about 10% for Data set 7, while less than 5% for Data set 8 and greater than 30% for Data set 26. In Fig. 4(b), the confidence threshold corresponding to the highest classification accuracy is greater than 95% for Data set 2, while about 70% for Data set 9 and about 90% for Data set 27. This implies that both support and confidence thresholds affect the feature subset chosen by FEAST, and the best thresholds are different for different data sets. 3. Sensitive analysis upon runtime
Figure 5 shows the sensitivity analysis results of the support and confidence thresholds on the runtime of our proposed FEAST algorithm.

From Fig. 5(a) we observe that for all the data sets, the runtime of FEAST decreases when the support threshold increases. This is because with the increment of the support threshold, the number of the frequent itemsets decreases. So the time used to mine the frequent itemsets decreases as well. At the same time, FEAST chooses from itemsets the feature subset that is frequent at least, thus the time consumed in the feature subset identification also deceases.

From Fig. 5(b) we observe that the runtime of FEAST can increase, decrease and fluctuate when the confidence threshold increases. The reason is that for a given confidence threshold, there are many support thresholds with varying values. Further, for the different confidence thresholds, the varying ranges of the support thresholds are different. This means that the corresponding numbers of the frequent itemsets and further the numbers of selected features are different as well. Thus, the time used to mine frequent itemsets and to identify feature subset is varying.

To summarize, the performance of the proposed algorithm FEAST is directly affected by the selection of these two input-parameters: support and confidence thresholds. However, the appropriate thresholds sets.
 Procedure Threshold Prediction&amp;Validation 5.3. Experimental results of support and confidence threshold prediction 5.3.1. Experimental process support and confidence thresholds are identified for each data set. Then jackknife validation method is
It should be noted that, when clustering data sets according to their metrics, the clustering tool gCLUTO [62] is employed. When building support and confidence threshold recommendation models, partial least square regression (PLSR) method [54] is used. 5.3.2. Results and analysis
Table 12 shows the support and confidence threshold prediction and validation results in terms of classification accuracy for the five different types of classifiers.
From Table 12 we observe that when using the predicted support and confidence thresholds instead of data sets decreases i) from 83.88% to 82.43%, the decrement is 1.73% for Naive Bayes; ii) from 84.40% to 81.39%, the decrement is 3.56% for IB1; iii) fro m 81.63% to 80.54%, the decrement is 1.34% for C4.5; iv) from 82.51% to 81.72%, the decrement is 0.96% for PART and v) from 86.07% to 84.3%, the decrement is 2.05% for SVM.

We also observe that the number of data sets whose classification accuracy decrement is less than 5% is 30 out of 35 for Naive Bayes, 27 out of 35 for IB1, 32 of out 35 for C4.5, 32 out of 35 for PART and 30 out of 35 for SVM. By excluding the few data sets whose classification accuracy decrement exceeds 5%, the average classification accuracy (Average2) decrement is 0.66% for Naive Bayes, 0.49% for IB1, 0.5% for C4.5, 1.59% for PART.

The results show that for most of the data sets, the classification accuracies of a classifier with the predicted thresholds keep invariant or just have a slight difference when compared to that with the most to predict the support and confidence thresholds for the proposed algorithm FEAST. 6. Conclusion
In this article, we have presented a novel association rule mining based feature subset selection algo-rithm (viz, FEAST) and the corresponding support and confidence threshold prediction method, with an aim to get proper features for machine learning and data mining algorithms, so as to further improve their performance.

We have also compared the proposed algorithm FEAST with the other seven representative feature se-lection algorithms, including five well-known algorithms CFS, Consistency, FCBF, FRanker and MIFS, the algorithm INTERACT aiming at solving feature interaction, and an associative-rule-based algo-rithm FSBAR, upon both the five synthetic data sets and the 35 real world data sets. The results on the synthetic data sets show that FEAST can identify relevant features and remove redundant ones while reserving feature interaction. The results on the real world data sets show that our proposed algorithm FEAST outperformed all the other seven feature selection algorithms in terms of the average accuracy improvement and the Win/Draw/Loss records of all the five different types of classifiers Naive Bayes, IB1, C4.5, PART and SVM.

What X  X  more, the corresponding threshold prediction method has been tested extensively using the 35 real world data sets as well. The results showed that the proposed support and confidence threshold prediction (viz, PLSR-based) method can be used to provide proper support and confidence thresholds for FEAST.
 Acknowledgements This work is supported by the National Natural Science Foundation of China under grant 61070006. The authors would like to thank the editor and the referees for their helpful comments. References Appendix
The CARset and AARset of D xor (see Table 1) under minSupp = 20%, minConf = 70% and minLift = 1 are listed as follows. Where Supp and Conf denote the support and confidence of a rule, respectively. 1. Classification association rule set (CARset) 2. Atomic association rule set (AARset)
