 The use of automatic methods for evaluating machine-generated text is quickly becoming main-stream in natural language processing. The most notable examples in this cate gory include measures such as BLEU and ROUGE which dri ve research in the machine translation and text summarization communities. These methods assess the quality of a machine-generated output by considering its simi-larity to a reference text written by a human. Ideally , the similarity would reect the semantic proximity between the two. In practice, this comparison breaks down to n -gram overlap between the reference and the machine output. 1a. Ho we ver, Israel' s reply failed to completely clear the U.S. suspicions. 1b. Ho we ver, Israeli answer unable to fully remo ve the doubts.
 Table 1: A reference sentence and corresponding machine translation from the NIST 2004 MT eval-uation.

Consider the human-written translation and the machine translation of the same Chinese sentence sho wn in Table 1. While the two translations con-vey the same meaning, the y share only auxiliary words. Clearly , any measure based on word over-lap will penalize a system for generating such a sen-tence. The question is whether such cases are com-mon phenomena or infrequent exceptions. Empiri-cal evidence supports the former . Analyzing 10,728 reference translation pairs 1 used in the NIST 2004 machine translation evaluation, we found that only 21 (less than 0.2%) of them are identical. Moreo ver, 60% of the pairs dif fer in at least 11 words. These statistics suggest that without accounting for para-phrases, automatic evaluation measures may never reach the accurac y of human evaluation.

As a solution to this problem, researchers use multiple references to rene automatic evaluation. Papineni et al. (2002) sho ws that expanding the number of references reduces the gap between au-tomatic and human evaluation. Ho we ver, very few human annotated sets are augmented with multiple references and those that are available are relati vely small in size. Moreo ver, access to several references does not guarantee that the references will include the same words that appear in machine-generated sentences.

In this paper , we explore the use of paraphras-ing methods for renement of automatic evalua-tion techniques. Given a reference sentence and a machine-generated sentence, we seek to nd a para-phrase of the reference sentence that is closer in wording to the machine output than the original ref-erence. For instance, given the pair of sentences in Table 1, we automatically transform the reference sentence (1a.) into
Thus, among man y possible paraphrases of the reference, we are interested only in those that use words appearing in the system output. Our para-phrasing algorithm is based on the substitute in con-text strate gy. First, the algorithm identies pairs of words from the reference and the system output that could potentially form paraphrases. We select these candidates using existing lexico-semantic resources such as WordNet. Ne xt, the algorithm tests whether the candidate paraphrase is admissible in the con-text of the reference sentence. Since even synon yms cannot be substituted in any conte xt (Edmonds and Hirst, 2002), this ltering step is necessary . We pre-dict whether a word is appropriate in a new conte xt by analyzing its distrib utional properties in a lar ge body of text. Finally , paraphrases that pass the lter -ing stage are used to rewrite the reference sentence.
We apply our paraphrasing method in the conte xt of machine translation evaluation. Using this strat-egy, we generate a new sentence for every pair of human and machine translated sentences. This syn-thetic reference then replaces the original human ref-erence in automatic evaluation.

The key ndings of our work are as follo ws: (1) Automatically generated paraphrases im-prove the accuracy of the automatic evaluation methods. Our experiments sho w that evaluation based on paraphrased references gives a better ap-proximation of human judgments than evaluation that uses original references. (2) The quality of automatic paraphrases de-termines their contrib ution to automatic evalua-tion. By analyzing several paraphrasing resources, we found that the accurac y and coverage of a para-phrasing method correlate with its utility for auto-matic MT evaluation.

Our results suggest that researchers may nd it useful to augment standard measures such as BLEU and ROUGE with paraphrasing information thereby taking more semantic kno wledge into account.
In the follo wing section, we pro vide an overvie w of existing work on automatic paraphrasing. We then describe our paraphrasing algorithm and ex-plain how it can be used in an automatic evaluation setting. Ne xt, we present our experimental frame-work and data and conclude by presenting and dis-cussing our results. Automatic Paraphrasing and Entailment Our work is closely related to research in automatic para-phrasing, in particular , to sentence level paraphras-ing (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). Most of these approaches learn para-phrases from a parallel or comparable monolingual corpora. Instances of such corpora include multiple English translations of the same source text writ-ten in a foreign language, and dif ferent news arti-cles about the same event. For example, Pang et al. (2003) expand a set of reference translations us-ing syntactic alignment, and generate new reference sentences that could be used in automatic evaluation.
Our approach dif fers from traditional work on au-tomatic paraphrasing in goal and methodology . Un-lik e pre vious approaches, we are not aiming to pro-duce any paraphrase of a given sentence since para-phrases induced from a parallel corpus do not nec-essarily produce a rewriting that mak es a reference closer to the system output. Thus, we focus on words that appear in the system output and aim to determine whether the y can be used to rewrite a ref-erence sentence.

Our work also has interesting connections with research on automatic textual entailment (Dagan et al., 2005), where the goal is to determine whether a given sentence can be inferred from text. While we are not assessing an inference relation between a reference and a system output, the two tasks face similar challenges. Methods for entailment recognition extensi vely rely on lexico-semantic re-sources (Haghighi et al., 2005; Harabagiu et al., 2001), and we belie ve that our method for conte x-tual substitution can be benecial in that conte xt. Automatic Ev aluation Measur es A variety of au-tomatic evaluation methods have been recently pro-posed in the machine translation community (NIST , 2002; Melamed et al., 2003; Papineni et al., 2002). All these metrics compute n -gram overlap between a reference and a system output, but measure the overlap in dif ferent ways. Our method for reference paraphrasing can be combined with any of these metrics. In this paper , we report experiments with BLEU due to its wide use in the machine translation community .

Recently , researchers have explored additional kno wledge sources that could enhance automatic evaluation. Examples of such kno wledge sources in-clude stemming and TF-IDF weighting (Babych and Hartle y, 2004; Banerjee and La vie, 2005). Our work complements these approaches: we focus on the im-pact of paraphrases, and study their contrib ution to the accurac y of automatic evaluation. The input to our method consists of a reference sen-tence R = r tence W = w and W respecti vely . The output of the model is a synthetic reference sentence S RW that preserv es the meaning of R and has maximal word overlap with W . We generate such a sentence by substituting words from R with conte xtually equi valent words from W .

Our algorithm rst selects pairs of candidate word paraphrases, and then checks the lik elihood of their substitution in the conte xt of the reference sentence.
Candidate Selection We assume that words from the reference sentence that already occur in the sys-tem generated sentence should not be considered for substitution. Therefore, we focus on unmatched pairs of the form { ( r, w ) | r  X  R  X  W , w  X  W  X  R} . From this pool, we select candidate pairs whose members exhibit high semantic proximity . In our experiments we compute semantic similarity us-ing WordNet, a lar ge-scale lexico-semantic resource emplo yed in man y NLP applications for similar pur -2a. It is hard to belie ve that such tremendous changes have tak en place for those people and lands that I have never stopped missing while living abroad. 2b. For someone born here but has been sentimentally attached to a foreign country far from home , it is difcult to belie ve this kind of changes.
 Table 2: A reference sentence and a corresponding machine translation. Candidate paraphrases are in bold. poses. We consider a pair as a substitution candidate if its members are synon yms in WordNet.

Applying this step to the two sentences in Table 2, we obtain two candidate pairs ( home , place ) and ( difcult , hard ).

Contextual Substitution The next step is to de-termine for each candidate pair ( r i , w j ) whether w j is a valid substitution for r i in the conte xt of r . . . r i tial because synon yms are not uni versally substi-tutable 2 . Consider the candidate pair ( home , place ) from our example (see Table 2). Words home and place are paraphrases in the sense of  X habitat X , but in the reference sentence  X  place  X  occurs in a dif fer -ent sense, being part of the collocation  X tak e place X . In this case, the pair ( home , place ) cannot be used to rewrite the reference sentence.

We formulate conte xtual substitution as a binary classication task: given a conte xt r . . . r i w j can occur in this conte xt at position i . For each candidate word w j we train a classier that models conte xtual preferences of w j . To train such a classier , we collect a lar ge corpus of sentences that contain the word w j and an equal number of randomly extracted sentences that do not contain this word. The former cate gory forms positi ve instances, while the latter represents the negati ve. For the negati ve examples, a random position in a sentence is selected for extracting the conte xt. This corpus is acquired automatically , and does not require any manual annotations.
We represent conte xt by n -grams and local col-locations, features typically used in supervised word sense disambiguation. Both n -grams and collocations exclude the word w j . An n -gram is a sequence of n adjacent words appearing in r . . . r i tak es into account the position of an n -gram with respect to the tar get word. To compute local colloca-tions for a word at position i , we extract all n -grams ( n = 1 . . . 4 ) beginning at position i  X  2 and ending at position i + 2 . To mak e these position dependent, we prepend each of them with the length and starting position.

Once the classier 3 for w j is trained, we ap-ply it to the conte xt r positi ve predictions, we rewrite the string as r . . . r i substitutions are tested independently .

For the example from Table 2, only the pair ( difcult , hard ) passes this lter , and thus the sys-tem produces the follo wing synthetic reference: The synthetic reference keeps the meaning of the original reference, but has a higher word overlap with the system output.

One of the implications of this design is the need to develop a lar ge number of classiers to test con-textual substitutions. For each word to be inserted into a reference sentence, we need to train a sepa-rate classier . In practice, this requirement is not a signicant burden. The training is done off-line and only once, and testing for conte xtual substitution is instantaneous. Moreo ver, the rst ltering step ef-fecti vely reduces the number of potential candidates. For example, to apply this approach to the 71,520 sentence pairs from the MT evaluation set (described in Section 4.1.2), we had to train 2,380 classiers.
We also disco vered that the key to the success of this approach is the size of the corpus used for train-ing conte xtual classiers. We deri ved training cor -pora from the English Giga word corpus, and the av-erage size of a corpus for one classier is 255,000 sentences. We do not attempt to substitute any words that have less that 10,000 appearances in the Giga-word corpus. Our primary goal is to investigate the impact of machine-generated paraphrases on the accurac y of automatic evaluation. We focus on automatic evalu-ation of machine translation due to the availability of human annotated data in that domain. The hypoth-esis is that by using a synthetic reference transla-tion, automatic measures approximate better human evaluation. In section 4.2, we test this hypothesis by comparing the performance of BLEU scores with and without synthetic references.

Our secondary goal is to study the relationship between the quality of paraphrases and their con-trib ution to the performance of automatic machine translation evaluation. In section 4.3, we present a manual evaluation of several paraphrasing methods and sho w a close connection between intrinsic and extrinsic assessments of these methods. 4.1 Experimental Set-Up We begin by describing rele vant background infor -mation, including the BLEU evaluation method, the test data set, and the alternati ve paraphrasing meth-ods considered in our experiments. 4.1.1 BLEU
BLEU is the basic evaluation measure that we use in our experiments. It is the geometric average of the n -gram precisions of candidate sentences with respect to the corresponding reference sentences, times a bre vity penalty . The BLEU score is com-puted as follo ws: where p n is the n -gram precision, c is the cardinality of the set of candidate sentences and r is the size of the smallest set of reference sentences.

To augment BLEU evaluation with paraphrasing information, we substitute each reference with the corresponding synthetic reference. 4.1.2 Data We use the Chinese portion of the 2004 NIST MT dataset. This portion contains 200 Chinese doc-uments, subdi vided into a total of 1788 segments. Each segment is translated by ten machine transla-tion systems and by four human translators. A quar -ter of the machine-translated segments are scored by human evaluators on a one-to-ve scale along two dimensions: adequac y and uenc y. We use only ad-equac y scores, which measure how well content is preserv ed in the translation. 4.1.3 Alter nati ve Paraphrasing Techniques
To investigate the effect of paraphrase quality on automatic evaluation, we consider two alternati ve paraphrasing resources: Latent Semantic Analysis (LSA), and Bro wn clustering (Bro wn et al., 1992). These techniques are widely used in NLP applica-tions, including language modeling, information ex-traction, and dialogue processing (Haghighi et al., 2005; Seran and Eugenio, 2004; Miller et al., 2004). Both techniques are based on distrib utional similarity . The Bro wn clustering is computed by considering mutual information between adjacent words. LSA is a dimensionality reduction technique that projects a word co-occurrence matrix to lower dimensions. This lower dimensional representation is then used with standard similarity measures to cluster the data. Two words are considered to be a paraphrase pair if the y appear in the same cluster .
We construct 1000 clusters emplo ying the Bro wn method on 112 million words from the North Amer -ican Ne w York Times corpus. We keep the top 20 most frequent words for each cluster as paraphrases. To generate LSA paraphrases, we used the Infomap softw are 4 on a 34 million word collection of arti-cles from the American Ne ws Text corpus. We used the def ault parameter settings: a 20,000 word vocab-ulary , the 1000 most frequent words (minus a stop-list) for features, a 15 word conte xt windo w on either side of a word, a 100 feature reduced representation, and the 20 most similar words as paraphrases.
While we experimented with several parameter settings for LSA and Bro wn methods, we do not claim that the selected settings are necessarily opti-mal. Ho we ver, these methods present sensible com-Table 4: Pearson adequac y correlation scores for rewriting using one and two references, averaged over ten runs.
 Table 5: Paired t-test signicance for all methods compared to BLEU as well as our method for one reference. Two triangles indicates signicant at the 99% condence level, one triangle at the 95% con-dence level and X not signicant. Triangles point towards the better method. parison points for understanding the relationship be-tween paraphrase quality and its impact on auto-matic evaluation.

Table 3 sho ws synthetic references produced by the dif ferent paraphrasing methods. 4.2 Impact of Paraphrases on Machine The standard way to analyze the performance of an evaluation metric in machine translation is to com-pute the Pearson correlation between the automatic metric and human scores (Papineni et al., 2002; Koehn, 2004; Lin and Och, 2004; Stent et al., 2005). Pearson correlation estimates how linearly depen-dent two sets of values are. The Pearson correlation values range from 1, when the scores are perfectly linearly correlated, to -1, in the case of inversely cor -related scores.

To calculate the Pearson correlation, we create a document by concatenating 300 segments. This strate gy is commonly used in MT evaluation, be-cause of BLEU' s well-kno wn problems with docu-ments of small size (Papineni et al., 2002; Koehn, 2004). For each of the ten MT system translations, Paraphrased words are in bold and ltered words underlined. the evaluation metric score is calculated on the docu-ment and the corresponding human adequac y score is calculated as the average human score over the segments. The Pearson correlation is calculated over these ten pairs (Papineni et al., 2002; Stent et al., 2005). This process is repeated for ten dif ferent documents created by the same process. Finally , a paired t-test is calculated over these ten dif ferent cor -relation scores to compute statistical signicance. Table 4 sho ws Pearson correlation scores for BLEU and the four paraphrased augmentations, averaged over ten runs. 5 In all ten tests, our method based on conte xtual rewriting (Conte xtWN) impro ves the correlation with human scores over BLEU. Moreo ver, in nine out of ten tests Conte x-tWN outperforms the method based on WordNet. The results of statistical signicance testing are sum-marized in Table 5. All the paraphrasing methods except LSA, exhibit higher correlation with human scores than plain BLEU. Our method signicantly outperforms BLEU, and all the other paraphrase-based metrics. This consistent impro vement con-rms the importance of conte xtual ltering.
The third column in Table 4 sho ws that auto-matic paraphrasing continues to impro ve correlation scores even when two human references are para-phrased using our method. 4.3 Ev aluation of Paraphrase Quality In the last section, we saw signicant variations in MT evaluation performance when dif ferent para-phrasing methods were used to generate a synthetic reference. In this section, we examine the correla-tion between the quality of automatically generated paraphrases and their contrib ution to automatic eval-uation. We analyze how the substitution frequenc y and the accurac y of those substitutions contrib utes to a method' s performance.

We compute the substitution frequenc y of an au-tomatic paraphrasing method by counting the num-ber of words it rewrites in a set of reference sen-tences. Table 6 sho ws the substitution frequenc y and the corresponding BLEU score. The substitution frequenc y varies greatly across dif ferent methods  X  LSA is by far the most prolic rewriter , while Bro wn produces very few substitutions. As expected, the more paraphrases identied, the higher the BLEU score for the method. Ho we ver, this increase does Table 6: Scores and the number of substitutions made for all 1788 segments, averaged over the dif-ferent MT system translations Table 7: Accurac y scores by two human judges as well as the Kappa coef cient of agreement. not translate into better evaluation performance. For instance, our conte xtual ltering method remo ves approximately a quarter of the paraphrases sug-gested by WordNet and yields a better evaluation measure. These results suggest that the substitu-tion frequenc y cannot predict the utility value of the paraphrasing method.

Accurac y measures the correctness of the pro-posed substitutions in the conte xt of a reference sen-tence. To evaluate the accurac y of dif ferent para-phrasing methods, we randomly extracted 200 para-phrasing examples from each method. A paraphrase example consists of a reference sentence, a refer -ence word to be paraphrased and a proposed para-phrase of that reference (that actually occurred in a corresponding system translation). The judge was instructed to mark a substitution as correct only if the substitution was both semantically and grammat-ically correct in the conte xt of the original reference sentence.

Paraphrases produced by the four methods were judged by two nati ve English speak ers. The pairs were presented in random order , and the judges were not told which system produced a given pair . We emplo y a commonly used measure, Kappa, to as-sess agreement between the judges. We found that Table 8: Confusion matrix for the conte xt ltering method on a random sample of 200 examples la-beled by the rst judge. on all the four sets the Kappa value was around 0.7, which corresponds to substantial agreement (Landis and Koch, 1977).

As Table 7 sho ws, the ranking between the ac-curac y of the dif ferent paraphrasing methods mir -rors the ranking of the corresponding MT evalua-tion methods sho wn in Table 4. The paraphrasing method with the highest accurac y, Conte xtWN, con-trib utes most signicantly to the evaluation perfor -mance of BLEU. Interestingly , even methods with moderate accurac y, i.e. 63% for WordNet, have a positi ve inuence on the BLEU metric. At the same time, poor paraphrasing accurac y, such as LSA with 30%, does hurt the performance of automatic evalu-ation.

To further understand the contrib ution of conte x-tual ltering, we compare the substitutions made by WordNet and Conte xtWN on the same set of sen-tences. Among the 200 paraphrases proposed by WordNet, 73 (36.5%) were identied as incorrect by human judges. As the confusion matrix in Table 8 sho ws, 40 (54.5%) were eliminated during the lter -ing step. At the same time, the ltering erroneously eliminates 27 positi ve examples (21%). Ev en at this level of false negati ves, the ltering has an overall positi ve effect. This paper presents a comprehensi ve study of the impact of paraphrases on the accurac y of automatic evaluation. We found a strong connection between the quality of automatic paraphrases as judged by humans and their contrib ution to automatic evalua-tion. These results have two important implications: (1) rening standard measures such as BLEU with paraphrase information mo ves the automatic evalu-ation closer to human evaluation and (2) applying paraphrases to MT evaluation pro vides a task-based assessment for paraphrasing accurac y.
We also introduce a novel paraphrasing method based on conte xtual substitution. By posing the paraphrasing problem as a discriminati ve task, we can incorporate a wide range of features that im-pro ve the paraphrasing accurac y. Our experiments sho w impro vement of the accurac y of WordNet paraphrasing and we belie ve that this method can similarly benet other approaches that use lexico-semantic resources to obtain paraphrases.

Our ultimate goal is to develop a conte xtual lter -ing method that does not require candidate selection based on a lexico-semantic resource. One source of possible impro vement lies in exploring more power -ful learning frame works and more sophisticated lin-guistic representations. Incorporating syntactic de-pendencies and class-based features into the conte xt representation could also increase the accurac y and the coverage of the method. Our current method only implements rewriting at the word level. In the future, we would lik e to incorporate substitutions at the level of phrases and syntactic trees.
 The authors ackno wledge the support of the Na-tional Science Foundation (Barzilay; CAREER grant IIS-0448168) and DA RPA (Kauchak; grant HR0011-06-C-0023). Thanks to Michael Collins, Charles Elkan, Yoong Keok Lee, Philip Koehn, Igor Maliouto v, Ben Sn yder and the anon ymous revie w-ers for helpful comments and suggestions. An y opinions, ndings and conclusions expressed in this material are those of the author(s) and do not neces-sarily reect the vie ws of DARP A or NSF .

