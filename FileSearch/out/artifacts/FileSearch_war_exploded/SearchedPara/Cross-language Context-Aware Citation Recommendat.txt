 Te xt of a 
Chinese paper and its English translation Eng lish 
Citations 
Figu re 1: An example of a snippet of a Chinese paper and its Adeq uacy of citations is very important for a scientific paper. However, it is not an easy job to find appropriate citations for a given context, especially for citations in different languages. In this paper, we define a novel task of cross-language context-aware citation recommendation, which aims at recommending English citations for a given context of the place where a citation is made in a Chinese paper. This task is very challenging because the contexts and citations are written in different languages and there exists a language gap when matching them. To tackle this problem, we propose the bilingual context-citation embedding algorithm (i.e. BLSRec-I), which can learn a low-dimensional joint embedding space for both contexts and citations. Moreover, two advanced algorithms named BLSRec-II and BLSRec-III are proposed by enhancing BLSRec-I with translation results and abstract information, respectively. We evaluate the proposed methods based on a real dataset that contains Chinese contexts and English citations. The results demonstrate that our proposed algorithms can outperform a few baselines and the BLSRec-II and BLSRec-III methods can outperform the BLSRec-I method. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  retrieval models Cross-language Citation Recommendation, Context-aware Citation Recommendation, Bilingual Information. Adeq uacy of citations is very important for a scientific paper. Firstly, it demonstrates that authors are aware of relevant prior research by citing related papers. Secondly, the space of a paper is limited, authors cannot put all the detail into a paper, so adequate citations can provide more background information and make a paper easier to understand. However, finding appropriate junior ones. There are millions of papers out there and lots of new papers are published every year. Even in a specific research field, researchers do not have enough time to read all the related papers. What X  X  worse, potential citations are probably written in a different language. In order to address the above problems and alleviate researchers X  burden of finding citations, a few researches on citation recommendation have been conducted in recent years [12, 13, 14]. Generally speaking, the citation recommendation researches can be coarsely categorized as global citation recommendation and local citation recommendation. Global citation recommendation aims to recommend a list of citations (references) for a given query paper, while local citation recommendation aims to recommend citations for specific context of each place where a citation should be made in a paper, which is also called context-aware citation recommendation and is the focus of this study. Most existing citation recommendation algorithms work in a single language, which means the contexts and citations are written in the same language. However, cross-language citations are ver y common in scientific papers, especially the non-English papers (e.g. Chinese papers). For example, Figure 1 is a snippet of text and several example citations in a Chinese paper, and the translated English text for the Chinese text is placed below the Chinese text. As it shows, this paper is written in Chinese and some of the citations are written in English. The citation placeholders in the text are marked as  X  X ] X , we consider the words surrounding the placeholder citation context ( context for short). The Chinese contexts usually describe the main content of the corresponding English citations. In this paper, we define a novel task, cross-language context-aware citation recommendation , which aims at recommending English citations for a given Chinese context. There are several challenges for this task. Firstly, we cannot compute the relevance between Chinese contexts and English citations directly, because they are in different languages. One reasonable method is to use machine translation (MT) to translate contexts or citations, and then the problem is reduced to the monolingual context-aware citation recommendation. However, machine translation could be not accurate. As a result, this straightforward method would suffer from noises brought by machine translation. Another straightforward method is to find the most similar contexts to a given context and recommend the corresponding citations. However, it is necessary to compare the contexts in the whole dataset with the given context, which is computationally expensive. This method also suffers from the cold-start problem, if one paper has not been cited by others, it can never be recommended. More seriously, different authors usually use different words and expressions in their contexts when they cite a same paper, which results in poor performance for existing similarity metrics. Last but not the least, as lots of new papers are published every year, a good algorithm should have the capability to be incrementally updated. In order to address the above challenges, we propose the bilingual context-citation embedding algorithm (i.e. BLSRec-I), which can learn a low-dimensional joint embedding space for both contexts and citations. Moreover, two advanced algorithms named BLSRec-II and B LSRec-III are proposed by enhancing BLSRec-I with translation results and abstract information, respectively. We evaluate the proposed methods based on a real dataset that contains Chinese contexts and English citations. The results demonstrate that our proposed algorithms can outperform a few baselines and the BLSRec-II and BLSRec-III methods can outperform the BLSRec-I method. Our contributions are summarized as follows: 1) We propose a novel task of cross-language citation recommendation in order to help researchers find cross-language citations. 2) We propose three methods to address this new task, and BLSRec-I is a bilingual context-citation embedding model, which can capture the latent semantics of contexts and citations. The training method is an on-line algorithm, which is suitable for incremental updating. BLSRec-II and BLSRec-III enhance BLSRec-I with translation results and abstract information. 3) Experiments are performed on a real dataset, and the results show the efficacy of our proposed methods. The rest of this paper is organized as fellow. We discuss related work in section 2. The problem is defined in section 3. Our proposed methods are formulated and described in section 4. The details of the experiments are discussed in section 5. In section 6 we conclude this paper and discuss some future work. Th ere are several efforts on recommending scientific documents. Two common approaches for paper recommendation are content-based recommendation and collaborative recommendation. Content-based recommendation infers users X  interests based upon Chandrasekaran et al. [1] present a profile-based method to recommend papers to users according to their profiles stored in CiteSeer. The profile-based recommendation compares candidate item X  X  content with user X  X  profile. Sugiyama et al. [2] describe a method that models user X  X  profile based on past publications and its neighboring papers such as citation and reference papers. Collaborative recommendation algorithms infer users X  interests based on preference list, or partial list of citation. In particular, for a document d , t hey extract the citation list from the document. Give a proportion of citations in a list, the goal is to recover the rest of citations. For example, McNee et al. [4] explore the use of collaborative filtering to recommend research papers. They build the rating matrix based on the citation web. Zhou et al. [5] apply multiple graphs model on the document recommendation problem. Their model jointly combines multiple graphs including citation, author and venue information. Torres et al. [8] introduce a hybrid recommender algorithm which combines collaborative filtering and content-based filtering to recommend research papers to users. Wang et al. [9] introduce Collaborative Topic Modeling (CTR) for recommending scientific articles; their approach combines the merits of collaborative filtering and probabilistic topic modeling [10]. Gori et al. [11] propose a scholarly paper recommendation algorithm based on the citation graph and random-walker properties. Most document recommendation algorithms need some additional information, such as users X  profile, preference list, or partial list of citation. In our method, we only require a context or a context with an abstract as a query to recommend a list of citation candidates, which alleviates the users X  burden for providing additional profiles. To pic model is a powerful tool to discover  X  X opics X  in a document set. This model is also used to predict citations for bibliographies. Tang and Zhang [7] present a study of topic-based citation recommendation. Two-layer restricted Boltzmann machine model is used for modeling paper contents and citation relationships. Nallapati et al [15] jointly model the text and the citation relationship under a framework of topic model. The proposed model Pair-Link-LDA is too expensive to scale to large digital libraries. They also introduce a simpler model, Link-PLSA-LDA, where citations are modeled as a sample from a probability distribution associated with a topic. Kataria et al [16] extend Link-PLSA-LDA by combing context information. They assume that context information can help in improving the topic identification for words and document. Kataria et al [17] propose a method to model the influence of cited authors along with the interests of citing authors. They hypothesize that contexts provide extra topical information about cited authors. Jointly modeling context, distribution. Overall, Models under the framework of topic model require long training process. When new documents come, the models have to be retrained. Local citation recommendation aims to recommend citations for specific context of each place where a citation should be made in a paper, which is also called context-aware citation recommendation. He et al. [12] use contexts of a citation and its abstract to represent a paper. Then they propose a probabilistic model to compute the relevance score. The shortcoming of the method is that it requires citation contexts of a paper, however, lots of papers do not have any citation. What X  X  more, citation contexts of a paper probably have similar semantics, but different words. This model cannot capture such similar semantics. Lu et al. [13] regard context and citation as parallel corpus and take advantage of translation model to bridge the gap between citations and context. They translate one word in context to one word in citation. Huang et al. [14] regard a paper as a new  X  X ord X  in another language, and they estimate the probability of translating a context to a citation. The aforementioned work in [7] formalizes citation recommendation as a topic discovery task. Citation candidates are ranked based on their topic similarity to the context. However, all the above researches on local citation recommendation focus on a monolingual setting, and cross-language local citation recommendation has not been investigated yet. In this study, we aim to propose new models for addressing the new task, and our proposed models are related to Supervised Semantic Indexing (SSI) [19]. SSI can be used for document-document or query-document retrieval. However, their model requires that queries and documents are in a same feature space and it has not been attempted for cross-language tasks. Glo bal citation recommendation aims to recommend a list of simple text similarity computation is not enough for global citation recommendation. They take paper content and author information into their evaluation model and use bibliography similarity and Katz centrality measurement to rank citation candidates. Meng et al [3] propose a unified graph-based model with random walk that incorporates various types of information (e.g. content, authorship, citation and collaboration network), which can provide personal global citation recommendation. He et al. [18] seek to not only recommend citations, but also identify candidate locations in a query manuscript where citations are needed. The cross-language citation recommendation task is related to the cross-language retrieval task [20, 21, 22]. Cross-language retrieval addresses the situation where the query, that a user presents to an IR system, is not in the same language as the documents being searched. Translation-based models are common for this task. Some models require some resources such as a bilingual dictionary, machine translation tools, or a parallel corpora to map terms in source language to terms in target language ([20], [22], [23], [24]). Ballesteros et al [25] describe a phrasal translation and query expansion techniques for cross-language information retrieval. Dumais et al. [26] propose a method which constructs a multi-lingual semantic space using Latent Semantic Indexing (LSI [27]). Potthast et al. [28] introduce CL-ESA, which exploits the multilingual alignment of Wikipedia. In this section, we define concepts and problems used in this paper. DEFINITION 3.1 Let d represent a scientific document written in Chinese. In document d , a con text is a bag of words, which surround a citation placeholder. The citation placeholder includes one or several English citation papers (short for citation ). We use the title and abstract of the corresponding English paper to denote the content of a citation. The context with a few Chinese sentences is considered a query, and a citation recommendation system is required to return a list of English citation candidates. We define this new task as follows: DEFINITION 3.2 (cross-language context-aware citation recommendation ) Given a few Chinese sentences as a context in paper d, the task aims to return a ranked list of English citations. The citations in the ranked list are recommended to users and users can select one or more citations for the place of the context in d. Besides the given context, a user can provide the Chinese abstract of document d as an input, and then the system is required to return a few citation candidates. The task can be defined as follows: DEFINITION 3.3 ( cross-language context-aware citation recommendation with abstract ) Given a Chinese context and a Chinese abstract, the task aims to return a ranked list of English citations. The citations in the ranked list are recommended to users as candidates.
 Both the above tasks are very helpful when researchers write papers. The task of cross-language context-aware citation recommendation task requires only a few Chinese sentences as a context in a Chinese paper to recommend English citation candidates, without using any global information of the Chinese paper. The task of cross-language context-aware citation recommendation with abstract corresponds to the situation that when the title and the abstract of the Chinese paper have been written down, and researchers want to get some English citation candidates for a specific context in the paper. Since there exists a language gap between the Chinese contexts and English citations, machine translation is usually used for eliminating the language gap and the Chinese contexts and the English citations can be matched after translation. However, due to the unsatisfactory machine translation quality and the different expressions and word usages in different papers written by different researchers, existing content similarity based methods usually result in poor performances. In order to address the big challenges mentioned in Section 1, we have to propose more suitable methods in order to achieve satisfactory performances. Note that though in this study we focus on Chinese-to-English cross-language citation recommendation, our system framework and proposed methods are language independent. That X  X  to say, our system and methods can also be used for recommending cross-language citations between any other pair of languages. In this section, firstly, we introduce a joint embedding model proposed by Weston et al [30]. We then extend this model to tackle the cross-language citation-aware recommendation problem. Furthermore  X  we enhance our model by introducing translated corpus and abstract information. Weston et al [30] propose a joint word-image embedding model to find annotations for images. In the model, bags-of-visual terms are used to represent images. We will introduce this model, and then extend it to address our problem. representation of an annotation term {1 ,..., } i Y  X  , w hich indicates a possible annotation. Then the model tries to learn a mapping The model also tries to jointly learn a mapping for annotations: k n  X  matrix. W and F will be learnt in the training process. n is the dimension of the embedding space where annotations and images will be represented. We then use the following function to compute the relevance score between a query and an annotation term: The goal is to rank the possible annotations according to the relevance scores such that the highest annotations best match the semantic content of the given image. In the model, finding annotations for images is reduced to find some annotation terms for the given bags-of-visual terms, which is similar to our cross-language citation recommendation problem. In this section, we will extend the above joint word-image embedding model to address our problem. Table 1 summarizes the notations used in our models. In the cross-language citation recommendation, we start with the TF-IDF feature vector of a Chinese context k c q R  X  and the TF-IDF feature vector of an English citation g e d R  X  . T hen we learn a mapping from the TF-We also t ry to learn a mapping for citations jointly: We adopt linear mappings for both Q  X  and D  X  , i.e . matrix, and F is a g n  X  para meter matrix. W , F will be learnt in the training process. n is th e dimension of the embedding space where contexts and citations will be represented (this is a hyper-parameter, typically n k  X  X  and n g  X   X  ). Our goal is to rank the possible citations such that the highest citation best matches a given context. The score function for measuring the relevance between a context and a citation is modeled as follows: Th e intuitive explanation of our model is that it maps the contexts (via T c q W ) and the citations (via T e d F ) in to the same low dimensional space for computing their similarity. The contexts and the citations are mapped by different functions, hence our model is suitable if contexts and citations are in different languages. 
W , e F , F ,
W , c F , W , One b asic assumption is that if a context cites several papers, these papers should have similar topic and be mapped into similar low-dimensional vectors. Given a context, our model aims to give a high score to any positive citation, even if the positive ones use synonymy and polysemy to describe a same topic. Moreover, if a paper has been cited in several different contexts, these contexts should be similar and be mapped into similar low-dimensional vectors. In other words, our model can capture the semantics of citations by embedding synonymy and polysemy into a similar position in the low dimensional space. For the same reason, the semantics in the contexts also can be captured by our model. In this section, we try to make use of the translated corpus to enhance MLSRec-I. We can use machine translation to translate contexts and citations and get two views (Chinese-Chinese, English-English). Let g e q R  X  be the TF-IDF feature vector of the Eng lish version of c q , k c d R  X  be t he TF-IDF feature vector of the Chinese version of e d . For monolingual context and citations (Chinese-Chinese or English-English), we adopt Supervised Semantic Index (SSI) [19] to model their relevance score. Here, we use English corpus as an example: where e W , e F are g n  X  matrices , e I is an identity matrix. persevering approximation of a g g  X  dense matrix. The latent semantics of citations and contexts can be embedded in the dense matrix. We can consider a joint model that takes into account both cross-language data and monolingual data. In this case, our score function can be written as follows: where F are g n  X  matrices. c W , c F , W are k n  X  matrices and I are tw o identity matrices. In Equation (2), the first term captures the cross-language context-citation relevance. The second term captures the relevance of English contexts to English citations and the last term captures the relevance of Chinese contexts to Chinese citations. The relevance scores are combined in a smooth way. An abstract describes the general idea of a paper, which is very useful for citation recommendation. We propose BLSRec-III to vector of the English version of c u via machine translation. The score function of our proposed method can be written as follows: W will be learnt in the training process. In equation (3), the first three terms capture the context-citation relatedness; the last three terms capture the abstract-citation relatedness. We now introduce the training methods for BLSRec-I, BLSRec-II and BLSRec-III. In the cross-language context-aware citation recommendation task, we are interested in learning a ranking score function ( , ) fo r a given Chinese context c q and e d D  X  , w here the top k ranking do cuments will be presented to the user. Let Q deno te the set of Chinese contexts, let D deno te the set of English citations. Suppose we have a set of tuples T (trainin g data), where each tuple contains a Chinese context c q , a corresponding English citation e d  X  . F or any non-relevant English paper function should satisfy the condition ( , ) ( , ) c e c e means e d  X  should be ranke d higher than e d  X  fo r the give context We use Weighted Approximate Rank Pairwise loss (WARP) [30, 31] as our loss function. WARP focuses on the top of the ranked list of the returned citation papers. WARP has been widely used for information retrieval [29] and recommender system [31]. In our setting, the loss function is defined as: ( , ) q d  X  : wh ere  X  is th e indicator function. Function L transfo rms the rank into a loss: We could choose different a to define different weights of the relative position of the positive example in the ranked list. a i  X  is a common choice which has been used in [31, 33]. a i  X  pref ers the top position and decays its weight for lower positions. It was shown experimentally that the choice of 1/ could lead to state-of-the-art results when we want to optimize precision at K fo r a variety of different values of K . Th e loss function can be optimized by stochastic gradient descent (SGD) following [30, 31] which updates the parameters for samples of each random draw. However, to perform the SGD, we have to compute ( , ) c e ran k q d  X  fo r all ( , ) d D  X  , w hich is too expensive for large dataset. We solve the problem with an approximating approach: for a given positive label, we sample negative labels until we find a violating label. The total number of violating labels in D is v r ank q d  X   X  , th e number of the trials in our sampling process 
N wh ich follows a geometric distribution of parameter | | 1
D  X  approximated by: sampling step, and | | D is the nu mber of citations in the database. In this method, many parameters need to be learnt. We constrain the parameters with the following norm: Where C is a parameter and 0 C  X  . E quation 8 acts as a regularizer in the same way as is used in lasso [32]. Pseudo code for training with WARP loss is described in Algorithm 1. We choose a fixed learning rate  X  . Th e algorithm is terminated when the performance no longer improves on a validation set. Algorithm 1 Online WARP Loss Optimization Inp ut: labeled data T Repeat Pick a labeled example ( , ) c e q d  X  randomly from T Set 0 N  X  Repeat Until 1 f f  X   X   X   X  or | | 1 N D  X   X  If 1 f f  X   X   X   X  then Make a gradient step to minimize: End if Until performance on validation set does not improve. For BLSRec-II, most steps of the training process are similar to the steps discussed in section 4.5.1. There are two places which should be modified. The first one is replacing equation (1) with equation (2). The second one is adding the following regularizers into equation (8): For BLSRec-III, most steps of the training process are similar to the steps discussed in section 4.5.1(BLSRec-I). Based on BLSRec-I, There are two places which should be modified. The first one is replacing equation (1) with equation (3). The second one is adding the following regularizers into equation (8): In our experiments, 10 0 n  X  , 0.0 1  X   X  , th e parameter matrices are initialized at random with mean 0, standard deviation 1 There is no standard benchmark dataset for Chinese-to-English cross-language citation recommendation. So we built our dataset based on a Chinese Journal -Chinese Journal of Computers 1 . T his journal is a top-ranking Chinese journal in China, and the papers published in this journal focus on research topics in computer science and technology. Since most research findings in computer science and technology have been published in English, the Chinese papers in this journal usually cite a considerable portion of English papers. We collected the papers published in this journal from year 2002 to 2012. Then we extracted titles, abstracts, citation contexts and its corresponding citations from the papers. We only considered the English citations in this study. Following [13, 17], three sentences around a citation placeholder were extracted as the context of the citation (including the sentence containing the citation placeholder and the previous and following sentences). And we looked up the titles of the English citations by using the Microsoft academic search service 2 and got the corresponding abstract information. We removed the citations which did not have abstract information, and combined the abstract and title texts to represent the content of a citation. Note that the contexts are in Chinese, and the contents of citations are in English. We used Google Translation 3 to translate the Chinese contexts into English, and translate the content of English citations into Chinese. We used a Chinese segmentation tool 4 to segment Chinese corpus and removed stop words and the words which appeared only one time. After preprocessing, our dataset contains 2061 Chinese papers, 30912 context-citation pairs, 21735 Chinese contexts, 17693 English citations. There are 35368 unique words in English corpus and 43747 unique words in Chinese corpus. All 17693 English citations make up the citation set D , and for each query context, the citations in D are ranked. We split the dataset into five sets randomly, and conducted training and testing on our dataset like 5-fold cross validation. At each time, four sets were used as training set and the remaining one set was used as test set. We further split a small portion of examples in the training dataset as a validation set. The training and testing processes are performed five times and the performance values are then averaged. For the cross-language context-aware citation recommendation task, we are given a context c q and compute ( , ) d D  X  and rank them in decreasing order of ( , ) c e language context-aware citation recommendation with abstract, the setting is the similar: given an abstract c u , a context positions of the right citations in the ranking list S fo r each given context. In this study, we adopt three common metrics as follows: Recall @K : It means the recall score for top K results in the ranking list. It is computed by using Equation (11) for each context and then we average the scores across all the contexts in the test set. R means the number of positive (correct) citations in top K results and |C| means the total number of correct citations provided by researchers. Mean Average Precision (MAP) : Recall@K considers the top K ranking results, but it does not consider the exact ranking position. Hence we use MAP as a metric to evaluate the performance by considering exact ranking positions. Let q T be the set of all the test contexts. For a context c q in q T , th e positive (correct) citation set is C , and the recommendation system returns a list S . Not e that we only consider the top 100 results in the ranking list. In other 
S , let ( ) r ank r be its position in the list S , o therwise let ranking higher than r . The MAP score is then computed as follows: Mean Reciprocal Rank (MRR) : Usually the first correctly recommended citation is important. So we adopt MRR [34] to evaluate based on the position of the first correct one. Let the first correct citation in the ranking list for q q T  X  . T he MRR score is then computed as follows: We compare our proposed methods with several popular methods as follows: Similarity-based method: The cosine similarity is a basic method in information retrieval. After we translate the original contexts and citations, we can compute the cosine similarity between each Chinese context and each translated Chinese citation, and compute the cosine similarity between each translated English context and each English citation, and compute the cosine similarity by using both Chinese and English information. We denote them as Cosine (C) , Cosine (E) , and Cosine (E and C) respectively. Context-aware Relevance Model ( CRM ) [12]: we apply CRM on both Chinese corpus and English corpus, which are denoted as CRM (E) and CRM (C) respectively. We follow the setting mentioned in [12]. Translation Model ( TM ) [13]: Lu et al. [13] proposed a translation model to overcome the language gap between contexts and citation papers. For the context-aware citation recommendation task, we follow the setting in [13]. For context-aware citation recommendation with abstract, we merge abstract and context as a new context. We tune the parameters as the authors suggested in [13]. TM (with translation) : To investigate the effect of machine translation, we extend the TM model [13]. We combine the original contexts and citations with the corresponding translated contexts and citations, respectively. Then we apply the TM model [13] on the new dataset. Citation Translation Model ( CTM ) [14]: Different from TM [13] which aims to  X  X ranslate X  contexts to content of citations, CTM aims to  X  X ranslate X  contexts to references. We follow the setting mentioned by Huang et al. [14]. Note that all the above baselines can be applied to both the two recommendation tasks: cross-language context-aware citation recommendation and cross-language context-aware citation recommendation with abstract information. For the task with abstract information, the abstracts can be simply combined with the contexts and then used by any of the above baselines. We will compare the performances of the baselines with and without using the abstract information. Th e comparison results are shown in Table 2 and Figure 2. Our proposed methods, BLSRec-I and BLSRec-II, significantly outperform all the baselines. We can also see that BLSRec-II outperforms BLSRec-I significantly. The standard deviation values of 5-fold cross-validation show the robustness of the proposed methods. 
Table 2: Evaluation results for cross-language context-aware In more details, our proposed methods, BLSRec-I and BLSRec-II outperform the translation-based models (i.e. TM, TM (with translation information), CTM) significantly on the MAP and MRR metrics and @ Rc all N when N is b elow 10, and thus our methods beat them overall. Translation-based models utilize co-occurrence of words to overcome the vocabulary gap between citation contexts and citations. However, the translation-based models also bring noise at the same time, which hurts their performance. Compared to our proposed method, we can see that TM, TM (with translation), CTM show low recall value when N is sm aller than 10. In practice, the top 10 returned citations are more important than the rest to users. This is more significant when it comes to the MAP and MRR metrics. Because MAP and MRR focus more on the top ranked items and decay its score quickly as the ranking position becomes larger. Our proposed methods outperform the baselines significantly on both metrics. This is due to our models X  capability of capturing the semantics in contexts and citations. Figure 2: Recall@K for cross-language context-aware citation The comparison results are shown in Table 3 and Figure 3. As we can see, for the cross-language context-aware citation recommendation task with abstract information, our proposed methods, BLSRec-I and BLSRec-III, outperform all the baselines significantly over all evaluation metrics. Comparing Table 3 with Table 2 and comparing Figure 3 with Figure 2, BLSRec-III performs better than BLSRec-II and BLSRec-III is the best-performing one among all methods with or without using abstract information. The reason is that the paper abstract can provide additional background information for a context. Almost all the methods with abstract information perform better than the corresponding ones utilizing only the context information, except TM and TM (with translation). We believe this is due to the noise that bought by machine translation. In contrast, our proposed method BLSRec-III shows its robustness and it can smoothly combine the abstract information and the context information. Overall, our proposed methods, BLSRec-I, BLSRec-II, BLSRec-III, are very competitive methods for the cross-language context-aware citation recommendation tasks. Among the three proposed methods, BLSRec-II is more reliable than BLSRec-I, and BLSRec-III is more reliable than BLSRec-II if the abstract information is available. We tune the dimension n fro m 50 to 300 to observe its influence on recommendation performance. The larger n is, th e more flexibility the model has. However, as n goes u p, the model complexity goes up as well. Hence we need to find an appropriate n fo r our problem. Figures 4 and 5 demonstrate the influence of parameter n on BL SRec-II and BLRec-III over different metrics. As we can see, when n go es up from 50 to 100, the performances of both methods have significant improvement. When n is larg er than 200, the performances almost do not change any more. 
Table 3: Evaluation results for cross-language context-aware Figu re 3: Recall@K for cross-language context-aware citation Our proposed methods are very efficient in practice. Contexts and Citations can be mapped into the low dimension space. Hence, each context or citation is associated with a low dimensional vector, respectively. Measuring the relevance score between a citation and a context is reduced to dot product. Additionally, our methods are trained by an on-line learning algorithm. It is convenient to update incrementally when newly published papers arrive. In this paper, we define a novel task called cross-language context-aware citation recommendation. This task is very challenging because the contexts and citations are written in different languages and there exists a language gap when matching them. To tackle this problem, we propose the bilingual context-citation embedding algorithm (i.e. BLSRec-I), which can learn a low-dimensional joint embedding space for both contexts and citations. Moreover, two advanced algorithms named BLSRec-II and BLSRec-III are proposed by enhancing BLSRec-I with translation results and abstract information, respectively. We evaluate the proposed methods based on a real dataset that contains Chinese contexts and English citations. The results demonstrate that our proposed algorithms can outperform a few baselines and the two advanced algorithms are more reliable than BLSRec-I In the future, we will try to simultaneously recommend both Chinese citations and English citations in a Chinese paper, which is a more challenging task. We will also test our proposed methods in other language pairs to show their robustness. Furthermore, we will implement a real citation recommendation system based on the proposed methods in this paper and evaluate the system in practice through user studies. Th is work was supported by NSFC (61170166, 61331011), Beijing Nova Program (2008B03) and National High-Tech R&amp;D Program (2012AA011101). We thank the anonymous reviewers for their helpful comments. [1] Chandr asekaran, Kannan, Susan Gauch, Praveen Lakkaraju, [2] Sugiyama, K., Kan, M. Y. Scholarly paper recommendation [3] Meng, Fanqi, Dehong Gao, Wenjie Li, Xu Sun, and Yuexian [4] McNee, Sean M., Istvan Albert, Dan Cosley, Prateep [5] Zhou, Ding, Shenghuo Zhu, Kai Yu, Xiaodan Song, Belle L. [6] Strohman, Trevor, W. Bruce Croft, and David Jensen. [7] Tang, Jie, and Jing Zhang. A discriminative approach to [8] Torres, Roberto, Sean M. McNee, Mara Abel, Joseph A. [9] Wang, Chong, and David M. Blei. Collaborative topic [10] Steyvers, Mark, and Tom Griffiths. Probabilistic topic [11] Gori, Marco, and Augusto Pucci. Research paper [12] He, Qi, Jian Pei, Daniel Kifer, Prasenjit Mitra, and Lee Giles. [13] Lu, Yang, Jing He, Dongdong Shan, and Hongfei Yan. [14] Huang, Wenyi, Saurabh Kataria, Cornelia Caragea, Prasenjit [15] Nallapati, Ramesh M., Amr Ahmed, Eric P. Xing, and [16] Kataria, Saurabh, Prasenjit Mitra, Cornelia Caragea, and C. [17] Kataria, Saurabh, Prasenjit Mitra, and Sumit Bhatia. Utilizing [18] He, Qi, Daniel Kifer, Jian Pei, Prasenjit Mitra, and C. Lee [19] Bai, Bing, Jason Weston, David Grangier, Ronan Collobert, [20] Grefenstette, Gregory. The problem of cross-language [21] Oard, Douglas W., and Anne R. Diekema. Cross-language [22] Oard, Douglas W., and Bonnie J. Dorr. A survey of [23] Nie, Jian-Yun, Michel Simard, Pierre Isabelle, and Richard [24] Gollins, Tim, and Mark Sanderson. Improving cross [25] Ballesteros, Lisa, and W. Bruce Croft. Phrasal translation and [26] Dumais, Susan T., Todd A. Letsche, Michael L. Littman, and [27] Dumais, S., G. Furnas, T. Landauer, S. Deerwester, and S. [28] Potthast, Martin, Benno Stein, and Maik Anderka. A [29] Usunier, Nicolas, David Buffoni, and Patrick Gallinari. [30] Weston, Jason, Samy Bengio, and Nicolas Usunier. Large [31] Weston, Jason, Chong Wang, Ron Weiss, and Adam [32] Tibshirani, Robert. Regression shrinkage and selection via [33] Joachims, Thorsten. Optimizing search engines using [34] Voorhees, Ellen M. The TREC-8 Question Answering Track 
