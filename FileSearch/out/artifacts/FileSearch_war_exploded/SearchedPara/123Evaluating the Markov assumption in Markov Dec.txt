 Abstract The goal of dialogue management in a spoken dialogue system is to take actions based on observations and inferred beliefs. To ensure that the actions optimize the performance or robustness of the system, researchers have turned to reinforcement learning methods to learn policies for action selection. To derive an optimal policy from data, the dynamics of the system is often represented as a Markov Decision Process (MDP), which assumes that the state of the dialogue depends only on the previous state and action. In this article, we investigate whether constraining the state space by the Markov assumption, especially when the struc-ture of the state space may be unknown, truly affords the highest reward. In simu-lation experiments conducted in the context of a dialogue system for interacting with a speech-enabled web browser, models under the Markov assumption did not per-form as well as an alternative model which classifies the total reward with accu-mulating features. We discuss the implications of the study as well as its limitations. Keywords Spoken dialogue  X  Dialogue management  X  Markov assumption 1 Introduction The goal of dialogue management in a spoken dialogue system is to take actions based on observations and inferred beliefs. Dialogue management plays a crucial role in the overall performance of the system because speech recognition is often quite poor, due to noisy or unexpected input. With robust dialogue management, the system can still take actions that maintain the task at hand. Unfortunately, coming up with a suitable set of dialogue management strategies is no easy task. Traditional methods typically involve authoring and tuning complicated hand-crafted rules that Tim Paek  X  David Maxwell Chickering require considerable deployment time and cost. Statistical methods, on the other hand, hold the promise of robust performance from models that can be trained on data and optimized, so long as the data is representative of what the dialogue system can expect to encounter during deployment (Young, 2000 ).

Among the more popular statistical methods, researchers have turned to rein-forcement learning methods because it is possible to derive a policy for action selection. Given that the dynamics of the system is represented as a Markov Deci-sion Process (MDP), which assumes that the state of the dialogue depends only on the previous state and action, this policy is guaranteed to be optimal with respect to the data. The Markov assumption is made as a modelling choice for the data. Hence, an important topic of inquiry is whether this choice is appropriate and beneficial.
In this article, we explore the Markov assumption on both theoretical and empirical grounds. In particular, we investigate whether constraining the state space by the Markov assumption truly affords the highest reward, especially when the structure of the state space may be unknown, which is typically the case. This article is organized as follows. In Sect. 2, we discuss the modelling assumptions relevant to spoken dialogue and provide relevant background on reinforcement learning ap-plied to spoken dialogue management. In Sect. 3, we challenge the modelling assumptions by proposing alternative models to the MDP that vary the temporal relations among features. All competing models generate dialogue management strategies for interacting with a speech-enabled web browser, and we explain in detail how we built these models from data. In Sect. 4, we evaluate the performance of all the models in simulation experiments and assess the best performing model. Finally, in Sect. 5, we conclude with a discussion of the implications and limitations of the experimental study. 2 Background Before discussing the assumptions underlying the MDP, it is important to consider the basic units of dialogue modelling; that is, what basic units form a dialogue process. Because all dialogue systems respond to user utterances, perhaps the sim-plest way to model the dynamics of the interaction is to divide the temporal process by user utterances. In other words, a dialogue  X  X  X urn X  X  begins at the start of each new user utterance. While alternative ways of measuring dialogue progression exist, such as question X  X nswer pairs or contributions (Clark, 1996 ), they typically require knowledge about the type of utterance or action that was produced; for example, that an utterance was an  X  X  X ptake X  X . For simplicity, we take the user utterance as the most basic unit of dialogue progression.

Given an utterance then, the most basic features that a system can observe before taking an action are those that pertain to the utterance itself. As such, we consider that at every turn, a dialogue system can observe at least the features that can be known about the current utterance at hand. In a state-based representation, the features of the current utterance would constitute the state space, and all state space variables would be indexed by the time in which the utterance occurred. In principle, state space variables can be engineered to aggregate observations arbitrarily far back in time. We consider such features later. For now, suppose that only the most basic information, that is, features of the current utterance, can be observed. We now discuss modelling assumptions that can be made on top of this basis. 2.1 Assessing assumptions The MDP framework relies on several assumptions, not all of which may be valid in the context of spoken dialogue. Supposing for now a basic decomposition of dia-logue progression, or the  X  X  X ystem dynamics X  X , into user utterances, where states can be indexed by those utterances, as discussed above, the most obvious assumption is the Markov assumption, which declares that the current state of the dialogue de-pends only on the previous state and action. One reason for making the Markov assumption is that it allows the Bellman equations (for Eq. 4 as we get to below) to exploit the  X  X  Optimality Principle , X  X  which states that whatever the initial state may be, all remaining decisions must be optimal with regard to the state following the first decision (Bellman, 1957 ). This allows the optimal policy (for Eq. 5) to be solved efficiently using dynamic programming. Furthermore, by maintaining just local dependencies between the current state and previous state for potentially long se-quences of actions, the system can benefit from having just those local parameters to estimate. However practical these reasons may be, whether or not a model con-strained by the Markov assumption yields the highest reward as compared to models constrained by other assumptions is still an empirical question, one which we investigate later.

From a linguistic perspective, it seems counter-intuitive to believe, as the Opti-mality Principle implies, that an optimal policy based just on the previous turn (i.e., the features of the previous utterance) provides as good a policy as that based on the full history of interaction. After all, most linguists acknowledge that in a conversa-tion, participants collaboratively build up shared knowledge about what has been said and mutually understood (Clark, 1996 ). This shared knowledge, or common ground, is cumulative in nature and underlies all future interactions.

A response to this criticism is to argue that if aspects of history are important for making future decisions, they could be incorporated with states that summarize what has been learned so far; that is, summary states that are not time-indexed but cumulative. However, this argument merely avoids the problem by adding additional assumptions, this time relating to what variables should be included in the state space. Most policy-guided dialogue systems specify the state space up front, delin-eating all state variables that are assumed to be relevant for receiving a reward. These variables are defined and restricted so as to not only facilitate the Markov assumption, but also expedite tractable inference. Unfortunately, in practice, most of the time dialogue designers do not know in advance what variables should be in-cluded in the state space. In the next section, we discuss what a dialogue designer could do in such a situation. For now, it is enough to say that if possible, we should like to build models that rely on as few assumptions as necessary.

Finally, another assumption underlying the MDP is that the probabilities of making state transitions or receiving specific rewards do not change over time; that is, they are  X  X  X tationary. X  X  For dialogue systems that provide services across a large population of users, the stationary assumption may indeed hold because individual differences are generalized. However, for dialogue systems that provide services to a limited number of users, it is not unreasonable to believe that people may change their preferences about how they want the system to behave around them over time. If unobservable states such as user frustration are included in the model, they may change over time as well. In such cases, it is incumbent upon the system to continually adapt its policy. In Chickering and Paek ( 2006 ), we discuss how a dialogue system could adapt its policy in real-time to a particular user through online feedback. 2.2 MDP framework Reinforcement learning addresses the problem of how an agent should act in dy-namic environments so as to maximize a scalar reward signal (Sutton &amp; Barto, 1998 ). This problem is manifest in spoken dialogue systems because the system must take sequential actions based on its observations, such as user utterances, and its beliefs. A central debate in the literature concerns the use of models. Model-free approaches do not explicitly represent the dynamics of the environment, but instead directly approximate a value function that measures the desirability of each environment state. These approaches offer near-optimal solutions that depend on systematic exploration of all actions in all states (Watkins &amp; Dayan, 1992 ). On the other hand, model-based approaches explicitly represent a model of the dynamics of the envi-ronment to compute an estimate of the expected value of each action. With a model, the agent can reduce the number of steps to learn a policy by simulating the effects of its actions at various states (Sutton &amp; Barto, 1998 ). Perhaps for this reason, and for the fact that it is possible to derive a policy that is guaranteed to be optimal with respect to the data, spoken dialogue researchers have by and large pursued model-based reinforcement learning methods (see e.g., Levin, Pieraccini, &amp; Eckert, 1998 ; Singh, Litman, Kearns, &amp; Walker, 2002 ; Williams, Poupart, &amp; Young, 2005 ). The framework underlying model-based reinforcement learning is that of the MDP, which can be characterized by a tuple ( S , A , P , R ) with:  X  A state space S with states s 2 S . The state space may consist of features related to spoken utterances, and so forth. We discuss this further in the next section.  X  An action space A with actions a 2 A . The action space comprises all system actions in dialogue management, such as confirming various slots, or engaging in a user requested service.  X  Unknown state transition probabilities P : S A S 7 ! X  0 ; 1 , where P ( S t +1 | S t , A t ) gives the probability of a transition from a state s 2 S and action a 2 A at time slice t to another state s 2 S in the next time slice. The distribution P defines the dynamics of the environment, and constitutes the formal basis for the Markov assumption.  X  A reward function R : S A 7 !&lt; , where R t = R ( S t , A t ) assigns an immediate reward at time slice t for taking action a 2 A in state s 2 S . R plays a critical role in the policy that is learned for dialogue management as we discuss further below.
In order for a dialogue system to take actions according to the MDP, it is nec-essary to be able to derive a policy p : S 7 ! A mapping states to actions so that it maximizes some specified objective function for the long term reward. How much of the future the system takes into account in making its decisions at any given moment depends upon the specified horizon for the objective function. Perhaps the simplest objective function is the total reward over a finite horizon, which specifies that at any given time t , the system should optimize its expected reward for the next h steps
Alternatively, the system can take the infinite long term reward into account with future rewards geometrically discounted by a discount factor c where 0  X  c &lt; 1 for computability purposes. The discount factor sets the present value of future rewards; so, for example, if c = 0, the system acts myopically and tries to maximize immediate rewards, but as c approaches 1, the system becomes more farsighted (Sutton &amp; Barto, 1998 ). Although it is theoretically possible for a spoken dialogue to continue ad infinitum, most systems are designed to avoid infinite regressions where, for example, the system engages in the same repair over and over (e.g.,  X  X  X  X  X  sorry, can you repeat that? X  X ). In practice, most dialogues (and especially task-oriented dialogues) are finite horizon, given that oftentimes growing user frustration ultimately leads to the termination of the interaction.

In place of (1) and (2) above, the objective function can also be based on post-hoc measures such as usability scores (Singh et al., 2002 ; Walker, Passonneau, &amp; Boland, 2001b), and construed to reflect whatever qualities a dialogue designer may want the system to possess, such as the ability to re-tool the system for future use (Walker et al., 2001a ). In short, the assignment of the reward function reflects the desired behaviour of the system.

For the rest of this paper, we confine our discussion to the finite horizon MDP where we assume for simplicity that all variables in the state space can be fully observed by the system. Although the infinite horizon MDP is more commonly used for spoken dialogue systems, we focus on the finite horizon for two reasons: first, because our domain task imposes a restriction on the length of the dialogue, as we discuss in Sect. 1, and second, because our representation of a finite horizon MDP as an influence diagram allows us to learn which state variables are associated with an immediate reward for each time step, as we discuss in Sect. 3. When state variables are included that are not fully observable, such as the user X  X  intention in producing an utterance, the dialogue constitutes a Partially Observable MDP (see e.g., Paek &amp; Horvitz, 2000 ; Roy, Pineau, &amp; Thrun, 2000 ; Williams &amp; Young, 2005 ; Zhang, Cai, Mao, &amp; Guo, 2001 ). The POMDP also employs the Markov assumption.

Building on (1), an optimal policy can be learned through various algorithms that involve finding the optimal value function: where the optimal value of a state s is the expected reward for the next h steps, if the system starts in s at time t and executes the optimal policy p . The optimal value function (3) is unique and can be defined recursively using the Bellman equations where the value of a state s at time t is the expected immediate reward plus the expected value of the next state at time t + 1 using the best possible action. The simultaneous equations engendered by (4) can be solved efficiently with dynamic programming. Given the optimal value function, the optimal policy is simply 2.3 Influence diagram The finite horizon MDP can be viewed as a special case of an influence diagram ,a more general framework for graphical modelling that facilitates decision X  X heoretic optimization. An influence diagram is a directed acyclic graph composed of three types of nodes: chance nodes, decision nodes and value nodes. The nodes correspond to variables which can be constants, probabilities, decisions, or objectives. The influence diagram also contains a single utility node that is a deterministic function of all the value nodes. Connecting the nodes are two types of arcs: probabilistic arcs and informational arcs. Arcs pointing into chance or value nodes specify a proba-bilistic dependency between a child and its parents. Arcs pointing into a decision node are  X  X  X nformational X  X  in that the parents of the decision node are assumed to be known or observed before a decision is made. Similar to the more commonly known Bayesian network model, which is essentially an influence diagram without decision and value nodes, detailed data about the variables are stored within the nodes, so the graph is compact and visually represents the relationship among variables (see Shachter, 1998 for examples). Although the traditional definition of an influence diagram (Howard &amp; Matheson, 1981 ) permits only one value or utility node, our use of multiple value nodes is simply a way of factoring the utility function and has been used by other researchers (Lauritzen &amp; Nilsson, 2001 ; Tatman &amp; Shachter, 1990 ).
Figure 1 displays an influence diagram for a finite horizon MDP where all states s 2 S have been mapped to chance nodes, all actions a 2 A to decision nodes, and all r 2 R to value nodes expressing the immediate reward for taking action a in state s at time t . As shown in the figure, the unknown state transition probabilities P are visually represented in the arcs going into S t +1 from S t and A t in the top left-hand corner. One of the three reward functions R : S A 7 !&lt; is represented in the top right-hand corner with the arcs going from S h and A h to the value node R h . Finally, a utility node at the bottom of the figure sums all the immediate rewards as in Eq. 3.
Technically, because the MDP is fully observable at any given time slice, infor-mational arcs point into each decision node from the previous time slice, though we have left them out to reduce clutter. The influence diagram also contains a set of parameters Q that characterize the conditional distributions of the non-decision nodes, defined as where Pa ( X ) denotes the set of parents for node X , and Q X denotes the subset of parameters in Q that define the local probability distribution of X . The transition probabilities P for the MDP are clearly subsumed by (6) and reside in the node S t +1 , as shown in the figure.

We discuss influence diagrams for two reasons. First, influence diagrams allow us to understand what kinds of alternative models we could experiment with in com-petition to the MDP, because the transition probabilities P could easily depend on state variables other than just those in the previous time slice, as we demonstrate in the next section. And second, influence diagrams provide a framework in which to address what state variables should be included in S at all if a dialogue designer is unsure about what variables may be important for receiving an immediate reward. 3 Alternative models In the previous section, we discussed how the Markov assumption can be tied together with the selection of state space. Unfortunately, dialogue designers who want to use reinforcement learning for dialogue management typically do not know in advance what variables are relevant for receiving a reward and how they are related to each other: that is, the structure of the state space is unknown. Rather than choosing variables so as to facilitate the Markov assumption, we propose a different approach: include all variables that might be predictive of reinforcement and let the data decide which ones to include. This can be done using techniques for learning the parameters and structure of a Bayesian network, extended to influence diagrams (Chickering &amp; Paek, 2006 ; Heckerman, 1995 ).
 To derive the structure of the graphical models we describe below, including the MDP, we learned influence diagrams employing decision trees to encode local conditional distributions using a tool that performs Bayesian structure search (Chickering, 2002 ). As described in Chickering, Heckerman, and Meek ( 1997 ), a heuristic search method is conducted over feasible probabilistic dependency models guided by a Bayesian score that ranks all candidate models, where the score is an estimation of the likelihood of the data given the proposed dependency structure. For each variable, the method constructs a decision tree containing multinomial distributions at each leaf for discrete variables, and Gaussian distributions at each leaf for continuous variables. In learning the influence diagrams, we only specified constraints on the graphical structure of the state space, such as the Markov assumption, for which we wanted to conduct experiments. In particular, we built competing models to the MDP that vary the nature of the temporal relations be-tween features. We did not assume that the state space was known beforehand, and as such, we included all variables that we were able to log from interacting with a command-and-control, speech-enabled web browser. We now describe our data collection procedure and the models we built. 3.1 Data collection The data from which we built all models was for spoken dialogue interaction with a speech-enabled web browser. As we describe in (Chickering &amp; Paek, 2006 ), the data was generated using a simulation environment, where all possible system actions to a user command were systematically explored. The simulation proceeded as follows. First, we randomly selected a command from the command-and-control grammar for the browser (e.g.,  X  X  X o back X  X ,  X  X  X o forward X  X ,  X  X  X o to link x  X  X ). Using state-of-the-art text-to-speech (TTS) generation, we produced an utterance for the command, varying all controllable TTS parameters: in particular, the TTS engine (5 engines), pitch (6 levels), volume (6 levels), and speaking rate (6 levels). In Sect. 4, where we evaluate the models, we describe how we selected specific configurations of those parameters as  X  X  X est X  X  voices for the experiment. Because we were interested in building models that were robust to noise, we included empty commands and added various types of background noise to see if a model could learn to ignore spurious commands. The produced utterance was then recognized by a Microsoft Speech API (SAPI) recognition engine, whereupon we logged all possible SAPI events. These events, and functions of these events, constituted the feature set. Many of the fea-tures are listed in the appendix. The features roughly fell into the following three broad categories 1. Within-utterance ASR (Automatic Speech Recognition) features : Features per-2. Between-utterance ASR features : Features pertaining to matches across utter-3. Dialogue features : Features pertaining to the overall dialogue such as the
It is important to note that both the between-utterance ASR features and dia-logue features span multiple time slices, and as such, are not Markovian per se. By including these features, we decided to leave open the possibility that summary variables may very well be relevant for receiving a reward. In building the models, we let the learning algorithm decide whether to include these variables in its decision trees.

Once the produced utterance was recognized and events recorded, the simulation took a random action. For the first utterance, the simulation could either execute the most likely command in the n -best list (DoTop), confirm among the top three choices while giving the option that it may not be any of them (Confirm), ignore the utterance as spurious (Ignore), or ask for a repetition (Repeat). For the second utterance, only DoTop, Confirm, and Repeat were possible, and for the third utterance, only DoTop and Bail, an action which makes an apology and terminates the dialogue, were possible. We did not allow the interaction to extend beyond the third utterance given the typically low tolerance users have in command-and-control settings for extended repairs.

If the simulation selected DoTop, Ignore, or Bail, the session finished and the simulation could now be rewarded. For taking the action DoTop, if the result of executing the most likely command matched the  X  X  X rue X  X  command (i.e., the one sampled from the grammar), the simulation received a positive scalar reward (+100); otherwise, it received a negative reward ( X 100). For taking the action Ignore, if the true command was empty, the simulation was awarded +100. Otherwise, it received a penalty of  X 100. For Bail, the simulation always received a penalty of  X 100. If either the repair action Confirm or Repeat was selected, a smaller penalty ( X 75) was in-curred, and the simulation proceeded to produce a confirmation choice that matched the true command or a repetition of the previous utterance, respectively.

In noisy environments, the n -best list frequently failed to include the true com-mand. In order to handle this type of situation, as stated previously, we included with the top three choices the option that it might not be any of them so that even if the true command was absent from the n -best list, the system could still gracefully conclude that it did not hear the appropriate command. In such a situation, if the system correctly guessed  X  X  X one of the above, X  X  it received the usual scalar reward of (+100) plus the penalty ( X 75) for each of the one or more repairs it took to get there.
In summary, by sampling a command from the grammar and then responding to it with random actions until the dialogue finished and rewards were assigned, the simulation environment amassed data that could be used to learn which features and actions were most relevant for receiving a reward. 3.2 Types of models In order to validate empirically whether constraining the state space by the Markov assumption, especially when the structure of the state space may be unknown, truly affords the highest reward, we built three types of alternative influence diagrams that vary in the extent of their temporal dependencies using the simulation data.
Figure 2 displays all the models we learned. To make room for visual comparison, we have left out the utility node in the zero, first, and second order models. High-lighted in the figure is the MDP, which is the same as Fig. 1 . The MDP has a first order Markov dependency; hence, S 2 depends on S 1 , and S 3 depends on S 2 .If S 3 also depends on S 1 , which occurs two time slices in the past, then we have a second order Markov model, as shown in the top of the figure. In preparing the data for the second order model, we added between-utterance ASR features such as where the top command in the third slice occurred in the n -best lists of the second and first time slices. On the other hand, if no state S t depends on the previous state S t -1 , then we have no Markov dependency. We call this a  X  X  X ero order X  X  model. 3.2.1 Total reward model As an altogether different approach, we built a total reward model for each time slice where state features accumulate with each successive time slice, as shown in the bottom of the figure, and the local system actions optimize the total utility function, as opposed to the immediate reward function. Unlike the Markov models which use inference in order to estimate the policy, as we discuss later, the total reward model simply predicts the total reward given all observable features. This kind of super-vised learning model had been applied previously in the call-routing domain to transfer calls so as to minimize caller time and operational expense (Paek &amp; Horvitz, 2004).

When building a model to predict the total reward at a particular time step t ,itis important to limit the training data to those instances that correspond to optimal behaviour at future time steps. To understand this point, recall that we generated the training data by randomly performing actions. If we used all of this training data to learn a model for the total reward at time t , this would give us the optimal action assuming that we were to act randomly after this point. To take care of this issue, we work backwards through time, building first the model for the last time step. Then, before learning the model for time step t , we remove from the training data all instances for which the (random) action at time step t + 1 does not match the optimal action according to the model for time step t + 1. As a result, we get a model for the total reward at each time step under the assumption that we act optimally after that point.

In effect, the total reward model falls under the rubric of a  X  X  X odel-free X  X  approach in that it does not explicitly represent the dynamics of the environment, but rather directly predicts the long term reward. Fundamentally, the total reward model attempts to classify the total reward, which given the small state space for our domain, was discrete. In particular, there were only five possible total reward outcomes, and as such, we could treat the total reward as a discrete variable. In other domains where the number of distinct rewards possible may be large, we can learn a regression instead of a classification. 3.3 Model building Building each model entailed piecewise construction of portions of the model by the time slice variables involved. For those models involving just the first time slice variables, we had over 20,000 simulation sessions as training data. For those models involving the first and second time slices, we had over 10,000 sessions. And finally, for those models involving all three time slices, we had over 5,000 sessions.
Table 1 summarizes the complexity of the models as a function of the number of splits in the decision trees that make up the local probability distributions, and the number of state space variables. For the first and second order Markov models we also built  X  X  X imited X  X  versions, where the state space variables were limited to those that exhibited probabilistic dependence on the immediate reward. Note that some state variables only depended on other state variables. This had the effect of reducing the complexity of each respective model by a factor of 6 for the number of splits, and 4 for the number of state variables. The simplest model by far was the total reward model which had about 30 times fewer splits than any of the unlimited Markov models, and more than 4 times fewer variables than the MDP model. Al-though the total reward model had almost the same number of variables as the limited Markov models, the variables included were quite different. We discuss some of the differences later.

Figure 3 shows the limited MDP learned from the simulation data. Explanations for all of the state variables can be found in the appendix. The decision nodes contain only the actions that were available at a particular time slice in the sim-ulation, as explained in Sect. 1. For the first time slice, no dependencies between state variables are necessary because all features of the first utterance are fully observed. Note that each time slice has a number of features related to the n -best list confidence scores. This is not surprising given that most hand-crafted dialogue management strategies utilize some kind of confidence threshold for taking actions (e.g.,  X  X  X o the top recognized command if its confidence is greater than 95% X  X ). The mean confidence score in the n -best list appears in every time slice, suggesting that it pays off to build features that aggregate the confidence scores. In fact, many of the features in the first and second time slices of the limited MDP are aggregate features of the n -best list, such as the sum of all confidence scores ( X  X  X core Sum X  X ), the range of score values ( X  X  X core Range X  X ), and whether all the commands in the list were the same though the actual phrases or wording were different ( X  X  X mds All Same X  X ).

The MDP in Fig. 3 also shows that all time slices include a state variable related to the particular command that was observed, such as the first or top command in the n -best list. This indicates that the model is domain-dependent; that is, the commands specific to this application make a difference in whether or not the system receives a reward. For example, if we look at the decision tree for the value node of the second time slice, we would see that if the top command in the n -best list is  X  X  X ide numbers X  X  (i.e., numbered hyperlinks) after the system has asked for a repeat in the first time slice, and the system decides in this current time slice to execute this top command, the probability that it would receive an immediate negative reward for failure would be 90%. All the models we learned from the data were domain-dependent in this way.

Some of the variables that were not included in the limited MDP, but were included in the unlimited version, were most of the between-utterance ASR features. The notable exception that does appear in the limited version is the variable  X  X  X op Cmd Same As Previous X  X  which checks to see if the top command between the current time slice and previous time slice are the same. In general, most of the state variables included in the limited models were within-utterance ASR features. 3.4 Policy estimation To derive an optimal policy from data involves a two step process: first, learning an optimal model with respect to the data, and then, learning a policy with respect to the model. As discussed previously, we learned the models from the simulation data using Bayesian structure learning. In order to derive the optimal policy, the straightforward approach is to exploit the Markov assumption, at least for the MDP, and apply dynamic programming in the usual way. However, computing the policy for a state space that includes more than a handful of variables can be quite chal-lenging, even for the limited model in Fig. 3 . This has prompted researchers to investigate techniques for factoring the state space using graphical models as we have in our models (Meuleau et al., 1998 ). An alternative approach for computing the policy, which we utilized, is to use forward sampling to approximate the dynamic programming solution for the MDP. Leveraging the learned graphical models as a generative model for sampling, similar to Kearns, Mansour, and Ng ( 1999 ), we can approximate the expectation in (3) by sampling N random states from the next time slice so that the optimal value function becomes In particular, to determine the value function at time t , the algorithm works by, for each action a t , setting the action node A t = a t and then repeatedly sampling the chance nodes for time step t + 1, recursively solving for the best action in time step t + 1. For each sample, we examine the total utility, and we choose the action a t that maximizes the average total utility among the samples. Only a few hundred samples were needed for the algorithm to converge. This technique allowed us to compute the action with the greatest expected reward at any given time slice in the same way for every Markov model so that we could more easily compare and evaluate them.
Note that for the total reward model, because the model directly predicts the total reward, we do not need to use sampling for inference as it contains its own policy for each time slice in the decision tree for the utility node. 4 Evaluation Because it is often difficult to know in advance what variables in the state space are relevant to receiving a reward, we learned both the structure of the state space and policy for all the models discussed in the previous section. The only factor that we varied was the set of constraints we put on the state space. In particular, we varied the Markov order from zero to three. We also either limited the model to just the state variables that predicted the immediate reward or not, and finally, we built three total reward models that each contain their own policy and combined them (we henceforth refer to the combination of the three models as simply the  X  X  X otal reward model X  X ). All other factors being equal, the question remains as to whether or not constraining the state space by the Markov assumption truly affords the highest reward.

To answer this question, we used the same simulation environment we described earlier to create the training data to also generate test data. Instead of varying all controllable TTS parameters, we created 75  X  X  X est X  X  voices by permuting the TTS engine (5 engines), pitch (3 levels), and speaking rate (3 levels). All 75 voices were screened for naturalness. In the simulation environment, for any utterance, different models could take different actions, and as a consequence, receive different re-sponses. To maintain controlled experimental conditions, we generated utterances and features for the entire tree of all possible action sequences. That way we ensured that every model received the exact same features if they took the same action, which also meant that between-utterance features utilized the same previous utterance features. For each of the 75 voices, we simulated 1000 experimental trials, where a trial corresponded to the generated tree of all possible actions. Instead of varying the different kinds of background noise as in the training data, this time we fixed the background noise to the challenging condition of people chattering in a cafeteria.

We evaluated the models against the intuitive baseline of how any system would do if it followed the simple policy of always executing the top command in the n -best list. This baseline is meant to characterize the behaviour of a system that follows no dialogue management strategies. It simply commits to the top recognized result and never engages in repair. We call this baseline  X  X  DoTop . X  X  4.1 Results Figure 4 displays the running average reward for all models compared to the baseline, and Fig. 5 the average total reward. Although seven lines should be rep-resented, only four are visible because the first and second order Markov models, as well as their limited versions, performed exactly the same with respect to both their running average reward and their average total reward; that is, the 1st order, 1st order (limited), 2nd order, and 2nd order (limited) models were identical in per-formance. Hence, while the complexity of the models may have differed, they all took the same actions. This suggests that it is not worthwhile to constrain the state space beyond the first order Markov relation, nor is it worthwhile to include any more variables than those that directly predict the local immediate reward.
As far as the best performance is concerned, the total reward model outper-formed all of its competitors. With respect to average total reward, the total reward model was significantly better than any of the identically performing Markov models model achieved this result with just 28 splits in its decision trees, combined between the three time slices. The identically performing Markov models in turn achieved significantly more total reward on average than the baseline ( t (74) = 2.79, p = 0.01 two-tail). The fact that all of these models outperformed the baseline by a significant margin suggest that it is quite beneficial to engage in dialogue repair.

Surprisingly, the zero order Markov model performed significantly worse than the baseline. The reason for this has to do with the two repair actions, Repeat and Confirm. Without knowing what action was previously selected, because depen-dencies were not permitted between time slices, the model assigned the same probability of receiving an immediate negative or positive reward to both Repeat and Confirm, thereby conflating the two. To get around this problem, separate models specifically for Repeat and Confirm would need to be learned, though it is doubtful that this would have changed the final result of the experiment. 4.2 Assessing the winner Despite the relatively small complexity of the total reward model, it outperformed all other models. It is instructive to look at the state space variables that were included in the model for each time slice, as shown in Fig. 6 . Although many of the state space variables for the total reward model also appear in the limited MDP displayed in Fig. 3 , new variables come into play when predicting the total reward, as opposed to the immediate reward, such as the  X  X  X umber of Sound Ends X  X  in the first time slice, the  X  X  X econd Rule X  X  in the second time slice, and the  X  X  X op Score X  X  in the third time slice. Furthermore, because features are accumulated, the state space for the total reward model in the third time slice contains variables from the previous time slices, such as the average confidence score for the n -best list in the second time slice. Two summary features also appear in the third time slice: namely, whether the top commands between the first and second time slices are the same, and whether the top hypotheses between the second and third time slices are the same.

It is important to remember that the total reward model is model-free. It makes no assumptions about how features may be dependent on each other over time, but instead tries to predict the long term value of actions. In fact, while model-free approaches such as Q-learning (Watkins &amp; Dayan, 1992 ) still learn policies using a system of equations similar to (4), a policy for the total reward model is compara-tively easy to learn. It is simply learning a decision tree that classifies what is likely to be the total reward for all the features it has observed. Even if the total reward were continuous, learning a regression is relatively easy. Furthermore, during runtime, it requires no inference, as do the other Markov models. 5 Discussion Dialogue designers considering the use of reinforcement learning methods to learn policies for action selection in dialogue management could benefit from the impli-cations of this experimental study. If learning an MDP is the goal, and the designer is uncertain about what variables to include in the state space, our results suggest that an MDP that includes just those variables that are parents of the local immediate reward might perform just as well as a more complicated model with other variables that are not parents. In short, when we factored the state space in the experiment, only those variables relevant to receiving a reward mattered. 5.1 Implications for model selection Assuming the designer is open to considering other types of models, the total reward model offers several advantages over the Markov models. First, the total reward model does not require inference; the policy is easily extracted from the decision tree for the utility node. Second, unlike the Markov models, the total reward model does not need to model the local rewards, which can be an advantage in cases where only a final total reward is obtained. Third, the total reward model may perform just as well if not better than the Markov models. In our experiments, it indeed out-performed the Markov models, though we are not quick to generalize this result due to the limitation of the study. And finally, the total reward model makes fewer assumptions about the structure of the state space, which is quite appealing from a theoretical standpoint. It does not assume that an optimal policy based just on the previous turn is as good as a policy based on the full history of interaction. Because the full accumulation of knowledge is expressed in the model, it accords well with socio-linguistic sensibilities.
The downside of the total reward model is that for longer time horizons learning may be difficult because the number of state space variables can accumulate dra-matically. Furthermore, there may be disadvantages to not having a model of the environment, though that is a topic of debate in the reinforcement learning com-munity (Kaelbling, Littman, &amp; Moore, 1996 ). 5.2 Limitations of the study The above implications should be moderated by the limitations of the study. First and foremost, the domain in which we learned the models was quite small with a relatively restricted command-and-control grammar, and a small action space. We plan to extend the simulation and learning methods to larger domains, and it will be interesting to see if our results generalize. Second, although we assiduously delineated every feature we could think of for the state space, we may not have included the  X  X  X ight X  X  set of features that could have allowed the MDP or any other model to outperform the winner. Good feature engineering has been shown to make a significant difference in other natural-language processing domains, so it is prudent to remain humble about the features we utilized. Finally, our results de-pend on the techniques we used for learning the structure of the state space. We are agnostic about how these results may have turned out with other model selection techniques. 6 Conclusion Spoken dialogue systems that learn optimal policies for dialogue management have typically utilized the MDP framework. In so doing, they are committed to the Markov assumption as a modelling choice, and very often, to assumptions about the structure of the state space. In this paper, we explored whether this choice is appropriate and empirically beneficial. In particular, we investigated whether con-straining the state space by the Markov assumption truly affords the highest reward, especially when the structure of the state space may be unknown, which is typically the case. We examined four models that vary in terms of the extent of their temporal dependencies and found that the total reward model, a model-free approach that predicts the total reward for each time slice with state space features accumulating over time, outperformed the MDP and all other models in terms of total and average reward. This model had the smallest complexity by far, made the fewest number of assumptions, and is relatively easy to compute. For finite horizons, the total reward model offers an attractive alternative to the MDP.
 Appendix The following list includes some of the features utilized for the models. The term Cmd in a feature name refers to the user command, and the term Token refers to the user utterance. For example, if the user says  X  X  X o back X  X , the Cmd will be BACK and the Token will be  X  X  X o back X  X .
 A. within-utterance ASR features (for each turn i ) 1. {Top|Second|Third} Cmd ( i ): The command that occupies the {first|second|third} 2. {Top|Second|Third} token ( i ): Token that occupies the {first|second|third} posi-3. {Top|Second|Third} score ( i ): Confidence score that occupies the {first|sec-4. Number of false recognitions ( i ): Number of passes through SAPI X  X  word lattice 5. Number of interference ( i ): Number of times SAPI raised an event indicating 6. Most freq interference ( i ): Most common type of audio-distortion event type 7. Number of sound starts ( i ): Number of times any sound start point is detected. 8. Number of sound ends ( i ): Number of times any sound end point is detected. 9. Number of phrase starts ( i ): Number of times a phrase is detected from an audio 10. Maximum redundant {Cmd|Token|Combined} ( i ): The cardinality of the most 11. Maximum number {Cmd|Token|Combined} Matches ( i ): The cardinality of 12. Score count ( i ): Number of items in the n-best list. 13. Score sum ( i ): Sum of all the confidence scores. 14. Maximum score ( i ): Maximum confidence score. 15. Minimum score ( i ): Minimum confidence score. 16. Score range ( i ): Difference between the maximum and minimum confidence 17. Score median ( i ): Median confidence score if any. 18. Score mean ( i ): Arithmetic mean of the confidence scores. 19. Score geometric mean ( i ): Geometric mean of the confidence scores. 20. Greatest consecutive difference ( i ): Greatest difference between any two 21. Score variance ( i ): variance of the confidence scores. 22. Score Stdev ( i ): Standard deviation of the confidence scores. 23. Score Stderr ( i ): Standard error of the confidence scores. 24. Score mode ( i ): Mode of the confidence scores. 25. Cmds all same ( i ): Whether the commands of the current n-best list are all the B. Between-utterance ASR features (for i = 2 and i =3) 1. Index of top Cmd in previous ( i ): The position of the current top command in 2. Index of top Cmd in first slice ( i ): The position of the current top command in 3. Top Cmd same as previous ( i ): Whether the current top command was the 4. Index of top token in previous ( i ): The position of the current top token in the 5. Index of top token in first slice ( i ): The position of the current top token in the 6. Score more than previous ( i ): Whether the average confidence score is greater 7. Gap between top scores ( i ): Difference between the current top confidence score 8. Gap between top scores with first slice ( i ): Difference between the current top C. Dialogue features 1. Turn: The current dialogue step. 2. Has Confirm: Whether or not a confirmation has been performed anytime in the 3. Number of repairs so far ( i ): Number of repairs up to i . 4. Number of confirms so far ( i ): Number of confirmations up to i .
 References
