 Network proximity is at the heart of a large class of net-work analytics and information retrieval techniques, includ-ing node/ edge rankings, network alignment, and random-walk based proximity queries, among many others. Owing to its importance, significant effort has been devoted to ac-celerating iterative processes underlying network proximity computations. These techniques rely on numerical proper-ties of power iterations, as well as structural properties of the networks to reduce the runtime of iterative algorithms.
In this paper, we present an alternate approach to ac-celeration of network proximity queries using Chebyshev polynomials. We show that our approach, called Chopper , yields asymptotically faster convergence in theory, and sig-nificantly reduced convergence times in practice. We also show that other existing acceleration techniques can be used in conjunction with Chopper to further reduce runtime. Us-ing a number of large real-world networks, and top-k proxim-ity queries as the benchmark problem, we show that Chop-per outperforms existing methods for wide ranges of pa-rameter values. Chopper is implemented in Matlab and is freely available at http://compbio.case.edu/chopper/. Network proximity, Random walk with restarts, Chebyshev polynomials
Proximity measures on networks are at the core of a large number of analytics and information retrieval techniques. In information retrieval, nodes/ edges are ranked based on their random walk distance from other nodes [18,25]. In net-work alignment, high scoring node alignments (node pairs  X  one drawn from each network) can be identified by their ran-dom walk distance from other high scoring alignments in the product graph of the two input networks [10]. In disease-gene prioritization, genes are ranked according to their net-work proximity to genes that are previously identified as associated with clinically similar diseases [13].
The general setting for network proximity queries is as follows: For a given query node, we are interested in com-puting a score for each node that indicates the proximity of that node to the query node. For example, shortest path queries ask for the minimum number of hops (or minimum total weight of edges) connecting two nodes. Random walk based proximity measures, on the other hand, simulate ran-dom walks that make frequent restarts on the query node, and estimate proximity to the query node in terms of the probability of being at each node at steady state. In top-proximity queries, the k closest nodes to a specified source node are returned.

In many applications, random walk based proximity is preferred over shortest path distance because of its robust-ness to the inherent noise in the network. This noise may be due to inaccuracies in modeling (not all interactions in the underlying system are modeled in the graph) or noise in data (missing or spurious edges). In such cases, random walk based proximity measures provide a more robust esti-mate of network proximity.

Random walk based proximity measures have been used in a wide variety of applications, including web search [14], link prediction [6,11], clustering [2], disease-gene prioritiza-tion [7,13], and integration of disparate  X  X mic X  data in sys-tems biology [9]. Some of the well known random walk based proximity measures include discounted hitting time [17], per-sonalized hitting probability [24], network propagation [20], diffusion state distance [5], and random walk with restarts (RWR) [1,19].

Motivated by widespread use of random-walk proximity, significant effort has been devoted to reducing operation counts associated with computation of proximity. These ef-forts exploit numerical and structural characteristics of the problem to reduce operation counts. For instance, in the context of the top-k proximity queries, during the iterative procedure, if it can be guaranteed that some nodes cannot enter into the top-k set any further, the associated compu-tations can be eliminated [23]. Likewise, owing to the struc-ture of the network, the proximity of each node to the query node can be bounded systematically by considering nodes in a breadth-first-like order [21]. These techniques have been demonstrated to yield significant improvement in runtime in the context of diverse applications.

In this paper, we take an alternate approach, based on accelerating the convergence of the underlying iterative pro-cess. To achieve this, we adapt a result from classical linear algebra, based on Chebyshev polynomials. Traditional itera-tive procedures use the result from one iteration to compute the next iterate. The iterations are terminated when con-vergence is detected, i.e., when the proximity vector does not change significantly between two iterations. We can, however, define the next iterate as a linear combination of a predefined set of previous iterates. The coefficients of this linear combination can be optimally computed using Cheby-shev polynomials. We demonstrate this process and show that the resulting the iterate converges much faster than the iterate in the original formulation. This results in sig-nificant speed-up in the computation of random walk based proximity scores. Furthermore, we show that our method can be combined with existing methods to further improve the processing of top-k proximity queries.

We provide detailed theoretical justification for our re-sults, and experimentally demonstrate the superior perfor-mance of our method on a number of real-world benchmark problems. Specifically, we show that (i) our method yields significant performance improvements over state of the art methods (reducing the number of iterations required to com-pute network proximity to all nodes many-fold on networks with hundreds of thousands of edges); and (ii) for top-K proximity queries, our method yields two-fold improvement in runtime on graphs with millions of nodes and edges. The asymptotic acceleration of our scheme implies that this per-formance improvement increases as problem sizes are scaled up.

The rest of the paper is organized as follows: in the next section, we provide an overview of the literature on efficient computation of network proximity. In Section 3, we describe the terminology, establish background on random walk prox-imity and top-k proximity queries, and describe our method. In Section 4, we provide detailed experimental evaluation of our method. We draw conclusions and summarize avenues for further research in Section 5.
Network proximity querying has received significant at-tention over the past years. In particular, top-k proximity queries in networks involve identifying the k nodes that are in close (random walk) proximity to a query node (or a set of query nodes). One of the basic approaches to computing random walk based proximity is the power iteration [16]. An alternate approach to power iterations is offline computation through LU decomposition and storing the factors for prox-imity estimation during online query processng [8, 19, 22]. However, LU decomposition is expensive and usually not feasible for very large networks. Furthermore, since the un-derlying networks are often dynamic, even small perturba-tions to the network require repetition of this costly proce-dure.

Recently, a number of approaches have been proposed to scale top-k proximity queries to very large and sparse networks. These methods take advantage of the numerical properties of the iterative methods to bound the proximity of nodes in the network in early iterations, thereby stopping the computation early when only k contenders are left [23]. Other methods utilize the structure of the network to per-form a local search around the query node(s), based on the notion that nodes with high random-walk based proximity to the query node are also in close neighborhood of the query node in terms of the number of hops [3,4,12,24]. However, most of these local search based methods are approximate, in the sense that they do not provide guarantees for identi-fying the exact set of of k nodes that are most proximate to the query node. Recently, two local search based methods, FLoS [21] and Ripple [23] have been shown to enable exact computation of top-k proximity queries without compromis-ing efficiency.

In this paper, we also focus on exact computation of net-work proximity. Our method is fundamentally different from existing approaches in that it exploits the numerical proper-ties of the iterative procedure to speed-up its computation. This method can be used to efficiently compute the random walk with restarts based proximity of all nodes in the net-work to a given node, or to speed up the processing of top-proximity queries.
In this section, we first define random walk with restarts (RWR) and and top-K network proximity queries based on RWR. We then present insights from numerical linear al-gebra to motivate the use of polynomials for accelerating convergence of iterative procedures used to compute RWR based proximity. Subsequently, we show that Chebyshev polynomials can be used to optimize the convergence of it-erative computation of RWR, and bound the relative error in each iteration. Finally, we discuss how these bounds can be used to efficiently process top-K network proximity queries. We conclude this section by showing that our method gen-eralizes to any proximity measure that can be iteratively computed, for which error in each iteration is bounded.
Let G =( V , E , w ) represent a given network, directed or undirected, where V denotes the set of nodes, E denotes the set of edges, and w : E  X  R denotes the weight function assigned to the edges. Given a node q  X  V , the random walk with restart based proximity to q is defined as follows: Here, P denotes the column-normalized stochastic matrix derived from the adjacency matrix of G by dividing each en-try by the corresponding column sum, r q denotes the restart vector that contains a 1 at its q th entry and a 0 in all other entries, and 0 &lt; X &lt; 1 denotes the damping factor. This parameter determines the probability of restarting at q in a random walk of the network. Defined this way, x q ( u )rep-resents the probability of being at node u at a random step of a sufficiently long random walk that starts at q and ei-ther moves to an adjacent node (with probability 1  X   X  )or restarts at node q (with probability  X  )ateachstep.
While we focus on random walk with restarts here for clar-ity of discussion, the method described in this paper directly applies to any proximity measure that can be written in the form x = Ax + c , where A is a matrix with largest eigen-value  X  ( A ) &lt; 1and c is constant vector. Such proximity measures include penalized hitting probability [24], effective importance [4], and discounted hitting time [17].
In practice, x q is computed iteratively, by setting x q (0) 0 and performing the following computation: in the t th iteration. This iterative procedure terminates plying convergence. As we discuss below, the residual in this iterative procedure is proportional to the t thpowerofthe largest eigenvalue of the matrix (1  X   X  ) P . Therefore the it-erative procedure is guaranteed to converge since the largest eigenvalue of P is equal to 1 and (1  X   X  ) &lt; 1. Throughout this paper, we refer to this procedure as the standard power iteration.
Given undirected network G =( V , E , w ), a query node q  X  V , and a positive number k , the top-k proximity query for RWR proximity returns the k nodes in V correspond-ing to the largest values in x  X  q [23]. Note that the largest values correspond to the highest proximity (smallest RWR distance). The current state of the art in efficiently pro-cessing top-k proximity queries is based on two approaches. The first approach uses the bound on the residual in the iterative computation of x  X  q to eliminate nodes whose prox-imity values cannot exceed that of the notes that are already among the k . This process terminates when all but the top k nodes are eliminated. This approach improves efficiency by reducing the number of iterations, and is implemented by the Squeeze algorithm developed by Zhang et al. [23]. The second approach uses network structure, in combination with the residual, to bound the proximity of each node to the query node and avoids computing proximity scores for nodes that are sufficiently distant from the query node in terms of the number of hops. This approach improves effi-ciency by further reducing the number iterations, as well as the number of operations in each iteration. It was proposed by Wu et al. [21] and is also implemented in the Ripple algorithm developed by Zhang et al. [23].

All of these methods are based on the standard power it-erations, i.e., the computation of the current vector iterate only uses its value from the previous iteration. Here, we show that the convergence rate of the iterative procedure can be significantly improved by utilizing the information gathered from all previous iterations. Specifically, we use Chebyshev polynomials to aggregate the values of x q across iterations, obtaining a better approximation to the steady state vector in each iteration. This results in faster conver-gence, which in turn also yields more effective pruning of nodes. The number of iterations required for such a pro-cedure to process top-k proximity queries is consequently much lower. The core idea behind the proposed Ch ebyshev Po lynomial Based E fficient P roximity R etrieval ( Chopper ) algorithm is to utilize the previously computed x q ( t ) vectors in Equa-tion (2) to obtain a better approximation to x  X  q in the next iteration. To describe this idea, for a given query node q we first define W =(1  X   X  ) P and rewrite the (2) as follows: Observe that, since P is a stochastic matrix, we have |  X  (1  X   X  ) &lt; 1, hence the iterative procedure described by (3) converges to the solution of (1), x  X  q .

The idea behind Chebyshev acceleration is as follows: For the t th iteration of the iterative procedure, and a given series  X  ( m )for0  X  m  X  t , we define vector y ( t ) q : Our objective is to choose a sequence  X  t such that the se-quence y ( t ) q converges to x  X  q faster than x q ( t ) as possible. Observe that, if we use y ( t ) q to approximate x the residual in the t th iteration is given by: Thus, for each t ,thechosen  X  t should minimize || e ( t ) || subject to the constraint: t m =0  X  t ( m )=1. Thisconstraint ensures that the linear combination also converges to x  X 
We can reformulate the residual at the t th iteration as a Algorithm 1: The Chopper Algorithm Input : G =( V , E ), q , k ,  X  ,and r q
Output: aset R  X  V that contains the top-K most 1 Construct matrix W 2
R  X  V , t  X  1, y (0) q  X  0 4  X   X  2  X   X  5  X  0  X  1 , X  1  X  1 6 while | R | &gt;k do 9  X   X  k  X  th largest score in y ( t +1) q ( u ); 10 foreach u  X  R do 11 if ( y ( t +1) q ( u )+4  X  t ) &lt; X  then 12 Remove u from R ; 13 end 14 end 15 end polynomial as follows:
This observation suggests that, if we can find a sequence of polynomials p t such that p t ( W ) W t , the error in each iteration is much smaller than that in the standard power iteration.
It is an established result in linear algebra that for any matrix W and polynomial p t ,wecanwrite  X  ( p t ( W )) = p (  X  ( W )), where  X  ( . ) denotes the set of eigenvalues of the matrix [15]. Therefore, as described by Saad [15], || e ( t ) || be minimized by solving the following minimization prob-lem: Here, P t denotes the family of all polynomials of order t
Clearly, computing the eigenvalues of W would defeat our purpose, since this computation is at least as expensive as solving (1). However, since P is column normalized, we know that W  X   X  ( W )  X  (1  X   X  ) [23]. Hence, we can relax (7), and rewrite it as: where
As stated in the following theorem, the solution to min-imax optimization problems of the form of (9) is provided by the well known Chebyshev polynomial of first kind, given by the following recurrence: T 0 ( z )=1 ,T 1 ( z )= z,T t +1 2
Theorem 1. Let [ a, b ]  X  R be non-empty interval and  X  be a real number such that a&lt;b&lt; X  .Then, and F (  X  t ) = 1 where  X  = Detailed proof of this theorem can be found in [16]. We can immediately apply this result to our problem. From the construction of W , we know that  X  ( W )=1  X   X  , where  X  ( . ) denotes the largest eigenvalue of a matrix. Then, letting a =  X   X  1 ,b =1  X   X , and  X  =1,wehave
Therefore, using Theorem 1 and (6), we obtain where  X  = 2
This result demonstrates the power of Chebyshev polyno-mials in accelerating the computation of random walk with restart based proximity scores. Specifically, for standard power iteration, we have In contrast, if we use Chebyshev acceleration, we have Since  X &lt; 1  X   X  ,thesequence y ( t ) q converges much faster
We now describe an efficient technique for computing y ( t ) putation requires the addition of t vectors in the t th itera-tion, and that we need to store all x q ( t ) vectors computed throughout the power iteration. However, exploiting the observation that Chebyshev polynomials are defined as a re-currence, we can show that y ( t +1) q can be computed from y q and y
For this purpose, let  X  t = T t ( 1  X  isfies three-term recurrence. Therefore, we can write Finally, we can use above last equation and basic algebraic manipulations to obtain [16]: In other words, we can compute y ( t +1) q from y ( t ) q without requiring storage of the previous iterates.
While Chebyshev acceleration can be used to efficiently compute the proximity of all nodes in V to a query node q it often suffices to identify the k nodes that are most prox-imate to q , where k | V | . As discussed above, existing algorithms for efficiently processing top-k proximity queries use the convergence properties of power iteration and the topology of the network to quickly identify nodes that are not sufficiently proximate to k , so that such nodes can be pruned out [21, 23]. In Squeeze , Zhang et al. [23] use the bound on the norm of the residual for standard power iter-ation ((1  X   X  ) t ) to obtain a bound on the proximity score of a node. Subsequently, for  X  X n-bound proximity queries X  (i.e., the proximity is quantified in terms of the probability of being at the query node for a random walk that makes frequent restarts at each other node), they show that the proximity score of each node forms a monotonically non-decreasing sequence throughout the power iterations. They utilize this monotonicity, along with the bound on the norm of the residual, to identify the nodes that can be pruned.
When Chebyshev acceleration is used to compute prox-imity scores, y ( t ) q ( u ) does not produce a monotonically non-decreasing sequence for all nodes u  X  V . However, as we show by the following theorem, this is not required, and that the error bound provided by Chebyshev acceleration can indeed be used to quickly identify nodes that are not sufficiently proximate to q . More importantly, this theorem shows that the idea is not limited to  X  X n-bound proximity queries X ; rather, it can be applied to any proximity measure that can be computed via power iterations, for which the error in each iteration can be bounded.

Theorem 2. Let S q denote the set of the k nodes in V that are most proximate to q .Let u t bethenodesuchthat u
Proof. There are two possible cases to consider. In the first case, y ( t ) q ( u )isamongthetop k values in y ( t ) case, it is clear that y ( t ) q ( u )  X  y ( t ) q ( u t ) y q ( u t ) by definition of u t .

In the second case, y ( t ) q ( u ) is not among the top k y q . In this case, there must be at least one node v  X  V such that y ( t ) q ( v )isamongthetop k values in y ( t ) q , but (at least one node must drop out of the top-k list to make space for u in the top-k list). Now, using y ( t ) q  X  x we obtain the following inequalities: and Since u  X  S k but v/  X  S k ,wehave x  X  q ( u )  X  x  X  q ( v follows that Since v is in the top-k at the t th iteration, we have y y q ( u t ), so we obtain y q ( u )+2  X 
Using this result, at any step of the Chebyshev iteration, we can identify nodes that are not sufficiently proximate to q to make it to the top-k list. Specifically, at iteration y q ( u ) &lt; y pruned out from the list of candidates for the top-k list. The resulting algorithm for computing the top-k most proximate nodes to q is given in Algorithm 1.

Since  X &lt; (1  X   X  ), we have 4  X  t (1  X   X  ) t for the val-ues of t that are of interest (i.e., the number of iterations is large enough for very large networks). Therefore, Cheby-shev acceleration provides a more efficient method for pro-cessing top-K queries than algorithms that utilize the con-vergence characteristics of standard power iteration, e.g., Squeeze [23]. Furthermore, as we demonstrate in the next section via comprehensive experimental results, although Chopper does not directly utilize information on network structures to speed up computation, it also outperforms al-gorithms that utilize the network structure. as function of the number of iterations, for damping factor . 01 , 0 . 05 , 0 . 1 , 0 . 9.
In this section, we systematically evaluate the performance of the proposed algorithm, Chopper , in accelerating the computation of network proximity scores and processing of top-k proximity queries. As shown in the previous section, the proposed algorithm is X  X xact X  X n the sense that it is guar-anteed to correctly identify the set K nodes that are most proximate to the query node. For this reason, we here focus on computational cost (measured in terms of number of it-erations and runtime) here, and compare Chopper against other exact algorithms.

We start our discussion by describing the datasets and the experimental setup. We then assess the performance of Chebyshev acceleration in the computation of random walk based proximity globally, i.e., for all nodes in the network. Subsequently, we compare the performance of Chopper in processing top-k proximity queries with two state-of-the-art algorithms, Squeeze and Ripple [23], using both number of iterations and runtime as performance criteria.
We use four real-world network datasets from the Stan-ford Network Analysis Project 1 for our experiments. Details of these four networks are given on Table 2. The Enron E-mail dataset represents the undirected e-mail communica-tion network at Enron. Brightkite and Gowalla datasets represent the Brightkite and Gowalla location based online social networks. The Skitter dataset represents the undi-rected internet topology graph, constructed from traceroutes run daily in 2005. These datasets are selected as represen-tative samples for network sizes in terms of the number of nodes and of edges. Furthermore, network proximity queries are meaningful on all of these networks.

For Squeeze and Ripple algorithms, we use the Java im-plementation provided by Zhang et al [23]. We implement Chopper in both Matlab and Java, and the results reported for runtime reflect that of the implementation in Java. We assess the performance of the algorithms for different values https://snap.stanford.edu/data/ Figure 2: The number of iterations required for Chopper ,
Squeeze ,and Ripple in computing the top-k nodes that are most proximate to a query node, as a function of of damping factor (restart probability), as well as the pa-rameter k for top-k proximity queries. For all experiments involving top-k proximity queries, we randomly select 1000 query nodes and report the average of the performance fig-ures for these 1000 queries. In all experiments, r q is set to the identity vector for node q . All of the experiments are performed on a Dell PowerEdge T5100 server with two 2.4 GHz Intel Xeon E5530 processors and 32 GB of memory.
We assess the performance of Chebyshev polynomials in accelerating the computation of network proximity scores for all nodes in the network. The purpose of this analysis is to quantify the improvement using Chebyshev accelera-tion beyond specific applications such as top-k proximity queries, and to demonstrate its applicability to a broader range of problems that involve iterative computation of net-work proximity scores.

Figure 1 shows the convergence rate for Chebyshev accel-eration in comparison to the standard power iteration for computing random walk based restart based proximity. We limit this analysis to the Enron Email dataset, since global proximity computation is expensive (and often unnecessary) for larger networks. In the figure, for the restart probability  X  ranging from 0 . 01 to 0 . 9, we report the norm of the differ-ence between two successive values of the proximity vector at each iteration. In all cases, the threshold for convergence is set to 10  X  10 . As seen in the figure, Chebyshev accelera-tion significantly reduces the number of iterations required to compute RWR based proximity.

It is important to note that Chebyshev acceleration pro-vides larger performance gains for smaller values of  X  . This observation is consistent with the theoretical analysis of the convergence of Chebyshev acceleration, reflected in the re-lationship between the variable  X  , which characterizes the convergence of Chebyshev acceleration and the parameter  X  . Intuitively, for large values of  X  , the random walk is largely localized  X  thus acceleration of convergence is of lim-ited benefit. However, queries that involve large  X  are of limited practical interest, since they do not fully utilize the information provided by the network (e.g., for  X &gt; 0 . 5the random walk is expected to move away from the query node Figure 3: The number of iterations required for Chopper ,
Squeeze ,and Ripple in computing the top-k =20 averages across 1000 randomly chosen query nodes for each . by two hops on the average). For this reason, larger perfor-mance gains for smaller values of  X  are valuable for practical applications.
In top-k proximity queries, the two major parameters are the damping factor,  X  , and the number of nodes to be iden-tified as most proximate to the query node, k .Weevaluate the performance of Chopper as a function of both parame-ters, using four large real-world networks.

The performance of Chopper in comparison to two algo-rithms, Squeeze and Ripple as a function of k is shown in Figure 2. For this analysis, we fix the damping factor  X  to 0 . 2. Recall that Squeeze uses the convergence char-acteristics of the standard power iteration to terminate the power iterations early. Ripple , on the other hand, also uses network structure to bound the proximity of the nodes in the network to the query node, thereby reducing the num-ber of iterations further. As seen in the figure, Chopper consistently outperforms both algorithms for all four net-works. The favorable performance of Chopper as compared to Ripple , is particularly notable, since Ripple also uses information on network structure to speed up computation, whereas Chopper only utilizes the convergence properties of the numerical scheme.

Theonlycaseinwhich Ripple requires fewer iterations than Chopper is for k =4onthe Skitter data set. When the network diameter is large and the query results are local-ized around the seed node, utilization of network structure is particularly beneficial. However, even for such networks, if the query seeks a larger number of proximate nodes (  X  leveraging network structure does not reduce the number of iterations for standard power iterations as much as Cheby-shev acceleration does.

We also compare the performance of Chopper with Squeeze and Ripple as a function of the damping factor  X  .Forthis purpose, we fix k = 20 and process top-k proximity queries for 1000 randomly chosen queries for  X  ranging from 0 . 01 to 0 . 9. The results of this analysis for all four data sets is shown in the Figure 3. In each panel, close-ups of the curves for Chopper and Ripple are also shown for smaller values of  X  to facilitate better comparison. As seen in the figure, the number of iterations required for processing top-k queries is similar for all algorithms on all datasets for  X   X  0 . 5. This Figure 4: The runtime in seconds required by Chopper ,
Squeeze ,and Ripple to process queries for top k nodes that are most proximate to a query node, as a function of is expected since the search is  X  X asier X , i.e., it is limited to nodes that are very close to the query node in this case. However, for smaller (and more relevant) values of  X  , Chop-per consistently achieves best performance. Finally, we compare the performance of Chopper with Squeeze and Ripple in terms of running time as a function of k . For this purpose, we fix  X  =0 . 2 and process top-k proximity queries for 1000 randomly chosen queries for k ranging from 4 to 4096. The results of this analysis for all four data sets is shown in the Figure 4. As expected, the time required to process top-k queries increases with k for all methods. However, across all datasets and for all values of  X  , Chopper consistently delivers best performance, with two-fold improvement over the runtime of Ripple for the largest dataset Skitter .
In this paper, we propose an alternate approach to accel-erating network proximity queries. The proposed approach is based on Chebyshev polynomials, an established accel-eration technique for iterative methods in numerical lin-ear algebra. We show that our approach, Chopper , pro-duces asymptotically faster convergence in theory, and sig-nificantly decreased convergence times in practice on real-world problems. Using a number of large real-world net-works, and top-k proximity queries as the benchmark prob-lem, we show that Chopper outperforms existing methods significantly for wide ranges of parameter values. When in-tegrated with existing methods, Chopper yields further im-provement in performance over state of the art techniques.
Future efforts in this direction would include incorpora-tion of other acceleration techniques into our framework, extensions to other iterative proximity measures, and their applications. Furthermore, while Chopper is an  X  X xact X  al-gorithm and our experiments focus on runtime performance for this reason, there also exist approximate methods that compromise accuracy for improved runtime. Comparison of Chopper against such approximate algorithms can provide further insights into the trade-off between runtime and ac-curacy in the context of network proximity problems. We would like to thank Xiang Zhang, Yubao Wu, and Sean Maxwell for many useful discussions, and anonymous re-viewers for their valuable comments. This work was sup-ported in part by US National Science Foundation (NSF) grants CCF-1533795, CCF-0953195, and CNS 1422338, and US National Institutes of Health (NIH) grant U01-CA198941. [1] D. Aldous and J. A. Fill. Reversible markov chains [2] R. Andersen, F. Chung, and K. Lang. Local graph [3] R. Andersen, F. Chung, and K. Lang. Using pagerank [4] P. Bogdanov and A. Singh. Accurate and scalable [5] M. Cao, H. Zhang, J. Park, N. M. Daniels, M. E. [6] M. Co  X  skun and M. Koyut  X  urk. Link prediction in large [7] S. Erten, G. Bebek, R. M. Ewing, and M. Koyut  X  urk. [8] Y. Fujiwara, M. Nakatsuji, M. Onizuka, and [9] M. Hofree, J. P. Shen, H. Carter, A. Gross, and [10] H. Jeong and B. Yoon. Accurate multiple network [11] D. Liben-Nowell and J. Kleinberg. The link-prediction [12] Q. Mei, D. Zhou, and K. Church. Query suggestion [13] S. Navlakha and C. Kingsford. The power of protein [14] L. Page, S. Brin, R. Motwani, and T. Winograd. The [15] Y. Saad. Chebyshev acceleration techniques for [16] Y. Saad. I terative Methods for Sparse Linear Systems. [17] P. Sarkar and A. W. Moore. Fast nearest-neighbor [18] T. Tao and C. Zhai. An exploration of proximity [19] H. Tong, C. Faloutsos, and J.-Y. Pan. Fast random [20] O. Vanunu, O. Magger, E. Ruppin, T. Shlomi, and [21] Y.Wu,R.Jin,andX.Zhang.Fastandunifiedlocal [22] W. Yu and X. Lin. Irwr: incremental random walk [23] C. Zhang, S. Jiang, Y. Chen, Y. Sun, and J. Han. Fast [24] C. Zhang, L. Shou, K. Chen, G. Chen, and Y. Bei. [25] J. Zhao and Y. Yun. A proximity language model for
