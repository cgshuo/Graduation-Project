 In this paper we approach the problem of providing correc-tions for incorrectly typed URLs. This problem is signifi-cantly different from the classical spelling correction prob-lem. We describe our contribution -building a custom data structure and a search algorithm that can find approximate matches for incorrect URLs. We evaluate the quality of our results through experiments with analysts. Our system is now being used in the Google search engine.
 H.3.3 [ Information Search and Retrieval ]: [Search pro-cess]; H.3.1 [ Content Analysis and Indexing ]: [Indexing Methods] Algorithms, Experimentation
The problem of incorrectly typed URLs affects a large number of Internet users. Many users make spelling mis-takes frequently while typing URLs into the address bar of a web browser, or entering them into the search box of a web search engine. Our analysis of Google search logs has shown that, on a global scale, this happens many millions of times per day, so a solution is desirable.

URL correction presents some important differences from the classical word-level spelling correction problem. URLs are longer than usual words. The dictionary is several or-ders of magnitude larger; we are using many tens of millions of URLs, selected from our web search index. We have to refresh our data more often, as the set of valid URLs from the web is changing over time. Trying to correct spelling errors in individual URL terms does not guarantee that the result is an existing URL. The quality metrics and ranking algorithms that are suitable fo r URL correction are different from the ones used in traditional spelling correction. Finally, the solution has to be fast enough for real-time usage.
As this direction of research has practical applications for search engines, we were motivated to pursue it.
 We have not discovered a previous similar approach to URL correction.

Matching an invalid URL against each of the valid URLs in our index would be too slow for real-time usage. We need an efficient data structure that can store a large set of URLs and provides support for fast approximate matching. We use a custom data structure, which we are calling URLTrie.
The URLTrie is based on the Patricia trie [2]; it uses a tree to store a collection of strings -in our case, URLs. Each string is stored as a path from the root to one of the leaves. The strings in the nodes of the tree, further known as node strings , are substrings of the initial URLs. The nodes in our URLTrie also store additional information that can be used during the search, and for a final result filtering. and the URLs stored in the trie. So we can use each leaf of the trie to store informatio n associated with the corre-sponding URL, including several quality metrics.

While doing approximate search, an exhaustive browsing of a large URLTrie would be too slow. So we augment the data structure by adding information that can guide the search, as supplemental fields in the non-terminal nodes. For this purpose, we have defined a guiding function .
For any non-terminal node of the URLTrie, the guiding function attempts to express how useful it is to explore the subtree with the root in that node, during the search. The search algorithm takes into account the values of the guiding function when deciding which nodes to explore.

As an example, the value of the guiding function in a non-terminal node can be the maximum PageRank [3] of any page represented by nodes in the associated subtree.
The guiding function that we are using is a combination of several page-level and query-independent quality metrics. We can not provide the complete details due to competitive reasons. The approach also works well with simpler func-tions.
Given a URLTrie containing tens of millions of URLs, and a query URL, we are trying to find the best approximate matches. The results need to be within a small edit distance from the query. We explore a set of relevant nodes in the URLTrie, using a priority queue.

A state in our search is defined by the following values: the current node to be matched, the length of the URL prefix that was matched so far, and the edit distance so far. The initial state of the search consists of the tuple: the root of the tree, zero characters matched, zero edit distance.
The algorithm is summarized below. Algorithm 1 ApproximateStringMatching( Q , U , MAX )
Input: query string Q [0 ..len  X  1], URLTrie U ,maximum allowed edit distance MAX
Add initial state I ( root, 0 , 0) to queue while not finished do end while
We are storing states in a priority queue. The priority metric depends on the edit distance so far, the number of characters still left to match, and the value of the guiding function in the current node.

At each step, we explore the state with the highest prior-ity. We know the length of the prefix of the URL that was matched so far, and start at the position immediately after that. We use a version of the standard dynamic program-ming algorithm [1] to compute the edit distance between the string in the node, and all substrings of the query starting at that position. Having a small upper limit for the edit dis-tance allows us to perform the computation in linear time.
After this step, we select the positions in the query string that lead to small, acceptable edit distances -if such posi-tions exist. We generate new search states, using the descen-dants of the current node, with the appropriate values for edit distance and number of matched characters. We add these states to the queue. Then, we resume the process, by retrieving the state with the hig hest priority from the queue.
We generate solutions for the search when we match the string in a leaf node with the final part of the query.
We stop the search after a few good results are obtained, or after exploring a few thousand states.

It is also easy to use the URLTrie to provide completion for truncated URLs (example: www.my -&gt; www.myspace.com ).
After the search finishes, we recompute the edit distance between the original query and each solution, using a metric that is more suitable for comparing URLs. For example, a top-level domain change from .com to .edu is not heavily penalized. We combine the edit distance with the query-independent quality values to obtain the final score of each solution. We choose the best results according to this score. We evaluated our results several times during our research. We are presenting two such experiments. For each evalua-tion, we used incorrectly typed URLs extracted from our logs. We selected 1000 random test cases where our system returned corrections, and provid ed our first result for rating. Each test case was rated independently by three analysts.
For each test case, the analysts tried to navigate to the original URL and to our correction, and then had to answer Table 1: Evaluation results for our two experiments. Question Experiment 1 Experiment 2 Good result? 70.0% 71.5% 81.7% 78.0% Browsing error? 93.4% 93.7% 94.9% 94.8%
Parked domain? 4.3% 91.5% 2.3% 93.9% a few Yes/No questions. The main question was if our result is relevant. We also asked if navigating to the original URL results in a browsing error, an d if our correction is a parked domain. Parked domains contain an  X  X nder construction X  message, or have no relevant content besides advertisement; they are undesirable suggestions most of the time.
For each test case and each question, we selected the an-swer provided by at least two analysts. For each question, we also counted how many times all the three analysts agreed.
Table 1 presents the results for the two experiments. The second experiment shows notable improvements. We have achieved a large increase in the number of good results, and reduced the number of results that were parked domains. The agreement numbers were also higher the second time, as the analysts gained experience with the process.
Before the second experiment, we made several important improvements. We were able to detect and remove a lot of parked domains and low-quality URLs from our index. We fixed an issue with queries containing spaces. We improved the guiding function and the URL comparison metric.
When preparing the experiments, we used only URLs that resulted in a browsing error for actual users. However, the analysts reported that some of the query URLs were func-tional. This is due to the time difference between the setup of the experiment and the analysts X  attempt to view the respective URLs. In the meantime, some domains were reg-istered at addresses that were not functional earlier, and some sites may have recovere d from temporary problems.
The problem of handling incorrectly typed URLs is im-portant for search engines. We have designed and imple-mented an efficient solution, based on building a customized Patricia trie and performing fast approximate search. Our system achieved 81.7% precision in our evaluation with an-alysts, and is now integrated into the Google search engine.
In the future, we plan to improve our metric for compar-ing URLs, experiment with different guiding functions, and improve our quality evaluation strategies. Also, we can work towards making URL correctio n a standard part of the web browsing experience. [1] V. I. Levenshtein. Binary codes capable of correcting [2] D. R. Morrison. Patricia -practical algorithm to [3] L. Page, S. Brin, R. Motwani, and T. Winograd. The
