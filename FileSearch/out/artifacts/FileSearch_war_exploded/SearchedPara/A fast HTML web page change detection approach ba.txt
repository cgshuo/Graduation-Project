 1. Introduction
Changes occurring in web pages are best classified as content changes (e.g., deletions, and additions of text), layout change (e.g., changes in the position of elements in the page), and attributes change (e.g., changes in fonts and colors) [9] . Hence, in addition to tracking content changes, any useful detection system should also be able to track changes in layout.

Most change detection approaches are computationally complex and require non-polynomial running time [4,5] . Some of
CMW [9] . These basically work by estimating the rate of change that occurred between the reference web page and its up-dated version, and eventually locating the differences between them.

In this work, we propose an efficient method for detecting web page changes. It generates subtrees corresponding to ele-ments that are directly connected to the BODY HTML tag. The tags found are used to mark the nodes in the subtrees belonging to the two pages being compared and are employed to limit the similarity computations to nodes having the same mark.
Subtrees with the highest average similarity coefficients are considered to be the most similar. Using this information, changes in the updated version of the web page are identified and located. Additionally, a scheme was employed to speed up the algorithm through hashing the web page in order to provide direct access to subtree nodes during the comparison process. 2. Related work
One challenge that has not been addressed sufficiently in the literature is the large time it takes to compare HTML web tween two HTML web pages, all the different HTML nodes (corresponding to content and attributes of tags) have to be com-pared, typically leading to an NP-hard problem [4,5] . In this regard, the approach in [9] uses the O( N to compute the maximum weighted matching on a weighted bipartite graph and has a running time in O  X  N and N 2 are, respectively, the number of nodes in the old page and in the new (changed) page. This running time becomes for changes) is increased from a small portion to the whole page.

An existing web change detection product is Copernic Tracker [23] , which is a software aimed at monitoring websites. It can track changes in the text and images and monitor for the presence of specific text. The system however does not allow provide objective performance data other than subjective user reviews. A third system is WebCQ [27] , which offers person-to the user reporting content changes only. These describe modifications to text, hyper-links, image references, and key-words, in addition to reporting modification date and page size changes. The authors however promise to implement into WebCQ a structure-aware change detection and difference algorithm in the future [18] .

In terms of published research work, several papers were found that tackle the design of efficient algorithms for detecting rithm in [12] is based on finding and then extracting the matching nodes from the two trees that are being compared. From the non-matching nodes, the change operations are next detected. Matching of nodes is based on comparing signatures (functions of node content and children) and order of occurrence in common order subsequences of nodes. The works in [7,26] use edit scripting to compare two documents and transform the pages to trees according to the XML structure. The mance cannot be achieved when comparing HTML documents as it relies on certain XML features. Edit scripting alone is not sufficient for achieving O( n log n ) or polynomial running time, especially if move operations (parts of the document are moved around) are to be considered. In fact, it has been shown that edit scripting with move operations is NP-hard other tree T 2 . The discussed diff algorithms consider that if a node in T matched. Under this convention, two trees that have unmatched roots can never include matches: if M is a matching from T to T achieved since a matching between two nodes can now be propagated bottom-upwards in the tree. Unlike HTML, XML en-h book i is a unique child of the tag type h library i : ship between tag types and their ancestors does not hold, as we obviously can have the same tag type present in several
The work in [9] transforms an HTML document into a tree structure and categorizes node information into content, struc-similar attributes), respectively. In searching for the most similar subtree between two pages, the system supposedly uses the Hungarian algorithm [15] , but no details on its use are given. The experimental results only show the effects of the emphasis measures on detection accuracy and do not discuss speed performance. It should be mentioned that this approach is able to detect node type changes (e.g., a UL node changing to an proposed approach applies the node comparison between same types, and thus will not detect such changes. In [14] , a recent change detection approach for static html pages was proposed. It is based on three heuristic-derived optimizations to the
Hungarian algorithm. First, though, the similarity coefficients between all nodes of every subtree in the reference page and those of every subtree in the updated page are computed and placed in a cost matrix, which is needed for the Hungarian similar subtrees, the second one deals with stopping the algorithm if the size of the updated page X  X  subtree becomes less pares subtree nodes if their root nodes are of the same type. This simple strategy, along with implementation techniques mostly dealing with hashing, has served to generate a more efficient detection system. More specifically, the speed perfor-mance results in [14] , which were reported for web pages of less than 1100 nodes, under-perform those presented in this paper by a margin of up to 3 s. 3. Improved detection framework is launched by a daemon process that runs periodically. Every time this process runs, it checks a schedule to determine if there are web pages that need to be downloaded and subsequently compared to their corresponding stored versions. The schedule in turn is populated through a user interface that allows the user to specify the web page to monitor in addition to supplying information that controls the monitoring process. Moreover, when a user adds a URL to the list of pages to be monitored, he or she can specify a zone within the page that limits the change detection to this zone. At the conclusion of each change detection occurrence, the application writes to the disk data that describe the changes and their locations within the page. This allows the user or another program to query the application to view the actual changes on the page
Change detection is accomplished by comparing the newly-downloaded web page to a previously-downloaded version stored on disk. Detection is based on calculating the similarity among the different parts of two HTML documents and on deducing the ones that are most similar. The web page parts that are not 100% similar are considered to have been changed. 3.1. General design
To speed up the process of web change detection to the greatest extent, the design of the detection system implemented several hashing-based techniques for direct lookup of subtree node information during comparisons, and eliminated irrele-vant node comparisons by limiting them to nodes of the same type (i.e., same HTML tag). We build on the system in [9] , which describes a complete framework for detecting changed parts in web pages, and as such we will refer to it throughout mance of the original approach, which has a running time in O  X  N mapping between the updated page (with N 2 nodes) and its previous version (having N computations to nodes (corresponding to HTML tags) having the same tag type. In contrast, and in order to reduce the num-defined threshold in an attempt to remove the redundant nodes that do not have the same type as the compared one.
Moreover, it has to perform the similarity computation in order to evaluate the similarity weight, while the enhanced ap-proach avoids it altogether by simply comparing two tag types. A second remark could be made concerning finding the most the original page. The factor of four was deduced from experimental results. After the division, the algorithm proceeds to comparing all the nodes, computes nodes similarity, and then subtree similarity for every variable subtree until it finds be mentioned though that this property introduces one limitation associated with the enhanced approach concerning the and explain that it does not pose a serious issue.

Another technique that we integrated into our enhanced approach is the use of hash tables to significantly speed up the access to subtree nodes during the subtree comparison process. This technique along with limiting node comparison to those with the same tag types have allowed for achieving HTML web page change detection times that are in the order of seconds approaches, which were designed specifically to work with XML documents, such as the X-Diff algorithm described in [26] . 3.2. Framework steps attributes . Given two pages P 1 and P 2 (updated version of P while assuming that P 1 was previously parsed and stored. P formed into a tree, in which each node represents a tag of the web page and holds information and properties about it. In a hash table when needed (described in Section 3.2.2 ). The subtrees within the tree of P (referred to as marks ) that will form the basis of the performance improvement made to the web change detection problem.
The similarity coefficients of the compatible nodes are then calculated for each subtree in P ilarities, a similarity for each subtree of P 1 is computed relative to all the subtrees of P similar subtrees are different than 1.

In the following, each step is described in more details in a separate subsection. 3.2.1. Web page cleaning
Web page cleaning is implemented using HTML Tidy [11] , which has many features, including detecting and correcting missing or mismatched end tags plus correcting end tags that are out-of-order. At the end of this phase, the processed HTML
HTML, and thus the same issues that were discussed in Section 2 still hold. 3.2.2. Page hashing and subtree generation
The nodes of the web page to be compared are hashed into a table. This table is extensively used during the O( N comparison process in which the nodes of each subtree in the updated page are compared to nodes in all the subtrees of the reference page that have similar marks. The hash table for the reference web page is saved on disk after its creation along with the subtree table that is described below. It is later read into memory when a new version of the web page is down-nodes from the list and examining them for the following attributes that become data members of the hash table: node (node X  X  HTML tag name), type (path from root to the node), of a hash table.

The generation of the subtrees was done while hashing the page by checking the depth of each node: if it is 3 then a new a specifically designed table that comprises the columns Mark ( div node) has level 3 and encloses a subtree of all elements beneath it until the next node with level 3 is encountered, which happens to be node 71 ( p node, as shown in Table 2 ). 3.2.3. Subtree comparison and mapping
To further improve performance, a look up table (LUT), which contains references to the subtrees of the second page, was mark. Actually, these references are row positions in the subtree table. For instance and as shown in Fig. 2 , the responds to rows 0, 2, and 3 in Table 2 . This arrangement minimizes comparison time, since searching for the subtrees of a specific mark can be done in O(1) instead of O( n ) sequential search through the whole subtree table.
To store comparison results for use later in the node mapping operation, a temporary table is created that includes seven columns ( node _ id1, node _ id2, attribute, type, weight weight _ position and weight _ total correspond, respectively, to the numerator and denominator used in Eq. (1) to figure out the content similarity between two subtrees, while attribute current subtree with all the subtrees having the same mark in the second page (with the aid of the LUT of the second page).
Actually, this involves comparing the nodes in the current subtree with those in the subtrees with the same mark, by making use of the hash table to directly access the data associated with the desired nodes. It should be noted that memory is allocated only for comparing one subtree, which involves computing the number of rows in the temporary table, as follows. calculated (end2-start2+1: retrieved from the subtree table) and next multiplied by the number of nodes in the considered number of rows for which memory is allocated.

Node mapping is then applied to find the most similar subtree from the second page to the considered subtree of the first page. This continues for the consequent subtrees, where memory is reclaimed and then allocated. 3.2.4. Subtree similarity computations
To describe the computations, we first define variables that represent elements of the subtree. First however, it should be mentioned that the tree representation of the web page was implemented using XSLT and XPath. The extraction of essential information from the XML file and transformation into a tree representation was done with the help of the Oracle XSLT par-file into nodes where each one represents an HTML tag.
 will be dealing with two trees T 1 and T 2 representing two web pages, their respective subtrees are denoted by T and T 2 j , j =1, ... , J . Hence, I and J are the number of subtrees in T m denotes the set of element types (marks) that are detected in all subtrees. For HTML, usually this set mostly includes
Moreover, we define m i as the set of subtree marks that denotes the group of element (HTML tag) types contained in sub-tree T i (i.e., m i m ). For example, if subtree T i contains an image and an unordered list, then m denotes a mark found in subtree T i , with k 2 {1,2, ... , K } being the k th index in the set m .
If { r 1 , r 2 , ... , r i } is the path from the root node r elements starting from the root of the tree and ending at r tag. w ( r i ) is the set of words in the text associated with the leaves of subtree rooted at node r denotes the page index (1 for stored page and 2 for new page), while m k th HTML tag in the set of possible tags. a ( r i ) is the set of attributes associated with r i . Hence, in a  X  r 1 those in w .

The word content similarity between two nodes is expressed using the definition of the function Intersect , which returns the percentage of words appearing in both w  X  r 1 i ; m For computing the similarity between node attributes , the function Attdist is used respect to all their attributes. As indicated in Eq. (2) , specifically the function Weight ( a ently according to their relevance of use. For example, HREF rules . Attribute weights vary between 0, least important, and 100, most important. Some of the attributes weights were in one column and weights as  X  X  X ntegers X  in the other column.

To compute the similarity between the paths from the root nodes of T respectively, the function  X  X  Typedist  X  is used where suf and max represent, respectively, the length of the common suffix (number of common nodes from root to the two concerned nodes, i.e., number of common HTML tags) and the maximum cardinality (number of HTML tags of the longest subtree mark m k , is computed as follows: varies between ( 1,1], where 1 corresponds to maximum difference and 1 to maximum similarity.

After computing the node CS values, the next step is to calculate similarity coefficients between subtrees. Given two sub-trees T 1 i and T 2 j belonging to trees T 1 and T 2 , a mapping M  X  T m , the Subtree Source Node Similarity is given by
That is, for each node in subtree T 1 i the above finds the most similar node in T
Next, the similarity of two subtrees T 1 i and T 2 j having nodes of the same mark, r 1 follows:
The above states that the similarity of each pair of subtrees belonging to T nodes of T 1 i ( P i is the number of nodes in T 1 i ).

Finally, the Document Subtree Similarity is the maximal subtree similarity between T interested in the node where the change took place, but rather in the subtree where the changed node resides. 4. Performance results
To produce relative performance results, both the enhanced and original approach were implemented. Tests were per-formed on a 1.8 GHz Pentium M NEC laptop, model i-Select M5410, running Windows XP SP2, with 1 GB RAM, and 1 MB cache. For implementation, Java was used to program the functionality of both approaches, Tidy [11] for cleaning HTML web pages and tags, and Oracle Parser for parsing and building the XML trees. 4.1. Results validation
Before proceeding with illustrating the performance results, we describe a procedure we used to validate the results. We focused on a highly dynamic page (home page of CNN.com ) and downloaded from the way-back machine a total of 520 pages covering the whole year of 2006, and therefore had an average of one to two pages per day. For this experiment, we con-ducted three trials, each of which involved choosing four pages randomly from the pool of 520 pages and comparing the next 20 pages that follow each one of those four pages to it using a balanced configuration (4,3,3). The obtained results showed cay and corresponds to a Poisson distribution that resembles the output reported in [6] . In this regard, it should be men-tioned that the web change frequency has been studied in the literature and modeled as a Poisson distribution [20,2,6] .
The reason for choosing 20 pages for this experiment is because they correspond to 10 days of monitoring, after which the web page contains content that is mostly new, as evidenced by the fact that the similarity coefficients tended to an asymp-our approach produces. 4.2. Presentation of results
For the performance tests, over 250 web pages were downloaded from the Internet in order to run tests for assessing the their content and size. From this collection, 26 representative web pages were chosen.
The experiments focused on the performance of the approach in terms of the number of node similarity computations and the time consumed to completely produce and store the similarity coefficients. For every web page tested, we noted the number of node comparisons for both the enhanced and original algorithms. Fig. 4 shows the number of saved similarity computations plotted on a logarithmic scale to illustrate the improvements for small and large web pages. One can notice the savings depicted as percentages as well, which translate to appreciable time savings, especially for large web pages.
The data in the figure reveals that our approach is faster by at least 30% for pages having more than 500 nodes, while for less than 500 nodes, we still observe savings that can add up to significant times when many pages are processed.
Fig. 5 shows the average change detection time measured after running the enhanced algorithm 10 times on each pair (modified and original) of the 26 web pages. This time includes reading the updated web page, building the hash and subtree tables, doing the comparisons and computing the similarity coefficients, and inferring the updated subtree mapping. The re-mostly consumed by subtree node comparisons. In fact, the total time it takes to clean and parse the HTML document into an of the time consumed by comparing the nodes and computing the similarity coefficients. This demonstrates the key role of the hashing mechanisms that was integrated for providing direct in-memory access to node information during the comparisons.

Related to processing time, the work in [16] describes an analysis of over 21,000 web pages that were surveyed using three methods of seed generation via search engines. The first was the Yahoo random page CGI, which redirects to a random
URL, the second used the Open Directory Project (ODP) database [22] to randomly select seed documents, while the third utilized two random English words as a query to the Google search engine and then used the top ten documents as seeds for the crawl. The average size of HTML web page was found to be 281 tags (nodes). For pages of this size, our proposed HTML web change detection approach can complete the detection process in less than one second (0.87 s on average).
Next, the detection level is studied by applying different values of a , b , and c while processing the web pages used for testing. For this, different versions of those pages were downloaded from the way back machine, and then each parameter, all changes across all types. The former case was meant to study the effect of the a , b , and c emphasis parameters on the age, which is why it drops dramatically when any of the parameters X  values is one (implying that the other two parameters non-zero valued parameter, thus giving a detection reliability very close to one third.
Several remarks can be made about the graph in Fig. 6 . The update detection is generally independent from the emphasis tance of the similarity coefficient from 1. Then one can visualize the magnitude of the changes right on the updated page using, for example, color coding or some other mechanism. 4.3. RSS feeds change detection The proposed change detection algorithm has also been applied to RSS feeds that may be present in certain web pages.
RSS (really simple syndication) is a format for delivering regularly changing web content, such as news. The most common standard for this format is RSS 2.0, which is used by more than 80% of the web sites [25] that publish dynamic content, including Microsoft, CNN, Google News, Yahoo News, and AFP. RSS content is defined in an XML file, consisting of one chan-nel per feed that includes many items which represent stories or events, and each item comprises attributes that include title, description, publication date, and so on [24] . Many websites, however, ignore other attributes that are optional, although important (like the guid ), and only supply title two attributes when applying the detection algorithm, which makes the mapping process between feeds reliable, yet more complex.

To include RSS feeds in the comparison process of two web pages, we need to define criteria for detecting changes. In [1] , two criteria to monitor news are defined, and since RSS mainly represent news and events, we will adopt the same criteria (termed event detection and event tracking) and apply our algorithm so that each RSS feed becomes a subtree represented by an RSS tag (e.g., RSS news or RSS sports) so as to avoid comparing non-compatible RSS feeds to each other. Each RSS tag encompasses a subtree consisting of the items mentioned above. The description of the item is analogous to the weight def-the comparison weight to the title and 70% to the description.

Now concerning the detection process, when an RSS tag is encountered when a page is being retrieved, the XML file is processed and then hashed in a manner similar to the web page. All items are then compared to each other using the sim-the item of the next page that corresponds to the highest similarity coefficient.

To classify the type of change, the similarity coefficient of the mapping between two items will be used as indication that will be concluded that the item was added.

Table 4 illustrates an example of a comparison between two RSS feeds retrieved from yahoo.com on September 3, 2007 at 11:30 GMT and then at 15:46 GMT. From the perspective of the first page ( P from the perspective of the updated page ( P 2 ), we deduce that items 1, 5, 7, 9, 10, 11, 18, and 19 were added. We should stress while examining the coefficient values that Table 4 only shows the titles that only make up 30% of the weight, and does not reveal the descriptions that account for 70%. 5. Discussion and conclusion
This paper described an improved web change detection approach based on restricting the similarity computations to subtree nodes having the same HTML tag, and hashing the web page for direct in-memory access to node information. A practical server application was developed to allow for scheduling web page monitoring jobs and producing reports and graphs against stored similarity data that describe processed comparisons between a page and its previous version(s). Per-formance measurements using a group of web pages selected from a pool of over 200 pages showed that the enhanced algo-rithm can perform change detection in the order of seconds for small and medium-sized web pages, and in the order of few tens of seconds for large web pages.

Concerning the limitation that was mentioned in Section 3.1 , there are three situations in which changes using our ap-proach will not be detected: the same level in the old page. that no other tag of type j now exists in the new page at level 3.

Other than the above situations, the algorithm will always detect the changes. The important question, however, is how probable are the above situations? To answer this question, we refer to the study that was reported in [8] , which analyzed the changes made to web pages by crawling over 150 million HTML pages once a week, over a span of 11 weeks. Of concern to our work are the types of changes made to the HTML markup. Out of the almost 1.5 million changes in the markup that adding or deleting tags. The tags that were affected (out of the 6%) were the h and the rest were distributed among non-popular tags ( h META i that we used for performance testing, we noticed that the probability of the h three situations occurring in practice is very low (about 38 in one million).
 For future works, the algorithm can be parallelized through multithreading, which can increase performance significantly. subtree. Given that the comparisons (for computing the similarity coefficients) are by far the most time-consuming tasks of the algorithm, multithreading could potentially divide the running time of the algorithm by M , where M is the number of threads. Another suggested future work involves the application of this algorithm to the Deep (Hidden) web, mostly concern-ing dynamic web pages that are generated in response to submitted queries. Moreover, and since in this work we have ap-they are defined in XML files just like RSS contents are.

References
