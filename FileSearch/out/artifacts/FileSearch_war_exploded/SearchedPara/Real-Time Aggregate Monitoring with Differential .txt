 Sharing real-time aggregate statistics of private data has given much benefit to the public to perform data mining for understanding im-portant phenomena, such as Influenza outbreaks and traffic conges-tion. However, releasing time-series data with standard differen-tial privacy mechanism has limited utility due to high correlation between data values. We propose FAST, an adaptive system to re-lease real-time aggregate statistics under differential privacy with improved utility. To minimize overall privacy cost, FAST adap-tively samples long time-series according to detected data dynam-ics. To improve the accuracy of data release per time stamp, filter-ing is used to predict data values at non-sampling points and to es-timate true values from noisy observations at sampling points. Our experiments with three real data sets confirm that FAST improves the accuracy of time-series release and has excellent performance even under very small privacy cost.
 H.2.8 [ DATABASE MANAGEMENT ]: Database Applications X  Data Mining ; G.3 [ PROBABILITY AND STATISTICS ]: Time series analysis Differential Privacy, Estimation, Sampling, Time Series
Sharing real-time aggregate statistics of private data enables many importatnt data mining applications. Consider examples below:
Disease Surveillance : A health care provider gathers data from individual visitors. The collected data, e.g. daily number of In-fluenza cases, is then shared with third parties, for instance, re-searchers, in order to monitor and to detect seasonal epidemic outbreaks at the earliest.

Traffic Monitoring : A GPS service provider gathers data from individual users about their locations, speeds, mobility, etc. The aggregated data, for instance, the number of users at each region during each time period, can be mined for commercial interest, such as popular places, as well as public interest, such as con-gestion patterns on the roads.
 A common scenario of such applications can be summarized by Figure 1, where a trusted server gathers data from a large number of individual subscribers. The collected data may be then aggregated and continuously shared with other un-trusted entities for various purposes. The trusted server, i.e. publisher, is assumed to be bound by contractual obligations to protect the user X  X  interests, therefore it must ensure that releasing the data does not compromise the pri-vacy of any individual who contributed data. The goal of our work is to enable the publisher to share useful aggregate statistics over individual users continuously (aggregate time series) while guaran-teeing their privacy.

The current state-of-the-art paradigm for privacy-preserving data publishing is differential privacy , which requires that the aggregate statistics reported by a data publisher be perturbed by a randomized algorithm A , so that the output of A remains roughly the same even if any single tuple in the input data is arbitrarily modified. Given the output of A , an adversary will not be able to infer much about any single tuple in the input, and thus privacy is protected.
Most existing works on differentially private data release deal with one-time release of static data [4, 7, 13, 14, 15, 16]. In the applications we consider, data values at successive timestamps are highly correlated. A straightforward application of differential pri-vacy mechanism which adds a Laplace noise to each aggregate value at each time stamp can lead to a very high overall perturba-tion error due to the composition theorem [11]. Few recent works [3, 5, 12] studied the problem of releasing time series or contin-ual statistics . Rastogi and Nath [12] proposed an algorithm which perturbs k Discrete Fourier Transform (DFT) coefficients of the en-tire time series and reconstructs a released version from the Inverse DFT. Since the entire time-series is required to perform those op-erations, it is not applicable to real-time applications. We included this method in the empirical study for utility comparison. Dwork et al. [5] proposed a differentially private continual counter over a binary stream with a bounded error at each time step. Chan et al. [3] studied the same problem and concluded with a similar upper bound. However, both works adopt an event-level privacy model, with the perturbation mechanism designed to protect the presence of an individual event, i.e. a user X  X  contribution to the data stream at a single time point, rather than the presence or privacy of a user.
In this paper, we propose FAST, a novel approach with Filtering and Adaptive Sampling for releasing Time series under differential privacy. It uses sampling to query and perturb selected values in the time series with the differential privacy mechanism, and simultane-ously uses filtering to dynamically predict the non-sampled values and correct the sampled values. To improve the accuracy of data re-lease at each time stamp, we propose a state space model and use of the Kalman filter [8].To minimize the overall privacy cost, hence, the overall perturbation error, we propose an adaptive sampling al-gorithm with PID control. We study the accuracy and robustness of FAST with real world time-series data sets, which confirms that FAST provides accurate results in real-time and stability despite different data dynamics.

The rest of the paper is organized as follows: Section 2 provides the problem formulation and the background for differential pri-vacy. Section 3 presents an overview of our solution, as well as the technical details of filtering and adaptive sampling. Section 4 presents a set of empirical results. Section 5 concludes the paper and states possible directions for future work. Formally a time series is defined as follows:
Definition 1. [Aggregate Time Series] A univariate, discrete time series X = { x k } is a set of values of a variable x observed at dis-crete time k , with 0  X  k &lt; T , where T is the length of the series.
In our example applications, X is an aggregate count series, such as, the daily number of patients diagnosed of Influenza, or the hourly count of drivers passing by a gas station. This assumption will hold true for the rest of the paper.

We measure the quality of a released time series R = { r k average relative error E : where  X  is a user-specified constant (also referred to as sanitary bound in [13]) to mitigate the effect of excessively small query re-sults. Here we set  X  = 1 throughout the entire time-series.
Clearly, the quality of a published series increases as each r approaches x k , the extreme case of which would have r k for each k . However, a privacy-preserving algorithm is likely to perturb original data values in order to protect individual privacy. Thus, a publishing mechanism that guarantees user privacy and yields high utility is desired.
A mechanism is differentially private if its outcome is not sig-nificantly affected by the removal or addition of a single user. An adversary thus learns approximately the same information about any individual user, irrespective of his/her presence or absence in the original database.

Definition 2. [Differential Privacy [2]] A non-interactive privacy mechanism A gives  X  -differential privacy if for any dataset D D 2 differing on at most one record, and for any possible anonymized dataset e D  X  Range ( A ) , where the probability is taken over the randomness of A . The privacy parameter  X  , also called the privacy budget [11], spec-ifies the degree of privacy offered. Intuitively, the lower value of  X  implies stronger privacy guarantee and a higher value implies a weaker guarantee while possibly achieving higher accuracy. Laplace Mechanism. Dwork et al. [4] show that  X  -differential privacy can be achieved by adding i.i.d. noise to each query result:
The magnitude of the noise added conforms to a Laplace distri-bution with the probability density function p ( x |  X  ) = with  X  = GS ( q ) / X  , where GS ( q ) denotes the global sensitiv-ity [4] of a query q which is defined as the maximum difference between the query results from any two neighboring databases. In our example, GS ( count ) = 1 .
 Composition. The composition properties of differential privacy provide privacy guarantees for a sequence of computations.
T HEOREM 1. [Sequential Composition [11]] Let A i each pro-vide  X  i -differential privacy. A sequence of A i ( D ) over the dataset D provides ( P i  X  i )-differential privacy.
 Given the composition theorem, a baseline solution to sharing dif-ferentially private time series can be derived: Laplace perturbation is applied at every time stamp to guarantee  X /T -differential pri-vacy, where T is the length of entire series.
We propose FAST: a novel solution to sharing time-series data with differential privacy. It allows for fully automated adaptation to changing data dynamics and highly accurate time-series predic-tion/estimation. Figure 2 shows the framework of FAST.

We outline the workflow of FAST below. For detailed algorithm, we refer the readers to our full study [6].  X  For each time stamp k , the adaptive sampling component de-termines whether to sample/query the input time-series or not.  X  If k is a sampling point, the data value at k is perturbed by the
Laplace mechanism to guarantee  X  0 -differential privacy.  X  The filtering component produces a prediction of data value based on an internal state model at every time stamp. The predic-tion, i.e. prior estimate, is released to output at a non-sampling point, while a posterior estimate, i.e. correction of the noisy ob-servation and prediction, is released at a sampling point.  X  The error between the prior and the posterior is then fed through the adaptive sampling component to adjust the sampling rate.
Once the user-specified privacy budget  X  is used up, the system will stop sampling the input series. T HEOREM 2. FAST satisfies  X  -differential privacy.

P ROOF . Suppose the maximum number of samples FAST allows for a time series is M . By setting  X  0 =  X /M , the above algorithm satisfies  X  -differential privacy according to Theorem 1.
There are two types of error which we would like to balance in our solution: perturbation error by Laplace perturbation mecha-nism at sampling points and prediction error by the filtering predic-tion procedure at non-sampling points. The more we sample, the more perturbation error we introduce, while the prediction error might be reduced due to more available feedback, and vice versa. Our goal is to balance the trade-off between the two types of error by adaptively adjusting the sampling rate.
Each released data value is an estimate derived by the filtering component in FAST. To improve the accuracy of each released value, we established a state space model for time series data and proposed to use the Kalman filter for estimation.
 Process Model. Let x k denote the internal state (true value) of a process at time stamp k . The states at consecutive time stamps can be modeled by the following equations: This constant process model indicates that adjacent data values from the original time-series should be consistent except for a white Gaussian noise  X  with variance Q .
 Measurement Model. The noisy observation, which is perturbed data from the Laplace mechanism, can be modeled by: The measurement noise  X  is a Laplace noise which follows p (  X  )  X  Lap (0 , X  ) where  X  is determined by differential privacy mecha-nism. In this paper, we approximate the Laplace noise with a white Gaussian error with variance R and define the distribution of  X  as
We have also studied in [6] several more complex filtering tech-niques, such as Masreliez filter [10] and particle filters [1], with more precise modeling of the Laplace noise. The results showed that the Kalman filter with the Gaussian measurement noise is suf-ficient in utility and computationally efficient. For brevity, we only present the Kalman filter model in this paper.
 Prior and Posterior Estimates. If noisy observation is available at every time stamp,the Kalman filter will repeat a pair of operations: Prediction and Correction , for every k . Figure 3 gives a high-level diagram of the two procedures specific to our state space model.
At time k , the apriori state estimate  X  x  X  k is made based on the process model in Equation (4) and is related to the aposteriori state estimate of last step, while the aposteriori state estimate  X  x on a linear combination of  X  x  X  k and the noisy measurement z
The value K k , called Kalman Gain , is adjusted with each mea-surement in order to minimize the error variance P k of the posterior estimate  X  x k . We refer interested readers to [6] for a complete set of definitions and detailed derivation.

One advantage of the Kalman filter is that it maintains and up-dates the best estimate of the internal state by properly weighing and combining all available data (prior estimate and noisy observa-tion) to form an educated guess. Another advantage is the compu-tation efficiency which is crucial to real-time applications, as only O (1) computations are required for each time stamp from Figure 3.
When combined with sampling in the overall solution, a noisy observation, which is needed by the Correction step, may not be available at every time stamp. Therefore, we propose to release the prior estimate at non-sampling points instead. We will evaluate the estimation accuracy in the experiment section.
Since each noisy observation from Laplace mechanism comes with a cost (privacy budget spent), we are motivated to sample data values through the differential privacy interface only when needed in our overall solution. Below we briefly introduce two sampling strategies and detailed algorithms can be found in [6].
 Fixed Rate Sampling. Given a pre-defined interval I , the fixed-rate algorithm samples the time series periodically and releases the posterior estimate per I timestamps. As for the time points between two adjacent samples, a predicted value is released. Privacy budget (  X I ) /T will be spent on each sample to guarantee  X  -differential privacy according to Theorem 2.
 The challenge of fixed-rate sampling is to define the interval I . Increasing the sampling rate, i.e. when I is low, an extreme case of which is to issue a query at each time step as in the baseline solution, the overall perturbation error will grow with the number of samples. On the other hand, when we decrease the sampling rate, i.e. when I is high, the perturbation at each sampling point will drop, but the published series will not reflect up-to-date data values, resulting large prediction error. Apriori knowledge of the data is required to find the optimal sampling rate in order to minimize the average relative error. However, that is impractical for a real-time publishing scenario.
 Adaptive Sampling. With no apriori knowledge of the time series, it is desirable to detect data dynamics and to adjust the sampling rate on-the-fly. Figure 4 illustrates the idea of adaptive sampling. We plot the original time-series, traffic count , as well as the number of queries (samples) issued by the adaptive sampling mechanism during each corresponding time unit. As is shown, the adaptive sampling mechanism increases sampling rate between day 20 and day 100, when the traffic count exhibits significant fluctuations, and decreases sampling rate beyond day 100, when there X  X  little varia-tion among data values.

We implement adaptive sampling in FAST with feedback con-trol. It is based on the model error between the posterior and the prior estimates. At time step k n ( 0  X  k n &lt; T ), where the sub-script indicates the n -th sampling point ( 0  X  n &lt; M ), we define this error E k n as follows: Note that no error is defined at a non-sampling point.

The model error measures how well the internal state model de-scribes the current data dynamics, assuming  X  x k n is close to the true value. Since  X  x  X  k that data is going through rapid changes if the error E k with time. In response, the controller in our system will detect the error and increase the sampling rate accordingly.

FAST adopts a PID controller, the most common form of feed-back control [9], to measure the performance of sampling over time. We re-define the three PID components, Proportional, Inte-gral , and Derivative , with the model error defined in Equation (10). The full PID algorithm is thus Control gains C p , C i , and C d denote how much each of the pro-portional , integral , and derivative counts for the final calibrated PID error and T i represents the integral time. The control gains are constrained by:
Given the PID error  X  , a new sampling interval I 0 can be deter-mined: where  X  and  X  are pre-determined parameters. Intuitively, the sam-pling interval increases as the  X  error drops, and vice versa. We have implemented FAST in Java with JSC (Java Statistical Classes 1 ) for simulating the Laplace distribution. Our study has been conducted with three real time-series data sets:  X  Flu is the weekly surveillance data of Influenza-like illness pro-vided by the Influenza Division of the Centers for Disease Con-trol and Prevention 2 . We collected the weekly outpatient count of the age group [5-24] from 2006 to 2010. This time-series con-sists of 209 data points.  X  Traffic is a daily traffic count data set for Seattle-area highway traffic monitoring and control provided by the Intelligent Trans-portation Systems Research Program at University of Washing-ton 3 . We chose the traffic count at location I-5 143.62 south-bound from April 2003 till October 2004. This time-series con-sists of 540 data points.  X  Unemployment is the monthly unemployment level of black or African American women of age group [16-19] from ST. Louis Federal Reserve Bank 4 . This data set contains observations from
January 1972 to October 2011 with 478 data points. http://www.jsc.nildram.co.uk http://www.cdc.gov/flu/ http://www.its.washington.edu/ http://research.stlouisfed.org/ (a) Estimation Comparison
Figure 5: Estimation and Sampling Methods with Traffic Data Set Figure 6: Impact of Noise Parameters with Unemployment Data Set
The default parameter settings are as follows: the overall pri-vacy budget  X  = 0 . 01 , the estimation noise variances ( Q,R ) = the interval adaptation parameters (  X , X  ) = (10 , 1000) . We will describe how to choose default values and show the impact of indi-vidual parameters later in this section.
 Accuracy of Filtering and Adaptive Sampling. We first evaluate the accuracy of the Kalman filter estimation and adaptive sampling against alternative methods. Figure 5 shows the results with traffic data set, while the other data sets exhibit similar trends. We com-pare the Kalman filter posterior estimation against linear regres-sion (fitted with 2 data points) and the baseline Laplace perturba-tion algorithm mentioned in Section 2. As seen in Figure 5(a), the Kalman filter outperforms linear predictor as well as Laplace per-turbation algorithm especially when given small privacy budgets(  X  = 0 . 0001 , 0 . 001 , 0 . 01 ). With large budget (  X  = 1 ), which we note does not provide sufficient privacy protection, we observed no sub-stantial advantage of using the Kalman filter, which can be ex-plained by the nature of the aposteriori estimate defined by Equa-tion (9): it only partially relies on the noisy measurement z does not fully reflect reduced perturbation error. As for sampling strategies, we plot the utility comparison in Figure 5(b), with the interval for fixed-rate sampling varying from 1 to 10 . We found the adaptive sampling strategy is comparable to the optimal fixed-rate with no need of a priori knowledge. Thus we believe that adaptive sampling can be adopted by a wider range of applications. Effects of Kalman Filter Parameters. To understand the impact of process noise variance Q and measurement noise variance R , we vary their values independently and plot the estimation accu-racy. The results with the unemployment data set are presented in Figure 6, while the other data sets show similar trends. Given R fixed, we observe that the accuracy of estimation drops as Q in-creases; given Q , the accuracy rises as R increase. This can be interpreted by the definition of the Kalman gain K k in Figure 3: K k increases as Q does, resulting the aposteriori estimate favoring the noisy observation. Similarly, when increasing R , the Kalman gain decreases, resulting the aposteriori estimate favoring the state prediction. Both results confirm that it X  X  beneficial to rely more on the state prior than the noisy measurement when privacy budget is small, which implies larger noise in observed values. Effects of Control Parameters. We study the impact of control gains and find that as long as their values are chosen according to the common practice: proportional &gt; integral &gt; derivative , the resulting relative error does not differ beside the randomness introduced by the Laplace mechanism. Therefore we conclude that there X  X  no extra  X  X ule of thumb X  than the common practice for tun-ing the control gains in our system.

We also study the impact of T i ,  X  , and  X  on the accuracy of the released series. Varying the value of any of them does not result in substantial change in relative error. They are considered not influ-ential and thus can be set by users as needed. Detailed figures are excluded for brevity.
 FAST vs. Alternative Approaches. Figure 7 presents the perfor-mance of FAST against the baseline Laplace mechanism (which adds a Laplace noise to each aggregate value at each time stamp) and the DFT [12] algorithm (mentioned in Section 1) with respect to different scales of privacy budget. The number of DFT coef-ficients to preserve is set to be 20, which is near optimal accord-ing to [12]. Again, our adaptive approach shows superior perfor-mance when the privacy budget  X  is limited. This confirms our hypothesis that with accurate estimate by the Kalman filter, the PID control mechanism can adjust the sampling rate as needed, thus improving the overall utility of the published series. Note that when  X  is high and approaching 1, the baseline Laplace pertur-bation algorithm achieves smaller relative error because of the re-duced perturbation error. Since the reconstruction error of the DFT approach and the prediction error of our adaptive approach both outweighs the perturbation error in this case, their released series contain larger relative error. However, a privacy budget greater than 1 does not provide sufficient privacy protection any more. We find that our adaptive approach outperforms the alternative methods un-der strong privacy guarantee.
We have proposed FAST, an adaptive approach with filtering and sampling for monitoring real-time aggregate under differential pri-vacy. The key innovation is that our approach utilizes feedback loops based on observed (perturbed) values to dynamically adjust the prediction/estimation model as well as the sampling rate. To minimize the overall privacy cost, FAST uses the PID controller to adaptively sample long time-series according to detected data dy-namics. As to improve the accuracy of data release per time stamp, the Kalman filter is used to predict data values at non-sampling points and to estimate true values from perturbed values at sam-pling points. Our experiments with three real data sets show that it is beneficial to incorporate feedback into both the estimation model and the sampling process. The results confirmed that our adaptive approach improves utility of time-series release and has excellent performance even under very small privacy cost.

As for the future, we plan to expand our solution to enable moni-toring of differentially private spatial-temporal statistics, for exam-ple, real-time traffic conditions at all intersections of a city. This research was supported in part by NSF grant CNS-1117763, AFOSR grant #12RSE136, and an Emory URC grant.
