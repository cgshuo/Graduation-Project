 Machine learning in domains such as bioinformatics, drug discovery, and web data mining involves the study of relationships between objects. Graphs are natural data structures to model such rela-tions, with nodes representing objects and edges the relationships between them. In this context, one often encounters the question: How similar are two graphs? Simple ways of comparing graphs which are based on pairwise comparison of nodes or edges, are possible in quadratic time, yet may neglect information represented by the structure of the graph. take the structure of the graph into account. They work by counting the number of common random walks between two graphs. Even though the number of common random walks could potentially be exponential, polynomial time algorithms exist for computing these kernels. Unfortunately for the where n is the number of vertices in the input graphs. This severely limits their applicability to large-scale problems, as commonly found in areas such as bioinformatics.
 In this paper, we extend common concepts from linear algebra to Reproducing Kernel Hilbert Spaces (RKHS), and use these extensions to define a unifying framework for random walk kernels. We show that computing many random walk graph kernels including those of G  X  artner et al. [1] and Kashima et al. [2] can be reduced to the problem of solving a large linear system, which can then be solved efficiently by a variety of methods which exploit the structure of the problem. Let  X  : X  X  H denote the feature map from an input space X to the RKHS H associated with the extend  X  to matrix arguments by defining  X  : X n  X  m  X  H n  X  m via [ X ( X )] ij :=  X  ( X ij ) . We can now borrow concepts from tensor calculus to extend certain linear algebra operations to H : Definition 1 Let A  X  X  n  X  m , B  X  X  m  X  p , and C  X  R m  X  p . The matrix products  X ( A ) X ( B )  X  R n  X  p and  X ( A ) C  X  X  n  X  p are Given A  X  R n  X  m and B  X  R p  X  q the Kronecker product A  X  B  X  R np  X  mq and vec operator are defined as where A  X  j denotes the j -th column of A . They are linked by the well-known property: Definition 2 Let A  X  X  n  X  m and B  X  X  p  X  q . The Kronecker product  X ( A )  X   X ( B )  X  R np  X  mq is It is easily shown that the above extensions to RKHS obey an analogue of (2): Lemma 1 If A  X  X  n  X  m , B  X  R m  X  p , and C  X  X  p  X  q , then If p = q = n = m , direct computation of the right hand side of (4) requires O ( n 4 ) kernel evalua-tions. For an arbitrary kernel the left hand side also requires a similar effort. But, if the RKHS H is puted in O ( n 3 r ) operations. Our efficient computation schemes described in Section 4 will exploit this observation. Random walk kernels on graphs are based on a simple idea: Given a pair of graphs perform a random walk on both of them and count the number of matching walks [1, 2, 3]. These kernels mainly differ in the way the similarity between random walks is computed. For instance, G  X  artner et al. [1] count the number of nodes in the random walk which have the same label. They also include a decay factor to ensure convergence. Kashima et al. [2], and Borgwardt et al. [3] on the other hand, use a kernel defined on nodes and edges in order to compute similarity between random walks, and define an initial probability distribution over nodes in order to ensure convergence. In this section we present a unifying framework which includes the above mentioned kernels as special cases. 3.1 Notation identity matrix. When it is clear from context we will not mention the dimensions of these vectors and matrices.
 connected by an edge. G is said to be undirected if ( v i ,v j )  X  E  X  X  X  ( v j ,v i )  X  E for all edges. 0 otherwise. If G is weighted then P can contain non-negative entries other than zeros and ones, i.e., P ij  X  (0 ,  X  ) if ( v i ,v j )  X  E and zero otherwise.
 Let D be an n  X  n diagonal matrix with entries D ii = P j P ij . The matrix A := PD  X  1 is then called the normalized adjacency matrix, or simply adjacency matrix. A walk w on G is a sequence to the number of edges encountered during the walk (here: t ). A graph is said to be connected if any two pairs of vertices can be connected by a walk; here we always work with connected graphs. A w define random walk kernels on graphs.
 Let X be a set of labels which includes the special label . Every edge labeled graph G is associated which are present in the graph get a non-label. Let H be the RKHS endowed with the kernel  X  : X  X X  X  R , and let  X  : X  X  H denote the corresponding feature map which maps to the zero element of H . We use  X ( L ) to denote the feature matrix of G . For ease of exposition we do not consider labels on vertices here, though our results hold for that case as well. Henceforth we use the term labeled graph to denote an edge-labeled graph. 3.2 Product Graphs vertices, each representing a pair of vertices from G and G 0 , respectively. An edge exists in E  X  iff the corresponding vertices are adjacent in both G and G 0 . Thus graph G  X  is A  X  = A  X  A 0 . An edge exists in the product graph iff an edge exits in both G and G , therefore performing a simultaneous random walk on G and G 0 is equivalent to performing a random walk on the product graph [4].
 Let p and p 0 denote initial probability distributions over vertices of G and G 0 . Then the initial probability distribution p  X  of the product graph is p  X  := p  X  p 0 . Likewise, if q and q 0 denote stopping probabilities ( i.e., the probability that a random walk ends at a given vertex), the stopping probability q  X  of the product graph is q  X  := q  X  q 0 .
 If G and G 0 are edge-labeled, we can associate a weight matrix W  X   X  R nn 0  X  nn 0 with G  X  , using our Kronecker product in RKHS (Definition 2): W  X  =  X ( L )  X   X ( L 0 ) . As a consequence of the in the product graph. The weight matrix is closely related to the adjacency matrix: assume that adjacency matrix of the product graph.
 To extend the above discussion, assume that H = R d endowed with the usual dot product, and that its value between any two edges is one iff the labels on the edges match, and zero otherwise. The weight matrix W  X  has a non-zero entry iff an edge exists in the product graph and the corresponding edges in G and G 0 have the same label. Let l A denote the adjacency matrix of the graph filtered by of brevity) shows that the weight matrix of the product graph can be written as 3.3 Kernel Definition Performing a random walk on the product graph G  X  is equivalent to performing a simultaneous the probability of simultaneous k length random walks on G (starting from vertex v i and ending in simultaneous k length random walks on G and G 0 measured via the kernel function  X  . Given the weight matrix W  X  , initial and stopping probability distributions p  X  and q  X  , and an ap-propriately chosen discrete measure  X  , we can define a random walk kernel on G and G 0 as In order to show that (8) is a valid Mercer kernel we need the following technical lemma. Lemma 2  X  k  X  N 0 : W k  X  p  X  = vec[ X ( L 0 ) k p 0 ( X ( L ) k p ) &gt; ] .
 Proof By induction over k . Base case: k = 0 . Since  X ( L 0 ) 0 =  X ( L ) 0 = I , using (2) we can write Induction from k to k + 1 : Using Lemma 1 we obtain Lemma 3 If the measure  X  ( k ) is such that (8) converges, then it defines a valid Mercer kernel. Proof Using Lemmas 1 and 2 we can write kernel. The lemma follows since a convex combination of kernels is itself a valid kernel. 3.4 Special Cases A popular choice to ensure convergence of (8) is to assume  X  ( k ) =  X  k for some  X  &gt; 0 . If  X  is sufficiently small 1 then (8) is well defined, and we can write Kashima et al. [2] use marginalization and probabilities of random walks to define kernels on graphs. Given transition probability matrices P and P 0 associated with graphs G and G 0 respectively, their kernel can be written as (see Eq. 1.19, [2]) where T  X  := (vec( P ) vec( P 0 ) &gt; ) ( X ( L )  X   X ( L 0 )) , using to denote element-wise (Hadamard) G  X  artner et al. [1] use the adjacency matrix of the product graph to define the so-called geometric kernel To recover their kernel in our framework, assume an uniform distribution over the vertices of G and G , i.e., set p = q = 1 /n and p 0 = q 0 = 1 /n 0 . The initial as well as final probability distribution and W  X  = A  X  , we can rewrite (8) to obtain which recovers (11) to within a constant factor. In this section we show that iterative methods, including those based on Sylvester equations, conju-gate gradients, and fixed-point iterations, can be used to greatly speed up the computation of (9). 4.1 Sylvester Equation Methods Consider the following equation, commonly known as the Sylvester or Lyapunov equation: Here, S,T,X 0  X  R n  X  n are given and we need for solve for X  X  R n  X  n . These equations can be readily solved in O ( n 3 ) time with freely available code [5], e.g. Matlab X  X  dlyap method. The generalized Sylvester equation can also be solved efficiently, albeit at a slightly higher computational cost of O ( dn 3 ) . We now show that if the weight matrix W  X  can be written as (7) then the problem of computing the graph kernel (9) can be reduced to the problem of solving the following Sylvester equation: where vec( X 0 ) = p  X  . We begin by flattening the above equation: Using Lemma 1 we can rewrite (15) as ( I  X   X  X use (7), and solve for vec( X ) : vec( X ) = ( I  X   X W  X  ) Multiplying both sides by q &gt;  X  yields q &gt;  X  vec( X ) = q &gt;  X  ( I  X   X W  X  )  X  1 p  X  . (18) The right-hand side of (18) is the graph kernel (9). Given the solution X of the Sylvester equation Sylvester equation takes O ( dn 3 ) time, computing the graph kernel in this fashion is significantly faster than the O ( n 6 ) time required by the direct approach.
 Where the number of labels d is large, the computational cost may be reduced further by computing matrices S and T such that W  X   X  S  X  T . We then simply solve the simple Sylvester equation (12) involving these matrices. Finding the nearest Kronecker product approximating a matrix such as W  X  is a well-studied problem in numerical linear algebra and efficient algorithms which exploit sparsity of W  X  are readily available [6]. 4.2 Conjugate Gradient Methods Given a matrix M and a vector b , conjugate gradient (CG) methods solve the system of equations Mx = b efficiently [7]. While they are designed for symmetric positive semi-definite matrices, CG solvers can also be used to solve other linear systems efficiently. They are particularly efficient if the matrix is rank deficient, or has a small effective rank , i.e., number of distinct eigenvalues. Furthermore, if computing matrix-vector products is cheap  X  because M is sparse, for instance  X  the CG solver can be sped up significantly [7]. Specifically, if computing Mv for an arbitrary vector v requires O ( k ) time, and the effective rank of the matrix is m , then a CG solver requires only O ( mk ) time to solve Mx = b .
 The graph kernel (9) can be computed by a two-step procedure: First we solve the linear system that if G and G 0 contain n vertices each then W  X  is a n 2  X  n 2 matrix. Directly computing the matrix-vector product W  X  r , requires O ( n 4 ) time. Key to our speed-ups is the ability to exploit Lemma 1 to compute this matrix-vector product more efficiently: Recall that W  X  =  X ( L )  X   X ( L 0 ) . Letting r = vec( R ) , we can use Lemma 1 to write 4.3 Fixed-Point Iterations Fixed-point methods begin by rewriting (19) as x = p  X  +  X W  X  x. (21) the value of x at iteration t , we set x 0 := p  X  , then compute tolerance. This is guaranteed to converge if all eigenvalues of  X W  X  lie inside the unit disk; this can be ensured by setting  X  &lt; 1 / X  max , where  X  max is the largest-magnitude eigenvalue of W  X  . The above is closely related to the power method used to compute the largest eigenvalue of a matrix [8]; efficient preconditioners can also be used to speed up convergence [8]. Since each iteration of (22) involves computation of the matrix-vector product W  X  x t , all speed-ups for computing the matrix-vector product discussed in Section 4.2 are applicable here. In particular, we exploit the fact that W  X  is a sum of Kronecker products to reduce the worst-case time complexity to O ( n 3 ) in our experiments, in contrast to Kashima et al. [2] who computed the matrix-vector product explicitly. To assess the practical impact of our algorithmic improvements, we compared our techniques from Section 4 with G  X  artner et al. X  X  [1] direct approach as a baseline. All code was written in MATLAB Release 14, and experiments run on a 2.6 GHz Intel Pentium 4 PC with 2 GB of main memory running Suse Linux. The Matlab function dlyap was used to solve the Sylvester equation. By default, we used a value of  X  = 0 . 001 , and set the tolerance for both CG solver and fixed-point iteration to 10  X  6 for all our experiments. We used Lemma 1 to speed up matrix-vector multiplication for both CG and fixed-point methods ( cf. Section 4.2). Since all our methods are exact and produce the same kernel values (to numerical precision), we only report their runtimes below. We tested the practical feasibility of the presented techniques on four real-world datasets whose size mandates fast graph kernel computation; two datasets of molecular compounds (MUTAG and PTC), and two datasets with hundreds of graphs describing protein tertiary structure (Protein and Enzyme). Graph kernels provide useful measures of similarity for all these graphs; please refer to the addendum for more details on these datasets, and applications for graph kernels on them. Figure 1: Time (in seconds on a log-scale) to compute 100  X  100 kernel matrix for unlabeled (left) resp. labelled (right) graphs from several datasets. Compare the conventional direct method (black) to our fast Sylvester equation, conjugate gradient (CG), and fixed-point iteration (FP) approaches. 5.1 Unlabeled Graphs In a first series of experiments, we compared graph topology only on our 4 datasets, i.e., without considering node and edge labels. We report the time taken to compute the full graph kernel matrix for various sizes (number of graphs) in Table 1 and show the results for computing a 100  X  100 sub-matrix in Figure 1 (left).
 On unlabeled graphs, conjugate gradient and fixed-point iteration  X  sped up via our Lemma 1  X  are consistently about two orders of magnitude faster than the conventional direct method. The Sylvester approach is very competitive on smaller graphs (outperforming CG on MUTAG) but slows down with increasing number of nodes per graph; this is because we were unable to incorporate Lemma 1 into Matlab X  X  black-box dlyap solver. Even so, the Sylvester approach still greatly outperforms the direct method. 5.2 Labeled Graphs In a second series of experiments, we compared graphs with node and edge labels. On our two protein datasets we employed a linear kernel to measure similarity between edge labels representing distances (in  X  angstr  X  oms) between secondary structure elements. On our two chemical datasets we used a delta kernel to compare edge labels reflecting types of bonds in molecules. We report results in Table 2 and Figure 1 (right).
 On labeled graphs, our three methods outperform the direct approach by about a factor of 1000 when using the linear kernel. In the experiments with the delta kernel, conjugate gradient and fixed-point iteration are still at least two orders of magnitude faster. Since we did not have access to a generalized Sylvester equation (13) solver, we had to use a Kronecker product approximation [6] which dramatically slowed down the Sylvester equation approach.
 Table 1: Time to compute kernel matrix for given number of unlabeled graphs from various datasets.
Table 2: Time to compute kernel matrix for given number of labeled graphs from various datasets. We have shown that computing random walk graph kernels is essentially equivalent to solving a large linear system. We have extended a well-known identity for Kronecker products which allows us to exploit the structure inherent in this problem. From this we have derived three efficient techniques to solve the linear system, employing either Sylvester equations, conjugate gradients, or fixed-point iterations. Experiments on real-world datasets have shown our methods to be scalable and fast, in some instances outperforming the conventional approach by more than three orders of magnitude. Even though the Sylvester equation method has a worst-case complexity of O ( n 3 ) , the conjugate gradient and fixed-point methods tend to be faster on all our datasets. This is because computing matrix-vector products via Lemma 1 is quite efficient when the graphs are sparse, so that the feature matrices  X ( L ) and  X ( L 0 ) contain only O ( n ) non-entries. Matlab X  X  black-box dlyap solver is unable to exploit this sparsity; we are working on more capable alternatives. An efficient generalized Sylvester solver requires extensive use of tensor calculus and is part of ongoing work. As more and more graph-structured data becomes available in areas such as biology, web data min-ing, etc. , graph classification will gain importance over coming years. Hence there is a pressing need to speed up the computation of similarity metrics on graphs. We have shown that sparsity, low effective rank, and Kronecker product structure can be exploited to greatly reduce the computational cost of graph kernels; taking advantage of other forms of structure in W  X  remains a challenge. Now that the computation of random walk graph kernels is viable for practical problem sizes, it will open the doors for their application in hitherto unexplored domains. The algorithmic challenge now is how to integrate higher-order structures, such as spanning trees, in graph comparisons. Acknowledgments
