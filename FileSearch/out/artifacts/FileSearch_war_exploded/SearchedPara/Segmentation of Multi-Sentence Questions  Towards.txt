 Existing question retrieval models work relatively well in finding similar questions in community-b ased question answering (cQA) services. However, they are designed for single-sentence queries or bag-of-word representations, and are not sufficient to handle multi-sentence questions compleme nted with various contexts. Segmenting questions into parts th at are topically related could assist the retrieval system to not only better understand the user X  X  different information needs but al so fetch the most appropriate fragments of questions and answers in cQA archive that are relevant to user X  X  query. In this paper, we propose a graph based approach to segmenting multi-sentence questions. The results from user studies show that our segmentation model outperforms traditional systems in question segmentation by over 30% in user X  X  satisfaction. We incorporate the segmentation model into existing cQA question retrieval framework for more targeted question matching, and the em pirical evaluation results demonstrate that the segmentati on boosts the question retrieval performance by up to 12.93% in Mean Average Precision and 11.72% in Top One Precision. Our model comes with a comprehensive question detector equipped with both lexical and syntactic features. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Retrieval Models ; I.2.7 [ Artificial Intelligence ]: Natural Language Processing  X  Text Analysis Algorithms, Design, Experimentation Question Answering, Question Se gmentation, Question Matching, Yahoo! Answers 
Community-based Question Answering (cQA) services begin to emerge with the blooming of Web 2.0. They bring together a network of self-declared  X  X xperts X  to answer questions posted by other people. Examples of these services include Yahoo! Answers (answers.yahoo.com) and Baidu Zh idao (zhidao.baidu.com) etc. Over times, a treme ndous amount of historical QA pairs have been built up in their databases, and this transformation gives information seekers a great alternative to web search [2,18,19]. Instead of looking through a list of potentially relevant documents from the Web, users may directly search for relevant historical questions from cQA archives. As a result, the corresponding best answer could be explicitly extracted and returned. In view of the above, traditional information retrieval tasks like TREC [1] QA are transformed to similar que stion matching tasks [18,19]. 
There has been a host of work on question retrieval. The state-of-the-art retrieval systems empl oy different models to perform the search, including vector space model [5], language model [5,7], Okapi model [7], translation model [7,14,19] and the recently proposed syntactic tree matching model [18]. Although the experimental studies in thes e works show that the proposed models are capable of improving question retrieval performance, they are not well designed to handle questions in the form of multiple sub-questions complement ed with sentences elaborating the context of the sub-questions. This limitation could be further viewed from two aspects. From th e viewpoint of user query, the input to most existing models is simply a bag of keywords [5,19] or a single-sentence question [18]. It leads to a bottleneck in understanding the user X  X  different information needs when the user query is represented in a complex form with many sub-questions. From the viewpoint of the archived questions, none of the existing work attempts to distinguish context sentences from question sentences, or tries to segment the archived question thread into parts that are topically based. It prevents the system from presenting the user the most appropriate fragments that are relevant to his/her queries. 
Figure 1 illustrates an example of a question thread extracted from Yahoo! Answers. There are three sub-questions (Q 1 , Q Q ) asked in this thread, all in different aspects. If a user posts such example as a query, it is hard for existing retrieval systems to find all matches for the three sub-questions if the query is not well segmented. On the other hand, if a new similar query such as  X  what are the requirements of being a dentist?  X  is posted, it is also difficult for existing retrieval systems to return Q match if Q 3 is not explicitly separated from its surrounding sub-questions and contexts. Given all these constraints, it is thus highly valuable and desirable to topically segment multi-sentence questions, and to properly align individual sub-questions with their context sentences. Good segmentation not only helps the question retrieval system to better analyze the user X  X  complex information needs, but also assists it in matching the query with the most appropriate portions of th e questions in the cQA archive. Figure 1: Example of multi-sentence questions extracted from 
It appears to be natural to exploit traditional text-based segmentation techniques to segment multi-sentence questions. Existing approaches to text segment boundary detection include similarity based method [3], gr aph based method [13], lexical chain based method [10], text tiling algorithm [6] and the topic change detection method [12] et c. Although experimental results of these segmentation techniques are shown to be encouraging, they mainly focus on general text relations and are incapable of modeling the relationships betw een questions and contexts. A question thread from cQA usually comes with multiple sub-questions and contexts, and it is desirable for one sub-question to be isolated from other sub-questi ons while closely linked to its context sentences. 
After extensive study of the characteristics of questions in cQA archive, we introduce in this paper a new graph based approach to segment multi-sentence questions. The basic idea is outlined as follows. We first attempt to de tect question sentences using a classifier built from both lexical and syntactic features, and use similarity and co-reference chai n based methods to measure the closeness score between the question and context sentences. We model their relationships to form a graph, and use the graph to propagate the closeness scores. The closeness scores are finally utilized to group topically related question and context sentences. 
The contributions of this paper are threefold: First, we build a question detector on top of both lexical and syntactic features. Second, we propose an unsupervised graph based approach for multi-sentence segmentation. Finally, we introduce a novel retrieval framework incorpora ting question segmentation for better question retrieval in cQA archives. 
The rest of the paper is organized as follows: Section 2 presents the proposed technique for questi on sentence detection. Section 3 describes the detailed algorithm and architecture for multi-sentence segmentation, together with the new segmentation aided retrieval framework. Section 4 pres ents our experimental results. Section 5 reviews some related works and Section 6 concludes this paper with directions for future work. 
Human generated content on the Web are usually informal, and it is not uncommon that standard features such as question mark or utterance are absent in cQA questions. For example, question mark might be used in cases other than questions ( e.g. denoting uncertainty), or could be overlooke d after a question. Therefore, traditional methods using certain heuristics or hand-crafted rules become inadequate to cope with various online question forms. To overcome these obstacles, we propose an automated approach to extracting salient sequentia l and syntactic patterns from question sentences, and use these patterns as features to build a question detector. Research on se quential patterns has been well discussed in many literatures, including the identification of comparative sentences [9], the detection of erroneous sentences [17] and question sentences [4]. However, works on syntactic patterns have only been partia lly explored [17,18]. Grounded on these previous works, we next e xplain our pattern mining process, together with the learning algorithm for the classification model. Sequential Pattern is also referred to as Labeled Sequential Pattern ( LSP ) in the literatures. It is in the form of C S  X  , where sequence S is classified to. In the problem of question detection, a sequence is defined to be a seri es of tokens from sentences, and the class is in the binary form of { Q, NQ } ( resp . question and non-question). The purpose of sequential pattern mining is to extract a set of frequent subsequence of words that are indicative of questions. For example, the word sequence  X  anyone know what ... to  X  is a good indication to characterize the question sentence  X  anyone know what I can do to make me less tired  X . Note that the mined sequential tokens need not to be contiguous as appeared in the original text. 
There is a handful of algorithms available to find all frequent subsequences, and the Prefixspan algorithm [11] is reported to be efficient in discovering all rela tive frequents by using a pattern growth method. We adopt this algorithm in our work by imposing the following additional constraints: 1) Maximum Pattern Length: We limit the maximum number of 2) Maximum Token Distance: The two adjacent tokens t 3) Minimum Support: We set the minimum percentage of 4) Minimum Confidence: We set the probability of the pattern p
To overcome the sparseness probl em, we generalize the tokens by applying Part-of-Speech (POS) taggers to all tokens except some keywords including 5W1H wo rds, modal words, stop words and the most frequent occurring words mind from cQA such as  X  any1  X ,  X  im  X ,  X  whats  X  etc. For example, the pattern &lt; any1, know, pattern makes up a binary feature for the classification model as we will introduce in Section 2.3. 
We found that sequential patterns at the lexical level might not always be adequate to categorize questions. For example, the lexical pattern &lt; when, do &gt; presumes the non-question  X  Levator scapulae is used when you do the traps workout  X  to be a question, and the question  X  know someone with an eating disorder?  X  could be missed out due to the lack of indicative lexical patterns. These limitations, however, could be alle viated by syntactic features. The tree pattern (SBAR(WHADVP(WRB ))(S(NP)(VP)) ) extracted from the former example has the order of NP and VP being switched, which might indicate th e sentence to be a non-question, whereas the tree pattern (VP(VB)(NP(NP)(PP))) may be evidence that the latter example is indeed a question, because this pattern is commonly observed in the archived questions. 
Syntactic patterns have been pa rtially explored in erroneous sentence detection [17], in which all non-leaf nodes are flattened for frequent substructure extracti on. The number of patterns to be explored, however, grows exponentia lly with the size of the tree, which we think is inefficient. The reason is that the syntactic pattern will become too specific if mining is extended to a very deep level, and nodes at certain levels do not carry much useful structural information favored by question detection ( e.g. , the production rules NP  X  DT X NN at the bottom level). 
For better efficiency, we focus only on certain portion of the parsing tree by limiting the depth of the sub-tree patterns to be within certain levels ( e.g. 2  X  D  X  4). We further generalize each syntactic pattern by removing some nodes denoting modifiers, preposition phrases and conjunctions etc. For instance, the pattern SQ(MD)(NP(NN))(ADVP(RB))(VP(VP)(NP)(NP)) extracted from the question  X  can someone also give me any advice? '' could be generalized into SQ(MD)(NP(NN))(VP(VP)(NP)(NP)) , where the redundant branch ADVP(RB) that represents the adverb  X  also  X  is pruned. The pattern extraction process is outlined in Algorithm 1. The overall pattern mining strategy is analogous to the mining of sequential patterns, where the measures including support and confidence are taken into consideration to control the significance of the mined patterns. The discove red patterns are used together with the sequential patterns as features for the learning of classification model. 
The input to an algorithm that learns a binary classifier consists normally of both positive and nega tive examples. While it is easy to discover certain patterns fro m questions, it becomes unnatural to identify characteristics for non-questions. The imbalanced data distribution leads normal classifiers to perform poorly on the model learning. To address this issue, we propose to learn with the one-class SVM method. One-class SVM is built on top of the standard two-class SVM method, and its basic idea is to transform features from only positive examples via a kernel to a hyper-plane, and treats the origin as the only member of the negative class. It further uses relaxation parameters to separate the image of positive class from the origin, and finally applies the standard two-class SVM techniques to learn a decision boundary. As a result, data points outside the boundary are considered to be outliers, i.e. non-questions in our problem. 
The training data as used by traditional supervised learning methods usually require human labelling, which is not cheap. To save human efforts on data annota tion, we take a shortcut by assuming all questions ending with question marks as an initial set of positive examples. This assump tion is acceptable, as according to the results reported in [4], the rule-based method using only question mark achieves a very hi gh precision (97%) in detecting questions. It in turn indicates th at questions ending with  X ? X  are highly likely to be real questions. To reduce the effect of possible outliers ( e.g. non-questions ending with  X ? X ), we need to purify the initial training set. There are many techniques available for training data refinement, such as bootstrapping, condensing, and editing. We choose a SVM-based data editing and classification method proposed by [15] to iteratively remove the samples likely to be outliers. The detail is not covered here as it is beyond the scope of this paper. 
For one-class SVM training, the lin ear kernel is used, as it is shown to outperform other kernel functions. In the iterations of training data refinement, the parameter  X  that controls the upper bound percentage of outliers is set to 0.02. The question detector model learned ultimately serves as a component for the multi-sentence question segmentation system. 
Unlike traditional text segmentation, question segmentation ought to group each sub-question with its context sentences while separating it from the other sub-que stions. Investigations show that the user posting styles in the online environment are largely unpredictable. While some users ask multiple questions in an interleaved manner, some prefer to list the whole description first and ask all sub-questions later. Th erefore, naive methods such as using distance based metrics will be inadequate, and it is a great challenge to segment multi-senten ce questions especially when the description sentences in various aspects are mixed together. 
In the remainder of this section, we present a novel graph-based propagation method for segmenting multi-sentence questions. While the graph based method has been successfully applied in many applications like web search, to the best of our knowledge, this is the first atte mpt to apply it to the question segmentation problem. The intu ition behind the use of graph propagation approach is that if two description sentences are closely related and one is the context of a question sentence, then the other is also likely to be its context. Likewise, if two question sentences are very close, then the context of one is also likely to be the context of the other. We next introduce the graph model of the multi-sentence question, followed by the sentence closeness score computation and the graph propagation mechanism. 
Given a question thread comprising multiple sentences, we represent each of its sentences as a vertex v . The question detector is then applied to divide sent ences into question sentences and non-question sentences (contexts), forming a question sentence vertex set V q and a context sentence vertex set V c respectively. 
We model the question thread into a weighted graph ( V, E ) with a set of weight functions  X   X  E w : , where V is the set of vertices V q  X  V c , E is the union of three edge sets E w(E) is the weight associated with the edge E. The three edge sets E , E c and E r are respectively defined as follows: -E q : a set of directed edges u  X  v , where u, v  X  -E c : a set of directed edges u  X  v , where u, v  X  -E r : a set of undirected edges u X  X  , where u  X  V
While the undirected edge indicates the symmetric closeness relationship between a question sentence and a context sentence, the directed edge captures the asymmetric relation between two question sentences or two cont ext sentences. The intuition of introducing the asymmetry relations hip could be explained with the example given in Figure 1. It is noticed that C 1 is the context sentence Q 2 . Furthermore, Q 2 is shown up to be motivated by Q but not in the opposite direction. This observation gives us the sense that C 1 could also be the context of Q Q . We may reflect this asymmetr ic relationship using the graph model by assigning higher wei ght to the directed edge Q than to Q 2  X  Q 1 . As a result, the weight of the chain C becomes much stronger than that of C 2  X  Q 2  X  Q 1 , indicating that C is related to Q 2 but C 2 is not related to Q 1 , which is consistent to our intuition. From another point of view, the asymmetry helps to regulate the direction of the closeness score propagation.
We give two different weight functions for edges depending on whether they are directed or not. For the directed edge ( u  X  v ) in E and E c , we consider the following factors in computing weight: 1) KL-divergence: given two vertices u and v , we construct the 2) Coherence: it is observed that the subsequence sentences are 3) Coreference: coreference commonly occurs when multiple 
Note that all the metrics introduced above are asymmetric, meaning that the measure from u to v is not necessarily the same as that from v to u . Given two vertices u, v  X  E q of the edge u  X  v is computed by a linear interpolation of the three factors as follows: where 1 , , 0 3 2 1  X   X   X   X   X  . (4) 
Since D KL ( M v ||M u )  X  0, 0  X  Coh(v|u)  X  1, and 0  X  Ref(v|u)  X  1, need to apply normalization on this weight. We employed grid search with 0.05 stepping space in our experiments and found that the combination of {  X  1 = 0.4,  X  2 = 0.25,  X  3 = 0.35} gives the most satisfactory results. 
While the weight of the directed edges in E q and E the throughput of the score propagation from one to another, the weight of the undirected edge ( u X  X  ) in E r demonstrates the true closeness between a question and a context sentence. We consider the following factors in computing the weight for edges in E 1) Cosine Similarity: given a question vertex u and a context 2) Distance: questions and contexts separated far away are less 3) Coherence: the coherence between a question and a context 4) Coreference: similarly, it measures the number of the same 
The final weight of the undirected edge ( u  X  v ) is computed by a linear interpolation of the abovementioned factors: where 1 , , , 0 4 3 2 1  X   X   X   X   X   X  (9) 
The combination of {  X  1 = 0.4,  X  2 = 0.1,  X  3 produces best results with grid s earch. Note that normalization is not required as each factor is valued between 0 and 1. With the weight of each edge defined, we next introduce the propagation mechanism of the edge scores. 
For each pair of vertices, we assign the initial closeness score to be the weight of the edge in-between using the weight function introduced in Section 3.1, depe nding on whether the edge is in E E or E r . Note that if the edge weight is very low, two sentences might not be closely related. For fast processing, we use a weight threshold  X  to prune edges with weight below  X  . The parameter  X  is empirically determined, and we found in our experiments that the results are not very sensitive to  X  value below 0.15. 
With the initial closeness scores, we carry out the score propagation using the algorithm outlined in Algorithm 2. The basic idea of this propagation algorithm is that, given a question sentence q and a context sentence c , if there is an intermediate question sentence q i such that the edge weight w 1 ( q with the closeness score w ( q i ,c ) between q i relatively high, then the closeness score w ( q,c ) between q and c could be updated to  X  w 1 ( q i  X  q ) w ( q i ,c ) in case the original score is lower than that. In other words, q i becomes the evidence that q and c are related. The propagation algorithm works similarly in propagating scores from question se ntences to context sentences, where an intermediate context c i could be the evidence that c and q are related. Notice that the direction of propagation is not arbitrary. For example, it makes no sense if we propagate the score along the path of c  X  c i  X  q , because c i is simply the receiver of c , which could not be the evidence that a question and a context are correlated. When considering a pair of q and c , the possible directions of propagation are illustrated in Figure 2, in which the dashed lines indicate invalid propagation paths. Figure 2: Illustration of the direction of score propagation 
The damping factor  X  in the algorithm controls the transitivity among nodes. In some circumstan ces, the propagated closeness score might not indicate the true relatedness between two nodes, especially when the score is propagated through an extremely long chain. For example, { ABC } is close to { BCD }, { BCD } is close to { CDE }, and { CDE } is close to { DEF }. The propagation chain could infer { ABC } to be related to { DEF }, which is not true. The introduction of damping factor  X  can leverage this propagation issue by penalizing the closeness score when the chain becomes longer. We empirically set  X  to 0.88 in this work. 
The propagation of the closeness score will eventually converge. This is controlled by our propagation principle that the updated closeness score is a multip lication of two edge weights whose value is defined to fall betw een 0 and 1. Hence the score is always upper bounded by the maximu m weight of the edges in E . 
After the propagation reaches the stationary condition, we need to extract all salient edges in E r for the alignment of questions and contexts. One straightforward met hod is to pre-define a threshold  X  , and remove all edges weighted under  X  . However, this method is not very adaptive, as the edge weights vary greatly for different questions and a pre-defined thres hold is not capable to regulate the appropriate number of ali gnments between questions and contexts. In this work, we take a dynamical approach instead: we first sort edges in E r by the closeness score and extract them one by one in descending order &lt; e 1 , e 2 , ... , e process terminates at e m when one of the following criteria is met: 1. ) 1 ( 2. ew m+1 &lt;  X  , where  X  is a pre-defined threshold controlling the 3. m = n , meaning all edges have been extracted out from E
When the extraction procedure terminates, the extracted edge set { e 1 , ... ,e m } represents the final al ignment between questions and contexts. For each edge e i connecting between a context c and a question q , c will be considered as the context to question q , and they belong to the same question segment. For example, a final edge set {( q 1 ,c 1 ), ( q 2 ,c 2 ), ( q 1 ,c 2 ), ( q three question segments: ( q 1  X  c 1 ,c 2 ), ( q 2  X  c Note that the segmentation works in a fuzzy way such that no explicit boundaries are defined between sentences. Instead, a question could have multiple cont ext sentences, whereas a context sentence does not necessarily belong to only one question. By applying segmentation on the multi-sentence questions from cQA, sub-questions and their corresponding contexts that are topically related could be groupe d. Figure 3 shows an improved retrieval framework with segmen tation integrated. Different from existing models, the question matcher matches two question sentences with the assistance of additional related contexts such that the users X  query can be matched with the archived cQA questions more precisely. More specifically, the user query is no longer restricted to a short single-sentence question, but can be in the form of multiple sub-quest ions complemented with many description sentences. An archived question thread asking in various aspects could also be indexed into different question-context pairs such that the matching is performed on the basis of each question-context pair. 
Figure 3: Retrieval framework with question segmentations 
In this section, we present empirical evaluation results to assess the effectiveness of our quest ion detection model and multi-sentence segmentation technique. In particular, we conduct experiments on the Yahoo! Answers QA archive and show that our question detection model outpe rforms traditional rule based or lexical based methods. We furthe r show that our segmentation model works more effectively than conventional text segmentation techniques in segm enting multi-sentence questions, and it gives additional perfo rmance boosting to cQA question matching. Dataset: We issued getByCategory API query to Yahoo! Answers, and collected a total of around 0.8 million question threads from Healthcare domain. From the collected data, we generate the following three da tasets for the experiments: -Pattern Mining Set: Around 350k sentences extracted from 60k question threads are used for lexical and syntactic pattern mining, where those ending with  X ? X  are treated as question sentences and the others as non-question sentences 1 . -Training Set: Around 130k sentences ending with  X ? X  from another 60k question threads are used as the initial positive examples for one-class SVM learning method. -Testing Set: Two annotators ar e asked to tag some randomly picked sentences from a third post set. A total of 2004 question sentences and 2039 non-question sentences are annotated. 
Method: To evaluate the performance of our question detection model, we use five di fferent systems for comparison: 1) 5W1H (baseline1): a rule ba sed method determines that a 2) Question Mark (baseline2): a ru le based method judges that a 3) SeqPattern: Using only sequentia l patterns as features. 4) SynPattern: Using only syntactic patterns as features. 5) SeqPattern+SynPattern: Using both sequential patterns and A grid search algorithm is perfo rmed to find the optimal number of features used for model training, and a set of 1314 sequential patterns and 580 syntactic patterns are shown to give the best performance. Table 1 illustrates some pattern examples mined. 
Metrics &amp; Results: We employ Precision, Recall, and F metrics to evaluate the questi on detection performance. Table 2 tabulates the comparison results. From the table, we observe that 5W1H performs poorly in both precision and recall. Question mark based method gives the highe st precision, but the recall is relatively low. This observation is in line with the reported results in [4]. On the other hand, SeqPa ttern gives relatively high recall and SynPattern gives relatively high precision. The combination of both augments the performance in both precision and recall by a lot, and it achieves statistically significant improvement (t-test, p-value&lt;0.05) as compared to SeqPattern and SynPattern. We believe that the improvement stems from the ability of the detection model to capture the salient characteristics in questions at both the lexical and syntactic levels. The results are also consistent with our intuition that sequential patterns could misclassify a non-question to a que stion, but syntactic patterns may leverage it to certain extent. It is noted that our question detector exhibits a sufficiently high F 1 score for its use in the multi-sentence question segmenta tion model in the later phase. 
Table 2: Performance comparisons for question detection on 
We first evaluate the effectiveness of our multi-sentence question segmentation model (denoted as MQSeg ) via a direct user study. We set up two baselines using the traditional text segmentation techniques for comp arison. The first baseline (denoted as C99 ) employs the C99 algorithm [4], which uses a similarity matrix to generate a local sentence classifier so as to isolate topical segments. The second baseline (denoted as TransitZone ) is built on top of the method proposed in [12]. It measures the thematic distance between sentences to determine a series of transition zones, and uses them to locate the boundary sentences. To conduct the user study , we generate a small dataset by randomly sampling 200 question threads from the collected data. We run the three segmentation systems for each question thread, and present the segmentation results to two evaluators without telling them from which sy stem the result was generated. The evaluators are then asked to rate the segmentation results using a score from 0 to 5 with respect to their satisfaction. Figure 4 shows the score distributions from the evaluators for three different segmentation systems. We can see from Figure 4 that users give relatively moderate scor es (avg. 2 to 3) to the results returned by two baseline systems, whereas they seem to be more satisfied with the results given by MQSeg . The score distribution in MQSeg largely shifts towards high end as compared to the two baseline systems. The average rating scores for three different systems are 2.63, 2.74, and 3.6 respectively. We consider two evaluators to be agreeable to the segmentation result if their score difference does not exceed 1, and the average level of peer agreement obtained between the two evaluators is 93.5%. Figure 4: Score distribution of user evaluation for 3 systems It is to our expectation that MQSeg performs better than C99 or TransitZone segmentation systems. One straightforward reason is that MQSeg is specifically designed to segment multi-sentence questions, whereas the traditional systems are designed for generic purpose and do not disti nguish question sentences from contexts. While the conventional systems fail to capture the relationship between questions a nd their contexts, our system aligns the questions and contexts in a fuzzy way that one context sentence could belong to different question segments. As online content is usually freely posted and does not strictly adhere to the formal format, we believe that our fuzzy grouping mechanism is more suitable to correlate sub-questions with their contexts, especially when there is no obvious sign of association. 
In cQA, either archived questions or user queries could be in the form of a mixture of questi on and description sentences. To further evaluate our segmentation model and to show that it can improve question retrieval, we set up question retrieval systems coupled with segmentation modules for either question repository or user query.
Methods: We select BoW, a simp le bag-of-word retrieval system that matches stemmed words between the query and questions, and STM, a syntactic tree matching retrieval model proposed in [18] as two baseline systems for question retrieval. For each baseline, we further set up three different combinations: 1) Baseline+RS: a baseline retrieval system integrated with 2) Baseline+QS: a baseline retrieva l system equipped with user 3) Baseline+RS+QS: the retrieval system with segmentations for It gives rise to a total of 6 diffe rent combinations of methods for comparison. 
Dataset: We divide the collected 0. 8 million question dataset from Yahoo! Answers into two part s. The first part (0.75M) is used as a question repository, whil e the remaining part (0.05M) is used as a test set. For data pr eprocessing, systems coupled with RS will segment and index each question thread in the repository accordingly, whereas systems without RS simply performs basic sentence indexing tasks. From the test set, we randomly select 250 sample questions, each of which is in the form of one single-sentence question with some context sentences. The reason that we do not take queries of multi sub-questions as test cases is that traditional cQA question retrieva l systems cannot handle complex queries, making it impossible to conduct the comparison test. Nevertheless, it is sufficient to us e single-question queries here as our purpose is to testify that the context extracted by the segmentation model could he lp question matching. 
For systems equipped with user query segmentation (QS), we simply use the testing samples as they are, whereas for systems without QS, we manually extract the question sentences from the samples and use them as queri es without their corresponding context sentences. For each retrieval system, the top 10 retrieval results are kept. For each query, we combine the retrieval results from different systems, and ask tw o annotators to label each result to be either  X  X elevant X  or  X  X rre levant X  without telling them from which system the result is gene rated. The kappa statistic for identifying relevance between two evaluators is reported to be 0.73. A third person will be invol ved if conflicts happen. By eliminating some queries that have no relevant matches, the final testing set contains 214 query questions. Table 3: Performance of different systems measured by MAP, MRR, and P@1 (%chg shows the improvement as compared to BoW or STM baselines. All measures achieve statistically significant improvement wi th t-test, p-value&lt;0.05) 
Metrics &amp; Results: We evaluate the performance of retrieval systems using three metrics: Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), and Precision at Top One (P@1). The evaluation results are presented in Table 3. We can see from Table 3 that STM consistently outperforms BoW. Applying question repositor y segmentation (RS) over both BoW and STM baselines boosts system performance by a lot. All RS coupled systems achieve stat istically significant improvement in terms of MAP, MRR and P@1. We believe that the improvement stems from the abil ity of the segmentation module to eliminate irrelevant content that is favored by traditional BoW or STM approaches. Take the query question  X  What can I eat to put on weight?  X  as an example, traditional approaches may match it to an irrelevant question  X  I X  X  wearing braces now. what am I allowed to eat?  X  due to their high similarity on the questioning part. The mismatch however, could be alleviated if repository segmentation gets involved, where the context sentence can give clear clue that the above archived sentence is not relevant to the user query. 
Performing user query segmenta tion (QS) on top of baseline systems also brings in large improvements in all metrics. This result is in line with our exp ectation. The introduction of QS is based on the intuition that contex ts could complement questions with additional information, whic h help the retrieval system to better understand the user X  X  inform ation need. For example, given an example question from our testing set  X  Questions about root canal?  X , it makes no sense for retrieva l systems to find its related questions if the context is absent, because there could be hundreds of irrelevant questions in the QA archive as long as they are concerned about  X  root canal  X . 
Interestingly, STM+QS gives more improvement over STM as compared to BoW+QS over BoW. Our reading is that, BoW is less sensitive to the query context as compared to STM. To be more specific, the query context provides information at the lexical level, and BoW handles ba d-of-word queries at the lexical level, whereas STM matc hes questions at the syntactic level. As such, it is reasonable that matching at both lexical and syntactic levels (STM+QS) gives more performance boosting as compared to only at lexical level (BoW+QS) . Similar interpretation could be applied to explain the finding th at BoW+RS system gives more significant improvement over BoW as compared to BoW+QS. Furthermore, we conjecture that , without RS, BoW is likely to match the query with some context sentences, whereas having question repository properly segmen ted overcomes this issue to a large extent. 
Lastly, the combination of both RS and QS brings in significant improvement over the other methods in all metrics. The MAP on systems integrated with RS and QS improves by 12.93% and 11.45% respectively over BoW and STM baselines. RS+QS embedded systems also yield bette r top one precision by correctly retrieving questions at the firs t position on 143 and 150 questions respectively, out of a total of 214 questions. These significant improvements are consistent to our observations that RS and QS complement each other in not only better analyzing the user X  X  information need but also organi zing the question repository more systematically for efficient question retrieval. 
Error Analysis: Although we have shown that RS together with QS improves question retrieval, there is still plenty of room for improvement. We perform mi cro-level error analysis and found that the segmentation some times fails to boost retrieval performance mainly for the following three reasons: 1) Question detection error: The performance of question 2) Closeness gaps: The true closeness score between sentences is 3) Propagation errors: The propagate d closeness score could be 
There have been many literature works in the direction of question retrieval, and these work s could generally be classified into two genres: the early FAQ retrievals and the recent cQA retrievals. Among FAQ related wo rks, many retrieval models have been proposed, including the conventional vector space model [8], noisy channel model [ 16], and translation based model [14] etc. Most of these works tried to extract a large number of FAQ pairs from the Web, and use the FAQs dataset to do training and retrieval. 
The cQA archive is different from FAQ collections in the sense that the content of cQA archive is much noisier and the scope is much wider. The state-of-the-art cQA question retrieval systems also employ different models to perform the search, including the vector space model [5], language model [5,7], Okapi model [7], and translation model [7,14,19] et c. Claiming that purely lexical level models are not adequate to cope with natural languages, Wang et al. [18] proposed a syntactic tree matching model to rank historical questions. 
However, all these previous wo rks handle bag-of-words queries or single-sentence questions only. On the contrary, we take a new approach by introducing a question segmentation module, where the enhanced retrieval system is capable of segmenting a multi-sentence question into parts that are topically related and perform better question matching thereafter. To the best of our knowledge, no previous work has attempted to look into this direction, or use question segmentation to im prove the question search. 
In this paper, we have presen ted a new segmentation approach for segmenting multi-sentence que stions. It separates question sentences from non-question sentences and aligns them according to their closeness scores as de rived from the graph based model. The user study showed that our system produces more satisfactory results as compared to the traditional text segmentation systems. Experiments conducted on the c QA question retrieval systems further demonstrated that segm entation significantly boosts the performance of question matching. 
Our qualitative error analysis revealed that the segmentation model could be improved by inco rporating a more robust question detector, together with more advanced semantic measures. One promising direction for future work would be to also analyze the answers to help question segmentation. This is because answers are usually inspired by questions, where certain answer patterns could be helpful to predict the linkage between question and context sentences. The segmentation system in this work takes all noisy contexts as they are, w ithout further analysis. The model could be further improved by extracting the most significant content and align them with que stion sentences. Finally, it is important to evaluate the efficien cy of our proposed approach as well as to conduct additional empirical studies of the performance of question search with segm entation model incorporated. [1] Trec proceedings. http://trec.nist.gov/. [2] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. [3] F. Y. Y. Choi. Advances in domain independent linear text [4] G. Cong, L. Wang, C.-Y. Lin, Y.-I. Song, and Y. Sun. [5] H. Duan, Y. Cao, C.-Y. Lin, and Y. Yu. Searching questions [6] M. A. Hearst. Multi-para graph segmentation of expository [7] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar questions [8] V. Jijkoun and M. de R ijke. Retrieving answers from [9] N. Jindal and B. Liu. Identifying comparative sentences in [10] M.-Y. Kan, J. L. Klavan s, and K. R. McKeown. Linear [11] J. Pei, J. Han, B. Mortazavi-asl, H. Pinto, Q. Chen, U. Dayal, [12] V. Prince and A. Labadi  X e. Text segmentation based on [13] J. C. Reynar. Topic segmentation: Algorithms and [14] S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and [15] X. Song, G. Fan, and M. Rao. Svm-based data editing for [16] R. Soricut and E. Brill. Automatic question answering: [17] G. Sun, G. Cong, X. Li u, C.-Y. Lin, and M. Zhou. Mining [18] K. Wang, Z. Ming, a nd T.-S. Chua. A syntactic tree [19] X. Xue, J. Jeon, and W. B. Croft. Retrieval models for 
