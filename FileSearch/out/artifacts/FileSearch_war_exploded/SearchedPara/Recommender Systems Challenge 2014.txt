 The 2014 ACM Recommender Systems Challenge invited re-searchers and practitioners to work towards a common goal, this goal being the prediction of users engagement in movie ratings expressed on Twitter. More than 200 participants sought to join the challenge and work on the new dataset released in its scope. The participants were asked to de-velop new algorithms to predict user engagement and evalu-ate them in a common setting, ensuring that the comparison was objective and unbiased, within the challenge.
 D.2.8 [ Software Engineering ]: Metrics -complexity mea-sures, performance measures; H.3.3 [ Information Storage and Retrieval ]: Information search and retrieval -infor-mation filtering, relevance feedback; H.3.4 [ Information Technology and Systems Applications ]: Decision sup-port; H.3.5 [ Online Information Services ]: Data Shar-ing; H.5.1 [ Multimedia Information Systems ]: Evalua-tion/methodology Algorithms; Design; Experimentation; Human Factors; Measurement Recommender Systems; Dataset; Challenge; Competition; Context-aware; Benchmarking
The recommender systems community has a long standing tradition of organized benchmarking events, the most promi-nent one being the Netflix Prize 1 . Benchmarking events serve the purpose of creating a focused attempt at solving http://www.netflixprize.com a predefined problem. By providing a dataset, a certain setting and the means of evaluation, participants only have to focus on algorithmic performance as everything else is already provided.

The 2014 edition of the RecSys Challenge was the fifth annual ACM RecSys competition, and the first of the Rec-Sys Challenges to focus on user engagement . Previous chal-lenges had focused on movie rating prediction [1, 6], scien-tific paper recommendation and user targeting [4], as well as point-of-interest recommendation [2]. In recent years, the recommender systems community has put considerable fo-cus on the users of these systems, which is also reflected in the context of the Challenge.

For the 2014 edition, an extended version of the Movi-eTweetings dataset [3] was used. MovieTweetings contains movie ratings expressed by Twitter users who have tied their IMDb accounts to their Twitter account. The dataset contains the expected information found in a movie rating dataset, i.e. the user-movie rating matrix, timestamps, etc. This extended dataset also includes the interactions given to tweets that express ratings, i.e. the number of favorites and retweets each of the tweets containing movie ratings got from other users. This engagement could be considered as an additional signal reflecting e.g., the importance of a given rating with respect to other provided ratings.

Participants were asked to rank a given number of ratings within tweets by their predicted engagement for which we used the sum of the number of retweets and favorite counts as a proxy. The goal of the challenge is to provide insight on user interaction behavior with publicly posted ratings and to learn how being able to predict user engagement may be used for the benefit of recommender systems and its users.
For the purpose of the challenge, the MovieTweetings dataset was extended with additional meta-data from Twit-ter API. Therefore, besides the typical rating information consisting of user ids, item ids, ratings and timestamps, also many other types of data such as location and user profile information were available. The dataset contained 212 . 857 tweets collected over a period of 13 months and was chrono-logically split up in a training set (80% of the data), a test set (10% of the data) and an evaluation set (the remaining 10%). While the training set contained all data, the retweet count and favorite count data fields were removed in the test and evaluation set. Participants were given the training and test set to allow them to train their models and evaluate their have been omitted in the figure. results. Final evaluation was however performed by the or-ganizers using the private evaluation set.

For the evaluation the NDCG@10 metric  X  as available in the Rival evaluation framework 2  X  was chosen [5]. The met-ric expresses how well the challenge participants were able to capture the amount of interaction and engagement that each of the tweets in the test set obtained. This is done by com-paring the ranked list generated by participant X  X  algorithm to the ground truth that we provided in the dataset. For each user the true ranked list is considered to be a ranked list of tweets ordered by their engagement in descending or-der.
The challenge attracted more than 200 teams of interna-tional participants, Figure 1 shows the distribution of teams by country. All participating teams were invited to update the challenge leaderboard in order to allow the other teams to compare their current results. The challenge workshop took place on October 10, 2014. At the workshop, partici-pants presented their approaches and discussed methods and possibilities of predicting user engagement.
 The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement n  X  610594. http://rival.recommenders.net
