 Kihyuk Sohn kihyuks@umich.edu Honglak Lee honglak@eecs.umich.edu In recent years, unsupervised feature learning algo-rithms have emerged as promising tools for learning representations from data (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2006). In particular, it is an important problem to learn invariant representa-tions that are robust to variability in high-dimensional data (e.g., images, speech, etc.) since they will en-able machine learning systems to achieve good gen-eralization performance while using a small number of labeled training examples. In this context, sev-eral feature learning algorithms have been proposed to learn invariant representations for specific transfor-mations by using customized approaches. For exam-ple, convolutional feature learning methods (Lee et al., 2011; Kavukcuoglu et al., 2010; Zeiler et al., 2010) can achieve shift-invariance by exploiting convolution op-erators. As another example, the denoising autoen-coder (Vincent et al., 2008) can learn features that are robust to the input noise by trying to reconstruct the original data from the hidden representation of the perturbed data. However, learning invariant represen-tations with respect to general types of transforma-tions is still a challenging problem.
 In this paper, we present a novel framework of transformation-invariant feature learning. We focus on local transformations (e.g., small amounts of trans-lation, rotation, and scaling in images), which can be approximated as linear transformations, and incor-porate linear transformation operators into the fea-ture learning algorithms. For example, we present the transformation-invariant restricted Boltzmann ma-chine, which is a generative model that represents in-put data as a combination of transformed weights. In this case, a transformation-invariant feature rep-resentation is obtained via probabilistic max pooling of the hidden units over the set of transformations. In addition, we show extensions of our transformation-invariant feature learning framework to other unsu-pervised feature learning algorithms, such as autoen-coders or sparse coding.
 In our experiments, we evaluate our method on the variations of the MNIST dataset and show that our algorithm can significantly outperform the baseline re-stricted Boltzmann machine when underlying trans-formations in the data are well-matched to those con-sidered in the model. Furthermore, our method can learn features that are much more robust to the wide range of local transformations, which results in highly competitive performance in visual recognition tasks on CIFAR-10 (Krizhevsky, 2009) and STL-10 (Coates et al., 2011) datasets. In addition, our method also achieves state-of-the-art performance on phone classi-fication tasks with the TIMIT dataset, which demon-strates wide applicability of our proposed algorithms to other domains.
 The rest of the paper is organized as follows. We pro-vide the preliminaries in Section 2, and in Section 3, we introduce our proposed transformation-invariant fea-ture learning algorithms. In Section 4, we review the previous work on invariant feature learning. Then, in Section 5, we report the experimental results on sev-eral datasets. Section 6 concludes the paper. In this paper, we present a general framework for learning locally-invariant features using transforma-tions. For presentation, we will use the restricted Boltzmann machine (RBM) as the main example. 1 We first describe the RBM below, followed by its novel ex-tension (Section 3).
 The restricted Boltzmann machine is a bipartite undi-rected graphical model that is composed of visible and hidden layers. Assuming binary-valued visible and hidden units, the energy function and the joint prob-ability distribution are given as follows: 2 where v  X  X  0 , 1 } D are binary visible units, h  X  X  0 , 1 } are binary hidden units, and W  X  R D  X  K , b  X  R K , and c  X  R D are weights, hidden biases, and visible bi-ases, respectively. Z is a normalization factor that de-pends on the parameters { W , b , c } . Since RBMs have no intra-layer connectivity, exact inference is tractable and block Gibbs sampling can be done efficiently using the following conditional probabilities: where sigmoid( x ) = 1 1+exp (  X  x ) . We train the RBM parameters by minimizing the negative log-likelihood via stochastic gradient descent. Although computing the exact gradient is intractable, we can approximate it using contrastive divergence (Hinton, 2002). 3.1. Transformation-invariant RBM In this section, we formulate a novel feature learning framework that can learn invariance to a set of lin-ear transformations based on the RBM. We begin the section with describing the transformation operator. The transformation operator is defined as a mapping T : R D 1  X  R D 2 that maps D 1 -dimensional input vec-tors into D 2 -dimensional output vectors ( D 1  X  D 2 In our case, we assume a linear transformation matrix tor is represented as a linear combination of the input coordinates.
 With this notation, we formulate the transformation-invariant restricted Boltzmann machine (TIRBM) that can learn invariance to a set of transformations. Specifically, for a given set of transformation matrices T s ( s = 1 ,  X  X  X  ,S ), the energy function of TIRBM is defined as follows: where v are D 1 -dimensional visible units, and w j are D 2 -dimensional (filter) weights corresponding to the j -th hidden unit. The hidden units are represented as a matrix H  X  X  0 , 1 } K  X  S with h j,s as its ( j,s )-th entry. In addition, we denote z j = P S s =1 h j,s ,z j  X  X  0 , 1 } as a pooled hidden unit over the transformations.
 In Equation (6), we impose a softmax constraint on hidden units so that at most one unit is activated at each row of H . This probabilistic max pooling 3 al-lows us to obtain a feature representation invariant to linear transformations. More precisely, suppose that the input v 1 matches the filter w j . Given an-other input v 2 that is a transformed version of v 1 , the TIRBM will try to find a transformation matrix T s j so that the v 2 matches the transformed filter T T s v activate z j after probabilistic max pooling. Figure 1 illustrates this idea.
 Compared to the regular RBM, the TIRBM can learn more diverse patterns, while keeping the number of pa-rameters small. Specifically, multiplying transforma-tion matrix (e.g., T T s w j ) can be viewed as increasing the number of filters by the factor of S , but without significantly increasing the number of parameters due to parameter sharing. In addition, by pooling over local transformations, the filters can learn invariant representations (i.e., z j  X  X ) to these transformations. The conditional probabilities are computed as follows: Similar to RBM training, we use stochastic gradient descent to train TIRBM. The gradient of the log-likelihood is approximated via contrastive divergence by taking the gradient of the energy function (Equa-tion (5)) with respect to the model parameters. 3.2. Sparse TIRBM The sparseness of the feature representation is often a desirable property. By following Lee et al. X  X  (2008) approach, we can extend our model to sparse TIRBM by adding the following regularizer for a given set of data { v (1) ,  X  X  X  , v ( N ) } to the negative log-likelihood: where D is a distance function; p is the target sparsity. The expectation of pooled activation is written as Note that we regularize over the pooled hidden units z j rather than individual hidden units h j,s . In our experiments, we used L2 distance for D (  X  ,  X  ), but one can also use KL divergence for the sparsity penalty. 3.3. Generating transformation matrices In this section, we discuss how to design the trans-formation matrix T . For the ease of presentation, we assume 1-d transformations, but it can be extended to 2-d cases (e.g., image transformations) straightfor-wardly. Further, we assume the case of D 1 = D 2 = D here; but, we will discuss general cases later. As mentioned previously, T  X  R D  X  D is a linear trans-formation matrix from x  X  R D to y  X  R D ; i.e., each coordinate of y is constructed via linear combination of the coordinates in x with weight matrix T as follows: For example, shifting by s can be defined as For 2-d image transformations such as rotation and scaling, the contribution of input coordinates to each output coordinate is computed with bilinear interpo-lation. Since T  X  X  are pre-computed once and usually sparse, Equation (11) can be computed efficiently. 3.4. Extensions to other methods We emphasize that our transformation-invariant fea-ture learning framework is not limited to the energy-based probabilistic models, but can be extended to other unsupervised learning methods as well.
 First, it can be readily adapted to autoencoders by defining the following softmax encoding and sigmoid decoding functions: Following the idea of TIRBM, we can also formulate the transformation-invariant sparse coding as follows: where  X  is a constant. The second constraint in (15) can be understood as an analogy to the softmax con-straint in Equation (6) of TIRBMs.
 Similar to standard sparse coding, we can optimize the parameters by alternately optimizing W and H while fixing the other. Specifically, H can be (approx-imately) solved using Orthogonal Matching Pursuit, and therefore we refer this algorithm a transformation-invariant Orthogonal Matching Pursuit (TIOMP). Researchers have made significant efforts to de-velop invariant feature representations. For exam-ple, the rotation-or scale-invariant descriptors, such as SIFT (Lowe, 1999), have shown a great success in many computer vision applications. However, these image descriptors usually demand a domain-specific knowledge with a significant amount of hand-crafting. As an alternative approach, several unsupervised learning algorithms have been proposed to learn robust feature representations automatically from the sensory data. As an example, the denoising autoencoder (Vin-cent et al., 2008) can learn robust features by trying to reconstruct the original data from the hidden repre-sentations of randomly perturbed data generated from the distortion processes, such as adding noise or mul-tiplying zeros for randomly selected coordinates. Among types of transformations relating to temporally or spatially correlated data, translation has been ex-tensively studied in the context of unsupervised learn-ing. Specifically, convolutional training (LeCun et al., 1989; Kavukcuoglu et al., 2010; Zeiler et al., 2010; Lee et al., 2011; Sohn et al., 2011) is one of the most popular methods that encourages shift-invariance dur-ing the feature learning. For example, the convolu-tional deep belief network (CDBN) (Lee et al., 2011), which is composed of multiple layers of convolutional restricted Boltzmann machines and probabilistic max pooling, can learn a representation invariant to local translation.
 Besides translation, however, learning invariant fea-tures for other types of image transformations have not been extensively studied. In contemporary work of ours, Kivinen and Williams (2011) proposed the transformation equivariant Boltzmann machine, which shares a similar mathematical formulation to our mod-els in that both try to infer the best matching fil-ters by transforming them using linear transformation matrices. However, while their model was motivated from the  X  X lobal equivariance X , the main purpose of our work is to learn locally-invariant features that can be useful in classification tasks. Thus, rather than considering an algebraic group of transformation ma-trices (e.g.,  X  X ull rotations X ), we focus on the variety of local transformations that include rotation, trans-lation as well as scale variations. Furthermore, we effectively address the boundary effects that can be highly problematic in scaling and translation operators by forming a non-square T matrix, rather than zero-padding. 5 In addition, we present a general framework of transformation-invariant feature learning and show extensions based on the autoencoder and sparse cod-ing. Overall, our argument is strongly supported by the state-of-the-art performance in image and audio classification tasks.
 As another related work, feature learning methods with topographic maps can also learn invariant repre-sentations (Hyv  X arinen et al., 2001; Kavukcuoglu et al., 2009). Compared to these methods, our model is more compact and has fewer parameters to train since it fac-tors out the (filter) weights and their transformations. In addition, given the same number of parameters, our model can represent more diverse and variable input patterns than topographic filter maps. We begin by describing the notation. For images, we assume a receptive field size of r  X  r pixels (for input image patches) and a filter size of w  X  w pixels. We define gs to denote the number of pixels corresponding to the transformation (e.g., translation or scaling). For example, we translate the w  X  w filter across the r  X  r receptive field with a stride of gs pixels (Figure 2(a)), or scale down from ( r  X  l  X  gs )  X  ( r  X  l  X  gs ) to w  X  w (where 0  X  l  X  b ( r  X  w ) /gs c ) by sharing the same center for the filter and the receptive field (Figure 2(b)). For classification tasks, we used the posterior probabil-ity of the pooled hidden unit (Equation (10)) as a fea-ture. Note that the dimension of the extracted feature vector for each image patch is K , not K  X  S . Thus, we argue that the performance gain of the TIRBM over the regular RBM comes from the better represen-tation (i.e., transformation-invariant features), rather than from the classifier X  X  use of higher-dimensional fea-tures. 5.1. Handwritten digit recognition with prior First, we verified the performance of our algorithm on the variations of a handwritten digit dataset, assum-ing that the transformation information is given. From the MNIST variation datasets (Larochelle et al., 2007), we tested on  X  X nist-rot X  (rotated digits, referred to as rot ) and  X  X nist-rot-back-image X  (rotated digits with background images, referred to as rot-bgimg ). To fur-ther evaluate with different types of transformations, we created four additional datasets that contain scale and translation variations with and without random background (referred to as scale , scale-bgrand , trans , and trans-bgrand , respectively). 6 Some examples are shown in Figure 3.
 For these datasets, we trained sparse TIRBMs on the image patches of size 28  X  28 pixels with data-specific transformations. For example, we considered 16 equally-spaced rotations (i.e., the step size of for the rot and rot-bgimg datasets. Similarly, for the scale and scale-bgrand datasets, we generated scale-transformation matrices with w = 20 and gs = 2, which can map from (28  X  2 l )  X  (28  X  2 l ) pixels to 20  X  20 pixels with l  X  { 0 ,..., 4 } . For the trans and trans-bgrand datasets, we set w = 24 and gs = 2 to have total nine translation matrices that cover the 28  X  28 regions using 24  X  24 pixels with a horizontal and vertical stride of 2 pixels. For classification, we trained 1 , 000 filters for both sparse RBMs and sparse TIRBMs and used a softmax classifier. We used 10,000 examples for the training set, 2,000 examples for the validation set, and 50,000 examples for the test set. As reported in Table 1, our method (sparse TIRBMs) consistently outperformed the baseline method (sparse RBMs) for all datasets. These results suggest that the TIRBMs can learn better representations for the fore-ground objects by transforming the filters. It is worth noting that our error rates for the mnist-rot and mnist-rot-back-image datasets are also significantly lower than the best published results obtained with stacked denoising autoencoders (Vincent et al., 2010) (9 . 53% and 43 . 75%, respectively).
 For qualitative evaluation, we visualize the learned fil-ters on the mnist-rot dataset trained with the sparse TIRBM (Figure 3(e)) and the sparse RBM (Fig-ure 3(f)), respectively. The filters learned from sparse TIRBMs show much clearer pen-strokes than those learned from sparse RBMs, which partially explains the impressive classification performance. 5.2. Learning invariant features from natural For handwritten digit recognition in the previous sec-tion, we assumed prior information on global trans-formations on the image (e.g., translation, rotation, and scale variations) for each dataset. This assump-tion enabled the proposed TIRBMs to achieve signifi-cantly better classification performance than the base-line method, since the data-specific transformation in-formation was encoded in the TIRBM.
 However, for natural images, it is not reasonable to as-sume such global transformations due to the complex image structures. In fact, recent literature (Yu et al., 2011; Lee et al., 2011; Vincent et al., 2008) suggests that some level of invariance to local transformations (e.g., few pixel translation or coordinate-wise noise) leads to improved performance in classification. From this viewpoint, it makes more sense to learn represen-tations with local receptive fields that are invariant to generic image transformations (e.g., small amounts of translation, rotation, and scaling), which does not require data-specific prior information.
 We visualize the learned TIRBM filters in Figure 4, where we used the 14  X  14 natural image patches taken from the van Hateren dataset (van Hateren &amp; van der Schaaf, 1998). The baseline model (sparse RBM) learns many similar vertical edges (Figure 4(a)) that are shifted by a few pixels, whereas our methods can learn diverse patterns, including diagonal and horizon-tal edges, as shown in Figure 4(b), 4(c), and 4(d). These results suggest that TIRBMs can learn diverse sets of filters, which is reminiscent of the effects of con-volutional training (Kavukcuoglu et al., 2010). How-ever, our model is much easier to train than convolu-tional models, and it can further handle generic trans-formations beyond translations. 5.3. Object recognition We evaluated on image classification tasks using two datasets. First, we tested on the widely used CIFAR-10 dataset (Krizhevsky, 2009), which is composed of 50,000 training and 10,000 testing examples with 10 categories. Rather than learning features from the whole image (32  X  32 pixels), we trained TIRBMs on local image patches while keeping the RGB channels. As suggested by Coates et al.(2011), we used the fixed filter size w = 6 and determined the receptive field size depending on the types of transformations. 7 Then, af-ter unsupervised training with TIRBM, we used the convolutional feature extraction scheme, also follow-ing the Coates et al.(2011). Specifically, we computed the TIRBM pooling-unit activations for each local r  X  r pixel patch that was densely extracted with a stride of 1 pixel, and averaged the patch-level activations over each of the 4 quadrants in the image. Eventually, this procedure yielded 4 K -dimensional feature vectors for each image, which were fed into an L2-regularized lin-ear SVM. We performed 5-fold cross validation to de-termine the hyperparameter C .
 For comparison to the baseline model, we separately evaluated the sparse TIRBMs with a single type of transformation (translation, rotation, or scaling) us-ing K = 1 , 600. As shown in Table 2, each single type of transformation in TIRBMs brought a signifi-cant performance gain over the baseline sparse RBMs. The classification performance was further improved by combining different types of transformations into a single model.
 In addition, we also report the classification results obtained using TIOMP-1 (see Section 3.4) for unsu-pervised training. In this experiment, we used the fol-lowing two-sided soft thresholding encoding function: where  X  is a constant threshold that was cross-validated. As a result, we observed about 1% im-provement over the baseline method (OMP-1/T) using 1,600 filters, which supports the argument that our transformation-invariant feature learning framework can be effectively transferred to other unsupervised learning methods. Finally, by increasing the number of filters ( K = 4,000), we obtained better results (82 . 2%) than the previously published results using single-layer models, as well as those using deep networks.
 We also performed the object classification task on STL-10 dataset (Coates et al., 2011), which is more challenging due to the smaller number of labeled train-ing examples (100 per class for each training fold). Since the original images are 96  X  96 pixels, we down-sampled the images into 32  X  32 pixels, while keeping the RGB channels. We followed the same unsuper-vised training and classification pipeline as we did for CIFAR-10. As reported in Table 3, there were con-sistent improvements in classification accuracy by in-corporating the various transformations in learning al-gorithms. Finally, we achieved 58 . 7% accuracy using 1,600 filters, which is competitive to the best published single layer result (59 . 0%). 5.4. Phone classification To show the broad applicability of our method to other data types, we report the 39-way phone classification accuracy on the TIMIT dataset. By following (Ngiam et al., 2011), we generated 39-dimensional MFCC fea-tures and used 11 contiguous frames of them as an input patch. For TIRBMs, we applied three temporal translations with the stride of 1 frame.
 First, we compared the classification accuracy using the linear SVM to evaluate the performance gain com-ing from the unsupervised learning algorithms, by fol-lowing the experimental setup in (Ngiam et al., 2011). 8 As reported in Table 4, the TIRBM showed an im-provement over the sparse RBM, as well as the sparse coding and sparse filtering.
 In the next setting, we used the RBF-kernel SVM (Chang &amp; Lin, 2011) on the extracted features that are concatenated with MFCC features. We used de-fault RBF kernel width for all experiments and per-formed cross-validation on the C values. As shown in Table 5, combining MFCC with TIRBM features was the most effective and resulted in 1% improvement in classification accuracy over the baseline MFCC fea-tures. By increasing the number of TIRBM features to 512, we were able to beat the best published results on the TIMIT phone classification tasks using hierar-chical LM-GMM classifier (Chang &amp; Glass, 2007). In this work, we proposed novel feature learning algo-rithms that can achieve invariance to the set of pre-defined transformations. Our method can handle gen-eral transformations (e.g., translation, rotation, and scaling), and we experimentally showed that learn-ing invariant features for such transformations leads to strong classification performance. In future work, we plan to work on learning transformations from the data and combine it with our algorithm. By automat-ically learning transformation matrices from the data, we will be able to learn more robust features, which will potentially lead to significantly better feature rep-resentations.
 Acknowledgments This work was supported in part by a Google Faculty Research Award.

