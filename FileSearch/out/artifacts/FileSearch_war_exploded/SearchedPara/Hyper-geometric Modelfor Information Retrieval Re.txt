 A crucial issue underlying an IR system is to rank the returned documents by decreasing order of relevance. Generally , ranking is based on a weighting model. Most current probabilistic IR models assume a Poisson distribution of a query term X  X  occurrences, namely the within-d ocument frequency, in the document col-lection. Take BM25 for instance, its ranking function [11] is usually considered as a variant of the tf-idf weighting scheme, where the inverse document frequency ( idf ) is given by the Robertson Sparck-Jones weight [9] and the term frequency ( tf ) component is an approximation of the 2-Poisson distribution using a sat-uration function [10,11]. The PL2 model from the DFR family also follows the same Poisson assumption of the tf distribution.

An underlying assumption of the Poisson distribution is that the document prior P ( d ), the probability of observing a query term in a given document, is uniform across all documents in the collection. Imagine a document corpora is a collection of balls (tokens) in different colors, where a set of balls in color t represents a unique term and a basket d is a document 1 . The occurrence of query term t can be seen as the event of randomly throwing a ball in color t ,and P ( d ) is the probability of the ball is found to be thrown into basket d . Poisson distribution assumes that the document prior P ( d ) follows a uniform distribution, usually 1 N ,where N is the number of documents in the collection. In practice, since the lengths of different documents are highly diverse, it is natural that this assumption of the uniform prior distribution does not hold, hence the need for the so-called length normalization to penalize for the over-estimated term frequency in long documents.

Most current length normalization methods such as the saturation function of BM25 [11] and the Normalization 2 of PL2 [2] involve the use of a tunable parameter to control the trade-off between a  X  X enerous X  and  X  X arsh X  normaliza-tion. Although the length normalization methods exhibit certain effectiveness, they also suffer from the robustness pro blem caused by the parameter tuning. More specifically, the optimal parameter setting varies with different types of queries and search tasks. Given that it is difficult to obtain the prior knowledge of query types in practice, the retrieval effectiveness can be hurt if an inappro-priate parameter setting were used. Therefore, it is appealing to eliminate the normalization parameters while still having an adequate retrieval performance.
To this end, Amati proposed the DLH model based on the assumed hyper-geometric distribution of term frequen cy [1]. For the convenience of deriving a workable weighting function, the hyper-geometric distribution function is re-duced to a binomial distribution with non-uniform term priors. Despite the en-couraging performance shown by DLH, it s retrieval effectiveness may still not be comparable with other state-of-the-art IR models such as BM25 and PL2, as shown in our experiments in Section 5 on four standard TREC collections.
In this paper, we first revisit the hyper-geometric model by deriving an equiv-alent parameter free model based on binomial distribution with non-uniform document priors. Thus, a query term t with tf occurrences in document d is seen as TF binomial trials with tf successes, where TF is the frequency of t in the whole collection. Next, since DLH takes only the tf information into account in the relevance weighting, we suggest the possibility of further improvement in the retrieval performance by extending DLH with an idf component. Finally, experimental results on multiple search tasks on various standard TREC test collections demonstrate the benefit brought by the additional idf component.
The major contributions of this paper are two-fold. First, we provide a link be-tween the hyper-geometric model and the classical PL2 DFR model by deriving a parameter-free model based on the non-uniform document prior distribution. Second, we demonstrate the benefit of adding an idf component to DLH, which has been used as a standalone weighting model. To the best of our knowledge, this work is the first to combine the standalone DLH model with an explicit idf component.
The remainder of this paper is organized as follows. In Section 2, we review the above mentioned DLH model, which is then revisited in Section 3. In Section 4 and 5, we present our experimental settings and evaluation results. Finally, Section 6 concludes the work and suggests future research directions. In 2006, Amati proposes to eliminate the length normalization by assuming of the hyper-geometric distri bution of term frequency [1]: where tf is the frequency of term t in document d , l ( d ) is the document length, TF is the number of occurrences of t in the whole collection, and TFC is the number of tokens in the who le collection. There are TF tf different ways of com-to choose ( l ( d )  X  tf ) occurrences of a different term.

For the convenience of deriving a workable model, the above hyper-geometric distribution is reduced to a binomial distribution as follows: where p is the probability of occurrence of term t , i.e. a success of the binomial trial.

The above binomial distribution assumes that a document d with length l(d) is a sample of l(d) trials from the collect ion, where a query term t is seen as tf successes of the l(d) trials. Thus, a document can be considered as l ( d ) binomial trials whose outcome can be either a su ccess, namely an occurrence of the query term t , or a failure, namely an occurrence of a term other than t .
The prior probability of occurrence of the term t is given by the relative term-frequency in the collection, namely P ( t )= TF TFC ,where TF and TFC are the frequency of t in the whole collection, and the number of tokens in the whole collection, respectively. Thus, the inform ation content is used for measuring the importance of the query term in the document, which is a decreasing function of the probability as follows:
The final weighting function of DLH is given by a combination of the Laplace succession with an approximation of the above information content: where qtw ( t ) is the query term weight of t .Itisgivenby qtf qtf the number of occurrences of t in the query, and qtf max is the maximum qtf of all terms in the query. As aforementioned, most current probabilistic models such as BM25 and PL2 based on the uniform document prior assumption introduce length normalization components to deal with the over-estima ted term frequency in long documents. Amati proposes the parameter free DLH model based on the hyper-geometric term frequency distribution, which is r educed to a binomial distribution with non-uniform term priors [1].

In this section, we revisit the hyper-geometric model by providing an alter-native explanation on the reduction. We derive a tf weight based on the non-uniform document priors, which is shown to be highly similar to the DLH model. We then suggest not to use DLH as a standalone model as it is right now. In-stead, we propose to extend DLH with an idf component so that the model takes both tf and idf into consideration for the relevance weighting.

To derive a tf weight based on non-uniform document priors, we start with assuming that a term t is a sample of TF binomial trials from the collection, where TF is the number of occurrences of t in the collection. Therefore, the term occurrence probability P ( tf | d, P ( d )) with a document prior P(d) can be described by the binomial distribution as: where tf is the number of successes of the binomial trials.

If the document prior P(d) is assumed to be uniform for all documents in the collection, the above formula is equival ent to the Poisson randomness model as in PL2 [2]. As our aim is to derive a parameter free model, here the document prior P(d) is assumed to be non-uniform and depends on the document length:
In this way, the length normalization 2 [2] of PL2 is no longer required. Then the information content of the term occurrence probability becomes:
Following a similar method with [1] to simplify the above formula, we use the approximation by Renyi [8]: where  X  p is tf TF which is the maximum likelihood estimate (MLE) of the term in the document. The divergence D ( X  p, P ( d )) can be given by the asymmetric Kullback-Leibler divergence: Using the above approximation, we can further derive a simplified information content normalized by Laplac e succession [2] as follows:
Despite the fact that the above formula and DLH in Equation (4) are derived based on different assumptions of the prior distribution, they share a similar form -the only difference is the last addendum of the numerator ( tf TF against l ( d ) ). Indeed, the tf weight in Equation (10) is practically equivalent to DLH as they have almost identical results in our preliminary experiments.

Our above derivation provides an alternative explanation of the reduction from the hyper-geometric distribution to the binomial trials with non-uniform term priors. That is, the hyper-geometric model in its reduced form, namely the DLH model, can be seen as the classical PL2 model with non-uniform document priors without length normalization. Furthermore, as it turns out that DLH, having been used as a standalone model, takes only tf into account for the relevance weighting, it can be beneficial to expand DLH with an idf component. The inverse document frequency, namely the idf factor, considers the presence and absence of a given query term in the do cuments collection-wide. It implies that the more documents in which a term occurs, the less information is carried by the term. We then define a parameter free model called PF1 after adding BM25 X  X  idf component [9]:
We define the relevance weight as the sum instead of the product of the idf and tf weights, as both are negative logarithms of probabilities.

Since the tf weight based on the non-uniform document priors in Equation (10) is practically equivalent to DL H, the above PF1 model can be seen as an extension of DLH by adding an idf component. PF1 can also be seen as a parameter free version of BM25 by replacing its tf weight with Equation (10), a DFR-based tf weight normalized by Laplace succession. In the following sections, we conduct extensive experiments to evaluate PF1. 4.1 Datasets and Indexing All our experiments are conducted using an in-house extension of the open source Terrier 3.0 [7]. Moreover, the implementation of language model and its asso-ciated relevance feedback mechanism is migrated from the Lemur toolkit 2 .We use three standard TREC test collections in our study. Basic statistics about the test collections and topics are given in Table 1. The disk4&amp;5 collection contains mainly the newswire articles from newspapers and journals. The WT10G collec-tion is a medium size crawl of Web documents, which was used in the TREC 9 and 10 Web tracks. It contains 10 Gigabytes of uncompressed data. The DOT-GOV2 collection, which has 426 Gigabytes of uncompressed data, is a crawl from the .gov domain.

Each topic contains three fields, namely title, description and narrative. We only use the query terms in the title field for retrieval. The title-only queries are very short which are usually regarded as a realistic snapshot of real user queries. 4.2 Evaluation Methodology We conduct two levels of comparisons to evaluate the effectiveness of our pro-posed parameter free model as follows.

The first level of evaluation compares the retrieval effectiveness of our pro-posed PF1 model with two popular statistical IR models, namely BM25 and KLLM (Kullback-Leibler divergence language model)[15,16], and also the pa-rameter free DFR model, DLH. As both BM25 and KLLM have parameters that require tuning to provide a reliable retri eval performance, they are evaluated by cross-validation on the test collections us ed. In particular, they are evaluated by 2-fold cross-validation on disk4&amp;5 and WT10G, and by 3-fold cross-validation on DOTGOV2. The query topics are partitioned into equal-size subsets by the year of release on DOTGOV2, and by parity on WT10G and disk4&amp;5. We use the TREC official evaluation measure in our experiments, namely the Mean Average Precision (MAP) [13]. All statistical tests are based on Wilcoxon matched-pairs signed-rank test. In all the tables pres enting the experimental results, a * and  X  indicate a statistically significant difference at 0.05 and 0.01 level, respectively. Moreover, since DLH and PF1 are parame ter-free, they are evaluated over all queries without any training.

In the second level of evaluation, the PF1 model is also evaluated when query expansion (QE), or pseudo relevance fee dback, is applied. In particular, KLLM uses the RM3 method that is widely considered a state-of-the-art algorithm for relevance feedback [5,6]. The other retrieval models use an improved version of Rocchio X  X  relevance feedback algorithm based on the KL-divergence term weight-ing, which shows comparable effectiven ess to RM3 on the TREC collections used [4,14]. As the query expansion methods have multiple parameters that require tuning, all the models are evaluated by cross-validation. The query topics parti-tioning are the same as that for the first level of evaluation. 4.3 Parameter Tuning The baseline retrieval models such as BM25 and KLLM, and the query expansion methods used, involve the use of several tunable parameters which need to be optimized on the training subsets. In all o ur experiments, the parameters with continuous values, such as BM25 X  X  k 1 and b , are optimized using Simulated Annealing by directly maximizing MAP. The parameters wit h discrete values, such as the feedback document set size | ED | and the number of expansion terms | ET | for query expansion, are optimized by grid search by maximizing MAP.
For BM25, we first tune parameters k 1 and k 3 by a 2-dimensional optimiza-tion. Next, we optimize the length normalization parameter b . For KLLM, only a one-dimensional optimization of its smoothing parameter  X  is required. For the query expansion methods used, we first optimize the parameters | ED | and |
ET | by grid search of every integer value within [3, 20] for | ED | , and of every 5 integer value within [10, 60] for | ET | . 5.1 Evaluation Results without Query Expansion Table 2 compares the retrieval perfor mance measured by MAP of the baselines with our proposed parameter free model o n the three collections used. In this table, the numbers after the MAP values of PF1 stand for the difference between their MAP values and the baseline mod el X  X  MAP in percentage. From Table 2, we have the following observations. First, comparing to BM25, on all the three collections used, PF1 model performs equally well. No statistically significant dif-ference between them are observed. S econd, comparing to KLLM, there exists a statistically significant difference bet ween the retrieval performance of KLLM disk4&amp;5 0.2563 0.2553,-0.410% 0.2531 0.2553,+0.869% 0.2487 0.2553,+2.65%  X  WT10G 0.2043 0.2066,+1.13% 0.2152 0.2066,-4.01%* 0.1845 0.2066,+12.0%  X 
GOV2 0.3005 0.2980,-0.831% 0.2977 0.2980,+0.101% 0.2612 0.2980,+14.1%  X  and PF1 on WT10G. However on the other two collections, disk4&amp;5 and DOT-GOV2, PF1 has comparable retrieval performance with KLLM. Finally, com-paring to DLH, PF1 consistently outperforms this parameter free baseline. The difference between their retrieval effect iveness measured by MAP is statistically significant on all three collections used. This shows that the introduction of an idf weight to the DLH model has a positive effect on the retrieval performance.
We then take a close look at how the tunable parameters of BM25 and KLLM affect their retrieval performance, comp ared to the parameter free model. Fig-ure 1 and 2 plot the MAP obtained by BM25 against its parameters b and k 1 , respectively. For each of these two parameters, we scan a wide range of its possi-ble values, while fixing the other parameter to its optimal setting. We do not plot for parameter k 3 since this parameter has only n egligible effect on the retrieval performance for title-only queries. Also, Figure 3 plots the MAP obtained by KLLM against its smoothing parameter  X  . From Figures 1, 2 and 3, we can see that PF1, clearly outperforming DLH, appears to be able to provide the retrieval performance that is close to the optimized BM25 and KLLM. On the other hand, BM25 and KLLM are shown to need parameter tuning to guarantee their retrieval performan ce, as their MAP values vary with their parameter set-tings. Even though, the parameter of BM25 or KLLM appears to have a small range of safe settings across different co llections. However, the optimal setting of the length normalization parameters depends on the search task, as shown by the experimentation in the literature, e.g. [13]. Exper iments in Section 5.3 with non ad-hoc search tasks also show that the optimal settings of the length nor-malization parameters largely vary for different search tasks. Given the difficulty in guessing the information needs of different users in the dynamic real-world applications, it is therefore difficult to g uarantee the retrieval effectiveness of BM25 and KLLM by using a fixed parameter setting.

Overall, the experimenta l results in this section show that our proposed PF1 model by adding an idf weight to DLH is indeed effective on ad-hoc search tasks without the use of query expansion. 5.2 Evaluation Results with Query Expansion Table 3 contains the results obtained using query expansion. Notations in this table are similar to those in Table 2 in the previous section.
 disk4&amp;5 0.2935 0.3016,+2.76% 0.2991 0.3016,+0.836% 0.3036 0.3016,-0.675% WT10G 0.2280 0.2327,+2.04%* 0.2270 0.2327,+2.49% 0.2163 0.2327,+7.56%  X  GOV2 0.3246 0.3416,+5.24%* 0.3350 0.3416,+1.97% 0.3074 0.3416,+11.1%  X 
From Table 3, we can see that our proposed PF1 model has at least compa-rable retrieval performance with the baselines. In particular, PF1 significantly outperforms BM25 and DLH on WT10G and DOTGOV2. We suggest that the marked effectiveness of PF1 over BM25 is due to the fact that the queries with the added expansion terms are much longer than the original title-only queries, which may affect the parameter sensitivity of BM25. 5.3 Evaluation Results for Non-adhoc Tasks In this section, we further evaluate PF1 for non-adhoc search tasks. The ex-periments are conducted on the DOTGOV collection which is a medium crawl of the .gov domain with 1,247,753 indexed documents. The 225 topics of the TREC 2004 Web track [3] are used in our experiments. In particular, these 225 topics can be partitioned into three search tasks, namely topic-distillation (TD), named-page finding (NP), and homepage finding (HP). Each task has 75 associ-ated query topics. These non-adhoc search tasks in general demand high-quality resources, for which query expansion usually hurts the retrieval performance be-cause of the very small numbers of relevant documents for each query [3]. We therefore only conduct experiments without the use of query expansion.
For each search task, the 75 query topics are partitioned into three equal-size subsets by their topic numbers. We conduct a 3-fold cross-validation for the BM25 and KLLM baseline models. Table 4 contains the retrieval performance of the baseline models and our proposed par ameter free model on three collections. According to the official TREC setting [3], the retrieval performance for the topic distillation task is measured by MAP, and that for the name-page finding and homepage finding tasks are measured by the mean reciprocal rank (MRR). Notations in this table are similar to those in Table 2 in the previous sections.
As shown by Table 4, PF1 provides again at least comparable retrieval perfor-mance with the baselines, and can achieve statistically significant improvement over KLLM and DLH for the topic distillation and named-page finding tasks, respectively. The encouraging retrieval effectiveness of PF1 may be due to the fact that these non-adhoc search tasks demand high-quality key resources, for which the models with tunable parameters may have high parameter sensitivity. TD 0.0888 0.0852,-4.12% 0.0607 0.0852,+40.4%  X  0.0833 0.0852,+2.28% NP 0.4158 0.4329,+4.11% 0.3961 0.4329,+9.29% 0.3872 0.4329,+11.8%  X  HP 0.4091 0.4231,+3.43% 0.4022 0.4231,+5.21% 0.4146 0.4231,+2.05%
To support this hypothesis, we again plot the retrieval performance of BM25 against its parameter b . We do not plot for k 1 and KLLM X  X  parameter  X  for space reason, as these two parameters have similar observations. From Figure 4, we can see that BM25 indeed suffers from the parameter sensitivity issue, as its parameter b has only a small range of  X  X afe X  values. Also, its optimal setting varies for different search tasks, from approximately 0.95 for topic distillation and homepage finding, 0.70 for named-page finding, and 0.35 for ad-hoc search. This supports our argument that it is not reliable to use a fixed parameter setting in practice. On the other hand, PF1 can provide near optimal retrieval performance in most cases without the need for parameter tuning.

In summary, we have evaluated the PF1 model by extensive experiments on four standard TREC test collections for a va riety of search tasks that are different in nature. The experimental results can be summarized as follows. For the adhoc tasks without query expansion (see Section 5.1), it is encouraging to find out that the retrieval performance of PF1 is comparable with the state-of-the-art BM25 and KL-divergence language model (KLLM), and provide statistically significant improvement over DLH. For the adhoc tas ks with query expansion (see Section 5.2), PF1 performs equally well to KLLM, and outperform BM25 and DLH. For the three non-adhoc tasks on DOTGOV (see Section 5.3), PF1 has overall comparable retrieval performance with BM25, and outperforms KLLM and DLH for the topic distillation and named-page finding tasks, respectively. We have revisited the parameter free hyper-geometric model in order to pro-vide an alternative explanation on how the model is reduced to the binomial distribution. We follow a different approach for the tf weight estimation by as-suming the non-uniform document priors, which depend on the actual lengths of the documents. Based on this assumption, we show that DLH can be seen as the classical PL2 model with non-uniform document priors without length nor-malization. Thus, we suggest that since DLH consists of only the normalized tf weight, it can be extended by adding an idf component which is also parameter free. Extensive experiments on the standa rd TREC test collections demonstrate the effectiveness of the new model , called PF1, which combines the tf weight based on the non-uniform document priors and BM25 X  X  idf .

In the future, we plan to further evaluate PF1 on other test collections for other search tasks, such as the real-time Twitter search in the TREC microblog track and the Web search tasks on the larg e-scale ClueWeb 09 collection, where the relevance judgments are relatively shallow compared to the test collections used in this paper. We also plan to extend PF1 model by proposing a term proximity component that take the adja cency between query terms into consid-eration. We also plan to extend this work to the language modelling based on the hyper-geometric distribution such as [12].
 Acknowledgements. This work is supported in part by the National Nat-ural Science Foundation of Chin a (61103131/F020511), the President Fund of GUCAS (Y15101FY00 /Y25102HN00), and the National Key Technology R&amp;D Program of China (2012BAH23B03).

