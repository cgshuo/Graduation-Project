 Consider the subject of density estimation for high-dimensional continuous random variables, like images. Approaches for normalized density estimation, lik e mixture models, often suffer from the curse of dimensionality. An alternative approach is Product-of-Experts (PoE) [7], where we model the density as a product , rather than a sum, of component ( expert ) densities. The multiplicative nature of PoE models make them able to form complex densities : in contrast to mixture models, each expert has the ability to have a strongly negative influence o n the density at any point by assigning it a very low component density. However, Maximum Likelihoo d Estimation (MLE) of the model requires differentiation of a normalizing term, which is in feasible even for low data dimensionality. A recently introduced estimation method is Score Matching [ 10], which involves minimizing the square distance between the model log-density slope ( score ) and data log-density slope, which is independent of the normalizing term. Unfortunately, appli cations of SM estimation have thus far been limited. Besides ICA models, SM has been applied to Mark ov Random Fields [14] and a multi-layer model [13], but reported results on real-worl d data have been of qualitative, rather than quantitative nature. Differentiating the SM loss with respect to the parameters can be very challenging, which somewhat complicates the use of SM in man y situations. Furthermore, the proof of the SM estimator [10] requires certain conditions that ar e often violated, like a smooth underlying density or an infinite number of samples.
 Other estimation methods are Constrastive Divergence [8] ( CD), Basis Rotation [23] and Noise-Contrastive Estimation [6] (NCE). CD is an MCMC method that h as been succesfully applied to Restricted Boltzmann Machines (RBM X  X ) [8], overcomplet e Independent Component Analysis (ICA) [9], and convolution variants of ICA and RBM X  X  [21, 19] . Basis Rotation [23] works by re-stricting weight updates such that they are probability mas s-neutral. SM and NCE are consistent estimators [10, 6], while CD estimation has been shown to be g enerally asymptotically biased [4]. No consistency results are known for Basis Rotation, to our k nowledge. NCE is a promising method, but unfortunately too new to be included in experiments. CD a nd Basis Rotation estimation will be used as a basis for comparison.
 In section 2 a regularizer is proposed that makes Score Match ing applicable to a much broader class of problems. In section 3 we show how computation and di fferentiation of the SM loss can be performed in automated fashion. In section 4 we report enc ouraging quantitative experimental results. Consider an energy-based [17] model E ( x ; w ) , where  X  X nergy X  is the unnormalized negative log-In other words, low energies correspond to high probability density, and high energies correspond to low probability density.
 Score Matching works by fitting the slope ( score ) of the model density to the slope of the true, underlying density at the data points, which is obviously in dependent of the vertical offset of the log-density (the normalizing constant). Hyv  X arinen [10] shows that under some conditions, this objective is equivalent to minimizing the following expression, whic h involves only first and second partial derivatives of the model density: with N -dimensional data vector x , weight vector w and true, underlying pdf p x ( x ) . Among the practice, the true pdf is unknown, and we have a finite sample o f T discrete data points. The sample version of the SM loss function is: which is asymptotically equivalent to the equation (1) as T approaches infinity, due to the law of large numbers. This loss function was used in previous publi cations on SM [10, 12, 13, 15]. 2.1 Issues Should these conditions be violated, then (theoretically) the pdf cannot be estimated using equation (1). Only some specific special-case solutions exist, e.g. f or non-negative data [11]. Unfortunately, situations where the mentioned conditions are violated are not rare. The distribution for quantized data (like images) is discontinuous, hence not differentia ble, since the data points are concentrated at a finite number of discrete positions. Moreover, the fact t hat equation (2) is only equivalent to equation (1) as T approaches infinity may cause problems: the distribution of any finite training set of discrete data points is discrete, hence not different iable. For proper estimation with SM, data can be smoothened by whitening; however, common whitening m ethods (such as PCA or SVD) are computational infeasible for large data dimensionality, a nd generally destroy the local structure of spatial and temporal data such as image and audio. Some previ ous publications on Score Matching apply zero-phase whitening (ZCA) [13] which computes a weig hed sum over an input patch which removes some of the original quantization, and can potentia lly be applied convolutionally. However, the amount of information removed from the input by such whit ening is not parameterized and potentially large. 2.2 Proposed solution Our proposed solution is the addition of a regularization te rm to the loss, approximately equivalent to replacing each data point x with a Gaussian cloud of virtual datapoints ( x +  X  ) with i.i.d. Gaussian noise  X   X  N ( 0 ,  X  2 I ) . By this replacement, the sample pdf becomes smooth and the c onditions for proper SM estimation become satisfied. The expected value of the sample loss is: We approximate the first and second term with a simple first-or der Taylor expansion. Recall that The expected value of the first term is: The expected value of the second term is: Putting the terms back together, we have:
E J S ( x +  X  ; w ) = where E = E ( x ; w ) . This is the full regularized Score Matching loss. While min imization of above x which scales like O ( W 2 ) . However, the off-diagonal elements of the Hessian are ofte n dominated by the diagonal. Therefore, we will use the diagonal approxi mation: This regularized loss is computationally convenient: the a dded complexity is almost negligible since for unregularized Score Matching. The regularizer is relat ed to Tikhonov regularization [22] and curvature-driven smoothing [2] where the square of the curvature of the energy surface at the data points are also penalized. However, its application has bee n limited since (contrary to our case) in the general case it adds considerable computational cost. Figure 1: Illustration of local computational flow around so me node j . Black lines: computation of tional flow for differentiation of the Score Matching loss: c omputation of e.g.  X  X / X  X  j and  X  X / X  X  j . The influence of weights are not shown, for which the derivati ves are computed in the last step. In most optimization methods for energy-based models [17], the sample loss is defined in readily obtainable quantities obtained by forward inference in the model. In such situations, the required derivatives w.r.t. the weights can be obtained in a straight forward and efficient fashion by standard application of the backpropagation algorithm.
 For Score Matching, the situation is more complex since the ( regularized) loss (equations 2,7) is defined in terms of {  X  X / X  X  i } and {  X  2 E/ (  X  X  i ) 2 } , each term being some function of x and w . In earlier publications on Score Matching for continuous va riables [10, 12, 13, 15], the authors rewrote {  X  X / X  X  i } and {  X  2 E/ (  X  X  i ) 2 } to their explicit forms in terms of x and w by manually differentiating the energy 2 . Subsequently, derivatives of the loss w.r.t. the weights c an be found. This manual differentiation was repeated for different mod els, and is arguably a rather inflexible approach. A procedure that could automatically (1) compute and (2) differentiate the loss would make SM estimation more accessible and flexible in practice.
 A large class of models (e.g. ICA, Product-of-Experts and Fi elds-of-Experts), can be interpreted as a form of feed-forward neural network. Consequently, the te rms {  X  X / X  X  i } and {  X  2 E/ (  X  X  i ) 2 } can be efficiently computed using a forward and backward pass: th e first pass performs forward inference (computation of E ( x ; w ) ) and the second pass applies the backpropagation algorithm [3] to obtain the loss J ( x ; w ) is obtained by these two steps. For differentiation of this loss, one must perform an additional forward and backward pass. 3.1 Obtaining the loss Consider a feed-forward neural network with input vector x and weights w and an ordered set of nodes indexed 1 . . . N , each node j with child nodes i  X  children ( j ) with j &lt; i and parent nodes k  X  parents ( j ) with k &lt; j . The first D &lt; N nodes are input nodes, for which the activation value is g j = x j . For the other nodes (hidden units and output unit), the acti vation value is determined by computed by backpropagation. However, backpropagation of the full Hessian scales like O ( W 2 ) , where W is the number of model weights. Here, we limit backpropagati on to the diagonal approx-one-layer models and the models considered in this paper. Re writing the equations for the full Hes-for inference and backpropagation are given as the first two f or -loops in Algorithm 1.
Input: x , w (data and weight vectors) for j  X  D + 1 to N do // Forward propagation  X  N  X  1 ,  X   X  N  X  0 for j  X  N  X  1 to 1 do // Backpropagation for j  X  1 to D do for j  X  D + 1 to N do // SM Forward propagation for j  X  N to D + 1 do // SM Backward propagation for w  X  w do // Derivatives wrt weights 3.2 Differentiating the loss Since the computation of the loss J ( x ; w ) is performed by a deterministic forward-backward mech-anism, this two-step computation can be interpreted as a com bination of two networks: the original eventually J ( x ; w ) . See figure 1. The combined network can be differentiated by a n extended version of the double-backpropagation procedure [5], with the main difference that the appended network not only computes {  X  j } , but also {  X   X  j } . Automatic differentiation of the combined network consists of two phases, corresponding to reverse traversal of the appended and original network respectively: (1) obtaining  X  X / X  X  j ,  X  X / X  X  j and  X  X / X  X   X  j for each node j in order 1 to N ; (2) ob-taining  X  X / X  X  j for each node j in order N to D + 1 . These procedures are given as the last two f or -loops in Algorithm 1. The complete algorithm scales like O ( W ) . Consider the following Product-of-Experts (PoE) model: where M is the number of experts, w i is an image filter and the i -th row of W and  X  i are scaling parameters. Like in [10], the filters are L2 normalized to pre vent a large portion from vanishing.We also a Product of Student X  X  t -distribution model [24]. The parameter c is a non-learnable horizontal  X  i = exp  X  i where  X  i is the actual weight. 4.1 MNIST The first task is to estimate a density model of the MNIST handw ritten digits [16]. Since a large number of models need to be learned, a 2  X  downsampled version of MNIST was used. The MNIST dataset is highly non-smooth: for each pixel, the extreme va lues (0 and 1) are highly frequent lead-ing to sharp discontinuities in the data density at these poi nts. It is well known that for models with square weight matrix W , normalized g ( . ) (meaning R  X  normalizing constant can be computed [10]: Z ( w ) = | det W | . For this special case, models can be compared by computing the log-likelihood for the training-and test set. Unregularized, and regu-larized models for different choices of  X  were estimated and log-likelihood values were computed. Subsequently, these models were compared on a classificatio n task. For each MNIST digit class, a small sample of 100 data points was converted to internal fea tures by different models. These fea-tures, combined with the original class label, were subsequ ently used to train a logistic regression sification error on the test set was compared against reporte d results for optimal RBM and SESM models [20].
 Results. As expected, unregularized estimation did not result in an a ccurate model. Figure 2 shows how the log-likelihood of the train-and test set is optimal a t  X   X   X  0 . 01 , and decreases for smaller  X  . Coincidentally, the classification performance is optima l for the same choice of  X  . 4.2 Denoising Consider grayscale natural image data from the Berkeley dat aset [18]. The data quantized and therefore non-smooth, so regularization is potentially be neficial. In order to estimate the correct regularization magnitude, we again esimated a PoE model as i n equation (8) with square W , such that Z ( w ) = | det W | and computed the log-likelihood of 10.000 random patches un der different regularization levels. We found that  X   X   X  10  X  5 for maximum likelihood (see figure 2d). This value is lower than for MNIST data since natural image data is  X  X ess unsmooth X . Subsequently, a convolutional PoE model known as Fields-of-Experts [21] (F oE) was estimated using regularized SM: where p runs over image positions, and x ( p ) is a square image patch at p . The first model has the same architecture as the CD-1 trained model in [21]: 5  X  5 receptive fields, 24 experts ( M = 24 ), and  X  i and g ( . ) as in our PoE model. Note that qualitative results of a simila r model estimated with SM have been reported earlier [15]. We found that for bes t performance, the model is learned on images  X  X hitened X  with a 5  X  5 Laplacian kernel. This is approximately equivalent to ZCA whitening used in [15].
 Models are evaluated by means of Bayesian denoising using maximum a posteriori (MAP) estima-tion. As in a general Bayesian image restoration framework, the goal is to estimate the original sumption is white Gaussian noise such that the likelihood is p ( y | x )  X  N ( 0 ,  X  2 I ) . The model E ( x ; w ) =  X  log p ( x ; w )  X  Z ( w ) is our prior. The gradient of the log-posterior is: Denoising is performed by initializing x to a noise image, and 300 subsequent steps of steepest parison, we ran the same denoising procedure with models est imated by CD-1 and Basis Rotation, from [21] and [23] respectively. Note that the CD-1 model is t rained using PCA whitening. The CD-1 model has been extensively applied to denoising before [21] and shown to compare favourably to specialized denoising methods.
 Results. Training of the convolutional model took about 1 hour on a 2Gh z machine. Regularization turns out to be important for optimal denoising (see figure 2[ e-g]). See table 1 for denoising perfor-mance of the optimal model for specific standard images. Our m odel performed significantly better Figure 2: (a) Top: selection of downsampled MNIST datapoints. Middle and bottom: random sample of filters from unregularized and regularized (  X  = 0 . 01 ) models, respectively. (b) Average for maximum likelihood and optimal classification. (c) Test set error of a logistic regression classifier learned on top of features, with only 100 samples per class, f or different choices of  X  . Optimal error rates of SESM and RBM (figure 1a in [20]) are shown for comparis on. (d) Log-likelihood of 10.000 random natural image patches for complete model, for differ ent choices of  X  . (e-g) PSNR of 500 denoised images, for different levels of noise and choices o f  X  . Note that  X   X   X  10  X  5 , both for maximum likelihood and best denoising performance. (h) Some natural images from the Berkeley dataset. (i) Filters of model with 5  X  5  X  24 weights learned with CD-1 [21], (j) filters of our model with 5  X  5  X  24 weights, (k) random selection of filters from the Basis Rotation [23] mode l with 15  X  15  X  25 weights, (l) random selection of filters from our model with 8  X  8  X  64 weights. (m) Detail of original Lena image. (n) Detail with noise added (  X  noise = 5 / 256 ). (o) Denoised with model learned with CD-1 [21], (p) Basis Rotation [23], (q) and Score Matching with (near) optimal regularization. than the Basis Rotation model and slightly better than the CD -1 model. As reported earlier in [15], we can verify that the filters are completely intuitive (Gabo r filters with different phase, orientation and scale) unlike the filters of CD-1 and Basis Rotation model s (see figure 2[i-l]).
 Table 1: Peak signal-to-noise ratio (PSNR) of denoised imag es with  X  noise = 5 / 256 . Shown errors are aggregated over different noisy images.
 4.3 Super-resolution In addition, models are compared with respect to their perfo rmance on a simple version of super-resolution as follows. An original image x orig is sampled down to image x small by averaging blocks of 2  X  2 pixels into a single pixel. A first approximation x is computed by linearly scaling up x small and subsequent application of a low-pass filter to remove fal se high frequency information. The image is than fine-tuned by 200 repetitions of two subsequent steps: (1) refining the image slightly using x  X   X  x +  X   X  x E ( x ; w ) with  X  annealed from 2 10  X  2 to 5 10  X  4 ; (2) updating each k  X  k block of pixels such that their average corresponds to the do wn-sampled value. Note: the simple block-downsampling results in serious aliasing artifacts in the Barbara image, so the Castle image is used instead.
 Results. PSNR values for standard images are shown in table 2. The cons idered models made give slight improvements in terms of PSNR over the initial soluti on with low pass filter. Still, our model did slightly better than the CD-1 and Basis Rotation models.
 We have shown how the addition of a principled regularizatio n term to the expression of the Score Matching loss lifts continuity assumptions on the data dens ity, such that the estimation method becomes more generally applicable. The effectiveness of th e regularizer was verified with the dis-continuous MNIST and Berkeley datasets, with respect to lik elihood of test data in the model. For both datasets, the optimal regularization parameter is app roximately equal for both likelihood and subsequent classification and denoising tasks. In addition , we showed how computation and differ-entiation of the Score Matching loss can be automated using a n efficient algorithm.
