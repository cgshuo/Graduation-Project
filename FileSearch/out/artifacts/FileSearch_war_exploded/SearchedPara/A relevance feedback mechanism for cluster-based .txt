 1. Introduction
Relevance feedback is the mechanism whereby the relevance judgements made on a number of documents returned as the result of the query, are used to modify the query, with the goal that the modified query will prove more effective in returning relevant documents ( Baeza-Yates &amp; Ribeiro-Neto, 1999 ). Typically, query modification will result in the reweighting of query terms or the expansion of the query with new terms. It is possible for this process to go through a number of iterations. Relevance feedback has a long tradition of research and a number of methods have been developed both for both classical vector space and probabi-listic models ( Harman, 1992; Salton &amp; Buckley, 1990 ). Relevance feedback has been effective in improving the effectiveness of queries however there are some limitations to its use, as users have to provide a sufficient num-ber of relevance judgements in order to improve search effectiveness ( Iwayama, 2000 ). In real world interactive systems such as Internet or enterprise search, users are often unlikely to provide relevance feedback beyond a very small number of judgements. As such, it is important that a feedback mechanism can be effective with only a few relevance judgements.

There are approaches to relevance feedback which circumnavigate the necessity for users to make relevance judgements. Global analysis approaches take the approach that the query can be expanded using words or phrases with similar meaning to the query terms. This can be carried out by using a general thesaurus, although for this to be effective a manual thesaurus has to be constructed for each specific document domain.
Most work in this area tends to focus on statistical methods to automatically generate a thesaurus based either on the co-occurrence of terms or through the analysis of grammatical relations. Automatic local analysis alter-natively (also referred to as pseudo-relevance or blind feedback) assumes that the top M retrieved documents are relevant to the query ( Mitra et al., 1998; Xu &amp; Croft, 1996 ). More sosphicated implicit feedback mecha-nisms unobtrusively monitor the user search behaviour. In this case, implicit interest indicators or metrics are used to determine whether a user found a document interesting ( ClayPool, Le, Waseda, &amp; Brown, 2001 ). Such indicators can include actions such as book-marking a page but they can also use temporal measures such as how long a document was displayed for ( Kelly &amp; Belkin, 2004 ). White, Jose, and Ruthven (2006) have devel-oped a mechanism for implicit relevance feedback based on relevance paths which allows for adaptive search.
An alternative approach to improve retrieval effectiveness is by means of cluster-based retrieval, whereby a query returns a ranked list of clusters rather than documents ( Cutting, Karger, Pedersen, &amp; Tukey, 1992;
Hearst &amp; Pedersen, 1996 ), or the ranked list of documents is reordered based on the inferred clustering infor-mation ( Kurland &amp; Lee, 2004 ). Cluster-based retrieval follows from the cluster hypothesis ( Van Rijsbergen, 1979 ) which states that  X  X  X losely associated documents tend to be relevant to the same requests X  X . This hypoth-esis has been supported by a number of subsequent studies ( Hearst &amp; Pedersen, 1996; Liu &amp; Croft, 2004 ). A key failing in document based retrieval systems is often the lack of complete coverage of all aspects of a given information need ( Buckley, 2004 ). Cluster-based retrieval has the key advantage that semantically related doc-uments can be discovered in clusters that are relevant to the information need, but yet share no terms with the query, thus providing potentially a wider aspect coverage.

There are two general distinct methods for cluster-based retrieval, either the document corpus in its entirety is clustered independent of user queries ( static clustering ), or clustering is performed dynamically on a set of documents deemed relevant to the query ( query-specific clustering ). Query-specific clustering may use the query itself to influence the clustering process, in which case the clustering is query biased ( Iwayama, 2000 ). Query specific clustering is usually preferred, as static clustering requires a technique that is not only effective but computationally inexpensive.

Document clustering has traditionally made the distinction between partitional or hierarchical clustering approaches, however recently Zhong and Ghosh (2003) made perhaps the more important distinction between discriminative (distance or similarity based) or generative (model based) approaches, where the data is assumed to be generated by a mixture of parametric models, e.g. spherical Gaussians in k -means. In the for-mer area, an important direction of research has focused on similarity-based distributional approaches to doc-ument and feature clustering ( Dhillon, Mallela, &amp; Kumar, 2003; Tishby, Pereira, &amp; Bialek, 1999 ), of which perhaps the largest body of work focuses on the Information Bottleneck method ( Bekkerman, El-Yaniv, Tishby, &amp; Winter, 2003; Slonim, Friedman, &amp; Tishby, 2002; Slonim &amp; Tishby, 2000; Tishby et al., 1999 ).
Dobrynin, Patterson, and Rooney (2004) introduced a distributional document clustering technique, referred to as contextual document clustering (CDC) based on the concept that the entire document collection can be partitioned in a scaleable fashion, based on the identification of a large number of highly specific themes where each identified theme forms a seed to cluster documents. The effectiveness of this static based approach for cluster-based retrieval was demonstrated in Dobrynin, Patterson, Galushka, and Rooney (2005) .
In this paper, we propose a mechanism to improve the effectiveness of static cluster-based retrieval, using an explicit relevance feedback method requiring only a few relevance judgments. This is different to the approach presented by Iwayama (2000) who showed that the effectiveness of document retrieval could be improved using a relevance feedback mechanism based on query specific clustering. The cluster-based retrieval mecha-nism used in this paper is based on CDC. In this case, both documents and queries are represented as a prob-ability distribution of terms. A query produces a list of all clusters in the collection, ranked in order of their relevancy to the query. Relevance feedback involves modifying the term probabilities taking into consider-ation the probability distribution of terms for documents that are considered relevant to the query. Relevant documents are discovered through an explicit mechanism where the user identifies them through browsing of the original cluster results. The modified query produces a re-ranking of the list of retrieved clusters which in turn improves the effectiveness of the query result, as measured by the integrated measure ( F -measure) of pre-cision and recall ( Van Rijsbergen, 1979 ). In the next section we outline in detail how CDC identifies themes and forms clusters, how cluster-based retrieval is carried out and how queries are modified using explicitly judged relevant documents. 2. Methodology
X is the set of documents in a given document corpus and Y is the set of distinct words/terms in the selfsame corpus. Variable X represents a random variable with possible values, X = x , x 2 X and Y represents a random variable with possible values, Y = y , y 2 Y . Let C be the set of clusters and C represent a random variable with possible values, C = c , c 2 C . CDC represents each document as a probability distribution of terms, p ( Y j x ) where the probability of term y given x is estimated by a narrow theme term z , a process described in full in Dobrynin et al. (2004, 2005) . In this paper, we summarize the most pertinent details. A theme is a probability distribution p ( Y j z ) where z is the theme term. Narrow themes informally allow related documents (according to the implicit theme) to be grouped together in a cluster.
The identification of narrow theme terms is based on a consideration of the document frequency of the term, higher entropy. To take into account the dependency between the document frequency df( z ) of the term z and the entropy of its theme, we divide theme terms into disjoint subsets, based on their document frequency:
In Rooney, Patterson, Galushka, and Dobrynin (in press) , we describe this heuristic process of how many doc-ument frequency intervals to use and how their interval bounds are set. However in this study, we decided to alter this process slightly by excluding terms from consideration that occur too infrequently or too frequently across documents. Removing such terms has a long history in information retrieval ( Luhn, 1958 ), and deter-mining whether terms were based as too  X  X  X nfrequent X  X  or too  X  X  X requent X  X  is arbitrary and often carried out based on trial and error ( Van Rijsbergen, 1979 ). In our process for removing terms, we ranked each term by its document frequency and excluded from consideration the bottom 90% of terms and the top 0.05% of terms. The remaining terms we divided into five document frequency intervals, where the interval bounds df i were set so that the number of terms in each interval was equal.

We assume in total there are N narrow theme terms. For every i =1, ... , r , a set Z and
As such we select from each subset Y i , a proportionate number (proportional to the size of subset j Y to the size of the union of all subsets Y 0 ) of theme terms that fall in this interval and which have the lowest entropy. Clustering then follows a single pass process of assigning a document to a cluster based on the distance, in distributional terms, between itself and the cluster X  X  narrow theme, i.e., this is a hard clustering approach. Distance between a document x and the theme for the term z is calculated by the Jensen X  X hannon-divergence, between the document X  X  probability distribution and the theme X  X  distribution.

A document x is therefore assigned to a cluster with narrow theme z if
We consider the following mechanism for cluster-based retrieval: clusters are retrieved based on the distance of a cluster theme to a given query q . This distance is measured by the Jensen X  X hannon divergence between the query X  X  probability distribution and the theme X  X  distribution:
Cluster-based retrieval returns a sequence of document clusters  X  c order of divergence. However, only the first i max clusters are considered in terms of evaluating the mechanism.
For a given query, a subset R of documents in the collection have been identified by experts, as being relevant to the query. Let the coverage of the sub-sequence of i clusters  X  c
We assess the precision and recall of the clusters for a given query as follows. The precision of i retrieved clus-ters is ment) and 0 otherwise. The recall for c 1 , ... , c i is defined as being equal to the coverage.
The precision and recall of the sub-sequence of clusters  X  c Two conditions are considered for the value of i max .

Condition 1. i max is the value required for cov  X  c 1 ; ... ; c
Condition 2. It may be that i max as defined in Condition 1 requires inspecting a relatively large number of clusters for certain queries. In a real world system, a user is unlikely to look beyond the top N in this condition, i max = N c , where N c is a small fixed size constant.

The F -measure provides an integrated measure for accuracy, by taking the harmonic mean of the precision and recall, and determining the maximum F -measure as F max  X  q ; i  X  X  max
Given a set of queries Q , q 2 Q , the average F -measure F
Relevance feedback operates by modifying the probability distribution for query terms allowing for the fact that k documents have been judged as relevant to query q . We consider four approaches to query modification, based on different hypotheses: Query modification method 1 (QMM1) H1: The initial query and known relevant documents are considered as being equally important.

This results in the following re-weighting: Query modification method 2 (QMM2) H2: The initial query and the summation of known relevant documents are considered equally important.
This results in the following re-weighting: Query modification method 3 (QMM3)
H3: Known relevant documents are considered marginally important to query modification. Query modification method 4 (QMM4)
H4: The initial query is considered only as marginally important to query modification. 3. Experimental setup
Experiments were carried out on the TREC-9 filtering task derivative of the OHSUMED collection ( Rob-ertson &amp; Hull, 2000 ), first presented to the research community by Hersh, Buckley, Leone, and Hickman (1994) . This collection was chosen as it contains a number of predefined queries, where each query has a num-ber of documents from the collection identified by an expert as relevant to the query. The OHSUMED col-lection consists of more than 300,000 MEDLINE references collated from 270 medical journals published in the years 1987 to 1991. 233,445 references have each a title, a set of expert assigned MeSH descriptors and an abstract. We only used the abstract and title part of references that have both to discover themes and cluster documents. Document abstracts were parsed to remove stop words, and Porter stemming was applied. We tried to identify 2000 narrow themes, this resulted in 1996 non-empty clusters. MeSH descriptors were not used in the clustering of the document collection, and, as such, the approach is purely unsupervised.
In TREC-9 there were 63 queries, each consisting of a title and description field. We excluded two of these queries as there are insufficient relevant documents in the collection, to properly evaluate them. Only the description field was used in the query. The remaining 61 queries were stemmed in a similar fashion to doc-uments. For Condition 2 , N c was set to 10, so that only a relatively small number of clusters need be consid-ered, and any relatively small value would haven been appropriate. We calculated the F modification technique in the presence of k =0, ... ,7 relevant documents and compared it to a baseline where no query modification occurred. The k relevant documents were excluded from the evaluation of F each method, so as not to bias the F max,av results for retrieval. However a given query searched for j R j k documents, where R is the set of all documents in the collection relevant to the query and k is the number of explicitly relevant documents chosen from R by order of their document number. Therefore F for the baseline, is not necessarily constant over all values of k . We compared F and the baseline at a given k value, for significant difference using the Wilcoxon Signed-Rank Test. We used the same test to compare F max,av for the same QMM technique but for different values of k . 3.1. Experimental results
Fig. 1 shows F max,av for each QMM, for k =0, ... ,7 in comparison to the baseline. For k = 0, no query modification occurs and the value shown for each technique refers to the original F queries.

The first observation that can be made from Fig. 1 , is that relevance feedback for the query modification methods always improves upon the baseline, except in the case of QMM3, which as it provides little weighting to relevant documents, is practically equivalent to the baseline. Fig. 1 shows clearly that even with only 1 rele-vant document ( k = 1), the improvement in F max,av , for QMM1 and QMM2, over the baseline, showed a dra-matic and significant increase of 0.154 or 33% ( p = 0.001) (for k = 1, the improvement for both of these techniques is exactly the same). This value increases slightly with QMM2 as more documents are considered as relevant within the feedback mechanism. The maximum difference between QMM2 and the baseline occurs when k = 6 and the difference at this point is 0.188. QMM1 follows a similar pattern but one, which appears to be slightly, lower than QMM2. As the difference between QMM1 and QMM2 is not significant at any k value, we therefore can conclude that both these feedback mechanism are the same in terms of their improvements over the baseline. It should be noted that the most dramatic increase is with one relevant document. Subse-quent documents improve F max,av only slightly. QMM4 shows a slightly different profile in that it provides a slower rate of improvement to the feedback documents when k is less than 3. For example at k =2,
QMM2 has an improvement over QMM4 of 0.027 ( p = 0.01). Beyond k = 2, there was no significant differ-ence between F max,av for QMM4 and F max,av for QMM1 and QMM2. QMM4 also has a maximum improve-ment over the baseline at k = 6, with a value of 0.176.

Therefore we can conclude from comparing QMM1 and QMM2 that it is important to give the relevant documents sufficient weighting during feedback but the level of weighting is relatively flexible. If we consider
QMM4 in relation to QMM1 and QMM2, it can be concluded that giving the initial query sufficient weighting is also important, especially when only considering a small number of relevant documents (less than 3 in this query becomes subsumed by information from the feedback documents and it is this information that domi-nates the quality of the modified query.

Having examined the respective feedback mechanisms with respect to the baseline we now focus on exam-ining the effects that each additional feedback document has on the feedback system performance. We focus on QMM2 as this provides optimal performance. Table 1 provides an understanding of how each additional feedback document affects F max,av for this particular feedback mechanism. Increases between F different k values are shown in the right hand column. Significant increases are shown in bold.
From Table 1 it can be seen that the improvement in F max,av significant and large improvement of 0.149 ( p = 0.001). If we consider the improvement provided by 2 feed-back documents over 1 we see that there is a small and non-significant increase of 0.017. Similarly the improvement provided by 3 feedback documents over 1 is small and not significant and it is only when we compare the difference between 4 feedback documents and 1 that we see a significant improvement in F of 0.022 ( p = 0.02). It should be noted that although the difference is significant, it is much smaller than the improvement provide by the difference between 0 and 1. Therefore, there is a small gain associated with identifying 4 relevant documents as opposed to 1. Similarly, if we compare the differences in improvement between 2 and 3, 2 and 4, 2 and 5, and 2 and 6 relevant documents, no significant improvement is observed until 6 relevant documents are utilized. Again the actual improvement achieved is very small and the signif-ment between 3 relevant documents and 4, 5, 6 and 7 relevant documents no significant improvement is ever obtained.

From this we can conclude that utilizing 3 feedback documents is optimal as no further significant improve-ments are obtained. It should also be noted that the value of using more than 1 feedback document is debat-able as the difference between using 1 and 3 was not significant. These findings advocate using considerably fewer feedback documents than Iwayama (2000) found in a study into the effect of relevance judgements on query specific cluster-based retrieval. The latter author showed a more gradual increase in average preci-sion-recall from k = 0 to 10 (although it should be clarified that this was on different data set than used in this study).

Fig. 2 shows the comparison of the QMMs to the baseline for Condition 2 . Overall the pattern of behaviour of the QMMs in Fig. 2 shows a similar pattern to that in Fig. 1 . Again, it only takes one relevant document for
QMM1 and QMM2 to improve F max,av by almost 38%, over the baseline, with a significant increase in F of 0.164 ( p = 0.001). QMM1, QMM2, QMM4 reach a maximum absolute F over the baseline of 0.187, 0.196, 0.177. QMM1 and QMM2 both show a significant improvement over
QMM4 for k = 1 and k = 2. The pattern for QMM4 shows that excluding the initial query does not preclude an improvement in performance for k = 1, but it requires a value of k = 3, for the technique to match QMM1 and QMM2. QMM3 again shows little improvement over the baseline.

One point of difference is that QMM1-4 and the baseline give slightly higher values of F 1 than Condition 2 , for example, F max,av is 0.026 higher for QMM1 at k = 6, for Condition 1 than for Con-dition 2 . This is not unexpected as more clusters may be considered for relevancy under Condition 1 . However the fact that the results for Condition 1 only slightly outperform Condition 2 , indicates the quality of the CDC cluster-based retrieval mechanism, in that to maximize F max,av beyond the first 10 clusters.

In summary, query refinement improves retrieval, which in itself may not be a significant finding but the fact that utilising only a few documents can have such a large impact is particularly important. Other findings include, if only a few relevant documents are employed in the process, it is important to give the initial query sufficient weighting and only considering the first 10 clusters for relevancy ( Condition 2 ) is a good approxima-tion to considering as many clusters as required to maximize the recall ( Condition 1 ). 4. Conclusions
We have demonstrated that cluster-based retrieval based on a distributional approach to thematic cluster-ing and query similarity, can be improved upon considerably using a query refinement technique. This tech-nique is based on explicit relevance feedback requiring only very few relevance judgments and was compared to a baseline retrieval without any relevance feedback. It required only one relevant document with adequate term re-weighting in the query modification to significantly improve the effectiveness of the retrieval process by 33%. When only a few documents are considered for query refinement, methods which give sufficient weight-ing to the initial query, maximized the results. No further significant improvement was shown after consider-ing 3 or more documents. This was the case even when only the first 10 clusters in the retrieval list were assessed ( Condition 2 ), and in fact there was very little reduction in performance for this situation, compared with the situation where no such restriction was placed on the number of clusters considered ( Condition 1 ).
This has implications for real-world enterprise search systems. Firstly the CDC approach is a linear time based algorithm ( Rooney et al., in press ) and therefore suitable for the search needs of a modern enterprise, sec-ondly, it facilitates a realistic approach for query modification that requires only 1 relevant document to be identified by a user in order to achieve large performance improvements and thirdly we have shown that only 10 clusters need to be inspected for relevancy to yield close to optimal performance.
 References
