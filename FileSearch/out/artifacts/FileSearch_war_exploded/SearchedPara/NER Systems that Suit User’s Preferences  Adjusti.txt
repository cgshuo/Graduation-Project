 einat,rcwang@cs.cmu.edu Named entity recognition (NER) is the task of iden-tifying named entities in free text X  X ypically per-sonal names, organizations, gene-protein entities, and so on. Recently, sequential learning methods, such as hidden Markov models (HMMs) and con-ditional random fields (CRFs), have been used suc-cessfully for a number of applications, including NER (Sha and Pereira, 2003; Pinto et al., 2003; Mc-callum and Lee, 2003). In practice, these methods provide imperfect performance: precision and re-call, even for well-studied problems on clean well-written text, reach at most the mid-90 X  X . While performance of NER systems is often evaluated in terms of F 1 measure (a harmonic mean of preci-sion and recall), this measure may not match user preferences regarding precision and recall. Further-more, learned NER models may be sub-optimal also in terms of F1, as they are trained to optimize other measures (e.g., loglikelihood of the training data for CRFs).

Obviously, different applications of NER have different requirements for precision and recall. A system might require high precision if it is designed to extract entities as one stage of fact-extraction, where facts are stored directly into a database. On the other hand, a system that generates candidate ex-tractions which are passed to a semi-automatic cu-ration system might prefer higher recall. In some domains, such as anonymization of medical records, high recall is essential.

One way to manipulate an extractor X  X  precision-recall tradeoff is to assign a confidence score to each extracted entity and then apply a global threshold to confidence level. However, confidence thresholding of this sort cannot increase recall. Also, while confi-dence scores are straightforward to compute in many classification settings, there is no inherent mecha-nism for computing confidence of a sequential ex-tractor. Culotta and McCallum (2004) suggest sev-eral methods for doing this with CRFs.

In this paper, we suggest an alternative simple method for exploring and optimizing the relation-ship between precision and recall for NER systems. In particular, we describe and evaluate a technique called  X  X xtractor tweaking X  that optimizes a learned extractor with respect to a specific evaluation met-ric. In a nutshell, we directly tweak the threashold term that is part of any linear classifier, including se-quential extractors. Though simple, this approach has not been empirically evaluated before, to our knowledge. Further, although sequential extractors such as HMMs and CRFs are state-of-the-art meth-ods for tasks like NER, there has been little prior re-search about tuning these extractors X  performance to suit user preferences. The suggested algorithm op-timizes the system performance per a user-provided evaluation criterion, using a linear search procedure. Applying this procedure is not trivial, since the un-derlying function is not smooth. However, we show that the system X  X  precision-recall rate can indeed be tuned to user preferences given labelled data using this method. Empirical results are presented for a particular NER task X  X ecognizing person names, for three corpora, including email and newswire text. Learning methods such as VP-HMM and CRFs op-timize criteria such as margin separation (implicitly maximized by VP-HMMs) or log-likelihood (ex-plicitly maximized by CRFs), which are at best indi-rectly related to precision and recall. Can such learn-ing methods be modified to more directly reward a user-provided performance metric?
In a non-sequential classifier, a threshold on confi-dence can be set to alter the precision-recall tradeoff. This is nontrivial to do for VP-HMMs and CRFs. Both learners use dynamic programming to find the label sequence y = ( y sequence x = ( x the function W  X  P the learned weight vector and f is a vector of fea-tures computed from x , i , the label y previous label y the most likely state sequence, and does not output probability for a particular sub-sequence. (Culotta and McCallum, 2004) suggest several ways to gen-erate confidence estimation in this framework. We propose a simpler approach for directly manipulat-ing the learned extractor X  X  precision-recall ratio. We will assume that the labels y include one label O for  X  X utside any named entity X , and let w weight for the feature f If no such feature exists, then we will create one. The NER based on W will be sensitive to the value of w programming method to label tokens as inside enti-ties, and large positive values will force it to label
We thus propose to  X  X weak X  a learned NER by varying the single parameter w to optimize some user-provided performance metric. Specifically, we tune w line search, where the objective function is itera-the search when two adjacent evaluation results are
A variety of performance metrics might be imag-ined: for instance, one might wish to optimize re-call, after applying some sort of penalty for pre-cision below some fixed threshold. In this paper we will experiment with performance metrics based on the (complete) F-measure formula, which com-bines precision and recall into a single numeric value based on a user-provided parameter  X  :
A value of  X  &gt; 1 assigns higher importance to re-call. In particular, F as precision. Similarly, F as much as recall.

We consider optimizing both token-and entity-level F tracted entities and no credit for incorrect entity boundaries, respectively. Performance is optimized over the dataset on which W was trained, and tested on a separate set. A key question our evaluation should address is whether the values optimized for the training examples transfer well to unseen test ex-amples, using the suggested approximate procedure. 3.1 Experimental Settings We experiment with three datasets, of both email and newswire text. Table 1 gives summary statis-tics for all datasets. The widely-used MUC-6 dataset includes news articles drawn from the Wall Street Journal. The Enron dataset is a collection of emails extracted from the Enron corpus (Klimt and Yang, 2004), where we use a subcollection of the mes-sages located in folders named  X  X eetings X  or  X  X al-endar X . The Mgmt-Groups dataset is a second email collection, extracted from the CSpace email cor-pus, which contains email messages sent by MBA students taking a management course conducted at Carnegie Mellon University in 1997. This data was split such that its test set contains a different mix of entity names comparing to training exmaples. Fur-ther details about these datasets are available else-where (Minkov et al., 2005).
We used an implementation of Collins X  voted-percepton method for discriminatively training HMMs (henceforth, VP-HMM) (Collins, 2002) as well as CRF (Lafferty et al., 2001) to learn a NER. Both VP-HMM and CRF were trained for 20 epochs on every dataset, using a simple set of features such as word identity and capitalization patterns for a window of three words around each word being clas-sified. Each word is classified as either inside or out-3.2 Extractor tweaking Results Figure 1 evaluates the effectiveness of the optimiza-tion process used by  X  X xtractor tweaking X  on the Enron dataset. We optimized models for F different values of  X  , and also evaluated each op-timized model with different F graph shows the results for token-level F bottom graph shows entity-level F graph illustates that the optimized model does in-deed roughly maximize performance for the target  X  value: for example, the token-level F the model optimized for  X  = 0 . 5 indeed peaks at  X  = 0 . 5 on the test set data. The optimization is first, there are differences between train and test sets; in addition, the line search assumes that the perfor-mance metric is smooth and convex, which need not be true. Note that evaluation-metric optimiza-tion is less successful for entity-level performance, which behaves less smoothly than token-level per-formance.
 Similar results were obtained optimizing baseline CRF classifiers. Sample results (for MUC-6 only, due to space limitations) are given in Table 2, opti-mizing a CRF baseline for entity-level F as  X  increases, recall monotonically increases and precision monotonically falls.

The graphs in Figure 2 present another set of re-sults with a more traditional recall-precision curves. The top three graphs are for token-level F mization, and the bottom three are for entity-level optimization. The solid lines show the token-level and entity-level precision-recall tradeoff obtained by varying 6  X  and optimizing the relevant measure for F and recall in token and entity level of the baseline model, learned by VP-HMM. These graphs demon-strate that extractor  X  X weaking X  gives approximately smooth precision-recall curves, as desired. Again, we note that the resulting recall-precision trade-off for entity-level optimization is generally less smooth. We described an approach that is based on mod-ifying an existing learned sequential classifier to change the recall-precision tradeoff, guided by a user-provided performance criterion. This approach not only allows one to explore a recall-precision tradeoff, but actually allows the user to specify a performance metric to optimize, and optimizes a learned NER system for that metric. We showed that using a single free parameter and a Gauss-Newton line search (where the objective is itera-tively approximated by quadratics), effectively op-timizes two plausible performance measures, token-level F fact general, as it is applicable for sequential and/or structured learning applications other than NER.
