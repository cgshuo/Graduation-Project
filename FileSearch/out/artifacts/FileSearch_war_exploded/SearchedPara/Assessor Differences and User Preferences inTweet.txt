 In information retrieval evaluation, when presented with an effectiveness difference between two systems, there are three relevant questions one might ask. First, are the differences statistically significant? Second, is the comparison stable with respect to assessor differences? Finally, is the differ-ence actually meaningful to a user? This paper tackles the last two questions about assessor differences and user prefer-ences in the context of the newly-introduced tweet timeline generation task in the TREC 2014 Microblog track, where the system X  X  goal is to construct an informative summary of non-redundant tweets that addresses the user X  X  informa-tion need. Central to the evaluation methodology is human-generated semantic clusters of tweets that contain substan-tively similar information. We show that the evaluation is stable with respect to assessor differences in clustering and that user preferences generally correlate with effectiveness metrics even though users are not explicitly aware of the semantic clustering being performed by the systems. Al-though our analyses are limited to this particular task, we believe that lessons learned could generalize to other eval-uations based on establishing semantic equivalence between information units, such as nugget-based evaluations in ques-tion answering and temporal summarization.
 Categories and Subject Descriptors : H.3.4 [Information Storage and Retrieval]: Systems and Software X  X erformance evaluation Keywords: TREC evaluation; microblog search; user study
In response to information needs, search systems should strive to return as much relevant content as possible. How-ever, there are other factors that systems might consider beyond topical relevance: often, users may not wish to see multiple pieces of text that  X  X ay the same thing X ; frequently, users desire diverse results that cover multiple aspects of the topic they are interested in. These considerations are partic-ularly important in the context of searching tweets on Twit-ter. Due to the nature of the medium, there are frequently duplicate, near-duplicate, and highly-similar posts X  X or ex-ample, a breaking news event may be reported by multiple outlets at roughly the same time, each using slightly different language. In turn, these posts are retweeted, sometimes with additional commentary, copy-and-pasted, and discussed as part of a global conversation. This creates a cacophony of voices that obscures the underlying information content.
The Microblog track at TREC began in 2011 to explore information retrieval challenges in the context of social me-dia services such as Twitter. The main task since 2011 has been temporally-anchored ad hoc retrieval ( X  X t time T , give me the most relevant tweets about an information need ex-pressed as query Q  X ). In response to the considerations above, the tweet timeline generation (TTG) task was in-troduced in 2014, where the goal is to construct a  X  X imeline X  of relevant and non-redundant tweets that best addresses the user X  X  information need.

This paper presents a meta-evaluation of the TTG task from two perspectives: First, do assessor differences affect evaluation stability? Ultimately, TTG evaluation boils down to judgments about the semantic content of tweets, for which we would expect inter-assessor disagreements. Do these dif-ferences prevent us from drawing conclusions about the rel-ative effectiveness of systems? Second, do user preferences correlate with our evaluation metrics? That is, are our met-rics meaningful in being able to capture aspects of what users care about in timelines for making system comparisons? Our major findings are summarized as follows:  X  We find that assessors do exhibit substantial differences in semantic clustering. However, these differences do not impact the stability of system comparisons.  X  We find that user preferences correlate with metric dif-ferences, and for precision and unweighted F 1 , agreement increases as the magnitude of the difference increases.
Although this work focuses on a specific TREC task, tweet timeline generation is representative of a class of information retrieval evaluations based on identifying atomic units of information and establishing semantic equivalences between these units. Thus, we believe that lessons learned from our study can be applied to similar types of evaluations.
At a high level, the tweet timeline generation task bears a family resemblance to topic detection and tracking (TDT) [33, 3],  X  X ther X  nuggets in question answering [32, 9, 8], nugget-based evaluations in DARPA X  X  BOLT program, 1 and temporal summarization at TREC [12, 5]. These tasks all share the insight that atomic units of information should be grouped together into semantic equivalence classes to pro-vide more useful responses to users (we refer to this gener-ically as  X  X lustering X ). In TDT the atomic units were doc-uments, which were grouped into those discussing the same event. For question answering, nuggets were short phrases that provided interesting information about a target entity. In temporal summarization, sentences formed the atomic units that conveyed essential information about a particular event. These notions are also reflected in variants of ad hoc retrieval such as aspect retrieval [21] (or, alternatively, facet or sub-topic retrieval [34]) and result diversification [1, 28], although less explicitly.

The two biggest challenges of formulating such an eval-uation are (1) defining the atomic unit of information and (2) defining semantic equivalence between those units. Once these two issues are resolved, computing a metric is reason-ably straightforward, although variations abound. The first issue is more difficult than it initially seems if the atomic unit does not correspond to a logical entity such as a document. Experience from question answering evaluations has shown that users disagree about the granularity of nuggets X  X or ex-ample, whether a piece of text encodes one or more nuggets and how to treat partial semantic overlap between two pieces of text [20]. The solution that most evaluations adopt to-day is to simply declare the atomic information unit by fiat : in tweet timeline generation, tweets form the atomic units, and in temporal summarization [5], they are sentences. This sweeps many nuances under the rug, but yields workable evaluations in practice.

The second issue X  X emantic equivalence between atomic information units X  X s challenging because making such judg-ments requires taking into account context and fine-grained distinctions in meaning. In the same way that assessors dis-agree over relevance judgments (see [6] for a nice summary), humans also disagree about whether two pieces of text have the same semantic content. This issue is typically resolved by acknowledging these assessor differences and simply ac-cepting the opinion of a single assessor. In the same way that Voorhees [31] demonstrated the stability of system rankings with respect to assessors X  divergent document-level relevance judgments, the implicit assumption is that for these seman-tic clustering tasks, assessor differences don X  X  matter if our goal is to induce a set of system comparisons.

To our knowledge, however, this assumption has never been validated at scale. For example,  X  X round truth X  nuggets for question answering evaluations were generated by a sin-gle assessor (for each entity) [32, 9]. 2 Thus, it is unclear if an alternative set of nuggets (i.e., generated by another as-sessor) would have altered system rankings. A more recent iteration of the nugget-based evaluation methodology [22] has not to our knowledge examined assessor differences in the  X  X uggetization X  process either. One of the major contri-butions of this paper is that, for tweet timeline generation, we devoted the resources necessary to answer this question by procuring two independent sets of semantic clusters for all topics in the test collection. Our results verify that sys-tem rankings are stable with respect to assessor differences in the TTG task, which gives us some confidence that the same results will carry over to similar tasks as well.
With respect to the second part of our paper X  X n ex-ploration of user preferences in tweet timeline generation X  there is a long history of research that examines the cor-relation between effectiveness metrics from system-oriented evaluations with task-based metrics from user-oriented eval-uations [13, 29, 4, 30, 14, 2, 26, 25, 27]. The broader goal of this thread of work is to understand when system effec-tiveness improvements are meaningful or useful in improving users X  information seeking abilities in practice. Early stud-ies suggest that  X  X etter X  systems (as measured by system-oriented metrics) don X  X  necessarily translate into better task performance [13, 29, 30]. Smith and Kantor [26] explained that users  X  X o well X  with poor search engines because they reformulate queries. Along these lines, Lin and Smucker [19] suggested that browsing can compensate for poor search re-sults. However, more recent work has been able to detect a correlation between various system effectiveness metrics and human preferences [2, 25] (by using larger sample sizes) and between precision and user performance [27] (by more carefully controlling the experimental setup). These results give the community some degree of confidence that system-oriented evaluations can guide progress in producing more useful systems.

Our study follows the general setup of Sanderson et al. [25], i.e., asking users which of two system outputs they prefer. There are, however, a few important differences. Whereas their work examined ad hoc retrieval, we explore the more complex task of tweet timeline generation. Whereas they re-stricted system comparisons to those with  X  X arge X  differences and  X  X mall X  differences in effectiveness, we sampled our com-parison conditions and analyzed the results in a way that allows us to characterize user sensitivity to different magni-tudes of differences. Finally, whereas Sanderson et al. used judgments from Amazon X  X  Mechanical Turk service, we took the route of training local assessors, Tweet timeline generation (TTG) was introduced at the TREC 2014 Microblog track to supplement the existing ad hoc retrieval task. The putative user model is as follows:  X  X t time T , I have an information need expressed by query Q , and I would like a summary that captures relevant in-formation. X  The system X  X  task is to produce a  X  X ummary X  timeline, operationalized as a list of non-redundant, chrono-logically ordered tweets. It is imagined that the user would consume the entire summary (unlike a ranked list, where the user might stop reading at any time).

We conceived of a reference architecture in which a TTG module processes the output of an ad hoc retrieval system to generate the timeline. Thus, tweet timeline generation introduces two additional challenges beyond ad hoc retrieval:  X  Systems must detect (and eliminate) redundant tweets.
This is equivalent to saying that systems must detect if a tweet contains novel information. Figure 1: Screenshot of the annotation interface. Tweets are presented one at a time in chronological order (bottom). For each tweet, the assessor can add it to an existing cluster or create a new cluster.  X  Systems must determine how many tweets to return. Some topics have more relevant and non-redundant tweets than others and a system must be able to automatically infer this. Systems can make different precision/recall tradeoffs along these lines.

We operationalized redundancy as follows: for every pair of tweets, if the chronologically later tweet contains substan-tive information that is not present in the earlier tweet, the later tweet is considered novel; otherwise, the later tweet is redundant with respect to the earlier one. In our definition, redundancy and novelty are antonyms, and so we use them interchangeably, but in opposite contexts.

Due to the temporal constraint, redundancy is not sym-metric. If tweet A precedes tweet B and tweet B contains substantively similar information found in tweet A , then B is redundant with respect to A , but not the other way around. We also assume transitivity. Suppose A precedes B and B precedes C : if B is redundant with respect to A and C is redundant with respect to B , then by definition C is redun-dant with respect to A . In this task setup, redundancy boils down to the definition of  X  X ontains substantively similar in-formation X , which is more precisely defined below.
The TTG definition of redundancy and the assumption of transitivity means that the task can be viewed as semantic clustering X  X hat is, we wish to group relevant tweets into clusters in which all tweets share substantively similar in-formation. Within each cluster, the earliest tweet is novel; all other tweets in the cluster are redundant with respect to all earlier tweets.
 Our annotation methodology builds exactly on this idea. We begin with a list of all relevant tweets, ordered chrono-logically, from earliest to latest. These tweets are presented, one at a time, to a human assessor. For each tweet, the asses-sor can add it to an existing cluster if she thinks the tweet contains substantively similar information with respect to tweets in the existing cluster, or she can create a new clus-ter for the tweet. We have developed a JavaScript-based annotation interface to help assessors accomplish this task. A screenshot is shown in Figure 1.

In the interface, the next tweet to be clustered is shown at the bottom of the screen. The assessor can either add the tweet to an existing cluster by clicking the  X  X dd X  button next to the cluster or create a new cluster by hitting the space bar. At any time, the assessor can expand a cluster to show all tweets contained in it, or collapse the cluster to show only the first tweet. The interface also implements an undo feature that allows the assessor to reverse the action taken and go back to the previous tweet.

The TTG evaluation methodology boils down to this cen-tral question: what exactly does  X  X ubstantively similar infor-mation X  mean? Like document relevance in ad hoc retrieval, assessors make the final determination and we expect natu-ral variations among humans. However, pilot studies helped us devise a set of guidelines, which were provided as instruc-tions to the assessors. We told them: a good rule of thumb is that if two tweets  X  X ay the same thing X , then they X  X e sub-stantively similar. To speed up the clustering process, the annotators were asked not to consider external content (e.g., follow links in the tweets).

To provide further guidance, we devised a few questions that the assessors might consider in determining whether two tweets should be in the same cluster:  X  If I had already seen the first tweet, would I have missed out on some information if I didn X  X  see the second tweet?  X  If two tweets are similar but the second contains an ad-dition to or endorsement of the first, is the addition or endorsement important enough that I would be interested in seeing both tweets?  X  Sometimes two tweets look similar but actually narrate the development of an event. Are the tweets different enough from each other that I would want to see both tweets to understand how an event develops or unfolds?
Since TTG was originally conceived as a stage following ad hoc retrieval, the track guidelines asked TTG partici-pants to also submit ad hoc runs. For the evaluation, NIST assessors developed 55 topics and used a standard pooling methodology to generate tweet-level relevance judgments; see the track overview for more details [18]. Tweets in the pool were assigned one of three judgments: not relevant, rel-evant, and highly relevant. In the cluster annotation pro-cess, assessors at UMD and UIUC worked on the relevant and highly-relevant tweets from the NIST judgment pools. Due to resource constraints, NIST assessors were not able to perform the clustering, and thus a weakness of our setup is that the individual with the information need was not the one who created the clusters. The two assessors at UMD were graduate students in computer science (both male). The two assessors at UIUC were graduate students in li-brary and information science (one male, one female). All were familiar with Twitter.

Assessors were first trained in the laboratory: the session included an introduction to the task and an overview of the annotation interface. After that, assessors were free to per-form annotations at their own pace on their own machines, at any location of their choosing. This was possible because the annotation interface was implemented in JavaScript and hence accessible over the web. All assessors began with a throwaway  X  X ractice topic X  (although they were not aware of the throwaway nature) and then proceeded to annotate topics in batches (roughly ten topics per batch). Topics were for all submitted runs in the TREC 2014 TTG task, overlaid with iso-F grouped into batches of roughly equal size (in terms of the number of relevant tweets) prior to the beginning of the an-notation process. When an assessor completed a batch, he or she could request another batch to work on.

Each site annotated the topic batches in the opposite or-der, and when a covering set for all topics had been ob-tained, we designated those clusters to be the  X  X fficial X  judg-ments. However, the annotation process continued until both sites had processed all topics, giving us alternate judg-ments. Thus, both the official and alternate clusters con-tained annotations generated from both sites.
The output of the human annotation process is an or-dered list of tweet clusters. Within each cluster, the tweets are sorted by temporal order (earliest to latest). The clus-ters themselves are sorted by the temporal order of their earliest tweet. Following the heuristic of using the most straightforward metric when defining a new task (and then subsequently refining the metric as needed), we decided to measure cluster-based precision and recall. The measure is cluster-based in the sense that systems only receive credit for returning one tweet from each cluster X  X hat is, once a tweet is retrieved, all other tweets in the cluster are automatically considered not relevant. From this, we can compute preci-sion, recall, and F-score in the usual way (lacking any basis for setting the  X  parameter, we simply computed F 1 ). Since the user model assumes that the user will consume the entire summary, set-based metrics seemed appropriate.

The only additional refinement is that we computed both weighted and unweighted variants of recall. In weighted re-call, each cluster is assigned a weight proportional to the sum of relevance grades from every tweet in the cluster (relevant tweets receive a weight of one and highly-relevant tweets re-ceive a weight of two). This weighting scheme implements the heuristic that larger clusters and those containing more highly-relevant tweets are more important, and the denom-inator in the weighted recall computation is the sum of all cluster weights. In unweighted recall, all clusters are consid-ered equally important, and the denominator is simply the total number of clusters.

Note that this setup gives equal credit to retrieving any tweet from a cluster. Intuitively, however, this seems overly simplistic X  X sers would certainly prefer seeing certain tweets over others, even if they contain substantively similar infor-mation [15]. For example, users might prefer the earliest tweet, a tweet from the most  X  X uthoritative X  user (e.g., a verified news account), or a tweet from someone close by in their network (e.g., a tweet from someone they follow). We currently do not have sufficient understanding to accu-rately model such preferences, and thus explicitly made the decision not to tackle this challenge.

The evaluation metrics for TTG represent straightforward extensions of previous work: aspect recall [21], sub-topic re-call [34], and the  X  X ugget pyramid X  approach from the TREC question answering evaluations [17]. Alternative metrics we had considered include those based on gain [7, 5] and the ex-tension of mean average precision to graded relevance judg-ments [24]. The challenge with gain-based approaches is the complex parameterization necessary to fully instantiate a particular model; we lacked the empirical data to prop-erly develop such a model. The graded extension to mean average precision is elegant, but our underlying user model is better captured by set-based metrics.

In total, 13 groups submitted 50 runs to the tweet timeline generation task at TREC 2014. Recognizing that systems make different choices with respect to balancing precision and recall, it is illustrative to visualize the tradeoffs in a scat-ter plot. Figure 2 shows precision vs. unweighted recall (left) and precision vs. weighted recall (right) for all runs. Iso-F contours are plotted in blue; points on the same contour line have the same F 1 score, but with different precision/recall tradeoffs. Note that the effectiveness of individual runs is not relevant for the purposes of our meta-analysis, so we re-fer readers to the track overview for additional details [18].
Our first research question revolves around assessor dif-ferences in the semantic clustering task for tweet timeline generation and their impact on evaluation stability. For the TREC 2014 evaluation, we devoted the necessary resources to generate two independent sets of reference clusters, which we refer to as the  X  X fficial X  and  X  X lternate X  judgments. Note that the official judgments were simply the ones obtained first and used to report evaluation results at TREC; there is no implication that they are somehow more  X  X uthoritative X . Figure 3: Visualization of the clusters for two topics.
We begin with a descriptive characterization of the seman-tic clusters generated by the assessors. In this and subse-quent analyses, relevant and highly-relevant tweets are both considered  X  X elevant X . Each topic (55 in total) contains 194 relevant tweets on average: the official judgments averaged 89 clusters per topic, while the alternate judgments aver-aged 73 clusters per topic. These differences suggest that humans perform the semantic clustering task at different levels of granularity.

In Figure 3 we attempt to visualize these differences. In each plot, a point represents a tweet, and its x -axis position denotes the time when it was posted. Tweets that were as-signed to the same cluster are at the same y -axis position and connected by horizontal lines. Thus, each horizontal  X  X evel X  represents a semantic cluster, ordered by the first tweet; each cluster is assigned a different color for clarity. On the left, we show topic 179  X  X are of Iditarod dogs X  and on the right, we show topic 225  X  X arbara Walters, chicken pox X . The top row shows the official judgments and the bottom row shows the alternate judgments. These topics were se-lected primarily for visual clarity: enough relevant tweets to show interesting clusters, but not too many relevant tweets as to be overly cluttered. From these visualizations, it is apparent that there are substantial assessor differences in the formation of the clusters. For example, the alternate as-sessor created fewer clusters for topic 179, and in topic 225, the alternate assessor did not seem to agree with two early clusters found by the official assessor.

To quantitatively characterize the differences between the official and alternate clusters, we computed the Adjusted Rand Index [23], which is a measure of similarity between two clusterings that is corrected for chance groupings (rang-ing from  X  1 to +1.). This metric has two other desirable properties: first, it is symmetric, which is appropriate since the official judgments aren X  X  any more  X  X orrect X  than the al-ternate judgments; second, it ignores permutations in that the similarity values don X  X  depend on the  X  X luster labels X  (which, in our case, are just arbitrary numeric identifiers). We computed the Adjusted Rand Index on a per topic basis ( N = 55) between the official and alternate judgments and obtained a mean of 0.445, a median of 0.492, and a standard Figure 4: Comparison between scores based on the official judgments and the alternate judgments for various metrics. Runs are sorted by score based on the official judgments in descending order. deviation of 0.225. As a point of reference, for topic 179 (left, Figure 3), the Adjusted Rand Index is 0.687, and for topic 225 (right, Figure 3), the Adjusted Rand Index is 0.305. We observe that the topics exhibit a wide range of similarity values, ranging from a minimum of 0.007 to a maximum of 0.977, which suggests that agreement is to a large extent dependent on the nature of the information need. However, it would be fair to say that there is substantial disagreement between assessors overall.
With the two independent sets of judgments, we can con-duct a stability analysis of the evaluation to determine the extent to which assessor differences impact our ability to make system comparisons, i.e., that system X is more ef-fective than system Y . An evaluation is considered stable if system rankings and pairwise system comparisons are in-sensitive to assessor differences. Here, we follow the well-established methodology of Voorhees [31], who examined assessor differences in document-level relevance judgments for ad hoc retrieval.

Figure 4 shows scores for all runs based on the official judgments and the alternate judgments for each of the five metrics. Results are sorted by scores based on the official judgments. We see that the rankings produced by both sets of judgments are highly correlated, with the exception of a few cases in weighted F 1 . Furthermore, the absolute val-ues of the metrics are also quite similar (particularly the unweighted metrics). Table 1: Count of rank swaps and Kendall X  X   X  corre-lation based the official and alternate judgments for each metric. Figure 5: Histogram of rank swaps for unweighted F 1 and weighted F 1 , binned by score differences.

In Table 1, we show the Kendall X  X   X  correlation between rankings induced by the two different sets of judgments. These values are in the same range as those reported by Voorhees [31] for ad hoc retrieval, which is generally re-garded by the IR community to be stable with respect to assessor differences in document-level relevance judgments. Table 1 also shows the number of rank swaps for each met-ric. A rank swap is a pairwise comparison where, according to one set of judgments, run A scores higher than run B , but according to the other set of judgments, run B scores higher than run A . There are a total of (50  X  49) / 2 = 1225 pair-wise comparisons, so the numbers of rank swaps observed in Table 1 are quite small.

A tally of the rank swaps does not tell the complete story because rank swaps do not capture the magnitude of the score differences. In particular, we are less concerned with rank swaps in which the absolute score differences between the two conditions are small. Histograms of the rank swaps for unweighted F 1 and weighted F 1 binned by absolute score differences are shown in Figure 5. We see that, indeed, most of the rank swaps are small. For space considerations, we only show the histograms for these two metrics, which have the lowest Kendall X  X   X  correlations. The histograms for the other metrics show even larger fractions of rank swaps where the score differences are small.

The conclusion from these analyses is fairly clear: assessor differences do not appear to impact the stability of the eval-uation, at least in terms of the metrics we have examined. Although our findings are limited to tweet timeline genera-tion, there is no reason to believe that these results would not carry over to other types of evaluations built around the notion of semantic clustering.
The second major research question we tackle in this pa-per concerns user preferences: when comparing TTG sys-tems, do our metrics capture differences that are actually meaningful to users? Following previous work, we opera-tionalize the notion of  X  X eaningful X  in terms of user pref-erences [2, 25]. If the evaluation tells us that system X is better than system Y , and a human can actually detect this difference (better than chance), we might reasonably claim that the improvement is meaningful from a user perspective.
Unlike most studies discussed in Section 2, which focus on ad hoc retrieval (and variants), our analyses have a more complex setup because tweet timelines are variable in length. We are specifically interested in three distinct types of effec-tiveness differences:  X  For two systems that exhibit roughly the same recall, how sensitive are users to differences in precision?  X  For two systems that exhibit roughly the same precision, how sensitive are users to differences in recall?  X  For two systems that exhibit similar precision-recall trade-offs, how sensitive are users to differences in F 1 ?
Our general strategy was to sample system output from the submitted TREC 2014 TTG runs and to generate sys-tem comparisons on a per-topic basis for user preference assessment. Unlike some previous work that artificially ma-nipulated system output to generate results of a particular level of effectiveness (e.g., [4, 30]), our procedure yields more realistic comparisons.

The sampling process proceeded as follows: First, we se-lected a set of 30 topics, biased toward  X  X ypical X  topics that have neither too many nor too few relevant tweets. Let x = r i  X   X  r be the difference between r i , the number of rele-vant tweets for topic i , and  X  r , the median number of relevant tweets over all topics: the probability of  X  X rawing X  topic i is proportional to the density at x i of a normal distribution with  X  = 0 and  X  = 20 (chosen heuristically).

Next, we performed some filtering of the submitted runs: we discarded all runs that contained unjudged tweets to eliminate the effects of missing relevance judgments. We also discarded all runs longer than 41 tweets, which is the median length of submitted runs (after the first filter). A pi-lot study indicated that long runs are very difficult to judge, and this is roughly the point after which the judgments be-come too onerous to make.

After filtering, we randomly (i.e., uniformly) selected 20  X  X ase X  runs, and for each, sampled up to 20 different  X  X om-parison X  runs per metric based on the following criteria:  X  For what we call precision sampling , we selected runs that differed by less than 0.1 in recall, but more than 0.1 in pre-cision. Previous work suggests that users have a hard time distinguishing small differences in effectiveness metrics, so we did not want to waste assessor effort.  X  For what we call recall sampling , we selected runs that differed by less than 0.1 in precision, but more than 0.1 in recall. This is the opposite of precision sampling.  X  For what we call F 1 sampling , we want the two comparison runs to make approximately the same tradeoff in terms of precision and recall. We selected runs where the value of T = | P  X  R | for the base run is within 0.1 of the T for the comparison run, as long as the difference in either precision or recall between the two runs exceeds 0.1. Figure 6: A screenshot of the web-based assessment interface for eliciting preference judgments. System outputs are presented in the left and right columns, with shared content in the middle column.
 Note that all sampling was performed on the cluster-based TTG metrics, i.e., taking into account redundancy. We used the unweighted variants of the metrics for simplicity. Finally, we discarded all pairs where the two sampled runs differ in length by more than 20 tweets. Based on the pilot study, we found that assessors struggled to compare runs that differed significantly in length.

Each trial of the above sampling procedure yields a large number of comparisons, from which we further sampled 180 pairs comprised of roughly an equal number of pairs from precision, recall, and F 1 sampling. This represents a single batch that we gave to assessors to judge; assessors could request additional batches if they desired after completing a batch. The first batch for each assessor contained a shared set of 30 comparisons (i.e., all assessors judged the same pairs), but for the remaining pairs in the first batch (and all subsequent batches), each assessor worked on unique pairs, which we ensured in the batch preparation process.

For eliciting user preference judgments, we designed a web-based assessment interface (see screenshot in Figure 6). The comparison screen displays the sampled TTG outputs for a given topic side by side: System #1 on the left and System #2 on the right. Sampled pairs were mapped to the two positions randomly to avoid introducing any sys-tematic biases. The topic is shown at the top. The output of each system is ordered chronologically, and each tweet is displayed with its timestamp. To help the assessor compare the two timelines, tweets returned by both systems are dis-played in the middle  X  X hared X  column. This means that an output might have gaps, indicating that one system returned more tweets than the other prior to the shared tweets (as is the case in Figure 6). We converged on this design af-ter trying various display alternatives in a pilot study X  X e learned that assessors wanted an easy way to compare the two systems in terms of the tweets they both returned, so we designed the interface to facilitate this comparison.
In the assessment instructions, we asked the assessors which of the two system outputs they thought was better. They were reminded to evaluate system outputs as time-lines, rather than as ranked lists. To be consistent with the clustering task, assessors were told to evaluate tweets based solely on their contents, ignoring any links they may con-tain. The assessor could click  X  X refer System #1 X  or  X  X refer System #2 X  above each of the two conditions, to indicate a preference, or a button labeled  X  X refer Neither X  in the mid-dle if the assessor could not decide.

We avoided prescriptively dictating what it meant for a timeline to be  X  X ood X . Specifically, the instructions made no reference to precision, recall, or redundancy. This is an important feature in our evaluation design to mitigate the effects of demand characteristics. 3 However, we tried to help the assessors frame their task with a few neutral reminders:  X  We are asking you about relative quality. Even if you feel that both systems do a bad job, do your best to determine which does a better job.  X  The tweets are presented in chronological order (earliest first), so you should keep in mind the timeframe covered by each result.  X  The timelines are displayed with a  X  X hared X  column in the middle. This column contains all tweets that were re-turned by both systems, and should help you more quickly determine how similar the results are. Since the timelines are aligned by their shared tweets, this middle column will also help you compare the results according to chronolog-ical blocks.

Assessors were first trained in the laboratory. They were given an introduction to the task and an explanation of the web interface, which included time spent with practice top-ics that were discarded for analysis. After this training ses-sion, the assessors could proceed at their own pace anywhere they had an internet connection. The server that hosted the web assessment interface recorded all interactions: both the preference judgments and their timestamps.
 Evaluation proceeded concurrently at UMD and UIUC. Judgments at UMD were performed by two male computer science graduate students. One of these assessors completed two batches of comparisons, while the other completed one. The assessors at UIUC were two graduate students in library and information science (one male, one female) and one for-mer library and information science graduate student (male) who was working nearby. One of the assessors completed two batches, while the remaining assessors completed one each. These assessors were not the same as the annotators who created the clusters (at both sites).
After the assessments were completed, the user prefer-ences were correlated against the preferences implied by the sampled metric, using Cohen X  X   X  as the agreement metric. Cohen X  X   X  ranges from  X  1 to +1, where 0 indicates that any agreement is attributable to chance. To compute  X  , we first discarded  X  X refer neither X  judgments. In the precision sampling case, we correlated the human preferences against the implied preference based on the precision scores. In the recall sampling case, we correlated the human preferences against the implied preference based on both unweighted recall and weighted recall. In the F 1 sampling case, we cor-related the human preferences against the implied preference based on unweighted F 1 and weighted F 1 .

Figure 7 shows these correlations binned by differences in the sampled metric, aggregated across all assessors. The Figure 7: Cohen X  X   X  for each metric binned by differ-ences in the sampled metric. Error bars show 95% confidence intervals. The numbers in the bottom left of each bar show the number of  X  X refer neither X  while those in the bottom right show the number of preference judgments in that condition. error bars show the 95% confidence intervals, computed fol-lowing Fleiss et al. [11]. The numbers in the bottom of each bar show the number of  X  X refer neither X  (on the left) and the number of preference judgments, i.e., users preferred one system over the other (on the right). Note that by the de-sign of the sampling procedure, no comparisons fell into the [0 , 0 . 1) bucket for precision and unweighted recall, although some samples did fall into that bucket for the other metrics.
In Figure 8, we show agreement broken down by each in-dividual assessor in a  X  X mall multiples X  visualization scheme. Each row shows a particular metric, and each column shows the agreement for an individual assessor for that metric. For clarity, the bar charts are shown without labels, but they are organized in exactly the same manner as in Figure 7. Note that confidence intervals here are much larger because fewer samples fall into each bin.

In both Figures 7 and 8, the bars are arranged by putative difficulty of the preference judgments, from left to right. Since there were generally fewer samples in which the metric differed by more than 0.4, all difference greater than 0.4 were placed in the same bin. The leftmost bar represents those runs where the metric difference is less than 0.1, for which we would expect humans to have the most difficulty distinguishing differences in output quality. Similarly, the rightmost bar should represent the  X  X asiest X  comparisons, in that the metric differences are the largest. We can intuitively think of these charts as quantifying how  X  X ensitive X  users are to differences in the underlying system-oriented metrics. Put another way, they tell us how much  X  X etter X  or  X  X orse X  a system would have to be in order for users to notice.
Overall, we do see a general trend of the agreement in-creasing from left to right, both at the aggregate and at the individual level. At the aggregate level, we see that there tend to be more  X  X refer neither X  judgments in cases where Table 3: ANOVA results for each metric. Metrics displayed in bold show a statistically significant dif-ference ( p &lt; 0 . 05 ) in mean  X  across bins. the metric difference is small, which makes sense. However, there are clearly individual differences that buck the overall trend, as shown in Figure 8. For example, assessor 4 (fourth column from the left) does not appear to be sensitive to pre-cision at all X  X greement remains near and sometimes worse than chance, even for very large differences in precision.
One of the downsides of  X  is that it lacks an intuitive inter-pretation. To address this, Landis and Koch [16] proposed a guide, which we replicate in Table 2. Under this rubric, the minimum  X  values we observe per metric range from  X  X oor X  (for precision) to  X  X air X  (for weighted F 1 ), whereas the maximum  X  values we observe for each metric range from  X  X air X  (for precision) to  X  X lmost perfect X  (for unweighted and weighted F 1 ). The differences between maximum and mini-mum  X  values per metric support the apparent relationship between metric differences and annotator preferences.
It is worth emphasizing here that all our metrics already take into account redundancy (i.e., they are cluster-based). Since we did not explicitly ask the assessors to consider re-dundancy in the assessment instructions, these results show that humans are nevertheless sensitive to the redundancy removal performed by the systems.

To add statistical rigor to our analyses about the rela-tionship between agreement and metric differences, we per-formed analysis of variance (ANOVA) comparing the  X  val-ues across bins. That is, we treat the  X  of each individual assessor for a particular bin as a point estimate of the  X  X rue X   X  for that bin (magnitude of metric difference), and want to know if the mean  X  values across the bins are significantly different. The results are shown in Table 3. We found sta-tistically significant differences in mean  X  across bins for precision and unweighted F 1 at p &lt; 0 . 05. From this, we can conclude that increases in metric differences are associated with better agreement for those metrics.

A surprising result is the lack of statistical significance for both unweighted and weighted recall. What does this mean? The recall bar charts in Figure 7 show that differences in recall between pairs of runs are detectable by annotators; however, the magnitude of the difference does not appear to affect the agreement, i.e., the  X  values are not significantly different across the bins. We believe that this is due to the nature of evaluating recall, which requires knowledge of the Each bar chart is organized in the same manner as those in Figure 7. complete relevance judgments. When comparing two runs, it is relatively easy to detect recall differences (i.e., if one system returned a relevant tweet that was absent from the other system), but without access to all the relevance judg-ments, it is difficult to tell how much better one is than the other (i.e., the marginal recall increase from returning one more relevant tweet is dependent on the number of relevant tweets for that topic). Note that in contrast, for precision, annotators do appear capable of detecting the magnitude of the difference. This is again unsurprising since precision can be assessed directly from the displayed tweets.

We were surprised at the high p -value for weighted F particularly in comparison to unweighted F 1 . This result indicates that although assessors consistently achieved bet-ter than chance agreement, they either had difficulty assess-ing the importance of individual tweets or our weighting scheme does not accurately capture users X  notion of impor-tance. The second possibility points to future work in de-veloping more meaningful metrics for modeling importance.
Figure 7 shows another surprising result: we observe rel-atively low agreement for precision (compared to the other metrics). We believe that this may be an artifact of our sampling strategy, where we discarded overly verbose runs. This choice was made out of necessity, as our pilot study suggested that it was not feasible to ask assessors to com-pare timelines with, say, hundreds of tweets. The problem is this: while it is certainly possible to obtain low precision in a short timeline, the amount of  X  X ain X  associated with such low precision timelines is relatively low (bounded by the total number of returned tweets). However, if a timeline were an order of magnitude longer, say, the burden imposed by poor precision would be much heavier. In other words, our sampling procedure selected only those runs where dif-ferences in precision were  X  X ot a big deal X  from the assessor X  X  point of view because the timelines were relatively short.
We also analyzed the relationship between the time as-sessors spent on each comparison (reconstructed from our logs) and the observed agreement. This is shown in Fig-ure 9, binned in intervals of 30 seconds for each metric. The numbers at the top of each bar show the number of  X  X refer neither X  and preference judgments.  X  X refer neither X  judgments were not included in the agreement calculation. Missing bars indicate an undefined  X  while dashes indicate a  X  of zero. We see no clear relationship between  X  and the amount of time it takes to make a judgment; this is some-what surprising as we would have expected  X  X asy judgments X  to result in higher agreement.

Finally, we analyzed agreement on the set of 30 shared comparisons provided to each of the five annotators at the beginning of the first batch. We calculated Fleiss X   X  [10], a variant of Cohen X  X   X  intended for more than two raters, and found a value of 0.294 for this shared set. Under Landis and Koch X  X  scheme, this represents only a fair level of agree-ment among annotators. This finding is noteworthy because it indicates that user preferences correlate with metric dif-ferences, despite the fact that humans only achieve  X  X air X  agreement among themselves.

Summarizing these results, our user study suggests that user preferences do agree with system preferences as mea-sured by the TTG metrics X  X hat users are sensitive to dif-ferences in precision, recall, and F 1 , although to different degrees. We can therefore conclude that system-oriented metrics are meaningful in being able to capture aspects of what users care about in timeline summaries. Figure 9: Cohen X  X   X  binned by time spent on each comparison (at 30 second intervals). Numbers at the top of each bar show the number of  X  X refer neither X  and preference judgments. Missing bars indicate an undefined  X  while dashes indicate a  X  of zero.
As primarily an empirical discipline, progress in informa-tion retrieval is built on system comparisons. This paper explores assessor differences and user preferences in the con-text of the tweet timeline generation task in the TREC 2014 Microblog track. Analyses show that the evaluation method-ology appears to be sound, which strengthens our confidence in existing and future results. Although our findings are lim-ited to this particular task, we believe that lessons learned could generalize to other  X  X luster-based X  evaluations, inform-ing related tasks such as question answering and temporal summarization. This work was supported in part by the National Science Foundation under IIS-1217279 and IIS-1218043. Any opin-ions, findings, conclusions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the sponsor. We are grateful to Ellen Voorhees and the assessors at NIST for making TREC possible.
