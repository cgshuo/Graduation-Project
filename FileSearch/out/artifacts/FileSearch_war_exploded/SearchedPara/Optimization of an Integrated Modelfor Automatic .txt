 Dawei Song 1 , 2 , Yanjie Shi 1 , Peng Zhang 1 ,YuexianHou 1 ,BinHu 3 ,YuanJia 4 , Long queries can be viewed as a rich expression of a user X  X  information need and have recently attracted much attention [1,12]. An example long query (the description field of TREC topic 382) is shown in Figure 1.

Long queries may only form a fraction of queries actually submitted by searchers on the Web but they do represent a significa nt part [6]. For more specialized search engines long queries are very common, e.g. the queries submitted to the legal search service Westlaw are on average about 10 words long [15]. Medical search engines have also been developed to handle long queries such as plain English text [14]. Furthermore, one could also see pseudo-relevance feedback as an attempt to make the query longer.

Intuitively, the useful hints and rich information carried by a long query can be leveraged to improve search performance. However, the rich information is also likely to bring interference caused by possible irrelevant terms and the verbosity surrounding the key concepts in the long queries [1]. One type of  X  X ad X  terms are those completely irrelevant to user X  X  information need, such as  X  X dentify X  and  X  X ocuments X  in the above example, and can be viewed as noise. The other type are terms that are weakly relevant, such as  X  X rive X  and  X  X torage X  in the example. They may result in query shift although they do carry some useful information. Table 1 shows the retrieval performances of TREC Topics 251  X  300 on ROBUST05 collection using the title and description fields of each topic, respectively. The title field X  X  average length is about 3 terms per topic. The description field is much longer, on average about 17 terms per topic. We can find that using title field as query generat es higher precision and recall than the use of description field, over two typical IR models (vector space model based on TF-IDF and language model using Kullback-Leibler (KL) divergence), and a widely used pseudo-relevance feedback model -the Relevance Model (RM) [13].
Therefore, to improve the retrieval effect iveness for long queries, it is crucial to identify and boost the important terms and meanwhile eliminate the negative effects of  X  X ad X  terms in the original or expanded queries. 1.1 Related Work One way of alleviating the problem is to use interactive techniques and let users select some terms or combinations of various terms, as key concepts, from long queries, through interactive query re duction (IQR) and interactive query ex-pansion (IQE) [11,10,4,18,9]. IQR in general aims to help users remove noisy information -the first type of interference we described earlier. However, it is not good at handling the second type of  X  X ad X  terms. This is because users of-ten can only decide, approximately, whether a term is important or not, rather than accurately quantifying the import ance of these terms. Moreover, it cannot introduce additional relevant terms to the original query. Thus interactive query expansion (IQE) is used to help users remove irrelevant terms suggested by au-tomatic query expansion techniques. Further, [11] integrates IQR and IQE into a single framework for Selective Interactive Reduction and Expansion (SIRE). Although both IQR and IQE can result in salient improvements for document re-trieval with long queries, such user-dependent approaches have their limitations. Too much user interference may increase the time and cognitive overhead, if users are required to read through each long query and the search results from a baseline system and decide which terms should be selected. On the other hand, users may not be able to easily give an accurate estimation of the weight for each selected term, which can be effectiv ely computed from automatic methods. In any case, it is well recognized that users are reluctant to leave any explicit feedback when they search a document collection [3,8,16].

Therefore, it makes sense to process l ong queries automatically to estimate the importance of query terms. In [1], a method to identify key concepts in verbose queries using supervised learning is proposed. However, it relies on pre-labeled training data. In term of automatic query expansion, there has been many approaches in the literature. One of the state of the art automatic query expansion methods is the Relevance Model. Although it does not distinguish the combinations of multiple query terms as IQR or IQE does in [18], it estimates the probability distribution of expanded terms. Thus it can improve the positive effects of some key terms and reduce the negative effects caused by noisy and/or redundant information.

However, as shown in Table 1, existing automatic approaches such as TFIDF, as a basis for query reduction, and RM, as a basis for selecting expansion terms, are less effective for long queries. We th ink a possible reason is that these ap-proaches treat the query terms as independent of each other. In reality, partic-ularly for long queries, the cohesion of query terms, largely determined by the intra-query term dependencies, plays a key role in deciding which terms should be used to represent the information need.

The overall aim of this paper is to develop an effective automatic query re-duction and expansion approach for long queries, which should take into account the intra-query term dependencies. 1.2 Our Approach In this paper, we propose a method that effectively integrates query expan-sion and query reduction. First, an existing robust query expansion approach, called the Aspect Query Language Model (AM) [19], is used as a basis for an overall framework to derive an expanded query model (Query Expansion). In AM, the query terms are considered as l atent variables over a number of ob-served top ranked documents after initial retrieval. Secondly, a Hidden Markov Model (HMM) is established over the AM structure, leading to an Aspect Hid-den Markov Model (AHMM), to estimate the dependencies between the latent variables (as states of the HMM), and in turn to identify an optimal probabilistic distribution over the states (Query Reduction). The details of our approach will be introduced in the next section. 2.1 The Aspect Query Language Model (AM) This section gives a brief description of the AM [19], where the subsets of query terms are viewed as latent variables (re presenting query aspects) over a number of top-ranked documents from the initial r etrieval. The query aspects are treated as latent variables, as they (and their optimal weighting) can only be derived through the top ranked documents that we observe.
 The AM structure is shown in Figure 2. S j is a latent variable in the set S ( S = { S 1 ,  X  X  X  ,S N } ), and w is a word whose occurrence probability in the expanded query model (formally denoted as  X  Q ) to be estimated. The latent variable S j is generated from the original query Q = { q 1 ,  X  X  X  ,q M } ,whereeach S j is in general defined as a query term or a combination of query terms. The size of S is (2 M  X  1) if we use all the combinations of query terms. For instance, given Q = { q 1 ,q 2 } , the set of latent variables can be transformed into S = {{ query terms as latent variables in order to reduce the computational complexity. Indeed, our experimental results indicate little difference in effectiveness between the use of combinations of 2-3 terms and the combinations of more terms as states.

The relationship between the word w and the latent variable S j is derived from the relevance feedback documents. In practice, such as in Web search, the number of top ranked documents actually observed by users is often small [7], which will lead to the data sparsity problem given the large number of latent variables for a long query. Furthermore, not all the top ranked documents are truly relevant to the query. Even for a relevant document, it is not necessarily true that every part within the document is relevant. Thus, smaller chunks of the documents (e.g., segmented through a sliding window) are used to connect S j and w in order to expand the observation space to overcome the data sparsity problem and improve the quality of parameter optimization.
 Based on the structure in Figure 2, the following formula can be derived: of an observed document chunk d i given a latent variable S j ,and P ( w | d i ,S j )is the probability of a word w in a chunk d i given S j .

An on-the-fly training data constructio n method is developed to automatically label the document chunks with query term(s). The Expectation Maximization (EM) algorithm is then used to fit the parameters of Equation 1. Despite the proven effectiveness of the AM, it does n ot take into account the intra-query term dependencies (i.e., dependencies between the latent variables). 2.2 Aspect Hidden Markov Model (AHMM) We now present an AHMM approach that extends the original AM and allows a natural incorporation of the intra-query dependencies through the HMM mech-anism. There has been evidence that the source of natural language text can be modelled as an  X  X rgodic X  Markov process, meaning the corresponding Markov chain is aperiodic (i.e., words can be separated by any number of intermediate words) and irreducible (i.e., we can always get from one word to another by con-tinuing to produce text) [5]. As shown in Figure 3, based on the dependencies among S j , d j and w , we extend the AM by adding links between S j and S j +1 .
The HMM is a finite set of states ( S = { S 1 ,  X  X  X  ,S M } ), each of which is as-among the states ( A = { S j ,j } , S j ,S j  X  S ) are governed by a set of proba-bilities called transition probabilities. For a particular state, an observation d i can be generated according to the associated probability distribution denoted as B = { P ( d i | S j ) } . Figure 4 shows an example structure of a 3-state ergodic HMM.

We now need to design a parameter esti mation framework based on effective optimization mechanisms in HMM. The application of the HMM can not only estimate the prior distribution of each S j , but also integrate the dependence between any two latent variables (as states) and their underlying observables (document chunks) through a state transition matrix. Given the observation chunks d = { d 1 ,  X  X  X  ,d T } and a model  X  =( A, B,  X  ), the HMM can choose a corresponding state sequence ( S 1 ,  X  X  X  ,S T ) that is optimal (i.e., best  X  X xplains X  the observations). For the purpose, the Viterbi algorithm is used. Due to space limit, the algorithm is not detailed here (See [17] for details).

In this paper, the query terms are used as HMM states ( S j ), which are not really  X  X idden X  in a strict sense. We adopt the HMM structure for the effective optimization mechanisms that HMM provides. In the process of learning the model, we utilize the Baum-Welch algorithm to optimize the state distribution and transition matrix and the Viterbi algorithm to search the optimal path [17] to update the probability distribution of each chunk in different states. The update of P ( d t | S j ) is based on: computation detailed in Fig. 5.

In summary, the HMM would seem to provide a mechanism to estimate the parameters listed in Eq. 1. For estimation of the probabilities in Eq. 1 within the AHMM structure, we adapt the original parameter estimation algorithms in AM [19] to AHMM, as shown in Figure 5. 3.1 Data Pre-processing (Step 1) The same data pre-processing procedure used in AM is applied here. Step 1.1 takes the query term combinations as states, as discussed in Section 2.1. Step 1.2 selects relevance feedback documents. In Step 1.3, each feedback document is segmented with an overlapped sliding window, as discussed in Section 2.1.
Step 1.4 can be seen as a coarse data refinement by keeping only the chunks containing at least one query term. Step 1.5 can be considered as an automatic on-the-fly training data construction p rocess, which labels the selected chunks with different states. With these automatically labeled chunks, we can compute the initial word probability given a state S j , denoted as P ( w | S j ). These initial computations are then used to optimize the AHMM in Step 2. 3.2 Model Estimation (Steps 2 &amp; 3) According to the description of AHMM in Section 2.2, we initialize the model parameters by setting the state distribution P ( S j ) and the state transition prob-ability P ( S j | S j ) to be the chance probability 1 M .Here M is the number of states in the HMM. A recursive method is used to compute P ( d i | S j ). This method for d i,k denotes the first k words in chunk d i and d i,K = d i ,where K is the window P ( w k | S j ) is computed as: where # w k,i is the occurrence frequency of w k in d i . The probability P ( w k | S j ) is then applied to the recursive equation to compute P ( S j | d i ).
In Step 2.2, we apply the Viterbi algorithm to searching the optimal state sequence, then we update the HMM iterat ively by re-computing the model pa-rameters  X  = {  X ,A,B } . Finally, the two probability parameters P ( S j )and P ( d i | S j ) in Eq. 1 are updated according to the AHMM. Since the contribution of P ( d i | S j ) has been considered in Eq. 3 to compute P ( w k | S j ), Eq. 1 can be simplified as: We evaluate our method using TREC topics 251 X 300 on the TREC5 collection (TREC disk 2 and 4), topics 303 X 689 (50 selected queries) on the ROBUST05 collection (AQUAINT collection), and topics 301 X 450 and 601 X 700 on the RO-BUST04 collection (TREC disk 4 and 5 excluding the Congressional Record ). The description field of the topics are used as queries (with an average query length of 15-17 words). They are selected because they have varied content and document properties. The Robust track s are known to be difficult, and conven-tional IR techniques have failed on some of them [11]. In our experiments, a standard stopword list and the Porter stemmer are applied to all data collec-tions. Note that the same experimental setting was also used in [11] for user interactive IQR and IQE. This allows a direct comparison of our method with this interactive approach.

Three baselines are used for comparison including a language model based on Kullback-Leibler ( KL ) divergence, the Relevance Model ( RM )andtheAM. We also compare our methods with the user based interactive system based on its results reported in [11]. For pseudo-r elevance feedback, all the methods are tested with a certain number N of top-ranked documents from the initial re-trieval results. Here, we only select N = 30 documents as feedback, as we think this is close to real web search scenarios. Note that we have tested a range of N (10, 20, ..., 100), and the performance of ou r approach is in general quite stable with respect to N . The documents are then segmented into chunks using a slid-ing window of 15 words with 1/3 overlapping between two consecutive windows. After applying each query expansion algorithm, we choose the top ranked 100 terms as expansion terms. All the experiments are carried out using the Lemur toolkit 4.0 1 . The initial retrieval is run by a widely used language model based on KL-divergence. The expanded queries derived from three different query lan-guage modeling methods (RM, AM and AHMM) are used to perform the second round of retrieval using KL divergence. Our primary evaluation metric is mean average precision (MAP). The Wilcoxon singed rank test is used to measure the statistical significance of the results. 5.1 Illustration of AHMM Optimization When the AHMM is applied, its probability parameters is iteratively optimized. Let us consider the example in Figure 1 again. Table 2 shows the changes of the probability of each query term and the query X  X  retrieval performance (average precision) for each iteration. In the original query, some terms, such as cell , safe , storage and gener , seem only weakly relevant to the intention of the query. After a few iterations, their probability values are reduced. Simultaneously, the corresponding values of some key terms, such as car and fuel , are increased, This table also shows the positive trend of the performance when more iterations are run. The performance peaks at iteration 4 but then drops slightly at the 5th. This may be due to the overfitting caused by the data sparsity when optimizing the model. An in-depth investigation on this issue is left as future work. 5.2 Comparisons in Query Expansion The results of KL baseline, RM, AM and our approach are listed in Table 3. We can find that the AHMM shows good performance on all three data collections, and generates significa nt improvements over the KL baseline, RM and AM.
As a further comparison, we list the performances using user based interactive system (UIS) [11] in Table 4. In [11], the interactive query expansion (IQE) relies on users X  help to select query expansion terms. In the comparison with IQE, our AHMM can also generate better performance on two data collections, TREC5 and ROBUST05, while the MAP value of our approach is lower than IQE on ROBUST04. Since the query set for ROBUST04 is a known difficult one, our model, as an automatic method, still gives a good performance. 5.3 Robustness of the Model To further test the robustness of our model, we run another set of experiments, where we remove some  X  X oisy X  information in advance from the queries, through a process of pre-query reduction . This test is based on the assumption that the retrieval performance of the AHMM should keep a stable status, i.e., it is robust regardless of the selective removal of some noisy query terms.

Unlike the interactive query reduction, which is based on user manual selec-tion, in our experiment, we use a simple automatic method for the pre-query reduction. We first collected all the queries we are using. Since there is an overlap between the query sets for ROBUST04 and ROBUST05, we obtain 300 queries altogether. We then counted the query frequency of each query term.

Our idea is to remove several query ter ms with high query frequencies, which can be viewed as  X  X top words X  in the query set, such as  X  X dentifi X ,  X  X ocument X , etc. We set a threshold in term of the ra tio of the query frequency to the total number of queries. In our experiments, i t is set to be 0.1, a small number, because we want to be conservative and not to risk too much of mistakenly removing some useful terms.

Table 5 summarizes the performance of KL, RM, AM and the AHMM after applying the query term reduction. We observe a performance improvement on all models and collections, compared with the results without applying pre-query reduction (Table 3). The AHMM still outperforms the other three methods. The performance difference of AHMM with and without applying pre-query reduction is smaller than that of the other models. The observations reflect the robustness of the AHMM.
 In both scenarios (i.e., with and without pre-query reduction), for RO-BUST04, a well-known difficult collection where RM and AM underperform the KL baseline, AHMM achieves significant improvements over all the other mod-els. The AM itself performs less well f or long queries (in contrast of the good performance for short queries as reported in [19]). By adding the HMM layer on top of the AM structure, the perfomance is largely improved. This indicates our method learns more reasonable weights for query terms than the AM through the HMM-based model optimization process. We have presented an AHMM method for effective query expansion and query reduction for long queries, by estimating the intra-query term dependencies and the relationships between query terms and other words through observed rel-evance feedback documents. Our exper imental results show that our method achieves significant improvements in co mparison with three baselines: KL, RM and AM, showing the effectiveness and robustness of the proposed approach. Even when compared with the user-base d interactive system used in [11], our approach, which is automatic, still shows a comparable performance. In the fu-ture, it would be interesting to study how to effectively and efficiently combine the user interactions with our automatic algorithm. In addition, to tackle the data sparsity problem when selecting f ewer number of documents, we will con-sider smoothing our model with the background collection model.
 Acknowledgments. This work is funded in part by the Chinese National Pro-gram on Key Basic Research Project (973 Program, grant no. 2013CB329304 and 2014CB744604), the Natural Science Fo undation of China ( grant no. 61272265), and the European Union Framework 7 Marie-Curie International Research Staff Exchange Programme (grant no. 247590).

