 Most machine-learning tasks, including classification, involve dealing with high-dimensional data. It was recently shown that the phenomenon of hubness, inherent to high-dimen-sional data, can be exploited to improve methods based on nearest neighbors (NNs). Hubness refers to the emergence of points (hubs) that appear among the k NNs of many other points in the data, and constitute influential points for k NN classification. In this paper, we present a new probabilis-tic approach to k NN classification, naive hubness Bayesian k -nearest neighbor (NHBNN), which employs hubness for computing class likelihood estimates. Experiments show that NHBNN compares favorably to different variants of the k NN classifier, including probabilistic k NN (PNN) which is often used as an underlying probabilistic framework for NN classification, signifying that NHBNN is a promising alterna-tive framework for developing probabilistic NN algorithms. I.2.6 [ Artificial Intelligence ]: Learning; I.5.1 [ Pattern Recognition ]: Models Algorithms, Experimentation, Theory
High dimensionality is known to pose certain limitations on the effectiveness of many machine-learning algorithms. Nowadays, the majority of data one has to deal with is, unfortunately, high-dimensional. This includes text, im-ages, video, medical records, data streams etc. All such data is sparse, since data size requirements for proper den-sity estimates rise exponentially with the number of dimen-sions. Furthermore, there exists the phenomenon of dis-tance concentration, which has been thoroughly explored in the past [4]. In high-dimensional spaces, distances between points drawn from the same distribution tend to become relatively similar to one another, so some doubts have been raised about the usefulness of the notion of nearest neigh-bors in high-dimensional settings [3]. Nevertheless, the k -nearest neighbor ( k NN) classifier is a widely used classifica-tion method that operates by a majority vote on a k -nearest-neighbor set [1].

Hubness is a phenomenon related specifically to nearest-neighbor methods. Let N k ( x ) denote the number of k -occurrences of point x , i.e., the number of times x occurs in k -nearest-neighbor lists of other data points. When the (intrinsic) dimensionality of data is high, significant skew in the distribution of N k ( x ) can be observed. This leads to the emergence of hubs , data points which are included in many more neighbor lists than other points. The number of k -occurrences of point x , N k ( x ), will be referred to as the hubness score of x . A hubness-based weighted voting scheme for k NN, proposed in [8], was shown to often lead to improvement when working with high-dimensional data.
Our goal is to examine if it is possible to incorporate hub-ness information for Bayesian class prediction. Hubness is interpreted as prior informat ion about occurrences of ele-ments in k -neighbor sets and is used to set up a Bayesian k -nearest neighbor model. The proposed approach is fun-damentally different from related methods, from which a representative selection is reviewed in Section 2. Section 3 presents our probabilistic framework for using hubness, and proposes the naive hubness Bayesian k -nearest neighbor clas-sification algorithm (NHBNN). Experimental comparison of NHBNN with related approaches is performed in Section 4, while the last section gives concluding remarks and an out-line for future work.
Introducing weights to votes in k NN is not all that un-common. Defining vote weights w k ( x i ) based on hubness was introduced in [8]. The hubness of each point can be decomposed into two parts, which will be referred to as good hubness and bad hubness , respectively: N k ( x i )= GN k BN k ( x i ). Good hubness represents those k -occurrences of x i where labels match, i.e., the label of x i and the label of the data point which has x i in its neighbor set are the same. Bad hubness, on the other hand, originates from la-bel mismatches. In high-dimensional spaces, hubs become increasingly influential, since they are found in many k -neighbor sets. We can distinguish between good hubs and bad hubs based on how often label mismatches occur. Bad hubs exhibit a distinctively detrimental influence. Hubness-weighted k NN [8] aims to reduce the influence of bad hubs, which is how it improves on the basic k NN classifier. The approach is as follows. Standardized bad hubness is com-puted as: h b ( x i ,k )=( BN k ( x i )  X   X  BN k ) / X  BN k is mean bad hubness and  X  BN k the standard deviation. The weight associated with x i is then w k ( x i )= e  X  h b (
One of the disadvantages of the standard k NN classifier is that it does not output meaningful probabilities associated with class prediction. Performing a count of the number of votes given to each class and making the class probabilities somehow proportional to those counts is, admittedly, simple enough  X  but is not a really good measure of confidence, especially for lower k values. A Bayesian solution to this problem was proposed in [6] and is known as probabilistic k -nearest neighbor (PNN). It has been shown that PNN does not offer an improvement in accuracy over the basic k NN [7]. Extensions of PNN have been proposed which improve its overall performance [2, 5].
In our approach, we wanted to exploit the information provided by reverse nearest neighbors, while being aware of hubs in high dimensions. The way in which hubness had previously been exploited in hubness-weighted k NN is some-what rigid, so we decided to extend it. In multi-class sce-narios, all label mismatches in k -occurrences of point x were collapsed to a single quantity  X  bad hubness .Ourideawas to observe class-specific label mismatches. This led us to define class hubness .

Thereby, the total number of k -occurrences is decomposed in the following manner: N k ( x i )= signifies the number of k -occurrences of point x i in neigh-borhoodsofelementsofclass c .Let D k ( x )denotethe k -neighbor set of x . We interpret the k -occurrence of x i D k ( x ) as an event which carries information that gives cer-tain evidence to class affiliation of x .

Before we continue, we should first emphasize the main difficulty of such reverse nearest neighbor reasoning. Some points may never appear in k NN sets and therefore have  X  no reverse nearest neighbors. In high-dimensional data this happens more often, due to the hubness phenomenon. Such points which appear either never or rarely as nearest neighbors we will refer to as anti-hubs .

On the other hand, in hubs there is a lot of occurrence information to infer from. Suppose that x h is one such hub and c h its label. In some cases, the labels of the majority of reverse neighbors of x h might be different from c h .In such cases, when we observe x h  X  D k ( x ), this should not be considered as strong evidence in favor of class c h .This is why we feel that taking class hubness into consideration may prove quite beneficial.

Let D =( X, Y ) be the data set, so that y i  X  Y is the label of x i  X  X . We would like to estimate p ( y = c | D k ( x )). We view D k ( x ) as being the result of k random draws from D where each x i has probability p t ( x i | D t ( x ) ,x ) of being drawn at time t ,where t  X  X  1 , 2 ,...,k } . Since the concepts of t -neighbor and ( t + 1)-neighbor are quite similar, we disregard the exact position of x i in the list, which endows us with more information to work with.

Let x i  X  X and x it ,for t  X  X  1 , 2 ,...,k } ,bethe k nearest neighbors of x i . We focus on a naive Bayesian estimate which takes the form shown in Eq. 1:
The independence assumption obviously does not hold in most cases. However, it was shown in the past that this is not necessary for the naive Bayesian approach to work [9].
The main problem lies in estimating the probabilities on the right-hand side of Eq. 1, especially for anti-hubs, which will be treated as a special case. Also, each point was in-cluded to its own neighbor set at the 0th position, ensuring that each N k ( x i )  X  1.
 For clarity we shorten the notation by denoting p ( x it  X  D ments of class c . Also, let N k,c 1 ( c 2 ) be the total number of k -occurrences of elements from class c 2 in neighborhoods of elements belonging to c 1 .

Let ( x, y ) be an element from the neighborhood of x i Also, let ( x g ,y g )  X  probability estimate is given in Eq. 2: where  X  is the Laplace estimator, ensuring non-zero proba-bilities. When dealing with anti-hubs, however, the outlined approximation can not be expected to yield reliable proba-bility estimates. This is why for anti-hubs we partly infer the probability from what is known about the typical points from the same class and their hubness, as shown in Eq. 3.
We propose two approaches to defining  X  p x i ,c,k ( x it are based on the approximation given in Eq. 4:
We will refer to using this estimate as the global approxi-mative approach . We could also take local information into account by inferring N k,c ( y it )fromsome k est -neighborhood of x it instead, as given in Eq. 5. We will refer to this estimate as the local approximative approach .Inourexperimentswe invariably used k est = 20.
In the end we obtain  X  p x i ,c,k ( x it ) from Eq. 3 by plugging either the local or the global class hubness estimate instead
Class affiliation of x is determined as y =argmax c p ( y = c |
D k ( x )). In case of an unlikely tie we assign according to the prior class probabilities, i.e., the majority class. We name the algorithm which performs this classification, based on approximations given in Eqs. 1, 2, 3, 4, and 5, the naive hubness Bayesian k-nearest neighbor ( NHBNN ). Several parameters are required for the algorithm to work, but they can also be inferred from the training set by leave-one-out cross-validation.
 Algorithm 1 NHBNN: Training
The data sets used for evaluation are summarized in Ta-ble 1. Eighteen data sets of various dimensionalities origi-nate from the University of California, Irvine (UCI) Machine Learning Repository, two high-dimensional gene expression microarray data sets (colonTumor, ovarian) are from the Kent Ridge (KR) Bio-Medical Data Set Repository, and five data sets represent subsets of images taken from the Ima-geNet repository. We extracted SIFT features from all im-ages, formed quantized 400-dimensional representations [11], and then appended 16-bin color distribution information to obtain the final 416-dimensional feature vectors. More de-tails on these five image data sets are given in [10].
All data sets are summarized in Table 1, whose columns respectively denote data-set name, size, dimensionality ( d ), number of classes ( C ), and the observed skewness of the distribution of N 5 ( S N 5 ) [8]. Euclidean distance was used as a dissimilarity measure for the first 20 data sets from the table and Manhattan distance for the last five image data sets. We see that the hubness phenomenon is much more pronounced in image data.

Experiments were performed in three different setups. The first two setups used only the first 20 listed data sets. The image data sets were considered separately, for reasons that will be explained later.
 In the first round of experiments we examined how NHB-NN performs for a fixed k value and some fixed parameter configuration over all data sets. The conclusion from the results from this round (omitted due to space considerations) is that global and local versions of NHBNN exhibit similar performance, and are both superior to k NN and hw-k NN.
In the second round of experiments our aim was to com-pare the algorithms when automatically searching for proper parameters (as well as estimation schemes) on each fold of 10 times 10-fold cross validation, with the recorded accura-cies compared using the corrected resampled t -test. Here, we also included PNN in the comparison with NHBNN. For k NN, hw-k NN and NHBNN we used k  X  X  1 , 2 ..., 10 } .For PNN we allowed a larger maximum neighborhood size, this being the way it was used in the original article [6]. The maximum k for PNN was set to min(500 , n/ 4 ), where n is the data-set size. We set PNN to use 1500 evaluation points. The results are shown in Table 2.

In both experimental settings, NHBNN performs quite well. Significant improvements over k NN were observed, with 5-1 and 4-0 in statistically significant wins in the two observed experimental setups, respectively. NHBNN com-pares favorably to PNN, since it achieves 7-0 in statistically significant wins.

In the first two experimental setups no significant differ-ence between hw-k NN and NHBNN was observed. This is not all that surprising, since most UCI data sets are bi-nary classification problems, low-to-medium (intrinsically) dimensional with small skewness in the distribution of k -occurrences, as seen in Table 1. Furthermore, many of the Table 2: Comparison of NHBNN with several other classifiers, featuring automatic search for best pa-rameters and approaches on each cross-validation fold. The symbols  X  /  X  denote statistically significant better/worse performance with respect to NHBNN, at significance level p&lt; 0 . 01 .
 UCI data are small, rendering st atistical comparisons more difficult. This is why we decided to run a separate compar-ison on the five ImageNet data sets. They are multi-class classification problems and also exhibit a very high skew in the distribution of N k ( x ).

The three algorithms  X  k NN, hw-k NN and NHBNN were compared on five ImageNet subsets and the results are given in Table 3. The comparisons were performed for a fixed neighborhood of k =5. Thethreshold  X  in NHBNN was set to zero so that neither local or global estimate is used, only what information is available from the point label and hubness. We opted for such a configuration to show that, even though the local and global approach described in the paper provide greater flexibility in dealing with anti-hubs, they are by no means necessary for the basic idea to function.
The results from this third experimental setup differ great-ly from the previous two, showing an even more convincing difference between NHBNN and k NN on high-dimensional data, as well as enabling us to differentiate between hw-k NN and NHBNN. On these data sets, we see that using class-specific hubness does, indeed, lead to better results. The improvement is usually modest, but this is due to the fact that both algorithms exploit the same phenomenon  X  hubness. However, NHBNN is to be preferred in cases when dealing with intrinsically high-dimensional multi-class data.
We presented a novel algorithm for probabilistic NN clas-sification, naive hubness Bayesian k -NN (NHBNN). Hubness is a phenomenon inherent to high-dimensional data which has only recently been taken into serious consideration and has never before been used in a Bayesian framework. We have shown in this paper that taking point hubness into ac-count may be beneficial to nearest-neighbor methods and that it should be more thoroughly investigated. The pro-posed algorithm differs in its conception greatly from the probabilistic k -nearest neighbor (PNN) and related meth-Table 3: Comparison of NHBNN with k NN and hw-k NN on ImageNet data for k =5 .Thesymbols  X  /  X  denote statistically significant better/worse perfor-mance with respect to NHBNN for p&lt; 0 . 01 and  X  denotes those cases when NHBNN is deemed better at significance level 0 . 01  X  p&lt; 0 . 05 .
 ods, therefore representing an alternative approach that is open for further improvements.
 [1] T. Cover and P. Hart. Nearest neighbor pattern [2] L. Cucala, J. M. Marin, C. P. Robert, and D. M. [3] R.J.DurrantandA.Kab  X  an. When is  X  X earest [4] D. Fran  X  cois, V. Wertz, and M. Verleysen. The [5] R. Guo and S. Chakraborty. Bayesian adaptive [6] C. C. Holmes and N. M. Adams. A probabilistic [7] S. Manocha and M. A. Girolami. An empirical [8] M. Radovanovi  X  c, A. Nanopoulos, and M. Ivanovi  X  c. [9] I. Rish. An empirical study of the naive Bayes [10] N. Toma X  sev, M. Radovanovi  X  c, M. Ivanovi  X  c, and [11] Z. Zhang and R. Zhang. Multimedia Data Mining: a
