 (NLP) applications. Some of these typical app lications include voice dialing, call routing, Intuitively, improving the ASR's performance will improve the related NLP applications. sioned to be the dominant human-machine interface in the near future. 
The substantial goal of ASRs is to enable people to communicate more naturally and effectively. But this ultimate dream faces many obstacles such as different speak-ing styles, continuous speech, and small words. The small words problem appears when augmenting two adjacent small words to form one compound word that does Vocabulary (OOV) problem. OOV is a percentage of the unsatisfied requests among sequence of nearest match pronunciation (might be wrong) will be chosen, conse-ASRs performance, OOV should be reduced as much as possible. This reduction in compound words) to the dictionary. 
Augmenting small words is like the cross-word modeling, a well known approach to augment words according to the phonolog ical rules. As a comparison, the cross-word method does not pay attention to the words length, unlike the small word model-ing which is based exclusively on the words length, without caring about phonology. Cross-word modeling demonstrated in many publications, and for many languages. English. Research work for other languages such as Dutch, French, Malay, Mandarin, Korean, etc., also exists. 
In contrast with phonological rules appr oach, a data-driven approach can also be used to augment small words. The Data-drive n approach depends solely on the corpus transcription to generate compound words. In [5], a significant enhancement was achieved when applying the data-driven approach to model small-words for US Eng-lish. To the authors' knowledge, the small-word problem has not been tackled in mod-formal speeches. 
This paper is organized as follows: Section 2 elaborates on the motivation. We de-scribe the Baseline system in Section 3. In Section 4, we present the Arabic phoneme Sections 7 and finally conclude the work in section 8. Unlike isolated speech, continuous speech is known to be a source of augmenting words. This augmentation depends on many factors such as the phonology of the language and the lengths of the words. Our work is focused on adjacent small words being a source of this merging of words. During our previous research work in Arabic speech recognition, it became ev ident that adjacent small wo rds contribute negatively to achieving less than optimal performance. Table 1 presents an example of the small-word problem. Table 1 shows that small words were negatively affected by the concatenations. The decoder mistakenly recognized two separated small words as one word, although words as one word, a better performance will be achieved. The Baseline system used is the Arabic speech recognition system developed at KFUPM. The system was built using CMU Sphinx 3 speech recognition engine by Ali M. [9]. The engine uses 3-emmiting states Hidden Markov Models (HMM) for density of 8 Gaussian mixture distributions. The Baseline system is trained using (144 by male speakers, 105 by female speakers), summing up to 5.4 hours of speech. was taken to exclude recordi ngs with background music or excessive noise, some of mental noise such as that of a reporter in an open area, e.g., a stadium or a stock mar-ket, and low level overlapping foreign speech, occurring when a reporter is translating foreign statements. The 4572 wav files were completely transcribed with fully diacri-tized text. even if they were grammatically wrong. It is a common practice in MSA and most Arabic dialects to drop the vowels at the end of words; this situation is represented in the transcription by either using a silence mark (Sukun) or dropping the vowel, which words. The vocabulary list contains 14,234 words. The Baseline WER is 13.39%. A phoneme is the basic unit of the speech that is used in ASR systems. Table 2 shows the listing of the Arabic phoneme set used in the training, and the corresponding pho-neme symbols. The table also shows illustrative examples of vowel usage. This pho-neme set is chosen based on the previous experience with Arabic text-to-Speech sys-tems in [6],[7],[8], and the corresponding phoneme set which is successfully used in the CMU English pronunciation dictionary. 
Ali M. In [9] contains more elaborate information about the Arabic phoneme set (40 phonemes) used in this work. Phonetic dictionaries are essential components of large vocabulary natural language speaker-independent speech recognition system. For each transcription word, the phonetic dictionary contains its pronunciation in terms of phonemes sequence. Ali M. in [9] developed an application to generate a dictionary for transcriptions. We utilized proposed method. Ali M. dictionary already takes care for some of within-word varia-tion such as: dictionary:  X  X   X  X  X  X   X  X   X  E AE D IH M B R AA H (default)  X  X   X  X  X  X   X  X   X  (2) E AE D IH M B R AA T  X  X   X  X  X  X   X  X   X  (3) E AE D IH N B R AA H  X  X   X  X  X  X   X  X   X  (4) E AE D IH N B R AA T 
The Baseline dictionary contains 14234 words (without variants) and 23840 words (with variants) Modeling the small-word problem is a data-driven approach in which a compound word is distilled from the corpus transcrip tion. The compound word length is the total length of the two adjacent small words that form the corresponding compound word. experiments were made to choose the best small word X  X  length. As an illustrative example, suppose as shown in Figure 1 that the sentence has many words, and that w2 and w3 are small words. According to our method, w2 and w3 will be merged to gen-erate a compound word. It is worth mentioning that no phonological rules or any kind of knowledge based approaches are involved in this merging. Figure 1 also shows that the boundary appearing between word 2 and word 3disappears after merging. 
The generated compound words are then filtered to remove all duplicates. Finally, The process can be explained in the following example: text in bold represent two words, one 2-letter word followed 1 by one three-letter word. The second one shows that the two small words found in the first sentence were sentences will be appended during corpus transcription to generate the enhanced pro-nunciation dictionary and the enhanced language model. The expansion of the pronunciation dictionary and the language model depends on the length of small expand more. The proposed method can be described by the following steps: Small-word pronunciation modeling algorithm While there are more corpus transcription Read next two adjacent words If their length is less than or equal certain threshold Merge them into a new compound word Add the compound word to the Baseline dictionary Represent the compound word in the related sentence Add the generated sentence to corpus transcription Build the language model 
After finishing the recognition process, the results will be scanned for decompos-ing compound words to return them back to their original form. This process can be training set and a testing set. The training set contains around 4.3 hours of audio while the testing set contains the remaining 1.1 hour. We use the CMU language toolkit to of 5.4 hours of audio. The enhanced language model is built after expanding the Base-line transcription. In order to analyze the effect of the length of the small words on the system performance, we compare the results of our approach when applied on com-pound words of lengths 5,6,7,8,9,10,11,12 and 13. Table 3 summarizes the results of executing the 9 experiments. We use the following shorthand for the keys in Table 3: TL: Total Length of the two adjacent small words. 
Table 3 shows that the best reduction of 2.16% in (WER) is achieved when the length of the compound word is 10. It also shows that performance noticeably de-creases when the number of characters in the compound words exceeds 10. Figure 2 shows the accuracy of the system with respect to the words length. 
With 87.79% accuracy of the Baseline system, Figure 2 shows that the accuracy of the Enhanced system starts increasing until a specific compound word X  X  length (10), and then starts decreasing. The reason of this reduction in accuracy is the confu-compound words does not openly increase the performance. There is a maximum limit to utilize these compound words, after this limit the performance start decreasing according to the ambiguity occurred in the language model. Figure 3 shows that 510 compound words used (see table 3, TL=13) do not help to maintain the performance. 
The standard measure for language model quality is perplexity. The perplexity for the Baseline language model is 32.88 which is based on 9288 words (testing set words) words. For the Enhanced system, the perplexity is 7.14 computed based on the same testing set words (9288 words). This means that the performance of the En-hanced system is better than the Baseline system since it has a lower perplexity value. With regard to the enhancement, we us ed the performance detection method sug-gested in [10] to investigate the signific ance of the achieved enhancement. We used a the total number of words in the Baseline corpus and the word accuracy of the Base-Since our achieved enhancement is greater (2.16%), we can conclude that it is a statis-tically significant improvement. 
The great impact on the perplexity could be understood in two ways: first, the ro-W=w1,w2,...,w N ., therefore reducing the perplexity according the perplexity formula: Where PP is the perplexity, P is the probability, and N is the total number in the test-ing set. Second, the 2153 compound words (see table 3, TL=10) added to the tran-scription as new words have an extremely low perplexity. For example: consider the two words (  X  X  ) and (  X  X  X  ). These two words have an average certain perplexity. When the compound word (  X  X  X  X  X  ) represented in the language model, it will share others with about the effect of compound words, Saon and Padmanabhan in [5] showed mathe-matically that compound word will enhance the performance. They demonstrated that compound words have the effect of incorporating a trigram in dependency in a bigram language model, as an example. acoustic model of all training utterances has not been changed during the experiments. We proposed and tested a data-driven approach to model the small words for modern standard Arabic MSA. Our method depends on augmenting adjacent small words and have them represented in the pronunciation dictionary and the language model as well. The results show that our method significantly improves the performance, par-ticularly for compound words of lengths 10. The perplexity of the language model was also enhanced. Acknowledgements. This work has been supported by Saudi Arabia National Science and Technology Program grant # NSTP 08-INF100-04. The authors would like also to support of this research work. 
