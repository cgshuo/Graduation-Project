 Marco Cuturi mcuturi@i.kyoto-u.ac.jp Graduate School of Informatics, Kyoto University Alexandre d X  X spremont alexandre.daspremont@m4x.org CNRS -Ecole Polytechnique Isolating stable linear combinations of variables of multivariate time series is a fundamental problem in econometrics. A classical formulation of the prob-lem reads as follows: given a vector valued process x =( x t ) t taking values in R n and indexed by time t  X  N , and making no assumptions on the stationarity of each individual component of x , can we estimate one (or many) directions y  X  R n such that the univariate process ( y T x t ) is stationary? When such a vector y ex-ists, the process x is said to be cointegrated. The goal of cointegration techniques is to detect and estimate such directions y .
 Here, we use financial applications as the main test-ing ground for these techniques. Stationary processes typically exhibit significant mean-reversion, i.e. aten-dency to pull back to their mean, and mean rever-sion creates statistical arbitrage opportunities: a sim-ple strategy of buying the asset below the mean and selling it short above will produce positive returns on average. Of course, stationary assets are not common in financial markets, assets that exhibit fast mean re-version even less so, hence arbitrageurs often resort to creating synthetic assets that have this property. Such synthetic assets are usually long-short baskets that are built by combining positive (long) and negative (short) positions in di ff erent liquid assets. The size of each of these positions (vector y in the previous paragraph) is usually computed using cointegration methods. Since the original work of Engle &amp; Granger (1987), several techniques have been proposed to form coin-tegrated baskets under various modeling assumptions and we refer the reader to (Maddala &amp; Kim, 1998) and (Johansen, 2005) for a more complete survey. Financial applications (Tsay, 2005,  X  8.6) and in par-ticular optimal trading strategies for mean reverting baskets were discussed in (Jurek &amp; Yang, 2007; Liu &amp; Timmermann, 2010; Elie &amp; Espinosa, 2011), while the problem of isolating sparse mean reverting baskets was discussed in (d X  X spremont) using a criterion de-rived in (Box &amp; Tiao, 1977).
 Mean-reverting strategies cannot, however, only rely on mean-reversion to be profitable. Arbitrage oppor-tunities can only exist if they are large enough to be traded without using too much leverage or incur-ring too much transaction costs. For mean-reverting baskets, this condition translates naturally into the requirement that the gap between the basket valua-tion and its long term mean is large enough on aver-age, namely that the basket price has su ffi cient vari-ance. Here, we argue that classical cointegration tech-niques are ill-suited for identifying statistical arbi-trage opportunities because they focus exclusively on mean-reversion without considering variance. In con-trast, the methods we develop here maximize a proxy for mean reversion, while constraining variance to be higher than a certain threshold.
 We use three di ff erent criteria as proxies for mean re-version: predictability ;the portmanteau statistic; the crossing statistic. To our knowledge, the latter two criteria were never considered before as criteria to esti-mate cointegrated relationships, and for a good reason: the problem of optimizing these criteria over normal-ized basket weights is nonconvex. We show however that these problems can be e ffi ciently approximated by semidefinite programs. These relaxations are ex-act in some of the settings detailed below, and explicit tightness or uniform approximation results related to the S -lemma (Ben-Tal et al., 2009) control the quality of the solutions.
 The paper is organized as follows. We focus first in Section 2 on various measures and proxies for mean reversion. Section 3 defines the basket optimization problems corresponding these quantities. We show in Section 4 that each of these problems translate nat-urally into semidefinite relaxations which produce ei-ther exact or approximate solutions. Section 5 briefly summarizes the complexity of solving the resulting semidefinite programs. Finally, we present numerical evidence in Section 6. Throughout this paper, we write S n for the n  X  n cone of positive definite matrices. We consider in the fol-lowing a multivariate stochastic process x =( x t ) t  X  N taking values in R n . We write A k = E [ x t x T t + k ] ,k  X  0 for the lag-k autocovariance matrix of x t if it is finite. Using a sample path x of ( x t ), where x =( x 1 ,..., x T and each x t  X  R n , we write A k for the empirical coun-terpart of A k computed from x , Given y  X  R n , we now define three measures which can all be interpreted as proxies for the mean rever-sion of y T x t . Predictability  X  defined for station-ary processes by Box &amp; Tiao (1977) and generalized for non-stationary processes by Bewley et al. (1994)  X  measures how close to noise the series is. The port-manteau statistic (Ljung &amp; Box, 1978) is used to test whether a process is white noise. Finally, the crossing statistic (Ylvisaker, 1965) measures the probability that a process crosses its mean per unit of time. In all three cases, low values for these criteria imply a fast mean-reversion. 2.1. Predictability We briefly recall the canonical decomposition derived in (Box &amp; Tiao, 1977). Suppose that x t follows the recursion: where  X  x t  X  1 is a predictor of x t built upon past val-ues of the process recorded up to t  X  1, and  X  t is a vector of i.i.d. Gaussian noise with zero mean and co-variance  X   X  S n independent of all variables ( x r ) r&lt;t The canonical analysis in (Box &amp; Tiao, 1977) starts as follows.
 Univariate case. Suppose n =1andthus  X   X  R + , Equation (2) leads thus to by introducing the variances  X  2 and  X   X  2 of x t and  X  x respectively. Box &amp; Tiao measure the predictability of x t by the ratio The intuition behind this variance ratio is simple: when it is small the variance of the noise dominates that of  X  x t  X  1 and x t is almost pure noise, when it is large however,  X  x t  X  1 dominates the noise and x t is al-most perfectly predictable.
 Multivariate case. Suppose n&gt; 1 and consider now the projected process ( y T x t ) t with weights y  X  R n . Using (2) we know that y T x t = y T  X  x t  X  1 + y T  X  and we can measure its predicability as where  X  A 0 and A 0 are the covariance matrices of x t and  X  x t  X  1 respectively. Minimizing predictability  X  ( y ) is then equivalent to finding the minimum generalized eigenvalue  X  solving Assuming that A 0 is positive definite, the basket with minimum predictability will be given by y = A  X  1 / 2 0 y where y 0 is the eigenvector corresponding to the small-Estimation of  X  ( y ) . All of the quantities used to de-fine  X  above need to be estimated from sample paths. A 0 can be estimated by A 0 following Equation (1). All other quantities depend on the predictor  X  x t  X  1 .Box&amp; Tiao assume that x t follows a vector autoregressive model of order p  X  VAR(p) in short  X  and therefore  X  x t  X  1 takes the form, where the p matrices ( H k ) contain each n  X  n autore-gressive coe ffi cients. Estimating H k from the sample path x , Box &amp; Tiao solve for the optimal basket by inserting these estimates in the generalized eigenvalue problem above. If one assumes that p =1(thecase p&gt; 1 can be trivially reformulated as a VAR(1) model with adequate reparameterization), then and thus the Yule-Walker estimator (L  X utkepohl, 2005,  X  3.3) of H 1 would be H 1 = A  X  1 0 A 1 . Minimizing pre-dictability boils down to solving in that case min which is equivalent to computing the smallest eigen-variance matrix A 0 is invertible.
 The machinery of Box &amp; Tiao to quantify mean-reversion requires defining a model to form  X  x t  X  1 ,the conditional expectation of x t given previous observa-tions. We consider in the following two criteria that do without such modeling assumptions. 2.2. Portmanteau Criterion Recall that the portmanteau statistic of order p (Ljung &amp; Box, 1978) of a centered univariate stationary pro-cess x (with n = 1) is given by of x t . The portmanteau statistic of a white noise pro-cess is by definition 0 for any p . Given a multivariate ( n&gt; 1) process x we write for a coe ffi cient vector y  X  R n . By construction,  X  ( y )=  X  p ( ty )forany t % = 0 and in what follows, we will impose &amp; y &amp; 2 = 1. The quantities  X  p ( y )arecom-puted using the following estimates (Hamilton, 1994, p.110): 2.3. Crossing Statistics Kedem &amp; Yakowitz (1994,  X  4.1) define the zero cross-ing rate of a univariate ( n =1)process x (its expected number of crosses around 0 per unit of time) as A result known as the cosine formula states that if x t is an autoregressive process of order one AR(1), namely if | a | &lt; 1,  X  t is i.i.d. standard Gaussian noise and then (Kedem &amp; Yakowitz, 1994,  X  4.2.2): Hence, for AR(1) processes, minimizing the first order autocorrelation a also directly maximizes the crossing rate of the process x .For n&gt; 1, since the first order autocorrelation of y T x t is equal to y T A 1 y ,wepropose to minimize y T A 1 y and ensure that all other absolute autocorrelations | y T A k y | , k&gt; 1 are small. Given a centered multivariate process x ,weform its covariance matrix A 0 and its p autocovariances ( A 1 ,...,A p ). Because y T Ay = y T ( A + A T ) y/ 2, we symmetrize all autocovariance matrices A i .Wefocus in this section on baskets that exhibit both mean re-version and su ffi cient volatility, that is have a variance that exceeds a given threshold  X  &gt; 0. Note that for the variance of ( y T x t ) to exceed a level  X  , the largest eigen-value of A 0 must necessarily be larger than  X  , which we always assume in what follows. To highlight the central role of the covariance matrix A 0 , we rename it to B def = A 0 in the rest of the paper. 3.1. Minimizing Predictability Minimizing Box-Tiao X  X  predictability  X   X  defined in  X  2.1 while ensuring that the variance of the resulting pro-cess exceeds  X  , means solving the following QCQP: in the variable y  X  R n with M def = A 1 B  X  1 A T 1 and B = A 0 ,where M, B  X  S n . Without the normaliza-a generalized eigenvalue problem in the pair ( M, B ). That problem quickly becomes unstable when B is ill-conditioned or M is singular. Adding the normaliza-tion constraint &amp; y &amp; 2 = 1 resolves those problems yet does not a ff ect the relaxation results that follow in  X  4. 3.2. Minimizing the Portmanteau Statistic Using a similar formulation, we can also minimize the order p portmanteau statistic defined in  X  2.2 while en-suring a minimal variance level  X  by solving: in the variable y  X  R n ,forsomeparameter  X  &gt; 0. Problem (P2) has a natural interpretation: the ob-jective function directly minimizes the portmanteau statistic, while the constraints normalize the norm of the basket weights to one and impose a variance larger than  X  . We will see in what follows that (P2) can be solved exactly using a semidefinite relaxation when p = 1. Also, while solving (P2) exactly is hard when p&gt; 1, semidefinite relaxations produce tractable solu-tions with uniform approximation bounds. 3.3. Minimizing the Crossing Statistic Following the results in  X  2.3, maximizing the crossing rate while keeping the rest of the autocorrelogram low, in the variable y  X  R n ,forsomeparameters  X ,  X  &gt; 0, will produce processes that are close to being AR(1), while having a high crossing rate. In this section, we detail convex relaxations to the problems detailed above in Section  X  3. 4.1. Exact Solutions for Predictability We can form a convex relaxation of the predictability optimization problem (P1) over the variable y  X  R n : using the lifting argument of (Lov  X asz &amp; Schrijver, 1991), i.e. writing Y = yy T ,so(P1)becomes We can relax this last problem by dropping the rank constraint, to get which is a semidefinite program in Y  X  S n . We call Y ! the optimum solution to this problem. By construc-tion, the optimal value of (SDP1) is an upper bound on that of (P1). Here however, the two problems are in fact equivalent. Brickman (1961) showed that the range of two quadratic forms over the unit sphere is a convex set when the ambient dimension n  X  3, which means in particular that for any two square matrices A, B of dimension n { ( Tr ( AY ) , Tr ( BY )) : Y  X  S n , Tr Y =1 ,Y ' 0 } We refer the reader to (Barvinok, 2002,  X  II.13) for a more complete discussion of this result. This means that for any solution Y ! of the relaxation (SDP1) there exists a vector y ! which satisfies &amp; y &amp; 2 2 = Tr ( Y y ! T By ! = Tr ( BY ! )and y ! T My ! = Tr ( MY ! ) which means that y ! is an optimal solution of the original problem (P1). Boyd &amp; Vandenberghe (2004, App. B) show how to explicitly extract such a solution y ! from a matrix Y ! solving (SDP1). We detail this in  X  5. 4.2. Portmanteau: Exact Solution when p =1 Using the same lifting argument and writing Y = yy T , we can bound the optimum of problem (P2) by solving a semidefinite program in Y  X  S n .When p =1the objective becomes | Tr ( A 1 Y ) | and this program can be further simplified to which is a semidefinite program in the variables Y  X  S n and t  X  R . Using Brickman X  X  theorem as above, we can recover a vector y ! which solves (P2) from an op-timal solution Y ! to problem (SDP2bis). When p&gt; 1 this tightness result does not hold and we detail uni-form approximation results in what follows.
 4.3. Portmanteau: Approximations for p&gt; 1 Program (P2) is a nonconvex quadratically con-strained quadratic program which is hard to solve exactly. However, randomization arguments (Ne-mirovski et al., 1999; Nemirovski, 2007; So, 2009; Ben-Tal et al., 2009) show that if we call OPT the solution of the original problem in (P2) and SDP the solution to its relaxation in (SDP2), we have uniform approxi-mation bounds with where c&gt; 0 is an absolute constant. These results are constructive and good approximate solutions to (P2) can be constructed from optimal solutions to (SDP2) using randomization. 4.4. Approximate Solutions for Crossing Stats As above, we can write a semidefinite relaxation for problem (P3), and solve the semidefinite program minimize Tr ( A 1 Y )+  X  ' p i =2 Tr ( A i Y ) 2 subject to Tr ( BY )  X   X  The same randomization arguments show that this re-laxation produces solutions which are suboptimal by afactoratmost c log p (except when p = 1 where it is exact, since the right-hand side term weighted by  X  disappears). In this section, we detail how to e ffi ciently solve the semidefinite relaxations (SDP1) and (SDP2). 5.1. Predictability We can form the dual of problem (SDP1) by writing the Lagrangian in the variables Y  X  S n and w  X  R . Minimizing this Lagrangian over the set { Y : Tr Y =1 ,Y ' 0 } and using means the dual of problem (SDP1) is written in the variable w  X  R .TheKarush-Kuhn-Tucker (KKT) optimality conditions (Boyd &amp; Vandenberghe, 2004,  X  5.9.2) for this pair of problems are then given by Two scenarios arise depending on the multiplicity of  X  min ( M + wB ):  X  Nondegenerate case. Suppose that  X  min ( M + wB )  X  Degenerate case. Let Y  X  S n and w  X  R be op-The result above shows that in the nondegenerate case (typical here), it su ffi ces to solve the minimum eigen-value maximization problem in (6) to get a solution to (P1). We X  X l see below that the complexity of solv-ing that program is in fact very low. In the degenerate case, if k is the multiplicity of the maximum eigenvalue  X  min ( M + wB ) at the optimal w ,weneedtocompute the matrix U defined above at a cost of O ( kn 2 )and solve the semidefinite feasibility problem in (8) to find Y (Ben-Tal &amp; Nemirovski, 2001,  X  6.6.3).
 Proposition 1. Let ( &gt; 0 be a target precision, the complexity of solving the dual problem in (6) in the variable w  X  R ,growsas O ( n 2 log 2 (1 / ( )) . Proof. The function  X  min ( M + wB ) is convex in w , hence we can minimize it by bisection. At each iteration, forming a gradient amounts to comput-ing a leading eigenvector using iterative algorithms such as the power or Lanczos methods (see Golub &amp; Van Loan (1996, Chap. 8-9) for example), at a cost of O ( n 2 ). Reaching a target precision ( then requires O ( n 2 log 2 (1 / ( )) flops.
 5.2. Portmanteau Statistic We first compute a dual of problem (SDP2) by rewrit-ing the problem as because this is a convex saddle-point problem where one of the feasible sets is compact, we can get a dual by switching the min and the max, to get in the variables y  X  R p and w  X  R . Using a smoothing argument, Nesterov (2007) showed that given a bound  X  on the Euclidean norm of the solution, the complex-ity of solving (9) using a first-order method grows as where ( is the target precision and is computed from the autocovariance operators. A similar bound holds for (SDP3). In this section, we evaluate the ability of our tech-niques to extract mean-reverting baskets with su ffi -cient variance from tradeable assets. We measure per-formance by applying to these baskets a trading strat-egy designed specifically for mean-reverting processes. We show that, under realistic trading costs assump-tions, selecting mean-reverting baskets with su ffi cient variance translates into lower incurred costs and thus improves the performance of trading strategies. 6.1. Historical Data We consider daily time series of option implied volatili-ties for 210 stocks from January 4 2004 to December 30 2010. A key advantage of using option implied volatil-ity data is that these numbers vary in a somewhat limited range. Volatility also tends to exhibit regime switching, hence can be considered piecewise station-ary, which helps in extracting structural relationships. We illustrate a sample time series from this dataset in Figure 1 corresponding to Apple X  X  stock. 6.2. Mean-reverting Basket Estimators We compare the three basket selection techniques detailed here  X  predictability , portmanteau and crossing statistic ( p = 3)  X  with three classical cointegration estimators: that which arises from the Johansen VEC model (Johansen, 1991), orthogonal least-squares ( OLS ) estimation  X  equivalent to select-ing the eigenvector with the smallest eigenvalue of the variance matrix A 0 (Maddala &amp; Kim, 1998,  X  6.7.1)  X  and the fully modified OLS ( FM-OLS )procedure described by Phillips (1995). None of these classical techniques takes into account variance when estimat-ing the weights of a co-integrated relationship. 6.3. Jurek &amp; Yang (2007) Trading Strategy While option implied volatility is not directly trad-able, it can be synthesized using baskets of call options, and we assimilate it to a tradable asset with (signif-icant) transaction costs in what follows. For baskets of volatilities isolated by the techniques listed above, we apply the (Jurek &amp; Yang, 2007) strategy for log utilities to the basket process recording out of sample performance. Jurek &amp; Yang propose to trade a station-ary autoregressive process ( x t ) t of order 1 and mean  X  governed by the equation x t +1 =  X  x t +  X  X  t ,where |  X  | &lt; 1, by taking a position N t in the asset x t which is proportional to In e ff ect, the strategy advocates taking a long (resp. short) position in the asset whenever it is below (resp. above) its long-term mean, and adjust the position size to account for the volatility of x t and its mean reversion speed  X  . Given basket weights y , we apply standard AR estimation procedures on the in-sample portion of y T x to recover estimates for  X   X  and  X   X  and plug them directly in Equation (10). This approach is illustrated for two baskets in Figure 2.
 6.4. Transaction Costs We assume that fixed transaction costs are negligible, but that transaction costs per contract unit are in-curred at each trading date, varying the size of these costs across experiments. We let the transaction cost per contract unit vary between 0 and 0.14 cents by increments of 0.02 cents. Since the average value of a contract over our dataset is about 40 cents, this is akin to considering trading costs ranging from 0 to 35 Base Points (BP), that is 0 to 0.35%. 6.5. Experimental Setup We consider 20 sliding windows of one year (255 trad-ing days) taken in the history, and consider each of these windows independently. Each window is split between 85% of days to estimate and 15% of days to test-trade our models, resulting in 38 test-trading days. We do not recompute the weights of the bas-kets during the test phase. All 210 stocks are divided into 13 di ff erent groups depending on their economic sector, resulting in 13 asset pools whose size varies be-tween 3 assets and 43 assets. Because all combinations of stocks are not necessarily mean-reverting, we select smaller candidate pools of n assets through a greedy backward-forward selection scheme, where 2  X  n  X  8. We score each pool by the mean-reversion speed ob-tained, the faster the better. We use the OLS estima-tor for this computational intensive phase because it is the fastest to compute. We keep the best 50 asset pools of each window, and use each technique separately on these candidates. One such pool was, for instance, composed of the stocks { STI,ZION,GE,FITB,BAC,XL } history in 2010. Figure 2 examines this pool in detail, and shows the results of two trading experiments. 6.6. Results In Figures 3 and 5, we plot the average of the Sharpe ratio and the total return (computed during the 38 days trading period) over the 20  X  50 = 1 , 000 baskets estimated in our experimental set versus transaction costs. In all cases, we have set the variance bound  X  to be 0 . 3 times the median of all variances of assets available in a given asset pool. In both figures, we observe that returns and Sharpe ratio decrease faster for the three classical cointegration methods than for the three techniques detailed here. These empirical observations agree with the intuition of this paper: cointegration techniques can produce synthetic baskets with high mean-reversion but low variance. Trading an asset with low variance translates in practice into high trading costs and thus badly performing trading strategies. The three techniques detailed in this paper manage instead to achieve a trade-o ff between desir-able mean-reversion properties with su ffi cient variance to allow for lower overall transaction costs. Finally, the bell-shaped curves of Figure 5 show the importance of setting a variance threshold  X  within a reasonable range as trading costs increase. Indeed, in a typical trading environment (where costs are between 10 or 20 BP), Figure 5 shows that trading o ff some mean-reversion to gain variance instead is needed to remain profitable. We have described three di ff erent criteria to quantify the amount of mean reversion in a time series. For each of these criteria, we have detailed a tractable al-gorithm to isolate a vector of weights that has opti-mal mean reversion, while constraining the variance (or signal strength) of the resulting univariate series to be above a certain level. We show that this bound on variance, together with our new criteria for mean reversion can significantly improve the performance of mean reversion statistical arbitrage strategies. These approaches can also be used in more general settings, for instance in the context of estimating alarm func-tionals (Cuturi et al., 2010) for anomaly detection in non-stationary multivariate time series, or might be applied to detect relationships of interest to analyze EEG data (von B  X unau et al., 2009; Hara et al., 2010). We thank anonymous reviewers for their comments. MC acknowledges the support of the Japanese Society for the Promotion of Science grant 23700172 and AA acknowledges the support of the European Research Council (starting grant SIPA).

