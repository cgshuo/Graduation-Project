 Automatic grading systems for summaries and essays have been studied for years. Most commercial and research im-plementations are based in statistical methods, such as La-tent Semantic Analysis (LSA), which can provide high ac-curacy on similarity between the essay and the graded or standard essays, but they can offer very limited feedback. In the present work, we propose a novel method to provide both grades and meaningful feedback for student summaries by Ontology-based Information Extraction (OBIE). We use ontological concepts and relationships to create extraction rules to identify correct statements. Based on ontology con-straints (e.g., disjointness between concepts), we define pat-terns that are logically inconsistent with the ontology to cre-ate rules to extract incorrect statements. Experiments show that the grades given to 18 student summaries on Ecosys-tems by OBIE are correlated to human gradings. OBIE also provide meaningful feedback on the errors those students made in their summaries.
 H.4 [ Information Systems Applications ]: Miscellaneous X  Experimentation Automatic Grading, Information Extraction, Ontology
A reading-comprehension strategy that forces a student to retrieve or recall information from memory has a potent ef-fect on learning, enhancing long-term retention of the tested An alysis (LSA) [3]. LSA treats each essay as a matrix of word frequencies and applies singular value decomposition (SVD) to the matrix to find an underlying semantic space . It then represents each to-be-graded essay in that space as vectors and assesses the cosine similarity between the es-say and the graded or standard essays or the text students read. The cosine similarity can be transformed to the grade. LSA-based systems have shown to be very accurate and with significant correlation when compared with human grading.
SAGrader [1] provides a representation of the domain from which the summary is going to be made by combining a pat-tern matching approach with semantic networks. SAGrader analyzes students X  essays and then provides students with a grade, and limited feedback that indicates the items a stu-dent mentioned and those they did not based on the domain representation. However, SAGrader does not provide feed-back on any additional or inaccurate content a student may have included.

Following the ideas presented by SAGrader, it seems promis-ing that Ontology-based Information Extraction (OBIE) can be used for automatic summary and essay grading because OBIE provides a domain representation to help identify con-cepts and relationship over free text, which is a similar task to grading summaries and essays. Using OBIE for automatic summary and essay grading offers the same advantages that semantic networks offer, such as the possibility of generat-ing feedback and no need of gold standard summaries. Also, ontologies are more expressive than semantic networks by al-lowing to represent disjointness and negations, which cannot be done by semantic networks [8].

To accurately evaluate a summary and provide meaning-ful feedback, we need to identify the correct and incorrect statements that are in it. However, to the best of our knowl-edge, no research has been done in OBIE to identify and/or extract statements that are logically inconsistent with re-spect to the ontology. An ontology can become logically inconsistent through changes made to it [4]. Research has been done to prevent inconsistency [2], and to eliminate in-consistency by detecting its source [4, 11]. Although the mentioned work does not analyze text inconsistency based on an ontology, it does give an insight on how this problem could be approached. Since the statements of a summary should be entailed from the domain ontology, if a statement of a summary is incorrect, it will be inconsistent with the ontology. So, understanding how ontology inconsistency is managed can lead to mechanisms to identify and extract incorrect summary statements.
In this section, details regarding the design and imple-mentation of the proposed OBIE method are presented.
The student summaries we will use in this paper were collected in an earlier study that looked at the use of elec-tronic strategies (eStrategies) for reading comprehension for college students [9]. The study included 18 subjects with a range of reading abilities (from high to low). As part of the study, adult students were asked to read four 500-word passages that were drawn from introductory college science textbooks, then provide oral summaries of each arti-cle. The oral summaries were manually transcribed into the text form. The study produced a two-part score for each sc ale well but require large data sets for training and test-ing [13]. Since the data size (i.e., number of summaries) for the present work is small, extraction rules are selected as information extractors for the summary grading system.
From the summaries, we identified three types of state-ments made by the students: correct statements, incorrect statements, incomplete statements.
An ontology formally defines the concepts and relation-ships in a domain. The relationships can be seen as triples of the form concept1 relationship concept2 . The triple can be mapped to the typological form of a sentence ( subject verb object ), where subject maps to concept1 , verb maps to relationship , and object maps to concept2 .
 This leads to an extraction rule for each relationship. Since properties of a concept are inherited by its sub-concepts or by its equivalent concepts, we consider the use of first or-der logic (FOL) rules to combine sets of axioms from the ontology into a smaller set of logical rules to avoid the cre-ation of an oversized set of extraction rules. The resulting logical rules contain concepts and properties from the orig-inal set of ontological axioms. In other words, the set of original axioms entail the new logical rules, which is an ex-panded representation of the original set.

Rules for correct statements can identify which concepts and relationships are presented in the summary, and which are not presented. A total of 31 extraction rules are created from the ontology. Similar to SAGrader [1], the feedback tells how much of a student X  X  summary is contained in the ontology and how much is missing from it.
If we consider that statements in a summary should be en-tailed from the domain ontology, an incorrect statement will be inconsistent with the ontology. Wang et al. [11] proposed a heuristic to identify the cause of inconsistency in an ontol-ogy based on common errors they observed in tutorials and workshops regarding ontology creation. They proposed a set of rules to detect these common errors (properties with conflicting domain or range, ignoring disjointness between classes, and conflicting axioms through propagation of ax-ioms) in inconsistent ontologies.

Common errors are having properties with conflicting do-main or range, ignoring disjointness between classes, and conflicting axioms through propagation of axioms. Follow-ing the common errors identified by Wang et al. [11] and the constraints presented in the ontology, we can create a set of logic rules on inconsistency. The extraction rules cre-ated from the inconsistency logic rules should be able to help identify incorrect statements.

From the consistent logic rules, approximately 84 rules on inconsistency can be derived. However, only 16 extraction rules for incorrect statements are used since this small set already covers almost all the incorrect statements made by 18 students.
Statements that include a concept or relationship that is not defined in the ontology are considered incomplete. The most frequent type of incomplete statements is related to a OBIE (Relevance, Completeness, Importance). extraction is correct, Recall measures how complete is the extraction. The F1 measure is the average between Precision and Recall that provides an overall measure of the system.
Using the set of 18 summaries from the Sohlberg et al. study [9], OBIE provided numeric scores for each. The three previously mentioned grading metrics are presented in Ta-ble 3 where they are compared to human grading and LSA based grading [9].
 Table 4 presents the Spearman correlation between the OBIE grades (Relevance, Completeness, and Importance) and grades from the Sohlberg et al. study (Expert Grade and LSA) [9]. Spearman X  X  correlation measures the depen-dency between variables described by a monotonic function. T able 4: Spearman correlation between grading metrics ( p&lt;: 05 )
Because the ranges of grades from human graders, LSA, and our systems are totally different, we only conducted correlation studies among them. We found that the grades from our OBIE system resulted in a positive correlation with human grading. In other words, agreement was suggested between scoring by the human grader and OBIE. On the other hand, no correlation occurred between the LSA and OBIE grades, as seen in Table 4. Interestingly, LSA grading in sight into her high score given the number of errors we found. Our note to her prompted her to look at her raw scores again and find a typo -her raw score was 7 not 17. We used her change in Table 3.

We can speculate that the high LSA score is because of many article concepts and relationships being mentioned, whether they are rightly or wrongly stated.

For STIR26, LSA provides a high-water-mark in scoring with 1.148. The human score is 3 (low-water mark). The OBIE score is 0 across the board (low-water mark). OBIE found 2 errors in the first statement: 1. Carnivores are not a sub-class of fish. 2. Assuming a transpose error, the statement  X  X ish are
The second statement is marked as non-relevant by OBIE -it has no concepts or relationships that occur in the on-tology. We can only guess why LSA gave this summary its highest score.

A key point here is that we are comparing numeric scores to make sure we are in the right ballpark with the human grader -lacking her score breakout, we have nothing other to compare against. However, the key goal of our work is to provide not a numeric score to a student, but feedback. And OBIE provides the raw basis for such feedback. The errors that we delineate are the grist for helping a student rework his or her conceptualization. While we did not attempt to provide tutoring as part of this study, it is one of our overall goals. Beyond simply showing errors, we can foresee a more sophisticated approach to hold tutoring dialogs similar to that reported in [10]. Recognizing conceptually correct, in-correct, and incomplete ideas at the statement level is what drives our work. We have presented a novel Ontology-based Information Extraction system for grading summaries that correlates well, on a numeric level, with an expert human grader. At the same time, our approach provides meaningful feedback to the students about the incorrect statements they have made in the summaries.

In terms of the underlying OBIE system, there are sev-eral goals we have that are discussed below. We consider integrating text taxonomy as a complement to the ontology to allow a better link between the domain knowledge and importance of each idea in the text. Bring some automation to the process would widen the available material that we could support with our approach, such as machine learning (e.g., classification) techniques as information extractors, or automatically generating extraction rules.
We thank Daya C. Wimalasuriya for discussions on the component based OBIE systems. This research is partially supported by the National Science Foundation grant IIS-1118050 and grant IIS-1013054. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the of-ficial policies, either expressed or implied, of the NSF.
