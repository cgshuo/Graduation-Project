 In scientific literature, scholars use citations to re-fer to external sources. These secondary sources are essential in comprehending the new research. Previous work has shown the importance of cita-tions in scientific domains and indicated that ci-tations include survey-worthy information (Sid-dharthan and Teufel, 2007; Elkiss et al., 2008; Qazvinian and Radev, 2008; Mohammad et al., 2009; Mei and Zhai, 2008).

A citation to a paper in a scientific article may contain explicit information about the cited re-search. The following example is an excerpt from Eisner X  X  work on bottom-up parsers and the notion of span in parsing:  X  X nother use of bottom-up is due to Eisner However, the citation to a paper may not always include explicit information about the cited paper:  X  X his approach is one of those described in Eis-Although this sentence alone does not provide any information about the cited paper, it suggests that its surrounding sentences describe the proposed approach in Eisner X  X  paper:  X ... In an all pairs approach, every possible pair of two tokens in a sentence is considered and some score is assigned to the possibility of this pair having a (directed) dependency rela-tion. Using that information as building blocks, the parser then searches for the best parse for the sentence. This approach is one of those de-scribed in Eisner (1996) . X 
We refer to such implicit citations that contain information about a specific secondary source but do not explicitly cite it, as sentences with con-text information or context sentences for short. We look at the patterns that such sentences cre-ate and observe that context sentences occur with-ing a small neighborhood of explicit citations. We also discuss the problem of extracting context sen-tences for a source-reference article pair. We pro-pose a general framework that looks at each sen-tence as a random variable whose value deter-mines its state about the target paper. In summary, our proposed model is based on the probabilistic inference of these random variables using graphi-cal models. Finally we give evidence on how such sentences can help us produce better surveys of re-search areas. The rest of this paper is organized as follows. Preceded by a review of prior work in Section 2, we explain the data collection and our annotation process in Section 3. Section 4 explains our methodology and is followed by experimental setup in Section 5. calculate inter-judge agreement. Analyzing the structure of scientific articles and their relations has received a lot of attention re-cently. The structure of citation and collaboration networks has been studied in (Teufel et al., 2006; Newman, 2001), and summarization of scientific documents is discussed in (Teufel and Moens, 2002). In addition, there is some previous work on the importance of citation sentences. Elkiss et al, (Elkiss et al., 2008) perform a large-scale study on citations in the free PubMed Central (PMC) and show that they contain information that may not be present in abstracts. In other work, Nanba et al, (Nanba and Okumura, 1999; Nanba et al., 2004b; Nanba et al., 2004a) analyze citation sen-tences and automatically categorize them in order to build a tool for survey generation.

The text of scientific citations has been used in previous research. Bradshaw (Bradshaw, 2002; Bradshaw, 2003) uses citations to determine the content of articles. Similarly, the text of cita-tion sentences has been directly used to produce summaries of scientific papers in (Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Determining the scientific attribu-tion of an article has also been studied before. Siddharthan and Teufel (Siddharthan and Teufel, 2007; Teufel, 2005) categorize sentences accord-ing to their role in the author X  X  argument into pre-defined classes: Own, Other, Background, Tex-tual, Aim, Basis, Contrast.

Little work has been done on automatic cita-tion extraction from research papers. Kaplan et al, (Kaplan et al., 2009) introduces  X  X itation-site X  as a block of text in which the cited text is dis-cussed. The mentioned work uses a machine learning method for extracting citations from re-search papers and evaluates the result using 4 an-notated articles.

In our work we use graphical models to ex-tract context sentences. Graphical models have a number of properties and corresponding tech-niques and have been used before on Information Retrieval tasks. Romanello et al, (Romanello et al., 2009) use Conditional Random Fields (CRF) to extract references from unstructured text in dig-ital libraries of classic texts. Similar work include term dependency extraction (Metzler and Croft, 2005), query expansion (Metzler and Croft, 2007), and automatic feature selection (Metzler, 2007). lished in the Computational Linguistics journal and proceedings from ACL conferences and work-shops and includes more than 14 , 000 papers over a period of four decades (Radev et al., 2009). AAN includes the citation network of the papers in the ACL Anthology. The papers in AAN are publicly available in text format retrieved by an OCR process from the original pdf files, and are segmented into sentences.

To build a corpus for our experiments we picked 10 recently published papers from various areas tal of 203 candidate paper-reference pairs. Table 1 lists these papers together with their authors, titles, publication year, number of references, number of references within AAN, and the number of sen-Table 2: Part of the annotation for N03-1003 with respect to two of its references  X  X in and Pan-tel (2001) X  (the first column)  X  X hinyama et al. (2002) X  (the second column). C s indicate explicit citations, 1s indicate implicit citations and 0s are none. tences. 3.1 Annotation Process We annotated the sentences in each paper from Ta-ble 1. Each annotation instance in our setting cor-responds to a paper-reference pair, and is a vec-tor in which each dimension corresponds to a sen-tence and is marked with a C if it explicitly cites the reference, and with a 1 if it implicitly talks about it. All other sentences are marked with 0 s. Table 2 shows a portion of two separate annota-tion instances of N03-1003 corresponding to two of its references. Our annotation has resulted in 203 annotation instances each corresponding to one paper-reference pair. The goal of this work is to automatically identify all context sentences, which are marked as  X  1  X . 3.1.1 Inter-judge Agreement two of our datasets that are marked with  X  in Ta-ble 1. For each paper-reference pair, the annotator was provided with a vector in which explicit cita-Table 3: Average  X  coefficient as inter-judge agreement for annotations of two sets tions were already marked with C s. The annota-tion guidelines instructed the annotator to look at each explicit citation sentence, and read up to 15 sentences before and after, then mark context sen-tences around that sentence with 1 s. Next, the 29 annotation instances done by the external annota-tor were compared with the corresponding anno-tations that we did, and the Kappa coefficient (  X  ) was calculated. The  X  statistic is formulated as where Pr ( a ) is the relative observed agreement among raters, and Pr ( e ) is the probability that an-notators agree by chance if each annotator is ran-domly assigning categories. To calculate  X  , we ig-nored all explicit citations (since they were pro-vided to the external annotator) and used the bi-nary categories (i.e., 1 for context sentences, and 0 otherwise) for all other sentences. Table 3 shows the annotation vector size (i.e., number of sen-tences), number of annotation instances (i.e., num-ber of references), and average  X  for each set. The average  X  is above 0 . 85 in both cases, suggest-ing that the annotation process has a low degree of subjectivity and can be considered reliable. 3.2 Analysis In this section we describe our analysis. First, we look at the number of explicit citations each reference has received in a paper. Figure 1 (a) shows the histogram corresponding to this distri-bution. It indicates that the majority of references get cited in only 1 sentence in a scientific arti-cle, while the maximum being 9 in our collected dataset with only 1 instance (i.e., there is only 1 reference that gets cited 9 times in a paper). More-over, the data exhibits a highly positive-skewed distribution. This is illustrated on a log-log scale in Figure 1 (b). This highly skewed distribution indicates that the majority of references get cited only once in a citing paper. The very small number of citing sentences can not make a full inventory of the contributions of the cited paper, and therefore, extracting explicit citations alone without context Table 4: The distribution of gaps in the annotated data sentences may result in information loss about the contributions of the cited paper. Figure 1: (a) Histogram of the number of differ-ent citations to each reference in a paper. (b) The distribution observed for the number of different citations on a log-log scale.

Next, we investigate the distance between con-text sentences and the closest citations. For each context sentence, we find its distance to the clos-ets context sentence or explicit citation. Formally, we define the gap to be the number of sentences between a context sentence (marked with 1) and the closest context sentence or explicit citation (marked with either C or 1) to it. For example, the second column of Table 2 shows that there is a gap of size 1 in the 9 th sentence in the set of con-text and citation sentences about Shinyama et al. (2002). Table 4 shows the distribution of gap sizes in the annotated data. This observation suggests that the majority of context sentences directly oc-cur after or before a citation or another context sentence. However, it shows that gaps between sentences describing a cited paper actually exist, and a proposed method should have the capability to capture them. In this section we propose our methodology that enables us to identify the context information of a cited paper. Particularly, the task is to assign a bi-nary label X where X to a given cited paper, C . To solve this problem we propose a systematic way to model the net-work level relationship between consecutive sen-tences. In summary, each sentence is represented with a node and is given two scores (context, non-context), and we update these scores to be in har-mony with the neighbors X  scores.

A particular class of graphical models known as Markov Random Fields (MRFs) are suited for solving inference problems with uncertainty in ob-served data. The data is modeled as an undirected graph with two types of nodes: hidden and ob-served. Observed nodes represent values that are known from the data. Each hidden node x responding to an observed node y true state underlying the observed value. The state of a hidden node is related to the value of its cor-responding observed node as well as the states of its neighboring hidden nodes.

The local Markov property of an MRF indi-cates that a variable is conditionally independent on all other variables given its neighbors: x bors of v , and cl ( v ) = { v } X  ne ( v ) is the closed neighborhood of v . Thus, the state of a node is as-sumed to statistically depend only upon its hidden node and each of its neighbors, and independent of any other node in the graph given its neighbors.
Dependencies in an MRF are represented using two functions: Compatibility function (  X  ) and Po-tential function (  X  ) .  X  potential of an edge between two nodes u , v of classes x indicate a strong association between x at nodes u,v . The Potential function,  X  shows the statistical dependency between x y
In order to find the marginal probabilities of x s in a MRF we can use Belief Propagation (BP) (Yedidia et al., 2003). If we assume the y are fixed and show  X  find the joint probability distribution for unknown variables x
In the BP algorithm a set of new variables m is introduced where m from i to j about what state x message, m mensionality of x i  X  X  opinion about j being in the corresponding class. Therefore each message could be consid-ered as a probability distribution and its compo-nents should sum up to 1 . The final belief at a Figure 2: The illustration of the message updating rule. Elements that make up the message from a node i to another node j : messages from i  X  X  neigh-bors, local evidence at i , and propagation function between i,j summed over all possible states of node i . node i , in the BP algorithm, is also a vector with the same dimensionality of messages, and is pro-portional to the local evidence as well as all mes-sages from the node X  X  neighbors: where k is the normalization factor of the be-liefs about different classes. The message passed from i to j is proportional to the propagation func-tion between i,j , the local evidence at i , and all messages sent to i from its neighbors except j : m (2) Figure 2 illustrates the message update rule.
Convergence can be determined based on a va-riety of criteria. It can occur when the maximum change of any message between iteration steps is less than some threshold. Convergence is guaran-teed for trees but not for general graphs. However, it typically occurs in practice (McGlohon et al., 2009). Upon convergence, belief scores are deter-mined by Equation 1. 4.1 MRF construction To find the sentences from a paper that form the context information of a given cited paper, we build an MRF in which a hidden node x an observed node y The structure of the graph associated with the MRF is dependent upon the validity of a basic as-sumption. This assumption indicates that the gen-eration of a sentence (in form of its words) only Figure 3: The structure of the MRF constructed based on the independence of non-adjacent sen-tences; (a) left, each sentence is independent on all other sentences given its immediate neighbors. (b) right, sentences have dependency relationship with each other regardless of their position. depends on its surrounding sentences. Said dif-ferently, each sentence is written independently of all other sentences given a number of its neigh-bors. This local dependence assumption can result in a number of different MRFs, each built assum-ing a dependency between a sentence and all sen-tences within a particular distance. Figure 3 shows the structure of the two MRFs at either extreme of the local dependence assumption. In Figure 3 a, each sentence only depends on one following and one preceding sentence, while Figure 3 b shows an MRF in which sentences are dependent on each other regardless of their position. We refer to the former by BP erally, we use BP each sentence is connected to i sentences before and after. Table 5: The compatibility function  X  between any two nodes in the MRFs from the sentences in scientific papers 4.2 Compatibility Function The compatibility function of an MRF represents the association between the hidden node classes. A node X  X  belief to be in class 1 is its probability to be included in the context. The belief of a node i , about its neighbor j to be in either classes is as-sumed to be 0 . 5 if i is in class 0 . In other words, if a node is not part of the context itself, we assume it has no effect on its neighbors X  classes. In con-trast, if i is in class 1 its belief about its neighbor j is determined by their mutual lexical similarity. If this similarity is close to 1 it indicates a stronger tie between i,j . However, if i,j are not similar, i  X  X  probability of being in class 1, should not af-fect that of j  X  X . To formalize this assumption we use the sigmoid of the cosine similarity of two sen-tences to build  X  . More formally, we define S to be
The sigmoid function obtains a value of 0.5 for a cosine of 0 indicating that there is no bias in the association of the two sentences. The matrix in Ta-ble 5 shows the compatibility function built based on the above arguments. 4.3 Potential Function The node potential function of an MRF can incor-porate some other features observable from data. Here, the goal is to find all sentences that are about a specific cited paper, without having explicit cita-tions. To build the node potential function of the observed nodes, we use some sentence level fea-tures. First, we use the explicit citation as an im-portant feature of a sentence. This feature can af-fect the belief of the corresponding hidden node, which can in turn affect its neighbors X  beliefs. For a given paper-reference pair, we flag (with a 1) each sentence that has an explicit citation to the reference.

The second set of features that we are inter-ested in are discourse-based features. In particu-lar we match each sentence with specific patterns and flag those that match. The first pattern is a bi-gram in which the first term matches any of  X  this; that; those; these; his; her; their; such; previ-ous  X , and the second term matches any of  X  work; approach; system; method; technique; result; ex-ample  X . The second pattern includes all sentences that start with  X  this; such  X .

Finally, the similarity of each sentence to the reference is observable from the data and can be used as a sentence-level feature. Intuitively, if a sentence has higher similarity with the reference paper, it should have a higher potential of being in class 1 or C . The flag of each sentence here is a value between 0 and 1 and is determined by its cosine similarity to the reference. Once the flags for each sentence, S normalized f tion of individual features. Based on f pute the potential function,  X  , as shown in Table 6. Table 6: The node potential function  X  for each node in the MRFs from the sentences in scientific papers is built using the sentences X  flags computed using sentence level features. The intrinsic evaluation of our methodology means to directly compare the output of our method with the gold standards obtained from the annotated data. Our methodology finds the sen-tences that cite a reference implicitly. Therefore the output of the inference method is a vector,  X  , of 1  X  X  and 0  X  X , whereby a 1 at element i means that sentence i in the source document is a con-text sentence about the reference while a 0 means an explicit citation or neither. The gold standard for each paper-reference pair,  X  (obtained from the annotated vectors in Section 3.1 by changing all C s to 0 s), is also a vector of the same format and dimensionality.

Precision, recall, and F fined as where 1 is a vector of 1  X  X  with the same dimen-sionality and  X  is a non-negative real number. 5.1 Baseline Methods The first baseline that we use is an IR-based method. This baseline, B as an input but use them to find context sentences. Given a paper-reference pair, for each explicit ci-tation sentence, marked with C , B ceding and following sentences if their similarities to that sentence is greater than a cutoff (the median of all such similarities), and repeats this for neigh-boring sentences of newly marked sentences. In-tuitively, B around citing sentences.

As the second baseline, we use the hand-crafted discourse based features used in MRF X  X  potential function. Particularly, this baseline, B Table 7: Average F B method ( SVM ) and three MRF-based methods. each sentence that is within a particular distance (4 in our experiments) of an explicit citation and matches one of the two patterns mentioned in Sec-tion 4.3. After marking all such sentences, B also marks all sentences between them and the closest explicit citation, which is no farther than 4 sentences away. This baseline helps us under-stand how effectively this sentence level feature can work in the absence of other features and the network structure.

Finally, we use a supervised method, SVM , to classify sentences as context/non-context. We use 4 features to train the SVM model. These 4 features comprise the 3 sentence level features used in MRF X  X  potential function (i.e., similar-ity to reference, explicit citation, matching certain regular-expressions) and a network level feature: distance to the closes explicit citation. For each source paper, P , we use all other source papers and their source-reference annotation instances to train a model. We then use this model to clas-sify all instances in P . Although the number of references and thus source-reference pairs are dif-ferent for different papers, this can be considered similar to a 10-fold cross validation scheme, since for each source paper the model is built using all source-reference pairs of all other 9 papers.
We compare these baselines with 3 MRF-based systems each with a different assumption about in-dependence of sentences. BP in which each sentence is only connected to 1 sen-tence before and after. In BP relaxed and each sentence is connected to 4 sen-tences on each sides. BP which all sentences are connected to each other regardless of their position in the paper.

Table 7 shows F shows how BP on average. The value 4 may suggest the fact that although sentences might be independent of dis-tant sentences, they depend on more than one sen-tence on each side.

The final experiment we do to intrinsically eval-uate the MRF-base method is to compare differ-ent sentence-level features. The first feature used to build the potential function is explicit citations. This feature does not directly affect context sen-tences (i.e., it affects the marginal probability of context sentences through the MRF network con-nections). Therefore, we do not alter this fea-ture in comparing different features. However, we look at the effect of the second and the third fea-tures: hand-crafted regular expression-based fea-tures and similarity to the reference. For each pa-per, we use BP absence of each feature and one including all fea-tures. Figure 4 shows the average F experiment. This plot shows that the features lead to better results when used together. We also performed an extrinsic evaluation of our context extraction methodology. Here we show how context sentences add important survey-worthy information to explicit citations. Previous work that generate surveys of scientific topics use the text of citation sentences alone (Mohammad et al., 2009; Qazvinian and Radev, 2008). Here, we show how the surveys generated using citations and their context sentences are better than those generated using citation sentences alone.

We use the data from (Mohammad et al., 2009) Figure 4: Average F ferent features. that contains two sets of cited papers and corre-sponding citing sentences, one on Question An-swering (QA) with 10 papers and the other on De-pendency Parsing (DP) with 16 papers. The QA set contains two different sets of nuggets extracted by experts respectively from paper abstracts and citation sentences. The DP set includes nuggets extracted only from citation sentences. We use these nugget sets, which are provided in form of regular expressions, to evaluate automatically gen-erated summaries. To perform this experiment we needed to build a new corpus that includes con-text sentences. For each citation sentence, BP used on the citing paper to extract the proper con-text. Here, we limit the context size to be 4 on each side. That is, we attach to a citing sentence any of its 4 preceding and following sentences if CT nuggets 0.416 0.634 AB nuggets 0.397 0.594 CT nuggets 0.324 0.379 Table 9: Pyramid F surveys of QA and DP data. The QA surveys are evaluated using nuggets drawn from citation texts (CT), or abstracts (AB), and DP surveys are evaluated using nuggets from citation texts (CT). BP4 marks them as context sentences. Therefore, we build a new corpus in which each explicit ci-tation sentence is replaced with the same sentence attached to at most 4 sentence on each side. After building the context corpus, we use LexRank (Erkan and Radev, 2004) to generate 2 QA and 2 DP surveys using the citation sentences only, and the new context corpus explained above. LexRank is a multidocument summarization sys-tem, which first builds a cosine similarity graph of all the candidate sentences. Once the network is built, the system finds the most central sentences by performing a random walk on the graph. We limit these surveys to be of a maximum length of 1000 words. Table 8 shows a portion of the sur-vey generated from the QA context corpus. This example shows how context sentences add mean-ingful and survey-worthy information along with citation sentences. Table 9 shows the Pyramid F data. The QA surveys are evaluated using nuggets drawn from citation texts (CT), or abstracts (AB), and DP surveys are evaluated using nuggets from citation texts (CT). In all evaluation instances the surveys generated with the context corpora excel at covering nuggets drawn from abstracts or cita-tion sentences. In this paper we proposed a framework based on probabilistic inference to extract sentences that appear in the scientific literature, and which are about a secondary source, but which do not con-tain explicit citations to that secondary source. Our methodology is based on inference in an MRF built using the similarity of sentences and their lexical features. We show, by numerical exper-iments, that an MRF in which each sentence is connected to only a few adjacent sentences prop-erly fits this problem. We also investigate the use-fulness of such sentences in generating surveys of scientific literature. Our experiments on generat-ing surveys for Question Answering and Depen-dency Parsing show how surveys generated using such context information along with citation sen-tences have higher quality than those built using citations alone.

Generating fluent scientific surveys is difficult in absence of sufficient background information. Our future goal is to combine summarization and bibliometric techniques towards building au-tomatic surveys that employ context information as an important part of the generated surveys. The authors would like to thank Arzucan  X  Ozg  X  ur from University of Michigan for annotations. This paper is based upon work supported by the National Science Foundation grant  X  X OPENER: A Flexible Framework to Support Rapid Learning in Unfamiliar Research Domains X , jointly awarded to University of Michigan and University of Mary-land as IIS 0705832. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessar-ily reflect the views of the National Science Foun-dation.

