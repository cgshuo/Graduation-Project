 settings, the entity relationships provide additional structural information. this and similar situation where it is not natural to transform the data into a flat structure. relationship?), and link regression (i.e., how does the user rate the item?). In this paper we propose a family of stochastic relational models (SRM) for link prediction and on a conditional model of links. We present various models of SRM and address the computational with the entity size. SRM has shown encouraging results in our experiments. results in Sec. 7, followed by conclusions and extensions in Sec. 8. We first consider pairwise asymmetric links r between entities u  X  X  and v  X  X  . The two sets or their identity when entity attributes are unavailable. Note that r to be identical to r symmetric links are straightforward and will be briefly discussed in Sec. 8. We assume that the observable links r are derived as local measurements of a real-valued latent relational function t : U X V X  R , and each link r modeled by the likelihood p ( r We call them processes because U and V can both encompass an infinite number of entities. Let likelihood (also called evidence) under such a prior is where R I = { r 2.1 Choices for the Piror p ( t |  X  ) extending conventional GP models. 2.1.1 A Brief Introduction to Gaussian Processes A GP defines a nonparametric prior distribution over functions in Bayesian inference. A random { x and covariance (or kernel)  X  = {  X ( x where y for inference is required. A comprehensive coverage of GP models can be found in [9]. 2.1.2 Hierarchical Gaussian Processes By observing the relational data collectively, one may notice that two entities u function  X  . Optimizing the GP kernel  X  via evidence maximization means to learn the dependency of entities in U , summarized over all the entities v  X  X  .
 sense to explore the dependency between movies, or the dependency between authority web pages. 2.1.3 Tensor Gaussian Processes GP kernel functions  X : U X U X  R and  X : V X V X  R . The model explains the relational stochastic relational process p ( t |  X  ) as the following: { u matrix, have a matrix-variate normal distribution characterized by mean B = { b ( u { t as a whole sample from a TGP, instead of being formed by i.i.d. functions in the HGP model. Let vec ( T )=[ t 1 vec ( T )  X  X  (0 ,  X  ) , where the covariance  X  =  X   X   X  is the Kronecker product of two co-kernel function  X :( U X V )  X  ( U X V )  X  R is defined via a tensor product of two GP kernels Cov ( t structure of links by the dependence structure of participating entities. Gaussian prior. A similar connection exists for TGP.
 Theorem 2.2. Let U X  R P , V X  R Q , and W  X  X  follows TGP (0 ,  X  ,  X ) with  X ( u The proof is straightforward through Cov [ t ( u E efficient discriminative approach to link prediction.
 prohibitive. 2.1.4 A Family of Stochastic Processes for Entity Relationships TGP, we propose a family of stochastic processes p ( t |  X  ) for entity relationships. Definition 2.3 (Stochastic Relational Processes) . A relational function t : U X V X  R is said to follow a stochastic relational process (SRP), if t ( u, v )= 1  X  g ( v ) iid  X  X P (0 ,  X ) . We denote t  X  X RP d (0 ,  X  ,  X ) , where d is the degrees of freedom. Interestingly, there exists an intimate connection between SRP and TGP: Theorem 2.4. SRP Proof. Based on the central limit theory, for every ( u Gaussian random variable. In the next steps, we prove E [ t E will provide a close approximation to TGP.
 links between the same set of entities. If we build a generative process where U = V ,  X = X  and f 2.2 Choices for the Likelihood p ( r ploy the probit function to model the Bernoulli distribution over class labels, i.e. p ( r  X ( r i,n t i,n ) , where  X (  X  ) is a cumulative normal function, and r i,n  X  X  X  1 , +1 } .  X  Regression : In this case we consider r by a noise model, e.g. a univariate Gaussian noise with variance  X  2 and zero mean. the hyperlinks between web pages. Based on the open-world assumption , if a web page does not We have described the relational model under a prior of SRP, in which HGP and TGP are subcases. tribution of latent variables.
 Let F = { f f distribution of the complete data : p R I , F , G |  X   X  An exact inference is intractable due to the coupling between f by finding the mode in the posterior, We solve the minimization by the conjugate gradient method. The gradients are calculated by where S  X  R N  X  M have elements s if the covariances matrices. This follows the facts: (1) Each f f only loosely dependent to each other, especially for a large d ; (2) The dependency between f g each group separately and obtain the covariances: where  X  and {  X  shuffling the order of latent dimensions or changing the signs of both f probability.). However each mode is equally well in constructing the relational function t . of kernel functions  X ( u inverse-Wishart priors  X   X  X W we apply an iterative expectation-maximization (EM) algorithm to solve the problem (5). In the maximizing the expected log-likelihood of the complete data where E mization have an analytical solution,  X  In the experiments of this paper we use p ( r F  X  and G  X  . In a longer version the predictive uncertainty of t from data. Very recently a GP model was developed to learn from undirected graphs [4], which turns out to be a special rank-one case of SRM with d =1 ,  X = X  , and f deficient or unavailable, the model does not work well, while SRM can learn informative kernels l ( r MMMF only estimates the mode of the latent relational function with fixed Dirac kernels. Synthetic Data : We generated two sets of entities U = { u such that u matrices  X  and  X  whose diagonal block structure reflects the clusters. Binary links between U with noninformative Dirac kernels  X  0 =  X  0 = I , and compared with MMMF [10]. In all the cases really helps SRM to achieve the best accuracy.
 SRM with MMMF in a regression task to predict the  X  X ating link X  between users and movies. In SRM we set  X  0 =  X  0 = I . For both methods the dimensionality was chosen as d =20 . In MMMF we used the square error loss. We repeated the experiments for 10 times, where at each time we to MMMF, SRM significantly reduces the prediction error by over 12% in terms of both RMSE and MAE. on a synthetic data set and a user-movie rating prediction problem.
 The authors thank Andreas Krause, Chris Williams, Shenghuo Zhu, and Wei Xu for the fruitful discussions.

