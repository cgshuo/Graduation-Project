 1. Introduction
Microblogging platforms, such as Twitter, 1 have become indispensable communication channels through which hundreds range of other ranking criteria, including, e.g., existence of hyperlinks, hashtags and retweets.  X  relevant and interesting tweets.

Fusing multiple document lists that have been retrieved from a corpus in response to a query so as to compile a single
SUM family (CombMax, CombMin, CombSUM, CombANZ, CombMNX, CombMNZ, etc. Lee, 1995 ) of fusion methods being the e.g., query representations or document representations ( Croft, 2000 ). Many effective fusion methods are based on the 2001; Croft, 2000; Dwork, Kumar, Naor, &amp; Sivakumar, 2001; Kozorovitsky &amp; Kurland, 2011; Lee, 1995; Montague &amp; fused list if it appears only in a single list and is ranked low in this list.

The characteristics of microblog environments suggest a different perspective. In such environments news events trigger talked about the  X  X 2014 Eastern Synchronized Skating Sectional Championship X  X  mainly between January 30 and February 1, 2014, which is when the championship was held. Posts created before the beginning or after the ending of the event to the following intuition about fusing ranked lists of microblog posts. If a post d and (other) relevant posts d published within the same narrow time window, and the relevant posts d it is ranked low. Fig. 1 illustrates this intuition; there, post d be rewarded as it was published in the same narrow time window in which a large number of posts occur that are ranked high in many lists; in contrast, d 8 , while ranked high in L window.

To tackle the problem of microblog post search, we propose BurstFuseX, a novel probabilistic model that not only utilizes information traditionally used when merging ranked lists, such as ranks, but also exploits temporal information, i.e., the publication timestamps of microblog posts. In our fusion model, we focus on the case where only ranks and pub-lication timestamps are available and no additional information is provided X  X uch as the content of the posts, the post X  X  ficient and hence inappropriate in dynamic environments such as microblog search. In addition, the content may not be available in all scenarios ( Salakhutdinov &amp; Mnih, 2008 ). Briefly, BurstFuseX first calls a standard document fusion on the fused scores produced by method X, we detect windows of timestamps of high-scoring posts. These windows give rise to bursts of posts. We then reward posts that are published in the temporal vicinity of a burst that contains high-scoring posts.
 In our experiments aimed at assessing the performance of BurstFuseX, we sample runs that have been submitted to the
TREC 2011 and 2012 Microblog tracks and fuse them using BurstFuseX, respectively. For the underlying fusion method X (on top of which BurstFuseX builds), we consider three alternatives: two unsupervised fusion methods, CombSUM ( Shaw et al., ther comparisons, we consider a number of burst or time-sensitive microblog retrieval baselines. As BurstFuseX detects line that detects bursts based on the contents. As we will see below, BurstFuseX significantly outperforms most fusion approaches and burst or time-sensitive retrieval methods.

Our contributions in this paper can be summarized as follows. i. We propose a novel and effective probabilistic data fusion model to microblog post search, BurstFuseX, which not only ii. To the best of our knowledge, this is the first attempt to frame the problem of searching microblog posts as a data iii. We provide a detailed analysis of the performance of BurstFuseX and offer a number of examples where we observe
In Section 2 we discuss related work; in Section 3 we detail BurstFuseX; we follow with a description of our experimental
Section 6 concludes the paper. 2. Related work of performing burst detection in information retrieval, and finally we survey state-of-the-art approaches of searching microblogs since the TREC 2011 Microblog track ran. 2.1. Data fusion
The task of fusing document lists that have been retrieved in response to a given query so as to compile a single more into supervised and unsupervised methods.

Supervised data fusion approaches first extract a large number of features, either from documents or lists, and then 2008; Wu, 2012 ). Supervised data fusion approaches become feasible when we can leverage the use of information exist-ing in labeled training data. Adopting a supervised learning approach to data fusion has some advantages. For instance, we can apply existing optimization techniques to the data fusion problem, and the approaches become more easily ame-conducting supervised data fusion, in which learning is formalized as an optimization problem in which one minimizes disagreements between ranking results and the labeled data. Tsai et al. (2008) propose a learning approach for the merg-ing process in multilingual information retrieval. To conduct the learning data fusion approach, they extract a number of features from the given query, the documents to be retrieved and the translation, and then use an existing learning to rank algorithm to construct a merge model from a large number of labeled data. Qin, Geng, and Liu (2010) propose a supervised probabilistic data fusion model, which is based on coset-permutation distance and defined in a stage-wise proposed by Sheldon et al. (2011) first extracts features from both the lists and the documents appearing in any of the lists, and then uses a learning to rank method to optimize a given metric, like NDCG, MAP, to fuse the lists into a final merging list in response to a given query. We use k -Merge as a representative example of supervised fusion methods.
Recently, Hong and Si (2012) propose a novel supervised fusion model for result merging by utilizing multiple central-ized retrieval algorithms. However, the fact that a large amount of labeled data has to be available, together with other supervised problems (for instance over-fitting noted above in k -Merge), makes supervised data fusion less useful when labeled data is hard to come by. Our experimental results show that in many cases, even a traditional unsupervised data fusion can beat a state-of-the-art supervised data fusion method.
 tion only use order information of the documents appearing in any of the lists to be fused as input. Data fusion methods where only order information is available.

Unsupervised data fusion has a long history with the CombSUM family of fusion methods being the oldest and one of the
Crestani (2012, 2013a) provide theoretical arguments on why some traditional unsupervised fusion methods work, and based on these insights, they propose other unsupervised fusion methods. Through the use of unsupervised data fusion,
Loia, Pedrycz, and Senatore (2007) offer a new way of organizing web documents that emphasizes a direct separation between syntactic and semantic facets.

Khalaman and Kurland (2012) utilize the content of documents appearing in any of the result lists to be fused to get an scores. This fusion model makes strong assumptions: the content of documents is assumed available and it is very easy assumptions are somewhat unrealistic. For instance, some posts with only links but without any words are still labeled as posts into data fusion. 2.2. Burst detection Our framework for detecting bursts is similar to that in ( Chen et al., 2010; Hoonlor et al., 2012; Lappas et al., 2009; and our purpose in detecting bursts differs strongly. Much prior work detects bursts mainly based on document frequency the information in our burst detection method is the score of documents generated by standard data fusion algorithm from dard fusion scores to detect bursts outperforms using the content of posts to detects bursts. 2.3. Microblog retrieval Microblog retrieval has become an active research topic in IR, especially following the launch of the Microblog track at TREC in 2011 ( Ounis et al., 2011 ). Earlier work, however, already explored the task of retrieving microblog posts.
O X  X onnor, Krieger, and Ahn (2010) present TweetMotif , an exploratory search application for Twitter. Unlike traditional approaches to information retrieval, which present a simple list of messages, TweetMotif groups messages by frequent sig-and URL presence are very strong features.

Following the launch of the Microblog track at TREC in 2011, many approaches have been proposed. At TREC in 2011, queries with many unique characteristics, and utilize the temporal information to help searching posts. The method pro-posed by Metzler and Cai (2011) combines a Markov random field model with a learning to rank model for searching posts, which achieves the best p@30 performance at TREC in 2011. A combination strategy is also used by Zhang, Hui,
He, and Luo (2011) to search posts in 2011, where they combine a field-based model that takes the frequency of a query term in different document fields into account with query expansion. In contrast, work present in ( Bandyopadhyay, Mitra, &amp; Majumder, 2011 ) uses query expansion only for searching posts, but the way their query expansion method works is different; they use the Google Search API to retrieve pages from the web, and use the titles to expand the queries.
At the TREC 2012 Microblog track, Luo, Osborne, Petrovic, and Wang (2012) consider a microblog post to be a structured features in a learning to rank method, they show good retrieval performances. At TREC 2012, the best performing run also information based on a language model. Kim, Yeniterzi, and Callan (2012) present two approaches to address the problem of back and the other is document expansion of tweets using web documents linked from the body of a tweet. Jabeur et al. (2012) experiment with a bayesian network retrieval model for posts search and a feature learning model for relevance classification.

Beside the approaches presented at TREC 2011 and 2012, many microblog post retrieval approaches have been proposed outside TREC since the launch of the TREC 2011 Microblog track. For instance, Massoudi, Tsagkias, de Rijke, and Weerkamp (2011) and Miyanishi et al. (2013a) propose a method for query expansion in the microblog domain and find that this is highly effective. Naveed, Gottron, Kunegis, and Che Alhadi (2011) explore the impact of document length normalization ity model using surrogate judgments based on retweets that can be collected automatically to train a microblog search model. Chang et al. (2013) propose a method to utilize Twitter TinyURL (shortened URL links) to detect fresh and high-quality documents, and leverage Twitter data to generate novel and effective features for ranking documents. The work by Miyanishi, Seki, and Uehara (2013b), Dakka, Gravano, and Ipeirotis (2012), Choi and Croft (2012) and Massoudi et al. (2011) utilizes burst (time) information to boost the performance of searching posts.

Another related line of work concerns retrieval score regularization for improving the performance of ad hoc search ( Diaz, 2005, 2007 ). Specifically, Diaz (2005, 2007) present a framework for improving document retrieval scores under a regularization framework, where retrieval scores of documents are adjusted to respect inter-document consistency. Our microblog search algorithm, BurstFuseX, builds on the same intuitions as the algorithms proposed in ( Diaz, 2005, 2007 ): closely related documents should have similar retrieval scores, given the same information request; in our algorithm, we assume that posts published within the same time intervals are more likely to be talking about the same topic. There are several dissimilarities, though, between the algorithms in ( Diaz, 2005, 2007 ) and our algorithm. For instance, the trast, our algorithm utilizes time information for data fusion score regularization. The input of the former algorithms consists of the retrieval scores of documents generated by a single retrieval model and the documents X  content; in con-trast, the input of our algorithm is a number of result lists generated by multiple retrieval models plus the timestamps of the posts.

In sum, the work that we present in this paper differs in important ways from the related work just discussed. We observe the first attempt to integrate temporal characteristics of result sets into data fusion techniques. 3. Fusion approach
In this section, we first provide the main research question we address. Then we briefly describe standard unsupervised and supervised data fusion methods that will be integrated in our proposed data fusion methods and taken as baselines in we detail our proposed data fusion methods for microblog search in Section 3.3 .
 data fusion method BurstFuseX consists of a query and a set of ranked lists of posts; the output is a single fused list. Algorithm 1 gives a high level overview of BurstFuseX.
 Algorithm 1. BurstFuseX: Burst-aware data fusion for microblog post search Input: A query q
Output: A final fused list of posts. 1 Calculate the (standard) fusion score F X  X  d ; q  X  according to X for each post d 2C 2 Detect bursts based on the timestamps and F X  X  d ; q  X  scores; see Section 3.2 ; 3 Calculate the BurstFuseX fusion score for each d 2C L using the bursts and the standard fusion score; see Section 3.3 ; 4 Construct the final fused list based on the BurstFuseX score of d 2C The fusion methods we consider as building blocks for BurstFuseX all assign a non-negative fusion score F 3.1. Standard fusion methods any fusion method can be integrated into our model. Below, we briefly review the three standard fusion methods that we consider in this paper: two unsupervised ones and a supervised method. 3.1.1. Unsupervised fusion
Classical unsupervised methods include the so-called CombSUM family. Methods in this family assume that documents F in which d appears.

Let R L i  X  d  X  denote d  X  X  score based on the rank of d in list L R  X  d  X  is often defined as: where rank  X  d ; L i  X 2f 1 ; ... ; k L i g is the rank of d in L for instance, scores d by the sum of its rank scores in the lists: while CombMNZ ( Shaw et al., 1994; Wu, 2012 ) rewards documents d that rank high in many lists: 3.1.2. Supervised fusion
Recently, several supervised methods for merging ranked lists have been proposed, one of which is k -Merge ( Sheldon are currently available. 2
Given a query, k -Merge can directly optimize a retrieval metric (e.g., MAP) to enhance retrieval effectiveness under the uments from multiple reformulations of the given query by combining features that indicate document quality (such as tures about the quality of the reformulation ( gating features ) are used in our data fusion method.
Our settings for k -Merge are detailed in an appendix to the paper (see Appendix A ). 3.2. Bursts and burst detection
To ground our intuitions about utilizing burst information to boost the performance of microblog search, we choose four test queries as examples and examine plots of the number of relevant documents distributed over their document ages data fusion method for microblog search aims to do.

Next, we move on to the next step (step 2) of Algorithm 1 and detail how we detect bursts. Let t ( 2C L ) denote a post d with timestamp t i . We regard posts published during the same hour as having the same timestamp. before we detect bursts, we need to define S t i  X C L  X  , the burst-time score at time t under consideration. Let F X  X  d t i ; q  X  be the score of d where 1 6 j 6 t C L and t C L is the total number of different timestamps belonging to posts in C score S t i  X C L  X  &gt; 0 if it is above the average score (i.e., 1 = t
We compute a burst-time score S t i  X C L  X  at each time point t time score sequence I  X C L  X  X fS t 1  X C L  X  ; S t 2  X C L  X  ; ... ; S
Following ( Ruzzo &amp; Tompa, 1999 ), a segment I  X C L  X  X  t imal segment in I  X C L  X  if: i. All proper subsequences of I  X C L  X  X  t i : t j have a lower score. ii. No proper super-segments of I  X C L  X  X  t i : t j in I  X C imal segment I  X C L  X  X  t i : t j gives rise to a burst of posts b  X C any post d 2C L whose timestamp is between t i and t j is within this segment. We write B X C set of all bursts in response to q .
 our burst detection method is in the problem of finding all maximal segments: this problem can be solved in linear time ( Ruzzo &amp; Tompa, 1999 ), so that the computational complexity of our burst detection method is O  X jC 3.3. Burst-aware fusion
We turn to the key steps 3 and 4 of Algorithm 1 and define our burst-aware fusion algorithm. Motivated by the fact that erated by a standard fusion method X, to estimate P  X  d j q  X   X  X he final probability that d ( 2C 3.3.1. The model probability of a post d being about q ; P  X  d j q  X  , as: free parameter l is used ( Kurland &amp; Lee, 2004; Markovits, Shtok, &amp; Kurland, 2012 ) such that: and define our BurstFuseX model as: will be ranked high in the final fused list.
 in a burst, as indicated by p  X  d j b  X  , then the final fused score of d ; F the final ranking of d . 3.3.2. Estimating the key components
Our next task is to derive estimates for the following key components in Eq. (5) : p  X  d j q  X  : post-level relevance X  X ow likely d is talking about q . p  X  b j q  X  : burst-level relevance X  X ow likely a set of posts as a whole are talking about q . p  X  d j b  X  : post-burst association strength X  X ow likely d belongs to b .

Post-level relevance . To obtain p  X  d j q  X  in Eq. (5) , we apply Bayes X  Theorem, such that p  X  d j q  X  X  p  X  q  X / P d 0 2C post d 0 2C L .So p  X  d j q  X  can be rewritten as: We use an estimate p h  X  q j d  X / F X  X  d ; q  X  ( Khalaman &amp; Kurland, 2012 ), where F
X for d given q : reduce to the standard fusion method X if we let l  X  0 in Eq. (5) ,as F this case. In other words, the effect of merging result lists according to F result lists according to F X  X  d ; q  X  .

Burst-level relevance . Next, to obtain p  X  b j q  X  in Eq. (5) , we apply Bayes X  Theorem again, such that p  X  b j q  X  X  use the probability rule, and have p  X  q  X / P b 0 2B X C detection method. Assuming a uniform prior for each burst in C mation pertaining to q can be represented as: has shown that product-based representations somewhat outperform sum-based representations ( Khalaman &amp; Kurland, 2012; Liu &amp; Croft, 2008; Seo &amp; Croft, 2010 ). Accordingly, we let:
As we use an estimate p h  X  q j d  X / F X  X  d ; q  X  (see above), p  X  b j q  X  can be estimated as: where j b j and j b 0 j are the number of posts in b and b observe that p  X  b  X / P d 0 2C sented as: Here, p  X  b j d  X  is the probability of d belonging to b .
 the average of the sum of the score. 5 We set: to estimate p  X  b j d  X  , where d 00 2C L is a post in b .

Three factors affect the association strength between d and b : the temporal relationship between d and posts d relevance of d given q and the relevance of d 00 given q . We estimate the time relationship between d and d
Here, t d and t d 00 are the timestamps of post d and d 00 where n b  X  j i  X  1 is the total number of different timestamps of posts in the burst b .
The bigger the temporal distance between t d and t d 00 is, the smaller p burst b ; d is rewarded less by post d 00 .

Now, to estimate p  X  d 00 j d  X  (Eq. (8) ) we build on the following intuition. If d then d 00 should be able to boost d  X  X  score. Hence, we estimate p  X  d p  X  d 00 j q  X  p t  X  d 00 ; d  X  . When we substitute this term in ( 8 ) we obtain:
Putting everything together, we can now estimate the post-burst association strength, p  X  d j b  X  , as: between d and b increases. In this case, d  X  X  scores will be boosted. Note, by the way, that d ( 2C d in C L can have a non-zero association strength to any b in B X C 4. Experimental setup 4.5 and 4.6 detail how BurstFuseX is trained and optimized, and the settings of the experiments. 4.1. Research questions
The research questions guiding the remainder of the paper are: iii Does BurstFuseX outperform the best run to be fused? (See Section 5.1 ). vi Can we observe the hypothesized effect sketched in Fig. 1 (See Section 5.4 ). vii How fast is BurstFuseX compared to other data fusion methods? (See Section 5.5 ). viii Can BurstFuseX beat burst or time-sensitive microblog search algorithms? (See Sections 5.6 and 5.7 ). ix Can BurstFuseX aid a single run that does not take time into account? (See Section 5.8 ). 4.2. Data set In order to answer our research questions we work with the Tweets 2011 corpus ( Macdonald, Ounis, Lin, Choudhury, &amp; million tweets collected over a period of 2 weeks (23th January until 8th February 2011, inclusive) sampled courtesy of
Twitter. Different types of tweets in this data set are present, including replies and retweets. Each tweet has its own timestamp. Descriptive statistics about the collection are provided in Table 2 .

The task studied at the TREC 2011 Microblog track was: given a query with a timestamp, return relevant and interesting 2012 ). In our experiments, we rank tweets by relevance. 7 We use two sets of test topics (queries) in our experiments, the 2011 test set and the 2012 test set. In total, NIST (the
National Institute of Standards and Technology) created 50 test topics for TREC 2011 Microblog track, each representing were used in the TREC and 2965 tweets were deemed relevant; some topics have just two relevant tweets while some have the pool, and it was therefore dropped from the evaluation. To assess the tweets, the assessors judged the relevance of a tweet after reading it. Tweets in the Tweet11 corpus were judged on the basis of the defined information need using a three-point scale: Not Relevant , Minimally Relevant and Highly Relevant ( Macdonald et al., 2011 ).
A total of 59 groups participated in the TREC 2011 Microblog track, with each team submitting at most four runs, which of 0.4551 and the worst run achieving 0.000. In our experiments below, we do not use any runs whose p@30 scores are below 2011 Microblog track can be found in ( Macdonald et al., 2011; Ounis et al., 2011 ).

The Microblog search track continued in 2012 using the same corpus, Tweet11 ( Soboroff et al., 2012 ). NIST created 60 or highly relevant. The TREC 2012 Microblog track received 121 runs that were subsequently re-ranked. 4.3. Baselines
We compare BurstFuseX to 3 data fusion baselines: 2 traditional unsupervised methods, i.e., CombSUM, CombMNZ, and a formance, we also compare BurstFuseX to 4 state-of-the-art burst-sensitive microblog search algorithms: time-based lan-et al., 2011 ), direct time-sensitive BM25 retrieval model (DIRECT-BM25 (mean)) ( Dakka et al., 2012 ) and temporal tweet posts.

To illustrate the merits of detecting bursts from fusion scores, we implement an alternative algorithm, BurstFuseX (BurstFuseCombSUM posts and BurstFuseCombMNZ posts ), which detects bursts using the burst detection approach presented other words, the only difference between BurstFuseX and BurstFuseX index of the dataset that some of our baselines require, we apply Porter stemming, tokenization, and stopword removal (using INQUERY lists) to posts using the Lemur toolkit. 9
Appendix A . 4.4. Metrics and significance testing
For performance evaluation in our experiments we consider both minimally relevant and highly relevant posts relevant and use the official metric, p@30 (precision at rank 30). We also report on p@5, p@10, p@15 and MAP (mean average pre-simply calculated by: where rel  X  d  X  is a binary function that indicates whether the document d (MAP) is a commonly used recall-oriented metric. For each relevant document in the result list we take the precision at gives us the average precision ( AP ) for a query, which can be formulated as: for a test collection is obtained by taking the mean of AP scores over a set of test queries.
We use trec_eval 10 to compute the performance scores. We expect BurstFuseX to have a recall-enhancing effect. This may of observed differences between the performance of two runs is tested using a two-tailed paired t -test; we use (or )to denote significant differences for a  X  : 01, or (and ) for a  X  : 05. 4.5. Training and optimization using 10-fold cross validation performed over the entire set of queries in the TREC 2011 Microblog track. In the learning phase, the performance of BurstFuseX is optimized with respect to MAP. In other words, the set of 49 queries is randomly value of l that maximizes MAP performance over the remaining subsamples (44 queries). We repeat the experiment 10 same as that in TREC 2011 Microblog track. Our baseline fusion methods, i.e., CombSUM, CombMNZ and k -Merge, do not incorporate free parameters. 4.6. Experiments
We report on 8 main experiments in this paper. First, to understand the overall performance of BurstFuseX, we sample p@30 distribution, respectively: 18 out of the 174 runs in TREC 2011 and 18 out of the 117 runs in TREC 2012, 6 each with p@30 scores between 0.20 and 0.30 (Class 3), between 0.30 and 0.40 (Class 2), and over 0.40 (Class 1). We also randomly choose two runs from each class to construct Class 4. See Tables 3 and 4 for details of our sample runs from the TREC run5 and run6 to refer to the runs in descending order of p@30 score.

To understand the influence of bursts and see whether burst information is helpful to boost fusion performance and to dard fusion information are to be mixed. Then, to understand the effect of the number of lists to be merged, we randomly fusion methods. We repeat the experiments 20 times and report the average results as well as the corresponding standard deviation scores.

In order to understand the topic-level performance of BurstFuseX, we provide an analysis of topic-level performance against the standard fusion method it cooperates. Next, to determine how fast BurstFuseX can merge result lists, we again standard fusion methods. To understand whether BurstFuseX can improve over microblog search approaches that already incorporate time-sensitive search algorithms, we compare the performance of BurstFuseX, the standard fusion method it builds on and 5 time-sensitive baselines of searching microblogs.

To understand whether detecting bursts based on standard fusion scores works better than detecting based on the textual content of posts, we make a comparison between our fusion methods and those detecting bursts based on the textual con-tents of posts. We also compare burst-sensitive component lists to be fused and the fusion methods to see whether fusion the single result list and the output of BurstFuseX on that list.

As described in Section 3 , in our experiments we use two unsupervised data fusion methods, CombSUM and CombMNZ, and one supervised method, k -Merge, as representatives of the standard methods that can be integrated by BurstFuseX. 5. Results and analysis
In this section, we present our experimental results and perform an analysis. We follow the order of the research ques-performance; Section 5.4 reports on a topic level analysis; Section 5.5 is devoted to look at the runtime performance of
BurstFuseX and in Section 5.6 we examine whether BurstFuseX is able to add anything in terms of performance on top of result lists produced by retrieval methods that already use temporal information; Section 5.7 provides a further analysis on single result list. 5.1. Fusing the sample lists
We begin by addressing research questions i X  X ii. The performance of BurstFuseX and of the standard fusion methods X burst-aware fusion methods, i.e., CombSUM, CombMNZ, BurstFuseCombSUM and BurstFuseCombMNZ, is better than that of
TREC 2011 Microblog track (e.g., the p@30 score for BurstFuseCombSUM is 0.5578 while that of the best run in the track is posts, but many of the same relevant posts.

It is worth noting that in most cases BurstFuseX outperforms the standard fusion method X that it incorporates for all significant. For instance, when fusing the runs in class 4, the MAP and p@30 metrics of BurstFuseCombMNZ are 0.2883 and incorporating burst information into data fusion and shows that using burst information can improve the performance of existing data fusion methods in terms of MAP and p@30.

Interestingly, in Table 5 when we consider the p@5 scores, we see that BurstFuseX always outperforms the best single run but that it loses against the standard fusion method X on which it builds in most cases. One reason is that some relevant cut-offs, resulting in the improvements of p@10, p@15 and p@30 scores.

Additionally, from Table 5 we see that, in terms of MAP, BurstFuseCombSUM outperforms BurstFuseCombMNZ, and both of them outperform BurstFuse k -Merge in Class 2 (0.2651, 0.2587, 0.2161, respectively). In other words, BurstFuseCombSUM outperforms BurstFuseCombMNZ, followed by BurstFuse k -Merge. This is quite obvious in Class 1, Class 2 and Class 4 for instance. In terms of the standard fusion methods, CombSUM performs almost the same as CombMNZ without statistically significant differences. Both CombSUM and CombMNZ easily beat the supervised standard fusion method k -Merge; in same fitting of k -Merge.

As a sanity check, so as to confirm our observations about the performance of BurstFuseX, we also test BurstFuseX on the 10% sample runs from TREC 2012 Microblog track. We present the experimental results of BurstFuseX and the standard fusion methods when fusing the runs from TREC 2011 Microblog track are confirmed by the 2012 data. For instance, in
Table 6 we see that all data fusion methods, both BurstFuseX and other ones, when fusing runs in Class 1, outperform the tion: the 2012 collection consistently yields the same overall results and trends. 5.2. The use of burst information
Next we address research question iv and examine the effect of using different amounts of burst information in our burst-aware fusion method. Put differently, we examine the impact of the free parameter l in Eq. (5) . Fig. 4 depicts the p@30 performance curves for BurstFuseX and the corresponding standard fusion methods it integrates when fusing result posts (according to method X) as well as the burst information are utilized for fusing the lists. the standard fusion method it integrates, CombSUM achieves only 0.3918. As we observed before, BurstFuseX works better when it integrates one of the unsupervised standard fusion methods than the supervised fusion method k -Merge. single retrieval microblog search algorithms in Sections 5.7 and 5.8 . 5.3. Effect of the number of lists to be merged
We have already seen that BurstFuseX outperforms the standard fusion methods it incorporates when fusing 6 lists in different quality classes. We now address research question v and explore the effect on the performance of BurstFuseX of varying the number of lists being merged, in terms of MAP, p@5, p@15 and p@30. In Fig. 5 , we randomly choose p@30 p@30 we repeat the experiment 20 times and report on the average scores as well as the standard deviation. We use CombSUM and
BurstFuseCombSUM as a representative example; for the other combinations with a standard fusion method X qualitatively similar results can be observed.

As can be seen in Fig. 5 , with fewer than 12 lists to be merged by either BurstFuseX or the standard fusion method it see more high quality lists, more low quality lists may show up as well. Unlike the MAP, p@15 and p@30 performance of
BurstFuseCombSUM where it always enhances that of CombSUM, the early precision p@5 performance of BurstCombSUM is worse than that of CombSUM especially when fewer lists are merged. This observation is consistent with those in both
Tables 5 and 6 . Performance gains in terms of p@15 of BurstFuseCombSUM and CombSUM continue even when more than
FuseCombSUM over CombSUM decreases. As more lists are being fused, more noise is being brought in, especially with posts ranked lower. 5.4. Topic-level analysis
Next, we take a closer look at per query improvements of BurstFuseX over the underlying standard fusion method X, thereby addressing research question vi. For brevity, we only consider BurstFuseCombSUM as a representative to report similar.
 Fig. 6 shows the per query performance differences in terms of AP, p@5, p@15 and p@30, respectively, between BurstFuse-
CombSUM and CombSUM. Overall, gains by BurstFuseCombSUM over CombSUM outnumber losses for p@15 and p@30 as well as MAP, but not for very early precision, i.e., p@5. Gains by BurstFuseCombSUM over CombSUM are due mainly to topics
MB011 (Kubica crash) and MB015 (William and Kate fax save-the-date). Invariably, for such topics we found evidence of the MAP p@15
BurstFuseX because they are central to a burst. For instance, in response to topic MB010 (Egyptian protesters attack museum), post #30354903104749568 is ranked near the bottom in only two lists (at ranks 26 and 27 in runs clarity1 and DFReekLIM30, respectively). Because many posts for the topic were generated around the same time interval (January 26 X 29, 2011, when the event happened) and are ranked highly in many lists to be fused, post #30354903104749568 is rewarded and ranked as high as top 6 in the merged list.

Topics for which BurstFuseCombSUM cannot beat ComSUM tend to be quite general and unrelated to any specific time windows. Examples include topics MB023 (Amtrak train service) and MB027 (reduce energy consumption). For a very small number of queries, BurstFuseCombSUM X  X  performance, in terms of MAP or p@30 is worse than that of CombSUM. One reason that we observed for this phenomenon is that a very small number of posts are not relevant to the topics even if they are central to the bursts according to their timestamps, and hence they should not be rewarded. An example here is topic 5.5. Run-time analysis
We now turn to research question vii and examine the run-times of BurstFuseX. In particular, we explore what the added
BurstFuseX is developed in C++ and the experiments are run on a 10.6.8 MacBook Pro computer with 4 GB memory and a
CombSUM, BurstFuseCombMNZ and BurstFuse k -Merge), record the wall clock time. The results are recorded in Table 7 and plotted in Fig. 7 .
 As can be seen in Table 7 and Fig. 7 , the overhead of running BurstFuseX over simply running the standard fusion method
SUM and CombMNZ. BurstFuseCombSUM and BurstFuseCombMNZ merge the lists within 0.01 s when given 30 result lists and within 0.001 s when fusing two lists. In contrast, however, compared to any of the fusion methods, BurstFuse k -Merge
FuseX and the standard fusion method as well. For instance, the time needed to fuse 8 lists by BurstFuseCombSUM is nearly double the time needed for fusing 4 lists (2.96e 3 s and 1.50e 3 s, respectively). 5.6. Effect of fusing time-sensitive result lists
BurstFuseX uses temporal information in an essential way. Research question viii asks what happens when BurstFuseX by using BurstFuseX? To answer this question, we explore the performance of BurstFuseX using five result lists that them-as well as the standard fusion methods, CombSUM, CombMNZ and k -Merge to fuse those result lists, and report the compar-ison results.
 Table 8 shows the result of the comparisons between BurstFuseX and the five time-sensitive result lists. Obviously, Comb-SUM and CombMNZ perform on a par with the best result list (isiFDRML). For all metrics but p@5, BurstFuseCombSUM and
BurstFuseCombMNZ outperform the best result run as well as the standard fusion method they incorporate; many of the main reasons behind this is that posts within intervals in which many relevant posts appear can only be confirmed to be the best time-sensitive result list; as before, BurstFuse k -Merge does improve over k -Merge. 5.7. Further analysis of using burst information
We continue with research question viii and provide a further analysis of the use of burst information in the setting of microblog search. More specifically, the component lists that we consider next are generated using a temporal model for microblogs (TMM) ( Choi &amp; Croft, 2012 ), a textual quality factor model with temporal query expansion (LM-T(qe)) poral tweet selection feedback method (TSF + QDRM) ( Miyanishi et al., 2013b ).

Table 9 shows a comparison between fusion methods and the component lists that are burst-sensitive. As can be seen in underlines the merit of fusion for searching microblog posts and of using bursts information in the fusion step.
In Table 9 we also compare our BurstFuseX with CombSUM, CombMNZ, k -Merge, and BurstFuseX seX posts is the fusion method that detects bursts based on the content of posts by the detection approached in ( Lappas with BurstFuseX posts . We also see that detecting bursts based on one of the standard fusion methods, CombSUM or Comb-MNZ, works better than detecting bursts based on the content of posts.
 5.8. Performance of BurstFuseX on single result list
Finally, to address our final research question, ix, and understand whether BurstFuseX requires multiple result lists or compare the output against the single input list.
 challenging. 6. Conclusion
Various features of microblog posts make searching such posts a real challenge: their limited length, their dynamic nat-in part on the bursty nature of many discussions on microblog platforms. Our approach is based on integrating information generated by a standard fusion method, such as CombSUM, CombMNZ or k -Merge, detecting bursts of posts across the lists being fused, and rewarding posts that are published in or near a burst containing highly ranked posts. Our experimental oriented measures. Our new fusion method has a strong recall-enhancing effect; compared to the standard fusion method it and models that detect bursts based on the content of posts.

As to future work, we have only explored data fusion techniques in microblog search. But data fusion can be, and has mation into web search X  X or so-called fresh results ( Lefortier, Serdyukov, &amp; de Rijke, 2014 ). Acknowledgements Programme (FP7/2007-2013) under grant agreements nr 288024 (LiMoSINe) and nr 312827 (VOX-Pol), the Netherlands
Organisation for Scientific Research (NWO) under project nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013, the Center for Creation, Content and Technology (CCCT), the QuaMerdes project funded by the CLARIN-nl program, the TROVe project funded by the CLARIAH program, the Dutch national program COMMIT, the ESF Research Network Program ELIAS, the Elite
Network Shifts project funded by the Royal Dutch Academy of Sciences (KNAW), the Netherlands eScience Center under pro-ject number 027.012.105 the Yahoo! Faculty Research and Engagement Program, the Microsoft Research PhD program, and the HPC Fund.
 Appendix A. Detailing k -Merge
In this appendix we detail one of the standard fusion methods we use, k -Merge. To be able to define the fusion score for document d in response to query q according to k -Merge, we need to consider the sum of weighting the individual document scores in each list by the weight of the corresponding list: where a m is the weight of list L m ; f  X  x L m d ; h  X  is the scoring function for d in L f  X  x d ; h  X  , due to its widespread use ( Atrey, Hossain, Saddik, &amp; Kankanhalli, 2010 ), such that: where h n , the n -th dimension of vector h , is the weight of the n -th feature, and x L m the n -th feature of d in L m .
Now, writing C to denote the smoothed objective, according to k -Merge the parameters a on the gradients @ C =@ a m  X  P d  X  @ C =@ g  X  d ; q  X  X  X  @ g  X  d ; q  X  =@ a @ C =@ g  X  d ; q  X  is defined as: which we focus; hence, we optimize k -Merge for MAP.

Let r d and r e denote the rank positions of d and e in L r &gt; r e but the relevance level of d ; l  X  d  X  , is larger than that of e ; l  X  e  X  , then is the precision at rank k , and l 0  X  k  X  is the relevance value after the documents at positions r matically, the remaining derivatives can be presented as: @ g  X  d ; q  X  =@ a that: refer to ( Donmez, Svore, &amp; Burges, 2008; Sheldon et al., 2011 ).
 Before training k -Merge, a number of features have to be extracted for d 2C lizing burst information.
 References
