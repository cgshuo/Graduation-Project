 In recent years, online regret minimizing algorithms have become widely used and empirically suc-cessful algorithms for many machine learning problems. Notable examples include efficient learn-Most of these empirically successful algorithms are based on algorithms which are tailored to gen-eral convex functions, whose regret is O ( O (log T ) . These algorithms have potential to be highly applicable since many machine learning convex regularizer effectively makes this a strongly convex optimization problem (e.g. the SVM op-a template for deriving a wider class of regret-minimizing algorithms for online strongly convex programming.
 Online convex optimization takes place in a sequence of consecutive rounds. At each round, the ! : S  X  R . The goal of the learner is to minimize the difference between his cumulative loss and the cumulative loss of the optimal fixed vector, ! T vector.
 Roughly speaking, the family of regret minimizing algorithms (for general convex functions) can be By the aggressiveness of the update, we mean how much the algorithm moves its decision to be consistent with most recent loss functions. For example, the preceptron algorithm makes no update the loss when there is a margin mistake. These algorithms are shown to have improved performance (see for example the experimental study in Shalev-Shwartz and Singer [2007b]). in retrospect, the proof techniques have become somewhat boilerplate, which has lead to growing body of work to unify these analyses (see Cesa-Bianchi and Lugosi [2006] for review). Perhaps the most unified view of these algorithms is the  X  X rimal-dual X  framework of Shalev-Shwartz and Singer [2006], Shalev-Shwartz [2007], for which the gamut of these algorithms can be largely viewed as distance, then one obtains multiplicative updates). Second, the algorithm maintains both  X  X rimal X  and  X  X ual X  variables. Here, the the primal objective function is ! T value.
 This paper focuses on extending the duality framework for online convex programming to the case algorithms for online strongly convex programming. An important observation we make is that any  X  online rounds, the amount of intrinsic strong convexity we have in the primal objective ! t algorithm of Hazan et al. [2006]. Indeed, we show that our framework includes the gradient descent algorithm of Hazan et al. [2006] as an important special case, in which the aggressiveness level is minimal. At the most aggressive end, our framework yields the Follow-The-Leader algorithm. Furthermore, the template algorithm serves as a vehicle for deriving new algorithms (which enjoy logarithmic regret guarantees).
 a warmup, in Section 3, we present an intuitive primal-dual analysis of Follow-The-Leader (FTL), when f is the Euclidean norm. This naturally leads to a more general primal-dual algorithm (for which FTL is a special case), which we present in Section 4. Next, we further generalize our algorithmic framework to include strongly convex complexity functions f with respect to arbitrary Shwartz and Singer [2007a], but the analysis is rather specialized and does not have a knob which can tune the aggressiveness of the algorithm. Finally, in Sec. 6 we conclude with a side-by-side comparison of our algorithmic framework for strongly convex functions and the framework for (non-strongly) convex functions given in Shalev-Shwartz [2007]. the L  X  norm, &amp; x &amp;  X  = max i | x i | . We next recall a few definitions from convex analysis. A function f is  X  -strongly convex if then the function g ( w )= f ( w )  X   X  The Fenchel conjugate of a function f : S  X  R is defined as If f is closed and convex, then the Fenchel conjugate of f " is f itself (a function is closed if for ( cf ) " (  X  )= cf " (  X  /c ) .
  X  of f at w and is denoted by  X  f ( w ) .
 then the Fenchel-Young inequality holds with equality.
 for all  X  $  X   X  f ( w $ ) , we have f ( w $ )+ f " (  X  $ )= We make use of the following variant of Fenchel duality (see the appendix for more details): In this section, we provide a dual analysis for the FTL algorithm. The dual view of FTL will help convex functions.
 Recall that FTL algorithm is defined as follows: For each i  X  [ t  X  1] define g i ( w )= ! i ( w )  X   X  i therefore rewrite the objective function on the right-hand side of Eq. (2) as the following dual objective function Let (  X  t the optimal primal vector is given by (see again Sec. 2) setting for w t is in fact a minimizer of the primal objective, since (  X  t objective (see the appendix). The primal-dual view of Follow-the-Leader is presented in Figure 1. Denote To analyze the FTL algorithm, we first note that (by strong duality) Second, the fact that (  X  t +1 sufficiently large.
 Lemma 2 Let (  X  1 , . . . ,  X  t  X  1 ) be an arbitrary sequence of vectors. Denote w =  X  1 let v  X   X ! t ( w ) , and let  X  = v  X   X  t w . Then,  X   X   X  g t ( w ) and Proof We prove the lemma for the case t&gt; 1 . The case t =1 can be proved simi-larly. Since ! t ( w )=  X  t  X   X  Since  X   X   X  g t ( w ) , Lemma 1 thus implies that  X  w ,  X   X  X  X  g " the definition of  X  into the above we conclude our proof.
 Combining Lemma 2 with Eq. (7) and Eq. (8) we obtain the following: convex. Assume that the FTL algorithm runs on this sequence and for each t  X  [ T ] , let v t be in  X ! t ( w t ) . Then, If we are dealing with the square loss ! t ( w )= &amp; w  X   X  holding with equality. This equality is the underlying reason that the FTL strategy is a minimax strategy (See Abernethy et al. [2008] for a proof of this claim). a more general algorithmic framework for online optimization.
 We start by examining the analysis of the FTL algorithm. We first make the important observation that Lemma 2 is not specific to the FTL algorithm and in fact holds for any configuration of dual variables. Consider an arbitrary sequence of dual variables: (  X  2 and denote  X  t as in Eq. (6). Using weak duality, we can replace the equality in Eq. (7) with the following inequality that holds for any sequence of dual variables: A summary of the algorithmic framework is given in Fig. 2.
 The following theorem, a direct corollary of the previous equation and Lemma 2, shows that all instances of the framework achieve logarithmic regret.
 convex. Then, any algorithm that can be derived from Fig. 2 satisfies where v t  X   X ! t ( w t ) .
 Proof Let  X  t be as defined in Eq. (6). The last condition in the algorithm implies that The proof follows directly by combining the above with Eq. (10) and Lemma 2. We conclude this section by deriving several algorithms from the framework. Example 1 (Follow-The-Leader) As we have shown in Sec. 3, the FTL algorithm (Fig. 1) is equiva-in Fig. 2 and is therefore a special case.
 Example 2 (Gradient-Descent) Following Hazan et al. [2006], Bartlett et al. [2007] suggested the following update rule for differentiable strongly convex function the following update rule of the dual variables algorithm as a special case.
 Example 3 (Online Coordinate-Dual-Ascent) The FTL and the Gradient-Descent updates are two extreme cases of our algorithmic framework. The former makes the largest possible increase of increase requirement. Intuitively, the FTL method should have smaller regret as it consumes more as it requires a full blown optimization procedure at each online round. A possible compromise is extreme case, we optimize only with respect to the last dual variable. Formally, we let bound. The computational complexity of performing this update is often small as we optimize over problem and in these cases the computational complexity of the coordinate-dual-ascent update is identical to that of the gradient-descent method. tions. We first need the following generalized definition of strong convexity. Definition 1 A continuous function f is  X  -strongly convex over a convex set S with respect to a It is straightforward to show that the function f ( w )= 1 Euclidean norm. Less trivial examples are given below.
 Example 4 The function f ( w )= ! n plex, S = { w  X  R n f Example 5 For q  X  (1 , 2) , the function f ( w )= 1 respect to the L q norm. Its conjugate function is f " (  X  )= 1 For proofs, see for example Shalev-Shwartz [2007]. In the appendix, we list several important properties of strongly convex functions. In particular, the Fenchel conjugate of a strongly convex function is differentiable. that  X  t is known to the forecaster before he defines w t .
 For each round t , we now define the primal objective to be The dual objective is (see again Sec. 2) An algorithmic framework for online optimization in the presence of general strongly convex func-tions is given on the right-hand side of Fig. 3.
 The following theorem provides a logarithmic regret bound for the algorithmic framework given on the right-hand side of Fig. 3.
 be derived from Fig. 3 (right) satisfies where v t  X   X  g t ( w t ) and &amp;  X  &amp; " is the norm dual to &amp;  X  &amp; . The proof of the theorem is given in Sec. B In this paper, we extended the primal-dual algorithmic framework for general convex functions from Shalev-Shwartz and Singer [2006], Shalev-Shwartz [2007] to strongly convex functions. The tem-convex functions from Shalev-Shwartz and Singer [2006], Shalev-Shwartz [2007]. Here, f is the complexity function, (  X  t mal space. At the least aggressive extreme, in order to obtain [Beck and Teboulle, 2003, Grove et al., 2001, Kivinen and Warmuth, 1997], which specializes to gradient descent when f is the squared 2-norm or the exponentiated gradient descent algorithm when f is the relative entropy. At the most aggressive extreme, where D t is maximized at each round, we have  X  X ollow the Regularized Leader X , which is w t = arg min w ! t  X  1 2006, Shalev-Shwartz, 2007].
 The right algorithm in Figure 3 is our new contribution for strongly convex functions. Any  X  -strongly convex loss function can be decomposed into ! t =  X  f + g t , where g t is convex. The ! . At the most aggressive end of the spectrum, where D t is maximized at each round, we have the  X  X ollow the Leader X  (FTL) algorithm: w t = arg min w ! t  X  1 thermore, we provide algorithms which lie in between these two extremes  X  it is these algorithms which have the potential for most practical impact.
 Empirical observations suggest that algorithms which most aggressively close the duality gap tend to perform most favorably [Crammer et al., 2006, Shalev-Shwartz and Singer, 2007b]. However, at the FTL extreme, this is often computationally prohibitive to implement (as one must solve a full blown optimization problem at each round). Our template algorithm suggests a natural compromise, which is to optimize the dual objective but only with respect to a small number of dual variables (say the most current dual variable)  X  we coin this algorithm online coordinate-dual-ascent. In gradient-descent method. This variant update still enjoys a logarithmic regret bound.
