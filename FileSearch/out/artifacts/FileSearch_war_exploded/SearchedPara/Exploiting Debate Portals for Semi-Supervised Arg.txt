 Argumentation mining, an evolving sub-field of ious genres, such as legal cases (Mochales and Moens, 2011), student essays (Stab and Gurevych, 2014a), and medical and scientific articles (Green, 2014; Teufel and Moens, 2002). Recently, the fo-cus of argumentation mining has also shifted to the Web registers (such as comments to articles, forum posts, or blogs) which is motivated by the need of retrieving and understanding ordinary people X  X  ar-guments to various contentious topics on the large scale. Applications include passenger rights and protection (Park and Cardie, 2014), hotel reviews (Wachsmuth et al., 2014), and controversies in ed-ucation (Habernal et al., 2014).

Despite the plethora of existing argumentation theories (van Eemeren et al., 2014), the preva-lent view in argumentation mining treats argu-ments as discourse structures consisting of sev-eral argument components, such as claims and premises (Peldszus and Stede, 2013). Current approaches to automatic analysis of argumenta-tion usually follow the fully supervised machine-learning paradigm (Biran and Rambow, 2011; Stab and Gurevych, 2014b; Park and Cardie, 2014) and rely on manually annotated datasets. Only few publicly available argumentation cor-pora exist, as annotations are costly, error-prone, and require skilled human annotators (Stab and Gurevych, 2014a; Habernal et al., 2014).

To overcome the limited scope and size of the existing annotated corpora, semi-supervised meth-ods can be adopted, as they gain performance by exploiting large unlabeled datasets (Settles, 2012). However, unlike in other NLP tasks where data can be cheaply labeled using for example distant supervision, employing such methods in argumen-tation mining is questionable. First, argumenta-tion is an act of persuasion (Nettel and Roque, 2011; Mercier and Sperber, 2011) but not all user-generated texts can be treated as persuasive (Park and Cardie, 2014; Habernal et al., 2014), thus the selection of an appropriate unlabeled dataset rep-resents a problem on its own. Second, argument components (e.g., claims or premises ) are highly context-dependent and cannot be easily labeled in distant data using predefined patterns. So far, semi-supervised methods for argumentation min-ing remain unexplored.

In this article, we tackle argumentation min-ing of user-generated Web data by exploiting de-bate portals  X  X emi-structured discussion websites where members pose contentious questions to the community and allow others to pick a side and provide their opinions and arguments in order to is whether debate portals (which contain noisy user-generated data) can be utilized in a semi-supervised manner for fine-grained identification of argument components. As a second research question, we investigate to what extent our meth-ods are domain independent and evaluate their adaptation across several domains and registers.
Our contribution is three-fold. First, to the best of our knowledge, we present the first successful attempt to semi-supervised argumentation mining in Web data based on exploiting unlabeled exter-nal resources. We leverage these resources and derive features in an unsupervised manner by pro-jecting data from debate portals into a latent argu-ment space using unsupervised word embeddings and clustering. Second, our novel features sig-nificantly outperform state-of-the-art features in all scenarios, namely in cross-validation, cross-domain evaluation, and cross-register evaluation. Third, to ensure full reproducibility of our experi-ments, we provide all data and source codes under Analysis of argumentation has been an active topic in numerous research areas, such as philosophy (van Eemeren et al., 2014), communication studies (Mercier and Sperber, 2011), and informal logic (Blair, 2004), among others. In this section, we will focus on the most related works on argumen-tation mining techniques in NLP in the first part, with an emphasis on Web data in the second part.
Mochales and Moens (2011) based their work on argumentation schemes (Walton et al., 2008) and experimented with Araucaria and ECHR datasets using supervised models to classify ar-gumentative and non-argumentative sentences (  X  0 . 7 F 1 ) and their structure. Feng and Hirst (2011) classified argument schemes on the Araucaria dataset, reaching 0.6-0.9 accuracy. Experiments on this dataset were also conducted by Rooney et al. (2012), who classified sentences to four cate-gories ( conclusion , premise , conclusion-premise , and none ) and achieved 0.65 accuracy. These approaches assume the text is already segmented into argument components. Stab and Gurevych (2014b) examined argumentation in persuasive essays and classified argument components into four categories ( premise , claim , major claim , non-argumentative ) using SVM and achieved 0.73 macro F 1 score. They further classified argu-ment relations (support and attack) and reached 0.72 macro F 1 score. The best-performing fea-tures were structural features (such as the location or length ratios), as persuasive essays usually com-ply with a certain structure which can be seen as a potential drawback of this approach.
 Regarding user-generated Web data, Biran and Rambow (2011) used naive Bayes for classifying justification of subjective claims from blogs and Wikipedia talk pages, relying on features from RST Treebank and manually-processed n-grams. In similar Web registers, Rosenthal and McK-eown (2012) automatically determined whether a sentence is a claim using logistic regression and various lexical and sentiment-related features and achieved accuracy about 0.66-0.71. Park and Cardie (2014) classified propositions in user comments into three classes ( verifiable experi-ential , verifiable non-experiential , and unverifi-able ) using SVM and reached 0.69 macro F 1 score. Goudas et al. (2014) identified premises in Greek social media texts using BIO encoding and achieved 0.42 F 1 score with Conditional Random Fields. The research gaps in the above-mentioned approaches are the following. First, the argumen-tation models are simplified to either claims or a few types of premises / propositions . Second, the segmentation of discourse into argument compo-nents is ignored (except the work of Goudas et al. (2014)). Recently, Boltu  X  zi  X  c and  X  Snajder (2015) employed hierarchical clustering to cluster argu-ments in online debates using embeddings projec-tion, but in contrast to our work they performed only intrinsic evaluation of the clusters.

Debate portals have been used in a related body of research, such as classifying support and attack between posts by Cabrio and Villata (2012), or stance detection by Hasan and Ng (2013) or Got-tipati et al. (2013). These approaches consider the complete documents (posts) but do not ana-lyze the micro-level argumentation (e.g., claims or premises ). As data for training and evaluation of our methods, we use a corpus consisting of 340 English docu-mentation by Habernal et al. (2014). Compared to other corpora mentioned in the related work, this corpus is the largest one to date that covers dif-ferent domains and spans several registers of user-generated Web content. In particular, the corpus comprises four registers (comments to articles, fo-rum posts, blogs, and argumentative newswire ar-ticles) and covers six domains related to educatio-nal controversies (homeschooling, private vs. pub-lic schools, mainstreaming, single-sex education, prayer in schools, and redshirting).

The argumentation model used in this corpus is based on extended Toulmin X  X  model (Toulmin, 1958). Each document contains usually one ar-gument, where each argument consists of several argument components. There are five different components in this model, namely, the claim (the statement about to be established in the argument which conveys author X  X  stance towards the topic), the premise(s) (propositions that are intended to give reasons of some kind for the claim), the back-ing (additional information used to back-up the argument), the rebuttal (attacks the claim), and the refutation (which attacks the rebuttal). Rela-tions between the argument components are en-coded implicitly in the function of the particular component type, for instance, premises are always attached to the claim. We made two observations in the data: the claim is often implicit (must be inferred by the reader), and some sentences have no argumentative function (thus are not labeled by
Figure 1 depicts two example annotations from the corpus. Argument components were annotated on the token level as non-overlapping annotation spans. We therefore represent the argument anno-tations using BIO encoding. Each token is labeled with one of the 11 categories (5 argument com-ponent types  X  B or I tag + one O category for non-argumentative text). We cast the task of identifying argument compo-nents as a sequence tagging problem and employ tic annotations and feature engineering, we rely on two UIMA-based frameworks  X  DKProCore (Eckart de Castilho and Gurevych, 2014) and DKProTC (Daxenberger et al., 2014).

Although the argument component annotations in the corpus are aligned to the token boundaries (token-level annotations), the minimal classifica-tion unit in our sequence tagging approach is set to the sentence level. First, this allows us to cap-ture rich features that are available for entire sen-tences as opposed to the token level. Second, by modeling sequences on the token level we would dencies between labels, as the label context is lim-ited due to computational feasibility. On the token level, the label sequences are rather static (long se-quences with the same label), as opposed to the sentence level. Before the classification step, we adjust all annotation boundaries (note that we use 11 BIO labels) so that they are aligned to the sen-tence boundaries and each sentence is then treated as a single classification unit with one label (for example, the first sentence from Figure 1 with token labels Claim-B, Claim-I, Claim-I, ... be-comes Claim-B ). After classification, the labels are mapped back to tokens (so that, for example, Claim-B sentence label is transformed to Claim-B, Claim-I, ... token labels). However, all eval-uations are performed on the token level and the performance is always measured against the orig-inal token labels. Using this approximation, we lose only about 10% of F 1 performance. 7 4.1 Baseline features Lexical baseline (FS0) We encode the presence of unigrams, bigrams, and trigrams in the sentence as  X  X ne-hot X  (binary) features.
 Structural and syntactic features (FS1) Since the presence of discourse markers has been shown to be helpful in argument component analysis (e.g,  X  X herefore X  and  X  X ince X  for premises or  X  X hink X  and  X  X elieve X  for claims ), we encode the first and last three words as binary features. Further-more, we capture the relative position of the sen-tence in the paragraph and the document, the num-ber of part of speech 1-3 grams, maximum de-pendency tree depth, constituency tree produc-tion rules, and number of sub-clauses (Stab and Gurevych, 2014b). We used Stanford POS Tagger (Toutanova et al., 2003), Berkeley parser (Petrov et al., 2006), and Malt parser (Nivre, 2009). Sentiment and topic features (FS2) We as-sume that claims express sentiment, thus we com-pute five sentiment categories (from very nega-tive to very positive) using Stanford sentiment an-alyzer (Socher et al., 2013) and use these values directly as features. Furthermore, in order to help detecting off-topic and non-argument sentences, we employ topic model features. In particular, we use features taken from a vector representation of the sentence obtained by using Gibbs sampling on LDA model (Blei et al., 2003; McCallum, 2002) with topics trained on unlabeled data provided as Semantic and discourse features (FS3) Fea-tures based on semantic frames has been intro-duced in relevant works on stance recognition (Hasan and Ng, 2013). Our features, based on PropBank semantic role labels and obtained from NLP Semantic Role Labeler (Choi, 2012), ex-tract various semantic information (agent, predi-cate + agent, predicate + agent + patient + (op-tional) negation, argument type + argument value) and discourse markers. Discourse relations also play an important role in argumentation analysis (Cabrio et al., 2013). We thus employ binary fea-tures (such as the presence of the sentence in a chain, the transition type, the distance to previ-ous/next sentences in the chain, or the number of inter-sentence coreference links) obtained from Stanford Coreference Chain Resolver (Lee et al., 2013). Furthermore, we include features result-ing from a PTDB-style discourse parser (Li et al., 2012), such as the type of discourse relation (ex-plicit, implicit), the presence of discourse connec-tives, and attributions. 4.2 Unsupervised features We enrich the above-mentioned features by uti-lizing external large unlabeled resources  X  debate portals . They fulfill several criteria, namely (a) they are  X  X rgumentative X  (meant as opposed to, for example, prose or encyclopedic genres), (b) they are comprised of user-generated content and (c) and there is at least some overlap with topics from our experimental corpus. On the other hand, they contain noisy texts of questionable quality and they do not provide any specific argumentative structure (in fact, these debates are simple discus-sions to a topic, where each post is only labeled with a pro or contra stance). Nevertheless, we as-sume that the posts from (unlabeled) debate por-tals contain valuable information that will help us with classifying argument components in labeled data. In order to do so, we employ clustering based on latent semantics, which we now formalize as argument space features.

We assume that phrases (sentences or docu-ments) can be projected into a latent vector space, using, typically, a sum or a weighted average of all the word embeddings vectors in the phrase; see for example (Le and Mikolov, 2014). Neighbor-ing vectors in the latent vector space exhibit some interesting properties, such as semantic similarity (thoroughly studied within the distributional se-mantics area). If the latent vector space is clus-tered, each n-dimensional vector gets reduced to a single cluster number; such clusters have been used directly as features in many tasks, such as NER (Turian et al., 2010), POS tagging (Owoputi et al., 2013), or sentiment analysis (Habernal and Brychc  X   X n, 2013).

We build upon the above-mentioned approach (described by S X gaard (2013) as  X  X lusters-as-features X  semi-supervised paradigm) and extend it further. We take both sentences and posts from the unlabeled debate portals, project them into a latent space using word embeddings and cluster them. The motivation is that these clusters will contain similar phrases or (similar  X  X rguments X ). Centroids of these clusters would then represent a  X  X rototypical argument X  (note that the centroids exist only in the latent vector space and thus do not correspond to any existing sentence or post). Then we project each sentence (classification unit) in the labeled data to the latent vector space, com-pute its distance vector to all the cluster cen-troids, and encode this distance vector directly as real-valued features. By contrast to the above-mentioned works using a single cluster label as a feature, the distance vector to cluster centroids resembles a soft labeling where each sentence be-longs to several clusters with a certain  X  X eight X . We also use the latent vector space representation of the sentence directly as a feature vector.
As unlabeled data, we use data from two largest removed all posts with less than one  X  X oint X  Lucene framework and the top 100 debates for each of the 6 domains were retrieved which re-sulted into 5,759 posts (  X  35k sentences) in the unlabeled data in total. Our approach is formal-ized in the following paragraph.
 Argument space features (FS4) Let ~e ( w ) be the embedding vector of word w and tfidf( w ) be the TD-IDF value of w . Sentence ~s = ( w 1 , . . . , w n ) is then projected into the embed-ding space E as ~s e = so dim( ~s e ) = dim( E ) . Analogically to ~s , we project the entire post ~a = ( w 1 , . . . , w m ) to the same embedding space E such that ~a e = P Let K be the number of sentence clusters in E and ~c k a centroid vector of cluster k  X  K . Then ~s c denotes the distance of sentence ~s e to the sentence cluster centroids such that ~s c = (cos( ~s e , ~c 1 ) , . . . , cos( ~s e , ~c k )) where dim( ~s K and cos(  X  ,  X  ) denotes cosine similarity. Ana-logically, let L be the number of post clusters in E and ~a l a centroid vector of cluster l  X  L . Then ~s a denotes the distance of sentence ~s e to the post cluster centroids such that ~s a = feature vector by concatenating ~s e , ~s c and ~s a .
For word embeddings, we use pre-trained skip-al. (2013) ( dim( E ) = 300 ). To create clus-ters for the argument space features, we used section clustering method (Zhao and Karypis, 2002). We clustered the data using different hyper-parameters K and L (we experimented with K = { 50 , 100 , 500 , 1000 } and L = { 50 , 100 , 500 , 1000 } ). We investigate three evaluation scenarios. First, we report 10-fold cross validation over all 340 documents, where the data are randomly dis-tributed across the folds regardless of the domain or register. In this scenario, the model can bene-fit from domain-dependent features for the testing data, such as lexical knowledge (FS0) or domain-relevant argument space features (FS4). Second, we evaluate the cross-domain performance; the model is always trained on five domains and tested on the sixth one. In this settings, we also re-move all features that exploit distant data relevant to the test set. For instance, if the test domain is mainstreaming , we exclude all debates relevant to this domain before constructing the argument space features (FS4). This evaluates the model X  X  cross-domain performance without any target do-main data available. Finally, we test cross-register performance in two set-ups: we train the models using comments and forum posts and test on blogs and newswire articles, and then the other way round. We divided the data into these two parts based on similar properties of blogs/articles and comments/forums, such as the length, or the dis-tribution of argumentative and non-argumentative text.

In the evaluation, we focus on F 1 scores achieved on claims , premises , backing , and non-argumentative text (the  X  X  X  class). Although the classifier is trained and tested on all 11 classes in-cluding rebuttal and refutation , we do not report performance of these two argument components X  the results are very poor regardless of the param-eters for two reasons. First, these classes are un-derrepresented in the data ( Rebuttal-B , Rebuttal-I , Refutation-B and Refutation-I are present in only about 4% of sentences). Second, the inter-annotator agreement reached on these classes were reported to be very low (Habernal et al., 2014). Cross validation results Table 1 shows results for the cross-validation scenario. The human base-line in the first row is an average score between three original annotators of the dataset. The base-line features (FS0) perform poorly, yet they beat the random assignment and majority vote ( &lt; 0.12 F ). The argument space features (FS4) increase the performance in every combination. The best results for claims are achieved when only dis-course, sentiment, and argument space features are involved (FS3 and FS4), whereas premises and backing benefit from the presence of lexical, syn-tactic, and semantic features (the richest feature set). The overall average best results are obtained from a feature combination with higher level of abstraction, in particular without low-level lexical features from FS0.

After the cross validation experiments, we also fixed the hyperparameters (using grid search) to K = 1000 , L = 100 for the cluster sizes and t = 1 and e = 0 for the hyperparameters of SVM hmm . Cross-domain results For each domain, the cross-domain results are shown in Table 2. On average, the best results are about 0.10 F 1 points worse than in the cross-validation settings (Table 1). In all domains, the best average performance was achieved using only the argument space fea-tures (FS4); in four cases this system significantly outperforms all other systems ( p &lt; 0 . 001 ). More-over, more high-level feature set combinations that also contain argument space features (such as FS2+FS3+F4 or FS3+FS4) yield usually bet-ter results for particular argument components in contrast to features based on lexical or syntactic information (FS0 and FS1). For identifying non-argumentative texts, there is no clear winner with respect to feature set abstraction (in three domains the best results are achieved using FS4 but in other three domains the baseline FS0 performs best). Cross-register results The argument space fea-tures (FS4) performs best in average also in the cross-register evaluation (see Table 3). In recog-nizing premises , better results were achieved by a system trained on blogs and articles and tested on comments and forum posts. Recognizing claims exhibits similar behavior. On the other hand, recognizing non-argumentative text performs bet-ter in the opposite direction. On average, the cross-register results are much worse than cross-validation and slightly worse than cross-domain results. 5.1 Error analysis First, we quantitatively investigate errors in the cross-validation scenario. The confusion matrix in Table 4 shows that about 50-60% of errors for each argument component were caused by misclassify-ing it as non-argumentative (the  X  X  X  class). The system tends to prefer the  X  X  X  predictions because of the high presence of non-argumentative sen-tences in the corpus (about 57%). Backing is often confused with premises ; in particular, Backing-B with Premise-B in 14%, Backing-I with Premise-I in 17%. These two argument components have a similar function X  X o support the claim X  X o the dif-ferences in the discourse (which are sometimes very subtle) confuse the system. Note that despite the confusion between these classes, the -I and -B tags mostly remain the same (the system correctly predicts whether the argument component begins
We also analyzed the errors of the best-randomly sampled 40 documents and manually compared the predicted arguments with the gold data. We found that 11 predicted documents were simply wrong or no argument components were predicted at all (e.g., document #1640, #1658, #1021, #5258). Most of these errors occur in blogs, which seem to convey rather complex argumentation structure (#1666, #1197, #4586, #5258). In 8 documents, we identified that only some premises were (correctly) spotted by the sys-tem. This happened mostly in long comments (#452) and blogs (#400, #697, #4583). In 7 inves-tigated documents, we identified errors caused by slightly different boundaries of recognized argu-ment components (#4517, #2447, #2252, #4840) or when multiple segments were merged/split (#1604, #2180, #2310).

By analyzing the predicted output, we also found that in 12 documents the recognized argu-ment components seemed to be valid to some ex-tent, although this was our subjective judge. For instance, in #4285 (see Figure 2), the first premise was misclassified as a claim . The gold-data ar-gument was annotated as an enthymeme (with im-plicit claim that advocates private schools), while in the prediction, the same proposition was iden-tified as the an explicit claim supporting private schools with one premise why the education was not satisfying, which might be also another valid interpretation. The second example #2180 in Fig-ure 2 shows that the boundaries of the predicted premises are mixed up (two recognized instead of three), but the longer backing is also meaningful. These examples demonstrate that argument analy-sis is in some cases ambiguous and allows for dif-ferent valid interpretations. In this article, we proposed a semi-supervised model for argumentation mining of user-generated Web content. We developed new unsupervised features for argument component identification that exploit clustering of unlabeled argumenta-tive data from debate portals based on word em-beddings representation. With the help of these features we significantly improved performance of the argumentation mining system and outper-formed several baselines. While the improvement was decent in cross-validation scenario, we gained almost 100% improvement in cross-domain and cross-register settings.

We evaluated the methods on a publicly avail-able corpus annotated with argumentation that ori-gins from user-generated Web data. By a de-tailed analysis of the errors, we pointed out the strengths (such as domain adaptability) and weak-nesses (such as unsatisfying results for rebuttal and refutation components), as well as the chal-lenges for the argumentation mining task (such as boundary identification issues or ambiguous argu-ments). If we put our results into the context of existing works, the most relevant one by (Goudas et al., 2014) achieved 0.42 F 1 score on identifying only premises. We get comparable results in the cross-validation settings ( F 1 0.31-0.40) yet with more complex argumentation model (five different components).

Although argumentation mining in user-generated Web discourse has a long way to go (our methods currently achieve only about 50% of human performance), we see a huge potential for various future tasks, such as information seeking for better-informed personal decision making or support for argument quality assessment. To foster the research within the community, we provide all source codes and data required for the experiments under free licenses.
 This work has been supported by the Volk-swagen Foundation as part of the Lichtenberg-and by the German Institute for Educatio-nal Research (DIPF). Access to the CERIT-SC computing and storage facilities provided under the programme Center CERIT Scientific Cloud, part of the Operational Program Re-search and Development for Innovations, reg. no. CZ. 1.05/3.2.00/08.0144, is greatly appreciated. Lastly, we would like to thank the anonymous re-viewers for their valuable feedback.

