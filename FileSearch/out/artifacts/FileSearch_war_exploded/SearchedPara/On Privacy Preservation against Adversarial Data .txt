 Privacy preserving data processing has become an important topic recently because of advances in hardware technology which have lead to widespread proliferation of demographic and sensitive data. A rudimentary way to preserve privacy is to simply hide the infor-mation in some of the sensitive fields picked by a user. However, such a method is far from satisfactory in its ability to prevent adver-sarial data mining. Real data records are not randomly distributed. As a result, some fields in the records may be correlated with one another. If the correlation is sufficiently high, it may be possible for an adversary to predict some of the sensitive fields using other fields.

In this paper, we study the problem of privacy preservation against adversarial data mining , which is to hide a minimal set of entries so that the privacy of the sensitive fields are satisfactorily preserved. In other words, even by data min ing, an adversary still cannot ac-curately recover the hidden data entries. We model the problem concisely and develop an efficient heuristic algorithm which can find good solutions in practice. An extensive performance study is conducted on both synthetic and real data sets to examine the effectiveness of our approach.
 Categories and Subject Descriptors: H.2.8 [ Database Applica-tions ]: [Data Mining] General Terms: Security, Algorithms, Performance keywords: Privacy preservation, data mining, association rules
In recent years, large amounts of data about individuals have be-come available with corporations as well as public entities. This has led to serious concerns about the misuse and privacy of such data. Some interesting discourses on the nature of privacy in the context of recent trends in information technology may be found Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00.

Table 2: Table Employee after hiding some sensitive entries. in [6]. This has led to a considerable amount of research on the subject, such as [1, 2, 3, 4, 8, 9, 10, 13].

The most basic model of privacy preserving data processing is one in which we erase the sensitive entries in data. These erased entries are usually particular fields which are decided by the user, who may either be the owner or the contributor of the data. The ad-vantage of this approach is that it is extremely simple to implement in practice, and can be tailored easily to a variety of user prefer-ences. As a result, many current applications use this straightfor-ward method for privacy preservation. A variety of methods such as conceptual reconstruction [5] can be used to apply existing data mining algorithms on such data with missing values. A key weak-ness of this approach is that a data mining proficient adversary may use the correlations among the fields in the data to guess the sensi-tive fields from other (non-sensitive) fields.

E XAMPLE 1(M OTIVATION ). Consider a table Employees in Table 1. Suppose some users want to hide some information as follows. (1) Cathy wants to hide her salary level; (2) Frank wants to hide his education background; (3) Grace wants to hide her mar-riage status; (4) Ian wants to hide his gender; (5) Janet wants to hide her gender as well; and (6) All names should be hidden and replaced by random row-ids. The table after hiding is shown in Ta-ble 2. However, it is possible to generate the following association rules from Table 2: R 1 : Assistant  X  SL-3 (support: 2 , confidence 100% ),
R 2 : Manager  X  SL-5  X  University (support:3, confidence 66 and
R 3 : Manager  X  Fem al e  X  Married (support:3, confidence
These rules have a revealing effect on the values of the individual records. For example, one may accurately predict that (1) the miss-ing value in tuple 5 is  X  X niversity X  (by rule R 2 ); (2) the missing value in tuple 8 is  X  X L-3 X  (by rule R 1 ); and (3) the missing value in tuple 9 is  X  X arried X  (by rule R 3 ). However, the missing values in tuples 6 and 10 cannot be predicted accurately.

The inference of sensitive fields with the use of correlations is undesirable from a privacy preservation perspective. Therefore, in order to prevent such inference, it may be desirable to also hide some of the non-sensitive entries. The corresponding tradeoff here is that unnecessary hiding of entries loses information for the pur-pose of data analysis applications. Therefore, it is important to hide a minimal set of entries (i.e. a set of minimum size) in order to pre-vent such privacy violations.

We define the problem of privacy preservation against adversar-ial data mining as that of hiding a minimal set of entries so that the privacy of the sensitive fields are satisfactorily preserved.
Our study is critically different from the currently active studies on privacy-preserving data mining, such as [1, 2, 3, 8, 10, 13]. Pri-vacy preserving data mining tries to transform the data in some way such that a certain types of data mining tasks can still be conducted with guaranteed privacy. Therefor e, the focus is on effective min-ing using partially hidden or distorted data. In privacy preservation against adversarial data mining, we do not aim at any specific data mining tasks. Instead, we generally want to preserve the privacy against any attacks by abuse of data mining. Therefore, the hidden data cannot be recovered by data mining. Our study is also different from [14], which investigates how to hide a set of association rules. In particular, the method in [14] can only hide rules supported by disjoint frequent itemsets. This is done by decreasing their support or confidence, and can hide only a rule at a time.

We make several contributions. First, we model the problem of privacy preservation against adversarial data mining concisely . Our model is general and is independent of specific adversarial data mining techniques. We also point out that finding an optimal so-lution to the problem of privacy preservation against adversarial data mining is NP-hard. Second, we develop an effective and ef-ficient heuristic algorithm to find practically effective solutions to the problem of privacy preservation against adversarial data min-ing. Last, we conduct an extensive empirical evaluation on both real data sets and synthetic data sets to examine the performance of our method . The results show that our method is effective and efficient in practice.

The remainder of the paper is organized as follows. We formu-late the problem in Section 2. In Section 3, we overview the proce-dure of privacy preservation against adversarial data mining. The details of the proposed approach are developed in Sections 4 and 5. An empirical evaluation is reported in Section 6. Consider a database T of n records t 1 ,...,t n and m attributes D ,...,D m . Without loss of generality, we assume that the do-mains of dimensions are exclusive.

The value of record t i on attribute D j is denoted by t i,j also refer to t i,j as an entry . We note that an entry is a specific value in a tuple , instead of an attribute value appearing in the ta-ble. At this moment, we assume that all attributes are categorical. We note that any continuous attribute can be transformed to a cat-egorical domain by using the process of discretization. After the process of discretization, the methods discussed in this paper can be utilized for deciding which entries to remove. At the end of this process, the discretized attributes are replaced by the mid-points of the corresponding ranges. The process of discretization results is an additional level of approximation of the attribute values. All results in this paper will continue to hold for this discretized con-tinuous case, except that these results need to be expressed in terms of the discretized attributes.

A set of entries in one record is called an entry-set .Thesetof privacy sensitive entries in the data is called the directly private set , denoted by P . In the most general case, the sensitivity of the data may be defined not only by the fields, but also by a combination of the data records as well as their attributes. For example, one user may wish to be private about his or her education level, whereas another user may be private about the age attribute. Furthermore, there may be other fields (such as salary) which may be defined as globally sensitive at the administrator level. Therefore, the directly private set is defined at the entry level rather than at the attribute level .

For the sake of simplicity, we call a value in an attribute an item , and a combination of multiple items an itemset . Clearly, an item or an itemset can appear in multiple tuples. In other words, an item can match multiple entries and an itemset can match multiple entry-sets. The use of this terminology helps us to leverage on existing machinery for association rules and large itemset generation.
It is further assumed that the information hiding is done at the server end. Thus, while the administrator at the server end is privy to the entire set of records, they do not make the entire data set publicly available. The advantage of information hiding at the server end is that it is possible to use the inter-attribute correla-tions among the different records in order to decide which entries should be masked .

The primary question in the problem of privacy preservation against adversarial data mining is the choice of entries which should be hidden . While it is clear that the entries in P should be hidden, it is also important to remove other entries which have predictive power. We use t i,j =# to denote that the value is hidden. The ta-ble in which the privacy sensitive entries are blanked out is denoted by ( T  X  P ) .

An itemset X is said to appear in a tuple t if X matches a set of entries in t . Moreover, for a table T and a directly private set P , X is said to publicly appear in a tuple t if the entries in t match-ing X are not in P (i.e., not blanked out in ( T  X  P ) ). Consider table Employee in Table 2. The set of sensitive entries is P { t
As shown in Example 1, if we publicly publish Table 2, i.e., (
T  X  P ) , it may not preserve the privacy sufficiently. Therefore, only blanking out the sensitive entries is inadequate. If some of the fields have known correlations to the other fields, they may be used to predict the sensitive entries in the data. In other words, entries which have strong predictive power to any entry in set P need to be removed from the data. Therefo re, the first step is to determine the entries which have strong predictive power to entries in set P .
A user may find correlation/association rules from ( T  X  P use the rules to predict the values in the blank entries. The problem of privacy preservation against adversarial data mining is to find a set of entries K such that predictive methods using only the infor-mation in ( T  X  P  X  K ) cannot have an accuracy at least  X  to predict the values of any entries in P ,where  X   X  (0 , 1] is a user-specified parameter. To make the information loss as little as possible, we want to minimize the size of K . We will refer to the set K as the derived private set .

One way to select entries for the derived private set is to use those entries to invalidate confident rules.

E XAMPLE 2(R ULE INVALIDATION ). Consider rule R 2 : Man-ager  X  SL-5  X  University discussed in Example 1. In order to in-validate this rule, we can blank some occurrences of  X  X anager X ,  X  X L-5 X  and/or  X  X niversity X  in tuples 5 and 7 . Note that we do not have to blank all those occurrences. For example, if t 1 , Education University is removed, then the confidence of the rule R 2  X  SL-5  X  University is lowered down to 50% .

Moreover, we note that removing t 1 , Title = Manager is more ad-vantageous than removing t 1 , Education = University since t affects two rules R 2 and R 3 rather than just one.

If the minimum support of a rule is 2 and the confidence thresh-old is 60% , then by blanking out only the entry t 1 , Title both rules R 2 and R 3 .

The process of removing derived private entries in order to re-duce the confidence level of the rules in the data is called rule in-validation .

A second way of protecting the sensitive entries is to prevent rules from being fired by blanking out the entries in the sensitive records corresponding to the antecedents of the rules. In such a case, even though the rules may continue to have high relevance (confidence level), the entries in the antecedents of the rules may get blanked out for the sensitive records only. This process is re-ferred to as rule marginalization .
 Assistant  X  SL-3 in Example 1. In Table 2, only sensitive entry t 8 , Salary-level can be predicted using this rule. Therefore, instead of invalidating the rule, we can simply blank out entry t Assistant. Then, sensitive entry t 8 , Salary-level cannot be predicted accurately.

Rule marginalization refers to the fact that the rules may continue to have high confidence level, but get marginalized because they are no longer relevant to any sensitive entry in the data. Therefore, the predictive power of the rule is effectively removed. Rule marginal-ization is especially useful when only a small number of users have chosen to keep their records private for a particular column. In such a case, it is possible to block a small number of antecedents from the rules in order to keep the entries private.

Generally, it is a tricky question to determine whether it is more useful to remove entries in order to prevent rules from being fired or whether it is more useful to aim for reducing the confidence level of rules. Moreover, blanking out an entry may simultaneously prevent some rules from firing and reduce the confidence level of some other rules. In a later section, we will explore this tradeoff, and discuss an effective strategy for balancing rule invalidation and marginalization.

We note that the predictive nature of the problem has some simi-larity to the problem of adversarial classification [7], but with some critical differences. First, while the adversarial classification prob-lem concentrates on the prediction of a single field, the problem of privacy preservation against adversarial data mining studied here may involve prediction of any sensitive field in the data . This makes the problem much more general and more difficult to solve than a standard classification problem. At the same time, it is also more difficult for an adversary who may need to be able to make pre-diction on multiple fields in the data from an incomplete data set. Second, while the adversarial classification problem concentrates on the method of data perturbation as an adversarial measure, the problem of privacy preservation against adversarial data mining concentrates on the issue of information hiding as a measure to thwart privacy attacks .

In this paper, we utilize association rules to construct the model which decides the entries to be hidden. This is because association based methods are easy to generalize to the case where the predic-tion may be performed over any fields in the data. Furthermore, such methods are relatively robust for a large number of applica-tions in which the data records have high dimensionality. In such cases, there exist an exponential number of subspaces that such a predictor can explore over the data. The related work in [15] com-pares the effects of different schemes for creating privacy leakage in the context of learning based systems. This is orthogonal to the goal of our paper, which uses attribute suppression in order to pre-serveprivacyinamoregeneralsetting.
We want to determine association rules with strong predictive power. However, in this case it is more important to find asso-ciation rules with a high confidence level than those with a high support level. In fact, it is important to be exhaustive in the asso-ciation rule generation process in order to ensure that none of the sensitive fields are divulged. The overall algorithm for adversarial data suppression is in the following two steps:
Step 1: Mining adversarial rules . We mine all association rules from ( T  X  P ) in the form of X  X  y where X is a set of attribute values and y is a value on an attribute Y such that (1) the confidence of the rule is no less than  X  in ( T  X  P ) ; and (2) for tuples t where X publicly appears and the value of t on attribute Y is blanked out, t has value y on Y with a probability of at least  X  . Such confident association rules related to the adversarial data mining are called adversarial rules .

Step 2: Determining derived private set . We select a set of en-tries K  X  ( T  X  P ) such that by deleting the entries corresponding to P  X  K , no adversarial rules can be fired to predict any value in P accurately. We note that the process of blanking out entries in K reduces the amount of information available in the data. This is an unfortunate consequence of the privacy preservation process. These effects need to be minimized. Therefore, we would like to minimize the cardinality of the derived private set K .
 We establish the correctness of the above algorithm as follows. For a table T and a directly private set P , consider a rule R y ,where X is an itemset and y is a value on attribute Y . All tuples t in which X publicly appears and the attribute value on Y is not blanked out form the public set of R , denoted by pub ( R pub ( R )= { t | t  X  T,X publicly appears in t, t.Y  X  P } can derive the confidence of R from its public set. The confidence of R in the public set is called the public confidence of R .
On the other hand, X may publicly appear in some tuples in T but the Y attribute values in those tuples are blanked out. Then, R can be used to predict the value of Y in those tuples. Such tuples are called the hidden set of R , denoted by hid ( R ) .Thatis, hid { t | t  X  T,X publicly appears in t, t.Y  X  P } . The confidence of R in the hidden set is called the hidden confidence of R . Clearly, an external user can only calculate the public confidence from P ) . Since we assume that information hiding is done at the server end, only the owner/administrator of the original data can calculate the hidden confidence.

Generally, let  X  be a user specified confidence threshold. For atable T and a directly private set P ,an adversarial rule (with respect to  X  )isanassociationrule R : X  X  y such that both the public and hidden confidence of R are at least  X  .

An adversarial rule is potentially revealing when it has high pub-lic confidence and high hidden confidence. Otherwise, it cannot be used to predict accurately. That is, if a rule has high public confi-dence but low hidden confidence, then it cannot accurately predict the blanked entries. On the other hand, if a rule has high hidden confidence but low public confidence, then a user who can only read ( T  X  P ) cannot identify the rule from the public data.
T HEOREM 1(C ORRECTNESS ). For a table T and a directly private set P , any entry z  X  P on attribute Y cannot be predicted with a confidence  X  or higher using only the information from P ) if and only if there exists no adversarial rule R : X  X  (
T  X  P ) such that y is on attribute Y and X publicly appears in the tuple t containing z .

We will mine all adversarial rules in a depth-first search frame-work. The complete set of itemsets can be enumerated using a set enumeration tree [11]. A set enumeration tree employs a total order on the set of all items. Each itemset is treated as a string such that all items in the itemset are sorted in the total order. Then, an item-set X is an ancestor of another itemset Y in the set enumeration tree if X is a prefix of Y .

The general idea of our algorithm to mine the complete set of adversarial rules is to conduct a depth-first search of the set enu-meration tree. The nodes in the tree are the itemsets publicly ap-pearing in some tuples. We check whether such itemsets can be an antecedent of some adversarial rules.

Clearly, for any adversarial rule R : X  X  y , X must publicly appear in some tuples. Hence, the search of adversarial rules is complete if all the itemsets publicly appearing in some tuples can be enumerated completely. The correctness and the completeness of the mining of adversarial rules by a depth-first search of a set enumeration tree immediately follow the following property.
T HEOREM 2(A NTI -MONOTONICITY ). If an itemset X does not publicly appear in any tuple, then any superset of X cannot publicly appear in any tuple.

Often, a complete set enumeration tree can be huge in real ap-plications, when there are hundreds or even thousands of items. Therefore, it is important to prune significant portions of the set enumeration tree.

First of all, we can prune all those itemsets that do not publicly appear in a data set.
 SETS ) In the depth-first search of the set enumeration tree, an item-set X which does not publicly appear in any tuple, as well as all the supersets of X can be pruned.

Not every itemset publicly appearing in some tuples is an an-tecedent of some adversarial rule. This fact can be used to improve the effectiveness of the depth-first search algorithm. This means that if we can determine that all nodes in a subtree cannot be an-tecedents of any adversarial rule, then the subtree can be pruned and the search space is narrowed. We can examine whether an itemset and its supersets are antecedents of some adversarial rules from the projected databases .
 Let X be an itemset, T be a table, and P be a directly private set. For a tuple t in T ,if X publicly appears in t , then the projection of t with respect to X , denoted by t | X , is the set of entries in t that are not matched by X .If X does not publicly appear in t ,then t | X =  X  .

The projected database with respect to X is the set of nonempty projections with respect to X in T .

The concept of projected databases and its utilization are illus-trated in the following example.

E XAMPLE 4(P ROJECTED DATABASE ). Consider the example illustrated in Table 2. The projected database for itemset X {
College } is T | X 1 = { (Assistant, Female, Unmarried, SL-3), (As-sistant, Male, Married, SL-3) } . The projected database for itemset X 2 = { Accountant } is T | X 2 = { (#, Unmarried, MBA, SL-5), (#, Married, University, SL-4) } . The projected database for item-set X 3 = { Unmarried, SL-5 } is T | X 3 = { (Accountant, #, MBA), (Manager, Female, University) } . The projected database for item-set X 4 = { Manager, Female } is T | X 4 = { (Married, University, SL-5), (Married, Ph.D., SL-5), (Unmarried, University, SL-5), (#, MBA, SL-7) } .

The itemsets can be divided into five categories according to the projected databases of the itemsets. The categorization of itemsets is very useful from the perspective of rule pruning. For some cate-gories of itemsets, it is possible to design efficient pruning rules by using specific properties of those categories.

An itemset is privacy-free if its projected database does not con-tain any directly private entry at all.
 X 1 = { College } from Example 4. Since its projected database contains no directly private entry, X 1 cannot be the antecedent of any adversarial rule. Moreover, any superset of X 1 ,suchas {
Assistant, College } and { Assistant, Male, College } , cannot be the antecedent of any adversarial rule, either.
 the depth-first search of the set enumeration tree, a privacy-free itemset and all supersets of it can be pruned.

An itemset X is non-discriminative if every tuple in the pro-jected database of X contains directly private entries in the same attribute(s).
 itemset X 2 in Example 4. Every tuple in T | X 2 has a blanked value in attribute Gender . Therefore, any association rule having X any superset of X 2 cannot make an accurate prediction of the gen-der of accountants. In other words, those itemsets cannot be the antecedents of any adversarial rules with respect to gender. SETS ) In the depth-first search of the set enumeration tree, if an itemset X is non-discriminative with respect to Y , then any super-set of X is also non-discriminative with respect to Y . Y can be pruned from the projected databases of X and any superset of X . Moreover, if an itemset X is non-discriminative with respect to all other attributes that contain some entries in the directly private set, then X and its supersets can be pruned.

An itemset X is said to be a contrast itemset if for any entry y  X  P such that X  X  y appears in some tuples in T , X  X  y has a public confidence of 0 . Clearly, X as well as any supersets of X cannot be used to accurately predict y .
 in Example 4. In the projected database, there is one blanked en-try in attribute Gender , whose value is  X  X emale X . However, in the public set of X 3 ,rule X  X 3  X  Male X  has a public confidence of 100% . Thus, X 3 or any of its supersets cannot be used to predict the gender accurately.
 depth-first search of the set enumeration tree, a contrast itemset and all supersets of it can be pruned.

An itemset X is discriminative if X is the antecedent of some adversarial rules. This can be determined by checking the projected database of X . Technically, if there is a value y such that X has public and hidden confidence of at least  X  with respect to the projected database of X ,then X is discriminative.
 set X 4 in Example 4. In the projected database, there is one blanked entry in attribute Married-or-not , whose value is  X  X arried X . More-over, two out of the three tuples in the projected database have non-blanked value  X  X arried X  in attribute Married-or-not . Thus, we can generate a confident association rule Manager  X  Fem al e  X  ried , which is the adversarial rule R 3 discussed in Example 1. Input: atable T , a directly private set P , a confidence threshold  X  , Output: the complete set of adversarial rules; Method: 1: let A 1 ,..., A n be an order of attributes, extend the order to an 2: conduct a depth-first search on the set enumeration tree of 3: if the support of X is less than  X  then return; 4: create the projected database for X ; 5: // applying Pruning Rules 2, 3 and 4 6: if X is discriminative then output an adversarial rule; 7: remove unpredictable attributes with respect to X ; 8: for each item z appearing in the projected database in any
We note that an itemset may be the antecedent of more than one adversarial rule. However, all adversarial rules having the same antecedent can be generated by only one scan of the projected database. We only have to maintain a set of counters which track the number of occurrences of different attribute values in their pub-lic and hidden sets respectively. During the scan the counters can be updated by examining each record sequentially.
 The supersets of discriminative itemsets should still be checked. This is because we may find confident adversarial rules among these supersets. Note that we have to either invalidate or marginal-ize all adversarial rules.

An itemset is undetermined if it is not in any of the previous four categories. For such an itemset, we can neither prune it, nor generate adversarial rules. Therefore, the depth-first search needs to be continued at such nodes in order to make judgements about the itemsets in the corresponding subtrees.

The efficiency of the depth-first search method can be improved further by utilizing the following observation. If the support of an adversarial rule is very low, then the adversarial rule can be statis-tically insignificant . In a real application, a user may often specify a minimum support threshold. We only determine adversarial rules whose support is at least equal to this threshold. Therefore, any itemset whose support is less than this threshold can be pruned, and so can its supersets. The corresponding algorithm FA i R (F inding A dversari al R ules) is illustrated in Figure 1.
We can use the set of adversarial rules to determine the set of entries which need to be removed from the data. Unfortunately, the problem of finding the smallest derived private set is NP-hard (limited by space, we omit the formal result here). Thus, we need to design a heuristic algorithm to find practically effective solutions.
We need to find a good tradeoff between rule invalidation and rule marginalization. we can construct an effective solution for the task by quantifying the level of information revealed by the differ-ent entries. The greater the level of information revealed by an en-try, the greater the weight of the corresponding entry. This weight is denoted by c i,j for entry t i,j .

Initially, we set c i,j =0 for all the entries. Let us consider a database containing N entries. We note that when an entry is deleted, it could either prevent a rule from being fired because of rule marginalization, or it could prevent a rule from being found because of rule invalidation.
 Input: atable T , a directly private set P , a confidence threshold  X  , Output: a set of derived entry sets; Method: 1: set c i,j =0 for all entries; 2: while rule set R is not empty do 3: for each rule r  X  X  and entry t i,j  X  T do 4: compute the contribution of blanking t i,j to invalidation 5: add the contribution to c i,j ; 6: blank a fraction f of the original number of entries with the 7: remove the rules invalidated or marginalized; Consider an entry t i,j and a rule R : X  X  y where y  X  Y . Three cases may arise: tuple t i (the tuple containing entry t in the public set pub ( R ) , t i is in the hidden set hid irrelevant to R . We will compute the contribution of blanking out the entry t i,j in each case as follows.

First, if tuple t i is in pub ( R ) , i.e., X  X  X  y } publicly appears in t , then blanking out t i,j will reduce either the public confidence of R (when t i,j  X  Y ) or the size of the public set of R (when t matches an item in X )by 1 / | pub ( R ) | . This is the contribution of blanking t i,j to the invalidation of R .

Second, if tuple t i is in hid ( R ) , i.e., X publicly appears in t but t i .Y  X  P ( t i has a blanked value on the attribute that y may appear), then blanking out t i,j marginalizes R in one instance (i.e., this tuple). In order to fully marginalize all instances of R , we need to blank out a total of | hid ( R ) | entries. Thus, the contribution of blanking out t i,j to the marginalization of R is 1 / | hid
Last, if t i is not in pub ( R ) or hid ( R ) , then blanking out t not make any contribution to the invalidation or marginalization of R .

Therefore, the weight c i,j can be calculated as the sum of con-tributions of blanking out t i,j to all adversarial rules. The entries with the highest weights should be blanked out. Once an entry is blanked out, the weights of other entries should be adjusted. The blanking process can be conducted iteratively.

The process of blanking the entries can turn out to be cumber-some if the weights need to be re-computed at each step. Therefore, we batch the process of blanking out the entries after computing the corresponding weights. The process of computing weights of the different entries is done using a single pass in which all rules are iterated over the entries of a given record in order to determine the corresponding weights. The weights are then stored, and then the batch process of blanking out is started. In the process of batching the blanking of entries, we blank out k sensitive entries (i.e., the entries with c i,j &gt; 0 ) in each pass. This ensures that a maximum of at most m/k passes need to be performed on the data, where m is the total number of sensitive entries. We note that the process of batching leads to some reduction in accuracy, but this is a natural tradeoff with the efficiency of the entire process. The parameter k is called the blanking factor , and can correspondingly be tuned in order to achieve the desired tradeoff between accuracy and ef-ficiency. The algorithm GraDeS (for G enera ting De rived S et) is shown in Figure 2. All algorithms were implemented in Microsoft Visual C++ V6.0. All experiments were run on an IBM ThinkPad T42 laptop com-puter which has one Intel Pentium M 1.5 GHz processor and 768 M main memory, and runs Microsoft Windows XP operating sys-tem. We used both synthetic and real data sets in our experiments. Figure 3: Zipf factor vs. num-ber of adversarial rules.
Figure 5: # of adversarial rules/ sensitive entries vs. confidence. Figure 7: Derived private set size vs. blanking factor.
Figure 9: # of adversarial rules/ sensitive entries vs. db size.
We generated synthetic data sets using the Zipf distribution to determine the tuple values on each dimension. The data generator uses the following parameters (default values in brackets): (1) di-mensionality ( 10 ); (2) cardinality ( 10 ); (3) Zipf factor ( number of tuples in the table ( 10 , 000 ). We generated the directly private sets so that a p % of randomly selected entries are hidden. The parameter p is chosen to be 1% by default. This means that a table containing 10 , 000 tuples and 10 dimensions will contain 1 , 000 entries in the directly private set.

Figure 3 shows the variation in the number of adversarial rules with varying Zipf factor for different dimensionalities and cardinal-ities of the data set. The number of tuples was set at 10 confidence threshold was set at 60% , the minimum support thresh-old was set at 0 . 05% , and the directly private set randomly hid of entries in the table. We tested different cases of data set dimen-sionality and cardinality. The data becomes much more correlated when the Zipf factor increases. When the Zipf factor is in the range of 2 to 3 , the number of adversarial rules is large. When the Zipf factor keeps increasing to 5 , the number of adversarial rules de-creases. The reason is that when Zipf factor is large, there are some strong rules with high support across many tuples. However, there are not many such rules. In the same figure, we also show the num-ber of sensitive entries (the entries not in directly private set, but which are relevant to at least one adversarial rule). It goes up as the Zipf factor increases, but is bounded by the total number of non-blanked entries in the table. Interestingly, the number of sensitive entries does not drop with high Zipf factor, whereas the number of adversarial rules decreases. In such situations, the support of rules increases and the number of tuples affected by the adversarial rules remains stable. This helps in containing the final number of values in the derived private set.

The major observation is that the number of rules increases with dimensionality, but reduces with increasing cardinality on each cat-egorical attribute. This is because increasing cardinality makes the data set more diverse whereas increasing dimensionality increases the number of possibilities for finding adversarial rules.
In Figure 4, we tested the effect of support threshold on the num-ber of adversarial rules and the number of sensitive entries. The parameters of the data set were set to default, and the confidence threshold was set to 60% . As can be seen, the number of adver-sarial rules increases exponentially as the support threshold goes down, which is similar to the well-known effect in frequent pattern mining. However, the number of sensitive entries changes linearly, since the Zipf distribution embeds some correlations with high sup-port, which affects many tuples. The number of entries sensitive to adversarial rules with low support is limited. This is an encourag-ing observation since a reduced number of sensitive entries would indicate a modest size of the derived private set. We will examine this issue in more detail in the next section.

The effect of confidence threshold on the number of adversar-ial rules and the number of sensitive entries was tested using the same synthetic data set, as shown Figure 5. The support threshold was set to 0 . 05% . As expected, a lower confidence threshold intro-duces more adversarial rules and sensitive entries. An interesting observation is that the data set following a Zipf distribution has a non-trivial number of adversarial rules of 100% confidence, which affect about 20% of the tuples in the table. We note that the cor-responding entries are very valuable from the point of view of an adversary.

Using the same synthetic data set, we tested the effect of size of directly private set on the number of adversarial rules, as shown in Figure 6. The support threshold and confidence threshold were fixed to 0 . 05% and 60% , respectively. When the directly private set is small, the number of adversarial rules is also small since the rules must be associated with some directly private entries. When the directly private set becomes large, the number of adversarial rules increases linearly. The relatively modest increase in the num-ber of adversarial rules with increasing direct private set size is an encouraging observation, since it tends to indicate that the derived private set is also likely to increase modestly.

The cost of our approach consists of two parts: mining the adver-sarial rules and computing the derived private set. The first step is quite efficient. In our experiments, it takes less than 5 data sets with 10 , 000 tuples and is linearly scalable with respect to database size. This is only a very minor component of the total cost. Limited by space, we omit these details and concentrate on the more computationally challenging issue of finding the derived private set from the rules. In our heuristic approach (Section 5), the blanking factor can be leveraged as a useful parameter to control the tradeoff between the size of derived private set and efficiency.
Figure 7 plots the size of derived private set with respect to the blanking factor. As a reference, the total number of sensitive en-tries in this test is 61 , 964 . The support threshold and the confi-dence threshold were set to 0 . 1% and 80% , respectively. The cor-responding running time is shown in Figure 8. We note that the running time is quite modest for most practical settings.
From Figures 7 and 8, we observe the following. First, the pri-vacy can be preserved by blanking out only a very small subset of sensitive entries. In the setting of this experiment, if we blank one Figure 11: # of sensitive en-tries/adverserial rules/derived private set size vs. support (Adult). entry at a time, we only need to blank 545 entries in the derived private set, which is less than 1% of the set of all sensitive entries. Blanking out less than 600 entries to preserve the privacy of directly private entries allows the approach to remain practical from the point of view of information preservation.

In general, the less entries we blank out in a round, the smaller the derived private set we get. On the other hand, it increases the running time. We show that the tradeoff is such that a minor in-crease of the derived private set can lead to a substantial gain in efficiency. For example, by increasing the blanking factor from to 30 the runtime was drastically reduced from 1 , 988 seconds to 157 seconds, while the derived private set size only modestly in-creased from 546 entries to 1 , 320 entries. The iterative use of a blanking factor helps in substantially improving the efficiency of the algorithm with only a modest loss in the data entries.
We tested the scalability of privacy preservation against adver-sarial data mining with respect to the number of tuples in the table. The results are shown in Figure 9. The data sets took the default parameters except that the number of tuples ranged from 10 to 100 , 000 . The directly private set randomly hid 1% entries in the table. We fixed the confidence and support threshold to and 0 . 05% , respectively. Interestingly, the number of adversarial rules decreases as the size of table increases. In a data set follow-ing the Zipf distribution, as the database size goes up, the rules about the bias values in dimensions gain support much more than the other rules. Since we kept the support threshold constant in percentage, those rules whose support do not grow in the same rate became infrequent. This resulted in fewer rules in large table of high support. This phenomenon matches the scenarios in real ap-plications. In small tables, we can often observe many occasional correlations. However, in large tables, only the strong correlations become statistically meaningful and affect the privacy preservation against adversarial data mining.

The number of sensitive entries relies on two factors: the number of adversarial rules and the size of directly private set. We observed from our experiments that the runtime of our method is mainly pro-portional to the number of sensitive entries, since the determination of derived private set dominates the cost. Since the number of sen-sitive entries show more modest scalability behavior, this also helps to contain the running time of our method.
To examine the effectiveness of our approach in real applica-tions, here we report the experimental results on real data set Adult from the UCI Machine Learning Repository (http://www.ics.uci.edu/  X  mlearn/). It was extracted from the cen-sus bureau database in year 1994. It has 48 , 842 tuples and dimensions, 6 of which are continuous and 8 are nominal. We re-moved tuples containing missing values. After removal, the data set still has 45 , 222 tuples. We also removed 4 attributes in which most tuples have the same value. We discretized continuous at-tributes ages , fnlwgt and education-num . After discretization, those attributes have cardinality 10 , 150 and 17 , respectively.
Figure 10 shows the number of sensitive entries and size of de-rived private set as the size of directly private set changes from 10 , 000 to 50 , 000 . The support threshold and the confidence thresh-old were set to 2% and 80% , respectively. The blanking factor was fixed to 10 , 000 . The number of sensitive entries is not very sen-sitive to the change of private set, since it is bounded by the total number of non-blanked entries. On the other hand, the number of derived private entries increases with the size of directly private set. If we want to hide more entries, we likely have to blank out more entries as well. However, when we hide more entries in the directly private set, some rules may also be hidden. Consequently, one in-teresting observation from Figure 10 is that the number of derived private entries increases quite slowly with increasing number of di-rectly private entries. This tends to indicate that the (proportionate) loss in entries with increasing level of incompleteness of the data set is likely to be lower. The running time with increasing size of the directly private set roughly follows a similar trend in Figure 10. Limited by space, we omit the curves.

Figure 11 shows the numbers of adversarial rules and sensitive entries, as well as the size of derived private set on the support threshold. The directly private set has 30 , 000 entries and the con-fidence threshold was set to 80% . The blanking factor was set to 10 , 000 . All three measures go up as the support threshold goes down. They follow similar trends. The running time is shown in Figure 12. Again, we observe that the size of the derived private set is within a small factor of the directly private set over all ranges of the support parameter. This tends to indicate only a modest level of information loss. In fact, these results show that the derived private set does not change very much for different values of user-specified support.

In summary, the empirical study on both synthetic and real data sets strongly suggests that privacy preservation against adversar-ial data mining is practical from an information-loss perspective. This is because the results seem to indicate that the derived private set does not increase as fast as the directly private set, and tends to be quite stable over a wide range of user parameters. At the same time, our heuristic approach is also efficient from a computa-tional perspective and requires a few seconds over many practical settings on data sets containing t housands of records. In addition, the scheme scales modestly over a wide range of user-specified pa-rameters. This contributes to the practicality and usability of the approach.
