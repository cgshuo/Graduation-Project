 The objective of multi-dimensional classification is to learn a function that accurately maps each data instance to a vec-tor of class labels. Multi-dimensional classification appears in a wide range of applications including text categoriza-tion, gene functionality classification, semantic image label-ing, etc. Usually, in such problems, the class variables are not independent, but rather exhibit conditional dependence relations among them. Hence, the key to the success of multi-dimensional classification is to effectively model such dependencies and use them to facilitate the learning. In this paper, we propose a new probabilistic approach that repre-sents class conditional dependencies in an effective yet com-putationally efficient way. Our approach uses a special tree-structured Bayesian network model to represent the condi-tional joint distribution of the class variables given the fea-ture variables. We develop and present efficient algorithms for learning the model from data and for performing ex-act probabilistic inferences on the model. Extensive experi-ments on multiple datasets demonstrate that our approach achieves highly competitive results when it is compared to existing state-of-the-art methods.
 I.2.6 [ LEARNING ]: General Multi-dimensional classification, Bayesian network, MAP in-ference
In traditional classification learning, each data instance is associated with a single class variable and the goal is to predict the class label for future instances. However, there are many real-world applications where each data instance is naturally associated with multiple class variables (a vector of labels). To name a few, in text categorization [8, 17], a document can simultaneously cover multiple topics, such as politics and economy ; In bioinformatics [4, 17], each gene may be associated with several functional classes, such as protein synthesis , transcription and metabolism ;Insemantic scene and video classification [2, 10], a scene object can be classified as urban , building and road ; In clinical informatics, a patient may become resistant to multiple drugs for HIV treatment.

Multi-Dimensional Classification (MDC) learning deals with the above mentioned situations and assumes each instance is associated with d discrete-valued class variables Y 1 , ..., Y The goal is to learn a function that assigns to each instance, represented by its feature vector x =( x 1 , ..., x m ), the most probable assignment of the class variables y =( y 1 , ..., y Learning such a function can be very challenging due to the exponential number of possible class labelings that may be assigned to each instance.

The simplest solution to MDC is to completely ignore class correlations by constructing a classifier that predicts each class variable independently from the other classes [2, 4, 12]. This approach often fails because it does not take advantage of the dependency relations between the classes, which is the key to facilitate the learning of MDC. For example, a document that is related to politics is unlikely to be labeled as sports . To overcome this limitation, several methods have been proposed in the literature. These methods account for class dependencies by using different strategies, such as instance-based learning [18, 3], output coding [7, 19, 20], multi-dimensional Bayesian networks [14, 1] and classifier chains [11, 5, 16, 15].

In this paper, we propose a new probabilistic method called Conditional Tree-structured Bayesian Network (CTBN) which models the conditional dependencies between classes in an effective yet computationally efficient way. Briefly, CTBN learns a Bayesian network to represent the distribu-tion of all class variables conditioned on the feature vari-ables P ( Y 1 , ..., Y d | X ). The network structure is defined by making the feature vector X a common parent for all class variables. Besides that, each class variable Y i is allowed to have at most one other class variable Y  X  ( i ) as a parent (in addition to X ). In other words, the conditional dependency relations between class variables follow a directed tree .The conditional probability distribution of each class conditioned on its parents P ( Y i | X ,Y  X  ( i ) ) is modeled using a probabilistic classifier, such as logistic regression. An example of CTBN with four class variables is depicted in Figure 1.
To learn CTBN structure from data, we present an effi-cient algorithm for finding the structure that has the highest conditional log likelihood score. To classify new instances, we present an exact inference algorithm, which computes the most probable assignment of the classes in linear time in the number of classes. Figure 1: An example of a CTBN model that defines
Note that CTBN is related to some recent MDC meth-ods. In particular, it shares some similarities with the multi-dimensional Bayesian network classifier (MBC) approach [14, 1], in that both use Bayesian networks to solve the MDC problem. The main difference, however, is that MBC at-tempts to model the full joint distribution over both features and classes P ( Y , X ) (a generative model), while CTBN di-rectly models the conditional distribution of the classes given the features P ( Y | X ) without wasting effort on modeling the features (a discriminative model). As a result, learning MBC structure from data is much harder than learning CTBN 1 . Furthermore, MBC can only handle discrete features, while CTBN can handle both numeric and discrete features. Note that CTBN is also related to the classifier chains (CC) ap-proach [11, 5, 16, 15] as both use classifiers that incorporate other classes as input. However, the classification process is very different: CC-based methods apply an ad-hoc heuris-tic by simply propagating class predictions along the chain, while CTBN performs proper probabilistic inference to find the optimal output.
Multi-Dimensional Classification (MDC) [1] are classifi-cation problems in which each data instance is associated with d discrete-valued class variables Y 1 , ..., Y d .Wearegiven labeled training data D = { x ( k ) , y ( k ) } n k =1 ,where x ( x 1 , ..., x k -th instance (the input) and y ( k ) =( y ( k ) 1 , ..., y dimensional class vector (the output). The goal is to learn from D a function h that assigns to each instance, repre-sented by a vector of m features, a vector of d class values: where  X ( Y i ) denotes the sample space of class variable Y and  X ( Y 1 )  X  ...  X   X ( Y d ) denotes the space of joint configura-tions of all class variables.

Note that Multi-Label Classification (MLC) can be seen as a special case of MDC. In the MLC setting, each instance is associated with a subset of labels from a set of d labels. By using the notation above, each instance x ( k ) would be associated with a binary class vector y ( k )  X  X  0 , 1 } d y i =1if x otherwise.

Developing a probabilistic approach to MDC requires mod-eling and learning the conditional joint distribution P ( Y where Y =( Y 1 , ..., Y d ) is a random variable for the class vec-
The search space of MBC (increases with the dimensional-ity of the input m ) is much larger than that of CTBN (does not depend on m ). tor and X is a random variable for the feature vector. Under the 0-1 loss function, the Bayes optimal classifier h  X  should assign to instance x the most probable assignment of the class variables, known as the maximum a posterior (MAP) estimation:
In general, solving Equation 1 has exponential complexity because it requires evaluating all possible value assignments to the class variables.

The goal of this paper is to develop a parametric model that allows us to effectively estimate P ( Y | X )fromdataand to perform MAP inference (i.e., classification) in a compu-tationally efficient way.

Notation: From hereafter, we will sometimes abbreviate the expressions by omitting variable names, for example we write P ( Y 1 = y 1 , ..., Y d = y d | X = x )simplyas P ( y 1 , ..., y
The simplest solution to MDC is to learn an independent classifier for each class variable [2, 4, 12], which is known as the binary relevance (BR) approach. More specifically, BR learns a separate classifier to predict each class Y i { 1 , ..., d } and determines the output of a new instance x by simply aggregating the predictions of all classifiers.
Probabilistically, BR relies on the simplifying assumption that all class variables are conditionally independent of each other given x :
Under this assumption, the optimal prediction of BR (solv-ing Equation 1) is simply the class vector that has the high-est conditional marginal probability for each element:
However, this simple approach does not always produce correct results, as we show in the following example.
Example 1. Assume the conditional joint distribution of class variables Y 1 and Y 2 for a specific instance x is shown in Table 1. The optimal classification for x (according to Equation 1) is h  X  ( x )=( Y 1 =1 ,Y 2 =0) . However, the result of BR (according to Equation 2) is h BR ( x )=( Y 1 =0 ,Y 2 =0) . Table 1: The joint distribution of class variables (MAP) prediction is h  X  ( x )=( Y 1 =1 ,Y 2 =0) . In this section, we propose the Conditional Tree-structured Bayesian Network (CTBN) model to allow more elaborate conditional dependency relations between the class variables. In the CTBN, the feature vector X is defined to be a com-mon parent for all class variables (similar to BR). In ad-dition to X , each class variable can have at most another class variable as a parent (without creating a cycle). That is, the conditional dependency relations between class variables follow a directed tree . We chose to restrict the dependency structure to a tree for the following reasons: 1. The optimal structure can be learned using a simple 2. The prediction can be done efficiently using exact in-
Representation: Let T be a CTBN model and let  X  ( i, T ) be the parent class of class Y i in T (by convention,  X  ( i, T )=  X  if Y i does not have a parent class). The joint distribution of class vector ( y 1 , ..., y d ) conditioned on feature vector x is now expressed as follows:
In Figure 1, we showed an example CTBN with four class of class assignment ( y 1 , y 2 , y 3 , y 4 )given x according to this network is defined as follows: P ( y 1 ,y 2 ,y 3 ,y 4 | x )= P ( y 3 | x )  X  P ( y 2 | x ,y 3 )
Parametrization: The parametrization of the CTBN model corresponds to specifying the conditional probabil-ity distribution (CPD) of each class variable Y i conditioned on its parents: P ( Y i | X ,Y  X  ( i,T ) ). The standard parametriza-tion of Bayesian networks uses conditional probability tables (CPT) to define the distribution of each variable conditioned on every possible configuration of its parents. However, the CPT style parametrization is not feasible for the CTBN model. The reason is that the feature vector X ,whichisa common parent for all variables, can be a high-dimensional vector of continuous values, discrete values or a mixture of both (we cannot enumerate all possible configurations of X ).
To overcome this difficulty, we represent the CPDs using probabilistic prediction functions. More specifically, for each class Y i : i  X  X  1 , ..., d } , we approximate its CPD by learning a different probabilistic classifier f iv ( X ) for each possible value v of the parent class:
Note that we can use several standard probabilistic clas-sifiers in the CTBN model, such as logistic regression, na  X   X ve Bayes or the maximum entropy model. In our experiments, we use logistic regression with L 2 regularization.
In the previous section, we described the CTBN model and how to learn its parameters when the structure is known. In this section, we describe how to automatically learn the structure from data.

Our objective is to find the tree structure that best ap-proximates the conditional joint distribution P ( Y | X ), which corresponds to the tree that maximizes the conditional log likelihood (CLL) of the data. To do this, we partition the data into two parts: training data D t and hold-out data D Given a CTBN T ,weuse D t to train its parameters and we use D h to compute its score, which corresponds to the CLL of D h given T (adopting the standard iid assumption): Score ( T )=
In the following, we provide an algorithm to efficiently obtain the optimal CTBN T  X  (the model that has the max-imum score) without having to explicitly evaluate all of the exponentially many possible tree structures.

We start by defining a weighted directed graph G =( V, E ) as follows:
By using this definition of edge weights and switching the order of the summations in Equation 4, we can rewrite the score of T simply as the sum of its edge weights (by conven-tion, a node without a parent has a self loop):
Now we have transformed the problem of finding the op-timal CTBN into the problem of finding the directed tree in G that has the maximum sum of edge weights. The solution can be obtained by solving the maximum branching (ar-borescence) problem [13], which finds the maximum weight directed tree in a weighted directed graph.

Complexity: Computing the edge weights for the com-plete graph G requires estimating  X  P ( Y i | X ,Y j ) for all d of classes. Finding the maximum branching in G can be done in O ( d 2 ) using Tarjan X  X  implementation [13]. Therefore, the overall complexity is O ( d 2 ) times the complexity of learning the probabilistic classifiers (e.g., logistic regression).
In order to make a prediction for a new instance x ,we should find the MAP assignment of class variables according to the CTBN model (solve Equation 1). This problem is NP-hard for general structure Bayesian networks. However, sincewehaverestrictedourstructuretoatree,wecansolve the problem efficiently using exact inference.

In particular, we perform inference using a variant of the max-sum algorithm [9] that we design for the CTBN model. This algorithm first computes the local CPTs for each node Y by applying the corresponding classifier for each possible value of the parent class (see Equation 3). After that, it performs two phases to obtain the optimal prediction. In the first phase, the algorithm sends messages upward (from the leaves to the root) where each node Y i applies the following steps: i) it computes the sum of the logarithm of its local CPT and all messages sent from its children, ii) maximizes the result over its value and iii) sends it to the parent node. In the second phase, the algorithm propagates the optimal assignments downward.

Complexity: The inference algorithm described above runs in O ( d ), where d is the number of class variables.
Example 2. Consider the example in Figure 2, where we show the conditional probability tables of a CTBN model for aspecificinstance x (obtained by applying the classifiers on x ). The optimal prediction for x is ( Y 3 =0 ,Y 2 =1 ,Y 1 = 0 ,Y 4 =0) (has a probability of 0.2016), which can be obtain by our exact inference algorithm. On the other hand, if we apply a CC-based method [11, 16, 15] using the topological order of classes in the tree, we get the suboptimal prediction ( Y 3 =1 ,Y 2 =0 ,Y 1 =1 ,Y 4 =0) (has a probability of 0.1512). The reason CC fails is that it starts incorrectly by predicting Y 3 =1 and propagates this error down the tree.
 Figure 2: An example showing the CPTs of a CTBN
We use ten publicly available datasets that are obtained from different domains such as music recognition (emotions), biology (yeast), semantic image labeling (scene) and text classification (enron, TMC 2007 and RCV1 datasets). The RCV1 datasets are obtained from manually categorized news articles made available by Reuters. The characteristics of the datasets are summarized in Table 2. For each dataset, we show the number of instances, number of feature vari-ables (input dimensionality), number of class variables (out-put dimensionality). In addition, we show two statistics: 1) label cardinality (LC), which is the average number of class variables per instance that have value one and 2) distinct label set (DLS), which is the number of all distinct configu-rations of classes that appear in the data.
We compare the performance of CTBN to several recently proposed methods:
Note that BR, CHF, CC, IBLR and CTBN are all con-sidered meta-learners because they can apply different clas-sifiers (e.g., different probabilistic classifiers can be used to estimate the CPDs in CTBN). So to eliminate additional ef-fects that may bias the results, we use L 2 -penalized logistic regression for all of these methods and choose the regular-ization parameters by cross validation.
We consider three different measures to evaluate the suc-cess of MDC:
Note that EMA is appropriate in the MDC settings be-cause it evaluates the success of the method in finding the mode of the conditional joint distribution P ( Y | X )(seeSec-tion 2). However, EMA may become overly stringent when the output dimensionality is large. CLL-loss is very use-ful for probabilistic methods because it evaluates how much probability mass the method assigns to the true class vec-tor. For example, if two methods misclassify an instance (according to EMA), CLL-loss still favors the one that as-signs higher probability to the correct output. Finally, note that micro F1 is not very suitable for MDC because it does not consider the correlations between classes (see [5, 16]). However, we report it in our results as it has been used in several other papers such as [20, 21].
All experimental results (Tables 3 to 5) are obtained us-ing ten-fold cross validation . To evaluate the statistical sig-nificance of performance difference, we apply paired t-tests at 0.05 significance level. We use markers  X  / to indicate wether CTBN is significantly better/worse than the com-pared method.

Table 3 shows the EMA of the methods. We show the re-sults of MMOC for only three datasets (emotions, yeast and scene) because it did not finish on the remaining data 2 .We can see that CTBN outperforms the other methods for most datasets. For example, CTBN is significantly better than
MMOC did not finish one round of the learning within a 24 hours time limit. CC on seven datasets, significantly better than MLKNN on nine datasets and significantly better than IBLR on six datasets (see the last row of Table 3). The only exception is the MMOC method, which outperforms CTBN on Yeast and Scene datasets. Although very accurate, MMOC is com-putationally very expensive (see Table 6) and does not scale up to large data.

Table 4 shows the CCL-loss of the methods. Note that we could not compute CLL-loss for MMOC because it is not a probabilistic method. We can see that CTBN clearly outper-forms all other methods in terms of CLL-loss. The reason is that CTBN is learned to optimize the conditional log likeli-hood. Furthermore, it applies proper probabilistic inference for prediction, which produces very accurate probabilistic estimates. On the other hand, CC performs very poorly (it provides a bad estimate of the conditional distribution as noted by [5]) because of its ad-hoc classification heuristic. Table 5 shows that CTBN also produces competitive results intermsofthemicroF1score.

Lastly, Table 6 shows the learning times of the different methods. BR, CC and CHF are all very fast because they do not involve any structure learning. CTBN is also efficient and it can scale up to large data. We can see that on all of the RCV1 top10 datasets, CTBN is more than four times faster than MLKNN and fifteen times faster than IBLR. Fi-nally, although very accurate, MMOC has a very high com-putational cost even on the smallest datasets (on the first three datasets, CTBN is around two orders of magnitude faster than MMOC).
In this paper, we proposed a novel probabilistic approach to multi-dimensional classification. Our approach encodes the conditional dependence relations between classes using a special tree-structured Bayesian network, whose conditional distributions are defined using probabilistic classifiers. We presented an efficient algorithm to learn the tree structure that maximizes the conditional log likelihood. Furthermore, we presented an efficient exact inference algorithm that has a linear complexity in the number of class variables. Our ex-perimental evaluation on a broad range of datasets showed that our approach outperforms several state-of-the-art meth-ods and produces much more reliable probabilistic estimates. Moreover, it is efficient and can scale up to large data.
