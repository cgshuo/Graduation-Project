 It has long been recognized that capturing term relationships is an import ant aspect of information retrieval. Even with large amounts of data, we usually only have significant evid ence for a fraction of all potential term pairs. It is therefore important to consider whether multiple sources of evidence may be combined to predict term relations more accurately. This is particularly important when trying to predict the probability of relevance of a set of terms given a query, which may involve both lexical and semantic relations between the terms.
 We describe a Markov chai n framework th at combines multiple sources of knowle dge on term associations. The stationary distribution of the model is used to obtain probability estimates that a potential expansion term reflects aspects of the original query. We use this model for query expansion and evalua te the effectiveness of the model by examining the accuracy and robustness of the expansion methods, and investigate the relative effectiveness of various sources of term evidence. Statistically significant di fferences in accuracy were observed depending on the weighting of evidence in the random walk. For example, using co-occurrence data later in the walk was generally better than using it early, suggesting further improvements in effectiveness may be possible by learning walk behaviors.
 Associative models consider relationships between terms in addition to the terms themselves. They have been extensively considered and studied for information retrieval, e.g. by Bush [3], Stiles [29], van Rijsbergen [31] and Salton &amp; Buckley [25] among many others. There are many lexical and semantic relations that may be considered for associatin g a pair of terms. For example: stemming, based on common morphology; synonymy, where aspects of meaning are shared; co-occurrence, in which both word s tend to appear together; and general association, where a person is likely to give one word as a free-associat ion response to the other. Each relation may be thought of as an inference step, in which a source word v has some property R ( v ), and a new word w can be inferred to have the property value R ( w ) with probability P R ( w | v ), based on their shared relation. For example, if v is the word  X  X atrix X  and the property R (.) is  X  X elevancy to a query X , then one possible way to calculate P R ( w | v ) is based on co-occurrence, so that, for exampl e, the term  X  X ow X  also has some measure of relevance. Note that this is not symmetric:  X  X ow X  having more senses and being more common, it is less likely to imply relevance of  X  X atrix X , unless another term is also present for co ntext, such as  X  X olumn X . While lexical and semantic relations may be useful individually, it is important to consider how they may be used in combination. One reason for this is a common problem in language processing called sparsity: for co-occurrence relations for example, even with a huge corpus, we only have reliab le co-occurrence data for a fraction of all potential term pairs. External semantic resources such as WordNet or stemming dictionaries supply a broad set of terms but are limited in the depth and currency of their voca bulary. By combining multiple relations into chains of inference, we can help bridge the gaps that exist in the data.
 A second reason is that the various relations between words represent potentially complimentary sources of evidence that may help to distinguish and disambiguate terms. For example, if  X  X  ank X  and  X  X erger X  are known to be relevant to a query, then the following inference chains would provide evidence that  X  X egotiations X  may also be relevant: where C, S, and M represen t co-occurrence, synonymy, and morphology relations resp ectively. Note that chains can emphasize different type s of evidence at different walk stages. In the above example, co-occurring terms are found first, followed by their synonyms or stems. In this paper we propose and evaluate a Markov chain-based framework for modeling term relations that can perform such combination of behavior and apply this model to query expansion. Given a small set of initial query terms, we construct a term network and use a random walk to estimate the likelihood of relevance for potential expansion terms. The features used by the random walk can come from a variety of sources, such as term co-occurrence in an external corpus, co-occurrence in the top retr ieved documents, synonym dictionaries, general word a ssociation scores, and so on. Unlike many previous related models used for information retrieval, we define a much richer set of potential walk behaviors that support a variety of link types, where different comb inations of evidence can be used at different stages of the walk. For example, co-occurrence may initially be gi ven higher weight early in the walk, with synonyms weighted more highly in later steps. We also do not use a pre-defined network for all queries, but customize each network for each query. We apply our model to the problem of query expansion in the language modeling approach to information retrieval. By estimating the probability that the various aspects of the query can be inferred from a potential expansion term, we essentially perform a form of  X  X emantic smoothing X  of the query language model.
 The main hypothesis of this paper is that combining query-specific term dependen cies from multiple sources can lead to more accurate and/or robust expansion algorithms. The general motivation for using a Markov chain on a network of terms is that we want to infer a particular property (the label ) of a target word given a set (usually small) of labeled source words.
 In the case of query expansion, the target words are potential expansion terms, the source words are query terms, and the labels are prob abilities of relevance. We then define a random pro cess to propagate the label information through the graph. The stationary distribution of this process gives us a probability distribution over expansion terms. Figure 1 shows a portion of a term network for the query  X  X arkinson X  X  disease X . Solid connections denote explicit term associations, while the dash ed line shows an implied connection inferred between  X  X rain X  and  X  X opamine X  based on a short chain through the shared node  X  X arkinson X  X  disease X . We extend earlier work by Lafferty and Zhai [1 6] on using Markov chains for query expansion, by using a more flexible family of random walks similar to that describe d in Toutanova et al. [30], whose terminology we follow here. Let W = { w i } be a vocabulary set of words. The relationship between words w i and w j is modeled as a combination of directional links , represented by link functions  X  1 , ...,  X  m Each link function  X  represents a specific type of lexical or semantic relation between w i and w j , such as synonyms, stems, co-occurrence, and so on. De tails on the specific link functions we used here are given in section 2.1.
 We imagine a generative process where an author U creates a document of length N as follows: Step 0: Choose an initial word w 0 with probability P( w 0 | U ) (If we have already generated N words, stop.) Step i : Given we have chosen w i-1 , then with probability 1- X  output the word corresponding to w i-1 and reset the process to step 0. Othe rwise, with probability  X  sample a new word w i according to the distribution: where Z is the normalization quantity. This conditional probability may be interpreted as a mixture model in which a particular link type  X  m ( . ) is chosen with probability  X  m ( i ) at timestep i. Note that the mixture is tremors allowed to change at each ti mestep. For simplicity, we limit the number of such changes by grouping the timesteps of the walk into three stages : early, middle, and final. The function  X ( i ) defines how timestep i maps to stage s , where s is a value in {0, 1, 2}, and we now refer to  X  m ( s ) instead of  X  m ( i ).
 Suppose we now have a query q consisting of the set of words { q i } . For each link type  X  m ( . ) we define a transition matrix C ( q,m ) based on the query q . The reason q influences the transition matrix is that some link types, such as co-occurrence on top retrieved documents, are query-sp ecific. Each stage s for a query q has an overall transition matrix C ( q,s ) as the mixture of the individual C ( q,m ): Combining the stages over k steps into a single transition matrix, which we denote C k , we have: Then for a query term q i , the probability that a chain reaches q i after k steps, starting at word w is: where denotes the ( w , q i ) entry in the matrix C k The overall probability p ( q i | w ) of generating a query term q i given a word w is therefore: To ensure the Markov chain has a unique stationary distribution and avoid being trapped in short loops, we add a special last-stage walk step that has uniform transition probability to any node in the graph. This is implemented by using the  X  X ackground smoothing X  link type as the final walk stage.
 The walk continuation probability  X  can be viewed as a penalty for long chains of inference. In practice, we use a small number of steps (up to 4) on a sparse representation of the adj acency graph to perform the random walk steps.
 In section 4, we discuss the specifics of how this model is used for query expansion, and in particular how the probabilities p ( q i | w ) are used. We chose to include the foll owing variety of semantic and lexical link types for ou r experiments. Each link type has a corresponding link function  X  m ( w i , w j ):  X  Synonyms (SYN): From Extended Wordnet [18].  X  Stemming (STEM) : Stems of a term v were  X  General word asso ciation (ASSOC): A human  X  Co-occurrence in a large general Web corpus  X  Co-occurrence in the top retrieved documents  X  Background smoothing (SM): With uniform Our goal is to train the link weights from training data, but for this study we hand-coded the weights  X  m described in the eval uation in Section 5. The Markov chain approach for modeling term assocations is related to previous models based on term clustering and spreading activation networks, both of which have a long history that will only be briefly summarized here.
 Stiles [29] described heuristics for using sets of term associations in improved indexing, and later Quillian [23] proposed a semantic network of concepts for binary relations between words. Gotlieb and Kumar [8] devised a semantic clustering of index terms using maximal complete subgraphs in a term network, although their method X  X  effectiveness was never evaluate d for retrieval. Early work in term clustering for query expansion by Sparck Jones [28] focused on constructing a similarity matrix of single index terms before any user queries were submitted. Wong and Raghavan [33] proposed the use of a matrix of term-term associ ations in ranking document vectors against query vector s, focusing on the special case of term correlation based on co-occurrence. Salton and Buckley [25], van Rijsbergen [31], and many others explored organizing of associations into networks for expanding the search vocabul ary. These networks used various node activation heuristics and decay rules that were intuitively plausible but had limited retrieval success. Crestani [5] give s a summary of earlier work on spreading activation networks.
 Stationary distributions have been used previously in information retrieval for  X  X nfluence weighting X  schemes such as PageRank [1] and hub-authority [10][11], and also for query expansion [16]. Lafferty and Zhai considered a bipartite gr aph on query terms and documents [16] and calculated an approximate stationary distribution using a random walk. In their scheme, the random walk was defined in terms of words and documents, not words only. Our local co-occurrence link is calculated in a similar way, but our random walk framework is more general in that we can use multiple sources of le xical and semantic evidence, not just co-occurrence, with the potential to weight these sources differently at diffe rent stages of the walk. With regard to other quer y expansion approaches, the idea that we present below of rewarding expansion terms reflecting multiple aspects of the original query was previously noted by Xu and Croft [34] for Local Context Analysis (LCA), which has shown good empirical performance. Their method us es an empirically derived formula to score potential expansion terms that is similar in effect to our probabilistic term scoring. A number of studies have used external resources for query expansion. For example, Voorhees used Wordnet with limited success [32]. Shah an d Croft [26] used Wordnet synonyms to perform query expansion for high-precision retrieval, selecting terms with high clarity scores. In contrast to many early spreading activation systems, the Markov chain approach to query expansion is relatively simple and offers a well-motivated probabilistic framework that fits well within the language modeling approach to information retrieval. Moreover, its close relation ship with semi-supervised learning [27] means that we may make use of insights from that area to help illuminate the nature of query expansion and learn more robust expansion algorithms. To index and search the coll ection we used Indri [17], a new search engine in the Lemur toolkit [20]. Indri combines a language modeling approach with inference networks and supports an extended set of probabilistic structured query operators based on INQUERY [4]. For our baseline we chose an algorithm supplied with Indri. This hybrid method selects terms using a method described by Ponte [22], but assigns final term weights using Lavrenko X  X  relevance model [14]. Specifically, a log-odds ratio is calculated for each potential expansion term w by calculating the log-odds ratio over all documents D containing w, with the document coming from collection C : Next, the expansion candidate s are sorted by descending o ( w ), and the top k are chosen. Finally, the term weights r ( w ) used in the expanded query are calculated based on Lavrenko X  X  relevance model. A mu factor of 1000 is used for the Dirichlet smoothing of p ( w | D ) in the relevance model: The quantity p ( q | D ) is the probability score assigned to the document in the initial retrieval. This expansion method appears competitive with other systems in practice on the same TREC collections [17].
 The baseline unexpanded quer y for each topic used not only the original title terms, but also likely phrases, as determined with the Link Parser [15]. All terms were then combined with Indri X  s #combine operator. For example a typical baseline title query is: The query was expanded by adding a weighted combination of the expansion terms, with the original and expanded query weighted equally. For example: We start with this hypothes is: a desirable property of good expansion terms is that they somehow reflect one, and preferably more, aspects of the query. Xu &amp; Croft [34] used this idea in their work on LCA.
 Let A be a set of aspects associated with a query q. In our model, an aspect A i in A is represented by one or more sets of words A i ={ t j } taken from the query. Assuming exchangeability of aspects, and of words within aspects, to evaluate a potential expansion term v , we calculate: Taking logarithms: The Markov chain model defined in section 2 now provides us with a method for estimating p ( t j | v ), the probability that the expansion term v will generate an aspect term t j of the query. The value log p ( t j | v ) may be thought of as a semantic di stance. An example of the language models for various query aspects generated from the Markov chain distribution is given in Table 1. Note that a term like  X  X nionist X  may have high scores for many aspects but rank lower in the final expansion selection because its probability in the top documents is slightly lower than that of other good expansion terms. Expansion terms are chosen by discounting the original log-odds with the combined aspect log-probability: This has the effect of rewa rding terms that are closely related to the main aspects of the query, even if they may be less rare the collection than other expansion terms. The terms are sorted by n ( v ) and the top k are chosen. The term weight assigned to term v in the expanded query is just a rescaled version of n ( v ). In addition to having expansion terms that reflect multiple query aspects, we also want documents that reflect all aspects of the que ry, not just a subset. An analysis at the RIA work shop [2] showed that a significant number of retrieval failures could be attributed to incomplete aspe ct coverage by the retrieval model. Kek X l X inen and J X rv elin [9] showed that one of the most effective structured query operators for query expansion was the probab ilistic AND operator, in combination with maximall y expanded query aspects. We therefore modified the expansion formula from the baseline to use #wand instead of #weight in the expanded query portion to combine the aspect-based expansion terms. The final query looks like: We examined the performan ce of our Markov chain-based expansion in three wa ys. First, we compared retrieval statistics to the Indri baseline and previously published results for the same topics and collections. Second, we compared differe nt versions of the random walk that used different weightings of the evidence. Third, we compared the ro bustness of the expansion methods to Indri baseline expansion.
 Our experiments are based on three different TREC datasets: the AP89 collection (topics 1-50), the TREC8 ad-hoc collection (disks 4&amp;5 minus CR, topics 401-450), and the TREC 2001 wt10g (topics 501-550). These were chosen to vary the style and amount of content. All queries here use the  X  X itle X  field of TREC topics only. In order to test the effectiveness of our modeling techniques we did not perform stemming. The main link types used in the random walk can be divided into two broad groups: links using co-occurrence data (CWEB, CTOP) and associative links (ASSOC, SYN, and STEM). To make it easier to compare the relative effect of these two groups, and to simplify our experiments, we kept these groups separate during different st eps: a walk could use either an  X  X ssociative X  step that combin ed all associative types, or a  X  X o-occurrence step X  usin g co-occurrence data only. (See section 2.1 for the definition of link type names.) The runs we chose are listed below. Links in square brackets indicate an equal mixture during a walk step, and the number in parentheses gives the maximum number of walk steps. We set the walk continuation probability  X  = 0.8. The top 5 documents retrieved, and top 50 expansion terms, were used for the expansion since this tended to give superior performance for both the baseline and our method. Significance testing was performed using the Wilcoxon matched-pair signed-ranks test.
 The results for these runs are shown in Table 2. The External row gives compar ative results for high-performing external system s on the same topics. When measured by mean av erage precision (MAP), the best Markov chain results an d Indri X  X  baseline expansion were comparable for th e 3 collections, with no statistically significant di fferences. For precision at 10% recall (P10%), a gain of 7% (significant at the 0.05 level) was obtained on TREC-8 using Markov E.
 Compared to previous top ex ternal results (the External row), the Markov chain result s were slightly better than the Okapi run at TREC 2001 [24], but slightly lower than the results for AP 1-50 reported by Lafferty &amp; Zhai X  X  bipartite Markov ch ain method [16]. Our best TREC-8 run, with MAP of 0.2942, was slightly lower than the 0.3063 score of one of the top TREC runs [13]. We examined results on 4 RI A  X  X ailure topics X  requiring coverage of multiple query aspects (355, 363, 372, 422). Markov expansion helped substantially for topic 372 ( Identify documents that discuss the growth of Native American casino gambling ), for which both  X  X ambling X  and  X  X ative American X  aspects needed to be present. Markov E obtained a MAP of 0.4621, a 30.8% improvement over the Indri baseline of 0.3532: slightly better than the best RIA system score of 0.4603, and far better than the median RIA MAP of 0.1607. The Markov E query was distinguished by its high weighting of several terms closely related to both aspects, such as specific trib es engaged in casino-building ( X  X equots X ). MAP for the other three topics was comparable to the Indri baseline, and more study is needed to understand when the aspect coverage of Markov expansion is most effective. We compared the Markov D and Markov F runs, which have identical walk parameters except that co-occurrence relations are emphas ized late in the walk for Markov F, and early for Markov D. There was an improvement in MAP (in the case of AP89, more than 15%) when co-occurrence was used late in the random walk. This was true across all three collections. The reasons for this require further study, but it suggests that how the evidence is weight ed by time-step does indeed matter. In this case, term s that co-occur with terms semantically close to the query appear to be more valuable than terms semantical ly close to many potential co-occurrence terms. After examining expansion terms for the various runs, we noted that the Wikipedia tends to act as a background  X  X opic X  model for the query by emphasizing more general terms, while the local co-occurrence data acts to provide additional details on top of the topic model that reflect the corpus style and details, including specific names and places. As shown in Table 2, the addition of the Wikipedia evidence (Markov E) was marginally more effective than lo cal evidence (Markov D).
 A representative comparison of expansion terms is given in Table 3. While the lists tend to be fairly consistent in this case, the CTOP terms lean toward news-like terms that mention times, people and places, while the CWEB terms are more generic. We hypothesized that even in the cases where the overall accuracy of the Markov expa nsion algorith m was similar to existing methods, the bias in favor of adding more general but related terms would reduce the likelihood of query drift caused by choosing an off-topic term, which in turn would result in more robust expansion. The trade-off is that a related, more general term may also be less likely to significantly increase precision.
 The data in Table 4 suggest this is actually happening. We examined all queries that satified one of two cases: 1) where expansion hurt for both methods and 2) where it helped for both methods, as measured by relative change in MAP. We compar ed the size of the relative error in both cases. For queries where expansion hurt, the Markov chain expansion had consistently more queries with lower relative error for all three collections (based on the Markov A run). Conversely, the Indri expansion method had consistently more queries with higher relative gains in cases where expansion helped. While the number of potentially active term-nodes in the network is large (our vocabulary size was around 300,000), the Markov tran sition matrices are very sparse: a typical matrix has about 30,000 non-zero entries. Limiting a walk to a maximum of 4 steps typically results in less than 3000 (1%) of potential nodes becoming active for title-length queries. Even so, using the network efficiently requires some planning. For example, the first time a word is seen, its score is cached since the aspect probabilities p ( t j | v ) will not change over the lifetime of th e query. The time to build the network can be reduced by doing off-line indexing to precompute a language model for each article. The idea of using a spreadin g activation network on a network of terms has been re-discovered many times. While the intuition behind these heuristics was sound, there was limited understand ing of the objective effect of these rules, for example, in terms of statistical language models. Other factors in the poor performance of spreading activation models may have been lack of training data, especially larg e, diverse external language resources like the Wikipedi a that only recently have come into existence. Furthermore, the rules governing these past models were fairly rigid and did not generalize well, especially when the same network was applied to any query. Our model is a first step in a principled explorat ion of the properties that a flexible semantic kernel [6] should have to be most effective for query-specific tasks like relevance estimation. We described a Markov ch ain model that allows chaining of multiple infere nce steps with different link types to perform  X  X emantic smoothing X  on language models, and applied this model to query expansion. A query is modeled as a comb ination of aspects, and expansion terms are favored th at are not only more rare relative to the collection, but also semantically close to multiple query aspects. Our framework supports a richer set of potential behavior than past models, such as early, mid-, and late-stage variation in walk behavior, and arbitrary link weights.
 Our initial results show that this model is comparable with the best results from other methods and can give modest improvements in precision, accuracy, and robustness for some test sets . Statistically significant differences in accuracy were observed depending on the weighting of evidence in the random walk. For example, using co-occurrence da ta later in the walk was generally better than using it early. This suggests that further improvements in accu racy are likely with more study of learned walk behaviors.
