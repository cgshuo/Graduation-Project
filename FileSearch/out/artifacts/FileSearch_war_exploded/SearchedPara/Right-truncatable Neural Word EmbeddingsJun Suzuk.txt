 Word embedding vectors obtained from  X  neural word embedding methods  X , such as SkipGram, continuous bag-of-words (CBoW) and the fam-ily of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tag-ging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), ma-chine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answer-ing (Wang and Nyberg, 2015).

The main purpose of this paper is to further en-hance the  X  X sability X  of obtained embedding vectors in actual use. To briefly explain our motivation, we first introduce the following concept: Definition 1 ( D 0 -right-truncated vector 1 ) . Let w 0 and w 00 be vectors, whose dimensions are D 0 and the concatenation of w 0 and w 00 , that is, w = a D 0 -right-truncated vector of w .

This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depends on applications and uses, and is basically determined based on the performance and memory space (or calculation speed) trade-off. Indeed, the actual di-mensions of the previous studies listed above are di-verse; often around 50, and at most 1000. It is worth noting here that each dimension of embedding vec-tors obtained by conventional methods has no inter-pretable meaning. Thus, we basically need to re-train D 0 -dimensional embedding vectors even if we already have a well-trained D -dimensional vector. In addition, we cannot take full advantage of freely available high-quality pre-trained embedding vec-fixed, i.e. , D = 300 .

To reduce the additional computational cost of the retraining, and to improve the  X  X sability X  of embed-ding vectors, we propose a framework for incremen-tally determining embeddings one dimension at a time from 1 to D . As a result, our method always offers the relation that  X  any D 0 -right-truncated em-bedding vector is the solution for D 0 -dimensional embeddings of our method  X . Therefore, in actual use, we only need to construct a relatively higher-dimensional embedding vector  X  X ust once X , i.e. , D = 1000 , and then truncate it to an appropriate dimen-sion for the application. Let U and V be two sets of predefined vocabularies of possible inputs and outputs. Let |U| and |V| be the number of words in U and V , respectively. Then, neural word embedding methods generally assign a D -dimensional vector to each word in U and V . We denote e i as representing the i -th input vector, and o j for the j -th output vector. In the rest of this paper, for convenience the notation  X  i  X  is always used as the index of input vectors, and  X  j  X  as the index of output vectors, where 1  X  i  X |U| and 1  X  j  X |V| .

We introduce E and O that represent lists of all input and output vectors, respectively. Namely, E = ( e 1 ,  X  X  X  , e |U| ) and O = ( o 1 ,  X  X  X  , o |V| ) . X represents training data. Then, embedding vectors are obtained by solving the following form of a min-imization problem defined in each neural word em-bedding method: where  X  represents the objective function, and  X  E and  X  O are lists of solution embedding vectors. Hereafter, we use  X  as an abbreviation of  X ( E , O |X ) . For example, the objective function  X  of  X  X kipGram with negative sampling (SGNS) X  can where x i,j = e i  X  o j , and L ( x ) represents a logistic loss function, namely, L ( x ) = log(1 + exp(  X  x )) . Moreover, c i,j and c 0 i,j represent co-occurrences of the i -th input and j -th output words in training data and negative sampling data, respectively.

Another example, the objective function  X  of the  X  X lobal Vector (GloVe) X  can be written in the fol-lowing form (Pennington et al., 2014): where m i,j and  X  i,j represent certain co-occurrence and weighting factors of the i -th input and the j -th output words, respectively. For example,  X  i,j = in (Pennington et al., 2014), where x max and  X  are tunable hyper-parameters. This section explains our proposed method. The ba-sic idea is very simple and clear: we convert the minimization problem shown in Eq. 1 to a series of minimization problems, each of whose individ-ual problem determines one additional dimension of each embedding vector. We refer to this formulation of embedding problems as  X  ITerative Additional Co-ordinate Optimization (ITACO)  X  formulation. Fig. 1 shows our entire optimization algorithm for this for-mulation. 3.1 Bias terms and optimization variables Suppose d represents a discrete time step, where d  X  { 1 ,...,D } . Let B ( d ) be a matrix representation of bias terms at the d -th time step, and b ( d ) ( i,j ) and d have the following recursive relation: where we define b (0) tion implies that the solutions of former optimiza-tions are used as bias terms in latter optimizations.
Next, we define  X  q d and  X  r d as the vector represen-tations of the concatenation of all the input and out-put parameters at the d -th step, respectively, that is,  X  q Note that e i used in the former part of this paper is a D -dimensional vector while  X  q d and  X  r d defined here are |U| -dimensional and |V| -dimensional vec-tors, respectively. Moreover, there are relations that e i,d is the d -th factor of e i , and, at the same time, the i -th factor of  X  q d .

Fig. 2 illustrates the relation of e i and  X  q d in this paper. We omit to explicitly show the relation of o j and  X  r d , which are used to represent output vectors because of the space reason. However obviously, they also have the same relation as e i and  X  q d . 3.2 Individual optimization problem Then, we define the d -th optimization problem in our ITACO formulation as follows: subject to: where || X || p represents the L p -norm. We generally assume that p = { 1 , 2 ,  X  X  , and often select p = 2 . Note that  X  q d is optimization parameters in the d -th
We assume that the objective function  X   X  takes an identical form as used in one of the conventional methods such as SGNS and GloVe as shown by Eqs. 2 and 3. The difference appears in the vari-ables; our ITACO formulation uses x i,j = e i o j + b i,j rather than x i,j = e i  X  o j as described in Sec. 2. 3.3 Improving stability of embeddings The additional norm constraint in Eq. 5 is introduced to improve stability. The optimization problems of neural word embedding methods including SGNS and GloVe can be categorized as a bi-convex op-timization problem (Gorski et al., 2007); they are convex with respect to the parameters E if the pa-rameters O are assumed to be constants, and vice versa. One well-known drawback of unconstrained bi-convex optimization is that the optimization pa-rameters can possibly diverge to  X  X  (See Exam-ple 4.3 in (Gorski et al., 2007)). This is because the objective function only cares about the inner prod-uct value of two vectors. Therefore, each parameter can easily have a much larger value, i.e. , o 1 = 10 9 , if e 1 is smaller and approaches a zero value i.e. , e scale problem. Thus, our norm constraint in Eq. 5 can eliminate this problem by maintaining the scale of  X  q and  X  r at the same level. 3.4 Optimization algorithm To solve Eq. 5, we employ the idea of the  X  Alternat-ing Convex Optimization (ACO)  X  algorithm (Gorski et al., 2007). ACO and its variants have been widely developed in the context of (non-negative) matrix factorization, i.e. , (Kim et al., 2014), and are empir-ically known to be an efficient method in practice. The main idea of ACO is that it iteratively and al-ternatively updates one parameter set, i.e. ,  X  q , while the other distinct parameter set is fixed, i.e. ,  X  r . In our case, ACO solves the following two optimiza-tion problems iteratively and alternately: There are at least two advantages of using ACO; (1) Eqs. 6 and 7 both become convex optimization prob-lems. Therefore, the global optimum solution can be obtained when  X  e all j , respectively. (2) ACO guarantees to converge
For example, by a simple reformulation of  X  e 0 , we obtain the closed form solution of Eq. 6 with the GloVe objective, that is, Similarly, the closed form solution of Eq. 7 is: Thus, we can solve Eqs. 6 and 7 without performing iterative estimation. Next, we obtain the following equation by a simple reformulation of  X  e the SGNS objective:
X where  X  ( x ) represents a sigmoid function, that is,  X  ( x ) = 1 following form of the equation for Eq. 7:
X These equations are efficiently solvable by a sim-ple binary search procedure since each equation only has a single parameter, that is, e i or o j ,
During the optimization, there is no guarantee that the constraint |V| nately, the following transformations always satisfy this norm constraint: which also maintain  X  e i  X  o j = e i o j , and the objective value. Thus, we can safely apply them at any time during the optimization.

Finally, Fig. 4 shows the optimization procedure when using the ACO framework. As in previously reported neural word embedding papers, our training data was taken from a Wikipedia dump (Aug. 2014). We used hyperwords tool 5 for our data preparation (Levy et al., 2015).
We compared our method, ITACO , with the widely used conventional methods, SGNS and GloVe . We used the word2vec implementa-glove implementation 7 for GloVe. Many tunable hyper-parameters were selected based on the rec-ommended default values of each implementation, or suggestion explained in (Levy et al., 2015). For ITACO, we selected the Glove objective to solve Eqs. 6 and 7 since it requires a lower calculation cost than the SGNS objective.

We prepared three types of linguistic benchmark tasks, namely word similarity estimation ( Similar-ity ), word analogy estimation ( Analogy ), and sen-tence completion ( SentComp ) tasks. We gathered nine datasets for Similarity (Rubenstein and Goode-nough, 1965; Miller and Charles, 1991; Agirre et al., 2009; Agirre et al., 2009; Bruni et al., 2014; Radin-sky et al., 2011; Huang et al., 2012; Luong et al., 2013; Hill et al., 2014), three for Analogy (Mikolov et al., 2013a; Mikolov et al., 2013c) , and one for SentComp (Mikolov et al., 2013a).
 The rows labeled  X (trunc) X  show the performance of D -right-truncated embedding vectors, whose origi-nal vector of dimension is D = 1000 . Thus, they were obtained from a single set of embedding vec-tors with D = 1000 for each corresponding method. Next, the rows labeled  X (retrain) X  show the perfor-mance provided by SGNS or GloVe that were in-dependently constructed with using a standard set-ting and corresponding D . Note that the results of  X  X TACO (retrain) X  are identical to those of  X  X TACO (trunc) X . Moreover,  X  X loVe (trunc) X  and  X  X loVe (re-train) X  in D = 1000 are equivalent, as are  X  X GNS (trunc) X  and  X  X GNS (retrain) X . Thus, these results were omitted from the table.

First, comparing  X (retrain) X  and  X (trunc) X  in SGNS and GloVe, our experimental results first explicitly revealed that SGNS and GloVe with the simple trun-cation approach  X (trunc) X  cannot provide effective lower-dimensional embedding vectors. This obser-vation strongly supports the significance of exis-tence of our proposed method, ITACO.

Second, in most cases ITACO successfully pro-vided almost the same performance level as the best SGNS and GloVe (retrain) results. We emphasize that ITACO constructed embedding vectors  X  X ust once X , while SGNS and GloVe required us to retrain embedding vectors in the corresponding times. In addition, single run of ITACO for D = 1000 took approximately 12,000 seconds in our machine envi-ronment, which was almost equivalent to run 4 itera-tions of SGNS and 8 iterations of GloVe. The results of SGNS and GloVe in Table 1 were obtained by 10 iterations and 20 iterations, respectively, which are one of the standard settings to run SGNS and ciently as in the same level as SGNS and GloVe. This paper proposed a method for generating in-teresting right-truncatable word embedding vectors. Our experiments revealed that the embedding vec-tors obtained with our method, ITACO, in any lower dimensions work as well as those obtained by SGNS and Glove. In addition, ITACO can also be a good alternative of SGNS and GloVe in terms of the exe-cution speed of a single run. Now, we are free from retraining different dimensions of embedding vec-tors by using ITACO. Our method significantly re-duces the total calculation cost and storage, which We thank three anonymous reviewers for their help-ful comments.
