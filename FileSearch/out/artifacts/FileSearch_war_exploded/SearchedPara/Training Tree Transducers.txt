 University of Southern California University of Southern California University of Southern California tree-to-treeandtree-to-stringtransducers. 1. Introduction
Much natural language work over the past decade has employed probabilistic finite-state transducers (FSTs) operating on strings. This has occurred somewhat under the exist, such as Viterbi decoding (Viterbi 1967) and forward X  X ackward training (Baum
Carmel. 1 Moreover, a surprising variety of problems are attackable with FSTs, from part-of-speech tagging to letter-to-sound conversion to name transliteration. models have been proposed not only for machine translation (Wu 1997; Alshawi,
Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and
Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker these tasks and more.
 generalization of FSTs. Rounds was motivated by natural language: it works top-down, pursuing subtrees independently, with each subtree transformed depending only on its own passed-down state. This class of transducer, called R in is often nowadays called T, for  X  X op-down X .
 may have several productions with the same left-hand side, and therefore some free choices to make during transduction.
 we think of strings written down vertically, as degenerate trees, we can convert any
FST into a T transducer by automatically replacing FST transitions with T produc-tions, as follows: If an FST transition from state q to state r reads input symbol A and outputs symbol B, then the corresponding T production is q A(x0) the FST transition output is epsilon, then we have instead q A(x0) T transducer.
 deleting are called LT (for linear ) and NT (for nondeleting ), respectively. literature about these automata; two excellent surveys are G X cseg and Steinby (1984) (Rounds 1970), and neither are LT or B (the  X  X ottom-up X  cousin of T), but the non-copying LB is closed under composition. Many of these composition results are first found in Engelfriet (1975).
 the subject PRO into position between the verb V and the direct object NP. First,
T productions have no lookahead capability X  X he left-hand-side of the S production 392 apply only when it faces the entire structure q S(PRO, VP(V, NP)). However, we can simulate lookahead using states, as in these productions: move the subtrees around. The next problem is how to get the PRO to appear between output only the V, and delete the right child X : sentation of tree transducers.
 many language problems (machine translation, paraphrasing, text compression, etc.), knowledge automatically. Our problem statement is: Given (1) a particular transducer 394 to produce (3) a probability estimate for each rule in R such that we maximize the probability of the output trees given the input trees. As with the forward X  X ackward algorithm, we seek at least a local maximum. Tree transducers with weights have been know of no existing training procedure.
 and future work. 2. Trees T set. (see Table 1) leaves may be labeled by X ( T  X  (  X  ) = T  X  ). Leaves are nodes with no children. to the root is the empty sequence (), and p 1 extended by p concatenation operator: ( label t ( p ), rank t ( p )). For 1  X  i  X  rank t ( p ), the i 396 path p  X  ( i ). The subtree at path p of t is t  X  p , defined by paths labelandrank t  X  p ( q )  X  labelandrank t ( p  X  q ).
 paths are all valid for t  X  F t  X  F  X  paths t .
 at path p is replaced by s . For a frontier F of t ,the parallelsubstitutionoft
F  X  F of substitutions sharing a common prefix.) For example: t [ p splice out each node p  X  F , replacing it by its first subtree.
 t = S(NP, VP(V, NP)) has labelandrank t ((2)) = (VP, 2) and labelandrank
Commas, written only to separate symbols in  X  composed of several typographic  X   X   X  , t  X  T Using this notation, we can give a definition of T  X  ( X ): to leaves l  X  paths t (such that rank t ( l ) = 0) of label is yield t  X  yield t (  X  ). More precisely, 3. Regular Tree Grammars
In this section, we describe the regular tree grammar , a common way of compactly
 X  = { S, NP, VP, PP, PREP, DET, N, V, run, the, of, sons, daughters N= { qnp, qpp, qdet, qn, qprep } S=q
P= { q  X  1 . 0 S(qnp, VP(VB(run))), Sample generated trees: S(NP(DT(the), N(sons)),
VP(V(run))) S(NP(NP(DT(the), N(sons)),
VP(V(run))) in a regular language; when discussing weights, we assume the commutative semiring ( { r  X  R | r  X  0 } , + ,  X  , 0, 1) of nonnegative reals with the usual sum and product. and P  X  N  X  T  X  ( N )  X  R + is a finite set of weightedproductions ( 1 is assumed). Productions whose rhs contains no nonterminals ( rhs terminalproductions , and rules of the form A  X  w B ,for A , B ( T
 X  ( N 398 l nonterminal may be expanded into entirely terminal trees: ( paths  X  P )  X  :( t , h )  X   X  G ( t , h ).
 ing the leftmost nonterminal in its string representation):
G , where L ( G ) = { ( t , w ) | W G ( t ) = w  X  w &gt; 0 the first projection).
 and the yield of any regular tree language is a context-free string language (G X cseg grammar with alphabet  X   X  X  (,) } . automaton is described in Doner (1970) and is actually a closer mechanical analogue to an FSA than is the rewrite-rule-based wRTG. RTGs are closed under intersection (G X cseg and Steinby 1984), and the constructive proof also applies to weighted wRTG and Satta 2002) is a finite wRTG without loops X  X or all valid derivation trees, each nonterminal may only occur once in any path from root to a leaf: (TAG; Joshi and Schabes 1997), which generate string languages strictly between the and the elementary tree substitution operation. 4. Extended-LHS Tree Transducers (xT) touched on, though not defined, in Section 4 of Rounds (1970).

Q  X  Q is the initial (or start, or root) state ,and R  X  Q  X  is a finite set of weighted transformation rules . xTPAT  X  number of fixed paths of their input. A rule ( q ,  X  , rhs , w ) is written q  X  ing that an input subtree matching  X  while in state q is transformed into rhs ,with
Q  X  paths leaves replaced by their (recursive) transformations. The Q alphabet  X  ). pend only on the root: TPAT  X   X { p  X  , r ( t ) } where p  X  , r subtree. Multiple initial states are not needed: we can use a single start state Q instead of each initial state q with starting weight w add the rule Q True ( t )  X  1,  X  t ).
 formed terms and derivation histories T  X   X   X   X  Q  X  ( paths ( q , i ) replaced by the input subtree at relative path i in state q . 400 input and output instead of just output, with initial terms Q place of S : finally shrinks and disappears. In wRTG, the frontier consisted of the nonterminal-
 X   X   X   X  Q , where the state is the parent of the input subtree. In fact, given an M start with Q i ( t ): ( d , h )  X  D ( G )iff ( t , d , h ) outputsofaninputtree t are produced: when the resulting term d is in T ( t , d ) is in the tree relation and that d is an output of t .
 that domain ( M )  X { i | X  o , w :( i , o , w )  X  L ( M ) } is always a recognizable tree language (Rounds 1970).
 of T, sources ( r )arethe variables x i , standing for the i and the right hand sides of rules refer to them by name: ( q refer to the mapped input subtrees by path (and we are not limited to the immediate of it).
 once:  X  p 1 , p 2  X  paths rhs ( Q  X  paths ), p  X  paths  X  X  istic if for any input, at most one rule matches per state: entire subtrees are not used in their rhs .
 such thing as an empty tree), and because TPAT can only inspect the root node while to transform NP(DET, N) (but not, say, NP(ADJ, N)) into the tree N using rules in T.
Although this is a simple xT rule, the closest we can get with T would be q NP(x0, x1)  X  q.N x1, but we cannot check both subtrees without emitting two independent subtrees in the output (which rules out producing just N). Thus, xT is a bit more powerful than T. 5. Parsing an xT Tree Relation Derivation trees for a transducer M = (  X  ,  X  , Q , Q i , R )are T instead of normal output trees. M is (  X  , R , Q , Q i , R ), with ducers, a single input tree.
 in the other direction, list out the rules of the derivation tree in order.
Because the transformations of an input subtree depend only on that subtree and its state, we can build a compact wRTG that produces exactly the weighted derivation trees corresponding to M -transductions ( I ,())  X   X  M ( O , h ) (Algorithm 1). 402 avoiding needless recomputation. Even if we prove that there are no derivations for but we do avoid adding productions we know can X  X  succeed. We have in the worst case to visit all | Q | X | I | X | O | ( q , i , o ) pairs and apply all output wRTG, are both O ( | Q | X | I | X | O | X | R | ), or O ( Gn Algorithm 1 . Deriv (derivation forest for I  X   X  xT O )
Input : xT transducer M = (  X  ,  X  , Q , Q i , R ) and observed tree pair I Output : derivation wRTG G = ( R , N  X  Q  X  paths I  X  paths begin
S  X  ( Q i ,(),()), N  X  X  X  , P  X  X  X  , memo  X  X  X  if PRODUCE I , O ( S ) then else end
PRODUCE I , O (  X  = ( q , i , o )  X  Q  X  paths I  X  paths O if  X  (  X  , r )  X  memo then return r memo  X  memo  X  X  (  X  , true ) } anyrule?  X  false for r = ( q ,  X  , rhs , w )  X  R :  X  ( I  X  i ) = 1  X  Match memo  X  memo  X  X  (  X  , anyrule? ) } return anyrule? end Match t ,  X  ( t , p )  X  X  X  p  X  path ( t ): label ( t , p )  X  labelandrank t ( p  X  p ) input and output trees, and G is the grammar constant accounting for the states and rules (and their size).
 forest may have infinitely many trees in it, and thus the memoization of PRODUCE which is an implementation (for wRTG) of a well-known method of pruning useless 404 Algorithm 2 . RTGPrune (wRTG useless nonterminal/production identification)
Input :wRTG G = (  X  , N , S , P ), with P = ( p 1 , ... , p begin
M  X  X  X  for n  X  N do B [ n ]  X  false , Adj [ n ]  X  X  X  for i  X  1 to m do for n  X  M do REACH (n) /* Now that B [ n ] are decided , compute A [ n ]*/ for n  X  N do A [ n ]  X  false
USE ( S ) end REACH ( n )  X  begin
B [ n ]  X  true for i  X  Adj [ n ] do end USE ( n )  X  begin
A [ n ]  X  true for n s.t.  X  ( n , t , w )  X  R : n  X  yield t ( N ) do end productions from a CFG (Hopcroft and Ullman 1979). 7 We eliminate all the remains where Algorithm 2 gives A [ n ] = false .
 the weighted trees produced by a wRTG, in a generalization of Algorithm 2 that gives us weights that we accumulate per rule over the training examples, for EM training. 6. Inside X  X utside for wRTG ing each production by adapting the well-known inside X  X utside algorithm for weighted context-free (string) grammars (Lari and Young 1990).
 that can be derived from it: wRTG generated by Deriv ( M , I , O ), this is exactly W M convergent sum over an infinite number of trees.
 (in the unweighted case) is just to make all implicated states equivalent. complete derivations in the wRTG of the weight of the whole tree, excluding the but in practice we may use division by  X  G ( n ) to achieve the same result).  X  and  X  . Conversely, if useless nonterminals weren X  X  removed, they will be detected when computing inside X  X utside weights by virtue of their having zero values, so they may be safely pruned without affecting the generated weighted tree language. particular production is  X  G (( n , r , w )  X  P )  X   X  G ( n ) part, even when it wasn X  X  originally the last term).

O ( | 406 the rule weights of state-change productions less than one. 7. EM Training
Expectation-Maximization (EM) training (Dempster, Laird, and Rubin 1977) works on the parameters, 8 by repeatedly: reached. Normalization may be affected by tying or fixing of parameters. The deriva-tions for training examples do not change, but the model weights for them do. Us-ing inside X  X utside weights, we can efficiently compute these weighted sums over all a given input/output tree pair.
 output tree could similarly be used to train weights for wRTG rules. possible states and rules will apply to a given input/output subtree mapping. probability distribution over input/output tree pairs.
 which always match (with pattern True ), would make the distribution inconsistent by transform inputs that could fall in any of them).
 to all the derivations for the input tree in question.

EM may still compute some empirically useful local maximum. For instance, placing each qlhs in its own normalization group might be of interest; although the inside earned by each participating rule by s (Algorithm 3). 8. Strings We have covered tree-to-tree transducers; we now turn to tree-to-string transducers. translation (Aho and Ullman 1971), and are used to specify compilers that (deter-ministically) transform high-level source-language trees into linear target-language natural languages (Yamada and Knight 2001; Eisner 2003). Tree-to-string transduction is appealing when trees are only available on the input side of a training corpus.
Furthermore, tree/string relationships are less constrained than tree/tree, allowing the possibility of simpler models to account for natural language transformations. induced by tree transformations are sometimes called translations in the automata literature.) the i th letter is s [ i ]  X  s i , for all i  X  indices s  X { { i  X  indices of s are spans s = { ( a , b )  X  X  N 2 | 1  X  a  X  b  X  n + 1 i  X  N ,so s  X  [ i ] = s [ i ]. The substitution of t for a span ( a , b ) ( s  X  (1, a ))  X  t  X  ( s  X  ( b , n + 1)). 10 c  X  d  X  ( a , b ) = ( c , d ), and the parallel substitution of s 408 composition of the individual substitutions, because the replacement substrings may a series of individual substitutions, in right to left order X ( a ( a Algorithm 3 . Train (EM training for tree transducers)
Output : New rule weights W  X { w r | r  X  R } . begin for ( i , o , w )  X  T do i  X  0, L  X  X  X  X  ,  X   X  for r = ( q ,  X  , rhs , w )  X  R do w r  X  w while  X   X   X  i &lt; maxit do end 9. Extended Tree-to-String Transducers (xTs) set of states, Q i  X  Q is the initial (or start, or root) state ,and R ( Q  X  paths ))  X  R + is a finite set of weighted transformation rules , written q  X  transformation. yields of the weighted trees generated by its progenitor.
 an xT transducer. The yields of that transducer X  X  output trees for any input are the not have the canonical right-branching monadic spine that we use to encode strings, the same output yield may be derived via many output trees, which may differ in the relations.
 comparable to a Synchronous CFG (SCFG), with the tree being the CFG derivation tree must be introduced which must be excluded from yield-taking, after which the string-to-string translations are identical.
 output letters and state-labeled input trees and their derivation history) 410 a rhs from a rule that matches it. Of course, the variables ( q , i ) apply if the paths in them exist in the input (if i  X  paths mention them.
 follow from the single-step  X  M exactly as they did in Section 4. 10. Parsing an xTs Tree-to-String Relation
Derivation trees for an xTs transducer are defined by an analogous xT transducer, to be applied preorder, with the i th child rewriting the i node.
 tree grammar that generates all the weighted derivation trees explaining an observed input tree/output string pair for an xTs transducer.
 output subtrees. The looser alignment constraint causes additional complexity: There input subtrees and q different rule states).
 trees with the given output yield.
 for each item.
 alignments of output subspans to nonterminals, giving us minimal-sized subproblems tackled by VarsToSpan .
 to a canonical representative if desired. Algorithm 4 . SDeriv (derivation forest for I  X   X  xTs O )
Input : xTs transducer M = (  X  ,  X  , Q , Q i , R ), observed input tree I
Output : derivation wRTG G = ( R  X  X  } , N  X  N , S , P ) generating all weighted begin
S  X  ( Q i ,(),(1, n )), N  X  X  X  , P  X  X  X  , memo  X  X  X  if PRODUCE I , O ( S ) then else end
PRODUCE I , O (  X  = ( q  X  Q , in  X  paths I , out = ( a , b ) if  X  (  X  , r )  X  memo then return r memo  X  memo  X  X  (  X  , true ) } anyrule?  X  false for rule = ( q , pat , rhs , w )  X  R : pat ( I  X  in ) = 1 memo  X  memo  X  X  (  X  , anyrule? ) } return anyrule? end
Feasible O ( rhs , span )  X  X  X  l  X  letters rhs : l  X   X =  X  l 412
Algorithm SDeriv (cont.) -labeled nodes are generated as artifacts of sharing by cons-nonterminals of derivations for the same spans.

VarsToSpan I , O ( wholespan = ( in  X  paths I , out = ( a , b )  X  spans O , nonterms
N  X  X  false } X  /* Adds all the productions that can be used to map from parts of the begin ret  X  false if | nonterms | = 1 then wholespan  X  ( in , CANONICAL O ( out ), nonterms ) if  X  ( wholespan , r )  X  memo then return r for s  X  b to a do memo  X  memo  X  X  ( wholespan , ret ) } return ret end
CANONICAL O (( a , b ))  X  min { ( x , y ) | O  X  ( x , y ) = O of presentation only. We use an FSA of subsequences of the output string (skipping index. The choice of expansion sites against an input subtree proceeds by exhaustive rules is further indexed against the input tree in a kind of leftmost trie. redundant in the presence of such indexing.
 an output span may align to a rhs substring of multiple consecutive variables; as a consequence, we must create some non-rule-labeled nodes, labeled by (with rank 2). binarization to consider exponentially many productions, corresponding to choosing an n -partition of the span length; the binarized nonterminals in our derivation RTG effectively share the common suffixes of the partitions.
 complicate collecting counts for the original, unbinarized transducer rules. worst-case results for the memoized top-down recursive descent parsing of SDeriv , consumed to guide the top-down choice of rules). The worst-case time and space bounds would be the same, but (output) lexical constraints would be used earlier. moving useless productions with Algorithm 2) exactly as before to perform EM train-ing with Train . In doing so, we generalize the standard inside X  X utside training of probabilistic context-free grammar (PCFG) on raw text (Baker 1979). In Section 12, dummy input tree is paired with each training string as its output. 11. Translation Modeling Experiment (2001) and train it using the EM algorithm. 414 in Yamada and Knight (2001) and here. Figures 7 and 8 show the generative model and parameters; the parameter values shown were learned via specialized EM re-estimation becomes a Japanese string in four steps.
 pre-processing because the model cannot perform complex re-orderings such as the one we described in Section 1, S(PRO,VP(V,NP))  X  V, PRO, NP.
 insert X  X nd it depends on the labels of the node and its parent.
 of context.
 416 conditional probabilities across the whole tree/string corpus.
 P(Japanese string | English tree).
 this (sub)tree, X  we have three rules: rule for every Japanese word in the vocabulary: include a rule for every parent/child sequence and every permutation thereof: subsequent Japanese function word insertion: occurring English and Japanese words:
This follows Yamada and Knight (2001) in also allowing English words to disappear (the rhs of the last rule is an empty string).
 exactly one of the model parameters.
 in one respect so far: The insert-function-word decision is independent of context, whereas Yamada and Knight (2001) specifies it is conditioned on the node and parent of insert-function-word rules: states: specified in Yamada and Knight (2001). However, upon closer inspection one can see and Knight only conditions reordering on the child sequence, thus, for example, the in Train a separate parameter is estimated for each rule in the transducer. We thus introduce rule tying to ensure the exact transducer is not misnamed. By designating
NN) and NN(JJ NN) reordering rules described previously are modified as follows: same tied class, and thus receive the same probability, independent of their parent and the modification that introduces parent-dependent q states and tied reordering of Yamada and Knight (2001).
 built both the simple and exact transducers and trained them using the EM algorithm described in Section 7. We next compare the alignments and transition probabilities of Yamada and Knight (2001).
 418 as described in Yamada and Knight. There are on average 6.9 English words per sen-tence and sentences range in size from 2 to 20 words. We built the simple and exact unweighted transducers described above; Table 2 summarizes their initial sizes. The exact model has 24 more states than the simple; this is due to the parent-dependent modification to q. The 480 additional rules are due to insertion rules dependent on parent and child information.
 corpus. Because the derivation tree grammars produced by SDeriv can be large and time-intensive to compute, we calculated them once prior to training, saved them to disk, and then read them at each iteration of the training algorithm.
Yamada and Knight (2001), we chose a normalization partition ( Z in Train )suchthat we obtain the probabilities of all the rules given their complete left hand side, conditional probability 0.0001 or less.
 (2001), we wish to compare the word-to-word alignments discovered by that work to those discovered by ours. We recovered alignments from our trained transducers as applied rules we can also determine which English words translate to which Japanese ments induced in Yamada and Knight and compared them to the alignments learned from our transducers.

The simple transducer is clearly only a rough approximation of the model of Yamada and Knight (2001). The exact model is much closer, but the low percentage of exact sentence matches is a concern. When comparing the parameter table values reported by Yamada and Knight with our rule weights we see that the two systems learned value differences can be seen in Figure 10.
 parameter values learned in our exact transducer representation of Yamada and Knight immediately prior to a NULL English word translation. We incorporate this change to our model by simply modifying our transducer, rather than by changing our program-transducer as follows.
 after an unaligned foreign function word insertion. We then introduce the following additional rules.
 transition rules described previously must also be added, for example: 420 with s instead of t, and tie these rules, for example: accordance with the insertion/NULL translation restriction.
 transducer are virtually identical to those reported in Yamada and Knight (2001). No rule probability in the learned transducer differs from its corresponding parameter value in the original table by more than 0.000066. The 11 sentences with different having the same probability; this was true in Yamada and Knight (2001) as well, and the choice between equal-scoring derivations is arbitrary. Transducer rules that correspond to the parameter tables presented in Figure 8 and a comparison of their learned weights over the three models with the weight learned in Yamada and Knight are in Figure 10. Note that the final perfect model matches the original parameter tables perfectly, indicating we can reproduce complicated models with our transducer formalism.
 clear, in the same way that Knight and Al-Onaizan (1998) and Kumar and Byrne (2003) elucidate other machine translation models in easily grasped FST terms. Second, the out model-specific re-estimation formulae and implementing custom training software, model in interesting ways. For example, we can add rules for multi-level and lexical re-ordering: incorporate flattening rules into the explicit model.

This can include non-constituent phrasal translations: (Och, Tillmann, and Ney 1999; Marcu and Wong 2002) and are vital to accuracy (Koehn,
Och, and Marcu 2003). We can also eliminate many epsilon word-translation rules in favor of more syntactically-controlled ones, for example: decoding (Yamada and Knight 2002).
 as we stick to the tree automata.
 weighted tree automata toolkit described in May and Knight (2006) and available at http://www.isi.edu/licensed-sw/tiburon/ . 422 12. PCFG Modeling Experiment free grammar (PCFG) on string examples. Consider the following grammar:
Also consider the following observed string data: the father saw the window the father saw the mother through the window the mother sees the father of the mother an xTs transducer:
We also transform the observed string data into tree/string pairs: verbs:
After more iterations, values converge to: by the SDeriv procedure. We note that our use of xTs training relies on copying. 13. Related and Future Work
Concrete xLNT transducers are similar to (weighted) Synchronous TSG (STSG). STSG,
L of the STSG rules X  input and output trees are allowed to differ. Regular lookahead Riccardi 2002).
 nal forward X  X ackward algorithm for Hidden Markov Models. Eisner (2002) describes and could accelerate string-to-string training as well.
 424 translation data.
 input-subtree/output-subtree mappings, we can build wRTG for the xT derivation trees each derivation tree implies a unique input tree, except where deletion occurs (the deleted input subtree could have been anything). For copying transducers, backward potheses possible for all their derived output subtrees are allowed. For noncopying xTs transducers with complete tree patterns, backward application is just exhaustive context-free grammar parsing, generating a wRTG production from the left-hand-side instead of single strings. 21 14. Conclusion
We have motivated the use of tree transducers for natural language processing, and presented algorithms for training them. The tree-input/tree-output algorithm runs in and the tree-input/string-output algorithm runs in O( Gnm We have also presented an implementation and experimental results. References 426
