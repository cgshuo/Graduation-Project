 We study the problem of approximating the 3-profile of a large graph. 3-profiles are generalizations of triangle counts that specify the number of times a small graph appears as an induced subgraph of a large graph. Our algorithm uses the novel concept of 3-profile sparsifiers: sparse graphs that can be used to approximate the full 3-profile counts for a given large graph. Further, we study the problem of estimating lo-cal and ego 3-profiles, two graph quantities that characterize the local neighborhood of each vertex of a graph.

Our algorithm is distributed and operates as a vertex program over the GraphLab PowerGraph framework. We introduce the concept of edge pivoting which allows us to collect 2-hop information without maintaining an explicit 2-hop neighborhood list at each vertex. This enables the computation of all the local 3-profiles in parallel with mini-mal communication.

We test our implementation in several experiments scaling up to 640 cores on Amazon EC2. We find that our algorithm can estimate the 3-profile of a graph in approximately the same time as triangle counting. For the harder problem of ego 3-profiles, we introduce an algorithm that can estimate profiles of hundreds of thousands of vertices in parallel, in the timescale of minutes.
Given a small integer k ( e.g. k = 3 or 4), the k -profile of a graph G ( V,E ) is a vector with one coordinate for each distinct k -node graph H i (see Figure 1 for k = 3). Each coordinate counts the number of times that H i appears as an induced subgraph of G . For example, the graph G = K 4 (the complete graph on 4 vertices) has the 3-profile [0 , 0 , 0 , 4] since it contains 4 triangles and no other (induced) sub-graphs. The graph C 5 (the cycle on 5 vertices, i.e. a pen-tagon) has the 3-profile [0 , 5 , 5 , 0]. Note that the sum of the k -profile is always | V | k , the total number of subgraphs.
One can see k -profiles as a generalization of triangle (as well as other motif) counting problems. They are increas-Figure 1: Subgraphs in the 3 -profile of a graph. We call them (empty, edge, wedge, triangle). The 3-profile of a graph counts how many times each of H i appears in G . ingly popular for graph analytics both for practical and theoretical reasons. They form a concise graph description that has found several applications for the web [4, 24], so-cial networks [35], and biological networks [26] and seem to be empirically useful. Theoretically, they connect to the emerging theory of graph homomorphisms, graph limits and graphons [6, 35, 21].

In this paper we introduce a novel distributed algorithm for estimating the k = 3-profiles of massive graphs. In ad-dition to estimating the (global) 3-profile, we address two more general problems. One is calculating the local 3-profile for each vertex v j . This assigns a vector to each vertex that counts how many times v j participates in each subgraph H These local vectors contain a higher resolution description of the graph and are used to obtain the global 3-profile (simply by rescaled addition as we will discuss).

The second related problem is that of calculating the ego 3-profile for each vertex v j . This is the 3-profile of the graph N ( v j ) i.e. the neighbors of v j , also called the ego graph of v . The 3-profile of the ego graph of v j can be seen as a projection of the vertex into a coordinate system [35]. This is a very interesting idea of viewing a big graph as a collection of small dense graphs, in this case the ego graphs of the vertices. Note that calculating the ego 3-profiles for a set of vertices of a graph is different (in fact, significantly harder) than calculating local 3-profiles.

Contributions: Our first contribution is a provable edge sub-sampling scheme: we establish sharp concentration re-sults for estimating the entire 3-profile of a graph. This al-lows us to randomly discard most edges of the graph and still have 3-profile estimates that are provably within a bounded error with high probability. Our analysis is based on mod-eling the transformation from original to sampled graph as a one step Markov chain with transitions expressed as a function of the sampling probability. Our result is that a random sampling of edges forms a 3-profile sparsifier, i.e. a subgraph that preserves the elements of the 3-profile with sufficient probability concentration. Our result is a general-ization of the triangle sparsifiers by Tsourakakis et al. [34]. Our proof relies on a result by Kim and Vu [15] on concentra-tion of multivariate polynomials, similarly to [34]. Unfortu-nately, the Kim and Vu concentration holds only for a class of polynomials called totally positive and some terms in the 3-profile do not satisfy this condition. For that reason, the proof of [34] does not directly extend beyond triangles. Our technical innovation involves showing that it is still possible to decompose our polynomials as combinations of totally positive polynomials using a sequence of variable changes.
Our second innovation deals with efficiently designing a distributed algorithm for estimating 3-profiles on the sub-sampled graph. We rely on the Gather-Apply-Scatter model used in Graphlab PowerGraph [10] but, more generally, our algorithm fits the architecture of most graph engines. We introduce the concept of edge pivoting which allows us to collect 2-hop information without maintaining an explicit 2-hop neighborhood list at each vertex. This enables the computation of all the local 3-profiles in parallel. Each edge requires only information from its endpoints and each vertex only computes quantities using data from incident edges. For the problem of ego 3-profiles, we show how to calculate them by combining edge pivot equations and local clique counts.

We implemented our algorithm in GraphLab and per-formed several experiments scaling up to 640 cores on Ama-zon EC2. We find that our algorithm can estimate the 3-profile of a graph in approximately the same time as triangle counting. Specifically, we compare against the PowerGraph triangle counting routine and find that it takes us only 1%-10% more time to compute the full 3-profile. For the signifi-cantly harder problem of ego 3-profiles, we were able to com-pute (in parallel) the 3-profiles of up to 100 , 000 ego graphs in the timescale of several minutes. We compare our parallel ego 3-profile algorithm to a simple sequential algorithm that operates on each ego graph sequentially and shows tremen-dous scalability benefits, as expected. Our datasets involve social network and web graphs with edges ranging in num-ber from tens of millions to over one billion. We present results on both overall runtimes and network communica-tion on multicore and distributed systems.
In this section, we describe several related topics and dis-cuss differences in relation to our work.
 Graph Sub-Sampling: Random edge sub-sampling is a natural way to quickly obtain estimates for graph parame-ters. For the case of triangle counting such graphs are called a triangle sparsifiers [34]. Related ideas were explored in the Doulion algorithm [32, 33, 34] with increasingly strong con-centration bounds. The recent work by Ahmed et al. [1] develops subgraph estimators for clustering coefficient, tri-angle count, and wedge count in a streaming sub-sampled graph. Other recent work [29, 5, 14] uses random sampling to estimate parts of the 3 and 4-profile. These methods do not account for a distributed computation model and require more complex sampling rules. As discussed, our theoretical results build on [34] to define the first 3-profile sparsifiers, sparse graphs that are a fortiori triangle sparsifiers. Triangle Counting in Graph Engines: Graph engines ( e.g. Pregel, GraphLab, Galois, GraphX, see [27] for a com-parison) are frameworks for expressing distributed computa-tion on graphs in the language of vertex programs. Triangle counting algorithms [28, 4] form one of the standard graph analytics tasks for such frameworks [10, 27]. In [7], the authors list triangles efficiently, by partitioning the graph into components and processing each component in parallel. Typically, it is much harder to perform graph analytics over the MapReduce framework but some recent work [25, 31] has used clever partitioning and provided theoretical guarantees for triangle counting.
 Matrix Formulations: Fast matrix multiplication has been used for certain types of subgraph counting. Alon et al. pro-posed a cycle counting algorithm which uses the trace of a matrix power on high degree vertices [2]. Some of our edge pivot equations have appeared in [16, 17, 36], all in a cen-tralized setting. Related approximation schemes [32] and randomized algorithms [36] depend on centralized architec-tures and computing matrix powers of very large matrices. Frequent Subgraph Discovery: The general problem of finding frequent subgraphs, also known as motifs or sub-graph isomorphisms, is to find the number of occurrences of a small query graph within a larger graph. Typically fre-quent subgraph discovery algorithms offer pruning rules to eliminate false positives early in the search [37, 19, 11]. This is most applicable when subgraphs have labelled vertices or directed edges. For these problems, the number of unique isomorphisms grows much larger than in our application. In [35], subgraphs were queried on the ego graphs of users. While enumerating all 3-sets and sampling 4-sets neighbors can be done in parallel, forming the ego subgraphs requires checking for edges between neighbors. This suggests that a graph engine implementation would be highly preferable over an Apache Hive system. Our algorithms simultaneously compute the ego subgraphs and their profiles, reducing the amount of communication between nodes. Our algorithm is suitable for both NUMA multicore and distributed archi-tectures, but our implementation focus in this paper is on GraphLab.
 Graphlets: First described in [26], graphlets generalize the concept of vertex degree to include the connected sub-graphs a particular vertex participates in with its neighbors. Unique graphlets are defined at a vertex based on its degree in the subgraph. Graphlet frequency distributions (GFDs) have proven extremely useful in the field of bioinformatics. Specifically, GFD analysis of protein interaction networks helps to design improved generative models [12], accurate similarity measures [26], and better features for classification [30]. Systems that use our edge pivot equations (in a dif-ferent form) appear in prior literature for calculating GFDs [13, 22] but not for enabling distributed computation.
In this section, we are interested in estimating the number of 3 node subgraphs of type H 0 , H 1 , H 2 and H 3 , as depicted in Figure 1, in a given graph G . Let the estimated counts n , i  X  X  0 , 1 , 2 , 3 } . This set of counts is called a 3-profile of the graph, denoted with the following vector: Because the vector is a scaled probability distribution, there are only 3 degrees of freedom. Therefore, we calculate
In the case of a large graph, computational difficulty in es-timating the 3-profile depends on the total number of edges in the large graph. We would like to estimate each of the 3-profile counts within a multiplicative factor. So we first sub-sample the set of edges in the graph with probability p . We compute all 3-profile counts of the sub-sampled graph exactly. Let { Y i } 3 i =0 denote the exact 3-profile counts of the random sub-sampled graph. We relate the sub-sampled 3-profile counts to the original ones through a one step Markov chain involving transition probabilities. The sub-sampling process is the random step in the chain. Any specific sub-graph is preserved with some probability and otherwise tran-sitions to one of the other subgraphs. For example, a 3-clique is preserved with probability p 3 . Figure 2 illustrates the other transition probabilities.

In expectation, this yields the following linear system:  X   X   X   X  E [ Y 0 ] E [ Y 1 ] E [ Y 2 ]
E [ Y 3 ] from which we obtain unbiased estimators for each entry in X ( G ) = [ X 0 , X 1 , X 2 , X 3 ]: Lemma 1. X ( G ) is an unbiased estimator of n ( G ) .
Proof. By substituting (3) X (6) into (2), clearly E [ X i ] = n for i = 0 , 1 , 2 , 3.

We now turn to prove concentration bounds for the above estimators. We introduce some notation for this purpose. Let X be a real polynomial function of m real random vari-ables { t i } m i =1 . Let  X  = (  X  1 , X  2 ,..., X  m )  X 
E (  X   X  X ) = E ( Further, we call a polynomial totally positive if the coeffi-cients of all the monomials involved are non-negative. We state the main technical tool we use to obtain our concen-tration results.

Theorem 1 (Kim-Vu Concentration [15]). Let X be a random totally positive Boolean polynomial in m Boolean random variables with degree at most k . If E [ X ]  X  E  X  1 then for any  X  &gt; 1 , where a k = 8 k k ! 1 / 2 .
 The above theorem was used to analyze 3-profiles of Erd  X os-R  X enyi random ensembles ( G n,p ) in [15]. Later, this was used to derive concentration bounds for triangle sparsifiers in [34]. Here, we extend  X  4.3 of [15] to the 3-profile estimation pro-cess, on an arbitrary edge-sampled graph.

Theorem 2. (Generalization of triangle sparsifiers to 3 -profile sparsifiers) Let n ( G ) = [ n 0 , n 1 , n 2 , n profile of a graph G ( V,E ) . Let | V | = n and | E | = m . Let tained by sampling each edge in G with probability p . Let  X , X  and  X  be the largest collection of H 1  X  X , wedges and tri-angles that share a common edge. Define X ( G ) according to (3)  X  (6) , &gt; 0 , and  X  &gt; 0 . If p, satisfy: then k X ( G )  X  n ( G ) k  X   X  12 | V | 3 with probability at least
Proof. The following sketch outlines the new polynomial decomposition technique required to apply the Kim-Vu con-centration. Full proof details can be found in [9]. Let m be the total number of edges in the original graph G . If e is an edge in the original graph G , let t random indicator after sampling: t e = 1 if e is sampled and 0 otherwise. Let H 0 , H 1 , H 2 , H 3 denote the set of distinct subgraphs of the kind H 0 ,H 1 ,H 2 and H 3 (anti-clique, edge, wedge and triangle) respectively. Let A, ( e ) ,  X ( e,f ) and  X ( e,f,g ) denote an anti-clique with no edges, a H edge e , a H 2 with two edges e,f and a triangle with edges e,f,g respectively in the original graph G . Our estimators (3)-(6) are a function of Y i  X  X  and each Y i can be written as a polynomial of at most degree 3 in all the variables t e
Y 0 = n 0 + X Y 1 = S 1 + D 1  X  2 D 2 + T 1  X  2 T 2 + 3 Y 3 Y 2 = D 2 + T 2  X  3 Y 3
We observe that in the above even by change of variables y e = (1  X  t e ), Y 1 and Y 2 are not totally positive polyno-mials. This means that Theorem 1 cannot be applied di-rectly to the Y 0 i s or X i  X  X . The strategy we adopt is to split the Y 1 and Y 2 into many polynomials, each of which is to-tally positive, and then apply Theorem 1 on each of them. P = { Y 0 ,Y 3 ,S 1 ,D 1 ,D 2 ,T 1 ,T 2 } form the set of totally pos-itive polynomials (proved below). Substituting the above equations into (3)-(6), we have the following system of equa-tions that connect X i  X  X  and the set of totally positive poly-nomials P . We only show the equation for X 0 for brevity:
Let  X  e ,  X  e , and  X  e be the maximum number of H 1  X  X , H and H 3  X  X  containing an edge e in the original graph G. Let  X , X  and  X  be the maximum of  X  e , X  e , and  X  e over all edges e . We now show concentration results for the totally posi-tive polynomials alone. The detailed proof of the following lemma is in the extended version [9].

Lemma 2. Define variables y e = 1  X  t e . Then Y 0 is totally positive in y e . With respect to the variables y e 3 max {  X , X ,  X  } . Further, Y 3 ,S 1 ,D 1 ,T 1 ,T 2 and D ables t e , we have: 1. E  X  1 [ Y 3 ]  X  max { 1 ,p 2  X  } , E  X  1 [ S 1 ]  X   X  . 2. E  X  1 [ D 1 ]  X   X  , E  X  1 [ T 1 ]  X   X  . 3. E  X  1 [ D 2 ]  X  max { p X , 1 } , E  X  1 [ T 2 ]  X  max { 2 p  X  , 1 } .
Let A  X  P . Then E  X  1 [ A ]  X  E [ A ] and Lemma 2 imply the following conditions: n 0  X  3 max {  X , X ,  X  } , p  X  max { 1
We apply Theorem 1 to all the totally positive polynomi-als, along with condition (12). Let the  X  variables in The-orem 1 be denoted by  X  1 ( for Y 0 ),  X  2 ( for Y 3 ),  X  3  X  ( for D 1 ),  X  5 ( for T 1 ),  X  6 ( for D 2 ), and  X  7 ( for T an &gt; 0. We force the following conditions: a q a q a q a q Let  X  &gt; 0. For the right hand side of the inequalities in Theorem 1 to be O (exp(  X   X  log m )), assuming all the bounds in Lemma 2, it is sufficient to have
These conditions are obtained via intermediate conditions on the  X  i variables and (12). Again, the missing details are found in the extended version of this paper [9]. Therefore, subject to (13), all totally positive polynomials concentrate within a multiplicative factor of (1  X  ) with probability at least 1  X  X  1 m  X  .

Under the above concentration result, let the deviations of X i  X  X  be denoted by  X X i . Now we calculate the deviation of X 0 using (11).  X X 0  X  E [ Y 0 ] + 1  X  p
Similarly for other X i  X  X , we get  X X 1  X  12 ( n 1 + n 2 + n 3 ) ,  X X 2  X  6 ( n 2 + n 3 ) ,  X X
Therefore, sampling every edge independently with prob-ability p satisfying all conditions in (13), all X 0 i s concentrate within an additive gap of (1  X  12 ) | V | 3 with probability at least 1  X  1 m  X  . The constants in this proof can be tightened by a more accurate analysis.

The sampling probablilty p in Theorem 2 depends poly-logarithmically on the number of edges and linearly on the fraction of each subgraph which occurs on a common edge. For example, if all of the wedges in G depend on a sin-gle edge, i.e.  X  = n 2 , then the last equation suggests the presence of that particular edge in the sampled graph will dominate the overall sparsifier quality.
H 0 ( v ) H e 1 ( v ) H d 1 ( v ) H c 2 ( v ) H e 2 ( v ) H Figure 3: Unique 3 -subgraphs from vertex perspec-tive (white vertex corresponds to v ).
In this section, we describe how to obtain two types of 3-profiles for a given graph G in a deterministic manner. These algorithms are distributed and can be applied independently of the edge sampling described in Section 3.

The key to our approach is to identify subgraphs at a vertex based on the degree with which it participates in the subgraph. From the perspective of a given vertex v , there are actually six distinct 3 node subgraphs up to isomorphism as given in Figure 3. Let n 0 ,v ,n e 1 ,v ,n d 1 ,v ,n e denote the corresponding local subgraph counts at v . We will first outline an approach that calculates these counts and then add across different vertex perspectives to calculate It is easy to see that the global counts can be obtained from these local counts by summing across vertices:
We will now give our approach for calculating the local 3-profile counts of G ( V,E ) using only local information com-bined with | V | and | E | .
 Scatter: We assume that every edge ( v,a ) has access to the neighborhood sets of both v and a , i.e.  X ( v ) and  X ( a ). Therefore, intersection sizes are first calculated at every edge, i.e. |  X ( v )  X   X ( a ) | . Each edge computes the follow-ing scalars and stores them: n c 2 ,va = |  X ( a ) | X  X   X ( v )  X   X ( a ) | X  1 . The computational effort at every edge is at most O ( d max where d max is the maximum degree of the graph, for the neighborhood intersection size.
 Gather: In the next round, vertex v  X  X athers X  the above scalars in the following way: Here, relations (a) and (b) are because triangles and wedges from center are double counted. (c) comes from noticing that each triangle and wedge from endpoint excludes an ex-tra edge from forming H d 1 ( v ). In this gather stage, the com-munication complexity is O ( M ) where it is assumed that Figure 4: 4 -subgraphs for Ego 3 -profiles (white ver-tex corresponds to v ).
  X ( v ) is stored over M different machines. The correspond-ing distributed algorithm is described in Algorithm 1.
In this section, we give an approach to compute ego 3-profiles for a set of vertices V  X  V in G . For each vertex v , the algorithm returns a 3-profile corresponding to that vertex X  X  ego N ( v ), a subgraph induced by the neighborhood set  X ( v ), including edges between neighbors and excluding v itself. Formally, our goal is to compute { n ( N ( v )) } Clearly, this can be accomplished in two steps repeated se-rially on all v  X  V : first obtain the ego subgraph N ( v ) and then pass as input to Algorithm 1, summing over the ego vertices  X ( v ) to get a global count. The serial implemen-tation is provided in Algorithm 2. We note that this was essentially done in [35], where ego subgraphs were extracted from a common graph separately from 3-profile computa-tions.

Instead, Algorithm 3 provides a parallel implementation which solves the problem by finding cliques in parallel for all v  X  X  . The main idea behind this approach is to realize that calculating the 3-profile on the induced subgraph N ( v ) is exactly equivalent to computing specific 4-node subgraph frequencies among v and 3 of its neighbors, enumerated as F ( v ) , 0  X  i  X  3 in Figure 4. Now, the aim is to calculate F ( v ) X  X , effectively part of a local 4-profile.
 Scatter: We assume that every edge ( v,a ) has already com-puted the scalars from (15). Additionally, every edge ( v,a ) also computes the list N va =  X ( v )  X   X ( a ) instead of only its size. The computational complexity is still O ( d max ). Gather: First, the vertex  X  X athers X  the following scalars, forming three edge pivot equations in unknown variables F ( v ):
By choosing two subgraphs that the edge ( v,a ) partici-pates in, and then summing over neighbors a , these equa-tions gather implicit connectivity information 2 hops away from v . However, note that there are only three equa-tions in four variables and we must count one of them di-rectly, namely the number of 4-cliques F 3 ( v ). Therefore, at the same gather step, the vertex also creates the list edges in the subgraph induced by  X ( v ). This requires worst case communication proportional to the number of edges in N ( v ), independent of the number of machines M . Scatter: Now, at the next scatter stage, each edge ( v,a ) accesses the pair of lists CN v , CN a . Now, each edge ( v,a ) computes the number of 4-cliques it is a part of, defined as follows: This incurs computation time of |CN v | .
 Gather: In the final gather stage, every vertex v accu-mulates these scalars to get F 3 ( v ) = 1 3 P ing O ( M ) communication time. As in the previous section, the scaling accounts for extra counting. Finally, the vertex solves the equations (17) using F 3 ( v ).
In this section, we describe the implementation and the experimental results of the 3-prof , Ego-par and Ego-ser algorithms. We implement our algorithms on GraphLab v2.2 (PowerGraph) [10]. The performance (running time and network usage) of our 3-prof algorithm is compared with the Undirected Triangles Count Per Vertex (hereinafter referred to as trian ) algorithm shipped with GraphLab. We show that in time and network usage comparable to the built-in trian algorithm, our 3-prof can calculate all the local and global 3-profiles. Then, we compare our paral-lel implementation of the ego 3-profile algorithm, Ego-par with the naive serial implementation, Ego-ser . It appears that our parallel approach is much more efficient and scales much better than the serial algorithm. The sampling ap-proach, introduced for the 3-prof algorithm, yields promis-ing results  X  reduced running time and network usage while still providing excellent accuracy. We support our findings with several experiments over various data sets and systems. Vertex Programs: Our algorithms are implemented using a standard GAS (gather, apply, scatter) model [10]. We im-plement the three functions gather() , apply() , and scat-ter() to be executed by each vertex. Then we signal subsets of vertices to run in a specific order.
 Algorithm 1 3-prof Input: Graph G ( V,E ) with | V | vertices, | E | edges
Gather: For each vertex v union over edges of the  X  X ther X  vertex in the edge,  X  a  X   X ( v ) a =  X ( v ).

Apply: Store the gather as vertex data v.nb , size auto-matically stored.

Scatter: For each edge e va , compute and store scalars in (15).

Gather: For each vertex v , sum edge scalar data of neigh-bors
Apply: For each vertex v , calculate and store the quan-tities described in (16). return [ v: v.n0 v.n1 v.n2 v.n3 ] The Systems: We perform the experiments on three sys-tems. The first system is a single power server, further re-ferred to as Asterix. The server is equipped with 256 GB of RAM and two Intel Xeon E5-2699 v3 CPUs, 18 cores each. Since each core has two hardware threads, up to 72 logical cores are available to the GraphLab engine.
 Algorithm 2 Ego-ser
Input: Graph G ( V,E ) with | V | vertices, | E | edges, set of ego vertices V for v  X  X  do end for return [ v: vego.n0 vego.n1 vego.n2 vego.n3 ] Algorithm 3 Ego-par
Input: Graph G ( V,E ) with | V | vertices, | E | edges, set of ego vertices V
Gather: For each vertex v union over edges of the  X  X ther X  vertex in the edge,  X  e va a =  X ( v ).

Apply: Store the gather as vertex data v.nb , size auto-matically stored.

Scatter: For each edge e va , compute and store as edge data: Gather: For each vertex v , sum edge data of neighbors:
Apply: Obtain CN v and equations in (17) using the scalars and g . CN .
 Scatter: Scatter CN v , CN a to all edges ( v,a ).
 Gather: Sum edge data n 4 ,va of neighbors at v .

Apply: Compute F 3 ( v ). return [ v: vego.n0 vego.n1 vego.n2 vego.n3 ]
The next two systems are EC2 clusters on AWS (Ama-zon Web Services) [3]. One is comprised of 12 m3.2xlarge machines, each having 30 GB RAM and 8 virtual CPUs. Another system is a cluster of 20 c3.8xlarge machines, each having 60 GB RAM and 32 virtual CPUs.
 The Data: In our experiments we used five real graphs. These graphs represent different datasets: social networks (LiveJournal and Twitter), citations (DBLP), knowledge con-tent (Wikipedia), and WWW structure (PLD  X  pay level domains). Graph sizes are summarized in Table 1.
 Experimental results are averaged over 3  X  10 runs. Local 3-profile vs. triangle count: The first result is that our 3-prof is able to compute all the local 3-profiles in almost the same time as the GraphLab X  X  built-in trian com-putes the local triangles (i.e., number of triangles including each vertex). Let us start with the first AWS cluster with less powerful machines (m3.x2large). In Figure 5 (a) we can see that for the LiveJournal graph, for each sampling probability p and for each number of nodes (i.e., machines in the cluster), 3-prof achieves running times comparable to trian . Notice also the benefit in running time achieved by sampling. We can reduce running time almost by half, without significantly sacrificing accuracy (which will be dis-cussed shortly). While the running time is decreased as the number of nodes grows (more computing resources become available), the network usage becomes higher (see Figure 5 (c)) due to the extensive inter-machine communication in GraphLab. We can also see that sampling can significantly reduce network usage. In Figures 5 (b) and (d), we can see similar behavior for the Wikipedia graph: running time and network usage of 3-prof is comparable to trian .

Next, we conduct the experiments on the second AWS cluster with more powerful (c3.8xlarge) machines. For Live-Journal, we note modest improvements in running time for nearly the same network bandwidth observed in Figure 5. On this system we were able to run 3-prof and trian on the much larger PLD graph. In Figures 6 (b) and (d) we com-pare the running time and network usage of both algorithms. For the large PLD graph, the benefit of sampling can be seen clearly; by setting p = 0 . 1, the running time of 3-prof reduced by a factor of 4 and the network usage is reduced by a factor of 2. Figure 7 shows the performance of 3-prof and trian on the LiveJournal and Wikipedia graphs. We can see that the behavior of running times and the network usage of the 3-prof algorithm is consistently comparable to trian across the various graphs, sampling, and system parameters.

Let us now show results of the experiments performed on a single powerful machine (Asterix). Figure 11 (a) shows the running times for 3-prof and trian for Twitter and PLD graphs. We can see that on the largest graph in our dataset (Twitter), the running time of 3-prof is less than 5% larger than that of trian , and for the PLD graph the difference is less than 3% (for p = 1). Twitter takes roughly twice as long to compute as PLD, implying that these algorithms have running time proportional to the graph X  X  number of edges.

Finally, we show that while the sampling approach can significantly reduce the running time and network usage, it has negligible effect on the accuracy of the solution. No-tice that the sampling accuracy refers to the global 3-profile count ( i.e. , the sum of all the local 3-profiles over all vertices in a graph). In Figure 12 we show accuracy of each scalar in the 3-profile. For the accuracy metrics, we use ratio between the exact count (obtained running 3-prof with p = 1) di-vided by the estimated count (i.e., the output of our 3-prof when p &lt; 1). It can be seen that for the three graphs, all the 3-profiles are very close to 1. E.g., for the PLD graph, even when p = 0 . 01, the accuracy is within 0 . 004 from the ideal value of 1. Error bars mark one standard deviation from the mean, and across all graphs the largest standard deviation is 0 . 031. As p decreases, the triangle estimator suffers the greatest loss in both accuracy and consistency.
 Ego 3-profiles: The next set of experiments evaluates the performance of our Ego-par algorithm for counting ego 3-profiles. We show the performance of Ego-par for vari-ous graphs and systems and also compare it to a naive se-rial algorithm Ego-ser . Let us start with the AWS sys-tem with (c3.8xlarge machines). In Figure 8 we see the Figure 5: AWS m3_2xlarge cluster. 3-prof vs. trian algo-running time of Ego-ser and Ego-par on the LiveJournal graph. The task was to find ego 3-profiles of 100, 1K, and 10K randomly selected nodes. Since the running time de-pends on the size and structure of each induced subgraph, Ego-ser and Ego-par operated on the same list of ego ver-tices. While for 100 random vertices Ego-ser performed well (and even achieved the same running time as Ego-par for the PLD graph), its performance drastically degraded for a larger number of vertices. This is due to its iterative nature  X  it finds ego 3-profiles of the vertices one at a time and is not scalable. Note that the open bars mean that this experiment was not finished. The numbers above them are extrapolations, which are reasonable due to the serial design of the Ego-ser .

On the contrary, the Ego-par algorithm scales extremely well and computes ego 3-profiles for 100, 1K, and 10K ver-tices almost in the same time. In Figure 9 (a), we can see that as the number of nodes (i.e., machines) increases, run-ning time of Ego-par decreases since its parallel design al-lows it to use additional computational resources. However, Ego-ser cannot benefit from more resources and its running time even increases when more machines are used. The in-crease in running time of Ego-ser is due to the increase in network usage when using more machines (see Figure 9 (b)). The network usage of Ego-par also increases, but this algo-rithm compensates by leveraging additional computational power. In Figure 10, we can see that Ego-par performs well even when finding ego 3-profiles for all the LiveJournal vertices (4.8M vertices).

Finally in Figure 11 (b) and (c), we can see the comparison of
Ego-par and Ego-ser on the PLD and the DBLP graphs Figure 6: AWS c3_8xlarge cluster. 3-prof vs. trian algo-on the Asterix machine. For both graphs, we see a very good scaling of Ego-par , while the running time of Ego-ser scales linearly with the size of the ego vertices list.
In summary, we have reduced several 3-profile problems to triangle and 4-clique finding in a graph engine framework. Our concentration theorem and experimental results confirm that local 3-profile estimation via sub-sampling is compara-ble in runtime and accuracy to local triangle counting.
This paper offers several directions for future work. First, both the local 3-profile and the ego 3-profile can be used as features to classify vertices in social or bioinformatic net-works. Additionally, we hope to extend our theory and al-gorithmic framework to larger subgraphs, as well as special classes of input graphs. Our edge sampling Markov chain and unbiased estimators should easily extend to k &gt; 3. Equations in (17) are useful to count local or global 4-profiles in a centralized setting, as shown recently in [17, 36]. Tractable distributed algorithms for k &gt; 3 using similar edge pivot equations remain as future work. Our observed dependence on 4-clique count suggests that an improved graph engine-based clique counting subroutine will improve the parallel algorithm X  X  performance.
 We would like to thank the anonymous reviewers for their useful comments. The authors also acknowledge support from NSF CCF-1344364, NSF CCF-1344179, ARO YIP W911NF-14-1-0258, DARPA XDATA, and research gifts by Google and Docomo.
 Figure 7: AWS c3_8xlarge cluster with 20 nodes. 3-prof Figure 8: AWS c3_8xlarge cluster. Ego-par vs. Ego-ser [1] N. K. Ahmed, N. Duffield, J. Neville, and [2] N. Alon, R. Yuster, and U. Zwick. Finding and [3] Amazon web services. http://aws.amazon.com, 2015. [4] L. Becchetti, P. Boldi, C. Castillo, and A. Gionis. [5] M. A. Bhuiyan, M. Rahman, M. Rahman, and M. Al [6] C. Borgs, J. Chayes, and K. Vesztergombi. Counting [7] S. Chu and J. Cheng. Triangle listing in massive [8] T. A. Davis and Y. Hu. The University of Florida [9] E. R. Elenberg, K. Shanmugam, M. Borokhovich, and Figure 10: AWS c3_8xlarge cluster with 20 nodes. [10] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and [11] W. Han and J. Lee. Turbo iso : Towards Ultrafast and [12] F. Hormozdiari, P. Berenbrink, N. Przulj, and S. C. [13] T. Ho X cevar and J. Dem X sar. A Combinatorial [14] M. Jha, C. Seshadhri, and A. Pinar. Path Sampling: [15] J. H. Kim and V. H. Vu. Concentration of [16] T. Kloks, D. Kratsch, and H. M  X  uller. Finding and [17] M. Kowaluk, A. Lingas, and E.-M. Lundell. Counting [18] H. Kwak, C. Lee, H. Park, and S. Moon. What is [19] J. Lee, W.-S. Han, R. Kasperovics, and J.-H. Lee. An [20] J. Leskovec and A. Krevl. SNAP Datasets: Stanford [21] L. Lov  X asz. Large Networks and Graph Limits , [22] D. Marcus and Y. Shavitt. RAGE -A Rapid Graphlet [23] R. Meusel, S. Vigna, O. Lehmberg, and C. Bizer. [24] D. O X  X allaghan, M. Harrigan, J. Carthy, and [25] R. Pagh and C. E. Tsourakakis. Colorful triangle [26] N. Przulj. Biological Network Comparison Using [27] N. Satish, N. Sundaram, M. A. Patwary, J. Seo, [28] T. Schank. Algorithmic Aspects of Triangle-Based [29] C. Seshadhri, A. Pinar, and T. G. Kolda. Triadic [30] N. Shervashidze, K. Mehlhorn, and T. H. Petri. [31] S. Suri and S. Vassilvitskii. Counting Triangles and [32] C. E. Tsourakakis. Fast Counting of Triangles in [33] C. E. Tsourakakis, U. Kang, G. L. Miller, and [34] C. E. Tsourakakis, M. Kolountzakis, and G. L. Miller. [35] J. Ugander, L. Backstrom, M. Park, and J. Kleinberg. [36] V. V. Williams, J. Wang, R. Williams, and H. Yu. [37] X. Yan and J. Han. gSpan: Graph-Based Substructure
