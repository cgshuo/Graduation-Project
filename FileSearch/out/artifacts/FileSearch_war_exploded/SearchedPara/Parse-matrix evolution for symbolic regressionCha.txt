 1. Introduction
Symbolic regression aims at finding a symbolic, mathematical model that can describe and predict a given system based on observed input-response data. It plays an increasingly important role in many engineering applications including structural mechanics ( Yang et al., 2005 ), signal processing ( Yao and Lin, 2009 ), system identification ( Patelli and Ferariu, 2010 ), industrial data analysis ( Vladislavleva et al., 2010 ), etc.

As is well known, regression analysis ( Chatterjee and Hadi, 2006 ) is a statistical technique for the modelling and analysis of numeric input-response data. Conventional linear/nonlinear regression is based on a certain model structure and optimizes the coefficients in the model, which can be described as follows: a n  X  arg min where x  X  i  X  A R d , y i A R are numeric input-response data, and the model function f : R d  X  k / R .

However, if the model structure is unknown or wrong, or the concerned system has changed and the old model becomes untrustful, conventional regression does not work any more. In these cases, data-driven models are very helpful and desirable.
Fortunately, symbolic regression can solve such problems ( Vladislavleva et al., 2009 ). Symbolic regression can optimize the model structure and coefficients simultaneously. The objec-tive of symbolic regression is to find an appropriate model from a space of all possible expressions S defined by a set of given operations (e.g.,  X  ,-, n , /, etc.) and functions (e.g., sin(), cos(),exp(),ln(), etc.), which can be described as follows: f n  X  arg min
Genetic programming (GP) ( Koza, 1992 ) is a classical method for symbolic regression. The chromosome of GP is the parse tree of an actual program. This tree-based representation makes GP difficult (although not impossible) to be implemented in general-purpose programming languages such as C/C  X  X  and Fortran. As a result, GP is originally implemented in LISP ( Goldberg, 1996 ), which is a functional programming language. Most of us are more familiar with and prefer to use the general-purpose programming languages. Consequently, GP X  X  applications are limited.
To overcome this difficulty, O X  X eill and Ryan (2001) proposed a grammatical evolution (GE). GE introduced a binary string for its chromosome presentation. This is the essential difference between GE and the classical GP. Due to the string-based representation, GE can be implemented in any programming language. However, GE is still not so easy to use. First, the implementation of GE is complicated because it needs an addi-tional function parser for the encoding and decoding process.
Next, the incomplete mapping and extra codons problems ( O X  X eill and Ryan, 2001 ; O X  X eill and Brabazon, 2006 ) are common but difficult to handle.

GE is not easy to programme without an external function parser and difficult to control without a careful treatment of incomplete mapping and extra codons. These difficulties are caused by its chromosome representation. In fact, GE uses a binary string to represent an individual. The chromosome is a one-dimensional string, which can be regarded as a highly compressed parse tree. In our view, GE has over compressed the parse tree, and useful information might be lost due to this compression. The decoding process of GE becomes not straight-forward any more. Therefore, the difficulties (including incom-plete mapping and extra codons) arise.

To keep more useful information, we propose a less com-pressed parse tree called parse-matrix in this paper. It uses a two-dimensional matrix with integer entries for its chromosome representation.

The two-dimensional matrix with integer entries can carry more information than the one-dimensional binary string (used in
GE). Consequently, the decoding process (of mapping from the parse-matrix to its corresponding expression) are simplified and becomes straightforward. The evolutionary algorithm that uses the parse-matrix representation is referred to as parse-matrix evolution (PME). PME is very easy to use and free to control. It does not need any special target programming language or additional function parser. Numerical experiments show that
PME can solve the symbolic regression problems effectively. 2. Chromosome of genetic programming
Chromosome presentation is the fundamental and most important element to be addressed in genetic programming.
Besides the above mentioned Koza X  X  tree-based and O X  X eill X  X  string-based presentations, linear-based and graph-based presen-tations are also widely used. Graph-based presentation is a natural extension to the tree-based one (Trees are special types of graphs). Several graph-based GPs including parallel distributed genetic programming (PDGP) ( Poli, 1996 ) and Genetic Network
Programming (GNP) ( Mabu et al., 2007 ) have been proposed. They use directed edges to connect the nodes. This structure enables them to re-use the subtree evaluations for those are repeatedly emerged in an individual. In order to make a better use of computer architectures (computer programs are represented in a linear fashion) and avoid the use of computationally expensive interpreters or compilers, linear genetic programming (LGP) is proposed ( Poli et al., 2008 ; Brameier and Banzhaf, 2007 ). In LGP, a chromosome is represented as a sequence of instructions.
Only tree-based and string-based representations are closely related to our new proposal and will be discussed in detail as follows. 2.1. Tree-based chromosome representation
In canonical genetic programming (GP), an individual is represented as a parse tree, in which every internal node has an operator/function and every leaf node has an operand. For example, the analytical expression sin  X  x 2 1 x 2  X  X  ln  X  x represented as the parse tree shown in Fig. 1 . The crossover between two individuals is applied by exchanging their subtrees.
The mutation of an individual is applied by replacing a randomly chosen subtree by another randomly generated subtree. Because of its tree-based representation, GP is easy to be implemented in functional programming languages ( Goldberg, 1996 ) (e.g.,
LISP), but difficult to be coded in general-purpose programming languages such as C/C  X  X  and Fortran. Obviously, most people know about general-purpose programming languages better and prefer using it if possible. There are a number of C/C  X  X  /JAVA implementations of GP like Lil-GP, GPC  X  X  , and DGPF (See Wiki-pedia item Genetic programming at http://en.wikipedia.org/wiki/ Genetic_programming ). Although it may be not so difficult to use any one of them as a standalone program, none of them is easy to be adapted for your own needs because of its complexity. 2.2. String-based chromosome representation
Grammatical evolution (GE) ( O X  X eill and Ryan, 2001 ) is a new variant of genetic programming, it uses a variable-length binary string to describe an individual. The binary string (genotype) has the following form: I  X  b 1 b 2 b 3 b M , where b k A f 0 ; 1 g : GE decodes its chromosome as follows. First, the binary string is converted to an integer string of the form: I  X  X  i 1 , i 2 , ... , i M = 8 , where each integer entry i k maps to an 8-bit segment in the binary string. Next, the integer string is decoded component-wise to a derivation tree using a Backus-Naur form (BNF) grammar. Then, the derivation tree is decoded to a parse tree. Finally, the parse tree will be parsed to an analytical expression (phenotype) by a function parser (see O X  X eill and Brabazon, 2006 for more details).

The BNF grammar used in GE can be represented by the tuple { N , T , P , S }, where N is the set of nonterminals, T the set of terminals, P a set of production rules that maps the elements of N to T , and S is a start symbol that is a member of N . Now we give a simple example BNF grammar as follows: (a) N  X f expr , op g ; (b) T  X f X  , , n , x , y g ; (c) S  X  / expr S ; (d) the production rules P satisfies:
Suppose we have a chromosome I  X  [88, 211, 8, 35 133, 81, 68], and we want to decode it based on the above BNF grammar, then the decoding process can be listed in Table 1 . We can see that the phenotype of I is x n y . Table 1 shows that the decoding process terminates until the codon  X 81 X . As a result, the last codon  X 68 X  is an extra codon. Obviously, if the chromosome I has more codons besides the listed codons, e.g., I  X  [88, 211, 8, 35, 133, 81, 68, 54,108, 36, y ], all codons after the codon  X 81 X  will be regarded as extra codons and ignored by the function parser. On the contrary, if the chromosome I has less codons, e.g., I  X  [88, 211, 8, 35, 133], we will get x n / var S , which is an illegal analytical expression. In this case, the decoding process is incomplete.

GE uses a string-based chromosome. The string is one-dimen-sional, and it can be regarded as a highly compressed parse tree.
This linearized non-tree representation makes GE easier to implement in general-purpose programming languages than classical GP.

However, GE is still not so easy to use. As can be seen from the above example, GE X  X  mapping process from the genotype (a binary string) to the phenotype (an expression) is rather compli-cated. Consequently, GE needs an additional function parser for the decoding process. We can also see that the incomplete mapping and extra codons are common problems during the decoding process, which make GE harder to control.

In our view, GE has encountered these difficulties because it has over compressed the parse tree, and some useful information is lost due to the over-compression. This motivates us to design a less compressed, two-dimensional, non-tree chromosome repre-sentation, which has a simpler decoding process. The proposed chromosome representation called parse-matrix. It represents an individual as a two-dimensional matrix with integer entries.
Obviously, it contains more information than one-dimensional binary string. Detailed description of parse-matrix will be pre-sented in the next subsection.

Recently, McKay et al. (2010) present a survey of grammar-based genetic programming. They summarized different ways of chromosome representation, and classified them into several groups: tree-based grammar, linearized grammar, semantic grammars, logic grammars, tree adjoining grammar, etc. We confine our discussion to the tree-based and linearized grammar in this paper because these two chromosome representations are closely related to our proposal. 2.3. Parse-matrix chromosome representation
The new proposed chromosome representation, parse-matrix, is also derived from the parse-tree of genetic programming. First, let us take a deep look at the parse tree of GP and its evolutionary process. Usually, a parse tree consists of many subtrees. These subtrees can be classified into different levels. For example, the parse tree in Fig. 1 has four levels of subtrees, which is denoted by , S  X  from lower level subtrees.

For general symbolic regression problems, there are two kinds of operators  X  unary operators U (such as sin(), cos(), exp(), ln(), ln  X  X  , etc : g . For the sake of simplicity, we define T  X  s to unify the two kinds of operators:
T  X  s , s 2  X  X 
Let S  X  0  X  be the set of terminal expressions (level zero subtrees), subtrees produced by the operators in T with terminals from , then S  X  2  X  is the set of all possible subtrees produced by the set of all possible subtrees produced by the operators in T with the highest-level subtrees be S  X  K  X  , the objective of symbolic regression is to find an optimal tree (represents an optimal expression) within all level sets of subtrees S  X  0  X  [ S
In other words, symbolic regression needs to find an optimal function f n , such that f n  X  arg min where x  X  i  X  A R n , y i A R are numeric input-response data.
We can see that there are inclusion relationships between the sets of different level subtrees. First, we have S  X  0  X  S  X  8 s A
S  X  0  X  , ( s A S  X  1  X  , s.t. s  X  1 : 0 n s . Since S  X  2  X  level subtrees S  X  K  X  . Based on this fact, we can define a parse-matrix to represent the set of top level subtrees. Thus, the objective of symbolic regression becomes to find an optimal parse matrix which maps to the optimal function f n . To store the lower level subtrees for reuse, intermediate expressions such as f also introduced.

Based on the above discussion, we introduce a matrix with four columns of the form
A  X  to represent an individual (or  X  X hromosome X ) , where the entries a are bounded integers near zero. The first column a 1 indicates the active operator from set T . The 2nd and 3rd columns a indicate the active operands. The 4th column a 4 decides which intermediate expression ( f 1 or f 2 ) will be updated. 3. Parse-matrix evolution
The evolutionary algorithm that uses parse-matrix (see Section 2.3 ) as its chromosome presentation is referred to as parse-matrix evolution (PME) in this paper.

PME does not use the BNF grammar for its encoding or decoding. Alternatively, it uses a table of mapping rules which is much easier to understand and use. Table 2 gives an example mapping table, where d is the dimension of the target model. We will use this table to do our numerical experiments in the following section (see Section 6 ). Of course you can define your own table of mapping rules according to different problems. 3.1. Encoding of PME
The phenotype of PME (i.e., the target function of symbolic regression) is an analytical function/expression. The encoding from the phenotype to the genotype (a parse-matrix) is straight-forward. For example, the expression sin  X  x 2 1 x 2  X  X  ln  X  x be produced in the steps listed in Table 3 .

According to the mapping rules in Table 2 and the encoding steps in Table 3 , the expression sin  X  x 2 1 x 2  X  X  ln  X  x described by the parse-matrix as follows:
A  X 
We can see that the encoding of PME is a natural and easy process. Note that it is hard to imagine how to encode an expression to a binary string in GE. 3.2. Decoding of PME
The genotype of an individual (a chromosome) in PME is a parse-matrix. Its corresponding phenotype is an expression (a function). A parse-matrix will be decoded according to the mapping rules listed in Table 2 row by row as follows.
Procedure of decoding : for ( i  X  0; i o m ; i  X  X  ){ Suppose we have an individual (a parse-matrix, the genotype) A  X  and we want to get its phenotype. The decoding process can be listed in Table 4 .

From Table 4 , we can see that the decoding process is also natural and easy.

Although the mapping between the set of parse matrices and the set of symbolic expressions is not one-to-one, the interpretation between them is very straightforward and easy. We can see from the above examples that the coding plan (encoding and decoding) of PME is almost invertible. This is a favourable feature. As we know that it is almost impossible to reduce a symbolic expression to its original binary string in GE. Furthermore, GE X  X  decoding from a binary string to its phenotype (a symbolic expression) is also not so easy. For example, the incomplete mapping might result in an illegal analytical expression; the extra codons often confuse users, and you could not predict where the first extra codon occurs unless you decode it bit by bit and find that the decoding process comes to an end. This makes GE hard to control.
 Remark A. The height of the parse-matrix m reflects the level of top subtree K . It is easy to see that K r m . As a result, m can be used to control the depth of searching process.
 Remark B. Provided f 1 , f 2 , f are properly initialized (e.g., set f  X  f 2  X  f  X  1), every row of the parse-matrix maps to a complete operation and updating process. Thus, there is no incomplete mapping problem in PME.
 Remark C. Only unary and binary operators are considered here. As a result, the column number of the parse-matrix equals to 4.
However, parse-matrix could easily be extended so that it can cope with multiple operators (e.g., the scalar triple product a  X  b c  X  X  . In general, suppose the operator T has p operands, we can just increase the column number to p  X  2 and define
T  X  s , ... , s p  X  X  then the operators with different number of operands can be unified. 3.3. Biological background
The representation of PME is inspired from the structure of genetic system ( Hartl and Jones, 2008 ). In fact, the meaning of a parse-matrix in PME may be compared to a chromosome in biology (see Fig. 3 ): 1. An element of the parse-matrix itself has no explicit meaning.
It functions like a base in a codon. 2. Every row of the parse-matrix maps to a complete operation and updating process. It functions like a four-base codon. 3. The whole parse-matrix carries all information of an expres-sion. It plays the role of a chromosome. 3.4. Evolutionary operators
PME uses a parse-matrix for its chromosome representation, and a table of mapping rules for its encoding and decoding. Note that each row of the matrix represents a complete operation and updating process. Therefore, it is very easy to design evolutionary operators for PME.
 Crossover : Traditional crossover is easy to be adapted for PME.
For example, we can define a one-point crossover as follows random, and exchange the last m r c rows of two parents of parent matrices. We can also define a two-point crossover (see
Fig. 4 (b)) that the two parents exchange their middle rows over points such that r 1 o r 2 . A cut-and-splice crossover (see Fig. 4 (c)) might also be used, where the first r 1 rows of parent
A and the first r 2 rows of parent B are exchanged. We can see that the offsprings generated by any of the three kind of crossovers are valid if and only if their parents are valid (because each row of the parse-matrix represents a complete operation and updating process). That is, there is no incom-plete mapping issue.

Mutation : The mutation operator is also easy to design. For example, a uniform mutation (see Fig. 4 (d)) can be defined as follows. Generate a random number r ij A  X  0 ; 1 for each entry of the parse-matrix to decide whether to mutate a ij or not (accord-ing to the preset mutation probability P m ). The mutated entry a 0 will be randomly chosen from the set a j \ f a ij g ,where a thesetofallpossiblevaluesofthe j -thcolumnaslistedin Table 2 . 3.5. Search engine
Suppose that the dimension of the target system is d , the height of the parse-matrix m , and we use the mapping Table 2 , then the parse-matrix entries a 1 A f 4 , 3 , ... , 3 ; 4 g , a  X  4  X  d  X  X  4  X  d  X  3  X  m combinations of the parse-matrix  X  a d  X  2, m  X  4, we have  X  9  X  4  X  d  X  X  4  X  d  X  3  X  m  X  2 : 8 10 12 : We can see that the search space of PME is very large in general.
Therefore, enumeration method is impractical and effective search engines are necessary.

Fortunately, many state-of-the-art search engines are still applicable to PME. In fact, PME uses a parse-matrix as its chromosome presentation, and the evolutionary operators (cross-over and mutation) are slightly modified from traditional ones.
Therefore, PME does not require any special search engine. For example, the classic genetic algorithm (GA) with roulette-wheel selection or the steady-state GA ( Blickle and Thiele, 1996 ) with tournament selection could easily be adapted to PME. In this paper, we choose the steady-state GA as PME X  X  search engine to do our numerical experiments because it is very easy to imple-ment and it has less stochastic noise. 3.6. Procedure
Despite different search engines might be used, the general procedure of PME could be described as follows.
 Procedure of PME :
Step 1 : Input evolutionary parameters: population size N ,initial
Step 2 : Initialize population A !  X  t  X  X f A 1  X  t  X  , A 2
Step 3 : Repeat the following steps until some stopping criter-3.7. Exceptions
Note that infeasible individuals might be generated due to the existence of logarithmic (log()) and divisional (/) operators. An individual is said to be infeasible if its expression contains a logarithmic operator and the argument is negative or zero at some trial points, or its expression contains a divisional operator and the denominator happens to be zero at some trial points.
In our implement of PME, infeasible individuals are differently treated at different evolutionary stages. At the initialization stage, an infeasible individuals will be replaced by a random-generated feasible individual. As a result, all individuals in the initial population will be feasible. During the reproduction stage, once an infeasible individual is detected, it will be labelled as a looser and its fitness will be defined as zero. Consequently it cannot survive to the next generation. 4. Remarks for engineering applications 4.1. Practical difficulties
Practical engineering problems are hard. Calculation of the response can be computationally/experimentally expensive. The design variables are not necessarily independent. The input-response data might be high dimensional, not designed, noisy, redundant, sparse, and/or with multiple time scales. They may cause many difficulties to genetic programming (including PME) for extracting valuable information from the data. Over-fitting, bloat, premature convergence are  X  X lassical X  difficulties in the GP field. Many problems are beyond the scope of this paper, and will be addressed in future studies. 4.2. Complexity control
Let us compare the target functional spaces of conventional linear/nonlinear regression: a n  X  arg min and symbolic regression f n  X  arg min
For the conventional linear/nonlinear regression, the target is, symbolic regression has a larger target function space, and linear/nonlinear regression might be considered as a special case of symbolic regression. This is the reason why a symbolic regression algorithm is expected to get a better model than a linear/nonlinear regression method.

Beware that too large searching space might result in unreal models (over-fitting). This phenomenon has been detected in numerical analysis. A case in point is Runge X  X  phenomenon, in which a problem of oscillation that occurs when interpolating with high degree polynomials. Such phenomenon might also appear if the symbolic regression uses high level parse trees.
Therefore, it is very important to properly confine the target function to a subset of function space. If we can find the minimum space where the minimum complexity model lies, the symbolic regression becomes easier.

PME is designed easily to control the complexity of the evolved expression. The height of the parse matrix ( m ) in PME is an explicit control parameter that can upper-bound the subtree-level of evolved expression. So it can be used to confine PME X  X  target function space. The parse tree will not bloat if we choose a fixed m (as we did in this work) or a controlled m , and the over-fitting difficulty is also avoided. It is easy for PME to balance the trade-off between the accuracy (model error) and complexity. In this sense, PME is free to control. 4.3. Premature avoidance
To avoid premature convergence, a multi-start technique ( Mart X   X  , 2003 ) is used in PME. Every T generations the population is re-initialized. The archived best individuals remain survived separately from the population. 4.4. Cost of symbolic regression is negligible
We have to point out that symbolic regression is not the most expensive part to construct a data-driven model in practical applications. In fact, symbolic regression is just an analytical process. Usually, it only consumes CPU time from minutes to hours. But the training points are obtained by costly physical experiments or computational intensive simulation process. So it is much more expensive to get the set of training points (the input-response data). Therefore, it is very important for a sym-bolic regression algorithm to rebuild the best structure of the data-driven model at  X  X ny X  cost. 5. Coefficient optimization
The mission of symbolic regression is to optimize both the structure and coefficients of a target function that describes an input-response system. PME, as well as other symbolic regression algorithms, distinguishes from linear/nonlinear regression in that its capability of  X  X tructure optimization X , which is its true value.
We do have enabled PME capable of optimizing the structure and coefficients simultaneously, and it works well (see Section 6.2 ). But in our real-world application of ground-to-flight data correlation, we find that the revolved results are difficult to physically interpret and the physicist (decision maker) do not accept them. Therefore, we introduced some techniques to avoid the  X  X oefficient optimization X , and only use PME X  X   X  X tructure optimization X  capability to carry out adaptive space transforms.
The results become interpretable and acceptable. We conclude that PME is NOT just an alternative to symbolic modeling. It can be used to detect the underlying physical relationships between different parameters.

To enable PME X  X  capability of coefficient optimization, we have re-designed the decoding and individual evaluation process. In the modified decoding process, coefficients are introduced, and during the individual evaluation process, a global optimization algorithm, low dimensional simplex evolution (LDSE) ( Luo and
Yu, 2012 ), is embedded to complete the online optimization of the introduced coefficients.

There are several possible ways to introduce coefficients into the evolving function. (a) Using multi-gene techniques as in the GP software GPTIPS ( Searson et al., 2010 ), in which the evolved functions are linear combinations of low order non-linear transformations of the input variables. (b) Using the random numeric terminal as the reference ( Streeter and Becker, 2003 ). (c) Adding coefficients during the decoding process and optimiz-ing them during the individual evaluation (our proposal).
We choose plan (c) in our work. Here is the reason: Plan (a) combines linear and non-linear optimization process, so the revolved model accuracy is likely high. But the result is hard to interpret. Plan (b) is very ease to implement and will not increase much computation cost, but the inherent randomness is hard to be accepted by the decision maker.

Symbolic regression is superior to linear/non-linear regression in that it can detect the desired function structure (usually non-linear) without making any priory assumptions on the target function. Plan (c) can hold this advantage.

In this work, coefficients are added in the following way: whenever the intermediate expression f 2 is updated (in case a  X  1in Table 2 ), a new coefficient will be introduced and the intermediate expression will be further updated to f 2  X  l where k starts from 1 and k r m .

For example, suppose we have two individuals from Sections 3.1 and 3.2 respectively, then the new decoding process will get respectively.

Of course, one can introduce more intermediate expressions (e.g., f 3 , f 4 , ) and attach coefficients whenever they are updated.
The modified PME process can be described as a bi-level optimization process of the following form. min where MSE denotes the mean square error, A is the individual (parse matrix) of PME, f A is the phenotype of A (an analytical function), l is the coefficient vector, J J 2 denotes 2-norm.
The inner optimization (LDSE) will be invoked to optimize the coefficients in the model whenever an individual A is evaluated.
The outer optimization (basic PME described in the previous sections) aims at optimizing the model structure. Their relations is shown in Fig. 5 . 6. Numerical results
PME has been implemented in C programming language, using steady-stage GA with multi-start technique ( Mart X   X  , 2003 ) as its search engine. For the sake of easy use, we use a Boolean variable to switch on/off the coefficient optimization process described in Section 5 .

Our test problems are partitioned into two groups: Exact fitting problems ( Section 6.1 ) and test problems from literature ( Section 6.2 ). As we know, practical engineering applications of symbolic regression are generally complex (see Section 4.1 ), so we do not expect a GP algorithm (including PME) capable of getting the exact solution. However, exact fitting problems (they are usually man-made) can help us evaluate PME X  X  capability of  X  X tructure optimization X . Test problems from literature give a comprehensive test of PME. They help us evaluate PME X  X  overall capability of  X  X tructure and coefficient optimization X .
In all our numerical experiments (except Prob. 6 in Section 6.2 ), uniform inner grid points are used as training points. For example, if we consider a function within the region  X  0 ; 1  X  1 ; 0 and we want to use two training points at each direction, there will be four training points located at (0.333, 0.333), (0.333, 0.666), (0.666, 0.333) and (0.666, 0.666). Note that the number of inner grid points will be quite large for high dimensional problems. To avoid unnecessary sampling, experi-mental design method such as Latin Hypercube ( McKay et al., 1979 ) and Uniform Design ( Fang and Wang, 1994 ) might be applied. However, according to our experience, too few training points might result in unreliable results.

In our tests, the current and intermediate expressions are initialized as follows: f  X  1 , f 1  X  1 and f 2  X  1 : 6.1. Exact fitting problems In this test group, 15 problems (see Table 5 ) are chosen to test
PME X  X  capability of  X  X tructure optimization X . The coefficient opti-mization process of PME is switched off. All of them have exact fitting models in the search space. So you may expect to get the target model itself by using PME. However, PME might give you an equivalent alternative to the target expression. Suppose you have a set of training points whose target model is solution sin  X  x 2  X  x 1  X  X  ln  X  x 2  X  x 1  X  , sin  X  x 1  X  x users using symbolic processors such as Maple or Mathematica to simplify the result from PME.

In case the target model is out of the search space, PME is capable of providing an approximate model. Suppose that we use then it is not in the search space. (Note that the coefficient optimization process of PME is switched off.) By PME, we can get its best approximate model within the search space ln  X  x 1  X  x 2  X  = cos  X  1  X  .

The control parameters of PME are set as follows. The height of the parse-matrix m  X  6. The population size N  X  100 and the maximum generations for re-initialization T is set to 10,000 (i.e., t r T  X  . The tournament size in steady state GA s  X  5. PME will terminate immediately if the exact target function (or its equiva-lent alternative) is detected, and restart automatically if it fails until generation T . To reduce the influence of randomness, 100 runs for each test problems are carried out. The average number of residual evaluations and the average number of restarts are recorded to show the efficiency of PME.

Table 5 shows the test models and PME X  X  average performance of the 100 independent runs with different initial populations. The full names of the notations in Table 5 are the dimension of modelled system (Dim), the target model (Target model), the domain of the target model (Domain), the number of training points (No. samples), the average number of residual evaluations of all successful runs (Ave no. eval), and the average number of restarts of 100 runs to get the target model (Ave no. start).
The computation results from Table 5 show that PME can recover the target models for all these test problems. The computational cost depends on the complexity of target model. 6.2. Test problems from literature
In this test group, we take all test problems from Keijzer (2003) into account to give PME an overall evaluation of its performance, including its  X  X tructure optimization X  and  X  X oeffi-cient optimization X  capabilities. The coefficient optimization pro-cess of PME is switched on. The control parameters are set as follows. The population size of PME N  X  500, and the maximum generations for re-initialization T is set to 20,000, the maximum number of restart M  X  20. The tournament size in steady state GA s  X  5. In the embedded LDSE, the population size N LDSE  X  20, the maximum generations for termination T LDSE  X  200, the adsorption probability p a  X  0.1 ( Luo et al., 2012 ). The coefficients are bonded by l A  X  50 ; 50 6 . The default target accuracy e target
Similar to the first test group, 100 runs for each test problems are carried out. The algorithm will exit immediately if the mean square error is small enough (MSE r e target ), and the number of restarts (no.start) is recorded.

Table 6 shows the overall performance of the modified PME in the 100 independent runs with different initial populations. We can see that PME can solve most of the problems (12 of 15) without changing any control parameters. The number of restart range from 1 to 9 (except Prob. 4), and the average number of restart to solve these problems is acceptable. By increasing the model complexity (set m  X  7), Prob. 15 is also solved. You can also expect PME capable of solving Prob. 4 by increasing m . But we would rather let PME concentrate on  X  X tructure optimization X  as discussed in Section 5 . For detailed results, refer to Appendix A . 7. Conclusion
We have introduced a new symbol ic regression method: parse-matrix evolution (PME). It uses parse-matrix as its chromosome presentation. It is easier than ever. It can easily be implemented in any programming language and free to control. Furthermore, it does not need any additional function parsing process. Numerical results show that PME can recover the st ructure of a target model with plenty of input-response data, provided that the structure is not so complex. PME is also capable of optimizing the structure and coefficients simultaneously.
 PME is NOT just an alternative to linear/nonlinear regression.
The true value of PME is its  X  X tructure optimization X  ability, which can be used to detect the underlying physical relationships between different parameters.

Further research is necessary to make PME more effective for symbolic regression. For example, more efficient search engine is desired. The steady-stage GA is used as the search engine of PME in this paper, but its efficiency is not satisfactory for complex problems with high level optimal subtrees. Other search engines might be more efficient.

The idea of PME is generic. Although PME is only applied to symbolic regression in this paper, it could be easily adapted for other automatic programming problems such as the analytical solution of partial differential equations ( Tsoulos and Lagaris, 2006 ), credit classification ( Brabazon and O X  X eill, 2006 ), Santa Fe ant trail and symbolic integration, etc.
 Acknowledgements This work was partially supported by the National Natural Science Foundation of China (Grant No. 90916028).
 Appendix A. Test results in detail
To comply with the literature, we use x , y , z instead of x Prob 1 X 3. f  X  x  X  X  0 : 3 x sin  X  2 p x  X  , where x A  X  1 ; 1 , x n  X  X  x
 X  n  X  sin  X  l 1 n  X  X  1  X  n  X  x 1  X  X  X  X  X  X  n  X  1  X  X  l 2 x 1 sin  X  l  X  X  6 : 283 , 0 : 300 , 2 : 570 , 25 : 04 , 38 : 57 , 7 : 433  X  . Prob 5. f  X  x , y , z  X  X  30 xz =  X  x 10  X  y 2 , where x , z
 X  n  X  l n  X  X  x 1  X  =  X  X  l 1 n  X  X  1  X  n  X  x 1  X  X  X  1  X  X  n  X  X  x 2  X  n  X  x are evolved. An example best individual (with MSE  X  7 : 426 10 n  X  X  1  X  n  X  1  X  X  X  ln  X  x 1  X  l 1 n  X  X  1 : 0  X  n  X  1  X  X  X  X  l 2  X  ln  X  x  X  l  X  X  0 : 522 , 0 : 576 , 30 : 74 , 8 : 339 , 9 : 744 , 30 : 12  X  . n  X  n  X 
Prob 10. f  X  x , y  X  X  x y , where x A  X  0 ; 1  X  , and y A  X  X  n  X  x 2  X  X  X  exp y ln x .
 Prob 11. f  X  x , y  X  X  xy  X  sin  X  X  x 1  X  X  y 1  X  X  , where x
 X  X  sin  X  l 1 n  X  1  X  x 2  X  x 1  X  X  X  X  x 1  X  n  X  x 2  X  X  X  xy  X  sin  X  l Prob 12. f  X  x , y  X  X  x 4 x 3  X  y 2 = 2 y , where x A  X  3 ; 3 , and y
 X  X  X  x 2  X  X  X  X  x 1  X  n  X  x 1  X  X  x 1  X  X  n  X  X  x 1  X  n  X  x 1  X  X  X  l  X  X  0 : 500 , 41 : 49 , 2 : 118 , 39 : 83 , 11 : 00 , 16 : 06  X  . Prob 13. f  X  x , y  X  X  6 sin  X  x  X  cos  X  y  X  , where x A  X  3 ; 3 , and y
 X  X  n  X  l Prob 14. f  X  x , y  X  X  8 =  X  2 x 2  X  y 2  X  , where x A  X  3 ; 3 , and y n  X  l 1 n  X  X  x 1  X  n  X  x 1  X  X  X  x 2  X  n  X  x 2  X  X  X  1  X  X  X  1 = l Prob 15. f  X  x , y  X  X  x 3 = 5  X  y 3 = 2 y x , where x A  X  3 ; 3 , and y
 X  X  X  n  X  x 1  X  X  x 2  X  X  X  x 2  X  n  X  l 1 n  X  X  x 2  X  n  X  x 2  X  X  X  X  x References
