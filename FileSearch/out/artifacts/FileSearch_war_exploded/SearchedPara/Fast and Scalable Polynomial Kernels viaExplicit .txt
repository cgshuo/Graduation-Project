 Approximation of non-linear kernels using random feature mapping has been successfully employed in large-scale data analysis applications, accelerating the training of kernel ma-chines. While previous random feature mappings run in O ( ndD ) time for n training samples in d -dimensional space and D random feature maps, we propose a novel random-ized tensor product technique, called Tensor Sketching , for approximating any polynomial kernel in O ( n ( d + D log D )) time. Also, we introduce both absolute and relative error bounds for our approximation to guarantee the reliability of our estimation algorithm. Empirically, Tensor Sketching achieves higher accuracy and often runs orders of magni-tude faster than the state-of-the-art approach for large-scale real-world datasets.
 I.5.2 [ Pattern Recognition ]: Design Methodology X  Clas-sifier design and evaluation Algorithms, Performance, Experimentation polynomial kernel; SVM; tensor product; Count Sketch; FFT
Kernel machines such as Support Vector Machines (SVMs) have recently emerged as powerful approaches for many ma-chine learning and data mining tasks. One of the key proper-ties of kernel methods is the capability to efficiently find non-linear structure of data by the use of kernels. A kernel can be viewed as an implicit non-linear data mapping from original * This work is supported by the Danish National Research Foundation under the Sapere Aude program.
 data space into high-dimensional feature space, where each coordinate corresponds to one feature of the data points. In that space, one can perform well-known data analysis al-gorithms without ever interacting with the coordinates of the data, but rather by simply computing their pairwise in-ner products. This operation can not only avoid the cost of explicit computation of the coordinates in feature space but also handle general types of data (such as numeric data, symbolic data).

While kernel methods have been used successfully in a va-riety of data analysis tasks, their scalability is a bottleneck. Kernel-based learning algorithms usually scale poorly with the number of the training samples (a cubic running time and quadratic storage for direct methods). This drawback is becoming more crucial with the rise of big data applica-tions [12, 3]. Recently, Joachims [9] proposed an efficient training algorithm for linear SVMs that runs in time lin-ear in the number of training examples. Since one can view non-linear SVMs as linear SVMs operating in an appro-priate feature space, Rahimi and Recht [16] first proposed a random feature mapping to approximate shift-invariant kernels in order to combine the advantages of both linear and non-linear SVM approaches. This approach approxi-mates kernels by an explicit data mapping into relatively low-dimensional random feature space. In this random fea-ture space, the kernel of any two points is well approximated by their inner product. Therefore, one can apply existing fast linear learning algorithms to find data relations corre-sponding to non-linear kernel methods in the random feature space. That leads to a substantial reduction in training time while obtaining similar testing error.

Following up this line of work, many randomized approaches to approximate kernels are proposed for accelerating the training of kernel machines [10, 12, 20, 21]. While the train-ing algorithm is linear, existing kernel approximation map-pings require time proportional to the product of the number of dimensions d and the number of random features D . This means that the mapping itself is a bottleneck whenever dD is not small. In this paper we address this bottleneck, and present a near-linear time mapping for approximating any polynomial kernel.

Particularly, given any two points of a dataset S of n points, x = { x 1 ,  X  X  X  ,x d } ,y = { y 1 ,  X  X  X  ,y d }  X  S  X  an implicit feature space mapping  X  : R d 7 X  F , the inner product between these points in the feature space F can be quickly computed as  X   X  ( x ) , X  ( y )  X  =  X  ( x,y ) where  X  () is an easily computable kernel. An explicit random feature mapping f : R d 7 X  R D can efficiently approximate a kernel  X  () if it satisfies: So we can transform data from the original data space into a low-dimensional explicit random feature space and use any linear learning algorithm to find non-linear data relations.
Rahimi and Recht [16] introduced a random projection-based algorithm to approximate shift-invariant kernels (e.g. the Gaussian kernel  X  ( x,y ) = exp(  X  X  x  X  y k 2 / 2  X  2 0). Vempati et al. [21] extended this work to approximate generalized radial-basic function (RBF) kernels (e.g. the exponential- X  2 kernel  X  ( x,y ) = exp(  X   X  2 ( x,y ) / 2  X   X  &gt; 0 and  X  2 is the Chi-squared distance measure). Re-cently, Kar and Karnick [10] made use of the Maclaurin se-ries expansion to approximate inner product kernels (e.g. the polynomial kernel  X  ( x,y ) = (  X  x,y  X  + c ) p , for c  X  0 and an integer p ).

These approaches have to maintain D random vectors  X  ,  X  X  X   X  D  X  R d in O ( dD ) space and need O ( ndD ) operations for computing D random feature maps. That incurs sig-nificant (quadratic) computational and storage costs when D = O ( d ) and d is rather large. When the decision bound-ary of the problem is rather smooth, the computational cost of random mapping might dominate the training cost. In addition, the absolute error bounds of previous approaches are not tight. Particularly, the Maclaurin expansion based approach [10] suffers from large error because it approxi-mates the homogeneous polynomial kernel  X  ( x,y ) =  X  x,y  X  by Q p i =1  X   X  i ,x  X  Q p i =1  X   X  i ,y  X  where  X  i  X  { +1 ,  X  1 } experiments show that large estimation error results in ei-ther accuracy degradation or negligible reduction in training time.

In this work, we consider the problem of approximating the commonly used polynomial kernel  X  ( x,y ) = (  X  x,y  X  + c ) to accelerate the training of kernel machines. We develop a fast and scalable randomized tensor product technique, named Tensor Sketching , to estimate the polynomial kernel of any pair of points of the dataset. Our proposed approach works in O ( np ( d + D log D )) time and requires O ( pd log D ) space for random vectors. The main technical insight is the connection between tensor product and fast convolution of Count Sketches [2, 14], which enables us to reduce the com-putational complexity and space usage. We introduce both absolute and relative error bounds for our approximation to guarantee the reliability of our estimation algorithm. The empirical experiments on real-world datasets demonstrate that Tensor Sketching achieves higher accuracy and often runs orders of magnitude faster than the state-of-the-art ap-proach for large-scale datasets.

The organization of the paper is as follows. In Section 2, we briefly review related work. Section 3 describes back-ground and preliminaries. The proposed approach is pre-sented and analyzed in Section 4 and 5. In Section 6, we show experimental evaluations of our proposed approach on real world datasets. Section 7 concludes the paper.
Traditional approaches for solving non-linear SVMs on large datasets are decomposition methods [1, 13]. These methods divide the training set into two sets, named work-ing set and fixed set; and iteratively solve the optimization problem with respect to the working set while freezing the fixed set. In other words, they iteratively update a subset of kernel methods X  coefficients by performing coordinate as-cent on subsets of the training set until KKT conditions have been satisfied to within a certain tolerance. Although such approaches can handle the memory restrictions involving the dense kernel matrix, they still involve numerical solutions of optimization subproblems and therefore can be problematic and expensive to large-scale datasets.

In order to apply kernel methods to large-scale datasets, many approaches have been proposed for quickly approxi-mating the kernel matrix, including the Nystr  X  om methods [5, 23], sparse greedy approximation [19] and low-rank kernel approximation [7]. These approximation schemes can reduce the computational and storage costs of operating on a kernel matrix while preserving the quality of results. An assump-tion of these approaches is that the kernel matrix has many zero eigenvalues. This might not be true in many datasets. Furthermore, there is a lack of experiments to illustrate the efficiency of these approaches on large-scale datasets [16].
Instead of approximating the kernel matrix, recent ap-proaches [10, 12, 16, 20, 21, 24] approximate the kernels by explicitly mapping data into a relatively low-dimensional random feature space. The explicit mapping transforms data into a random feature space where the pairwise in-ner products of transformed data points are approximately equal to kernels in feature space. Therefore, we can apply existing fast linear learning algorithms [6, 9, 18] to find non-linear data relations in that random feature space. While previous such approaches can efficiently accelerate the train-ing of kernel machines, they incur significant computational cost (quadratic in the dimensionality of data). That results in performance degradation on large-scale high-dimensional datasets.
Charikar et al. [2] described and analyzed a sketching ap-proach, called Count Sketch, to estimate the frequency of all items in a stream. Recently, the machine learning com-munity has used Count Sketch as a feature hashing tech-nique for large-scale multitask learning [22] because Count Sketches can preserve the pairwise inner products within an arbitrarily small factor. In our work, we view Count Sketch as a specific random projection technique in high-dimensional space because it maintains linear projections of a vector with the number of random vectors defined impli-citly by simple independent hash functions.

Definition 1. Given two 2-wise independent hash func-tions h : [ d ] 7 X  [ k ] and s : [ d ] 7 X  { +1 ,  X  1 } . Count Sketch of a point x = { x 1 ,  X  X  X  ,x d }  X  R d is denoted by Cx = { ( Cx ) 1 ,  X  X  X  , ( Cx ) k } X  R k where ( Cx ) j = P i : h ( i )= j
Note that the two Count Sketches C (1) x,C (2) x of a point x are different if they use different hash functions h 1 6 = h and s 1 6 = s 2 . The following lemma provides the bias and variance of the pairwise inner product of Count Sketches. Lemma 2. Given two points x,y  X  R d , we denote by Cx,Cy  X  R k the respective Count Sketches of x,y on the same hash functions h,s .
 Proof. See [22, Appendix A].

We derive an upper bound of variance of any pairwise inner product of Count Sketches as follows: Lemma 3. Given two points x,y  X  R d , we denote by Cx,Cy  X  R k the respective Count Sketches of x,y on the same hash functions h,s .

Proof. Given any two points x = { x 1 ,  X  X  X  ,x d } ,y = { y 1 ,  X  X  X  ,y d } , we have: By the lemma 2, we have:
It is worth noting that Count Sketch might not distort a sparse vector. This is due to the fact that non-zero elements will always be hashed into a cell of Count Sketch. In other words, they are retained after sketching with high probabil-ity. In addition, Count Sketch requires O ( nd ) operations for n points in d -dimensional space. Therefore, Count Sketch might provide better performance than traditional random projections in applications dealing with sparse vectors.
Given a vector x = { x 1 ,  X  X  X  ,x d }  X  R d , the 2-level tensor product or outer product x (2) = x  X  x is defined as follows: Given an integer p , we consider a p -level tensor product  X 
The following lemma justifies that tensor product is an explicit feature mapping for the homogeneous polynomial kernel.

Lemma 4. Given any pair of points x,y and an integer p , we have: Proof. See [17, Proposition 2.1].
 By taking y = x on the lemma 4, we have:
Lemma 5. Given any point x and an integer p , we have:
It is obvious that the tensor product requires d p dimen-sions to comprise the polynomial feature space. Therefore, it fails for realistically sized applications.
As elaborated above, it is infeasible to directly perform any learning algorithm in the polynomial feature space. In this section, we introduce an efficient approach to randomly project the images of data without ever computing their coordinates in that polynomial feature space. The proposed approach runs in O ( np ( d + D log D )) time for n training examples in d -dimensional space and D random projections; and outputs unbiased estimators of the degree-p polynomial kernel of any pair of data points.
Recently, Pagh [14] has introduced a fast algorithm to compute Count Sketch of an outer product of two vectors. Instead of directly computing the outer product, the ap-proach compresses these vectors into their Count Sketches and then computes the Count Sketch of their outer product by those sketches. Due to the fact that the outer product of two different Count Sketches can be efficiently computed by the polynomial multiplication (using FFT), we can compute the Count Sketch of an outer product of any two vectors in time near-linear in the dimensionality of the sketches. More precisely, given a vector x  X  R d , we denote by C (1) x,C (2) x  X  R D its two different Count Sketches using 2-wise independent hash functions h 1 ,h 2 : [ d ] 7 X  [ D ] and s ,s 2 : [ d ] 7 X  { +1 ,  X  1 } . We consider the outer product x  X  x  X  R d 2 and its Count Sketch Cx (2)  X  R D using in-dependent and decomposable hash functions H : [ d 2 ] 7 X  [ D ] and S : [ d 2 ] 7 X  X  +1 ,  X  1 } . We decompose H and S as follows:
H ( i,j ) = h 1 ( i ) + h 2 ( j ) mod D and S ( i,j ) = s We note that the hash functions H and S are 2-wise in-dependent [15]. We then represent a Count Sketch in D -dimensional space as a polynomial of degree D  X  1 where each coordinate corresponds to one term of the polynomial. For example, we consider two degree-( D  X  1) polynomials representing for C (1) x,C (2) x : P x (  X  ) = We can fast compute the degree-( D  X  1) polynomial for Cx using hash functions H and S : where (  X  ) is the component-wise product operator and FFT uses D interpolation points. In other words, the Count Sketch Cx (2) of x  X  x can be efficiently computed by Count Sketches C (1) x,C (2) x in O ( d + D log D ) time.
Inspired by the fast convolution of Count Sketches, we are able to efficiently compute the the polynomial P x ( p ) for the Count Sketch in D -dimensional space, Cx ( p ) , of the tensor product x ( p ) of any point x  X  R d by using indepen-dent and decomposable hash functions H : [ d p ] 7 X  [ D ] and S : [ d p ] 7 X  X  +1 ,  X  1 } . We decompose H and S as follows: are chosen independently from 2-wise independent family.
The proposed approach works in O ( p ( d + D log D )) time by using 2 p different and independent hash functions as elab-orated above. This idea motivates the intuition for Tensor Sketching approach to approximate polynomial kernels.
We exploit the fast computation of Count Sketches on ten-sor domains to introduce an efficient algorithm for approxi-mating the polynomial kernel  X  ( x,y ) = (  X  x,y  X  + c ) p integer p and c  X  0. It is obvious that we can avoid the constant c by adding an extra dimension of value data points. So, for simplicity, we solely consider the homo-geneous polynomial kernel  X  ( x,y ) =  X  x,y  X  p for the proposed algorithm and theoretical analysis.
 For each point x  X  S  X  R d , Tensor Sketching returns the Count Sketch of size D of the tensor product x ( p ) as random feature maps in R D for the polynomial kernel. The pseudo-code in Algorithm 1 shows how Tensor Sketching works. We maintain 2 p independent hash functions (lines 2 -3), where h ,  X  X  X  ,h p and s 1 ,  X  X  X  ,s p are 2-wise independent. For each point x , we create p different Count Sketches of size D using these 2 p different and independent hash functions (line 5). We then compute the Count Sketch of x ( p ) by the usage of polynomial multiplication (using FFT) (lines 6-8). As a result, we have obtained a random feature mapping f which provides unbiased estimators for the polynomial kernel.
Now, we analyze the complexity of Tensor Sketching. It requires O ( pd log D ) space usage to store 2 p hash functions. For each point, the running time of computing the Count Sketch of its p -level tensor product is O ( pd + pD log D ) due to applying FFT. Therefore, the total running time of Ten-sor Sketching is O ( np ( d + D log D )). To increase the accu-racy of estimates, we choose D = O ( d ); therefore, we need O ( npd log d ) operations compared to O ( nd 2 ) of the previous approaches [10, 16].
In this section we analyze the precision of estimate of the kernel  X  ( x,y ) =  X  x,y  X  p , where x,y  X  R d and p is an integer, showing bounds on the number of random features ( D ) to achieve a given absolute or relative precision . It is worth noting that the previous approaches [10, 16, 20, 12] only introduced bounds of an absolute error estimate. Often, however, the kernel has small value and a good absolute error Algorithm 1 Tensor Sketching( S,p,D ) Require: A dataset S of size n , the number of random Ensure: Return Count Sketches of the point set S as a 1: f ( S )  X  X  X  2: Pick p independent hash functions h 1 ,  X  X  X  ,h p : [ d ] 7 X  3: Pick p independent hash functions s 1 ,  X  X  X  ,s p : [ d ] 7 X  4: for each data point x  X  S do 5: Create p different Count Sketches: C (1) x,  X  X  X  ,C ( p ) 7: Obtain f ( x ) in frequency domain by the component-8: f ( x )  X  FFT  X  1 ( f ( x )) 9: Insert f ( x ) into f ( S ) 10: end for 11: return Return f ( S ) approximation is typically a poor relative error estimate. Large errors of estimate might result in either performance degradation or negligible reduction in computational cost.
In contrast to the previous techniques, our approach can be viewed as a specific random projection technique applied to images of data in the explicit polynomial feature space. In fact, Tensor Sketching maintains random projections of images of data in the feature space via independent hash functions of Count Sketches. Therefore, its estimators are unbiased and have tight error bounds.

Given two points x,y  X  R d , we denote by Cx ( p ) ,Cy ( p ) R
D the Count Sketches of x ( p ) , y ( p )  X  R d p , respectively. The lemma 4 and 5 guarantee that
D So applying the lemma 2 and 3, we have: Lemma 6.

While previous works on random feature mappings do not provide bounds on the variance of estimates, the variance of our estimate can be bounded. We make use Chebyshev X  X  inequality to bound the relative error, which depends on the cosine of the angle  X  xy between x and y .
 Lemma 7.
 P Proof. Consider the random variable X = D Cx ( p ) ,Cy ( p ) Chebyshev X  X  inequality guarantees that:
It is obvious that we need more random features to ap-proximate polynomial kernels of large degree p . In addition, the relative error depends on the pairwise angles of data points. So we have to use large D for almost orthogonal data points to achieve a good approximation.
Following up on the work of Kar and Karnick [10], we assume that the 1-norm of any point of data can be bounded, such that k x k 1  X  R for any point x and a nonnegative real the bound of D Cx ( p ) ,Cy ( p ) E for any pair of points x,y as follows: Lemma 8.

Proof. The H  X  older inequality says that D Cx ( p ) ,Cy k Cx ( p ) k 1 k Cy ( p ) k  X  . So it suffices to prove that k Cx R p for any x due to k Cx ( p ) k  X   X  k Cx ( p ) k 1 . By apply-ing the Cauchy-Schwarz inequality, we have: k Cx ( p ) P For any pair of points x,y , we use t different pairs of Count Hoeffding X  X  inequality, we achieve a tighter absolute error bound than the previous approach [10] as follows:
Lemma 9. Let X = 1 t P t i =1 X i be an average of the sum of independent random variables X i = D C ( i ) x ( p ) R . For any &gt; 0 ,
Our absolute error bound depends on the largest value taken by the polynomial kernel in the data space (e.g. R 2 p In fact, no algorithm guaranteeing an absolute error can avoid this dependence due to the unbounded nature of the polynomial kernel.
Empirically, it has been shown that normalizing a kernel may improve the performance of SVMs. A way to do so is to normalize the data such as k x k = 1 so that the exact kernel is properly normalized, i.e.  X  ( x,x ) =  X  x,x  X  p following lemma shows that Count Sketches can preserve the normalization of kernels.

Lemma 10. Given fixed constants , X  &lt; 1 and a point x such that k x k = 1 , we denote by Cx ( p )  X  R D the Count 72 log (1 / X  ) / 2 , we have that Proof. See [22, Appendix B].

It is obvious that our kernel approximation can maintain the normalization of kernels within an arbitrarily small fac-tor. In contrast, the Maclaurin expansion based approach [10] does not satisfy this property.
We implemented random feature mappings in Matlab-7.11.0 and conducted experiments in a 2.67 GHz core i7 Windows platform with 3GB of RAM. We compared the performance of random feature mappings, including Tensor Sketching (TS) and Random Maclaurin (RM) [10] with non-linear SVMs on 4 real world datasets: Adult [8], Mnist [11], Gisette [1], and Covertype 1 [8]. We used LIBSVM-3.14 [1] for non-linear kernels and LIBLINEAR-1.92 [6] for random feature mappings for classification task. All averages and standard deviations are over 5 runs of the algorithms.
This subsection presents the accuracy experiments to eval-uate the reliability of our estimation algorithm. We car-ried out experiments to compare the accuracy of estimators based on the number of random features ( D ) on two ran-dom feature mappings: Tensor Sketching (TS) and Random Maclaurin (RM). We measured the relative error of the ap-proximation of the homogeneous and inhomogeneous poly-nomial kernels of degree p = 2 , 3 , 4. We took D in ranges [500, 3000] and conducted experiments on Adult dataset with size n = 48 , 842 and dimensionality d = 123. Fig-ure 1 displays the relative error ( ) from expectation of the two approaches on different polynomial kernels.
 It is obvious that TS provides a smaller error than the RM approach on those polynomial kernels. The difference is most dramatic on the homogeneous kernels because of the use of Rademacher vectors  X  i  X  { +1 ,  X  1 } d in RM. In fact, it estimates  X  x,y  X  p as Q p i =1  X   X  i ,x  X  Q p i =1  X   X  very large variance, especially for large p . Due to the fact that we have to normalize data before applying any kernel method, RM gives small error on inhomogeneous kernels. In this case, the value of Maclaurin expansion concentrates on some low order terms that have small variance of estimate. When the accuracy of kernel machines depends on higher order terms, RM either suffers from low accuracy or needs large D due to large variance of estimate. In contrast, TS is a specific random projection in the polynomial feature space. So it greatly outperforms RM and does not require a large number of random features to achieve a small error. For example, on the inhomogeneous kernels, TS only needs D = 500 to achieve &lt; 1 while RM requires more than 3000 random features.
We sample 100,000 points for Covertype datasets due to the limit of RAM in color.)
This subsection compares the random feature construc-tion time of the two approaches, TS and RM, on two large high-dimensional datasets: Adult ( d = 123) and Mnist ( d = 780). As analyzed above, TS requires O ( np ( d + D log D )) time while RM demands O ( ndD ) time and much random-ness. It is obvious that the running time of TS is faster and less dependent on the original dimensionality of data, a very desirable property since random feature mapping of-ten contributes a significant computational cost in training large-scale high-dimensional datasets.

Figure 2.a shows the CPU time requirements in seconds of the two approaches on the kernel  X  = (1+  X  x,y  X  ) 4 when vary-ing the number of random features D in ranges [0, 4000] and fixing the number of training samples n = 10 , 000. It is clear that the running time of TS approach is almost independent from the dimensionality of data d when using large D . On both Adult ( d = 123) and Mnist ( d = 779) datasets, TS approach scales well when increasing D compared to RM on Adult dataset. In contrast, RM shows a linear dependence with d , as depicted on Mnist dataset ( d = 780). When the dataset (e.g. Mnist) has a smooth decision boundary, RM feature construction time dominates the training time. This property might limit the use of RM.

When the dimensionality of data d increases, we need to increase the number of random features D = O ( d ) to boost the accuracy. Figure 2.b demonstrates a quadratic running time of RM in terms of dimensionality of data on the syn-thetic dataset with setting d = D and n = 10 , 000. This means that RM will be a bottleneck of kernel machines on high-dimensional datasets. The next section will show a sig-nificant domination of RM feature mapping when training on the Gisette dataset ( d = 5000).
In this experiment, we compare the performance of ran-dom feature mappings (TS, RM) along with LIBLINEAR [6] and non-linear kernel mapping along with LIBSVM [1] for classification tasks on 4 large-scale datasets. We measured the training accuracy and time of these approaches on a va-riety of polynomial kernels. We note that training time of Figure 2: Comparison of CPU time (s) between Ten-sor Sketching (TS) and Random Maclaurin (RM) approaches on 3 datasets: (a) Adult ( d = 123) and Mnist ( d = 780); (b) Synthetic ( d = D ) using  X  = (1 +  X  x,y  X  ) 4 . (Figures best viewed in color.) random feature mapping approaches include time for feature construction and linear SVMs training.
 Figure 3 demonstrates a comparison of accuracy between TS, RM and non-linear SVMs on degree-2 polynomial ker-nels. The results impressively show that TS provides higher accuracy than RM on 4 datasets. The most dramatic dif-in color.) ference is on the homogeneous kernels due to large error of estimate of RM. Moreover, the accuracy of TS converges faster than RM to that of non-linear kernels when increas-ing the number of random features D . RM even decreases the accuracy on Gisette dataset because it requires a signif-icantly large number of random features for approximating higher order terms of Maclaurin expansion well.

Figure 4 shows the CPU time requirements in seconds of the two approaches in training linear SVMs using LIB-LINEAR on the kernel  X  = (1 +  X  x,y  X  ) 2 . It is obvious that TS provides performance benefits on high-dimensional datasets, such as Mnist and Gisette. On Covertype and Adult datasets, RM is slightly faster than TS. This is be-cause RM generates more features for the low order terms of Maclaurin expansion which do not require high compu-tational cost. When p is large, TS significantly outperforms RM, as illustrated in Table 1.

It is obvious that RM performs quite poorly on homoge-neous kernels on the 4 datasets. Due to the large error of estimate in homogeneous kernels, RM provides low accuracy on 4 datasets, especially in the kernel  X  =  X  x,y  X  4 . In fact, the large error of estimate produces meaningless results of training linear SVMs (e.g. 41.45 % of accuracy on the Mnist dataset). In contrast, TS shows stronger results than both RM and non-linear SVMs because it requires rather small time for feature construction and linear SVMs training while obtaining similar accuracy. TS performs exceptionally well on datasets of non-smooth decision boundaries, including Covertype and Adult, where it can achieve speed-ups of 50 and 1600 times, respectively, compared to non-linear SVMs on the kernel  X  = (1 +  X  x,y  X  ) 4 .

RM works better on inhomogeneous polynomial kernels because the value of Maclaurin expansion concentrates on some low order terms. However it suffers from large compu-tational cost of random mapping in high-dimensional datasets (e.g. Gisette and Mnist). Because these datasets have smooth decision boundaries, their training time is dominated by the random feature construction time. So RM gives similar per-formance to non-linear SVMs on the Gisette dataset. When RM suffers from large error of estimate, it can influence the smoothness of decision boundaries of linear SVMs algorithm and therefore require more training time. This explains the inefficiency of RM compared to non-linear SVMs on Mnist dataset on the kernel  X  = (1 +  X  x,y  X  ) 4 .
In contrast, the TS approach gives more stable and better performance than RM and non-linear SVMs approaches on 4 datasets. In particular, it has a slightly lower accuracy but runs much faster than non-linear SVMs. It not only achieves higher accuracy (up to 7%) but also runs faster (up to 13 times) than RM on the Mnist and Gisette datasets. Table 2 shows the speedup of TS compared to RM and non-linear SVMs on 4 datasets on the kernel  X  = (1 +  X  x,y  X  ) 4 . Table 2: Speedup of Tensor Sketching compared to Random Maclaurin and non-linear SVMs on  X  = (1+  X  x,y  X  ) 4 .
 Covertype ( D = 500) 50  X  Mnist ( D = 1000) 2  X  9  X  2  X  Gisette ( D = 5000) 9  X  25  X  8  X 
The TS random mapping does not show any speedup on low-dimensional datasets (e.g. Covertype, Adult) compared to RM, except for achieving smaller error. However, TS runs 8 times faster than RM in training the Adult dataset due to smaller estimation error. For high-dimensional datasets (e.g. Mnist and Gisette), TS shows speedup on both ran-dom mapping and training time. Compared to non-linear kernels, TS achieves significant speedup on Adult and Cover-type which have non-smooth decision boundaries.
In the previous work, the authors [10] introduce a heuristic named H0/1 for fast training. Due to the fact that we have to normalize data before applying any SVM-based learning algorithms, the value of Maclaurin expansion often concen-trates on the low order terms. Therefore, we can precompute the first and second terms of the Maclaurin expansion to achieve higher accuracy. For example, consider a Maclau-rin expansion of a degree-4 polynomial kernel as follows:  X  = (1+  X  x,y  X  ) 4 = 1+4  X  x,y  X  +6  X  x,y  X  2 +4  X  x,y  X  We can easily compute 1 + 4  X  x,y  X  in advance and use D 0 random features to estimate 6  X  x,y  X  2 + 4  X  x,y  X  3 This means that H0/1 needs D = d + D 0 random features and is able to achieve higher accuracy due to the use of D random features for approximating higher order terms.
However, H0/1 shows some disadvantages: (1) it cannot be used for homogeneous kernels; (2) it is not a dimension-ality reduction technique because of using d + D 0 random features and (3) H0/1 requires longer feature construction times due to the use of more randomness. When d is large, the feature construction time is even larger and often dom-inates the training time. Table 3 shows the comparison be-tween Tensor Sketching and Random Maclaurin with H0/1. Note that we do not use H0/1 on the Gisette dataset because of the large computational cost of random feature construc-tion.
 Table 3: Comparison of Tensor Sketching and Ran-dom Maclaurin with H0/1 on  X  = (1 +  X  x,y  X  ) 4 .

Although RM with H0/1 can offer better accuracy than plain RM, its accuracy is still lower than TS, except on the Adult dataset. In fact, the Adult dataset works well and achieves higher accuracy (84.92%) on the kernel  X  = 1 + 4  X  x,y  X  than with  X  = (1 +  X  x,y  X  ) 4 (79.31%). That explains why the accuracy of RM with H0/1 is exception-ally high. Due to the use of more randomness, the feature construction time of RM with H0/1 is much longer than TS on 3 datasets. In general, H0/1 is only suitable for low-dimensional datasets and works well when the value of poly-nomial kernel highly concentrates on the first and second terms of Maclaurin expansion.
In this paper, we have introduced a fast and scalable ran-domized tensor product technique for approximating poly-nomial kernels, accelerating the training of kernel machines. By exploiting the connection between tensor product and fast convolution of Count Sketches, our approximation algo-rithm works in time O ( n ( d + D log D )) for n training samples in d -dimensional space and D random features. We present a theoretical analysis of the quality of approximation to gua-rantee the reliability of our estimation algorithm. We show empirically that our approach achieves higher accuracy and often runs orders of magnitude faster than the state-of-the-art approach on large-scale real-world datasets.

An interesting research direction is analyzing and evalu-ating Tensor Sketching on other learning tasks, such as clus-tering [4] and multitask learning [22] on large-scale datasets. We also intend to apply Tensor Sketching on other kernels (e.g. Gaussian kernel, sigmoid kernel) by exploiting Taylor-series approximations of these kernels. By applying Tensor Sketching on Taylor-series approximations, we might achieve a substantial speedup in training these kernel machines.
We thank P. Kar for useful discussion and released source code in the early state of the project. We also thank the anonymous reviewers for their constructive comments and suggestions. [1] C.-C. Chang and C.-J. Lin. LIBSVM: A library for [2] M. Charikar, K. Chen, and M. Farach-Colton. Finding [3] R. Chitta, R. Jin, T. C. Havens, and A. K. Jain. [4] R. Chitta, R. Jin, and A. K. Jain. Efficient kernel [5] P. Drineas and M. W. Mahoney. On the Nystr  X  om [6] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, [7] S. Fine and K. Scheinberg. Efficient SVM training [8] A. Frank and A. Asuncion. UCI machine learning [9] T. Joachims. Training linear SVMs in linear time. In [10] P. Kar and H. Karnick. Random feature maps for dot [11] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. [12] S. Maji and A. C. Berg. Max-margin additive [13] E. Osuna, R. Freund, and F. Girosi. An improved [14] R. Pagh. Compressed matrix multiplication. In [15] M. P X atra  X scu and M. Thorup. The power of simple [16] A. Rahimi and B. Recht. Random features for [17] B. Sch  X  okopf and A. J. Smola. Learning with kernels: [18] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: [19] A. J. Smola and B. Sch  X  okopf. Sparse greedy matrix [20] A. Vedaldi and A. Zisserman. Efficient additive kernels [21] S. Vempati, A. Vedaldi, A. Zisserman, and C. V. [22] K. Q. Weinberger, A. Dasgupta, J. Langford, A. J. [23] C. K. I. Williams and M. Seeger. Using the Nystr  X  om [24] T. Yang, Y.-F. Li, M. Mahdavi, R. Jin, and Z.-H.
