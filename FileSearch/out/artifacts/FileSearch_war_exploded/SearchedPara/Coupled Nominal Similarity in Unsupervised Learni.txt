 The similarity between nominal objects is not straightfor-ward, especially in unsupervised learning. This paper pro-poses coupled similarity metrics for nominal objects, which consider not only intra-coupled similarity within an attribute (i.e., value frequency distribution) but also inter-coupled similarity between attributes (i.e. feature dependency ag-gregation). Four metrics are designed to calculate the inter-coupled similarity between two categorical values by consid-ering their relationships with other attributes. The theoret-ical analysis reveals their equivalent accuracy and superior efficiency based on intersection against others, in particular for large-scale data. Substantial experiments on extensive UCI data sets verify the theoretical conclusions. In addition, experiments of clustering based on the derived dissimilarity metrics show a significant performance improvement. Categories and Subject Descriptors : H.2.8 [ Database Management ]: Database Applications X  data mining General Terms : Algorithms, Measurement, Performance Keywords : Similarity measure, Complexity, Accuracy
Similarity analysis has been a problem of great practical importance in several domains, including data mining, for decades [8]. By defining certain similarity measures between attribute values, it gauges the strength of the relationship between two data objects: the more two objects resemble each other, the larger the similarity is [7].

When objects are described by numerical features, their similarity measures geometric analogies which reflect the relationship of data values. For instance, the values 10 m and 12 m are more similar than 10 m and 2 m .Avarietyof similarity metrics have been developed for numerical data,  X 
The first author of this paper for correspondence.  X  The third author of this paper.
 such as Euclidean and Minkowski distances [7]. By con-trast, the similarity analysis between records described by nominal variables has received much less attention. Hetero-geneous Distances [10] and Modified Value Distance Matrix ( MVDM ) [5], for example, depict the similarity between cat-egorical values in supervised learning. For unlabeled data, only a few works [7], including Simple Matching Similarity ( SMS , which only uses 0s and 1s to distinguish similarities between distinct and identical categorical values) and Occur-rence Frequency [2], discuss the similarity between nominal values. We illustrate the problem with these works and the challenge of analyzing similarity for categorical data below.
Taking the Movie data (Table 1) as an example, six movie objects are divided into two classes with three nominal fea-tures: director, actor and genre. The SMS measure between directors  X  Scorsese  X  X nd X  Coppola  X  X s0,but X  Scorsese  X  X nd  X  Coppola  X  are very similar directors 1 . Another observation by following SMS is that the similarity between  X  Koster  X  and X  Hitchcock  X  X s equal to that between X  Koster  X  X nd X  Cop-pola  X ; however, the similarity of the former pair should be greater since it belongs to the same class G 2 .

Both instances show that it is much more complex to an-alyze similarity between nominal variables than continuous data, and SMS and its variants fail to capture the genuine relationship between nominal values. With the increase of categorical data such as that derived from social networks, it is important to develop effective and efficient measures for capturing similarity between nominal variables.

Thus, we discuss the similarity for categorical values by considering data characteristics. Two attribute values are similar if they present analogous frequency distributions for one attribute [2]; this reflects the intra-coupled similarity within a feature. For example, two directors are very simi-lar if they appear with almost the same frequency, such as  X  Scorsese  X  X ith X  Coppola  X  X nd X  Koster  X  X ith X  Hitchcock  X . However, the reality is that the former director pair is more
A conclusion drawn from a well-informed cinematic source. similar than the latter. To improve the accuracy of intra-coupled similarity, it is believed that the object co-occurrence probabilities of attribute values induced on other features are comparable [1]. To this end, the similarity between di-rectors should also cater for the dependencies on other fea-tures such as  X  X ctor X  and  X  X enre X  over all the movie objects, namely, the inter-coupled similarity between attributes. The coupling relationships between values and between attributes contribute to a more comprehensive understanding of ob-ject similarity [4]. No work that systematically considers both intra-coupled and inter-coupled similarities has been reported in the literature. This fact leads to the incomplete description of categorical value similarities, and apart from this, the similarity analysis on dependency aggregation is usually very costly.

In this paper, we propose a Coupled Object Similarity ( COS ) measure by considering both Intra-coupled and Inter-coupled Attribute Value Similarities ( IaAVS and IeAVS ), which capture the attribute value frequency distribution and feature dependency aggregation with a high learning accu-racy and relatively low complexity, respectively. We com-pare accuracies and efficiencies among the four proposed metrics for IeAVS , and come up with an optimal one from both theoretical and experimental aspects; we then evaluate our proposed measure with an existing metric on a variety of benchmark categorical data sets in terms of clustering quali-ties; and we develop a method to define dissimilarity metrics flexibly with our fundamental similarity building blocks ac-cording to specific requirements..

The paper is organized as follows. In Section 2, we briefly review the related work. Preliminary definitions are speci-fied in Section 3. Section 4 proposes the coupled similarities, and the theoretical analysis is given in Section 5. We demon-strate the efficiency and effectiveness of COS in Section 6 with experiments. Finally, we end this paper in Section 7.
There are some surveys [2, 7] that discuss the similar-ity between categorical attributes. Cost and Salzberg [5] proposed MVDM based on labels, while Wilson and Mar-tinez [10] studied heterogeneous distances for instance based learning. Unlike our focus here, the measures in their study are only designed for supervised approaches.

For unsupervised learning, there exist some data mining techniques for nominal data [1, 2]. The most famous are the SMS measure and its diverse variants such as Jaccard coefficients [7], which are all intuitively based on the prin-ciple that the similarity measure is 1 with identical values and is otherwise 0. More recently, attribute value frequency distribution has been considered for similarity measures [2]; neighborhood-based similarities [8] are explored to describe the object neighborhood by using an overlap measure. They are different from our proposed method, which directly re-veals the similarity between a pair of objects.

Recently, increasing numbers of researchers have argued that the attribute value similarities are also dependent on their coupling relations [2, 4]. Das and Mannila presented the Iterated Contextual Distances algorithm, believing that the feature and object similarities are inter-dependent [6]. Ahmad and Dey [1] proposed computing the dissimilarity by considering the co-occurrence. While the dissimilarity metric of the latter leads to high accuracy, the computation is usually very costly, which limits its application in large-scale problems.
A large number of data objects with the same features can be organized by an information table S = &lt;U,A,V,f &gt; , where U = { u 1 ,  X  X  X  ,u m } is composed of a nonempty finite set of data objects; A = { a 1 ,  X  X  X  ,a n } is a finite set of fea-tures; V = n j =1 V j is a set of all attribute values, in which V j is the set of attribute values of feature a j (1  X  j  X  m ); and f =  X  n j =1 f j ( f j : U  X  V j ) is an information function which assigns a particular value of each feature to every ob-ject. For instance, Table 2 consists of six objects and three features, with f 2 ( u 1 )= B 1 and V 2 = { B 1 ,B 2 ,B 3
Generally speaking, the similarity between two objects u values x, y  X  V j for all the features a j . The basic concepts below are defined to facilitate the formulation for attribute value similarities, where | H | is the number of elements in H . Definition 3.1. Given an information table S ,three Set Information Functions (SIFs) are defined as f  X  j :2 U  X  2 j , g j : V j  X  2 U ,and g  X  j :2 V j  X  2 U . Specifically: where u i ,u k 1 ,  X  X  X  ,u k t  X  U ,and W  X  V j .

These SIF s describe the relationships between objects and attribute values from different levels. For example, f  X  2 u } )= { B 1 ,B 2 } , g 2 ( B 1 )= { u 1 ,u 2 } for value B g ( { B 1 ,B 2 } )= { u 1 ,u 2 ,u 3 ,u 6 } if given W = { B
Definition 3.2. Given an information table S ,its Inter-information Function (IIF)  X  j  X  k : V j  X  2 V k is defined:
This IIF  X  j  X  k is the composition of f  X  k and g j .Itob-tains the k th attribute value subset for the corresponding objects, which are derived from the j th attribute value x . For example,  X  2  X  1 ( B 1 )= { A 1 ,A 2 } .

Definition 3.3. Given an information table S ,the k th attribute value subset W  X  V k ,andthe j th attribute value x  X  V j ,the Information Conditional Probability (ICP) of W with respect to x is P k | j ( W | x ) :
Intuitively, when given all the objects with the j th at-tribute value x , ICP is the percentage of the common objects whose k th attribute values fall in subset W and j th attribute value is exactly x as well. For example, P 1 | 2 ( { A 1 }|
All these concepts and functions are composed to formal-ize the so-called coupled interactions between categorical at-tribute values, as presented below.
In this section, Coupled Attribute Value Similarity (CAVS) is proposed in terms of both intra-coupled and inter-coupled value similarities. When we consider the simi-larity between attribute values,  X  X ntra-coupled X  X ndicates the involvement of attribute value occurrence frequencies within one feature, while the  X  X nter-coupled X  means the interaction of other features with this attribute. For example, the cou-pled value similarity between B 1 and B 2 concerns both the intra-coupled relationship specified by the repeated times of values B 1 and B 2 : 2 and 2, and the inter-coupled interaction triggered by the other two features ( a 1 and a 3 ). Suppose we have the Intra-coupled Attribute Value Similarity (IaAVS) measure  X  Ia j ( x, y )and Inter-coupled Attribute Value Similarity (IeAVS) measure  X  Ie j ( x, y ) for feature a j and x, y  X  V j ,then CAVS  X  A j ( x, y )isnatu-rally derived by simultaneously considering both of them.
Definition 4.1. Given an information table S ,the Cou-pled Attribute Value Similarity (CAVS) between attribute values x and y of feature a j is: where  X  Ia j and  X  Ie j are IaAVS and IeAVS, respectively.
According to [7], it is a fact that the discrepancy of at-tribute value occurrence times reflects the value similarity in terms of frequency distribution. Thus, when calculating attribute value similarity, we consider the relationship be-tween attribute value frequencies on one feature, proposed as intra-coupled similarity in the following.

Definition 4.2. Given an information table S ,the Intra-coupled Attribute Value Similarity (IaAVS) between at-tribute values x and y of feature a j is:
In this way, different occurrence frequencies indicate dis-tinct levels of attribute value significance. Gan et al. [7] reveal that greater similarity is assigned to the attribute value pair which owns approximately equal frequencies. The higher these frequencies are, the closer such two values are. Thus, function (4.2) is designed to satisfy these two prin-ciples. Besides, since 1  X | g j ( x ) | , | g j ( y ) | X  m ,then  X  [1 / 3 ,m/ ( m + 2)]. For example, in Table 2, both values B and B 2 are observed twice, so  X  Ia 2 ( B 1 ,B 2 )=0 . 5.
Hence, by taking into account the frequencies of cate-gories, an effective measure ( IaAVS ) has been captured to characterize the value similarity in terms of occurrence times.
In terms of IaAVS , we have considered the intra-coupled similarity, i.e., the interaction of attribute values within one feature a j . This does not, however, involve the couplings between other features a k ( k = j ) and feature a j when cal-culating attribute value similarity. Accordingly, we discuss this dependency aggregation, i.e., inter-coupled interaction. In 1993, Cost and Salzberg [5] proposed a powerful method, MVDM , for measuring the dissimilarity between categorical values. MVDM considers the overall similarities of classifi-cation of all objects on each possible value of each feature. The idea is that attribute values are identified as being sim-ilar if they occur with the same relative frequency for all classifications. In the absence of labels, the above measure is adapted to satisfy our target problem by replacing the class label with some other feature to enable unsupervised learning. We regard this interaction between features as inter-coupled similarity in terms of the co-occurrence com-parisons of ICP . The most intuitive variant is IRSP :
Definition 4.3. Given an information table S ,the Inter-coupled Relative Similarity based on Power Set (IRSP) between attribute values x and y of feature a j based on an-other feature a k is: where W = V k \ W is the complementary set of a set W under the complete set V k .

In fact, two attribute values are closer to each other if they have more similar probabilities with other attribute value subsets in terms of co-occurrence object frequencies. In Table 2, by employing (4.3), we want to get  X  P 2 | 1 ( B i.e. the similarity between two attribute values B 1 ,B 2 ture a 2 regarding feature a 1 . Since the set of all attribute values of feature a 1 is V 1 = { A 1 ,A 2 ,A 3 ,A 4 } ,thenumberof all power sets within V 1 is 2 4 , i.e., the number of the com-binations consisting of W  X  V 1 and W  X  V 1 is 2 4 .The minimal value among them is 0.5, which indicates that sim-ilarity  X  P 2 | 1 ( B 1 ,B 2 )=0 . 5.

This process shows the combinational explosion brought about by the power set needs to be considered when calcu-lating attribute value similarity by IRSP . We therefore try to define three more similarities based on IRSP as follows.
Definition 4.4. Given an information table S ,the Inter-coupled Relative Similarity based on Universal Set (IRSU), Join Set (IRSJ), and Intersection Set (IRSI) between attribute values x and y of feature a j based on an-other feature a k are the following formulae respectively:  X   X  where w  X  and w  X  denote w  X   X  j  X  k ( x )  X  j  X  k ( y ) and w  X   X  j  X  k ( x )  X  j  X  k ( y ) , respectively.
Each k th attribute value w  X  V k , rather than its value subset W  X  V k , is considered to reduce computational com-plexity. In this way, IRSU is applied to compute similarity  X  only concerns all the single attribute values rather than exploring the whole power set, it has solved the combina-tional explosion issue to a great extent. In IRSU , ICP is merely calculated 8 times compared with 32 times by IRSP , which leads to a substantial improvement in efficiency. Then fied since A 3  X   X  2  X  1 ( B 1 )  X  2  X  1 ( B 2 ). Thus, we obtain  X  2 | 1 ( B 1 ,B 2 )=0 . 5, which reveals the fact that it is enough to compute ICP with w  X  V 1 that belongs to  X  2  X  1 ( B 1 )  X  ( B 2 ) instead of all the elements in V 1 . From this perspective, IRSJ reduces the complexity further when compared with IRSU . Based on IRSU ,analternative IRSI is considered. For example, with (4.6), the calculation of  X  I 2 | 1 ( B once again simplified since only A 2  X   X  2  X  1 ( B 1 )  X  2  X  1 ficient to compute ICP with w  X  V 1 which only belongs to  X  2  X  1 ( B 1 )  X  2  X  1 ( B 2 ). It is trivial that the cardinality of intersection is no larger than that of join set .Thus, IRSI is further more efficient than IRSU due to the reduc-tion of intra-coupled relative similarity complexity.
Intuitively speaking, it is a fact that IRSI is the most efficient of all the proposed inter-coupled relative similarity measures: IRSP , IRSU , IRSJ , IRSI . In addition, all four measures lead to the same similarity result, such as 0 . 5.
According to the above discussion, we can naturally define the similarity between the j th attribute value pair ( x, y )on top of these four optional measures by aggregating all the relative similarities on features other than attribute a j
Definition 4.5. Given an information table S ,the Inter-coupled Attribute Value Similarity (IeAVS) between at-tribute values x and y of feature a j is: where  X  k is the weight parameter for feature a k , n k =1 tive similarity candidates.

Accordingly, we have  X  Ie j  X  [0 , 1], then  X  A j =  X  Ia j [0 ,m/ ( m + 2)] since  X  Ia j  X  [1 / 3 ,m/ ( m + 2)]. In Table 2, for example,  X  Ie 2 ( B 1 ,B 2 )=0 . 5  X   X  2 | 1 ( B 1 ,B 2 )+0 . 5 (0 . 5+0) / 2=0 . 25 if  X  1 =  X  3 =0 . 5 is taken with equal weight. Furthermore, coupled attribute value similarity (4.1) is obtained as  X  A 2 ( B 1 ,B 2 )=  X  Ia 2 ( B 1 ,B 2 )  X   X  0 . 5  X  0 . 25 = 0 . 125. For the Movie data set in Section 1, then  X 
Director ( Scorsese, Coppola )=  X  A Director ( Coppola, Coppola ) =0 . 33, and  X  A Director ( Koster, Coppola ) = 0 while  X  ( Koster, Hitchcock )=0 . 25. They correspond to the fact that  X  Scorsese  X  X nd X  Coppola  X  X re very similar directors just as X  Coppola  X  X s to himself, and the similarity between X  Koster  X  and  X  Hitchcock  X  is larger than that between  X  Koster  X  X nd  X  Coppola  X , as clarified in Section 1.

After specifying IaAVS and IeAVS , a coupled similarity between objects is built based on CAVS . Then, we consider the sum of all these CAVS s analogous to the construction of Manhattan dissimilarity [7]. Formally, we have:
Definition 4.6. Given an information table S ,the Cou-pled Object Similarity (COS) between objects u i 1 and u i Table 3: Computational Complexity for CAVS where  X  A j is the CAVS measure defined in (4.1), x i 1 j x j are the attribute values of feature a j for objects u i u 2 respectively, and 1
For COS ,allthe CAVS switheachfeaturearesummed up for two objects. For example (Table 2), COS ( u 2 ,u 3 j =1  X  j ( x 2 j ,x 3 j )=0 . 5+0 . 125 + 0 . 125 = 0 . 75.
This section compares four proposed inter-coupled relative similarity measures ( IRSP , IRSU , IRSJ and IRSI )interms of their computational accuracies and complexities. 1) Computational Accuracy Equivalence
From the aspect of set theory, these four measures are equivalent to one another in calculating value similarity.
Theorem 5.1. IRSP, IRSU, IRSJ and IRSI are all equiv-alent to one another. 2 The above theorem also explains the similarity result in Section 4.2. Thus, these measures induce exactly the same computational accuracy in machine learning tasks. 2) Computational Complexity Comparison
Suppose we have an information table S with m objects and n features, the maximal number of attribute values for all the features is R . In total, the number of attribute value pairs for all the features is at most n  X  R ( R  X  1) / 2, which is also the number of calculation steps. For each inter-coupled relative similarity, we calculate ICP for | ICP ( M ) j | k measure IRS M .Aswehave n attributes, the total ICP time we have four options for M , the computational complexities for calculating all the CAVS s are shown in Table 3.
As indicated in Table 3, all the measures have the same calculation steps, while their flops per step are sorted in de-scending order since 2 R &gt;R  X  P  X  Q ,inwhich P and Q are the join and intersection sets of the corresponding IIF s, respectively. This evidences that the computational com-plexity essentially depends on the time costs of ICP linearly with given data. Specifically, IRSP has the largest complex-ity O ( n 2 R 2 2 R ), compared to the smaller equal ones O ( n presented by the other three measures ( IRSU , IRSJ ,and IRSI ). Of the latter three candidates, though they have the same computational complexity, IRSI is the most efficient due to Q  X  P  X  R . In fact, the dissimilarity that Ahmad and Dey [1] have used for mixed data clustering corresponds to the worst measure IRSP discussed here.

Considering both the accuracy analysis and complexity comparison, we conclude that IRSI is the best performing because it indicates the least complexity but still maintains an equal accuracy to present coupling.
All detailed proofs of Theorem 5.1 are available on request.
Figure 1: Scalability on | A | and R respectively.
In this section, several experiments are performed on ex-tensive UCI data sets to show the effectiveness and efficiency of our proposed coupled similarities. The experiments are divided into two categories: coupled similarity comparison and COS application. For simplicity, we just assign the weight vector  X  =(  X  k ) 1  X  n with values  X  ( k )=1 /n in (4.7).
To compare efficiencies, we conduct extensive experiments on the inter-coupled relative similarity metrics: IRSP , IRSU , IRSJ ,and IRSI . The goal in this set of experiments is to show the obvious superiority of IRSI ,comparedwiththe most time-consuming measure IRSP . As discussed in Sec-tion 5, the computational complexity linearly depends on the time costs of ICP with given data. Thus, we consider a comparison of complexities represented by the time costs of ICP . Also explained in Section 5, the complexity for IRSP is O ( n 2 R 2 2 R ), while the other three have equal smaller com-plexity O ( n 2 R 3 ). Here, scalability analysis is explored in terms of these two factors separately: the number of fea-tures | A | and the maximal number of attribute values R .
From the perspective of | A | , Soybean-large data set is considered with 307 objects and 35 features. Here, we fix R to be 7, and focus on | A | ranging from 5 to 35 with step 5. In terms of the total time costs of ICP , the computational complexity comparisons among four measures ( IRSP , IRSU , IRSJ ,and IRSI ) are depicted in Figure 1( | A | ). The result indicates that the complexities of all these measures keep increasing when | A | becomes larger. The acceleration of IRSP (from 3328 to 74128) is the greatest compared with the slightest acceleration of IRSI (from 632 to 15704). Apart from these two, the scalability curves are almost the same for IRSU and IRSI , though the complexity of IRSU is slightly higher than that of IRSJ with varied | A | . Therefore, IRSI is the most stable and efficient measure to calculate the intra-coupled relative similarity in terms of | A | .

From the perspective of R ,thevariationof R is con-sidered when | A | is confirmed. Here, we take advantage of the Adult data set with 30718 objects and 13 features cho-sen. Specifically, the integer feature  X  X nlwgt X  is discretized into different intervals (from 10 to 10000) to form distinct R ranging from 16 to 10000, since one of the existing categorial attributes  X  X ducation X  already has 16 values. The outcomes are shown in Figure 1( R ), in which the horizontal axis refers to R , and the vertical axis indicates the relative complex-ity ratios in terms of  X  ( J/U ),  X  ( I/J ), and  X  ( I/U ). From this figure, we observe all the ratios between 10% and 100%, which again verifies the complexity order for these four mea-sures indicated in Section 5. Another issue is that all three curves decrease as R grows, which means the efficiency ad-vantages of IRSJ upon IRSU (from 85 . 5% to 46 . 8%), IRSI upon IRSJ (from 78 . 2% to 40 . 2%), and IRSI upon IRSU (from 66 . 9% to 18 . 8%) all become more and more obvious with the increasing of R . The general trend of these ratios always falling comes from the fact that there is a higher probability of getting a join set smaller than the whole set, and an intersection set smaller than the join set, with larger R . The same conclusion also holds for the ratio  X  ( U/P ), but this is due to the fact that q  X  1 ( x )= x/ 2 x is a strictly monotonously decreasing function when x&gt; 1. We omit this ratio in Figure 1( R ) since the denominator | ICP ( P ) | becomes exponentially large when R grows, e.g., it equals to 5 . 12  X  10 83 when R = 500. Hence, IRSI is the least time-consuming intra-coupled similarity with regard to R .
In summary, all the above experiment results clearly show that IRSI outperforms IRSP , IRSU ,and IRSJ in terms of the computational complexity. In particular, with the increasing numbers of either features or attribute values, IRSI demonstrates superior efficiency compared to the oth-ers. IRSJ and IRSU follow, with IRSP being the most time-consuming, especially for the large-scale data set.
In this part of our experiments, we focus on the compu-tational accuracy comparison. In the following, we evaluate the COD which is derived from (4.8): COD ( u i 1 ,u i 2 )= where h 1 ( t )and h 2 ( t ) are decreasing functions. Based on intra-coupled and inter-coupled similarities, h 1 ( t )and h can be flexibly chosen to build dissimilarity measures ac-cording to specific requirements. Here, we consider h 1 ( t )= 1 /t  X  1and h 2 ( t )=1  X  t to reflect the complementarity of similarity and dissimilarity measures. In terms of the capa-bility on revealing the relationship between data, the better the dissimilarity induced, the better is its similarity.
To demonstrate the effectiveness of our proposed COD in application, we compare two clustering methods based on two dissimilarity metrics on six data sets. Here, COD is used with the outperforming measure IRSI .

One of the clustering approaches is the k-modes ( KM )al-gorithm [7], designed to cluster categorical data sets. The main idea of KM is to specify the number of clusters k and then to select k initial modes, followed by allocating every object to the nearest mode. The other is a branch of graph-based clustering, i.e., spectral clustering ( SC )[9],which makes use of the Laplacian Eigenmaps on dissimilarity ma-trix to perform dimensionality reduction for clustering prior to the k-means algorithm. In respect of feature dependency aggregations, however, Ahmad and Dey [1] evidenced that
Figure 2: Clustering evaluation on six data sets their proposed metric ADD outperforms SMD in terms of KM clustering. Thus, we aim to compare the performances of ADD [1] and COD (6.1) for further clustering evaluations.
We conduct four groups of experiments on the same data sets: KM with ADD , KM with COD , SC with ADD ,and SC with COD . The clustering performance is evaluated by com-paring the obtained cluster of each object with that provided by the data label in terms of accuracy ( AC ) and normalized mutual information ( NMI )[3]. AC  X  [0 , 1] is a degree of closeness between the obtained clusters and its actual data labels, while NMI  X  [0 , 1] is a quantity that measures the mutual dependence of two variables: clusters and labels. AC =1or NMI = 1 if the clusters and labels are identical, and AC =0or NMI = 0 if the two sets are independent. In fact, the larger AC or NMI is, the better the clustering is, and the better the corresponding dissimilarity metric is.
Figure 2 reports the results on six data sets with different |
U | , ranging from 15 to 699 in increasing order. In terms of AC and NMI , the evaluations are conducted with KM-ADD , KM-COD , SC-ADD ,and SC-COD individually. Followed by Laplacian Eigenmaps, the subspace dimensions are de-termined by the number of labels in SC . For each data set, the average performance is computed over 100 tests for KM and k-means in SC with distinct start points.

As can be clearly seen from Figure 2, the clustering meth-ods with COD ,whether KM or SC , outperform those with ADD in terms of both AC and NMI measures. That is to say, dissimilarity metric COD is better than ADD on clustering qualities. Specifically for KM ,the AC improving rate ranges from 5 . 56% (Balloon) to 16 . 50% (Zoo), while the NMI improving rate falls within 4 . 76% (Soybean-s) and 37 . 38% (Breastcancer). With regard to SC , the former rate takes the minimal and maximal ratios as 4 . 21% (Balloon) and 20 . 84% (Soybean-l), respectively; however, the latter rate belongs to [5 . 45% (Soybean-l) , 38 . 12% (Shuttle)]. Since AC and NMI evaluate clustering quality from different as-pects, they generally take minimal and maximal ratios on distinct data sets. Another significant observation is that SC mostly outperforms KM a little whenever it has the same dissimilarity metric; in fact, Luxburg [9] has indicated that SC very often outperforms k-means for numerical data.
We draw the following two conclusions: 1) intra-coupled relative similarity IRSI is the most efficient one when com-pared with IRSP , IRSU and IRSJ , especially for large-scale data; 2) our proposed object dissimilarity metric COD is better than others, such as dependency aggregation only ADD , for categorical data in terms of clustering qualities.
We have proposed COS , a novel coupled object similarity metric which involves both attribute value frequency distri-bution (intra-coupling) and feature dependency aggregation (inter-coupling) in measuring attribute value similarity for unsupervised learning of nominal data. Theoretical analysis and substantial experiments have shown that inter-coupled relative similarity measure IRSI significantly outperforms the others ( IRSP , IRSU , IRSJ ) in terms of efficiency, in par-ticular on large-scale data, while maintaining equal accuracy. Moreover, our derived dissimilarity metric is more compre-hensive and accurate in capturing the clustering qualities in accordance with substantial empirical results.

We are currently applying the COS measure with IRSI to feature discretization, clustering ensemble, and other data mining tasks. We are also considering extending the notion of  X  X oupling X  for the similarity of numerical data. Moreover, the proposed concepts Inter-information Function and In-formation Conditional Probability for the information table have potential for other applications. This work is sponsored by Australian Research Council Grants (DP1096218, DP0988016, LP100200774, LP0989721), Tianjin Research Project (10JCYBJC07500), and QCIS (Cen-ter for Quantum Computation and Intelligent Systems). [1] A. Ahmad and L. Dey. A k-mean clustering algorithm [2] S. Boriah, V. Chandola, and V. Kumar. Similarity [3] D. Cai, X. He, and J. Han. Document clustering using [4] L. Cao, Y. Ou, and P. Yu. Coupled behavior analysis [5] S. Cost and S. Salzberg. A weighted nearest neighbor [6] G. Das and H. Mannila. Context-based similarity [7] G. Gan, C. Ma, and J. Wu. Data clustering: theory, [8] M. Houle, V. Oria, and U. Qasim. Active caching for [9] U. Luxburg. A tutorial on spectral clustering. [10] D. Wilson and T. Martinez. Improved heterogeneous
