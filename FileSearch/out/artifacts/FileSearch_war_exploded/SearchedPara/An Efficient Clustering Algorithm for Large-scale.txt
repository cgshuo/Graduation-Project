 The clustering of topic-related web pages has been recog-nized as a foundational work in exploiting large sets of web pages such as the cases in search engines and web archive systems, which collect and preserve billions of web pages. However, this task faces great challenges both in efficiency and accuracy. In this paper we present a novel clustering algorithm for large scale topical web pages which achieves high efficiency together with considerately high accuracy. I n our algorithm, a two-phase divide and conquer framework is developed to solve the efficiency problem, in which both link analysis and content analysis are utilized in mining th e topical similarity between pages to achieve a high accuracy . A comprehensive experiment was conducted to evaluate our method in terms of its effectiveness, efficiency, and quality of result.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Measurement, Experimentation Clustering, topical similarity, link analysis, content an alysis, topic model
Billions of web pages have been collected and preserved in search engines and web archive systems, such as Google and Internet Archive respectively. The clustering of topic -related web pages plays an important role in mining and utilizing such kind of large set of web pages, which could  X 
This work is supported by NSFC Grant 60773162 and Na-tional 863 Hi-Tech Grant 2007AA01Z100.
 improve the form of information expression and accelerate information extraction.

However, the clustering of large scale documents accord-ing to their topics has not been implemented satisfactorily because of the great challenges in both efficiency and accu-racy. A new framework is hence desirable for approaching high efficiency together with considerately high accuracy at the same time. We develop a divide and conquer framework which plays an essential role in achieving high efficiency. With that our approach has reduced the computation time from years to days.

The rest of the paper is organized as follows: Section 2 describes related work. Section 3 presents data preparatio ns and the framework of our algorithm, and then describes the algorithm in detail. The evaluation methodology and the experiment results are presented in section 4, and section 5 discusses about future works.
Clustering large scale datasets efficiently is always a hard problem. In order to improve efficiency, researchers nat-urally expect to parallelize the computation. Forman et al. [4] described a parallel clustering technique that work ed well for clustering data that was inherently geographicall y distributed. Xu et al. [9] raised a parallel density-based clustering algorithm, PDBSCAN, based on their former re-search, which performed well in speed-up, scale-up and size -up. Bekkerman [1] tried a novel parallel clustering approac h based on information-theoretic, and carried out experimen ts on the entire RCV1 collection on which there was little clus-tering carried. McCallum et al. [6] considered the efficient issue from the divide &amp; conquer perspective. Rooney et al. [7] clustered the RCV1 collection based on CDC [3]. The ap-proaches mentioned above demonstrated the scalability and effectiveness of their method for large scale datasets throu gh experiments on collection RCV1, which consists of 806,791 documents. To the best of our knowledge, experiment of document clustering carried out on larger datasets does not exist, such as order of magnitude of millions of datasets, while our approach has been evaluated on a set of 33 million articles which demonstrates its effectiveness. The problem we are going to solve can be described as: Given a set D of articles and a corresponding graph G rep-resenting the link relationship between articles in D , how to cluster articles in D into classes of topic-related articles efficiently with high accuracy?
Generally, articles and article graph can be obtained from the original set of web pages through a preprocessing step: we pick out the topical type web pages from this set, and then (1) extract their main content, and implement near-duplicate detection to get the text , and (2) extract the tem-poral information from these web pages, and refine it to get the pubication time by inference such as link analysis and near-duplicate detection. A link between two articles (an edge on the article graph) exists if there are hyperlinks between the corresponding topical type web pages (before near-duplicate detection) of these two articles, and the nu m-ber of the hyperlinks is the weight of this edge.

Based on these definitions and assumptions, given a set D of articles and the corresponding article graph G , the frame-work of our approach can be divided into two key phases as shown in Figure 1. An optional hierarchical agglomerative clustering phase could be performed upon the resulted top-ics if we want to get bigger topics or to improve the recall, which is not the focus of us and thus we don X  X  discuss further in this paper.
In this phase we partition the original large set of articles into subsets of perhaps topic-related articles by analyzin g the link relationships between articles. The thinking behi nd this method is that some links, such as the hyperlinks be-tween two news pages which describe a same news event or two sales pages which describe a same product, implicate human judgment of the potential topical relevance between them which is reliable intuitively and contributes to the ac -curacy.

Measurement of topical similarity : We measure the topical similarity between article a and its neighbor b in arti-cle graph based on the observation of three features: tempo-ral distance ( I t a,b ), link strength ( I s a,b ) and out-degree ( I
We assume that the topical similarity between two directly linked articles a and b is a comprehensive function of these three feature:
We further assume that the topical similarity has transi-tivity, that is if article a links to b directly and b links to c directly, a has some topical relevance with c even if a does not links to c directly. More generally, if there is a path p similarity between a and b contributed by this path is: r
The final topical similarity r a,b between two articles a and b is defined as the sum of the contributions made by each path from a to b . Because contributions of paths longer than certain steps are usually quite insignificant, in practice w e take certain steps (such as three) into account and the time complexity is O ( n ). We should notice that in our measure-
Partitioning Procedure : Based on the similarity mea-surement we designed a heap-based procedure to partition the original large set of articles into subsets of perhaps to pic-related articles. It involves three steps.

First, compute topical similarity scores. Given an article a , we just compute similarity scores r between a and each of the articles that can be reached from a in L steps on article graph, without regard to paths longer than L steps when computing r p . And then we save each article i ( r i,a &gt; 0) in a queue Q a in no ascending sequence of r i,a .

Secondly, choose seed articles, and regard each article as an initial subset. In our work we choose articles whose out degree equals to zero on the article graph as seeds. Third, partition each other articles into a certain subset. We define (1) the similarity between an article and a subset as the max similarity between this article and those hav-ing existed in this subset, and (2) the  X  X aximum expected similarity X  ( MES ) of a subset as the maximum similarity be-tween this subset and all the un-partitioned articles. Each time we pick out the overall maximum MES , add the corre-sponding article into the corresponding subset to be a child of the article which has the max similarity with it, and up-date MES . Iterate this operation until there is no article left alone. More concretely, this procedure can be implemented utilizing a priority queue (maximum heap).

There are two important properties of our procedure: (1) the overall time complexity is O ( n log s ), where n is the total number of articles and s is the number of seed articles, and (2) each subset is organized into a tree-structure, each nod e of which represents an article. The parent of article a is the article which is added into the subset prior to a and has the maximum similarity with a . There is some kind of locality of similarity between the nodes of the tree, which will play an critical role in the procedure of clustering phase.
In this phase we cluster the articles within each subset to get the final topics by analyzing the content of the articles. There are two key points in the designing of this phase as well: how to design the topical similarity measurement of articles X  content, and how to design the clustering procedu re based on this measurement.

Measurement of topical similarity : We take the se-mantic measurement, not the syntactic measurement such as TFIDF , as the measurement of topical similarity. More concretely, we utilize the probabilistic topic models (PTM ) [8]. There are two important distributions in PTM: (1) p w the distribution over words for topic t , which indicates which words are important for which topic, and (2) p t d , the distri-bution over topics for document d , which indicates which topics are important for which document. We can obtain these two distributions by training PTM.

Based on PTM, we developed a method to represent an article with a vector space of topics (here  X  X opic X  refers to the definition in PTM), and then used cosine distance on this space to measure the similarity between articles. The corresponding measurement is illustrated as the following formula, in which v t d is the weight of topic t in the topics vector space of article d . Two articles d i and d j are consid-ered to be topic-related if Similarity ( d i , d j ) exceeds some threshold.
 Similarity ( d i , d j ) =
In the formula above, v t d has the similar meaning with p in PTM but is computed in a more efficient way as illus-trated as the following formula , in which p t w represents the probability that word w is describing topic t , and tf w is the term frequency of word w . p t w can be computed heuristically from the matrix of p w t .
Clustering Procedure : We discovered that there is strong locality of topical similarity in each subset. As mentioned previously, each subset is organized into a tree-structure , each node of which represents an article. More concretely, we discovered that in the tree structure of a subset (1) if a node is not topic-related with its parent node, it is general ly not topic-related with the parent node of its parent node, and (2) if a node is not topic-related with its sibling node, it is generally not topic-related with the children nodes of this sibling node. This characteristic helps us to avoid com -pletely pair-wise comparisons when clustering articles in a subset, which plays an essential role in improving efficiency while maintaining high accuracy.

Based on the locality of topical similarity in subsets, we transform the clustering procedure within a subset into pru n-ing and merging operations on a tree: (1) prune operation . Compare the root node of a sub-tree with it X  X  parent node (by the measurement designed in this section), if they are not topic-related, prune the sub-tree from the tree, and (2) merge operation . For the pruned sub-trees whose root nodes are sibling nodes mutually, sort them in ascending order of the publication time of their root nodes. And then compare the root nodes of two sub-trees picked out in order each time, if they are topic-related, merge the second sub-tree to the first sub-tree by making the root node of the second one be a child node of root node of the fist one. We iterate these two operations from the deepest layer to the root of the tree to complete the clustering. Because each node just needs to compare with its parent node and sibling nodes, the time complexity of this algorithm is O ( n (log n ) 2 ) on average and O ( n 2 ) at worst case, where n is the number of articles in a sub-set.
Before the implementation of clustering, a preprocessing step was implemented on these 430 million pages to extract articles and to build article graph as described in section 3 .1. As a result, we get (1) a set D of 38 million articles (in fact we have got 55 million articles from the 430 million pages through extraction and near-duplicate detection, but ther e are 17 million articles which do not link to any other article and thus they are not applicable for our approach and are not considered here), and (2) a corresponding article graph G which represents the link relationships between these 38 million articles and has an average out-degree of 25 per node .
Then we implemented our clustering algorithm. After the partitioning phase, 460 thousand no-single (contains more than one articles) subsets were obtained. After the cluster -ing phase, these subsets were refined into 1 million non-sing le topics which covered 33 million articles, and 5 million sin-gle topics each of which contained only one article. These subsets and topics form the dataset for our evaluation.
We compare the overall precision, recall and efficiency of our algorithm with the two well-known algorithms K-means (KMA) and hierarchical agglomerative clustering (HAC) in this subsection.

Firstly, we evaluate the precision and recall. The def-initions of these two criteria in our work may have some difference with other works. Here we refer to the clustering results of algorithms as  X  X lasses X  while refer to the cluste r-ing results of human judgments as  X  X opics X . Suppose there was a set of articles which were clustered into m classes by algorithm and n topics by human judgment. Given a class C i and a topic T j (0  X  i  X  m, 0  X  j  X  n ), let | C i | and | T be the number of articles in C i and T j respectively.The pre-cision, recall and F -value of class C i to topic T j are defined as:
The precision and recall of the algorithm to topic T j are defined as the precision and recall of class C t , which has the largest F -value to T j in all classes, to topic T j :
The precision and recall of the algorithm to the whole set of articles are defined as the weighted means of the precision and recall of the algorithm to all topics:
We describe the contenders as following: LCA which is the proposed algorithm in this paper, KMA-1 whose number of the classes was same as the number of topics resulted by hu-man evaluation,KMA-2 which adjusted the number of the resulted classes and picked out the one which reached the highest F-value, and HAC-1 and HAC-2 whose topical sim-ilarity measurement and the meaning were similar as what was used in KMA. The experiment results are illustrated in Table 1. Our LCA algorithm achieved close or even better precision and recall than KMA and HAC algorithms. For we could not evaluate the whole 33 million articles by hands, we are not able to get the real recall. In this sense, what we really care about is the relative performance between these algorithms and this is why we use a relative recall here.
 Rivals LCA KMA-1 KMA-2 HAC-1 HAC-2 Precision 0.92 0.92 0.83 0.69 0.87 R-Recall 1 0.42 0.77 0.94 0.91 F-value 0.94 0.50 0.71 0.71 0.86
Secondly, we evaluate efficiency. LCA took about 450 hours to cluster 33 million articles into 1 million classes o n a cluster of four boxes, each of which has 2 CPUs (Intel Xeon 3.0GHz) and 6G main memories, while under the same hard-ware and system conditions as LCA, it may cost hundreds years to complete the first iteration for KMA and HAC.
The effect of the partitioning phase is to partition the orig-inal large set of articles into smaller subsets of perhaps to pic-related articles. We made two evaluations for this phase:
Firstly, we made statistics on the size (the number of arti-cles in a subset) of the resulted subsets after this phase. Th e statistic result seems to be a power-law distribution. More concretely, the subsets each of which contains more than 1000 articles account for only 0.3% of all subsets while they cover more than 88% of all articles, and the subsets each of which contains more than 100000 articles account for only 0.01% of all subsets while they cover more than 50% of all articles. The largest subset contains about 2 million artic les.
Secondly, we evaluated the precision of subsets. The preci-sion of the subsets decreases when the size of them increases . When the size is less than 100, the precision is higher than 0.74, while when the size increase to 1000 the precision de-crease to lower than0.5.
In the evaluation of the clustering phase we care about two things: (1) whether our topical similarity measurement (the topics-space based cosine) is truly better than TFIDF based cosine, and (2) whether our clustering strategy de-signed based on the locality of similarity improves the accu -racy considerately when achieving high efficiency.
Firstly, we compared the two topical similarity measure-ments. To implement the topics-spaced based cosine, we trained an LDA topic model [2] by utilizing Gibbs sampling method [5] to extract topics from a Chinese corpus. The re-sults are illustrated in Table 2: topics-space based cosine has achieved both higher precision and higher recall than TFIDF based cosine. The reason of the improvements may be the same as what we have mentioned in subsection 3.3: the top-ical similarity between articles X  content is more a kind of semantic similarity than a kind of syntactic similarity, an d the topics-space based cosine is able to measure the semanti c similarity well.

Secondly, we evaluated the performance of our clustering procedure designed based on the locality of topical similar -ity. The results are illustrated in Table 3. Our locality Table 2: Comparison of TFIDF and Topics-space Contenders Precision Relative-recall TFIDF based cosine 0.65 0.57 Topics-space based cosine 0.94 1 Contenders Precision Relative-recall HAC 0.91 1
Locality algorithm in LCA 0.94 0.96 algorithm achieved close performance on precision and re-call to HAC. The precision is 3 percents higher than that of HAC, while the recall is 96% of that of HAC. This result may prove to some extent that our procedure designed based on the locality of similarity did not sacrifice the potential improvement on effectiveness significantly when achieving a high efficiency. It could further prove that the locality of similarity did exist in the subsets.
The effectiveness of our algorithm may be improved if (1) some hyperlinks in the navigating pages could be used to improve the connectivity of the article graph, or (2) more effective methods could be developed to represent the link-relationship and articles X  content better. [1] R. Bekkerman and M. Scholz. Data weaving: Scaling [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [3] V. Dobrynin, D. Patterson, and N. Rooney. Contextual [4] G. Forman and B. Zhang. Distributed data clustering [5] T. L. Griffiths and M. Steyvers. Finding scientific [6] A. McCallum, K. Nigam, and L. H. Ungar. Efficient [7] N. Rooney, D. W. Patterson, M. Galushka, and [8] Steyvers and Griffiths. Probabilistic topic models. In T. [9] X. Xu, J. Jager, and H. P.Kriegel. A fast parallel
