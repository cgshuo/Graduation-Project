 1. Introduction
Retrieving sentences that are relevant to a given user need is a problem that has been addressed in the literature from tion answering, novelty detection and text summarization ( Murdock, 2006 ).

Many of the approaches proposed in the SR literature are direct adaptations of document retrieval methods. These meth-ods are usually based on matching query and sentence terms. Nevertheless, sentences are very short pieces of text and, therefore, there are usually very few matching terms. Some researchers tried to alleviate this problem by applying query expansion. However, we take here an alternative approach focused on combining query-independent evidence (related to the sentences) with sentence retrieval scores, leading to effective estimations of the relevance of sentences. More specifi-cally, we consider opinion-related information, named entities and the sentence length as query-independent features. etc.) are likely more important than objective statements related to the topic. For instance, given a query  X  X  partial birth  X  tains explicit references to certain African locations, might be more important than another sentence such as  X  X  At least 82 were killed and more than 1,700 injured, officials said as dawn broke Saturday  X  X . Finally, the use of sentence length as a query-independent feature may also be helpful because long sentences usually provide more information than short ones and, therefore, they are more likely relevant (some short sentences act solely as connectors between the pieces of the discourse).

Summing up, the set of features considered in our study are: (a) opinion-based features , including the subjectivity nature of sentences (a sentence may be objective or subjective) and (b) named entities features , i.e. names of persons, locations, organizations, etc. (c) sentence length , i.e. the number of terms in a sentence, ignoring stopwords.

The features described above are considered in isolation or in combination. This helps to understand the configuration of query-independent features that performs the best. In order to incorporate these sentence features as query-independent evidence into SR models, we follow a formal methodology based on kernel density estimation ( Craswell, Robertson, Zara-goza, &amp; Taylor, 2005 ). We show that the combination of these query-independent features with state of the art SR scores yields to important improvements in performance with negligible computational costs at retrieval time.
The rest of the paper is organized as follows. Section 2 comments on some related work. The methodology followed to dent features and the software utilized to estimate them. Section 5 reports the experiments and analyzes their outcomes. The paper ends with Section 6 , where we expose the conclusions of our study. 2. Related work
In the sentence retrieval literature, most of the proposals consist of addressing the SR problem by adapting document nored. Sentences are short pieces of information. Most sentence retrieval methods are based on a regular matching between query and sentences. However, sentences that do not contain query terms may be relevant for a query. Query expansion is a mechanism that tries to address this vocabulary mismatch problem, which is rather severe in sentence retrieval. The study conducted by Losada in Losada (2010) analyzes carefully different query expansion methods applied to sentence retrieval. This included well-known term selection techniques, such as those based on regular pseudo-relevance feedback and Local
The paper concludes that the ideal expansion configuration depends strongly on the quality of the initial query. Evolved expansion methods, based on selective feedback, were studied by Jaleel et al. in Abdul-Jaleel et al. (2004) . They are more stable than standard feedback methods but require training data. On the other hand, other authors resort to lexical expan-approach may not be appropriate because noisy terms are likely introduced into the expanded query ( Voorhees, 1993 ) and, moreover, a large terminological resource is not always available.

Given the inconsistent effects on performance and the time requirements involved at query time, query expansion is
We claim that the estimation of relevance could be more accurate by using query-independent information. In the literature, there is not much evidence about the combination of query-dependent and query-independent information to estimate rel-evance for SR problems. We consider some opinion-based features and study whether or not they help to improve sentence we analyze whether the combination of features of the same or different nature improves performance over individual incorporations.

The use of opinions for sentence retrieval was also explored by Kim et al. (2004) . For the TREC 2003 and 2004 opinion topics, relevant opinion sentences were recognized using opinion-bearing word lists. However, the authors assumed that opinion-based methods are only effective for opinion topics. We demonstrate here otherwise. Furthermore, the performance achieved by the methods described in Kim et al. (2004) was not higher than the performance of state of the art methods.
Unfortunately, the experiments reported in Kim et al. (2004) cannot be replicated here because they are based on collecting manually opinion-bearing words from resources such as WordNet. Since the manual lists are not publicly available we based patterns and, next, they filtered out redundant sentences by applying a novelty detection method. This is a way to study the impact of opinion-based features on novelty detection. Our work differs significantly from the study conducted by Li and Croft because we focus here on sentence retrieval (rather than novelty detection) and, furthermore, we apply a methodology that is totally different to the methods suggested in Li and Croft (2008) .

In Fern X ndez and Losada (2009) we conducted a preliminary study on the incorporation of opinion-based features into sentence retrieval baselines. In this work we expand this study in several ways. First, we consider here a wider range of query-independent features that includes not only opinion-based features but also other kinds of features. Second, we study how to combine individual sentence features and how this affects performance. Third, we analyze here the effect of opinion-or opinions.
 2009 ). As a matter of fact, this parameter-free method has been shown to perform at least as well as the best performing empirically tuned and trained SR models based on BM25 or Language Models (LMs) ( Losada &amp; Fern X ndez, 2007; Fern X ndez &amp; Losada, 2009 ). Therefore, in this paper we will use this method as our SR baseline.

Sentence retrieval is a particular case of passage retrieval. In Kaszkiel and Zobel (2001) , Kaszkiel and Zobel proposed methods to estimate the relevance of documents by aggregating query-passages scores. Some of the best performing query-passage matching functions are also vector-space measures. Although we evaluate here SR (rather than document re-trieval) this further supports our choice to select a vector-space measure as our baseline.

In the next section we explain the formal methodology adopted to incorporate query-independent features into SR models. 3. Combining content match and query-independent scores We adopt the definition of sentence retrieval done in the TREC 2002, 2003 and 2004 Novelty Tracks ( Harman, 2002; ing sentences in a document base that are relevant to the user need. To meet this aim, a set of potential relevant documents must be retrieved first. Next, the set of sentences that are relevant to the query needs to be found.
The SR methods proposed in the literature are often based on a regular similarity function between the query and every of-the-art SR models is still weak. This problem is especially aggravated when handling short queries because of the little overlap between queries and sentences. The vocabulary mismatch problem arises severely in SR and, therefore, the models should not be solely based on query-sentence matching scores.

Along this work, we use tfisf as our SR baseline. 1 This matching function is defined as follows: where tf t , Q and tf t , S are the number of occurrences of the term t in the query Q and sentence S , respectively; sf of sentences where t appears, and n is the total number of sentences in the collection.

As argued above, SR methods need to include additional evidence besides content match evidence. A natural way to ad-dress the problem consists of defining query-independent weights that modify sentence retrieval scores. To this aim we ap-ply a feature X  X  Log Odds Estimation (FLOE), which is a formal methodology designed by Craswell et al. (2005) . FLOE is a density analysis method that models the transformation needed in order to add query-independent features into existing retrieval models. It is a formal and powerful method that suggests good functional forms to transform feature values into relevance scores, without assuming independence between the feature and the baseline. In web document retrieval, FLOE has been used to define transformations for BM25 in order to include features such as PageRank, indegree or URL length.
FLOE is, therefore, a natural choice to combine tfisf with query-independent features in our SR scenario. 3.1. Score adjustment under independence
In this section we present the score adjustments under independence as proposed by Craswell et al. (2005) , but adapted
The probability score in a log-odds way that preserves the rank order with respect to a query ( Q ) can be expressed as:
We can now consider that sentences have two components: a content match component ( M ) and a query-independent variable associated to the PageRank of the document). Given these two components, Eq. (2) can be separated into two addi-tive scores:
A standard matching function (e.g. tfisf) can play the role of the first addend ( Craswell et al., 2005 ):
If components I and M were independent, then and, therefore, the adjustment under this independence assumption would be:
Because the number of relevant sentences is very small compared to the number of sentences in the collection, the indepen-dence-based adjustment can be approximated as: 3.2. Feature X  X  logs-odd estimator
The adjustment described above would perform well if the baseline and the query-independent feature would be actually so, the indep score would overstate the score boost of the feature (double-counting). The Feature X  X  Logs-Odd Estimator (FLOE), described next, is a method designed by Craswell et al. (2005) that avoids double counting by analyzing the levels of the feature in the baseline.

FLOE is a method that computes the probability estimates for a set of sentences. Given r , the number of known relevant sentences for a given query, FLOE takes the top r retrieved sentences from the baseline for each query and computes the probability estimates for this set as we describe next.
 defined as: obtain the part of the feature weight that is not captured by the baseline: Therefore, FLOE corrects the behavior of the baseline to achieve the overall adjustment suggested by indep. In Craswell et al. (2005) , FLOE was used to combine BM25 and features such as PageRank, indegree, URL Length and Click
Distance. However, rather than applying Eq. (9) directly, a number of functional forms to approximate this adjustment were proposed and successfully evaluated. In our work, we will demonstrate that FLOE  X  X  X s is X  X  (i.e. the score in Eq. (9) with no further adjustment) can directly help to significantly enhance the SR performance. 4. Query-independent features
In this section we explain the query-independent sentence features used in our study. 4.1. Opinion-based features
We hypothesize that SR methods can be further improved by leading the retrieval process towards opinionated sen-tences. For instance, in domains such as news, a given statement from a politician might be more relevant than the objective information about where this speech was given.

Opinion mining (also known as Sentiment Analysis, Subjectivity Analysis, Review Mining or Appraisal Extraction) deals of challenging goals including opinion detection, identification of opinion holders and their authority, estimation of the ionated nature. For instance, sentences can be labeled as objective or subjective. Additionally, subjective material can be notation (about the death penalty). The research problems involved in these estimations are currently being addressed from cessive TREC Blog Tracks ( Macdonald et al., 2007, 2009; Ounis et al., 2006, 2008 ) were created to explore the information seeking behavior in the blogosphere. These tracks are regarded as standard benchmarks to help researchers in designing new efficient and effective opinion retrieval algorithms.

Our intuition is that users tend to be mostly interested in subjective information, especially when they look for news arti-related to the topic. We think that, by extracting opinionated information in sentences, we could improve the estimation of the relevance of the sentences.

We extracted opinion-based features associated to every sentence with a highly effective opinion mining software named OpinionFinder ( Wiebe &amp; Riloff, 2005 ). OpinionFinder was originally used for retrieval tasks in He, Macdonald, and Ounis (2008), Santos et al. (2009) in the context of the TREC Blog Track. In these studies, the authors use Opinion-
Finder as a tool to identify the subjectivity of sentences, which is used to improve the estimation of relevance in blog search.

OpinionFinder is a state of the art subjectivity detection system ( Pang &amp; Lee, 2008a, 2008b ) that processes texts and labels sentences (and parts of sentences) following their subjectivity and polarity nature. The text is first processed using part-of-speech tagging, named entity recognition, tokenization, stemming and sentence splitting. Next, using a dictionary-based method, a parsing module builds dependency parse trees where subjective expressions are identified. guage versions of articles from the world press. The data comes from a variety of countries and publications and cov-ers many different topics. Among them, 535 texts from this collection have been manually annotated with respect to subjectivity. The test set used to adjust OpinionFinder consists of 9289 of the sentences in these texts. The unanno-tated text corpus consists of 298809 sentences from the world press collection, which is distinct from the annotated corpus. These classifiers have been shown to perform very well with several opinion corpora ( Wiebe &amp; Riloff, 2005 ).
Observe that we did not run any additional training to OpinionFinder classifiers (i.e. we use the default trained con-figuration explained above).

OpinionFinder classifies sentences as subjective or objective (or unknown if it cannot determine the nature of the sen-tence). Subjective sentences express private states, which are internal, mental or emotional states, including speculations, but  X  X  X isaster X  X  is a term with negative polarity).

In this paper we work with the following set of opinion-based features: the subjective nature of the sentence ( F ( F ); the number of negative terms in a sentence ( F neg ); and the number of opinionated terms in a sentence ( F number of either positive or negative terms. 4.1.1. Indep and FLOE adjustments Now, we analyze the indep and FLOE adjustments considering each of the opinion-based features explained above. opinion-based feature. We show the graphs considering one of our training collections ( Soboroff &amp; Harman, 2003 )). With the features F neg shape preserving interpolation. 3
The probabilities shown in Fig. 1 let us predict the behavior of indep and FLOE adjustments. With F retrieve enough subjective sentences and, therefore, promoting subjective sentences should improve performance. Moreover, the independence assumption does not hold for this feature.

It is harder to find clear trends with the polarity features ( F not help to estimate the relevance of sentences as much as F evance and, because F opt is the sum of F pos and F neg , results obtained with the F
F
The indep score represents the adjustment suggested under the independence assumption, i.e. baseline and features are independent. In Fig. 2 we show the indep curves. For any of the opinion-based features explained above, indep suggests assigning more weight to sentences with opinionated material ( F to those sentences that do not contain any opinionated information ( F
FLOE corrects the behavior of the baseline to achieve the overall adjustment suggested by indep. In Fig. 3 we represent tences containing opinionated information and decreasing scores to the rest of sentences. Note that, in the case of F
FLOE adjustment is erratic (as argued above, we do not expect any benefit from adjustments based on F 4.2. Features based on named entities
Named entities (NEs) are proper names, such as names of persons, locations, organizations, etc. We hypothesize that the introduction of named entities into existing sentence retrieval models may help to estimate the relevance of sen-tences. These named entities are usually core components of a given text and user information needs might depend strongly on people names (  X  X  X ow many albums does Madonna have? X  X  ), locations (  X  X  X lympic Games in Barcelona X  X  ) or organi-features and incorporate them into existing sentence retrieval methods by applying FLOE. Observe that, regardless of the presence of named entities in the query, sentences themselves might be indicative and representative of some entities.
This might be an important aspect to study the relevance of sentences. In fact, the use of these named entities as query-independent features was considered by Li and Croft in Li and Croft (2008) . However, their work was focused on using these features for novelty detection.

Name Entity Recognition (also known as entity identification or entity extraction) is a subtask of Information Extraction (a framework for building probabilistic models to segment and label sequence data, Lafferty et al., 2001 ) provided by the tions and organizations names) and classifies them as names of entities.

In our work, different NE evidences are considered as query-independent features: the number of person X  X  names in a sentence ( F pers ), the number of location X  X  names in a sentence ( F and the overall number of named entities (person, location and organization names) in a sentence ( F 4.2.1. Indep and FLOE adjustments
Given the NE features explained above, we analyze now the indep and FLOE adjustments. trends are similar in any of the collections.

Trends are similar for all NE features: p ( I =0)&gt; p ( I =0 j R ) and p ( I ) moting sentences with named entities might be a way to improve performance. Additionally, p ( I =0 j T )&lt; p ( I = 0) and are more sentences with named entities in the retrieved set compared with the collection as a whole).
In Fig. 5 we show the adjustment suggested under the independence assumption. Given the feature F ment suggests to increase the weight to sentences with at least a person name, but where value of F ( F pers P 1 and F pers &lt; 5), and to remove some weight to sentences with no person names. For the remaining features, the adjustment suggests to increase the weight of sentences that contain at least one named entity ( F to remove some weight to sentences with no named entities.

However, the independence assumption does not hold here either because the baseline already retrieves sentences that contain named entities. In Fig. 6 we show the FLOE adjustment given the NE features. Unlike indep, FLOE adjustments sug-gest, in general, to remove some weight to sentences that contain at least a named entity (except in the cases F
F org P 5) and to increase the weights to sentences with no named entities. These trends are the opposite as indep because, as set ( p ( I =0 j R )&gt; p ( I =0 j T ) and p ( I j R ) 6 the behavior of the SR baseline. 4.3. Sentence length feature
In document retrieval, document length has been repeatedly used by several length-based normalizations, leading to length corrections, such as those implemented by BM25, do not work well in SR. In fact, as we will show in Section 5 , the
SR. In this respect, we include here the impact of sentence length as another feature in our study. 4.3.1. Indep and FLOE adjustments sentence length feature ( F len ). In this case, p ( I )&gt; p ( I j R ) with 0 tion of relevant sentences tends to contain longer sentences on average compared to the collection. On the other hand, val and relevance patterns.

In Fig. 8 we show the adjustment under the independence assumption, and in Fig. 9 we show log adjustment. The indep adjustment suggests to increase the weight to those sentences that contain at least 9 terms ( F and to reduce the weight to shorter sentences. In contrast, FLOE suggests to remove weight to sentences longer than 15 and giving more weight to sentences that contain less than 15 terms.

In the next section we show the performance of the SR baseline after applying these adjustments, given the different features. 5. Experiments 2004 ( Soboroff, 2004 ). These test collections supply relevance judgments at sentence level for each topic.
The TREC novelty datasets were built as follows. Each collection contains 50 topics. In TREC 2002, these topics were ob-each topic, a ranked set of documents was obtained by NIST by using a regular retrieval system. The aim of TREC 2002 and
TREC 2003 was to find relevant sentences within relevant documents and, therefore, the ranked set contained only relevant documents. In contrast, TREC 2004 mixed relevant and non-relevant documents. Sentence-tagged documents were supplied of relevant sentences in TREC 2003 and TREC 2004 is 38% and 19%, respectively. In TREC 2002 only 2% of the sentences were estimated as relevant. Given this marginal amount of relevant sentences, this collection is not appropriate for estimating opinion-based features. The statistics of subjective material constructed upon this data would not be reliable. As a matter of fact, the characteristics of the TREC 2002 Novelty Track data have been criticized in the past and TREC 2003 and TREC 2004 are regarded as more robust sentence retrieval and novelty benchmarks ( Li, 2006 ).

Furthermore, the TREC 2003 and TREC 2004 topics are of two classes: events (e.g.  X  X  Find details about the bombing at the methods with an assorted set of information needs. This helps to understand when opinion-based features are useful (e.g.
Our preprocessing of documents and queries consisted simply of stopword
TREC 2003 and testing with TREC 2004, and vice versa. In this way, we can check how robust the methods are with respect to the test collection.

Performance was measured in terms of P@10 and MAP (mean average precision). Statistical significance was estimated using the two-sided t-test (confidence levels of 95% and 99%, marked with  X  and , respectively). 5.1. Is the baseline competitive enough?
As argued above, we consider tfisf as our sentence retrieval baseline. In this section, we provide empirical evidence to demonstrate that this is a very competitive baseline. We compare here tfisf against other popular sentence retrieval meth-ods, such as Okapi-BM25 ( Robertson et al., 1994 ) and a Language Modeling (LM) approach (with Dirichlet smoothing) based on the Kullback X  X eibler divergence (KLD), as described in Larkey et al. (2002) .

BM25 depends on three parameters: k 1 , which controls term frequency; b , which is a length normalization factor; and k
On the other hand, the LM approach using KLD with Dirichlet smoothing is dependent on l , which indicates the amount of smoothing. We experimented with the following values of l : 1, 5, 10, 25, 50, 100, 250, 500, 1000, 2500, 5000 and 10000.
The results are reported in Table 1 . Statistically significant differences between the baseline and BM25 or LMs with KLD at confidence levels of 95% and 99% are indicated with  X  and , respectively. The BM25 and KLD models have been optimized performs at least as well as tuned BM25 or KLD. These results demonstrate that tfisf is an effective method that performs similarly to an optimal BM25 model. Unlike BM25, tfisf is parameter-free, which is an added value. This supports strongly
Fern X ndez (2007) , where tfisf was compared against similar SR models, obtaining equivalent results. 5.2. Experiments with opinion-based features the training collection to obtain query-independent adjustments such as those shown in Fig. 3 . Next, these adjustments are applied in the test collection. Given a sentence S and a query Q , the final score assigned to the sentence is where R is the set of relevant sentences, I is the feature, and T is the top r ranked sentences (tfisf).

Table 2 and Fig. 10 report the performance of this method for the test collections given the opinion-based features. The best results are bolded in the table.

The adjustment modeled by FLOE leads to improvements in performance and many improvements are statistically sig-nificant. This is a remarkable achievement because FLOE was unable to produce directly (i.e. without further adjustments) significant improvements for document retrieval ( Craswell et al., 2005 ).

The results can be summarized as follows. First, the number of positive terms in a sentence, F tical significant improvements. As argued in Section 4.1 , we already expected this outcome for F clear distinction between p ( F pos ) and p ( F pos j R ). Second, the models incorporating the F the baseline for both performance measures but the improvements are only statistically significant with MAP in TREC 2004. significant improvements in both P@10 and MAP. 5.2.1. Other functional forms inspired by FLOE
In the previous section we showed that opinion-based features help significantly to retrieve relevant sentences. This po-sitive outcome motivated us to go further and test other functional forms inspired by FLOE. Observe that the adjustments suggested by FLOE (e.g. Fig. 3 ) might be less trustworthy in the regions of the plot with fewer examples. For instance, the number of sentences with more than four opinionated terms is much smaller than the number of sentences with one or two opinionated terms. This means that the right-hand end of the F the forms of the FLOE curves can be easily approximated by simple functions such as lines. These functional forms might generalize better than the original FLOE adjustment and, therefore, they would avoid overfitting. We therefore propose in this section other alternatives to modify the relevance weight with opinion-based evidence. Given a query-independent fea-ture I , we tested the following functions: tuning w to optimize performance. 11 The test results are shown in Table 3 and Fig. 11 . For F tion X  X  results because this feature is binary and, therefore, all methods are virtually equivalent.
The relative merits of F subj , F neg , F pos and F opt remain the same: F
The experiments reported so far demonstrate that opinion-based features are important components that should not be disregarded when retrieving sentences. As a matter of fact, the performance of a state of the art sentence retrieval model improves very significantly when opinion-based features are included (e.g. F 5.2.2. Post-combination checking
Having shown that these opinion-based features can individually produce benefits in terms of performance, it is natural to consider their combination. Rather than approaching this in an ad hoc way, we resort again to FLOE. After adding a given feature, FLOE can predict whether or not another feature is still useful. Since F cision, we took the strongest model designed so far (column in bold in Table 3 , tfisf + linear ( F tences retrieved, we analyzed whether or not F neg , F pos This is shown in Fig. 12 .

Given a retrieved set produced by tfisf + linear ( F subj closely the ideal indep curve. This makes that the resulting FLOE X  X  adjustment is flat around 0, indicating that no further not help to retrieve additional relevant material. 5.2.3. Event and opinion topics
Having demonstrated that opinion-based features help to retrieve relevant sentences, we analyze here the behavior of the sentence retrieval methods with different types of topics. As argued above, some TREC topics concern events while the remaining topics focus on opinions about controversial subjects such as cloning, gun control, and same-sex marriages. ken down by topic type.

In Table 4 and Fig. 13 we compare the baseline and the strongest opinion-based model, tfisf + linear ( F of topics. The improvements achieved by the opinion-based model are consistent across query types. This clearly demon-strates that our models are very effective when handling opinion-oriented needs but they also lead to important improve-ments for event queries. For instance, given the event topic #N2 ( X  X  Cloning of the sheep Dolly  X  X ), the tfisf + linear ( F this model takes only into account standard matching heuristics. This illustrates a limitation of tfisf because, as shown in
Fig. 2 , the proportion of subjective sentences in the relevant set is higher than the proportion of subjective sentences in ranked sentences obtained with tfisf and tfisf + linear ( F jective terms (as tagged by OpinionFinder) are underlined. Observe that sentences with a high number of subjective terms $ 100,000 reward in its investigation [ ... ] X  X  is in rank #2 in the tfisf + linear ( F by tfisf. Similarly, objective sentences tend to be demoted by tfisf + linear ( F ing. This behavior of the opinion-based model is clearly good for topics such as N45, which is an event topic but demands  X  X  X eactions of praise or condemnation of the murder, and the victim X  X  views on access to abortion X  X .
The interest in subjective material might be either a particular feature of news datasets, such as most TREC collections, or a more general circumstance that holds in other domains. This will be subject to further research. 5.3. Experiments with features based on named entities
We test now the incorporation of features based on named entities into tfisf. In Table 7 and Fig. 14 we show the perfor-mance in the test stage after incorporating these features (with the adjustment suggested by FLOE after training). In general, NE features do not help to improve performance. The adjustment suggested by FLOE is not beneficial here.
There are few cases with improvements in performance but, anyway, those were not statistically significant. Anyway, we formance of these methods for the different NE features 12
In general, tfisf is not outperformed by any of these NE-based variants in terms of P@10. With MAP, F performs the best, regardless of the method (linear, log or step). F perform well with one of the collections.

Anyway,theimprovementswithrespectto thebaselinearemodestand,usually,statisticallyinsignificant.This maybehap-pening because most queries contain named entities as explicit query terms. Therefore, the retrieval of sentences with named entities might be already guaranteed by the content match score (tfisf). In Table 9 we show the number of queries with and without named entities. There is a high number of queries containing named entities. Although FLOE still suggests some cor-rection over the baseline (see Fig. 3 ) it seems that this adjustment is not beneficial in terms of retrieval performance. 5.4. Experiments with the feature based on sentence length
Sentence length was shown in the literature to be a helpful factor to improve standard document retrieval methods. How-ever, regular length corrections have been demonstrated to not work well in SR. For instance, BM25 is optimal when sentence length correction is ignored. However, FLOE could suggest alternative length normalizations that work properly in additionally, the performance obtained after incorporating sentence length into tfisf with the empirical methods.
The performance of tfisf adjusted by FLOE does not outperform the standard tfisf method. With respect to the empirical cases and, particularly, the gains are more remarkable for MAP.
 To sum up, we have improved our SR baseline by considering sentence length as a query-independent component.
Although FLOE does not yield to improvements in performance over the baseline, we were able to outperform tfisf with an empirical method (linear or log). This might be because the FLOE adjustment for length ( Fig. 9 ) tends to be flat around 0 (for most of the length values) and, therefore, its effect is negligible. Furthermore, FLOE might be less trustworthy in the regions of the plot with a high number of terms (e.g. there are few sentences longer than 30 terms, as indicated by p ( I ) plot in Fig. 7 ) and, thus, empirical approximations might be more reliable in such situations. 5.5. Combining opinion and sentence length features
In this subsection we explore the combination of different types of features. So far, we have designed high performing sentence retrieval methods based on opinion estimation and sentence length. We will now assess the quality of combination further improved by combining features of different kind.

We do not consider here combinations involving named entity features because, as argued in Section 5.3 , they lead to modest improvements in performance. Therefore, in order to study the combination of features, we only consider those fea-tures with higher impact on the estimation of sentence relevance: sentence length and opinion-based features. The approach followed here is to test sentence length on top of the best opinion-based sentence retrieval method, i.e. tfisf + linear ( F
First, we incorporate sentence length as a feature by using FLOE:
As reported in Table 11 and Fig. 17 (first two columns), after applying FLOE, improvements are statistically significant with respect to the SR baseline in terms of performance.

Besides FLOE, we also experimented with other functional forms such as linear and log transformations of the sentence length. Again, we do not apply the step function because sentences contain always at least a query term. The performance of these methods is also reported in Table 11 and Fig. 17 (last two columns).

Empirical methods lead to significant improvements in performance. The combination of F that the effectiveness of sentence retrieval can be further improved not only by incorporating opinion-based features but also by combining them with sentence length weights. In Appendix A we separately analyze event and opinion topics and study the performance after incorporating the subjectivity feature combined with the sentence length. We found that trends are similar for both types of queries (overall, we obtain statistically significant improvements with respect to the baseline, for both event and opinion topics).

To further check that this combination is better than the model tfisf + linear ( F ison between the best opinion-based model (tfisf + linear ( F that, in terms of P@10, there is no need to include a sentence length factor. In contrast, a sentence length weight helps to improve performance in terms of MAP.

This evaluation demonstrates how powerful opinion-based features can be when used as a priori evidence for estimating the relevance of the sentences. The high performing baseline, tfisf, has been significantly enhanced by including formally stantial (11 X 29%) and robust across collections.

A very interesting research line is to apply learning to rank (L2R) techniques to combine these and other features for sen-tence retrieval purposes. To the best of our knowledge, there has been no much activity in learning to rank for sentence re-trieval and, given our discovery of effective query-independent features, it makes sense to further study the combination of multiple features based not only on subjectivity, length and named entities but also on grammar, sentence positions, etc. 6. Conclusions
In this paper we studied the impact of query-independent information on a sentence retrieval problem. We considered different opinion-based, named entity-based and length-based features and included them into high performing sentence retrieval models as query-independent weights. To this aim, we followed FLOE, a formal methodology to include properly query-independent evidence into existing models. Most of the models proposed in this paper (either the ones derived di-rectly from FLOE or those ones inspired by FLOE) outperform significantly a competitive sentence retrieval baseline.
With opinion-based features, we provided experimental evidence to show that the subjectivity of a sentence, the number of terms with negative orientation and the number of opinionated terms are sentence features that help to estimate rele-vance. Notably, the model that combines a regular baseline with the subjectivity of a sentence is very encouraging and boosts performance (improvements from 9 to 26%). Named entities do not help to improve the estimation of sentence rel-evance. This happens because queries contain usually explicit references to named entities and, therefore, the retrieval of sentences with named entities is already accounted for the matching between the query and sentences. We also demon-strated that sentence length is a feature that helps to improve the retrieval estimation. Additionally, we also showed that sentence length can be combined with opinion-based features to further improve effectiveness.

The use of query-independent features in sentence retrieval is a novel contribution and the results reported here opens up subjectivity is the feature that makes standard models be highly outperformed, we will study different retrieval scenarios trying to understand when and why subjective content is more amenable to users. We believe that opinionated content might be valuable not only in sentence retrieval but also in other classical information retrieval problems. Acknowledgment This research was funded by Ministerio de Ciencia e Innovaci X n under Project TIN2010-18552-C03-03. Appendix A. Combining subjectivity and sentence length features: performance results for event and opinion topics Tables A.13, A.14, A.15 , Fig. A.19 .
 References
