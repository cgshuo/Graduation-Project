
This paper presents a simple but powerful extension of the maximum margin clustering (MMC) algorithm that optimizes multivariate performance measure specifically defined for clustering, including Normalized Mutual In-formation, Rand Index and F-measure . Different from previous MMC algorithms that always employ the error rate as the loss function, our formulation involves a multivariate loss function that is a non-linear combination of the individual clustering results. Computationally, we propose a cutting plane algorithm to approximately solve the resulting optimization problem with a guaranteed accu-racy. Experimental evaluations show clear improvements in clustering performance of our method over previous maximum margin clustering algorithms.

Recently, maximum margin clustering (MMC) has at-tracted considerable interests in the data mining and ma-chine learning communities [18], [19], [20], [23], [26], [27], [24]. The key idea of MMC is to extend the maximum margin principle of support vector machines (SVM) to the unsupervised learning scenario. Given a set of data samples, MMC performs clustering by labeling the sam-ples such that the SVM margin obtained is maximized over all possible cluster labelings. Recent studies have demonstrated its superior performance over conventional clustering methods.

Existing maximum margin clustering algorithms all directly adopt the formulation of support vector machines . Specifically, the (regularized) hinge loss is minimized with respect to both the data labels and separating hyperplane. In a classification problem, the classification error is an obvious performance measure, and the hinge loss, which upper bounds the classification error, is thus well justified to be used as the loss function. However, its justification for use in clustering is not so clear. Often, the objective of clustering is not on learning the exact labels. Instead, a good clustering solution should minimize the intra-cluster variance and maximize the inter-cluster variance. The most suitable and well-defined performance measures for clustering include the Normalized Mutual Information [28], Rand Index [13] and F-measure [12]. Empirical studies show that MMC optimized for the error rate may have suboptimal performance in terms of the Rand Index (or NMI, F-measure ) [26]. Indeed, despite the battery of clustering algorithms that have been proposed, we are not aware of any clustering algorithm (not necessarily based on MMC ) that can directly optimize clustering-specific per-formance measures. So, a natural question is: Why do we minimize a loss function that is defined for classification, instead of directly minimizing those loss functions that are specifically defined for measuring clustering performance? the context of ranking . Consequently, researchers in the ranking community have proposed algorithms that directly optimize the ranking performance measures such as the mean average precision ( MAP ) [21] and the normalized discounted cumulative gain ( NDCG ) [1], [10]. In this paper, we reformulate maximum margin clustering so that loss functions that are defined for clustering can be explicitly optimized in the objective function. However, different from the error rate, these clustering-specific loss functions cannot be decomposed linearly into a sum of loss functions over the individual examples. Instead, they are non-linear combinations of the individual clustering results.
 ing problem, the resulting optimization problem contains an exponential number of constraints. In this paper, we propose an efficient algorithm for solving the resulting optimization problem based on the cutting plane algorithm [8], [17]. As will be shown in the sequel, one can construct a nested sequence of successively tighter relaxations of the original problem, and each optimization problem in this sequence can be efficiently solved as a quadratic programming problem by using the constrained concave-convex procedure ( CCCP ) [16]. A key issue to the success of this approach is that the most violated constraint in each cutting plane iteration can be found efficiently. Experimen-tal evaluations on toy and real-world data sets show that the proposed method outperforms existing maximum margin clustering algorithms in terms of these clustering-specific performance measures.

The rest of this paper is organized as follows. In Section 2, we first review the basics of maximum margin clustering and performance measures widely used in clustering. The formulation of MMC with clustering-specific loss functions is presented in Section 3. Section 4 presents the experimen-tal results on several toy and real-world data sets, followed by some concluding remarks in Section 5.

As briefly introduced in Section I, the key idea of maximum margin clustering ( MMC ) is to extend the maximum margin principle from supervised learning to unsupervised learning. In the two-cluster case, given a set of examples  X  = { x 1 ,..., x  X  } , MMC aims at finding the best label combination y = {  X  1 ,..., X   X  } X  X  X  1 , +1 }  X  such that an SVM trained on { ( x 1 , X  1 ) ,..., ( x  X  , X  yield the largest margin. Although it is indeed possible to extend our approach to the multi-class case [20], [26], the extension is not simple and for ease of presentation we focus on simple two class clustering in this paper. Binary clustering is still a reasonably well-motivated problem, as binary clustering algorithms can play an important part in hierarchical clustering.

Computationally, MMC solves the following optimiza-tion problem [26]: where the data samples  X  are mapped into a high (pos-sibly infinite) dimensional feature space using a possibly nonlinear function  X  ( x ) , and by using the kernel trick, this mapping could be done implicitly. Specifically, we define the kernel matrix  X  formed from the inner products of feature vectors, such that  X   X  X  X  =  X  ( x  X  )  X   X  ( x  X  ) transform the problem such that  X  ( x ) only appears in the inner product. However, in those cases where kernel trick cannot be applied, if we still want to use a nonlinear kernel, it is possible to compute the coordinates of each sample in the kernel PCA basis [14] according to the kernel  X  . More directly, as stated in [2], one can also compute the Cholesky decomposition of the kernel matrix  X  =  X  X  X  X  X  , and set  X  ( x  X  )=(  X  X  X , 1 ,...,  X  X  X , X  notational simplicity, we will use x instead of  X ( x ) in the sequel.

The last constraint in (1) is often called the class balance constraint, and  X &gt; 0 is a constant controlling the class imbalance. More precisely, this constraint is a relaxation of the integer constraint  X   X   X  used in maximum margin clustering [25], [26] and spectral clustering [15]. Specifically, this constraint prevents the trivially  X  X ptimal X  solution that assigns all patterns to the same class and thus achieves  X  X nfinite X  margin. It also avoids the unwanted solution of separating a single outlier or a very small group of samples from the rest of the data.
In this section, we review three performance measures that have been widely used in clustering, namely, Nor-malized Mutual Information (NMI) , Rand Index and the F-measure .

The NMI measures the clustering quality from an information-theoretic perspective. In the context of clus-tering, let  X  = {  X  1 ,  X  2 ,...,  X   X  } be the set of clusters obtained on the  X  samples, and  X  = {  X  1 ,  X  2 ,...,  X   X  } the set of true class labels. Moreover, let  X  (  X   X  ) ,  X  (  X  and  X  (  X   X   X   X   X  ) be the probabilities of an example in cluster  X   X  , class  X   X  , and the intersection of  X   X  and respectively. Then, NMI is defined as [28]: where is the mutual information between  X  and  X  , and  X  (  X  )=  X  is the entropy of  X  . A higher NMI value indicates better clustering quality.

On the other hand, the Rand Index views clustering as a series of decisions, one for each of the 1 2  X  (  X   X  1) pairs of examples in the data set  X  . Ideally, a clustering algorithm should assign two examples to the same cluster if and only if they are similar. Thus, there are four possible scenarios for each assignment. A true positive (TP) decision assigns two similar examples to the same cluster, and a true negative (TN) decision assigns two dissimilar examples to different clusters; while a false positive (FP) decision assigns two dissimilar examples to the same cluster, and a false negative (FN) decision assigns two similar examples to different clusters. The Rand index (RI) then measures the percentage of correct decisions, as: As with NMI , higher values indicate higher quality.
A potential problem with the Rand index is that it gives equal weights to false positives and false negatives. How-ever, in many real-world applications, having dissimilar examples in the same cluster is often less undesirable than separating similar examples into different clusters. To alleviate this problem, the F measure introduces a user-defined parameter  X  to penalize false negatives more strongly than false positives. Specifically, it is defined as where  X   X  1 , and are the precision and recall, respectively.
 As can be seen from (1), MMC (as in the standard SVM) uses the hinge loss, which is an upper bound of the classification error. However, the objective of clustering is often not on learning the exact labels, and therefore the error rate may not be suitable. In this section, we consider a direct optimization of the performance measures for clustering, including the Normalized Mutual Information , Rand Index and F-measure .

However, one advantage of using the hinge loss as the loss function is that it can be decomposed linearly as a combination of loss functions over all the examples. Specifically, let  X (  X  (  X  ) , y ) be the loss function that mea-sures the disagreement between hypothesis  X  (  X  ) and the learned labels y . Then, However, the aforementioned performance measures for clustering cannot be decomposed this way. For example, to compute NMI , we need to have complete information of the entire clustering result. In fact, these measures are nonlinear combinations of the individual clustering results. To employ the performance measures defined for clustering as the loss function in maximum margin clustering ,we need to design algorithms that finds the hypothesis  X  (  X  ) which minimizes  X (  X  (  X  ) , y ) over the entire data set
In this paper, we present a new formulation of maximum margin clustering that directly optimizes multivariate per-formance measures defined for clustering (Section III-A). Specifically, we build upon the approach used by [5], [4] for optimizing multivariate performance measures for classification. Unlike the supervised problem, however, maximum margin clustering with multivariate loss func-tion is formulated as a non-convex integer programming problem and requires a substantially extended algorithm, which we describe in Section III-B.

Conventionally, maximum margin clustering defines hy-pothesis  X  as a univariate function from a single example x to its corresponding label  X   X  X  X  1 , +1 } :  X  :  X   X   X  . This hypothesis is suitable for loss functions that can be decomposed as linear combinations over the examples, such as the error rate. However, for loss functions involving nonlinear combinations of the individual clustering results, this decomposition is no longer possible. Therefore, to employ those performance measures defined specifically for clustering as the loss function in maximum margin clustering , we need to consider a multivariate hypothesis  X   X  that maps a tuple of examples  X  x =( x 1 ,..., x  X  )  X   X   X  =  X   X  X  X  X  X  X   X  to a tuple of labels  X   X  =(  X  1 ,..., X   X  )  X  { X  1 , +1 }  X  .

To implement this multivariate mapping, a generalized linear model is employed to decode the top-scoring output  X   X  w (  X  x )= as in the conventional formulation of MMC , and  X  is a function that returns a feature vector measuring the match between tuples  X  x =( x 1 ,..., x  X  ) and  X  y =(  X  1 ,..., X  For maximum margin clustering , we define  X  as
The multivariate prediction rule  X   X  allows us to refor-mulate maximum margin clustering to enable the inclusion instead of an example-based loss function  X (  X  ( x ) , X  ) is only suitable for decomposable loss functions. For a non-negative multivariate loss function  X   X  , we can reformulate maximum margin clustering as the following optimization problem: min Recall that the optimal label vector  X  y is given by since each  X   X  can be optimized individually. Thus, we have  X   X   X  X  1 ,..., X  } :  X   X   X  = sgn( w  X  x  X  ) , and the corre-sponding value for w  X   X (  X  x ,  X  y ) is  X   X  = sgn( w  X  x  X  ) into problem (10), performing maximum margin clustering with the multivariate loss function  X   X  can therefore be equivalently formulated as the indicator function) selects the optimal  X  y out from the summmation.

The major difficulty with problem (12) lies in the fact that there are an exponential number of constraints involved. For each admissible tuple  X  y  X   X  X  X  1 , +1 }  X  a corresponding constraint (13) must be satisfied. The massive amount of constraints renders solving the problem exactly very time demanding. Therefore, the algorithm we propose in this paper targets to find an approximate solution to problem (12) with user-defined precision  X  . That is to say, the approximate solution we seek satisfies all the constraints in problem (12) up to precision  X  , where  X  can be arbitrarily small. Technically, the algorithm finds a small subset of constraints from the whole set of constraints in problem (12) that ensures a sufficiently accurate solution. Specifically, we employ an adaptation of the cutting plane algorithm [8], [6], [17], [5], [26], where we construct a nested sequence of successively tighter relaxations of problem (12). Computationally, the cutting plane algorithm keeps a subset  X  of working constraints and computes the optimal solution to problem (12) subject to the constraints in  X  . The algorithm then adds the most violated constraint for problem (12) into  X  . In this way, a successively strengthening approximation of the original maximum margin clustering with the multivariate loss function is constructed by a cutting plane that cuts off the current optimal solution from the feasible set [8]. The algorithm stops when no constraint in (12) is violated by more than  X  .
 is presented in Algorithm 1. There are two steps involved in the main procedure: solving problem (12) subject to the constraint subset  X  and finding the most violated constraint. We will elaborate on these two steps in the following sections.
 Algorithm 1 Cutting plane algorithm for maximum margin clustering with multivariate loss function.

Initialize: Constraint subset  X =  X  . repeat until the newly selected constraint  X  y  X  is violated by no more than  X  . cutting plane algorithm, we need to solve problem (12) to obtain the optimal hypothesis  X   X  under the current working constraint subset  X  . Although the objective function in (12) is convex, the constraints are not, and this makes problem (12) difficult to solve. Fortunately, the constrained concave-convex procedure (CCCP) is just designed to solve those optimization problems with a concave-convex objective function under concave-convex constraints [22], [16].
 is a method for solving non-convex optimization problem whose objective function could be expressed as a differ-ence of convex functions. It can be viewed as a special case of variational bounding [7] and related techniques including lower(upper) bound maximization(minimization) [11], surrogate functions and majorization [9]. While in [22] the authors only considered linear constraints, Smola et al. [16] proposed a generalization, the constrained concave-convex procedure ( CCCP ), for problems with a concave-convex objective function under concave-convex constraints.

Assume we are solving the following optimization problem [16] where  X   X  and  X   X  are real-valued convex functions on a vector space  X  and  X   X   X   X  for all  X  =1 ,..., X  . Denote by  X  1 {  X , z } ( z  X  ) the first order Taylor expansion of where  X  z  X  ( z ) is the gradient of the function  X  at z .For non-smooth functions, it can be easily shown that the gradient  X  z  X  ( z ) would be replaced by the subgradient [3]. Given an initial point z 0 , the CCCP computes z  X  +1 from z by replacing  X   X  ( z ) with its first-order Taylor expansion at the following relaxed optimization problem The above procedure continues until z  X  converges, and Smola et al . [16] proved that the CCCP is guaranteed to converge.

For problem (12), both the objective function and class balance constraint are convex. Moreover, con-straint (13) is, although non-convex, the difference of two convex functions. Notice that while  X  function of w . To use the CCCP , we need to calculate the subgradients : Given an initial point w 0 , the CCCP computes w  X  +1 from w  X  by replacing in constraint (13) with its first-order Taylor expansion at w , i.e., = By substituting the above first-order Taylor expansion into problem (12), we obtain the following quadratic program-ming (QP) problem: The above QP problem can be solved in polynomial time. Following the CCCP , the obtained solution ( w , X  ) from this QP problem is then used as ( w  X  +1 , X   X  +1 ) , and the iteration continues until convergence. The algorithm for solving problem (12) subject to the constraint subset  X  is summarized in Algorithm 2. As for its termination criterion, we check if the difference in objective values from two successive iterations is less than  X  % (which is set to 0 . 01 in the experiments).
 Algorithm 2 Solve problem (12) subject to constraint subset  X  via the CCCP.

Initialize w 0 . repeat until the stopping criterion is satisfied. 2) The Most Violated Constraint: In each iteration of the cutting plane algorithm, we need to locate the most violated constraint. Here, the feasibility of a constraint is measured by the corresponding value of  X  . Therefore, the most violated constraint is the one that would result in the largest  X  , i.e., where  X  y is the optimal label vector in (11). The tuple of labels  X  y  X  can take an exponential number of different values, and this renders an exhaustive search over all  X  y  X   X  { X  1 , +1 }  X  infeasible. Luckily, as in [5], the performance measures defined for clustering can all be computed from the contingency table (Table I).

In the following, we show how the performance mea-sures for clustering can be calculated from the contingency table.
Table I. Contingency table for binary cluster-ing.
Normalized Mutual Information: Using the definition of Normalized Mutual Information in (2), it can be com-puted from the contingency table as  X  X  X  X  = The corresponding loss function is then defined as  X   X   X  X  X  X  =1  X   X  X  X  X  .

Rand Index: Unlike NMI , the computation of Rand index is more complicated as it considers whether each pair of examples are correctly grouped together or not. Recall that from Table I, there are two clusters obtained, one for  X   X  =1 and the other for  X   X  =  X  1 . For the cluster corresponding to  X   X  =1 ,  X  patterns have true label  X  =1 while  X  patterns have true label  X  =  X  1 . Let pattern pairs in this cluster, of which  X   X  2 +  X   X  2 pairs are true positives (i.e., pattern pairs that should be and are in the same cluster), while  X  X  X  pairs are false positives (i.e., patterns pairs that are in the same cluster but should be in different clusters). Similarly, for the obtained cluster corresponding to  X   X  =  X  1 ,  X   X  2 +  X   X  2 pairs are true positives, while  X  X  X  pairs are false positives. Hence, and Similarly,  X  X  X  counts the number of pattern pairs that should be and are in different clusters, and it is obvious that Analogously, we also have Hence, the Rand Index in (5) can be computed from the contingency table as: Similar to the NMI , we define the corresponding loss function as  X   X   X  X  X  =1  X   X  X  X  .
 compute the precision and recall . Using (7) and the equa-tions for  X  X  X , X  X  X , X  X  X , X  X  X  above, these can be computed from the contingency table as: The  X   X  score can therefore be computed using (6), as where  X  =  X   X  2 +  X   X  2 +  X   X  2 +  X   X  2 . In particular, note that (19) is different from the computation of the  X   X  score in [5], as we are considering example pairs in this clustering context. The corresponding loss function is defined as  X   X   X  X  X  X  X  X  X  X  X  X  X  X  =1  X   X   X  . Thus, all the performance measures designed for clustering can be directly computed from the contingency table.
 ning all different contingency tables. Since the elements in the contingency table satisfies the conditions:  X  +  X  = #(  X  =1) and  X  +  X  =#(  X  =  X  1) , there are only  X  (  X  2 ) different contingency tables for a binary clustering problem with  X  examples. Therefore, the loss function introduced in Section II-B can take at most  X  (  X  2 ) different values. Specifically, we denote the loss function value that is computed from the contingency table as  X   X (  X , X , X , X  ) . Our algorithm for locating the most violated constraint iterates over all possible contingency tables. Let  X   X   X   X  X  X  X  X  X  be the set containing all tuples  X  y  X  X  X  1 , +1 }  X  that results in contin-gency table (  X , X , X , X  ) [5]. As the value of  X   X (  X , X , X , X  ) the same for all tuples in  X   X   X   X  X  X  X  X  X  , maximization over can be calculated as The above objective function is a linear combination over the individual examples. Therefore, the solution  X  y  X  X  X  X  X  X  be computed elementwise. Specifically, for a particular contingency table (  X , X , X , X  ) , denote the set of examples with  X  =1 as  X  + and the set of examples with  X  =  X  1 as  X   X  . The maximum value of when the  X  examples from  X  + with the largest values of w  X  x  X  are classified as positive, and the  X  examples from  X   X  with the lowest values of w  X  x  X  are classified as negative. Finally, the overall argmax can be computed by maximizing over the stratified maxima plus their constant loss  X   X (  X , X , X , X  ) in  X  (  X  2 ) time [5].
 straints from the whole set of constraints in problem (12) and calculates an approximate solution. The following theorem characterizes the accuracy of the solution returned by the proposed algorithm.
  X &gt; 0 , the cutting plane algorithm returns a point ( w , X  ) for which ( w , X  +  X  ) is feasible in problem (12). the newly selected constraint satisfies the inequality  X   X  y  X   X   X   X  : w  X  [ X (  X  x ,  X  y )  X   X (  X  x ,  X  y  X  )]  X  If this holds, then since the newly selected constraint is the most violated one, all the other constraints will also satisfy inequality (20). Therefore, if ( w , X  ) is the solution returned by the proposed algorithm, then ( w , X  +  X  ) is a feasible solution to problem (12).
 wants the error rate of the best hypothesis to be, and it can thus be used as a stopping criterion.
 with loss function being the error rate as MMC uni , and the formulation proposed in this paper as MMC multi . It would be interesting to study the relationship between these two formulations of maximum margin clustering . Specifically, we have the following theorem: function, MMC uni arises as a special case of MMC multi . being error rate, define  X   X =2  X  X  X  X  X  X  X  [5]. Specifically, we can show that for every w , the smallest feasible  X  and  X   X  X  X  X  X  X  X  =  X   X  X  X  X  / 2 , the two optimization problems have the same objective value and an equivalent set of constraints. of the maximum margin clustering with multivariate loss function algorithm on several data sets. All the experiments are performed with MATLAB 7.0 on a 1.66GHZ Intel Core TM 2 Duo PC running Windows XP with 1.5GB main memory.
 wide range of properties: ionosphere, digits, letter and satellite (from the UCI repository), ringnorm 1 and 20 newsgroup 2 . For the digits data, we follow the experi-mental setup of [26] and focus on those pairs (3 vs 8, 1 vs 7, 2 vs 7, and 8 vs 9) that are difficult to differentiate. For the letter and satellite data sets, there are multiple classes and we use their first two classes only [26]. For the 20-newsgroup data set, we choose the topic rec which contains autos , motorcycles , baseball and hockey from the version 20-news-18828. We preprocess the data in the same manner as in [29] and obtain 3970 document vectors in a 8014-dimensional space. Similar to the digits data set, we focus on those pairs ( autos vs. motorcycles (Text-1) , and baseball vs. hockey (Text-2) ) that are difficult to differentiate. A summary of all these data sets is in Table 2.

Besides our MMC multi algorithm, we also implement some other competitive algorithms and present their results for comparison. Specifically, we use K-Means (KM) and Normalized Cut (NC) [15] as baselines, and also com-pared with the Maximum Margin Clustering algorithm proposed in [26] which achieves state-of-the-art cluster-ing accuracy among all MMC algorithms. For notational convenience, we denote this algorithm as MMC uni .For k-means , the cluster centers are initialized randomly. For NC , the implementation is the same as in [15], and the width of the Gaussian kernel is set by exhaustive search from the grid { 0 . 1  X  0 , 0 . 2  X  0 ,..., X  0 } , where  X  range of distance between any two data points in the dataset. Moreover, for MMC uni , the implementation is the same as in [26]. For MMC uni and MMC multi ,a linear kernel is employed and parameters are selected via grid search using the same grid. For k-means , MMC uni and MMC multi , we present the results averaged over 50 random runs.

Table 3 shows the results for Normalized Mutual In-formation on the various data sets. As can be seen, our algorithm MMC multi outperforms other competing algorithms on almost all data sets. Similarly, Table 4
Table III. Normalized Mutual Information com-parisons.
 digits1v7 0.975 0.008 1.00 1.00 digits2v7 0.801 0.075 1.00 1.00 digits3v8 0.714 0.069 0.816 0.861 digits8v9 0.562 0.002 0.845 0.860 ionosphere 0.072 0.164 0.122 0.197 ringnorm 0.199 0.220 0.207 0.422 letterAvB 0.323 0.219 0.695 0.703 also demonstrates the advantage of MMC multi over other clustering algorithms in terms of the Rand Index . For the digits1v7 0.995 0.504 1.00 1.00 digits2v7 0.940 0.550 1.00 1.00 digits3v8 0.904 0.545 0.945 0.961 digits8v9 0.835 0.500 0.956 0.962 ionosphere 0.564 0.626 0.599 0.640 ringnorm 0.635 0.653 0.642 0.676 letterAvB 0.706 0.644 0.873 0.895 F-measure ,weset  X  =1 . 5 and provide the clustering results in Table 5. Again, it is clear that MMC multi achieves higher  X   X  . Clearly, experimental results shown in Table 3,4,5 demonstrate the advantage of MMC multi in clustering accuracy over competing algorithms. Moreover, while MMC uni may sometimes be inferior to NC (say, e.g. on ionosphere and ringnorm in table 3), MMC multi is always the best.

One potential concern on MMC multi is that its im-provement on clustering accuracy might come from sacri-digits1v7 0.994 0.503 1.00 1.00 digits2v7 0.940 0.549 1.00 1.00 digits3v8 0.904 0.544 0.945 0.961 digits8v9 0.835 0.498 0.956 0.962 ionosphere 0.594 0.652 0.627 0.672 ringnorm 0.689 0.685 0.688 0.724 letterAvB 0.706 0.644 0.897 0.900 ficing time efficiency. Here, we compare the CPU time of MMC multi (using  X   X   X  X  X  ) with the maximum margin clustering algorithm in [26], which is so far the most efficient MMC algorithm. As shown in Figure 1, the speed of MMC multi is comparable to that of MMC uni . Indeed, MMC multi even converges faster than MMC uni on several data sets. However, unlike MMC uni which can only employ the error rate as the loss function, MMC multi can handle much more complicated loss functions. rectly optimizes multivariate performance measure defined for clustering, including Normalized Mutual Information, Rand Index and F-measure . To tackle the exponential number of constraints involved in the optimization prob-lem, we employ cutting plane algorithm to efficiently solve the problem. Preliminary theoretical analysis of the algorithm is provided, where we show that the formulation for maximum margin clustering proposed in this paper generalizes the conventional MMC . Experimental evalu-ations show clear improvements in clustering performance of our method over previous maximum margin clustering algorithms.

In the future, we plan to extend the current method to multi-cluster problems. Note that while a direct multi-cluster extension is possible, it is complicated by the fact that it might involve an exponential number of contingency tables while finding the most violated constraint. There-fore, an efficient strategy for searching in this space of contingency tables is required.

This work is supported by projects (60835002) and (60675009) of the National Natural Science Foundation of China.

