 Roman Garnett rgarnett@cs.cmu.edu Yamuna Krishnamurthy ykrishna@andrew.cmu.edu Xuehan Xiong xxiong@andrew.cmu.edu Jeff Schneider jschneide@cs.cmu.edu Richard Mann rmann@math.uu.se Uppsala Universitet, Uppsala 751 06, Sweden In many real-world classification scenarios, it can be much easier to collect input data than to observe asso-ciated labels, which could require relatively expensive human action. For this reason, considerable research in semi-supervised and active learning has considered how to construct models exploiting unlabeled data and also how to intelligently request unknown labels to achieve a given goal as cheaply as possible.
 The bulk of active classification research has considered obtaining labels to maximize some measure of predic-tive power or model accuracy. Here, we consider two distinctly different problems. In the first, which we call active search , the members of one particular class are deemed important and are to be located as quickly as possible. Many real-world problems are of this form; fraud detection, drug discovery, and product recom-mendation are just a few examples. In the second task, which we call active surveying , we seek to determine the portion of a dataset belonging to a particular class. Targeted opinion polling is an important and natural real-world problem of this type.
 Typical model-based active classification strategies are not appropriate for either of these problems. The consequence of catching a fraudster, discovering a new cancer drug, or selling a product can be measured in monetary terms. Learning an accurate model, on the other hand, is only useful if it can help us locate more items. Indeed we could make observations that give very high performance on either task and nonetheless produce a model that is uncertain or even completely inaccurate on large swathes of the domain.
 Rather than proposing heuristics to adapt typical active learning algorithms to these problems, we will instead begin  X  X rom the beginning X  and analyze these problems using Bayesian decision theory. We will first define natural utility functions for each problem and then derive the optimal policies.
 The active search problem has been previously de-scribed (Garnett et al., 2011); here we extend that preliminary work with two contributions. First, we will prove that a myopic approximation to the opti-mal active search policy can perform arbitrary worse than an even slightly less-myopic approximation. Our second contribution is more practical. In the general case, the optimal search policy requires time that grows exponentially in the number of unlabeled points. Here we show how for a certain class of classifiers (including k -nn ), we may identify and discard points that can-not possibly be optimal with trivial extra computation. In practice, this can increase the efficiency of the al-gorithm by orders of magnitude and allow us to use policies that might not otherwise be possible. The rest of this paper is arranged as follows. In Sections 2 and 3, we formally describe the problems at hand. In Section 4, we provide and discuss the Bayesian optimal policies for these problems. We proceed by proving a result about the potential benefit of using the optimal active search policy with increasingly long horizons. In Section 6, we discuss a branch-and-bound technique to limit the search space required for the optimal search policy. Finally, we evaluate our methods empirically. Suppose we have a finite set of elements X , { x i } and an identified subset R X  X  , the members of which we will call targets . We consider the following problem. Suppose we do not know which members of X belong to R a priori, but can successively request binary ob-servations y ,  X  ( x  X  X ) , for an unlabeled element x  X  X  . We wish to actively select a sequence of queries to maximize a given utility function.
 For the active search problem, we define the utility of a set of observations D , ( x i ,y i ) to be the number of targets found: This simple expression naturally captures the spirit of the problem as defined above. For the active surveying problem, we define the utility of a set of observations to be the variance in our induced probability distribution over the cardinality of R : Again this expression encapsulates the goal of surveying: polls with smaller margins of error are to be preferred. Active learning is a mature field with a large associated body of literature (Settles, 2010). In the active binary-classification problem, the chosen objective is usually related to properties of the associated probabilistic model. Examples include generalization error (Zhu et al., 2003) and optimality criteria related to the Fisher information, such as A-optimality (Schein &amp; Ungar, 2007). One of the simplest active learning techniques for binary classification is uncertainty sampling (Lewis &amp; Gale, 1994), which successively requests the label for the point x  X  with the greatest posterior variance: x  X  , arg min Both objectives considered in this paper are unusual in an active-learning context as far as the authors know. A problem similar to active search that has been considered is the active discovery of previously unseen classes (He &amp; Carbonell, 2008). Weitzman (1979) considers an active search problem where there is no dependence between outcomes and derives the optimal policy. The problem as defined there can also be seen as a Bayesian multi-armed bandit, and the optimal policy can also be recovered via a Gittins index (Gittins et al., 2011). Here we consider the case where the  X  X rms X  of the bandits are correlated, which is a so-called restless bandit problem.
 There is a long history of statistical research investi-gating the selection of respondants when conducting a survey. A particular focus of such research is identify-ing and correcting selection bias , where certain people are more likely to be selected for a survey than others (Berger, 2005). Here we take a completely different ap-proach: we actively and intentionally bias our selection of points to query, choosing those that we believe will increase our understanding of the class proportion as much as possible. In the context of polling a social network, we can reasonably expect that opinions are in some way correlated under a notion of  X  X imilarity X  or  X  X loseness X  in the network. For this reason, polling people with many connections throughout the network might be more fruitful than polling people who are rel-atively isolated. We embrace and leverage this notion of correlated opinions and influence in our design. As mentioned previously, our approach to the active search and surveying problems will be motivated by Bayesian decision theory. This will require selecting a classification model that provides the posterior prob-ability of a point x belonging to R conditioned on previously observed data D , Pr ( y = 1 | x, D ) . We will assume that this model is given a priori ; the decision theoretic analysis does not depend on its nature. Without loss of generality, we will assume that at the onset we will be allowed a fixed number of queries t. In applications where the cost of obtaining a label is high, it is the total cost of the queries that limits their number, rather than the quantity of unlabeled points. We now derive the policy for deciding the locations of our queries, which will entail successively calculating the expected utility of each of the remaining unlabeled points then observing the label for the point that with maximal expected utility. At time i , then, we will observe the label for the point We begin by considering the case when we are allowed to make exactly one more query and will then address the general case. Suppose that we have already made t  X  1 observations D t  X  1 . To select our final observation, we calculate the expected utility of a candidate point x , marginalizing out the unknown value of y t . For active search, the expected utility is E u ( D t ) | x t , D t  X  1 = P y u ( D t ) Pr( y t = y | x Because u ( D t  X  1 ) does not depend on x t , the optimal decision x  X  t is therefore the point with the largest poste-rior probability of being a target. This makes intuitive sense: with only one evaluation remaining, there is no possible benefit to explore, and we might as well make a purely greedy last try.
 For active surveying, the expected utility is
P The optimal decision x  X  t is therefore the point with the smallest expected variance of p ( card R | D t ) . This is a bit more opaque than the active search expression above, but is still intuitively reasonable.
 Given the optimal policy for selecting x t , we now con-sider the problem of choosing the location of the second-to-last point x t  X  1 . When making our decision in this case (as well as with any other x i with i &lt; t ), the problem becomes more difficult because we must now contemplate the possible consequences of our choices and how they will impact our future decisions. The mechanical manifestation of this remark is that during the calculation of the expected utility for the two-step lookahead case, we must integrate out the unknown location of the final observation x t , as well as its label: Note, however, that the integral over x t can be evalu-ated trivially because p ( x t |D t  X  1 ) is simply  X  ( x where  X  is the Dirac delta function X  X hat is, given the value of y t  X  1 , the location of the last choice x t is de-terministic and known from our discussion above. To evaluate the two-step expected utility at a point x t  X  1 , we therefore sample over the unknown value y t  X  1  X  { 0 , 1 } ; for each possible value of y t  X  1 , we find the optimal last observation x  X  t given that fictitious observation as described above. Note that sampling over y  X  t is not required in the search case. We may repeat the procedure described above recur-sively to calculate the expected ` -step lookahead utility of choosing a point for any `  X  t , allowing us to oper-ate on any horizon. We note that some authors would equivalently discuss the preceding analysis in terms of Bellman X  X  equation and Markov decision processes ( mdp s); our choice of presentation is purely stylistic. 2 As noted in (Garnett et al., 2011), the optimal policies for both of these problems in general requires running time O (2 card X ) ` . For lookahead more than a few steps into the future, this procedure can become daunt-ing due to the sampling required. This is a common issue in sequential Bayesian decision problems. One typical way to address this problem is to approximate exact inference by shortening our horizon (Jones et al., 1998; Osborne et al., 2009). For timestep t  X  m with m &gt; ` , we myopically pretend that there are only ` ob-servations remaining and choose x t  X  m by maximizing the ` -step lookahead expected utility. We will address this issue further in Section 6 and show how in some reasonable cases we can restrict the exponential search space required to find the ` -step optimal decision. In this section we will discuss the behavior of the ` -step optimal search policy versus the m -step policy, for ` &lt; m . Let us first consider the behavior of the two-step policy versus the simple greedy one-step policy. Notice that the two-step policy allows us to make deci-sions that do not maximize the posterior probability of observing a target at the current step. Instead, we might choose to explore a region where the probability of immediate reward is lower, but where there is a chance of discovering more targets overall during the next two evaluations. We will give a very simple exam-ple that demonstrates the effect of this tradeoff in the active search case. Figure 1 shows a three-point space. The two connected points have the same label; they are known to either both be targets or both be nontargets, with marginal probability of their being targets  X  . The point on the right is independent of the others and has probability of being a target  X  &gt;  X  . Consider being allowed two label queries with the goal of locating as many targets as possible. We may calculate the ex-pected performance of the one-and two-step policies directly. The one-step policy will always choose the right point first and then will be compelled to choose either of the left points, with expected final utility  X  +  X  . The expected two-step utility of the left points is each 2  X  + (1  X   X  )  X  , and the expected two-step utility of the right point is  X  +  X  . The difference in two-step expected utility between either left point and the right point is  X  (1  X   X  ) &gt; 0 ; therefore one of the left points will always be chosen, and the two-step policy will outperform the one-step greedy policy on average for any value of  X  . This example demonstrates the sort of nontrivial de-cisions that the optimal policy can make X  X t can be better to explore a region where labels are expected to be highly correlated, even when the probability of being  X  X ucky X  and finding many targets is much smaller than the current most likely single point. A welcome side effect of this behavior is that when such a decision is made, we learn about the labels of the points in the chosen region even if we do not observe a target. Such evaluations can therefore be advantageous by our having improved the overall quality of our probabilistic model, despite this goal not having been specified at any step during our derivation of the optimal policy. We can extend the ideas in the above example to prove that, in the case of active search, increasing our hori-zon can always improve performance by any arbitrary degree. Let P , ( X  , 2  X  , Pr ) be a (discrete) probability space on  X  . Given P , we will denote the expected util-ity of the ` -step-lookahead policy after t evaluations with E D u ( D ) | `,t, P . We may prove the following. Theorem 1. Let `,m  X  N + ,` &lt; m . For any q &gt; 0 , there exists a P and t such that that is, the m -step active-search policy can outperform the ` -step policy by any arbitrary degree.
 Proof. As in the simple example above, the key to the argument is that ` -step lookahead cannot differentiate between a  X  X lump X  of correlated points of size ` and one of size greater than ` . To formalize this concept, define a ( k, X  ) -clump , denoted k  X  , to be a collection of k discrete points that all have the same label, with marginal probability of being all targets  X  . Consider applying the m -step lookahead policy for querying t labels on the space The policy is easy to analyze in this case. After no evaluations, every point in the domain has the same expected m -step utility by symmetry. After observing that point, either a clump of targets will have been dis-covered (with probability  X  ), or a clump of nontargets. In the former case, the remaining t  X  1 evaluations will all be spent querying the remaining points in the selected clump, because they will all have maximal expected utility for all horizons. In the latter case, a point in another unobserved clump will be chosen, and the response to the outcome will be the same. Given this, we may calculate:
E D u ( D ) | m,t, P = We now augment P with t copies of `  X  , with  X  &gt;  X  : It is trivial to show that the ` -step utility of a point in a `  X  is greater than the ` -step utility of a point in a t  X  . The form for each is of the same form as (2) , and their difference may be calculated directly; it is (1  X   X  ) `  X  (1  X   X  ) ` +  X   X  1 (1  X   X  ) `  X  1  X   X   X  1 (1  X   X  ) Despite the fact that the t -clump has more potential targets, the ` -step lookahead policy greedily chooses the more immediately fruitful ` -clump.
 With this, we may find an upper bound for E D u ( D ) | `,t, P . Consider build a string S (initially empty) as follows. Sample r from U (0 , 1) . If r &lt;  X  , append ` 1s to S ; otherwise, append a 0. Repeat a k times. At termination, the probability of a character in S being a late the ` -step method by stopping when length ( S )  X  t and taking the first t characters. When we stop, the expected number of 1s in the first t can obviously not be greater than the expected number in S , which has length at most ( t + `  X  1) : The final component of the proof is showing that even if  X  &lt;  X  , the m -step lookahead expected utility of a point in one of the t -clumps is greater than the m -step lookahead expected utility of a point in one of the ` -clumps. One can in fact prove the following, which generalizes the situation in Figure 1 (consider as  X   X   X  ).
 Lemma 1. Let `,m,k  X  N + , ` &lt; m  X  k , and  X   X  (0 , 1) be given. Then there is a  X  &lt;  X  such that the expected m -step utility of a point in an unobserved k  X  is greater than that of a point in an unobserved `  X  .
 Set  X  &lt;  X  such that the m -step policy selects the t  X  clumps. Notice that the m -step policy will behave iden-tically as before, except that it can now switch to the `  X  clumps with fewer than m evaluations remaining in  X  X nlucky X  cases. The right-hand side of (2) therefore still serves as a lower bound on its performance in this new space.
 Finally, combining the lower bound in (2) and the upper bound in (3), we have E D u ( D ) | m,t, P which may be made arbitrarily large by taking small enough  X  and large enough t . We will now discuss how we may, in certain situations, reduce the O (2 card X ) ` search space required by the ` -step optimal active search policy. Our approach will entail a  X  X ranch and bound X  X  X tyle strategy, where we will leverage relatively inexpensive-to-calculate in-equalities to prune suboptimal branches of the search space from consideration. This will require establish-ing two inequalities. First, we find a lower bound on the maximal ` -step active search expected utility among the unlabeled points. Next we find an upper bound on the ` -step expected utility of a given unla-beled point, as a function of its current probability. Combining these bounds together will ultimately pro-vide us with a threshold  X  such that any point x with Pr ( y = 1 | x, D ) &lt;  X  cannot possibly be the optimal ` -step action. In the below we will assume we start at timestep 1 and progress to timestep ` , beginning with an arbitrary starting set D 1 . 6.1. A lower bound on max E u ( D ` ) | x 1 , D 1 We will first establish a trivial lower bound on the maximal ` -step expected utility. Let be the point with the highest posterior probability of being a target at the first timestep, along with its probability. Let Clearly then u 0 is a trivial bound on the maximal expected ` -step utility. 6.2. An upper bound on E u ( D ` ) | x 1 , D 1 We now find an upper bound on the ` -step expected utility for any arbitrary point x 1 . Our approach will require the chosen classification model to meet two conditions. First, we must have that conditioning on a new nontarget observation cannot raise the target probability for any unlabeled point. Second, we must be able to bound the maximum target probability among unlabeled points after conditioning on a given number of additional targets; that is, we assume there is a function p  X  ( n, D ) such that p ( n, D )  X  max With the p  X  function in hand, we will define a function u ( `,n, D ) that represents a bound on the maximum ` -step utility among any unlabeled point after n addi-tional target observations. For ` = 1 , we define That this bound is valid follows immediately from the analysis of the simple one-step active search case. For ` &gt; 1 , we may build u  X  recursively: u  X  ( `,n, D ) , p  X  ( n, D ) u  X  ( `  X  1 ,n + 1 , D ) + 1 + With u  X  now defined, for a given point x , we have E u ( D ` ) | x 1 = x, D 1  X  Combining (4) and (6) , we may eliminate any point such that the right-hand side of (6) is less than u 0 , because it cannot possibly be the optimal action. Of course, we may apply this pruning technique at all depths of the search tree, allowing for deeper subopti-mal subtrees to be found and eliminated as well. We implemented the optimal active search and survey-ing policies in matlab , as well as uncertainty sampling. Using this implementation, we evaluated the perfor-mance of our policies on both synthetic and real data. In our search experiments, we used a simple k -nearest neighbor classifier. Let nn ( x ) represent the k -nearest neighbors of the point x in X , and let l-nn ( x ) represent the subset of nn ( x ) for which we currently have label observations. We define Here the constant  X   X  [0 , 1] serves as a  X  X seudocount, X  which smooths the probabilities on points that have few labeled neighbors. In our experiments, we fixed  X  , 1 / 10 . This model worked well empirically, and we may also easily derive the bound on maximum probabilities in (5) required for pruning the search space as described above. If we consider a point x with current probability then after conditioning on new observations D 0 con-taining at most n more positive observations, we have Note that this bound can be trivially modified to allow for arbitrary weight functions to be in-cluded in the model; there n could be replaced with 7.1. Illustrative example We begin with a simple example problem that illus-trates the behavior of the active search and active surveying approaches versus uncertainty sampling. Let I , [0 , 1] 2 be the unit square. We repeated the following experiment 100 times. We selected 250 points uniformly at random from I , which formed our input space X . Any point landing within Euclidean distance 4 of any of the points (0 , 0) , (0 , 1) , (1 , 0) , (1 , 1) or 2 , 1 / 2 ) (the four corners and the center point) formed the set of targets R . We picked one point uniformly at random from R and added it and its label to a training set. We then used the one-step optimal active search policy, the one-step optimal active surveying policy, and uncertainty sampling to select ten more points. Figure 2 shows kernel density estimates of the points selected by the algorithms across all experiments. The difference in behavior is immediate. Uncertainty sam-pling strongly focuses on the corners, where variance is typically the highest, the search policy strongly focuses on the learned locations of the targets, including the center, and the surveying policy strongly avoids the corners, which, despite having high variance, are not terribly informative about the space overall. 7.2. CiteSeer x data For our next experiment, we created a graph from a subset of the CiteSeer x citation network. Papers in the database were grouped based on their venue of publi-cation (after extensive data cleaning), and papers from the 48 venues with the most associated publications were retained. The graph was defined by having these papers as its nodes (38 079 in total) and undirected citation relations as its edges. We designated all papers appearing in nips proceedings (2 198 in total, 5.2% of the dataset) as targets. Differentiating these papers is difficult; many highly related venues are also prevalent. For our results presented here, we computed what Fouss et al. (2007) calls  X  X raph principal component analysis, X  which is equivalent to performing principal component analysis on card V vectors (one corresponding to each node) embedded in R (card V )  X  1 that are separated by a graph metric called commute time. 3 The first 20 graph principal components formed our set X , and our model was as in (7) with k , 50 .
 Again, we selected a single point at random from R to form an initial training set, then ran 500 steps of the one-, two-, and three-step active search policies with the goal of finding as many nips papers as possible. This experiment was repeated ten times.
 On average, the one-step algorithm found 167 nips papers; the two-step algorithm found 180, and the three-step algorithm found 187. Random search would be expected to find only 29 papers given the same number of evaluations. Figure 3 shows the cumulative number of targets found by each of the methods. The three-step lookahead procedure was able to find 8.5% of the targets after scanning only 1.3% of the data, 6.5 times better than expected by random search.
 To test active surveying, we selected 75 evaluations (starting again with a single training point from R ) using three different approaches: random search, un-certainty sampling, and the one-step optimal active-surveying policy. To estimate the class proportion, we subsampled the remaining unlabeled points, selecting 5% each time, and averaged the inferred means and variances of p (card R|D ) from five such samples. After each evaluation of each method, we estimated the mean and variance of card R given the training data collected thus far, as described above. We evalu-ated each method X  X  performance by approximating the beta distribution whose parameters (  X , X  ) were selected via moment-matching to the mean and variance of the induced posterior distribution over this quantity. We then computed the likelihood of the true unknown class proportion under this beta distribution. Figure 4 shows the progression of these likelihoods for each method over the course of the experiment. After a period of time where all methods have similar performance, the optimal policy begins to significantly outperform the other two methods, which behave nearly identically. There is clear utility to our active-surveying approach, even when the number of samples taken is very small. 7.3. The effect of search-space pruning Finally, we measured the effect of our branch-and-bound method described in Section 6. With the same data and experimental setup as in the CiteSeer x ex-periment, we measured the time required by one itera-tion of the optimal ` -step lookahead search policy, for 2  X  `  X  4 , both with and without the advantage of our pruning method. These times were measured given 100 random starting configurations, chosen as before. pruning 0.228 s 15.0 s 745 s no pruning 166 s  X  146 days  X  30 500 years speedup 731 8 . 42  X  10 5 1 . 29  X  10 9 The results are summarized in Table 1. The effect of our pruning strategy in this case is dramatic, enabling us to extend our search horizon far beyond what the realm of possibility would have been otherwise. We have presented the Bayesian optimal policy to two atypical active-learning problems related to binary clas-sification, which we call active search and active sur-veying. The former focuses on actively seeking out members of a set of identified targets as quickly as pos-sible, and the latter focuses on predicting the portion of the dataset belonging to an identified class. Our approach was to define sensible utility functions for these problems and then to derive the optimal Bayesian policy for each of them. The optimal policy for each takes the same form, but in practice the behavior of each can be dramatically different due to the sharply contrasting underlying utility functions.
 In addition to introducing the active surveying prob-lem, we have extended previous preliminary work on active search in two ways. We first proved a theoretical result showing that the potential advantage of farther lookahead horizons is unbounded. We then presented a branch-and-bound method for pruning the exponential search space required for active search in certain cases, which we showed can improve the computational per-formance of the optimal policy by orders of magnitude.
