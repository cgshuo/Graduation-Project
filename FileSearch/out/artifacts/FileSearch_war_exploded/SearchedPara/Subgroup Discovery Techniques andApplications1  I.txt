 Rule learning is an important form of predictive machine learning, aimed at inducing classification and prediction rules from examples [2]. Developments in descriptive induction have recently also gained much attention of researchers interested in rule learning. These include mining of association rules [1], subgroup discovery [11, 4, 6] and other approaches to non-classificatory induction. discovery. The term actionability is described in [10] as follows:  X  X  pattern is in-teresting to the user if the user can do something with it to his or her advantage. X  As such, actionability is a subjective measure of interestingness.
 learned from two applications that motivated our research in actionable knowl-edge generation for decision support. In an ideal case, the induced knowledge should enable the decision maker to perform an action to his or her advantage, for instance, by appropriately selecting individuals for population screening con-cerning high risk for coronary heart disease (CHD). Consider one rule from this application: This rule is actionable as the general practitioner can select from his patients the overweight patients older than 63 years.
 through recently developed subgroup discovery approaches, where a subgroup discovery task is informally defined as follows [11, 4, 6]: Given a population of individuals and a specific property of individuals that we are interested in, find population subgroups that are statistically  X  X ost interesting X , e.g., are as large as possible and have the most unusual distributional characteristics with respect to the property of interest.
 and induce individual rules (describing individual subgroups) from labeled train-ing examples (labeled positive if the property of interest holds, and negative otherwise), thus targeting the process of subgroup discovery to uncovering prop-erties of a selected target population of individuals with the given property of interest. Despite the fact that this form of rules suggests that standard super-vised classification rule learning could be used for solving the task, the goal of subgroup discovery is to uncover individual rules/patterns, as opposed to the goal of standard supervised learning, aimed at discovering rulesets/models to be used as accurate classifiers of yet unlabeled instances [4].
 symbolic form and must be relatively simple in order to be recognized as ac-tionable for guiding a decision maker in directing some targeted campaign. We provide arguments in favour of actionable knowledge generation through recently developed subgroup discovery algorithms, uncovering properties of individuals for actions like population screening and functional genomics data analysis. For such tasks, actionable rules are characterized by the experts X  choice of the  X  X c-tionable X  attributes to appear in induced subgroup descriptions, as well as by high coverage (support), high sensitivity and specificity 1 ,evenifthiscanbe achieved only at a price of lower classification accuracy, which is the quality to be optimized in classification and prediction tasks.
 research in actionable knowledge generation are described in Section 2. Section 3 introduces the ROC and the TP/FP space needed for better understanding of the task and results of subgroup discovery. Section 6 introduces the functional genomics domain in more detail, where the task is to distinguish between differ-ent cancer types. The motivation for this work comes from practical data mining problems in a medical and a functional genomics domain. and description of Coronary Heart Disease (CHD) risk groups [4]. Typical data collected in general screening include anamnestic information and physical ex-amination results, laboratory tests, and ECG at-rest test results. In many cases with significantly pathological test values (especially, for example, left ventricular hypertrophy, increased LDL cholesterol, decreased HDL cholesterol, hyperten-sion, and intolerance glucose) the decision is not difficult. However, the hard problem in CHD prevention is to find endangered individuals with slightly ab-normal values of risk factors and in cases when combinations of different risk factors occur. The results in the form of risk group models should help gen-eral practitioners to recognize CHD and/or to detect the illness even before the first symptoms actually occur. Expert-guided subgroup discovery discovery is aimed at easier detection of important risk factors and risk groups in the population.
 (gene chips) provides an important source of information that can help in under-standing many biological processes. The database we analyze consists of a set of gene expression measurements (examples), each corresponding to a large num-ber of measured expression values of a predefined family of genes (attributes). Each measurement in the database was extracted from a tissue of a patient with a specific disease; this disease is the class for the given example. The domain, described in [9, 5] and used in our experiments, is a typical scientific discovery domain characterised by a large number of attributes compared to the number of available examples. As such, this domain is especially prone to overfitting, as it is a domain with 14 different cancer classes and only 144 training examples in total, where the examples are described by 16063 attributes presenting gene ex-pression values. While the standard goal of machine learning is to start from the labeled examples and construct models/classifiers that can successfully classify new, previously unseen examples, our main goal is to uncover interesting pat-terns/rules that can help to better understand the dependencies between classes (diseases) and attributes (gene expressions values). A point in the ROC space (ROC: Receiver Operating Characteristic) [8] shows classifier performance in terms of false alarm or false positive rate FPr = the covering properties of the rule. The ROC space is appropriate for measur-ing the success of subgroup discovery, since rules/subgroups whose TPr/FPr tradeoff is close to the diagonal can be discarded as insignificant; the reason is that the rules with TPr/FPr on the diagonal have the same distribution of covered positives and negatives as the distribution in the training set. Con-versely, significant rules/subgroups are those sufficiently distant from the di-agonal. Subgroups that are optimal under varying TPr/FPr tradeoffs form a convex hull called the ROC curve. Figure 1 presents seven rules on the convex hull (marked by circles), including X 1and X 2, while two rules B 1and B 2below the convex hull (marked by +) are of lower quality in terms of their TPr/FPr tradeoff.
 point to the ROC diagonal is proportional to the significance of the rule. Hence, the goal of a subgroup discovery algorithm is to find subgroups in the upper-left corner area of the ROC space, where the most significant rule would lie in point (0 , 1) representing a rule covering only positive and none of the negative examples ( FPr =0and TPr = 1).
 hand side of Figure 1), where FPr on the X -axis is replaced by | FP | and TPr on the Y -axis by | TP | . 2 The TP/FP space is equivalent to the ROC space when comparing the quality of subgroups induced in a single domain. The reminder of this paper considers only this simpler TP/FP space representation. Subgroup discovery is a form of supervised inductive learning of subgroup de-scriptions of the target class. As in all inductive rule learning tasks, the language bias is determined by the syntactic restrictions of the pattern language and the vocabulary of terms in the language. In this work the hypothesis language is restricted to simple if-then rules of the form Class  X  Cond , where Class is the target class and Cond is a conjunction of features. Features are logical condi-tions that have values true or false , depending on the values of attributes which describe the examples in the problem domain: subgroup discovery rule learning is a form of two-class propositional inductive rule learning, where multi-class problems are solved through a series of two-class learning problems, so that each class is once selected as the target class while examples of all other classes are treated as non-target class examples.
 covery that can be applied to actionable knowledge generation. 4.1 Constraint-Based Subgroup Discovery with the SD Algorithm In this paper, subgroup discovery is performed by SD, an iterative beam search rule learning algorithm [4]. The input to SD consists of a set of examples E and a set of features F that can be constructed for the given example set. The output of the SD algorithm is a set of rules with optimal covering properties on the given example set. The SD algorithm is implemented in the on-line Data Mining Server (DMS), publicly available at http://dms.irb.hr . 3 group discovery involves a set of constraints that induced subgroup descriptions have to satisfy. The following constraints are used to formalize the SD constraint-based subgroup discovery task.
 Language Constraints  X  Individual subgroup descriptions have the form of rules Class  X  Cond ,  X  For discrete (categorical) attributes, features have the form Attribute =  X  To simplify rule interpretation and increase rule actionability, subgroup dis-Evaluation/Optimization Constraints  X  To ensure that induced subgroups are sufficiently large, each induced rule R  X  Other evaluation/optimization constraints have to ensure that the induced Early detection of artherosclerotic coronary heart disease (CHD) is an impor-tant and difficult medical problem. CHD risk factors include artherosclerotic attributes, living habits, hemostatic factors, blood pressure, and metabolic fac-tors. Their screening is performed in general practice by data collection in three different stages.
 A Collecting anamnestic information and physical examination results, includ-B Collecting results of laboratory tests, including information about risk factors C Collecting ECG at rest test results, including measurements of heart rate, In this application, the goal was to construct at least one relevant and interesting CHD risk group for each of the stages A, B, and C, respectively.
 diagnosis, collected at the Institute for Cardiovascular Prevention and Rehabil-itation, Zagreb, Croatia, was used for subgroup discovery [4]. The database is in no respect a good epidemiological CHD database reflecting actual CHD oc-currence in a general population, since about 50% of gathered patient records represent CHD patients. Nevertheless, the database is very valuable since it in-cludes records of different types of the disease. Moreover, the included negative cases (patients who do not have CHD) are not randomly selected persons but individuals considered by general practitioners as potential CHD patients, and hence sent for further investigations to the Institute. This biased dataset is ap-propriate for CHD risk group discovery, but it is inappropriate for measuring the success of CHD risk detection and for subgroup performance estimation in general medical practice. 5.1 Results of Subgroup Discovery The process of expert-guided subgroup discovery was performed as follows. For every data stage A, B and C, the SD algorithm was run for values g in the range 0.5 to 100 (values 0.5, 1, 2, 4, 6, ...), and a fixed number of selected output rules equal to 3. The rules induced in this iterative process were shown to the ex-pert for selection and interpretation. The inspection of 15 X 20 rules for each data stage triggered further experiments, following the suggestions of the medical ex-pert to limit the number of features in the rule body and avoid the generation of rules whose features would involve expensive and/or unreliable laboratory tests.
 lected five most interesting CHD risk groups. Table 1 shows the induced sub-group descriptions. The features appearing in the conditions of rules describing the subgroups are called the principal factors . The described iterative process was successful for data at stages B and C, but it turned out that medical history data on its own (stage A data) is not informative enough for inducing subgroups, i.e., it failed to fulfil the expert X  X  subjective criteria of interestingness. Only af-ter engineering the domain, by separating male and female patients, interesting subgroups A 1and A 2 have actually been discovered.
 the induced rules are the best in terms of the TP/FP tradeoff, i.e., which of them are used to define the convex hull in the TP/FP space. At stage B, for instance, seven rules (marked by +) are on the convex hull of the TP/FP space shown in Figure 1. Notice that the expert-selected subgroups B1 and B2 are significant, but are not among those lying on the convex hull in Figure 1. The reason for selecting exactly those two rules at stage B are their simplicity (con-sisting of three features only), their generality (covering relatively many posi-tive cases) and the fact that the used features are, from the medical point of view, inexpensive laboratory tests. Additionally, rules B1 and B2 are interesting because of the feature body mass index below 30 kgm  X  2 , which is intuitively in contradiction with the expert knowledge that both increased body weight as well as increased total cholesterol values are CHD risk factors. It is known that increased body weight typically results in increased total cholesterol val-ues while subgroups B1 and B2 actually point out the importance of increased total cholesterol when it is not caused by obesity as a relevant disease risk factor. 5.2 Statistical Characterization of Subgroups The next step in the proposed subgroup discovery process starts from the discov-ered subgroups. In this step, statistical differences in distributions are computed for two populations, the target and the reference population. The target popu-lation consists of true positive cases (CHD patients included into the analyzed subgroup), whereas the reference population are all available non-target class examples (all the healthy subjects). Statistical differences in distributions for all the descriptors (attributes) between these two populations are tested using the  X  2 test with 95% confidence level ( p =0 . 05).
 partitioned in up to 30 intervals so that in every interval there are at least 5 instances. Among the attributes with significantly different value distributions there are always those that form the features describing the subgroups (the principal factors), but usually there are also other attributes with statistically significantly different value distributions. These attributes are called supporting attributes , and the features formed of their values that are characteristic for the discovered subgroups are called supporting factors .
 more complete and acceptable for medical practice. Medical experts dislike long conjunctive rules which are difficult to interpret. On the other hand, they also dislike short rules providing insufficient supportive evidence. In this work, we found an appropriate tradeoff between rule simplicity and the amount of sup-portive evidence by enabling the expert to inspect all the statistically significant supporting factors, whereas the decision whether they indeed increase the user X  X  confidence in the subgroup description is left to the expert. In the CHD ap-plication the expert has decided whether the proposed supporting factors are meaningful, interesting and actionable, how reliable they are and how easily they can be measured in practice. Table 2 lists the expert selected supporting factors. The gene expression domain, described in [9, 5] is a domain with 14 differ-ent cancer classes and 144 training examples in total. Eleven classes have 8 examples each, two classes have 16 examples and only one has 24 examples. The examples are described by 16063 attributes presenting gene expression val-ues. In all the experiments we have used gene presence call values ( A , P ,and M ) to describe the training examples. The domain can be downloaded from http://www-genome.wi.mit.edu/cgi-bin/cancer/datasets.cgi. There is also an in-dependent test set with 54 examples. The standard goal of machine learning is to start from such labeled examples and construct classifiers that can successfully classify new, previously unseen examples. Such classifiers are important because they can be used for diagnostic purposes in medicine and because they can help to understand the dependencies between classes (diseases) and attributes (gene expressions values). 6.1 Choice of the Description Language of Features Gene expression scanners measure signal intensity as continuous values which form an appropriate input for data analysis. The problem is that for contin-uous valued attributes there can be potentially many boundary values sepa-rating the classes, resulting in many different features for a single attribute. There is also a possibility to use presence call (signal specificity) values com-puted from measured signal intensity values by the Affymetrix GENECHIP software. The presence call has discrete values A (absent), P (present), and M (marginal). Subgroup discovery as well as filtering based on feature and rule relevancy are applicable both for signal intensity and/or the presence call at-tribute values. Typically, signal intensity values are used [7] because they impose less restrictions on the classifier construction process and because the results do not depend on the GENECHIP software presence call computation. For sub-group discovery we prefer the later approach based on presence call values. The reason is that features presented by conditions like Gene = P is true (mean-ing that Gene is present, i.e., expressed) or Gene = A is true (meaning that Gene is absent, i.e., not expressed) are very natural for human interpretation and that the approach can help in avoiding overfitting, as the feature space is very strongly restricted, especially if the marginal value M is encoded as value unknown.
 not want to increase the relevance of features generated from attributes with M values. The M values are therefore handled as unknown values as follows: unknown values in positive examples are replaced by value false , while unknown values in negative examples are replaced by value true . As for the other two values, A and P , it holds that two features for gene X , X = A and X = P ,are identical. Consequently, for every gene X there are only two distinct features X = A and X = P . 6.2 The Experiments The experiments were performed separately for each cancer class so that a two-class learning problem was formulated where the selected cancer class was the target class and the examples of all other classes formed non-target class exam-ples. In this way the domain was transformed into 14 inductive learning prob-lems, each with the total of 144 training examples and between 8 and 24 target class examples. For each of these tasks a complete procedure consisting of fea-ture construction, elimination of irrelevant features, and induction of subgroup descriptions in the form of rules was repeated. Finally, using the SD subgroup discovery algorithm [4], for each class a single rule with maximal q g value was selected, for q g = | TP | | FP | + g being the heuristic of the SD algorithm and g =5the generalization parameter default value. The rules for all 14 tasks consisted of 2 X 4 features. The procedure was repeated for all 14 tasks with the same default parameter values. The induced rules were tested on the independent example set.
 ous classes (diseases) and the precision higher than 50% was obtained for only 5 out of 14 classes. There are only three classes (lymphoma, leukemia, and CNS) with more than 8 training cases and all of them are among those with high precision on the test set, while for only two out of eleven classes with 8 training cases (colorectal and mesothelioma) high precision was achieved. The classification properties of rules induced for classes with 16 and 24 tar-get class examples (lymphoma, leukemia and CNS) are comparable to those reported in [9] (see Table 3), while the results on eight small example sets with 8 target examples were poor. An obvious conclusion is that the use of the subgroup discovery algorithm is not appropriate for problems with a very small number of examples because overfitting can not be avoided in spite of the heuristics used in the SD algorithm and the additional domain-specific tech-niques used to restrict the hypothesis search space. But for larger training sets the subgroup discovery methodology enabled effective construction of relevant rules. 6.3 Examples of Induced Rules For three classes (lymphoma, leukemia, and CNS) with more than 8 training cases the following rules were induced by the constraint-based subgroup dis-covery approach involving relevancy filtering and handling of unknown values described in this chapter.

Lymphoma class: (CD20 receptor EXPRESSED) AND (phosphatidylinositol 3 kinase regulatory alpha subunit NOT EXPRESSED)
Leukemia class: (KIAA0128 gene EXPRESSED) AND (prostaglandin d2 synthase gene NOT EXPRESSED)
CNS class: (fetus brain mRNA for membrane glycoprotein M6 EXPRESSED) AND (CRMP1 collapsin response mediator protein 1 EXPRESSED) two rules (for the lymphoma and leukemia classes) are judged as reassuring and one (the CNS class) has a plausible, albeit partially speculative explanation. Namely, the best-scoring rule for the lymphoma class in the multi-class cancer recognition problem contains a feature corresponding to a gene routinely used as a marker in diagnosis of lymphomas (CD20), while the other part of the conjunction (phosphatidylinositol, the PI3K gene) seems to be a plausible bi-ological co-factor. The best-scoring rule for the leukemia class contains a gene whose relation to the disease is directly explicable (KIAA0128, Septin 6). Both M6 and CRMP1 appear to have multifunctional roles in shaping neuronal net-works, and their function as survival (M6) and proliferation (CRMP1) signals may be relevant to growth promotion and CNS malignancy.
 interpretation of induced rules prove the effectiveness of described methods for avoiding overfitting in scientific discovery tasks.
 The paper describes joint work with Dragan Gamberger from Rudjer Bo X  skovi  X  c Institute, Zagreb, Croatia, supported by the Slovenian Ministry of Higher Edu-cation, Science and Technology.

