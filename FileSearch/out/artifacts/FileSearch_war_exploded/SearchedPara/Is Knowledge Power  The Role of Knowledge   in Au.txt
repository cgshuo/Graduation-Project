 In information systems development multiple transformations are performed. In a first step, collected requirements are transfor med to analysis models. Building on the code. While all latter ones are model to model transformations, the creation of analysis models usually starts from unstructured natural language documents [1]. communicate between a broad range of stakeholders and users [2]. However, the manual transformation of potentially ambiguous and inconsistent natural language requirements to models can be time-consuming, error-prone, and monotonous [3, 4]. 
Consequently, much effort has been devoted to leverage tools and technologies to possible range from a first identification of a certain domain X  X  key concepts [5, 6], via generation of analysis models [8, 9]. Although most of these tools require knowledge done exploring how different amounts and types of knowledge determine the outcome of the requirements elicitation process. An according study could help to guide future results. Our work addresses this gap by investigating the following research question: requirements elicitation tools? 
To address this question, we propose a reference dataset consisting of a collection of natural language documents and different kinds of knowledge that can be leveraged for the automatic elicitation process. Based on this reference dataset we perform two developed in our previous work [10, 11]. We vary both, the amount and type of knowledge and measure the effect on elicitation quality. Our work complements existing studies on automated requirements elicitation and provides guidelines for future research and developments. 
The remainder of the paper is organized in the following sections: The second with respect to related work. Subsequently, the requirements elicitation system REMINER used for the simulations is presented shortly. Section 4 describes the evaluation framework including the data set and the simulation methodology. Section 5 presents the simulation results from different perspectives. The sixth section future activities, and contributions of our work. development of corresponding software tools. 2.1 Conceptual Foundations requirements elicitation provides system-supported elicitation of requirements from be filled with hundreds of different requirements, appearing close to each other within the same paragraph or sentence. Thus, requirements identification is first about delimiting each requirement within the provided text documents. Once the text passages for each requirement have been identified, subsequent processing can begin. During requirements classification, each previously identified requirement is assigned to a category. Various taxonomies have been proposed to classify requirements (e.g., the Volere template [14]) or the IEEE recommended practice for RE specification requirements, specific sub-categories (e.g., requirements describing performance, security or look and feel aspects) are distinguished [8]. Automated requirements elicitation can be successfully implemented based on Natural language processing (NLP) and information retrieval (IR) techniques [16]. Examples include the artifacts presented by Goldin and Berry [17], Cleland-Huang et then compared to text bricks within a knowledge base. Knowledge bases can vary in structure and complexity. They often consist of either dictionaries [7, 18], which hold additionally include relations between different concepts. 
Research about effects of different types of knowledge on the results of automatic requirements elicitation is scarce. However, outside the requirements elicitation field, two characteristics of knowledge are frequently discussed in the context of IS-knowledge. Knowledge origin describes the way the knowledge bases required for upload of existing knowledge to the system (referred to as  X  X mported knowledge X ) or contrast to imported knowledge, retrieved knowledge can usually be acquired in specific requirements category. easier to be reused in a different project, than knowledge with a high project-specificity. Transferred to requirements elic itation, it can be expected that documents originating from the same project share specific requirements which are not included in general knowledge (e.g., the data field  X  X requent flyer number X  in a project within the traveling domain). Similarly, specific writing styles or standards for single significant increase of identified requiremen ts and therefore improve the overall result of automated requirements elicitation. 
Apart from the type of knowledge, the amount of knowledge can also determine the outcomes of the automation algorithm. A larger knowledge base can be assumed to assignments within the knowledge base. Therefore, a more extensive knowledge base only results in improved elicitation outcomes if at the same time a high quality of the knowledge base contents is ensured. are presented and categorized. Subsequently , the consideration of knowledge aspects (origin, project-specificity and amount of knowledge) in these works is analyzed. 2.2 Related Work Berry et al. [16] distinguish four categories of tools that can be used to process natural language requirements. Two of them can be related to automated requirements elicitation as previously defined: requirements engineer in achieving an understanding of a previously unfamiliar AbstFinder [17] and RAI [5] support the analyst in collecting the main concepts and negotiation with the customer. Going beyond the creation of simple word lists, according approach, constructing domain onto logies with the help of NLP techniques. Abstraction identification tools can be seen as an early step in requirements elicitation primarily aiming at the creation of knowledge, which can then be used for the analysis of further documents in the following. (2) Tools to generate models from natural language descriptions. This group of tools goes beyond the mere identification of abstractions, aiming at the identification, abstraction identification, the mentioned approaches support a later development phase, namely the transition from requirements to design. Mich [24] describes NL-OOPS, a CASE tool that supports requirements analysts by generating object oriented models from natural language requirements documents. The tool implements an the requirements modeling process. Ambriola and Gervasi [3] present CIRCE, an environment for the analysis of natural language requirements. CIRCE uses various transformation steps, including a technique for parsing natural language requirements and an expert system based on modular agents, embodying intentional knowledge about software systems in general. The transformations result in a set of models (e.g. UML models) which can be used in further development steps or the requirements document itself. Sampaio et al. [7] desc ribe EA-Miner, a tool-based approach providing semi-automated support for mining various types of concept models specific to Aspect-Oriented Requirements Engineering. Starting from unstructured input documents the tool employs NLP techniques such as frequency analysis, part-al. [18] developed a tool for model-driven engineering of user interfaces. Their tagging to elicit models from textual scenarios. we refer to as (3) Tools for requirements classification . These tools can be positioned between structured requirements from unstructured or semi-structured sources of information. category, they do not generate models yet but identify single requirements statements and classify them according to an existing taxonomy. Cleland-Huang et al. [8] focus on non-functional requirements (NFRs) as e.g. security, performance or usability requirements. Based on the notion that each sub-group of NFRs has its unique keywords, information retrieval methods are applied to find and classify NFRs. While an initial classifier is learned from manually categorized requirements, retrieved knowledge is also considered by iteratively training the classifier based on the employ a semi-supervised categorization approach that only needs a small set of manually classified requirements for the initial training of the classifier. Additionally, underlying characteristics of text bricks ar e taken into account, and the classification Vlas and Robinson [13] present an automated approach for the identification and parsing rules based on ontologies consisting of grammatical and requirements-related knowledge. 
In the presented works of the three categories, the effect of knowledge on elicitation results has hardly been investigated: For the first group (tools that identify of knowledge have been analyzed. Within the remaining two groups (tools to generate evaluations have been conducted without formal measurement of the elicitation results [3, 18, 24], without variations in the utilized knowledge base [7, 13] or with a restriction on different amounts of non-functional requirements [8, 9]. 
In summary, although significant effects of the type and amount of knowledge on is scarce. An according study can help to guide future requirements elicitation The system allows both the variation of project-specificity of knowledge and origin of knowledge. Additionally both types of knowledge can be easily investigated based on different amounts of requirements documents (and hereby different amounts of knowledge). This system, which provides the basis for our analyses, is briefly presented in the following. Automated requirements processing (NLP) and inf o the artifacts presented by Sampaio et al. [7]. Our sys t provides capabilities to us e be able to create retrieved k elicitation capabilities. Fi g possible process to make possible, for example the p be a one-time activity just t
First during, manual k n uploaded to the knowled g consists of text bricks, w h (e.g.,  X  X redit card number X  
Second, during prepro c single text bricks which se r used NLP techniques like Word Elimination and Wo r framework (MorphAdorne r POS tag), for example ( X  X u p
Third, automatic elicita t consists of various algori t Baeza-Yates &amp; Ribeiro-N similarity of text bricks e knowledge base and h e fundamental functioning o categories are indexed like several algorithmic steps, i n the total probabilities for categories are calculated. B selected for each word. In word will be eventually ass
Fourth, during manual approved. During approva l can be changed or even d e finally approved requirem e retrieved knowledge. Retri e requirements categories a n elicitation process. For exa manually as one category w text brick or POS. As the categories (e.g., by differ e only calculate probabilities number of previous manua l Figure 2 shows a sc r Requirements are highlig h transcripts, workshop mem o are represented by differe n text markers in physical do indicate if a requirement i s The text (in this case an i n words or entire text passag e 4.1 Dataset requirements documents and the knowledge to be used for automatic elicitation. Furthermore we use a gold standard, which is the expert solution to assess the results of automatic elicitation. 
The natural language requirements documents consist of previously conducted were finally selected for the study, four of them referring to the train reservation project associated with the  X  X ravel management X  domain. The first application is a train while the second application is a car sharing app, which allows users to connect to other domain, it is worthwhile to investigate corresponding example websites for train website as another example for a traveling app), there are also differences. For example option to use a sleeper train can be chosen. Similarly on the car sharing website features to select  X  X omen-only lifts X  or  X  X moking allowed lifts X  are provided. 
The knowledge used for the automation algorithm consists of both imported and retrieved knowledge. Imported knowledge was uploaded from different data sources, SAP Travel Management application [27]; for the non-functional category, we sets of retrieved knowledge were used: one set was retrieved from texts about the train reservation app and one set from texts about the car sharing app. documents has been manually highlighted by three requirements engineering experts. experts was taken as the gold standard. 4.2 Evaluation Model Addressing our research question we want to investigate how the amount and type of knowledge which is used for automated requirements elicitation affects the quality of measure [5, 8, 9] which we equally apply in our study. It is calculated by comparing Recall can be seen as a measure of completeness, comparing the number of correctly identified requirements with the total number of requirements existing in a document. 
The independent variable amount of knowledge is operationalized by the number of documents used to build up the knowledge base. We are thereby simulating how knowledge would probably be extended in practice: starting from an initial, imported amount of knowledge, the knowledge base would be gradually augmented through retrieved knowledge from already processed documents. 
The type of knowledge is represented by two independent variables: Origin and knowledge, or a combination of both. Project-specificity of knowledge is operationalized by using retrieved knowledge for either the same or a different project. Both projects do reuse of knowledge to specific types of requirements (e.g. non-functional requirements). The resulting evaluation model is depicted in Figure 3. 4.3 Simulation Series performed. In both simulations, requirements were automatically elicited from four single result. Subsequently, the simulations were repeated with a different amount of retrieved knowledge. For each result, the r ecall rate was examined by comparing the results of the automatism to the gold standard. 
The first series of simulations focused on the effects of different origins of knowledge on elicitation quality. Additionally to the origin of knowledge, the amount the knowledge base with retrieved knowledge. This resulted in a series of 11 different simulation runs. The first run only used imported knowledge, the following five runs only used retrieved knowledge (for 0-4 texts) and the final five runs a combination of both (for 0-4 texts). The analyzed natural language documents as well as the retrieved knowledge for this series of simulation orig inated from the project for the car sharing application, resulting in a constantly high project-specificity of the knowledge. Table 1 summarizes the performed simulation runs. 
Simulation Run # Origin of Knowledge Number of texts 5 2 to 6 Retrieved Knowledge 0 to 4 7 to 11 Imported &amp; Retrieved Knowledge 0 to 4 
The second series of simulations (Table 2) focused on the effects of project-runs, only retrieved knowledge from the car sharing project was taken. For the project-independent runs, only retrieved knowledge from the train reservation project Similar to the first series, the amount of retrieved knowledge was additionally varied. This resulted in a series of 10 different simulation runs. The first five runs simulated a project-specific knowledge base (for 0-4 texts) the next five runs simulated a knowledge base with knowledge from a different project (for 0-4 texts). In this series, the origin of knowledge was kept constant, as only retrieved knowledge was used. 
Simulation Run # Project-Specificity Number of texts 3 1 to 5 Project-Specific Knowledge 0 to 4 6 to 10 Project-Independent Knowledge 0 to 4 Figure 4 depicts the results of the first series of simulation runs, which focused on the Furthermore it can be observed that for new projects, which have not elicited knowledge is necessary to achieve a relevant recall rate. However it can be seen that in our simulation the recall from retrieved knowledge approximately equaled the recall from imported knowledge after three documents had been analyzed and outperformed it for more than three documents. Additionally it is interesting to notice, that imported knowledge i n than three documents had b combination of imported a rate for retrieved knowledg e
Figure 5 depicts the res u on the effects of project-sp e
Interestingly in our si m ambiguous effect. Recall r a amount of knowledge, but specificity itself. Althoug h management) this was an unexpected result, as the two applications which the interviews based on (a train reservation and a car sharing application) provided significant differences. These observations allow the interpretation that automated knowledge across projects within the same domain. The first series of simulation runs demonstrated the effects of the variable  X  X rigin of knowledge X  on elicitation quality. Interestingly, we observed that the usage of retrieved knowledge outperformed the usage of imported knowledge already after degrees of domain-specificity of the utilized knowledge. Retrieved knowledge can potentially provide a higher degree of domain-specificity than imported knowledge. to the core domain (in our case travel management), this domain can be divided into sub-domains using their own vocabulary. In our exemplary travel management associated to general domain knowledge. However more specific terms like  X  X ype of rail card X  or  X  X moking allowed lifts X  are specific to the sub-domains of train transport and shared car transport. Consequently, while imported knowledge can be used to correctly identify and classify a core set of general requirements (resulting in a recall of almost 0.4 in our simulation), more specific requirements (which required sub-domain knowledge) were only captured after using retrieved knowledge. 
In the second series of simulation runs, we investigated how the project-specificity of knowledge affected elicitation quality. Although we expected the usage of project-surprisingly not observable in our simulation. Before running the simulation we expected Building on a basic recall provided by the usage of imported knowledge (KL), retrieved recall. Then, through additional knowledge from the same project (and therefore also the same sub-domain) we expected the level of recall to rise further. provide sufficient additional knowledge, which was not already contained in the project-independent documents. We therefore plan to repeat the second simulation project-specific and project-independent texts then materializes. 
The question whether to build knowledge  X  X ottom-up X  by a group of regular project-members (as we did with retrieved knowledge) or  X  X op-down X  by individual domain experts (as we did with imported knowledge) has been widely discussed in engineering field proposed a systematic top-down approach to acquire and maintain knowledge from stakeholders. Various knowledge engineering methodologies such as CommonKADS [31] and tools such as Prot X g X  [32] have been suggested. To reduce knowledge acquisition efforts, one important principle from the very early beginning was to establish reusable knowledge bases [33]. Complementing manual knowledge engineering, advanced knowledge discovery techniques to extract knowledge from source data such as documents have been suggested. For example the field of ontology learning [34] extracts and suggests ontological structures from existing domain data to the knowledge engineer. Recently, the rather expert-driven knowledge engineering approach for establishing knowledge has been complemented by an end-user-driven bottom-up approach following a Web 2.0 paradigm; user-generated classifications, also known as folksonomies [35] represent one important example. Following this approach, users incrementally build knowledge bases by themselves. These bottom-up knowledge bases can be leveraged to create suggestions, e.g. such as done by the social bookmarking and citation management system Bibsonomy [36]. 
Looking at these different paradigms, the question arises how to build and maintain knowledge for advanced requirements elicitation systems. Our evaluation results provide evidence for the huge potential of following a bottom-up approach. Supplying an initial knowledge base positively impacts recall at the beginning of a requirements elicitation process. However, the bottom-up approach outperformed the top-down pre-development projects within the same or similar domains seems to be a promising knowledge across releases. Second, from a customer perspective, knowledge can be accumulated within a Line-of-Business such as a procurement department running multiple IS implementation projects within this domain. While our simulation, using a possible using more extensive datasets [2, 8]. Although these results show, that automated requirements elicitation cannot fully replace manual efforts performed by a requirements engineer, it can significantly support humans and thereby reduce the number of overseen and omitted requirements [16]. Leveraging our previously build requirements elicitation system REMINER we Based on the system and the dataset, we carried out a simulation study that explored the impact of different knowledge sources as well as project-specificity of knowledge on elicitation quality. 
We are aware of several limitations of our work. First, assessing external validity, (REMINER), the results can be generalized to other knowledge-based requirements management), we expect our results to be generalizable to a large amount of similarly complex domains. Future work may complement our study by a replication of the utilized requirements documents (like readability and length). Instead of varying these variables, we decided to use documents of comparable readability and length. Future research might investigate how these two variables affect requirements elicitation number of documents as a measure for the amount of knowledge. Although it can be assumed, that additional documents added further knowledge and the results show that in fact more documents led to a larger amount of recognized requirements, simulation to real life conditions in which entire documents instead of single knowledge items would be added to retrieved knowledge. 
From a research perspective, our work pr ovides the following contributions. First, we created a reference dataset for requirements elicitation from natural language documents that may be leveraged by other re searchers to evaluate their tools. Second, we extend the body of knowledge by exploring the impact of different forms of background knowledge on elicitation outcomes. From a practical point of view, we provide guidance on how to leverage knowledge in requirements engineering tool development. In commercial requirements tools, automated requirements elicitation capabilities are still scarce. With our results we hope to motivate commercial software vendors to include knowledge-based automation mechanisms in their products. 
