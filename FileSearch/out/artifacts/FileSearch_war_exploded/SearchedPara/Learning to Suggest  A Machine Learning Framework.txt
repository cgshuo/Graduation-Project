 We consider the task of suggesting related queries to users after they issue their initial query to a web search engine. We propose a machine learning approach to learn the prob-ability that a user may find a follow-up query both use-ful and relevant, given his initial query. Our approach is based on a machine learning model which enables us to gen-eralize to queries that have never occurred in the logs as well. The model is trained on co-occurrences mined from the search logs, with novel utility and relevance models, and the machine learning step is done without any labeled data by human judges. The learning step allows us to generalize from the past observations and generate query suggestions that are beyond the past co-occurred queries. This brings significant gains in coverage while yielding modest gains in relevance. Both offline (based on human judges) and online (based on millions of user interactions) evaluations demon-strate that our approach significantly outperforms strong baselines.
 H.3.3 [ Information Search and Retrieval ]: Query For-mulation, Retrieval Models search assistance, query suggestion, machine learning, query log mining
Query suggestions are an integral part of the modern search experience. Here we are concerned with query reformulation This work was completed while the author was at Yahoo! Labs.
 suggestions that are presented the users after they submit their query, rather than query completion suggestions. The latter is useful in helping the user formulate an original query by aiding in spelling and term selection while typing in the query. Query reformulation suggestions, on the other hand, are useful for changing direction once a search query has been already issued. The needs for such an action could in-clude disambiguation once the user realizes after inspection of the results that the concept he had in mind had possi-bly other stronger meanings different than his. Or, the user may want to formulate an even more precise and succinct query to hone in on better answers. It could also involve modification of the information need based on the informa-tion gained by inspection of the search result, to either dive deeper into research or move to a different aspect of a task. The needs are much varied.

Past scientific research is focused on a few main meth-ods to address most user needs for query reformulation. A large group of methods are based on leveraging the  X  X isdom of crowds X  by analyzing the search logs. Another group of methods use term semantics to derive new queries from ex-isting ones. The former group of methods can be divided into those that exploit query co-occurrences in the search logs, and those that leverage the document click informa-tion such as random walks over query-document bipartite graphs. In the latter group, a number of query synthesis methods exist, either synthesizing new queries with active user participation, or directly without any user input. We delve into a review of these existing methods in the related works section, and contrast them with our method.
There are a few fundamental shortcomings of these meth-ods, (i) the utility of a query reformulation suggestion to the user X  X  search task is indirectly handled (ii) all co-occurrences in the query logs are treated equally, without modeling the probability that the pair of queries belong to the same search task or not (iii) models that are solely based on collocated queries in the past logs will have limited coverage.
In this paper, we develop a machine learning framework which addresses the above shortcomings. First, the utility argument is handled explicitly by building a utility model that takes into account positions of URLs that are common to result sets of the original query and the suggestions. Sec-ond, we propose an implicit task boundary method to model whether a following query is a continuation of the preceding queries; as a result, co-occurrences in the search logs are not treated equally, but weighted with the probability that they belong to the same task. Finally, we learn to predict the utility of suggestion by using a rich feature space including lexical and result set features that capture the salient fac-tors of why a suggestion could be useful to the user given his initial query, rather than only relying on the observed query co-occurrences in the past. We train this machine learning model with a learning target derived from the past query co-occurrences rather than relying on human judgments. This allows us to generalize the knowledge trained on the past query co-occurrences and the machine learned model can generate both relevant and useful (query,suggestion) pairs that have never observed in the past logs, leading to a sig-nificant increase in coverage. Also, it avoids the costly and time consuming human labeling process. Furthermore, the machine learning framework also has the advantage of pro-viding the apparatus that can effectively blend disparate sources of query suggestion candidates such as those based on wisdom of crowds and those based on synthesis, which has been addressed in an ad-hoc fashion at best so far.
In the next section, we review the existing literature and contrast our method with related works. In Section 3, we describe the proposed framework in more detail. Section 4 dives into the description of how we address utility ex-plicitly by constructing a suitable learning target. Section 5 describes the feature space, the training set and the learning method. Section 6 talks about one strength of this method which can score and blend in synthetic suggestion candidates as well as query extensions observed in the search logs that are too rare for the collocation based methods to capture. These are followed by experimental results in section 7 and conclusions in section 8.
Leveraging  X  X isdom of crowds X  has been very popular in generating query suggestions. A large set of research articles focused on leveraging the session structure and other infor-mation to find alternate queries to suggest. The main idea is to find pairs of queries that frequently co-occur in users X  search sessions and use them as suggestions for each other. For example, Huang et al. [10] find such co-occurrences and rank the suggestions for an input query by frequency of co-occurrence, freq ( q 1 ,q 2 ), where q 1 is the input query and q 2 is a suggestion candidate (within a certain time win-dow in the same user session). Jensen et al. [12] considered point-wise mutual information (PMI) along with frequency of co-occurrence. PMI is defined as For a particular q 1 ,thePr( q 1 ) term is constant for all the possible suggestion candidates, and ordering given by PMI only depends on the ratio of the frequency of q 2 conditioned on q 1 divided by the marginal frequency of q 2 . While, fre-quency of co-occurrence is not a test of dependence, PMI is a valid test of independence. However, PMI favors rare events [24]. A preferred test of collocations in text corpora has been G 2 log-likelihood-ratio (LLR), introduced to NLP by Dunning [7]. Moore gives several formulations of G 2 in [17] and shows it is equivalent to mutual information (MI) where q denotes the set of all queries except q . MI addresses the bias of PMI by taking into account the probabilities of the complement events as well. LLR was found to be among the best performing statistical tests of dependence by Thanopoulos et al. [24] for extracting collocations. LLR was used by Jones et. al. [13] as a feature in a machine learning framework to expand queries for matching advertisements to an input query.

Boldi et al. [3] move to a more structured processing of the sessions by building a query-flow graph where the nodes are queries and edges are associated with weights that capture how likely a user is to move from one query to the next within a session. Then, neighbors with the largest edge weights are selected as suggestions for an input query. In [2], they also classified the transitions as specializations which are essentially query extensions, generalizations which are usually contractions, errors corrections and lateral moves, and use these categories for selecting suggestions for different purposes.

Another group of methods focused on leveraging the clicked documents by building a query-document bipartite graph. Assumption here is that similar queries have larger overlap between their respective clicks. For example, Mei et al. [15] use a random walk over the bipartite graph to find similar queries. Beeferman et al. [1] u se hierarchical agglomerative clustering iteratively to find groups of queries that are simi-lar and can be used as suggestions for one another, however, this is expensive. Baeza-Yates et al. [21] used an efficient k-means algorithm to find similar queries, but it requires the number clusters ahead of time. Sadikov et al. [22] combined the query-flow graphs with document click information to find query suggestions.

Query synthesis methods looked into generating sugges-tions by leveraging search logs as well as external informa-tion sources. Szpektor et al. [23] use a template generation method by leveraging WordNet [16]. Use of all token bound-aries in segmentation of the queries leads to many poor sug-gestion candidates. Jain et al. [11] use Conditional Random Fields to segment to queries, yielding better results. They also use a machine learned stage to filter bad suggestions but they do not address blending the suggestions with other methods such as those based on session analysis. Both in [11] and [13] the machine learning for suggestion scoring is based on editorial labels, whereas in our method we use probabil-ities estimated from the query logs as regression target. This section gives an overview of our proposed pipeline. The detailed description of each component follows in the subsequent sections. Below is a brief summary of each com-ponent.
The targets to be used in our machine learning model depend on Pr( q 2 ,q 1 ), namely the probability of query co-occurrence . In this paper, we define query co-occurence as when two queries are manually 1 issued by the same user within the same session . We define the user session as all the user activity within a time window limited by 10 minutes of inactivity. Nevertheless, the models we develop in this paper do not strictly depend on this definition and can be used with other timeout limits or other definitions of user session.

A first design choice is whether to consider all pairs of queries within the same session or consecutive queries only. The latter option makes the query co-occurrence set robust to intent drift in the search session. The intent drift can be defined as the gradual change in user X  X  intent as the search session progresses, and perhaps can be explained best by an example. Here is a real search session:  X  X ost cast X   X   X  X exter cast X   X   X  X ichael c. hall X   X   X  X ichael c. hall cancer X . The argument against restricting to consecutive pairs is that the majority of sessions do not have such intent drift and it results in a significant decrease in coverage. We will provide experiments with both of these choices.

The simplest approach for scoring suggestion candidates is to measure the reformulation probabilities from the fre-quency counts in the logs and pick the queries with the high-est reformulation probabilities Pr( q 2 | q 1 ),
We ignore queries suggested by the system because they may introduce a presentation bias .
 Here Pr( q 2 ,q 1 ) is the probability that q 2 occurs after q within the same session and Pr( q 1 ) is the marginal prob-ability of q 1 . There are two problems with this approach. Firstitcouldbethatthequery q 2 is unrelated to q 1 .This happens when q 1 and q 2 belong to different tasks that the user wants to solve. One way to address that issue would be to consider only queries within the same task, but this re-quires a system for detecting task boundaries [2] which itself can be prone to errors. In this paper we propose a solution that does not rely on task boundary detection. The second problem is that the result page associated with query q 2 not be useful, for example if the documents that q 2 retrieves are identical of those of the original query q 1 .
For these reasons, we say that, given a query pair ( q 1 ,q the query q 2 was a helpful reformulation of query q 1 if and only if the following two conditions are satisfied: 1. The query q 2 is a continuation of q 1 .Ifthequery 2. The query q 2 has a positive utility , that is the search
The details of these two conditions are described in the next two sections.
When trying to assess the utility of a reformulation, a simple criterion is to say that reformulations that lead to a click in the result page of q 2 are useful, and the others are not useful. In the extreme case, a query for which the search engine does not return any results cannot be a good suggestion by definition.

But reformulations followed by a click are not always use-ful. Consider the following query reformulations that ap-pear frequently in query logs  X  X ank of america X   X   X  X ank of america online X  or  X  X acebook X   X   X  X acebook login X . Although co-occurrences like these lead to a click on the result set of the second query q 2 , they do not likely take the user to a destination URL that is not already directly accessible from the original query q 1 . We thus define a reformulation to be useful only if it leads to a click on a URL that either is not existing in the search result page of q 1 or that is ranked higher than that in the search result page of q 1 . For formalizing this idea we compare the ranks of the same URLs in q 1 and q 2 (if any) and use the rank discounts that DCG uses. Let D c be the set of clicked documents on the result page of q 2 and r ( q, d ) returns the rank of the given document d  X  D c for the given query q and returns + inf if the URL is not ranked. The total difference in discounts of the clicked documents  X  is defined as When there was a click on q 2 , the reformulation is defined to be useful if  X  &gt; 0. This occurs only if at least one of the two conditions above holds for one of the clicked url.
In addition to the condition in (4), we also consider the queries that can address the user need directly in the search result page (without any clicks) as useful [5]. For example, weather information in the weather direct answer module for the query  X  X ake tahoe weather X , or current stock quote Table 1: Effect of the implicit session model (see section 4.3).
 facebook 0.0794 chase 0.0415 wells fargo 0.0227 capital one 0.0223 google 0.0185 and other details in the finance module for the query X  X mzn X  are examples of such queries, where the user does not need any clicks to get to the desired information. Therefore in addition to the condition in (4), if there is no further query reformulation in the session, and if the final query of the session contains such direct answer modules, we consider these queries to be useful reformulations as well.
To sum up, we define a query reformulation to be useful if either there was at least a click on q 2 and  X  &gt; 0; or there was no click on q 2 , the result page of q 2 contained a direct answer module and there was no further reformulation afterwards.
Remember that we want to compute the probability that q follows q 1 under the condition that q 2 is a continuation of q 1 (denoted by c = 1) and that it is useful (denoted by u = 1) as defined in the previous section; that is, First, note that u is an observed variable, while c is not. It is in particular straightforward to compute Pr( q 2 | q 1 ,u =1) by simply filtering out the pairs for which u =0. Inthe rest of this section, we will thus assume that all probabili-ties are implicitly conditioned on u = 1 and will drop that conditioning from the equations.
 The following equation holds:
Let us denote  X  =Pr( c =0 | q 1 ). In the case that the user issues an unrelated query ( c =0), q 1 and q 2 are inde-pendent and thus Pr( q 2 | q 1 ,c =0)=Pr( q 2 | c =0)thatwe approximate as the marginal distribution Pr( q 2 ). Equation (6) becomes:
In equation (7), both Pr( q 2 | q 1 )andPr( q 2 )areknown distributions and the distribution that we want to compute is Pr( q 2 | q 1 ,c = 1). At a high level, this can be achieved by subtracting  X  Pr( q 2 ) from Pr( q 2 | q 1 ). But we cannot do this subtraction naively as this could lead to negative probabilities. The correct way of computing Pr( q 2 | q 1 1) is a maximum likelihood estimation as explained below.
Let us consider for now that  X  is known and fixed. For a given query q 1 , let us denote the n unique queries q 2 that co-occured with q 1 as q 1 ,...,q n and let N q i be the number of co-occurences. With these notations, we have that Pr( q 2 q i | q 1 )  X  N q i . Finally, let p i =Pr( q 2 = q i | q 1 the probabilities that we want to evaluate. The maximum likelihood estimate can be formulated as the solution of the following optimization problem: under constraints,
In practice, in order to avoid solving a constrained opti-mization problem, we perform the following change of vari-ables, and we are left with a convex and unconstrained optimiza-tion problem on x i which can be solved using standard op-timization techniques such as non-linear conjugate gradient descent.

Note that in the extreme case  X  = 0, the optimal solution of (8) is p i = N q i / j N q j =Pr( q 2 = q i | q 1 ), which concurs with equation (7).

Theonlypartleftishowtoselect,foreach q 1 ,  X  =Pr( c = 0 | q 1 ). We use the following intuition: Pr( q 2 )hasavery large entropy, but the entropy of Pr( q 2 | q 1 ,c = 1) should be much smaller because there are only a limited number of queries that users are likely to type as a continuation of q . Our heuristic to select  X  is to find the value such that the resulting distribution p i has minimum entropy. On top of this entropy criterion, we add a Beta(1,10) prior on  X  , reflecting that for most queries q 1 , the following query q unrelated (large  X  ).

The importance of conditioning on c = 1 is illustrated in table 1. Without this conditioning, irrelevant but frequent queries such as  X  X acebook X  tend to have a high Pr( q 2 | q value.
This section first discusses the feature space that the rank-ing model uses and then the training process.
We use two main categories of features: lexical features and result set features as shown in Tables 2 and 3. Most of the features are intuitively simple and the short descriptions are enough, and the rest is detailed here. We do not use ses-sion log features because they are not defined for synthetic suggestions as well as other candidate suggestions.
The lexical features use the query strings themselves, like number of words in each query, number of words or charac-ters they share in the beginning or at the end, and Leven-shtein distance [14] between the two queries.

In the result set features, there are mainly two types: those that consider the quality of the results and those that consider the overlap of the two result sets. For assessing the quality of the results we use scores given by a learning to rank system (LTR), but any other relevance score such as BM25 could have been used. Queries that have better results should have higher average LTR scores. We use the LTR scores themselves, as well as the difference and the ra-tio, intuitively, a high quality query suggestion should have good results, even better if it has better results than the original query, which creates assisted paths to relevant and well formed queries.

Another group of features is the number of common URLs and domains in top K ranks. These aim to determine the overlap in the result sets of q 1 and q 2 , which has been used as a query-pair level relevance metric [20, 8]. We typi-cally expect the predictions to be high when the values of these features are in a medium range: when the overlap be-tween queries is too small, they are probably not related; andwhenitistoolarge,thequery q 2 does not contain any new information relative to q 1 (see section 4.2). Number of common URLs gives a strong signal but it is non-zero for only a very small portion of query pairs. To overcome this sparsity issue, we also use the number of common do-mains. One clear observation is that having results from the same domain is much more informative if the domain is a tail one. As the domain becomes more generic such as wikipedia.org or cnn.com having such a domain in com-mon becomes less meaningful. To capture this intuition, we use inverse query frequency (IQF) [6], similar to the inverse document frequency well-known in TF-IDF. IQF of a given domain d is In words, this is the log of the ratio of number of all queries to the number of queries that lead to at least one result with the given domain d . We generate another set of features using the sum of IQF of the common domains in top K ranks.

Result set aboutness : Aboutness vector similarity is by far the most important (and also the most complicated) result set feature that we use and merits special attention. The problem with the result set overlap based measures is that although they are effective at identifying queries that are very close in meaning (almost synonymous), the overlap drops very sharply to zero when the compared queries are relevant, but not almost identical in meaning. In the con-text of query suggestions, it is important to identify relevant queries not only almost identical ones. CODOMAINIQF solves this problem up to some extent. Yet, one needs compare not only whether the pair of queries return the same results, but also whether the two queries have results that are not identical but are about the same or similar concept. This will give an idea on the semantic relation between a pair of queries.

For example,  X  X oyota prius X  and  X  X oyota yaris X  are quite related and would be considered good suggestions although have no results in common in top 10, based on the results returned by a web search engine. Hence, the result set over-lap is insufficient to assess relevance. This particular one is an example that CODOMAINIQF cannot also handle, because although the queries are quite relevant, the common do-mains that they have are quite popular domains with low IQF scores ( toyota.com , autos.yahoo.com , autos.msn.com , wikipedia.org ).

To build a semantic similarity metric, we construct an aboutness vector of each query, which can be considered as a bag-of-words representation based on the web results, which consists of the concepts  X  X hat are part of a predefined con-cept dictionary X  X n the documents returned for this query. This is based on previous work [19] which computed for each concept and each document an aboutness score defined as the probability that a user interested in this concept would find the document relevant. Algorithm 1 explains how to extend the computation of this aboutness score from docu-ments to query. The score for a query is a weighted sum of the scores of the documents returned for that query, where the weights put more emphasis on documents ranked higher. Finally the aboutness similarity of a query pair is the cosine similarity between the corresponding aboutness vectors. Ta-ble 4 shows examples of queries, and their top 20 concept terms ranked in descending order with respect to S ( t ), the aboutness score of the concept term t . Although quite re-lated, the pairs of queries presented here have zero results in common. Algorithm 1 Algorithm to compute the aboutness vector 1: Retrieve set R of top-k results for q . 2: for concept t  X  D do 3: S ( t )=0 4: for i =1 , .., k do 5: d = i -th document in R . 6: if concept t is in d then 7: a = aboutness score of concept t in D [19]. 9: end if 10: end for 11: end for 12: Set S ( t ) to 0 for the concepts t that are not in the top 20
We first compute all probabilities of co-occurences Pr( q based on one year of a commercial search engine logs. For each query pair, we use the utility and the implicit task boundary detection models to compute the training targets Pr( q 2 | q 1 ,c =1 ,u = 1), and the features as explained above.
Since training a model on all query pairs would be time consuming, the pairs are subsampled as follows. We first select a subset of queries q 1 .Eachquery q 1 is included in that set with probability min(1 ,c 1 Pr( q 1 )), where c a constant. This results in a set of 13,220 queries q 1 .Sim-ilarly, for each q 1 , we select a subset of q 2 with probability min(1 ,c 2 Pr( q 2 | q 1 )). This sampling insures, at both levels, that the head queries are selected as well as some of the tail queries. The median size of q 2 for a given q 1 is 30 and the total number of training instances is 382,740.

The learning algorithm is Gradient Boosted Decision Trees (GBDT) [9]. All the hyperparameters of this algorithm X  number of trees, number of nodes, shrinkage factor X  X re tuned on a separate validation set.
We review in this section the three different sources of candidates that will be scored by our model.
The most obvious source of candidates come from the co-occurences in the logs. In order to reduce the number of suggestions, for head queries in particular, we only consider the queries q 2 which co-occured at least 3 times with q 1
But relying only on query logs still limits the coverage of query suggestions and many queries with a low query log fre-quency will remain with a few or no suggestions. To further increase the coverage, we use additional sources of infor-mation as well, and the rest of this section briefly presents these sources; synthetic suggestions [11], and most frequent specializations.
Here we use a recent work on a synthetic query suggestion generation method that combines a number of unit level operations such as dropping words or several ways of word replacements, to build synthetic query suggestions. Below we give a very brief summary of the method, and for further details please refer to the original paper [11]. Also, note that there is nothing in the relevance model or the feature space that is specifically tuned for this particular method, and one could have used any other query suggestion generation method as well [16, 23]. This is one of the biggest advantages of the proposed framework since it is flexible to work with any type of suggestion candidates since the features do not require historical data from the past session logs.
The first step of the synthetic query generation method we use is a unit importance model, which segments the query into units and assigns importance weights to each unit. Af-terwards, less important units of the query are dropped, or replaced with other contextually relevant units. The unit re-placements come from sources like: (i) queries that co-occur frequently (same as in pmi , llr ) (ii) phrases from queries that lead to clicks to same URLs ( X  X hanksgiving recipe X  and  X  X urkey recipe X  lead to clicks on same documents, then  X  X hanksgiving X  and  X  X urkey X  are substitutable units in the context of  X  X ecipe X ).

Probably the best way to briefly explain how the synthetic query generation works is walking through a few examples. The query  X  X ig lots store X  does not have many good sugges-tions as compared to X  X ig lots X , due to much lower frequency. Here, after the importance model decides the term  X  X tore X  can be dropped, it brings the queries that are frequently co-occurring with  X  X ig lots X  as suggestions for the query  X  X ig lots store X . The query  X  X ost cutters new jersey X  does not have any suggestions, again due to low frequency. The im-portance model decides X  X ost cutters X  X s more important, and drops  X  X ew jersey X . Queries that frequently co-occur with  X  X ost cutters X  are identified and the dropped place name is added back to these to generate synthetic suggestions like  X  X upercuts new jersey X ,  X  X reat clips new jersey X . For details on the importance models determines which terms to drop and so on, please refer to the original paper [11].
A significant portion of the query reformulations are ex-tensions, where the user adds more terms to the original query. Even in the cases without a reliable co-occurrence signal, in many cases the most frequent queries that contain the current query can be relevant and useful suggestions. In fact, this idea also has been the backbone of the query com-pletion features in the search industry. The drop-down table that suggests completions of the query as the user is typing is based on the most frequent queries that contain the user entered portion of the query.
We will use up to 20 most frequent queries that contains the original query as a suggestion source, but with one mod-ification; we use a word boundary condition to bring these completions where the query is a full word, so that if the query is  X  X wk X  it is not completed to  X  X wkward family pic-tures X  or  X  X wkward tv moments X  as it would in query com-pletion, but rather bring suggestions like  X  X wk example X  or  X  X wk tutorial X .

Although it does not bring nonsensical queries as in the synthetic suggestions method, this can also bring many ir-relevant suggestions and again cannot be used directly. For example for the query  X  X ream X , among many good sugges-tions like  X  X ream interpretations X  or  X  X ream dictionary X  this also brings X  X ream theater X , a progressive rock band that has nothing to do with the meaning of the original query. We use the machine learning model to get rid of such irrelevant suggestions.
We conducted a careful, thorough analysis of the proposed system to verify its effectiveness. In this section, we first review the experimental setup and then offer the empirical analysis through the discussion of the major findings. Query Set : We collected a random sample of 912 fully anonymized queries issued on a commercial search engine according to their frequency.
 Manual Annotation : All manual annotation tasks de-scribed were performed by a group of eight professional search engine quality evaluators experienced with assessing the qual-ity of query suggestions and search results.
 Annotation Guidelines : Professional annotators provided 4-level ratings (excellent, good, fair, bad) for the ranked suggestions for these 912 unique queries. Annotators were asked to base their judgement after looking at the results page and comparing those for the query and the suggestion; this is important to capture the utility of a suggestion. Variations of the system : We designed a systematic evaluation where we tested different versions of our system against the baseline. We briefly review each version below:
B : As a baseline, the candidates are ranked according to mutual information (2). Only the suggestions with a score larger than 50 are kept, and the threshold value is set em-pirically by optimizing the trade-off between the quality and coverage. 10M : Score of a suggestion is directly computed by the utility estimation Pr( q 2 | q 1 ,u =1 ,c =1). Thereisno machine learning model and only the suggestions from the query logs are considered. Co-occurrence of all query pairs are considered within a session of 10 minutes, as described in Section 4. 10M-ML : Scores are predicted by the GBDT model. Only the suggestions from the query logs are considered 10M-ML-SY :Sameas 10M-ML with the difference that syn-thetic suggestions (section 6.2) and most frequent special-izations (section 6.3) are also scored. 10MC, 10MC-ML, 10MC-ML-SY : Same as the above three models except that only consecutive pairs in a session are treated as co-occurrences -to avoid the intent drift in the sessions.

We did not consider a baseline trained on the labeled data alone as an interesting baseline to report on. The reasons can be summarized as follows. With the exception of query substitutions work of Jones et. al. [13], most notable query suggestion research focused on sources other than editorial labels for training machine learning systems. Editorial data is expensive, and generalizing to the tail at web scale re-quires a lot of labeled data. Search result ranking is proba-bly a simpler problem than query suggestion ranking since, the latter attempts to learn utility left over from the cur-rent search. Yet, state of the art methods for learning to rank search results utilize tens of thousands of labeled ex-amples (see Yahoo! Learning to Rank Challenge [4]). Learn-ing to suggest properly would probably require more. Jones et. al. can do with a training set of 1000 query pairs be-cause their task is bid term generation which is much simpler than suggesting queries for various reasons. First, the task is designed to retrieve query substitutions for the current search (so ad coverage can be increased), rather than utility for subsequent searches. Second, non-sensical substitutions are filtered by matching to the advertisers X  bid terms. And last but not least, even the surviving ones are not shown to the user directly. Training on the session data holds other advantages as well. User feedback on additional utility of a suggestion given the current search is directly captured, which is difficult in the editorial data. Our system, on the other hand, does not rely on a large set of labeled data but rather simulates target labels based on the likelihood of the suggestions.
 Offline Evaluation : The offline analysis relies on the edito-rial judgments and there are two major criteria that we deem important to measure. First is the ability of the system to rank the good quality suggestions higher. The second mea-sures the number of queries for which the system is not able to bring any suggestions. These two criteria is analogous to the precision-recall tradeoff in standard document retrieval problems. We adopt DCG and Precision at various cut-off points to measure the quality of the ranked suggestions. where l i q  X  X  0 , 1 , 2 , 3 } is the graded relevance (3 is the best) of the suggestion ranked at position i for a given query q . r is the binary version where the good and excellent labels are transformed into 1 and the rest is assigned 0. The overall DCG and Precision are calculated by averaging over all the queries in the test data.

We defined the coverage as the ratio of the number of queries the system could bring suggestions for to the total number of test queries. Coverage measures the likelihood of a system to find suggestions for a given query. Therefore, it complements the DCG and Precision metrics.
 Online Evaluation : For the online analysis, we conducted an A/B test where we tested and compared the proposed sys-tem ( 10M-ML-SY implementation) against the baseline on live traffic. Both systems are tested on randomly sampled user populations without imposing any bias towards a particu-lar group of users. The suggested queries appear as related searches at the bottom of the result page. Users interactions with the suggested queries are logged in the query logs for a period of one week. To ensure production quality and suffi-cient diversity of the suggestions, we further remove the low utility queries that are near-dup licates of already suggested queries. The duplicate elimination method is identical in both baseline and test buckets; the details of the model is explained in detail in [18]. We measured the CTR on the suggestions that our system ranked and compared it against the CTR on those ranked by a MI baseline. Here CTR at position k is defined as number of clicks within the first k suggestions divided by number of result pages with at least k suggestions.

A/B tests are expensive in the sense that they require en-gineering resources to build the necessary online platform to do a full comparison. For this reason, instead of imple-menting online tests for each variation of the model, for the online test we picked 10M-ML-SY , one of the best performing models in the offline comparisons, with respect to coverage and the quality metrics based on human judgments.
In Table 5, we show the relative performance of different versions of the proposed system against the baseline with respect to DCG, Precision and Coverage metrics at vari-ous cut-off points (depths). The DCG and Precision val-ues are calculated on the queries that both systems bring suggestions at the specified depth; hence we refer to it as common coverage in Table 5. Generally, the performance gap increases with depth where the largest gaps occur at depths 9 and 12. At depths 9 and 12, the best performer in terms of DCG against the baseline is 10MC-ML-SY followed by 10M-ML-SY and 10M-ML . The same is also true in terms of Precision. This suggests that i) the machine learned models outperform the systems that directly uses estimated target scores Pr( q 2 | q 1 ,u =1 ,c = 1) without any learning, ii) the system is able to blend the additional candidates very well that the overall quality of the suggestions are as good as the organic counterpart. Furthermore, the performance of the proposed approach (in both DCG and Precision) ei-ther outperforms or is comparable to the strong MI (Mutual Information) baseline at all depths.

The coverage remains relatively flat across different meth-ods at small depths. This is not surprising since all methods are effective in bringing at least a few suggestions for a given query. However, the coverage difference becomes more evi-dent at larger depths where the real benefit of the implicit task boundary model and the machine learning step can be assessed. At depths 9 and 12, the best coverages are those of 10M-ML-SY and 10MC-ML-SY . We are quite encouraged by this result since it demonstrates that our approach can blend the additional candidates into organic ones without hurting the performance while maintaining a high coverage. The cover-age of 10MC-ML is quite low compared to these two, since the higher quality is suffered by a loss in coverage for 10MC-ML . The coverage of the baseline, on the other hand, is signifi-cantly lower than the rest at larger depths even though it has comparable coverage to others at smaller depths.
In Figure 1 we demonstrate how the editorial judgments are distributed for the baseline, and the full machine learn-ing model 10M-ML-SY that is used in the online test. To be able to show how much of the gain comes from the implicit task boundary model, and how much of it comes from the machine learning step, we present the grade distributions of the implicit task boundary model 10M as well. We compared the grade distribution at different cut-off points from top 1 to top 12. Also, the grades presented in the histograms are computed over the each individual coverage of the methods, hence they are different than those in 5, which compares the relevance over the common coverage. A few things to note in Figure 1 are: (i) 10M and 10MC-ML-SY do perform better not only with respect to the common coverage (as presented in Table 5) but also over their entire coverage, which is signif-icantly bigger than the common coverage with the baseline. (ii) At every rank cut-off, the ratio of good and excellent suggestions are higher and the ratio of fair and bad sug-gestions are lower for both 10M and 10MC-ML-SY ,hencethe grade distributions are more skewed towards better grades. (iii) Although the implicit task boundary model 10M is bet-ter than the baseline in terms of total number of good or excellent suggestions, the machine learning step has a more important contribution in bringing a lot more good quality suggestions without increasing the ratio of bad suggestions.
The results of the online analysis are similarly encourag-ing. We compared our system against the MI baseline in terms of both CTR and coverage. We note that the defini-tion of coverage and CTR in the online evaluation is slightly different that those used in the offline analysis. In the online version, the coverage is weighted by query frequency. On the other hand, the click ratios are calculated separately for each system on the respective queries where there is a suggestion. The results are presented in table 6. They show that our sys-tem not only increases the chance of finding suggestions for a given query, it also ensures a significantly higher quality of these suggestions. Additionally, we report a 0 . 9% decrease in the next-page clicks using our system. Users X  needs for a suggestion are higher at the bottom of a page since they view the top 10 results before they see the related searches at the bottom. The fact that the CTR increases and the next-page clicks decreases indicate to some extent that the suggestions were useful.

To understand what the model does, it is instructing to look at which features were the most relevant in the GBDT model. Overall result set features are much more important than lexical features, with ABOUTNESS and some LTR based scores being the most relevant ones. This is not surprising since the result set features are more sophisticated features. Among the lexical features LEV and more generally features that depend on both q 1 and q 2 were more heavily used by the GBDT model.

Finally Table 7 lists the results of the two models for a few sample queries.
In this paper we present an end-to-end query suggestion method that implements novel ideas such as incorporating usefulness of reformulations, an implicit session boundary model, and a machine learning model to further improve the suggestion relevance and be able to add more sources of suggestions beyond the co-occurrences in query logs. The idea of incorporating usefulness into the query co-occurrence models is not specific to our particular selection of target, and it can be used directly with PMI, LLR, and other sim-ilar measures as well. Even without the machine learning step, the reformulation relevance model shows significant im-provements over MI, significant gains in coverage and mod-est gains in relevance. The feature generation and machine learning step brings some further relevance improvement, and allows us to add other candidate sources, which signifi-cantly increases the coverage. This is a particular advantage of our approach against the related work in the literature. We trained the machine learning model with targets gener-Table 7: sample queries and their suggestions for 10M and the baseline ated from session logs via the utility and imp licit ta sk bound-ary models, which removes the dependency on large labeled data. The offline as well as the online evaluation demon-strate the effectiveness of the proposed framework against MI. We observed significant improvements in coverage and quality metrics. The click through rates on the online tests are very promising and we plan to extend this work with personalization and further diversification of suggestions. versus the baseline.
