 Automatic Document Classification (ADC) is still one of the major information retrieval problems. It usually employs a supervised learning strategy, where we first build a classi-fication model using pre-classified documents and then use this model to classify unseen documents. The majority of supervised algorithms consider that all documents provide equally important information. However, in practice, a do-cument may be considered more or less important to build the classification model according to several factors, such as its timeliness, the venue where it was published in, its authors, among others. In this paper, we are particularly concerned with the impact that temporal effects may have on ADC and how to minimize such impact. In order to deal with these effects, we introduce a temporal weighting func-tion ( TWF ) and propose a methodology to determine it for document collections. We applied the proposed methodol-ogy to ACM-DL and Medline and found that the TWF of both follows a lognormal. We then extend three ADC algo-rithms (namely kNN, Rocchio and Na  X   X ve Bayes) to incorpo-rate the TWF . Experiments showed that the temporally-aware classifiers achieved significant gains, outperforming (or at least matching) state-of-the-art algorithms. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.5.4 [ Applications ]: Text process-ing; Algorithms, Experimentation This work was partially supported by CNPq, CAPES, FINEP, Fapemig, and INWEB.
 Classification and Clustering, Text Mining
Text classification is still one of the major information re-trieval problems, and developing robust and accurate classi-fication models continues to be a relevant demand, as a con-sequence of the increasing complexity and scale of current application scenarios, such as the Web. The task of Auto-matic Document Classification (ADC) aims to create models that associate documents with semantically meaningful cat-egories, and these models are key for building spam filters and topic directories, identifying documents writing style, creating digital libraries, and guiding a user X  X  search on the World Wide Web.

ADC usually follows a supervised learning strategy, in which a classification model is built using some training (pre-classified) documents and later employed to classify a new set of unseen documents. The majority of supervised algo-rithms consider that all documents provide equally impor-tant information. However, in practice, a document may be considered more or less important to build the classification model according to several factors, such as its timeliness, the venue where it was published in, its authors, among others.
In this work we are particularly concerned with the im-pact that temporal effects may have on ADC and how to minimize such impact. Consider, for instance, the terms pheromone and ant colony . Before the 1990s, they referred exclusively to documents in the area of Natural Sciences. However, after the introduction of the technique of Ant Co-lony Optimization in the area of Artificial Intelligence, these terms became relevant for classifying Computer Science doc-uments too. Previous work has demonstrated that temporal effects, such as the variation of the strenght of term-class re-lationship over time, may have a significant impact on ADC, and strategies for assessing these effects and their impact on ADC have already been devised [19].

In general, methods proposed to deal with temporal ef-fects are based on three main approaches: instance selection, instance weighting, and ensembles. Instance selection [20] uses heuristics to decide which instances should be used (or the time intervals that contain those instances) to create a classification model. However, tuning these heuristics to select the most relevant documents is a challenge, since we may easily gather too many or too few documents. Methods based on instance weighting [11] may ameliorate this prob-lem 1 . However, this strategy raises the additional challenge of determining the weighting functions and their parame-ters, which are collection-dependent and usually performed in ad-hoc way. Finally, approaches based on ensembles, that correspond to the combination of various classification mod-els generated from different cla ssification algorithms, present the challenge of how to manage, efficiently, several models simultaneously [7].

This paper proposes a strategy to incorporate temporal models to document classifiers, aiming to address the two main drawbacks of instance selection and instance weighting approaches. Our strategy is based on the evolution of the term-class relationship over time, captured by a metric of dominance . We start by determining a temporal weighting function for a collection according to its characteristics. We found that this function follows a lognormal distribution for the datasets we used.

The next step is to incorporate the temporal weighting function to ADC algorithms and we propose two strategies that follow a lazy classification approach. In both strate-gies, the weights assigned to each example depend on the notion of a temporal distance  X  , defined as the difference between the time of creation p of a training example and a reference time point p r . The first strategy, named temporal weighting on documents , weights training instances accord-ing to  X  . The second strategy, called temporal weighting on scores , is based on an ensemble of classifiers, one for each pair class c, time point p . In this case, the scores (e.g., similarities, probabilities) returned by the respective classi-fiers for each pair c, p are weighted according to  X  .The combined weighted scores are then used to take the final classification decision. We specifically show how these two strategies are implemented in three traditional ADC algo-rithms, namely, Rocchio, k Nearest Neighbors (KNN), and Na  X   X ve Bayes.

We evaluated our strategies using two actual digital li-braries that span for decades, ACM-DL and MedLine, and achieved significant improvements on classification effective-ness for all classifiers. For instance, the temporal-aware ver-sion of Na  X   X ve Bayes outperformed by up to 10% the state-of-the-art classifier (SVM), while presenting an execution time up to hundreds of times faster.
Although document classification is a widely studied sub-ject, the analysis of temporal aspects in this class of al-gorithms is quite recent  X  it has been studied just in the last decade. As previously mentioned, strategies to deal with these effects involve one or more of three approaches, namely: instance selection, instance weighting, and ensem-bles. Here we review the most relevant works of two broad areas where there has been significant efforts in terms of temporal effects on classification: adaptive document classi-fication and concept drift.

Adaptive Document Classification [4] encompasses a set of techniques related to temporal aspects with the goal of improving the effectiveness of document classifiers through their incremental and efficient adaptation. Adaptive Do-
The first strategy may be seem as an instance weighting strategy with binary weights. cument Classification brings three main challenges to text mining [17]. The first one, and most relevant to this re-search, is the notion of context and how it may be exploited towards better classification models. Previous research in document classification identified two essential forms of con-text: neighbor terms that are close to a certain keyword [14] and terms that indicate the scope and semantics of the do-cument [2]. The second challenge is creating the models incrementally [10]. The third challenge has to do with the computational efficiency of the document classifiers. Meth-ods for Adaptive Document Classification usually follow an instance selection approach, as they select semantic contexts based on, for instance, the co-occurrence of terms, not tak-ing into account all documents in the training set.
Concept or topic drift [22] comprises another relevant set of efforts to deal with temporal effects in classification. To deal with concept drift, a prevailing approach in the lit-erature is to completely retrain the classifier according to a sliding window. This involves instance selection and in-stance weighting techniques [12, 11, 23, 15]. The method presented in [12], for instance, maintains a window with documents sufficiently  X  X lose X  to the current target concept and automatically adjusts the window size so that the esti-mated generalization error is minimized. In [11], the meth-ods presented either maintain an adaptive time window on the training data, select representative training examples, or weight the training examples. In [23] the authors describe a set of algorithms that react to concept drift in a flexible way and can take advantage of situations where contexts reap-pear. The main idea of these algorithms is to keep only a window of currently trusted examples and hypothesis, and store concept descriptions in order to reuse them if a previ-ous context reappears. Unlike previous works, which use a single window to determine drift in the data, in [15] the au-thors present a method that uses three windows of different sizes to estimate the change in the data. While algorithms that use a window of fixed size impose hard constraints over drift patterns, those that use heuristics to adjust the win-dow size to the current extent of concept drift often involve lots of parameters to be calibrated. The approach proposed in this paper relies on statistical properties of the collection to assess the temporal effects, solving such drawbacks, while promoting a high quality classification.

Other common approach to deal with concept drift fo-cuses on the combination of various classification models generated from different algorithms (ensembles) for classifi-cation, pruning or adapting the weights according to recent data [21, 13, 7]. In [21], the authors propose a boosting-like method to train a classifier ensemble from data streams. It naturally adapts to concept drift and allows to quantify the drift in terms of its base learners. The algorithm was shown to outperform learning algorithms that ignore con-cept drift. In this same direction, Kolter et al. [13] present a technique that maintains an ensemble of base learners, predicts instance classes using a weighted-majority vote of these  X  X xperts X , and dynamically creates and deletes experts in response to changes in performance. In [7], a method that builds an ensemble of classifiers using Genetic Programming (GP) to inductively generate decision trees is presented. However, how to manage, efficiently, several models simul-taneously remains a challenge. To address such drawback, we propose an approach based on the combination of vari-ous classification models, but with a simpler way to manage them.

Another approach that can be easily compared to ours, and is based on instance selection, is the one proposed in [20]. In [20], the authors introduce the concept of temporal con-text , defined as a subset of the documents collection that minimizes the impact of temporal effects in classifiers X  per-formance. An algorithm named Chronos was proposed to identify these contexts based on the stability of the terms in the training set. The temporal contexts were then used to sample the training documents for the classification process. Hence, training documents that were considered to be out-side the temporal context were discarded by the classifier.
In contrast with the aforementioned works, here we pro-pose an approach to classify documents in scenarios where we may have information about both the past and the future, and this information may change over time. It should be no-ticed, however, that our approach may be easily adapted for scenarios where we only have past information, such as Adaptive Document Classification and Concept Drift. Moreover, we address the drawbacks of which instances to select by approximating a temporal weighting function using a lognormal distribution, and may easily tune its parameters using statistical methods.
As mentioned before, the potential impact that certain temporal effects have on term-class relationships may have a great influence on the results of the classification process, as showed in [19]. Thus, incorporating information about these changes into the classification process has the potential to improve its effectiveness.

We address this issue through a temporal weighting func-tion (TWF) that quantifies the influence of a training do-cument while classifying a test document, as a function of the temporal distance between their creation times. We dis-tinguish two major steps in determining such function: its expression and its parameters. The expression is usually harder to determine, since it may express the generative pro-cess behind the function, while the parameters are usually obtained using approximation strategies.
 Intuitively, given a test document to be classified, the TWF must set higher weights to training documents that are more similar to that test document w.r.t. the strength of term-class relationships. One metric that expresses such strength is the dominance [20], since the more exclusive a term is to a given predefined class, the stronger this rela-tionship. Dominance can be formally defined as: where N tc stands for the number of documents in class c that contain term t .

For ease of understanding, before we continue the discus-sion about the temporal weighting function, we describe the two document collections for which we want to determine the functions: ACM Digital Library (ACM-DL) and the Med-Line. The ACM-DL has 24 . 897 documents containing arti-cles related to Computer Science created between the years of 1980 and 2002. We considered only the first level of the taxonomy adopted by ACM, including 11 categories, which did not vary during this period of time. The second one is derived from the MedLine collection, and has 861 . 454 doc-uments, classified into 7 distinct classes related to Medicine and created between the years of 1970 and 1985. In both collections, each document is assigned to a single class.
We start by defining the tempora l granularity of the weight-ing function, which should be the minimum time interval be-tween relevant changes in the collection (e.g., days, weeks, or years). Since both collections contain scientific documents, it is intuitive that a year granularity is representative, once documents are usually published yearly (scientific confer-ences are usually annual).

The simplest approach would be to use a pulse function at temporal distance 0, that is, the pulse magnitude is propor-tional to the term dominance associated with the training documents produced in the same year of the test document. However, as pointed by [19], considering a larger time inter-val instead of a single time point is better, since the influ-ence decreases with the increase of the temporal distance. We then need to determine the time period that must be considered, which we call stability period. Notice that each term may present a different stability period for each year when it occurred in the collection. We first determine the stability period for each term and then combine them, as follows.

One approach for the first step is presented in [20], where a stability period S t,r of a term t , considering the refer-ence time point p r in which the test document was created, consists of the largest continuous period of time, starting from p r and growing both to the past and the future, where Dominance ( t, c ) &gt; X  (for some predefined  X  and any class c ). In the case of the collections ACM-DL and Medline, we investigated different values for  X  when computing stabil-ity periods and, as they lead to similar results, we adopted  X  = 50%, ensuring that the terms will have a high degree of exclusivity with some class.

We then combine the stability periods S t,r for each term t and each reference time point p r in the collection. A difficulty in this case is related to the fact that a term may present different stability periods for different reference years. In order to avoid this problem, we mapped all the time points in a stability period to temporal distances, where the reference year is considered as distance 0. For instance, aterm t 1 may have different stability periods when consider-ing the years 1989 or 2000 as a reference. More specifically, if the stability period of t 1 is { 1999,2000,2001 } regarding p = 2000, and { 1988,1989,1990 } regarding p r = 1989, these periods would be both mapped to { -1,0,1 } . Considering S as the set of temporal distances that occur on the stability periods of term t (considering all reference moments r ,then S = {  X   X  p n  X  p r | X  rp n  X  S t,r } . Making the stability peri-ods easily comparable is important because our real interest is to know what kind of distribution this temporal distances follow w.r.t. different terms.

The next step is to determine the function expression and, towards this goal, we considered the stability period of each term as a random variable (RV), where the occurrence of each possible temporal distance in its stability period is an event. More formally, as Table 1 shows, we are interested in the frequencies of the temporal distances  X  1 to  X  n ,for terms t 1 to t k . An interesting property that we may test is whether these RV X  X  are independent. This hypothesis can be corroborated by the Fisher X  X  Exact Test to assess the independence of each RV i and RV j ,  X  i = j [3], where, as mentioned, each RV represents the occurrence of a temporal distance  X  for a term t .

We applied this test to both ACM-DL and Medline and obtained a p-value of 0 . 99 through a Monte Carlo simulation, which allows us to state that the random variables consid-ered are indeed independent. Thus, the observed variability of occurrences of  X  for different terms is a result of indepen-dent effects [16]. However, it is still not clear whether the ef-fects responsible for the observed variability can be additive (leading to a normal distribution) or multiplicative (leading to a lognormal distribution). We then apply a statistical normality test. According to D X  X gostino X  X  D-Statistic Test of Normality [6], with 0 . 01 significance level, we found that the lognormal distribution best fits both the ACM-DL and Medline collections, as presented in Table 3.

Consider that the RV D  X  related to the occurrences of  X  , which represents the distribution of each  X  i over all terms t , is lognormally distributed if lnD  X  is normally distributed. More generally, since  X  i are RV X  X  under the independence assumption with finite mean and variance, then, by the Cen-tral Limit Theorem, lnD  X  = approach a normal distribution and, by definition, converges to a lognormal distribution [5]. For a lognormal distribution, the asymptotically most efficient method for estimating its associated parameters relies on a log-transformation [16]. Using a Maximum Likelihood method, we estimated those parameters for both collections, and then back-transformed them, as shown in Table 2. We considered a 3-parameter gaussian function, F = a i e the height of the curve X  X  peak, b i is the position of the cen-tre of the peak, and c i controls the width of the curve. The last one, also called the shape parameter, reflects the nature of the variations of term-class relationships over time. Since abrupt or smooth variations lead to small or greater stability periods, respectively, the shape of the distribution changes accordingly, being a matter of parameter estimation to cap-ture such distinct natures. We performed two curve fitting procedures, considering a single gaussian F and a mixture of two gaussians, given by G = G 1 + G 2 ,whereeach G i de-notes a gaussian function. The last one was the model that best fitted D  X  , and its parameters are presented in Table 2, along with the goodness of fitting measure Adjusted-R 2 .The Adjusted-R 2 measure denotes the percentage of variance ex-plained by the model and, for both collections, the obtained model explains 99% of such variance.

The greater the frequency of  X  on stability periods, the more suitable training documents created in  X  are to build an accurate classification model, making the modelling of the Temporal Weighting Function as a lognormal distribution an effective strategy. To account for the problem faced when the scale of the score is not compatible with the algorithm input, we include a scaling factor  X   X  R , that is algorithm specific and will be defined in Section 5.
 Figure 1 shows the distribut ion of temporal scores, when Table 2: Estimated parameters for both collections, with 99% confidence intervals.  X  = 1, for each possible temporal distance between the cre-ation time of test document d and the training documents for both the ACM-DL and the Medline collections.
 Table 3: D X  X gostino X  X  D-Statistic Test of Normality. Bold-face for tests that we can not reject the null hypothesis of normality. Figure 1: Fitted temporal weighting function with log-transformed data.
This section shows how three well-known text classifiers, namely Rocchio, KNN and Na  X   X ve Bayes [18], can be mod-ified to take into account the temporal weighting function defined in Section 3. The three algorithms are modified fol-lowing two strategies: temporal weighting on documents and temporal weighting on scores, as detailed below.
The temporal weighting on documents strategy weights each training document by the temporal weighting function according to its temporal distance to the test document d , as detailed next.

The strategy to incorporate the weight of each training document to a given classifier depends inherently on the characteristics of the classification algorithm being modified. For example, while Rocchio and KNN classify new instances basedonadistancemetric,Na  X   X ve Bayes is a probabilistic classifier that assigns to a test document the most probable class that would have generated d , adopting some na  X   X ve as-sumptions such as positional and conditional independence of terms.

In the case of distance-based classifiers, the temporal wei-ghting function can be easily applied when calculating the distance between the training and test documents, by weight-ing each training document ( TF -IDF vector) by its associ-ated temporal weight. In the case of the Na  X   X ve Bayes, the temporal function can be used to weight the impact of each training example in both the a priori and conditional prob-abilities, in order to generate a more accurate a posteriori probability.

Rocchio Rocchio is an eager classifier that uses the cen-troid of a class to find boundaries between classes. As an eager classifier, Rocchio does not require any information from d to create a classification model. Hence, we will have to adapt it to become a lazy classifier when using the tem-poral weighting function, since the weights depends on the creation time of a test document.

The centroid of a class is defined as the average value of all its training examples. When classifying a new document d , Rocchio associates it to the class represented by the centroid closest to d . In order to make Rocchio a lazy classifier, we have to change the separation boundaries of classes accord-ing to the temporal weights produced by our function.
Hence, it needs to calculate each Rocchio X  X  class centroid based on the creation time p r of a test document d .Con-sider the set of training documents d of class c .Thissetcan be partitioned into subgroups of documents created at the same temporal distance  X  from p r , i.e., subgroups of docu-ments created at a temporal distance of 1 or  X  1,2or  X  2, and so on. The centroid ing the documents vector representations with the score pro-duced by the temporal function TWF (  X  ), obtained using the temporal distance  X  between the creation time point of d and d .Thus,acentroid where D c is the number of documents in class c , X isthe set of all possible temporal distances between the training documents and the test document d ,and training document with temporal distance  X  from d .
This approach redefines the centroid X  X  coordinates in the vectorial space considering document X  X  representativeness on class c w.r.t. the reference time point p r . Both training and classification procedures are presented in Algorithm 1. Algorithm 1 Rocchio-TWF-Doc: Rocchio with Temporal Weighting on Documents
KNN KNN is a lazy classifier that assigns to a test do-cument d the majority class among those of its k nearest neighbor documents in the vector space. Determining the test document X  X  class from the k nearest neighbors training documents may not be ideal in the presence of term-class re-lationships that vary considerably over time. To deal with it, we apply the proposed temporal weighting function during the computation of similarities among d and the documents in the training set, aiming to select the closest documents, in terms of both similarity and temporality.

Let s be the cosine similarity between a training document d and d .If d is similar to d but is temporally distant, then it is moved away from d , reducing the probability of being among the k nearest documents of d .Let TWF (  X  )be the temporal weight associated with the temporal distance between the time of creation of documents d and d . Then, the documents X  similarity is given by:
Both training and classification procedures are presented in Algorithm 2.
 Algorithm 2 KNN-TWF-Doc: KNN with Temporal Weighting on Documents
Na  X   X ve Bayes Na  X   X ve Bayes is a probabilistic learning me-thod that aims to infer a model for each class, assigning to d the class associated to the most probable model that would have generated d . Here we adapt the Multinomial Na  X   X ve Bayes approach [18], since it is widely used for the proba-bilistic text classification. Similarly to the previously defined  X  X emporal weighting on documents X  approaches, here we ap-ply the temporal weighting function on the information used by the learning method, namely the relative frequencies of documents and terms, as follows: P ( d | c )=  X   X  where  X  denotes a normalizing factor, N cp is the number of training documents of D assigned to class c and created at time point p , N p is the number of training documents created at time point p , f tcp stands for the frequency of oc-currence of term t in training documents of class c that were created on time point p and, finally,  X  denotes the temporal distance between time point p and the creation time of d .
The main goal of this strategy is to reduce the impact that temporally distant information have when estimating a posteriori probabilities. Algorithm 3 presents this strategy.
A more sophisticated approach to exploit the temporal weighting function considers the  X  X cores X  produced by the traditional classifiers, as listed in Algorithm 4. By score we mean: (i) the smallest distance from the test document d to a class centroid for Rocchio; (ii) the smallest sum of the Algorithm 3 Na  X   X ve Bayes TWF-Doc: Na  X   X ve Bayes with Temporal Weighting on Documents distances of the K-nearest neighbors to document d assigned to class c in the case of KNN; or (iii) the probability to generate d with the model associated to some class c for Na  X   X ve Bayes. From now on, we refer to this approach as temporal weighting on scores .

Let C and P be the set of classes and creation time points of the training documents. First, each training document class c  X  C is associated with the corresponding creation time point p  X  P , generating a new class defined as c, p . Then, we use a traditional classification algorithm to gener-ate scores for each new class c, p . Note that this scenario isolates term-class relationship variations, since it considers only one time point. To decide to which class c the do-cument d should be assigned to, we sum up all the scores c, p , for all p r  X  P , weighting them by the TWF (  X  ), where  X  = p  X  p r corresponds to the temporal distance between p and the creation moment of d . At the end of this process, d will be assigned to the class c with highest score, as listed in Algorithm 4.
 Algorithm 4 TWF-Sc: Temporal Weighting on Scores
In order to evaluate the impact that the proposed TWF has on the classification task, we evaluate both the tradi-tional and temporally-aware versions of Rocchio, KNN and Na  X   X ve Bayes in the ACM-DL and Medline collections, and contrast them. The methods were compared using two stan-dard information retrieval measures: Accuracy and macro average F 1 (MacroF 1 ). While the Accuracy measures the classification effectiveness over all decisions, the MacroF measures the classification effectiveness for each individual class and averages them. All experiments were executed us-ing a 10-fold cross-validation [1] procedure considering train-ing, validation and test sets. The parameters were set using the validation set, and the effectiveness of the algorithms measured in the test partition.
In order to run the experiments, two important parame-ters had to be set: the value of k for KNN and the scaling factor  X  .

We first performed some experiments with KNN to de-fine the value of k . This parameter significantly impacts the quality of classifier, and must be carefully chosen. Four values were tested for each version of the traditional and temporally-aware algorithms: 30, 50, 150, and 200. For the traditional version of the algorithm k = 30 presented better results, while for both temporally-aware versions of KNN the best value of k was 50. The intuition for the tradi-tional KNN to perform better with smaller values of k is that, as the number of neighbors increases, the variation on term-class relationships also increases, and the probability of misclassification increases. When considering temporal information by means of the proposed temporal weights, in contrast, more consistent information becomes available, al-lowing a more accurate model.

As discussed in Section 4, the TWF scale must be compat-ible with the classifiers scores, ensuring that it effectively im-proves the classifier X  X  decision rules without dismissing them. We empirically tested three values for  X  : 1, 10, and 100. The best value of each version of each classifier was considered. For Rocchio and KNN, the best results were obtained with  X  =1. ForNa  X   X ve Bayes, the best value was  X  = 10. This is due to the multiplicative nature of this classifier: many con-ditional probabilities are multiplied, leading to even smaller values.
After setting the parameters, we perform experiments com-paring the traditional and the proposed temporally-aware versions of Rocchio, KNN and Na  X   X ve Bayes. The results for the ACM-DL and Medline collections are reported in Tables 4 and 5. In both tables, each line presents the re-sults achieved by the versions of the classifiers identified in the first row and column. The values obtained for MacroF 1 ( X  m a c F 1  X ) and accuracy ( X  X cc. X ) are reported, as well as the percentage difference between values achieved by the temporally-aware methods and the traditional version of the classifiers. This percentage difference is followed by a sym-bol that indicates whether the variations are statistically significant according to a 2-tailed paired t-test, given a 99% confidence level. denotes a significant positive variation,  X  a non significant variation and a significant negative variation.

As we can see in Tables 4 and 5, all modified versions of Rocchio and KNN achieved better results than the base-line in ACM-DL. In Medline, the versions on score achieved gains, while the versions on documents were statistically the same as the baseline. In particular, Rocchio with TWF on scores presents the most significant improvements in both collections, with gains up to +18 . 87 and +11 . 46 for MacroF and Accuracy, respectively. Similarly, KNN with TWF on scores achieves the best results among all KNN variations, with gains of +8 . 85% and +3 . 80% for MacroF 1 and Accu-racy in the ACM-DL collection. In the case of Rocchio, the improvements achieved using the TWF can be explained by the fact that, in the traditional version, the documents are summarizedinauniquerepresent ative vector (centroid), ag-gregating documents from distinct creation time points, and affecting the prediction ability of the classifier. In the case of KNN, the definition of class boundaries is done considering each training document independently. KNN assumes that documents of same class are located close by on the vecto-rial space. By using the TWF ,the k nearest documents are reorganized, and the most temporally relevant are placed closer to the example being classified.

The Na  X   X ve Bayes with TWF on documents presents better results for MacroF 1 on both ACM-DL and Medline, and bet-ter Accuracy in Medline. Note that the best improvement was achieved in MacroF1, pointing out that this strategy effectively reduces the Na  X   X ve Bayes bias towards the most frequent classes. However, in contrast with Rocchio and KNN, the Na  X   X ve Bayes with TWF on scores performs poorly in both collections. We attribute this to two major weak-nesses of traditional Na  X   X ve Bayes version. First, when facing skewed data distributions, Na  X   X ve Bayes unwittingly prefers larger classes over others, causing decision boundaries to be biased. Second, when data is scarce, there is not enough information to perform accurate estimates, leading to bad results.

The skewness of data distribution among classes c, p can be quantified by the Coefficient of Variation CV =  X   X  of their sizes, where  X  and  X  stand for the standard deviation and mean. To explore the impact of data skewness on Na  X   X ve Bayes, we sampled MedLine, creating two sub-collections composed by the least and most frequent classes c, p ,min-imizing data skewness. While the entire collection presents CV =1 . 33, the sub-collections with the least and most fre-quent classes present CV equal to 0 . 57 and 0 . 43, respec-tively. As we can observe in Tables 5 and 6, the greater the CV, the worse are the results.

ACM-DL has an even more skewed data distribution over each time point, preventing us to sample it in sub-collections with smaller CV. Figure 2 shows that in the ACM-DL col-lection data scarcity is also prominent, contributing to the poor performance. Notice that 70% of classes c, p have less than 100 documents, a number too low to guarantee accurate estimates.
As observed, using TWF on scores in most cases led to better results than those applying TWF on documents. We believe this is because, in many applications, terms present distinct evolutive patterns, and the proposed function ne-glects this fact. Hence, when the temporal TWF is ap-plied on documents, all terms are multiplied by the same score (i.e., using a fine-grained representation of the docu-ment), given in function of the temporal distance  X  .Thus, we consider an uniform evolution among terms. In contrast, the TWF on scores minimizes this problem, as it applies the score in a coarse-grained representation of a document. Hence, one could argue that the TWF is more suitable to the  X  X n scores X  approach. The definition of a temporal weighting for each term independently is left for future work.
Finally, we also compared the best version of the meth-ods previously proposed, i.e., KNN with TWF on scores and Na  X   X ve Bayes with TWF on documents, to the state of the art classifier Support Vector Machine [8], in terms of effective-ness (classification quality) and efficiency (execution time). We run an efficient SVM implementation, SVM Perf [9], which is based on the maximum-margin approach and can be trained in linear time. We used an one-against-all [18] methodology to adapt binary SVM to multi-class classifi-cation, since the collections present 11 (ACM-DL) and 7 (MedLine) classes. The results are presented in Table 7. For ACM-DL collection, the significant gains are of 3 . 74% and 2 . 66% in macroF 1 and accuracy, respectively. For Med-Line collection, the most significant gains are of 12 . 87% and 5 . 06% in macroF 1 and accuracy, respectively. Considering that SVM is a state of the art classifier, and that both col-lections are very unbalanced (which significantly difficulties classification), our results evidence the quality of the pro-posed solution, with better performance.
This work discussed the impact that temporal effects may have in ADC, and proposed two new strategies for instance weighting that leads to more accurate classification. We started by proposing a methodology to model a Temporal Weighting Function ( TWF ) that captures changes in term-class relationships for a given period of time. For our real datasets, we showed that TWF follows a lognormal distri-bution, whose parameters may easily be tuned using statis-tical methods. In order to incorporate this TWF to classi-fiers, we presented two approaches: TWF on documents and TWF on scores. TWF on documents weights each training document by the TWF according to its temporal distance to the test document. TWF on scores, in contrast, takes into account scores produced by the traditional classifiers on scenarios without temporal variability on term-class re-lationships, performing a weighted sum of them, where the weights come from the TWF . Both strategies were incorpo-rated to three traditional classifiers, namely Rocchio, KNN, and Na  X   X ve Bayes.

Results with the traditional versions of these classifiers and the temporally-aware ones showed that considering tem-poral information significantly improves the results of the versus SVM for both collections traditional classifiers. Also, both temporally-aware KNN and Na  X   X ve Bayes achieved better results than SVM, with better performance. Considering that SVM is a state of the art classifier, and that both collections are very unbalanced, our results evidence the quality of our solution, coupled with an efficient implementation.

Given the results obtained when comparing SVM to the temporal versions of KNN and Na  X   X ve Bayes, as a future work, we will incorporate temporal information to the SVM classifier, by defining kernel functions that use the proposed TWF . We also plan to refine our TWF , in order to account for distinct evolutive patterns that terms may present, by means of a more sophisticated statistical analysis. More-over, similarly to time, social and geographical aspects may induce variations on term-class relationships, and we will also explore those dimensions in order to achieve an even better classification quality. [1] L. Breiman and P. Spector. Submodel selection and [2] N. H. M. Caldwell, P. J. Clarkson, P. A. Rodgers, and [3] D. B. Clarkson, Y.-a. Fan, and H. Joe. A remark on [4] W. W. Cohen and Y. Singer. Context-sensitive [5] S. K. Crow EL. Log-normal distributions: Theory and [6] P. E. D X  X gostino R.B. Tests for departure from [7] G. Folino, C. Pizzuti, and G. Spezzano. An adaptive [8] T. Joachims. Making large-scale support vector [9] T. Joachims. Training linear svms in linear time. In [10] Y. S. Kim, S. S. Park, E. Deards, and B. H. Kang. [11] R. Klinkenberg. Learning drifting concepts: Example [12] R. Klinkenberg and T. Joachims. Detecting concept [13] J. Kolter and M. Maloof. Dynamic weighted majority: [14] S. Lawrence and C. L. Giles. Context and page [15] M. M. Lazarescu, S. Venkatesh, and H. H. Bui. Using [16] E. Limpert, W. A. Stahel, and M. Abbt. Log-normal [17] R. Liu and Y. Lu. Incremental context mining for [18] C. D. Manning, P. Raghavan, and H. Schtze.
 [19] F. Mourao, L. Rocha, R. Ara  X  ujo, T. Couto, [20] L. Rocha, F. Mourao, A. Pereira, M. A. Gon  X  calves, [21] M. Scholz and R. Klinkenberg. Boosting classifiers for [22] A. Tsymbal. The problem of concept drift: Definitions [23] G. Widmer and M. Kubat. Learning in the presence of
