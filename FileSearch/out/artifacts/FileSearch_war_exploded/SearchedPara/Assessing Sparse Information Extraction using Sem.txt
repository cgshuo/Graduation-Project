 One important assumption of information extraction is that extractions occurring more frequently are more likely to be correct. Sparse information extraction is challenging be-cause no matter how big a corpus is, there are extractions supported by only a small amount of evidence in the cor-pus. A pioneering work known as REALM learns HMMs to model the context of a semantic relationship for assess-ing the extractions. This is quite costly and the semantics revealed for the context are not explicit. In this work, we in-troduce a lightweight, explicit semantic approach for sparse information extraction. We use a large semantic network consisting of millions of concepts, entities, and attributes to explicitly model the context of semantic relationships. Experiments show that our approach improves the F-score of extraction by at least 11.2% over state-of-the-art, HMM based approaches while maintaining more efficiency. H.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Miscellaneous; I.2 [ Computing Methodologies ]: ARTI-FICIAL INTELLIGENCE X  Knowledge Representation For-malisms and Methods Sparse Information Extraction; Semantic Context; Semantic Network; Semantic Relationship
A massive body of text available on the World Wide We-b presents an unprecedented opportunity for Information  X 
Peipei Li was a student intern in Microsoft Research Asia when the paper was developed.
 Extraction (IE). In many cases, the outcome of IE can be classified into two categories: heads and tails .The heads are those that occur very frequently in the corpus. For exam-ple, from numerous distinct sentences we can extract the fact that USA is a country . The assumption is that the higher frequency the occurrence the more likely it is correct. How-ever, there are correct results that occur very infrequently. For example, suppose from a corpus we extract a statement that says Rhodesia 1 is a country , and its occurrences in the corpus are few and far between. How to verify the correct-ness of a tail extraction (also known as sparse extraction) is one of the most important and challenging problems in IE. As we know, the distribution of words and phrases in a corpus of natural language utterances follows the Zipf X  X  law. Their occurrences in a particular syntactic pattern we use for extraction are very small. Without a good mechanism to identify correct extractions from incorrect ones, sparse in-formation extraction will be plagued by either low precision or low recall.

Recent work on sparse extraction uses one important hy-pothesis known as the distributional hypothesis [11], which says that different instances of the same semantic relation tend to appear in similar textual contexts. The challenge then lies in modeling contexts and measuring the semantic similarity of two contexts. In a na  X   X ve approach, we use a bag of words to represent the context of the concept c or the en-tity e in each sparse extraction. The method can easily be extended by using bigrams, trigrams, etc., instead of uni-gram words. The bag-of-words approach for context model-ing is not semantic and has low accuracy. A more advanced approach known as REALM [8, 3] was proposed recently. It uses the entire corpus to train a Hidden Markov Model (HMM). Specifically, each term in the corpus is considered to be generated by a single hidden state variable, which is itself generated by k previous hidden states. Given a term e , the learned HMM outputs P ( t | e ), which is the distribution of hidden states corresponding to e . Then, it uses KL diver-gence to measure the average difference between P ( t | e )and the state distribution of the seeds. However, there are sev-eral disadvantages of the HMM based approach as follows. Firstly, representing the context of a term using the distri-Rhodesia was an unrecognised state located in southern Africa that existed between 1965 and 1979 following its U-nilateral Declaration of Independence from the United K-ingdom on 11 November 1965. bution of a set of hidden states is hardly semantic. Secondly, to compute P ( t | e )foraterm e ,theHMMmodelmustbe very large in terms of the number of parameters. Since we are interested in less popular or tail terms, which are nu-merous, this generative mechanism is becoming unrealistic. Lastly, training the HMM is costly. The time required to learn the parameters of a k -th order HMM is proportional to O( N  X  T k +1 ), where N is the size of the corpus (number of sentences) and T is the count of hidden states. This is infeasible for a large corpus.

We address the following problem in this work. We want to verify the correctness of hundreds o f millions of isA re-lationships. That is, given a candidate pair c, e ,wewant to evaluate how likely e is an instance of class c .Wefirst give the background and then the challenges of the problem. We want to create large, open domain knowledgebases or taxonomies, whose scale or coverage is especially important to the applications built on top of them. Because manual-ly constructed taxonomies cannot reach sufficient scale and coverage, much recent work [13, 4, 17, 16, 20] uses data driv-en approaches to automatically acquire taxonomies from a large corpus such as the World Wide Web. Take Probase [1, 20] as an example. The Probase taxonomy contains mil-lions of entities and concepts, and the backbone of the tax-onomy is the isA relationship. Besides isA relationships, there are all kinds of other relationships such as located-in, is-CEO-of, etc. Since the data and the relationships in the taxonomy are acquired from a huge web corpus through syntactic-based information extraction, naturally there are many errors. Hence, data cleansing is extremely important for using the taxonomy.

We now analyze the challenges in the above problem. The first challenge is the scale. For example, there are hundreds of millions of isA relationships in Probase. One possibility is to divide and conquer. That is, we learn a single HMM to model the context of terms in each concept. The benefit is that the number of terms is reduced, and the HMM can be trained from only those sentences that contain the terms in the concept. However, there are 2.7 million concepts, and the HMMs can hardly be reused. Thus, it is quite impossible to train HMMs for each single concept. The second challenge lies in improving the effectiveness of the verifier. The third challenge is the interpretability of the verifier. Neither a bag of words nor a set of hidden states provides good semantics to understand the relationship between a candidate pair.
Therefore, we introduce a semantic, scalable, and effective approach for sparse information extraction. First, our ap-proach achieves much better precision and recall than state-of-the-art approaches. Second, we introduce a semantic ap-proach for solving the two problems. In other words, we come up with a semantic representation for the contexts. This approach is natural because we are dealing with a large taxonomy or semantic network, which provides semantic in-formation in various aspects. Using this information, we are able to introduce semantic features to describe a con-text, which also leads to a lightweight solution of context learning. Third, our approach is scalable. Unlike genera-tive models such as HMMs, our approach enables modeling contexts of any multi-word expression.

The rest of the paper is organized as follows. Section 2 describes several syntactic approaches and semantic ap-proaches for context representation. Section 3 discusses how we acquire open domain knowledge and perform inferencing on the knowledge to support semantic context methods in detail. Section 4 presents experimental results. We discuss related work in Section 5, and conclude in Section 6.
In this section, we present syntactic and semantic ap-proaches for context representation, and we show that se-mantic approaches are superior in performance.
The core task of the two problems we are addressing in this paper is context representation and context comparison. For the first task, that is, given a pair c, e and verifying whether e is an instance of class c , our goal is to find T ( e ), the context of e , and then compare it with the context of the seed instances of c . This can be formulated as follows: where S c denotes a set of seed instances of concept c ,and sim () is a similarity function for context. We consider e as an instance of c if f ( e, c ) is beyond a certain threshold. It is clear that our primary task is to define T ( e )inEq.1.Fora corpus that contains sentences s 1 ,  X  X  X  ,s n , we aim to extract the context in a fixed-size window centered at e on sentence s i ,where1  X  i  X  n . In the following, we discuss some syntactic approaches and semantic approaches for context extraction.
There are many different ways to use syntactic features to represent a context. In this section, we introduce a simple method, bag-of-words as the baseline method for context representation.
 Bag-of-words context representation. For a term e or apairofterms e 1 ,e 2 , we collect sentences that contain e or centered at the terms or from the entire sentences. We then word and w i refers to its importance in terms of tf-idf. We obtain the context vectors for seeds and pairs of seeds in the same way. Finally, we use the cosine function 2 to compare their similarity.
Syntactic contexts are easy to obtain, but they are not semantic. They are unstructured, and they are often noisy and confusing. On the other hand, HMM based approaches focus on a more structured representation of the context, but it is costly to obtain. In this paper, we focus on a lightweight semantic representation of contexts.
 Attribute-based context representation
The bag-of-words approach uses words surrounding a ter-m indiscriminately as its context. This leads to inaccurate representations of contexts. Is there a way to use surround-ing words more selectively?
Consider the example of Rhodesia again. From syntactic patterns (e.g., Hearst Patterns [12]), we do not have suffi-cient evidence to support or refute the claim that Rhodesia is
Our experiments reveal that cosine outperforms other simi-larity/distance evaluation functions, such as jaccard, jensen-shannon and the KL divergence. a country. However, when we talk about a country, no mat-ter big or small, rich or poor, we are likely to mention things such as capital city , president , congress , currency ,whetherit is a republic or a kingdom , etc. In other words, if we repre-sent the context of Rhodesia by the presence of such terms, then it will be clear whether Rhodesia appears in contexts where real countries appear. As a matter of fact, the pat-tern  X  X epublic of Rhodesia X  appears 50 to 100 times more frequently in the web corpus than the pattern  X  X ountries such as Rhodesia X .

We assume we are given a set of attributes { a 1 ,  X  X  X  ,a n any concept c . For example, capital city, GDP, population are possible attributes for country. We are also given seed instances of concept c . For example, the seed instances of country could be USA, Japan, Germany, etc. We can obtain a vector for any concept c in the form of A c = w 1 ,  X  X  X  where w i indicates how frequently c  X  X  seed instances appear together with attribute a i of c in a corpus. To determine whether a term e is of type c , we use syntactic pattern  X  X he a of e is X  to obtain each candidate attribute a of e from the web corpus. This gives us a vector for e ,namely A e = w 1 ,  X  X  X  ,w k ,where w i indicates the frequency a i appears in the pattern with e . We then use the cosine similarity to decide how likely e is an instance of c : Isa-based context representation
We assume we have the following data: i) For each concept c , we are given the seed instances of c . ii) For any instance e , we are given the set of concepts that e belongs to. For example, India may belong to concepts such as country, e-merging market, developing country, democracy, etc. iii) For any pair of instance e and concept c , we know how typ-ical c is as a concept for e . From the above information, we derive the following vector for concept c : I c = w 1 ,  X  X  X  instances of c ,and p ( c i | e ) is the typicality of score for e and concept c i (that is, how typical c i is among all the concepts e belongs to). Next, for each extraction c, e ,wederivethe vector I e : I e = w 1 ,  X  X  X  ,w k ,where w i = p ( c i | e ). Finally, we use cosine similarity to decide how likely e is an instance of c : Concept-based context representation
The isA-based and the attribute-based approaches still re-ly heavily on specific syntactic patterns, which limit them to a much smaller corpus. This limits the amount of evi-dence we can find, especially when the extraction is about rare terms. In our new approach, instead of using fixed syn-tactic patterns, we map an arbitrary piece of text to a point in a semantic space, and then measure the distance in the semantic space between the point and the points that cor-respond to the seed terms or seed pairs of terms. We call the technique  X  X onceptualization X . Intuitively, when given terms such as China and India, we  X  X onceptualize X  them in-to concepts with country ranked the highest; when given China, India, Russia, we  X  X onceptualize X  them into concepts with emerging markets or BRIC ranked the highest. This enables us to process arbitrary piece of texts instead of texts that match fixed syntactic patterns.

Formally, for an arbitrary piece of text, we conceptualize ittoavector C ( t )= w 1 ,  X  X  X  ,w k ( w i is the weight of c i ). To decide if a term, say Rhodesia, is an instance of a concept, say country, we perform conceptualization twice. First, we find the seed instances of the concept, and collect the textual context of seed instances in the web corpus, and then we conceptualize the context to C c . Second, we find the textual context of e , and conceptualize it to C e . Finally, we use cosine similarity to decide whether the relationship holds:
In the previous section, we described three semantic con-text approaches. Clearly, in order to support these ap-proaches, we need to i) have common knowledge, and ii) perform certain type of inferencing called conceptualization. We focus on these two tasks in this section.
The semantic approaches described in the previous sec-tion require the following open domain knowledge: i) a large concept space; ii) instances of each concept; iii) attributes of each concept; and iv) weights (typicality scores). In other words, we need open domain, probabilistic isA and isProp-ertyOf knowledge (e.g., China is a country, and population is a property or attribute of country, etc.)
We start with Probase [1, 20], which provides probabilis-tic isA knowledge for 2.7 million concepts. The concept space is big enough to cover almost every aspect of worldly facts. The isA relationships in Probase are harvested using syntactic patterns (e.g., the Hearst patterns [12]). Further-more, the isA knowledge in Probase comes with the weights that are needed in our work. For a concept/instance pair c, e ,itprovidestwo typicality scores: P ( e | c )and P ( c Typicality scores are derived like the following. In our approach described in the previous section, we also need to know the  X  X eed X  instances of each concept. These are just instances of high typicality scores, that is, instance e with P ( e | c ) larger than a threshold.

The original Probase data does not contain information of attributes. We obtain knowledge about attributes using Probase and the web corpus. Specifically, for a given concept c in Probase, and the seed instances e 1 ,  X  X  X  ,e k of c ,weuse the following syntactic pattern to derive attributes of c : where &lt; a &gt; is an attribute and e i is a seed instance of c . After obtaining all candidate attributes &lt; a &gt; ,weclus-ter and weight the attributes. After collecting the evidence, we perform a graph cut to find clusters of surface forms that represent the same attributes. Besides, we weight the attributes. Two of the most important scores are P ( a | the typicality of attribute a of instance e ,and P ( e | a ), the typicality of instance e for attribute a . Both scores are ap-proximated by frequencies like the definition of P ( e | c ).
Conceptualization aims to derive concepts hidden in the text. We use a 3-step approach for conceptualization. First, given a piece of text (textual context of a single term or a pair of terms), we identify instances (including concepts and entities) and attributes in the text, using our knowl-edgebase as a dictionary. Next, instead of using the in-stances/attributes as a bag of words, we derive the most like-ly concepts from the instances and attributes. That is, we find the distribution of concepts C given instances/attributes t  X  X  X  ,t k : P ( C | t 1 ,  X  X  X  ,t k ). Third, we derive a similarity function based on clusters of concepts.
 Asingleterm t i canbebothanattributeandaninstance. For example, population is an attribute of the country con-cept, but it can also be an instance of the geographical data concept. We use an auxiliary variable z i as an indicator for if t i is an attribute. We use a noisy-or model to infer the probability:
P ( c k | t i )=1  X  (1  X  P ( c k | t i ,z i = 1))(1  X  P ( c or it is an attribute of c k . Here, we have P ( c k | t i ,z i =1)= E is the set of instances that are related to attribute a i and concept c k . Then, using the naive Bayes rule, we derive the concept posterior given a set of terms by: where P ( c k | t i )isgivenbyEq.6.
 After deriving the concept distribution, we can implement Eq. 4 to compare two contexts (a context in question and a typical context, which is often derived from seed terms or seed term pairs). However, Probase has 2.7 million concepts and many concepts are correlated, e.g.,  X  X ountry X  and  X  X a-tion X . Ignoring this correlation impairs the quality of the similarity function. We cluster the concepts before com-paring two concept-based contexts. We use the k-Medoids clustering algorithm for this purpose. We use  X  X ontent over-lap X  as the similarity function for clustering, that is, two concepts are considered more similar if they have more i-dentical instances. Each of the resulting cluster represents sort of a sense, and we apply Eq. 4 over the senses, instead of the original concepts.
In this section, we evaluate and analyze semantic approach-es of context representation as well as their effectiveness in sparse information extraction. The web corpus we use contains 1.68 billion web pages. Probase, which contains 2.7 million concepts and 45 million pairs of relationships, is itself harvested from the corpus. Our goal is to clean Probase, that is, to identify wrong in-stances in a concept and wrong pairs in a relationship. Given the scale of the problem, we perform data cleaning jobs on a map-reduce system with 10 machines. The cleaning process takeslessthan10hours.

For performance evaluation, we selected sparse extractions with no more than 10 occurrences from 4 isA relationships, and we also picked the 10 most frequent extractions for each relation which serve as seeds. Details of all test relationships are shown in Table 1. All experimental results reported in this section are averaged over 10 runs.
We compare the performance of our approach and six baseline ones in type checking, namely checking whether a term is a true instance of a concept. We also evaluate the efficiency of our approach and the HMM-based method.
We show the results in Table 2 and Figure 1. We report the precision, recall and F-score of six approaches, name-ly, bag-of-words, attribute-based, isA-based, concept-based, HMM-based and our final approach. Our final approach is a combination of three semantic approaches, namely the attribute-based, isA-based and concept-based approaches us-ing the logistic regression. To show the trade-off of precision and recall, the table used six measures that are defined as follows: where variables a, b, c and d are defined as:
We observe the following from results in Table 2. Firstly, for the 4 concepts and the measures of BF 1 and GF 1 ,the semantic methods win in all concepts. Secondly, the four semantic approaches (attribute-based, isA-based, concept-based and our combined approach) win 2/1/0/1 times in while, the combined approach and the isA-based method win once respectively on both BF 1 and GF 1 measures. Third-ly, the combined approach kills 56.7% bad extractions while killing 11.5% good ones, which kills more bad pairs at the cost of killing the least number of good pairs compared to other three semantic methods. These data show that our combined semantic approach performs the best compared to other syntactic and semantic methods. Figure 1 reports the performance of our combined approach on six evaluation metrics compared to the HMM-based method. We observe the following: First, under the measures of BR , BP , GR and GP , our approach beats the HMM-based method in 4, 2, 2, and 4 out of all 4 concepts, respectively. Second, under the overall measures of BF 1 and GF 1 , our approach beats the HMMmethodin4and3outofall4concepts,respectively. Figure 1: Prediction results of our approach com-pared to the HMM-based method Table 2: Prediction results on 8 isA relationships Our approach actually wins 3 times for both BF 1 and GF 1 . Overall, our approach improves BF 1 and GF 1 by 34.6% and 11.2% respectively over the HMM-based method. These da-ta reveal that our approach is superior to the HMM-based method.
Figure 2 reports the time complexity of our approach com-pared to the HMM-based method in type checking. Experi-ments are performed on an Intel Core 2 Duo CPU, 2.66GHz PC with 4G main memory, running Windows 7 Enterprise. For our approach, the time overhead consists of the time for computing three features (attribute-based, isA-based, and Figure 2: Time overhead of our approach vs. HMM-basedintypechecking concept-based), while for the HMM-based method, the total time overhead consists of learning the HMM and relational n -gram-based models. Overall, our approach is much faster than the HMM-based method. For example, on the coun-try concept, our approach consumes 545 seconds, while the HMM-based method takes 6037 seconds.
We introduce some related work in information extrac-tions. To assess the quality of IE, main works are as follows. 1) Heuristic-based approaches, such as an unsupervised relation extraction system called URES [10] using short de-scriptions of the target relations and their attributes, and a bootstrapping system called statistical Snowball [23] based on the Snowball system [2]. These approaches start with a set of seed instances of a given relation or some prior label distributional knowledge, and identify extraction patterns for the relation iteratively. During the iteration, however, random extraction errors may result in dubious extraction patterns, which lead to erroneous instances. 2) Redundancy-based approaches, such as a conditional-random-field-based model for assessing the accurate con-fidence of extracted information [5], and a combinatorial  X  X alls-and-urns X  X odel called URNS [7, 9]. Unlike the heuris-tic-based approaches, these redundancy-based methods uti-lize the distribution of the target and error sets to build models, but they are not good at assessing which extraction is more likely to be correct for sparse extractions. 3) Context-based model building approaches, such as a HTML tables-based clustering method for coordinate terms [6], a conditional random field model and clustering model based approach for relationship extractions [18], a genera-tive model with a distributional prior for the unsupervised IE task [15] and a state-of-the-art language modeling-based method called REALM [8][3]. In these methods, only the REALM approach addresses assessing sparse extractions. It utilizes all sentences crawled from web pages as the contex-t to build HMM and n -gram-based language models. Ex-tensive studies [8][3] show that REALM outperforms the heuristic-based and redundancy-based methods. However, it is not semantic because selected contexts only contain the state distributions based on  X  X ags of words in the corpus X . Meanwhile, it is time-consuming due to the building of the HMM-based language model. 4) Knowledge-based approaches, such as a Wikipedia-based open IE system called WOE [19], a Freebase-based weak su-pervision for overlapping relation extraction [14], a Freebase-based unsupervised relation extraction method [22], and a domain knowledge-based relation extraction method [21]. These methods are built on the semantic information in knowledge databases (e.g., Freebase and WikiPedia), which have a limited scale and coverage in terms of concept space.
Unlike many of the aforementioned approaches, our ap-proach uses semantic context for sparse extraction. Mean-while, our approach is lightweight and supports web scale data. To improve its accuracy, we aggregate three differen-t semantic contexts to increase the data redundancy. All semantic contexts are extracted from the Web by specified patterns and preprocessed as the knowledge databases in Probase. As we know that the scale of Probase is one order of magnitude larger than the previously known large corps [20], such as WordNet, Freebase and WikiPedia. There-fore, as compared to the aforementioned knowledge-based approaches [19][14], our approach is more scalable in assess-ing sparse extractions.
Sparse Information Extraction is a challenging task. In this paper, we presented a lightweight, semantic context-based assessment approach for spare extractions. Our ap-proach utilizes three different semantic contexts, including the attribute-based context, isA-based context and concept-based context. As compared to the state-of-the-art context-based language model building assessment method and oth-er five syntactic context-based and semantic context-based baseline methods, our approach has better precision, recall and F-score. Meanwhile, it is more applicable to the large s-cale data sets of spares extractions due to its lower time cost compared to the HMM-based method. Our future work aim-s to apply the proposed approach in the assessment of more general sparse extractions (not only in Probase harvested from the Web using the Hearst patterns).
This work is supported in part by National 863 Program of China under gra nt 2012AA011005, the National 973 Pro-gram of Chin a under grant 2013CB329604, the Natural Sci-ence Foundati on of China unde r grants ( 61273292, 61273297, 61229301, 61070131), the Post doctoral Science Foundati on of Hefei University of Tec hnology unde r grant 2013HGB-H0025, and the US National Sc ience Foundati on (NSF) un-der grant CCF-0905337. [1] http://research.microsoft.com/en-[2] E. Agichtein and L. Gravano. Snowball: Extracting [3] A. Ahuja and D. Downey. Improved extraction [4] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. [5] O. Culotta and A. McCallum. Confidence estimation [6] B.Dalvi,W.W.Cohen,andJ.Callan.Websets: [7] D. Downey, O. Etzioni, and S. Soderland. A [8] D. Downey, S. Schoenmackers, and O. Etzioni. Sparse [9] D. Downeya, O. Etzionib, and S. Soderland. Analysis [10] R. Feldman and B. Rosenfeld. Boosting unsupervised [11] Z. Harris. Distributional Structure . The Philosophy of [12] M. A. Hearst. Automatic acquisition of hyponyms [13] J. Hoffart, F. M. Suchanek, K. Berberich, E. L. [14] R. Hoffmann, C. Zhang, X. Ling, L. Zettlemoyer, and [15] C. W. Leung, J. Jiang, K. M. A. Chai, H. L. Chieu, [16] T. Li, P. Chubak, L. V. Lakshmanan, and [17] S. P. Ponzetto and M. Strube. Deriving a large-scale [18] W. Wang, R. Besan  X  con, O. Ferret, and B. Grau. [19] F. Wu and D. S. Weld. Open information extraction [20] W. Wu, H. Li, H. Wang, and K. Q. Zhu. Probase: a [21] F. Xu, H. Uszkoreit, S. Krause, and H. Li. Boosting [22] L. Yao, S. Riedel, and A. McCallum. Collective [23] J. Zhu, Z. Nie, X. Liu, B. Zhang, and J. rong Wen.
