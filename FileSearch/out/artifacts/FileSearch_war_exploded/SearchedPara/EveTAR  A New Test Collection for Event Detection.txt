 Research on event detection in Twitter is often obstructed by the lack of publicly-available evaluation mechanisms such as test collections; this problem is more severe when consider-ing the scarcity of them in languages other than English. In this paper, we present EveTAR , the first publicly-available test collection for event detection in Arabic tweets. The col-lection includes a crawl of 590M Arabic tweets posted in a month period and covers 66 significant events (in 8 different categories) for which more than 134k relevance judgments were gathered using crowdsourcing with high average inter-annotator agreement (Kappa value of 0.6).

We demonstrate the usability of the collection by eval-uating 3 state-of-the-art event detection algorithms. The collection is also designed to support other retrieval tasks, as we show in our experiments with ad-hoc search systems. Evaluation; Crowdsourcing; Twitter; Ad-hoc Search
The overwhelming popularity of Twitter led several parts of the world (e.g., the Arab region) to use it as a medium for continuous exchange of short messages (i.e., tweets). While it became essential for everyday Arab users, popular Ara-bic news agencies (e.g., AlJazeera and AlArabiya) are also using it extensively to continuously update their followers on critical events and news as they happen. The Arab so-cial media report 1 shows that, as of March 2014, an average of 17M Arabic tweets are posted every day. Such tweets are extremely noisy, full of typos and redundancy, making it difficult to manually identify events [8]. With the grow-ing events in the Arab region, the need for automatic tools that can reliably track the huge stream of tweets and detect events before being publicly announced is increasing.  X 
Also affiliated with Qatar Foundation, Doha, Qatar. http://www.arabsocialmediareport.com 3. We demonstrate that our test collection can be used to
In this section, we discuss some of the existing test collec-tions for event detection and compare them with EveTAR .
The study of Alsaedi and Burnap [1] is the first on event detection in Arabic tweets, focusing on detecting events in Abu Dhabi. Around 1M Arabic tweets were collected and labelled by 3 annotators, however, the dataset was not made publicly-available. Additionally, it was restricted to events in Abu Dhabi, which introduces a bias towards types of events that happen in that location.
 Petrovi  X c et al. [13] built a test collection consisting of 50M English tweets for the task of First Story Detection. Given a set of manually-crafted events, authors recruited expert annotators to collect on-topic tweets for those events. The need for expert annotators makes the creation of the test collection expensive and limits its scale. Furthermore, the authors identified 27 events with 3K relevant tweets only, making it difficult to use the collection for conducting large-scale evaluation of event detection over the Twitter stream.
On a larger scale, McMinn et al. [8] built a publicly-available test collection for evaluating event detection. The authors crawled around 120M English tweets, covering more than 500 events identified using automatic and manual ways, and collected labels for over 150K tweets. We consider their approach as a basis for our work, however, we followed a slightly different approach to construct EveTAR with man-ual identification of events. On average, EveTAR has more tweets per event when compared to their collection. Addi-tionally, we designed our test collection to be general enough to support additional tasks like ad-hoc search.

Contrasting EveTAR to other event detection test collec-tions, we find it the largest (with 590M tweets) compared to collections described in [3], [6], [9] and [10]. Moreover, events in EveTAR were not limited to a specific location as opposed to test collections in [2] or [3] for example.
We built EveTAR based on the following pipeline: col-lecting the tweet dataset, identifying events in this dataset, extracting potentially-relevant tweets for those events, and finally obtaining their relevance judgments.
We used Twitter X  X  streaming API to collect a dataset of 590M Arabic tweets over the month of January 2015. Tweets were collected by tracking 400 most frequently-used Arabic words extracted from a previously-crawled Twitter stream.
Following the approach of McMinn et al. [8], we manu-ally collected a set of 357 events in the month of January 2015 listed over both the English 3 and Arabic 4 Wikipedia X  X  Current Events Portal (WCEP). We then applied our sig-nificance criteria over two phases. In the first, we only kept events for which we found at least one online Arabic news https://en.wikipedia.org/wiki/Portal:Current events https://ar.wikipedia.org/wiki/  X K P Ag . H @ Yg @ :  X K . @ X K .
Before annotators can start labeling, they were required to pass a qualification test by correctly labeling a minimum of 8 out of 10 gold tweets. Gold tweets were randomly sampled from the collection of potentially-relevant tweets per event. Once they start labeling, annotators had to maintain a min-imum accuracy of 80% over gold tweets within the task to continue labeling. An average of 8 annotators were blocked while labeling for an event, and only labels from trusted an-notators where included in EveTAR . We chose to have each tweet annotated by 3 annotators to ensure a majority label. We also restrict the annotators to be Arabic-speaking with an intermediate level per CrowdFlower X  X  ranking of annota-tors. An average of 59 hours were spent per event.
To decide the final label of a tweet out of the 3 given labels, we adopted a trust-based voting scheme that uti-lizes the trust scores provided by CrowdFlower per annota-tor (describing her accuracy in the current task). We chose the label that has the highest sum of trust scores over cor-responding annotators. For all events, 51,424 tweets were labeled as relevant and 82,645 were non-relevant, based on the above voting scheme.

We also computed an overall trust score per tweet using the following Equation 6 :
Tweet Trust Score = where r and n are the number of annotators labeling the tweet as relevant or non-relevant respectively, and trust i is the trust score for an annotator. Averaging this overall trust score over all tweets and all events results in an average quality score of 0.94 out of 1.

We measure the quality of the obtained labels by comput-ing the inter-annotator agreement using Fleiss X  Kappa [16] per event. We chose this measure over the widely-used Co-hen X  X  Kappa as it allows measuring annotators X  agreement when having more than 2 annotators labeling a single data item. In literature, the value of Kappa has been mapped into 6 categories based on how strong the agreement is [16]. Fig-ure 1 illustrates the distribution of events over Fleiss X  Kappa categories. Over all events, we got an average Kappa of 0.60, which is considered a moderate agreement. Moreover, more than half of the events got a substantial to an almost per-fect agreement. Additionally, eliminating the 3 events with slight annotator agreement results in an average agreement of 0.62, which is considered substantial . We observed that events with slight agreement tend to be less popular in the media than those with almost perfect agreement.The figure also shows a slight drop in the values of the average trust score per event with decreasing Kappa values.
In this section, we demonstrate the usability of EveTAR for event detection in addition to other retrieval tasks, taking ad-hoc search as an example.
We experimented with EveTAR to show how it can be used to evaluate state-of-the-art event detection algorithms The equation is stemming from the one used by Crowd-Flower to report confidence in the aggregated label given for a data item, see: http://bit.ly/20NmFkU results demonstrate that EveTAR can be used to automati-cally evaluate event detection systems.

While F 1 numbers over EveTAR (especially for MABED) are relatively high, this is probably an artifact of using only the judged subset of tweets instead of the full dataset for testing. Another factor we noticed is that the default value of a MABED parameter that indicates the number of events to be detected was 100, which is coincidentally close enough to the number of events in EveTAR .
In addition to event detection, EveTAR is designed to support evaluation of other tasks like ad-hoc search, as it provides a short query and a list of relevant tweets per topic (i.e., event). Using EveTAR , we experimented with two ad-hoc search systems, denoted by query likelihood (QL) and query expansion (QE), which were adopted by one of the top teams in the ad-hoc search task in TREC-2013 microblog track [5, 7]. We ran the search systems (using their reported parameter values) over the Lucene index of the 590M tweets. Both MAP and P@30 measures were used to evaluate the performance of the systems as summarized in Table 3. Table 3: Ad-hoc search performance with EveTAR
We notice that P@30 values for both models are in range of those reported in [7], however, MAP values are much lower than expected. The difference in ad-hoc search performance between EveTAR and the English collection in [7] might be due to the very big difference in dataset size, as the English dataset is much smaller than EveTAR . Surprisingly, the QE model is performing worse than QL; we believe that the lack of proper Arabic text normalization on tweets used to select expansion terms might cause a poor selection of terms for expansion. Further experiments are needed to understand the performance of ad-hoc search systems over EveTAR . In this paper, we present our work on constructing Eve-TAR , the first publicly-available Arabic test collection for event detection. The collection includes 66 events manually-identified during the month of January 2015. We used crowd-sourcing to collect a total of 134K labeled tweets. We demon-strate the usage of our collection in manually and automati-cally evaluating state-of-the-art event detection algorithms. We also show how EveTAR can be used to automatically evaluate other tasks such as ad-hoc search.

There are several possible directions for future work. First, the experimental results obtained in demonstrating the us-age of EveTAR are just preliminary; therefore we still need to further study the performance of event detection and ad-hoc search systems over Arabic tweets. Second, we plan to run more extensive studies to evaluate systems built for other retrieval tasks (e.g., tweet filtering) over our collection. Finally, we would like to explore the possibility of extending EveTAR with more events using the automatically-identified potential events by event detection systems.

