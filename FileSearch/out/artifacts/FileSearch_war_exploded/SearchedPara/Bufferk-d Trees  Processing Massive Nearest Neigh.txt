 Justin Heinermann JUSTIN Christian Igel IGEL @ DIKU . DK Finding the nearest neighbors for a query object is funda-mental in data analytics. Given both a large reference and query set, however, the involved nearest neighbor compu-tations can quickly become a bottleneck. Depending on the particular learning task at hand (e.g., the size of the refer-ence/query set or the dimensionality of the input space), various techniques can be used to accelerate the search. Prominent examples are spatial data structures such as k -d and cover trees (Bentley, 1975; Beygelzimer et al., 2006) or locality-sensitive hashing (Indyk &amp; Motwani, 1998). A recent trend is to resort to graphics processing units (GPU S ) for accelerating the search (Cayton, 2012; Garcia et al., 2010; Pan &amp; Manocha, 2011). One way to utilize such devices is to parallelize the search over the query points in a brute-force manner, which can lead to significant speed-ups. In low-dimensional feature spaces, however, the performance gain over spatial search struc-tures vanishes with increasing data set sizes. Therefore, a desirable goal is to combine the benefits of both worlds. A typical parallel k -d tree implementation assigns one thread to each query and all threads traverse the tree simul-taneously. While such a scheme performs well on multi-core machines, it is ill-suited for GPU execution since each query may induce a completely different tree traver-sal. This results in both branch divergence and irregular, prohibitively expensive accesses to global memory (negat-ing much of the potential benefits of using a k -d tree). This work presents a general purpose computation on graphics processing units (GPGPU) solution to exact near-est neighbor search in Euclidean spaces. Our approach re-lies on a memory-centric (Oancea et al., 2009; Shuf et al., 2002) refinement of a classical k -d tree, which we name buffer k -d tree, and enables an effective, massively-parallel processing of huge amounts of queries. Each leaf of the new tree structure corresponds to a set of reference patterns and uses a buffer to delay the processing of the queries reaching that leaf until enough work has been gathered. Processing all buffered queries is performed in a brute-force manner. Here, we take advantage of the fact that the queries collected in the same buffer are compared with the same pattern in the same block-wide SIMD (single instruc-tion, multiple data) instruction, and such memory accesses are effectively supported by today X  X  GPU S via hardware caches (Jia et al., 2012).
 Our approach is memory-centric in the sense that it re-organizes the program X  X  control flow to improve locality of reference. 1 The experimental evaluation, conducted on a commodity CPU + GPU system, demonstrates a signifi-cant performance gain of our approach over a brute-force GPU scheme ( 2  X  55  X  ) and a multi-threaded k -d tree im-plementation ( 5  X  34  X  ), given manually tuned CPU k -d tree heights (and using all four cores). Our approach is suitable for applications that permit  X  X azy querying X  (i.e., in cases where an increase of response latency for single test pat-terns is not harmful), for example when a large batch of test queries needs to be processed. We briefly sketch k -d tree-based and parallel nearest neigh-bor search (see Andoni &amp; Indyk, 2008, for other schemes). 2.1. Revisited: Nearest Neighbors via k -d-Trees A k -d tree (Bentley, 1975; Friedman et al., 1977) for a set P = { x , . . . , x n }  X  R d of reference points is a balanced binary tree. The root of the tree corresponds to all points and its two children represent disjoint (almost) equal-sized subsets of P . Splitting up the points into subsets is done in a level-wise manner, starting from the root (at level 0 ) down to the leaves. For a given node v at level i , the points associated with v are split into two halves by resorting to the median in dimension i mod d (other splitting rules may also be applied). The recursive construction ends as soon as a node v corresponds to a singleton or to a set of predefined size. A k -d tree can be constructed in O ( n log n ) time via linear-time median-finding and occupies linear space. The nearest neighbor search makes use of the hierarchical subdivision induced by the tree: Let q  X  R d be a query point. To find its nearest neighbor in P (the generalization to k &gt; 1 neighbors is straightforward), one traverses the tree in two phases. In the first phase, one navigates down from top to bottom to find the d -dimensional box that con-tains q (using the median values stored in the nodes). By computing the distances between q and all points stored in the associated leaf, one can identify an initial nearest neigh-bor candidate. In the second phase, one traverses the tree from bottom to top, and on the way back to the root, one checks if neighboring boxes potentially contain points that are closer to q as the current best candidate (using the me-dian values). In case a point might be closer, one recurses to the subtree that has not yet been visited.
 For low-dimensional spaces, a small number of leaf visits is often enough (yielding a logarithmic runtime). The perfor-mance can, however, significantly decrease with increasing d due to the curse of dimensionality (Hastie et al., 2009). 2.2. Nearest Neighbor Search on GPUs Nowadays GPU S offer massive parallelism (e.g., 2048 cores). They rely on a simplified control unit and an ex-plicitly programmable memory hierarchy, in which global memory is up to 100  X  slower than, e.g., local memory. Two memory access patterns can lead to smaller latencies: The first one corresponds to coalesced accesses , in which, informally, groups of threads access consecutive memory locations. The second one is caching , which is given when a group of consecutive threads (repeatedly) accesses nearby memory locations; such memory operations can effectively be accelerated using hardware caches (Jia et al., 2012). Most of the work on computing nearest neighbors us-ing GPU S so far has focused on the domain of computer graphics (e.g., on ray tracing ) with data structures that are adapted to the specific needs of such tasks (Popov et al., 2007; Zhou et al., 2008; Wald &amp; Havran, 2006; Horn et al., 2007). For the more general task of nearest neighbor search in higher dimensions (e.g., R 10 ), only few schemes have been proposed: A direct way is to parallelize the search over the query points. This approach, followed by Garcia et al. (2010), offers a tremendous speed-up given medium-sized data sets. However, it fails for large-scale settings with both many reference and query points. Pan &amp; Manocha (2011) propose an efficient GPU implemen-tation of locality-sensitive hashing, which is also applica-ble to large-scale settings, but possibly yields inexact an-swers (as its sequential analog). Other schemes are based on the efficient use of texture memory or on adapted sorting schemes (Bustos et al., 2006; Sismanis et al., 2012). Some k -d tree-based methods have been proposed for, e.g., the computation of forces between particles (Qiu et al., 2009; Wang &amp; Cao, 2010; Heinermann et al., 2013; Nakasato, 2012). However, these schemes are designed for X  X nd limited to X  X ery low-dimensional feature spaces such as R 3 or R 4 and are conceptually very different from our framework. The approach most related to our work is given by Cayton (2012) and is based on a random ball cover data structure. Similarly to k -d trees, this data struc-ture induces a subdivision of the search space (into d -dimensional balls), which can be used to accelerate the search. In contrast to our work, however, no associated tree structure is employed for guiding the pruning process. Hence, except for the tree-based schemes mentioned above, all approaches either provide efficient but possibly inexact answers or address special instances (e.g., small d or n ). Note that other parallel schemes (not using GPU S ) have been proposed such as  X  X istributed X  k -d trees, which can be queried in a map-reduce manner (Aly et al., 2011). The basis for the reorganized querying process is the con-cept of buffer k -d trees, which we describe next. 3.1. Buffer k -d Trees A buffer k -d tree is composed of four parts: (1) a top tree, (2) a leaf structure, (3) a set of buffers (one buffer per leaf of the top tree), and (4) two input queues, see Figure 1. The top tree consists of the first h levels of a standard k -d tree (i.e., its median values), laid out in memory in a pointer-less manner (the root is stored at index 0 and the children of a node v with index i are stored at indices 2 i and 2 i + 1 , respectively). During the construction of the top tree, all patterns are ordered in-place w.r.t. the median values such that all points of a leaf are stored consecutively in memory. 2 The leaf structure consists of blocks and stores all rearranged patterns. The blocks are in a one-to-one cor-respondence with the leaves of the top tree. Again, no pointers are needed to link the blocks with their associated leaves (the block of leaf i is stored at index i  X  (2 h  X  In addition to the rearranged patterns, two integer variables Algorithm 1 L AZY P ARALLEL NN are stored in each block to indicate the leaf/block bounds. The third component consists of buffers, one buffer for each leaf of the top tree. These buffers will be used to store query indices and can accommodate a predefined number B &gt; 1 of integers each. In addition, a variable that determines the filling status is stored for each buffer. Finally, we allo-cate space for two (first-in-first-out) queues of size m . The height of the buffer k -d tree is given by the height of its top tree ( h = 0 , 1 , . . . ) and B denotes its buffer size . Proposition 1. A buffer k -d tree of height h for a set P = { x , . . . , x n }  X  R d of reference and a chunk Q = { q , . . . , q m }  X  R d of query points uses O (2 h +1 tional floats and O ( n + m + 2 h B ) additional integers. It can be constructed in O ( hn )  X  X  ( n log n ) time. Since the height of the buffer k -d tree will be reasonably small (e.g., h = 8 ), the space overhead is negligible (and mainly dominated by the space for the buffers, which can further be reduced via dynamic memory allocation). Buffer k -d trees can be adapted to support the insertion and deletion of reference patterns (as k -d trees by, e.g., reserv-ing more space as initially needed for the leaf structure). 3.2. Lazy Parallel Nearest Neighbor Search We are now ready to describe the reformulation of the tree-based search: The main idea is to  X  X elay X  the querying process by performing several iterations. In each iteration, query indices are propagated through the top tree and are stored in the corresponding buffers. As soon as the buffers get full, all collected nearest neighbor queries are processed at a single blow. This essentially leads to a separation of the two main phases of the classical k -d tree-based search: (1) finding the leaf that needs to be processed next and (2) updating the nearest neighbors. The first phase cannot be parallelized easily on GPU S . However, the second one is Algorithm 2 F IND L EAF B ATCH much more amenable to such devices X  X nd constitutes by far the most significant part of the overall runtime. The modified workflow is shown in Algorithm 1: In the preprocessing phase, a buffer k -d tree T is constructed for the set P of reference points (Step 1). After initializing the queue input with all query indices (Step 2), the itera-tive process is started. In the first phase of each iteration, a large (user-defined) number M of indices is removed from both queues, where indices are only removed from input if reinsert is empty (Step 4). Afterwards, one invokes the procedure F IND L EAF B ATCH (Algorithm 2) to obtain, for each of these query indices, the corresponding leaf that needs to be processed next (Step 5). This procedure es-sentially simulates the (original) recursive tree traversal by keeping explicitly track of a recursion stack for each query index. 3 In case such an associated tree traversal has reached the root (and both subtrees have been visited), the query index has been processed completely and can be removed from the overall process. All other indices are moved into the appropriate buffers (Steps 6 X 10).
 In the second phase, all buffers are processed. In case one of the buffers has reached a certain filling status (e.g., half-full), the procedure P ROCESS A LL B UFFERS (Algorithm 3) is invoked, which empties all buffers and updates, for each query index being removed, the associated list of nearest neighbors found so far (Step 12). This is accomplished in a brute-force manner by comparing the query with all ref-erence patterns of the associated block of the leaf structure. Afterwards, all indices l 1 , . . . , l N taken from the buffers are inserted into reinsert (Step 13). The overall pro-cess stops as soon as no indices are left in both queues and the buffers (in practice, a brute-force step is applied as soon as the total number of remaining indices is small enough). To sum up, we do not process all queries separately, but split the search process into two phases each handling large chunks of indices. Keeping track of the current  X  X ree traver-sals X  is achieved by explicitly managing the induced recur-sion stacks. Note that exactly the same leaves are visited for each query index as for the classical k -d tree traversal. Proposition 2. Processing q 1 , . . . , q m  X  R d queries via Algorithm 1 takes the same (asymptotic) runtime as the original k -d tree-based search (given same tree heights). Thus, except for a small overhead caused by keeping track of the query indices, the runtime is the same as for the orig-inal k -d tree traversal. However, the order of the query in-dices is changed, i.e., indices belonging to the same leaf are now processed in chunks, providing data locality. 3.3. GPGPU Implementation We now sketch our GPGPU implementation and describe, in detail, the kernel implementation for the procedure P RO CESS A LL B UFFERS , which usually takes more than 99% of the total sequential runtime (see Section 4). The buffer k -d tree is built on the host system (CPU), since the construction time is negligible for processing huge query sets, and all relevant information is copied to the GPU. Other than this, the host system is only used (1) to ensure the flow of query indices between leaf buffers and the queues input and reinsert , (2) to spawn the GPU kernels, and (3) to transfer arrays of indices to the GPU that associate a given query to the training patterns of its corresponding leaf. It has to be stressed that only indices are moved between the host and the GPU during the iter-ative process (the d -dimensional training and test patterns need to be copied only once ). This leads to a negligible overall cost for memory transfer, see Section 4.
 The procedure F IND L EAF B ATCH operates on the top tree to determine, for each query index, the next leaf that may contain closer neighbors (if such a leaf is found, then the leaf index is returned, otherwise  X  1 to signal that the stack traversal has reached the root twice). Executing the tree traversal on the GPU exhibits massive flow divergence and irregular, hence expensive, accesses to memory. To mini-mize these inefficiencies, we use a relatively small top tree Algorithm 4 LeafNearestNeighbor (e.g., h = 8 ), which results in a suboptimal but still signif-icant speed-up over the sequential execution. The efficient execution of P ROCESS A LL B UFFERS is cru-cial for our approach and implemented via three kernels: (1) The first kernel ( TestSubset ) takes the query in-(2) The second kernel ( LeafNearestNeighbor ) (3) The third kernel ( Update ) merges, for each query The implementation for the LeafNearestNeighbor kernel is sketched in Algorithm 4: The arrays tests and trains hold the test and training patterns in global mem-ory arrays, respectively. Further, the arrays nn dists and nn inds will be filled with the distances and indices of the k nearest neighbors for all test indices. Note that tests , nn inds , and nn dists are maintained in transposed form to achieve coalesced access. Finally, the global arrays lbs and ubs associate the queries with the leaves (i.e., query tid  X  X  0 , . . . , N  X  1 } has to be compared with the training patterns from lbs[tid] to ubs[tid] ).
 In Steps 5 X 7, the test pattern is loaded from global to pri-vate memory. This access is coalesced since the last index is the global thread id, hence, consecutive threads neces-sarily access consecutive memory locations. Further uses of test patt , stored in private thread memory, exhibit efficient access. Steps 8 X 14 compute the distances between all training patterns and the test pattern, and, if necessary, update the nearest neighbors. Finally, Steps 15 X 18 com-mit the computed nearest neighbors to global memory and exhibit coalesced accesses.
 The access to trains[t,j] in Step 11 provides the main motivation for buffer k -d trees: First, the SIMD execution ensures that threads within a warp have the same value for j and t . Second, since many consecutive threads operate on the same leaf, all such threads access nearby memory locations. Hence, this access pattern exhibits both within-warp and within-block data locality (Jia et al., 2012) yield-ing latencies comparable to those of constant memory. 5 Our method speeds up nearest neighbor search in scenarios with a large amount of training and a huge amount of test patterns, given input spaces with moderate dimensionality. This is precisely the setting faced in astronomy, which we use to demonstrate the applicability of our approach. 4.1. Data Mining in Astronomy Current projects such as the Sloan Digital Sky Sur-vey (SDSS) (Ahn et al., 2013) have gathered terabytes of data for hundreds of million astronomical objects. Upcom-ing projects will produce these amounts per night (Ivezic et al., 2011), with final data volumes in the petabyte range. Machine learning techniques have been identified as  X  X ncreasingly essential in the era of data-intensive as-tronomy X  (Borne, 2008) and already led to new discov-eries (Mortlock et al., 2011). In astronomy, one is often given a huge amount of patterns (e.g., two billion) in a low-dimensional feature space (e.g., R 10 ). For this reason, near-est neighbor models (Polsterer et al., 2013; Stensbo-Smidt et al., 2013) are often among the state-of-the-art models. The two most common types of data are photometric and spectroscopic data (Ahn et al., 2013): The former one es-sentially corresponds to images obtained at different wave-length ranges. For a small subset of potentially  X  X nterest-ing X  objects (say, a thousandth), time-consuming follow-up observations in terms of spectra are made. One of the main challenges is to identify promising photometric targets for such follow-up observations (needed to verify an object X  X  nature). In a typical application, almost all of the spectro-scopically confirmed data are used to build models, which are then applied to all remaining objects. 6 Furthermore, computing exact answers is important since minor differ-ences w.r.t. the features are crucial to capture the charac-teristics of astronomical objects.
 While spatial search techniques are well-suited for this task, the application of the final model can still easily take hours (or even days) on today X  X  multi-core desktop ma-chines. In contrast, scanning large amounts of query pat-terns (i.e., transferring them between disk and main mem-ory) can be done efficiently due to the data being stored consecutively (e.g., in minutes only). Thus, reducing the testing to scanning time is desirable for today X  X  catalogs, and will play a crucial role for upcoming catalogs. Given photometric data, one usually extracts a small set of expressive features, called magnitudes . The basis for the extraction are five grayscale images obtained through The most established extraction schemes are the point-spread-function ( psf ), the Model ( model ), and the Pet-rosian ( pet ) approaches (Ahn et al., 2013), and the in-duced features often form the basis for data mining mod-els. Typical features are the colors , which are differences of magnitudes. Below, we consider various combinations of features, see Table 1 ( all denotes psf,model,pet ). 4.2. Experimental Setup All experiments were conducted on a standard PC with an Intel(R) Core(TM) i7-3770 CPU at 3.40GHz (4 cores, 8 hardware threads), 16GB RAM, and a GeForce GTX 770 GPU with 1536 shader units (4GB RAM). The operating system was Ubuntu 12.04 (64 Bit). We re-port runtimes for the query phase (all tree construction times are very small and irrelevant in the above scenario). All algorithms were implemented in C and OpenCL com-piled using Swig with gcc-4.6.3 and -fopenmp as additional compiler option; Python was used to set up the experiments. 7 For a buffer k -d tree of height h , we fixed B = 2 22  X  h . In each iteration of Algorithm 1, M = 5 B indices were fetched from input and reinsert . 8 We compared the following implementations: (1) bufferkdtree(gpu) : our approach with F IND -(2) bufferkdtree(cpu) : a corresponding sequential (3) kdtree(cpu,i) : a multi-core implementation of (4) kdtree(gpu) : a na  X   X ve GPU implementation, (5) bruteforce(gpu) : a brute-force GPU imple-The main baselines are bruteforce(gpu) and kdtree(cpu,i) . Both approaches have been evaluated extensively in the literature, which allows to place the runtimes reported in a broader context. 4.3. Results If not stated otherwise, we used psf colors and psf model mag as input data sets and searched for k = 10 nearest neighbors given n = 10 6 training patterns. Influence of Tree Heights. For all k -d tree-based ap-proaches, the height h can significantly influence the run-ning time. In Figure 2, the runtime behaviors for vary-ing tree heights are given. For bufferkdtree(gpu) , a smaller tree height was favorable (to be explained be-low), whereas larger assignments for h led to better run-times for the other schemes. For classical k -d trees, this is a well-known behavior since h determines the trade-off between pruning capability and the overhead for visiting too many leaves (i.e., many leaf visits vs. the overhead for traversing the tree); the na  X   X ve many-core implementation kdtree(gpu) exhibited a similar behavior. For the fol-lowing experiments, we fixed the optimal tree height for each method. Buffering  X  Caching. The main benefit of the delayed querying via a buffer k -d tree is the induced locality of the test and training patterns on the GPU. Note that a simi-lar  X  X elayed X  traversal can also be achieved without any buffers (by processing M indices in each step being stored in the same order as they enter the top tree). However, this way, the data locality is lost, i.e., two consecutive query indices will, in general, belong to different leaves. This, in turn, leads to arbitrary global memory accesses for the training patterns (i.e., no caching).
 To investigate the importance of buffering, we compared the runtimes obtained with and without buffering, see Fig-ure 3 (a). Obviously, buffering plays a crucial role in this context: Given arbitrary accesses to global memory of the GPU led to a significant drop in performance. Overhead &amp; Speed-Ups. The use of buffer k -d trees provides a separation of the overall workflow into two phases: (1) finding the leaves that need to be processed next and (2) updating the nearest neighbor candidates. In Figure 4, a comparison between bufferkdtree(cpu) and bufferkdtree(gpu) is given. Most of the run-time of bufferkdtree(cpu) was spent for processing the buffers ( &gt; 99 %), and bufferkdtree(gpu) yielded a significant speed-up (about 130) over its CPU analog. The speed-up was even larger for the P ROCESS A LL B
UFFERS phase (about 150), which demonstrates the ef-ficiency of the corresponding kernel implementation. A deeper insight into the runtimes for this phase is provided in Figure 3 (b). It can be seen that the nearest neighbor search (Algorithm 4) took most of the time; all remaining steps (e.g., rearranging the test patterns in global memory, see Section 3.3.2) consumed less than 10% of the time (in particular, all memory operations between the host and the GPU took less than 1% of the overall execution). Figure 5 shows runtimes for different k values: Increasing k led to more leaf visits/larger runtimes for all tree-based schemes; the decrease of performance, however, was sim-ilar (optimal tree heights were selected). The performance of bruteforce(gpu) was not significantly affected. In Figure 6, runtime results for varying training set sizes n are shown. To compensate the increase of training patterns, we adapted the tree heights dynamically. 9 The performance of the brute-force scheme decreased significantly for in-creasing n , whereas all tree-based methods could compen-sate the increase of n much better. A detailed runtime comparison is given in Table 2 for m = 10 7 test patterns (for all , two chunks of size 5  X  10 6 were processed). Two main observations can be made: First, our bufferkdtree(gpu) scheme yielded a valuable speed-up for all data sets. This was in particular the case for psf model mag and all mag . Here, speed-ups be-tween 15 and 22 could be obtained compared to the corre-sponding multi-core implementation ( kdtree(cpu,8) ). Second, our approach was always superior to the (known to be a strong) bruteforce(gpu) scheme. Even in higher dimensional feature spaces, our approach still yielded practical benefits (where all other tree-based approaches did not yield any performance gain). Hence, our bufferkdtree(gpu) implementation al-ways yielded significant performance gains over the multi-core k -d tree scheme and still provided benefits over bruteforce(gpu) for feature spaces up to d = 27 . Our framework can basically handle an arbitrary amount of test patterns by processing chunks of queries. 10 As a fi-nal experiment, we applied a nearest neighbor model with n = 2  X  10 6 training patterns to the whole SDSS catalog (DR 9) with a total amount of m  X  1178 million test pat-terns (using psf model mag and k = 10 ). One core of the CPU was used to parse the data, while a second one and the GPU were used for bufferkdtree(gpu) . The test patterns were processed in chunks of size 10 7 . The over-all runtime needed to apply the model was 4639 seconds (about 39 seconds per chunk on average), which is in line with the results reported in Table 2. The memory consump-tion was dominated by the space needed to accommodate the training and test patterns as well as to keep track of the nearest neighbors (distances and indices). On the GPU, a maximum amount of 3GB was allocated during the execu-tion of each chunk for this experiment (all resources were released after the completion of a single chunk). We proposed the concept of a buffer k -d tree for efficient k -d tree-based nearest neighbor search on GPU S . The method is designed for processing huge amounts of queries in
R d with d larger than 4 and up to  X  25 . The key idea is to reorganize the querying process such that queries be-longing to the same leaf are processed in batches. As both the training and the query patterns reside consecutively in memory, the processing is much more amenable to an ef-ficient GPU implementation. The principle of achieving  X  X ata locality X  can be found in other fields such as memory-centric or I/O-efficient algorithms, but is new for GPU-based nearest neighbor search and may be also useful for other tree structures. The experimental results obtained on various astronomical data sets clearly demonstrate a signif-icant performance gain over competing methods.
 Acknowledgements. FG acknowledges support from the German Academic Exchange Service and CI from The Danish Council for Independent Research ( SkyML project).
