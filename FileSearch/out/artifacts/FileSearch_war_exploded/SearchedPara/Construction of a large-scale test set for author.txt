 1. Introduction
Since different authors may use the same name, author names in bibliographic data have ambiguities in terms of repre-senting real-world authors. Author disambiguation resolves such ambiguity by mapping author names to real-world individ-uals. Thus, it can improve the effectiveness of author search by enabling author-centered clustering of articles. For example, locate all the other papers of the hyperlinked  X  X . Gupta  X  . If author disambiguation has been correctly applied to this digital library, the user would retrieve the right papers only authored by the  X  X . Gupta  X  of paper P rather than numerous articles written by each of name sakes named  X  X . Gupta  X  .
 Recently, literature reports over 90% performance in author resolving experiments ( Song, Huang, Councill, Li, &amp; Giles, 2007;
Huang, Ertekin, &amp; Giles, 2006; Kanani &amp; McCallum, 2007 ). Except Scopus, an author disambiguation function in large-scale scholarly literature search systems. In order that author disambiguation is commonly adopted by bibliographic search systems, extensive researches using the test sets reflecting real circumstances 2005; Kanani &amp; McCallum, 2007; McRae-Spencer &amp; Shadbolt, 2006 ) are not sufficient to meet the diversities of author names/ambiguities.AmongpresentEnglishtestsets, forexample,the numberofdifferent namesisat most24 ( Leeetal., 2005 ).
To overcome such limitation, we have attempted to create a new large-scale test set for author disambiguation. Its construction process is mainly as follows. First, as a bibliographic source from which name occurrences are gathered, DBLP data was chosen considering correctness, publicity, largeness, etc. Next, for highly frequent 1000 names in DBLP, name occurrences (in the form of citation records) including those 1000 names were collected from DBLP. Finally, author identities  X  were manually checked using automatically-found personal publication web pages. As a result, the newly-built test set is comprised of 41,673 name tokens, 881 names, and 6921 real-world author identifiers.

The remainder of this article is organized as follows. Section 2 describes previous test sets. Section 3 details the creation process of the new test set. Section 4 gives its characteristics. Section 5 presents its author resolving performance. Section 6 gives a conclusion. 2. Related work 2.1. Test sets
Table 1 shows the statistics of the previous test sets for author disambiguation. Columns indicate the test set name, the number of different names, the number of author-name instance records, the number of real-world authors, author ambi-guity, and the best performance reported. For example, test set psu-citeseer-14 has a total of 8442 author-name instance re-cords gathered for 14 person names which were resolved into 480 distinct persons. Author ambiguity is the degree of being ambiguous in classifying an author name into one of possible author(s), which is calculated as the number of real individuals divided by the counts of distinct names.
 Test sets psu-citeseer-9/10/14 were created from DBLP data by CiteSeer research group in Pennsylvania State University.
Researchers have employed psu-citeseer test sets of different sizes in their experiments. Figures for psu-citeseer-9/10/14 in Ta-Lee. D. W., 2006 ). Three test sets umass-dblp-17 , umass-rexa-8 and umass-penn-7 were constructed based on DBLP data and REXA corpus in University of Massachusetts Amherst ( Kanani &amp; McCallum, 2007 ). southampton-8 was built in University of Southampton to evaluate the author resolution function of AKTiveAuthor system ( McRae-Spencer &amp; Shadbolt, 2006 ).
For a single author name, not a name occurrence, there exists a set of same-name author-instance occurrences to which author resolution is applied. Such characteristics of the sets (of same-name author occurrences) as diversities of author names and ambiguities may be easily observed in real-world bibliographic data. From the perspective that an author reso-lution test set should be a representative sample of real citation data, however, the present test sets are not enough to satisfy these properties.

Figs. 1 and 2 show the ambiguity distributions of same-name author-instances groups respectively for psu-citeseer-14 and psu-pike-24 . Each point indicates author ambiguity (or the number of real-world individuals) for the size of a same-name group. The solid line is a reference line ( Y = X ) to denote points corresponding to groups where each name instance maps to different person. It is known from these figures that psu-citeseer-14 has many same-name groups of medium ambiguity and psu-pike-24 is skewed toward low-ambiguities irrespective of the size of name groups. It is not clear whether the com-plexity of author disambiguation problem depends on author ambiguity. Extremely, however, author occurrences of a low-est-ambiguity name group ( Y = 1) could be perfectly disambiguated by a single-clustering method, and those of a highest-ambiguity name group ( Y = X ) by a singleton-clustering technique. that it exhibits various author ambiguities.

In addition, there are numerous researchers using non-English names. Names in non-English-speaking countries are transliterated or transcribed with information loss to appear in international academic publication. Such romanized name occurrences should be dealt with differently from their native name occurrences. In this respect as well, the present test sets, where the number of different names ranges between 7 and 24, are too small to cover many non-English names. 2.2. Approaches to author disambiguation
Previous approaches to author disambiguation are well described in ( Smalheiser &amp; Torvik, 2009 ). This paper summarizes some state-of-the-art approaches used to yield performances shown in Table 1 .

Using Latent Dirichlet Allocation (LDA) modeling technique, Song et al. (2007) capture the probabilistic degree of each name occurrence being related to each of a predefined number of topics which model the contents of articles. Based on the agglomerative clustering method, name occurrences dealing with similar topics are then merged into a cluster indicating a real-world author. To represent an article where an author name to be disambiguated occurred, they used the first page of etc.

Huang et al. (2006) employed Support Vector Machine (SVM) to learn a supervised distance function which calculates the degree of two name occurrences referring to the same real-world individual. They used DBSCAN as the clustering method to handle transitivity problem in author disambiguation which can be caused by assigning an incorrect distance value between two name occurrences due to noisy data and/or imperfect distance function.

Pereira et al. (2009) exploited Web information to disambiguate author names. They first gathered a collection of web pages including citation records of which author names are to be disambiguated. From the gathered web documents, they identified single-author-documents, which are defined as the web pages that only contain citation records of a single author, to discriminate them from non-personal web pages such as digital library pages. A single-author-document is then used to merge citations within that document into a cluster which signifies a set of articles written by a single person. Inverse Host Frequency (IHF) were employed to assign the order of using single-author-documents in merging.

Tan et al. (2006) utilized Web evidence to resolve name ambiguity. They represent author name occurrence N to be dis-ambiguated as a vector of its URL features. Such URLs are gathered from Google through a query comprised of title of the citation record containing N. They defined Inverse Host Frequency (IHF) as URL feature value. For the hostname of each
URL, IHF assigns low values to aggregator web sites with high hostname frequencies, high values to personal web pages with low hostname frequencies. Finally, agglomerative clustering is applied to a set of name occurrences.

Kanani and McCallum (2007) viewed author disambiguation as the problem of graph partitioning. In their co-reference graph, a node signifies a citation mentioning an author, and an edge between two nodes means the probabilistic degree that the two nodes co-refer to the same person. After a graph partitioning algorithm is applied on the graph where edge weights are learned from a maximum entropy classifier, each partition is assumed to represent citation nodes belonging to the same author. They succeeded to improve the performance by augmenting their graph with Web information such as edge weights reflecting whether or not there exist web pages containing two citation records for edge-incident nodes, or newly added nodes corresponding to personal publication web pages.

McRae-Spencer and Shadbolt (2006) attempted a graph-based approach to author resolution by considering self-citation links within the citation graph for deciding authorship under the assumption that authors tend to cite their own research works. In addition to self-citation, they employed co-authorship and other information of document source.
In summary, researchers have focused on devising clustering techniques or on finding the better pair-wise similarity function that name occurrences refer to the same individual. Computing the pair-wise similarity have benefited from
Web evidences such as personal web pages containing publication lists. 3. Construction of a test set The process of creating a new test set for author disambiguation is as follows: Step-1: Determining a bibliographic data source.
 Step-2: Deciding the set of author names.
 Step-3: Generating author name occurrences.
 Step-4: Collecting author-resolving information.
 Step-5: Attaching author identifiers.
 Step-6: Verification and repeating Step-5.

Step-1 is the stage to fix the set of bibliographic records from which author name occurrences to be included in a test set should be extracted. For this, ArXiV, CiteSeer, CS BiBTeX, DBLP and NCSTRL were investigated selected in consideration of correctness, publicity, size, acquisition easiness, and popularity. As of June 2009, DBLP data holds more than 1.2 million bibliographic records in computer science ( Ley, 2009 ) which are manually checked, updated at proper times, online-serviced, and easily downloaded. Our test set construction started from late-2007 DBLP data (called DBLP-Bib ) of about 870 thousand bibliographic records.

Step-2 is the phase to decide the set of different author names to be considered in the test set. For this, top 1000 high-frequency author names were extracted from DBLP-Bib to make the set of author names (called DBLP-NameSet ). For instance, suppose the following example of a small bibliographic data source.
 J. Mitchell. 1983. File Servers. AC, 221 X 259.
 P. Lincoln, J. Mitchell. 1991. Algorithmic Aspects of Type Inference with Subtypes. POPL, 293 X 304.
 P. Lincoln, J. Mitchell, A. Scedrov. 1996. Linear logic proof games and optimization. BSL, 322 X 338.
 For these, top 2 high-frequency author names would be J. Mitchell (frequency 3) and P. Lincoln (frequency 2).
Step-3 generates, for each person name pn in DBLP-NameSet , the set (called DBLP-NameEntitySet ) of citation records con-taining name pn from DBLP-Bib . For instance, suppose that DBLP-Bib is the above example bibliographic data, and DBLP-NameSet is {J. Mitchell, P. Lincoln, A. Scedrov}. Then, DBLP-NameEntitySet would be the following: R1: J. Mitchell . 1983. File Servers. AC, 221 X 259.
 R2: P. Lincoln, J. Mitchell . 1991. Algorithmic Aspects of Type Inference with Subtypes. POPL, 293 X 304.
 R3: P. Lincoln, J. Mitchell , A. Scedrov. 1996. Linear logic proof games and optimization. BSL, 322 X 338. R4: P. Lincoln , J. Mitchell. 1991. Algorithmic Aspects of Type Inference with Subtypes. POPL, 293 X 304.
 R5: P. Lincoln , J. Mitchell, A. Scedrov. 1996. Linear logic proof games and optimization. BSL, 322 X 338.
R6: P. Lincoln, J. Mitchell, A. Scedrov . 1996. Linear logic proof games and optimization. BSL, 322 X 338. Top three records (R1, R2, R3) in the above correspond to citation records of J. Mitchell, next two records (R4, R5) those of
P. Lincoln, and last record (R6) that of A. Scedrov. The boldface term in the above, for example J. Mitchell in R1, signifies a target name to be disambiguated. Such a term is called t-name in this paper.

Step-4 is the stage to gather clues to resolve name occurrences in DBLP-NameEntitySet to individuals. Unlike the case of previous test sets, this study automatically harvests personal publication-list web pages (called PPLpage) as author-disam-biguating clues from Web. For each record in DBLP-NameEntitySet , the PPLpage that belongs to an individual using t-name of that record is searched by submitting a proper query to Google search engine. Basically, the query should include t-name (an author name to be resolved) and the title of the paper, as query terms. To have a search engine focus on personal tion  X  . 4 To find relevant PPLpages, a variety of query-coining techniques have been attempted using the combinations of the aforementioned terms and Google search option terms like intitle: , to find a web page that has the surname of t-name in the title field and the paper title in its main body. An example of such a query for the above citation record R2 is as follows, which searches PPLpage of the  X  X . Mitchell  X  who wrote a paper titled  X  X lgorithmic Aspects of Type Inference with Subtypes  X  .
 Google query= intitle:Mitchell Algorithmic Aspects of Type Inference with Subtypes.

For each bibliographic record r in DBLP-NameEntitySet , a query is formulated in the form of  X  X  intitle:  X  followed by the sur-name of t-name of r followed by the paper title of r . For example, the following queries would be generated for the 6-record example DBLP-NameEntitySet shown in the previous Step-3.
 Google query = intitle:Mitchell File Servers.
 Google query = intitle:Mitchell Algorithmic Aspects of Type Inference with Subtypes.
 Google query = intitle:Mitchell Linear logic proof games and optimization.
 Google query = intitle:Lincoln Algorithmic Aspects of Type Inference with Subtypes.
 Google query = intitle:Lincoln Linear logic proof games and optimization.
 Google query = intitle:Scedrov Linear logic proof games and optimization.

Using the above query-formulation method, a gathering program automatically collects as PPLpage-candidates top 20 web pages retrieved for each record in DBLP-NameEntitySet . Internally, the gathering program creates a web URL address like below for citation record R1, where colon and space characters are converted into %-prefixed hexadecimal ASCII values like % 3A and % 20 respectively, and then downloads a document returned for the specified URL. http://www.google.com/search?num=20&amp;q=intitle%3AMitchell%20Algorithmic%20Aspects%20of%20Type%20Inference% 20with%20Subtypes .

Fig. 3 shows a snapshot of the document retrieved from Google using the above query. The document contains summaries (like title, extract, URL, etc.) of up to 20 web pages (PPLpage-candidates) highly relevant to the above query. In other words,
PPLpage-candidates of t-name J. Mitchell of R1 would be the following. http://theory.stanford.edu/~jcm/publications.htm . http://theory.stanford.edu/~jcm/students.html . http://www.informatik.uni-trier.de/~ley/.../Mitchell:John_C=.html . http://www.informatik.uni-trier.de/~ley/db/indices/a.../Wand:Mitchell.html . http://en.scientificcommons.org/john_c_mitchell . http://citeseer.ist.psu.edu/old/710850.html . http://citeseer.ist.psu.edu/old/514439.html . http://research.microsoft.com/en-us/um/people/.../18.../18-eucliddesignocr.doc . http://community.haskell.org/~ndm/downloads/ .

Among the candidate PPLpages retrieved from the gathering program, there may be some incorrectly retrieved pages which are maintained not by individual researchers but by well-known digital libraries such as DBLP, CiteSeer, ACM Digital
Library, IEEE Explorer, etc. Such false positives are filtered out based on a PPLpage negative URL list which is compiled in the following way. First, assuming that a result retrieved for each record in DBLP-NameEntitySet is a document, the document frequency (df) of the host address of each candidate PPLpage  X  s URL is calculated. Then, highly-df-frequent host addresses are reviewed and assembled into the negative URL list, some of which are shown in Table 2 .
 Step-5 manually assigns author identifiers to t-name occurrences in DBLP-NameEntitySet based on clues gleaned in Step-4.
For each t-name occurrence, its PPLpages obtained from Step-4 are visited in order of Google-ranking to assess whether the pages are about the author using the t-name while skipping URLs belonging to the negative URL list, until a correct-PPLpage is found. For the example of Fig. 3 , a human assessor visits each of the nine PPLpage-candidate URLs to find a correct-PPL-
Fig. 4 shows a part of the contents of top 1 ranked URL http://theory.stanford.edu/~jcm/publications.htm , where the assessor confirms whether the page is about an author named  X  X . Mitchell  X  ( t-name ) and whether there exists citation record R1. In this case, top 1 ranked URL corresponds to the correct-PPLpage of  X  X . Mitchell  X  in R1.

During constructing the test set, a human assessor has spent on average 2 X 3 min to locate a correct PPLpage for a t-name occurrence. As you see in Fig. 3 , there are many cases that summaries of PPLpage-candidates returned by Google may have much information enough to filter out false positives without clicking the URL and seeing its entire content, reducing the time to find the correct PPLpage. For example, the last PPLpage-candidate in Fig. 3 has the title  X  X  X eil Mitchell  X  Downloads X  in its summary, meaning the PPLpage-candidate is not about J. Mitchell but about Neil Mitchell.

Then, each correct-PPLpage  X  s URL is assigned a unique author identifier. Thus, t-name occurrences having the same cor-rect-PPLpage  X  s URL are assigned the same identifier. Actually, t-name (J. Mitchell) occurrences in R1 and R2 have the same correct PPLpage URL http://theory.stanford.edu/~jcm/publications.htm , so they are given the same author identifier.
For some t-name occurrences, however, there were no PPLpage-candidates after eliminating false positives in the previous stage. In addition, there were the cases that no relevant PPL pages have been identified. Correctly handling such cases would require us to search full-text of related articles for biographic information mined not to deal with correct-PPLpage-missing t-name occurrences and to simply eliminate from DBLP-NameEntitySet the citation records that have no relevant PPLpages. As a result, the initial 1000 names in DBLP-NameSet were reduced to 881, and DBLP-NameEntitySet is comprised of a total of 41,673 name occurrence records.
 Step-6 inspects correctness of the result of Step-5 and may modify the result. If changes are made, Step-5 is re-performed.
There were two types in need for modification. The first is the case where two or more t-name occurrences have a common author-identifying prefix in their correct PPLpage URLs. The following citation records R7 and R8 show such an example, where t-names (J. Yan) in R7 and R8 share the common prefix string http://www.ece.ubc.ca/~josephy/ indicating author iden-tity in their URLs, highly supporting the fact that R7 and R8 were written by the same person named J. Yan. In this case,
Step-5 is re-performed for R7 and R8 to modify their correct PPLpage URLs to the common prefix string and to assign the same author identifier to t-names of R7 and R8.
 R7: J. Yan, R. Wood, S. Avadhanula, M. Sitti, R. Fearing. 2001. Towards flapping Wing Control for a Micromechanical Flying Insect. ICRA, 3901 X 3908. ( http://www.ece.ubc.ca/~josephy/cv.html )
R8: S. Salcudean, J. Yan. 1994. Towards a Force-Reflecting Motion-Scaling System for Microsurgery. ICRA, 2296 X 2301. ( http:// www.ece.ubc.ca/~josephy/pub.html/ ) R9: R. Fearing, K. Chiang, M. Dickinson, D. Pick, M. Sitti, J. Yan. 2000. Wing Transmission for a Micromechanical Flying Insect. ICRA, 1509 X 1516. ( http://robotics.eecs.berkeley.edu/~joeyan/cv.html )
The second is the case where an author maintains his or her publication pages at two or more different web sites like URLs of R7 and R9 in the above. To locate such cases, human assessors first gather academic/professional background information for the author named t-name (e.g. J. Yan in R7), which can be obtained from the homepage or separate resume page/file with-in the web site to which the correct PPLpage URL belong. Then, when two correct PPLpage URLs include web site addresses the same person, and pages of the two URLs share the same citation records, contents of the two URLs are assumed to be maintained by the same person. 4. Characteristics of the new test set
The newly constructed test set 7 is called KISTI-AD-E-01-TestSet meaning that it was created for the first time (01) for Eng-lish (E) author disambiguation (AD) by Korea Institute of Science and Technology Information (KISTI). As shown in Table 3 , the test set consists of 41,673 name occurrence citation records for 881 different names. In summary, each of 41,673 name instances among a total of 116,564 occurring in 37,613 articles has been manually disambiguated into one of 6921 real-world individuals with author ambiguity of 7.86.

Below is a part of the test set, showing that there are two persons named  X  X  X . Datta X  and name instances  X  X  X . Datta X  X  in first two citations refer to the same person. Different individuals are identified by authorID . A set of citations where the same name appears is denoted by nameGroupID . dblpkey is a key to a citation record that DBLP system uses. &lt;citation nameGroupID= X  X . Datta X  dblpkey= X  X ournals/informs/DattaV00 X &gt;&lt;author authorID=1 fullname= X  X nindya Dat-&lt;citation nameGroupID= X  X . Datta X  dblpkey= X  X ournals/jss/DattaMV98 X &gt;&lt;author authorID=1 fullname= X  X nindya Dat-246&lt;/pages&gt;&lt;/citation&gt; &lt;citation nameGroupID= X  X . Datta X  dblpkey= X  X onf/pomc/AnceaumeDGS02 X &gt;&lt;author fullname= X  X mmanuelle Ancea-scheme for mobile networks&lt;/title&gt;&lt;year&gt;2002&lt;/year&gt;&lt;pages&gt;74-81&lt;/pages&gt;&lt;/citation&gt;
Fig. 5 shows the size-distribution of same-name author-instances groups within the new test set, illustrating how many groups exist for a specific group size ranging over 1 through 325. For example, there are 7 same-name groups of size 65 in the 150, 200 cover respectively about 21%, 49%, 67%, 87%, 96%, 99% of the entire groups. This means that there are relatively more small-size groups (50% up to size 30) but there exists a considerable number of large-size groups (117 groups, 13% above size 100).

Fig. 6 shows the distribution of the number of real-world individuals in same-name author-instances groups, demonstrat-ing how many same-name groups exist for a specific number of authors which ranges over 1 X 71. For instance, there are 14 same-name groups in which 15 persons are identified in the test set. A dotted line has a similar interpretation. Groups up to person counts 1, 3, 5, 10, 20, 30, 50 correspond to respectively about 12%, 33%, 51%, 80%, 93%, 97%, 99% of the whole groups.
This means that most groups have low-ambiguities but there are many groups (69 groups, 7.8%) each name occurrence of which should be disambiguated into one of more than 20 persons.

Fig. 7 shows the ambiguity distribution of 881 same-name groups of the new test set. Compared to those of other test sets (see Figs. 1 and 2 ), the new test set exhibits more diversity in both author ambiguity and size of same-ambiguity groups, which was one of the goals of this study. Especially, author ambiguities for small-sized groups are very diverse, and the same is true for the size-distribution of low-ambiguity groups. As the group size increases, however, most groups show low to medium ambiguities and high-ambiguity groups are more difficult to find.

To examine the degree of name diversity in the new test set, the country distribution of surnames of some person names was produced as shown in Fig. 8 in the following way. First, a list
Wikipedia. The surname list contains a total of 1675 common surnames worldwide, counts of which were summarized by coun-tries in Fig. 9 . Regarding surnames, there are 381 different surnames among 881 person names to be disambiguated in the test set. In addition, 165 (43.3%) of 381 surnames are found in the surname list, which covers 29,985 (71.9%) of 41,673 name occur-rences in the test set.

Note that countries of namesakes may be different since same names can be used in different countries due to the use of pseudonym, the effect of immigration, etc. Table 4 shows top surnames among 165 surnames above that may be highly used in many countries. To acquire the country distribution of surnames, 29,985 surname appearances should be thus resolved into their original countries. To simplify the work, we rely on the assumption that the nationality of an author can be deter-mined by those of his or her co-workers. Under this assumption, an author-country mapping method is as follows.
Thus, the formula simply selects, among candidate countries of author p , the one where the surname of the most coauthors surnames of p are the same.

Using the above method, 29,985 surname appearances in the test set were discriminated into 42,450 countries (1.42 countries per surname) which were demonstrated by countries in Fig. 8 . Though the result is a rough approximation, it can be observed that names occur in the test set over diverse countries. 5. Performance of author disambiguation
To see the relationship between the complexity of author-resolving problem and author ambiguity, this section provides the author disambiguation performance of the new test set. First eliminating 14 name groups that have a single name occur-rence record, experiments have been carried out on 41,659 name instances of 867 name groups. To disambiguate name appearances into authors, the single-linkage clustering was performed using a binary distance function shown in the following.

In the above D ( u , v ) is a distance between entities u , vectors. fsim i ( u i , v i ) is a similarity value of i th feature of u and tion that returns 1 if expression e is true, 0 otherwise.

For feature-vector representation of a name occurrence, this study has utilized these features like coauthor name(s) (C), returns 1 if there is an overlap between feature values, 0 otherwise. For the paper title, fsim calculates a tfidf-based cosine similarity between titles of two name occurrence citation records considering a title as a document. For title terms, stop-words are eliminated and Porter stemming is performed. The tfidf weight for a title term is computed using the following of term t . Fig. 10 shows performances of author disambiguation for the new test set using the pair-wise F1 metric ( McCallum,
Nigam, &amp; Ungar, 2000 ). Among all possible combinations of features, the best was CPT which means clustering name occur-rences into authors based on such features as coauthor, publication title, and paper title with h
CPT-based author disambiguation, Figs. 11 and 12 show performance distributions of same-name author-instances groups respectively by group size and author ambiguity for 867 name groups. Dotted-lines show trend lines obtained from linear regression provided by Microsoft Excel. Dense distributions in the areas of small-size and low-ambiguity are due to the fact that most name groups are located at small-size and low-ambiguity areas as shown in Figs. 5 and 6 . In the case of the group size, performances do not seem to behave sensitively to the group size. As ambiguity increases, however, it can be witnessed that the performance gradually decreases. From Figs. 11 and 12 , thus, it could be interpreted that the complexity of the author resolution problem is relatively more dependent on author ambiguity than the size for the same-name author in-stances within the new test set.

The new test set shows diversities in author ambiguity, sizes of same-name groups, and non-English names, which are expected to be found in practical author disambiguation environments such as large-scale digital libraries and bibliographic search services. Thus, the test set could be said to be more balanced than previous test sets in terms of at least such features. These characteristics would be a new challenge to researchers attacking author disambiguation using machine learning.
Especially, the test set can be used to learn the degrees that name occurrences refer to the same real-world author. For this, the entire test set should be split into training and testing set each of which preserves diversities in ambiguity, group sizes, and non-English names. 6. Conclusion
In order to overcome the weaknesses of the previous test sets and to foster much research in the area of author disam-biguation, the construction of a new large-scale test set was attempted. This paper described its construction process, char-acteristics, and performance. Among 6-stage construction procedures, Step-4 has contributed in reducing the creation time by automatically acquiring Web evidences to resolve name occurrences to persons. Step-5 however has lengthened the time since it requires human assessors to identify relevant personal-publication-list pages among multiple candidates. An at-tempt to re-rank the initially-retrieved PPLpage-candidates such that the correct PPLpage is located in a higher rank could shorten the time needed in the manual checking part of Step-5. We believe that automating the Step-5 may enable in the future to convert the current semi-manual construction procedures into a semi-automatic method of author disambiguation which could be adopted for high-quality author resolution in large-scale digital libraries. Semi-automatically disambiguating author names could benefit from semi-supervised clustering techniques ( Wang, Li, and Zhang, 2008 ), which attempt to clus-ter objects using some supervised information.
 Acknowledgement This research was supported by Kyungsung University Research Grants in 2010.
 References
