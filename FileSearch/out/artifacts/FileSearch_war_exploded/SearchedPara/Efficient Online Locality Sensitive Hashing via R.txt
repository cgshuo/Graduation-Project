 Feature vectors based on lexical co-occurrence are often of a high dimension, d . This leads to O ( d ) op-erations to calculate cosine similarity, a fundamental tool in distributional semantics. This is improved in practice through the use of data structures that ex-ploit feature sparsity, leading to an expected O ( f ) operations, where f is the number of unique features we expect to have non-zero entries in a given vector.
Ravichandran et al. (2005) showed that the Lo-cality Sensitive Hash (LSH) procedure of Charikar (2002), following from Indyk and Motwani (1998) and Goemans and Williamson (1995), could be suc-cessfully used to compress textually derived fea-ture vectors in order to achieve speed efficiencies in large-scale noun clustering. Such LSH bit signa-tures are constructed using the following hash func-tion, where ~v  X  R d is a vector in the original feature space, and ~r is randomly drawn from N (0 , 1) d : h ( ~v ) =
If h b ( ~v ) is the b -bit signature resulting from b such hash functions, then the cosine similarity between vectors ~u and ~v is approximated by: where D (  X  ,  X  ) is Hamming distance , the number of bits that disagree. This technique is used when b d , which leads to faster pair-wise comparisons between vectors, and a lower memory footprint.
Van Durme and Lall (2010) observed 1 that if the feature values are additive over a dataset (e.g., when collecting word co-occurrence frequencies), then these signatures may be constructed online by unrolling the dot-product into a series of local oper-observed locally at time t in a data-stream.
Since updates may be done locally, feature vec-tors do not need to be stored explicitly. This di-rectly leads to significant space savings, as only one counter is needed for each of the b running sums.
In this work we focus on the following observa-tion: the counters used to store the running sums may themselves be an inefficient use of space, in that they may be amenable to compression through approximation. 2 Since the accuracy of this LSH rou-tine is a function of b , then if we were able to reduce the online requirements of each counter, we might afford a larger number of projections. Even if a chance of approximation error were introduced for each hash function, this may be justified in greater overall fidelity from the resultant increase in b .
Thus, we propose to approximate the online hash function, using a novel technique we call Reservoir Counting , in order to create a space trade-off be-tween the number of projections and the amount of memory each projection requires. We show experi-mentally that this leads to greater accuracy approx-imations at the same memory cost, or similar accu-racy approximations at a significantly reduced cost. This result is relevant to work in large-scale distribu-tional semantics (Bhagat and Ravichandran, 2008; Van Durme and Lall, 2009; Pantel et al., 2009; Lin et al., 2010; Goyal et al., 2010; Bergsma and Van Durme, 2011), as well as large-scale processing of social media (Petrovic et al., 2010). While not strictly required, we assume here to be dealing exclusively with integer-valued features. We then employ an integer-valued projection matrix in order to work with an integer-valued stream of on-line updates, which is reduced (implicitly) to a stream of positive and negative unit updates. The sign of the sum of these updates is approximated through a novel twist on Reservoir Sampling . When computed explicitly this leads to an impractical mechanism linear in each feature value update. To ensure our counter can (approximately) add and sub-tract in constant time, we then derive expressions for the expected value of each step of the update. The full algorithms are provided at the close.
 Unit Projection Rather than construct a projec-tion matrix from N (0 , 1) , a matrix randomly pop-ulated with entries from the set { X  1 , 0 , 1 } will suf-fice, with quality dependent on the relative propor-tion of these elements. If we let p be the percent probability mass allocated to zeros, then we create a discrete projection matrix by sampling from the multinomial: ( 1  X  p 2 :  X  1 ,p : 0 , 1  X  p 2 : +1) . An experiment displaying the resultant quality is dis-played in Fig. 1, for varied p . Henceforth we assume this discrete projection matrix, with p = 0 . 5 . 3 The use of such sparse projections was first proposed by Achlioptas (2003), then extended by Li et al. (2006). Figure 1: With b = 256 , mean absolute error in cosine Unit Stream Based on a unit projection, we can view an online counter as summing over a stream drawn from { X  1 , 1 } : each projected feature value unrolled into its (positive or negative) unary repre-sentation. For example, the stream: (3,-2,1), can be viewed as the updates: (1,1,1,-1,-1,1).
 Reservoir Sampling We can maintain a uniform sample of size k over a stream of unknown length as follows. Accept the first k elements into an reser-voir (array) of size k . Each following element at po-sition n is accepted with probability k n , whereupon an element currently in the reservoir is evicted , and replaced with the just accepted item. This scheme is guaranteed to provide a uniform sample, where early items are more likely to be accepted, but also at greater risk of eviction. Reservoir sampling is a folk-lore algorithm that was extended by Vitter (1985) to allow for multiple updates.
 Reservoir Counting If we are sampling over a stream drawn from just two values, we can implic-itly represent the reservoir by counting only the fre-quency of one or the other elements. 4 We can there-fore sample the proportion of positive and negative unit values by tracking the current position in the stream, n , and keeping a log 2 ( k + 1) -bit integer counter, s , for tracking the number of 1 values cur-rently in the reservoir. 5 When a negative value is accepted, we decrement the counter with probability . When a positive update is accepted, we increment the counter with probability (1  X  s k ) . This reflects an update evicting either an element of the same sign, which has no effect on the makeup of the reservoir, or decreasing/increasing the number of 1  X  X  currently sampled. An approximate sum of all values seen up to position n is then simply: n ( 2 s k  X  1) . While this value is potentially interesting in future applications, here we are only concerned with its sign.
 Parallel Reservoir Counting On its own this counting mechanism hardly appears useful: as it is dependent on knowing n , then we might just as well sum the elements of the stream directly, counting in whatever space we would otherwise use in maintain-ing the value of n . However, if we have a set of tied streams that we process in parallel, 6 then we only need to track n once, across b different streams, each with their own reservoir.

When dealing with parallel streams resulting from different random projections of the same vector, we cannot assume these will be strictly tied. Some pro-jections will cancel out heavier elements than oth-ers, leading to update streams of different lengths once elements are unrolled into their (positive or negative) unary representation. In practice we have found that tracking the mean value of n across b streams is sufficient. When using a p = 0 . 5 zeroed matrix, we can update n by one half the magnitude of each observed value, as on average half the pro-jections will cancel out any given element. This step can be found in Algorithm 2, lines 8 and 9.
 Example To make concrete what we have cov-ered to this point, consider a given feature vec-tor of dimensionality d = 3 , say: [3, 2, 1]. This might be projected into b = 4 , vectors: [3, 0, 0], [0, -2, 1], [0, 0, 1], and [-3, 2, 0]. When viewed as positive/negative, loosely-tied unit streams, they re-spectively have length n : 3, 3, 1, and 5, with mean length 3. The goal of reservoir counting is to effi-ciently keep track of an approximation of their sums (here: 3, -1, 1, and -1), while the underlying feature vector is being updated online. A k = 3 reservoir used for the last projected vector, [-3, 2, 0], might reasonably contain two values of -1, and one value of 1. 7 Represented explicitly as a vector, the reser-voir would thus be in the arrangement: These are functionally equivalent: we only need to know that one of the k = 3 elements is positive. Expected Number of Samples Traversing m con-secutive values of either 1 or  X  1 in the unit stream should be thought of as seeing positive or negative m as a feature update. For a reservoir of size k , let A ( m,n,k ) be the number of samples accepted when traversing the stream from position n + 1 to n + m . A is non-deterministic: it represents the results of flipping m consecutive coins, where each coin is in-creasingly biased towards rejection.

Rather than computing A explicitly, which is lin-ear in m , we will instead use the expected number of updates, A 0 ( m,n,k ) = E [ A ( m,n,k )] , which can be computed in constant time. Where H ( x ) is the harmonic number of x : 8
For example, consider m = 30 , encountered at position n = 100 , with a reservoir of k = 10 . We will then accept 10 log e ( 130 100 )  X  3 . 79 samples of 1.
As the reservoir is a discrete set of bins, fractional portions of a sample are resolved by a coin flip: if a = k log e ( n + m n ) , then accept u = d a e samples with probability ( a  X  b a c ) , and u = b a c samples otherwise. These steps are found in lines 3 and 4 of Algorithm 1. See Table 1 for simulation results using a variety of parameters.
 Expected Reservoir Change We now discuss how to simulate many independent updates of the same type to the reservoir counter, e.g.: five updates of 1, or three updates of -1, using a single estimate. Consider a situation in which we have a reservoir of size k with some current value of s , 0  X  s  X  k , and we wish to perform u independent updates. We de-note by U 0 k ( s,u ) the expected value of the reservoir after these u updates have taken place. Since a sin-gle update leads to no change with probability s k , we can write the following recurrence for U 0 k : U k ( s,u ) = with the boundary condition: for all s , U 0 k ( s, 0) = s .
Solving the above recurrence, we get that the ex-pected value of the reservoir after these updates is: which can be mechanically checked via induction. The case for negative updates follows similarly (see lines 7 and 8 of Algorithm 1).

Hence, instead of simulating u independent up-dates of the same type to the reservoir, we simply update it to this expected value, where fractional up-dates are handled similarly as when estimating the number of accepts. These steps are found in lines 5 through 9 of Algorithm 1, and as seen in Fig. 2, this can give a tight estimate.
 Comparison Simulation results over Zipfian dis-tributed data can be seen in Fig. 3, which shows the use of reservoir counting in Online Locality Sensi-tive Hashing (as made explicit in Algorithm 2), as compared to the method described by Van Durme and Lall (2010).

The total amount of space required when using this counting scheme is b log 2 ( k + 1) + 32 : b reser-voirs, and a 32 bit integer to track n . This is com-pared to b 32 bit floating point values, as is standard. Note that our scheme comes away with similar lev-els of accuracy, often at half the memory cost, while requiring larger b to account for the chance of ap-proximation errors in individual reservoir counters. Figure 2: Results of simulating many iterations of U 0 , Algorithm 1 R ESERVOIR U PDATE ( n,k,m, X ,s ) Figure 3: Online LSH using reservoir counting (red) vs. Algorithm 2 C OMPUTE S IGNATURE ( S , k , b , p ) Time and Space While we have provided a con-stant time, approximate update mechanism, the con-stants involved will practically remain larger than the cost of performing single hardware addition or subtraction operations on a traditional 32-bit counter. This leads to a tradeoff in space vs. time, where a high-throughput streaming application that is not concerned with online memory requirements will not have reason to consider the developments in this article. The approach given here is motivated by cases where data is not flooding in at breakneck speed, and resource considerations are dominated by a large number of unique elements for which we are maintaining signatures. Empirically investigat-ing this tradeoff is a matter of future work. Random Walks As we here only care for the sign of the online sum, rather than an approximation of its actual value, then it is reasonable to consider in-stead modeling the problem directly as a random walk on a linear Markov chain, with unit updates directly corresponding to forward or backward state Figure 4: A simple 8-state Markov chain, requiring transitions. Assuming a fixed probability of a posi-tive versus negative update, then in expectation the state of the chain should correspond to the sign. However if we are concerned with the global statis-tic, as we are here, then the assumption of a fixed probability update precludes the analysis of stream-ing sources that contain local irregularities. 9
In distributional semantics, consider a feature stream formed by sequentially reading the n-gram resource of Brants and Franz (2006). The pair: ( the dog : 3,502,485), can be viewed as a feature value pair: ( leftWord= X  X he X  : 3,502,485), with respect to online signature generation for the word dog . Rather than viewing this feature repeatedly, spread over a large corpus, the update happens just once, with large magnitude. A simple chain such as seen in Fig. 4 will be  X  X ushed X  completely to the right or the left, based on the polarity of the projection, irre-spective of previously observed updates. Reservoir Counting, representing an online uniform sample, is agnostic to the ordering of elements in the stream. We have presented a novel approximation scheme we call Reservoir Counting , motivated here by a de-sire for greater space efficiency in Online Locality Sensitive Hashing. Going beyond our results pro-vided for synthetic data, future work will explore ap-plications of this technique, such as in experiments with streaming social media like Twitter.
 Acknowledgments This work benefited from conversations with Daniel  X  Stefonkovi  X  c and Damianos Karakos.
