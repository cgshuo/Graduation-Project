 In real applications, the model of the problem at hand inevitably embodies some form of uncertainty: the parameters of the model are usually (roughly) estimated from data, which themselves can be uncertain due to various kinds of noises. For example, in finance, the return of a financial product can seldom be known exactly beforehand. Despite this uncertainty, one still usually has to take action in the underlying application. However, due to uncertainty, any attempt to behave  X  X ptimally X  in the world must take into account plausible alternative models.
 Focusing on problems where uncertain data/parameters are treated as random variables and the model consists of a joint distribution over these variables, we initially assume prior knowledge that the first and second moments of the underlying distribution are known, but the distribution is otherwise arbitrary. A parametric approach to handling uncertainty in such a setting would be to fit a specific parametric model to the known moments and then apply stochastic programming techniques to solve for an optimal decision. For example, fitting a Gaussian model to the constraints would be a popular choice. However, such a parametric strategy can be too bold, hard to justify, and might incur significant loss if the fitting distribution does not match the true underlying distribution very well. A conservative, but more robust approach would be to take a decision that was  X  X rotected X  in the worst-case sense; that is, behaves optimally assuming that nature has the freedom to choose an adverse distribution. Such a minimax formulation has been studied in several fields [1; 2; 3; 4; 5; 6] and is also the focus of this paper. Although Bayesian optimal decision theory is a rightfully well-established approach for decision making under uncertainty, minimax has proved to be a useful alternative in many domains, such as finance, where it is difficult to formulate appropriate priors over models. In these fields, minimax formulation combined with stochastic programming [7] have been extensively studied and successfully applied.
 We make a contribution to minimax probability theory and apply the results to problems arising in four different areas. Specifically, we generalize a classic result on the linear projection property of distribution families: we show that any linear projection between distribution families with fixed mean and covariance, regardless of their dimensions, is surjective . That is, given any matrix X and any random vector r with mean X T  X  and covariance X T X , one can always find another random vector R with mean  X  and covariance such that X T R = r (almost surely). Our proof imposes no conditions on the deterministic matrix X , hence extends the classic projection result in [6], which assumes X is a vector. We furthermore extend this surjective property to some restricted distribution families, which allows additional prior information to be incorporated and hence less conservative solutions to be obtained. In particular, we prove that surjectivity of linear projections remains to hold for distribution families that are additionally symmetric, log-concave, or symmetric linear unimodal. In each case, our proof strategy allows one to construct the worst-case distribution(s). An immediate application of these results is to reduce the worst-case analysis of multivariate expec-tations to the univariate (or reduced multivariate) ones, which have been long studied and produced many fruitful results. In this direction, we conduct worst-case analyses of some common restricted distribution families. We illustrate our results on problems that incorporate a classic worst case value-at-risk constraint: minimax probability classification [2]; chance constrained linear program-ming (CCLP) [3]; portfolio selection [4]; and Markov decision processes (MDPs) with reward un-certainty [8]. Although some of the results we obtain have been established in the respective fields [2; 3; 4], we unify them through a much simpler proof strategy. Additionally, we provide extensions to other constrained distribution families, which makes the minimax formulation less conservative in each case. These results are then extended to the more recent conditional value-at-risk constraint, and new bounds are proved, including a new bound on the survival function for symmetric unimodal distributions. First we establish a generalized linear projection property for distribution families. The key appli-cation will be to reduce worst-case multivariate stochastic programming problems to lower dimen-sional equivalents; see Corollary 1. Popescu [6] has proved the special case of reduction to one dimension, however we provide a simpler proof that can be more easily extended to other distribu-tion families 1 .
 Let (  X , ) denote the family of distributions sharing common mean  X  and covariance , and let  X  X = X T  X  and X = X T X . Below we denote random variables by boldface letters, and use to denote the identity matrix. We use  X  to denote the pseudo-inverse.
 Theorem 1 (General Projection Property (GPP)) For all  X  ,  X  0 , and X  X  R m  X  d , the projec-tion X T R = r from m -variate distributions R  X  (  X , ) to d -variate distributions r  X  (  X  X , X ) is surjective and many-to-one. That is, every r  X  (  X  X , X ) can be obtained from some R  X  (  X , ) via X T R = r (almost surely).
 Proof : The proof is constructive. Given a r  X  (  X  X , X ) , we can construct a pre-image R by letting R = X  X  X r +( I m  X  X  X  X X T ) M , where M  X  (  X , ) is independent of r , for example, one can choose M as a Gaussian random vector. It is easy to verify that R  X  (  X , ) and X T R = X  X  X r + (
I (the two random vectors on both sides have the same mean and zero covariance). Note that since M can be chosen arbitrarily in (  X , ) , the projections are always many-to-one.
 Although this establishes the general result, we extend it to distribution families under additional constraints below. That is, one often has additional prior information about the underlying distri-bution, such as symmetry, unimodality, and/or support. In such cases, if a general linear projection property can still be shown to hold, the additional assumptions can be used to make the minimax approach less conservative in a simple, direct manner. We thus consider a number of additionally restricted distribution families.
 Definition 1 A random vector X is called (centrally) symmetric about  X  , if for all vectors x , Pr( X  X   X  + x ) = Pr( X  X   X   X  x ) . A univariate random variable is called unimodal about a if its cumulative distribution function (c.d.f.) is convex on (  X  X  X  ,a ] and concave on [ a,  X  ) . A random vector X is called log-concave if its c.d.f. is log-concave. A random m -vector X is called linear unimodal about 0 m if for all a  X  R m , a T X is (univariate) unimodal about 0. Let (  X , ) S denote the family of distributions in (  X , ) that are additionally symmetric about  X  , and similarly, let (  X , ) L denote the family of distributions that are additionally log-concave, and let (  X , ) SU denote the family of distributions that are additionally symmetric and linear unimodal about  X  . For each of these restricted families, we require the following properties to establish our next main result.
 Lemma 1 (a) If random vector X is symmetric about 0, then A X +  X  is symmetric about  X  . (b) If X , Y are independent and both symmetric about 0, Z = X + Y is also symmetric about 0. Although once misbelieved, it is now clear that the convolution of two (univariate) unimodal distri-butions need not be unimodal. However, for symmetric, unimodal distributions we have Lemma 2 ([10] Theorem 1.6) If two independent random variables x and y are both symmetric and unimodal about 0, then z = x + y is also unimodal about 0.
 There are several non-equivalent extensions of unimodality to multivariate random variables. We consider two specific (multivariate) unimodalities in this paper: log-concave and linear unimodal. 2 Lemma 3 ([10] Lemma 2.1, Theorem 2.4, Theorem 2.18) 1. Linearity: If random m -vector X is log-concave, a T X is also log-concave for all a  X  R m . 2. Cartesian Product: If X and Y are log-concave, then Z = 3. Convolution: If X and Y are independent and log-concave, then Z = X + Y is also log-concave. Given the above properties, we can now extend Theorem 1 to (  X , ) S , (  X , ) L and (  X , ) SU . Theorem 2 (GPP for Symmetric, Log-concave, and Symmetric Linear Unimodal Distributions) For all  X  ,  X  0 and X  X  R m  X  d , the projection X T R = r from m -variate R  X  (  X , ) S to d -variate r  X  (  X  X , X ) S is surjective and many-to-one. The same is true for (  X , ) L and (  X , ) SU . 3 Proof: The proofs follow the same basic outline as Theorem 1 except that in the first step we now choose N  X  (0 m , I m ) S or (0 m , I m ) L or (0 m , I m ) SU . Then, respectively, symmetry of the constructed R follows from Lemma 1; log-concavity of R follows from Lemma 3; and linear uni-modality of R follows from the definition and Lemma 2. The maps remain many-to-one.
 An immediate application of the general projection property is to reduce worst-case analyses of multivariate expectations to the univariate case. Note that in the following corollary, the optimal distribution of R can be easily constructed from the optimal distribution of r .
 Corollary 1 For any matrix X and any function g (  X  ) (including in particular when X is a vector) The equality continues to hold if we restrict (  X , ) to (  X , ) S , (  X , ) L , or (  X , ) SU respectively. Proof: It is obvious that the right hand side is an upper bound on the left hand side, since for every R  X  (  X , ) there exists an r  X  ( X T  X ,X T X ) given by r = X T R . Similarly for (  X , ) S , (  X , ) L , and (  X , ) SU . However, given Theorems 1 and 2, one can then establish the converse. 4 We now apply these projection properties to analyze the worst case value-at-risk (VaR)  X  X  useful risk criterion in many application areas. Consider the following constraint on a distribution R for given x ,  X  and  X   X  (0 , 1) . In this case, the infimum over  X  such that (2) is satisfied is referred to as the  X  -VaR of R . Within certain restricted distribution families, such as Q -radially symmetric distributions, (2) can be (equivalently) transformed to a deterministic second order cone constraint (depending on the range of  X  ) [3]. Unfortunately, determining whether (2) can be satisfied for given x ,  X  and  X   X  (0 , 1) is NP-hard in general [8]. Suppose however that one knew the distribution of R belonged to a certain family, such as (  X , ) . 5 Given such knowledge, it is natural to consider whether (2) can be satisfied in a worst case sense. That is, consider Here the infimum of  X  values satisfying (3) is referred to as the worst-case  X  -VaR . If we have ad-ditional information about the underlying distribution, such as symmetry or unimodality, the worst-case  X  -VaR can be reduced. Importantly, using the results of the previous section, we can easily determine the worst-case  X  -VaR for various distribution families. These can also be used to provide a tractable bound on the  X  -VaR even when the distribution is known.
 Proposition 1 For alternative distribution families, the worst-case  X  -VaR constraint (3) is given by: where  X  x = x T  X , X  x = It turns out some results of Proposition 1 are known. In fact, the first bound (4) has been extensively studied. However, given the results of the previous section, we can now provide a much simpler proof. 6 (This simplicity will also allow us to achieve some useful new bounds in Section 4 below.) Proof: From Corollary 1 it follows that Given that the problem is reduced to the univariate case, we simply exploit classical inequalities: for t  X   X  . 7 Now to prove (4), simply plug (8) into (3) and notice that an application of (9) leads to (4) then follows by simple rearrangement. The same procedure can be used to prove (5), (6), (7). Figure 1: Comparison of the coefficients in front of  X  x for different distribution families in Proposi-tion 1 (left) and Proposition 2 (right). Only the range  X   X  (0 , 1 2 ) is depicted.
 Proposition 1 clearly illustrates the benefit of prior knowledge. Figure 1 compares the coefficients on  X  x among the different worst case VaR for different distribution families. The large gap between coefficients for general and symmetric (linear) unimodal distributions demonstrates how additional constraints can generate much less conservative solutions while still ensuring robustness. Beyond simplifying existing proofs, Proposition 1 can be used to extend some of the uses of the VaR criterion in different application areas.
 Minimax probability classification [2]: Lanckriet et al. [2] first studied the value-at-risk constraint in binary classification. In this scenario, one is given labeled data from two different sources and seeks a robust separating hyperplane. From the data, the distribution families (  X  1 , 1 ) and (  X  2 , 2 ) can be estimated. Then a robust hyperplane can be recovered by minimizing the worst-case error min where x is the normal vector of the hyperplane,  X  is the offset and  X  controls the error probability. Note that the results in [2] follow from using the bound (4). However, interesting additional facts arise when considering alternative distribution families. For example, consider symmetric distri-butions. In this case, suppose we knew in advance that the optimal  X  lay in [ 1 2 , 1) , meaning that no hyperplane predicts better than random guessing. Then the constraints in (12) become linear, covariance information becomes useless in determining the optimal hyperplane, and the optimiza-tion concentrates solely on separating the means of two classes. Although such a result might seem surprising, it is a direct consequence of symmetry: the worst-case distributions are forced to put probability mass arbitrarily far away on both sides of the mean, thereby eliminating any information brought by covariance. When the optimal  X  lies in (0 , 1 2 ) , however, covariance information becomes meaningful, since the worst-case distributions can no longer put probability mass arbitrarily far away on both sides of the mean (owing to the existence of a hyperplane that predicts labels better than random guessing). In this case, the optimization problems involving (  X , ) S and (  X , ) SU are equivalent to that for (  X , ) except that the maximum error probability  X  becomes smaller, which is to be expected since more information about the marginal distributions should make one more confident to predict the labels of future data.
 Chance Constrained Linear Programming (CCLP) [3]: Consider a linear program merely using the expected value of r could result in a solution x that was sub-optimal or even in-feasible. Calafiore and El Ghaoui studied this problem in [3], and imposed the inequality constraint with high probability, leading to the the so-called chance constrained linear program (CCLP): In this case,  X  is simply 0 and  X  is given by the user. Depending on the value of  X  , the chance constraint can be equivalently transformed into a second order cone constraint or a linear constraint. The work in [3] concentrates on the general and symmetric distribution families. In the latter case, [3] uses the first part of inequality (5) as a sufficient condition for guaranteeing robust solutions. Note however that from Corollary 1 and Proposition 1 one can now see that (5) is also a necessary condition. Although the symmetric linear unimodal case is not discussed in [3], from Proposition 1 again one can see that incorporating bound (6) in (13) yields a looser constraint than does (5), hence the feasible region will be enlarged and the optimum value of the CCLP potentially reduced, corresponding to the intuition that increased prior knowledge leads to more optimized results. Portfolio Selection [4]: In portfolio selection, let R represent the (uncertain) returns of a suite of financial assets, and x the weighting one would like to put on the various assets. Here  X &gt;  X  x T R represents an upper bound on the loss one might suffer with weighting x . The goal is to minimize an upper bound on the loss that holds with high probability, 8 say 1  X   X  , specified by the user This criterion has been studied by El Ghaoui et al. [4] in the worst case setting. Previous work has not addressed the case when additional symmetry or linear unimodal information is available. However, comparing the minimal value of  X  in Proposition 1, we see that such additional information, such as symmetry or unimodality, indeed decreases our potential loss, as shown clearly in Figure 1. This makes sense, since the more one knows about uncertain returns the less risk one should have to bear. Note also that when incorporating additional information, the optimal portfolio, represented by x , is changed as well but remains mean-variance efficient when  X   X  (0 , 1 2 ) .
 Uncertain MDPs with reward uncertainty: The standard planning problem in Markov decision processes (MDPs) is to find a policy such that maximizes the expected total discounted return. This nonlinear optimization problem can be efficiently solved by dynamic programming, provided that the model parameters (transition kernel and reward function) are exactly known. Unfortunately, this is rarely the case in practice. Delage and Mannor [8] extend this problem to the uncertain case by employing the value-at-risk type constraint (2) and assuming the unknown reward model and transi-tion kernel are drawn from a known distribution (Gaussian and Dirichlet respectively).Unfortunately, [8] also proves that the constraint (2) is generally NP-hard to satisfy unless one assumes some very restricted form of distribution, such as Gaussian. Alternatively, note that one can use the worst case value-at-risk formulation (3) to obtain a tractable approximation to (2) where R is the reward function (unknown but assumed to belong to (  X , ) ) and x represents a discounted-stationary state-action visitation distribution (which can be used to recover an optimal behavior policy). Although this worst case formulation (15) might appear to be conservative com-pared to working with a known distribution on R and using (2), when additional information about the distribution is available, such as symmetry or unimodality, (15) can be brought very close to us-ing a Gaussian distribution, as shown in Figure 1. Thus, given reasonable constraints, the minimax approach does not have to be overly conservative, while providing robustness and tractability. Finally, we investigate the more refined conditional value-at-risk (CVaR) criterion that bounds the conditional expectation of losses beyond the value-at-risk (VaR). This criterion has been of growing prominence in many areas recently. Consider the following quantity defined as the mean of a tail distribution: ^ f = E Here,  X   X  is the value-at-risk and ^ f is the conditional value-at-risk of R . It is well-known that the CVaR, ^ f , is always an upper bound on the VaR,  X   X  . Although it might appear that dealing with the CVaR criterion entails greater complexity than the VaR, since VaR is directly involved in the definition of CVaR, it turns out that CVaR can be more directly expressed as where ( x ) + = max(0 ,x ) [14]. Unlike the VaR constraint (2), (17) is always (jointly) convex in x and  X  . Thus if R were discrete, ^ f could be easily computed by a linear program [14; 5]. However, the expectation in (17) involves a high dimensional integral in general, whose analytical solution is not always available, thus ^ f is still hard to compute in practice. Although one potential remedy might be to use Monte Carlo techniques to approximate the expectation, we instead take a robust approach: As before, suppose one knew the distribution of R belonged to a certain family, such as (  X , ) . Given such knowledge, it is natural to consider the worst-case CVaR f = sup where the interchangeability of the min and sup operators follows from the classic minimax theorem [15]. Importantly, as in the previous section, we can determine the worst-case CVaR for various distribution families. If one has additional information about the underlying distribution, such as symmetry or unimodality, the worst-case CVaR can be reduced. These can be used to provide a tractable bound on the CVaR even when the distribution is known.
 Proposition 2 For alternative distribution families, the worst-case CVaR is given by: if R  X  (  X , ) SU then if R  X  X  (  X , ) then f =  X   X  x + where  X  x = x T  X , X  x = The results of Proposition 2 are a novel contribution of this paper, with the exception of (22), which is a standard result in stochastic programming [7].
 Proof: We know from Corollary 1 that which reduces the problem to the univariate case. To proceed, we will need to make use of the univariate results given in Proposition 3 below. Assuming Proposition 3 for now, we show how to prove (19): In this case, substitute (23) into (18) and apply (24) from Proposition 3 below to obtain This is a convex univariate optimization problem in  X  . Taking the derivative with respect to  X  and setting to zero gives  X  =  X   X  x + (2  X   X  1) A similar strategy can be used to prove (20), (21), and (22).
 As with Proposition 1, Proposition 2 illustrates the benefit of prior knowledge. Figure 1 (right) compares the coefficients on  X  x among different worst-case CVaR quantities for different families. Comparing VaR and CVaR in Figure 1 shows that unimodality has less impact on improving CVaR. A key component of Proposition 2 is its reliance on the following important univariate results. The following proposition gives tight bounds of the expected survival function for the various families. Proposition 3 For alternative distribution families, the expected univariate survival functions are: Here (26) is a further novel contribution of this paper. Proofs of (24) and (25) can be found in [1]. Interestingly, to the best of our knowledge, the worst-case CVaR criterion has not yet been applied to any of the four problems mentioned in the previous section 9 . Given the space constraints, we can only discuss the direct application of worst-case CVaR to the portfolio selection problem. We note that CVaR has been recently applied to  X  -SVM learning in [16].
 Implications for Portfolio Selection: By comparing Propositions 1 and 2, the first interesting con-clusion one can reach about portfolio selection is that, without considering any additional informa-tion, the worst-case CVaR criterion yields the same optimal portfolio weighting x as the worst-case VaR criterion (recall that VaR minimizes  X  in Proposition 1 by adjusting x while CVaR minimizes f by adjusting x in Proposition 2). However, the worst-case distributions for the two approaches are not the same, which can be seen from the relation (16) between VaR and CVaR and observing that  X  in (4) is not the same as in (19). Next, when additional symmetry information is taken into account and  X   X  (0 , 1 2 ) , CVaR and VaR again select the same portfolio but under different worst-case distri-butions. When unimodality is added, the CVaR criterion finally begins to select different portfolios than VaR. We have provided a simpler yet broader proof of the general linear projection property for distribu-tion families with given mean and covariance. The proof strategy can be easily extended to more restricted distribution families. A direct implication of our results is that worst-case analyses of mul-tivariate expectations can often be reduced to those of univariate ones. By combining this trick with classic univariate inequalities, we were able to provide worst-case analyses of two widely adopted constraints (based on value-at-risk criteria). Our analysis recovers some existing results in a simpler way while also provides new insights on incorporating additional information.
 Above, we assumed the first and second moments of the underlying distribution were precisely known, which of course is questionable in practice. Fortunately, there are standard techniques for handling such additional uncertainty. One strategy, proposed in [2], is to construct a (bounded and convex) uncertainty set U over (  X , ) , and then applying a similar minimax formulation but with respect to (  X , )  X  U . As shown in [2], appropriately chosen uncertainty sets amount to adding straightforward regularizations to the original problem. A second approach is simply to lower one X  X  confidence of the constraints and rely on the fact that the moment estimates are close to their true values within some additional confidence bound [17]. That is, instead of enforcing the constraint (3) or (18) surely , one can instead plug-in the estimated moments and argue that constraints will be satisfied within some diminished probability. For an application of this strategy in CCLP, see [3]. We gratefully acknowledge support from the Alberta Ingenuity Centre for Machine Learning, the Alberta Ingenuity Fund, iCORE and NSERC. Csaba Szepesv ` ari is on leave from MTA SZTAKI, Bp. Hungary. [1] R. Jagannathan.  X  X inimax procedure for a class of linear programs under uncertainty X . Oper-[2] Gert R.G. Lanckriet, Laurent El Ghaoui, Chiranjib Bhattacharyya and Michael I. Jordan.  X  X  [3] G.C.Calafiore and Laurent El Ghaoui.  X  X n distributionally robust chance-constrained linear [4] Laurent El Ghaoui, Maksim Oks and Francois Oustry.  X  X orst-case value-at-risk and robust [5] Shu-Shang Zhu and Masao Fukushima.  X  X orst-case conditional value-at-risk with application [6] Ioana Popescu.  X  X obust mean-covariance solutions for stochastic optimization X . Operations [7] Andr  X  as Pr  X  ekopa. Stochastic Programming . Springer, 1995. [8] Erick Delage and Shie Mannor.  X  X ercentile optimization for Markov decision processes with [9] Li Chen, Simai He and Shuzhong Zhang.  X  X ight Bounds for Some Risk Measures, with Ap-[10] Sudhakar Dharmadhikari and Kumar Joag-Dev. Unimodality, Convexity, and Applications . [11] Dimitris Bertsimas and Ioana Popescu.  X  X ptimal inequalities in probability theory a convex [12] Albert W. Marshall and Ingram Olkin.  X  X ultivariate Chebyshev inequalities X . Annals of Math-[13] Ioana Popescu.  X  X  semidefinite programming approach to optimal moment bounds for convex [14] R. Tyrrell Rockafellar and Stanislav Uryasev.  X  X ptimization of conditional value-at-risk X . [15] Ky Fan.  X  X inimax Theorems X . Proceedings of the National Academy of Sciences , [16] Akiko Takeda and Masashi Sugiyama.  X   X  -support vector machine as conditional value-at-risk [17] John Shawe-Taylor and Nello Cristianini.  X  X stimating the moments of a random vector with
