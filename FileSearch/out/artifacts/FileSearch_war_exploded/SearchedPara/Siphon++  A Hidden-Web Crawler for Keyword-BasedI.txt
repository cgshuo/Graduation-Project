 The hidden Web consists of data that is generally hidden behind form interfaces, and as such, it is out of reach for traditional search engines. With the goal of leveraging the high-quality information in this largely unexplored portion of the Web, in this paper, we pro-pose a new strategy for automatically retrieving data hidden behind keyword-based form interfaces. U nlike previous approaches to this problem, our strategy adapts the query generation and selection by detecting features of the index. We describe a preliminary experi-mental evaluation which shows that our strategy is able to to obtain coverages that are higher than those of previous approaches that use a fixed strategy for query generation.
 Categories and Subject Descriptors: H.4 [Information Systems Applications]: Information Search and Retrieval General Terms: Algorithms, Design.
 Keywords: Online databases, Hidden-Web crawler
The hidden Web has had an explosive growth as an increasing number of databases and document collections are made available online. It is estimated that there are several million hidden-Web sites. Although the hidden Web represents a substantial portion of the Web, it has been largely unexplored. The main reason being that hidden-Web data is both hard to find and access. These data are not indexed by existing search engines, which are limited to crawl pages that have a well-defined URL. Thus, to access a hidden-Web source, users (and applications) first need to know where to find it, and then fill out a form in order to access the content.
In this poster, we present a crawler for automatically retriev-ing Web content hidden behind keyword-based interfaces. Being able to retrieve and index this content has the potential to uncover hidden information and help users find useful information that is currently out-of-reach for search engines. Unlike multi-attribute forms, keyword-based interfaces are simple to query, since they do not require detailed knowledge of the schema or structure of the underlying data. It is thus possible to create automatic and effec-tive solutions to crawl these interfaces [1, 2]. We propose a new crawling strategy which adapts to the features of the indexes un-derlying the search interface. We show that this adaptation leads to improved coverage: our crawler retrieves more hidden content than previous approaches.
Figure 1 shows the architecture of Siphon++, the proposed adap-tive crawler. Siphon++ is composed of an adaptive component , words from the entry page of the the Web site that contains the database is obtained and issued to the interface. Once a word from this set returns a valid result page containing links to target doc-uments, the HC populates the sample with the top-k target docu-ments, i.e., the k documents considered as most relevant for this word. We select the first keyword from the entry page because it is very likely that words in this page are presented in the database, assuming both (database entry page and database) are on the same topic. In the next step, the HC randomly selects a word from this sample and submits it. The top-k target documents are again added to the sample. This iterative process goes on, randomly probing selected words from the sample and adding their result pages to the sample, until the sample converges . These queries that contribute to build the sample are called sampling queries (see Figure 1).
The Crawling phase is responsible for (1) issuing queries to the database; (2) retrieving the result pages and extracting from them the links to the target documents; and (3) downloading the docu-ments from the database.

The first step in this phase is to select the most promising terms from the sample to submit, i.e., the ones that are likely to result in high coverage. We call them crawling terms (Figure 1) and we store them in a crawling term list . This is an ordered list of words, such that first words will be submitted first. In the final step of the crawling phase, the crawler downloads the target documents, whose links were obtained from the result pages, and stores them as illustrated in Figure 1.
In this section, we report the results of an experimental evalua-tion of Siphon++ . Our goal is to verify the effectiveness (coverage) of our method.
 Dataset. We evaluated our crawler over a simulated search en-gine. To populate the simulated search engine, we used the TREC 2001 Web Track data (WT10g data set). We created an index over 306,210 pages using Lucene. 1 In the remainder of this section, we refer to this collection as WT10g server.
 Index Configurations. To verify how our crawling strategy is af-fected by different indexing features commonly adopted in hidden-Web sources, we built two distinct indexes over the WT10g server varying the indexing features: Stemming and Stopword Removal ( StemStopRemoval ) :thewords are stemmed and stopwords removed. This is the scenario whereby the index is more compact; No Stemming and Stopword Removal ( NoStemStop ): the words are not stemmed and stopwords are not removed. This represents a scenario where the index is not compacted.
 Crawling Strategies. We evaluate following crawling strategies: Baseline: This corresponds to the strategy proposed by Ntoulas et al. [2]. Their crawler selects the next query to issue based on the previously crawled pages. In contrast to our approach, it neither builds a database sample nor takes advantage of query interfaces that support disjunctive queries; Siphoning: This configuration corresponds to our previously pro-posed crawling strategy, the Hidden-Web Siphon [1]. It detects stopwords but it does not detect stemming in the index; Siphon++ : This configuration represents our proposed method.
Figure 2 shows the coverage results for the different crawling strategies for a compacted index (i.e., StemStopRemoval ). Siphon++ obtains a higher coverage value for the early iterations. For exam-ple, at 20 terms, Siphon++ reaches over 90% coverage, whereas Baseline is at around 70%. This indicates that adapting to the index features leads to a more efficient crawl. As depicted in Figure 3, http://lucene.apache.org/java/docs/index.html
