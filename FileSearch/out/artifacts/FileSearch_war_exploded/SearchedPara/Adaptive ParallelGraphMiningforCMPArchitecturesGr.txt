
Mining graph data is an incr easingly popular challeng e, whic h has practical applications in many areas, including molecular substructur e disco very , web link analysis, fraud detection, and social network analysis. The problem state-ment is to enumer ate all subgr aphs occurring in at least graphs of a database , wher e is a user specied par ame-ter. Chip Multipr ocessor s (CMPs) provide true par allel pro-cessing , and are expected to become the de facto standar d for commodity computing . In this work, building on the state-of-the-art, we propose an efcient appr oac h to par -allelize suc h algorithms for CMPs. We show that an algo-rithm whic h adapts its behavior based on the runtime state of the system can impr ove system utilization and lower ex-ecution times. Most notably , we incorpor ate dynamic state mana gement to allow memory consumption to vary based on availability . We evaluate our tec hniques on curr ent day shar ed memory systems (SMPs) and expect similar perfor -mance for CMPs. We demonstr ate excellent speedup, 27-fold on 32 processor s for sever al real world datasets. Addi-tionally , we show our dynamic tec hniques affor d this scal-ability while consuming up to 35% less memory than static tec hniques.
Computer architectures are experiencing a revolution in design. Major modications such as Chip Multiprocessing (CMP) and 64-bit computing afford signicant adv ance-ments in processing power , and also pro vide true MIMD (multiple instruction multiple data) parallel computing for desktop PCs. CMP designs (also kno wn as multicore ar-chitectures) incorporate more than one processing element on the same die. The y can execute multiple threads con-currently because each processing element has its own con-text and functional units. CMPs are designed to increase the computational throughput of a system without further increasing the frequenc y-memory performance gap, which has been gro wing for some time.

It is commonly accepted that CMP architectures will quickly become the de facto standard for commodity PCs. In addition, it is clear that the number of cores on a chip will rise signicantly over the next several years. In 2005, Intel released a two core chipset named the PentiumD R . This year , Sun Microsystems has released the Nia gar a R , an eight core CPU capable of running 32 concurrent threads. For these general-purpose MIMD CMPs, parallelization challenges share man y similarities with traditional single-core, multiple-processor machines (Symmetric Multipro-cessing machines, or simply SMPs). Ho we ver, with CMP architectures the cores not only share all off-chip resources, but man y on-chip resources, such as cache. This sharing of resources mak es efcient use of the afforded CPU cy-cles quite challenging. Therefore, it is par amount that al-gorithm designer s resear ch effective str ate gies to incorpo-rate task par allelism and efcient memory system usa ge to ensur e that application performance is commensur ate with suc h architectur al advancements .
 Frequent pattern mining was rst introduced by Agra wal, Imielinski, and Sw ami [1] in 1994, and has been studied extensi vely [7, 9, 12, 18, 20]. Recently the prob-lem statement has been generalized from transaction data to graph data. Finding frequent patterns in graph databases such as chemical and biological data [14 ], XML documents [17 ], web link data [16 ], nancial transaction data, social netw ork data, and other areas has posed an important chal-lenge to the data mining community . Representing these data sets in graphical form can capture relationships among entities which are lost when the data is represented as trans-actions. For example, in molecular data sets, graph nodes correspond to atoms, and edges represent chemical bonds. Frequent substructures may pro vide insight into the be-havior of the molecule, or pro vide a direction for further investigation[8 ].

A major challenge in substructure mining is that the search space is exponential with respect to the data set, forc-ing runtimes to be quite long. A simple chemical data set of 300 molecules can require man y hours to mine when the user species a low support threshold. One reason for the inherent dif culty is that identifying the embedding of one graph in another ( subgr aph isomorphism ) is computation-ally expensi ve. As such, leveraging architectural adv ance-ments is paramount to solving these problems efciently . The problem of excessi ve runtimes is compounded by the fact that data mining is an interacti ve and iterati ve process, requiring the user to query the data set man y times to obtain end results. Without redesigning graph mining algorithms to lever age the task level par allelism abilities of CMP pro-cessor s, graph mining runtimes will be up to 27 times long er than necessary .

In this work, we present a parallel graph mining al-gorithm designed to adapt its runtime beha vior based on system-le vel feedback, so as to use available resources as efciently as possible. It is based on the serial algorithm gSpan . We specically tar get CMPs, in an effort to solv e the challenges outlined abo ve. Most notably , we address the need to efciently utilize the memory subsystem, and effec-tively balance the load in the system. The contrib utions of this paper are
A graph is a set of vertices V and a set of edges E ( V V ) , where E is unrestricted (graphs can have cy-cles). Nodes and edges may have labels, and may be di-rected or undirected. Let us dene the labeling function l : ( E; V ) ! L , where all elements of E and V are mapped to exactly one label. A graph g 0 is a subgraph of g if there is a bijection from the edges E 0 E with matching la-bels for nodes and edges. This mapping is also called an embedding of g 0 in g . If both g 0 and g are subgraphs of each other , the y are considered isomorphic. A subgraph g 0 is frequent if it there is an embedding in at least graphs in the database, where is a user -dened threshold, or #( g j g 2 G j g 0 g ) . The problem is then to enumerate all frequent graphs g 0 in D. Note that this prob-lem denition has only one deterministic solution set. There is no 'goodness' measure to a frequent subgraph g 0 ; it either is a subgraph of at least database graphs, or it is not.
Adv antages for representing data as graphs has been widely studied [4, 5, 9]. Wang and Parthasarathy [18 ] de-veloped a parallel algorithm for their Motif Miner toolkit. Motif Miner searches for interesting substructures in noisy geometric graphs tar geted at lar ge biochemical molecules, such as proteins. Cook et al [4], have been developing a sys-tem called Subdue for several years. Their research has pro-duced a parallel implementation, which solv es a some what dif ferent problem, using approximations and user interac-tion to return potentially interesting substructures again on geometric graphs. Meinl et al [11 ], parallelized the serial frequent graph enumerator MoF a . Their algorithm ( Par-Mol ) achie ves a 7-fold speedup on 12 processors. ParMol' s single processor execution times are slo wer than what we have presented in this work, and it also uses more memory .
There have been several approaches for parallel mining of other frequent patterns [15 ]. Zaki [21 ], proposed parallel partitioning schemes for sequence mining, also illustrating the benets of dynamic load balancing in shared memory environments. Ho we ver, one dif ference is that the dispari-ties between task execution times are far easier to compute in sequence mining than for graph mining. Guralnik and Karypis had similar ndings [6]. Our own efforts in se-quence and itemset mining have sho wn us that static load balancing models beha ve poorly in pattern mining because the time to mine an equi valence class is highly variable [13 ].
Several serial algorithms exist that enumerate frequent subgraphs. FSG [9] mines for all frequent subgraphs using an Apriori breadth-rst fashion. gSpan , developed by Yan and Han [20 ], affords a signicant computational impro ve-ment. Another recent serial graph miner is GastonEL [12 ], developed by Nijssen and Kok. GastonEL incorporates em-bedding lists using an efcient pointer -based structure. This pro vides an impro vement in execution time at the cost of signicantly increased memory consumption. All frequent graph miners output the same result set for a given data set and support. To select a baseline algorithm, we evaluated ve efcient frequent graph miners, presented in Table 1.
GastonEL displayed the lowest execution times on av-erage, with gSpan second. For data sets with result sets containing a lar ge percentage of trees, GastonEL was faster than gSpan . This can be attrib uted to its efcient canonical labeling for trees. When the frequent result set was dom-inated by cyclic graphs, GastonEL and gSpan had about the same execution times. Ho we ver, GastonEL consumes a lar ge amount of memory . Further , GastonEL 's use of embedding lists hinders parallelization because child tasks share data structures. gSpan consumes very little memory because it does not use embedding lists. The other three al-gorithms we evaluated did not pro vide an impro vement in either memory consumption or execution time when com-pared to these two algorithms. A study by W  X  orlein et al [19 ], generally had similar ndings. Note that all ve algo-rithms are designed to output the same result set. In light of our rankings, we chose gSpan as a baseline for our par -allel algorithm. We also parallelized GastonEL as part of this study , using source code pro vided by the original au-thors [12 ]. The resulting scalability was lower than what we present in Section 5 due to the static embedding lists.
As our optimizations are based on gSpan , we now de-scribe the algorithm in detail. Given a data set D , all edges e which occur in at least graphs ( g 2 D ) are used as seeds to partition the search space. These are called frequent-1 edg es. Each frequent-1 edge is gro wn with all candidate edges. A candidate edge is any other frequent edge, which either gro ws the graph by adding an additional node, or merely creates a new cycle by adding just an edge. This simple process is then recursed upon, until the cur -rently gro wn graph has no frequent candidates for gro wing. gSpan only gro ws graphs with edges that result in subgraphs kno wn to be in the data set, thus reducing unnecessary work. gSpan represents a graph with a labeling system called DF-Scodes. A DFScode for a graph is generated by outputting newly disco vered nodes during a depth rst tra versal of a graph. For each edge tak en, a ve-tuple is added to the DFScode, namely (sr cNode , destNode ,srcLabel, edg eLabel, destLabel) . A graph may have man y valid DFScodes. gSpan restricts gro wth to the right-most path of the DFS tree. In addition, a cycle-closing renement (a back edge) can only be gro wn from the right-most node. While these restrictions greatly reduce the possibility of redun-dant graph mining, it is not completely avoided. Therefore, gSpan veries that each subsequent DFScode is the canon-ical code for the graph it represents prior to mining it, thus avoiding duplicate work. If a valid DFScode for the graph can be produced which is smaller (le xicographically) than the DFScode in question, the check fails. gSpan redisco v-ers the entire new child graph for each recursi ve mining call. For example, if graph (A-B-C) was found in data set graphs 1, 2 and 3, and there is suf cient support for child candidate (-D) , gSpan will redisco ver the parent (A-B-C) when min-ing (A-B-C-D) . In other words, gSpan does not maintain the mappings between recursi ve calls.

Pseudo code for gSpan is pro vided in Algorithms 1:gSpan and 2:SubMine . In the rst algorithm, lines 1-4 remo ve infrequent edges from the graph, where an edge is a 3-tuple (sr cLabel,edg eLabel,destLabel) . Then, in lines 5-7 each frequent-1 item is passed to 2:SubMine . In 2:Sub-Mine , the DFScode is evaluated to verify it is the canonical label for the graph it represents. For example, suppose we have graph A-B-C . There are four possible DFScodes for this graph, although only the DFS walk which starts with A will be the canonical representation. Ne xt, the data set is scanned to nd all embeddings of the graph. Once these embeddings are found, the y are tra versed to determine the list of possible child extensions. Each extension which oc-curs at least the minimum number of times is recursed upon. Further details of this algorithm can be found else where [20 ].
To achie ve high scalability in a parallel graph mining algorithm, one must overcome several fundamental obsta-cles. The two most challenging of these are load imbalance and efcient utilization of the memory hierarchy .W e outline these issues belo w.
A fundamental challenge in parallel graph mining is to maintain a balanced load on the system, such that each processing element is working at full capacity for the duration of the overall execution. This challenge is exacerbated in graph mining because the time to mine a task is not kno wn a priori .
 Task Partitioning . The basis for effecti ve load bal-ancing is a task partitioning mechanism possessing suf cient granularity so as to allo w each processing ele-ment to continue to perform useful work until the mining process is complete. In itemset mining, for lar ge databases frequent-1 items generally suf ce. This equates to the rst loop in Algorithm 1:gSpan , Line 5. Ho we ver, for graphs this is not the case. In graph mining, a single frequent-1 item may contain 50% or more of the total execution cost. This is because task length is lar gely dependent on the associati vity in the dataset. We studied man y real and synthetic data sets during the course of this work. Real data sets always sho wed more imbalance than synthetic data sets. We illustrate this imbalance with a depiction of the search space in Figure 1. Each circle represents a frequent subgraph to be mined (frequent-1 items colored gray), and each independent equi valence class has been box ed in with dashed lines.
 Task Allocation. The challenge in task allocation is to devise a strate gy that works in conjunction with a partitioning algorithm to eliminate processor idle time. Extant strate gies for graph mining [11 ] result in signicant idle time, which obviously lowers scalability . Dynamic allocation allo ws a task to be assigned to a processing element at runtime, allo wing idle processors to accept additional load. Under static allocation, tasks are assigned to processors a priori . Static techniques invariably suf fer a high performance penalty because of their load imbalance.
Several important issues with CMP systems arise when discussing the memory hierarchy . CMP systems typically have far less memory and cache than other parallel architec-tures such as message-passing clusters or shared memory multiprocessors. As such, algorithms which process lar ge data sets in parallel on these systems must tak e great care to minimize the use of main memory . In addition to simply ex-ceeding main memory , excessi ve memory use can also lead to bus contention. This is exacerbated with CMPs, since processing elements share memory bandwidth. We studied memory traf c on an 16-w ay SMP , and found that when in-creasing from 1 to 16 processors, bus utilization rose from 0.2% to 25%.

In addition, shared meta structures such as embedding lists can also degrade memory system performance. An em-bedding list (abbre viated EL) is a list of mappings between a potentially frequent object and its locations in the database. Embedding lists have been sho wn to pro vide a performance impro vement [12 ] in serial graph mining implementations because the y allo w for fast disco very of child graph candi-dates. Ho we ver, the dependence on the structure is costly in a parallel setting, because all child graphs of the same parent must be mined before the parent' s state can be freed. In addition, the y consume signicant memory , limiting the size of the problem which can be solv ed. An efcient algo-rithm must explore and optimize the trade off between no embeddings and full embeddings.

In graph mining data set accesses often lack temporal lo-cality because the projected data set is typically quite lar ge. These accesses also lack good spatial locality because most of the data structures emplo yed are pointer -based. Locality issues are magnied on CMP systems because these archi-tectures often have small caches. This is predicated by the real estate constraints of the chip, since for a x ed chip size each additional core will use silicon pre viously dedicated to cache. We performed a simple working set study to evaluate the benets of impro ved dataset graph accesses. We com-pared two strate gies for parallel graph mining. First, we forced threads to always mine their own child tasks. Sec-ond, we allo wed threads to mine other threads' tasks; that is, task stealing. The cache miss rates for the rst strate gy were, on average, three times lower than the second strat-egy. The reason is that when a task is stolen, typically most of the needed data set graphs are not in cache. As such, algorithm designer s should explor e and optimize the trade-offs between impr oved tempor al locality and proper load balancing .
We present our optimizations to impro ve graph mining performance on CMP architectures. Our goal is to minimize execution times pro vided a x ed number of computational units and an allotted memory budget.
To efciently parallelize the workload, a queuing model is required. We evaluated several models, including global, hierarchical, and distrib uted queuing. Distrib uted queuing was superior under a variety of supports and data sets. In this model, there are exactly as man y queues as processors. Each queue is unlimited in capacity , has a mute x, and is assigned to a particular processor . The def ault beha vior for a processor is to enqueue and dequeue using its own queue. If a processor' s queue is empty , it searches other queues in a round robin fashion, looking for work. If all queues are empty , it sleeps. Although this incurs a locking cost, there is generally no contention.
 Our pseudo code starts with Algorithm 3:P arallelMine . It generally resembles the code from Algorithm 1:gSpan , with the addition of lines 8-12 which insert frequent-1 edges into the queues. Although each frequent-1 item rep-resents a separate task, these tasks are further partitioned as necessary . We consider two methods to control the granularity of these tasks, described belo w.
 Adapti ve Partitioning . Ev ery call to Algorithm 4:PSub-Mine generates a number of child candidates from which to extend the currently mined graph. Each candidate is then recursed upon (see Line 16). With adaptive partitioning [3], each processor mak es a decision at runtime for each frequent child extension. A child is an edge tuple (sr cN-ode ,destNode ,srcLabel,edg eLabel,destLab el) which can be added to the currently mined graph to create a new frequent subgraph. The child task may be mined by the creating processor , or enqueued, as seen in our 4:PSubMine code on lines 16 and 17. Line 15 evaluates the boolean canMine , the rst of our three runtime hooks. The other two, canT ile and canELMine will be discussed in Section 4.2. These hooks allo w the algorithm to adapt to the properties of the system at runtime.

The value canMine is based on the current load of the system. We do not require a specic mechanism to mak e this decision, because in part the decision is based on the queuing mechanism used. In our experiments, we evaluate the size of the local thread' s queue, using a threshold of 10% of the total number of frequent-1 edges. Another option could be to set a threshold for the smallest current size of any queue.

At each step in the depth rst mining process, each sub-task can be enqueued into the system for any other proces-sor to mine. Thus the granularity of a task can be as small as a single call to 4:PSubMine , which is rather ne-grained. Adapti ve partitioning is depicted in Figure 2 (left). In this example, task (A-B) had two children, namely (A-B-C) and (A-B-E). Tasks which were enqueued are shaded gray . A circle has been dra wn around a task which was allo wed to gro w dynamically . Task (A-B-E) was enqueued, while task (A-B-C) was mined by the parent process. After mining (A-B-C), only one child was created, which was kept by the parent. The subsequent two children were both mined by the parent, because the queues had suf cient work so as to allo w it.

An alternati ve to adapti ve partitioning is levelwise par -titioning. We evaluate levelwise partitioning to pro vide a comparison to our approach. Le velwise partitioning deter -ministically enqueues tasks to a parameterized depth in the recursion tree. These tasks are then mined to completion by the dequeuing processor . Le vel-n partitioning implies level-(n-1) partitioning. If the work is balanced in the sys-tem, having the creator of a task mine the child will always be more efcient than enqueing the task, due to the benets of afnity [10 ]. The child is always located in a subset of the graphs of the parent, so when those graphs are scanned, the cache will benet from temporal locality . Figure 2 (right) il-lustrates level-wise partitioning, set to level-2. Again, tasks which were enqueued are shaded gray 1 .
To impro ve the efcienc y of the member subsystem, we increase temporal locality in the original 2:SubMine routine by tiling accesses to the projected data set. In addition, we incorporate embedding lists. For both optimizations, we manage their usage based on the amount of available main memory and current load balance. We term these optimizations adaptive state mana gement . Adapti ve Graph Tiling . A frequent subgraph typi-cally has multiple candidate extensions which must be mined. Each candidate is the result of extending the cur -rently mined graph with an additional edge. The resulting graph exists in a subset of the database graphs embedding its parent graph. To impro ve temporal locality , we can package multiple child extensions for the same parent graph into one lar ger task, as long as load balancing is not adv ersely affected. Packaging the child tasks together allows us to perform loop inter chang e [2], impr oving tempor al locality . Each data set graph is then treated as a single tile.

We perform the packaging only when the total number of enqueued tasks is suf cient to support it. Our threshold is a system load of at least 20% of the number of frequent-1 edges, and a current memory usage of less than 60% of our allotted memory budget. This decision introduces our sec-ond runtime hook, canT ile . It appears on Line 5 in 4:PSub-Mine . If canT ile returns true, the entire child extension list generated on Line 4 is sent to Algorithm 6:SubMineA Tile .
Algorithm 6:SubMineA Tile proceeds as follo ws. First, we initialize an array of data members, one for each child. Ne xt, we add the child to the parent graph (the subgraph we are currently mining for), and evaluate its label. If it is canonical, we add it to the result set and mark it. We use the syntax Code[c] to represent the DFScode of the current child extension. We then enumerate all the chil-dren concurrently in Algorithm 7:Enumer ateA Gr oup . The loop interchange is performed on Lines 1 and 2 of this code. Adapti ve State Management. It is common that al-gorithms can trade increased memory usage for impro ved execution times. We seek to lever age this principle to impr ove runtimes for graph mining applications by main-taining embedding lists when it is ine xpensive to do so . Essentially , if we can guarantee the miner of a task will also mine its children, and there is suf cient memory , our algorithm will use embedding lists to mine those children. gSpan does not maintain embeddings between recursi ve calls. Therefore, we illustrate our proposed embedding structure in Figure 3. At the top of the gure we sho w a data set of four graphs. The node labels V1..V17 are merely to allo w us to reference a particular node with a pointer in the diagram, the y do not exist in practice. Belo w these graphs are three dashed box es, representing the embedding lists for graph (A) , graph (A-B) , and graph (A-B-C) . Again, we use labels S1..S15 only for reference purposes. GID represents the graphID of the embedding. NP is a node pointer to the associated node in the graph. RMP is the right-most path of the graph we are gro wing. Although the right-most path can be determined strictly from the currently mined DFScode, we maintain it for efcienc y. SP is the pointer to the pre vious graph' s embedding list. For example, graph (A-B) is found in dataset Graph 2, and appears at S7 . When the graph is gro wn with child C to create subgraph (A-B-C) , the embedding S12 is extended to S7 . This embedding structure is similar to that used by GastonEL .

By maintaining the embedding lists between recursi ve mining calls, the algorithm need not redisco ver the entire parent graph in each data set graph. The last runtime hook, canELMine , appears in Algorithm 7:Enumer ateA Gr oup on Line 6. If there is suf cient memory available in the sys-tem, then the newly disco vered mapping for the child is ap-pended to the embedding list for future use. Ho we ver, when a task is enqueued, we do not include the embedding lists. This allo ws us to free its associated memory . We implement our algorithm in C using POSIX threads. Because real world CMP architectures are still in their in-fanc y, we evaluate our strate gies on an SMP system and constrain the memory consumption (in code), in an effort to create an environment capable of evaluating our optimiza-tions. All experiments are run on an SGI Altix 3000 with 64 GB of memory , and 32 1.3GHz Itanium 2 processors. Scalability numbers do not include the time to read in the data set, which is typically a small fraction of the total run-ning time, and was not parallelized. In all cases, the graph database ts within core memory . The baseline for scala-bility is a single-threaded execution of our implementation without using embedding lists or tiling. We note that this implementation compares favorably to the authors' binary [20 ] (we are typically 15-50% faster), which the y kindly pro vided. We use counters to maintain size of the mem-ory footprint , which we periodically verify by reading from /pr oc/[pid]/status les. The footprint size is required for our runtime state management.

We emplo y several real world data sets, sho wn in Table 2. PTE1 is a data set of molecules classied as carcinogens by the Predicti ve-T oxicology Ev aluation project 2 . We also emplo y the complete NCI database 3 . Weblogs is a data set of web sessions, generated from web serv er request and re-sponse logs [16 ]. D100k is a synthetic data set made from the PAFI toolkit 4 .
To measure overall scalability , we ran our algorithm on three of the data sets from Table 2. Speedups are ratios of the execution time for our serial implementation divided by our parallel implementation. All trials use the full data set. As can be seen in Figure 4, our algorithm scales nicely as we increase the computational throughput of the system. These graphs include a linear speedup curv e for reference. On 32 processors, the scalability ranges from 27 to 22.5 over the three data sets. Weblogs has the lowest scalability at 22.5. This can be attrib uted to its increased memory traf-c. The frequent graphs in the Weblogs data set are lar ger than those in the other data sets, which requires the algo-rithm to tra verse a lar ger percentage of each of the database graphs, deeper into the recursion. We note that scalability is maintained as support is reduced because there are more tasks, and those tasks can be mined by the parent node.
To evaluate our task partitioning strate gies, we emplo y data sets Weblogs, PTE, and D100k. D100k is used as a comparison to sho w that levelwise partitioning performs reasonably well with synthetic data (from PAFI).From the graphs in Figure 5, we see that adapti ve partitioning outper -forms levelwise partitioning.

For the Weblogs data set (Figure 5, left) adapti ve parti-tioning affords a 22.5-fold speedup, where levelwise parti-tioning affords at most a 4-fold speedup. As the number of partition levels increases from one to ve, scalability gen-erally increases, but it does not approach adapti ve partition-ing. Adapti ve partitioning pro vides two key benets. First, it pro vides an impro ved load balance in the system. Second, it affords the benet of impro ved temporal locality , since it is more lik ely that a parent is mining its own children tasks.
Figure 5 (right) presents scalability for the D100k data set. Clearly levelwise partitioning has lessened the gap in speedup, but it still is only 15-fold vs 24-fold for adapti ve partitioning. The impro vement in levelwise partitioning is due to the balanced nature of the synthetic data set.
We evaluated the benets of using adapti ve state man-agement by mining both the Weblogs and the NCI data sets. We evaluate three dif ferent execution beha viors, namely never using embedding lists or tiling, always using embed-ding lists (to a static depth in the search space), and dy-namically using embedding lists and tiling. The results are presented in Figure 6 (left and middle). We maintained a constant support value (as seen in the graph X axes). From these gures, we can observ e several trends.

First, adapti ve state management results in impro ved per -formance over both static methods (either using or not using embedding lists). For example, adapti ve state management requires 38 seconds to mine Weblogs with one core, almost twice as fast as not emplo ying state management. On 32 cores, it requires only 3 seconds, whereas statically using embedding lists requires 6 seconds.

Ne ver using embedding lists suf fers when the number of processors is 4 or less, and always using embedding lists suf fers when the number of cores is greater than 8. For example, with the NCI data set, adapti ve embedding lists are twice as fast as when not using embedding lists on one processor . On 32 processor s, adaptive state is thr ee times faster than statically using embedding lists, and yet con-sumes 35% less memory . The algorithm must incorporate runtime performance characteristics and use embeddings only when the current load balance and available memory support it.

Secondly , using embedding lists in a static fashion re-sults in the lar gest memory consumption. The scalability of this technique atlines because the task granularity with levelwise partitioning does not satisfy load balancing re-quirements when the number of processors is high. Due to memory constraints, we must use levelwise partitioning when statically using embedding lists. We mention that dynamically using tiling alone pro vided only a modest im-pro vement. Using only one processing element, a 15% im-pro vement was observ ed. Ho we ver, this degraded quickly as the number of processing elements increased. The reason is that the packaging required for tiling lowers task granu-larity signicantly , since the parent of a task must mine all its children.

In addition, we evaluated the effects of dif ferent mem-ory thresholds on performance of our adapti ve algorithm, sho wn in Figure 6 (right). There is a strong relationship between increasing memory and lowering execution times when executing on a small number of processors. Using 18MB of memory , the code required 105 seconds, whereas if 10MB was allotted, it required 175 seconds. As the num-ber of processors increases, this impro vement decays be-cause load balancing requirements tak e precedence. On 32 processors, the execution times were identical. This can be attrib uted to the adapti ve nature of the algorithm. When there are a lar ge number of processors executing, tasks are often queued to maintain proper load balancing. When these tasks are queued, their embeddings are deleted so as to remo ve synchronization constraints.
It is clear that processor vendors now favor impro ving computational throughput by adding additional compute cores, as opposed to increasing the frequenc y of a single the PTE data set (mid dle), and D100k (right). memor y budg et vs. execution time for Web logs (right). core chip. We belie ve this will soon require algorithm de-signers to address task parallelism. Data mining tasks of-ten scale exponentially with increasing data set sizes, and as such could mak e excellent use of impro ved CPU cycles. Ho we ver, as we have sho wn in this work, nai ve paralleliza-tion strate gies often perform poorly on these systems.
We belie ve our proposed adapti ve optimizations are ap-plicable to other algorithms. Our experimentation paral-lelizing GastonEL exhibited poor scalability due to exces-sive memory consumption. Ho we ver, although GastonEL statically uses embeddings (and GastonRE statically does not), our techniques could be used to modify this algorithm to dynamically control memory consumption as well. Fi-nally , we belie ve these techniques are applicable outside the area of graph mining, since the issues presented herein are fundamental to efcient parallelization.
We have sho wn that by allo wing the state of the system to inuence the beha vior of the algorithm, we can greatly im-pro ve parallel graph mining performance. Specically , our algorithm adapts by dynamically partitioning work when needed for impro ved load balancing, and dynamically man-aging memory usage. It adapts to emplo y embedding lists and tiling optimizations only when memory constraints and task dependenc y relationships afford it. Through these tech-niques, we have developed a parallel frequent graph mining algorithm which can decrease mining runtimes by up to a factor of 27 on 32 processors. In addition, our optimiza-tions afford this scaleup while using up to 35% less main memory than traditional strate gies. We present these tech-niques in the conte xt of emer ging CMP architectures, where available main memory and associated bandwidth will be limiting factors in performance.
 We would lik e to thank Anthon y Nguyen and Daehyun Kim of Intel Corporation for their insight during man y ex-tended round table discussions.

