 Corinna Cortes cortes@google.com Google Research, 76 Ninth Avenue, New York, NY 10011 Mehryar Mohri mohri@cims.nyu.edu Afshin Rostamizadeh rostami@google.com Google Research, 76 Ninth Avenue, New York, NY 10011 The problem of learning with multiple kernels has attracted much attention from the machine learning community in the last few years (see e.g. ( Lanckriet the vast list of references in ( Cortes et al. , 2011 )). Un-like thestandarduseofkernelmethodswherethecrit-ical step of selecting a suitable kernel for a task is left to the user, multiple kernel algorithms instead require the user only to supply a family of kernels. The al-gorithm then uses the training data to both select the appropriatekerneloutofthatfamilyandtodetermine a good hypothesis based on that kernel.
 Muchoftheliteraturedealswiththeproblemoflearn-ing kernels in the binary classification case or regres-sionsetting,whilethefocusofthispaperisonlearning with multiple kernels in the multi-class classification setting. Improvementsinmulti-classclassificationper-formance has emerged as one of the success stories in multiple kernel learning. While it has proven surpris-ingly difficult to outperform the simple uniform com-bination of base kernels for binary classification and regression problems, multi-class classification has ben-efited from a number of improvements due to multiple kernellearning. Zien&amp;Ong ( 2007 )presentaone-stage multi-class multiple kernel learning (MCMKL) algo-rithm as a generalization of the multi-class loss func-tion ( Crammer &amp; Singer , 2001 ; Tsochantaridis et al. , 2004 ). The kernel and the classifiers are trained as a joint semi-infinite linear program (SILP) problem. The optimization over the kernel combination is car-ried out with an L 1 regularization that enforces spar-sity in the kernel domain. They report significant performance improvements for this algorithm over the state-of-the-art in terms of AUC, Matthews Correla-tion Coefficient, and F1-score on a number of real-world datasets from cell biology.
 In ( Orabona et al. , 2010 ) and ( Orabona &amp; Jie , 2011 ), stochastic gradient decent methods (named OBSCURE and UFO-MKL, respectively) are used to optimize primal versions of equivalent problems that select linear combinations of kernels with L p -norm or mixed-norm regularization terms. The mixed regular-ization is selected specifically to allow for a strongly convex objective function, which can be optimized ef-ficientlyusingamirrordescent-basedalgorithm. Since the problem is solved in the primal, general loss func-tions including the multi-class loss function can be used. In ( Orabona &amp; Jie , 2011 ), the OBSCURE and UFO-MKL algorithms are compared against MCMKL and performance improvements in terms of misclassi-fication accuracy are reported for a multi-class image classification problem. The OBSCURE algorithm is also shown to perform comparably to the state-of-the-artLP- X  algorithmofGehleretal.( Gehler&amp;Nowozin , 2009 ). LP- X  is a two-stage ensemble-based algorithm where multi-class classifiers are first trained indepen-dently for each kernel, then the resulting classifiers are combined by solving an LP problem. Most recently, Kumar et al. ( 2012 ) modeled kernel selection as a bi-nary classification problem and introduced a multi-class kernel learning algorithm, BinaryMKL, which learns non-positive kernel weights that aim to maxi-mize the distance between points of differing classes. There are several technical issues with the paper ( Ku-mar et al. , 2012 ), regarding both the theory and the algorithm, some of which we mention specifically later in this paper.
 We present a new algorithm for multi-class classifica-tion with multiple kernels. Our algorithm is based on a natural notion of the multi-class margin of a kernel . We show that large values of this quantity guarantee the existence of an accurate multi-class predictor in the Hilbert space associated to the kernel. This leads us to the definition of a family of multiple kernel algo-rithms(M 3 K)basedonthemaximizationofthemulti-class margin of a kernel or its corresponding regular-ization. Wepresentanextensivetheoreticalanalysisin support of our algorithm, including novel multi-class Rademacher complexity margin bounds. We also re-port the results of experiments with several data sets, includingcomparisonswhereweimproveupontheper-formance of state-of-the-art both in binary and multi-class classification with multiple kernels. We consider a standard multi-class classification su-pervised learning problem with c  X  2 classes. Let X denote the input space and let Y = of classes. We assume that the learner receives a la-size m drawn i.i.d. according to an unknown distribu-tion D over X X Y .
 Consider a family H of hypotheses mapping from X X Y to R . In multi-class classification, the la-bel predicted by h  X  H for point x is chosen as denotes its multi-class margin for the pair ( x, y ): Wewillsaythat h misclassifiespoint x when  X  h ( x, y )  X  0 for a labeled example ( x, y ). The generalization er-ror of h is denoted by R ( h ) and defined by R ( h ) = E ical distribution defined by the sample S . The em-pirical error of h  X  H is then defined by  X  R ( h ) = E We assume that p  X  1 positive semi-definite (PSD) base kernels over X X X are given and we consider a hypothesis set based on a kernel K of the form K =  X  k =1  X  k K k where  X  = (  X  1 , . . . ,  X  p )  X  considerthecase q = 1, butmuchofouranalysisholds for q &gt; 1. The hypothesis set we consider is based on the kernel property introduced in the next section. We first introduce a natural measure of the quality of a PSD kernel in the multi-class setting.
 Definition 1 (multi-class kernel margin) . For any PSD kernel K , we define the multi-class kernel mar-gin of K for a labeled instance ( x, y )  X  X  X Y as the minimum difference between the average K -similarity of x to points belonging to its class and its similarity to points in any other class and denote this quantity by  X  K ( x, y ) :  X  K ( x, y ) = E We define the multi-class kernel margin of K as  X  K = E Our notion of kernel margin is distinct from the one maximizedbyBinaryMKL( Kumaretal. , 2012 )which, for every pair of points ( x, x  X  ), creates an instance then learns a weight vector  X  using a linear SVM ob-jective with a non-negativity constraint  X   X  0: the BinaryMKL objective aims to maximize the difference between any two distinct classes, while  X  K is defined based upon the difference of class y and only the clos-est distinct class y  X  . Our choice closely matches the margin quantity relevant in the multi-class setting ( 1 ) and is further supported by the following proposition, which shows that for a kernel K with a large multi-class margin, there exists a hypothesis h  X  : X X Y X  R mitting a small generalization error. We also point outthatthetheoreticalguaranteesprovidedin Kumar et al. ( 2012 ) do not appear to match the suggested al-gorithm: both the fact that the constructed training examples ( x, x  X  ) are no longer i.i.d. and the fact that thelearnedSVMweightsareconstrainedtobepositive are not addressed in their analyses.
 Proposition 1. Let K be a PSD kernel with K ( x, x )  X  1 for all x  X  X  . Then, the following upper bound holds for the multi-class generalization error of h  X  : Proof. By definition of R ( h  X  ), we can write For any ( x, y ), we can write 1  X   X  K ( x, y ) / X  K max . Therefore, 1  X  R ( h  X  )  X  E K ( x, x )  X  1 for all x  X  X  , by the Cauchy-Schwarz inequality, the inequality | K ( x, x  X  ) | X   X  the proof.
 This result further justifies the notion of margin intro-duced and motivates the algorithms described next. 4.1. Multi-class kernel margin maximization In view of the definition and results of the previous section, a natural kernel learning algorithm consists of selecting  X  to maximize the empirical multi-class margin of the combination kernel K  X  = Let C ( y ) denote the set of sample points in S labeled with y : C ( y ) = optimization problem can be written as follows: For any k  X  [1 , p ], i  X  [1 , m ], and y  X  X  , we define  X  k ( x i , y i , y ) = component is  X  k ( x i , y i , y ). Then, the optimization problem can be equivalently written as of  X  and can be precomputed for a given sample and set of kernels. Introducing new variables denoting the minima, the optimization problem can then be equiv-alently written as the following convex optimization problem which is a linear programming (LP) problem in the case q = 1: max An alternative idea consists of maximizing the minimum kernel margin: max  X   X   X  requirement for the existence of the good multi-class solution discussed in the previous section and may be too strong a condition. We have in fact verified that it typically leads to a poor performance. 4.2. Maximum margin multiple kernel (M 3 K) Given a training sample, we can define the empirical multi-class kernel margin as follows: which can then be used to define the data-dependent set  X  M q = incorporated as an additional form of regularization into a kernel learning optimization problem based on multi-class SVM ( Weston &amp; Watkins , 1999 ; Crammer &amp; Singer , 2001 ): where C  X  0 is a regularization parameter. Here, we have defined for any class y  X  X  the associated ( X  mapping associated to the kernel K . We refer to the algorithm based on optimization ( 7 ) as the multi-class maximum margin multiple kernel (M 3 K) algorithm. The additional constraint  X   X  K  X   X  0 in  X  M q ensures that  X  is selected such that the average empirical kernel margin is at least  X  0 . It is important to note that if  X  is chosen to be too large, then the optimization prob-lembecomesinfeasible. Thereareinfacttwoextremes in choosing  X  0 : setting it equal to the maximum fea-sible value will guarantee that the selected  X  is also a solution to the optimization problem in ( 5 ), while setting it equal to  X  X  X  in the case q = 1 will reduce the algorithm to the MCMKL algorithm presented by ( Zien &amp; Ong , 2007 ).
 The dual formulation of M 3 K is written as follows: min subject to:  X  i  X  [1 , m ] ,  X  i  X  e y Here,  X   X  R m  X  c is a matrix,  X  i is its i th row, and e this min-max problem can be solved using a standard reduction to a SILP problem as in ( Sonnenburg et al. , 2006 ):  X   X   X   X   X  ThisSILPproblemissolvedusingacutting-planetype algorithm, which considers only a finite subset of the constraints over  X  . Initially, we consider no  X  -based constraint and only find a feasible  X   X   X  M 1 . We then find a most violated constraint  X  = max  X  e straint defined by this  X  is added to the optimiza-tion problem. An optimal  X  that obeys all constraints addeduptothispointisthenfoundandanewmostvi-olated  X  isaddedtotheoptimization. Theseiterations continue until either a violating constraint cannot be found or the difference in successive choices of  X  is in-significant. At each iteration we solve an LP to find the current best choice of  X  and a quadratic program (QP) to find a most violated constraint  X  . Although in this paper we focus on L 1 -regularized choices of  X  , we note that it is also possible to solve the problem for other L q regularization ( q &gt; 1), or even using group norms over  X  , with different optimization techniques. In this section, we present generalization bounds for learning kernels in the multi-class setting for a hy-pothesis set basedon our notion of multi-class margin. We start with a general margin-bound for multi-class classification, then analyze the empirical Rademacher complexity of the hypothesis set we consider to derive margin-based guarantees for multiple kernel learning in the multi-class setting. 5.1. General multi-class margin bounds Let H be a set of hypotheses mapping from X to R . We will denote by  X  R S ( H ) the empirical Rademacher complexity of the set H for a sample S :  X  R S ( H ) = E dependent uniform random variables taking values in  X   X  1 , +1 calmarginlossof h inthemulti-classsettingcanbede-fined by  X  R  X  ( h ) = 1 m thesetoffunctionsdefinedover X andderivedfrom H asfollows: H X = thefollowinggeneralmarginboundcanbegiveninthe multi-class setting. This is a simpler version of a re-sult given by ( Koltchinskii &amp; Panchenko , 2002 ), a full proof is provided in the appendix.
 Theorem 1. Let H  X  R X X Y be a hypothesis set with Y = with probability at least 1  X   X  , the following multi-class classification generalization bound holds for all h  X  H : As for all margin guarantees, the bound expresses a trade-off between margin maximization (larger  X  val-ues) and empirical margin loss minimization (smaller empirical margin loss values,  X  R  X  ). The presence of the quadratic term c 2 suggests that larger margin values are required in the multi-class setting than in binary classification to achieve good generalization guaran-tees. 5.2. Multi-class margin bounds for multiple To apply this generalization bound in our context, we willanalyzetheempiricalRademachercomplexityofa hypothesis set based on convex combinations of p base kernels and with a lower bounded multi-class kernel margin. Forthesakeofbrevity,ourguaranteesarepre-sented in the case of an L 1 regularization for the mix-ture weights  X  , but much of our analysis can be gener-alizedtoother L q andgroup-normregularizationswith q &gt; 1. Each element of the hypothesis set H 1 is de-finedby c functions h 1 , . . . , h c belongingtotheHilbert space H K Thus, the formal definition of H 1 is where  X  0  X  R ,  X   X  0, and M 1 =  X   X  so that M 1  X  =  X  , that is  X  0 is not above the maximum multi-class margin of K  X  achievable by any  X   X   X  1 . The proof of our generalization bound is based on the following series of lemmas and partly makes use of some of the results and techniques given by ( Cortes et al. , 2010 ). The proof of the first lemma is given in the appendix.
 Lemma 1. For any labeled sample S of size m , we have  X  R S ( H 1 X )  X   X  m E  X  (  X   X  K 1  X  , . . . ,  X   X  K p  X  )  X  .
 In order to bound the Rademacher complexity, we first analyze and simplify the optimization problem y, y  X   X  X  with y  X  = y  X  , we define  X  K ( x, y, y  X  ) = Thus, by definition of  X  K , we have  X  K = E Lemma 2. For any k  X  [1 , p ] , we also define  X   X  k E R p the vector whose k th coordinate is  X   X  k . Then, the following inequality holds: Proof. By definition of  X  k s, we can write  X  Thus,  X  K the following LP problem: max Introducing the dual variables  X   X  R p ,  X   X  0,  X   X  R , and  X   X  0, the Lagrangian L for this problem can be written as
L =  X   X   X  u  X   X   X   X   X  +(  X  1+  X   X  1 )  X  +  X  (  X  0  X   X   X  Computing its gradient with respect to  X  and setting it to zero gives Solving for  X  and plugging in this identity in L leads to the equivalent dual problem: min Fixing  X  and solving for  X  gives the following equiva-lent convex optimization problem: min Theoptimizationproblemoflemma 2 istheminimiza-tionofapiecewiselinearfunction, whereeachlineseg-ment is indexed by some k  X  [1 , p ]. Assuming that the problem is feasible, the optimal solution falls into one oftwocasesillustratedinfigure 1 ,whereeachblueline corresponds to a choice of k and the red dotted line is the piecewise linear function that is being minimized over.
 In the left panel of the figure, we see the general sce-nario where the optimalchoiceof  X  is described by the intersectionoftwolinesindexedby k and k  X  . Notethat in this case, one line must have a non-positive slope, i.e.  X   X  k  X   X  0 and the other must have a non-negative the intersection of lines indexed by k and k  X  that sat-isfy  X   X  k  X   X  0  X   X   X  k  X  (with  X   X  k  X  =  X   X  k  X  ). The second case occurs iff for k max = argmax k u  X  ,k we have  X   X  k of  X  is met at the boundary value 0 and the value of the optimal is simply u  X  ,k describestheseobservations,withaformalprooffound in the appendix. We first define the following sets
I
J used throughout the remainder of the section. Lemma 3. Given the same definitions as in lemma 2 , the following equality holds: min max WenowusethisboundontheRademachercomplexity to derive our generalization bound.
 Theorem 2. Fix  X  &gt; 0 and let p  X  = Card( I p )  X  p probability at least 1  X   X  over the choice of a sample S of size m , the following multi-class classification gen-eralization bound holds for all h  X  H 1 :
R ( h )  X   X  R  X  ( h ) + where T  X  Proof. First, define the constant function K 0 = 0 and the constant  X   X  0 =  X  X  X  . With this notation, the en-tire right hand side of the equality in lemma 3 can be simply written as where M p =  X   X  subset of M p . Furthermore, if we fix k = 0 then for account exactly for the elements in I p .
 Using this and combining lemma 1 , lemma 2 , and lemma 3 , for any integer r  X  1, we can write m
 X   X  E = E  X  E , where we used for the second inequality the fact that  X  X  X  X   X  is upper bounded by  X  X  X  X  r for any r  X  1 and for the last inequality the concavity of x  X  X  X  x 1 / 2 r and Jensen X  X  inequality. By lemma 1 of ( Cortes et al. , 2010 ), the following inequality holds: E  X  log( p  X  + p  X  X  X  ), thus this yields the inequality  X  R Finally, by the definition of M p we have max PlugginginthisupperboundontheRademachercom-plexity of H 1 X in the learning guarantee of theorem 1 concludes the proof.
 The theorem gives a general margin bound for mul-tiple kernel learning based on  X  -combinations of p based kernels, with an L 1 regularization for  X  aug-mented with the multi-class kernel margin regulariza-tion  X  K on the complexity term is analyzed by the following lemma.
 Lemma 4. T  X  Due to space constraints we present the proof of the lemma in appendix E within the supplementary sec-tion. The lemma implies that the main complexity term that depends on  X  0 in the bound of theorem 2 becomes smaller as the regularization become more stringent. Also, note that the dependence on p is only logarithmic. This weak dependence strongly encour-ages the idea of using a large number of base kernels. 2 Altogether, thisanalysisprovidesstrongsupportinfa-vor of an algorithm minimizing the sum of the empir-ical margin loss of a multi-class hypothesis defined by ( h 1 , . . . , h c ) and a complexity term based on the norm ofthese c functions,whilecontrollingthe L 1 -normof  X  and maximizing the multi-class margin  X  K incides precisely with our M 3 K algorithm modulo the maximizationoftheempiricalmulti-classmargin  X   X  K the quantity computable from a finite sample, instead of  X  K ties are close modulo a term in O ( k/ also provides a strong foundation for our algorithm. Other results with a similar analysis can be given for a regularization based on the L q -norm of  X  . In particu-lar, for an L 2 -norm regularization, the dependency on instead of a logarithmic dependency. In this section, we report the results of several ex-periments with our multi-class multiple kernel M 3 K algorithm. First, we show that M 3 K performs well in binaryclassificationtasksbycomparingwithMCMKL ( Zien &amp; Ong , 2007 ) and the best published results ( Cortes et al. , 2012 ). Next, in the multi-class set-ting, we compare against other state-of-the-art algo-rithms that learn a kernel for multi-class SVM ( Cram-mer &amp; Singer , 2001 ). These include BinaryMKL ( Ku-mar et al. , 2012 ), OBSCURE ( Orabona et al. , 2010 ) and UFO-MKL ( Orabona &amp; Jie , 2011 ). In all the ex-periments that follow, we consider the case of an L 1 regularization of  X  for the M 3 K algorithm. 6.1. Binary Classification Table 1 shows the accuracy of several algorithms on the splice and spambase binary classification datasets. The accuracies are shown with  X  1-standard deviation as measured over a 5-fold cross-validation with 1000 examples. We use the same experimental setup as in ( Cortes et al. , 2012 ) which uses 7 X 8 Gaussian ker-nels with various bandwidths as the set of base ker-nels (we refer the reader to that reference for further details of the methodology and datasets). For both of these datasets, the parameter  X  0 of the M 3 K al-gorithm is simply set to the maximum feasible value, which can be found using ( 5 ), and C is found via a grid search. We not only find that M 3 K outperforms the uniform combination of kernels, which has proven to be a difficult baseline to beat, but also that it ei-ther matches or improves over the alignment-based algorithm ( alignf ) presented in ( Cortes et al. , 2012 ) and considered state-of-the-art on these datasets. As can be seen, M 3 K is also able to significantly outper-formMCMKL.Moregenerally, notethatM 3 Kstrictly generalizes the MCMKL algorithm of ( Zien &amp; Ong , 2007 ) and thus always performs at least as well as MCMKL. Hence, in the next section, we focus on comparingagainstotherstate-of-the-artalgorithmsfor multi-class kernel learning in the multi-class setting. 6.2. Multi-class Classification In the multi-class setting, we compare to the uniform combinationkernelbaselineaswellastheBinaryMKL algorithm using the biological datasets ( plant , non-plant , psortPos ,and psortNeg )thatarealsoconsidered in( Kumaretal. , 2012 )(andoriginallyin( Zien&amp;Ong , 2007 )), which consist of either 3, 4, or 5 classes and use 69 biologically motivated sequence kernels. 3 We also experiment with an additional biological dataset ( proteinFold ) of ( Damoulas &amp; Girolami , 2008 ), which consists of 27 classes and 12 base kernels. 4 Finally, we report the results of experiments with the caltech101 vision-task dataset with 48 base kernels. 5 For each of the 102 classes, we select 30 examples (for a total of 3060points)andthensplitthese30examplesintotest-ingandtrainingfolds,whichensuresmatchingtraining and testing distributions. For this dataset, we addi-tionally compare with the OBSCURE and UFO-MKL algorithms which achieve state-of-the-art performance in this task. The choice of C  X   X  0 (in the case of M 3 K) is optimized via a grid search. For the OBSCURE and UFO-MKL algorithm, we fol-low the methodology of ( Orabona et al. , 2010 ) and ( Orabona &amp; Jie , 2011 ), respectively, for selecting pa-rameters. Allkernelsarefirstcenteredandthenscaled so that for all i and k we have K k ( x i , x i ) = 1. The multi-class accuracy of the uniform combination, BinaryMKL, and M 3 K algorithms for the biological datasets is shown in table 2 with one standard devi-ation as computed over 10 random splits of the data. Weobservethatforthebiologicaldatasetsbothkernel learning algorithms can perform better than the lin-ear combination, while M 3 K always performs at least as well as BinaryMKL and sometimes performs sig-nificantly better (in particularly nonplant and pro-teinFold ). The results of figure 2 (right) show that M 3 K performs comparably to all algorithms in the range of 10-20 training points per class for the cal-tech101 dataset and performs even better than state-of-the-art algorithms when training with 25 points per class. Finally, we note that in simple multiclass tasks where overfitting is not an issue we observe that M 3 K doesnotalwaysprovideasignificantimprovementover MCMKL. However, for more challenging tasks with moreconfusableclasses,weexpectsignificantimprove-ments, as we found empirically even in some binary classification tasks.
 The training time of different algorithms is shown in table 3 for several datasets. BinaryMKL is substan-tially faster than M 3 K for the first two datasets of table 3 . However, we observe that, when the number of kernels p or the ratio m/c is large, the training time of M 3 K becomes more favorable than that of Bina-ryMKL,asinthenexttwodatasetsofthetable. 6 Inall cases, UFO-MKL, which uses a fast stochastic gradi-entdescentmethodtosolvetheoptimizationproblem, issignificantlyfasterthanallotheralgorithms. Webe-lievethatM 3 Kcanbenefitfromafastimplementation similar to that of UFO-MKL and will actively pursue thisquestion. Letusmention,however,that,asshown inthe caltech101 dataset, theincreasedspeedofUFO-MKL appears to come at some cost in performance. Overall, we find M 3 K to be a robust algorithm with a competitive performance in all datasets, including significant improvements in several cases. We presented a new analysis of the problem of multi-ple kernel learning in the multi-class classification set-ting. We defined the notion of multi-class kernel mar-gin, used it to define a new learning algorithm (M 3 K), and presented new generalization bounds for hypoth-esis sets defined in terms of this margin. We also pre-sented a series of empirical results demonstrating the good performance of our algorithm in practice. These results further motivate the search for more efficient solutions to the optimization problems introduced, as well as a finer analysis of alternative algorithms based on the multi-class kernel margin.
 Bach, Francis R., Lanckriet, Gert R. G., and Jordan,
Michael I. Multiple kernel learning, conic duality, and the smo algorithm. In ICML , 2004.
 Bartlett, Peter L. and Mendelson, Shahar.

Rademacher and Gaussian complexities: Risk bounds and structural results. JMLR , 3, 2002. Cortes, Corinna, Mohri, Mehryar, and Rostamizadeh, Afshin. Generalization bounds for learning kernels. In ICML , pp. 247 X 254, 2010.
 Cortes, Corinna, Mohri, Mehryar, and Rostamizadeh,
Afshin. Tutorial: Learning kernels. In ICML , 2011. Cortes, Corinna, Mohri, Mehryar, and Rostamizadeh,
Afshin. Algorithms for learning kernels based on centered alignment. Journal of Machine Learning , 2012.
 Crammer, Koby and Singer, Yoram. On the algo-rithmic implementation of multiclass kernel-based vector machines. Journal of Machine Learning Re-search , 2:265 X 292, 2001.
 Damoulas, Theodoros and Girolami, Mark A. Prob-abilistic multi-class multi-kernel learning: on pro-tein fold recognition and remote homology detec-tion. Bioinformatics , 24(10):1264 X 1270, 2008. Gehler, P. and Nowozin, S. On feature combination for multiclass object classification. In International Conference on Computer Vision , pp.221 X 228, 2009. Kloft, M., Brefeld, U., Sonnenburg, S., and Zien, A.
L p -norm multiple kernel learning. Journal of Ma-chine Learning Research , 12, 2011.
 Koltchinskii, Vladmir and Panchenko, Dmitry. Em-pirical margin distributions and bounding the gen-eralization error of combined classifiers. Annals of Statistics , 30, 2002.
 Kumar, A., Niculescu-Mizil, A., Kavukcoglu, K., and
Daum  X e III, H. A binary classification framework for two stage kernel learning. In ICML , 2012.
 Kwapien, Stanislaw and Woyczynski, W Wojbor An-drzej. Random series and stochastic integrals . Birkhauser, 1992.
 Lanckriet, Gert R. G., Cristianini, Nello, Bartlett, Pe-ter L., Ghaoui, Laurent El, and Jordan, Michael I.
Learning the kernel matrix with semidefinite pro-gramming. Journal of Machine Learning Research , 5:27 X 72, 2004.
 Ledoux, Michel and Talagrand, Michel. Probabil-ity in Banach Spaces: Isoperimetry and Processes . Springer, New York, 1991.
 Orabona, F. and Jie, L. Ultra-fast optimization algo-rithm for sparse multi kernel learning. In Proceed-ings of the 28th International Conference on Ma-chine Learning , 2011.
 Orabona, F., Jie, L., and Caputo, B. Online-batch strongly convex multi kernel learning. In
Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on ,pp.787 X 794.IEEE,2010. Sonnenburg, S., R  X atsch, G., Sch  X afer, C., and
Sch  X olkopf, B. Large scale multiple kernel learn-ing. Journal of Machine Learning Research , 7:1531 X  1565, 2006.
 Tsochantaridis,Ioannis,Hofmann,Thomas,Joachims,
Thorsten, and Altun, Yasemin. Support vector ma-chine learning for interdependent and structured outputspaces. In ICML 2004, Banff, Canada , 2004. Weston, Jason and Watkins, Chris. Support vector machines for multi-class pattern recognition. Eu-ropean Symposium on Artificial Neural Networks , 4 (6), 1999.
 Zien, Alexander and Ong, Cheng Soon. Multiclass multiple kernel learning. In ICML , pp. 1191 X 1198,
