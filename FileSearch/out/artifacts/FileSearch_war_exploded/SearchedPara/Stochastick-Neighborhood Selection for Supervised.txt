 Daniel Tarlow  X  dtarlow@microsoft.com Microsoft Research Cambridge University of Toronto Ilya Sutskever  X  ilyasu@google.com Google Inc.
 Richard S. Zemel zemel@cs.toronto.edu University of Toronto Distance metrics are used extensively in machine learning, both as an essential part of an algorithm like in k -means or k -nearest neighbors (kNN) algorithms, and as a regularizer in, e.g., semi-supervised learning. An obvious problem is that the space in which data is collected is not always suitable for the target task (e.g., nearest neighbor classification); the choice of pa-rameters like the scale of each dimension can signif-icantly impact performance of algorithms. Thus, a long-standing goal is to learn the distance metric so as to maximize performance on the target task.
 Neighborhood Component Analysis (NCA) is a method that aims at doing precisely this, adapting the distance metric to optimize a smooth approximation to the accuracy of the kNN classifier. A shortcoming of the NCA model is that it assumes k = 1, and in-deed, all of the experiments in ( Goldberger et al. , 2004 ) and many in follow-up applications are performed with k = 1. This choice appears to be made for the sake of computational convenience: generalizing NCA for arbi-trary k requires computing expected accuracies over all possible ways of choosing k neighbors from N points, which appears to be di ffi cult when k is large. But since kNN with k&gt; 1 tends to perform better in practice, it seems desirable to formulate an NCA method that directly optimizes the performance of kNN for k&gt; 1. The primary method we present, kNCA, is a strict generalization of NCA, which optimizes the distance metric for expected accuracy of kNN for any choice of k , and is equivalent to NCA when k is set to 1. The main algorithmic contribution of our work is a construction that allows the expected accuracy to be computed and di ff erentiated exactly and e ffi ciently. We also show that similar techniques can be applied to other problems related to the stochastic selection of k -neighborhoods, such as is required when extend-ing Stochastic Neighbor Embedding (SNE) methods to their k -neighbor analogs.
 Thus, we make several contributions. The primary contribution is the extension of NCA so that it is ap-propriate when we wish to use a kNN classifier at test time with k&gt; 1. Secondary contributions are the tech-niques for computing expected accuracy and the exten-sion of SNE to the k -neighborhood setting. We explore the methods empirically: quantitatively, by comparing performance to several popular baselines on a range of illustrative problems, and qualitatively, by visualiz-ing the learned embeddings by (supervised) kNCA and (unsupervised) kSNE. We begin with some basic notation. Input vectors are denoted by x  X  R D . In supervised settings, the class D is made up of N tuples ( x i ,y i ) N is the concatenation of all x i as column vectors. Sim-ilarly, Y is the vector of all the labels y i . We use z that is associated with x i , and Z to represent the con-catenation of all z i as column vectors. We use [  X  ] as the indicator function. 2.1. Neighborhood Components Analysis At a high level, the goal of NCA is to optimize a dis-tance metric under the objective of performance of kNN algorithms. There are many ways to parameter-ize a distance metric, and this choice is not fundamen-tal to the approach (for example, a non-linear exten-sion like in Salakhutdinov &amp; Hinton ( 2007 ) would be straightforward). For simplicity, we follow NCA and frame the presentation under the assumption that the distance between two vectors x and x " is defined as This choice has the interpretation that we are first lin-early projecting points x  X  R D into P -dimensional space via the matrix A  X  R P  X  D , then comput-ing Euclidean distances in the P -dimensional space. NCA also gives us the ability to visualize the learned metric by setting P to be small and plotting trans-formed points Z = AX .
 Optimizing the entries of A requires the specification of a learning objective. A first attempt might be the accuracy of a kNN classifier. This approach is not feasible, because as a function of A , the performance of a kNN classifier is a piecewise constant function, which is not possible to optimize with gradient meth-ods (note that any change in A that does not change the neighbor set of any point will not a ff ect this ob-jective). However, even if this approach were feasible, we still might prefer a smooth objective for the sake of robustness to noise in the data. For example, if a point has a neighbor of the proper class at a distance of b away, and many neighbors of other classes at a dis-tance of b + ! away, the kNN accuracy where k = 1 will attain the maximum objective. This clearly is not a robust solution, though, since a slight perturbation of the data would likely lead to an error on this example. These considerations lead to the central NCA idea of casting kNN in a probabilistic light. Specifically, ( Goldberger et al. , 2004 ) define a probability of select-ing each point as its 1-nearest neighbor, which is a function of distance in transformed space. The learn-ing objective is then the expected accuracy of a 1-nearest neighbor classifier under this probability dis-tribution: where p i ( j )  X  exp bility that i selects j as its (single) neighbor. This smooth objective has a nice interpretation as max-imizing the expected accuracy of a 1-nearest neigh-bor classifier. Note that ( Goldberger et al. , 2004 ) also proposes an alternative objective, which can be inter-preted as the (log) probability of obtaining an error free classification on the entire training set, L ( A )= $ is similar between the two objectives. Our generaliza-tion applies to either objective, and we similarly found performance to be similar between the two methods. 2.2. Stochastic Neighbor Embedding Stochastic Neighbor Embedding (SNE) ( Hinton &amp; Roweis , 2002 ) is an unsupervised dimensionality re-duction method that attempts to reproduce the lo-cal structure of high-dimensional data in a low-dimensional space. While this approach to dimen-sionality reduction is taken by popular methods such as Locally Linear Embedding (LLE) ( Roweis &amp; Saul , 2000 ) and Isomap ( Balasubramanian et al. , 2002 ), the SNE method di ff ers from these methods because it uses a fundamentally smooth objective that is based on matching distances between distributions. Given a set of high-dimensional points, { x 1 , . . . , x SNE defines a distribution p i ( j ) for each point i that assigns a greater probability to its closer neighbors: This distribution depends strongly on the length-scale  X  , which is chosen in order to bring the entropy of p i to a certain user-specified value (in experiments we set  X  ( 2002 )). This way, the distributions p i smoothly de-scribe the local neighborhood structure of the high-dimensional data.
 a low-dimensional space, we can define a similar distri-bution q i ( j )  X  exp local neighborhood structure of the embedded points. SNE finds an embedding whose local neighborhood structure matches that of the original data by mini-mizing the following objective: ! In contrast to many other dimensionality reduction methods such as multidimensional scaling methods (for a good explanation see ( van der Maaten et al. , 2009 )), SNE penalizes configurations that do not keep the neighbors close, while being more lenient to config-urations that bring points together that are far apart. SNE has been extended in several ways. Uni-SNE ( Cook et al. , 2007 ) alters the definition of q ( j ) by the addition of a small constant: q i ( j )  X  exp  X  X  ff ective space X  in the low-dimensional space, since it can now place far points arbitrarily far. t -SNE ( van der Maaten , 2009 ) alters the definition of q i ( j ) even more to work even better in practice due to the heavier tails of the Cauchy distribution. Following the work of NCA, several researchers have proposed approaches to the problem of metric learn-ing for kNN classification using the idea of stochas-tic neighbors. We note the method of Globerson &amp; Roweis ( 2006 ), Maximally Collapsing Metric Learning algorithm (MCML), which presents a convex optimiza-tion function that approximates the desiderata that all points belonging to a class should be mapped to a sin-gle location in the embedding space infinitely far away from the points in other classes. A non-linear version of NCA (NLNCA) has also been introduced where a neural net is used to learn a non-linear mapping from original to embedding space ( Salakhutdinov &amp; Hinton , 2007 ). We are not aware of other NCA-based work that tailors the objective to the case of k&gt; 1. There has also been a plethora of research on met-ric learning for kNN classification using determin-istic neighborhoods. Standard learning techniques such as random forests ( Xiong et al. , 2012 ), boost-ing ( Shen et al. , 2009 ), and large margin classification approaches ( Weinberger &amp; Saul , 2009 ) have been ap-plied to this problem. A good review of this work is provided in Yang ( 2007 ), which also surveys some of the classic methods first introduced in the field such as RCA and LDA. We highlight the work of Weinberger &amp; Saul ( 2009 ), which introduced a Large Margin Nearest Neighbor method (LMNN). They frame the problem as the optimization of a cost function which penal-izes large within-class distance and small out-of-class distances in the embedding space. The within-class distances is only taken with respect to a number of target neighbors . Setting the size of this target neigh-borhoods acts in a similar fashion as setting k (and we will compare experimentally to this). Finally, we briefly note the method Information Theoretical Met-ric Learning (ITML) ( Davis et al. , 2007 ), where the problem is framed as a Bregman optimization prob-lem and does not require the solution of an expensive semidefinite program. Empirically ITML often rivals LMNN in performance but its run-time is generally significantly shorter.
 For the unsupervised case, a brief review of existing approaches is provided in Section 2.2 ; a good survey is also available ( van der Maaten et al. , 2009 ). Our starting point is the NCA objective from Eq. 2 . While the objective has a desirable simplicity, it is heavily tailored to the case of a 1-nearest neighbor classifier: the distribution over neighbors assumes so, and the accuracy measure within the expected accu-racy objective is the accuracy relative to selecting 1-nearest neighbor. Our goal in this section is to tailor NCA to the case of a kNN classifier for arbitrary k . There are two components. First, we define a probabil-ity distribution over the selection of sets of k neighbors. Second, we modify the accuracy measure to reflect that the kNN procedure selects a label by majority vote of the k neighbors. Putting these two components to-gether yields the kNCA expected accuracy objective: where S i is the set of all subsets of neighbors of i , Maj ( s ) denotes the majority function X  X qual to the kNN classifier output that would result from choos-ing s as the set of neighbors X  X nd p i ( s | k ; A ) rep-resents the probability that i chooses s as its set of neighbors given that it chooses k neighbors. We define p ( s | k ; A ) to be a function of the distances between i and the points j  X  s as follows: p ( s | k ; A )= , where we use the shorthand d ij = d ij ( A )= || Ax i  X  Ax j || 2 2 . While it is not yet obvious that Eq. 5 can be optimized e ffi ciently, as the normalization Z = Z ( A ) entails considering all subsets of size k , it should be clear that it is the expected accuracy of a kNN clas-sifier in the same way that the NCA objective is the expected accuracy of a 1NN classifier. Indeed, with k = 1 we recover NCA. The main computational observation in this work is that Eq. 5 can be computed and optimized e ffi ciently. To describe how, we begin by rewriting the inner sum-mation from Eq. 5 (throughout this section, we will focus on the objective for a single point i , and will drop dependencies on i in the notation): Our strategy is to formulate a set of factor graphs to represent the components of this problem such that the objective can be expressed as a sum of ratios of parti-tion functions. Afterwards, we will show how e ffi cient exact inference can be done on these factor graphs. Factor graph construction. We start by con-structing a factor graph for which the associated par-tition function is equal to the denominator in Eq. 6 ; see Fig. 2 (b). Note this is all with respect to a single point i considering its set of k neighbors. First (at the bottom of the factor graph), there is a binary indica-tor variable h j for each possible neighbor j , indicating whether it is chosen to be a neighbor of i . Unary po-tentials (not drawn) are added, with potential set to  X  ( h j )= h j d ij . We group these variables according to class and introduce an auxiliary variable for each class c (next level up in the factor graph, denoted by  X  c ) that deterministically computes the number of neigh-bors that are chosen from class c . Finally (top level in the factor graph), there is an auxiliary variable  X  that counts the total number of neighbors that were chosen across all classes. At this point, we can add a constraint that the total number of neighbors selected across all classes is k , and this is simply a unary poten-tial on  X  that disallows all values other than k . Once this constraint is added, the partition function of the resulting model is equal to the denominator of Eq. 6 . To construct the numerator, we first observe that Maj ( s )= y i is true if and only if there is some k " such that $ all c % = y i . That is, the true label gets k " votes, and all other labels get fewer than k " votes. For a given k tials on the intermediate sum variables  X  c . Counts for the true class y i are constrained to exactly equal k to be less than k " . This is illustrated in Fig. 2 (c). By then summing over the partition function of these models for k " =1 , . . . , k , we cover all possible ways for Maj ( s )= y i to be true, and thus recover the numer-ator of Eq. 6 . Specifically, Eq. 6 can be rewritten as follows: E ffi cient inference. At this point, we have reduced the di ffi culty of the kNCA objective to computation of partition functions in the models constructed in the previous section. If these partition functions can be e ffi ciently computed and di ff erentiated, then the kNCA objective can be optimized.
 The key observation is that the models in Fig. 2 (b) and (c) are special cases of Recursive Cardinal-ity (RC) models ( Tarlow et al. , 2012 ). An RC model defines a probability distribution over binary vectors h =( h 1 , . . . , h N ) based on an energy function of the form E ( h )= $ is a set of subsets of { 1 , . . . , N } that must obey a nest-edness constraint: for all s, s "  X  S , either s  X  s " =  X  tions of the number of variables within the associated subset that take on value 1 and can be di ff erent for each s . Given this energy function, an RC model de-fines the probability of a binary vector h as a standard Gibbs distribution: p ( h )= 1 is the partition function that ensures the distribution sums to 1. The key utility of RC models is that the partition function (and marginal distributions over all h i variables) can be computed e ffi ciently. Briefly, in-ference works by constructing a binary tree that has h i variables at the leaves, and variables representing counts of progressively larger subsets at internal nodes, then doing fast sum-product updates up and down the tree. See Tarlow et al. ( 2012 ) for more details. For all applications of RC models considered here, this computation of each partition function and associated marginals would take O ( N log 2 N ) time. Below, we alternatively show how to implement the same com-putations in O ( Nk + Ck 2 ) time (where recall N is the total number of points considered as neighbors, k is the number of neighbors to select, and C is the number of classes). For our purposes where k is typically small, and due to the smaller constant factors, an e ffi cient C++ implementation of this algorithm outperformed a generic implementation from ( Tarlow et al. , 2012 ), so we used the special-purpose algorithm throughout. Algorithm 1 kNCA inference for point i Alternative O ( Nk + Ck 2 ) algorithm. Here we present the alternative algorithm for computing marginal probabilities (used for gradients) and par-tition functions (used to evaluate the expected kNCA objective) for the models illustrated in Fig. 2 . The structure of the algorithm is given in Alg. 1 . The overall idea is to do dynamic programming over two levels of chain-structures. The first level of the forward pass computes probabilities over the number of neigh-bors chosen from each class separately, then the second level combines the results across classes to compute probabilities over the total neighbors selected. The backward pass propagates information from the other classes backward to the individual classes.
 The computations use dynamic programming, incre-mentally computing a vector f c ( j )  X  R k +1 that stores the probability for each  X  k  X  0 , . . . , k that  X  k vari-ables from class c were chosen as neighbors of i from amongst the first j points of class c , assuming that the point j is chosen independently with probability p f ( j + 1) from f 1 that f 1 Intuitively, there are only two ways for  X  k variables to be chosen from amongst the first j variables: either  X  k were chosen from the first j  X  1 and the j th was not used, or  X  k  X  1 were chosen from the first j  X  1, and the j In the second level chain, we use the result of the first level forward pass to compute vectors f 2 ( c ), which store the upward probabilities that each possible num-ber of neighbors were chosen from amongst the first c classes. The Forward2 functions compute these updates on O ( k 2 ) time using the update f 2 ( c )[  X  k $ where  X  c ( k c ) expresses the constraints that are given as unary factors on the  X  c variables in Fig. 2 . Similar reasoning can be used to derive the backward updates. It can be shown that the above updates correspond to performing sum-product belief propagation on the fac-tor graphs in Fig. 2 , which are tree-structured, so the partition function and marginal probabilities of each neighbor being selected can be read o ff from the re-sults. See Tarlow et al. ( 2012 ) for more details on this interpretation. Algorithmically, relative to Tar-low et al. ( 2012 ), the main di ff erence is that we take advantage of the fact that any configuration with more than k neighbors chosen is disallowed.
 Objective Function Variations. The construc-tion from the previous section allows for other choices of the accuracy measure than the Maj one. For ex-ample, by requiring that all selected neighbors are of the target class (i.e., [ $ an accuracy measure that only rewards neighborhoods where all neighbors are of the target class. While this alternative measure which maximizes the num-ber of points with  X  X erfect X  k -neighborhoods does not correspond to optimizing the expected accuracy of a kNN classifier, we will show below that it can boost the classifier X  X  performance in practice. The preceding derivations can also easily be applied to a  X  X robability of error free classification X  variant of kNCA, analo-gous to the NCA variants discussed in Sec. 2.1 , which amounts to taking a sum of logs of Eq. 6 instead of just a sum. So far, we have focused on the supervised case, where class labels are available for all points. In this sec-tion, we consider the unsupervised analog of kNCA. The starting point is Stochastic Neighbor Embedding (SNE), which like NCA has an interpretation that in-volves the stochastic selection of one neighbor. Rather than selecting one neighbor, we proceed again by defining distributions over sets of k neighbors. For the unsupervised version, we have a target distribu-tion p and an approximating distribution q . The goal is to minimize the sum of KL divergences for each point i :  X  $ where p i (  X  ) is defined in terms of target distances d in the original space, while q i (  X  ) is defined in terms of distances in the lower dimensional space: p i ( s | k )= q i ( s | k )= We can leverage the previous e ffi cient computations of this objective after some re-arranging of the objective. Focusing on a single i : =  X  where p k given that i chooses sets of k neighbors according to p ( c | k ). The computations involved here are the same as are involved in computing Z 0 for kNCA. Note that p learning, and also that this formulation is agnostic to the distance measure used and therefore can be easily adapted to suit the measures used in SNE, t -SNE and other variants. We focus on the t -SNE variant. In a similar fashion to Goldberger et al. ( 2004 ), we experimented with various loss functions on several UCI datasets as well as the USPS handwritten digits dataset. For our experiments we divide the datasets into 10 di ff erent partitions of training and testing sets. Each partition uses 70% of the data for training and the remainder for testing. We inititialize the embed-ding matrix using PCA. We experimented with vari-ous values for k between 1 and 10, where k = 1 implies normal NCA. For each value of k , we trained separate models using the two loss functions discussed in the text: one that favors all k neighbors belonging to the same class (All) and one that favors that the majority belong to the same class (Majority) (when k = 1 these are equivalent). We experimented with both NCA ob-jective variants and found performance to be similar, but we found the sum of logs variant (probability of er-ror free classification) to be slightly easier to work with numerically, so we report results using it. Once each model is trained, we test using k -nearest neighbors on the learned models for k  X  { 1 , 2 , ..., 15 } . We experi-mented with learning parameters that either project the data down to 2 dimensions, or retain the origi-nal data dimensions. To optimize, we used stochastic gradient descent with momentum, subsampling a set of points to compute the gradients (but still always considering all points as possible neighbors). 7.1. kNN Classification and kNCA Embedding Our first set of experiments mimics those found in Goldberger et al. ( 2004 ). The extended results are given in the supplementary materials. For purposes of illustration, we show the learned embeddings from the di ff erent kNCA methods for one dataset (wine) in Fig. 3 . For k = 1 (in (a) and (b), top left) the green class can be seen to wrap around the blue in the training embedding. While this satisfies the 1NCA objective well, it leads to worse generalization, as the boundary between blue and green becomes confused in the test data (bottom left). With larger k , generaliza-tion is improved. The All-trained models promote a larger margin while the Majority-trained models allow for a smaller margin and more dispersion within the clusters. The key point to note is that the All models can be seen as trying to build a larger margin between classes with k =1 being the weakest example of this. Meanwhile the Majority models are given more free-dom to manipulate the projection. We found that this translates to the Majority objective converging more quickly in terms of optimization.
 In the supplementary materials, we present quantita-tive results and an extensive comparison to other dis-tance metric learning methods discussed in Sec. 3 , in-cluding LMNN, MCML, 1NCA, and ITML (when A is full rank since ITML cannot be used to reduce the di-mensionality of the data). In general, the UCI results are more variable, but kNCA compares favorably (ei-ther being the best, or near the best) in all cases. In the experiments on the USPS digits, we evaluate performance of the various algorithms when the data is more di ffi cult and noisy. To study this in a con-trolled manner, we created three variations of the data with increasing levels of corruption in the labels. The first variant is the uncorrupted, original dataset while the others have 25% and 50% of the labels resampled uniformly. To evaluate performance, the votes of the neighbors come from the corrupted data, but we re-port correctness based on the uncorrupted labels. As can be seen by the increasing trend of all the curves in Fig. 4 , using larger k at test time results in better performance. The improvements are steepest in the high noise cases. We also see that the kNCA methods substantially outperform 1-NCA, LMNN, and MCML. Although not reported here for lack of space, the above conclusions hold when comparing kNCA to ITML (in the full dimensional setting). Training accuracies are similar to test accuracies.
 For a final experiment in the supervised setting, we tried to better understand why (a) kNCA with larger k outperforms 1NCA, and (b) why the All-trained models outperformed the Majority-trained models on the USPS data. One hypothesis is that the perfor-mance can be explained in terms of the severity of non-convexity in the objectives: since 1NCA is so nar-rowly focused on its immediate neighborhood, there are many local optima to fall into; and since Majority is forgiving of impure neighborhoods, there are more configurations that it is satisfied with, and thus more local optima. To test this, we took the parameters learned by kNCA, with both the Majority and All ob-jective, and evaluated the 1NCA objective (Eq. 2 ). We repeated this several times across 10 di ff erent folds of the data (with di ff erent random initialization of the pa-rameters for each fold) to measure the variance, which we attribute to reaching di ff erent local optima. Results are shown in Fig. 5 . As hypothesized, the kNCA meth-ods with larger k do actually achieve better 1NCA ob-jectives, and the All training achieves better 1NCA objectives than the Majority training. 7.2. k t -SNE Embeddings We also experimented with k t -SNE. The details for construction of the target distribution p followed the details presented in ( Hinton &amp; Roweis , 2002 ). In Fig. 6 , we show the embeddings that have been learned by both t -SNE and k t -SNE with k = 5 at two points of learning: first, at iteration 25, where clusters are be-ginning to take form; second, at iteration 250, which had reached convergence. Note the global rearrange-ment that occurs even after iteration 25 when k = 5. Quantitatively, in Fig. 6 (e), we use the true labels to measure the leave-one-out accuracy for a kNN classi-fier applied to the points output by t -SNE and k t -SNE. t -SNE performs better on 1-nearest neighbor accuracy, but when k is increased, 5 t -SNE overtakes t -SNE. This further illustrates the myopic nature of using k =1 in t -SNE. In the supplementary material, we provide animations illustrating the evolution of kSNE embed-dings on USPS digits for various choices of k . In these animations, qualitative di ff erences are visible, where t SNE exhibits the myopic behavior illustrated in Fig. 1 . There are several desirable properties of kNCA. First, it provides a proper methodology for doing NCA-like learning when the desire is to use kNN with k&gt; 1 at test time. Our work here derives the NCA-like ob-jective that is properly matched to using kNN at test time. kNN classifiers are ubiquitous, and a choice of k&gt; 1 is nearly always used, so the method has wide applicability. Second, it provides robustness in two ways: first, the majority objective is relatively uncon-cerned with outliers, so long as the majority of neigh-bors in a region have the correct label; second, the objective optimizes an expectation over the selection of all sets of k neighbors, so we do not expect small perturbations in the data to have a significant e ff ect on the learning objective. Robustness is not achieved by the methods we compare to, which we attribute to either their global or non-probabilistic nature. Curiously, in most cases, the All objective outper-formed the Majority objective. The argument can be made that All is like a margin-enhanced version of Majority, which inherits robustness due to the prob-abilistic formulation, but generalizes well due to its margin-enforcing tendencies. We believe this to be an interesting result for those people wishing to design better learning objectives; it challenges the common intuition that the best learning objective is to mini-mize expected loss. However, our final supervised ex-periments suggest that the story may be more com-plicated, and that we might need to find better ways of initializing and optimizing the two methods before having a clear answer.
 One disadvantage of NCA (and thus also kNCA) is the inherently quadratic nature of the algorithm that comes from basing it on pairwise distances. We be-lieve the method to still have desirable properties when the set of potential neighbors is restricted (either ran-domly or deterministically) but a fuller exploration of the tradeo ff s involved require further investigation. Finally, we believe the general technique used to com-pute the expected majority function to be of interest beyond just for kNN classifiers and for kNCA learning. It would be interesting to find further applications.
