 With the rapid development of novel Web applications (e.g. blogs) and users X  acces-sibility to the Internet, opinionated information is turning to be an increasing impor-tant type of online content. Compared with traditional content (e.g. news reports), people can benefit more from such information. Take a major type of opinionated information, i.e., product reviews, for example, consumers can make decision on whether to purchase a product by reading the reviews, while product manufacturers could keep track of their products in order to improve the quality. 
However, the volume of opinionated in formation has recently become extremely large, which consequently prevents people from finding opinions on some entity (i.e. products, events, organizations, etc.) manual ly. Current search engines, like Google, Yahoo!, Live Search, have provided excellent performance in traditional search tasks. Unfortunately, they perform poorly in this novel opinion retrieval task, i.e., retrieving documents which are both relevant to the queries and containing opinions on them. Obviously, the keyword-based matching strategy in those search engines does not take into account the constraint of the occu rrence of opinions. Consequently, lots of researchers have recently conducted intensive work to address this opinion retrieval task [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]. Most previous work proposes solutions following the classical two stage framework, i.e., retrieving topic relevant documents using traditional retrieval models and then re-ranking the candidates according to opinion relevance [3, 4, 5, 6, 7, 8, 9, 10, 11]. There is also another line of recent work to solve this problem using a unified framework [1, 2]. 
One obvious drawback of some work is that as they were designed for the Blog track in TREC (http://trec.nist.gov) [12, 13] (especially those perform well in this track), they have heavy dependence on the unique properties of blogspace. However, these evidences can not be utilized in ordinary Web pages. Another problem of all previous work is that they neglected the problem of domain coherence between topic relevant candidate documents and queries. Problems would rise if we consider any candidate documents which contain opinion words as the opinionated results for a query. There are cases that the documents may contain opinions on topics of other domains rather than the queries themselves. Another example is that for the classical ambiguous query  X  Apple  X , it could be a kind of food or the well-known company, so the expressive opinions of the documents may be any one of these two kinds. Without imposing the domain coherence constraint, documents of different domains will be mixed up. Such results would not satisfy users X  information need consistently. 
In this paper, we propose to take use of only textual characteristics and address the second issue based on the idea of query classification. Specifically, in our work, we do not focus on how to classify queries into different categories but on addressing the mentioned issues using query category information. We make the assumption that the category information is known in advance. In practice, users could specify the cate-gory manually with little additional efforts when issuing the query. In detail, our work falls into the traditional two stage framework and the major difference with previous work is that we propose to re-rank the topic relevant candidates using the opinion similarity measure between the candidate opinion models and corresponding domain opinion models . An opinion model captures the distribution of opinion words in document(s) and the opinion similarity measures the similarity between two opinion has high opinion similarity with a domain opinion model, it is regarded as containing content that is not only opinionated but also coherence with the issued query in do-main category. This idea is based on the observation that the opinion words, which users employ to express their opinions, are domain dependent [14]. For example, in domain of  X  Food  X , opinion words like  X  delicious  X ,  X  yummy  X  would be used at large while in domain of  X  Sports  X , frequent opinion words would be  X  winning  X ,  X  athletic  X . So far as we know, we are the first to concern the domain coherence problem in opin-(i.e., topic relevant retrieval), which turns the issue to a traditional information re-trieval problem. Experiments on the Blog track testing data show that our framework performs comparatively with the state-of-the-art approaches. 
The remainder of this paper is organized as follows. In Section 2, we give an over-view on the related work. In Section 3, we describe the domain taxonomy used in our work. In Section 4 we elaborate our framewor k. The experimental results and analysis are demonstrated in Section 5, and conclusions are drawn in Section 6. Lots of research has been done on opinion retrieval in last few years. The major part of previous work is proposed for the Blog track [3, 4, 5, 6, 7, 8, 9, 10, 11] initiated by TREC. Additionally, there is also another line of work to solve the problem using a general opinion retrieval framework [1, 2]. 
Since 2006, TREC has launched a special track on opinion retrieval in blogspace (i.e., Blog track), which aims to retrieve opinionated information for given queries from Blog data [12, 13]. Lots of organizations have participated in this task, and the prevailing systems adopt the two stage framework. In the first step, topic relevant documents are retrieved using the classical information retrieval models (e.g., TFIDF, Okapi, Language Modeling, etc. [15]) and each document is assigned with a topic relevance score. In the second step, opinion relevant ranking is performed and opinion relevance score is calculated for each document. There are two main strategies for the second step, namely lexicon-based and machine learning approaches. Lexicon-based methods take use of an existing opinion lexicon [3, 4, 5] or the word distribution over rences of the opinion words in the documents, with constraint on the distance of topic words and the opinion words. Machine learning approaches treat the opinion rele-vance identification problem as a classification problem by employing a bunch of features learned from the training data [6, 10, 11]. The final re-ranking of documents is performed through some combination (mostly linear [3, 4, 5]) of the scores of topic relevance and opinion relevance. Our work falls into this category. Due to limited space, we only show some representative work following such framework. 
In [3], Mishne adopted the language modeling based retrieval model to select the topic relevant documents as the candidates, and then employed lexicon-based way to compute the opinion relevance values of those candidates. He also considered the post quality as a component in determining the final ranking scores. In his following work [7], he refined the system and improved the performance by considering the temporal and comment information and enhancing the quality measure according to the query dependent context. Zhang et al. [10] propos ed to use concept-based topic retrieval as the topic retrieval method and built the support vector machine classifier to identify the opinionated documents. The final ranking component utilized the NEAR operator to find the query relevant opinions. In [11], they proposed to improve the system effectiveness by employing techniques like spam filtering. 
In addition to the two stage framework, Zh ang et al. proposed to solve the problem under a unified framework [1]. They started from the classical probabilistic based retrieval model and considered the information need of opinion searchers as a subset of opinionated ones in the original topic relevant documents. They characterized this information need as the implied opinionated expressions towards the query and re-formulated the posterior probability of fitting document to a particular query to the query and the opinionated expressions. In their framework, the final ranking is a quadratic combination of topic and opinion scores. In the work of [2], Eguchi and Lavrenko also proposed a generative model for this task but did not get good results. 
However, all previous work neglected the domain coherence between queries and candidate documents. In cases of ambiguous queries or documents containing opin-and opinion words), the mismatch of domains would result in inconsistent perform-ance. Our work is the first to address such issues in opinion retrieval through the idea tion in ranking, thus our approach can be applied to the general Web opinion retrieval task. As described above, the foundation of our work is the classification of queries into different domains. This classification task is known as the query classification in traditional information retrieval area [16], which can be regarded as a type of short text classification task. However, it is still a challenging problem considering few words in queries. In our work, we focus on the utilization of the domain information rather than the classification task itself. Additionally, the domain information can be easily provided by users in practical appli cations. We argue that users would be will-ing to submit such information in addition to queries in order to refine the search results. Therefore, in following sections we make the reasonable assumption that the domain information of a query is known in advance. The automatic classification task can be investigated in future work. 
Another critical issue with domain classification is the design of a proper domain taxonomy. Some efforts have already been made in the area of query classification. In [16], they use a taxonomy containing 67 hierarchal categories, including  X  Entertain-ment\Movies  X ,  X  Entertainment\TV  X ,  X  Computers\Software  X , etc. Unfortunately, this fine-grained taxonomy is not suitable for our task. For example, in opinionated docu-ments of  X  Entertainment\Movies  X  and  X  Entertainment\TV  X , the opinion words may have high overlap. Our taxonomy is aimed to discriminate the usage of opinion words of different categories. Consequently, in this paper, we define a new coarse-grained taxonomy for our task by investigating the information need of opinion searchers. Table 1 shows the details. 
As can be seen, we only define totally six categories in our taxonomy. One important reason of using such small size taxonomy is to minimize users X  efforts when providing category information for the query. This kind of taxonomy also makes the future auto-extremely hard using fine-grained taxonomies. We will show the wide coverage of this taxonomy in a publicly available collection of queries for opinion retrieval. Given the taxonomy defined above, user queries are assigned with corresponding categories (which are provided by users in current work). In other words, our opinion retrieval system ( D omain S pecific Op inion R etrieval, DS-OpR ) has queries and their category information as input. In following sections, we first give an overview of our retrieval system, and then elaborate the critical components in detail. 4.1 Overview of the DS-OpR System As stated before, our retrieval system falls into the classical two stage opinion re-trieval framework, which typically contains a classical task of retrieving topic rele-vant documents and a re-rank module to sort the document candidates considering the opinion relevance. In this way, the resulted documents are both topic and opinion relevant to the queries. Figure 1 illustrates the framework of our retrieval system DS-OpR in detail. 
In the framework, given the query Q and its category C , we first employ a classical returned documents { d i } are those matching the keywords in Q and each document d i is assigned with a topic relevance score TScore i . Several retrieval models are available for this stage, such as TFIDF, Okapi, La nguage Modeling, etc. One main advantage model as long as the returned documents are assigned with TScore s. 
In our second stage OIR , we first construct the opinion model O i of each candidate indexing phase for TIR .), and then compute the opinion relevance score OScore i for d by measuring the opinion similarity OS i of O i and the domain opinion model of category C Dom-O C . Note that the domain models are computed offline in advance using a collection of documents of the corresponding domain and the opinion lexicon. Specifically, an opinion model is represented as a probability distribution of opinion words and thus the opinion similarity is measured between two distributions. 
To generate the final ranking list, TScore i and OScore i of the document d i are com-bined in some way. Most previous work adopts the linear strategy, while Zhang et al. take a quadratic combination [1]. In our work, we follow the first combination strat-egy as shown in equation 1. Alpha balances the importance of OScore and TScore . Documents are then ranked by the final combination Score . From the above description of our system, we can see that the most critical compo-nents in the framework are (1) Opinion Modeling for domains and documents; (2) Opinion Similarity measure between opinion models. In next section, we describe the details of these two components. 4.2 Opinion Modeling and Opinion Similarity Measure So far, no previous work in opinion retrieval has considered the opinion word distri-bution in computing the opinion relevance scores. We propose that the distribution is helpful in both measuring the opinion relevance scores and discriminating the domain ambiguity of queries (recall the example queries  X  Apple  X ). In this work, we call a named as opinion modeling. The main intuition of this idea is that opinionated docu-ments can be regarded as a mixture of opinion models and topic models (i.e., distribu-tion of topic relevant words other than opinion ones), which is enlightened by the work of topic modeling [17, 18]. Formally, we define opinion model as follows. Definition (Opinion Model): An opinion model O for a collection of documents { d m } or a single document d m is a probability distribution {P( w i | O )} of opinion words { w i } representing either positive or negative polarities. These opinion words { w i } are taken from an existing opinion lexicon. The construc-opinion mining and there are several publicly available lexica, such as General In-quirer [19], SentiWordNet [20], etc. When separately applying opinion modeling to { d m } and d m , we have two specific kinds of opinion models correspondingly. Definition (Domain Opinion Model): A domain opinion model Dom-O k for domain D is a general opinion model for documents of that domain. In other words, it captures the probability distribution {P( w i | D k )} of opinion words { w i } in those documents. In this paper, we totally construct six do main opinion models according to the catego-ries defined in our taxonomy. Clearly, the construction of a domain opinion model demands a collection of documents belonging to the corresponding domain. Definition (Document Opinion Model): Similar to domain opinion model, a docu-ment opinion model Doc-O m is a specific opinion model for a document d m , repre-sented as a probability distribution {P( w i | d m )} of opinion words { w i }. Specifically, we compute P( w i | O ) ( O can be D k or d m ) using following formula: where TF ( w i , O ) is the frequency of w i in O and (, ) words (including both opinion and non-opinion ones) in O . 
According to our definitions, for different domains, we have different domain opinion models, which means that the distributions of opinion words are different. This is quite natural in opinion expressions of different domains, i.e., people choose different words to express their attitudes towards topics of different domains. For example, considering domains of  X  Food and Health  X  and  X  Entertainment  X , people would choose  X  yummy  X  to express their satisfaction with some food, while using  X  moving  X  for some movie. Therefore, if the document opinion model of an opinion-ated document is similar to a domain opinion model, it indicates that there is high possibility that the opinionated document belongs to that domain. In this way, we can guarantee the domain coherence of the returned opinionated documents with the is-sued query with designated domain. From this perspective, we argue that previous work can be regarded as using a universal opinion model to cover opinions of all domains, which is not reasonable according to our analysis. 
As opinion models are represented as probability distributions of opinion words, we compute the similarity between two opinion models based on the well known probability distribution similarity measure Kullback-Leibler (KL) divergence [21]. similarity is measured using following formula: KL O O &amp; is defined similarly as equation 4. KL divergence measures the similarity asymmetrically; we take the average value to make it symmetric. In our experiments, we use the Blog track data set in TREC as the testing data as previous work. Details of the data set can be found in [12]. The 100 topics in the 2006 Blog track (Blog06) and the 2007 Blog track (Blog07) (topics 851-900 in Blog06 and topics 901-950 in Blog07) are evaluated. We only take the titles of topics as queries as in [1]. This strategy also suits the scenery of traditional search engines in which the issued queries are always composed of several keywords. The opinion lexicon used in our experiments is provided by Hu and Liu [22], which comprises of 1752 opinion words including 1098 negative ones and 654 positive ones. For the first stage of our framework (i.e., topic relevance retrieval) , we employ Lemur (www.lemurproject.org) as the retrieval tool and use the simple retrieval model TFIDF. Top 1000 documents are kept as the candidates for re-ranking as in previous work. 
To construct the domain opinion model for each category in our taxonomy, we use the TREC labeled results of the queries other than the testing ones as the documents Blog07 and Blog08 for model construction. Each of the 150 queries is assigned with one of the six categories and thus the labeled document results of a query can be cate-gorized into corresponding domain. Note that the 50 topics in the 2008 Blog track (Blog08) are only utilized for the construction of domain opinion models. 
The system performance is evaluated using the traditional evaluation metrics, i.e., precision in top 10 results (P@10), mean average precision (MAP) and R-precision (R-Prec). All these results are computed using the program in Lemur. In following Blog06, Blog07 and Blog08, and then demonstrate the detailed evaluation results of our system. 5.1 Taxonomy Examination The domain taxonomy is fundamental in our work. To validate the coverage of the taxonomy, we conduct an examination on the category distribution of Blog06, Blog07 and Blog08 queries (i.e., titles of the topics). The 150 queries are analyzed by annota-tors and the final category of a query is determined by voting. Note that the examina-tion should have been carried out in a large scale query collection for justified results. However, it is almost impossible for us to collect large number of such queries that users issue for opinionated information. Additionally, till now, there is no such query log publicly available. We believe that the results of the Blog track data from TREC would be capable of reflecting the ground truth to some extent. Figure 2 shows the detailed distribution of the queries among different domains. From the results we can see that majority queries (90%) can be categorized into one ment  X  and  X  Sports  X . The results indicate that our definition of the domain taxonomy is reasonable. Among these five ones, queries of  X  Society  X  make up the large percentage (29.3%) and then those of  X  Technology  X  and  X  Entertainment  X . This observation shows that when users are seeking opinions, they are mostly concerned with popular topics like politics, digital products and movies. This conclusion is quite obvious in real life. When checking the queries of the  X  Others  X  category, we find extremely ambiguous queries such as  X  brrreeeport  X  of Topic No. 907 in Blog07, which are hard to be assigned to any one of the five categories. 5.2 Performance Evaluation In our final combination of topic relevance and opinion relevance scores, we adopt the linear way. The optimal value of the weight parameter alpha is tuned empirically. Hence, we give the results of using different values for alpha first. 5.2.1 Parameter Selection We conduct the parameter selection experiments on queries of Blog06. Totally, we conduct 11 comparative experiments using different values of alpha ranging from 0 to 1 with a step length of 0.1. Furthermore, we argue that alpha may take different values for queries of different domains, which means that the opinion relevance plays different roles in different domains for the final ranking . Hence, we perform the same experiments for queries of each of the six domains individually. Figures 3 and 4 show the results of MAP and P@10 respectively. From the results we can see that queries belonging to  X  Society  X  reach the best P@10 and MAP when alpha is set to 0.2; while queries of the other five categories get their best performance when alpha takes the value of 0.6. The results show that we should impose less weight on the opinion relevance score for queries of  X  Soci-ety  X . One possible reason might be that  X  Society  X  includes topics about politics, policies, etc., whose topic relevant docum ents are mostly already opinionated in blogspace. Therefore, it makes sense to lower the weight of opinion relevance scores, because once the documents are retrieved as topic relevant, they are more likely to be opinionated already. However, documents of the other five categories are relatively objective in blogspace; therefore the weight of opinion relevance scores should be enhanced. 5.2.2 Evaluation Results and Discussions We take the optimal values of alpha from above parameter tuning for the performance comparison of our system with others. Table 2 demonstrates the results of the best title-run at Blog tracks, Zhang et al. ( Zhang ) [1] and ours ( DS-OpR ). From the re-sults, we can see that our approach outperforms the other approaches in P@10 in the Blog06 topics and has close result to the best one in Blog07. In the MAP measure, we outperform the best title-run and have close result to Zhang in Blog06. Unfortunately, we have relatively poor MAP in Blog07 compared to others, although not far from Zhang . However, as the retrieval performance of the opinion retrieval task is strongly dominated by the performance of the underlying topic relevance task [12], we argue that our approach still has large room for improvement considering the na X ve retrieval model employed in current experiments. Another reason is that we currently only take use of one simple opinion lexicon. The ex periments of Zhang et al. [1] show that different opinion lexica would affect the final performance. In all, the current results show that our approach of considering the domain coherence of queries and docu-ments is promising and comparative to others in some measures. 
Figure 5 illustrates the effect of opinion modeling re-ranking per topic in P@10. The results show that 22 out of 100 queries get improvement by over 50%, including 11 queries with improvement over 200%. Only 7 out of 100 get adverse effects. It indicates that the re-ranking method is helpful. Specific examples are also given in Table 3. These two queries demonstrate the effectiveness of our approach in re-ranking. On one hand, it proves the correctness of our re-ranking method; on the other hand, it indicates the po-tential of getting a better topic relevance retrieval model as well. problem; however none of them considers the domain coherence between topic relevant documents and queries. In this paper, we follow the classical two stage framework in opinion retrieval and propose to compute the opinion relevance scores according to the opinion similarity between the opinion models of candidate documents and correspond-ing domain opinion model of the query. High opinion similarity means that the docu-ments are both opinionated and coherent with the query from the perspective of domain set, and the results show that our framework performs comparatively with state-of-the-art methods. Acknowledgments. This work is supported by National Key Technology R&amp;D Program(2008BAH26B00 &amp; 2007BAH11B06). 
