 Huimin Zhao Abstract In real-world classification problems, different types of misclassification errors often have asymmetric costs, thus demanding cost-sensitive learning methods that attempt to minimize average misclassification cost rather than plain error rate. Instance weighting and post hoc threshold adjusting are two major approaches to cost-sensitive classifier learning. This paper compares the effects of these two approaches on several standard, off-the-shelf classification methods. The comparison indicates that the two approaches lead to similar results for some classification methods, such as Na X ve Bayes, logistic regression, and back-propagation neural network, but very different results for other methods, such as decision tree, decision table, and decision rule learners. The findings from this research have important implications on the selection of the cost-sensitive classifier learning approach as well as on the interpretation of a recently published finding about the relative performance of Na X ve Bayes and decision trees.
 Keywords Classification  X  Cost-sensitive learning  X  Instance weighting  X  Threshold adjusting  X  ROC curve 1 Introduction Classification is an important type of predictive data mining problem, where the objective is to predict the value of a categorical-dependent variable based on the values of several independent variables. Real-world classification problems are often cost-sensitive , meaning that different types of misclassification errors should incur different costs. For example, in bank loan application approval, approving a bad loan is much more costly than rejecting a potentially good loan [ 33 ], while in cancer diagnosis [ 7 ], failing to diagnose a cancer case has dire consequences and is far more costly than misidentifying an actually negative case.
While most basic classification methods have not been designed to solve cost-sensitive problems, two major approaches, instance weighting [ 8 , 26 , 31 , 34 ] and post hoc threshold adjusting [ 1 , 30 ], have been proposed for converting these methods into cost-sensitive clas-sifier learners. In instance weighting, different types of instances in the training dataset are weighted according to the misclassification costs during classifier learning, such that the classifier strives to make fewer errors of the more costly type, resulting in lower overall cost. In post hoc threshold adjusting, a classifier is first learned as usual without tempering the training data and then adjusted to account for the costs. Apparently, instance weighting is computationally more involved than post hoc threshold adjusting. The difference is especially considerable when the decision maker cannot precisely pinpoint the costs of different types of misclassification errors and has to consider a range of possible cost settings. To threshold adjusting, changing the cost setting simply involves an adjustment of the decision threshold and does not require retraining the classifier. To instance weighting, however, the classifier needs to be retrained from scratch whenever a new cost setting needs to be considered.
The objective of this research is to compare the effects of the two cost-sensitive learn-ing approaches on various standard, off-the-shelf base classification methods. Findings from such a comparative study will have important implications on the selection of the cost-sen-sitive learning approach with regard to a particular base classification method. If a base classification method is insensitive to the cost ratio when instance weighting is applied, the computationally cheaper post hoc threshold adjusting approach is appropriate. However, if a base classification method is very sensitive to the cost ratio when instance weighting is used, it becomes very important to incorporate the misclassification costs during training. This study will also help in interpreting a recently published finding about the relative perfor-mance of Na X ve Bayes and decision trees [ 19 ]: while the two methods often achieve similar cost-insensitive performance (i.e., classification accuracy), Na X ve Bayes tends to significantly outperform decision trees in terms of another aggregate performance measure, AUC (area under the ROC curve) [ 3 ]. We will further investigate whether decision tree learning is indeed inferior to Na X ve Bayes in dealing with cost-sensitive problems.

The remainder of the paper is organized as follows. We first briefly review some re-lated work in the next section. We then discuss cost-sensitive learning for several standard, off-the-shelf classification methods in Sect. 3 . We then report on some empirical results in Sect. 4 . Finally, we conclude the paper and discuss future research directions in Sect. 5 . 2 Related work Cost-sensitive classification is an active research area in data mining. Without considering costs, classification accuracy (the probability of making a correct prediction) is a reason-able measure for assessing the performance of a classifier. In cost-sensitive problems, the performance of a classifier should be assessed by the expected misclassification cost rather than accuracy [ 12 , 27 ]. A closely related problem is the classification of imbalanced datasets, where different classes are not approximately equally represented [ 5 , 20 ]. Real-world imbal-anced, cost-sensitive classification problems that have been studied in the literature include bank loan approval [ 30 ], financial distress prediction [ 25 ], credit card fraud detection [ 4 ], and unsolicited bulk email categorization [ 17 ], just to name a few.

Several methods have been proposed for dealing with cost-sensitive classification problems. Some methods extend particular cost-insensitive classification methods. Drummond and Holte [ 10 ] proposed methods for incorporating misclassification costs into decision tree learning. The Iterative Bayes method [ 15 ] turns Na X ve Bayes cost-sensitive by taking misclassification costs into account. The AdaCost algorithm [ 13 ] modifies the AdaBoost algorithm into a cost-sensitive boosting method.

While the above-mentioned methods extend particular classification methods, instance weighting [ 8 , 26 , 31 , 34 ] and post hoc threshold adjusting [ 1 , 30 ] are generic approaches to converting any classification method into a cost-sensitive one. Instance weighting has similar effects as instance re-sampling (over-sampling or under-sampling) [ 5 ]. Post hoc threshold adjusting mainly relies on the posterior class probabilities estimated by the underlying clas-sification method. This research compares these two approaches in relation to several base classification methods. 3 Cost-sensitive classifier learning To simplify the discussion, we consider binary classification problems, where the objective is to predict the value of a binary-dependent variable y , referred to as the class, based on a m is the number of attributes and x i is a scalar. A problem instance is considered positive (negative) if its class y = 1 ( 0 ) . A classification algorithm learns a prediction model, known as a classifier, from a training dataset consisting of previously solved problem instances, whose class values are known. The classifier is then applied to make classification decisions on other problem instances. A classification problem is cost-sensitive if the costs of the two types of misclassification errors (false positive and false negative) are different. 3.1 Na X ve Bayes Without considering costs, a rational classifier should estimate the odds ratio, ( y = 0 | x = x ) , and make a positive (negative) prediction if the odds ratio is above (below) one [ 32 ]. Assuming conditional independence of the attributes given a class value, Na X ve Bayes [ 9 , 32 , 35 ] estimates the odds ratio as O ( x = x ) = P where p 1 ( p 0 ) is the prior probability that a problem instance is positive (negative). This leads to the following condition for making a positive prediction:
When costs are considered, the optimal condition minimizing the expected misclassifi-cation cost has the right-hand side of ( 2 ) adjusted to p 0 C 10 p false positive (negative) error. A typical instance weighting scheme is to modify the weight of an instance proportional to the cost of misclassifying the actual class of the instance [ 31 ]. If the (prior and conditional) probabilities in the Na X ve Bayes classifier are directly esti-mated as sample proportions based on the training data set, it can be easily shown that such instance weighting using the cost-insensitive classification condition ( 2 ) leads to exactly the same condition as threshold adjusting does. The estimates of the conditional probabilities P ( x i = x i | y = j )( i = 1 , 2 ,..., m ; j = 0 , 1 ) are invariant under instance weighting. Let n 1 ( n 0 ) denote the number of positive (negative) instances in the training dataset, the right-hand side of ( 2 ) becomes 3.2 Discriminant analysis, logistic regression, and neural network It has also been a common practice in applications of statistical multivariate analysis techniques, such as linear discriminant analysis (LDA) and logistic regression [ 1 , 24 ], to adjust classification models post hoc based on prior probabilities and costs. For example, LDA adjusts the decision threshold on the linear discriminant function by ln p 0 C 10 p
Assuming that the independent variables follow a multivariate normal distribution for each of the two classes and that the two classes have equal dispersion and covariance struc-tures, LDA computes a linear discriminate function Z = m i = 1  X  i x i and makes a positive prediction if imizes the classification error rate, if the assumptions hold. The posterior probability that a problem instance belongs to the positive class is The odds ratio is The optimal condition for making a positive prediction in ( 4 ) is equivalent to
When asymmetric prior probabilities and costs are considered, the dividing point C should be chosen to minimize the expected misclassification cost and becomes This is equivalent to adjusting the threshold on the odds ratio in ( 7 )by p 0 C 10 p
Logistic regression models can be adjusted in a similar manner. We also posit that instance weighting has relatively small effects on these as well as other methods, such as neural net-works [ 29 ], which produce smooth continuous probability estimates and thus allow fine adjusting of the threshold on the odds ratio. A common characteristic of these methods is learned by these methods is fixed and only the parameters (mainly the threshold on the odds ratio) may change when the cost ratio varies. 3.3 Symbolic classifiers However, symbolic classifiers, such as decision trees [ 28 ], decision rules [ 6 ], and decision ta-bles [ 23 ], are very different. The structure of a symbolic classifier may experience substantial changes, with different sets of attributes being selected, when the cost ratio varies. For exam-ple, the heuristic goodness measure for selecting the branching attribute at an intermediate node of decision trees (e.g., information gain and gain ratio used in C4.5 [ 28 ]) does not preserve the ranking among the attributes X  X hus different attributes may be selected X  X nder different cost ratios. Previous experiments have shown that very different decision trees are learned via instance weighting under different cost ratios [ 31 ] (see examples later). In addi-tion, since only a small number (proportional to the number of tree leaves, rules, or selected attributes in a decision table) of thresholds are possible, threshold adjusting on symbolic classifiers can not be precise. We therefore expect that instance weighting has relatively large effects on symbolic classifiers. 4 Visualization using ROC curves The performance of a binary classifier can be visualized using an ROC (receiver operating characteristics) curve [ 14 ] or an alternative, such as cost curve [ 11 ]. Based on the odds ratio estimated by a classifier, a binary classification decision can be made by choosing a threshold on the odds ratio. Two quantities, sensitivity and specificity , can then be estimated. Sensitiv-ity (specificity) is defined as the proportion of positive (negative) instances that are correctly classified by the classifier. The ROC curve plots sensitivity against 1  X  specificity on a series of possible thresholds (it is only a convention to use 1  X  specificity, rather than specificity, as the horizontal axis). The threshold adjusting approach can be viewed as selecting an appro-priate point on the ROC curve that pertains to a cost ratio of 1. The area under the ROC curve, or simply AUC, is an aggregate performance measure over all possible thresholds, or equivalently, points on the ROC curve [ 3 ].

We have empirically evaluated the effects of instance weighting on the ROC curves produced by various classification methods. We used 13 binary classification datasets col-lected in the University of California, Irvine machine learning repository [ 2 ], including Pima Indians diabetes, ionosphere, hepatitis, horse colic, breast cancer, Wisconsin breast can-cer, chess king-rook vs. king-pawn, credit card application approval, congressional voting records, labor relations, sonar, German credit (Statlog project), and heart disease (Statlog project).
 We used the Weka machine learning toolkit [ 33 ] for the evaluation purpose. We used the CostSensitiveClassifier meta learner [ 33 ], which converts a base learner into a cost-sensitive learner by weighting training instances according to a given cost matrix, reflecting the ratio between the costs of the two types of misclassification errors. The base learners we used include Na X ve Bayes [ 9 ], logistic regression [ 18 ], backpropagation neural network [ 29 ], C4.5 decision tree learner (named J4.8 in Weka) [ 28 ], Ripper decision rule learner (named JRip in Weka) [ 6 ], and a decision table learner [ 23 ]. We kept the default parameter values for all methods. We did not use LDA, as it relies on the usually unrealistic assumption of multi-variate normality and is recommended for use only when logistic regression software is not available, and then only in preliminary analyzes [ 18 ]. We evaluated the classification methods under three cost ratios, 1:5, 1:1, and 5:1.

We used tenfold cross-validation [ 22 ] for estimating the ROC curves. Cross-validation ran-domly divides a dataset into several approximately equal-sized, stratified subsets, called folds, and repeatedly uses each fold for performance testing while the other folds are used for train-ing a classifier. To obtain even smoother ROC curves, we repeated tenfold cross-validation twenty times and pooled the probability estimations on the 200 testing folds.
Figures 1 , 2 , 3 , 4 , 5 show the ROC curves produced by the six base classification methods under the three cost ratios for five datasets: Pima Indians diabetes, ionosphere, hepatitis, horse colic, and heart disease (Statlog project). The results on other datasets (available from the author) show similar patterns and hence are not reported, for the interest of space. We observe several interesting patterns in the ROC curves for the five datasets. First, the ROC curves produced by Na X ve Bayes remain almost the same under different cost ratios, indicating that instance weighting has little effect beyond threshold adjusting. Second, the ROC curves produced by logistic regression and neural network change only slightly across different cost ratios. The benefit of instance weighting beyond that of threshold adjusting is therefore relatively small. Third, decision tree, decision rule, and decision table learners produce very different ROC curves under different cost ratios. For a given classifier/dataset, none of the ROC curves dominates the other two and each curve is superior in a particular section. This implies that different classifiers with substantially different structures are learned to fit differ-ent cost settings. Each classifier may excel in a particular range around the cost ratio under which it is trained. It is therefore important to weight the training instances according to the cost ratio when training decision trees, rules, and tables, as simply adjusting the threshold on the odds ratio (equivalent to selecting a particular point on the ROC curve) may result in sub-optimal performance. Figure 6 shows the decision trees learned under the three cost ratios for the Pima Indians diabetes dataset. The structures of these decision trees are very different. In summary, the post hoc threshold adjusting approach, which is computationally cheaper than instance weighting, is appropriate for Na X ve Bayes, logistic regression, and backpropagation neural network, but not for decision tree, decision table, and decision rule learners.

A recent study has found that Na X ve Bayes significantly outperforms decision trees in terms of AUC [ 19 ]. However, this should not be interpreted as that the former has better capability than the later in cost-sensitive learning. When the cost ratio is given, a cost-sensitive decision tree trained using instance weighting may outperform or compete well against Na X ve Bayes. AUC is an aggregate measure over all possible cost ratios. It adequately reflects the capability of Na X ve Bayes, whose ROC curve does not change across cost ratios, but not that of decision trees. It is indeed possible to use combinations of classifiers to operate on the convex hull of a set of ROC curves generated under various cost ratios [ 33 ]. The full capacity of decision trees is defined by the area under this convex hull, rather than the area under a single ROC curve. 5Conclusion We have compared two major approaches to cost-sensitive classifier learning in this paper. Our analysis resulted in some interesting findings. Some classification methods that produce continuous probability estimates, such as Na X ve Bayes, logistic regression, and backprop-agation neural network, are relatively insensitive to the cost ratio when instance weighting is applied. On the other hand, symbolic classification methods, such as decision tree, deci-sion rule, and decision table learners, are very sensitive to the cost ratio, meaning that the structures of the learned classifiers may change substantially as the cost ratio changes. This information helps in selecting the appropriate cost-sensitive classifier learning approach with regard to a base classification method.

There are several limitations in this paper, which can be addressed in future research. First, while we have evaluated six classification methods using 13 UCI datasets under three cost ratios, more extensive evaluations using more methods, larger real-world datasets, and more extreme cost ratios can be conducted to validate the generalizability of our findings. Future research may also carry out a more thorough analytical explanation for the difference in the effects of the two cost-sensitive learning approaches in relation to base classification methods. Second, while we have only investigated binary classification problems, the evaluation needs to be generalized to multiple-class problems [ 16 ]. Third, while we have compared several base classification methods, future research may further compare ensemble classification methods that combine multiple base classification methods via bagging, boosting, stacking, and cascading [ 21 , 33 , 36  X  38 ].
 References Author X  X  biography
