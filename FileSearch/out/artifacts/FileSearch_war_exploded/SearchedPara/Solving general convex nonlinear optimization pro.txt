 Alireza Nazemi n 1. Introduction minimize f  X  x  X  X  1  X  subject to g  X  x  X  r 0 ,  X  2  X  .

Throughout this paper, we assume that (1) X (3) has a finite-1976, p. 97 ), i.e., there exists a x 0 A R n such that g  X  x 0  X  o 0 , Ax 0 b  X  0 : blems, such as optimal control, structure design, mechanical applications, real-time online solutions of the GCNLP problems are often desired. Various numerical procedures have been pre-Fabien, 1999 ; Bertsekas, 1989 ; Boyd and Vandenberghe, 2004 ; 1996 , and the references therein). Since the computing time required for solving nonlinear optimization greatly depends on
Kalouptisidis, 1997 ). One promising approach to handle online applications is to employ recurrent neural networks based on circuit implementation. The main idea of the neural network approach for optimization is to construct a nonnegative energy function and establish a dynamic system that represents an the underlying optimization problem, starting from an initial point. An important requirement is that the energy function decreases monotonically as the dynamic system approaches an equilibrium point. Because of the dynamic nature and the potential of electronic implementation, neural networks can be implemented physically by designated hardware such as network approach can solve optimization problems in running time at the orders of magnitude much faster than the most popular optimization algorithms executed on general purpose problem (1) X (3) .

The neural network for solving mathematical programming problems was first proposed by Tank and Hopfield (1986) . Kennedy and Chua (1988) proposed an improved model which employs both gradient method and penalty function method for solving nonlinear programming problems. To avoid penalty para-meters, Rodriguez-Vazquez et al. (1990) proposed switched-capacitor neural network for solving a class of optimization problems. However, the right side of the system in Rodriguez-gence, they must assume that the feasible region O of the problem is bounded and that the interior set O J of O is none-mpty. In order to solve the equilibrium models arising in the proposed the globally projected dynamic systems, which can be viewed as a neural network model suitable for hardware imple-mentation. However, for some convex programming problems, Wang, 2000a ). Since then, neural networks for solving various optimization problems have been rather extensively studied and some important results have also been obtained ( Anguita and 2007 , 2008 ; Yang and Xu, 2007 ; Yang and Cao, 2008 ). For and Wang (2005 , 2004b) proposed neural networks for nonlinear convex programming problems based on the primal-dual method and projection method. Based on the idea of successive approx-model for solving convex nonlinear programming problems by using the gradient method. By modifying the multipliers asso-programming problem without nonnegative constraints of the multipliers associated with inequality constraints. With this network model for solving convex optimization problems and its Bian (2008) proposed a generalized neural network for solving general nonsmooth nonlinear programming problems based on the differential inclusion theory.

Motivated by the above discussions, in the present paper, a neural network for solving the GCNLP problem is presented. Based on the Saddle theorem, the equilibrium point of the proposed neural network is proved to be equivalent to the KKT point of the GCNLP problem. The existence and uniqueness of an equilibrium point of the proposed neural network are also network is obtained. This neural network model has been also successfully used for solving the minimax problems ( Nazemi, 2011 ), maximum flow problem ( Nazemi and Omidi, 2012 ) and shortest path problem ( Nazemi and Omidi, 2013 ).
In what follows, some necessary preliminaries and the net-arguments are given. The paper is then concluded with a summary in Section 5 . 2. A neural network model
In this section, using standard optimization techniques, we transform (1) X (3) into a nonlinear dynamical system. First, we introduce a few notations, definitions and one lemma. Through-vectors and T denotes the transpose. In what follows, J J l able mapping F  X  X  F 1 , ... , F m  X  T : R n -R m , r F  X  X  r F  X  x  X  A R n m , denotes the transposed Jacobian of F at x .If A A denoted by A : j .
 x , y A J F  X  x  X  F  X  y  X  J r L J x y J :
R has a neighborhood D 0 R n such that the above inequality holds for each pair of points x , y A D 0 .

Definition 2.2. A mapping F : R n -R n is monotone if  X  x y  X  T  X  F  X  x  X  F  X  y  X  X  Z 0 , 8 x , y A R n : whenever x a y .

Lemma 2.3 ( Ortega and Rheinboldt, 1970 ). If a mapping F is all x A O .
 equation. The solution trajectory of the system is said to be globally convergent to the set X n ,if x ( t ) satisfies lim dist  X  x  X  t  X  , X n  X  X  0 , at x n in the sense of Lyapunov.

Tucker (KKT) system:
Z 0 , g  X  x n  X  r 0 , u n T g  X  x n  X  X  0 , r f  X  x n  X  X  r g  X  x n  X  T u n  X  r h  X  x n  X  T v n  X  0 , h  X  x n  X  X  0 : 8 &gt; &lt; &gt; : x n is called a KKT point of (1) X (3) and a pair  X  u n T , v n and g of (1) X (3) , if and only if x n is a KKT point of (1) X (3) . dx dt  X  X  r f  X  x  X  X  r g  X  x  X  du dt  X  X  u  X  g  X  x  X  X  dv dt  X  h  X  x  X  ,  X  7  X   X  u  X  g  X  x  X  X   X   X  X  X  u 1  X  g 1  X  x  X   X  ,  X  u 2  X  g 2  X  x  X  To simply the discussion, we denote y  X  X  x T , u T , v T  X  , c  X  y  X  X 
Thus neural network (5) X (7) can be written as dy dt  X  t c  X  y  X  ,  X  9  X  y  X  t 0  X  X  y 0 ,  X  10  X  can be implemented on hardware is provided in Fig. 1 . following problem: minimize f  X  x  X  X  11  X  subject to g  X  x  X  r 0 , x A O ,  X  12  X  (11) and (12) is given as dx dt  X  X  x P O  X  x r f  X  x  X  r g  X  x  X  T u  X  X  ,  X  13  X  du dt  X  X  u  X  u  X  g  X  x  X  X  tion operator ( Gao, 2004 ) defined by P  X  x  X  X  argmin model (13) and (14) for a monotone and a symmetric mapping could not be guaranteed ( Xia and Wang, 2000a , 2000b ; Gao, problem (11) and (12) in some cases. For instance, one can see and Examples 4.1 and 4.2 in this paper and Example 5.1 in Nazemi and Omidi (2012) .

There is another kind of the neural network model called gradient model. In order to use the gradient neural network model, a constrained optimization problem can be approximated by an unconstrained optimization problem. Then the energy function is constructed by the penalty function method. It is noticeable that the gradient neural network model has an advantage as the model may be defined directly using the the convergence is not guaranteed, especially in the case of unbounded solution sets ( Yang and Cao, 2008 ). Moreover, the gradient neural network based on the penalty function requires any adjustable parameter called the penalty parameter. For tion problem (1) X (3) can be approximated by the following unconstrained optimization problem: minimize E 1  X  x  X  X  f  X  x  X  X  g 2 where g is a penalty parameter. The gradient neural network model is then given by dx dt The system in (15) is referred to as Kennedy and Chua X  X  (1988) neural network model. This network is not capable to find an approximate solution of (1) X (3) for any given finite penalty parameter. It can be also shown that the Kennedy X  X hua X  X  neural network (15) is not globally convergent to an exact optimal solution of some convex programming problems. For instance, Nazemi and Omidi (2012) .

Under the condition that f ( x ) is strictly convex and g for solving (1) X (3) as dx dt  X  r f  X  x  X  X  du dt  X  diag  X  u 1 , ... , u m  X  g  X  x  X  ,  X  17  X  dv dt  X  h  X  x  X  ,  X  18  X  with an initial point  X  x T 0 , u T 0 , v T 0  X  T and u k However, this model cannot solve some convex optimization problems. One can see Example 4.1 for more clarification. In comparison with the above models, the proposed neural network in (9) and (10) will be shown to converge globally to an exact optimal solution of (1) X (3) . 3. Stability and converge analysis network (9) and (10) are discussed.
 problem (1) X (3) , then there exist u n A R m and v n A R l network (9) and (10) .
 It follows easily that r f  X  x n  X  X  r g  X  x n  X  T  X  u n  X  g  X  x n  X  X   X   X  r h  X  x n  X   X  u n  X  g  X  x n  X  X   X   X  u n ,  X  20  X  h  X  x n  X  X  0 :  X  21  X  It is also clear that u n  X  X  u n  X  g  X  x n  X  X   X  if and only if Z 0 , g  X  x n  X  r 0 , u n T g  X  x n  X  X  0 :  X  22  X 
Moreover, substituting (20) into (19) we have conditions. The converse is straightforward. &amp; there exists a unique continuous solution y  X  t  X  X  X  x  X  t  X  for system (9) and (10) .
 are continuously differentiable on an open set D D R n  X  m  X  l (9) and (10) has a unique continuous solution y  X  t  X  , t some Z 4 t 0 ,as Z -1 . &amp; (8) is negative semidefinite matrix .
 such that  X  u  X  g  X   X   X  u 1  X  g 1  X  x  X  , u 2  X  g 2  X  x  X  , ... , u
With a simple calculation, it is clearly shown that r c  X  y  X  X  where r g  X  x  X  X  g  X  x  X  , k  X  1 , 2 , ... , p is given by g  X  x  X  X  and
S  X  f , g , ... , g m are assumed to be convex and twice differentiable; g matrix. From the stated observations, we can derive that the Jacobian matrix r c  X  y  X  is a negative semidefinite matrix. we attain r c  X  y  X  X  that r c  X  y  X  is a negative semidefinite matrix.
 r c  X  y  X  X  semidefinite matrix. This completes the proof. &amp; Proof. Consider the following Lyapunov function: E  X  y  X  X  J c  X  y  X  J 2  X  1 2 J y y n J 2 :  X  24  X 
From (8) , we have d c dt  X 
Then dE  X  y  X  t  X  X 
Employing Lemma 3.3 , we achieve
Moreover, from Definition 2.2 and Lemma 2.3 , we have  X  y y n  X  T  X  c  X  y  X  c  X  y n  X  X  X  X  y y n  X  T c  X  y  X  r 0 , 8 y
Thus dE  X  y  X  t  X  X  the sense of Lyapunov. Next since,
E  X  y  X  Z 1 2 J y y n J 2  X  28  X  there exists a convergent subsequence such that lim k -1  X  x  X  t k  X  T , u  X  t k  X  T , v  X  t  X  x T , u T , v T  X  T satisfies dE  X  y  X  t  X  X  largest invariant set in
K  X  X  x  X  t  X  T , u  X  t  X  T , v  X  t  X  T  X  T dE  X  y  X  t  X  X  dt dv = dt  X  0 3 dE  X  y  X  t  X  X  = dt  X  0. Thus  X  x T , u T , v Lyapunov function E  X  y  X  X  J c  X  y  X  J 2  X  1 2 J y y J 2 :  X  29  X  lim therefore lim k -1 E  X  x  X  t k  X  T , u  X  t k  X  T , v  X  t for t Z t q ,
J y  X  t  X  y J 2 r E  X  y  X  t  X  X  r E :
It follows that lim t -1 J y  X  t  X  y J  X  0 and lim t -1 optimal solution of (1) X (3) . &amp;
As an immediate corollary of Theorem 3.4 , we can get the following result.
 4. Simulation examples
In order to demonstrate the effectiveness of the proposed neural network (9) and (10) . For some test problems, we also compare the numerical performance of the proposed neural make the comparisons with the existing models in Kennedy and the ordinary differential equation solver engaged is ode45s.
Example 4.1 ( Bazaraa et al., 1990 ). subject to
This example is given by Beal and is a linear programming problem that have been shown to cycle (not converge) when solved by Dantzig X  X  original simplex algorithm. All simulation results show that the state trajectories of the proposed model
Figs. 2 and 3 show that the trajectories of the neural network converge to the optimal solution of the problem.
 To make a comparison, we solve the above problem by using the that this model with x 0  X  X  1 , 1 , 1 , 1 , 1 , 1 , 1  X  T ming problem.
 Example 4.2 ( Gao, 2004 ). subject to proposed neural network in (9) and (10) to solve this mathema-tical programming. Simulation results show the trajectory of (9) and (10) with any initial point is always convergent to shown in Fig. 7 .

To make a comparison, the above problem is solved by using in Example 4 in Gao (2004) by using the Kennedy and Chua equilibrium point is not feasible.  X 4  X 2 0 2 4 6 8 10 12  X 3  X 2  X 1 0 1 2 3 4 5
Example 4.3 ( Xia and Wang, 2004b ). minimize x 2 1  X  x 2 2  X  2 x 1 x 2  X  X  x 1 1  X  4  X  X  x 2 3  X  subject to
This problem has only one solution x n  X  X  0 , 1 : 196  X  T network (9) and (10) with initial points P 1  X  X  4 , 9  X  , P P 3  X  X  6 , 6  X  are converge to x n .
 In comparison, the above problem is solved by using the points and the penalty parameter g  X  5000 in Example 2 in Xia of the Kennedy X  X hua X  X  neural network with initial points P and P 3 are do not converge to x n .

Example 4.4 ( Yang and Cao, 2008 ). minimize  X  x 1  X  3 x 2  X  x 3  X  2  X  4  X  x 1 x 2  X  2 subject to the proposed neural network in (9) and (10) to solve the above problem with 20 random initial points converge to the optimal solution of this problem.

We test the influence of the parameter t in neural network when t  X  0 : 1, the neural network (9) and (10) generates the y convergence rate of the error J y  X  t  X  y n J 2 .

Example 4.5 ( Yang and Xu, 2007 ). minimize 1 2  X  X  x 1 x 2  X  4  X  X  x 2  X  x 3  X  2  X  X  x 1  X  x 3 subject to convergence with eight random initial points.
 Example 4.6 ( Xia and Feng, 2007 ). minimize  X  x 1 10  X  2  X  5  X  x 2 12  X  2  X  x 4 3  X  3  X  x 4 11  X  subject to 1 : 951 , 0 : 4775 , 4 : 366 , 0 : 625 , 1 : 038 , 1 : 594  X  of x ( t ) based on the proposed neural network with 5 random convergence rate.

Example 4.7 ( Malek et al., 2010 ). subject to 4  X 2 0 2 4 6 0 0.5 1 1.5 2  X 1 0 1 2 3 4 5 .
 100 random initial points.

Example 4.8 ( Xue and Bian, 2007 ). minimize 2 x 2 1  X  1 2 x 2 2  X  1 2 x 2 3  X  2 x 1 x 2  X  x subject to of all optimal solutions for this problem is E  X f X  x 1 , x 2 , x 3  X  T 9 x 1  X  1 , x 2  X  x 3  X  4 , 2 r x network model converges to an optimal solution of this degen-erate convex quadratic programming problem. Figs. 15 and 16 display the convergence with two random initial points, which x trajectories of the proposed neural network model (9) and (10) converges to one of the elements of E N .

Example 4.9. Consider the grasping force optimization problem fingers, the optimization problem can be formulated as subject to G external wrench, and m the friction coefficient.

As an example, we consider a polyhedral object with M  X  0.1 kg the grasp transformation matrix is G  X  varying external wrench applied to the center of mass of the object is f where g  X  9 : 8m = s 2 , y  X  t  X  X  n t = r A  X  0 , 2 p and f varying grasping force obtained from the proposed neural net-Euclidean norm of the optimal force from the proposed neural network model.
 Example 4.10. Consider the regression problem of approximating a set of data f X  x 1 , y 1  X  ,  X  x 2 , y 2  X  , ... ,  X  x N , y N  X g ,  X  30  X  with a regression function as f  X  x  X  X  of the model to be estimated. Here, we can use the recurrent neural network in (9) and (10) to estimate these parameters. By utilizing Huber loss function ( Xia and Wang, 2004a ), the regression function defined in (31) can be represented as f  X  x  X  X  where K  X  x , y  X  is a kernel function satisfying K  X  x i
According to the problem formulation in Liu and Wang (2008) , y programming problem: minimize 1 2 subject to where e 4 0 is an accuracy parameter required for the approx-point of the proposed neural network model to solve the above quadratic optimization problem.
 in Xia and Wang (2004a) . We use the proposed neural network for the regression problem. We choose the following Gaussian function:
K  X  x , y  X  X  exp J x y J more accurate than it is in Xia and Wang (2004a) . advantages of our proposed neural network compared to the existing ones? To answer this, we summarize what we have observed from numerical experiments and theoretical results as below.
 points in Example 4.3 causes the divergence of solution trajectory solved by the Kennedy Chua X  X  neural network (15) used in Xia and Wang (2004b) , while it does not affect anything by our neural network model. The reason is that the problem.

The proposed models in Nazemi (2012) and Xia and Wang (2004b) cannot solve general convex programming problems where objective function and all constraint functions are only convex, i.e., linear programming, degenerate convex quadratic programming problem, and so on. But the proposed model can solve all this optimization problems. 5. Conclusion
In this paper, we have presented a neural network for solving the GCNLP in real time. In contrast to three existing neural networks, the proposed neural network does not require any model is more suitable to be implemented in hardware. Further-more, we have shown that the proposed neural network is stable the original problem. Since linear programming and degenerate convex quadratic programming problems ( Xue and Bian, 2007 ) network can also solve them. Moreover, since convex nonlinear programming problem has wide applications, the new model and parameter t in dynamic model (9) and (10) on the convergence is to extend the proposed neural network model for various convex optimization problems, i.e., second order cone program-ming, fractional programming and stochastic optimization pro-blems, and establish its related stability accordingly. References
