 Automatic extraction of opinions, emotions, and sentiments in text ( subjectivity analysis ) to support applications such as product review mining, sum-marization, question answering, and information ex-traction is an active area of research in NLP.
Many approaches to opinion, sentiment, and sub-jectivity analysis rely on lexicons of words that may be used to express subjectivity. However, words may have both subjective and objective senses, which is a source of ambiguity in subjectivity and sentiment analysis. We show that even words judged in pre-vious work to be reliable clues of subjectivity have significant degrees of subjectivity sense ambiguity.
To address this ambiguity, we present a method for automatically assigning subjectivity labels to word senses in a taxonomy, which uses new features and integrates more diverse types of knowledge than in previous work. We focus on nouns, which are challenging and have received less attention in auto-matic subjectivity and sentiment analysis.

A common approach to building lexicons for sub-jectivity analysis is to begin with a small set of seeds which are prototypically subjective (or posi-tive/negative, in sentiment analysis), and then fol-low semantic links in WordNet-like resources. By far, the emphasis has been on horizontal relations, such as synonymy and antonymy . Exploiting vertical links opens the door to taking into account the infor-mation content of ancestor concepts of senses with known and unknown subjectivity. We develop novel features that measure the similarity of a target word sense with a seed set of senses known to be sub-jective, where the similarity between two concepts is determined by the extent to which they share in-formation, measured by the information content as-sociated with their least common subsumer (LCS). Further, particularizing the LCS features to domain greatly reduces calculation while still maintaining effective features.

We find that our new features do lead to signif-icant improvements over methods proposed in pre-vious work, and that the combination of all features gives significantly better performance than any sin-gle type of feature alone.

We also ask, given that there are many approaches to finding subjective words, if it would make sense for word-and sense-level approaches to work in tan-dem, or should we best view them as competing ap-proaches? We give evidence suggesting that first identifying subjective words and then disambiguat-ing their senses would be an effective approach to subjectivity sense labeling.
There are several motivations for assigning sub-jectivity labels to senses. First, (Wiebe and Mi-halcea, 2006) provide evidence that word sense la-bels, together with contextual subjectivity analysis, can be exploited to improve performance in word sense disambiguation. Similarly, given subjectivity sense labels, word-sense disambiguation may poten-tially help contextual subjectivity analysis. In addi-tion, as lexical resources such as WordNet are devel-oped further, subjectivity labels would provide prin-cipled criteria for refining word senses, as well as for clustering similar meanings to create more course-grained sense inventories.

For many opinion mining applications, polarity (positive, negative) is also important. The overall framework we envision is a layered approach: clas-sifying instances as objective or subjective, and fur-ther classifying the subjective instances by polar-ity. Decomposing the problem into subproblems has been found to be effective for opinion mining. This paper addresses the first of these subproblems. We adopt the definitions of subjective and objective from Wiebe and Mihalcea (2006) (hereafter WM ). Subjective expressions are words and phrases being used to express opinions, emotions, speculations, etc. WM give the following examples: His alarm grew.
 He absorbed the information quickly.
 UCC/Disciples leaders roundly condemned the Iranian President X  X  verbal assault on Israel. What X  X  the catch?
Polarity (also called semantic orientation ) is also important to NLP applications in sentiment analysis and opinion extraction. In review mining, for exam-ple, we want to know whether an opinion about a product is positive or negative. Even so, we believe there are strong motivations for a separate subjec-tive/objective (S/O) classification as well.
First, expressions may be subjective but not have any particular polarity. An example given by (Wil-son et al., 2005) is Jerome says the hospital feels no different than a hospital in the states. An NLP application system may want to find a wide range of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments.

Second, distinguishing S and O instances has of-ten proven more difficult than subsequent polarity classification. Researchers have found this at vari-ous levels of analysis, including the manual anno-tation of phrases (Takamura et al., 2006), sentiment classification of phrases (Wilson et al., 2005), sen-timent tagging of words (Andreevskaia and Bergler, 2006b), and sentiment tagging of word senses (Esuli and Sebastiani, 2006a). Thus, effective methods for S / O classification promise to improve performance for sentiment classification. In fact, researchers in sentiment analysis have realized benefits by decom-posing the problem into S / O and polarity classifica-tion (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005; Kim and Hovy, 2006). One reason is that different features may be relevant for the two subproblems. For example, negation fea-tures are more important for polarity classification than for subjectivity classification.

Note that some of our features require vertical links that are present in WordNet for nouns and verbs but not for other parts of speech. Thus we ad-dress nouns (leaving verbs to future work). There are other motivations for focusing on nouns. Rela-tively little work in subjectivity and sentiment anal-ysis has focused on subjective nouns. Also, a study (Bruce and Wiebe, 1999) showed that, of the major parts of speech, nouns are the most ambiguous with respect to the subjectivity of their instances.
Turning to word senses, we adopt the definitions from WM. First, subjective:  X  X lassifying a sense as S means that, when the sense is used in a text or con-versation, we expect it to express subjectivity; we also expect the phrase or sentence containing it to be subjective [WM, pp. 2-3]. X 
In WM, it is noted that sentences containing ob-jective senses may not be objective, as in the sen-tence Will someone shut that darn alarm off? Thus, objective senses are defined as follows:  X  X lassifying a sense as O means that, when the sense is used in a text or conversation, we do not expect it to express subjectivity and, if the phrase or sentence containing it is subjective, the subjectivity is due to something else [WM, p 3]. X 
The following subjective examples are given in WM: The following objective examples are given in WM:
WM performed an agreement study and report that good agreement (  X  =0.74) can be achieved be-tween human annotators labeling the subjectivity of senses. For a similar task, (Su and Markert, 2008) also report good agreement. Many methods have been developed for automati-cally identifying subjective ( opinion , sentiment , at-titude , affect-bearing , etc.) words, e.g., (Turney, 2002; Riloff and Wiebe, 2003; Kim and Hovy, 2004; Taboada et al., 2006; Takamura et al., 2006).
Five groups have worked on subjectivity sense la-beling. WM and Su and Markert (2008) (hereafter SM ) assign S / O labels to senses, while Esuli and Se-bastiani (hereafter ES ) (2006a; 2007), Andreevskaia and Bergler (hereafter AB ) (2006b; 2006a), and (Valitutti et al., 2004) assign polarity labels.
WM, SM, and ES have evaluated their systems against manually annotated word-sense data. WM X  X  annotations are described above; SM X  X  are similar. In the scheme ES use (Cerini et al., 2007), senses are assigned three scores, for positivity, negativity, and neutrality. There is no unambiguous mapping between the labels of WM/SM and ES, first because WM/SM use distinct classes and ES use numerical ratings, and second because WM/SM distinguish be-tween objective senses on the one hand and neutral subjective senses on the other, while those are both neutral in the scheme used by ES.

WM use an unsupervised corpus-based approach, in which subjectivity labels are assigned to word senses based on a set of distributionally similar words in a corpus annotated with subjective expres-sions. SM explore methods that use existing re-sources that do not require manually annotated data; they also implement a supervised system for com-parison, which we will call SMsup . The other three groups start with positive and negative seed sets and expand them by adding synonyms and antonyms, and traversing horizontal links in WordNet. AB, ES, and SMsup additionally use information contained in glosses; AB also use hyponyms; SMsup also uses relation and POS features. AB perform multiple runs of their system to assign fuzzy categories to senses. ES use a semi-supervised, multiple-classifier learning approach. In a later paper, (Esuli and Se-bastiani, 2007), ES again use information in glosses, applying a random walk ranking algorithm to a graph in which synsets are linked if a member of the first synset appears in the gloss of the second.
Like ES and SMsup, we use machine learning, but with more diverse sources of knowledge. Further, several of our features are novel for the task. The LCS features (Section 6.1) detect subjectivity by measuring the similarity of a candidate word sense with a seed set. WM also use a similarity measure, but as a way to filter the output of a measure of distri-butional similarity (selecting words for a given word sense), not as we do to cumulatively calculate the subjectivity of a word sense. Another novel aspect of our similarity features is that they are particular-ized to domain, which greatly reduces calculation. The domain subjectivity LCS features (Section 6.2) are also novel for our task. So is augmenting seed sets with monosemous words, for greater coverage without requiring human intervention or sacrificing quality. Note that none of our features as we specif-ically define them has been used in previous work; combining them together, our approach outperforms previous approaches. We use the subjectivity lexicon of (Wiebe and Riloff, create the experimental data sets. The lexicon is a list of words and phrases that have subjective uses, though only word entries are used in this paper (i.e., we do not address phrases at this point). Some en-tries are from manually developed resources, includ-ing the General Inquirer, while others were derived from corpora using automatic methods.

Through manual review and empirical testing on data, (Wiebe and Riloff, 2005) divided the clues into strong ( strongsubj ) and weak ( weaksubj ) subjectiv-ity clues. Strongsubj clues have subjective meanings with high probability, and weaksubj clues have sub-jective meanings with lower probability.

To support our experiments, we annotated the icon, using WM X  X  annotation scheme described in Section 2. Due to time constraints, only some of the data was labeled through consensus labeling by two annotators; the rest was labeled by one annotator.
Overall, 2875 senses for 882 words were anno-tated. Even though all are senses of words from the subjectivity lexicon, only 1383 (48%) of the senses are subjective.

The words labeled strongsubj are in fact less am-biguous than those labeled weaksubj in our analysis, thus supporting the reliability classifications in the lexicon. 55% (1038/1924) of the senses of strong-subj words are subjective, while only 36% (345/951) of the senses of weaksubj words are subjective.
For the analysis in Section 7.3, we form subsets of the data annotated here to test performance of our method on different data compositions. Both subjective and objective seed sets are used to define the features described below. For seeds, a large number is desirable for greater coverage, al-though high quality is also important. We begin to build our subjective seed set by adding the monose-mous strongsubj nouns of the subjectivity lexicon (there are 397 of these). Since they are monose-mous, they pose no problem of sense ambiguity. We then expand the set with their hyponyms, as they were found useful in previous work by AB (2006b; 2006a). This yields a subjective seed set of 645 senses. After removing the word senses that belong to the same synset, so that only one word sense per synset is left, we ended up with 603 senses.
To create the objective seed set, two annotators manually annotated 800 random senses from Word-Net, and selected for the objective seed set the ones they both agreed are clearly objective. This creates an objective seed set of 727. Again we removed multiple senses from the same synset leaving us with 722. The other 73 senses they annotated are added to the mixed data set described below. As this sam-pling shows, WordNet nouns are highly skewed to-ward objective senses, so finding an objective seed set is not difficult. 6.1 Sense Subjectivity LCS Feature This feature measures the similarity of a target sense with members of the subjective seed set. Here, sim-ilarity between two senses is determined by the ex-tent to which they share information, measured by using the information content associated with their least common subsumer. For an intuition behind this feature, consider this example. In WordNet, the hy-pernym of the  X  X trong criticism X  sense of attack is criticism . Several other negative subjective senses are descendants of criticism , including the relevant senses of fire , thrust , and rebuke . Going up one more level, the hypernym of criticism is the  X  X x-pression of disapproval X  meaning of disapproval , which has several additional negative subjective de-scendants, such as the  X  X xpression of opposition and disapproval X  sense of discouragement . Our hypoth-esis is that the cases where subjectivity is preserved in the hypernym structure, or where hypernyms do lead from subjective senses to others, are the ones that have the highest least common subsumer score with the seed set of known subjective senses.
We calculate similarity using the information-content based measure proposed in (Resnik, 1995), as implemented in the WordNet::Similarity pack-age (using the default option in which LCS values taxonomy such as WordNet, the information con-tent associated with a concept is determined as the likelihood of encountering that concept, defined as  X  log ( p ( C )) , where p ( C ) is the probability of see-ing concept C in a corpus. The similarity between two concepts is then defined in terms of information content as: LCS where C is the concept that subsumes both C C the least common subsumer (LCS) ).

For this feature, a score is assigned to a target sense based on its semantic similarity to the mem-bers of a seed set; in particular, the maximum such similarity is used.

For a target sense t and a seed set S , we could have used the following score:
Score ( t, S ) = max
However, several researchers have noted that sub-jectivity may be domain specific. A version of WordNet exists, WordNet Domains (Gliozzo et al., 2005), which associates each synset with one of the domains in the Dewey Decimal library classifica-tion. After sorting our subjective seed set into differ-ent domains, we observed that over 80% of the sub-jective seed senses are concentrated in six domains (the rest are distributed among 35 domains).
Thus, we decided to particularize the semantic similarity feature to domain, such that only the sub-set of the seed set in the same domain as the tar-get sense is used to compute the feature. This in-volves much less calculation, as LCS values are cal-culated only with respect to a subset of the seed set. We hypothesized that this would still be an effec-tive feature, while being more efficient to calculate. This will be important when this method is applied to large resources such as the entire WordNet.
Thus, for seed set S and target sense t which is in domain D , the feature is defined as the following score:
SenseLCSscore ( t, D, S ) = max
The seed set is a parameter, so we could have defined a feature reflecting similarity to the objec-tive seed set as well. Since WordNet is already highly skewed toward objective noun senses, any naive classifier need only guess the majority class for high accuracy for the objective senses. We in-cluded only a subjective feature to put more empha-sis on the subjective senses. In the future, features could be defined with respect to objectivity, as well as polarity and other properties of subjectivity. 6.2 Domain Subjectivity LCS Score We also include a feature reflecting the subjectivity of the domain of the target sense. Domains are assigned scores as follows. For domain D and seed set S :
DomainLCSscore ( D, S ) = ave d  X  D  X  S M emLCSscore ( d, D, S ) where:
M emLCSscore ( d, D, S ) =
The value of this feature for a sense is the score assigned to that sense X  X  domain. 6.3 Common Related Senses This feature is based on the intersection between the set of senses related (via WordNet relations) to the target sense and the set of senses related to members of a seed set. First, for the target sense and each member of the seed set, a set of related senses is formed consisting of its synonyms, antonyms and di-rect hypernyms as defined by WordNet. For a sense s , R ( s ) is s together with its related senses.
Then, given a target sense t and a seed set S we compute an average percentage overlap as follows:
The value of a feature is its score. Two features are included in the experiments below, one for each of the subjective and objective seed sets. 6.4 Gloss-based features These features are Lesk-style features (Lesk, 1986) that exploit overlaps between glosses of target and seed senses. We include two types in our work. 6.4.1 Average Percentage Gloss Overlap
For a sense s , gloss ( s ) is the set of stems in the gloss of s (excluding stop words). Then, given a tar-get sense t and a seed set S , we compute an average percentage overlap as follows:
As above, R ( s ) is considered for each seed sense s , but now only the target sense t is considered, not R ( t ) . We did this because we hypothesized that the gloss can provide sufficient context for a given target sense, so that the addition of related words is not necessary.

We include two features, one for each of the sub-jective and objective seed sets. 6.4.2 Vector Gloss Overlap Features
For this feature we also consider overlaps of stems in glosses (excluding stop words). The over-laps considered are between the gloss of the tar-get sense t and the glosses of R ( s ) for all s in a seed set (for convenience, we will refer to these as seedRelationSets ).

A vector of stems is created, one for each stem (excluding stop words) that appears in a gloss of a member of seedRelationSets . If a stem in the gloss of the target sense appears in this vector, then the vector entry for that stem is the total count of that stem in the glosses of the target sense and all members of seedRelationSets .

A feature is created for each vector entry whose value is the count at that position. Thus, these fea-tures consider counts of individual stems, rather than average proportions of overlaps, as for the previous type of gloss feature.

Two vectors of features are used, one where the seed set is the subjective seed set, and one where it is the objective seed set. 6.5 Summary In summary, we use the following features (here, SS is the subjective seed set and OS is the objective one). 1. SenseLCSscore ( t, D, SS ) 2. DomainLCSscore ( D, SS ) 3. RelOverlap ( t, SS ) 4. RelOverlap ( t, OS ) 5. GlOverlap ( t, SS ) 6. GlOverlap ( t, OS ) 7. Vector of gloss words ( SS ) 8. Vector of gloss words ( OS ) We perform 10-fold cross validation experiments on several data sets, using SVM light (Joachims,
Based on our random sampling of WordNet, it appears that WordNet nouns are highly skewed to-ward objective senses. (Esuli and Sebastiani, 2007) argue that random sampling from WordNet would yield a corpus mostly consisting of objective (neu-tral) senses, which would be  X  X retty useless as a benchmark for testing derived lexical resources for opinion mining [p. 428]. X  So, they use a mixture of subjective and objective senses in their data set.
To create a mixed corpus for our task, we anno-tated a second random sample from WordNet (which is as skewed as the previously mentioned one). We added together all of the senses of words in the lexi-con which we annotated, the leftover senses from the selection of objective seed senses, and this new sam-ple. We removed duplicates, multiple senses from the same synset, and any senses belonging to the same synset in either of the seed sets. This resulted in a corpus of 2354 senses, 993 (42.18%) of which are subjective and 1361 (57.82%) of which are ob-jective.

The results with all of our features on this mixed corpus are given in Row 1 of Table 1. In Table 1, the first column identifies the features, which in this case is all of them. The next three columns show overall accuracy, and precision and recall for finding sub-jective senses. The baseline accuracy for the mixed data set (guessing the more frequent class, which is objective) is 57.82%. As the table shows, the accu-7.1 Analysis and Discussion In this section, we seek to gain insights by perform-ing ablation studies, evaluating our method on dif-ferent data compositions, and comparing our results to previous results. 7.2 Ablation Studies Since there are several features, we divided them into sets for the ablation studies. The vector-of-gloss-words features are the most similar to ones used in previous work. Thus, we opted to treat them as one ablation group ( Gloss vector ). The Overlaps group includes the RelOverlap ( t, SS ) , RelOverlap ( t, OS ) , GlOverlap ( t, SS ) , and GlOverlap ( t, OS ) features. Finally, the LCS group includes the SenseLCSscore and the DomainLCSscore features.

There are two types of ablation studies. In the first, one group of features at a time is included. Those results are in the middle section of Table 1. Thus, for example, the row labeled LCS in this sec-tion is for an experiment using only the LCS fea-tures. In comparison to performance when all fea-tures are used, F-measure for the Overlaps and LCS ablations is significantly different at the p &lt; . level, and, for the Gloss Vector ablation, it is sig-nificantly different at the p = . 052 level (one-tailed t -test). Thus, all of the features together have better performance than any single type of feature alone.
In the second type of ablation study, we use all the features minus one group of features at a time. The results are in the bottom section of Table 1. Thus, for example, the row labeled LCS in this sec-tion is for an experiment using all but the LCS fea-tures. F-measures for LCS and Gloss vector are sig-nificantly different at the p = . 056 and p = . 014 lev-els, respectively. However, F-measure for the Over-laps ablation is not significantly different ( p = . 39 ). These results provide evidence that LCS and Gloss vector are better together than either of them alone. 7.3 Results on Different Data Sets Several methods have been developed for identify-ing subjective words. Perhaps an effective strategy would be to begin with a word-level subjectivity lex-icon, and then perform subjectivity sense labeling to sort the subjective from objective senses of those words. We also wondered about the relative effec-tiveness of our method on strongsubj versus weak-subj clues.

To answer these questions, we apply the full model (again in 10-fold cross validation experi-ments) to data sets composed of senses of polyse-mous words in the subjectivity lexicon. To support comparison, all of the data sets in this section have results are presented in Table 2.

For comparison, the first row repeats the results for the mixed corpus from Table 1. The second row shows results for a corpus of senses of a mix-ture of strongsubj and weaksubj words. The corpus was created by selecting a mixture of strongsubj and weaksubj words, extracting their senses and the S / O labels applied to them in Section 4, and then ran-domly removing senses of the more frequent class until the distribution is uniform. We see that the results on this corpus are better than on the mixed data set, even though the baseline accuracy is lower and the corpus is smaller. This supports the idea that an effective strategy would be to first identify opinion-bearing words, and then apply our method to those words to sort out their subjective and objec-tive senses.

The third row shows results for a weaksubj subset of the strong+weak corpus and the fourth shows re-sults for a strongsubj subset that is of the same size. As expected, the results for the weaksubj senses are lower while those for the strongsubj senses are higher, as weaksubj clues are more ambiguous. 7.4 Comparisons with Previous Work WM and SM address the same task as we do. To compare our results to theirs, we apply our full model (in 10-fold cross validation experiments) to
Table 3 has the WM data set results. WM rank their senses and present their results in the form of precision recall curves. The second row of Table 3 shows their results at the recall level achieved by our method (66%). Their precision at that level is sub-stantially below ours.

Turning to ES, to create S / O annotations, we ap-plied the following heuristic mapping (which is also used by SM for the purpose of comparison): any sense for which the sum of positive and negative scores is greater than or equal to 0.5 is S, otherwise it is O. We then evaluate the mapped tags against the gold standard of WM. The results are in Row 3 of Table 3. Note that this mapping is not fair to Sen-tiWordNet, as the tasks are quite different, and we do not believe any conclusions can be drawn. We include the results to eliminate the possibility that their method is as good ours on our task, despite the differences between the tasks.

Table 4 has the results for the noun subset of SM X  X  data set, which is the data set used by ES, reanno-tated by SM. CV* is their supervised system and SL* is their best non-supervised one. Our method focus of SM X  X  work is not supervised machine learn-ing. In this paper, we introduced an integrative approach to automatic subjectivity word sense labeling which combines features exploiting the hierarchical struc-ture and domain information of WordNet, as well as similarity of glosses and overlap among sets of semantically related words. There are several contributions. First, we learn several things. We found (in Section 4) that even reliable lists of sub-jective (opinion-bearing) words have many objec-tive senses. We asked if word-and sense-level ap-proaches could be used effectively in tandem, and found (in Section 7.3) that an effective strategy is to first identify opinion-bearing words, and then apply our method to sort out their subjective and objective senses. We also found (in Section 7.2) that the entire set of features gives better results than any individ-ual type of feature alone.

Second, several of the features are novel for our task, including those exploiting the hierarchical structure of a lexical resource, domain information, and relations to seed sets expanded with monose-mous senses.

Finally, the combination of our particular features is effective. For example, on senses of words from a subjectivity lexicon, accuracies range from 20 to 29 percentage points above baseline. Further, our combination of features outperforms previous ap-proaches.
 This work was supported in part by National Sci-ence Foundation awards #0840632 and #0840608. The authors are grateful to Fangzhong Su and Katja Markert for making their data set available, and to the three paper reviewers for their helpful sugges-tions.
