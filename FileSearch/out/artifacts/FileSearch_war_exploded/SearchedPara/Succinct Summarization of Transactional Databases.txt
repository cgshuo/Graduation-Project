 Transactional data are ubiquitous. Several methods, including fre-quent itemsets mining and co-clustering, have been proposed to analyze transactional databases. In this work, we propose a new research problem to succinctly summarize transactional databases. Solving this problem requires linking the high level structure of the database to a potentially huge number of frequent itemsets. We formulate this problem as a set covering problem using overlapped hyperrectangles; we then prove that this problem and its several variations are NP-hard. We develop an approximation algorithm HY P ER which can achieve a ln ( k ) + 1 approximation ratio in polynomial time. We propose a pruning strategy that can signif-icantly speed up the processing of our algorithm. Additionally, we propose an efficient algorithm to further summarize the set of hyperrectangles by allowing false positive conditions. A detailed study using both real and synthetic datasets shows the effective-ness and efficiency of our approaches in summarizing transactional databases.
 H.2.8 [ Database Management ]: Database Applications X  Data Min-ing Algorithms, Theory hyperrectangle, set cover, summarization, transactional databases
Transactional data are ubiquitous. In the business domain, from the world X  X  largest retailers to the multitude of online stores, trans-actional databases carry the most fundamental business informa-tion: customer shopping transactions. In biomedical research, high-throughput experimental data, like microarray, can be recorded as transactional data, where each transaction records the conditions under which a gene or a protein is expressed [13] (or alternatively, repressed). In document indexing and search engine applications, a transactional model can be applied to represent the document-term relationship. Transactional data also appear in several differ-ent equivalent formats, such as binary matrix and bipartite graph, among others.

Driven by the real-world applications, ranging from business in-telligence to bioinformatics, mining transactional data has been one of the major topics in data mining research. Several methods have been proposed to analyze transactional data. Among them, frequent itemset mining [2] is perhaps the most popular and well-known. It tries to discover sets of items which appear in at least a certain num-ber of transactions. Recently, co-clustering [12], has gained much attention. It tries to simultaneously cluster transactions (rows) and items (columns) into different respective groups. Using binary ma-trix representation, co-clustering can be formulated as a matrix-factorization problem.

In general, we may classify transational data mining methods and their respective tools into two categories (borrowing terms from economics): micro-pattern mining and macro-pattern mining . The first type focuses on providing local knowledge of the transactional database, exemplified by frequent itemset mining. The second type works to offer a global view of the entire database; co-clustering is one such method. However, both types are facing some ma-jor challenges which significantly limit their applicability. On the micro-pattern mining side, the number of patterns being generated from the transaction data is generally very large, containing many patterns which differ only slightly from one another. Even though many methods have been proposed to tackle this issue, it remains a major open problem in the data mining research community. On the macro-pattern mining side, as argued by Faloutsos and Mega-looikonomou [7], data mining is essentially the art of trying to develop concise descriptions of a complex dataset, and the con-ciseness of the description can be measured by Kolmogorov com-plexity. So far, limited efforts have been undertaken towards this goal of concise descriptions of transactional databases.
Above all, little work has been done to understand the relation-ship between the macro-patterns and micro-patterns. Can a small number of macro-pattern or high-level structures be used to infer or explain the large number of micro-patterns in a transactional database? How can the micro-patterns, like frequent itemsets, be augmented to form the macro-patterns? Even though this paper will not provide all the answers for all these questions, we believe the research problem formulated and addressed in this work takes a solid step in this direction, and particularly sheds light on a list of important issues related to mining transactional databases.
Specifically, we seek a succinct representation of a transactional database based on the hyperrectangle notation. A hyperrectangle is a Cartesian product of a set of transactions (rows) and a set of items (columns). A database is covered by a set of hyperrectangles if any element in the database, i.e., the transaction-item pair, is contained in at least one of the hyperrectangles in the set. Each hyperrectan-gle is associated with a representation cost, which is the sum of the representation costs (commonly the cardinality) of its set of trans-actions and set of items. The most succinct representation for a transactional database is the one which covers the entire database with the least total cost.

Here, the succinct representation can provide a high-level struc-ture of the database and thus, mining succinct representation corre-sponds to a macro-pattern mining problem. In addition, the number of hyperrectangles in the set may serve as a measurement of the in-trinsic complexity of the transactional database. In the meantime, as we will show later, the rows of the hyperrectangle generally cor-respond to the frequent itemsets, and the columns are those trans-actions in which they appear. Given this, the itemsets being used in the representation can be chosen as representative itemsets for the large collection of frequent itemsets, as they are more infor-mative for revealing the underlying structures of the transactional database. Thus, the hyperrectangle notation and the succinct cov-ering problem build a bridge between the macro-structures and the micro-structures of a transactional database.
Let the transaction database DB be represented as a binary ma-trix such that a cell ( i, j ) is 1 if a transaction i contains item j , otherwise 0 . For convenience, we also denote the database DB as Let the hyperrectangle H be the Cartesian product of a transaction set T and an item set I , i.e. H = T  X  I = { ( i, j ) : i  X  T and j  X  I } . Let CDB = { H 1 , H 2 ,  X   X   X  , H p } be a set of hyperrectan-gles, and let the set of cells being covered by CDB be denoted as
If database DB is contained in CDB c , DB  X  CDB c , then, we refer to CDB as the covering database or the summarization of DB . If there is no false positive coverage in CDB , we have DB = CDB c . If there is false positive coverage, we will have | CDB c \ DB | &gt; 0 .

For a hyperrectangle H = T  X  I , we define its cost to be the sum of the cardinalities of its transaction set and item set: cost ( H ) = | T | + | I | . Given this, the cost of CDB is Typically, we store the transactional database in either horizontal or vertical representation. The horizontal representation can be represented as CDB H = {{ t i }  X  I t i } , where I t i is all the set of items transaction t i contains. The vertical representation is as CDB V = { T j  X  { j }} , where T j is the transactions which con-tains item j . Let T be the set of all transactions in DB and I be the set of all items in DB . Then, the cost of these two representations are: In this work, we are interested in the following main problem. Given a transactional database DB and no false positive is allowed, how can we find the covering database CDB with minimal cost (or simply the minimal covering database ) efficiently? In addition, we are also interested in how we can further reduce the cost of the covering database if false positive is allowed.
Our contributions are as follows. 1. We propose a new research problem to succinctly summarize transactional databases, and formally formulate it as a variant of a weighted set covering problem based on a hyperrectangle notation. 2. We provide a detailed discussion on how this new problem is related to a list of important data mining problems (Section 2). 3. We study the complexity of this problem and prove this problem and its several variations are NP-hard (Section 3). 4. We develop an approximation algorithm HY P ER which can achieve a ln ( k ) + 1 approximation ratio in polynomial time. We also propose a pruning strategy that can significantly speed up the processing of our algorithm (Section 4). 5. We propose an efficient algorithm to further summarize the set of hyperrectangles by allowing false positive conditions. (Section 5). 6. We provide a detailed study using both real and synthetic datasets. Our research shows that our method can provide a succinct summa-rization of transactional data (Section 6).
In this section, we discuss how the summarization problem stud-ied in this work is related to a list of other important data mining problems, and how solving this problem can help to tackle those related problems.
 Data Descriptive Mining and Rectangle Covering: This problem is generally in the line of descriptive data mining. More specif-ically, it is closely related to the efforts in applying rectangles to summarize underlying datasets. In [3], Agrawal et al. define and develop a heuristic algorithm to represent a dense cluster in grid data using a set of rectangles. Further, Lakshmanan et al. [11] consider the situation where a false positive is allowed. Recently, Gao et al. [8] extend descriptive data mining from a clustering de-scription to a discriminative setting using a rectangle notation. Our problem is different from these problems from several perspective s. First, they focus on multi-dimensional spatial data where the rect-angle area forms a continuous space. Clearly, the hyperrectangle is more difficult to handle because transactional data are discrete, so any combination of items or transactions can be selected to form a rectangle. Further, their cost functions are based on the minimal number of rectangles, whereas our cost is based on the cardinalities of sets of transactions and items. This is potentially much harder to handle.
 Summarization for categorical databases: Data summarization has been studied by some researchers in recent years. Wang and Karypis proposed to summarize categorical databases by mining summary set [19]. Each summary set contains a set of summary itemsets. A summary itemset is the longest frequent itemsets sup-ported by a transaction. This approach can be regarded as a special case of our summarization by fixing hyperrectangle width (i.e. the transaction dimension) to be one. Chandola and Kumar compress datasets of transactions with categorical attributes into informa-tive representations by summarizing transactions [5]. They showed their methods are effective in summarizing network traffic. Their approach is similar to ours but different in the problem definition and research focus. Their goal is to effectively cover all transac-tions with more compaction gain and less information loss, while our goal is to effectively cover all cells (i.e. transaction item pair), which are finer granules of a database. In addition, our methods are shown to be effective not only by experimental results but also by the theoretical approximation bound.
 Data Categorization and Comparison: Our work is closely re-lated to the effort by Siebes et al. [15] [17] [18]. In [15] [17], they propose to recognize significant itemsets by their ability to com-press a database based on the MDL principles. The compression strategy can be explained as covering the entire database using the non-overlapped hyperrectangles with no false positives allowed. The set of itemsets being used in the rectangles is referred to as the code table, and each transaction is rewritten using the itemsets in the code table. They try to optimize the description length of both the code table and the rewritten database. In addition, they propose to compare databases by the code length with regard to the same code table [18]. A major difference between our work and this work is that we apply overlapped hyperrectangles to cover the entire database. Furthermore, the optimization function is also dif-ferent. Our cost is determined by the cardinalities of the sets form-ing the rectangles, and their cost is based on the MDL principle. In addition, we also study how the hyperrectangle can be further sum-marized by allowing false positive data. Thus, our methods can provide a much more succinct summarization of the transactional database. Finally, their approach is purely heuristic with no analyt-ical results on the difficulty of their compression problem. As we will discuss in Section 3, we provide rigorous analysis and proof on the hardness of our summarization problem. We also develop an algorithm with proven approximation bound under certain con-straints.
 Co-clustering: As mentioned before, co-clustering attempts si-multaneous clustering of both row and column sets in different groups in a binary matrix. This approach can be formulated as a matrix factorization problem [12]. The goal of co-clustering is to reveal the homogeneous block structures being dominated by ei-ther 1 s or 0 s in the matrix. From the summarization viewpoint, co-clustering essentially provides a so-called checkerboard struc-ture summarization with false positive data allowed. Clearly, the problem addressed in this work is much more general in terms of the summarization structure and the false positive assumption (we consider both).
 Approximate Frequent Itemset Mining: Mining error-tolerant frequent itemsets has attracted a lot of research attention over the last several years. We can look at error-tolerant frequent itemsets from two perspectives. On one side, it tries to recognize the fre-quent itemsets considering if some noise is added into the data. In other words, the frequent itemsets are disguised in the data. On an-other side, it provides a way to reduce the number of frequent item-sets since many of the frequent itemsets can be recognized as the variants of a true frequent itemset. This in general is referred to as pattern summarization [1][14]. Most of the efforts in error-tolerant frequent itemsets can be viewed as finding dense hyperrectangles with certain constraints. The support envelope notation proposed by Steinbach et al. [16] also fits into this framework. Generally speaking, our work does not directly address how to discover indi-vidual error-tolerant itemsets. Our goal is to derive a global sum-marization of the entire transactional database. However, we can utilize error-tolerant frequent itemsets to form a succinct summa-rization if false positives are allowed.
 Data Compression: How to effectively compress large boolean matrices or transactional databases is becoming an increasingly im-portant research topic as the size of databases is growing at a very fast pace. For instance, in [9], Johnson et al. tries to reorder the rows and columns so that the consecutive 1  X  X  and 0  X  X  can be com-pressed together. Our work differs because compression is con-cerned only with reducing data representation size; our goal is sum-marization, with aims to emphasize the important characteristics of the data.
In the following, we prove the complexity of the succinct sum-marization problem and several of its variants. We begin the prob-lem with no false positives and extend it to false positive cases in corollary 1 and theorem 4. Even though these problems can quickly be identified as variants of the set-covering problem, proving them to be NP-hard is non-trivial as we need to show that at least one of the NP-hard problems can be reduced to these problems. Due to space limitations, we will only provide the major proof for the suc-cinct summarization. The NP-hardness proof for its variants can be found in our technical report.

T HEOREM 1. Given DB , it is an NP-hard problem to construct a CDB of minimal cost which covers DB .

Proof: To prove this theorem, we reduce the minimum set cover problem, which is NP-hard, to this problem.

The minimum set cover problem can be formulated as: Given a collection C of subsets of a finite set D , what is the minimum | C such that C  X   X  C and every element in D belongs to at least one member of C  X  .

The reduction utilizes the database DB , whose entire set of items is D , i.e., each element in D corresponds to a unique item in DB . All items in a set c  X  C is recorded in 10 | c | transactions in DB , denoted collectively as a set T c . In addition, a special transaction w in DB contains all items in D . Clearly, this reduction takes poly-nomial time with respect to C and D . Note that we will assume that there is only one w in DB containing all items in D . If there were another one, it would mean there is a set c in C which covers the entire D ; the covering problem could be trivially solved in that case. We will also assume each set c is unique in C .

Below we show that if we can construct a CDB with minimum cost, then we can find the optimal solution for the minimum set cover problem. This can be inferred by the following two key ob-servations, which we state as lemmas 1 and 2.

L EMMA 1. Let CDB be the minimal covering of DB . Then, all the T c transactions in DB which record the same itemset c  X  C will be covered by a single hyperrectangle T i  X  I i  X  CDB , i.e. T  X  T i and c = I i .

L EMMA 2. Let CDB be the minimal covering of DB . Let transaction w which contains all the items in D be covered by k hyperrectangles in CDB , T 1  X  I 1 ,  X   X   X  , T k  X  I k . Then each of the hyperrectangles is in the format of T c  X  { w }  X  c , c  X  C . Further, the k itemsets in the hyperrectangles, I 1 ,  X   X   X  , I k , correspond to the minimum set cover of D .

Putting these two lemmas together, we can immediately see that the minimal CDB problem can be used to solve the minimum set cover problem. Proofs of the two lemmas can be found in our tech-nical report. 2
Several variants of the above problem turn out to be NP-hard as well.
T HEOREM 2. Given DB , it is an NP-hard problem to construct a CDB with no more than k hyperrectangles that maximally cov-ers DB .

T HEOREM 3. Given DB and a budget  X  , it is an NP-hard prob-lem to construct a CDB that maximally covers DB with a cost no more than  X  , i.e., cost ( CDB )  X   X  .

COROLLARY 1. When false positive coverage is allowed with problems in theorems 1, 2, and 3 are still NP-complete.

Assuming a set of hyperrectangles is given, i.e., the rectangles used in the covering database must be chosen from a predefined set, we can prove all the above problems are NP-hard as well.
T HEOREM 4. Given DB and a set S of candidate hyperrect-angles such that CDB  X  S , it is NP-hard to 1) construct a CDB with minimal cost that covers DB ; 2) construct a CDB to maxi-mally cover DB with | CDB |  X  k ; 3) construct a CDB to maxi-mally cover DB with minimal cost where cost ( CDB )  X   X  , (  X  is a user-defined budget). The same results hold for the false positive
In this section, we develop algorithms to find minimal covering database CDB for a given transactional database with no false pos-itives. As we mentioned before, this problem is closely related to the traditional weighted set covering problem. Let C be a candi-date set of all possible hyperrectangles, which cover a part of the DB without false positive, i.e., C = { T i  X  I i : T i  X  I Then, we may apply the classical greedy algorithm to find the mini-mal set cover, which essentially correspond to the minimal covering database, as follows.

Let R be the covered DB (initially, R =  X  ). For each possible hyperrectangle T i  X  I i  X  C , we define the price of H as: At each iteration, the greedy algorithm picks up the hyperrectangle H with the minimum  X  ( H ) (the cheapest price) and put it CDB . Then, the algorithm will update R accordingly, R = R  X  T i The process continues until CDB completely covers DB ( R = DB ). It has been proved that the approximation ratio of this al-gorithm is ln ( k ) + 1 , where k is the number of hyperrectangles in CDB [6].

Clearly, this algorithm is not a feasible solution for the minimal database covering problem due to the exponential number of can-didate hyperrectangles in C , which in the worst case is in the order of 2 |T | + |I| , where T and I are the sets of transactions and items in DB , respectively. To tackle this issue, we propose to work on a smaller candidate set, denoted as where F  X  is the set of all frequent itemsets with minimal support level  X  , and I s is the set of all singleton sets (sets with only one item). We assume F  X  is generated by Apriori algorithm. Essen-tially, we put constraint on the columns for the hyperrectangles. As we will show in the experimental evaluation, the cost of the mini-mal covering database tends to converge as we reduce the support level  X  . Note that this reduced candidate set is still very large and contains an exponential number of hyperrectangles. Let T ( I the transaction set where I i appears. | T ( I i ) | is basically the sup-port of itemset I i . Then, the total number of hyperrectangles in C is Thus, even running the aforementioned greedy algorithm on this reduced set C  X  is too expensive.

In the following, we describe how to generate hyperrectangles by an approximate algorithm which achieves the same approxima-tion ratio with respect to the candidate set | C  X  | , while running in polynomial time in terms of | F  X   X  I s | and T .
As we mentioned before, the candidate set C  X  is still exponential in size. If we directly apply the aforementioned greedy algorithm, it will take an exponential time to find the hyperrectangle with the cheapest price. The major challenge is thus to derive a polynomial-time algorithm that finds such a hyperrectangle. Our basic idea is to handle all the hyperrectangles with the same itemsets together as a single group. A key result here is we develop a polynomial time greedy algorithm which is guaranteed to find the hyperrectan-gle with the cheapest price among all the rectangles with the same itemsets. Since we only have | F  X   X  I s | such groups, we can then find the globally cheapest rectangle in C  X  in polynomial time.
Specifically, let C  X  = { T ( I i )  X  I i } , I i  X  F  X   X  I is the set of all supporting transactions of I i . We can see that C can easily be generated from C  X  , which has only polynomial size O ( | ( F  X   X  I s ) | ) .

The sketch of this algorithm is illustrated in 1. Taking C put, the HY P ER algorithm repeatedly adds sub-hyperrectangles to set R . In each iteration (Lines 4 -7 ), it will find the lowest priced sub-hyperrectangle H  X  from each hyperrectangle T ( I i )  X  I (Line 4 ), and then select the cheapest H  X  from the set of selected sub-hyperrectangles (Line 5 ). H  X  will then be added into CDB (Line 6 ). Set R records the covered database DB . The process continues until CDB covers DB ( R = DB , line 3 ).
 Algorithm 1 HYPER( DB , C  X  ) 1: R :=  X  ; 2: CDB :=  X  ; 3: while R 6 = DB do 8: end while 9: return CDB
The key procedure is OptimalSubHyperRectangle , which will find the sub-hyperrectangle with the cheapest price among all the sub-hyperrectangles of T ( I i )  X  I i . Algorithm 2 sketches the pro-cedure. The basic idea here is that we will decompose the hyper-rectangle T ( I i )  X  I i into single-transaction hyperrectangles H { t }  X  I i where t j  X  T ( I i ) . Then, we will order those rectan-gles by the number of their uncovered cells (Lines 1  X  4 ). We will perform an iterative procedure to construct the sub-hyperrectangle with cheapest price (Lines 6-13). At each iteration, we will simply choose the single-transaction hyperrectangle with maximal number of uncovered cells and try to add it into H  X  . If its addition can de-crease  X  ( H  X  ) , we will add it to H  X  . By adding H s into H  X  = T i  X  I i , H  X  will be updated as H  X  = ( T i We will stop when H s begins to increase H  X  .
 Algorithm 2 A Greedy Procedure to Find the Sub Hyper Rectangle with Cheapest Price 2: calculate the number of uncovered cells in H s , | H s 3: end for 6: while U 6 =  X  do 7: pop a single-transaction hyperrectangle H s from U ; 9: break; 10: else 12: end if 13: end while Figure 1: A hyperrectangle H  X  C  X  . Shaded cells are covered by hyperrectangles currently available in CDB .

Here is an example. Given hyperrectangle H  X  C  X  , consisting of H = T ( I )  X  I = { t 1 , t 3 , t 4 , t 6 , t 8 , t 9 } X { i struct H  X  with minimum  X  ( H  X  ) in the following steps. First, we order all the single-transaction hyperrectangles according to their uncovered cells as follows: { t 4 } X  I , { t 8 } X  I , { t { t 3 }  X  I , { t 9 }  X  I . Beginning with H  X  = { t 4 }  X  I , the price  X  ( H  X  ) is (4 + 1) / 4 = 5 / 4 = 1 . 25 .
 If we add { t 8 }  X  I ,  X  ( H  X  ) falls to 5+1+0 4+4 = 6 8 If we add { t 1 }  X  I ,  X  ( H  X  ) decreases to 6+1+0 8+2 = If we add { t 6 }  X  I ,  X  ( H  X  ) decreases to 7+1+0 10+2 However, if we then add { t 3 } X  I ,  X  ( H  X  ) would increase to 13 = 0 . 69 . Therefore we stop at the point where H I and  X  ( H  X  ) = 0 . 67 .
 Properties of HYPER: We discuss several properties of HYPER, which will prove its approximation ratio.

L EMMA 3. The OptimalSubHyperRectangle procedure finds the minimum  X  ( H  X  ) for any input hyperrectangle T ( I i )  X  I Proof: Let H  X  = T i  X  I i be the sub-hyperrectangle of T ( I I with the least  X  ( H  X  ) . Then, we first prove that if a single-transaction H j = { t j }  X  I i  X  H  X  , then for any other single-transaction H k = { t k }  X  I i , t k  X  T ( I i ) , if then H k will be part of H  X  . By way of contradiction, without loss of generality, let us assume H k is not in H  X  . Then, we have  X  ( H  X  ) = ( x + 1)( y + | H k \ R | + | H j \ R | )  X  ( x + 2)( y + | H This shows that we can add H k into H  X  to reduce the price (  X  ( H This contradicts the assumption that H  X  is the sub-hyperrectangle of T ( I i )  X  I i with minimal cost. This suggests that we can find the lowest cost H  X  by considering the addition of single-transaction hy-perrectangles in T ( I i )  X  I i , ordered by their number of uncovered cells. 2
COROLLARY 2. In OptimalSubHyperRectangle, if two single-transaction hyperrectangles with the same number of uncovered be added into H  X  or none of them.

COROLLARY 3. Assume that in iteration j of the while loop in the OptimalSubHyperRectangle procedure, we choose H j { t }  X  I i . We denote a j = |{ t j }  X  I i |\ R | , and let H with minimum  X  ( H ) contain the single-transaction hyperrectan-gles H 1 , H 2 ,  X   X   X  , H q . Then we have a q +1 &lt; P Proof: We know adding H q +1 to H  X  will increase  X  ( H  X  ( H  X  ) = x y before adding H q +1 into H  X  . According to the al-know that x = q + | I i | and y = P q i =1 a i . Therefore a The above two corollaries can be used to speed up the Optimal-SubHyperRectangle procedure. Corollary 2 suggests that we can process all the single-transaction hyperrectangles with the same number of uncovered cells as a single group. Corollary 3 can be used to quickly identify the cutting point for constructing H
Lemma 3 leads to the major property of the HYPER algorithm, stated as Theorem 5.

T HEOREM 5. The HY P ER algorithm has the exact same so-lution as the greedy approach for the weighted set covering prob-lem and has ln ( k )+1 approximation ratio with respect to the can-didate set C  X  .
 Time Complexity of HYPER: Here we do not take it into account the time to generate F  X  , which can be done through the classic Apriori algorithm. Assuming F  X  is available, the HY P ER al-the number of hyperrectangles in CDB . The analysis is as fol-lows. Assume the while loop in Algorithm 1 runs k times. Each time it chooses a H  X  with minimum  X  ( H  X  ) from C  X  , which con-tains no more than | F  X  | + |I| candidates. To construct H minimum  X  ( H  X  ) for H , we need to update every single-transaction hyperrectangle in H , sort them and add them one by one, which Since we need to do so for every hyperrectangle in C  X  , it takes that k is bounded by ( | F  X  | + |I| )  X  |T | since each hyperrectangle in C  X  can be visited at most |T | times. Thus, we conclude that our greedy algorithm performs in a polynomial time with respect to | F  X  | , |I| and |T | .
Although the time complexity of HY P ER is polynomial, it is still very expensive in practice since in each iteration, it needs to scan the entire C  X  to find the hyperrectangle with cheapest price. Theorem 6 reveals an interesting property of HY P ER , which leads to an effective pruning technique for speeding up HYPER significantly (up to | C  X  | = | F  X   X  I| times faster!).
T HEOREM 6. For any H  X  C  X  , the minimum  X  ( H  X  ) output by OptimalSubHyperRectangle will never decrease during the pro-cessing of the HYPER algorithm.
 Proof: This holds because the covered database R is monotonically increasing. Let R i and R j be the covered database at the i -th and j -th iterations in HYPER, respectively ( i &lt; j ). Then, for any H T  X  I i  X  T ( I i )  X  I i = H  X  C  X  , we have respectively. 2 Algorithm 3 HYPER( DB , C  X  ) 1: R  X  X  X  ; 2: CDB  X  X  X  ; 5: while R 6 = DB do 12: end while 15: call OptimalSubHyperRectangle to find the updated minimum 16: end while 17: return CDB ;
Using Theorem 6, we can revise the HY P ER algorithm to prune the unnecessary visits of H  X  C  X  . Simply speaking, we can use the minimum  X  ( H  X  ) computed for H in the previous itera-tion as its lower bound for the current iteration since the minimum  X  ( H  X  ) will be monotonically increasing over time .

Our detailed procedure is as follows. Initially, we compute the minimum  X  ( H  X  ) for each H in C  X  . We then order all H into a queue U according to the computed minimum possible price (  X  ( H from the sub-hyperrectangle of H . To find the cheapest hyperrect-angle, we visit H in the order of U . When we visit H , we call the OptimalSubHyperRectangle procedure to find the exact H the minimum price for H , and update its lower bound as  X  ( H We also maintain the current overall minimum price for the H vis-ited so far. If at any point, the current minimum price is less than the lower bound of the next H in the queue, we will prune the rest of the hyperrectangles in the queue.

Algorithm 3 shows the complete HYPER algorithm which uti-lizes the pruning technique.
In Section 4, we developed an efficient algorithm to find a set of hyperrectangles, CDB , to cover a transaction database. When false positive coverage is prohibited, the summarization is gener-ally not succinct enough for the high-level structure of the transac-tion database to be revealed. In this section, we study how to pro-vide more succinct summarization by allowing certain false pos-itive coverage. Our strategy is to build a new set of hyperrect-angles, referred to as the succinct covering database to cover the set of hyperrectangles found by HYPER. Let SCDB be the set of hyperrectangles which covers CDB , i.e., for any hyperrectangle H  X  CDB , there is a H  X   X  SCDB , such that H  X  H  X  . Let the false positive ratio of SCDB be where SCDB C is the set of all cells being covered by SCDB . Given this, we are interested the following two questions: 2. Given | SCDB | = k , how can we minimize both the false
We will focus on the first problem and we will show later that the same algorithm for the first problem can be employed for solving the second problem. Intuitively, we can lower the total cost by se-lectively merging two hyperrectangles in the covering set into one. We introduce the the merge operation (  X  ) for any two hyperrectan-gles, H 1 = T 1  X  I 1 and H 2 = T 2  X  I 2 , The net cost savings from merging H 1 and H 2 is To minimize cost ( CDB ) with given false positive constraint hyperrectangles in CDB together so that the merge can yield the best savings with respect to the new false positive coverage, i.e., for any two hyperrectangles H i and H j , arg max Algorithm 4 sketches the procedure which utilizes the heuristics.
The second problem tries to group the hyperrectangles in CDB into k super-hyperrectangles. We can see the same heuristic can be employed to merge hyperrectangles. In essence, we can re-place the while condition (Line 2 ) in Algorithm 4 with the condi-tion that SCDB has only k hyperrectangles. Finally, we note that the heuristic we employed here is similar to the greedy heuristic for the traditional Knapsack problem [10]. However, since we consider Algorithm 4 HYPER+( DB , CDB ,  X  ) 1: SCDB  X  CDB ; 3: find the two hyper rectangles H i and H j in SCDB whose merge is 5: end while 6: return SCDB ; only pair-wise merging, our algorithm does not have a guaranteed bound like the knapsack greedy algorithm. In addition, here we actually perform a random sampling merging in the experiments to speed up the algorithm. The full analysis will be available in our technical report. In Section 6, we show that our greedy algorithm works effectively for both real and synthetic transactional datasets.
In this section, we report our experimental evaluation on three real datasets and one synthetic dataset. All of them are publicly available from the FIMI repository 1 . The basic characteristics of the datasets are listed in Table 1. Borgelt X  X  implementation of the well-known Apriori algorithm [4] was used to generate frequent itemsets. Our algorithms were implemented in C++ and run on Linux 2.6 on an AMD Opteron 2.2 GHz with 2GB of memory.

In our experimental evaluation, we will focus on answering the following questions. 1. How can HYPER (Algorithm 3) and HYPER+ (Algorithm 4) 2. How can the false positive condition improve the summariza-3. How does the set of frequent itemsets at different minimum 4. When users prefer a limited number of hyperrectangles, i.e. 5. What is the running time of our algorithms?
To answer these questions, we performed a list of experiments, which we summarized as follows.
In this experiment, we study the summarization cost, the number of hyperrectangles, and the running time of HYPER and HYPER+ using the sets of frequent itemsets at different support levels.
In Figures 2(a) 2(b) 2(c), we show the summarization cost with respect to different support levels  X  on the chess, pumsb_star and T10I4D100K datasets. Each of these three figures has a total of six lines. Two of them are reference lines : the first reference line, http://fimi.cs.helsinki.fi/data/ named  X  X B X , is the value of | DB | , i.e. the number of cells in DB . Recall that in the problem formulation, we denote cost ( CDB |T | + | DB | and cost ( CDB V ) = |I| + | DB | . Thus, this reference line corresponds to the upper bound of any summarization cost. The other reference line, named  X  X in_possible_cost X , is the value of |T | + |I| . This corresponds to the lower bound any summariza-tion can achieve, i.e. SCDB contains only one hyperrectangle T  X  I . The  X  X DB X  line records the cost of CDB being produced by HYPER. The  X  X CDB_0.1 X ,  X  X CDB_0.2 X , and  X  X CDB_0.4 X  lines record the cost of SCDB being produced by HYPER+ with 10% , 20% , and 40% false positive budget.

Accordingly, in Figures 2(d) 2(e) 2(f), we show the number of hyperrectangles (i.e. k ) in the covering database CDB or SCDB at different support levels. The  X  X DB X  line records | CDB | , and the  X  X CDB_0.1 X , X  X CDB_0.2 X ,  X  X CDB_0.4 X  lines record | SCDB | being generated by HYPER+ with 10% , 20% , and 40% false posi-tive budget.

Figures 2(g) 2(h) 2(i) shows the running time. Here the line  X  X DB X  records the running time of HYPER generating CDB from DB . The  X  X CDB-0.1 X ,  X  X CDB-0.2 X , and  X  X CDB-0.4 X  lines record the running time of HYPER+ generating SCDB under 10% , 20% , 40% false positive budget respectively. Here, we include both the time of generating CDB from DB (HYPER) and SCDB from CDB (HYPER+). However, we do not count the running time of Apriori algorithm that is being used to generate frequent itemsets.
Here, we can make the following observations: 1. The summarization cost reduces as the support level  X  de-2. The summarization cost and the number of hyperrectangles 3. One of the most interesting observations is the  X  X hreshold be-
In this experiment, we will construct a succinct summarization with varying limited numbers of hyperrectangles ( k ). We perform the experiments on chess, mushroom and T10I4D100K datasets. We vary the number of k from around 100 to 10 .

In Figures 2(m) 2(n) 2(o), each graph has two lines which cor-respond to two different minimum support levels  X  for generating SCDB . For instance, support_15 is the 15% minimal support for the HYPER+.

Here in Figures 2(j) 2(k) 2(l) , we observe that the summarization costs converge towards minimum possible cost when k decreases. This is understandable since the minimum possible cost is achieved when k = 1 , i.e., there is only one hyperrectangle T  X I in SCDB . In the meantime, we observe that the false positive ratio increases when k decreases. Especially, we observe a similar threshold be-havior for the false positive ratio. This threshold again provides us a reasonable choice for the number of hyperrectangles to be used in summarizing the corresponding database.

We also observe that the sparse datasets, like T10I4D100K, tends to have a rather higher false positive ratio. However, if we com-pare with the worst case scenario, where only one hyperrectan-gle is used, the false positive ratio seems rather reasonable. For instance, the maximum false positive ratio is around 10000% for T10I4D100K, i.e., there is only around 1% ones in the binary ma-trix. Using the minimal support 0 . 5% and k = 200 , our false positive ratio is less than 500% , which suggests that we use around 6% of the cells in the binary matrix to summarize T10I4D100K.
In this paper, we have introduced a new research problem to succinctly summarize transactional databases. We have formulated this problem as a set covering problem using overlapped hyperrect-angles; we then proved that this problem and its several variations are NP-hard. We have developed two novel algorithms, HY P ER and HY P ER + to effectively summarize the transactional database. In the experimental evaluation, we have demonstrated the effec-tiveness and efficiency of our methods. In particular, we found in-teresting  X  X hreshold behavior X  and  X  X onvergence behavior X , which we believe can help us generate succinct summarizations in terms of the summarization cost, the number of hyperrectangles, and the computational cost. In the future, we plan to investigate those be-haviors analytically and thus produce better summarizations. We also plan to apply this method on real world applications, such as microarray data in bioinformatics, for which we conjecture the hy-perrectangles may correspond to certain biological process. [1] Foto N. Afrati, Aristides Gionis, and Heikki Mannila. [2] R. Agrawal and R. Srikant. Fast algorithms for mining [3] Rakesh Agrawal, Johannes Gehrke, Dimitrios Gunopulos, [4] Christan Borgelt. Apriori implementation. [5] Varun Chandola and Vipin Kumar. Summarization -[6] V. Chv X tal. A greedy heuristic for the set-covering problem. [7] Christos Faloutsos and Vasileios Megalooikonomou. On data [8] Byron J. Gao and Martin Ester. Turning clusters into [9] David Johnson, Shankar Krishnan, Jatin Chhugani, Subodh [10] D. Pisinger Kellerer, Hans; U. Pferschy. Knapsack Problems . [11] Laks V. S. Lakshmanan, Raymond T. Ng, Christine Xing [12] Tao Li. A general model for clustering binary data. In KDD , [13] Sara C. Madeira and Arlindo L. Oliveira. Biclustering [14] Jian Pei, Guozhu Dong, Wei Zou, and Jiawei Han. Mining [15] Arno Siebes, Jilles Vreeken, and Matthijs van Leeuwen. Item [16] Michael Steinbach, Pang-Ning Tan, and Vipin Kumar.
 [17] Matthijs van Leeuwen, Jilles Vreeken, and Arno Siebes. [18] Jilles Vreeken, Matthijs van Leeuwen, and Arno Siebes. [19] Jianyong Wang and George Karypis. On efficiently
