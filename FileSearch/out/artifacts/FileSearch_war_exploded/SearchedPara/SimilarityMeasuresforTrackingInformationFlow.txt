 Text similarity spans a spectrum, with broad topical similarity near one extreme and document identity at the other . Intermediate levels of similarity  X  resulting from summarization, paraphrasing, cop y-ing, and stronger forms of topical rele vance  X  are useful for appli-cations such as information o w analysis and question-answering tasks. In this paper , we explore mechanisms for measuring such intermediate kinds of similarity , focusing on the task of identifying where a particular piece of information originated. We consider both sentence-to-sentence and document-to-document comparison, and have incorporated these algorithms into RECAP , a prototype in-formation o w analysis tool. Our experimental results with indicate that new mechanisms such as those we propose are lik ely to be more appropriate than existing methods for identifying the intermediate forms of similarity .
 Categories and Subject Descriptors: H.3.3 [Information Storage and Retrie val]: Information Search and Retrie val General Terms: Algorithms, Experimentation, Theory Keyw ords: Text reuse, information o w, statistical translation
A text collection such as a newswire archi ve or web cra wl typ-ically contains a great deal of repeated information. Dif ferent au-thors may each present versions of a story or event; the same event may get presented in dif ferent ways for dif ferent audiences; and the facts of an event may get recapitulated each time it is dis-cussed. Sometimes such presentations have little in common with each other but the broad subject matter; at other times one may be a cop y of the other with minor edits.
 Given a topic of interest, a suf ciently extensi ve archi ve may Cop yright 2005 ACM 1 X 59593 X 140 X 6/05/0010 ... $ 5.00. contain much of the history of the topic. In particular , the archi ve might plausibly be used to identify when particular ideas or state-ments rst originated. In this work, our interest is in exploring whether we can identify alternati ve versions of the same informa-tion. It is not clear , howe ver, that standard approaches to informa-tion retrie val or cop y detection can be used for this task.
The extent to which passages of text are considered similar to each other can be thought of as lying some where on a similarity spectrum . At one end of this spectrum is identity; two documents that are the same as each other in every way clearly have the high-est level of similarity possible. Disco very of such documents is the aim of systems for detecting plagiarism or co-deri vation [Bernstein and Zobel, 2004; Broder et al., 1997; Heintze, 1996; Hoad and Zo-bel, 2003; Manber , 1994; Shi vakumar and Garc  X  a-Molina, 1995]. The other end of the spectrum is the standard task of information retrie val: two documents are a match if the y are topically related to the same information need.

Past research has lar gely focused on applications at one or other extreme of the spectrum, and there has been relati vely little inves-tig ation of similarity tasks between these two extremes. Ho we ver, in some applications, intermediate forms of similarity are of clear value. Examples include disco very of documents that summarize or paraphrase other documents, documents that are co-deri ved (that is, contain suf ciently similar material that the y must at some point have come from the same source) and documents that share struc-ture, or statements of fact.

In this paper , we explore the similarity spectrum in the conte xt of our information o w analysis tool, RECAP [Metzler et al., 2005]. The objecti ve of the RECAP project is to develop methods for track-ing and analyzing the o w of facts and concepts through a text cor -pus. In order to create such a tool, we need a similarity measure that can reliably identify passages or sentences that share concepts and facts, that is, where information has been reused . This level of semantic resemblance is signicantly stronger than simple topical similarity , but does not impose the syntactic similarity constraints typical of cop y detection systems. Thus, we need to be able to ac-curately discern similarity in the middle portion of the similarity spectrum. Furthermore, the nature of the task requires that matches be evaluated at the sentence level, a further variation on the more usual document-to-document similarity measures.

We propose a range of approaches to reuse detection at the sen-tence level, and a range of approaches for combining sentence-level evidence into document-le vel evidence. To evaluate these approaches, we emplo y a hierarch y of ve similarity levels:  X unre-lated X ;  X on the general topic X ;  X on the specic topic X ;  X same facts X ; and  X copied X . These levels can be regarded as points on the simi-larity spectrum.

Using rele vance assessments at these similarity levels, we found mark ed dif ferences in performance between the methods consid-ered. The best methods were highly effecti ve. Some of these meth-ods were simple techniques that we introduced to establish whether such matching was feasible and whether it could be assessed. As a result, our research has a broad range of outcomes, including meth-ods for reuse detection; measures of the quality of reuse detection; and a demonstration that reuse detection is both meaningful and feasible. As an aside, we have disco vered that there is a high rate of reuse in the standard text collections used for information retrie val experiments.

Our work is to some extent exploratory rather than deniti ve, in that this problem has not been investig ated before. On the other hand, our results sho w that even the preliminary methods we de-scribe are suitable for reuse detection in practice.
Our research is moti vated by the desire to develop effecti ve meth-ods for identifying and tracking idea and fact reuse within a collec-tion. A user who is bro wsing a document should be able to select a sentence or group of sentences and be presented with a history of where the ideas and facts in the sentences were used else where in the collection. In some cases, these ideas or facts may be alterna-tive presentations of the same concepts; in other cases, much the same wording may be used, demonstrating that the statements have a common origin.

Such an application would be of great use to information ana-lysts in both military and civilian elds. It would allo w the origin and o w of specic facts and concepts through a text corpus to be analyzed and visualized. With additional information such as datestamps, such a tracking system could help establish when in-formation was rst kno wn and in what conte xt.

We have created a prototype tool for this task called RECAP an example of the uses of this softw are, suppose a user is bro ws-ing an article discussing the 1980 eruption of Mount St. Helens in Washington State. The user encounters the follo wing sentence: The user is interested in nding other documents in the collection in which this information is used in the same way. Querying on this sentence with an appropriate tool should return the sentences sho wn in Table 1. These sentences illustrate dif ferent kinds of fact and concept reuse. Of possible interest to an information analyst is that the same idea appeared in an article that was published almost two years before the query article, using very similar wording. This may lead the analyst to the primary source for this fact or concept.
Note that all the sentences in Table 1 are clear examples of in-formation reuse, and it is dif cult to belie ve that these sentences were written independently of one another . Ho we ver, there are sig-nicant dif ferences in the presentation of information between the sentences. The degree of similarity between such passages is higher than simple topical overlap, but some what lower than the syntactic resemblance required by cop y detection systems. This moti vates us to explore the similarity spectrum, assessing the effecti veness of various techniques at detecting passage similarity at this level.
Furthermore, due to the nature of the application and also to the nature of the type of similarity we are hoping to nd, we are ini-tially interested in working at sentence-le vel, rather than document-level, granularity . Facts and ideas are, in general, cohesi ve struc-tural units. If a fact is presented in a particular sentence in one doc-ument, we expect it to be presented similarly in a single sentence in a corresponding document. This is dif ferent to topical similarity , in which the semantic sense of topicality may be broadly spread across the entire document.
Several techniques have been devised for estimating similarity of text passages (typically whole documents) to each other .
Relati ve-frequenc y measures [Hoad and Zobel, 2003; Shi vaku-mar and Garc  X  a-Molina, 1995] are a class of similarity functions based on comparison of relati ve frequenc y of word occurrences be-tween two passages of text. Two identical passages have identi-cal word frequencies relati ve to each other; insertions, deletions, and edits will slo wly degrade the value of such a relati ve-frequenc y score. In both cases where such measures have been used, the aim has been to detect cop ying between whole documents.
 Document ngerprinting [Brin et al., 1995; Broder et al., 1997; Heintze, 1996; Manber , 1994] techniques are also designed to de-tect cop ying. The y operate by passing a x ed-length sliding win-dow over a collection (a typical windo w size may be eight words) and storing a selection of these x ed-length chunks in an inverted inde x. Documents that have a number of such chunks in common are considered to be similar in the sense that there is cop ying.
The main dif ference between the various document ngerprint-ing techniques is in their choice of which chunks to inde x and which to discard. In general, unless no chunks are discarded, se-lection heuristics are lossy and systems are thus vulnerable to the possibility that all matching chunks between a pair of documents are discarded. The DECO system of Bernstein and Zobel used an efcient whole-collection analysis to discard only chunks that have no effect on determining similarity between documents [Bernstein and Zobel, 2004].

There are several standard approaches used for query-based in-formation retrie val that may also be effecti ve for reuse matching. Both vector -space and language modeling [Ponte and Croft, 1998] approaches are typically used for evaluating the rele vance of a doc-ument to a given (usually short) query . Substituting a document for a query allo ws these methods to give an estimate of the similarity between two documents. Sanderson used a standard vector -space algorithm in exactly this way, using whole documents as queries in an attempt to nd duplicate documents in a newswire collection [Sanderson, 1997].

Probably the main body of work concerning retrie val and com-parison of text at the sentence level is that which addresses the TREC novelty track [Harman, 2002; Soborof f and Harman, 2003]. The TREC novelty track is a forum for promoting the identica-tion of novel information in a result list. In the years that the track has run, the task has involv ed returning a list of sentences that are both rele vant to a given query and novel with respect to the sen-tences that have come before it. As such, a successful attempt at this task must have an effecti ve way of scoring sentences. Allan et. al analyze a number of dif ferent methods at the sentence level [Allan et al., 2003]. Ho we ver, it is to be noted that the correspon-dence with our task is not exact: the portion of a novelty system that scores for rele vance compares a sentence to a query , while the portion that compares sentences to sentences is attempting to score for novelty , which is a dif ferent  X  in fact, nearly opposite  X  notion to similarity .

The topic detection and tracking (TDT) initiati ve [Allan et al., 1998] is comprised of tasks in which similarity classication is required, sometimes at the sentence level. TDT consists of three classes of task: story segmentation, topic detection, and topic track-ing. The rst of these tasks, segmentation, require a stream of sen-tences from a news source to be divided up into distinct stories. Topic detection is an unsupervised learning task requiring stories discussing a new topic to be agged as the y come in, or the en-tire corpus to be retrospecti vely clustered by topic. Topic tracking is a supervised learning task in which stories must be assessed for membership of a number of predened topics.

The pre vious work is of rele vance to reuse detection from sev-eral perspecti ves. The segmentation task requires sentences to be assessed for similarity to other sentences in a story; topic detection and topic tracking both demand a level of similarity between doc-uments in a cluster that is signicantly stronger than broad topical overlap, and answers are expected to discuss the same event.
In this section we examine several approaches to evaluating the level of similarity between a pair of sentences. These are intended as a sample of the various methodologies that might be considered for evaluating text similarity .

All of the techniques calculate a similarity score S ( Q; R ) tween a query sentence Q and a candidate sentence R , intended to capture numerically the extent to which the y con vey the same information. The objecti ve is to be able to calculate S ( Q; R ) all sentences R in a collection and kno w that when the score maximized, the sentence R has a high degree of similarity to the query sentence Q .
 As a baseline measure we chose a simplistic word overlap fraction; that is, the proportion of words in Q that also appear in the candi-date sentence R : where j Q \ R j is the number of terms that appear both in The intuition here is simple  X  if two sentences have man y terms in common then the y are lik ely to be similar to some degree.
We also experimented with a variant of the word overlap measure in which the score was adjusted to tak e inverse document frequenc y into account: This models the fact that high IDF terms are typically stronger in-dicators of shared heritage between two sentences than are low IDF terms.
 TF-IDF measures are a broad class of functions used for estimating rele vance and similarity typically between queries and documents. The fundamental intuitions are that the more frequently a word ap-pears in a passage, the more indicati ve that word is of the topicality of that passage; and that the less frequently a term appears in a collection, the greater its power to discriminate between interesting and uninteresting passages.

Standard TF-IDF formulations, such as Okapi BM25 [Robertson et al., 1992], may not be appropriate here since we are focusing on sentence-le vel similarity . Therefore, we adopt the formulation used by Allan et al. for the TREC novelty track, which was sho wn to consistently  X  but not signicantly  X  outperform language model-ing based approaches for nding topically similar sentences [Allan et al., 2003]. The similarity function is: where tf tence Q ; tf date sentence R ; N is the total number of documents in the collec-tion; and df As discussed, relati ve-frequenc y measures have been sho wn to per -form well at nding co-deri vative documents. In this work we investig ate how well such methods work at nding co-deri vative pieces of text at the sentence level. We use a simple variation of the identity measure of Hoad and Zobel [2003]:
S ( Q; R ) = 1 with the various quantities dened as abo ve. The numerator is a standard IDF factor , while the denominator contains two parts, one designed to penalize inequalities in the relati ve frequenc y of a word between the two sentences, and the other to penalize dif ferences in the overall lengths of the sentences.
 Translation transforms text in one language to text in another , with the aim of preserving as much of the semantics of the original as possible. This is a reasonable model for the process that occurs when text is summarized, paraphrased, or otherwise has its facts and concepts reused, moti vating the investig ation of sentence sim-ilarity at the level of fact and concept reuse as an act of translation.
Statistical machine translation systems [Bro wn et al., 1993] aim to generate high-quality translations of sentences between natural languages. Such systems mak e use of parameterized statistical lan-guage models of both source and tar get language, and a parame-terized statistical translation model that estimates the probability that a given tar get sentence is a translation of the source sentence. Given these models and a parameterization, the system searches a space of possible translations and returns the sentence with the highest probability .

We propose using statistical translation models in much the same manner to estimate the probability that one sentence is a translation of another . This translation probability will then serv e as the basis of the similarity score for pairs of sentences.

Given an alignment A of corresponding words between the query sentence Q and tar get sentence R , and a distrib ution of term trans-lation probabilities P culated by taking a product of the translation probabilities of the aligned words: where P lation of length j Q j ; q term in sentence R that aligns to the i th term in sentence P ( A j R ) is the probability of an alignment given sentence
IBM' s Translation Model 1 assumes that the alignment between words in the two sentences is equi-probable and that no length dis-trib ution is favored over any other . That is, both P ( A j R ) P ( j Q j j R ) are uniform. After some algebraic manipulation, this leads to the follo wing form for the similarity function:
The original translation model assumes that each sentence has a special null term at position 1 ; this is the reason that the summation iterates through j R j + 1 terms. The null term is used to represent the fact that the current term in Q does not align to any terms in
We mak e the distrib utional assumption that P C ) , where C is the background model inferred from the collection as a whole. This proceeds from the intuition that  X  in the absence of any other evidence  X  an unaligned word is lik ely to be present in a sentence with a probability equal to its overall probability in the more generalized background language model. The probability of aligning to the null term dictates the inuence of the background language model on the resulting translation. The uniform distrib u-tional assumption on alignment means that the effecti ve probability that a term in Q will align to the null term is 1 = ( j R j +1) generalize the original model by assuming there exist null terms in each sentence, where is a non-ne gative inte ger . This results in each sentence having length j R j + , where j R j is the number of non-null terms in R . This model can be described as: S ( Q; R ) = 1 We now mak e the further simplifying assumption that each word translates to itself; that is, P be sho wn that this results in the follo wing form: giving precisely the language modeling query lik elihood ranking function using Dirichlet smoothing with smoothing parameter [Zhai and Laf ferty , 2001]. With = 1 , we get Ber ger and Laf-ferty' s Translation Model 0 [Ber ger and Laf ferty , 1999].
The models discussed here rely on strong simplifying assump-tions, in particular , the assumption that every term only translates to itself. Given a good thesaurus, it may be possible to impro ve on these models by incorporating a more rened estimate of the true translation probabilities. At present, we have established and moti vated a generalized and extensible frame work for represent-ing sentence-le vel similarity in terms of translation probabilities, and sho wn that the query lik elihood model under various types of smoothing is precisely an instantiation of this frame work. Other pa-pers have sho wn such a connection, but have articially introduced smoothing into the mix [Murdock and Croft, 2004]. We sho w that smoothing falls naturally out of the translation model itself under plausible assumptions. This pro vides a solid theoretical moti vation for using this model for evaluating strong sentence-le vel similarity .
In this paper we explore = 1 (Translation Model 0) and = 2500 (query lik elihood). The parameter can be vie wed as a knob that allo ws us to control the type of queries (translations) given a high probability for some document. As approaches 0, the model becomes a coordinate level (w ord overlap) measure that will lik ely be good at nding exact matches. At the other extreme, as gets lar ge more background terms are allo wed, which is lik ely (and kno wn to be) good at nding topically rele vant matches.
In order to explore the similarity spectrum at the sentence level, we devised the six-point qualitati ve similarity rating scale sho wn in Table 2. We belie ve that this scale accurately covers the similarity spectrum, and allo ws us to experiment with dif ferent techniques and evaluate their effecti veness at dif ferent levels of similarity . bronze likeness of the electr onics genius was dedicated in the U.S. Capitol.  X 
In general similarity matching, applications would dra w a simi-larity threshold at the boundary between cate gories 0 and 1, while cop y-detection applications would dene the boundary between cate gories 4 and 5. It is cate gories 2, 3 and 4, in which there is signicant semantic, factual and structural overlap, that dene the middle-ground of the similarity spectrum.

In order to evaluate the effecti veness of the various systems de-scribed abo ve, we created a set of 50 single-sentence queries from the topics used by the TREC question answering track. Using CAP , which is built on the Indri search engine [Metzler et al., 2004], the techniques described abo ve were run against the full TREC newswire collection, which is composed of: Associated Press ar-ticles (1988-1990), the Financial Times (1991-1994), the Los An-geles Times (1989-1990), the San Jose Mer cury Ne ws (1991), and the Wall Str eet Journal (1987-1992). The combined collection con-sists of 848,481 documents. All documents were stopped using a list of 418 common stopw ords, but not stemmed.

The top 25 rank ed results per query from each of the systems were placed into an evaluation pool, and each of the sentences in-dependently judged by two of the authors. In cases where the judg-ments disagreed, the conict was resolv ed by discussion. In total 2,711 indi vidual sentences were judged; the breakdo wn of judg-ments by similarity cate gory is presented in Table 3.

The judged sentences were then assigned to one of two cate-gories  X  similar or non-similar . The threshold at which this assign-ment took place  X  that is, the similarity level required for a sentence to be considered similar  X  was varied for the experiments. The nal  X cumulati ve X  column in Table 3 sho ws the number of sentences in the judged set that were considered rele vant at each threshold. All runs were then analyzed using the trec eval tool.

Figure 1 sho ws the mean average precision (MAP) for the var-ious techniques across the similarity thresholds. The graph sho ws that, for all the scoring methods, MAP increased as the similarity threshold became stricter . This is to be expected, as closely similar sentence pairs have more features in common that can be exploited by scoring techniques.

Of more interest is the relati ve performance of the various scor -ing functions at a given similarity threshold. At the general topic level (similarities of 1 and abo ve), query lik elihood was clearly the best performer . This is not une xpected, as much past research has sho wn query lik elihood to be effecti ve at identifying topicality . At the specic topic level (le vels 2 and over), query lik elihood still had the highest MAP , although its relati ve adv antage had lessened some what. At levels 3, 4 and 5, the relati ve performance dif ference between techniques was far smaller , but Translation Model 0 was consistently the most effecti ve.

The TF-IDF and identity measures were consistently poor . The other four measures  X  word overlap, IDF-weighted word overlap, Translation Model 0, and query lik elihood  X  were each at or near the highest level of effecti veness at one or more of the threshold levels tested. Ho we ver, the relati ve effecti veness of these four scor -ing functions between levels 2 and 3 was reversed. This means that it is dif cult to conclude that any one of the techniques is more ef-fecti ve in the middle similarity region. This may (or may not) be because all the functions tested were too closely modeled on tech-niques used for topical or syntactic similarity applications. Mean Average Precision Figur e 1: Mean average precision across similarity levels for sentence-le vel similarity measures.

It is also worth noting that the baseline word overlap function was quite competiti ve on level 2 similarity and equal best for level 3. This further suggests that none of the techniques are performing at a particularly sophisticated level in this region of the similarity spectrum.

Nonetheless, the functions tested are suf ciently effecti ve to ren-der the RECAP tool useful, and, despite our suspicion that the none of the scoring functions was ideal, in absolute terms the MAP val-ues are high enough to be used in a practical system. Because there is no clearly superior function, the current version of the softw are allo ws the user to choose which sentence-le vel scoring function to use.
A key hypothesis in this investig ation is that two documents that contain a signicant overlap of facts and concepts can be expected to contain pairs of corresponding sentences that score highly at the sentence-to-sentence level. This assumption suggests building doc-ument scores by combining indi vidual sentence-to-sentence scores in a bottom-up manner . The intuition is that, by examining seman-tic cohesion at the sentence level, we will be better able to dis-tinguish document pairs that share a common body of facts and concepts from those that simply have a strong general topicality .
Another benet of using a bottom-up approach is that the contri-bution of various sentences to the score is kno wn, allo wing corre-sponding blocks of concepts to be highlighted for the user . The CAP system pro vides a slider that allo ws the user to set a sentence-level threshold to lter out matches that are insuf ciently close.
We explored two dif ferent combination functions for calculating a bottom-up document similarity score S ( Q; D ) between two doc-uments Q and D . The rst of these, SUM , is an exhausti ve cross-alignment between all sentences in the two documents, similar in concept to Translation Model 1: where q and d are sentences in the query and document, respec-tively , S ( q; d ) is the similarity (or probability) between the query sentence s and document sentence d , and P ( d j D ) is the lik e-lihood (or weighting) of sentence d in D . In this case, all possi-ble sentence scores will contrib ute to the nal document similarity score. This means that, if a sentence has good correspondence to more than one sentence in the other document, all of these corre-spondences are able to mak e a contrib ution to the score. The disad-vantage is that the man y low scores caused by totally mismatching sentences will also contrib ute, possibly causing the function to be more susceptible to random noise.

Alternati vely , we can base the document score on the best sen-tence matches: We call this combination function MAX . We note that it is possible that a sentence d in document D may be the best scoring match for two (or possibly more) dif ferent query sentences.

This remo ves both the adv antage and the disadv antage discussed abo ve as only the best possible match is counted towards the score. This scoring function can be thought of as taking the score once the sentences have been optimally aligned between the two documents.
Our early experiments sho wed that the second maximizing com-bination function consistently outperforms the rst summing one, and it was adopted as the combination function for the experiments described in the next section. We also assumed that P ( d j D ) the sentence-weight distrib ution, is uniform, although variations are possible.
The aim of our next series of experiments was to examine the effecti veness of bottom-up document similarity scoring functions at evaluating document-le vel similarity in the middle of the similarity spectrum, in particular in identifying documents that share factual content. In the experiments we examine dif ferent sentence-le vel similarity measures S ( q; d ) . We compare these bottom-up schemes to a number of standard document-le vel similarity measures.
The methodology for these experiments was similar to that used for the experiment of sentence-le vel similarity functions, with the similarity spectrum for documents divided into the similarity levels described in Table 4.
 For these experiments we used a set of 40 documents from the TREC newswire collection that were kno wn to share facts and con-cepts with at least one other document in the collection. As with the sentence-le vel experiments, all techniques were run over the newswire collection for these 40 queries and the top 10 results ag-gre gated into a pool for human judgment. This resulted in a pool of 1,538 documents to be judged, with the work shared between two judges. Table 5 summarizes the judgments that were made, and denes three similarity levels in terms of the cate gories listed in Table 4.

We chose two representati ve document-le vel techniques to eval-uate as a basis for comparison. These techniques treat the entire document as a query rather than splitting it up into sentences. As pre viously discussed, the language-model deri ved query lik elihood function [Ponte and Croft, 1998] has frequently sho wn to be ef-fecti ve at identifying topical similarity . It was also sho wn in Sec-tion 5 to be effecti ve for sentence-le vel similarity , even at similarity thresholds in the middle of the similarity spectrum. The DECO tem [Bernstein and Zobel, 2004] has similarly been sho wn to be effecti ve and rob ust in detecting cases of syntactic similarity (te xt reuse). Thus, we have chosen as our baselines two techniques that have in the past been used for similarity assessment at the extreme ends of the similarity spectrum. Mean Average Precision Figur e 2: Mean average precision across varying types of similar -ity for each document-le vel measure.

In addition, we explored using the follo wing sentence-le vel simi-larity measures in a bottom-up fashion: Translation Model 0 (mt0); unweighted word overlap (overlap); query lik elihood (ql sent); and IDF-weighted overlap (idf overlap). The TF-IDF and relati ve fre-quenc y measures were not included because of their poor sentence-level effecti veness.

Figure 2 sho ws the MAP results for these experiments. Ov erall, the results for these experiments sho w that none of the bottom-up methods as tested were able to outperform both of the baseline all-of-document techniques at any of the three similarity thresholds.
Query lik elihood at the all-of-document level was the most effec-tive at levels 1 (topical similarity) and 2 (fact and concept reuse), while DECO was the most effecti ve at level 3 (near identity) and second best at level 2. As expected, DECO was poor at detecting broader topical similarity .

Of the bottom-up measures, IDF-weighted overlap and query lik elihood were the most effecti ve for levels 1 and 2, whereas Trans-lation Model 0 and unweighted word overlap were more effecti ve when the threshold was set at level 3. Thus, more hea vily smoothed measures did well when the similarity threshold was lower , whereas less smoothed measures were superior at matching documents that were near -identical. Not une xpectedly , for the bottom-up meth-ods there was a strong correlation between scoring functions that performed well at the sentence level and the effecti veness of the measures based on these functions. This suggests that if better sentence-le vel scoring functions were to be devised, an immediate impro vement in bottom-up scoring effecti veness would also result.
The inability of any of the bottom-up measures to signicantly outperform the two standard all-of-document measures is disap-pointing. Ho we ver, the dif ference in effecti veness  X  particularly when the similarity threshold is set at level 2  X  is not lar ge. There are two ways in which the bottom-up scoring methods can be made more effecti ve. The rst of these is to use a more effecti ve scoring function at the sentence level. Impro vements in the sentence level scores (as discussed abo ve) will most lik ely o w on immediately to impro ved effecti veness at the document level. The second area in which impro vements can be made is in the algorithms for aligning the sentences and combining the scores.
We have explored mechanisms for identifying passages of text that have varying degrees of similarity to a query passage. The ability to discern similarity between passages of text is valuable in a range of situations. Depending on the application, the threshold at which a pair of text passages are considered similar may vary . In general, a scoring technique that can effecti vely identify similar documents at one threshold of similarity might not be effecti ve for a dif ferent similarity threshold, so the similarity threshold appro-priate to an application plays an important role in determining an appropriate scoring technique.

Much past research has focused on nding effecti ve techniques for quantifying similarity at one or other extreme of the similarity spectrum  X  either broad topical similarity , or strong syntactic cor -respondence. Little research has focused on the intermediate points of the similarity spectrum, between these two extremes. In this pa-per we examined some of the issues involv ed in more thoroughly exploring these types of similarities, via the use of RECAP type tool for analyzing fact and concept reuse.

Our experiments on dif ferent similarity measures at the sentence level led to disco very of several reasonably effecti ve matching meth-ods, in particular those that are based on a simplication of the probabilistic translation model paradigm. Ho we ver, no one tech-nique was able to signicantly outperform the baseline measure, which was a simple measure of word overlap.

Our experiments with techniques that combine sentence-le vel scores in a bottom-up fashion for scoring document-to-document similarity sho wed that use of sentence-le vel evidence is a promis-ing area of future work. A particular benet of our bottom-up ap-proach is that it allo ws easy localization and presentation of pos-sible matches, and can be computed relati vely efciently . There are several avenues that may yield impro ved bottom-up methods. In particular , we intend to investig ate aggre gation and alignment methods from genomic search that may pro vide more precise scor -ing functions.

Ho we ver, our experiments  X  and our experience with RECAP have demonstrated that tracking of information reuse is practical and meaningful. The simple measures we have developed so far are able to accurately locate alternati ve instances of passages and, combined with timestamps, help a user to identify where a piece of information originated within a corpus. Further research may rene these methods, but the y are already suf cently effecti ve for use in practice.
 This work was supported in part by the Center for Intelligent Infor -mation Retrie val, in part by Adv anced Research and De velopment Acti vity and NSF grant #CCF-0205575, in part by the Australian Research Council, and in part by the ARC Center for Percepti ve and Intelligent Machines in Comple x En vironments. An y opin-ions, ndings and conclusions or recommendations expressed in this material are the authors and do not necessarily reect those of the sponsors.
 J. Allan, A. Boli var, and C. Wade. Retrie val and novelty detec-tion at the sentence level. In Proc. 26th Ann. International ACM
SIGIR Conf . on Resear ch and De velopment in Information Re-trie val , pages 314 X 321, 2003.
 J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang. Topic detection and tracking pilot study: Final report. In Proc. DARP A Broadcast Ne ws Transcription and Under standing Workshop , pages 194 X 218, 1998.
 A. Ber ger and J. Laf ferty . Information retrie val as statistical trans-lation. In Proc. 22nd Ann. International ACM SIGIR Conf . on
Resear ch and De velopment in Information Retrie val , pages 222 X  229, 1999.
 Y. Bernstein and J. Zobel. A scalable system for identifying co-deri vative documents. In Proc. String Processing and Informa-tion Retrie val Symp. , pages 55 X 67, 2004. Published as LNCS 3246.
 S. Brin, J. Da vis, and H. Garc  X  a-Molina. Cop y detection mecha-nisms for digital documents. In Proc. ACM SIGMOD Ann. Conf . , pages 398 X 409, 1995.
 A. Z. Broder , S. C. Glassman, M. S. Manasse, and G. Zweig. Syn-tactic clustering of the web . Computer Networks and ISDN Sys-tems , 29(8-13):1157 X 1166, 1997.
 P. F. Bro wn, V. J. D. Pietra, S. A. D. Pietra, and R. L. Mercer .
The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics , 19(2):263 X 311, 1993. D. Harman. Ov ervie w of the TREC 2002 novelty track. In Proc. 11th Text REtrie val Conf . (TREC 2002) . NIST , 2002.
 N. Heintze. Scalable document ngerprinting. In Proc. USENIX Workshop on Electr onic Commer ce , No vember 1996.
 T. Hoad and J. Zobel. Methods for identifying versioned and pla-giarised documents. Journal of the American Society of Infor -mation Science and Technolo gy , 54(3):203 X 215, 2003.
 U. Manber . Finding similar les in a lar ge le system. In Proc. USENIX Winter Technical Conf . , pages 1 X 10, San Fransisco, CA, USA, 17 X 21 1994.
 D. Metzler , Y. Bernstein, W. B. Croft, A. Mof fat, and J. Zobel.
The RECAP system for identifying information o w. In Proc. 28th Ann. International ACM SIGIR Conf . on Resear ch and De-velopment in Information Retrie val , Aug. 2005. Demonstration abstract, to appear .
 D. Metzler , T. Strohman, H. Turtle, and W. B. Croft. Indri at ter -abyte track 2004. In Proc. 13th Text REtrie val Conf . (TREC 2004) . NIST , 2004.
 V. Murdock and W. B. Croft. Simple translation models for sen-tence retrie val in factoid question answering. In Proc. SIGIR
Workshop on Information Retrie val for Question Answering , pages 31 X 35, 2004.
 J. M. Ponte and W. B. Croft. A language modeling approach to in-formation retrie val. In Proc. 21st Ann. International ACM SIGIR
Conf . on Resear ch and De velopment in Information Retrie val , pages 275 X 281, 1998.
 S. E. Robertson, S. Walk er, M. Hancock-Beaulieu, A. Gull, and
M. Lau. Okapi at TREC. In Proc. 1st Text REtrie val Conf . (TREC 2001) , pages 21 X 30. NIST , 1992.
 M. Sanderson. Duplicate detection in the Reuters collection. Tech-nical Report TR-1997-5, Uni versity of Glasgo w, 1997.
 N. Shi vakumar and H. Garc  X  a-Molina. SCAM: A cop y detection mechanism for digital documents. In Proc. 2nd Conf . on the Theory and Practice of Digital Libr aries , 1995.
 I. Soborof f and D. Harman. Ov ervie w of the TREC 2003 novelty track. In Proc. 12th Text REtrie val Conf . (TREC 2003) , pages 38 X 53. NIST , 2003.
 C. Zhai and J. Laf ferty . A study of smoothing methods for lan-guage models applied to ad-hoc information retrie val. In Proc. 24th Ann. International ACM SIGIR Conf . on Resear ch and De-velopment in Information Retrie val , pages 334 X 342, 2001.
