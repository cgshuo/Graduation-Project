 Aggregated search is the task of integrating results from potentially multiple specialized search services, or verticals , into the Web search results. The task requires predicting not only which verticals to present (the focus of most prior research), but also predicting where in the Web results to present them (i.e., above or below the Web results, or some-where in between). Learning models to aggregate results from multiple verticals is associated with two major chal-lenges. First, because verticals retrieve different types of results and address different search tasks, results from dif-ferent verticals are associated with different types of predic-tive evidence (or features). Second, even when a feature is common across verticals, its predictiveness may be vertical-specific. Therefore, approaches to aggregating vertical re-sults require handling an inconsistent feature representation across verticals, and, potentially, a vertical-specific relation-ship between features and relevance. We present 3 general approaches that address these challenges in different ways and compare their results across a set of 13 verticals and 1070 queries. We show that the best approaches are those that allow the learning algorithm to learn a vertical-specific relationship between features and relevance.
 H.3 [ Information Storage and Retrieval ]: Information Storage and Retrieval Algorithms aggregated search, federated search, query intent, learning to rank  X  Work done at Carnegie Mellon University.

In addition to providing Web search, commercial search engines provide access to specialized search services (referred to as verticals ) that focus on a specific information-seeking task (e.g., search for news, images, video, local businesses, items for sale, weather forecasts, etc.). Aggregated search refers to the detection and integration of relevant vertical content into the Web search results page. The problem is typically decomposed into two sub-tasks. It is impractical, if not impossible, to issue the query to every vertical. Thus, the first sub-task ( vertical selection ) is to predict which ver-ticals, if any, are relevant [2, 3, 6]. Typically, this is done without issuing the query to the vertical [2, 3]. The second sub-task ( vertical results presentation ) is to predict where in the web results to display those verticals selected [16, 1]. This work focuses on the vertical results presentation task.
If we assume that vertical results must be presented in spe-cific positions relative to the Web search results (i.e., above, below, or in between certain Web results), then the verti-cal results presentation task can be cast as block-ranking . A block is defined as a short sequence of Web or same-vertical results which must be presented grouped together X  vertically (e.g., news ) or horizontally (e.g, images ) X  X n the final search results page. A Web block corresponds to a set of Web results which cannot be partitioned in the final presentation (e.g., Web results 1-3, 4-6, and 7-10). A ver-tical block corresponds to the top results from a particular vertical. The goal of block-ranking is to predict a ranking of blocks that approximates, based on some metric, a gold standard ranking. We propose and evaluate three supervised machine learning approaches to block-ranking.

Casting block-ranking as a supervised machine learning problem is associated with two main challenges. First, be-cause verticals retrieve different types of results (e.g., news articles, images, videos, local business information, weather forecasts), blocks from different verticals are associated with different types of predictive evidence or features. For exam-ple, news results are associated with a publication date, lo-cal results are associated with a geographical location, and community Q&amp;A results are associated with a number of suggested answers. Thus, block-ranking requires approaches that can handle an inconsistent feature representation across verticals. Secondly, even if a feature is common to multiple verticals, it may not be equally predictive. For example, the number of results retrieved by the vertical will likely be more predictive for news than weather , which retrieves at most a single result. Alternatively, a feature may be predic-tive for different verticals, but in the opposite direction. For example, the query-term  X  X ics X  may be positive evidence for images , but negative evidence for shopping . Thus, block-ranking may require approaches that can exploit a vertical -specific predictive relationship between features and rele-vance. We propose methods that address both challenges in different ways.

We evaluate approaches that rank blocks as a function of a set of features. We partition features into two general classes: pre-retrieval and post-retrieval features. During ver-tical results presentation, we assume that the system has al-ready issued the query to each vertical, or at least those predicted relevant during vertical selection. Thus, post-retrieval features can be derived directly from the vertical results. We investigate the cost-benefit of post-retrieval fea-tures. Are they useful for predicting where a vertical should be presented? Can they also be used to re-evaluate vertical selection decisions in light of the vertical results? Answering these questions affects, for example, whether the end-to-end system should attempt to issue the query to as many ver-ticals as possible (to derive these features), or whether it should cache post-retrieval features for future impressions of the query.
Research in aggregated search can be described along sev-eral dimensions: number of verticals considered, number of positions in which vertical content can be integrated, and sources of training data. The earliest work in aggregated search integrated the content from a single vertical above the first Web result [6, 10, 12]. Unfortunately, evaluating the performance of a single vertical in isolation ignores possible contention with other verticals. As a result, more realistic models were developed which considered several verticals simultaneously [2, 3, 7]. Limiting integration to the top po-sition is rather crude when considering the nuanced nature of user intent. For example, if a query X  X  dominant intent is navigational, there may be a secondary, less popular, news intent which should be surfaced lower in the ranked list. In order to address this, more recent work has focused on in-tegrating at arbitrary positions in the ranked list [1, 16]. Finally, all of the prior work in aggregated search uses ma-chine learning methods and, as a result, sources of training data become important. Training data has included edito-rial judgments [2, 1, 12], click information [6, 10, 16], and indirect labels from other verticals [3].

We focus our experiments on methods of learning order-ings of generic Web and vertical content using pairwise labels [14]. This is quite different from prior work which focuses on minimizing misclassification of vertical relevance [2, 3, 7, 12] or regressing against a click-based target [6, 10, 16]. In the language of  X  X earning to rank X , these previous approaches are based on pointwise sources of relevance while our approach focuses on pairwise sources.
At query time, the aggregated search system issues the query to the Web search engine and to those verticals se-lected, which we refer to as the candidate verticals. The aggregation task is subject to a set of layout constraints. We assume the following constraints. First, we assume that results from the same vertical must be presented together. Second, vertical results can only be embedded in specific po-sitions relative to the Web results. We assume four slotting positions: above the first Web result, between Web results 3 and 4, between results 6 and 7, and below the tenth Web result. Third, Web results are always presented and main-tain their original order. Fourth, each candidate vertical is associated with a set of top results, which are given. That is, the system does not predict which results from a particular vertical to present. Finally, we assume that users prefer to not see non-relevant vertical results even below the last Web result. The system is free to suppress a candidate vertical, for example, based on its results.
 Combined, these layout constraints form a set of blocks. Each candidate vertical forms its own vertical block and our four slotting positions divide the top 10 Web results into three Web blocks: Web results 1-3 ( w 1 ), 4-6 ( w 2 ), and 7-10 ( w 3 ). We define the block-ranking task as follows. Let B denote the set of Web and vertical blocks associated with query q and let  X   X  q denote the optimal ordering of B query q , referred to as the reference ranking. The objec-tive of block ranking is to predict a ranking of blocks  X  that approximates  X   X  q . The quality of the approximation can be measured using a rank-based distance metric, such as Kendall X  X   X  . We discuss this in Section 6.5.

In addition to deciding where to present vertical results, the task is also to filter non-relevant candidate verticals. Suppressed verticals are modeled using an imaginary  X  X nd of search results X  block, denoted as eos . The eos block is in-cluded in B q , and, therefore, appears in both  X  q and  X   X  ( i ) denote the rank of block i in  X  q . Blocks ranked above eos are considered to be displayed to the user and those ranked below it are suppressed. Because the sub-ranking of blocks below rank  X  q ( eos ) is not observed by the user, for the purpose of comparing  X  q with  X   X  q , all blocks ranked below eos in  X  q are considered tied at rank  X  q ( eos ) + 1.
We propose machine learning approaches to rank blocks as a function of a set of features. We use various types of features which we believe are predictive of a particular block X  X  relevance to a query. These can be divided into two general classes. Pre-retrieval features can be generated with-out issuing the query to the Web or vertical search engine. These include, for example, the topic of the query or whether the query contains a particular named-entity type (e.g., the name of a person, product, or location). Post-retrieval fea-tures must be generated after the query is issued to the Web or vertical search engine. These include, for example, the to-tal number of results retrieved by the block X  X  search engine or the average text-similarity between the query and the re-sults presented in the block. We investigate the contribution of post-retrieval features to performance.
Pre-retrieval features can be generated before the query is issued to the Web or vertical search engine.

These binary features correspond to named-entity types possibly appearing in the query. Queries were automati-cally annotated using the BBN IdentiFinder named-entity tagger [4]. Named-entity features include location (possibly predictive for local , maps , and weather ), product (possibly predictive for shopping ), person (possibly predictive news and images ), and organization (possibly predictive for fi-nance ). In total, we focused on 24 named-entity types. Each binary feature equals 1 if the named-entity type appears at least once in the query and 0 otherwise.

Some verticals may be topically focused. Thus, knowing the general topic of the query may help in block ranking. We focused on 30 topical categories, derived as follows. First, we selected 150 categories from the Open Directory Project (ODP) hierarchy and crawled Web documents associated with these ODP nodes. Then, in order to reduce the num-ber of category features, these 150 categories were clustered into 30 clusters. Clustering was done using complete-link ag-glomerative clustering [15]. The distance between ODP cat-egories was computed using the symmetric Kullback-Leibler divergence [8] between category language models. We used unigram language models with add-one smoothing to avoid zero probabilities. Let  X  i denote the language model associ-ated with cluster/category i . We set the i th category feature for query q according to, 1 Z Q w  X  q P ( w |  X  i ) where Z normal-izes across clusters/categories.
 User clicks are often viewed as surrogates for relevance. The queries associated with clicks on a particular document convey the types of information needs the document satis-fies. Likewise, the queries associated with clicks on vertical results convey the types of information needs the vertical satisfies. Our click-through features harness this type of ev-idence by considering the similarity between the query and those associated with clicks on vertical content (or content very similar to the vertical X  X ).
 Click-through data was derived from the AOL query-log. We derived one click-through feature per vertical as follows. First, for each vertical, we manually selected a set of Web domains which we believe have content closely related to the vertical. For local , we selected www.local.yahoo.com, www.citysearch.com, and www.yellowpages.com. Then, we constructed vertical-specific (unigram) language models us-ing all queries (allowing duplicates) associated with click-events on the vertical X  X  corresponding domains. Finally, given a query, we generate one feature per vertical based on the query generation probability given the vertical X  X  query-based language model. Given query q , we set the click-through feature for vertical i according to, 1 Z Q w  X  q P ( w |  X  where Z normalizes across verticals.

Users often express vertical-intent explicitly using key-words such as  X  X ews X  (for the news vertical),  X  X ics X  (for the images vertical) or  X  X uy X  (for the shopping vertical). The goal of our explicit vertical-intent features is to determine vertical relevance based on how often the query co-occurs with keywords used in explicit requests for the vertical. For example, given the query  X  X ritney spears X , we may predict that images is relevant because users often issue the query  X  X ritney spears pics X . Co-occurrence statistics were derived from the AOL query-log.

Vertical-intent features were generated as follows. First, we manually associated each vertical with a small set of key-words which we believe are often used in explicit requests for the vertical. For example, the images vertical was associ-ated with  X  X icture(s) X ,  X  X hoto(s) X ,  X  X ic(s) X , and  X  X mage(s) X . Then, to measure the affinity between the query and a par-ticular vertical, we use the chi-squared statistic to measure the lack of independence between two events: the occur-rence of the query in the AOL query-log and the occurrence of any of the vertical X  X  vertical-intent keywords. To reduce sparsity (particularly for long queries), we compute the chi-squared statistic for each query-term individually and use the geometric average. The geometric average (rather than the arithmetic average) was used to favor queries with terms that consistently co-occur with keywords used in explicit re-quests for the vertical.
Post-retrieval features must be generated after the query is issued to the Web or vertical search engine.

This feature considers the number of results retrieved from the Web or vertical search engine. For some verticals, an abundance of query-related content in the index may be predictive of its relevance. This may be true, for example, for news , where the rate of content production may corre-late with the rate of content demand. However, we do not expect this feature to be useful for every vertical. For ex-ample, the number of retrieved results contributes no useful information for verticals that retrieve at most a single result ( finance , maps , and weather ).

Some verticals may be time sensitive. Prior work shows that recency is important in news search [6]. Temporal in-formation was available for four verticals: news , blogs , com-munity Q&amp;A , and twitter . Our assumption is that, for these verticals, users care primarily about recent results. If this is true, then the average age of results presented in the block should affect its relevance. Temporal features are generated as follows. For each individual vertical result, we measure the elapsed time (in hours) between the current and created date/time. We included four features for each time-sensitive vertical: the minimum, maximum, mean, and standard de-viation of the elapsed time across results within the block.
The goal of these features is to characterize the text-similarity between the query and the results presented in the block. The challenge in deriving text-similarity features is that results from different sources (i.e., results from the Web search engine and from different verticals) are associ-ated with different sets of textual representations. For ex-ample, each Web result is associated with three representa-tions: a title, URL, and summary snippet. Each community Q&amp;A result is associated with two representations: a ques-tion and an optional  X  X est answer X . Each weather result is associated with a single representation: the location, which we define as the concatenation of the city, state, and country of the weather forecast.
The full vertical-to-keyword and vertical-to-domain mappings are available at http://www.ils.unc.edu/ ~jarguell/cikm11/ .
Text-similarity features are generated for a query-block pair in two steps. In the first step, for each result within the block, we measure the text similarity between the query and each representation associated with the result. We use four different text-similarity measures: (1) the cosine similarity between the query and the representation, (2) the maximum number of query-terms appearing consecutively in the repre-sentation, (3) the percentage of query-terms appearing in the representation, and (4) the percentage of the representation corresponding to a query-term. Similar text-similarity fea-tures were used in prior learning-to-rank research (for doc-ument ranking) [14, 9].

The second step depends on whether the block-type is associated with a single result per block (e.g., weather , fi-nance , and maps ) or multiple results per block (e.g., news , local , and shopping ). For block-types with a single result, we simply include our four similarity measures for each of its text representations. For block-types with multiple results, for each query-representation similarity measure, we use the minimum, maximum, mean, and standard deviation across results within the block.
Pre-retrieval features are independent of the block. Thus, every block-type is associated with the same set of 80 pre-retrieval features: 24 named-entity type features, 30 cate-gory features, 13 click-through features, and 13 vertical in-tent features. Notice that we included all 13 click-through features (one per vertical) and all 13 vertical intent features (one per vertical) in every block X  X  feature representation, ir-respective of its type. Post-retrieval features, as opposed to pre-retrieval features, are derived directly the block (e.g., the average text-similarity between the query and the title of re-sults within the block) or from the search engine X  X  response to the query (e.g., its hit count). Different block-types were associated with a different set of post-retrieval features. Hit count features were omitted for verticals that retrieve at most a single result (i.e., finance , maps , and weather ). Tem-poral features were only available for news , blogs , commu-nity Q&amp;A , and twitter . Text-similarity features are incon-sistent because different block-types are associated with dif-ferent representations and a different number of results X  block-types with multiple results per block require aggre-gating evidence by taking the minimum, maximum, mean, and standard deviation of query-representation similarities across results.
As previously mentioned, casting block-ranking as a su-pervised machine learning problem is associated with two main challenges. First, different types of blocks are associ-ated with different features. Second, even when a feature is common to multiple block-types, it may have a type-specific relationship with relevance. We propose three general ap-proaches which address both challenges in different ways.
Our classification framework takes the form of n indepen-dent binary classifiers (one per vertical). We choose to use logistic regression due to its prediction accuracy and training speed on large-scale classification tasks [13].

Each binary classifier is trained to predict whether a par-ticular vertical should be presented (ranked above eos ) or suppressed (ranked below eos ). While training the classifier for vertical v , a query is considered a positive instance if v is ranked above eos in the reference ranking  X   X  q and a nega-tive instance otherwise. To compensate for class imbalance (verticals are more often suppressed), each positive training instance is weighted according to the number of negative instances in the training set and vice-versa [5].

The classification approach produces a block-ranking by assigning vertical blocks to slots. Consistent with our lay-out constraints, we assume four vertical slotting positions relative to the Web results: slot s 1 (above w 1 ), slot s tween w 1 and w 2 ), slot s 3 (between w 2 and w 3 ), and slot s (between w 3 and eos ). In the output ranking, a slot may contain zero or more vertical blocks.

The classification approach combines all n vertical-specific binary classifiers as follows. First, the query, represented as a vector of features, is submitted to each candidate vertical X  X  classifier. Each classifier outputs a prediction probability that its vertical should be presented (i.e., ranked above eos in the predicted ranking  X  q ). Let P (  X  q ( v ) &lt;  X  note the prediction probability that v should be presented. Then, each candidate vertical is assigned to a slot (or is sup-pressed) using four threshold parameters  X  1  X  4 . Vertical v is assigned to slot x if P (  X  q ( v ) &lt;  X  q ( eos ))  X   X  other words, vertical v is assigned to the highest slot for which v  X  X  prediction probability is greater than or equal to all thresholds below it. At this point, vertical blocks as-signed to the same slot are tied. Finally, ties are broken by ordering vertical blocks within the same slot by descending order of prediction probability.

The classification approach addresses the two challenges mentioned above by training a different binary classifier per vertical. Each classifier adopts its own feature representa-tion, which is unique to the vertical, and learns a vertical-specific relationship between features and block relevance.
In the classification approach, each independent binary classifier is trained to predict whether a particular vertical should be presented or suppressed. Our voting approach also combines independent binary classifiers. However, these are trained to make more fine-grained predictions. Independent binary classifiers are trained to predict the relative ordering between pairs of blocks of a particular type. Each classifier is trained to predict whether block i (of a particular type) should be ranked above or below block j (of another partic-ular type) for a given query. The voting approach uses one binary classifier per block-type pair.

The training phase proceeds as follows. Recall that B denotes the set of block associated with query q and in-cludes one block per candidate vertical, all three Web blocks ( w 1  X  3 ), and the eos block. While training a classifier spe-cific to a block-type pair, the query is considered a positive or negative instance depending on the pair X  X  relative rank. Certain block-type pairs occur more frequently in one order versus the other. To correct for class imbalance, each pos-itive training instance is weighted according to the number of negative instances in the training set and vice-versa [5].
To predict a block-ranking for query q , first, every block-pair i,j  X  B q is submitted to the appropriate classifier, de-pending on the type of i and the type of j . Let P (  X  q  X  ( j )) denote the prediction probability that i should be ranked above j . This probability can be treated as a prefer-ence score between i and j . The voting approach produces the output block-ranking  X  q by combining these preference scores as input to the Schulze voting method [17].

As in the classification approach, each binary classifier is associated with a unique feature representation. More specifically, each classifier is associated with three sets of features: one set of pre-retrieval features, which are inde-pendent of the block-types under consideration, and two separate sets of post-retrieval features (one set specific to each type in the block-type-pair).

The voting approach addresses both challenges mentioned above (block-type-specific features and a potentially differ-ent predictive relationship across types) by training a dif-ferent binary classifier per block-type pair. Each classifier adopts its own feature representation and learns a predictive relationship that is specific to the block-type-pair.
Given n verticals and m non-vertical block-types, the total number of binary classifiers used by the voting approach is given by ` n + m 2  X   X  ` m 2  X  . The second term accounts for the fact that Web results are always presented and always ranked in their original order (i.e.,  X  q ( w 1 ) &lt;  X  q ( w  X  ( eos )). Thus, it is not required to learn a classifier to de-termine the relative ordering between pairs of non-vertical blocks. In our case, n = 13 and m = 4, which results in 130 independent binary classifiers. The large number of binary classifiers used by this method may be viewed as a disadvan-tage. An alternative to learning one model per block-type pair is to learn one model per vertical/non-vertical block-type pair. This results in four models per vertical: three which predict the vertical X  X  relevance compared to each Web block ( w 1  X  3 ) and one which predicts its relevance compared to eos (i.e., it predicts whether to display/suppress the ver-tical). As before, given a query, every block-pair i,j  X  B where one is a vertical-and the other a non-vertical block, is submitted to the appropriate classifier. Then, the output prediction probabilities are combined as the input to the Schulze voting method in order to derive  X  q .
The block-ranking task can also be cast as a learning to rank (LTR) problem. Many different learning to rank meth-ods have been proposed. In this work, we adopt a pairwise learning to rank approach. Pairwise approaches optimize over the set of pairwise preferences implicit in the training data. More specifically, we adopt a method that solves the classic RankSVM optimization problem, first proposed by Joachims [9]. RankSVM learns a linear model f w parame-terized by feature weight vector w .

Casting block-ranking as an LTR problem requires train-ing a single model f w to predict a block X  X  rank irrespective of its type. In our situation, this is problematic because dif-ferent block-types are associated with different features (i.e., some features may be specific to a handful of types and some may be unique to a particular one). In addition, it is prob-lematic because those features that are common to multiple types (e.g., whether the query contains a city name) may be predictive for some types more than others, or even predic-tive for different types in the opposite direction. Next, we propose three LTR variants which address these challenges in different ways. Each variant makes a different assumption about how features may be correlated with block relevance across block-types.

One alternative is to assume that each feature is equally predictive of block relevance (in the same direction) inde-pendent of the block-type. The feature representation is as follows. Pre-retrieval features are independent of the block. This model uses a single copy of each pre-retrieval feature. Post-retrieval features are block-specific (i.e., they are gen-erated directly from the block or the block X  X  search engine results). Similar to pre-retrieval features, this approach also uses a single copy of each post-retrieval feature. If a block is not associated with a particular post-retrieval feature, then the feature is zeroed-out in that instance. Consider, for ex-ample, our post-retrieval features which determine the text-similarity between the query and the summary snippets pre-sented in the block. These features are only associated with news and Web blocks w 1  X  3 . Therefore, if the block is not one of these types, all these features are zeroed-out.
This approach assumes that features are equally corre-lated with relevance irrespective of the block-type. Once trained, model f w will apply the same weight to a feature independent of the instance X  X  block-type. We denote this LTR variant as LTR-G because it assumes a vertical-general relationship between features and relevance.

This approach makes the opposite assumption as the pre-vious one. It assumes that every feature X  X hether it is a pre-or post-retrieval feature X  X s uniquely correlated with relevance across different block-types. The feature repre-sentation is as follows. We make a separate, block-type-specific copy of each feature. So, for example, given 17 block-types (13 verticals + 3 Web blocks and the eos block), we make 17 copies of each pre-retrieval feature (one per block-type). Given an instance, all copies are zeroed-out except for those corresponding to the instance X  X  block-type. For post-retrieval features, we make one copy per block-type for which the feature is available. Consider, for example, our temporal features, which are available for blocks from blogs , community Q&amp;A , news , and twitter . We make 4 copies of each temporal feature.

This approach assumes that features are correlated dif-ferently with relevance depending on the block-type. Once trained, model f w can apply a different weight to a fea-ture, depending on the instance X  X  block-type. While this added flexibility may be advantageous, the increased num-ber of features may introduce predictive noise and result in overfitting. Thus, this LTR variant may require more train-ing data than LTR-G . We denote this LTR variant as LTR-S because it assumes a vertical-specific relationship between features and relevance.

The previous two approaches make opposite assumptions: features are either equally correlated or uniquely correlated with relevance for different block-types. A third alternative is to make neither assumption a priori , but to give the al-gorithm the freedom to exploit both types of relationships using training data.

For this approach, we maintain a single copy of each pre-and post-retrieval feature which is shared across all block-types. As before, if an instance X  X  block-type is not associated with a shared feature, the feature is zeroed-out for that in-stance. In addition to these shared features, we make one block-type-specific copy of each pre-and post-retrieval fea-ture. Given an instance, all copies corresponding to types other than the instance X  X  block-type are zeroed-out. The canonical feature representation for this approach is the union of features used by the previous two approaches.

This approach makes no assumption about how a feature is correlated with relevance across block-types. If a feature is equally correlated across block-types, then the algorithm can assign a large (positive or negative) weight to the copy of the feature which is shared across types. Alternatively, if a feature is correlated differently for different block-types, then the algorithm can assign a large positive weight to some copies of the feature and a large negative weight with to oth-ers. We denote this LTR variant as LTR-GS because it has the flexibility of learning a vertical-specific and vertical-general relationship for each feature. Of all three LTR variants, LTR-GS has the largest number of features and may therefore need the most training data to avoid overfitting.
We evaluate performance using a set of 13 verticals and 1070 queries. Our evaluation methodology is based on that proposed in Arguello et al. [1]. For each query, a reference block-ranking  X   X  q is derived from human preference judge-ments on block-pairs i,j  X  X  q . These 1070 reference presen-tations are used for training and evaluation. That is, during training, algorithms are tuned to predict a block-ranking  X  q that approximates the reference block-ranking  X   X  q . Dur-ing testing, we evaluate algorithms based on the quality of their approximation on unseen queries. Following Arguello et al. [1], we evaluate the predicted block-ranking  X  q based on its generalized Kendall X  X   X  distance to  X   X  q , denoted by K  X  (  X   X  q , X  q ) [11]. Arguello et al. [1] presented a user study which shows that when assessors strongly prefer one block-ranking over another, the preferred block-ranking is scored as being superior by this metric. That is, on average, the preferred presentation is the one closest to  X   X  q based on K
Approaches are compared based on average performance across queries using 10-fold cross-validation. Unless oth-erwise stated, statistical significance is tested using a one-tailed paired t-test (at the p &lt; 0 . 05 level) by comparing per-formance across test queries (i.e., the concatenation of all 10 test-folds). Next, we describe the methodology for deriving  X  , the human relevance assessment process, the evaluation metric K  X  (  X   X  q , X  q ), our set of verticals and queries, and some algorithm implementation details.
We provide a general overview of how  X   X  q is derived from human relevance assessments and refer the reader to Ar-guello et al. [1] for more details.

Given query q , the reference presentation  X   X  q is derived from redundant pairwise preference judgements collected for all block-pairs i,j  X  X  q . That is, every candidate block-pair for the query is shown to multiple assessors, who are asked to state a preference or to state that both blocks i and j should be suppressed. Let  X  q denote the full set of block-pair judgements collected for query q and let  X  q ( i,j ) denote the strength with which block i is preferred over j given q . We collected 3 redundant judgements per triplet ( q,i,j ) and set  X  q ( i,j ) equal to the number of assessors who prefer i over j given q .  X  q ( eos,i ) was set equal to the number of assessors who stated that i should be suppressed in conjunc-tion with another block j . Finally, to derive  X   X  q , block-pair judgements  X  q are input to the Schulze voting method [17], which converts these block-pair preferences into a ranking of blocks. A detailed explanation of the Schulze voting method is given in Schulze [17] and a description of how it is used to infer  X   X  q from  X  q is given in Arguello et al. [1].
Block-pair assessments for 1070 queries were collected us-ing Amazon X  X  Mechanical Turk (AMT). Assessors were com-pensated US$ 0.01 per block-pair assessment. For a given triplet ( i,j,q ), assessors were presented with the query q and blocks i and j presented side-by-side in random order. The only difference between our assessments and those collected in Arguello et al. [1] is that assessors were not given a topic description for the query. Instead, they were instructed to judge blocks based on their best guess of the hypothetical user X  X  intent. To do quality control, 10% of all block-pair assessments were traps. In a trap assessment, the assessor is given a triplet ( q,i,j ), but either block i or j is taken from a query other than q . If the assessor selects the extra-neous block as the preferred block, we consider it a failed trap. Assessors with more than 2 failed traps had all their judgements removed and re-submited to AMT.
We focused on 13 verticals constructed using free APIs from eBay ( shopping ), Google ( blogs , books , weather ), Recipe Puppy ( recipes ), Yahoo! ( community Q&amp;A , finance , im-ages , local , maps , news ), Twitter ( twitter ), and YouTube ( video ). For the local , maps , and weather verticals, queries were mapped to a geographical location using Yahoo! X  X  Geo-Planet API. For queries with no target location explicitly mentioned (e.g.,  X  X ome depot X ), we assumed a particular lo-cation and instructed assessors to interpret such queries as a resident of that location. Blocks were constructed using the vertical X  X  top results, assuming a maximum number of results (e.g., 5) per vertical block.

A set of 1,070 queries for model learning and evalua-tion was collected using sampling. We are interested not only in performance across queries, but also across verticals. Therefore, our sampling approach is biased towards cover-age across our set of 13 verticals. For space reasons, we omit the details of our sampling approach. 2
In some cases, one may prefer a system that is biased to-wards vertical results. The aggregated web search provider may want to tune the system so that it more frequently presents vertical results and/or presents them higher in the output presentation. This may occur, for example, in or-der to gather user-interaction data or to satisfy contractual obligations with vertical providers or advertisers.

Vertical bias can be modeled in our evaluation frame-work using vertical pseudo-votes , a parameter p in the range [0 ,  X  ). As previously noted,  X  q ( i,j ) corresponds to the num-ber of assessors who prefer i over j given q . A vertical bias can be introduced by incrementing this value by some num-ber of pseudo-votes p , but only if i corresponds to a vertical block. If i and j correspond to blocks from different ver-ticals, this method increments both  X  q ( i,j ) and  X  q ( j,i ) by
A description of our sampling approach is available at http://www.ils.unc.edu/~jarguell/cikm11 . an equal number of pseudo-votes. This has the effect that, after producing  X   X  q using the Schulze voting method, vertical results are ranked higher in  X   X  q . However, the ranking of ver-ticals relative to each other is unchanged. Modeling vertical bias using vertical pseudo-votes changes  X   X  q and, therefore, changes model learning and evaluation. Rather than se-lect a single pseudo-vote parameter p , we learn and evaluate models across several values of p (i.e., p = { 0 , 1 , 2 , 3 , 4 , 5 } ). Across values of p , the verticals ranked higher in  X  video , local , news , blogs , and community Q&amp;A .
The primary evaluation metric is the generalized Kendall X  X   X  distance between the predicted block-ranking  X  q and the reference block-ranking  X   X  q , denoted as K  X  (  X   X  q , X  eralized Kendall X  X   X  is related to Kendall X  X   X  , which measures the number of discordant pairs between two rankings of the same set of elements. For our purpose, however, Kendall X  X   X  has a major limitation: it considers all discordant pairs equally important. In aggregated search, users typically scan results from top to bottom. Thus, discordant pairs at the top of the ranking should be more influential. As suggested in Kumar and Vassilvitskii [11], discordances at the top of the ranking can be made more influential by using a DCG-like cost function. See Arguello et al. [1] for details.
The classification framework described in Section 5.1 re-quires tuning threshold parameters  X  1  X  4 , which are used to slot verticals based on each classifier X  X  prediction confidence value. In addition to these parameters, logistic regression requires tuning cost factor C , which determines the cost of misclassifying a training set instance. These 5 parameters were tuned using 10-fold cross-validation. More specifically, they were tuned for each train/test pair individually by do-ing a second level of 10-fold cross-validation on each primary fold X  X  training set. An exhaustive search was used to find the parameter values with the best average performance across secondary train/test pairs. The voting approach described in Section 5.2 also uses logistic regression models: one per block-type-pair. The C parameter was tuned as described above. In both approaches, models were trained using the LibLinear toolkit. 3
The prior probability that a particular vertical is ranked high is fairly low. This is a problem if during training the LTR model learns that the best alternative is to present w 1  X  3 and suppress all verticals. In the classification and vot-ing approach, we balanced the training data using instance weighting (i.e., assigning more weight to the minority class). Casting block-ranking as a learning-to-rank approach may also require some form of instance weighting.

Our approach to instance weighting is as follows. Let  X  q denote a block-ranking which presents Web blocks w 1  X  3 in their original order and suppresses all verticals. Given a measures the distance between  X   X  q and  X  w q . If  X  K  X  is close to +1, it means that  X   X  q has verticals presented in the top ranks. If  X  K  X  (  X   X  q , X  w q ) is close to  X  1, it means that  X  has Web blocks w 1  X  3 presented in the top ranks and verticals presented in the low ranks or suppressed. Our approach to instance weighting is to replicate queries in the training http://www.csie.ntu.edu.tw/cjlin/liblinear/ set proportional to this distance. This has the effect that queries which deviate from  X  w q are oversampled. The amount of replication is controlled using parameter  X  . First, we scale  X  K  X  (  X   X  q , X  w q ) to zero min and unit max (using queries from the training set). Then, we multiply this value by  X  . Given training query q , the number of additional copies of q in the training set is given by  X   X K  X  min-max (  X   X  q , X  that when  X  = 0, no additional copies of q are added to the training set. We evaluate the effect of parameter  X  in Section 8.1.

In addition to parameter  X  , RankSVM has a regulariza-tion parameter  X  . Both parameters were tuned using two levels of 10-fold cross-validation. An exhaustive search was used to find the parameter values with the best average per-formance across secondary train/test pairs. We trained LTR models using the sofia-ml toolkit. 4
We evaluate three general approaches to block-ranking: the classification approach, the voting approach, and the LTR approach. One limitation of the voting approach is that it requires a large number of independent binary clas-sifiers. Thus, we evaluate a second version of the voting approach that trains a binary classifier only for block-type-pairs where one type corresponds to a vertical block-type and the other corresponds to a non-vertical block-type. Both are included in this evaluation. We also include all three variants of the LTR approach, with parameter  X  tuned us-ing cross-validation.
 Results in terms of average K  X  (  X   X  q , X  q ) are presented in Table 1. As previously mentioned, pseudo-vote parame-ter p controls for vertical bias. The higher its value, the greater the bias towards verticals ranked high. Rather than choose a single parameter p , results are presented for p = { 0 , 1 , 2 , 3 , 4 , 5 } . When p = 0, the  X   X  q  X  X  used for training and evaluation have no vertical bias. The second row in Table 1 ( WEB ) corresponds to a degenerate baseline that suppresses all vertical results and simply presents w 1  X  3 above eos in their original order.

Table 1 shows several meaningful trends. Performance for all methods decreases with greater values of p (this was expected for WEB ). One possible reason is the following. As p increases, the number of queries with top-ranked verticals increases. This means that the room for error also increases. When p is low, ranking w 1  X  3 above all verticals is a reliable and effective strategy.

When p = 0, WEB performs well. In fact, the only two ap-proaches that significantly outperform WEB when p = 0 are LTR-S and LTR-GS . Significance with respect to WEB is not shown in Table 1. For values of p  X  3, all block-ranking approaches significantly outperform WEB . Thus, given a ver-tical bias, any of the three general block-ranking approaches is better than presenting only Web results.
 Both voting approaches perform similar to each other. This is interesting because the second version (which learns one binary classifier per vertical/non-vertical block-type) uses many fewer binary classifiers than the first (which learns one classifier per block-type). The performance for both voting approaches, however, deteriorates for p  X  2.

The classification approach does surprisingly well consid-ering that it uses a just one binary classifier per vertical. Its http://code.google.com/p/sofia-ml/ Table 1: Block-ranking results in terms of aver-age K  X  (  X   X  q , X  q ) . A M ( O ) , N ( H ) , and  X  (  X  ) denotes sig-nificantly better(worse) performance compared to the classification, voting (all pairs), and LTR-G ap-proaches, respectively. performance is competitive across all values of p , showing that it can be effectively trained to fit different degrees of vertical bias.

Comparing among the three LTR variants, performance is significantly worse for LTR-G . It never performs better and often performs significantly worse than LTR-S and LTR-GS , particularly for values of p  X  3. For values of p  X  4, LTR-G performs significantly worse than the classification approach. The improvement of LTR-S and LTR-GS over LTR-G reveals the importance of exploiting vertical-specific evi-dence. Imposing the constraint that features must be simi-larly correlated with relevance across different block-types degrades performance. LTR-G and LTR-S perform better by allowing the learning algorithm to exploit a block-type-specific relationship between features and block relevance. Compared to the classification approach, LTR-S and LTR-GS both perform better. The improvement is significant for all values of p , except p = 3, where all three perform at the same level. We view this as a positive result in favor of cast-ing block-ranking as a learning-to-rank task. In contrast with the classification approach, both of these LTR vari-ants perform better and require training only a single model (rather than one per vertical). Relative to each other, LTR-S and LTR-GS are statistically indistinguishable across all val-ues of p (statistical significance not shown in Table 1). We provide an explanation for this in Section 8.3.
The goal of parameter  X  is to focus LTR training on those queries that have verticals ranked high. Given enough evi-dence in favor of a particular vertical, we want to the LTR model to overcome the prior probability that the vertical is ranked low. Our method for instance weighting is to repli-cate queries in the training set proportional to the K  X  dis-tance between the reference block-ranking  X   X  q and a one that presents w 1  X  3 and suppresses all verticals. Parameter  X  con-trols the amount of replication.

We were interested in the effect of parameter  X  on per-formance. Each LTR variant is evaluated under two con-Table 2: Block-ranking results in terms of K (  X   X  q , X  q A M ( O ) denotes significantly better(worse) perfor-mance compared to the classification approach. A N ( H ) denotes a significant improvement(drop) in performance for an LTR variant (with  X  tuned) com-pared to its counterpart for which  X  = 0 . ditions. In the first condition,  X  = 0, which corresponds to no replication  X  X ach training query q appears just once in the training set. In the second condition,  X  is tuned by sweeping across  X  = { 0 , 10 , 25 , 50 } . Notice that  X  = 0 (no replication) is a parameter choice. These results are identical to those in Table 1. Table 2 presents results (in terms of av-erage K (  X   X  q , X  q )) for all three LTR variants under these two conditions. As a second point of reference, we also present results for the classification approach. To conserve space, we limit results to p = { 0 , 2 , 4 , 5 } .

The relative performance between LTR variants when  X  = 0 (rows 3-5) is consistent with their relative performance when  X  is tuned (rows 6-8). That is, LTR-S and LTR-GS outperform LTR-G . Thus, we focus the discussion on LTR-S and LTR-GS . For both approaches, tuning  X  (vs. setting it to zero) has either no effect or significantly improves per-formance. For p  X  4, setting  X  = 0 degrades performance even compared to the classification approach. Thus, casting block-ranking as a learning-to-rank task benefits not only from allowing the algorithm to learn a block-type-specific relationship between features and relevance, it also benefits re-weighting training-phase instances in order to overcome the prior probability that verticals are ranked low.
A feature ablation study was conducted to test each fea-ture group X  X  contribution to overall performance, measured in terms of average K (  X   X  q , X  q ). The analysis is conducted for both the classification approach and the LTR-S approach because they performed the best in Section 7. Each fea-ture group was individually omitted and this model was compared to a model with access to all features. Because features may be correlated, a non-significant drop in per-formance does not necessarily mean that the feature group contributes no useful information.

Results are presented in Table 3 for the classification ap-proach and the LTR-S approach. The top four feature groups are pre-retrieval features. The next four features are post-retrieval features. The last row corresponds to a model that ignores all post-retrieval features.

Table 3 shows several interesting results. The LTR-S ap-proach appears to be slightly more robust to missing fea-tures. One possible reason is the following. The classifica-tion approach trains one independent binary classifier per vertical. The LTR-S approach, on the other hand, trains a single model. It may be that training a single model allows the LTR-S approach to better shift its focus towards block-types for which it is more confident. Table 3: Feature ablation results for the classifica-tion and LTR-S approaches. A N ( H ) denotes a signif-icant improvement(drop) in performance compared to the model with access to all features.
The features with the greatest drop in performance (at least for p  X  4) are text-similarity features, which are a type of post-retrieval feature. This shows the importance of de-riving evidence directly from those results presented in the block. Also, it suggests the importance of issuing the query to as many vertical search engines as possible (in order to derive this type of evidence) or caching these post-retrieval features for future impressions of the query. Text-similarity features may have contributed the most to performance be-cause many of the block-types often ranked high in  X   X  q were associated with text-rich information. These included Web blocks w 1  X  3 , news , blogs , and community Q&amp;A .
Finally, omitting most feature groups did not result in a significant drop in performance. There are two reasons for this. First, features may be correlated. Second, most verticals are rarely ranked high in  X   X  q . Some of our features may be essential to minority verticals, but this may not have a noticeable effect on the average K  X  (  X   X  q , X  q ). We explore this further in the next section.
We also investigated each feature group X  X  contribution to per-vertical ranking performance. Evaluating a feature X  X  contribution to a particular vertical is not trivial. Suppose, for example, that we omit temporal features and want to evaluate the effect on the news vertical. One possibility might be to compare the news vertical X  X  predicted rank and its ideal rank across a set of queries. However, omitting temporal features may affect other verticals as well. And, if so, then ranking mistakes for those verticals would dis-place news from its ideal rank. For this reason, we focus our analysis on the classification approach, which trains one bi-nary classifier per vertical. Each vertical-specific classifier is Table 4: Feature contribution to per-vertical perfor-mance, based on AP. Statistical significance is tested using a one-tailed paired t-test, comparing across cross-validation test-folds. A N ( H ) denotes a signif-icant improvement(drop) in performance compared to the model with access to all features. trained to predict whether the vertical should be displayed (ranked above eos ) or suppressed (ranked below eos ) in  X 
Performance for a particular vertical is evaluated by con-sidering the quality of confidence values produced by the vertical X  X  corresponding classifier. Let Q v denote the queries for which v is a candidate vertical. We evaluate the qual-ity of confidence values output from v  X  X  classifier by com-puting the average precision (AP) metric on a ranking of Q v (ranked in descending order of confidence value that v should be presented). In computing AP, a ranked query q is considered  X  X elevant X  if  X   X  q ( v ) &lt;  X   X  q ( eos ) and  X  X on-relevant X  otherwise. Results in terms of AP are presented in Table 4. We limit our discussion to the case where p = 5 because it is associated with verticals ranked higher in  X   X  q .
Table 4 shows several noteworthy trends. First, perfor-mance across verticals varied widely (see row  X  X ll features X ). The best-performing verticals were weather (AP=0.938), fi-nance (AP=0.638), and news (AP=0.594). Interestingly, both weather and finance were minority verticals. Both were presented (i.e., ranked above eos in  X   X  q ) for only 14 and 21 queries, respectively. In spite of having few positive in-stances for training, performance for both these verticals was good. As it turns out, weather was easy because every query for which it was presented had the query term  X  X eather X  (e.g.,  X  X eather louisville ky X ). This explains why the most predictive feature for weather was the click-through feature (i.e., the query X  X  similarity to queries with clicks on weather-related content, many of which contain the term  X  X eather X ). Similarly, finance was easy because 10/21 queries for which it was presented had the query term  X  X tock(s) X  (e.g.,  X  X oeing stock X ). In other words, performance was high for weather and finance because their queries often had explicit vertical intent, which is easy to detect.
It is interesting that news was among the best performing verticals. This is inconsistent with previous results on ver-tical selection [2]. The most useful features for news were text-similarity features, which are a type of post-retrieval feature. Thus, while it is difficult to detect that a query is newsworthy using only pre-retrieval evidence (as in Ar-guello et al. [2]), useful evidence can be harnessed by issuing the query to the news vertical.

The worst performing verticals were twitter (AP=0.053), books (AP=0.088), and community Q&amp;A (AP=0.284). Both twitter and books were minority verticals, while community Q&amp;A was fairly common. Predicting twitter may require predicting that the query is about a trending topic, which we did not explicitly address. Predicting books and community Q&amp;A seems difficult. Queries for which these verticals were relevant had no clear pattern. For books , for example, some queries had the keyword  X  X ook X  (e.g.,  X  X ooks on giraffes X ), some corresponded to a book title (e.g.,  X  X olita X ), some cor-responded to an author name (e.g.,  X  X r. phil X ), and, finally, others corresponded to encyclopedic information needs (e.g.,  X  X edding cake ideas X ,  X  X hy don X  X  babies sleep at night X ,  X  X erennials X ). Community Q&amp;A queries showed a similar pattern. This may explain why text-similarity features were the only ones to significantly improve performance for both. In the absence of a clearly predictive query-pattern, it seems useful to derive evidence directly from the block.

Features contributed to performance differently for differ-ent verticals. Vertical intent features, which exploit vertical-related keywords, were predictive for maps and images . Many queries for which maps was relevant had the keyword  X  X ap(s) X . Similarly, many queries for which images was relevant had the keywords  X  X hoto(s) X ,  X  X ic(s) X , and  X  X icture(s) X . Cate-gory features were predictive for shopping because one of our category clusters was related to the shopping ODP node. Different features also hurt performance for different verti-cals. Named-entity type features hurt performance for books and news . Category features hurt performance for commu-nity Q&amp;A , finance , news , and recipe . Click-through features hurt performance for communtiy Q&amp;A and news .

The only features that did not significantly hurt perfor-mance for any vertical were text-similarity features. In the previous section, text-similarity features had the greatest contribution to overall performance. In this analysis, text-similarity features were the most consistently predictive for different verticals.
We proposed and evaluated three general machine learn-ing approaches to block-ranking X  X rdering blocks of Web and vertical results in response to a query. The best overall performance was obtained by casting this as a learning-to-rank problem. We showed, however, that to be successful, the LTR model must be able to learn a vertical-specific rela-tionship between features and block relevance. Our solution to this problem is through feature replication (i.e. by mak-ing a block-type-specific copy of each feature). This allows the model to weight a feature as positive evidence for some verticals and negative evidence for others.

Consistent with most prior work in aggregated search, ev-idence integration is key to block-ranking. Different fea-tures were predictive for different verticals. The features that contributed the most to overall performance, and those that were consistently predictive for different verticals, were text-similarity features, which are a type of post-retrieval feature. Their contribution to performance suggests the im-portance of issuing the query to a vertical when possible or of caching this type of evidence for future impressions of the query (or queries similar to it).
This work was supported in part by the NSF grant IIS-0916553 and a gift from Yahoo! through its Key Scientific Challenges program. Any opinions, findings, conclusions, and recommendations expressed in this paper are the au-thors X  and do not necessarily reflect those of the sponsors.
