 We present a direct multi-class boosting (DMCBoost) method for classification with the following properties: (i) instead of reducing the multi-class classification task to a set of binary classification tasks, DMCBoost directly solves the multi-class classification problem, and only requires very weak base classifiers; (ii) DMCBoost builds an ensemble classifier by directly optimizing the non-convex performance measures, including the empirical classification error and margin functions, without resorting to any upper bounds or approximations. As a non-convex optimization method, DMCBoost shows competitive or better results than state-of-the-art convex relaxation boosting methods, and it per-forms especially well on the noisy cases.
 I.2 [ Artificial Intelligence ]: Learning Classification; supervised learning; boosting; direct opti-mization; noise tolerance
Boosting is a machine learning and data mining approach [34] that combines a set of weak classifiers to produce a single strong classifier. Most boosting methods were de-signed for binary classification tasks, while many real-world applications involve multiple classes, such as handwritten digit recognition, image segmentation, and automatic speech recognition. To effectively extend well-studied binary boost-ing algorithms to solve multi-class problems is still an on-going research topic.

Multi-class boosting methods can be roughly divided into two categories. The first is to reduce the multi-class problem to multiple binary classification problems. Methods in this category include  X  X ne-vs-all X ,  X  X ll-vs-all X , and other general output coding based approaches [2, 8, 10, 12, 13, 17, 18, 27]. Binary classification is well-studied, but there are some problems with the binary reduction, including (i) it may produce imbalanced data distributions, which are known to have a negative effect on the classifier performance [30, 16], (ii) a lack of guarantees of an optimal joint predictor [25], or (iii) using binary boosting scores that do not represent true class probabilities [21].

The second category is to build a multi-class classifier directly by using multi-class base classifiers, such as deci-sion trees. Boosting methods of this category include Ad-aBoost.M1, SAMME, AdaBoost.MM, GD-MCBoost, et al. [10, 22, 25, 36, 37]. Usually, these methods require strong base classifiers which substantially increase complexity and have a high potential for overfitting [25]. Moreover, all of these methods formulate multi-class tasks as a convex opti-mization problem by using surrogates, and it has been shown that all boosting algorithms based on convex optimization are susceptible to random classification noise [19]. In addi-tion, none are designed to directly maximize the multi-class margin, although some of them have the effects of margin enforcing.

In this paper, we introduce a new direct multi-class boost-ing algorithm named DMCBoost that extends the work of DirectBoost in [35] from binary classification to multi-class classification. DMCBoost uses multi-class decision trees as base classifiers to build an ensemble classifier by directly op-timizing the performance measures, without reducing them to binary classification problems. The process of DMCBoost includes two phases: it first directly minimizes the empiri-cal classification error by iteratively adding base classifiers to the ensemble classifier. Once the classification error reaches a coordinatewise local minimum 1 , it continuously adds base classifiers by directly maximizing the average margin of a certain set of bottom samples.

Both DMCBoost and DirectBoost [35] can be viewed as a coordinate optimization in the hypothesis space, and in each iteration only one coordinate is chosen and the correspond-ing parameter is computed by line search approaches. How-ever, DMCBoost is a non-trivial extension of DirectBoost in [35] to multi-class classification, where we have to mathe-matically re-formulate the multi-class classification problem and identify the scenarios that lead to efficient computation of the empirical error of a weak classifier in the first phase, and identify the scenarios that lead to efficient computa-tion of the margin curve of a weak classifier in the second phase. Thus DMCBoost uses very different optimization
S ee the definition of coordinatewise minimum/maximum on page 479 in [31]. techniques: since the objectives are more complex and more d ifficult to optimize, we propose new efficient line search al-gorithms that can find the parameter with the optimal ob-jective value along one coordinate; moreover, constructing decision trees for DMCBoost is more challenging, while it is straightforward in [35].

DMCBoost is a non-convex optimization method. In gen-eral, non-convex problems are very difficult to solve. How-ever, for the special 0-1 loss minimization and margin max-imization problems, we propose efficient algorithms to find a local optima. In many real applications, it is possible that the local optima of non-convex optimization is less serious than the inconsistencies [20]. The computational cost of DMCBoost is K times larger than the computational cost of AdaBoost.M1 and SAMME, where K is the number of classes. Nevertheless, we will show that DMCBoost per-forms better than the convex relaxation algorithms such as AdaBoost.M1, SAMME, AdaBoost.MH, and GD-MCBoost in terms of accuracy under a bearable time limitation on a number of UCI datasets and it is more robust in noisy cases. Furthemore, DMCBoost only requires very weak base clas-sifiers, and it is more efficient in driving down the empiri-cal classification error than other multi-class boosting algo-rithms when the same depth trees are used as weak learners, as shown in our experimental results.
In a multi-class classification, we want to predict the labels of examples lying in an instance space X . Let D denote a distribution over X  X Y , where Y = { 1 ,  X  X  X  , K } be the set of all the labels. We are provided a training set of labeled x  X  X has a unique label y i in the set Y . Denoting H = { h 1 , ..., h |H| } as the set of all possible weak classifiers that can be produced by the weak learning algorithm, where a weak classifier h j  X  X  is a mapping from an instance space X to Y .

Boosting combines weak classifiers to form a highly accu-rate ensemble classifier for multi-class classification by mak-ing a prediction according to the weighted plurality vote of the classifiers: where f ( x, y ) = P h  X  X   X  h 1( h ( x ) = y ) ,  X  h  X  R is the infer-ence function which is used to infer the predicted label, and 1(  X  ) is the indicator function. Our goal is to find an ensemble classifier F that generalizes well on any unseen dataset in X . To this end, we design a boosting approach through the following two phases: we first directly minimize the 0-1 loss on training data, and then directly maximize the average margin of a certain set of bottom samples. Phase I, which is very efficient in minimizing the empirical 0-1 loss, serves as an initialization method of phase II. The motivation is that phase II often performs better when it starts with a low training error. In phase II, a margin objective is optimized, which leads to a further improvement of generalization.
In multi-class classification, the empirical error is given by Figure 1: Three scenarios to compute the empiri-c al error of a weak learner h t over an example pair ( x i , y i ) , where l denotes the incorrect label with the highest score, and q denotes the intersection point that results in an empirical error change. The red bold line for each scenario represents the inference function of example x i and its true label y i .
 Algorithm 1 0 -1 loss minimization algorithm. 1: I nitialize: t = 0 2: repeat 3: t  X  t + 1. 4: Select a weak classifier h t by Algorithm 1.2. 5: Get the interval that has the minimum classification 6: Update: f t ( x i , y ) = f t  X  1 ( x i , y ) +  X  t 1( h 7: until the training error reaches the local coordinatewise 8: Output: f t ( x i , y ).
Due to the nonconvexity, nondifferentiability, and disconti-nuity of the classification error function (2), many previous multi-class boosting algorithms optimize the convex upper bounds of (2). While the convex surrogate losses are compu-tationally efficient to globally optimize [3], they are sensitive to outliers [19, 23] and inconsistent under some conditions [20]. In contrast, our approach is to directly optimize 0-1 loss (2).

We use a greedy coordinate descent algorithm to directly minimize the empirical error (2) by constructing an ensemble classifier. Consider the t th iteration, the inference function t  X  1 weak classifiers h k ( x ) and corresponding weights  X  k = 1 ,  X  X  X  , t  X  1 have been selected and determined, and our goal is to select a weak classifier h t and its weight  X  t that (2) is minimized. Algorithm 1 outlines the greedy co-ordinate descent algorithm that sequentially minimizes 0-1 loss of (2), we will introduce the line search algorithm and the weak learning algorithm later. On each round, we first select a weak classifier h t by Algorithm 1.2 (line 4), then the 0-1 loss (2) is a stepwise function w.r.t  X  t . Next we can compute the interval that has the minimum classification er-ror by using the line search algorithm (Algorithm 1.1) along the h t coordinate. Since any value of  X  t within this inter-val will lead to the largest error reduction, we can simply choose the middle point of the interval (line 5). In the end of each iteration, the inference function is updated (line 6). We repeat this procedure until the training error reaches a local coordinatewise minimum (line 7). Algorithm 1.1 L ine search algorithm to find the interval with the minimum 0-1 loss. 1: I nput: a weak classifier h t  X  X  . 2: Let e : R  X  Q .  X  e map intersection points to error 3: for i = 1 ,  X  X  X  , n do 4: if h t ( x i ) = y i then  X  scenario 1 5: q = a ( x i , l )  X  a ( x i , y i ). 6: error update =  X  1 . 7: else if h t ( x i ) = y , y 6 = y i , and a ( x i , y i 8: q = a ( x i , y )  X  a ( x i , y i ). 9: error update = 1. 10: end if 11: e [ q ] = e [ q ] + error update .  X  e [ q ] = 0 if q is not in e 12: end for 13: Sort e by the keys in an increasing order. 14: Incrementally calculate classification error on each in-15: Output: the interval with minimum 0-1 loss.

The line search algorithm describes how to find the op-t imal value of  X  for any given hypothesis h  X  H such that (2) is minimized. The key idea is how to find the points that lead the 0-1 loss changes efficiently. Let a ( x i , y ) = P k =1  X  k 1( h k ( x ) = y ), then let the inference functions for example x i be which is a linear function of  X  t with intercept a ( x i , y ) and slope 1( h k ( x ) = y ). Obviously, the inference function is either a line with slope 1 or a horizontal line. The infer-ence functions are used to compute the empirical error (2). More specifically, given a weak learner h t  X  H , for each example pair ( x i , y i ), there are 3 scenarios to compute the empirical error, see Figure 1. Scenario 1 is the case that h ( x i ) = y i . f t ( x i , y i ) is a line with slope 1, and assume with slope 0. The intersection of f t ( x i , y i ) and f side of the intersection point, there is an error for exam-ple x i , otherwise there is no error. Scenario 2 is the case that h t ( x i ) = y , y 6 = y i , and a ( x i , y i ) &gt; a ( x y 6 = y i . Then f t ( x i , y ) is a line with slope 1, and f a line with slope 0. The intersection point of f t ( x i , y ) and f ( x i , y i ) is at  X  t = a ( x i , y )  X  a ( x i , y i ). Thus when  X  on the right side of the intersection point, there is an error for example x i , otherwise there is no error. Scenario 3 is the case that h t ( x i ) = y , and y 6 = y i , and  X  l  X  Y , l 6 = y no matter what value  X  t has.

Formally, Algorithm 1.1 describes the line search proce-dure. We use e (bold letter denotes a vector valued function or variable) to record all the intersection points and their corresponding error updates on the right-hand side (line 2). More specifically, for each training example, we first catego-rize it into the three scenarios. For an example in Scenario 1, the intersection point is at q = a ( x i , l )  X  a ( x error update on the right-hand side of q is  X  1 (line 4-6). In Scenario 2, the intersection point is at q = a ( x i , y )  X  a ( x and the error update on the right-hand side of q is 1 (line 7-9). We add the intersection points as the keys and their Algorithm 1.2 C onstructing tree algorithm. 1: I nput: a training set S , current tree depth dep . 2: Let dep  X  dep + 1. 3: if dep  X  max dep t hen 4: for a binary split do 5: Split S into S left and S right . 6: if |S left | = 0 or |S right | = 0 then continue. 7:  X  left = arg min 8: Let h t ( x ) =  X  left if x  X  X  left . 9:  X  right = arg min 10: Let h t ( x ) =  X  right if x  X  X  right . 11: end for 12: end if 13: Choose the optimal binary split which splits S into S 14: Let h t ( x ) =  X   X  left if x  X  S  X  left , and h t ( x ) =  X  15: Call constructing tree algorithm with input S  X  left and 16: Call constructing tree algorithm with input S  X  right 17: Output: a weak classifier h t  X  X  . corresponding error updates as the v alues into e (line 11). We only care about the examples in Scenario 1 and 2 since the examples in Scenario 3 do not lead to an error update no matter what value  X  t has. Once all the intersection points are added into e , we sort e by the keys in an increasing or-der (line 13). These intersections divide the coordinate to (at most) | e | + 1 intervals, the classification error on each interval can be incrementally calculated by the values of e (line 14), and hence the interval which gives the minimum error is easy to obtain.

The weak learning algorithm is described in Algorithm 1.2. Here we only consider the decision trees algorithm with binary splits. The binary splits are preferred [15] since (i) multiway splits fragment the data too quickly, leaving in-sufficient data at the next level down; and (ii) multiway splits can be achieved by a series of binary splits. For a binary splitting node that splits the training examples into two subsets, we denote them as S left and S right (line 5). We first enumerate all the possible labels from 1 to K on S left that produce K hypotheses that belong to H . We choose the optimal label which leads to the minimum value of (2) by running the line search algorithm (Algorithm 1.1) with these K hypotheses (line 7). We then fix the selected label for S left (line 8), and apply the same process on S right 9). We simply choose the attribute to split by minimizing the 0-1 loss (line 13), and use a top-down, greedy search approach to build trees. If the problem involves real-valued variables, they are first binned into intervals, each interval being treated as an ordinal attribute. Note that since the historical information f t  X  1 ( x i , y i ) is used in Algorithm 1.1 through building trees, Algorithm 1.2 will not end up with the same tree on each iteration t though DMCBoost does not maintain a distribution over training samples. Figure 2: Three scenarios of margin curve of a weak l earner h t over an example pair ( x i , y i ) , where l de-notes the incorrect label with the highest score.

The computational cost of Algorithm 1 is O ( nMK ) on each round when decision stumps 2 are used as weak learners, where M is the number of binary splits. It has the same computational cost as AdaBoost.MH, and is K times larger than the computational costs of AdaBoost.M1 and SAMME. Algorithm 1 may trap a coordinatewise local minimum of 0-1 loss. Nevertheless, we switch to the algorithm that directly maximizes various margins.
For boosting, the minimization of the training error is only one side of the story. To explain why boosting works, Schapire et al. [26] introduced the margin theory, which suggested that boosting is especially effective at maximiz-ing the margins of training data. In multi-class classifica-tion, the most direct generalization of the margin is simply the difference between the score (weighted fraction of votes) obtained by the correct label and the score of the highest scoring incorrect label. We denote the (normalized) margin of an example ( x i , y i ) with respect to an inference function f ( x i , y ) = P t k =1  X  k 1( h k ( x i ) = y ),  X  y  X  Y by M mally, This definition of margin (4) is given in some earlier stud-ies [2, 26]. A large margin implies the ensemble classifier confidently classifies the corresponding training sample.
While boosting X  X  success can be ascribed to maximizing the margins, most boosting methods were not designed to specially optimize any margin functions [28]. Some excep-tions, such as LPBoost [7], SoftBoost [33], and DirectBoost [35], explicitly maximize a relaxed minimum margin objec-tive, but they are designed for binary classification prob-lems. For the well-known multi-class boosting algorithms AdaBoost.M1 [10], SAMME [36], and AdaBoost.MM [22], none of them has been shown to maximize the multi-class margin [25]. The recently proposed algorithms CD-MCBoost and GD-MCBoost [25] optimize a margin enforcing loss func-tion, but actually this objective is not related to the margin in the sense that one can minimize the loss function while simultaneously achieving a bad margin even for binary prob-lems [24]. In this section, we introduce a coordinate ascent algorithm that directly maximizes the predefined margin ob-jective functions for multi-class classification.
D ecision stumps are the special decision trees with a depth of 1. When more powerful trees are used, the complexity of Algorithm 1 has the same increasing rate as other boosting algorithms.

We first introduce the objective function that we are work-ing on in this section. We can sort M i in an increasing order, and consider n  X  worst training examples n  X   X  n that have smaller margins, then define the average margin over those n labeled examples by g avg n  X  . Formally, where B n  X  denotes the set of n  X  labeled examples having the smallest margins. The minimum margin (hard mar-
P n i = 1 M i are special cases for n  X  = 1 and n  X  = n respec-tively. The parameter n  X  indicates how much we relax the hard margin on training examples, and we set n  X  based on knowledge of the number of noise examples in training data. The higher the noise rate, the larger the n  X  that should be used.

The following theorem shows the objective function (5) is equivalent to the soft margin, this conclusion is also given in [29] for binary classification, here we propose a general proof for multi-class classification and use different proof skills.
Theorem 1. Maximizing the average margin of the bot-tom n  X  examples (5) is equivalent to solving the soft margin maximization problem where  X  are slack variables.
 Due to the max operator in the definition of margin (4), the soft margin optimization problem (6) is very difficult to solve, unlike its binary counterpart. To the best of our knowledge, there is no multi-class boosting designed to solve this soft margin optimization problem. Thus, one of the reasons for using (5) as objective is to solve the soft margin problem but from another point of view.

Another motivation of optimizing (5) is that the average of the bottom n  X  margins can be used to measure the gener-alization performance of a combined classifier, as shown in the following theorem.

Theorem 2. If the set of possible base classifiers H is and n  X   X  { 1 ,  X  X  X  , n } , with the probability at least 1  X   X  over the random choice of the training set S with size n , each inference function f  X  X  ( H ) satisfies the following bound: Pr [ M ( f, x, y ) &lt; 0]  X  ( K  X  1) log |H| 2 n + w here Pr D [ M ( f, x, y ) &lt; 0] denotes the generalization error of the ensemble classifier F , M ( f, x, y ) denotes the margin of an example ( x, y ) associated with f , D  X  1 ( v 1 , v the inverse of K-L divergence between v 1 and v 2 when v 1 fixed [32], and
T he outline of the greedy coordinate ascent algorithm that sequentially maximizes the average margin of bottom n  X  ex-amples is described in Algorithm 2. Similar to the 0-1 loss minimization algorithm, we intend to select a weak classifier Algorithm 2 M argin maximization algorithm. 1: I nitialize: t and  X  from 0-1 loss minimization algo-2: repeat 3: t  X  t + 1. 4: Select a weak learner h t by weak learning algorithm. 5: Compute q  X  by Algorithm 2.1 which maximizes (5) 6: Update: f t ( x i , y ) = f t  X  1 ( x i , y ) +  X  t 1( h 7: until the average margin of bottom n  X  examples reaches 8: Output: f t ( x i , y ). h ( line 4) and its weight  X  t (line 5) on round t , but this time our target is maximizing (5). This procedure terminates if there is no increment in the average margin over the bottom n examples over h t (line 8). Its convergence can be proved in the same way as for the binary classification given in [35].
The key step is the line search algorithm which finds the value of  X  t that maximizes (5) for a given weak classifier h  X  X  . At t th iteration, let c = P t  X  1 k =1 |  X  k | , then the margin on the example ( x i , y i ) can be rewritten as, Consider the case that  X  t  X  0. For each example pair ( x i , y i ), there are three scenarios of the margin (7) to con-sider, as shown in Figure 2. Scenario 1 is the case that h ( x i ) = y i , and assume that l = arg max y  X  X  ,y 6 = y which is monotonically increasing in Figure 2. Scenario 2 is responds to the curve which is monotonically decreasing in Figure 2. Scenario 3 is the case that h t ( x i ) = y , and y 6 = y i , and  X  l  X  Y , l 6 = y i such that a ( x i , l ) &gt; a ( x this case the margin curve of M i has two pieces. When  X  t &lt; a ( x i , y )  X  a ( x i , l ), M i = a ( x i ,y i  X  narios for the case that  X  t &lt; 0 can be similarly identified.
Finding the exact solution of optimal  X  t along the h t co-ordinate is computationally difficult since the examples in Scenario 3 can either intersect with the examples in Sce-nario 1 or intersect with the examples in Scenario 2. Fortu-nately, we can prove that (5) is a quasi-concave function [4, 6], this property allows us to design an efficient line search algorithm.

Theorem 3. Denote the average margin of the bottom n X  examples with respect to the set of weak classifiers H and their weights  X  as Algorithm 2.1 L ine search algorithm to find the solution q that maximizes (5). 1: I nput: a weak classifier h t  X  X  , an interval [ begin, end ], 2: repeat 5: begin = q . 6: else 7: end = q . 8: end if 9: until end  X  begin &lt; th . where { B n  X  |  X  } d enotes the set of n  X  examples whose margins are at the bottom for fixed  X  . Then g avg n  X  (  X  ) is a quasi-concave function for any  X  .

Therefore, we can design an algorithm that maximizes (5) efficiently by checking the derivative of (5) as which is calculated as,
Assume q  X  is the optimal value of  X  t that maximizes (5), then (5) is monotonically increasing at  X  t &lt; q  X  , otherwise it is monotonically decreasing. Thus, (9) is less than 0 at  X  t &lt; q  X  , and (9) is greater than 0 at  X  t &gt; q  X  . Formally, the line search algorithm to calculate the value of q  X  with a small deviation threshold th is described in Algorithm 2.1.
To select the weak classifier, we use a similar procedure as in Algorithm 1.2 and replace the measure to the average margin of the bottom n  X  examples. Again, we only consider the decision trees algorithm with binary splits, and apply Algorithm 2.1 on the two subsets respectively. We choose the attributes to split by maximizing the average margin of the bottom n  X  examples, and use a top-down, greedy search approach to build trees. Same as Algorithm 1, the computa-tional cost of Algorithm 2 is O ( nMK ) on each round when decision stumps are used as weak learners, where M is the number of binary splits.

Since (5) is non-differentiable at turning points, the co-ordinate ascent algorithm may get stuck at a corner from which it is impossible to make progress along any coordinate direction. To overcome this difficulty, we use an  X  -relaxation method [5], which allows a single coordinate to change even if this worsens the objective value. When a coordinate is changed, it is set to  X  plus (or  X  minus) the value that max-imizes the objective function along that coordinate, where  X  is a small positive number. If  X  is small enough, the algo-rithm can eventually approach a small neighborhood of the optimal solution. Abalone 4177 28 8 5-CV CNAE-9 1080 9 856 5-CV Poker525k 525010/500000 10 11 test error Segmentation 210/2100 7 19 test error Waveform 5000 3 21 5-CV
T o evaluate the performance of the DMCBoost algorithm, we first conduct experiments with 13 datasets from the UCI repository [9], then examine its noise robustness on two datasets with random label noise. For comparison, we also report the results of AdaBoost.M1 [11], AdaBoost.MH [27], SAMME [36], and GD-MCBoost [25]. All these algorithms use multi-class base classifiers except AdaBoost.MH, which essentially reduces the multi-class problem to a set of bi-nary classification problems. The classification error is esti-mated either by a test error or five-fold cross-validation. The datasets which come with pre-specified training and testing sets are evaluated by the test error, where n  X  is set to DMCBoost and the number of rounds is set to the maximum of 5000 for each method. For datasets which are evaluated by cross-validation, we partition them into five parts evenly for 5-fold. In each fold, we use three parts for training, one part for validation, and the remaining part for testing. We use the validation data to choose the optimal model for each algorithm. For AdaBoost.M1, AdaBoost.MH, SAMME, and GD-MCBoost the validation data is used to perform early stopping. We run these algorithms with a maximum of 5000 iterations, and then choose the ensemble classifier from the round with minimal error on the validation data. For DM-CBoost, the parameter n  X  is chosen on the values { 1, n 10 DMCBoost is defined as line 7 in Algorithm 2 where DMC-Boost terminates at the margin maximization solution, thus we need not to apply early stopping. In all the experiments, the value of  X  is set to be 0.01 and the value of th is set to be 1 e -5.

An overview of these 13 UCI datasets is shown in Table 1. The datasets have different numbers of input variables (6-856), classes (3-26), and instances (178-1,025,010), and represent a wide area of types of problems. In the # Exam-ples column, the number of training/test examples are listed for datasets coming with pre-specified training and testing sets, and the entire number of examples is given for the rest datasets. The original Poker dataset has 25,010 training ex-amples and 1,000,000 examples for testing. Since the test data is very large, same as the way Li did in [17, 18], we randomly divide it equally into two parts, and add them to training and testing sets respectively, thus its training size becomes 525,010 and the test size becomes 500,000. There-Table 2: Test error (and standard deviation) of multi-class boosting methods AdaBoost.M1, SAMME, GD-MCBoost, and DMCBoost on 13 UCI datasets, using multi-class decision trees with a depth of 3.
 Abalone -74.20(1.8) 74.62(1.5) 7 4.03(2.0) CNAE-9 -14.91(2.4) 11.4(1.9) 7 .59(1.2) Nursery 9.70(1.5) 3.26(0.7) 0.2(0.0) 0 .02(0.0) Poker525k 49.16 69.09 -3 0.09 Segmentation 8.29 6.43 6.0 5 .1 Waveform 17.8(1.2) 16.96(1.2) 16.2(1.1) 1 4.38(1.1) fore, the datasets we selected include fairly large datasets ( Poker525k) as well as datasets of moderate sizes (Krkopt, Letter and Nursery).
We compare all multi-class boosting algorithms on 13 UCI datasets. First, we restrict the base classifiers to smaller trees to test the performance of each algorithm when the base classifiers are very weak. We exclude the results of Ad-aBoost.MH as all the rest algorithms use multi-class base classifiers, and we want to compare the performance of each algorithm with the same hypothesis space H . Table 2 shows the results of different methods when multi-class decision trees with a depth of 3 are used as weak learners 3 . With small trees, DMCBoost gives the best results on all datasets indicating that DMCBoost only requires very weak base classifiers even if there is no exact weak learner condition for DMCBoost. GD-MCBoost achieves the second best ac-curacy, this algorithm also requires weaker base classifiers since it is able to boost any type of weak learners with non-zero directional derivatives [25]. We do not report its re-sults on the Poker525k dataset since its one iteration takes more than 12 hours to run by the authors X  matlab code. For SAMME, the weak learner conditions can be satisfied easily, but it couldn X  X  drive down the training error when the base classifier is very weak, and its performance is much worse. AdaBoost.M1 gives the worst results, and it is not able to boost the base classifiers for 5 of 13 datasets, as shown in Table 2.

With the same hypothesis space H (trees with a depth of 3), 0-1 loss minimization algorithm (Algorithm 1) usually achieves a lower training classification error rate. The left panel of Figure 3 shows a typical training error curve on the
W hether a base classifier is weak or not often depends on the properties of the datasets, such as number of classes, examples and input variables. For the most multi-class datasets in Table 1, decision trees with a depth of 3 is weak enough. The previous multi-class boosting [10, 22, 36] stud-ies often use much larger trees in the experiments. the margin maximization algorithm on the Car dataset. 12.
 Car dataset, and the middle panel shows the corresponding t est error curve. Once the 0-1 loss minimization algorithm terminates at a coordinatewise local minimum, DMCBoost switches to the margin maximization algorithm (Algorithm 2), and it can still drive down the test error even when the training error does not decrease, as shown in the right panel of Figure 3.

We now analyze the running time of AdaBoost.M1, SAMME, and DMCBoost on the Poker525k dataset, which has 525,000 training examples. We implemented each algorithm by C++, and test them on a PC with Core2 Duo 2.6GHz. We left GD-MCBoost out of the comparison since it is unfair to compare a matlab implementation with C++ implementa-tions, and GD-MCBoost runs too slow to record the run-ning time. AdaBoost.M1 and SAMME are very efficient in terms of running time, they take about 10s on each round. For DMCBoost, it takes about 90s on each round, which is slower than AdaBoost.M1 and SAMME but it is bearable on such a scale as the dataset.

We next investigate how these algorithms perform with more powerful base classifiers. We tried all tree depths in the candidate set { 3,5,8,12 } for each dataset. This time we compare the algorithms not restricted in the same hypothe-sis space H , so we also add AdaBoost.MH in the comparison. As shown in Table 3, among all the methods, DMCBoost gives the most accurate results in 10 of the 13 datasets, and its results are close to the best results produced by the other methods for the remaining 3 datasets.
In many real-world applications, training samples are ob-tained through manual labeling and there will be unavoid-able human errors that provoke wrong labels. Hence, it is desirable that the designed classification algorithm is robust to noise. In the experiments conducted below, we check the noise robustness of each boosting algorithm on Car and Nursery datasets with additional label noise. We randomly change the labels on training and validation data at the rates of 5% and 20% respectively, and keep the the test data clean. Again, the tree depths are chosen from the candidate set { 3,5,8,12 } . The results via 5-fold cross-validation (as de-scribed in the begining of section 3) are reported in Table 4. The affection of label noise to the performance of DM-CBoost is very limited, especially for the Nursery dataset, the test error only increases from 0 to 1.6% when 20% of the training examples have wrong labels. Similar to AdaBoost in binary cases, AdaBoost.M1 and SAMME are quite sen-with a maximum depth of 12.
 sitive to noise, their performance is hurt badly even with a 5 % noise rate.

DMCBoost achieves good performance by varying the pa-rameter n  X  , the higher the noise rate, the larger n  X  should be used. Consider the Car dataset as an example, for the case that the training set is clean (noise rate is 0), the op-timal n  X  via cross-validation is n 10 a nd the training error is 1.5%, thus the algorithm focus on the difficult examples. For the case that the noise rate is 5%, the optimal n  X  is n 4 the training error is 7.1%, indicating that DMCBoost allows some misclassification to achieve a better performance. For a noise rate of 20%, DMCBoost considers more examples in the bottom set, the optimal n  X  via cross-validation is n this case, DMCBoost further allows more misclassification (the training error is 22.2%), but its corresponding test error is only 6.55%.
In this paper we have proposed a multi-class boosting ap-proach that directly optimizes the 0-1 loss and the targeted margins. Experiments show that our method gives better results in the case of restricting the weak learning algorithm to small decision trees, and performs highly competitively with the existing boosting algorithms in the case of deeper decision trees. More importantly, our method is more robust on the noisy data.

In this study we restrict the weak learners to multi-class decision trees as the combination of boosting with a decision trees is the state-of-the-art classification method [1]. For future works, we will consider more weak learning algorithms and the case that |H| is infinite, such as k-Nearest Neighbors, Naive Bayes, and Neural Networks. This research is supported in part by AFOSR under grant FA9550-10-1-0335, NSF under grant IIS:RI-small 1218863, DoD under grant FA2386-13-1-3023, and a Google research award.
Proof. : (i) We first show that when the optimal solution of soft margin optimization is achieved, M i +  X  i =  X  always holds for those  X  i &gt; 0. Suppose when the optimal solution is achieved, there exists an example such that M i +  X  i &gt;  X  and  X  i &gt; 0. The slack variable  X  i &gt; 0 indicates M then we can always find a  X   X  i &lt;  X  such that M i +  X   X  therefore replace  X  to  X   X  in objective function that produce a greater objective value, a contradiction. (ii) We next prove under the optimal solution, there are at most n  X  examples that are allowed to lie below (or equal)  X  , that is, |{ x i |M i  X   X  }|  X  n  X  . Suppose we arrived a optimal solution and n  X  &lt; |{ x i |M i  X   X  }| X  m  X  , with the correspond-ing slack variable  X  . Then we can always find a  X   X  &lt;  X  such that |{ x i |M i  X   X   X  }| X  n  X  , with the slack variable  X  note  X  =  X   X   X   X  , then by (i)  X   X  i =  X  i  X   X  for the examples with  X  &gt; 0. We use B m  X  to denote the set of m  X  examples having the smallest margins, then B n  X   X  B m  X  and B c n  X   X  B m  X  We have w hich is a contradiction. (iii) We further show that |{ x i |M i  X   X  }|  X  n  X  under the optimal solution. Suppose there is an optimal solution with |{ x i |M i  X   X  }|  X  m  X  &lt; n  X  and slack variable  X  , then there must exist a  X   X  &gt;  X  such that m  X  &lt; |{ x i |M i  X   X  }| X  n corresponding  X   X  . We denote  X  =  X   X   X   X  and we have w here the inequation holds since  X   X  i  X   X  if x i  X  B c m  X  A contradiction. (iv) Combine the results (ii) and (iii), we have |{ x i |M  X  }| = n  X  , and therefore P i  X  B ing by n  X  on both sides of the equation, we get 1 n  X  P i  X  B
P roof. : Our proof is inspired by the works in [14, 26, 32]. Let C ( H ) denote the convex hull of H , and let C N denote the set of unweighted averages over N elements from H . Formally,
We denote the distribution over H by the coefficients {  X  to be Q ( f ). Let B { n  X  | f } be the set of n  X  examples having the smallest margins associate with the inference function f . By [26] we know that for any fixed  X  &gt; 0
Let M ( g, x ( k ) , y ( k ) ) denote the k -th smallest margin with respect to g . For any n  X   X  { 1 ,  X  X  X  , n } and  X  N &gt; 0, we consider the following probability: by using the relative entropy Chernoff bound.
Let Z = { i / |H| : i = 1 ,  X  X  X  , |H|} , we only consider  X  at the values in Z . By the fact that |C N ( H ) |  X  |H| N applying the union bound, for any n  X   X  X  1 ,  X  X  X  , n } ,
Let  X  N = |H| N +1 exp(  X  nD ( n  X  n  X  ) ), then
T hus, with a probability at least 1  X   X  N over the training sample S , for all f  X  C ( H ), all  X   X  Z , and all fixed n have S ince A nd for any  X  &gt;  X  , Thus, we have
We now prove if M ( f, x ( n  X  ) , y ( n  X  ) )  X   X  and M ( g, x  X  with  X  &gt;  X  , there always exists an example ( x i , y ists a bijection between {M ( f, x (1) , y (1) ) ,  X  X  X  , M ( f, x and {M ( g, x (1) , y (1) ) ,  X  X  X  , M ( g, x ( n ) , y ( n )  X  n  X  n . If  X  n  X  n  X  , then ( x ( n  X  ) , y ( n  X  ) ) of M ( f, x desired. On the other hand, if  X  n &gt; n  X  , then there are at least n  X  n  X  examples greater than or equal to  X  in {M ( f, x i i 6 = n  X  } but at most n  X  n  X   X  1 examples greater than  X  in {M ( g, x i , y i ) : i 6 =  X  n } . Thus,
B y combining (11) and (12), we obtain that with probability at least 1  X   X  N over the training sample S , for all f  X  X  ( H ), all  X   X  Z and all  X  &gt;  X  and all n  X  = { 1 ,  X  X  X  , n } , but fixed N ,
T o let  X  takes values only in Z , we set  X  =  X  2  X   X  |H| holds for  X   X  8 |H|
S etting  X  = 2 N  X  N . Thus, with probability at least 1  X   X  over the random choice of the training data S of n examples, for all f  X  X  ( H ) and all n  X  = { 1 ,  X  X  X  , n } , we obtain w here
P roof. : By definition of quasiconcave, g avg n  X  (  X  ) is quasiconcave if and only if its upper contour sets are convex sets. The  X  -upper-contour set S  X  of g avg n  X  (  X  ) is denoted as
We now prove that S  X  is a convex set. For  X   X  (1) ,  X  (2) S ,  X   X   X  [0 , 1], we have
Therefore, (1  X   X  )  X  (1) +  X   X  (2)  X  X   X  . g avg n  X  (  X  ) is quasi-concave.
