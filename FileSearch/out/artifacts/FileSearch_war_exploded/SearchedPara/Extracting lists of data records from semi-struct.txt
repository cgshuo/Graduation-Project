 1. Introduction
In today X  X  Web, there are many sites providing access to structured data contained in an underlying data-base. Typically, these sources provide some kind of HTML form that allows issuing queries against the data-base, and they return the query results embedded in HTML pages conforming to a certain fixed template. These it allows data integration applications to access web information in a manner similar to a database. It also allows information gathering applications to store the retrieved information maintaining its structure and, therefore, allowing more sophisticated processing.

Several approaches have been reported in the literature for building and maintaining  X  X  X rappers X  X  for semi-an administrator to create an ad-hoc wrapper for each target data source, using some kind of tool which nature varies depending of the approach used. Once created, wrappers are able to accept a query against the data source and return a set of structured results to the calling application.

Although wrappers have been successfully used for many web data extraction and automation tasks, this approach has the inherent limitation that the target data sources must be known in advance. This is not pos-crawl the web looking for topic-specific information.

Several automatic methods for web data extraction have been also proposed in the literature [3,11,22,34] , need to extract data from them. Second, the proposed methods make some assumptions about the pages con-taining structured data which do not always hold. For instance, [34] assumes the visual space between two these issues in the related work section).

In this paper, we present a new method to automatically detect a list of structured records in a web page vious approaches. We have also validated our method in a high number of real websites, obtaining very good effectiveness. 1.1. Organization of the paper
The rest of the paper is organized as follows. Section 2 describes some basic definitions and models our approach relies on. Sections 3 X 5 describe the proposed techniques and constitute the core of the paper.
Section 4 explains how we segment the data region into individual data records. Section 5 describes how using our method with real web pages. Section 7 discusses related work. Finally, Section 8 concludes the paper. 2. Definitions, models and problem formulation
In this section, we introduce a set of definitions and models we will use through the paper. 2.1. Lists of structured data records
We are interested in detecting and extracting lists of structured data records embedded in HTML pages. In this section, we formally define the concept of structured data and the concept of list of data records.
A type or schema is defined recursively as follows [1,3] : of the paper, we define a token to be a text string or a HTML tag. 2. If T 1 , ... , T n are types, then their ordered list h T constructed from the types T 1 , ... , T n using a tuple constructor of order n .
An instance of a schema is defined recursively as follows: 1. An instance of the basic type, b , is any string of tokens. 2. An instance of type h T 1 , T 2 , ... , T n i is a tuple of the form h i types T 1 , T 2 , ... , T n , respectively. Instances i 1 3. An instance of type { T } is any set of elements { e 1 { h T 1 , T 2 , ... , T n i }. T 1 , T 2 , ... , T n are the types of the attributes of the record in the list.
For instance, the type of a list of books where the information about each book includes title, author, for-mat and price could be represented as { h TITLE, AUTHOR, FORMAT, PRICE i }, where TITLE, AUTHOR, FORMAT and PRICE represent basic types.
 it has a cardinality of 0 or 1. Similarly, if T 1 and T 2 h T , T 2 i where, in every instantiation of it, the instantiation of either T the other has zero occurrences. 2.2. Embedding lists of data records in HTML pages the kind described in the previous section. This list needs to be embedded by a program into an HTML page for presentation to the user. The model we use for page creation is taken from [3] .
A value x from a database is encoded into a page using a template T . We denote the page resulting from encoding of x using T by k ( T , x ).

A template T for a schema S , is defined as a function that maps each type constructor s of S as follows: 1. If s is a tuple constructor of order n , T ( s ) is an ordered set of n + 1 strings h C 2. If s is a set constructor, T ( s ) is a string C s .
 of encoding of subvalues of x : 1. If x is of basic type, b , k ( T , x ) is defined to be x itself. 2. If x is a tuple of form h x 1 , ... , x n i s t , k ( T , x ) is the string C instance of the sub-schema that is rooted at type constructor s 3. If x is a set of the form { e 1 , ... , e m } s s , k ( T , x ) is given by the string k ( T , e an instance of the sub-schema that is rooted at type constructor s
This model of page creation defines a precise way for embedding lists of data records in HTML pages. For instance, Fig. 2 shows an excerpt of the HTML code of the page in Fig. 1 , along with the used template.
This model of page creation captures the basic intuition that the data records in a list are formatted in a also establishes that the data records in a list are shown contiguously in the page. 2.3. Lists of records represented in the DOM tree of pages
HTML pages can also be represented as DOM trees [31] . For instance, Fig. 3 shows an excerpt of the DOM tree of the example HTML code shown in Fig. 2 .

From the model defined in the previous section to embed lists of data records in HTML, we can derive the following properties of their representation as DOM trees:
Property 1. Each record in the DOM tree is disposed in a set of consecutive sibling subtrees. Additionally, record, and the following three subtrees form the second record.

Property 2. The occurrences of each attribute in several records have the same path from the root in the DOM
DOM tree, and the same applies to the remaining attributes. 3. Finding the dominant list of records in a page the page.

From the Property 1 of the previous section, we know finding the data region is equivalent to finding the common parent node of the sibling subtrees forming the data records. The subtree having as root that node
Our method for finding the region containing the dominant list of records in a page p consists of the fol-lowing steps: 1. Let us consider N , the set composed by all the nodes in the DOM tree of p . To each node n assign a score called s i . Initially " i =1.. j N j s i =0. 2. Compute T , the set of all the text nodes in N . the DOM tree are contained in the same p i . To compute the paths from the root, we ignore tag attributes. 4. For each pair of text nodes belonging to the same group, compute n the DOM tree, and add 1 to s j (the score of n j ). 5. Let n max be the node having a higher score. Choose the DOM subtree having n data region.
 the dominant list in the page will typically contain more texts with the same path from the root than other regions. In addition, given two text nodes with the same path in the DOM tree, the following situations may occur: attribute in different records), then their deepest common ancestor in the DOM tree will be the root node of correct node is increased. For instance, in Fig. 3 the deepest common ancestor of d the subtree containing the whole data region. est common ancestor could be a deeper node than the one we are searching for and the score of an incorrect node would be increased. For instance, in the Fig. 3 the deepest common ancestor of d will output the right node. Now, we explain the reason for this. Let us consider the pair of text nodes ( t corresponding with the occurrences of attribute 1 and attribute by Property 2 , for each record r i in which attribute 1 and attribute ( t 12 , t i 1 ), ( t 12 , t i 2 ), which are in case 1.
 1. When optional fields exist, it is easy to see that it is still very probable.
This method tends to find the list in the page with the largest number of records and the largest number of information can be used to refine the above method. The idea is very simple: in the step 2 of the algorithm, the only text nodes considered in step 2 would be the ones marked with an  X  include any conditions with the aforementioned operators, then the algorithm would use all the texts. ability in the list of results than in other lists of the page. 4. Dividing the list into records
Now we proceed to describe our techniques for segmenting the data region in fragments, each one contain-ing at most one data record.

Our method can be divided into the following steps: data region into records.  X  Choose the best candidate record list. The method we use is based on computing an auto-similarity measure between the records in the candidate record lists. We choose the record division lending to records with the higher similarity.
 ilarity between two sequences of consecutive sibling subtrees in the DOM tree of a page. The method we use for this is described in Section 4.1 . 4.1. Edit-distance similarity measure
To compute  X  X  X imilarity X  X  measures we use techniques based in string edit-distance algorithms. More pre-cisely, to compute the edit-distance similarity between two sequences of consecutive sibling subtrees named r and r j in the DOM tree of a page, we perform the following steps: 1. We represent r i and r j as strings (we will term them s a. We substitute every text node by a special tag called text . 2. We compute the edit-distance similarity between r i and r between s i and s j ( ed ( s i , s j )). The edit distance between s operations (each operation affecting one char at a time) needed to transform one string into the other. We calculate string distances using the Levenshtein algorithm [23] . Our implementation only allow insertion and deletion operations (in other implementations, substitutions are also permitted). To obtain a similarity score between 0 and 1, we normalize the result dividing by ( len ( s result from 1 (1) . In our example from Fig. 4 , the similarity between r 4.2. Generating the candidate record lists chosen. Each candidate record list will propose a particular division of the data region into records.
By Property 1 , we can assume every record is composed of one or several consecutive sibling sub-trees, which are direct descendants of the node chosen as root of the data region.

We could leverage on this property to generate a candidate record list for each possible division of the subtrees verifying it. Nevertheless, the number of possible combinations would be too high: if the number of subtrees is n, the possible number of divisions verifying Property 1 is 2 in the same list may be composed of a different number of subtrees, as for instance r with each data record composed of an average of 4 subtrees). Therefore, this exhaustive approach is not fea-sible. The remaining of this section explains how we overcome these difficulties. Our method has two stages: 1. Clustering the subtrees according to their similarity. 2. Using the groups to generate the candidate record divisions.
 The next subsections respectively detail each one of these stages.

Grouping the subtrees . For grouping the subtrees according to their similarity, we use a clustering-based process we describe in the following lines: the data region. Each t i can be represented as a string using the method described in Section 4.1 . We will term these strings as s 1 , ... , s n . 2. Compute the similarity matrix . This is a n  X  n matrix where the ( i , j ) position (denoted m es ( t i , t j ), the edit-distance similarity between t i 3. We define the column similarity between t i and t j , denoted ( t trees as similar, the column similarity measure requires their columns in the similarity matrix to be very of subtrees in the set to be considered as similar. We have found column similarity to be more robust for estimating similarity between t i and t j in the clustering process than directly using es ( t is to start with one cluster for each element and successively combine them into groups within which inter-element similarity is high, collapsing down to as many groups as desired.

Fig. 5 shows the pseudo-code for the bottom-up clustering algorithm. Inter-element similarity of a set U is the average of the similarities between each pair of elements in the set (3) .
We use column similarity as the similarity measure between t must verify two thresholds:  X  The global auto-similarity of the group must reach the auto-similarity threshold X mentation, we set this threshold to 0.9.  X  The column similarity between every pair of elements from the group must reach the pairwise-similarity ilarity, contain some dissimilar elements. In our current implementation, we set this threshold to 0.8. Fig. 6 shows the result of applying the clustering algorithm to our example page from Fig. 3 .
Generating the candidate record divisions . Our method to generate the candidate record divisions is as follows.

Fig. 6 ).
The data region may contain, either at the beginning or at the end, some subtrees that are not really part of the data but auxiliary information. For instance, these sub-trees may contain information about the num-pre-process the string from the beginning and from the end removing tokens until we find the first cluster identifier that appears more than once in the sequence. In some cases, this pre-processing is not enough ically be removed from the output in the stage of extracting the attributes from the data records, which is described in Section 5 .

Once the pre-processing step is finished, we proceed to generate the candidate record divisions. By Property
From our page model, we know records are encoded consistently. Therefore, the string will tend to be formed is based on the following heuristic observations:  X  In many sources, records are visually delimited in an unambiguous manner to improve clarity for the human user. This delimiter is present before or after every record. usually mandatory fields which appear in every record (e.g. in our example source the delimiter may be the fragment corresponding to the title field, which appears in every record).

Based on the former observations, we will generate the following candidate record lists: cluster c i and another assuming every record ends with cluster c record divisions obtained for our example of Fig. 3 .  X  In addition, we will add a candidate record division considering each record is formed by exactly one subtree.

This reduces the number of candidate divisions from 2 n 1 one. 4.3. Choosing the best candidate record list will choose the candidate list showing the highest auto-similarity.

As we have already stated in previous sections, each candidate list is composed of a list of records. Each record is a sequence of consecutive sibling subtrees.

Then, given a candidate list composed of the records h r 1 of each pair to the average is weighted by the length of the compared registers. See Eq. (4) . similarity. 5. Extracting the attributes of the data records
In this section, we describe our techniques for extracting the values of the attributes of the data records identified in the previous section.
 between two strings matches the characters in one string with the characters in the other one, in such a way that the edit-distance between the two strings is minimized. There may be more than one optimal alignment between two strings. In that case, we choose any of them.

For instance, Fig. 8 shows an excerpt of the alignment between the strings representing the records in our example. As can be seen in the figure, each aligned text token roughly corresponds with an attribute of the record. Notice that to obtain the actual value for an attribute we may need to remove common prefixes/suf-values and will not appear in the output.
 mal multiple string alignment algorithms have a complexity of O( n mation algorithm. Several methods have been proposed in the literature for this task [26,13] . We use a (although they use tree alignment instead of string alignment). The algorithm works as follows: 1. The longest string is chosen as the  X  X  X aster string X  X , m . 2. Initially, S , the set of  X  X  X till not aligned strings X  X  contains all the strings but m . 3. For every s 2 S : a. Align s with m . b. If there is only one optimal alignment between s and m : 4. Repeat step 3 until S is empty or the master string m does not change.
 to extend it. Fig. 9 shows an example of this step. The alignment of the master record and the new record produces one optimal alignment where one char of the new record ( X  X  X ) is matched with a null position in the alignment.
 sequence of characters that are not aligned with any other record, then those characters are removed. 6. Experiments
This section describes the empirical evaluation of the proposed techniques with real web pages. During the development of the techniques, we used a set of 20 pages from 20 different web sources. The pilot tests per-formed with these pages were used to adjust the algorithm and to choose suitable values for the thresholds used by the clustering process (see Section 4.2 ). These pages were not used in the experimental tests.
For the experimental tests, we chose 200 new websites in different application domains (bookshops, music shops, patent information, publicly financed R&amp;D projects, movies information, etc.). We performed one that the collection includes pages having a very variable number of results (some queries return only 2 X 3 online. 1
While collecting the pages for our experiments, we found three data sources where our page creation model
In those three sources, the assumption does not hold and, therefore, our system would fail. We did not con-sider those sources in our experiments. In the related work section, we will further discuss this issue.
We performed the following tests with the collection:  X  We measured the validity of the heuristic assumed by the Property 1 defined in Section 2.3 .  X  We measured the effectiveness of the extraction process at each stage, including recall and precision measures.  X  We measured the execution times of the techniques.  X  We compared the effectiveness of our approach with respect to RoadRunner [11] , one of the most signif-icant previous proposals for automatic web data extraction.

The following sub-sections detail the results of each group of tests. 6.1. Validity of Property 1 derived from our page creation model). Nevertheless, Property 1 has a heuristic component which needs to be examined pages. 6.2. Effectiveness of the data extraction techniques
We tested the automatic data extraction techniques on the collection, and we measured the results at three stages of the process:  X  After choosing the data region containing the dominant list of data records.  X  After choosing the best candidate record division.  X  After extracting the structured data contained in the page.
 data region or the first records of the list, discarding the rest of the page.
 Table 1 shows the results obtained in the empirical evaluation.

In the second stage, we classify the results in two categories. auxiliary information (e.g. information about the number of results, web forms to refine the query or con-cess in stage 3 removes those irrelevant parts because they cannot be aligned with the remaining records.
Therefore, we consider these cases as correct. different records may be concatenated as one or one record may appear segmented into two. them. The main reason for the failures at this stage is that, in a few sources, the auto-similarity measure between the candidates, the auto-similarity measure ranks another one higher. This happens because, in these sources, some data records are quite dissimilar to each other. For instance, in one case where we have two consecutive data records that are much shorter than the rest, the system chooses a candidate division that rectly identified.

Regarding this stage, we also experimented with the pairwise similarity threshold (defined in Section 4.2 ), the previously chosen value of 0.8
In stage 3, we use the standard metrics recall and precision . Recall is computed as the ratio between the
Precision is computed as the ratio between the number of correct records extracted and the total number of records extracted by the system.

These are the most important metrics in what refers to web data extraction applications because they mea-are very high, reaching respectively to 98.29% and 97.93%. Most of the failures come from the errors propa-gated from the previous stage. 6.3. Execution times
We have also measured the execution time of the proposed techniques. The two most costly stages of the process are:  X  Generating the similarity matrix described in the Section 4.2 . This involves computing the similarity between each pair of first-level subtrees in the data region.  X  Computing the auto-similarities of the candidate record divisions to choose the best one (as described in
Section 4.3 ). This involves computing the similarity between each pair of records in every candidate record division.

Fortunately, both stages can be greatly optimized by caching similarity measures: generated from the first-level subtrees. Obviously, only the comparisons between distinct subtrees need to typically be many identical subtrees.  X  A similar step can be performed when computing the auto-similarities of the candidate record divisions.
Due to the regular structure of the data, there will typically be many identical candidate records across the candidate divisions.

As a consequence, the proposed techniques run very efficiently. In an average workstation PC (Intel Pen-tium Centrino Core Duo 2 GHz, 1 GB RAM), the average execution time for each page in the above exper-imental set was of 786 milliseconds (including HTML parsing and DOM tree generation). The process ran in sub-second time for 87% of the pages in the collection. 6.4. Comparison with RoadRunner
We also compared the effectiveness of the proposed techniques with respect to RoadRunner [11] . As far as we know, RoadRunner is the only automatic web data extraction system available for download.
Compared to our system, RoadRunner performs neither the region location stage nor the record division stage. Its function is comparable to the stage in our approach which extracts the individual attributes from each data record.

RoadRunner requires as input multiple pages. Typically, each one of these pages contains data from only one data record. For instance, a typical RoadRunner execution could receive as input a set of  X  X ook detail X  pages from an online bookshop, and would output the books data.

Therefore, to generate the input for RoadRunner, we split each page of the collection, generating a new the same template which can be fed as input to RoadRunner.
 the ones achieved by the techniques proposed in this paper over the same collection. 7. Related work
Wrapper generation techniques for web data extraction have been an active research field for years. Many approaches have been proposed such as specialized languages for programming wrappers [30,14,19] , inductive supervised graphical tools which hide the complexity of wrapper programming languages [5,27] . [21] provides a brief survey of some of the main approaches.

All the wrapper generation approaches require some kind of human intervention to create and configure the wrapper previously to the data extraction task. When the sources are not known in advance, such as in focused crawling applications, this approach is not feasible.

Several works have addressed the problem of performing web data extraction tasks without requiring human input. In [6] , a method is introduced to automatically find the region on a page containing the list of responses to a query. Nevertheless, this method does not address the extraction of the attributes of the structured records contained in the page.

A first attempt which considers the whole problem is IEPAD [9] which uses the Patricia tree [13] and string alignment techniques to search for repetitive patterns in the HTML tag string of a page. The method used by output is required. This is an important inconvenience with respect to our approach.
RoadRunner [11] receives as input multiple pages conforming to the same template and uses them to induce a union-free regular expression (UFRE) which can be used to extract the data from the pages conforming to the input schema. Another inconvenience with respect to our approach is that it requires receiving as input multiple pages conforming to the same template while our method only requires one. As it was previously mentioned, the page creation model we described in Section 2.3 was first introduced in
ExAlg [3] . As well as RoadRunner, ExAlg receives as input multiple pages conforming to the same template and uses them to induce the template and derive a set of data extraction rules. ExAlg makes some assumptions about web pages which, according to the own experiments of the authors, do not hold in a significant number from the root in the DOM tree of the pages.
 it. The method is based on locating the information redundancies between list pages and detail pages and not exist and, in addition, it requires of multiple pages to work.

Ref. [34] presents DEPTA, a method that uses the visual layout of information in the page and tree edit-it. As well as in our method, DEPTA requires as input one single page containing a list of structured data records. They also use the observation that, in the DOM tree of a page, each record in a list is composed of a set of consecutive sibling subtrees. Nevertheless, they make two additional assumptions: (1) that exactly the same number of sub-trees must form all records, and (2) that the visual space between two data records in a list is bigger than the visual space between any two data values from the same record. It is relatively easy to find counter-examples of both assumptions in real web sources. For instance, neither of the two assumptions holds in our example page of Fig. 3 . In addition, the method used by DEPTA to detect data regions is more expensive than ours, since it involves a potentially high number of edit-distance computations.
 do not conform to our page creation model and, therefore, our current method is unable to deal with them.
Although DEPTA implicitly assumes a page creation model similar to the one we use, after detecting a list of before continuing the process. These heuristics could be adapted to work with our approach.
There are several research problems which are complementary to our work (and to the automatic web data extraction techniques in general). Several works [32,4] have addressed the problem of how to automatically obtain attribute names for the extracted data records. Our prototype implementation labels the attributes using techniques similar to the ones proposed in [32] .

Another related problem is how to automatically obtain the pages which constitute the input to the auto-matic data extraction algorithm. Several works [24,28,2,18,10] have addressed the problem of how to automat-ically interpret and fill in web query forms to obtain the response pages. [12] has addressed the problem of examining a web site to automatically collect pages following the same template. All these works are comple-mentary to our work. 8. Conclusions
In this paper, we have presented a new method to automatically detecting a list of structured records in a web page and extracting the data fields that constitute them.

We use a page creation model which captures the main characteristics of semi-structured web pages and allows us to derive the set of properties and heuristics our techniques rely on.
Our method requires only one page containing a list of data records as input. The method begins by finding the data region containing the dominant list. Then, it performs a clustering process to limit the number of candidate record divisions in the data region and chooses the one having higher auto-similarity according the attribute values of each data record.

With respect to previous works, our method can deal with pages that do not verify the assumptions required by other previous approaches. We have also validated our method in a high number of real websites, obtaining very good effectiveness.
 Acknowledgements This research was partially supported by the Spanish Ministry of Education and Science under Project
TSI2005-07730 and the Spanish Ministry of Industry, Tourism and Commerce under Project FIT-350200-2006-78. Alberto Pan X  X  work was partially supported by the  X  X amo  X  n y Cajal X  programme of the Spanish Min-istry of Education and Science.

References
