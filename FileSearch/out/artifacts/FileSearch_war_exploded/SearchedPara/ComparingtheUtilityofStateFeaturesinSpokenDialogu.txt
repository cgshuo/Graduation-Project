 A host of issues confront spok en dialogue system designers, such as choosing the best system action to perform given any user state, and also selecting the right features to best represent the user state. While recent work has focused on using Reinforcement Learning (RL) to address the first issue (such as (W alk er, 2000), (Henderson et al., 2005), (W illiams et al., 2005a)), there has been very little empirical work on the issue of feature selection in prior RL ap-proaches to dialogue systems. In this paper , we use a corpus of dialogues of humans interacting with a spok en dialogue tutoring system to sho w the com-parati ve utility of adding the three features of con-cept repetition , frustr ation level , and student perfor -mance . These features are not just unique to the tu-toring domain but are important to dialogue systems in general. Our empirical results sho w that these fea-tures all lead to changes in what action the system should tak e, with concept repetition and frustration having the lar gest effects.

This paper extends our pre vious work (Tetreault and Litman, 2006) which first presented a method-ology for exploring whether adding more comple x features to a representation of student state will ben-eficially alter tutor actions with respect to feedback. Here we present an empirical method of comparing the effects of each feature while also generalizing our findings to a dif ferent action choice of what type of follo w-up question should a tutor ask the student (as opposed to what type of feedback should the tu-tor give). In comple x domains such as tutoring, test-ing dif ferent policies with real or simulated students can be time consuming and costly so it is important to properly choose the best features before testing, which this work allo ws us to do. This in turn aids our long-term goal of impro ving a spok en dialogue system that can effecti vely adapt to a student to max-imize their learning. We follo w past lines of research (such as (Le vin and Pieraccini, 1997) and (Singh et al., 1999)) for de-scribing a dialogue as a trajectory within a Mark ov Decision Process (MDP) (Sutton and Barto, 1998). A MDP has four main components: 1: states , 2: actions , 3: a policy , which specifies what is the best action to tak e in a state, and 4: a rewar d func-tion which specifies the worth of the entire pro-cess. Dialogue management is easily described us-ing a MDP because one can consider the actions as actions made by the system, the state as the dialogue conte xt (which can be vie wed as a vector of features, such as ASR confidence or dialogue act), and a re-ward which for man y dialogue systems tends to be task completion success or dialogue length.
Another adv antage of using MDP X  s to model a di-alogue space, besides the fact that the primary MDP parameters easily map to dialogue parameters, is the notion of delayed reward. In a MDP , since rewards are often not given until the final states, dynamic programming is used to propagate the rewards back to the internal states to weight the value of each state (called the V-value), as well as to develop an optimal polic y for each state of the MDP . This propaga-tion of reward is done using the policy iter ation al-gorithm (Sutton and Barto, 1998) which iterati vely updates the V-value and best action for each state based on the values of its neighboring states.
The V-value of each state is important for our pur -poses not only because it describes the relati ve worth of a state within the MDP , but as more data is added when building the MDP , the V-values should stabi-lize, and thus the policies stabilize as well. Since, in this paper , we are comparing policies in a fix ed data set it is important to sho w that the policies are indeed reliable, and not fluctuating.

For this study , we used the MDP infrastructure de-signed in our pre vious work which allo ws the user to easily set state, action, and reward parameters. It then performs polic y iteration to generate a polic y and V-values for each state. In the follo wing sec-tions, we discuss our corpus, methodology , and re-sults. For our study , we used an annotated corpus of 20 human-computer spok en dialogue tutoring ses-sions (for our work we use the ITSPOKE system (Litman and Silliman, 2004) which uses the text-based Why2-A TLAS dialogue tutoring system as its  X  X ack-end X  (VanLehn et al., 2002)). The content
Table 1: Potential Student State Features in MDP of the system, and all possible dialogue paths, were authored by physics experts. Each session consists of an interaction with one student over 5 dif ferent colle ge-le vel physics problems, for a total of 100 di-alogues. Before each session, the student is ask ed to read physics material for 30 minutes and then tak e a pretest based on that material. Each problem begins with the student writing out a short essay response to the question posed by the computer tutor . The fully-automated system assesses the essay for poten-tial flaws in the reasoning and then starts a dialogue with the student, asking questions to help the stu-dent understand the confused concepts. The tutor X  s response and next question is based only on the cor -rectness of the student X  s last answer . Informally , the dialogue follo ws a question-answer format. Once the student has successfully completed the dialogue section, he is ask ed to correct the initial essay . Each of the dialogues tak es on average 20 minutes and 60 turns. Finally , the student is given a posttest simi-lar to the pretest, from which we can calculate their normalized learning gain: .
 Prior to our study , the corpus was annotated for Tutor Mo ves, which can be vie wed as Dialogue Acts (Forbes-Rile y et al., 2005) 1 and consisted of Tutor Feedback, Question and State Acts. In this corpus, a turn can consist of multiple utterances and thus can be labeled with multiple mo ves. For example, a tutor can give positi ve feedback and then ask a question in the same turn. What type of question to ask will be the action choice addressed in this paper .
As for features to include in the student state, we annotated five features as sho wn in Table 1. Two emotion related features, certainty and frustration, were annotated manually prior to this study (Forbes-Rile y and Litman, 2005) 2 . Certainty describes how confident a student seemed to be in his answer , while frustration describes how frustrated the stu-dent seemed to be when he responded. We include three other automatically extracted features for the Student state: (1) Correctness: whether the student was correct or not; (2) Percent Correct: percentage of correctly answered questions so far for the cur -rent problem; (3) Concept Repetition: whether the system is forced to cover a concept again which re-flects an area of dif ficulty for the student. The goal of this study is to quantify the utility of adding a feature to a baseline state space. We use the follo wing four step process: (1) establish an action set and reward function to be used as con-stants throughout the test since the state space is the one MDP parameter that will be changed during the tests; (2) establish a baseline state and polic y, and (3) add a new feature to that state and test if adding the feature results in polic y changes. Ev ery time we create a new state, we mak e sure that the gen-erated V-values con verge. Finally , (4), we evaluate the effects of adding a new feature by using three metrics: (1) number of polic y changes (dif fs), (2) % polic y change, and (3) Expected Cumulati ve Re-ward. These three metrics are discussed in more de-tail in Section 5.2. In this section we focus on the first three steps of the methodology . 4.1 Establishing Actions and Rewards We use questions as our system action in our MDP . The action size is 4 (tutor can ask a simple answer question (SA Q), a comple x answer question (CA Q), or a combination of the two (Mix), or not ask a question (NoQ)). Examples from our corpus can be seen in Table 2. We selected this as the action because what type of question a tutor should ask is of great interest to the Intelligent Tutoring Systems community , and it generalizes to dialogue systems since asking users questions of varying comple xity can elicit dif ferent responses.

For the dialogue reward function we did a me-dian split on the 20 students based on their normal-ized learning gain, which is a standard evaluation metric in the Intelligent Tutoring Systems commu-nity . So 10 students and their respecti ve 5 dialogues were assigned a positi ve reward of 100 (high learn-ers), and the other 10 students and their respecti ve 5 dialogues were assigned a negati ve reward of -100 (lo w learners). The final student turns in each di-alogue were mark ed as either a positi ve final state (for a high learner) or a negati ve final state (for a low learner). The final states allo w us to propagate the reward back to the internal states. Since no action is tak en from the final states, their V-values remain the same throughout polic y iteration. 4.2 Establishing a Baseline State and Policy Currently , our tutoring system X  s response to a stu-dent depends only on whether or not the student an-swered the last question correctly , so we use correct-ness as the sole feature in our baseline dialogue state. A student can either be correct, partially correct, or incorrect. Since partially correct responses occur in-frequently compared to the other two, we reduced the state size to two by combining Incorrect and Par-tially Correct into one state (IPC) and keeping Cor -rect (C).

With the actions, reward function, and baseline state all established, we use our MDP tool to gener -ate a polic y for both states (see Table 3). The second column sho ws the states, the third, the polic y deter -mined by our MDP toolkit (i.e. the optimal action to tak e in that state with respect to the final reward) and finally how man y times the state occurs in our data (state size). So if a student is correct, the best action is to give something other than a question immedi-ately , such as feedback. If the student is incorrect, the best polic y is to ask a combination of short and comple x answer questions.
The next step in our experiment is to test whether the policies generated are indeed reliable. Normally , the best way to verify a polic y is to conduct exper -iments and see if the new polic y leads to a higher reward for new dialogues. In our conte xt, this would entail running more subjects with the augmented di-alogue manager , which could tak e months. So, in-stead we check if the polices and values for each state are indeed con verging as we add data to our MDP model. The intuition here is that if both of those parameters were varying between a corpus of 19 students versus one of 20 students, then we can X  t assume that our polic y is stable, and hence is not re-liable.

To test this out, we made 20 random orderings of our students to pre vent any one ordering from giving a false con vergence. Each ordering was then passed to our MDP infrastructure such that we started with a corpus of just the first student of the ordering and then determined a MDP polic y for that cut, then in-crementally added one student at a time until we had added all 20 students. So at the end, 20 random or-derings with 20 cuts each pro vides 400 MDP trials. Finally , we average each cut across the 20 random orderings. The first graph in Figure 1 sho ws a plot of the average V-values against a cut. The state with the plusses is the positi ve final state, and the one at the bottom is the negati ve final state. Ho we ver, we are most concerned with how the non-final states con-verge. The plot sho ws that the V-values are fairly stable after a few initial cuts, and we also verified that the policies remained stable over the 20 students as well (see our prior work (Tetreault and Litman, 2006) for details of this method). Thus we can be sure that our baseline polic y is indeed reliable for our corpus. In this section, we investigate whether adding more information to our student state will lead to inter -esting polic y changes. First, we add certainty to our baseline of correctness because prior work (such as (Bhatt et al., 2004), (Liscombe et al., 2005) and (Forbes-Rile y and Litman, 2005)) has sho wn the im-portance of considering certainty in tutoring sys-tems. We then compare this new baseline X  s pol-icy (henceforth Baseline 2) with the policies gener -ated when frustration, concept repetition, and per -cent correctness are included.

We X  X l first discuss the new baseline state. There are three types of certainty: certain (cer), uncertain (unc), and neutral (neu). Adding these to our state representation increases state size from 2 to 6. The new polic y is sho wn in Table 4. The second and third columns sho w the original baseline states and their policies. The next column sho ws the new pol-icy when splitting the original state into the three new states based on certainty (with the policies that dif fer from the baseline sho wn in bold). The final column sho ws the size of each new state. So the first row indicates that if the student is correct and certain, one should give a combination of a comple x and short answer question; if the student is correct and neutral, just ask a SA Q; and else if the student is correct and uncertain, give a Mix. The overall trend of adding the certainty feature is that if the student exhibits some emotion (either the y are certain or un-certain), the best response is Mix, but for neutral do something else.
We assume that if a feature is important to include in a state representation it should change the poli-cies of the old states. For example, if certainty did not impact how well students learned (as deemed by the MDP) then the policies for certainty , uncertainty , and neutral would be the same as the original polic y for Correct (C) or Incorrect (IPC). Ho we ver, the fig-ures sho w otherwise. When certainty is added to the state, only two new states (incorrect while being cer -tain or uncertain) retain the old polic y of having the tutor give a mix of SA Q and CA Q. The right graph in Figure 1 sho ws that for Baseline 2, V-values tend to con verge around 10 cuts.

Ne xt, we add Concept Repetition, Frustration, and Percent Correct features indi vidually to Base-line 2. For each of the three features we repeated the reliability check of plotting the V-value con-vergence and found that the graphs sho wed con ver-gence around 15 students. 5.1 Featur e Addition Results Policies for the three new features are sho wn in Ta-ble 5 with the policies that dif fer from Baseline 2 X  X  sho wn in bold. The numbers in parentheses refer to the size of the new state (so for the first +Concept state, there are 487 instances in the data of a student being correct, certain after hearing a new concept).
Concept Repetition Featur e As sho wn in col-umn 4, the main trend of incorporating concept rep-etition usually is to give a comple x answer question after a concept has been repeated, and especially if the student is correct when addressing a question about the repeated concept. This is intuiti ve be-cause one would expect that if a concept has been repeated, it signals that the student did not grasp the concept initially and a clarification dialogue was ini-tiated to help the student learn the concept. Once the student answers the repeated concept correctly , it signals that the student understands the concept and that the tutor can once again ask more dif ficult ques-tions to challenge the student. Given the amount of dif ferences in the new polic y and the original polic y (10 out of 12 possible), including concept repetition as a state feature has a significant impact on the pol-icy generated.

Frustration Featur e Our results sho w that adding frustration changes the policies the most when the student is frustrated, but when the student isn X  t frustrated (neutral) the polic y stays the same as the baseline with the exception of when the stu-dent is Correct and Certain (state 1), and Incorrect and Uncertain (state 6). It should be noted that for states 2 through 6, that frustration occurs very in-frequently so the policies generated (CA Q) may not have enough data to be totally reliable. Ho we ver in state 1, the polic y when the student is confident and correct but also frustrated is to simply give a hint or some other form of feedback. In short, adding the frustration feature results in a change in 8 out of 12 policies.

Percent Corr ectness Featur e Finally , the last column, sho ws the new polic y generated for incor -porating a simple model of current student perfor -mance within the dialog. The main trend is to give a Mix of SA Q and CA Q X  X . Since the original polic y was to give a lot of Mix X  s in the first place, adding this feature does not result in a lar ge polic y change, only 4 dif ferences. 5.2 Featur e Comparison To compare the utility of each of the features, we use three metrics: (1) Dif f X  X  (2) % Polic y Change, and (3) Expected Cumulati ve Re ward. # of Dif f X  X  are the number of states whose polic y dif fers from the baseline polic y, The second column of Table 6 summarizes the amount of Dif f X  X  for each new fea-ture compared to Baseline 2. Concept Repetition has the lar gest number of dif ferences: 10, follo wed by Frustration, and then Percent Correctness. Ho we ver, counting the number of dif ferences does not com-pletely describe the effect of the feature on the pol-icy. For example, it is possible that a certain feature may impact the polic y for several states that occur infrequently , resulting in a lot of dif ferences but the overall impact may actually be lower than a certain feature that only impacts one state, since that state occurs a majority of the time in the data. So we weight each dif ference by the number of times that state-action sequence actually occurs in the data and then divide by the total number of state-action se-quences. This weighting, % Polic y Change (or % P.C.), allo ws us to more accurately depict the impact of adding the new feature. The third columns sho ws the weighted figures of % Polic y Change. As an additional confirmation of the ranking, we use Ex-pected Cumulati ve Re ward (E.C.R.). One issue with % Polic y Change is that it is possible that frequently occurring states have very low V-values so the ex-pected utility from starting the dialogue could poten-tially be lower than a state feature with low % Polic y Change. E.C.R. is calculated by normalizing the V-value of each state by the number of times it occurs as a start state in a dialogue and then summing over all states. The upshot of both metrics is the ranking of the three features remains the same with Concept Repetition effecting the greatest change in what a tutoring system should do; Percent Correctness has the least effect.
 We also added a random feature to Baseline 2 with one of two values (0 and 1) to serv e as a base-line for the # of Dif f X  X . In a MDP with a lar ge enough corpus to explore, a random variable would not alter the polic y, howe ver with a smaller corpus it is possible for such a variable to alter policies. We found that by testing a random feature 40 times and averaging the dif fs from each test, resulted in an average dif f of 5.1. This means that Percent Cor -rectness effects a smaller amount of change than this random baseline and thus is fairly useless as a feature to add since the random feature is probably capturing some aspect of the data that is more use-ful. Ho we ver, the Concept Repetition and Frustra-tion cause more change in the policies than the ran-dom feature baseline so one can vie w them as fairly useful still.

As a final test, we investigated the utility of each feature by using a dif ferent tutor action -whether or not the tutor should give simple feedback (Sim-Feed), or a comple x feedback response(ComFeed), or a combination of the two (Mix) (Tetreault and Lit-man, 2006). The policies and distrib utions for all features from this pre vious work are sho wn in Ta-bles 7 and 8. Basically , we wanted to see if the rela-tive rankings of the three features remained the same for a dif ferent action set and whether dif ferent action sets evoked dif ferent changes in polic y. The result is that although the amount of polic y change is much lower than when using Questions as the tutor action, the relati ve ordering of the features is still about the same with Concept Repetition still having the great-est impact on the polic y. Interestingly , while Frus-tration and Percent Correctness have lower dif fs, % polic y changes, and E.C.R. then their question coun-terparts (which indicates that those features are less important when considering what type of feedback to give, as opposed to what type of question to give), the E.C.R. for concept repetition with feedback is actually higher than the question case. RL has been applied to impro ve dialogue systems in past work but very few approaches have look ed at which features are important to include in the dia-logue state. Paek and Chick ering X  s (2005) work on testing the Mark ov Assumption for Dialogue Sys-tems sho wed how the state space can be learned from data along with the polic y. One result is that a state space can be constrained by only using features that are rele vant to recei ving a reward. Henderson et al. X  X  (2005) work focused on learning the best pol-icy by using a combination of reinforcement and su-pervised learning techniques but also addressed state features by using linear function approximation to deal with lar ge state spaces. Singh et al. (1999) and Frampton et al. (2005) both sho wed the ef-fect of adding one discourse feature to the student state (dialogue length and user X  s last dialogue act, respecti vely) whereas in our work we compare the worth of multiple features. Although Williams et al. X  X  (2005b) work did not focus on choosing the best state features, the y did sho w that in a noisy environment, Partially-Observ able MDP X  s could be used to build a better model of what state the user is in, over traditional MDP and hand-crafted meth-ods. One major dif ference between all this related work and ours is that usually the work is focused on how to best deal with ASR errors. Although this is also important in the tutoring domain, our work is novel because it focuses on more semantically-oriented questions. In this paper we sho wed that incorporating more in-formation into a representation of the student state has an impact on what actions the tutor should tak e. Specifically , we proposed three metrics to determine the relati ve weight of the three features. Our empirical results indicate that Concept Repeti-tion and Frustration are the most compelling since adding them to the baseline resulted in major pol-icy changes. Percent Correctness had a negligible effect since it resulted in only minute changes to the baseline polic y. In addition, we also sho wed that the relati ve ranking of these features generalizes across dif ferent action sets.

While these features may appear unique to tutor -ing systems the y also have analogs in other dialogue systems as well. Repeating a concept (whether it be a physics term or tra vel information) is important be-cause it is an implicit signal that there might be some confusion and a dif ferent action is needed when the concept is repeated. Frustration can come from dif-ficulty of questions or from the more frequent prob-lem for any dialogue system, speech recognition er-rors, so the manner in dealing with it will always be important. Percent Correctness can be vie wed as a specific instance of tracking user performance such as if the y are continuously answering ques-tions properly or are confused by what the system requests.

With respect to future work, we are annotating more human-computer dialogue data and will triple the size of our test corpus allo wing us to create more complicated states since more states will have been explored, and test out more comple x tutor actions, such as when to give Hints and Restatements. In the short term, we are investigating whether other metrics such as entrop y and confidence bounds can better indicate the usefulness of a feature. Finally , it should be noted that the certainty and frustration feature scores are based on a manual annotation. We are investigating how well an automated certainty and frustration detection algorithm will impact the % Polic y Change. Pre vious work such as (Liscombe et al., 2005) has sho wn that certainty can be auto-matically generated with accurac y as high as 79% in comparable human-human dialogues. In our corpus, we achie ve an accurac y of 60% in automatically pre-dicting certainty . We would lik e to thank the ITSPOKE and Pitt NLP groups, Pam Jordan, James Henderson, and the three anon ymous revie wers for their comments. Sup-port for this research was pro vided by NSF grants #0325054 and #0328431.

