
This paper presents an approach to automatically optimiz-ing the retrieval quality of search engines using clickthrough data. Intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. While previous ap-proaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. This makes them difficult and ex-pensive to apply. The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking. 
Such clickthrough data is available in abundance and can be recorded at very low cost. Taking a Support Vector Ma-chine (SVM) approach, this paper presents a method for learning retrieval functions. From a theoretical perspective, this method is shown to be well-founded in a risk minimiza-tion framework. Furthermore, it is shown to be feasible even for large sets of queries and features. The theoreti-cal results are verified in a controlled experiment. It shows that the method can effectively adapt the retrieval function of a meta-search engine to a particular group of users, out-performing Google in terms of retrieval quality after only a couple of hundred training examples. 
Which WWW page(s) does a user actually want to re-trieve when he types some keywords into a search engine? There are typically thousands of pages that contain these words, but the user is interested in a much smaller subset. One could simply ask the user for feedback. If we knew the set of pages actually relevant to the user's query, we could use this as training data for optimizing (and even personal-izing) the retrieval function. 
Unfortunately, experience shows that users are only rarely willing to give explicit feedback. However, this paper argues that sufficient information is already hidden in the logfiles of WWW search engines. Since major search engines re-pemaission and/or a fee. SIGKDD 02 Edmonton, Alberta, Canada 
Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. clicked on. of the search engine. In particular, compared to explicit user feedback, it does not add any overhead for the user. The query q and the returned ranking r can easily be recorded whenever the resulting ranking is displayed to the user. For recording the clicks, a simple proxy system can keep a logfile. 
For the experiments in this paper, the following system was used. 
Each query is assigned a unique ID which is stored in the query-log along with the query words and the presented ranking. The links on the results-page presented to the user do not lead directly to the suggested document, but point to a proxy server. These links encode the query-ID and the 
URL of the suggested document. When the user clicks on the link, the proxy-server records the URL and the query-
ID in the click-log. The proxy then uses the HTTP Loca-tion command to forward the user to the target URL. This process can be made transparent to the user and does not influence system performance. 
This shows that clickthrough data can be recorded easily and at little cost. Let's now address the key question of how it can be analyzed in a principled and efficient way. 
There are strong dependencies between the three parts of (q, r, c). The presented ranking r depends on the query q as determined by the retrieval function implemented in the search engine. Furthermore, the set c of clicked-on links depends on both the query q and the presented ranking r. to q [16]. While this dependency is desirable and interesting for analysis, the dependency of the clicks on the presented ranking r muddies the water. In particular, a user is less likely to click on a link low in the ranking, independent of how relevant it is. In the extreme, the probability that the user clicks on a link at rank 10.000 is virtually zero even if it is the document most relevant to the query. No user will scroll down the ranking far enough to observe this link. Therefore, in order to get interpretable and meaningful and analyzed? not possible to infer that the links 1, 3, and 7 are relevant on an absolute scale, it is much more plausible to infer that link 3 is more relevant than link 2 with probability higher than random. Assuming that the user scanned the ranking from top to bottom, he must have observed link 2 before click-ing on 3, making a decision to not click on it. Given that the abstracts presented with the links are sufficiently infor-mative, this gives some indication of the user's preferences. 
Similarly, it is possible to infer that link 7 is more relevant than links 2, 4, 5, and 6. This means that clickthrough data does not convey absolute relevance judgments, but partial relative relevance judgments for the links the user browsed through. A search engine ranking the returned links accord-ing to their relevance to q should have ranked links 3 ahead of 2, and link 7 ahead of 2, 4, 5, and 6. Denoting the ranking preferred by the user with r*, we get partial (and potentially noisy) information of the form 
This 'strategy for extracting preference feedback is summa-rized in the following algorithm. FROM CLICKTHROUGH) ing the ranks of the clicked-on links, extract a preference example for all pairs 1 &lt; j &lt; i, with i  X  C and j ~ C. dard machine learning algorithms. The following derives a new learning algorithm, so that this "weak" type of relative feedback can be used as training data. as follows. For a query q and a document collection D = {dl, ...,din}, the optimal retrieval system should return a ranking r  X  that orders the documents in D according to their relevance to the query. While the query is often represented as merely a set of keywords, more abstractly it can also incorporate information about the user and the state of the information search. dering r*. Instead, an operational retrieval function f is evaluated by how closely its ordering rf(q) approximates the optimum. Formally, both r* and rf(q) are binary relations over D x D that fulfill the properties of a weak ordering, i.e. r* C D x D and rf(q) C D  X  D being asymmetric, and negatively transitive. If a document di is ranked higher than dj for an ordering r, i.e. d~ &lt;r dj, then (di,dj)  X  r, oth-erwise (di, di)  X  r. If not stated otherwise, let's assume for simplicity that r* and rf(q) are both strict orderings. This is maximal. Note that (6) is (proportional to) a risk func-tional [25] with -~ as the loss function. While the goal of learning is now defined, the question remains whether it is possible to design learning methods that optimize (6)? 
Most work on machine learning in information retrieval does not consider the formulation of above, but simplifies the task to a binary classification problem with the two classes "relevant" and "non-relevant". Such a simplification has several drawbacks. For example, due to a strong ma-jority of "non-relevant" documents, a learner will typically achieve the maximum predictive classification accuracy, if it always responds "non-relevant', independent of where the relevant documents are ranked. But even more importantly, 
Section 2.2 showed that such absolute relevance judgments cannot be extracted from clickthrough data, so that they are simply not available. Therefore, the following algorithm directly addresses (6), taking an empirical risk minimiza-tion approach [25]. Given an independently and identically distributed training sample S of size n containing queries q with their target rankings r* the learner  X  will select a ranking function f from a family of ranking functions F that maximizes the empirical 7-on the training sample. Note that this setup is analogous to e.g. classification by minimizing training error, just that the target is not a class label, but a binary ordering relation. 
Is it possible to design an algorithm and a family of rank-ing functions F so that (a) finding the function f  X  F maxi-mizing (8) is efficient, and (b) that this function generalizes well beyond the training data. Consider the class of linear ranking functions (di, dj)  X  fw(q) ~ ~&amp;(q, di) &gt; ~&amp;(q, dj). (9) is a weight vector that is adjusted by learning. ,Iv(q, d) is a mapping onto features that describe the match between query q and document d like in the description-oriented re-trieval approach of Fuhr et al. [10][11]. Such features are, for example, the number of words that query and document share, the number of words they share inside certain HTML tags (e.g. TITLE, H1, H2, ...), or the page-rank of d [22] (see also Section 5.2). Figure 2 illustrates how the weight vector ag determines the ordering of four points in a two-dimensional example. For any weight vector ~, the points are ordered by their projection onto ~g (or, equivalently, by their signed distance to a hyperplane with normal vector ~). 
This means that for ~g~ the points are ordered (1, 2, 3, 4), while ~2 implies the ordering (2, 3, 1, 4). 
Instead of maximizing (8) directly, it is equivalent to min-imize the number Q of discordant pairs in Equation (2). For the class of linear ranking functions (9), this is equivalent to finding the weight vector so that the maximum number of links the user clicked on are marked in bold. it is proven and empirically verified in [16] that the conclu-sions drawn from this method lead to the same result as an evaluation with explicit manual relevance judgments for large s. 
This experiment verifies that the Ranking SVM can in-deed learn regularities using partial feedback from click-through data. To generate a first training set, I used the 
Striver search engine for all of my own queries during Oc-tober, 2001. Striver displayed the results of Google and 
MSNSearch using the combination method from the previ-ous section. All clickthrough triplets were recorded. This resulted in 112 queries with a non-empty set of clicks. This data provides the basis for the following offline experiment. 
To learn a retrieval function using the Ranking SVM, it is necessary to design a suitable feature mapping ~(q,d) describing the match between a query q and a document d. 
The following features are used in the experiment. However, this set of features is likely to be far from optimal. While the attributes reflect some of my intuition about what could be important for learning a good ranking, I included only those features that were easy to implement. Furthermore, 
I did not do any feature selection or similar tuning, so that an appropriate design of features promises much room for improvement. The implemented features are the following: the ranking returned by the respective retrieval function. 
Figure 4: Generalization error of the Ranking SVM depending on the size of the training set. The error bars show one standard error. according to Algorithm 1 described in Section 2.2. In ad-dition, 50 constraints were added for each clicked-on docu-ment indicating that it should be ranked higher than a ran-dom other document in the candidate set V. While the lat-ter constraints are not based on user feedback, they should hold for the optimal ranking in most cases. These addi-tional constraints help stabilize the learning result and keep the learned ranking function somewhat close to the original rankings. 
SVM. To produce the graph, the full data set is split ran-domly into a training and a test set. The x-axis shows the number of training queries. The y-axis shows the percent-the test set. Each point is an average over 10 (5-20 training queries) / 20 (40-80 training queries) different test/training splits. When training the Ranking SVM, no kernel was used selected from C E {0.001, 0.003,0.005, 0.01} by minimizing that the Ranking SVM can learn regularities in the prefer-
Table 3: Features with largest and smallest weights as learned from the training data in the online ex-periment. 
The previous result shows that the learned function im-proves retrieval. But what does the learned function look like? Is it reasonable and intuitive? Since the Ranking SVM learns a linear function, one can analyze the function by studying the learned weights. Table 3 displays the weights of some features, in particular, those with the highest abso-lute weights. Roughly speaking, a high positive (negative) weight indicates that documents with these features should be higher (lower) in the ranking. The weights in Table 3 are reasonable for this group of users. Since many queries were for scientific material, it ap-pears natural that URLs from the domain "citeseer" (and the alias "nec") received positive weight. The most influ-ential weights are for the cosine match between query and abstract, whether the URL is in the top 10 from Google, and for the cosine match between query and the words in the URL. A document receives large negative weights, if it is not ranked top 1 by any search engine, if it not in the top 10 of any search engine (note that the second implies the first), and if the URL is long. All these weights are reasonable and make sense intuitively. The experimental results show that the Ranking SVM can successfully learn an improved retrieval function from click-through data. Without any explicit feedback or manual pa-rameter tuning, it has automatically adapted to the partic-ular preferences of a group of ~ 20 users. This improvement is not only a verification that the Ranking SVM can learn using partial ranking feedback, but also an argument for per-sonalizing retrieval functions. Unlike conventional search engines that have to "fit" their retrieval function to large and therefore heterogeneous groups of users due to the cost This paper presented an approach to mining logfiles of 
WWW search engines with the goal of improving their re-trieval performance automatically. The key insight is that such clickthrough data can provide training data in the form of relative preferences. Based on a new formulation of the learning problem in information retrieval, this paper de-rives an algorithm for learning a ranking function. Taking a 
Support Vector approach, the resulting training problem is tractable even for large numbers of queries and large num-bers of features. Experimental results show that the algo-rithm performs well in practice, successfully adapting the retrieval function of a meta-search engine to the preferences of a group of users. 
This paper opens a series of question regarding the use machine learning in search engines. What is a good size of a user group and how can such groups be determined? 
Clearly, there is a trade-off between the amount of train-ing data (ie. large group) and maximum homogeneity (ie. single user). Is it possible to use clustering algorithms to find homogenous groups of users? Furthermore, can click-through data also be used to adapt a search engine not to a group of users, but to the properties of a particular doc-ument collection? In particular, the factory-settings of any gines with learning capabilities would enable them to opti-mize (and maintain) their performance automatically after 
However, the algorithm is not limited to meta-search en-
Open questions regarding the algorithm itself concern its [15] T. Joachims. Learning to Classify Text Using Support [16] T. Joachims. Unbiased evaluation of retrieval quality [17] T. Joachims, D. Freitag, and T. Mitchell. [18] J. Kemeny and L. Snell. Mathematical Models in the [19] M. Kendall. Rank Correlation Methods. Hafner, 1955. [20] H. Lieberman. Letizia: An agent that assists Web [21] A. Mood, F. Graybill, and D. Boes. Introduction to [22] L. Page and S. Brin. Pagerank, an eigenvector based [23] G. Salton and C. Buckley. Term weighting approaches [24] C. Silverstein, M. Henzinger, H. Marais, and [25] V. Vapnik. Statistical Learning Theory. Wiley, [26] Y. Yao. Measuring retrieval effectiveness based on 
THEOREM 1. Let r~t be the ranking placing all relevant documents ahead of all non-relevant documents and let rsus be the learned ranking. If Q is the number of discordant pairs between r~et and r~us , then the average precison is at least 
AvgPrec(r,u,,r~,) &gt; ~ Q+ ( R+i ~x/i if there are R relevant documents. 
PROOF. If pl,...,pa are the ranks of the relevant docu-ments in rsu, sorted in increasing order, then Average Pre-cision can be computed as 
What is the minimum value of AvgPrec(rsvs,rr~l), given that the number of discordant pairs is fixed. It is easy to 
