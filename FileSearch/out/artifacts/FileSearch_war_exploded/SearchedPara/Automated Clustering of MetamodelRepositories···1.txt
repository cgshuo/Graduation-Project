
Francesco Basciani 1 , Juri Di Rocco 1 , Davide Di Ruscio The increasing adoption of Model-Driven Engineering (MDE) [ 1 ] in business organizations led to the need for gathering artifacts in model repositories [ 2 ]. Several model repositories (see [ 3  X  6 ] just to mention a few) have been introduced in the past decade. Among them metamodel zoos (as for instance the Ecore Zoo ) hold metamodels, which are typically categorized to improve search and/or browse operations. However, locating relevant information in a vast repository is intrinsically difficult, because it requires domain experts to manually annotate all metamodels in the repository with accurate metadata [ 7 ]: an activity that is time consuming and prone to errors and omissions. In fact, acquiring knowledge about a software artifact is a challenging task: it is estimated that up to 60 % of software maintenance is spent on comprehension [ 8 ].
 ous applications in reverse engineering and software maintenance. It promotes the automated categorization of software artifacts (like functions, classes, or files) into high-level structures based on their similarity distance [ 10 ]. Software clustering is applied, for instance, to detect misplaced software artifacts [ 11 ]. gorization of metamodels, we propose the application of clustering techniques for metamodel repositories able to automatically organize metamodels into clusters. Mutually similar metamodels are grouped together depending on a proximity measure, whose definition can be given according to specific search and brows-ing requirements. The approach is based on agglomerative hierarchical cluster-ing [ 12 ] (see Sect. 3 ) and explores well-known proximity measures as well as metamodel-specific ones, each providing different browsing characteristics. The method has been already implemented in the MDEForge repository [ 13 ]andit is available online 2 . Furthermore, an evaluation has been conducted by consid-ering different similarity measures, each characterized by specific accuracy and performance indexes. The evaluation permitted also to identify the application domains represented by the metamodels stored in an arbitrary repository. teristics of modeling repositories. Section 3 provides an overview about clustering techniques. In Sect. 4 , we present our approach and show how it is able to auto-matically categorize metamodel repositories. In the next section, the approach is evaluated by applying it to a corpus of about 300 metamodels. A discussion about the results is provided in Sect. 6 . In Sect. 7 related work is discussed and finally, in Sect. 8 , we conclude and outline future plans. Modelers can interact with model repositories for several purposes. For instance, as in the case of source code repositories, users can be interested in acquiring knowledge from already developed modeling artifacts that might represent pre-cious know-how to be conveyed to new modelers. Users that have clear require-ments about the desired modeling artifacts, can use model repositories with the aim of finding the modeling artifacts that best fit the user needs. Whatever the modeler intents, repositories should provide users with a dedicated support for properly organizing the contained artifacts, and to effectively search and retrieve them. Especially in the case of large repositories, the potential benefits related to the availability of reusable artifacts might be missed if they cannot be suitably discovered. By considering the ways artifacts are organized, and thus the pro-vided searching and browsing functionalities, it is possible to identify different kinds of repositories as discussed below. Flat Repositories: They are not structured in the sense that contained artifacts are not categorized, and searching and browsing functionalities are not available. With this kind of repositories we refer to any publicly available collection of artifacts that are stored in a file-system like manner. For instance, it X  X  plenty of GitHub repositories containing modeling artifacts. Modelers are obliged to download and inspect such models in order to gain some insights about them and to check if they might satisfy their requirements.
 Manually Classified Repositories Without Searching and Browsing Functionali-ties: Artifacts are manually categorized, and searching and browsing function-alities are not available. For instance, the Ecore Zoo repository consists of an organized list of metamodels. For each metamodel, different attributes are given including a short description about it, and a list of domains where the metamodel can be applied. To search for a specific metamodel in the repository, modelers have to rely on the in-page search facility of the used Web browser. Thus, when searching for specific words, modelers have to manually check where they occur (e.g., in the description, or in the domain attributes). For instance, in the case of the Ecore Zoo, different metamodels are available for supporting the manage-ment of projects in organizations, e.g., how to assign tasks and resources. Such metamodels can be identified by searching the string  X  X roject management X  in the Ecore Zoo web page. In addition, also the Maven 3 metamodel is found, as the searched string occurs in its description, although Maven is a building tool thus it refers to a different domain.
 Manually Classified Repositories with Searching and Browsing Functionalities: Artifacts are manually categorized, search-ing and browsing functionalities are avail-able. Similarly to repositories like Ecore Zoo, artifacts are manually classified according to a predefined set of labels. The available searching facility permits to give search strings as input and match them against the description field of the avail-able artifacts (e.g., see [ 2 ]).
 Most of the potential benefits of such repositories remain unexploited espe-cially when hundreds or even thousands of modeling artifacts have to be man-aged. In particular, by focusing on the provided functionalities for organizing, browsing, and searching metamodels , all the available repositories are affected by the following challenges: C1. they do not provide the means to automatically produce structured overviews of the contained metamodels, which are typically shown as merely lists of stored elements, and which are consequently difficult to browse.
Organizations like the one shown in Fig. 1 would permit to have an overview of the metamodels stored in the considered repository, e.g., with respect to the covered application domains; C2. all the available repositories do not provide mechanisms to automatically lenges by focusing on the management of metamodels stored in publicly avail-able repositories. We propose the application of an unsupervised metamodel clustering mechanism, which permits to automatically organize unstructured metamodel repositories, and provides the users with overviews of the available metamodels. Thus manually annotating metamodels is no longer necessary , since the provided approach is able to automatically catogorize metamodels according to their content and structure. Clustering is one of the techniques for doing data mining and can be defined as the process of organizing objects into groups of similar objects [ 14 ]. A cluster is therefore a collection of objects, which are similar between them and are dis-similar to the objects belonging to other clusters [ 12 ]. Clustering is also known as unsupervised classification since we do not know a priori neither the number of classes nor their attributes. Clustering techniques are applied in a wide spec-trum of areas including biology to classify plants and animals according to their properties, and geology to classify observed earthquake epicenters and thus to identify dangerous zones. Clustering has found numerous applications to software as well [ 9 ], where it is used in reverse engineering and software maintenance for categorizing software artifacts in many respects. Over the years several clustering methods have been developed like the hierarchical and partitional ones [ 12 ]. In the remainder of the section we focus on the hierarchical clustering technique since it underpins the approach proposed in the next section.
 By cutting the dendrogram at a desired distance threshold , a clustering of the data objects into disjoint groups is obtained. For instance, by considering the example in Fig. 2 if we choose as distance threshold the value 0.25 we obtain three or divisive . The former starts with one object clusters and recursively merges two or more of the most similar clusters. The latter starts with a single cluster consisting of all the elements in the source data set and recursively split the clusters according to some criterion for obtaining at the end of the process a partition of one object clusters (named singleton clusters hereafter) [ 14 ]. Algorithm 1. Basic agglomerative hierarchical clustering algorithm All the existing clustering methods share the fact that they can be applied when it is possible to specify a proximity (or distance) measure that per-mits to assess if elements to be clustered are mutu-ally similar or dissimilar. The basic idea is that the similarity level of two elements is inversely propor-tional to their distance . The definition of the prox-imity measure plays a key role in any clustering method and it depends on many factors including the considered application domain, available data, and goals. Once the proximity measure is defined, it is possible to produce a proximity matrix, which is an n by n matrix (where n is the number of objects to be clustered) containing all the pairwise similarities or dissimilarities between the considered objects. For instance, by considering a simple data set O consisting of the objects o corresponding proximity matrix based on a sample similarity function s : O  X  [0 , 1] can be given like the one shown in Fig. 3 a.
 A proximity matrix induces the definition of a weighted graph (like the one shown in Fig. 3 b) where nodes are the objects being clustered, and weighted edges represent the similarities between the connected objects. The availability of proximity graphs permits to see the clustering problem from a graph point of view as that of breaking the graph into connected components, one for each cluster [ 15 ]. By focusing on the agglomerative hierarchical clustering , a basic algorithm would use the proximity graph as shown in Algorithm 1 borrowed from [ 16 ]: cluster proximity is necessary to execute the second step of the algorithm. To this end, three different definitions of cluster distance can be used, namely sin-gle link , complete link ,and group average [ 16 ]. With the single (complete) link definition, the proximity of two clusters is defined as the minimum (maximum) distance between any two objects in the considered clusters. Whereas, with the group average technique, the proximity of two clusters is the average pairwise proximities of all pairs of objects from the considered clusters.
 ously outlined can be employed in the context of metamodel repositories. In order to deal with the issues discussed in Sect. 2 in this section we show how to apply clustering techniques in the domain of model-driven engineering. The proposed approach is able to automatically organize metamodels stored in a given repository by analysing their content and not their metadata that might be erroneous or misplaced. An overview of the approach is given in Sect. 4.1 , its implementation is presented in Sect. 4.2 . 4.1 Overview The main functionalities of the proposed clustering approach are shown in Fig. 4 . It is important to remark that the figure shows only the functionalities strictly related to the automated classification of metamodels. For an overview about the typically provided functionalities of existing model repositories, interested readers can refer to [ 17 ]. As shown in Fig. 4 , two different user roles are involved in the proposed clustering approach namely the Repository Maintainer and the Repository User discussed in the following.
 Repository Maintainer: The application of the whole metamodel clustering approach is performed by the maintainer of the repository who can have access to the functionalities described below.
 Apply Metamodel Clustering: It represents the key functionality of the proposed clustering approach. It consists of calculating the proximity matrix (as shown in Sect. 3 ) representing the similarities of all the metamodels available in the repository, and then applying the clustering algorithm in Algorithm 1 . Manage Singleton Clusters: When a new metamodel is being added to the repository, it may happen that accord-ing to the used proximity mea-sure it does not fit in any of the existing clusters and conse-quently it induces the creation of a singleton cluster, i.e., a cluster consisting of only one element. The repos-itory maintainer can periodically consider the available singleton clusters and verify if they have been created, e.g., because the used proximity measure has to be refined.
 Refine the Proximity Measure: The proximity measure plays a key role in the whole clustering approach, and consequently its definition is an iterative process, aiming at increasing the accuracy of the automatically obtained metamodel clus-ters. The refinement process relies on the availability of reference data, which are typically obtained by manual activities. Such data must be approximated by the automated clustering procedure as discussed in the next section. Repository User: Similarly to what happens in the case of open source soft-ware, the availability of public model repositories can give place to multitudes of users and developers that are willing to share their modeling artifacts. In this respect, by focusing on the metamodel clustering aspects, the proposed approach provides the users with the functionalities discussed below.
 Add New Metamodel: In contrast with existing metamodel archives, users that add new metamodels in the repository can omit the specification of correspond-ing metadata. Even in such cases, the provided approach is able to automatically classify the new metamodels. In fact the appropriate clusters are identified by considering the content of the metamodels without the need for additional user input. However, as previously mentioned, it might happen that newly added metamodels do not fit in any of the existing clusters. Then, the repository main-tainer takes care of such situations by means of the functionality Manage Sin-gleton Clusters shown in Fig. 4 .
 Visualize Metamodel Clusters: The approach produces overviews of the auto-matically produced metamodel clusters. Thus in addition to the list of avail-able metamodels, the system is able to generate graphical representations of the available metamodel clusters, and gives also the means to navigate them and to retrieve detailed information about their content if requested by the user. 4.2 Supporting Tool Atop of such core services, extensions can be developed to add new functionali-ties. For instance, in [ 18 ] we propose a service to automatically compose model transformations according to user requirements. We have also developed exten-sions to calculate several metrics on stored artifacts, and to support the under-standing of metamodel and transformation characteristics [ 19 , 20 ]. are shown in Fig. 5 in bold and that we have developed to support the proposed clustering approach. Concerning the other services of MDEForge the reader can refer to [ 13 , 17 ].
 Proximity Calculator: It plays a key role in the proposed clustering approach since it is responsible of calculating the mutual similarities between all the meta-models and thus create a corresponding proximity matrix like the one shown in Fig. 3 a. Such calculations rely on the definition of a given similarity measure. As discussed in Sect. 3 identifying the appropriate similarity measure is a difficult task that might depend on the available data set, on the considered application domain, on the goal of the analysis being performed, etc. [ 12 ]. Consequently, from an architectural point of view, the proximity calculator has been designed in terms of an interface consisting of a method calculateSimilarity(Metamodel mm provided. So far we have developed different similarity measures already avail-able in the system even though we plan to experiment and provide additional ones. In particular, several similarity measures have been proposed in litera-ture [ 14 ]. Among those typically applied to text documents we have considered the cosine similarity [ 14 ] and the Dice X  X  coefficient [ 21 ] with the aim of relating the similarity of two metamodels on the terms used therein and consequently on the corresponding application domains. In particular:  X  Cosine similarity: given two documents represented as term vectors, the sim-ilarity of the input documents corresponds to the correlation between the derived vectors. Such a correlation is calculated as the cosine of the angle between vectors [ 14 ]. In order to apply such a similarity measure on meta-models, for each of them we derive the corresponding string by borrowing the serialization mechanisms available in EMF-REST 4 ;  X  Dice X  X  coefficient: it is defined as twice the number of common bigrams (i.e., pairs of adjacent letters in the string) in the compared strings divided by the total number of terms in both strings [ 21 ]. Similarly to the application of the cosine similarity, to calculate the Dice X  X  coefficient between two metamodels, we first derive a string serialization of them.
 We have developed also two additional similarity functions specifically conceived for modeling artifacts. Both of them rely on the matching model calculated by means of EMFCompare 5 :  X  Match-based similarity: it is defined as the total number of matched elements identified by EMFCompare divided by the total number of elements contained in the analysed couple of metamodels;  X  Containment-based similarity: the previous index does not perform well when one of the input metamodels is contained in the other one. As an example we can consider the full specification of UML and the UML Class Diagrams. In such cases the match-based similarity value would be very low since the total number of matched elements would be much lesser than the total number of elements contained in the two metamodels. In order to deal with such cases, the containment-based similarity is defined as the total number of matched elements divided by the lesser of the total elements in the two input meta-models.
 The application on a concrete data set of the measures used by the proximity calculator is in-depth discussed in the next section.
 Clustering Creator: By using the proximity calculator previously discussed, it creates clusters of metamodels by applying the agglomerative hierarchical clus-tering algorithm shown in Algorithm 1 . As to the cluster proximity calculation, which is performed during each iteration of the algorithm, it is possible to specify the distance to be used, i.e., single link, complete link, and group average (see Sect. 3 ).
 Cluster Visualizer: It creates graphical and tabular representations of the cal-culated metamodel clusters. The user can explore the available metamodels by specifying the similarity measure to be applied, and the threshold value used to filter the identified metamodels pairs and show only those that have a similar-ity value greater than the given threshold. The left hand side of Fig. 6 shows the cluster visualizer at work. In particular, the shown connected graphs rep-resent the identified clusters and the thickness of the edges is proportional to the proximity value of the connected metamodels represented as nodes in the graph. For each cluster, the system permits to retrieve additional information as shown in the upper right-hand side of Fig. 6 . In particular, given a cluster all the contained metamodels are listed together with additional information like the most representative metamodel, i.e., the one most connected with the other ones in the cluster. Additionally, metamodels can be downloaded or even viewed by means of an integrated tree-based editor.
 interested developers can extend or even substitute the provided services, for instance, to refine the proximity measures if the proposed ones do not properly meet their requirements. In this section we discuss the application of the clustering approach on a concrete data set consisting of 295 metamodels retrieved from the Ecore Zoo. The main goal of this section is to discuss the ability of the clustering method (i) to automate the creation of metamodel groups according to different similarity measures, and (ii) to provide the user with organized and interactive views of the metamodel repository. We have applied the clustering technique by using the four similarity functions discussed in the previous section and by specifying different thresholds.
 idation technique based on external criteria [ 22 ]: the final goal is to validate the results of the employed clustering technique comparing them with the manually pre-specified clusters.
 Experimental Setting and Dataset. We have downloaded all the meta-models from the Ecore Zoo and we have manually grouped them with respect to their content and the domain descriptions when available. Thus, we have individually analysed the metamodels and incrementally defined the metamodel clusters. At the end of this manual process, that took about two working days of one senior researcher with a consolidated expertise in metamodeling and model-driven engineering, we identified 90 groups, including 43 singleton represents a specific application domain, e.g., database management, project management, building tools, and model transformation languages. Subsequently, we have applied on the downloaded metamodels the proposed clustering app-roach by using the four similarity functions previously discussed and by shifting the thresholds from the value of 0.1 to 1.0 with 0.05 steps. Clearly, a too low threshold corresponds to consider the repository population almost undistin-guished, whilst a too high threshold returns too many clusters with too few elements. The manually identified clusters and those automatically created are analysed as discussed later on this section. Evaluation Metrics. The manually defined clusters are used to generate the incidence matrix I defined as follows: where i, j =1 ,...,n ,and mm i and mm j are metamodels in the data set manu-ally processed. Moreover, for each selected threshold and similarity function we binarized the calculated similarity matrix. The basic idea of binarization con-sists in the introduction of binary values associated to the similarity values. Each binary value is 1 if the numerical value to which it is associated has a value above a certain threshold , 0 otherwise. More formally, a similarity matrix in B ( S ) as follow: where i, j =1 ,...,n ,and mm i and mm j are metamodels in the corpus, and Sim is the considered similarity measure.
 idated the metamodel clustering approach by measuring the similarity between the incidence matrix I , and the binarized similarity matrix the adopted similarity measure and threshold. Intuitively, the more similar the incidence matrix I and B ( S ) are the better is the considered clustering tech-nique. To this end we have applied the MATLAB corr2 function the correlation coefficient in the range of -1.00 (perfect negative correlation) and +1.00 (perfect positive correlation) between two matrices or vectors of the same size. A correlation with value 0 indicates that the two considered elements are not correlated.
 Data Analysis. The outcomes of the performed experiments are reported in Table 1 . In particular, for each similarity function and threshold (denoted with Thrsd in the table) the following data are shown:  X  #clst : the number of identified clusters;  X  Avg. clst size : the average number of metamodels in each cluster;  X  Max clst size : the number of metamodels in the bigger cluster;  X  #non-sing clst : the number of clusters consisting of more than one metamodel;  X  Corr. with IM : it indicates the correlation index between the automatically calculated clusters and the one manually identified.
 by interacting with the cluster visualizer component discussed in the previous section, which permits to select the similarity measure and the preferred similar-ity threshold. The graphical representation of the retrieved clusters is updated in real time accordingly.
 Table 1 (a X  X ), the match-based similarity with 0.25 as threshold value optimally approximates the metamodel clusters that we have manually defined. However, according to Table 1 e the match-based similarity is also one of the most time consuming measure like the containment-based similarity one. The text-based similarity measures take less time than that required by the structural-based sim-ilarities. This result depends on the matching function of EMFCompare, that is exponential and depends on the number of all the elements contained in all the possible metamodels couples. This is not the case of text-based similarities that instead are based on the textual distance between the vectors encoding as strings the metamodels being analysed. The main strengths of the approach proposed in this paper are related to the advantages of classifying metamodels in repositories in an automated way instead of using manual techniques that are strongly correlated to the maintainer back-ground. This permits to relieve maintainers and contributors of the responsibility of annotating metamodels, which is beneficial for the accuracy of core function-alities such as searching and browsing. Nevertheless, the approach as proposed in this paper can be enhanced in different directions as discussed below. Similarity Measure : The generation of metamodel clusters strongly depends on the adopted similarity measure. Depending on the purpose of the desired clus-tering, a corresponding measure has to be properly selected from existing ones in literature or even defined from scratch. As we have presented in Sect. 4 , besides using two similarity measures that are commonly applied to text documents, we have developed two additional similarity measures (namely match-based and containment-based) that are model specific (in fact, they rely on the match mod-els calculated by EMFCompare). However, even if we restrict our focus on meta-models, it is unlikely to define a similarity measure that meets the requirement of any modeler. This justifies the decision of conceiving an extensible proximity calculator that can be enriched by adding the implementation of further simi-larity measures that the user can then select and play with from the front-end of the application.
 Performance : The complexity of hierarchical agglomerative algorithms is O ( n 2 log n )[ 12 ] with n the number of elements in the considered data set. Consequently, for very large data sets the adoption of alternative algorithms is suggested. The hierarchical agglomerative clustering technique used in this work is only one of the many possible ones. Moreover, the developed supporting tool is agnostic of the clustering technique. It is important to remark that the developed system nightly updates all the proximity matrices in order to consider in the clustering calculation also newly added metamodels. Thus users do not experiment performance issues when playing with the cluster visualizer since all the required data are already pre-calculated.
 Cluster Characterizations : In the current version of the approach each meta-model is assigned to exactly one cluster only. However, in some cases a given metamodel might belong to different clusters simultaneously. Moreover, by exploiting the content of the considered metamodels and the corresponding descriptions, when available, it can be possible to create cluster labels auto-matically. Supporting such characterizations represents a relevant improvement that we intent to investigate in the future.
 Experimentation and Evaluation : The implementation of the approach has been applied by considering the metamodels available in the Ecore Zoo. However, to better assess the validity of the approach and of the optimal parameters presented in the previous section, and to obtain more extensive feedback, it is necessary to consider an extended data set consisting of additional metamodels that can be retrieved from further repositories like ReMoDD. Clustering techniques have been used in several applications including software and data comprehension, data migration, and reverse engineering.
 neering principles for software comprehension. In particular, authors start by extracting data from source code for the input data matrix construction. For the code extraction, they consider the paragraph as the smallest atomic unit and their cluster analysis is based on the hypothesis that record fields existing in the same paragraphs can be grouped. For the data matrix the chosen dis-tance of similarity for the cluster identification is the Euclidean distance. In [ 24 ] authors propose a software automatic categorization system called MUDABlue, based on Latent Semantic Analysis (LSA), which is a method to extract and rep-resent the usage of words in texts by means of statistical computations. Similarly to our approach, [ 23 , 24 ] propose techniques able to automatically categorize set of similar objects. Whereas they focus on software comprehension, our approach specifically focuses on metamodels stored in repositories.
 migration. The approach uses semantic clustering to facilitate the translation of extended entity relationship schema into complex objects schema. They start from an Extended Entity Relationships (EER) schema to create a set of clustered schemata such that each clustered schema corresponds to a level of abstraction and grouping of the initial schema. By iteratively shrinking portions of EER diagram into complex entities, the approach creates a schema of entities by adding a layer of abstraction. The user can select a level of clustering to show components at some degree of detail exactly like we do in our approach. diagrams, which are decomposed into clusters of model elements. Graph clus-tering techniques are employed by defining the node similarity in terms of node distance. Differently to our approach, in [ 26 ] authors apply clustering techniques to create sub-models whereas in our approach we categorize metamodels without splitting their contents.
 Petri nets, and Facets for the analysis and clustering of requirements diagrams. Intuitively, the approach is able to obtain the domain description in terms of the relations and dependencies of modeled services. Then the analysis and the clustering of requirements are automatically calculated accordingly. The work in [ 28 ] presents a semi-automatic technique for the construction of feature mod-els based on requirements clustering. This approach automates the activities of feature identification and reorganization using clustering techniques. Starting from an existing system, the main idea is that tight-related individual functional requirements are clustered into features, and functional features are organized into an application feature model, which is then merged into a domain feature model. Differently to our approach, [ 27 , 28 ] are specifically conceived for the management of functional requirements, and clustering techniques are employed at model level. In this paper, we studied the problem of the automated categorization of meta-model repositories. The proposed approach adopted an agglomerative hierarchi-cal clustering algorithm, which according to different similarity metrics (some of them specifically devised for metamodels) detects the application domains represented in an arbitrary repository. The effect of these similarity metrics is evaluated over a corpus of metamodels: while metamodel-specific similarities per-form better at the price of a high execution-time, generic text-based similarities still offer acceptable accuracy with much better execution-time.
 Future plans include a more systematic experimentation of the available sim-ilarity metrics to provide repository maintainers with tools, which can offer to the user a multi-dimensional browsing experience. In addition, we are interested in understanding how to use the variance between the incidence matrix and the experimental data as a feedback to improve similarity measures. We plan to experiment the application of clustering techniques also on other sets of mod-eling artifacts, like models and model transformations. In particular, we want to investigate to what extent it is possible to substitute model transformations that have source and/or target metamodels in common belonging to the same clusters. Positive results in such direction might be beneficial for enhancing the reuse of model transformations.

