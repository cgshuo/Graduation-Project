 come more and more difficult. Clustering has been long recognized as a powerful tool for the task. Text clustering or document clustering has become a hot topic for many ing length can contain correlated terms [3], and are high dimensional with respect to words, yet are sparse. 
Many successful text clustering techniques [4] have been proposed in the past, but they are designed for and tested on traditional long text corpus such as newswires and blogs. Long texts often span multiple topics and are high dimensional representation. methods must explicitly focus on modeling and accounting for these topics. Recently, poor results [11]. Wang et al. [16] proposed a frequent-term based parallel clustering algorithm specifically designed for short texts. 
However, many real datasets are not only composed of long, or short texts, but also texts of mixed length. 
Due to the existence of the short texts yet extremely sparse representations, cluster-IDF can be less than ideal as we would demonstrate in the experiments. On the other hand, if we use some short text clustering algorithms such as MAKM[9] for the mixed duction, which deteriorates the performance of clustering. 
In order to deal with clustering of mixed length texts, we present a LDA-based top-collections and thus achieve a balance state between sparseness and dimension. More specifically, the main contributions of this paper include:  X  clustering approach to deal with the mixed length texts in the previous work.  X  (ADLDA) , which can adjust the degree of mutual auxiliary based on the length of clustering.  X 
We have conducted experiments on public datasets. Experimental results show that clustering methods. 
The rest of the paper is organized as follows: Section 2 reviews related work. Sec-tion 3 presents the ADLDA model. Section 4 reports the experimental results. In Sec-tion 5, we conclude this paper. David el at. [2] did a survey on text clustering, they focused on recent advancements in text clustering separately: long and short texts. guous and disjoint blocks that represent passages, or subtopics. The Segmented Topic proposed Sequential LDA (LDSeq) model [7], an extension of STM that addresses the ments and provided a clustering strategy for texts modeled this way. 
Short text clustering has attracted growing interests in recent years with social net-works development. Because of its simplicity and efficiency, the K-means algorithm an improved K-means algorithm MAKM (MAFIA-based K-means) for short text clustering, in which the optimized initial centers and number k of clusters can be de-by using both corpus-based and knowledge-based measures when acquiring words similarity. Somnath et al.[15] proposed a method of improving the accuracy of clus-across domains by correlating the simultaneous training of two LDA models. 
Our topic model is similar to DLDA, both of them train two LDA models simulta-neously. However in our model long texts and short texts are mutual auxiliary on the model can adaptively determine hyper-parameter  X  instead setting it manually. To define the problem of length-aware text clustering, we first define some notations. Table 1 reports some main notations used throughout this paper. some latent auxiliary knowledge that can more effectively capture the semantic rela-unreasonable to assume that the topical stru cture of the two domains is mostly consis-the clustering accuracy of mixed length text. tion. Finally, we use hidden topics of corpus to do clustering. 3.1 Length-Aware Dual Latent Di richlet Allocation Model two types of topics when generating the text. This mechanism makes the model automat-ically to capture whether a text should be more related to the auxiliary collection. ADLDA is shown in Fig. 2. versus short text topics  X   X   X  with Beta prior  X   X  . 3.2 Parameters Estimation with Gibbs Sampling variables:  X   X   X , X , X , X , X , X  |  X  X  X  X , X , X  
The probability of a word  X   X , X   X  can be obtained by integrating out  X , X , X  : 
Finally, the likelihood of the whole data set is: Note that the inference and learning algorithms of the basic LDA model can be eas-ily applied to ADLDA model, because the ADLDA extends the basic model structure and change certain setting of the hyper-parameters. To estimate the latent parameters, Monte Carlo (MCMC) algorithm, we can get the following updating rules. 
For long text collection topics  X   X   X   X   X , ...,1  X   X  ,  X   X   X   X   X , X  X   X   X  X   X  |  X   X   X , X  X   X  X  X   X ,  X  X  X   X   X ,  X  X  X   X  X , X  X , X ,
For short text collection topics  X   X   X   X   X , ...,1  X   X  ,  X   X   X   X   X , X  X   X   X  X   X  |  X   X   X , X  X   X  X  X   X ,  X  X  X   X   X ,  X  X  X   X   X , X  X , X ,
For a topic  X  , its probability is calculated as:  X   X   X   X   X  X   X  |  X   X   X , X  X   X  X  X   X ,  X  X  X   X   X ,  X  X  X   X  X  X  X  X  X  X  X  X  X , X , X ,  X   X   X   X   X   X , X  X   X   X   X  X  |  X   X   X   X   X  and  X   X  X  represents that the i-th word is excluded from the computation. Finally, the proba-bility of a topic  X  can be obtained by  X  X  X  X  X  X  function. 3.3 Clustering Using Hidden Topics scale of each feature of a text  X  by  X   X  as: texts but lack the discriminative power. 
Then we apply the traditional clustering methods on the topic based representations achieves better results, as we demonstrate in the experiments. 4.1 Dataset selected by category. The dataset 1 contains the most of long texts, with the matching such as Twitter and Sina Microblog restrict the text length to be less than 140 charac-ters, which can be served as the reference of short text length bound. Table 3 shows the distribution of the datasets. 4.2 Comparison Result retrieval. 
We compared some standard clustering algorithms with our ADLDA:  X 
TFIDF-KM, we use K-means with TF-IDF representation as CLUTO [4] do.  X 
LDA, we learn a LDA from the whole text collection and directly cluster with the  X  learned.  X 
MAKM [9], is a text clustering algorithm for the dataset with the most of short texts.  X  superior performance. we don X  X  choose the DLDA as a baseline. We ran the TFIDF-KM, LDA, ADLDA and ADLDA-asymmetric 10 times on dataset 1, and ran the MAKM, LDA, ADLDA and ADLDA-asymmetric on dataset 2. The number of clustering is set to the actual num-ber of categories in datasets. The results are reported below. 
Fig.2. shows the comparisons of average F1 scores among the different methods on 
As expected, our method ADLDA outperformed all the other baseline methods on tion has the poorest performance, which demonstrates the benefit of using topic model ADLDA model beat the LDA algorithm, for the ADLDA is better than directly clus-tering. In the other hand, the ADLDA is slightly better than the ADLDA-asymmetric, which shows that mutual auxiliary has superior performance. the ADLDA outperforms state-of-the-art long and short text clustering approaches for mixed length texts. Acknowledgment. This work was supported by National Basic Research Program of China (973 Program) No.2011CB302302, Key S&amp;T Projects of Press and Publication Five-Year-Plan' of China under Grant No. 2012AA09A408, Tsinghua University Initiative Scientific Research Program. 
