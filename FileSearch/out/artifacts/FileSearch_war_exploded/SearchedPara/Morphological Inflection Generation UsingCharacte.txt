 Inflection is the word-formation mechanism to ex-press different grammatical categories such as tense, mood, voice, aspect, person, gender, number and case. Inflectional morphology is often realized by the concatenation of bound morphemes (prefixes and suffixes) to a root form or stem, but noncon-catenative processes such as ablaut and infixation are found in many languages as well. Table 1 shows the possible inflected forms of the German stem Kalb (calf) when it is used in different cases and numbers. The inflected forms are the result of both ablaut (e.g., a  X   X  a ) and suffixation (e.g., + ern ).

Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection gener-ation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Mod-eling inflection generation has also been used to im-prove language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications.

The traditional approach to modeling inflec-tion relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to lin-guistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the com-posed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes (e.g. affixation) or require careful feature engineer-ing.

In this paper, we present a model of inflection generation based on a neural network sequence to sequence transducer. The root form is represented as sequence of characters, and this is the input to an encoder-decoder architecture (Cho et al., 2014; Sutskever et al., 2014). The model transforms its in-put to a sequence of output characters representing the inflected form (  X  4). Our model makes no as-sumptions about morphological processes, and our features are simply the individual characters. The model is trained on pairs of root form and inflected forms obtained from inflection tables extracted from unlabeled data, by integrating a character language model trained on the vocabulary of the language.
Our experiments show that the model achieves better or comparable results to state-of-the-art meth-ods on the benchmark inflection generation tasks (  X  5). For example, our model is able to learn long-range relations between character sequences in the string aiding the inflection generation process re-quired by Finnish vowel harmony (  X  6), which helps it obtain the current best results in that language. Durrett and DeNero (2013) formulate the task of su-pervised inflection generation for a given root form, based on a large number of training inflection ta-bles extracted from Wiktionary. Every inflection ta-ble contains the inflected form of a given root word corresponding to different linguistic transformations (cf. Table 1). Figure 1 shows the inflection genera-tion framework. Since the release of the Wiktionary dataset, several different models have reported per-formance on this dataset. As we are also using this dataset, we will now review these models.

We denote the models of Durrett and DeNero (2013), Ahlberg et al. (2014), Ahlberg et al. (2015), and Nicolai et al. (2015), by DDN13, AFH14, AFH15, and NCK15 respectively. These models perform inflection generation as string transduction and largely consist of three major components: (1) Character alignment of word forms in a table; (2) Extraction of string transformation rules; (3) Appli-cation of rules to new root forms.

The first step is learning character alignments across inflected forms in a table. Figure 2 (a) shows alignment between three word forms of Kalb . Dif-ferent models use different heuristic algorithms for alignments such as edit distance, dynamic edit dis-tance (Eisner, 2002; Oncina and Sebban, 2006), and longest subsequence alignment (Bergroth et al., 2000). Aligning characters across word forms pro-vide spans of characters that have changed and spans that remain unchanged. These spans are used to ex-tract rules for inflection generation for different in-flection types as shown in Figure 2 (b) X (d).
By applying the extracted rules to new root forms, inflected words can be generated. DDN13 use a semi-Markov model (Sarawagi and Cohen, 2004) to predict what rules should be applied, using charac-ter n -grams ( n = 1 to 4 ) as features. AFH14 and AFH15 use substring features extracted from words to match an input word to a rule table. NCK15 use a semi-Markov model inspired by DDN13, but ad-ditionally use target n -grams and joint n -grams as features sequences while selecting the rules. Motivation for our model. Morphology often makes references to segmental features, like place or manner of articulation, or voicing status (Chom-sky and Halle, 1968). While these can be encoded as features in existing work, our approach treats seg-ments as vectors of features  X  X atively X . Our ap-proach represents every character as a bundle of con-tinuous features, instead of using discrete surface character sequence features. Also, our model uses features as part of the transduction rules themselves, whereas in existing work features are only used to rescore rule applications.

In existing work, the learner implicitly speci-fies the class of rules that can be learned, such as  X  X elete X  or  X  X oncatenate X . To deal with phe-nomenona like segment lengthening in English: run  X  running ; or reduplication in Hebrew: Kelev  X  Klavlav , Chatul  X  Chataltul ; (or consonant grada-tion in Finnish), where the affixes are induced from characters of the root form, one must engineer a new rule class, which leads to poorer estimates due to data sparsity. By modeling inflection generation as a task of generating a character sequence, one char-acter at a time, we do away with such problems. Here, we describe briefly the underlying framework of our inflection generation model, called the recur-rent neural network (RNN) encoder-decoder (Cho et al., 2014; Sutskever et al., 2014) which is used to transform an input sequence ~x to output sequence ~y . We represent an item by x , a sequence of items by ~x , vectors by x , matrices by X , and sequences of vectors by ~ x . 3.1 Formulation In the encoder-decoder framework, an encoder reads a variable length input sequence, a sequence of vec-tors ~ x =  X  x 1 ,  X  X  X  , x T  X  (corresponding to a sequence of input symbols ~x =  X  x 1 ,  X  X  X  ,x T  X  ) and generates a fixed-dimensional vector representation of the se-quence. x t  X  R l is an input vector of length l . The most common approach is to use an RNN such that: where h t  X  R n is a hidden state at time t , and f is generally a non-linear transformation, producing e := h T +1 as the input representation. The decoder is trained to predict the next output y t given the encoded input vector e and all the previously pre-dicted outputs  X  y 1 ,  X  X  X  y t  X  1  X  . In other words, the de-coder defines a probability over the output sequence ~y =  X  y 1 ,  X  X  X  ,y T 0  X  by decomposing the joint proba-bility into ordered conditionals: With a decoder RNN, we can first obtain the hidden layer at time t as: s t = g ( s t  X  1 , { e , y t  X  1 } ) and feed this into a softmax layer to obtain the conditional probability as: where, ~y &lt;t =  X  y 1 ,  X  X  X  ,y t  X  1  X  . In recent work, both f and g are generally LSTMs, a kind of RNN which we describe next. 3.2 Long Short-Term Memory (LSTM) In principle, RNNs allow retaining information from time steps in the distant past, but the nonlinear  X  X quashing X  functions applied in the calculation of each h t result in a decay of the error signal used in training with backpropagation. LSTMs are a vari-ant of RNNs designed to cope with this  X  X anish-ing gradient X  problem using an extra memory  X  X ell X  (Hochreiter and Schmidhuber, 1997; Graves, 2013). Past work explains the computation within an LSTM through the metaphors of deciding how much of the current input to pass into memory or forget. We refer interested readers to the original papers for details. We frame the problem of inflection generation as a sequence to sequence learning problem of charac-ter sequences. The standard encoder-decoder mod-els were designed for machine translation where the objective is to translate a sentence (sequence of words) from one language to a semantically equiv-alent sentence (sequence of words) in another lan-guage. We can easily port the encoder-decoder translation model for inflection generation. Our model predicts the sequence of characters in the in-flected string given the characters in the root word (input).

However, our problem differs from the above set-ting in two ways: (1) the input and output character sequences are mostly similar except for the inflec-tions; (2) the input and output character sequences have different semantics. Regarding the first differ-ence, taking the word play as an example, the in-flected forms corresponding to past tense and con-tinuous forms are played and playing . To better use this correspondence between the input and output sequence, we also feed the input sequence directly into the decoder: where, g is the decoder LSTM, and x t and y t are the input and output character vectors respectively. Be-cause the lengths of the input and output sequences are not equal, we feed an character in the decoder, indicating null input, once the input sequence runs out of characters. These character vectors are pa-rameters that are learned by our model, exactly as other character vectors.

Regarding the second difference, to provide the model the ability to learn the transformation of se-mantics from input to output, we apply an affine transformation on the encoded vector e : where, W trans , b trans are the transformation pa-rameters. Also, in the encoder we use a bi-directional LSTM (Graves et al., 2005) instead of a uni-directional LSTM, as it has been shown to capture the sequence information more effectively (Ling et al., 2015; Ballesteros et al., 2015; Bah-danau et al., 2015). Our resultant inflection gener-ation model is shown in Figure 3. 4.1 Supervised Learning The parameters of our model are the set of character vectors, the transformation parameters ( W trans , b trans ), and the parameters of the encoder and decoder LSTMs (  X  3.2). We use negative log-likelihood of the output character sequence as the loss function: We minimize the loss using stochastic updates with AdaDelta (Zeiler, 2012). This is our purely super-vised model for inflection generation and we evalu-ate it in two different settings as established by pre-vious work: Factored Model. In the first setting, we learn a separate model for each type of inflection indepen-dent of the other possible inflections. For example, in case of German nouns, we learn 8, and for Ger-man verbs, we learn 27 individual encoder-decoder inflection models (cf. Table 3). There is no param-eter sharing across these models. We call these fac-tored models of inflection generation.
 Joint Model. In the second setting, while learn-ing a model for an inflection type, we also use the information of how the lemma inflects across all other inflection types i.e., the inflection table of a root form is used to learn different inflection mod-els. We model this, by having the same encoder in the encoder-decoder model across all inflection representation of the input character sequence. Be-cause all inflection models take the same input but produce different outputs, we hypothesize that hav-ing the same encoder can lead to better estimates. 4.2 Semi-supervised Learning The model we described so far relies entirely on the availability of pairs of root form and inflected word form for learning to generate inflections. Although such supervised models can be used to obtain inflec-tion generation models (Durrett and DeNero, 2013; Ahlberg et al., 2015), it has been shown that unla-beled data can generally improve the performance of such systems (Ahlberg et al., 2014; Nicolai et al., 2015). The vocabulary of the words of a language encode information about what correct sequences of characters in a language look like. Thus, we learn a language model over the character sequences in a vocabulary extracted from a large unlabeled cor-pus. We use this language model to make predic-tions about the next character in the sequence given the previous characters, in following two settings. Output Reranking. In the first setting, we first train the inflection generation model using the su-pervised setting as described in  X  4.1. While mak-ing predictions for inflections, we use beam search to generate possible output character sequences and rerank them using the language model probability along with other easily extractable features as de-scribed in Table 2. We use pairwise ranking opti-mization (PRO) to learn the reranking model (Hop-kins and May, 2011). The reranker is trained on the beam output of dev set and evaluated on test set. Language Model Interpolation. In the second setting, we interpolate the probability of observing the next character according to the language model with the probability according to our inflection gen-Dataset root forms Infl.
 German Nouns (DE-N) 2764 8 German Verbs (DE-V) 2027 27 Spanish Verbs (ES-V) 4055 57 Finnish NN &amp; Adj. (FI-NA) 6400 28 Finnish Verbs (FI-V) 7249 53 Dutch Verbs (NL-V) 11200 9 French Verbs (FR-V) 6957 48 eration model. Thus, the loss function becomes:  X  log p ( ~y | ~x ) = where p LM ( y t | ~y &lt;t ) is the probability of observing the word y t given the history estimated according to a language model,  X   X  0 is the interpolation pa-rameter which is learned during training and Z is the normalization factor. This formulation lets us use any off-the-shelf pre-trained character language model easily (details in  X  5). 4.3 Ensembling Our loss functions (equ. 6 &amp; 7) formulated using a neural network architecture are non-convex in nature and are thus difficult to optimize. It has been shown that taking an ensemble of models which were ini-tialized differently and trained independently leads to improved performance (Hansen and Salamon, 1990; Collobert et al., 2011). Thus, for each model type used in this work, we report results obtained using an ensemble of models. So, while decoding we compute the probability of emitting a charac-ter as the product-of-experts of the individual mod-els in the ensemble: p ens ( y t | X  ) = 1 where, p i ( y t | X  ) is the probability according to i -th model and Z is the normalization factor. We now conduct experiments using the described models. Note that not all previously published mod-els present results on all settings, and thus we com-pare our results to them wherever appropriate. Hyperparameters. Across all models described in this paper, we use the following hyperparameters. In both the encoder and decoder models we use sin-gle layer LSTMs with the hidden vector of length 100 . The length of character vectors is the size of character vocabulary according to each dataset. The parameters are regularized with ` 2 , with the regular-ensembling are k = 5 . Models are trained for at most 30 epochs and the model with best result on development set is selected. 5.1 Data Durrett and DeNero (2013) published the Wik-tionary inflection dataset with training, development and test splits. The development and test sets con-tain 200 inflection tables each and the training sets consist of the remaining data. This dataset con-tains inflections for German, Finnish and Span-ish. This dataset was further augmented by (Nico-lai et al., 2015), by adding Dutch verbs extracted from CELEX lexical database (Baayen et al., 1995), French verbs from Verbsite, an online French con-jugation dictionary and Czech nouns and verbs from the Prague Dependnecy Treebank (Haji  X  c et al., 2001). As the dataset for Czech contains many in-complete tables, we do not use it for our experi-ments. These datasets come with pre-specified train-ing/dev/test splits, which we use. For each of these sets, the training data is restricted to 80% of the to-tal inflection tables, with 10% for development and 10% for testing. We list the size of these datasets in Table 3.

For semi-supervised experiments, we train a 5-gram character language model with Witten-Bell smoothing (Bell et al., 1990) using the SRILM toolkit (Stolcke, 2002). We train the character lan-guage models on the list of unique word types ex-tracted from the Wikipedia dump for each language after filtering out words with characters unseen in the inflection generation training dataset. We ob-tained around 2 million unique words for each lan-guage. 5.2 Results Supervised Models. The individual inflected form accuracy for the factored model (  X  4.1) is shown in Table 4. Across datasets, we obtain either com-parable or better results than NCK15 while obtain-ing on average an accuracy of 96 . 20 % which is higher than both DDN13 and NCK15. Our factored model performs better than DDN13 and NCK15 on datasets with large training set (ES-V, FI-V, FI-NA, NL-V, FR-V) as opposed to datasets with small training set (DE-N, DE-V). In the joint model set-ting (cf. Table 5), on average, we perform better than DDN13 and AFH14 but are behind AFH15 by 0 . 11 %. Our model improves in performance over our factored model for DE-N, DE-V, and ES-V, which are the three smallest training datasets. Thus, parameter sharing across different inflection types Semi-supervised Models. We now evaluate the utility of character language models in inflection generation, in two different settings as described ear-lier (  X  4.2). We use the factored model as our base model in the following experiments as it performed DE-V 97.87 97.90 96.79 97.11 DE-N 91.81 89.90 88.31 89.31 ES-V 99.58 99.90 99.78 99.94 FI-V 96.63 98.10 96.66 97.62 FI-NA 93.82 93.60 94.60 95.66 Avg. 95.93 95.88 95.42 95.93 NL-V  X  96.60 96.66 96.64 FR-V  X  99.20 98.81 98.94 Avg.  X  96.45 96.08 96.45 better than the joint model (cf. Table 4 &amp; 5). Our reranking model which uses the character language model along with other features (cf. Table 2) to se-lect the best answer from a beam of predictions, im-proves over almost all the datasets with respect to the supervised model and is equal on average to AFH14 and NCK15 semi-supervised models with 96 . 45% accuracy. We obtain the best reported results on ES-V and FI-NA datasets (99.94% and 95.66% re-spectively). However, our second semi-supervised model, the interpolation model, on average obtains 96 . 08% and is surprisingly worse than our super-vised model ( 96 . 20% ).
 Comparison to Other Architectures. Finally it is of interest how our proposed model compares to more traditional neural models. We compare our model against a standard encoder-decoder model, and an encoder-decoder model with attention, both trained on root form to inflected form character sequences. In a standard encoder-decoder model (Sutskever et al., 2014), the encoded input sequence vector is fed into the hidden layer of the decoder as input, and is not available at every time step in con-trast to our model, where we additionally feed in x t at every time step as in equ. 4. An attentional model computes a weighted average of the hidden layer of the input sequence, which is then used along with the decoder hidden layer to make a prediction (Bah-danau et al., 2015). These models also do not take the root form character sequence as inputs to the de-coder. We also evaluate the utility of having an en-coder which computes a representation of the input character sequence in a vector e by removing the en-coder from our model in Figure 3. The results in Ta-ble 7 show that we outperform the encoder-decoder model, and the model without an encoder substan-tially. Our model is slightly better than the atten-tional encoder-decoder model, and is simpler as it does not have the additional attention layer. Length of Inflected Forms. In Figure 4 we show how the prediction accuracy of an inflected form varies with respect to the length of the correct in-flected form.To get stable estimates, we bin the in-flected forms according to their length: &lt; 5 , [5 , 10) , [10 , 15) , and  X  15 . The accuracy for each bin is model and the best models of DDN13 and NCK15. Our model consistently shows improvement in per-formance as word length increases and is signifi-cantly better than DDN13 on words of length more than 20 and is approximately equal to NCK15. On words of length &lt; 5 , we perform worse than DDN13 but better than NCK15. On average, our model has the least error margin across bins of different word length as compared to both DDN13 and NCK15. Using LSTMs in our model helps us make better predictions for long sequences, since they have the ability to capture long-range dependencies.
 Finnish Vowel Harmony. Our model obtains the current best result on the Finnish noun and adjective dataset, this dataset has the longest inflected words, some of which are &gt; 30 characters long. Finnish ex-hibits vowel harmony , i.e, the occurrence of a vowel is controlled by other vowels in the word. Finnish vowels are divided into three groups: front (  X  a,  X  o, y), back (a, o, u), and neutral (e, i). If back vow-els are present in a stem, then the harmony is back (i.e, front vowels will be absent), else the harmony is front (i.e, back vowels will be absent). In compound words the suffix harmony is determined by the final stem in the compound. For example, our model cor-rectly inflects the word fasisti (fascist) to obtain fa-sisteissa and the compound t  X  arkkelyspitoinen (starch containing) to t  X  arkkelyspitoisissa . The ability of our model to learn such relations between these vowels helps capture vowel harmony. For FI-NA, our model obtains 99 . 87% for correctly predicting vowel har-mony, and NCK15 obtains 98 . 50% .We plot the char-acter vectors of these Finnish vowels (cf. Figure 5) using t-SNE projection (van der Maaten and Hin-ton, 2008) and observe that the vowels are correctly grouped with visible transition from the back to the front vowels. Similar to the encoder in our framework, Rastogi et al. (2016) extract sub-word features using a forward-backward LSTM from a word, and use them in a tra-ditional weighted FST to generate inflected forms. Neural encoder-decoder models of string transduc-tion have also been used for sub-word level transfor-mations like grapheme-to-phoneme conversion (Yao and Zweig, 2015; Rao et al., 2015).

Generation of inflectional morphology has been particularly useful in statistical machine transla-tion, both in translation from morphologically rich languages (Goldwater and McClosky, 2005), and into morphologically rich languages (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012). Modeling the morpholog-ical structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch  X  utze, 2015).
Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of mor-phology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologi-cally rich languages, as they are able to capture word formation patterns. We have presented a model that generates inflected forms of a given root form using a neural network sequence to sequence string transducer. Our model obtains state-of-the-art results and performs at par or better than existing inflection generation models on seven different datasets. Our model is able to learn long-range dependencies within character sequences for inflection generation which makes it specially suitable for morphologically rich languages. We thank Mans Hulden for help in explaining Finnish vowel harmony, and Garrett Nicolai for making the output of his system available for com-parison. This work was sponsored in part by the National Science Foundation through award IIS-1526745.

