 The task of retrieving information that really matters to the users is considered hard when taking into consideration the current and increasingly amount of available informa-tion. To improve the effectiveness of this information seeking task, systems have relied on the combination of many pre-dictors by means of machine learning methods, a task also known as learning to rank (L2R). The most effective learning methods for this task are based on ensembles of tress (e.g., Random Forests) and/or boosting techniques (e.g., Rank-Boost, MART, LambdaMART). In this paper, we propose a general framework that smoothly combines ensembles of ad-ditive trees, specifically Random Forests, with Boosting in a original way for the task of L2R. In particular, we exploit out-of-bag samples as well as a selective weight updating strategy (according to the out-of-bag samples) to effectively enhance the ranking performance. We instantiate such a general framework by considering different loss functions, different ways of weighting the weak learners as well as dif-ferent types of weak learners. In our experiments our rankers were able to outperform all state-of-the-art baselines in all considered datasets, using just a small percentage of the original training set and faster convergence rates. Learning to Ranking; Random Forests; Boosting
Today, we live in an era of massive available information, with a never-seen-before (and increasing) rate of information production. It is not surprising that such a scenario imposes hard to tackle challenges. For example, the availability of massive amounts of data is not of great help if one is not This work was partially funded by projects InWeb (grant MCT/CNPq 573871/2008-6) and MASWeb (grant FAPE-MIG/PRONEX APQ-01400-14), and by the authors X  indi-vidual grants from CNPq, FAPEMIG, Capes and Google Inc.
 able to effectively access relevant information that satisfies her information needs. Retrieval systems, such as search en-gines, question and answer, and expert search systems serve exactly this purpose: given an information need, expressed in the form of a query, and a set of possible information units (e.g., documents), the main goal is to provide an or-dered list of information units according to their relevance with relation to the query. The desideratum is to increase the likelihood of satisfying an user X  X  information need in an effective manner, which translates to maintain the truly rel-evant results on top of the less relevant ones.

One of the key aspects that influence retrieval systems is how they determine the relative relevance among can-didate results in order to produce a ranked list based on their relevance with regard to some information need, posed in the form of a query. The quality of those rankings is thus paramount to guarantee efficient and effective access to relevant information (and, hopefully, the satisfaction of the user X  X  information needs). Several approaches to gener-ate such ranked lists do exist, being traditionally performed by the specification of a function that is able to relate some user X  X  query to the set of known (indexed) information units. Usually, ranking functions consider several features, such as those that rely on the relatedness between query and possible results (e.g., BM25, edit distance, similarities in vector space models) or on link analysis information (e.g., PageRank, HITS). Such features must be somehow com-bined to provide accurate relevance scores (and, thus, a properly ranked list of results).

Unfortunately, to specify and tune ranking functions turns out to be a major problem, specially when the number of fea-tures becomes large, with non-trivial interactions. This mo-tivates the use of supervised machine learning techniques to devise such functions, since machine learning techniques are effective to combine multiple pieces of evidence towards opti-mizing some goal. This is the direction pursued by Learning to Rank (L2R) techniques, the primary focus of this work.
More specifically, based on a set of query-document pairs with known relevance judgments, the goal is to learn a func-tion f ( d,q ) that is able to accurately devise the relevance scores for a document d , with respect to a query q . Due to its importance, several approaches for L2R have been proposed in the literature. Ensemble methods, such as RankBoost [7], AdaRank [32] and Random Forests [1] (and the variations thereof, such as [11]), are deemed to be the techniques of choice for L2R, achieving higher effectiveness in published benchmarks when compared to other algorithms [11, 3]. Both RankBoost and AdaBoost are based on boosting [26], an iterative meta-algorithm that combines a series of weak-learners in order to come up with a strong final learner, fo-cusing on hard-to-classify regions of the input space as the iterations go by. The strategies based on Random Forest rely on the combination of several decision trees, learned us-ing bootstrapped samples of the training set, together with additional sources of randomization (such as random fea-ture selection) to produce decorrelated-correlated trees X  X  requirement to guarantee its effectiveness.

In this work, we propose a general framework for L2R, named Generalized BROOF-L2R that explores the advan-tages of boosting and Random Forests, by combining them in a non-trivial fashion. More specifically, at each itera-tion of the boosting algorithm, a Random Forest model is learned, considering training examples sampled according to a probability distribution. Such probability distribution is updated at the end of each iteration, in order to force the subsequent learners to focus on hard to classify regions of the input space. In particular, the use of RF models as weak learners has its own advantages, since it is capable of providing robust estimates of expected error through out of bag error estimates and, by means of selectively updating the weights of out of bag samples, one can effectively slow down the tendency of boosting strategies to overfit (a well known phenomenon that becomes critical as the noise level of the dataset being analyzed increases).

As we shall detail in the next sections, the key aspects of the proposed Generalized BROOF-L2R have to do with how to update the probability distribution and how such update should be performed, as well as the underlying ranker to be used to produce the final set of results. In this work, we dis-cuss a set of possible instances of the proposed general frame-work, in order to highlight the behavior and potential of the proposed L2R solution. In fact, the instances that makes use of out-of-bag samples and optimizes through gradient descent [12] over the residues is able to achieve the strongest results, in terms of Mean Average Precision (MAP) and Nor-malized Discounted Cumulative Gain (NDCG), with signif-icant improvements over the explored adversary algorithms, considering 5 traditional benchmark datasets. Our alterna-tive instances were also able to achieve competitive (or su-perior) results when compared to the baselines. Moreover, as our experimental evaluation shows, our approaches based on the proposed general framework are able to produce top-notch results with substantially less training samples when compared to the baselines. Such data efficiency is key to guarantee practical feasibility as obtaining labeled data is still a costly process.

To summarize, the contributions of this work are three-fold. We provide a general framework for L2R that is able to combine two strong methods (boosting and Random Forests) in an original way, which can be specialized in several ways and produce highly effective L2R solutions. We propose and discuss a set of alternative instantiations of such a frame-work, in order to highlight the behavior and effectiveness of each possible choice. Finally, we advance the state of the art in L2R by means of some instantiations of our proposed framework that are able to outperform top-notch solutions, according to an extensive benchmark evaluation considering five datasets and seven L2R baseline algorithms.

Roadmap: Section 2 discusses related work. Section 3 presents our proposed Generalized BROOF-L2R framework, as well as outlines our proposed set of possible instantiations of the proposed framework. We clarify our experimental setup and discuss the obtained results in Section 4. Finally, Section 5 concludes and highlights some future work.
Learn to Rank (L2R) [17] is the focus of active devel-opments due to its cross-industry and society importance. Here, we review some relevant work on this topic, position-ing our work in the literature.

L2R attempts to improve traditional strategies for ranking query results according to some relevance criteria, by explor-ing supervised machine learning algorithms to combine var-ious relevance related features into a more effective ranking function, based on a set of queries and associated documents with relevance judgments. L2R have been successfully ap-plied to a variety of tasks, such as Question and Answer [28], Recommender [29, 16] and Document Retrieval [14] systems.
Solutions specifically tailored to improve document re-trieval have been extensively studied in the past years [4, 19]. In general, there are three major L2R approaches: the pointwise, pairwise and listwise approaches. Pointwise L2R algorithms are probably the simplest (yet successful) ap-proaches, directly translating the ranking problem to a clas-sification/regression one. In this case, the training set for the supervised learning algorithm consists of pairs  X  q i , ( x of queries q i and a list of associated documents x i,j , each one with its relevance judgment y i,j . In this case, each triple  X  q ,x i,j ,y i,j  X  is considered to be a single training example. The goal is to learn a classifier/regressor model capable of accurately predicting the relevance score of a document x with relation to a query q i , thus producing a partial or-dering over documents. Pairwise algorithms, on the other hand, transform the ranking problem into a pairwise classifi-cation/regression problem. In this case, learning algorithms are used to predict orders of document pairs, thus explor-ing more ground-truth information than the pointwise ap-proaches. Unlike both mentioned strategies, the listwise ap-instance (that is, considering a ranked list of documents for a query q i as a single training example), capturing more infor-mation from the training set (namely, group structure) than the previous alternatives. Of course, being able to better capture training data information when learning a ranking function comes with a price: usually, pairwise and mainly listwise approaches are harder to train, since they require more sophisticated (e.g. query-level) loss functions [17].
In terms of the state-of-the-art in L2R, methods based on Random Forests (RFs) and boosting were shown to be strong solutions according to already published benchmarks [22, 11, 3]. More specifically, RFs (and the variations thereof [11]) as well as boosting algorithms such as Gradient Boosted Regression Trees (GBRT) [9] and LambdaMART [33], are considered by many [3, 22, 18] to be the state of the art in L2R tasks. This work is based on both RFs and boosting strategies. Thus, in the following we briefly review some previous literature on them.

The RF algorithm was proposed in [1] as a variation of bagging of low-correlated decision/regression trees, built with a series of random procedures, such as bootstrapping of training data and random attribute selection. The popu-larity of RFs is highlighted by their successful application in several domains, such as tag recommendation [3], object seg-mentation [27], human pose recognition [30] and L2R [3, 22], to name a few. Thus, it is natural to expect several exten-sions to it, in order to improve its effectiveness even more. One such extension is the extremely randomized trees (ERT) model [10] and its application to L2R [11]. The ultimate goal of ERTs is to reduce the correlation between the trees composing the ensemble, a requirement to guarantee high effectiveness of RF models. This is achieved by modifying the RF algorithm in, essentially, two aspects: each tree is learned considering the entire training set, instead of boot-strapped samples. Furthermore, in order to determine the decision splits after the random attribute selection, instead of selecting a cut-point that optimizes node purity, ERTs simply select a random cut-point threshold. This ultimately reduces tree correlation, potentially improving generaliza-tion capability of the learned model. As a final remark, such RF based models can be regarded as nonlinear point-wise approaches for L2R.

Boosting strategies have also been shown to produce state of the art results on L2R tasks, with GBRT [9] (a.k.a, MART (Multiple Additive Regression Trees) and Lambda-MART [33] as the two perhaps most widely used strategies. Both algo-rithms are additive ensembles of regression trees. GBRT learns a ranking function by approximating the root mean squared error (RMSE) on the training set through gradient descent. As with typical boosting algorithms, the goal of GBRT is to focus on regions of the input space where pre-dicting the correct relevance score is a hard task. Since this algorithm aims at approximating the RMSE on the train-ing data, it can be regarded as a pointwise approach. The Lambda-MART algorithm, on the other hand, is a listwise approach that directly optimizes the ranked list of docu-ments according to some retrieval measure, such as NDCG (instead of simply approximating the RMSE of the train-ing documents relevance scores in isolation). To this end, Lambda-MART learns a ranking function that generates a list of relevant documents to a query that is as close as pos-sible to the correct rank. As GBRT, it is based on gradient descent to optimize such metric.

Due to the successful application of RFs and boosting in machine learning tasks (such as classification and L2R), some authors propose to use both strategies in order to come up with better learned models. For example, in [22] GBRTs and RFs are independently explored in order to learn better ranking functions. More specifically, the GBRT model is initialized with the residues of the RF algorithm, followed by the traditional iterations of a GBRT model. The main motivation behind this approach is that RFs are less prone to overfitting, being ideal to initialize the GBRT algorithm instead of the usual uniform initialization. According to the reported benchmark, such strategy was shown to be superior to the GBRT algorithm.

Unlike [22], in [21] the authors propose an enhanced RF model for classification by boosting the decision trees com-posing the ensemble. In this case, each tree is learned with training examples weighted by w i , resembling boosting by re-weighting. In particular, training instances with higher weights influence more when determining the decision nodes (and cut-point threshold definition). Furthermore, each tree is evaluated according to this weighted training set, which enables the ensemble to focus on hard-to-predict regions. The observed effect of such combination is the ability to
From now on, we will use MART and GBRT as synonyms. come up with high quality models with substantially reduced training sets. As we shall detail, our proposed framework is tailored for the L2R task and, instead of introducing boost-ing into random forests, we apply boosting to several RF models, which act as weak learners.

Differently from the aforementioned previous work, we base ourselves in a recent development for text classification, namely, the BROOF algorithm [25]. In BROOF algorithm, RF and boosting strategies are tightly coupled in order to exploit their unique advantages: by exploiting out of bag er-ror estimates as well as selectively updating training weights according to out of bag samples, the BROOF model is able to focus on hard-to-classify regions of the input space, with-out being compromised by the boosting tendency to overfit. This ultimately leads to competitive results when compared to state of the art algorithms. In here, we generalize such approach specifically for L2R tasks in order to come up with better ranking functions: the Generalized BROOF-L2R. As we shall see, this general framework is flexible enough so that it can be instantiated in several ways, exploiting dis-tinct characteristics of the ranking tasks being addressed. In special, with this general framework we are able to achieve state of the art results, with rankers superior to the top notch algorithms proposed so far in all evaluated cases. In this section, we detail our proposed Generalized BROOF-L2R framework. Briefly speaking, this framework allows the definition of learners based on the combination of Random Forests and the Boosting meta-algorithm, in a non-trivial fashion. As we shall see, this framework establishes a set of operations to be performed during the boosting iterations, in a well defined order of application. The goal is to drive the weak learners towards hard to predict regions of the un-derlying data representation, in order to come up with an optimized additive combination of weak learners to form the final predictor. The extension points of the proposed frame-work can produce a heterogeneous set of instantiations that typically produces very competitive results for L2R. In the following, we present the generalized framework for L2R, as well as some pointwise instantiations. We stress that the set of instantiations discussed here is far from exhaustive, being possible to elaborate even better possibilities in future work.
Based on the BROOF algorithm, proposed in [25] to solve text classification tasks, we here extend the proposed ideas in order to exploit the combination of Random Forests and Boosting for the specific task of L2R. However, instead of directly adapting the original algorithm to a single L2R method, we here generalize it into an extensible framework that is flexible enough to permit a series of possible in-stantiations. The proposed framework, named Generalized BROOF-L2R is an additive model composed of several Ran-dom Forest models, which act as weak-learners. Each fitted model influences the final decision proportionally to its ac-curacy, focusing  X  as the boosting iterations go by  X  on ever more complex regions of the input space, in order to drive down the expected error. As usual in a boosting strat-egy, two aspects play a key role: ( i ) the influence  X  t learner in the fitted additive model, and ( ii ) the strategy to update the sample distribution w i,j in each iteration t of the boosting meta-algorithm.
The basic structure of the framework is outlined in Algo-rithm 1, together with a brief explanation of what we call its extension points X  X he general functions exploited by the framework to determine how the optimization process works. There are 5 general functions whose purpose is to specify the weight distribution update process, the error estimation and the underlying input representation. Particularly, the use of the Random Forest classifier as a weak learner extends the range of possible instantiations of the framework, since it enables us to come up with better error rate estimates and a more selective approach to update the examples X  weights, through the use of the so-called out-of-bag samples. scriptively the set of documents x i,j , with associated graded relevance judgment y i,j with relation to a query q i tially, associate a weight w i,j with each training example x i,j according to the general function InitializeWeights . For each boosting iteration t , the input data representation may be updated, through the general function UpdateEx-amples . This general function can considerably extend the range of possible implementations of the framework, allow-ing us for example, to instantiate a Gradient Boosting Ma-chine algorithm [20, 12]. Then, a Random Forest regressor model RF t is learned considering this data representation.
In order to evaluate the generalization capabilities of RF predict  X  y for a set of training documents given by Valida-tionSet . The output of this step is paramount to guide the optimization process towards hard to classify regions of the input space. Although being of great importance to boosting effectiveness, this focus on hard to classify regions of the input space may also be harmful to the optimiza-tion process, specially when dealing with noisy data. As noted by [8, 13], boosting tends to increase the weights of few hard-to-classify examples (e.g., noisy ones). Thus, the decision boundary may only be suitable for those noisy re-gions of the input space while not necessarily general enough for general examples. In order to offer a greater robustness against such a drawback, our framework exposes an interme-diary step related to how the examples weights get updates as the boosting iterations go by. The general function Val-idationSet serves the purpose of specifying which training examples should be used during error estimation and weights update. The main goal here is to provide some mechanism to slowdown overfitting as well as provide more robust esti-mates of error weight (to capture the generalization power of each weak learner and to determine how they should in-fluence the final predictor).

The selected training examples are then used to compute both the error rate of the model and the influence  X  the weak learner on the final model, through Compute-LearnerWeights . Finally, the training examples X  weight distribution is updated by UpdateExampleWeights . This update process should, ideally, take into account the gener-alization capability of the current weak learner RF t , as well as how hard is to correctly predict the ranked lists of the validation examples. Validation examples whose outcome is hard to predict by an accurate learner should influence more in the following boosting iterations. An early stopping strategy is adopted, terminating the boosting iterations if the current learner has an estimated error rate greater than 0 . 5. The final prediction rule is then given by an additive combination of the weak-learners RF t , weighted by  X  t . Table 1: Generalized BROOF-L2R: Possible instan-tiations.
In this section, we describe a set of possible instantiations of the proposed framework. Due to space limitations, we here focus on four possible instantiations, stressing that this is far from being an exhaustive list of possibilities. In fact, we consider some representative alternatives that highlight the flexibility of the proposed framework to produce L2R solutions that typically produces very competitive results.
In order to induce a L2R algorithm based on the Gener-alized BROOF-L2R framework, one needs to specify the 5 generic functions discussed earlier. Our proposed instantia-tions can be found in Table 1. In that table, we specify which alternative was chosen for each generic function, providing details on how they are implemented.
 As it can be observed, BROOF absolute , BROOF median and BROOF height rely on out-of-bag samples in order to drive the boosting meta-algorithm further on hard to predict re-gions of the input space. Such samples are explored when estimating the weak-learner X  X  error rate through out-of-bag estimates. Recall that in boosting, the usual way of assess-ing the errors is to use the training to measure the error. This is too optimistic, since the same data that was used to train the model is used as a measure of error. By using the out-of-bag samples we are able to produce better error estimates, since the out-of-bag are an independent set of samples that was left apart during the construction of the model. Thus, it is able to better approximate the expected error rate of the learner and is a more reliable measure than the usual training error rate [1].

In addition, the out-of-bag errors estimates are used to identify the weights X  distribution that should be applied on following iterations of the boosting procedure; allowing the model to focus on hard to predict regions of the in-put space. We hypothesize that such selective update strat-egy can slowdown the algorithm X  X  tendency to overfit. The major difference between them relates on how each weak-learner influence on the final predictor. The proposed in-stantiations can be found outlined in Algorithms 2 to 4. More specifically, we considered the absolute regression loss, | y i,j  X   X  y i,j | , computed for the out-of-bag samples. We call Algorithm 1 Generalized BROOF-L2R: Pseudocode this variant BROOF absolute . We also considered two other alternatives, that rely on the position of documents in the predicted ranked lists. One alternative, named BROOF-L2R median , relies on the intuition that documents with the same relevance judgment should be as nearer as possible to each other on the current ranked list. We thus consider as loss | Median ( R  X  y i,j )  X   X  y i,j | where R i denotes the list of pre-dictions  X  y i,j associated to documents whose real relevance score is i . The second alternative, named BROOF height is inspired on ideas of [5]. We define the height of a docu-ment x i,j as the total number of irrelevant documents ranked higher then x i,j if x i,j is relevant, or the total number of rel-evant documents ranked below x i,j if it is an irrelevant one.
Finally, in order to illustrate the generality of our pro-posed framework, we provide a fourth instantiation, BROOF-gradient , that resembles the gradient boosting machines (GBM), that optimizes through gradient descent [12] over the resi-dues. More specifically, by a suitable combination of alter-native implementations for each general function outlined in Algorithm 1, one can come up with an algorithm that could be named Gradient Boosted Random Forests (GBRF). This is achieved by considering an alternative representation of input data, that optimizes for the residues, such as y  X   X  y , in-stead of the original input representation, updating them ac-cording to the negative gradient of the cost function (in this case, RMSE). Such alternative is outlined in Algorithm 5.
As we shall see in our experimental evaluation (Section 5), our proposed instantiations achieve very strong results com-pared to seven state-of-the-art baselines algorithms in five representative datasets. In particular, BROOF absolute BROOF gradient were shown to be the strongest algorithms, obtaining significant improvements over the best baselines.
We conducted extensive experiments in well-known L2R benchmarks. In the following, we describe the characteris-tics of the used datasets, the baseline algorithms, the exper-imental protocol/setup and the experimental results.
The corpus we use are freely available online for scientific purposes. Such datasets can be divided into two groups Algorithm 2 BROOF absolute : Pseudocode Algorithm 3 BROOF median : Pseudocode considering the relevance judgments and their sizes. The two largest datasets contain  X  query, document  X  pairs with five relevance levels, ranging from 0 (completely irrelevant) to 4 (highly relevant). In this group we have one dataset from the  X  X AHOO! Webscope Learning to Rank Challenge X , divided into three partitions for training, validation and test. The second largest dataset, WEB10K, consists of 10 , 000 queries released by Microsoft. In contrast to the YAHOO! datasets, the Microsoft dataset is partitioned into 5 folds for cross-validation purposes, with 3 partitions used for training, 1 for validation and 1 for test.
 The second group of datasets corresponds to well-known LETOR 3.0 Topic distillation tasks, TD2003 and TD2004 (a.k.a., informational queries), of the Web track of the Text Retrieval Conference 2003 and 2004. These datasets con-tain binary relevance judgments. Similarly to the WEB10K benchmark, these datasets are partitioned into 5 folds to be used in a folded cross-validation procedure.

For comparative purposes, considering that the Microsoft and LETOR datasets were designed for a folded cross-vali-dation procedure, we applied this same strategy to the YA-HOO! dataset by merging the original partitions into a single set, and splitting the sorted queries into 5 folds, distributed using the same proportions: 3 folds for training, 1 for vali-dation and 1 for test. We report results for both splits: the original one (called YAHOOV1S2) and the new 5-fold split (called YAHOOV1S2-F5). In our experiments we consider as baselines freely avail-Algorithm 4 BROOF-L2R height : Pseudocode Algorithm 5 BROOF gradient : Pseudocode able implementations of state-of-the-art L2R methods, in-cluding AdaRank (with MAP and NDCG as loss functions), Random Forests, SVM rank , MART, LambdaMART and Rank-Boost. We used the RankLib 2 (under the Lemur project) implementations of RankBoost, MART and LambdaMART.
 For AdaRank we used the implementation freely available at Microsoft Research 3 . For SVM rank , we used the origi-nal implementation of [15] 4 . Finally, for Random Forests, we used the implementation available in Scikit-Learn[24] li-brary, which is also the basis of our implementations.
To validate the performance of our approaches, we use two statistical tests to assess the statistical significance of our results, namely, the Wilcoxon signed-rank test and the paired Student X  X  t-test. We consider the Wilcoxon signed-rank test since it is a non-parametric statistical hypothesis testing procedure that requires no previous knowledge of the samples distribution. In fact, some authors believe that it is one of the best choices for the analysis of two independent samples [6]. However, there is also some discussion in the literature favoring the Student X  X  t-test when comparing L2R methods [23]. Due to the lack of consensus, we perform our http://sourceforge.net/p/lemur/wiki/RankLib/ http://research.microsoft.com/en-us/downloads/ 0eae7224-8c9b-4f1e-b515-515c71675d5c/ https://www.cs.cornell.edu/people/tj/svm light/svm rank.html analysis with both tests, considering a two-sided hypothesis with significance level of 0 . 95% in both tests.
 The statistical tests are computed over the values for Mean Average Precision (MAP) and the Normalized Discounted Cumulative Gain at the top 10 retrieved documents (here-after, NDCG@10), the two most important and frequently used performance metrics to evaluate a given permutation of a ranked list using binary and multi-relevance order [31]. To compute these metrics we used the standard evaluation tool available for the LETOR 3.0 benchmark (for binary datasets), as well the tool available for the Microsoft dataset for all multi-label relevance judgment datasets 5 . For MAP, let Q be the set of all queries. These tools simply compute Regarding NDCG, we assume that NDCG@p is 0 (zero) for empty queries, i.e., queries with no relevant documents. Some of the available evaluations tools (e.g., the one from YAHOO!) assume the value of 1 for these cases, which may lead to higher values of NDCG [2]. We chose to standardize this issue, using the same criterion used by most evaluation tools, e.g., those available for the Letor (3.0 and 4.0) and Microsoft datasets, in order to allow fairer comparisons. Ac-cordingly, let IDCG p be the maximum possible discounted cumulative gain for a given query. These tools implement NDCG@p as follows: NDCG @ p = DCG p
In terms of algorithm tuning, we follow the usual pro-cedure of tuning the hyper-parameters using training and validation sets. Considering the Random Forest based ap-proaches we vary the number of trees ranging from 10 to 1000. We achieved convergence around 300 trees, We also optimized the percentage of features to be considered as can-didates during node splitting, as well as the maximum al-lowed number of leaf nodes. The optimal values were 0 . 3 and 100, respectively.
 limited the number of iterations to 500, reminding that the algorithms have an early stopping criterion that prevents further boosting iterations when the error rate exceeds 0 . 5. On average, our strategies converge at about 15 iterations on the LETOR datasets, and around 5 to 10 iterations on the multi-relevance judgment datasets. An exception was BROOF gradient which converged at about 100 iterations for the largest datasets.

Concerning the SVM rank baseline, we favored the use of a linear kernel considering the fact that we verified in our analysis that a polynomial kernel is infeasible on large scale benchmarks such as WEB10K. The cost parameter C was calibrated using the training and validation sets with the explored values: 0 . 001, 0 . 01, 0 . 1, 1, 10, 100 and 1000. For the boosting methods Mart and LambdaMART, we tuned, always considering the validation set, the number of itera-tions ranging from one to a hundred, with a step of 1, and then scaling it up to 1000 iterations, with steps of 100. For the shrinkage factor of the predictive models, we tested the
Reminding that, at the time of the writing of this paper, the evaluation tool used in the YAHOO! competition was not available online. values of 0 . 025, 0 . 05, 0 . 075 and 0 . 1. The best found values for the MART and LambdaMART were ensembles of 1000 trees with shrinkage factor  X  of 0 . 1. For the AdaRank MAP AdaRank NDCG @5 and for the RankBoost algorithm, similar procedures were performed in the validation set to configure the number of iterations.

Finally, we performed 5, 10 and 30 runs of the 5-fold cross validation procedure for WEB10K, YAHOO! and LETOR datasets, respectively. The differences in the number of rep-etitions are due to the size of the datasets and the need to properly address the variance of the results. The reported results on Tables 2 and 3 are the average of all these runs, being the statistical tests applied to these results.
In this section we analyze our proposals in terms of effec-tiveness, comparing them to the 7 explored baseline algo-rithms on the 5 described datasets. The results are reported on Tables 2 and 3.

We start by considering the MAP metric (Table 2). Briefly, the MAP results show that, overall, our proposed framework outperforms or ties with the strongest baselines in all cases. More specifically, with the TD2003 dataset, BROOF height outperformed the strongest baseline (RF) considering both statistical tests, with BROOF absolute and BROOF median as the winners according to at least one statistical test. In this dataset, BROOF gradient was statistically tied with the best baseline. Considering TD2004, BROOF absolute was consid-ered the top performer amongst the proposed solutions, be-ing tied with the strongest baseline  X  RankBoost  X  in this dataset. Regarding the WEB10K dataset, we can see that BROOF gradient was the top performer, according to both statistical tests, being superior to MART, the strongest base-line. Finally, in the YAHOOV1S2 dataset all four proposed algorithms were statistically superior to the strongest base-line (RF) according to both statistical tests, whereas in the YAHOOV1S2-F5 dataset BROOF gradient was the best ap-proach. In sum, according to the MAP metric, our results clearly show that the proposed instantiations of the General-ized BROOF framework produced very competitive results as the best algorithm, being superior in the majority of the cases (and tying in the others)  X  a very significant result. Turning our attention to the NDCG results, reported on Table 3, a similar behavior can be observed: our proposed instantiations are no worse than the strongest baselines in all cases, being superior in the majority of cases. Consid-ering the TD2003 and TD2004 datasets, our solutions were no worse than any baseline, being statistically tied with the strongest one (RF, in both cases). BROOF gradient was the best algorithm in the three remaining datasets, according to both employed statistical tests. Furthermore, BROOF median was also superior to the best baseline (MART) in the YA-HOOV1S2 dataset (according to the Student X  X  t-test), with BROOF absolute and BROOF height tied with the MART algo-rithm. Again, this set of results highlights the effectiveness of the proposed approaches.

We now turn our attention to some behavioral aspects of our algorithms, namely, convergence and learning effi-ciency. In order to better understand the convergence rate of our proposals, we provide an empirical evaluation of our most effective solution (i.e., BROOF gradient ), by analyzing the obtained NDCG@10 as we vary the number of boost-ing iterations, contrasting these results with the boosting baselines. We here focus on the three largest datasets: YA-HOOV1S2, YAHOOV1S2-F5 and WEB10K. Results can be found on Figure 1. As it can be observed, BROOF gradient share similar behavior with three explored boosting algo-rithms, namely, MART, RankBoost and AdaRank-NDCG: the four algorithms show fast convergence rates. The two key differences are: ( i ) our approach is able to achieve sig-nificantly better results at the initial boosting iterations and ( ii ) BROOF gradient converges to a higher asymptote than the other algorithms. On the other hand, the conver-gence rate of LambdaMART was significantly slower than the convergence rate of the mentioned algorithms. In sum, BROOF gradient enjoys faster convergence rates, with higher NDCG values at the initial boosting iterations and higher asymptote. This is paramount to guarantee practical fea-sibility of our solution: although high effectiveness is a re-quirement, achieving such high effectiveness with just a few boosting iterations is key to minimize running time.
Another aspect of direct impact on the practical feasibility of the solutions is to what extent the algorithms are  X  X ata efficient X . That is, to what extent each algorithm is capable of delivering highly effective rankings with reduced train-ing sets. We evaluate the solutions under this dimension by analyzing each algorithm X  X  learning curve. To this end, we measure the effectiveness of each algorithm as we vary training set size. We randomly sample s % examples from the training set, selected at random. We vary s from 10% to 100%, with steps of 10%. The obtained results can be found on Figure 2. Considering the WEB10K dataset, we can ob-serve a surprising result: BROOF gradient is able to outper-form all algorithms with just 20% of the training set, even considering the other algorithms trained with larger train-ing sets (including the entire training set). Also, it can be noted that BROOF absolute is no worse than the baseline algo-rithms, even with 10% of the training set. In fact, with about 40% of the training set BROOF gradient is able to achieve its maximum effectiveness, whereas for BROOF absolute 10% is enough. For the YAHOO datasets, a similar behavior was observed: with about 20% to 30% of the training set our approaches were able to outperform the baseline algorithms (or match, in the case of BROOF absolute ), even considering the baseline algorithms trained with the entire training set. In these datasets, our algorithms were able to achieve max-imum effectiveness at 50% to 80% of the training set. Con-sidering the TD2003 and TD2004 datasets, the RF baseline was a bit more competitive to our approaches, exhibiting a similar behavior in terms of effectiveness as the training set size varies. In these datasets, 50% to 60% of the train-ing set was enough to produce the best effectiveness on the TD2003, while 40% was enough to surpass all baselines on TD2004. These findings have also a direct influence on the practical feasibility of our solutions. First, smaller training sets translates to smaller runtimes. Second, obtaining la-beled data is critical but also costly. Clearly, being able to produce highly effective models from reduced training sets is an important characteristic of a successful approach.
Finally, we turn our attention to the effect of the use of out-of-bag samples by our approaches. Due to space re-strictions, we here focus on BROOF gradient , considering the WEB10K dataset. We analyze the effect of weak-learner er-ror rate estimation through out-of-bag samples by contrast-ing it with a variant whose generic function ValidationSet equals to Train . The effectiveness of BROOF gradient the mentioned variation, as the boosting iterations go by, can be found on Figure 3. From that figure, it is clear that the out-of-bag error estimation produces more effective re-sults than the simple training error estimate. In fact, for all boosting iterations, the BROOF gradient variation with Val-idationSet set to OOB produces better results than the results obtained with ValidationSet set to Train . This highlights the importance of exploiting the out-of-bag er-ror estimates in our proposed framework instantiations. As a final remark, as it can be observed in Figure 3, even the variant that uses the training error rate is able to outperform the explored baselines. This is also an important aspect that highlights the quality of the proposed framework.
In this work, we propose an extensible framework for L2R, called Generalized BROOF-L2R, which smoothly combines two successful strategies for Learning to Rank, namely, Ran-Figure 3: BROOF gradient : Effect of out-of-bag sam-ples versus entire training set. dom Forests and Boosting. Such combination, that uses Random Forests models as weak-learners for the boosting algorithm, relies on the use of the out of bag samples pro-duced by the Random Forests to ( i ) determine the influence of each weak-learner in the final additive model and ( ii ) update the sample distribution weights by means of a more reliable error rate estimate. In fact, the framework is general enough to provide a rather heterogeneous set of instantia-tions that, according to our empirical evaluation, are able to achieve competitive results compared to state-of-the-art al-gorithms for L2R. We proposed four different instantiations. Three instantiations closely follows the ideas of a recently proposed algorithm for text classification, namely, BROOF. The fourth instantiation is based on gradient descent op-timization, resembling gradient boosting machines. In fact, such instantiation can be seen as a gradient boosted random forests model. As our results show, despite the fact that all the four algorithms provide very competitive results, two of them are consistently the top-performers, highlighting the quality and effectiveness of our proposed framework. Also, our proposals have two properties that are paramount to guarantee their practical feasibility, namely, data efficiency and fast convergence rates.

The space of possible instantiations of the proposed gen-eral framework for L2R is rather large. This clearly makes room for further investigations regarding such possibilities. In fact, one can come up with improved instantiations of the framework, by means of extending the set of possible implementations for each generic function composing the framework. This is under investigation. We also plan to study a more comprehensive set of instantiations, in order to build a substantially larger catalog of algorithms based on the Generalized BROOF-L2R to better understand the effects of each choice on model effectiveness.

