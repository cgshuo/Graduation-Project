 1. Introduction
The forecasting of traffic flows, traffic volumes and travel times is a very important part of traffic management, control and information system. The traffic simulation is correspondingly needed to make these forecasting in a reliable way. The results of the traffic circumstance forecastings can be used for different purposes such as to influence travel behavior, to reduce traffic congestion and generally to improve the performance of traffic management system.
 As powerful computational tools for forecasting tasks, Artificial Neural Networks (ANNs) outperformed on early days of classical models. In real world issues, when there is no enough physical foundation and when data is mixed with colored noise, the neural and neurofuzzy models are still proficient of making good estimations ( Elsner, 1992 ). The Emotional Learning (EL) algorithm has been presented for the first time to train the neurofuzzy models ( Gholipour et al., 2003a ). They can be regarded as RBF ( Nelles, 1993, 2001 ). As the architecture of RBF presents, it is a locally linear model. Recently, MLP and RBF neural networks have produced appropriate results in various fields of forecasting. Their prominent features include limited forecasting errors, relatively low computational complexity, low computational time measure-ments and high generalization.

Temporal Difference Learning (TDL) algorithm has been pre-sented for the first time to change the training procedures in ANNs in control multivariable systems by Abdi et al. (2004 ). They proved that by augmenting emotions to TDL algorithm the capability of the networks can be increased. The obtained results have shown that the proposed algorithm as a simple algorithm with a low-volume computations is capable of improving rise time and controlling complex system. In some engineering applications, a multi-step forecasting in a class of ANN is implemented by TDL algorithm ( Hwang and Moon, 1991 ) and in some other applica-tions, it is implemented by combining several approaches with neural network forecasting, such as multiple-neural-network, time-delay neural network and adaptive time-delay neural net-work ( Xie et al., 2006 ). In both applications, it has provided satisfactory results.

Conventional statistical models or classical models usually suffer their strict mathematical assumptions when describing complex traffic circumstances. But, intelligent adaptive models are made for describing phenomenon under non-ideal physical environ-ments such as traffic-flows. The intelligent adaptive models can adapt itself for a wide variety of traffic circumstances. The studies show reliable performance of these models.
 of the forecast critical factors, features and objectives in traffic-flow as short-time ones to perform remarkably accurate forecast-ing. Flows in traffic foreseeing as outstanding contributions of
Emotional Temporal Difference (ETD) learning algorithm moti-vated the researchers in this study to introduce a multi-objective learning algorithm in a purposeful forecasting. The T X  X  neuro-fuzzy, BELBIC and LLNF models with Model Tree learning (LoLiMoT) algorithms are the three models that are updated by developing the emotional algorithms. In addition to these models,
MLP and RBF neural networks are studied and their results are compared. According to the importance of studying noise in the real world applications, the LLNF model is just studied as a good forecaster in noise condition based on a hybrid learning algorithm (combination of wavenet and LoLiMoT algorithm which is called WLoLiMoT).
  X  Providing a new learning algorithm to enhance the perfor- X  Applying the BELBIC and LLNF model in short-term traffic-flow  X  Providing a new hybrid algorithm to overcome the noise signal.  X  Solving the curse of dimensionality issues in Daubechies  X  Using different evaluation indices to specify the proper models. problem in Section 2 , the EL and TD as new learning algorithms are introduced in Section 3 , the combination of them is provided.
And this hybrid learning algorithm is used to train a class of neurofuzzy models. The classical model and the proposed hybrid
ARIMA and ANN models are described in Section 4 . The T X  X  neurofuzzy models and some general issues on intelligent models such as neurofuzzy models, the limbic model of mammalian brain and original model of BELBIC algorithm are discussed in Sections 5 and 6 , respectively. The LLNF model with LoLiMoT and with
WLoLiMoT algorithms are presented in Section 7 in order to study noise. Finally in Section 8 , the results of short-term traffic-flow forecasting performed by ETD learning algorithm to learning models are presented and a comparison is made between the results of forecaster, MLPNNs, RBF, neurofuzzy networks,
Autoregressive Integrated Moving Average Neural Network (ARIMANN), BELBIC and LLNF models with EL. The conclusion included in Section 9 confirms the capability of the new model to improve the accuracy and the computational time over the neurofuzzy models. 2. Background
Flow, speed and density are important macroscopic traffic features to demonstrate the load of traffic on the transportation system. The topic of forecasting short term traffic-flow has been discussed for three decades, and many models have been gener-ated. The general consideration in forecasting is shown in Fig. 1 . These models usually are classified as the following groups:
Historical Average ( Stephanedes et al., 1981 ), Kalman Filter (KF) ( Okutani and Stephanedes, 1984 ; Abdi et al., 2010 ), ANNs ( Smith and Demetsky, 1994 ; Chen et al., 2004 ; Tan et al., 2007 ; Zheng et al., 2006 ), ANN with parallel Back Propagation (BP) training ( Tan et al., 2007 ), Self-Scaling Parallel Quasi-Newton neural net-work ( Tan et al., 2009 ), Exponential Smoothing ( Messer, 1993 ), Fuzzy Logic System (FLS) methodology ( Zhang and Ye, 2008 ),
K-Nearest Neighborhood ( Davis and Nihan, 1991 ), ARIMA ( Ahmed and Cook, 1980 ), Traffic-flow simulator method ( Lam and Xu 1999 ), the Seasonal Autoregressive Integrated Moving Average (SARIMA) based on Bayesian estimator ( Ghosh et al., 2007 ),
Bayesian Network ( Jung and Cho, 2008 ), Wavelet Transform ( Xie and Zhang, 2006 ), Spectral Analysis ( Nicholson and Swann, 1974 ),
Neurofuzzy theory ( Park, 2002 ; Yin et al., 2002 ), Wavelet Trans-form and neural network theory ( Stephanedes et al., 1981 ),
Chaotic Time Series Analysis ( Xue and Shi, 2008 ), Neurogenetics theory ( Abdulhai et al., 2002 ), and Adaptive Network based Fuzzy Inference System (ANFIS) ( Dong et al., 2009 ).

The T X  X  neurofuzzy model with FIS, ELFIS and ANFIS learning algorithm is studied specially based on multilayer feedforward neural networks ( Roger, 1993 ; Halgamuge and Glesner, 1994 ;
Hou et al., 2003 ). The ANFIS networks have several structure varieties and functions of the basic ANFIS structural design which are similar to the RBF ( Roger, 1993 ). Short-term traffic-flow forecasting on the dynamic traffic-flow data has been discussed with a technique based on ANFIS and random factors ( Liu et al., 2008 ). Although many models are used to forecast traffic flow, but based on traffic circumstances complexity, traffic analysts can hardly select the proper model for a definite highway at a certain time. A forecasting model which acts better than the others in a certain traffic circumstance can completely fail or degrade in the other traffic circumstances. Furthermore, earlier models have their own disadvantages in forecasting, although the strengths are noticeable ( Smith and Demetsky, 1997 ). Some models have a main disadvantage in replying to unexpected events such as the historical average model. The exponential smoothing model X  X  weaknesses are indicated to lead in great states X  variations and the difficulty of obtaining a proper smooth constant.
The main disadvantages of the ANN are that it is mainly dependent on the training of the network. The important training data and other ANN X  X  parameters can be classified as the epochs, hidden layers and the components in hidden layers. In order to improve the ARIMA model usage, carefully recognition of the model, finding the difference and the best orders of Autoregres-sive (AR), and Moving Average (MA) by the traffic analysts is necessary. Furthermore, the ARIMA model is especially ready to miss values because of the malfunctions of data communication systems, loop detectors etc. ( Smith and Demetsky, 1997 ). Using a Rough-Fuzzy Neural Network (R-FNN) in forecasting traffic-flow to achieve better effects on real-time traffic control with the aim of targeting the signalized intersections of urban road network, have been examined by Zhou and Huang (2008 ).

Development of style of traffic at a demanding station x is basically a spatio-temporal analysis. By discretizing of time and space, traffic styles at a location x at time t rely on traffic patterns at location x , x j and x  X  j , j  X  1,2, y , m . For this comparatively restricted area as a figure of the highway, backward propagating shockwaves may be sent by forward traffic of upstream sections to the intended location and downstream sections. The shock-waves event is shown in Fig. 2 .

In time and space model, some dynamic feature such as the lack of information effects about drivers X  actions, traffic levels and spatio-temporal specifications of traffic cooperate in a com-plicated way. The complexity of forecasting modeling is high-lighted when these features are combined with the forecasting uncertainties.

The time series modeling is a general model to formulate the traffic-flow forecasting. Although it is known that the time series forecasting are slow and overfitting ( Smith and Demetsky, 1994 ). Non-parametric regression model is the replacement of the time series model ( Davis and Nihan, 1991 ). Several investigative efforts have been made using ANNs as a replacement for the more traditional regression and time-series approaches ( Smith and Demetsky, 1994 ).

As the forecasting definition plays a serious role, the traffic forecasting problem can be presented as the following. Assuming ing is performed to produce and estimate of T ( i  X  s ), in which s is the horizon of the forecasting. Based on the above mentioned studies, interrelationships emphasize the fact that there exists a systematic flow of consideration in developing short-term traffic-flow forecasting algorithms. Fig. 3 shows a three-step perception of modeling short-term traffic algorithms. The first step settles on the basic characteristics of the forecasting algorithms. At the end of the first step, a complete understanding of requirements in modeling such as area and type of implementation; horizon and the step; and multivariate approach should be obtained.
By considering several circumstances, the forecasting issue is expressed as the non-parametric regression ( Robinson (1983) ). The covariate vector i  X  1, y , t is a set of { T ( t d  X  1), may include other available data such as identical time period over past days. The response variable is y  X  T ( t  X  s ).
In Eq. (1), the covariate is the time series of past observations which is x  X f T  X  t d  X  1  X  , ... , T  X  t  X g X  1  X 
The selecting procedure of the d parameter has been discussed by Sun et al. (2002 ). Eq. (1) is used as a time series in forecasting models which are presented in the next sections. The general setting for traffic-flow forecasting flowchart is shown in Fig. 4 ( Sun et al., 2002 ).

The traffic observation system output could be the traffic volume, the density or the flow which is selected according to the application. In this application, the parameter of interest is the traffic-flow (indicated by n in Fig. 4 ) which is given on a daily-basis and time intervals of t . In this study, traffic flow is the requested signal for forecasting in the specific hour and day.
The resolution of the data or time interval used for forecasting is an important aspect which is determined by an optimization process in which the resolution of data is 1 min. Similarly, in a work by Abdulhai et al. (2002) , the effects of time interval for three different times (30 s, 1 min, and 5 min) were discussed ( Abdulhai et al., 2002 ).

According to the reviews of previous studies, it is found that decreasing the prior error of the forecasting is used as criterion in optimizing of the defined cost function. But in this study, a sequence of the forecasting prior errors with a specific coefficient is used for optimizing of the defined cost function. The results show that both error ratio and computational time consumption are reduced which are important in real time applications. This approach is implemented in some classic and also developed models of this study.

In this study, T X  X  neurofuzzy and LoLiMoT algorithms with emotional temporal signals enjoy the strengths of many different models under different traffic circumstances for generating better forecasting than those of the existing models. Application of these
BELBIC and LoLiMoT algorithms are tested for traffic-flow fore-casting for the first time. The system has the ability to forecast the traffic-flow according to the previous data or the input coming from neighbor detectors in the missing or the mistaken data events.
 Time Ye s 3. Proposed learning algorithm 3.1. Emotional learning algorithm forecasting issues and it is a psychological algorithm. Based on this model X  X  specification, several objectives are gained in fore-casting. In this algorithm, an emotional signal can be used instead of the reinforcement signal. This emotional signal is translated as the current state cognitive evaluation. The emotion concept is used in the traffic-flow forecasting issues in order to decrease the errors of forecasting in some areas or depending on requests. For instance, forecasting the traffic-flow is mostly important in the peak points of the working time or in holiday, or when forecasting the demand time of the path with better variance approximation can be preferred. Of course, there are some models that can meet these objectives as well, but using EL can result in solving complex and sophisticated multi-objective problem in an effec-tive and quick way ( Fatourechi et al., 2001 ). Multi-objective forecasting engages satisfying several objective functions, subject to constraints. Since a solution that reach one function often does not necessarily reach the others at the same time. There is usually no unique optimal solution. Sometimes the decision maker has in mind a goal for each objectives. For instance, combines a
Fuzzy Optimal Model (FOM) with a Genetic Algorithm (GA) is used to solve the multiple objective runoff routing parameters calibration issue. The peak discharge, peak time and total runoff volume are the calibrated parameters, simultaneously ( Cheng et al., 2002 ).

This emotional based method demonstrates a critic X  X  emotions about the forecaster performance. Emphasizing of some areas or some features by critic can be considered in its emotions and easily influences on the characteristics of the forecaster ( Gholipour et al., 2003a ).

Consequently, an emotional signal can be defined as an errors function, error change rates and other factors. It is dependent absolutely on the problem and the scope dimensions. Subse-quently, emotional signal is used to explain the loss function ( Gholipour et al., 2003a ; Fatourechi et al., 2001 ). A straightfor-ward form of a loss function can be defined in Eq. (2) J  X  1 2 K where K is constant number and E is the emotional signal. As already mentioned, a learning algorithm can regulate the weights of the model through a nonlinear optimization technique.
Using steepest descent model which is a nonlinear optimiza-tion technique, the weights can be set by the variations in Eq. (3)
D o  X  Z @ J @ o  X  3  X  where Z is the learning rate. According to the chain rule Eq. (4) is formulated as follows: @ J @ o  X  data exist (station l ) Criteria Evaluation Yes
Predicted traffic flow on station l
According to Eq. (2) @ J @ E  X  K : E  X  5  X  and @ y =@ o is obtained from the output of the model and it may vary. Calculation of @ E =@ y cannot be done easily. This is the result of the freedom in choosing any desired emotional cues as well as imposing any predefined model. However, it can be approximated through simplifying assumptions. In this study, error is defined as in Eqs. (6) and (7) e  X  ^ y y  X  6  X  @ E @ y  X  @ E @ e  X  7  X  where ^ y is the output signal which should be estimated and, in Eq. (4) it can be substituted by  X 1. By this modification, the algorithm is satisfying instead of optimizing. The new updating rule for the weights is as the following: D o  X  K U Z U es U @ y
In comparison with other neurofuzzy learning algorithm, EL algorithm is a model free method with four unique features: 1. There is no concern about the differentiability to render the ability of the recursive equation issues; 2. It uses the emotional signal complex concept but has no increased computational complexity of EL algorithm; 3. It is efficient and fast in computation and training; and 4. the best performance can be reached by simple setting of the parameters ( Nelles, 2001 ; Fatourechi et al., 2001 ).
The significant advantage of introducing the EL was arrange-ment of crucial aspects in real world application ( Gholipour et al., 2003a ). In short-term traffic-flow forecasting, the fluctuation makes some peak points which are more important than the other features. Forecasting these peak points before they happen has a lot of advantages in traffic literature. In order to reach the goals, the error and delay of forecasting error and delay may be considered in definition of emotional signal. Besides, model training and low computational complexity might be amongst other achievements.

The EL algorithm gradient-based optimization in neurofuzzy forecasters and the emotional signal definition are explained with a case study. In this study, some of the computational algorithms based on emotional signal are presented for forecasting short-term traffic-flow. The Fuzzy Inference System-based EL (ELFIS) algorithm is based on temporal difference learning algorithm for presenting new updating rule in forecasting short-term traffic-flow application as a contribution of this paper. 3.2. TD (lambda) learning algorithm
Reinforcement Learning (RL) and TDL algorithms have a long history of relationship in the game theory. RL is a kind of machine learning. It permits agents or machines to automatically deter-mine the perfect behavior inside a specific context in order to maximize its performance. In this study, maximizing the perfor-mance is equal to high accuracy of forecasting, so decreasing the value of Eq. (2) is the target. Simple reward feedback is required for the learning agent to learn its behavior; this is known as the reinforcement signal. This reinforcement signal reflects the reward signal or punishment signal of the whole system after it has performed some sequence of actions. Therefore, the reinforcement signal does not assign credit/fault to any action, or to any particular node or to any system element ( Sutton and Barto, 1987 ; Sutton, 1988 ).

In this sub-section, a new algorithm is proposed for updating the weights based on emotional learning algorithm. This new incremental algorithm is used for updating the learning X  X  weights in models.

TD learning algorithm is a common type of RL to solve delayed-reward forecasting problems. Dissimilar to RL which measures error values between every forecasted signal and its real value, TD uses the differences between two consecutive forecasted signals for learning and obtaining reward signal. By means of these consecutive forecasted signals, weights can be updated incrementally that can increase convergence and com-putation speed. The convergence speed of result and incremen-tally updating of weights are the main advantages of TD learning algorithm.
 TD learning algorithm is a forecasting method ( Sutton and Barto, 1987 ). It has been mostly used for solving the RL problems.
TD learning algorithm is an integration of Monte Carlo ideas and dynamic programming ideas. In games theory, TD learning algo-rithm have an enormous achievement ( Samuel, 1959 ; Tesauro, 1992 ). TD learning algorithm addresses the following problems ( Sutton and Barto, 1987 ): First, TD learning algorithm aim to increasing accumulated results rather than instant. Second, a TD agent usually learns to select an action based on its prior actions and the adversary. Third, TD learning algorithm is based on human intelligence.

Based on the abovementioned issues, the BP algorithm is the most common learning algorithm for ANN. But most of new learning algorithms based on the RL or TD learning algorithm due to their reduced error and low computational time in comparison with the classical learning algorithms are preferable. In forecast-ing domain of fast training, particularly in traffic decisions, the low complexity in computation and validity are the main two specifications of a good forecaster.
 come sequence where y is the sequence result and ^ y t is an observation vector in the time t ,1 r t r m . The forecasted signal of y is obtained by the learning agent for every observation and then a sequence of ^ y 1 , ^ y 2 , ... , ^ y m is formed.
In this study the learning agent is an ANN so the network weight, w , can be updated by the classical gradient descent. The updated rule for supervised learning algorithm is calculated with Eq. (9) D w  X  Z r w E  X  Z where Z is the learning rate and r w E the gradient vector. @ E =@ w of the Mean Square Error (MSE) algorithm which is a kind of supervised learning algorithm. Based on Eq. (9), the new learning algorithm is presented as follows:
D w  X  Z  X  ^ y t ^ y t 1  X 
To highlight recent forecasting, gradient term is multiplied in an exponential factor, l , as in Eq. (11)
D w
Eq. (11) is a group of learning rules which is called TD (Lambda) where Lambda is constant values. Lambda has a crucial role in learning as discussed by Dayan (1991) . There are two conditions based on the upper and lower bands of Lambda: D w t  X  a  X  ^ y t  X  1 ^ y t  X  r w ^ y k  X  12  X  a alters the performance of the system; if a  X  1 the weights alter so that ^ y t  X  ^ y t  X  1 . In other words, it means that the NN changes significantly after each epoch, it prevents stabilizing of the NN.
Therefore, practically using small value of a or otherwise starting with large value of a and decreasing it in learning progress is general. The similar training algorithm for supervised learning algorithm can be applied for TD(0). Based on the new delta rule which is presented in Eq. (12), BP algorithm can be modified to
Temporal Difference Back-Propagation Learning (TDBPL) algo-rithm which is described in the next section. 4. Classical model: hybrid ARIMA and ANN models forecasting traffic-flow is used. This hybrid model is well-known for the chaos time series forecasting and estimation. This hybrid model assumes that the stationary and non-stationary patterns of a time series can be separately modeled. In this study, the ARIMA model is applied in three different models as the ARIMA, ARI-
MANN with BP learning algorithm and ARIMANN with TDBPL as a forecaster. The last one is the other contribution of this paper and the ARIMA and ARIMANN with BP learning algorithm are applied just for comparing the simulation results. Time series patterns and ARIMA model are used in order to forecast future data or to have a better perception of the data.
 traffic-flow pattern, including those that are stationary, non-stationary and periodic ones. For example, the pattern of station 708 signal has been shown in Fig. 5 and it has stationary and non-stationary and periodic patterns. An ARIMA model can capture the stationary and the non-stationary and periodic components of the pattern in Fig. 5 .

Fishman provided the primitive example how the classical time-series models can be used for the estimation of confidential measures of response unpredictability ( Fishman, 1971 ). He fitted an autoregressive model of order p to an observations series of a chosen simulation-response in an intermediate step of his proce-dure. After that, the model was extended by fitting mixed ARMA models, ARMA ( p , q ), to a time series generated by simulation using an automated procedure ( Schriber and Andrews, 1984 ).
The ARIMA model is one of the main models of short-term traffic forecasting. Van Der Voort considered a hybrid integrative model called Kohonen X  X  self-organizing maps using ARIMA model ( Van Der Voort et al., 1996 ). Meanwhile, in comparing with ANNs or the k-nearest neighbor approaches, ARIMA models has benefits in accuracy. In ARIMA models development, the low time con-suming is crucial. By considering these benefits, ARIMA model is more preferable and referable than the ANNs and the k-nearest neighbor approaches, while one has important functional impli-cations concerning wide scale adoptions ( Smith et al., 2002 ).
The ideas that form the ARIMA model for forecasting are as follows: 1. The forecasts are according to the linear functions of the sample observations; and 2. The aim is to find the simplest models that provide an adequate description of the observed data.

For obtaining a stationary time series, it is recommended to use time series and forecasted signal differentiation in the ARIMA model. It is explained as follows as ARIMA ( p , d , q ) in which p is a constant, q is the order of the AR model and d is the order of the
MA model and the order of differencing. All parameters are positive integers. As presented in Eq. (13), the traffic-flow is a dependent time series. For the dependent time series { y ( k )} if 1 r k r N , the ARIMA model equation is as the following:  X  1 B  X  d ^ y  X  k  X  X  m  X  where ^ y  X  k  X  is the traffic-flow forecasted, k is the time, m is the constant, B is the lag operator that provides the prior value of the sequences  X  B : ^ y  X  k  X  X  ^ y  X  k 1  X  X  .

In Eq. 14, f ( B ) and y ( B ) are the degree p and q polynomials f B  X  X   X  1 f 1 : B 1 f 2 : B 2 ... f p : B p and y  X  B  X  X  1 y 1 : B 1 y 2 : B 2 ... y q : B q  X  14  X  where p is the AR operator, q is the MA operator, e ( k ) is the random error, and d is the degrees of difference involved.
In order to fit an ARIMA model to a forecasted time series, the identification of the model, parameters estimation, checking of model diagnostic, and verifications of the forecast are taken into consideration.

Many commonly used smoothing models are special instances of ARIMA. For instance, the Exponentially Weighted Moving Average (EWMA), is equivalent to ARIMA (0,1,1); linear exponen-tial smoothing, also known as Holt-Winters, is equivalent to ARIMA (0,2,2) ( SAS, 2004 ).

The ARMA models are complex models which may require more computational power, and the increase in performance will not be justified as the synthetic and real data results using the linear model.

Based on research findings, integrating different models can increase and improve the prospects to capture different patterns in a given data and improve forecasting performance. Moreover, the integrated model is more robust according to the possible organization changes in data. An integrative methodology using RBF and the Box X  X enkins models has been illustrated in Wedding and Cios (1996 ).

One of this integration is that of ANN and ARIMA. In this study, therefore, the integration of MLPNN and ARIMA as a hybrid model will be discussed (from now on called ARIMANN). As it is shown in Fig. 6 , the ARIMA X  X LPNN hybrid model is made up of an ANN in series with an ARIMA model ( Valenzuela et al., 2008 ).
The linear ( L t ) and Non-Linear ( NL t ) components of traffic-flow signal are the key causes that motivated the researchers in this study to present ARIMANN hybrid model, which is a linear combination of an ANN and ARIMA models. Consequently, the ANN is responsible for the additional complex nonlinearity part and the ARIMA model is responsible for approximating the linear model intrinsic to the basic process generating the series. Based on this illustration, the traffic-flow can divided in two parts, nonlinear and linear components as in Eq. (15) Y  X  L t  X  NL t  X  15  X 
It is important to indicate that, however ANNs are general approximators, there is no exact model for optimizing the net-works for a certain model. Many experts have proposed the references which show the greater the number of the ANN X  X  parameters the more complex to train it and the bigger the computational effort. As a result, in this study is Back Propagation learning with TDL algorithms as a decreasing factor of the time computational efforts.

To design the ANN, a fully interconnected MLPNN with one input layer, one hidden layer and one output layer has been used. The MLPNN and ARIMA models are, respectively, defined as the activation function g (.), the weights of the connections coming from the input node to the j th hidden neuron w hj , the weights of the connections coming from the bias unit j th hidden neuron b the weights on connections from the j th hidden neuron to the output node w oj . For these definitions the activation function for the j th hidden neuron and the output layer are obtained with Eq. (16) h  X  t  X  X  g  X  w hj x  X  t  X  X  b hj  X  and y  X  t  X  X  f
Considering Fig. 6 , the aim is to provide a recursive algorithm by means of which the weights of MLPNN and the parameters of the ARMA model can be adjusted so that applying a set of inputs creates a pleasing set of outputs. As MLP network is supervised, all inputs are mapped to the aimed outputs during the learning stage. In this stage, the criteria of setting weights of the neurons are defined using the error obtained from the difference between real value and the aimed value iteratively. Therefore, the accu-mulated error in this stage can be optimized for any input X  X utput pairs. The most commonly used algorithm in charge of this method or its learning rule is the BP algorithm rule. The BP algorithm uses the steepest descent rule for minimizing the MSE algorithm. Consequently, the inherent entrapment pitfall of the steepest descent algorithm is also inherited by the BP algorithm.
The BP algorithm is sensitive to the pick of initial weights. It will converge to a local minimum in the surrounding area of the initial solution. The error minimization using BP network is shown in Fig. 7 .

However, learning algorithms which apply nonlinear conju-gate gradient methods, such as Newton X  X  method, are computa-tionally intensive for ANNs with several hundred weights, derivative calculations as well as sub-minimization procedures and approximations of various matrices are required ( Tan et al., 2009 ). In addition, it is not clear that the extra computational cost speeds up the minimization process for non-convex functions when far from a minimizer, as is usually the case with the ANN learning problem. Thus, the development of improved Residual gradient-based BP algorithms receives significant attentions of ANN researchers in engineering application field.
 biases and weights can be achieved by defining the error measure for the output of the ARIMA model as follows:
E  X  t  X  X  1 2  X  y  X  i  X  ^ y  X  i  X  X  2  X  17  X  where y ( i ) and ^ y  X  i  X  are the desired and forecasted output, respectively. Replacing the ARIMA model in Eq. (17) turns into Eq. (18) E  X  t  X  X  1 2 y  X  t  X  formulated as
E  X  t  X  X  1 2 y  X  t  X  f 1 y t 1 ... f p y t p
E ( t ). The E ( t ) gradient can be determined as Eq. (20) r E  X  t  X  X  @ E  X  t  X  updated by the following Eq.: w oj  X  t  X  1  X  X  w oj  X  t  X  X  Z y 0  X  y  X  i  X  ^ y  X  i  X  X  h j where Z is the learning rate, D w kj the change of weight during the last iteration, and l the momentum rate. However, the fluctuation in weights holds only for neurons fitting in to the output layer. the Eq. (21) can be formulated as follows: w oj  X  t  X  1  X  X  w oj  X  t  X  X  Z y 0  X  y  X  i  X  ^ y  X  i  X  X  h j where t  X  1,2, y , m , and, ^ y m  X  1  X  y .
 a MLPNN model is utilized for evaluation of the nonlinear section of traffic-flow time series. Second, an ARIMA model is expanded for modeling the remaining of the MLPNN model.
 the BP algorithm cannot map the linear part of the data. The results of the ANNs can be led in a proper forecasting. The approach coming from the combination of ARIMA and MLPNN models for determination of different patterns provides the exceptional features. Therefore, this approach can be beneficial for modeling nonlinear and linear patterns individually by the use of different models and integrating the forecasting of them to make the total forecasting performance better. 5. T X  X  neurofuzzy models modified model is provided based on emotional learning concept for updating ANFIS network weights. Multi-objective approach in proposed method can have much better results in forecasting traffic signal with many correlated parameter. The ANFIS models fulfilled the principle of network parsimony and consequently led to a high performance in forecasting. According to the parsimony principle, the best models have simplest structures and finite number of adjustable parameters.

In this study, a new neurofuzzy model, i.e. ANFIS, is introduced to model traffic-flow forecasting. ANFIS is an adaptive network. An adaptive network is network of nodes and directional links.
Connected with the network is a learning rule  X  for instance PB learning algorithm. It is called adaptive because some, or all, of the nodes have parameters which affect the output of the node.
These networks are learning a relationship between inputs and outputs.

Membership Function object can be supposed as the mean to measure the compatibility degree of a data value to a fuzzy set.
The triangular, trapezoidal, Gaussian, and bell shaped are the most commonly used membership functions. Each of them can be applied in different cases. The triangular and trapezoidal mem-bership functions have been used very much, particularly in real-time implementations such as traffic-flow, since they have simple formulae and computational efficiency ( Zaheeruddin and Jain, 2004 ). But, the results show they are not smooth at the corner points specified by the parameters. Although, the parameters of these membership functions can be optimized using direct search methods but they are less efficient and more time consuming ( Himavathi and Umamaheshwari, 2001 ). As these functions X  derivatives are not continuous, the powerful and more efficient gradient methods cannot be applied for optimizing their para-meters. Gaussian and bell shaped membership functions are becoming increasingly popular for specifying fuzzy sets as they are nonlinear and smooth and their derivatives are continuous.
Optimization of their design parameters can be done easily by the gradient methods. Therefore, the bell shaped membership func-tion is applied here. Neural or fuzzy model validation (cross-validation) enables to determine the number of the fuzzy rules, the size of the training set as well as the size of learning-rate parameter in the training procedure. The standard tool in statis-tics to evaluate the adequacy of a fuzzy model (or of the equivalent neural network) and the efficiency of the learning that has been performed is cross-validation ( Amari et al., 1997 ). The available data set is partitioned into a training set and a test set.
The validation sub set is typically 10 X 20% of the training set. The cross-validation is shown in Fig. 8 .
 The structure of a FIS is presented in Fig. 9 ( Zaheeruddin and Garima, 2006 ). The implementation basically depends on five steps: input X  X utput system, data base and rule base (fuzzy table) which forms knowledge base, membership functions, and defuz-zification. The first step depends on traffic-flow time series input. The output is called fitness. The fitness scale is later mapped towards a scale based on traffic characteristics. The second step is a set of rules which defines the behavior between the input metrics. These rules are based on  X  X  X ommon sense X  X . The fuzzy logic translates the linguistic terms into a graph representation. Finally, the defuzzification translates the fuzzy output into a real number output ( Jang, 1993 ).

In Fig. 10 , the theoretical ANFIS based on last model is provided. It is made up of two main mechanisms, specifically, adaptive neural network and fuzzy inference system. The locally linear neurofuzzy model and Takagi X  X ugeno fuzzy inference system form this network.

The ANFIS architecture, the Takagi X  X ugeno model with the subsequent rules is taken into explanation in Eq. (23) Rule i : If x 1  X  a i 1 ... x p  X  a ip then ^ y  X  f i  X  x where i  X  1, y , M and M is the number of fuzzy rules, x 1 crisp function which is defined as a linear combination of inputs. Fig. 10 shows the ANFIS structure of implementing these rules. In this figure, the fixed node and the adaptive node are depicted by circle and square, respectively.

All the nodes located in the first layer are adaptive nodes. This layer outputs are the input fuzzy membership rank. The fixed nodes are in the second layer. They are tagged with P which multiplies the received signals and sends the result out. The nodes of third layer are fixed nodes, too. The nodes are tagged with Q , means that they have normalization task. In the fourth layer, the nodes are adaptive nodes. The output of each node in this layer is simply the product of the normalized firing strength and a first order polynomial. In the fifth layer, there is only one single fixed node which is calculating the weighted global output of the system.

As the mentioned network in Fig. 10 , ANFIS network functions as follows: the x 1 , y , x p are the representative input values added to the p input nodes and gives the output.
 The output of each node in Layer 1 is
O
So, the O 1, i ( x ) is essentially the membership grade for x
The membership function is bell and it performs as details  X  x 1  X  X  where a , b , and c are fitting parameters to be learnt and a fuzzy sets associated with nodes x 1 , y , x p . In order to obtain ( x p ), the same computations are performed for the input of x . By selecting the fitting parameters of the bell shaped member-ship function, the desired shapes can be achieved. More particu-larly, a and c can be set to differ the center and width of the membership function, and b to control the slope at the crossover points. One more degree of freedom than the Gaussian member-ship function is given by the parameter b to the bell shaped membership function. This makes the setting of the steepness at crossover points possible.

The second layer is multiplied in the membership functions as in Eq. (25) u  X  where u i is the output from a node. The i th rule X  X  firing strength ratio is calculated by the i th node of this layer in an inference layer as in Eq. (26) (layer 3) u  X  u 1  X  ...  X  u i  X  ...  X  u M  X  26  X 
The fourth layer nodes used as a weighting factor in the mentioned ratio and use the fuzzy IF X  X HEN rules create the output as the following equation: u f  X  :  X  X  u i where the output f i (.) is a linear combination of the parameter set { p , r }. Parameters f i (.) in this layer are referred to as resulting parameters. In this model, a two-step process is used to adjust the network parameters and to have faster training for the provided network. The BP algorithm is used to modify the initially chosen membership functions and the Least Mean Square (LMS) algorithm determines the coefficients of the linear output functions. Irre-spective of the patterns of training, the membership functions X  number for every input is the necessary user-specified information.
In Eq. (28), the output of the ANFIS network is ^ y  X  b  X  u i 1 x 1  X  u i 2 x 2  X  ...  X  u ip x p  X  28  X  calculated with Eq. 29 ^ y  X  A T  X  X  X  U U  X  29  X  ^ y  X  where u i  X  x  X  is the validity order of the i th rule, u membership function of j th input, and where f i (.) is a linear equation.
 j i  X  x  X  X  u i  X  x  X  P M
Gaussians membership functions and the rule resulting para-meters including the output linear weights in Eq. (28) are two adjustable parameters sets. It is recommended just to optimize the rule resulting parameters. Linear model such as least squares is a good choice for this optimization ( Gholipour et al., 2003a ).
However, the optimization several parameters in cooperation with each other can be done by nonlinear optimization techni-ques. As mentioned, the BP learning algorithm can be used in the optimization of consequent linear parameters. The loss function of Eq. (33) can be minimized with the aim of MSE algorithm: J  X  1 N number of samples.
 the quadratic form as in Eq. (34)
J  X  W T RW 2 W T P  X  X  Y T Y = N  X  X  34  X  where R  X  (1/ N ) A T A is the autocorrelation matrix, A the N p solution matrix in which a  X  x  X  i  X  X  is the i th row and P  X  (1/ N ) A the dimensional vector of cross-correlation. If q J / q W is equal to zero, the minimized form of J is obtained as in Eq. (35) RW  X  P  X  35  X  pseudo inverse.
 optimization technique.
 facing to the cost function gradient as in Eqs. (36) and (37) D W  X  i  X  X  @ J @ W  X  i  X   X  2 P 2 RW  X  i  X  X  36  X 
W  X  i  X  1  X  X  W  X  i  X  X  Z U D W  X  i  X  X  37  X  that the rate of learning is indicated by Z .
 for general T X  X  neurofuzzy model can be reached as follows: W  X  i  X  1  X  X  W  X  i  X  X  Z  X  ^ y i ^ y i 1  X 
S as a set of total parameters, S 1 as a set of nonlinear parameters, and S 2 as a set of linear parameters.

So, ANFIS uses a two pass learning algorithm. First, the S unmodified and the S 2 is computed using a MSE algorithm, which are done in forward pass, and second, the S 2 is unmodified and the
S 1 is computed using a TDBPL algorithm, which are done in backward pass. So the hybrid learning algorithm uses a combina-tion of TDBPL and MSE to adapt the parameters in the adaptive network.
 The summary of the process is given as follows: The forward pass The backward pass
Fuzzy Rule Extraction by Genetic Algorithm (FUREGA) ( Nelles, 1993 ), Adaptive Spline Modeling of Observation Data (ASMOD) ( Jang, 1993 ), ANFIS are the learning algorithms which are antici-pated for optimizing of Takagi X  X ugeno fuzzy inference system parameters and often the names of the neurofuzzy systems are used, such as, SANFIS ( Wang and Lee, 2002 ), DENFIS ( Kasabov, 2002 ) and FLEXNFIS ( Rutkowski and Cpalka, 2003 ), etc.
As mentioned above, for objectives including estimation, control application, identification, fo recasting, and modeling, ANFIS has become very referable. ANFIS has excellent ability of approximation and generalization. ANFIS with one-order Takagi X  X ugeno model has been proved to have universal approximation ability under certain circumstance. ANFIS is used to realize neurofuzzy modeling from numerical data of the traffic-flow. As long as numerical data of the traffic-flow that used to train the model are attained the forecasting model that consists of sets of fuzzy  X  X  X F-THEN X  X  rules will be formed.
In the system inputs are traffic-flow data obtained from January 1, 2009 is used for model training an d the forecasting output is the traffic-flow in the next day ( WEB/I-494 ). 6. Temporal difference brain emotional learning (TDBELBIC)
In this section, the BELBIC is introduced as a forecaster and it is modified by TD learning algorithm. With this modification the performance of this methodology is improved.

Humans have many remarkable capabilities. Amongst them there are two which stand out in importance. First, the capability to converse, communicate, reason and make rational decisions in an environment of imprecision, uncertainty, incompleteness of information and partiality of truth. And second, the capability to perform a wide variety of physical and mental tasks without any measurements and any computations. In the recent years, appli-cations of intelligent learning are increasing complex time series.
In the past years, emotion was assumed as a problem in decision-making and rational behaviors but new studies shows that emotion has main role in the human decision making process and emotional system is supposed as an expert system ( Chen et al., 1999 ). In engineering applications, the simple computation model of some parts of brain is utilized. Making models for emotional behavior of brain is very interesting field for cognitive scientist ( Leung et al., 2001 ).

Several attempts have been made to model the emotional behavior of human brain by Moren and Balkenius ( Balkenius and Moren, 1998 ; Moren, 2002 ). In Moren and Balkenius (2000 ) the computational models of the Amygdala and context processing were introduced. In the computational models of brain, emotions are signals that describe external environment.

In this study, the Brain Emotional Learning (BEL) algorithm is derived from this model to be used in forecasting applications, but based on the cognitively motivated open loop model, BELBIC algorithm was introduced for the first time in Lucas et al. (2004 ) and after that this controller was utilized in several application and control purposes. Applying this controller for eliminating stator oscillations through fin placement was done in Lucas et al. (2002 ). Application of BELBIC in Speed control of an interior permanent magnet synchronous motor was shown in Milasi et al. (2004 ) and in Sheikholeslami et al. (2006 ) a modified version of BELBIC was utilized in heating, ventilating and air conditioning control problem that is multivariable, nonlinear and non-minimum phase. In Milasi et al. (2005 ), this controller used for controlling an identified model of a washing machine and in Milasi et al. (2007 ) this controller with multi-objectives con-straints was tuned for washing machine with evolutionary algo-rithms. Also in Pouladzadeh et al. (2007 ) this controller was applied to automotive suspension system and the results are compared with classical controllers.

Although BEL context was applied as a controller in many simulation and real control tasks and proper forecaster in chaotic time series but the previous knowledge of the Amygdala has not been considered in these studies and the simulation and imple-mentation were done in limited simulation or real time. In this study, the previous knowledge and instability phenomena in BELBIC controller are analyzed and an improved version of it is introduced and applied to forecast traffic-flow. The results are compared with other algorithms.

The emotional processes is done by the Limbic System, as part of the mammalian creatures X  brain. The Limbic System located in the cerebral cortex consists mainly of following components: the Amygdala, Orbitofrontal Cortex, Thalamus, Sensory Cortex, Hypothalamus, Hippocampus and some other less important areas. Fig. 11 shows a simple graphical of limbic system in human brain ( Purves et al., 2001 ).

The Amygdala, which is a small almond-shaped, in the sub-cortical area plays role in the emotional system. This component is placed in a way to communicate with all other Sensory Cortices and areas within the Limbic System. In this section all parts of the Limbic system are described.

The simple BEL model is based on two main interconnected parts: the Amygdala and orbitofrontal cortex. For simplification of modeling, thalamus and sensory cortex are modeled with simple identity function. In real biological models, these parts perform simple preprocessing and signal routing ( Moren, 2002 ). The mathematical description of the Amygdala and orbitofrontal cortex will be presented in the next parts. The BEL model is shown in Fig. 12 . 6.1. Amygdala
The Amygdala is the basic part of the limbic system for conditioning process and it only can learn emotional reaction and cannot forget the learned reaction. Also, the Amygdala is an almond shaped mass of nuclei located deep within the temporal lobes, medial to the hypothalamus and adjacent to the hippocam-pus. The computing rules of this part are presented in Eqs.39 X 41.
There is one node for each element of stimuli in the network model of the Amygdala. The output of each node is calculated by the multiplication of a connection weight v i to s i . The output vector of the Amygdala nodes, A , is presented in Eq. (39)
A  X  V : S  X  39  X  where S is the stimuli inputs to the system and V is the Amygdala diagonal weight matrix.
 The total output is a simple summation of its elements
E  X 
X The learning algorithm is based on a RL signal, presented by R .
The connection weights are proportionally adjusted to the differ-ence between RL signal and the output of the Amygdala
D V  X  diag  X  a : max  X  R E a , 0  X  : S  X  X  41  X  where a is the learning rate of the Amygdala. This is a simple associative system, like the Rescorla Wagner model ( Rescorla and
Wagner, 1972 ). The distinctive feature of the BEL model is that the weights of the Amygdala cannot decrease. In other words, the emotional learning algorithm in the Amygdala is monotonic. 6.2. Orbitofrontal cortex (OFC)
The learning algorithm of orbitofrontal is same as the Amyg-dala learning algorithm. The only difference is that the orbito-frontal connection weight can both increase and decrease as needed to track the required inhibition.

When needed, the OFC will inhabit the Amygdala reaction. It is remarkable that the learning rate in the Amygdala is proportional to the strength of stimuli signals. Meaning the emotions are more sensitive to strong sensory inputs. This property is useful in some of the practical problems, when the large input signals are more important to be estimated or predicted. OFC has the significant role of inhibition. The output nodes of OFC are calculated as the multiplication of a diagonal weight matrix ( W )to S
O  X  WS  X  42  X  and its total output is the sum of all output nodes E o  X  sensory inputs and the internal reinforcement signal of OFC ( R
D W i  X  b S i  X  R O  X  X  44  X  b is the learning rate of OFC.
 following formula:
R o  X  predict and react to a given reinfo rcement signal. This subsystem cannot unlearn a connection. The OFC learns to inhibit the system output when there are mismatches between predictions and observed reinforcement signals. The system output is calculated by
E  X  E a E o  X  46  X  which depends linearly on the action of the Amygdala and the inhibition by OFC.
 degrees of freedom for multi-objective learning algorithms. A simple form is R  X  where r j is the factors of the reinforcement agent and w related weights. An appropriate choice of these variables is very important. The appropriate variables can be tuned as the following. OFC and Amygdala, i.e. the weights are updated in each epochs.
Here by considering that the traffic-flow time series is the input of the OFC part, the TD rule is applied in this part. With respect to
Eqs. (37), (44) and (47), the updating rule of weights in OFC can be formulated as Eq. (48) W  X  i  X  1  X  X  W  X  i  X  X  Z b S i and considering to Eqs. (10) and (37), the updating rule of weights in OFC based on the output can be formulated as follows (TDBEL): W  X  i  X  1  X  X  W  X  i  X  X  Z  X  ^ O i ^ O i 1  X  direct effect of input, S , is ignored and only its effect appeared in out is studied.
 principles should be taken into consideration as the following.
First, Sensory input is punished/reinforced according to emotional cue, so it should be chosen as a function of error. One of the big problems of using BELBIC is the tuning of the weights which and be rectified in this way. In this study at first initial weights can set the learning rates equal to zero, and during the learning algorithm tune the weights of sensory input and then proceed to tune the weights of the other parts of BELBIC in the direction of improving the performance of primary sensory input signal. In this situation
BELBIC has robustness in the case of forecasting chaotic time series such as traffic flow. Moreover, using this signal selection it does not need to concern on effect of noises on identification. So, an identification using less numbers of neurons can easily filter the noises and it will accelerate the online forecasting.
As presented in BELBIC equation, reinforcement occurs when the emotional cue is a positive number and in this situation the gain of the Amygdala connection will be increased, and punish-ment occurs when the emotional cue is a negative number, in this situation the gain of orbitofrontal connection will be increased
The emotional cue should increase when the absolute value of error decreases. It is defined in Eq. (8). However, based on the object of designer, one may choose another rational structure with the general structure of emotional cue. It may not be logical such as 8 but may work better for a certain case study. 7. Locally linear neurofuzzy (LLNF) model
In this section, the LLNF model with Locally Linear Model Tree (LoLiMoT) algorithm and the wavenet architecture for noise reduction are described. The wavenet is a wavelet function that its structure is based on neural network, and it is proposed for approximating arbitrary nonlinear functions as an alternative to feedforward neural networks.

The finite number of observations and the unidentified dynamics of noise (colored noise) are the sources of limitations for forecasting accuracy. So using the proper tool can help and improve the accuracy of forecasting. Here, after studying the LLNF model as a forecaster, the new methodology based on wavenet are introduced for noise reduction. As the results show, the wavenet tool improves the performance of forecasting accuracy.
But it should be considered that an optimal structure for the LLNF model based on WLoLiMoT (WLoLiMoT is the wavenet-based LoLiMoT) may increase the dimension of calculations.

As mentioned decreasing the error and time consumption play important role in short-term traffic-flow forecasting. In general, structure parameter identification and identification are two basic phases on neurofuzzy model ( Lee and Ouyang, 2003 ). The struc-ture identification is identifying an optimal partition of the input space into fuzzy set. The number of fuzzy rule is dependent on partitioning of the input space in neurofuzzy networks. The fuzzy
IF-THEN rule is the common way for partitioning input space, but in this model the number of IF-THEN rules increase exponentially as the dimension of the input space increases and makes curves of dimension ( Takagi and Sugeno, 1985 ). Clustering the input train-ing vectors in the input space is the other common way for partitioning input space ( Baralbiand Blonda, 1999a , 1999b ).
LoLiMoT is greedy and an incremental tree construction algorithm. By axis-orthogonal split, this model is partitioning the input space. For minimizing the error, it greedily adds the new model in every iteration ( Hoffmann and Nelles, 2001 ), so during the learning algorithm some of the formerly made divisions may become suboptimal. For rectify this problem a pruning model merges formerly divided local linear models ( Reed, 1993 ).
The LoLiMoT algorithm was presented by Nelles (1993, 2001 ) ( Holzmann et al., 1999 ) is based on the idea to approximate a nonlinear function with piece-wise linear model ( Nelles, 1993 ). There are some activities in forecasting by LoLiMot algorithms.
Using recurrent network with LoLiMot to make dynamic LoLiMot versus static LoLiMot to forecast Dow Jones index as a financial time series was studied ( Chegini and Lucas, 2010 ). Forecasting the churn in financial process, solar activity forecasting, long-term load demand with LoLiMoT and compared with other algorithms are studied ( Ghorbani et al., 2009 ; Maralloo et al., 2009 ; Gholipour et al., 2003b ). Combines wavelet decomposition and LoLiMoT algorithms, makes low-frequency information which helps to enhance more forecasting accurate. Arani presented new algo-rithms called WLoLiMoT ( Arani et al., 2011 ). The LoLiMoT archi-tecture is shown in Fig. 13 .

Every Local Linear Model (LLMi) takes the input vector u  X  u 1 u 2 ... u N T to compute its connected validity func-The validity functions are chosen as normalized Gaussians; normalization is necessary for a proper interpretation of validity functions f  X   X  u  X  X  exp
The M p parameters of the nonlinear hidden layer are the parameters of Gaussian validity functions: standard deviation ( s ij ) and center ( c ij ).

Optimization or learning algorithms are used to adjust the two sets of parameters, the rule consequent parameters of the locally linear models and the rule premise parameters of validity functions.

The LLMs X  outputs are ^ y  X  w i 0  X  w i 1 x 1  X  ...  X  w ip x p  X  52  X  where w ij is the LLM parameters for neuron i . The validity functions form a partition of unity such that
Finally the output of a local linear neurofuzzy model becomes ^ y  X 
The sum of all local linear model outputs, ^ y i , is the total network output.

LoLiMoT can be considered as an extended version of RBF network. Every neuron represents a LLM with its consequent validity function. It is an incremental tree-construction algorithm that partitions the input space by axis-orthogonal splits. A new IF-THEN fuzzy rule is added to the model in every iteration.
Therefore, LoLiMoT belongs to the class of incremental. It uses a heuristic search algorithm for the rule principle structure and avoids a time-consuming nonlinear optimization. In every itera-tion the validity functions which matched with the real input space X  X  partitioning are compared and the equivalent rule con-sequents are optimized by the local weighted least square technique ( Nelles, 1993, 2001 ). The model construction algorithm has an upper level which determines the parameters for nonlinear partitioning of the input space and an lower level which estimates the parameters of those LLMs ( Holzmann et al., 1999 ; Nelles, 1993, 2001 ).

The one-step-ahead forecasting errors are weighted with the corresponding basis function ( Halfmann et al., 1999 ). Certainly the overlap between neighbored local models is ignored and every LLM is calculated separately ( Halfmann et al., 1999 ; Nelles, 1993 ).
The standard deviations are chosen proportional to the size of the hyper-rectangle and it makes the size of the validity region of a
LLM proportional to its hyper-rectangle extension. A model may be valid over a wide operating range of one input variable but only in a small area of another one ( Holzmann et al., 1999 ).
The LoLiMoT algorithm is presented in 5 levels as the following ( Nelles, 1993, 2001 ): 1. Start with an initial model: Based on Eq. (55) the validity functions for the initially given input space partitioning is constructed and the local weighted LS algorithm is used for estimating the LLM parameters. In the case of empty input, space partitioning is a priori, so it begins with a single LLM with f 1  X  u  X  X  1 as the validity function w i  X  X  X where
X  X 
Q i  X  diag  X  f i  X  u  X  1  X  X  , f i  X  u  X  2  X  X  , , f i  X  u  X  N  X  X  X  2. Find worst LLM: For every of the i  X  1, y , M LLMs, a local loss function is calculated. The loss function is as following:
J  X  where N is the number of training data points, e is the error, y neuron. This step finds the worst performing LLM, that is, arg  X  max 3. Check all division: The further modification of LLM is consid-ered. The hyper-rectangle of this LLM is split into two halves with an axis orthogonal split.

Divisions in all dimensions are tried. For every division, dim  X  1, y , p , the following steps are considered: a. Construction of the multidimensional membership func-b. Construction of all validity functions; c. Local estimation of the rule consequent parameters for both d. Calculation of the loss function (2 X 4) for the current 4. Find best division : The best of the p alternates according to 5. Test for convergence : If the termination criterion is met then and also a greedy strategy. It minimizes in every iteration and improves the performance in the next iteration. Fig. 14 demon-strates splitting procedure in LOLIMOT algorithm.
 detecting and denoising and they are particularly well-matched for studying non-stationary signals. They are localized in both the time space and frequency space. Recently, wavelet threshold denoising is widely used on non-stationary signals. A theoretical framework for denoising signals using Discrete Wavelet Trans-form (DWT) is developed by Donoho and Johnstone (1994 ). This algorithm is simple and it has good results on stationary and non-stationary signals. The model applies the DWT to the traffic flow signal in two steps: (a) thresholding the detailed wavelet coeffi-cients and (b) inverse transforming the set of thresholded coefficients, to achieve the denoised signal. It is proved that these wavelet threshold denoising methods have the optimization properties ( Jian and Zhang, 2008 ). The wavelet can only use the same base in the whole process of signal analysis once selected and it causes the leakage of signal energy. According to decom-posed series, the original physical meaning of the signal will be lost. In this study, instead of using the common sigmoid activa-tion functions, the wavelet neural network (WNN) employing nonlinear wavelet basis functions has been developed as an alternative approach to nonlinear fitting problem and a denoising method for short-term traffic flow forecasting based on LoLiMot is presented.
 algorithm to be used for the training of the WNN are two main problems of the WNN designing? These problems are related to determine the optimal architecture of the WNN, to arrange the windows of wavelets, and to find the proper orthogonal wavelets basis such as Daubechies wavelets.

The Haar wavelets are presented for forecasting the financial time series ( Arani et al., 2011 ). The Haar wavelets are applied in multi-resolution of this application. There are a wide variety of popular wavelet algorithms, such as Daubechies wavelets ( Daubechies, 1992 ), Mexican Hat wavelets and Morlet wavelets ( Mallat, 1998 ). These wavelet algorithms have the advantage of better resolution for smoothly changing time series than Haar wavelets. But they have the disadvantage of more computational overheads. Curse-of-dimensionality is a mainly unsolved problem in WNN theory which brings some difficulties in applying the
WNN to high-dimension problems. In this study, the huge computational overheads due to Daubechies 4 (D4) occurrence in WNN model are solved using the TDL algorithm. D4 has a number of zero moments or vanishing moments equal to half the number of coefficients. For example, D2 (the Haar wavelet) has one vanishing moment, D4 has two, etc. A vanishing moment limits the wavelet X  X  ability to represent polynomial behavior or information in a signal. D4 encodes polynomials with two coefficients, i.e. constant and linear signal components. Scaling and wavelet functions for D4 is shown in Fig. 15 .

The following equation defines the Daubechies scaling func-tions f D 3 : f  X  x  X  X  where N is the length of the coefficients and here N  X  4 . So non-zero values of the two-scale sequence { p k } are p k  X  p 0 , p 1 , p 2 , p 3  X 
The following is the two-scale equation of Daubechies wavelets: c  X  x  X  X 
Therefore the non-zero values of the two-scale sequence { q } are q k  X  q 2 , q 1 , q 0 , q 1  X  p 3 , p 2 , p 1 , p 0
WNN and LoLiMoT-based forecasting method can be imple-mented as follows: 1. Decomposition of time series with multi-resolution analysis : suppose given time series Q ( t ) by the wavenet decomposition of level j and approximation c j and detail sections d i ( i  X  1, c j and d i reconstruct the main signal Q ( t ) Q  X  t  X  X  c j  X  2. Generation of forecasting model using LoLiMoT : generate the forecasting model for the wavenet transformation approxima-tion section and detail sections by using LoLiMoT. 3. Regeneration of forecasting process : forecasted values of time series Q ( t ) is obtained using local neurofuzzy forecaster. of approximation and detail sections, respectively. Regenera-tion of each section can be used as the final forecasting result ^ Q  X  t  X  X  ^ c j  X  The diagram of proposed model is shown in Fig. 16 .

The construction of networks and the minimization of error are the two processes of wavenet algorithms. In the first process, the network structure applied for representation of a given function is determined using wavelet analysis. Functions in hidden units are gradually selected by the network to cover effectively and sufficiently the time X  X requency region occupied by a given target. At the same time, the network parameters are updated to maintain the network topology and take advantage of the later process. In the second process, the approximations of immediate errors are minimized using an LMS-based adaptation technique. The parameter of the initialized network is updated using the BP method of minimization. In the time X  X requency plane, there is a square window in each hidden unit. The optimization rule is only applied to the hidden units where the selected point falls into their windows. Thus, the time consumed for training can be reduced ( Zhang and Benveniste, 1992 ). The effectiveness of this technique is guaranteed by two important properties of wavenets: (1) the sparsity of the wavelet coefficient matrix and (2) the decorrelation of the wavelet coefficients. 8. Short-term traffic-flow forecasting 8.1. Case study
Data used in this study are obtained from a real case. The test of the proposed traffic state forecaster is done with real traffic measurement data obtained from a 2-lane eastbound stretch of the I-494 freeway in Minnesota, USA. This simulated part of infrastructure is shown in Fig. 17 . The spatial location of the site under study is shown in Fig. 18 too. The test stretch is divided into 13 segments and it started from station 702 to station 708 (each with a length of 200 X 550 m). It had one on-ramp and one off-ramp while 9 detector stations (bold bars) are installed along the stretch.

The length of every detector (bar line) form station 702 to station 708 is: Length ( m )  X  [448 288 528 328 328 378 378 378 456 456 192 280 280]. Data recorded by the detectors are transformed into aggregated traffic factors measurements of flow and space mean speed at every minute. The minimum resolution for fore-casting is 1 min. Flow and speed measurements of September 1, 2009 are utilized for testing. In traffic state forecasting the measurements from stations 702, 705, and 708 are used to feed the forecaster. 8.2. Evaluation formula for model of forecasting appropriate performance indicators to measure the quality of models. The following error data, therefore, are computed in this study: Mean Absolute Percentage Error (MAPE), Normalized Root Mean Square Error (NRMSE), Variance of Absolute Percentage Error (VAPE), and Max Absolute Percentage Error (Max APE).
According to Chen and Grant-Muller (2001 ), MAPE is applied in traffic forecasting as following: MAPE  X  1 N where y and ^ y are observed data and forecasted, respectively, and N is the total number of samples.
 MSE (NRMSE) which is specified in Eq. (64)
NRMSE  X  Error (VAPE) to check forecasting stability in Eq. (65):
VAPE  X 
The four evaluation indicators are the key indicator to evaluate predictive effect in the traffic flow forecasting. MAPE is a com-prehensive indicator to evaluate the whole forecasting process;
NRMSE is not only able to respond to the error size, but also describe the concentration and degree of dispersion about the error distribution, the greater the mean square error, more discrete the error sequence, the forecast results is worse; VAPE reflect the accuracy and stability of the forecaster; Max APE responds to the maximum error in the forecasting. 8.3. Forecasting the traffic-flow time series
The time series model in traffic-flow indicating the trend of demand for traffic behavior has been a good field for testing different models of modeling and forecasting in a short or long time depending on the research purpose. For instance in real time and on line decision system, short-term traffic-flow is needed and for the management systems the long-term traffic-flow is required. The traffic-flow is the basis for many case studies and it has close relationship to the other traffic models. With fore-casting the flow we can controls the path suitably.

In this study, ETD learning algorithm is applied to forecasters that updated the ANNs X  weights. According to the differences in networks, this learning algorithm is tested over 6 different net-works to find some proper situation in short-term forecasting in real circumstance. The traffic-flow in station 708 is trained and the tested networks are shown in Figs. 19 and 20 , respectively.
They demonstrate that traffic-flow had a chaotic behavior within the period of a single day.

Experientially, the NRMSE, MAPE and VAPE cover all error indexes; therefore, they are the criticizers of the forecasted data in this study. At first all networks are learned by the trained data and then the test data are applied to the networks. Results demonstrated the models which are based on ANNs could achieve better results.

Initially in order to improve ANN-based neurofuzzy forecaster and ARIMANN-based neuro stochastic forecaster, the EL algorithm is applied. By feeding the error signal and the error change rate as inputs in ANFIS and ARIMANN networks, the emotional signal is computed with semantic FIS and MLPNN model. The training is performed by the least number of epochs and fuzzy rule. In this study, there are numerous trade-offs between learning algo-rithms. In order to remove the effects of the initial parameters on the final results, for each case, the simulation is done till the errors in test set began to enlarge. Fig. 20 shows the desired and forecasted values after using EL. For adjusting the weights in neurofuzzy model that is started and set by ANFIS, the emotional algorithm is applied.

According to the results provided in the tables, it is observable that ARIMANN, T X  X  neurofuzzy based on ANFIS, BELBIC and LLNF models with ETD learning algorithm generated the most accurate forecast in traffic-flow maximum; however the error indexes of ELFIS are the least. Obviously, forecasting of the peak points with small errors instead of the points in minimum regions is vital. This is resulted from the critic X  X  emotions in the traffic-flow forecaster.
Furthermore, ETD learning algorithm is applied for improving predictive characteristics of the ANNs which are educated by BP (MLPNN, ARIMANN and T X  X  neurofuzzy models).The emotional signal is computed by subtraction of past and present forecasting before annual data calculation took place. The mentioned MLPNN model, which is used in ARIMANN model with the specified neurons of Table 1 being educated by TD (Lambda) principle, shows that the indexes are considerably reduced.

The subsequent results are given out by comparing the excellence in forecasting the traffic-flow by ETD learning algo-rithm with BP learning algorithm, and the algorithms tested by MLPNN, ARIMANN, T X  X  neurofuzzy models (with ELFIS and ANFIS learning algorithm), BELBIC and LLNF models. The optimal func-tion of all models is applied. Observation of test data NRMSE error index through training can prevent overfitting problem. For implementing the model input, three regressors are applied. In Table 1 , the models features are displayed.

As Tables 1 and 2 show, every model has specific setup based on its structure. These setups are obtained from trial and error for the best results. The domain of the presented learning algorithms is shown in Table 2 .In Tables 3 and 4 , the forecasting accuracy and the computation time comparison between different models for the trained data are presented and in Tables 5 and 6 , the forecasting accuracy and the computation time comparison between different models for the trained data are available. The forecasting system is developed with double layer
MLPNN-based ETD learning algorithm (the layers are input and hidden layers). The emotional signal is the difference between two prosperous forecasts. Regarding to Eq. (12) and TDL algo-rithm this signal is useful in updating network weights. Based on the easy formulation to conclusion, as shown Table 6 , the BELBIC is approximately lower than other models in speed, but the results is not satisfactory. In accuracy, LLNF model with WLoLi-MoT has better results than the other ones, drastically.
As ANFIS and ELFIS are built on Takagi X  X ugeno FIS, the emotional signal is calculated using a fuzzy critic that weighted errors, rates of error change and the final desired output are the basis of it as the semantic rules. Therefore, the overstated emotions, which are produced by demands from the path, can be shown by the critic in the maximum areas of traffic-flow and forecasting this area is more important because of its conse-quences on entire path.
 sing time of MLPNN, T X  X  neurofuzzy models (with ELFIS and
ANFIS) and BELBIC model are done approximately ten times quicker than the other models and the accuracy of LLNF model error is noticeable. When using the emotional signal functional narration instead of its fuzzy narration, quicker algorithm could be generated but it is not easy to find such a function. By considering TDL algorithm in BELBIC model, the increase in error index is noticeable.

For evaluating the LLNF model in noisy environment, the simulations are done without noise and with additional noise.
When a white noise with standard deviation (SD) of 0.1 is added to both train and test data, the error of all optimal models will increase. Based on these results of a real case, the LLNF model with
WLoLiMoT algorithm has better generalization properties, but in the noisy environment it seems that the LLNF model with LoLiMoT learning algorithm is more robust. The performance of LLNF model without noise and with additional noise is shown in Table 7 .
To conclude, one can improve the accuracy of forecasting by lowering the forecasting error of each step, which is highly dependent on the accuracy of the model and the training model. Of course, when applied to a real case, the number of available training samples and the presence, power, and model of noise will have effects on the results. The effect of additional white noise on the computation time of the LLNF model and its forecasting accuracy is considered. As a result, the power of noise has a direct effect on the NRMSE test data. The wavenet is more robust in the presence of noise signal. 9. Conclusion
There are different parameters in the structure of neural and neurofuzzy models which have to be optimized for obtaining accurate and reliable forecasters. The factors which enforce us to use complicated learning algorithm during training are achieving maximum generalization and avoiding overfitting for decreasing the computational time. These factors are important for the noisy and uncertainty real environment.

The T X  X  neurofuzzy models have been modified and improve the forecasting traffic-flow toward several objectives and requirements by EL algorithm which intends to minimize an emotional signal. This emotional signal can be deducted as emotions of a critic in order to supervise the forecasting task i n training course. Even though optimizing a loss function rather than errors in emotional signals achieve more accurate results in desired values or in accordance with important features. Defining ap propriate performance indexes causes improvement in demanded results.

Here, the some proposed models such as T X  X  neurofuzzy trained by EL (ELFIS or ANFIS) and LLNF model trained by LoLiMoT and WLoLiMoT are used as a traffic-flow forecaster. In a real case forecasting, by considering on the traffic-flow highest point on a day and minimizing the criteria, the emotional signal is specified and in comparison of MLPNN, ARIMANN, T X  X  neurofuzzy, BELBIC and LLNF models, it has enhanced outcomes. Integrating errors and rates of error change leads to late overtraining of neurofuzzy model and more accurate forecasting. The definition of emotional signal is a main support in EL algorithms that approximates the proposed networks X  weights and provides high degrees of freedom.
A hybrid model based on ARIMA and ANNs (ARIMANN) can be efficient to make the forecasting accuracy better by the individual models used independently as these models which are capable of handling both linear and non-linear components of traffic-flow signal. However, some researchers believe that assumptions considered in the construction process of this hybrid model can affect the performance. In addition, it cannot be generally guar-anteed that the performance of the designed models based on this hybrid model will be better than their nonlinear component. These assumptions may be separately modeling the linear and nonlinear patterns of a time series by different models; addition of the relationship between the linear and nonlinear components; and being nonlinear relationship of residuals.

In this study, the proposed hybrid model in order to combine the linear and nonlinear models that has no above-mentioned assumptions will result in a more general and accurate forecasting model. These results confirm that in contrast to the classical hybrid linear and nonlinear models, the performance of the designed models will not be worse than using each components separately, so that it can be applied as an appropriate model for combination of linear and nonlinear models for time series forecasting.
The proposed model of forecasting time series using multi-resolution analysis and LoLiMoT has higher accuracy without affecting computational time. In fact, this model with decomposing the time series into its components breaks the complex system into simpler levels and then uses the forecasting algorithm based on LoLiMoT. The LoLiMoT is an incremental learning algorithm.
However, the D4 wavelet transform is more accurate, since change in the input data set is reflected in the high pass filter results at each transform step but it is a more complicated algorithm and the cost of In comparison with other neurofuzzy learning algorithm, EL algorithm is a model free method with four distinctive features: 1. There is no concern about the differentiability to render the ability of the recursive equation issues; 2. It uses the emotional signal complex concept but has no increased computational complexity of EL algorithm; 3. It is efficient and fast in computation and training; and 4. the best performance can be reached by simple setting of the parameters ( Nelles, 2001 ; Fatourechi et al., 2001 ).
The significant advantage of introducing the EL was arrange-ment of crucial aspects in real world application ( Gholipour et al., 2003a ). In short-term traffic-flow forecasting, the fluctuation makes some peak points which are more important than the other features. Forecasting these peak points before they happen has a lot of advantages in traffic literature. In order to reach the goals, the error and delay of forecasting error and delay may be considered in definition of emotional signal. Besides, model training and low computational complexity might be amongst other achievements using the Daubechies algorithm is higher in computational overhead. The computational overhead problem is solved by applying TDL in this model.

The TDL algorithm is introduced as a training model for the networks based on ANNs. By taking the differences between two successful past forecasts, the researchers implemented multi-step forecasting. This paper introduced dynamics in the training procedure by emphasizing the latest data over earlier ones, while avoiding too much computational burden in each step of the recover process. The total computational algorithm thus lacked the curse of dimensionality inherent in dynamic programming and fully rational decision-making models. The achievements of these new TDL algorithm did not require more computation per time-step than did classical algorithms such as BP, and they are more efficient in a statistical sense because they extract more information from training experiences. 10. Future works  X  For evaluating the capability of the mentioned models in other engineering applications, they can be applied on other times Acknowledgments sor Caro Lucas for his invaluable contributions to the overall analysis of emotional learning algorithm theory in this project.
We also wish to thank the anonymous reviewers for their careful review of the paper and their comments which greatly enhanced the paper quality.
 References
