 Domain experts are frequently interested to compare clustering results of two or more related datasets. For example, meteorologists may want to understand the change in this year X  X  sea water temperature patterns with respect to those observed in previous years. Zoologists may attempt to relate animals X  habitats and their source of foods. We can use traditional clustering algorithms to cluster each dataset separately and the following reasons: In this paper, we introduce a novel spatial clustering approach called correspondence clustering . Correspondence clustering clusters two or more spatial datasets by taking the correspondence between the different clustering into consideration. Therefore, the obtained clusterings relate to one another; that is, the clustering of one dataset depends produced by traditional clustering algorithms are reduced. Moreover, the hidden rela-tionships between related clusterings can be discovered. 
Applications for correspondence clustering include: Challenges to develop a good correspondence clustering framework include: The main contributions of the presented paper include: In this section, we propose a correspondence analysis framework. Basically, our The challenges of discovering interesting patterns in spatial data include the complex-autocorrelation. Moreover, spatial space is co ntinuous and contains many patterns at different levels of granularities. 
Let us assume that a set of related spatial datasets O={O 1 ,...,O n } are given. We are our framework seeks for clustering results that maximize two objectives: (1) the inte-restingness in each clustering, (2) the correspondence between the set of obtained clusterings. Correspondence clustering is defined as follows. Definition 1. A correspondence clustering algorithm clusters data in two or more 1  X  X maximizes the correspondence Corr(X 1 ,...,X n ) between itself and the other clusterings X for 1  X  j  X  n , j  X  i . In summary, correspondence clustering can be viewed as a multi-objective optimiza-tion problem in which the interestingness of clustering and their correspondence are maximized. Moreover, different interestingness functions i and correspondence func-tions Corr can be used for different correspondence clustering tasks. In the next section, a representative-based corresponde nce clustering approach is introduced. The approach allows for plug-in fitness functi ons that are capable to capture different interestingness functions i and correspondence functions Corr . Since our representative-based correspondence clustering approach employs a region discovery framework, we first introduce the region discovery framework. 3.1 Region Discovery Framework The region discovery framework [1] gears towards finding scientifically interesting places in spatial datasets. The framework adapts clustering algorithms for the task of region discovery by allowing plug-in fitness functions to support variety of region discovery applications corresponding to different domain interests. The goal of region discovery is to find a set of regions X that maximize an externally given fitness func-tion q(X) ; q is assumed to have the following structure: where i(c) is the interestingness of a region c and | c | is the number of objects belong-trolled by parameter  X  (  X  &gt;1 ). 3.2 Representative-Based Co rrespondence Clustering Algorithms In general, representative-based clustering algorithms seek for a subset of the objects in the dataset  X  called the  X  representatives  X  X  X nd form clusters by assigning the remaining objects to the closest representative. In this section, representative-based correspondence clustering algorithms are introduced. The proposed algorithms are modifications of a region discovery algorithm named CLEVER [2]. CLEVER is a representative-based clustering algorithm that applies randomized hill climbing to maximize the fitness function q . Figure 1 gives the pseudo-code of CLEVER. 
Given two datasets O 1 and O 2 , the goal of correspondence clustering is to discover The compound fitness function q  X  (X 1 ,X 2 ) is defined as follows: where q is a fitness function that assess the quality of X 1 and X 2 . The correspondence parameter  X  is a user-defined parameter. The correspondence function Corr(X 1 ,X 2 ) measures the correspondence between X 1 and X 2 . 
CLEVER is modified to maximize the compound fitness function q  X  instead of the fitness function q. Two approaches that implement correspondence clustering are introduced in the following: (1) The Inte rleaved approach (C-CLEVER-I), and (2) The Concurrent approach (C-CLEVER-C). The algorithms of C-CLEVER-I and C-CLEVER-C are given in Figure 2 and 3, respectively. 
The C-CLEVER-C uses the compound fitness function (equation 2) to cluster two data sets concurrently. For the C-CLEVER-I, dataset O 1 and O 2 are clustered one at a time X  X ot concurrently X  therefore, the compound fitness function (equation 2) sim-plifies to (3) and (4) when clustering the first and second dataset, respectively. In general, there are many possible choices in selecting initial representatives of C-CLEVER-I and C-CLEVER-C. Our current implementation supports three options: The first option is to randomly select a subset of size k X  from O as in CLEVER. The second option uses the final set of representative R from the previous iteration as the the dataset O as follows: There are many choices for termination condition ( TCond ). The possible choices are: (1) fix the number of iterations; (2) terminate the program if the compound fitness function in the present iteration does not improve from the previous iteration. Using agreement between two clusterings X 1 and X 2 is a popular choice for a corres-pondence function. In applications such as change analysis [7] or co-location mining some extent. In such case, Agreement(X 1 ,X 2 ) would be used as the correspondence function. In addition, domain experts might be interested to discover regions with disagreement between the two datasets in anti-co-location or novelty detection. In the later case, Corr(X 1 ,X 2 ) can be defined as (1-Agreement(X 1 ,X 2 )) . For the remaining of the paper, Agreement(X 1 ,X 2 ) will be used as correspondence function Corr(X 1 ,X 2 ) ; in the section we will introduce a measure to assess agreement. 
First, we introduce re-clustering techniques that use the clustering model of one tering, the cluster models are sets of representatives. Given two clusterings X 1 and X 2 representatives of one dataset to cluster the other dataset. More formally:  X  REC (O,R) denotes the result of re-clustering dataset O using the set of representatives R . The clusters of  X  REC (O,R) are created by assigning objects o  X  O to the closest rep-resentative r  X  R obtaining |R| clusters. backward re-clustering. To assess cluster similarity, the similarity between two representative-based cluster- X 
REC (O 2 ,R 1 ). To assess the similarity of two clusterings, we construct a co-occurrence matrix M X for each clustering X of O as follows: Let M X and M X X  be two co-occurrence matrices that have been constructed for two clusterings X and X X  of the same dataset O ; then the similarity between X and X X  can be computed as follows: 
Sim(X,X X ) in equation (5) is a generalization of the popular Rand Index [8] that ad-ditionally takes outliers into consideration. Finally, we define agreement between spective forward and backward re-clusterings as follows: 
The advantage of the proposed agreement assessment method based on re-clustering techniques and co-occurrence matric es is that it can deal with: (1) datasets with unknown object identity, (2) different number of objects in two datasets, (3) different number of clusters in the two clusterings. Therefore, we claim that it is suit-able for most types of spatial data. In the first experiment, we show that correspondence clustering provides comparable or better results than the traditional clustering. Moreover, the experimental results show that by enhancing agreement between two corresponding datasets, correspon-dence clustering produces clusterings that have lower variance than a traditional clus-tering algorithm. In the second experiment, we evaluate and compare different cluster initialization strategies for correspondence clustering. 5.1 Earthquake Dataset and Interestingness Function U.S. Geological Survey Earthquake Hazards Program http://earthquake.usgs.gov/. The data includes the location (longitude, latitude), the time, the severity (Richter magnitude) and the depth (kilometers) of earthquakes. We uniformly sampled earth-quake events from January 1986 to November 1991 as dataset O 1 and earthquake events between December 1991 and January 1996 as dataset O 2 . Each dataset contains 4132 earthquakes. 
Suppose that a domain expert interests in finding regions where deep and shallow earthquakes are in close proximity; that is, he/she is interested in regions with a high variance for the attribute earthquake depth. The following interestingness function captures the domain expert X  X  notion of interestingness. where  X  X  X   X   X , X   X   X   X  |  X  |  X  X   X  X  X   X   X   X   X   X  X   X   X   X   X  X  X  (8) captures what degree of earthquake depth variance the domain expert find news wor-the reward function form parameter. 5.2 Experiment Investigating Variance Reduction We run the interleaved approach of the representative based correspondence cluster-ing, C-CLEVER-I, and the traditional clustering algorithm, CLEVER, and compare the results with respect to cluster quality and agreement. 
First we run CLEVER on dataset O 1 and O 2 five times to generate five clusterings for each dataset. Then we run C-CLEVER-I for five iterations with  X  =1.0e-4 and  X  =2.0e-6 for five times each. Figure 4 summarizes the experiments conducted. Each CLEVER show that fitness values ( q(X 1 )+q(X 2 ) ), and Agreement(X 1 ,X 2 ) of CLEVER are computed from all twenty five possible pairs of X 1 and X 2 . When correspondence obtained by C-CLEVER-I (one for each run; indicated by solid lines with two ways arrows). For each clustering of C-CLEVER-I, the representatives from the previous tion. The parameter settings of CLEVER and C-CLEVER-I are shown in Table 1 and Table 2. All parameters for CLEVER and C-CLEVER-I are set to the same values each pair of clustering X 1 and X 2 , for a fair comparison, we set the p and p X  of CLEV-ER to be five times higher than C-CLEVER-I. The experiment is evaluated by fitness function (equation (1)), agreement (equation (6)) and similarity (equation (5)). Table 3 shows average values of all the experimental results. The computation time meas-ures the average wall clock time in milliseco nds used by the algorithms to generate a pair of clusterings X 1 and X 2 . We use similarity measure Sim(X,X X ) in equation (5) to assess variance between two clusterings generated using the same dataset. In general, the algorithm that produces higher Sim(X,X) creates clusterings that are more similar in different runs, thus, exhibiting lower variance. 
From Table 3, C-CLEVER-I with  X  =1.0e-5 produces higher fitness values for clustering X 1 and but lower fitness values of X 2 than CLEVER. For Agreement(X 1 ,X 2 ) and Sim(X 1 ,X 1 ) , C-CLEVER-I with  X  =1.0e-5 produces slightly higher values than CLEVER but for Sim(X 2 ,X 2 ) , C-CLEVER-I produces significantly higher value than indicate than each run of C-CLEVER-I creates more similar clustering results for each clustering X 1 and X 2 which means that C-CLEVER-I produces lower variance than CLEVER. With  X  =2.0e-6 , C-CLEVER-I is forced to emphasize agreement. lower than CLEVER but Agreement(X 1 ,X 2 ) , and Sim(X 2 ,X 2 ) of C-CLEVER-I are sig-nificantly higher than CLEVER. Moreover, C-CLEVER-I computes its results about half of the runtime CLEVER uses. 
From the experimental results, we conclude that correspondence clustering can re-duce the variance inherent to representative-based clustering algorithms. Since the two datasets are related to each other, using one dataset to supervise the clustering of the other dataset can lead to more reliable clusterings by reducing variance among clusterings that would have resulted from using traditional representative-based clus-Moreover, obtaining higher agreement could be accomplished with only a very slight decrease in cluster quality. 5.3 Experiment for Representative -Based Correspondence Clustering with We run experiments to compare results of three initialization strategies for C-CLEVER-I; the three tested strategies are as follows : (1) random representatives (C-CLEVER-I-R), (2) representatives from the nearest neighbor of representatives of the counterpart clustering (C-CLEVER-I-C), and (3) the final representatives from the previous iteration are used as the initial representatives for the next iteration (C-CLEVER-I-O). For each option of the initial representative setting techniques, five pairs of clustering X 1 and X 2 are generated, similar to the previous experiment. Table 4 shows parameter settings used in the experiments. The average values of the expe-rimental results are shown in Table 5. 
From Table 5, C-CLEVER-I-C produces clusterings with the highest agreement but the lowest compound fitness value. This is because C-CLEVER-I-C uses initial representatives that are closest to the representatives of its counterpart clustering. Then C-CLEVER-I-C generates clusterings X 1 and X 2 that are very similar which results in very high agreement. Though, the agreement is very high, the low fitness values lead to the low compound fitness value. For C-CLEVER-I-O, the initial repre-sentatives used are the final representatives from the previous iteration. In contrast to C-CLEVER-I-C, with  X  =2.0e-8 , C-CLEVER-I-O favors increasing fitness values rather than increasing agreement between the two clusterings. This is indicated by the highest fitness values but the lowest agreement value. As for C-CLEVER-I-R, it pro-duces comparable fitness values and intermediate agreement value but consumes the highest computation time. This is due to the fact that C-CLEVER-I-R randomizes its initial representatives, which allows the algorithm to explore the dataset more tho-roughly than the others but in return, it needs more time to find the solution. Correspondence clustering relates to coupled clustering, and co-clustering which both cluster more than one dataset at the same time. Coupled clustering [3] is introduced to discover relationships between two textual datasets by partitioning the datasets into corresponding clusters where each cluster in one dataset is matched with its counter-part in the other dataset. Consequently, the coupled clustering requires that the num-intra-dataset similarity and concentrates solely on inter-dataset similarity. Our ap-proach, on the other hand, provides no limitation on number of clusters. It considers both intra-dataset and inter-dataset similarities. The intra-dataset similarity is included through interestingness measures and the inter-dataset similarity is included through correspondences in sets of representatives. 
Co-clustering has been successfully used for applications in text mining [4], mar-ket-basket data analysis, and bioinformatics [5]. In general, the co-clustering clusters two datasets with different schemas by rearranging the datasets. In brief, datasets are represented as a matrix with one dataset is organized into rows while the other dataset is organized into columns. Then, the co-clustering partitions rows and columns of the spatial data mining, re-organizing the data into data matrix causes spatial relationships contiguous. Accordingly, co-clustering is not applicable to spatial clustering. 
Correspondence clustering is also related to evolutionary clustering [6] that is used for multi-objective clustering. Evolutionary clustering clusters streaming data based on two criteria: the clustering quality of present data and its conformity to historical data. 
In conclusion, the three reviewed clustering techniques cluster data based on dis-tances alone whereas the correspondence clustering approach allows to cluster multiple datasets based on a domain expert X  X  definition of interestingness and correspondence. Consequently, correspondence clustering is more general and can serve a much larger earthquake clustering problem we used in the experimental evaluation; in the experi-ment, clusters are formed by maximizing the variance of a continuous variable and not by minimizing the distances between objects that belong to the same cluster. In this paper, we introduce a novel clustering approach called correspondence clustering and correspondence between clusters for the different datasets. A representative-based correspondence clustering framework is introduced and two representative-based cor-respondence clustering algorithms are proposed. We conducted experiments in which two datasets had to be clustered by maximizing cluster interestingness and agreement between the spatial clusters obtained. The results show that correspondence clustering can reduce the variance inherent to representative-based clustering algorithms. Moreo-ver, high agreements could be obtained by only slightly lowering clustering quality. In general, correspondence clustering is beneficial for many applications, such as change analysis, co-location mining and applications that are interested in analyzing particular, domain-specific relationships between two or more datasets. 
In addition, the paper proposes a novel agreement assessment measure that relies on re-clustering techniques and co-occurrence matrices. The agreement assessment technique proposed is applicable for (1) datasets with unknown object identity, (2) clusterings. Therefore, it is suitable for most types of spatial data. 
