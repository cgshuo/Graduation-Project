 Test collection design eliminates sources of user variability to make statistical comparisons among information retrieval (IR) systems more affordable. Does this choice unnecessarily limit generalizabil-ity of the outcomes to real usage scenarios? We explore two aspects of user variability with regard to evaluating the relative performance of IR systems, assessing effectiveness in the context of a subset of topics from three TREC collections, with the embodied information needs categorized against three levels of increasing task complex-ity. First, we explore the impact of widely differing queries that searchers construct for the same information need description. By executing those queries, we demonstrate that query formulation is critical to query effectiveness. The results also show that the range of scores characterizing effectiveness for a single system arising from these queries is comparable or greater than the range of scores arising from variation among systems using only a single query per topic. Second, our experiments reveal that searchers display substan-tial individual variation in the numbers of documents and queries they anticipate needing to issue, and there are underlying significant differences in these numbers in line with increasing task complexity levels. Our conclusion is that test collection design would be im-proved by the use of multiple query variations per topic, and could be further improved by the use of metrics which are sensitive to the expected numbers of useful documents.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  performance evaluation .
 User behavior, test collections, relevance measures
In the Cranfield and TREC paradigm, information retrieval test collections (consisting of a corpus, topics, relevance judgments, and a relevance measure  X  collectively representing a sample of some population of a real-world information retrieval task) allow com-parative system performance experiments to be carried out. This approach, sometimes referred to as batch evaluation , assesses the system or algorithmic aspect of relevance, as defined by Saracevic [26] . Almost all sources of variability are removed in this classical design of test collections, including users and tasks, leaving topics as the primary source of variability within the collection. The rele-vance measure encodes (either explicitly or implicitly) an abstracted model of user behavior, and rewards systems which deliver relevant material more efficiently or comprehensively according to the model. Statistical assessments of comparative effectiveness as determined by the relevance measure can be used to determine improvements in algorithm design. Statistical power analysis calculations can be used to determine the number of topics needed to quantify the probabilities of making type I and type II errors.

An important aspect of test collection use that has perhaps been under-investigated is the degree to which they have external validity . Crudely put, external validity characterizes the extent to which an experiment (typically relative system effectiveness according to the relevance measure in the case of test collections) generalizes to other real world circumstances. Such circumstances might encompass use by different users (stay-at-home parents vs retired intelligence analysts) or over different document sets (a subset of the Web vs a collection of news articles) or for different interaction tasks (factoid question answering vs ad hoc topical information discovery). We propose some potential properties of a test collection that relate to the degree to which they have external validity. These include: fidelity  X  whether the relative effectiveness of systems is consistent when a population of users/topics/documents/task (of which the test collection is a sample) uses the systems; corpus-generalizability  X  whether outcomes from this test collection are consistent across other test collections modeling the same task, as investigated by Robertson and Kanoulas [24] ; task-generalizability  X  whether out-comes from this test collection are consistent across other test col-lections with different tasks (i.e., generalizability across situations); and user-generalizability  X  whether outcomes are consistent in the presence of different user behaviors for the same task and topics (i.e., generalizability across people). We provide brief definitions for some other key concepts in Figure 1.

In this work, we are motivated to improve the user-generalizability property of test collections. In particular, we seek to understand how introducing some simple sources of variability in users, namely individual query formulation and expectations of the quantities of relevant information needing to be found, might affect how test collections are constructed, and how batch evaluations are carried Task An information seeking activity.

Topic A description of information content or
Task complexity A categorized model of cognitive
T Expected number of useful documents
Q Expected number of queries required to
Q02, R03, T04 Our subsets of the topics and judgments AP, ERR, NDCG, RBP p , Q  X  Figure 1: Definitions of significant terms and abbreviations used. out. We use the lens of task complexity (discussed below) to help assess these issues across a range of information seeking scenarios.
To examine user-generalizability in a batch evaluation setting, we pose a series of research questions assessed in the sections below. RQ1 Does the existence of individual variation in initial query formulation for a single information need alter the evaluation of system performance? (Section 4) RQ2 Is there significant variation among users of the anticipated effort in terms of the number of documents viewed and queries to be issued, and is there a relationship between a user X  X  anticipated effort and the information task complexity? (Section 5) RQ3 Does incorporating anticipated effort within adaptive met-rics lead to changes in relative system performance assessments? (Section 6) The overarching issue we consider is: to what extent do measures of system effectiveness depend on (lack of) variation in user behavior and thus do test collections have insufficient user-generalizability ?
We are not the first to consider this issue. In a 1977 report on the design for an ideal test collection, Sp X rck Jones and Bates [29] recommend that:
The effects on the retrieval of relevant documents of such variations over requests should be counteracted by the use of additional queries specifically designed to exhaust the relevant document set.
 The 1999 TREC Query Track examined sets of queries for topics, and the coordinators Buckley and Walz [8] similarly conclude that:
We X  X e reaffirmed the tremendous variation that sometimes gets hidden underneath the averages of a typical IR experiment.
Topics are extremely variable; queries dealing with the same topic are extremely variable. . . ; and systems were only somewhat variable.
 In a comprehensive study examining different types and sets of judges as the source of user variability, Voorhees [32] found that the TREC-4 and TREC-6 collections were reasonably stable in relative outcomes for participating systems, both for similar users X  judgments and different users X  judgments. She also observed that inter-system comparisons required more substantial differences in measure scores than for intra-system comparisons. More recently, Bailey et al. [4] examined consequences of using relevance labels originating from judges of differing task and topic expertise. They found that variation in expertise levels led to consistent differences in relevance outcomes and also to questions about the robustness of relative system performance measures over the TREC Enterprise 2007 test collection. Kazai et al. [16] confirmed that such systematic bias between different kinds of judge may exist.

The project described here encompasses exploration of just two (among many) aspects of user variability, thereby to connect user experiences more closely with batch evaluation outcomes. Task complexity In information science, the complexity of a search task has long been recognized as having an important impact on information seeking behavior and use, including for example the type and complexity of information needed, and the number and diversity of sources consulted [31].

Bystr X m and J X rvelin [9] proposed a five-level task complexity taxonomy, ranging from automatic information processing tasks (tasks that are completely determinable so that they could in theory be automated) to genuine decision tasks (unexpected, unstructured tasks). This taxonomy was refined into three levels by Bell and Ruthven [7] , with the distinction between levels being based primar-ily on the initial determinability and clarity of the task.
Focusing more directly on task complexity in the context of in-teractive information retrieval, Wu et al. [35] proposed a hierarchy based on the Cognitive Process Dimension of Krathwohl X  X  Taxon-omy of Learning Objectives [ 17 ], which is itself a refinement of Bloom X  X  Taxonomy of educational objectives. Through a user study, Wu et al. demonstrated a tendency for participants to spend more time, issue a greater number of queries, and click on more search results for tasks with greater cognitive complexity. We use three levels of this taxonomy for our experiments, explained below. Factors that influence searcher behavior Wu et al. [36] investi-gated the relationship between information scent (signals of rele-vance on a search results page) and search behavior such as query re-formulation, search depth and stopping, demonstrating that a higher density of relevant items on the first page increases the probability of query reformulation, and decreases that of pagination.
The relationship between constraints and searcher behavior was studied by Fujikawa et al. [13] , who showed that when the number of queries that a searcher can enter is restricted, greater attention is given to query formulation and more time is invested in viewing search results pages. Similar effects were observed when constraints were placed on the number of documents that can be viewed.
Azzopardi et al. [3] studied the effect of query cost on the be-havior of searchers, examining the influence of different interfaces, designed to require differing amounts of effort. Users of the  X  X truc-tured X  (highest cost) interface displayed different behavior, sub-mitting fewer queries and spending longer when examining search result pages. A strong relationship between searcher behavior and task type and structure was also reported by Toms et al. [30] , with users showing different rates of query reformulation and page views.
In a focused study, White and Kelly [34] varied the threshold acquired from individual document examination times as an input to an implicit relevance feedback algorithm, across a number of individuals and search tasks. They found that there was substantial variation in individual examination times, and that it was possible to improve relevance performance by using task information to determine the threshold. Attempts to tailor the threshold on a per-individual basis led to degraded performance however, suggesting intra-task-consistency was higher than intra-individual-consistency.
Gwizdka and Spence [14] examined observable measures of in-formation seeking activities (such as documents viewed, time spent etc.) of a set of psychology students within a laboratory setting. They characterized relationships between the objective operational-ized task complexity (in a manner influenced by [ 7 ]) and subjective searcher assessments of task difficulty with respect to these observ-able measures, and analyzed which measures were more important in predicting the difficulty experienced by the searcher. They found that task complexity affected both the relative importance of these predictors and the subjective assessment of difficulty. They also observed that individual variation (in factors like experience, ver-bal ability, other cognitive abilities etc.) played an important part in affecting performance and relative assessment of difficulty. We use individual variation in query formulation and expected goals of search to examine how batch evaluation outcomes change, and use task complexity as an analysis factor.
 Query variability Searchers use an IR system to resolve an infor-mation need. To do so they need to translate their internal informa-tion requirement into an explicit query that is submitted to the search system. Multiple queries can represent a single information need, and indeed a single user may issue multiple queries within a single search session. Finally, interactive query (re-)formulation systems are increasingly common and have been demonstrated to assist in improving retrieval performance by (among others) Kumaran and Allan [18] . In that work, the authors also demonstrate how program-matic query expansion or relaxation can lead to significant increases in performance, across a selection of TREC test collections.
The 1999 TREC Query Track [ 8 ] investigated the issue of query variability through the creation of 23 query  X  X ets X , alternative query statements corresponding to 50 TREC topics. Analysis confirmed previous research showing that differences between topics intro-duces substantial variability into IR experimental results, and fur-ther showed that the variability of queries dealing with the same topic also introduced significant variability, typically greater than differences between retrieval systems. However, Buckley and Walz note that formal conclusions cannot be drawn from the full data set, due to the presence of  X  X lundered queries X  and the presence of multiple versions of the same basic system [ 8 ]. Other investigations of query variability in the TREC setting were shown to improve query performance through data fusion [5, 6].

Modave et al. [20] carried out a study of the quality of health-related information related for people seeking information about weight-loss using Google. While measuring query variability was not a focus of the study, this effect was accounted for by gen-erating a range of queries about the weight-loss topic, eliciting specific queries from 20 study participants as well as the Google auto-complete feature.
 Evaluation metrics Batch evaluations rely on objective scoring of search response listings. Long-standing mechanisms include Recip-rocal Rank (RR); Precision at depth k ; and Average Precision (AP), the average of the precisions achieved at the depths in the ranking of the relevant documents. A wide range of further alternatives have been developed over the last decade, including Normalized Discounted Cumulative Gain (NDCG) [ 15 ]; Rank-Biased Preci-sion ( RBP ) [ 22 ]; Expected Reciprocal Rank ( ERR ) [ 10 ]; and the Q-Measure [ 25 ]. Per-query scores from one or more of these metrics are then averaged in some way, and paired statistical tests applied in order to draw experimental conclusions.
 Metrics are sensitive to system performance in different ways. Precision at 10 and RBP with parameters less than about p = 0 . 8 are  X  X hallow X  metrics, and hence better match the behavior of a typical web search user than do  X  X eep X  metrics such as AP , NDCG , and the Q-Measure. In terms of judgment effort, shallow metrics are also cheaper to evaluate than deep metrics. On the other hand, deep metrics tend to lead to a higher fraction of statistically significant system differences being identified (the discrimination ratio), and to be just as predictive of the behavior of shallow metrics as are the shallow metrics themselves [ 33 ]. Moffat [21] provides further commentary on ways effectiveness metrics can be categorized. User goals and persistence Users vary in the way they process search response pages, and hence if a metric is to reflect the user X  X  perception of their experience, should be sensitive to that variation. Moffat and Zobel [22] argued that a metric should match a user model , a description of the behavior of the presumed user; and pa-rameterized their RBP metric with a persistence parameter p . Rather than quantifying persistence in terms of documents, Smucker and Clarke [28] used time as the primary persistence factor in the model, and make use of data from a user study to calibrate their gain calcu-lations. Moffat et al. [23] note that users may have differing goals, even for the same query or same information need, and introduce the notion of an  X  X xpected goal of search X , their parameter T , and use it to shape predictions about what happens when that user is viewing a page, thereby creating a more refined user model that in turn leads to further alternative effectiveness metrics. A user study provided evidence to support that hypothesis, bringing user and batch evalua-tions a step closer. We build on their work by examining how user variation in queries and expected goals can be combined. Next we describe our overall experimental framework.
Search can be viewed as a process that starts with an information need, out of which a particular query is formulated by a user and submitted to a retrieval system. However, batch evaluations typically start with a single query per information need and regard the system as being the primary variable that impacts on effectiveness. Our experimental framework attempts to reintroduce two aspects of user variability into the batch evaluation process. We start by describing the process we adopted for formulating information need statements that could then be used to investigate user-generalizability. Information needs To investigate user generalizability, several aspects of searcher behavior were studied through a crowd-sourced experiment. We first required a set of labeled search tasks for the experimental participants to carry out. To obtain a broad cross-section of information-seeking tasks, a set of 180 TREC topics was selected:
For each topic, a backstory was created; this was a short informa-tion need statement that was intended to motivate and contextualize the search request, making the topic statements less abstract and more engaging. Four annotators created the backstories, based on the full original TREC title, description and narrative fields. They were also free to explore related background information using on-line resources. An example topic from each of the three TREC tracks is shown in Figure 2. To encourage our eventual experimental participants to engage more fully with these search tasks, and to treat them as personal searches rather than abstract impersonal ones, Q02.1828, Remember ;  X  X hat was Thailand X  X  original name? X 
While visiting Thailand for a beach holiday last year, you decided to visit some local museums to learn more about Thailand X  X  history. You learned many interesting things about the country, including that it was not always called Thailand. What was it called originally? R03.356, Understand ;  X  X ostmenopausal estrogen Britain X  A friend, who lives in Britain, has started estrogen treatment. This surprises you as you thought it X  X  no longer recommended.
You want to find out more about the use of hormone replacement therapy or estrogen treatment in the U.K.
 T04.734, Analyze ;  X  X ecycling successes X 
Your city has recently embarked on an ambitious zero-waste policy for household and industrial garbage. Recycling is going to be a big component of the program. You wish to find out what recycling projects have been successful, including the places or product programs that have worked, and what they understood success to mean.
 Figure 2: Backstory associated with three TREC topics from differ-ent tasks in different years, together with the task type. Number : 734 Rec ycling successes Description : What recycling projects have been successful?
Narrative : Guidelines by themselves are not relevant. Titles in a table of contents are relevant if they identify places or product programs which have had success. Must be declared successful or success should be clearly assumed from the description. Name of state identified as successful recycler is relevant. Listing of recycled products for sale are relevant.
 the backstories were written to speak directly to the reader, and to include hypothetical family members or friends. Figure 3 shows the original TREC presentation of one of the topics shown in Figure 2.
The original topic statements from the Terabyte and Robust tracks contain substantial detail about what information a document should or should not contain to be considered relevant, and the created backstories aimed to reflect the bulk of these requirements. Nev-ertheless, we acknowledge that there is potential for drift between the interpretations of the backstory and the original TREC topic de-scription that led to relevance judgments being created. Topics from the QA Track were more difficult as they are typically presented simply as question statements, such as  X  How much gravity exists on mars?  X  (Q02.1871). Simply posing the question statement to the experimental participants might lead to these being entered directly as a search query, rather than then being read as an information need statement, so the QA topics are also presented with a backstory. When possible, pronouns or other indirect references to the query subject were used, to reduce the likelihood that participants would simply copy and paste the final question as their query. Task complexity Different information-seeking tasks have differ-ent characteristics, and task complexity is a key feature that may influence searcher behavior. For our experiments we adapt three levels from the cognitive complexity hierarchy proposed by Wu et al. [35] , derived from a taxonomy of learning objectives presented by Anderson and Krathwohl [2] . This hierarchy considers a spectrum of information needs, with the lowest level consisting of searches that involve  X  X etrieving, recognizing, and recalling relevant knowl-edge X  . Such Remember queries therefore involve finding a fact in response to a simple  X  X hen X ,  X  X here X  or  X  X hat X  question, such as  X  How did Eva Peron die?  X . The next level in the hierarchy, Un-derstand , involves  X  X onstructing meaning from oral, written, and graphic messages through interpreting, exemplifying, classifying, summarizing, inferring, comparing, and explaining X  . We also use a third level, Analyze ; tasks at this level of the hierarchy involve  X  X reaking material into constituent parts, determining how the parts relate to one another and to an overall structure or purpose through differentiating, organizing, and attributing X  . Each of the example tasks in Figure 2 indicates the corresponding complexity category.
Based on the created backstories, each of the 180 selected TREC topics were assigned to one of the task complexity types. Four anno-tators independently rated each topic: broadly speaking, topics that required a simple factoid answer tended to be assigned to the Remem-ber category; where topics required the production of list of things, even if relatively complex and sourced from different pages, they tended to be assigned as Understand ; and, where topics required synthesis of disparate information, and eventual summary, or bal-ancing of competing viewpoints and opinions, they were allocated to the Analyze category. 2 The overall inter-annotator agreement among the four judges for the initial ratings was 0 . 664 , measured by Fleiss X   X  [ 12 ], a statistic that measures agreement across multiple raters and corrects for agreement by chance. It is interesting to note that the per-category agreement varied substantially, from 0 . 456 for the highest of the three hierarchy levels ( Analyze ) to 0 . 907 for the lowest level ( Remember ), indicating that Remember tasks are rela-tively easy to identify and agree on, while differentiating between Remember and Analyze tasks is more difficult. For all cases where there was no majority rating among the four annotators, the tasks were carefully discussed until agreement was reached, resulting in a single confirmed type for each.
 Gathering data To investigate user variability in a test collec-tion setting, an experiment was carried out using the CrowdFlower crowd-sourcing platform. 3 The experiment was reviewed and ap-proved by the &lt;anonymous institution&gt; ethics board.
On signing up for the experiment, a participant was first presented with an information need statement, one of the created backstories. They were then required to answer three questions. First, partici-pants were asked: How many useful web pages do you think you would need to complete the search task? . Responses were selected from the following: 0 useful pages (I X  X  expect to find the answer in the search results listing, without reading any of the pages); 1 useful page (I X  X  expect to find the answer in the first useful page I found); 2 useful pages; 3-5 useful pages; 6-10 useful pages; 11-100 useful pages; 101+ useful pages. Second, they were asked: In total, how many different queries do you think you would need to enter to find that many useful pages? with answers selectable from the following: 1 query (I X  X  expect to be able to complete the search task after the first query); 2 queries; 3-5 queries; 6-10 queries; 11+ queries. Third, participants were asked: What would your first query be? ; answers to this question were entered in a textbox. Participants were free to complete as many topics as they liked, from one to a maximum of 180. The resulting data set had 10 , 800 responses from 115 workers, Cleaning crowd data It cannot be expected that all anonymous number of topics 70 81 29 av erage queries per topic 44.3 44.4 44.0 average query length (chars) 25.9 32.9 37.1 average query length (words) 5.6 6.0 6.5 average query entropy (bits) 19.9 26.0 30.5 Table 1: Query properties after normalization: average query length in characters, not counting white-space characters; average query length in words; and average query entropy in bits. To calculate the last, the frequency distribution of words appearing in the queries for each topic was computed, and then the average information cost of representing the queries for that topic computed using that frequency distribution, and averaged over task complexities. workers took their task seriously, and where it was possible to iden-tify clearly inappropriate responses, those workers were removed from further analysis (but still paid). First, if any worker suggested the same first query for two or more tasks, they were considered unreliable and all their responses were removed  X  recall that no worker got the same task twice, so it is extremely unlikely that two tasks would attract identical queries. This rule removed 15 of 115 workers. Two further workers who had copy/pasted apparently nonsensical parts of the topic statement as their first query were also identified and removed. This left 7 , 971 responses from 98 workers, covering all 180 topics with 41  X  48 responses per topic (median 44 ).
Having described the data collection process, we first examine the sets of queries suggested by the experimental subjects. Normalization One of the components in each interaction pane asked  X  What would your first query be?  X  Workers then entered text in to a textbox. As with all web queries, the resultant strings are noisy, with a wide range of spelling and grammatical errors. In this regard, the behavior of the crowd workers probably corresponds closely to other users. To ameliorate this type of behavior, web search systems include a  X  X id you mean? X  query modification fea-ture. To faithfully reflect that behavior, the query strings typed by the crowd-sourced subjects were converted to US English, and cor-rections applied whenever they could be unambiguously identified. For example,  X  X heropy X  was changed to  X  X herapy X  in the context of topic R03.356 (Figure 2). In some cases the correction was not clearcut, or the erroneous word was actually a correct spelling of something different. Manual interactions with a major search engine were used to decide whether to alter these queries. For example,  X  X heapskate bay X  was altered to  X  X hesapeake bay X , because that is what happened at a web search interface. On the other hand,  X  X al-gary provicence X  was altered to  X  X algary providence X  rather than  X  X algary province X , which would have better fitted the topic in ques-tion, because the first alteration was what was suggested at the same search interface. As a further part of the normalization process all punctuation characters were removed, including periods. Finally, two queries ( X  X dvfdzfvg X  and  X  X xghfsdg X ) not caught by the earlier quality-control mechanisms were removed. The resulting query set contained 7 , 969 queries, of which 5 , 046 were unique. Query diversity Table 1 lists some properties of the queries re-ceived, averaged over the three query classes after quality control and normalization mechanisms were applied. The table shows a clear trend to longer queries as the information need becomes more city recycling projects (2) city recycling scheme progress council website most successful recycling programs recycling policy update recycling projects (2) recycling projects for household and industrial garbage recycling projects program recycling projects successes and effects recycling projects that have been successful recycling successes reducing waste to zero success stories successful city recycling policies successful municipal recycling projects successful recycling programs (2) successful recycling projects (11) successful recycling projects place product programs successful zero waste what are the recycling projects that have been successful what does it take to make a successful recycling program what recycling projects have been successful (6) where have recycling projects been successful and zero waste policy (2) zero waste policy for household and industrial garbage zero waste policy for household and industrial garbage programs Figure 4: The 44 user-generated queries for Topic 734 (Figure 3). Numbers in parentheses indicate multiplicity. complex, both in terms of characters typed and in terms of words typed. The final row of the table represents the average diversity of the terms across the pool of queries generated for each topic, by computing the term frequencies of all terms used in queries for that topic, then calculating the entropy of each query relative to that distribution, and finally averaging those average entropies. The entropy of a query increases as the length of the query increases, and is also high if a broad set of term is being used across the pool of queries for that topic  X  if queries are less predictable. This measure confirms that the more complex the information need, the more expressive are the queries posed to resolve it.

As a single example, Figure 4 lists the complete set of queries generated for one of the 180 information need statements (see Fig-ure 2). One query dominates  X  an extended version of the TREC title-only query for this topic,  X  X ecycling successes X   X  but nearly half of the queries generated by the subjects occur only once. Query effectiveness  X  In the small Two different retrieval systems were then used to execute each query against the corresponding document collection: Indri 4 with an Okapi similarity computation, and Indri with a sequential dependency computation [ 19 ]. Using Indri for both ranking algorithms ensures the system effects are due to fundamental differences in the retrieval algorithms, rather than other factors related to query or document processing. Rankings of length 200 documents were generated and scored; with documents for which no judgment was available deemed to be not relevant.
Figure 5 shows the range of scores that resulted when four stan-dard relevance measures were applied to the rankings for the set of 44 queries generated in response to Topic 734 (Figures 2 and 4), with the Indri Okapi BM25 and SDM ranking functions. The blue diamonds show the corresponding scores for the canonical TREC Figure 5: Retrieval effectiveness measured by AP , NDCG , Q1 and RBP0 . 85 for Topic 734. Green and blue boxes show scores obtained from running the different user queries using BM25 and SDM, respectively. Grey points indicate the mean for each column, black bars the median, and the blue diamonds show the effectiveness of the corresponding TREC title-only query. title-only query (again, see Figure 2) when evaluated using the same two retrieval mechanisms. The results for this one query reflect a trend that we also saw more widely  X  that for typical user queries the mean performance of SDM is superior to that of Okapi. That difference is consistent, but not absolute, and for most combinations of metric and topic there are also queries for which Okapi out-scored SDM. On Topic 734 the title-only query was the highest-scoring query (of the 44 ) for AP , NDCG , and Q1 for both Okapi and SDM models, and also the highest-scoring for RBP0 . 85 for Okapi (the query  X  X uccessful recycling projects place product programs X  scored 0 . 703 when the SDM similarity model is used), but this topic was unusual in that regard. For example, for Topic 356 (Figure 2), more than half of the user-generated queries outperformed the canonical title-only query. The omission from the corresponding backstory of the word  X  X ostmenopausal X , which appears in the TREC topic description ( X  X dentify documents discussing the use of estrogen by postmenopausal women in Britain X ), may have had an effect. Some level of unintentional topic drift is always possible in our process.
A risk factor in any experimentation in which judgments are re-used is the extent to which they provide coverage of the documents retrieved by the systems being compared. For Topic 736, the RBP residuals when p = 0 . 85 are 0 . 518 and 0 . 469 for Okapi and SDM, respectively. These represent the assessment weight of the unjudged documents in RBP [ 22 ], with 0 . 0 representing a situation with all required judgments available, and larger values indicating that the RBP score would increase by that much if all unjudged documents were in fact relevant. For Topic 734 the average residual for the user-generated queries was in excess of 0 . 25 , and the available judgments covered less than 75 % of the RBP probability mass. That is, the relativities shown in Figure 5 need to be taken cautiously. With more complete judgment coverage, the RBP scores for this topic (and hence the scores for other metrics) might change considerably. Similar situations were encountered for several other topics. On the other hand, the title-only queries have consistently low residuals, because they were used by some of the systems that contributed to the pools from which the judgments were created.
 Query effectiveness  X  In the large The Q02 queries were espe-cially prone to the problems arising from sparse judgments, with Figure 6: Retrieval effectiveness as measured by AP , NDCG , Q1 and RBP0 . 85 , for two subcollections R03 and T04. Blue boxes show scores obtained from running different user queries with SDM retrieval, while red boxes show scores achieved by different TREC contributing system runs. Grey points indicate the mean for each column, black bars the median, and the blue diamonds show the effectiveness of an Indri SDM run using the corresponding TREC title-only query. The average residuals for the four RBP measure-SDM runs had RBP residuals of 0 . 017 for R03, and 0 . 056 for T04. RBP0 . 85 residuals that averaged around 0 . 5 over that set of 70 top-ics. One reason may be that QA relevance was focused on answer fragments, not document-level relevance. Coverage was somewhat better on the R03 and T04 queries (but with considerable variation, as already noted).
 Figure 6 incorporates score information derived from all of the TREC participating systems that contributed runs for the Robust-03 and Terabyte-04 tracks. The boxes show variation over the corre-sponding R03 and T04 topics of score responses to user-generated queries (blue) and score responses across the set of contributed TREC runs for that year (red), factoring in all of the user-generated queries when evaluated using the Indri SDM model. The average of the Indri scores for the title-only queries for those topics is also marked on each bar. It is clear that query-derived variations are just as broad as are the variations caused by system diversity, and hence that improved performance relative to the Indri SDM title-only runs is thus equally likely to be derived from query reformulation as it is from system improvement. Note also that for the user-generated queries (blue boxes) there is a considerable amount of metric weight still sitting in the residuals, which might be released with further judgments, and result in higher scores.
 Variability analyzed The effect of query choice is illustrated fur-ther in an analysis of variance for each metric, modeling score as a response to topic, system, and query. In this analysis  X  X opic X  is a nominal variable, one level per TREC topic;  X  X ystem X  has one level for each TREC system, plus two levels for our Indri runs; and  X  X uery X  has one level for all TREC systems plus one for each query processed by Indri. (We do not know the exact query used by each TREC system, but by assuming it is always the same we will underestimate the variability due to query phrasing and overestimate that due to system.) Q02 looks very different, as discussed above, and those runs are not included in this analysis since these measures are document-relevance centric. Table 2: ANOVA for four metrics, modeled as a response to system, topic, and query string. Partial  X  2 values reported; all F statistics significant at p  X  . 001 . In each case, the effect due to query phrasing is substantially larger than that due to topic or system.
Table 2 summarizes the results. Each of query, system, and topic has a statistically significant effect ( p  X  . 001 in all cases, Wald test) and the effect of each factor is medium/large, with the possible exception of topic for RBP, but the effects are of very different scales. The variation due to system is slightly larger than that due to topic (e.g. partial  X  2 of 0.20 and 0.14 for AP), so slightly more variation in final score is explained by changes to system than by changes to topic. The variation due to query phrasing, however, dwarfs other effects and over 50% of variation in final score can be attributed to phrasing even after system and topic are taken into account (partial  X  2 in the range 0.53 X 0.59).
 Observations Particular choices of query clearly can lead to widely different scores, independent of the topic, the system, or the metric. We commonly want to use variation in scores to say something about differences between systems (i.e.,  X  X ystem B is better X ); less commonly, we want to use variation in scores to say something about topics (i.e.,  X  X opic 734 is hard X ). In either case we need to be aware of query wording as a confound, and an extra source of variation which in fact dominates system and topic.

Two macro implications can be drawn from this analysis regarding test collection design and development. First, since this approach supplies between one and two orders of magnitude more queries for a given set of topics within a collection, even given some cross-query document overlap when judging pools on a per topic basis, this would sharply increase the required judging budgets. Given finite budgets, this implies that measures that accommodate missing judgments such as RBP or the suite of inferred AP and NDCG measures developed by Yilmaz and Aslam [37] are required and/or more cost-effective judgment acquisition methods such as crowd-sourcing approaches (e.g., as discussed by Alonso et al. [1] ) should be employed. Second, systems could be provided with each topic X  X  collection of queries, and can then make use of any methods desired to create a single top-K ranking for the topic. Document pools would be formed in the usual way, but on the scale of number of topics, not number of queries. In the absence of search engine logs, this might provide some partial subset of the data that is available to commercial search providers about variant phrasing, and hence techniques such as pseudo relevance feedback or query reformulation merging [27] could be employed.
 Table 3: Significant factors in fitted models for estimates of T and Q . Effect sizes &gt; 1 correspond to higher values of T or Q being more likely. All effects significant at p &lt; 0 . 05, Wald test.
As well as differing in their behavior (issuing different queries, in our example), users may have different expectations of a search system and of a task and it would be appropriate to consider this when evaluating search systems. For example, if one user expects to issue a query then read three or four documents  X  perhaps to compare information from different sources  X  then it would not be appropriate to evaluate based on the rank of only the first relevant result. If another expects to issue several queries in succession, then it may be appropriate to evaluate a session rather than a single question. Other, similar, scenarios are easy to imagine. Two questions in our instrument aimed to understand some of these varied expectations. Expected number of documents Cooper [11] noted that (p.31)  X  X ost measures do not take into account a crucial variable: the amount of material relevant ... which the user actually needs X . Following Moffat et al. [23] , we denote this quantity by T . Cooper further observes (p.33):
A search request is therefore to be conceived in the abstract as involving two parts: a relevance description (normally a subject specification) and a quantity specification. To put it another way, every search request has a definite quantification. To understand this quantification and how it varies, we asked:  X  X ow many useful web pages do you think you would need to complete the search task? X . We plot the responses for each task complexity category in Figure 7. The distribution of responses across the three types of task are significantly different (  X  2 = 2067 . 0 , df = 12 , p  X  . 001 ), and it seems that descriptions of more complex tasks prompt people to expect more reading.

Clearly estimates of T vary with task complexity, and they may vary with other factors as well. Some of these factors are captured in our instrument, and some are external and not captured. To clarify some of the instrument-captured factors, we used cumulative logistic regression (also called ordinal regression) to model T as a response to a number of potential explanatory variables: complexity (three levels), worker (98 levels, or one per worker), topic author (four levels), and CrowdFlower run (two levels). Model selection was performed to minimize the Akaike information criterion (AIC), which measures likelihood but with a penalty for complex models.
The model is summarized in Table 3, where effects are given as multipliers to odds ratios. Effects greater than 1 represent a higher probability of answers higher up the scale. For example, a multiplier of 2 would mean the odds of estimating T as 1 vs estimating T as 0 are twice as high as the baseline; likewise, the odds of estimating T as 2 vs either 1 or 0 would be twice the baseline, and so on.
The largest effects are due to the CrowdFlower workers. Our workers varied substantially, with a worker at one extreme claiming they expected to read documents for only five out of 65 topics and one worker at the other extreme expecting to read 11 or more documents in every case. This is reflected in the model, where per-worker effects are highly variable and again dominate all other effects  X  odds ratios change by six orders of magnitude for T .
There is a smaller but still notable effect due to task complexity, with Understand tasks more likely to prompt higher estimates of T and Analyze more likely still  X  the difference between Remember and Understand being larger than that between Understand and Analyze . Finally, there is an effect due to topic author: even after controlling for task complexity and worker, some authors provoked higher T estimates, an effect that was statistically significant, but practically negligible. We also checked for batch effects (the tasks were released to workers in two rounds), but they were not evident. Expected number of queries Users may also vary in the number of queries they expect to issue  X  be it a single query, if they think a task is simple or well-supported, or very many in succession, if they think the task is more complex. We denote this expected number of queries Q and plot it in the right-hand side of Figure 7. As before, per-judge effects dominate  X  the variability here is even more pronounced than for T , with odds ratios varying across thirteen orders of magnitude. Clearly different users have very different expectations of their search engine interactions, even for the same topic. There are again significant differences across complexity levels, with similar effects to those seen for T . Again the difference between a Remember and an Understand task is larger than the difference between Understand and Analyze . We also note a significant but small effect due to topic author, and no significant batch effect.
 Observations Just as with individual variation in query formation, we observed significant individual variation in expectations of doc-uments and queries to satisfy that need. Given that T is correlated with task complexity and is strongly influenced by user-specific expectations, a natural question to ask is whether it is possible to include T  X  and progress made towards attaining T as a ranked list is consumed  X  into an relevance measure in a meaningful way. In the next section, we develop this possibility using the data we collected.
Current effectiveness metrics are insensitive to both T , the ini-tial expectation of the user, and to the evolving expectation of T as the search is prosecuted. For example, while the p parameter of RBP [ 22 ] provides adjustment for user persistence and can be adjusted so as to influence the expected depth in the ranking that the user will examine, the user model associated with RBP requires that the user proceeds to the next document with fixed probability, regardless of how much information has already been accumulated, or what depth has been reached. We believe that as more relevance is accumulated, the user becomes less likely to continue their search. Expected search length Cooper X  X  definition of ESL [ 11 ] is simple: it is the total number of documents inspected before T relevant ones have been found. As such, it is always greater than or equal to T , with larger values representing inferior performance. To obtain a metric with the usual behavior (bounded by zero and one, with larger values indicating better performance) we scale by T and invert, to obtain an ESL-and RR-inspired metric: As is the case with RBP, Prec, and RR, the score returned by this metric corresponds to the average rate at which utility (relevance) is acquired per document inspected, with a user model defined by a per-son who seeks exactly T relevant documents, and stops their search immediately upon finding a T th answer in the ranking. Queries that have fewer than T answers in a system X  X  ranking are scored as zero. There is a clear connection between RRT and precision  X  RRT is equal to precision at the T th relevant document. Note also that RRT ( 1 ) is exactly reciprocal rank, RR.
 Probabilistic users A second option is to form a probabilistic com-posite of RBP and ERR [ 10 ]. Suppose that the user makes a biased decision after encountering each relevant document, continuing to scan with probability p = ( T  X  1 ) / T , and ending their search with probability ( 1  X  p ) = 1 / T . In this user model the expected utility per document inspected is given by: The geometric distribution means that the average number of rel-evant documents identified by the time the user stops scanning is 1 / ( 1  X  p ) = T , and that the value of ERRT is non-zero even if fewer than T relevant documents appear in the run. There is also a clear relationship between ERRT and AP, the latter being an unweighted sum of all R precision scores: AP = ( 1 / R )  X  R t = 1 RRT ( t ) , where R is the number of relevant documents for that topic. A key difference between AP and ERRT is that computation of the latter does not require knowledge of R . Nor is AP sensitive to T , of course.
Moffat et al. [23] have also considered effectiveness metrics that are sensitive to T . Their INSQ and INSQ  X  functions are weighted precision metrics defined in terms of the conditional probability C ( i ) Table 4: Expected search length for INSQ -based metrics for dif-ferent values of T , when no documents in the ranking are relevant (column  X  X pper X ); and when every document is (columns  X  X ower X ). of the user continuing from the document at depth i in the ranking to the document at depth i + 1. They define INSQ and INSQ respectively, where T i is the amount of relevance (or gain) that has not yet been accumulated by depth i , T i = T  X   X  i j = 1 0  X  r i  X  1 is the relevance of the i th document in the ranking. Both versions of INSQ are sensitive, in that higher values of T lead to more patient search behavior and a greater expected depth in the ranking. In addition, INSQ  X  is adaptive  X  as relevant documents are identified, the expected remaining search cost decreases. INST: An improved INSQ  X  In the formulation of Moffat et al. [23] , T is required to be positive. We remove that restriction, and allow T to be negative too, covering situations in which more gain has been accrued than was initially anticipated. The altered metric, still using the continuation function C  X  ( i ) , is denoted as INST . Table 4 shows why we prefer this change: it lists expected search depth for INSQ , INSQ  X  and INST in two extreme situations  X  when no documents in the ranking are relevant, and when every document in the ranking is relevant. As is evident in the table, INSQ is not adaptive, and has the same behavior in both extreme situations, examining an average of 2 T + 0 . 5 documents. On the other hand, the expected search length in INSQ  X  and INST decreases if relevant documents are encountered. Our preference for INST over INSQ  X  is based on its expected search length of approximately T + 0 . 25 when all documents are relevant, intuitively a better fit than the approximately 1 . 4 T expectation of INSQ  X  . That is, INST anticipates that a user seeking T relevant documents will examine, on average, between T and 2 T documents before leaving the ranking, with the actual exit depth depending on the number (and locations) of the relevant documents. In addition, INST retains the other features that made INSQ a more representative model for user behavior [ 23 ]. Compared to the INSQ / INST variants, RRT and ERRT give rise to models in which the user may only exit the ranking as they encounter each relevant document. On rankings that do not contain any relevant documents at all, the models associated with RRT and ERRT (like RR and AP before them) have the user scanning the full collection. Retrieval effectiveness We use RRT ( T ) , ERRT ( T ) , INSQ ( T ) , and INST ( T ) in our experimentation. To set the parameter T , the dis-tribution of T -bands indicated by the crowd-workers for each topic is employed, and the mapping 0  X  1 , 1  X  1 , 2  X  2 , 3 X 5  X  3 , 6 X 10  X  6 , 11 X 100  X  11 , and 101+  X  101 . That is, each score computed for RRT, ERRT, INSQ, and INST is a weighted average of up to six different T -based scores.

Table 5 lists scores for the 110 R03 and T04 queries topics and the user-generated queries, as measured by the four T -sensitive metrics; together with the expected depth at which users exit the result ranking. All of these metrics allow residuals to be computed; Table 5: Averages and standard deviations of topic means for 4 , 871 user-generated queries over 110 R03 and T04 topics, using Indri SDM retrieval, and weighted distributions of T for each query. The final column lists the expected retrieval depth in the corresponding user models, also as a weighted average. the variation in queries means that these are again relatively high compared to title-only queries. Note that the residuals associated with INST are smaller than those of INSQ , a consequence of the shallower expected depth; and the relatively implausible evaluation depths associated with RRT ( T ) and ERRT ( T ) (these would be even higher if the rankings were extended beyond 200 documents). Kendall X  X   X  for TREC systems Table 6 lists Kendall X  X   X  -b coef-ficients, computed by scoring TREC systems using a total of nine metrics, and ordering the 70 Terabyte 2004 systems (the coefficients above the diagonal) and the 78 Robust 2003 systems (below the diagonal) according to the average metric score across topics, using the R03 and T04 topic sets, and the same weighted-by-user-T com-putation as was employed for Table 5. The four T -aware metrics have relatively high similarity to each other in terms of the system orderings they induce, and fit in to the middle of the spectrum, in terms of being neither deep metrics (like NDCG ) nor shallow (like RRT1 , which is equivalent to RR ). They also yield system orderings that are similar to the ordering generated by RBP0 . 85 , for which the corresponding user model has an expected search depth of 6 . 7.
We have demonstrated that query variability among individuals leads to substantial changes across a range of standard relevance measures, and the effect of this source of variability is substantially more than that arising from topic or system effects. We also found that variation in expected goals of search in the number of docu-ments (and number of queries) arises substantially from user-based factors, and is broadly correlated with increasing task complexity. Finally, we found that relevance measures that capture expectations of relevant documents and are adaptive to individual behavior are more similar to each other in terms of system orderings, and sharply dissimilar from deep metrics like AP and shallow metrics like RR.
We conclude that the aspects of variability among users regarding individual query formulation and expected goals of search can be incorporated within a batch evaluation process. The use of mul-tiple queries per topic arising from different searchers provides a more representative characterization of the mapping from informa-tion need than just one. Systems which can perform well across such a range of queries per topic are more likely to exhibit user-generalizability. Incorporating estimates of variance due to user query-factors in a statistical power calculator would help determine the number of topics needed to reliably detect certain effect sizes.
We also suggest that the adaptive and expectation-sensitive mea-sures we presented (especially INST) display potential in having more user-generalizability and more task-generalizability than exist-ing measures, which tend to overemphasize either shallow or deep recall user behaviors. We hope to build a test collection in future to carry out more conclusive experiments on this matter. Acknowledgment This work was supported by the Australian Re-search Council X  X  Discovery Projects Scheme (projects DP110101934 and DP140102655). We thank Alec Zwart and Xiaolu Lu.

