 One way of evaluating the reusability of a test collection is to deter-mine whether removing the unique contributions of some system would alter the preference order between that system and others. Rank correlation measures such as Kendall X  X   X  are often used for this purpose. Rank correlation measures are appropriate for ordi-nal measures in which only preference order is important, but many evaluation measures produce system scores in which both the pref-erence order and the magnitude of the score difference are impor-tant. Such measures are referred to as interval . Pearson X  X   X  offers one way in which correlation can be computed over results from an interval measure such that smaller errors in the gap size are pre-ferred. When seeking to improve over existing systems, we care the most about comparisons among the best systems. For that purpose we prefer head-weighed measures such as  X  AP , which is designed for ordinal data. No present head weighted measure fully leverages the information present in interval effectiveness measures. This pa-per introduces such a measure, referred to as Pearson Rank. Evaluation Metric; Correlation Coefficient
In information retrieval evaluation, we often wish to compare the effectiveness of alternative systems by using some single-valued evaluation metric (e.g., F 1 or Mean Average Precision) on some publicly available collections. Judgments of all the items in the collections are required to get the ground-truth evaluation of the systems, which is often infeasible. Hence, sampling or pooling techniques are used in shared tasks (e.g., TREC, CLEF) to choose the documents that will be assessed. Naturally, we want to quantify the adequacy of test collections created this way for assessing the effectiveness of different systems, especially systems that did not participate to the creation of the pool [5]. We might also want to know whether we can approximate the effectiveness of these sys-tems by reducing the number of relevance judgments [1]. We can attempt to answer these questions by measuring the correlation be-tween two ranked lists of system scores (e.g., the reference and the approximated).

When making comparisons in the aforementioned cases, we fo-cus on two considerations. First, we prefer the comparison to be on an interval rather than ordinal scale. In shared tasks, the systems are ranked according to their measured effectiveness on a test collec-tion. In addition to system ranks, it is important to know whether some systems are substantially better than the others. Moreover, for formative evaluation it can be useful to characterize small in-cremental improvements, even when the new evaluation scores do not alter a system X  X  rank with regard to other systems. For this rea-son, we prefer a correlation coefficient that is sensitive to the size of the gaps between system scores. Second, we care more about comparisons among the best systems, so we prefer a correlation coefficient that is influenced more by differences near the top of a ranked list of systems. We call such measures head-weighted.
Several statistics have been proposed to quantify the correlation between two ranked lists of scores. Pearson X  X   X  [4] assumes that the scores are on an interval scale (i.e., one in which all score dif-ferences of the same size have the same meaning). Mean Aver-age Precision can properly be treated as being on an interval scale because precision is interval and expected values computed on in-terval scales are interval . Kendall X  X   X  [3] and Yilmaz et al. X  X   X  [6], by contrast, make the weaker assumption of an ordinal scale in which score differences are not necessarily informative, but the relative ordering of systems by those scores is informative.
Gao and Oard suggest a head-weighted gap-sensitive correlation coefficient called  X  GAP [2], giving greater penalty to larger gaps when a swap occurs, but assigning no penalty for misestimating the gap size if no swap occurs. Consider, for example, the situation depicted in Figure 1, which occurred in an unpublished system-ablation study in which we were studying the reuse of a test col-lection by systems that did not contribute to a judgment pool. The x-axis is the reference system score (measured in this case as mean precision at rank one) obtained using relevance judgments for the unique contributions of every system, while the y-axis is the score that we estimated for that same system without using any relevance judgments for documents uniquely contributed by that system. As Pearson X  X   X  = 0 . 89 shows, the estimates exhibit a strong linear cor-relation with the reference scores. Note, however, the substantial gap in the reference scores between the best and second-best sys-tems; the estimate does not preserve that gap. Pearson X  X  correlation coefficient is dominated by the many other well approximated gaps, however, and does not emphasize the problem. Kendall X  X   X  = is affected by several swaps in the preference order, but as the high value of  X  indicates, most of these swaps are among systems with small gaps that should not trouble us much. There are several rever-sals near the top of the ranked list that affect  X  AP = 0 Figure 1: Different correlation coefficient values when the gap between the best system and a lower one is mischaracterized. as  X  GAP = 0 . 91 shows, these reversals are among systems that had very small differences in the reference condition.

Intuitively, this seems like the wrong answer, since if we were to make a system that was considerably better than the second-best one, we have no reason to believe that the effectiveness score esti-mates on the y-axis would reflect that. None of the standard mea-sures capture the fact that the difference we should care the most about X  X he large gap in the reference system scores between the best two systems, is completely mischaracterized by the approx-imated scores as being of only negligible gap size. Pearson X  X   X  misses this because it is not head-weighted,  X  AP misses this because it is not gap-sensitive, and  X  GAP misses it because it is gap-sensitive only when a swap occurs. None of these measures focus on what we care about the most when we have an evaluation measure that is interval , which is that the size of the gaps among the best systems be correctly estimated.

This example illustrates the need for a new correlation coeffi-cient that is at the same time head weighted and sensitive to both swapped and unswapped gaps. Section 2 introduces Pearson Rank (  X  ), our novel correlation coefficient, and shows that it has sev-eral desirable properties. Through extensive simulation, Section 3 contrasts some behaviors of  X  r with those of rank-based correlation coefficients. We conclude in Section 4.
This section defines Pearson Rank, demonstrates its satisfaction of desired properties, and contrasts it with other correlations.
Let S = { s 1 ,  X  X  X  , s m } be a list of m items ranked in descending or-der by their reference scores X = { x 1 ,  X  X  X  , x m } ; and Y be their approximated scores, after scaling all of the scores so that x , y i  X  [ 0 , 1 ] . We define the new (asymmetric) Pearson Rank corre-lation coefficient (  X  r ) X and Y as:  X  r ( Y | X )= Figure 2 is a toy example for calculating  X  r of two lists, where X score differences between item pairs considered when calculating  X  at x 2 . The value of  X  r at x 2 is: Figure 2: Example for calculating  X  r correlation coefficient. The dotted arrows show the score differences between item pairs considered for  X  r at x 3 . The value of  X  r at x 3 is:  X  The value of  X  r is the sum of  X  r , i at items from 2 to m = 3.
T HEOREM 1. The value of  X  r is always between  X  1 and 1 .  X  = 1 if and only if Y = X; and  X  r =  X  1 if and only if Y =
P ROOF . Due to the Cauchy-Schwarz inequality, we have: where the two sides are equal if and only if X and Y are linearly dependent (or, in a geometrical sense, they are parallel). Since the scores are scaled x i , y i  X  [ 0 , 1 ] , we have where the upper bound 1 is reached when Y = X , and lower bound -1 is reached when Y = 1  X  X . Therefore,
T HEOREM 2. Let X = { x 1 ,  X  X  X  , x i  X  1 , x i ,  X  X  X  , x a reference score list; Y 1 = { x 1 ,  X  X  X  , x i , x i  X  1 and Y 2 = { x 1 ,  X  X  X  , x i  X  1 , x i ,  X  X  X  , x p , x p  X  1 proximated scores, where Y 1 has only one swapped adjacent item pair s i  X  1 and s i near the head of the lists; Y 2 has only one swapped adjacent item pair s p  X  1 and s p near the end of the list. If the score difference between the swapped pairs are identical ( x i  X  1 ( x p  X  1  X  x p ) , the  X  r score for Y 1 with error near the head will be lower than Y 2 with error near the end.

P ROOF . Let  X  = 1  X   X  r be the loss of a list of approximated from 2 to m . For Y 1 , we have  X  2 =  X  X  X  =  X  i  X  2 = 0;  X  Let n = x j  X  x i  X  1 and c = x i  X  1  X  x i , the derivative of could be represented in an alternative form of  X  ) which is positive when both n and c are positive. Therefore, the value of equation (6) increases with increasing i ; and thus the value of  X  i  X  1 decreases with increasing i . Similarly, As can be seen, the value of  X  i decreases with increasing i . The penalty for  X  t  X  [ i + 1 , m ] depends only t . Therefore, the value of  X  =  X  i  X  1 +  X  i larger when the swapped error appears in the head of the list.
T HEOREM 3. Let X = { x 1 ,  X  X  X  , x i  X  1 , x i ,  X  X  X  , x score vector, and Y = { x 1 ,  X  X  X  , x i , x i  X  1 ,  X  X  X  , tion list with one adjacent swapped pair x i  X  1 and x i , then the value of  X  r is inversely related to the score difference between the swapped pair ( x i  X  1  X  x i ) .

P ROOF . The derivative of  X  n ( n + c )  X  is negative. Therefore, from equations (5) and (7), we can con-clude that the value of  X  i  X  1 is positively related to the value of c is positive. Therefore, from equation (8) and (11) we can conclude also shows that the value of  X  t is positively related to c x . As a conclusion, when excluding the effect of item weights ( x /  X  m i = 2 x i ), the approximation list is penalized more when swap-ping item pairs with larger score differences.
  X  AP yes no yes no  X 
GAP yes yes yes no  X  r yes yes yes no
Table 1 shows the properties of our proposed  X  r and the other correlation measures. As can be seen, all the measures can be ap-plied with ordinal effectiveness measures, but only  X  GAP  X  leverage the fact that typical information retrieval effectiveness measures are interval .  X  GAP , however, is a hybrid that treats the reference scores as interval but the approximated scores as ordinal .  X 
AP ,  X  GAP and  X  r are head-weighted measures giving more weights to the items at the head of the list. As a result, all the head-weighted measures are asymmetric, yielding different results if the roles of the reference and the approximation are swapped. If a symmetric measure is desired, the results in both directions could be averaged. For example, we could define  X  r ( X , Y )=(  X  r ( X | Y )+
In Section 2.2 we proved some properties of  X  r when a swap occurs between two items. Another aspect we explore next is its reaction to score changes that maintain the ranking of the systems.
One way of characterizing the behavior of  X  r is to simulate cases in which the rank of each system is held invariant but the gap sizes are allowed to differ in some systematic way. When we do this, the rank correlation coefficients we have considered (  X  ,  X  show a perfect correlation of 1. Some approximations are better than others, however, and we can use simulation to characterize the behavior of  X  r in such cases. We assume that the reference and approximated scores follow some distributions. To implement this condition, we sample N systems from the distribution of the refer-ence scores X , and sort them in a descending order of scores. We scale these scores to the interval [ 0 , 1 ] by subtracting the minimum score before dividing by the difference between the maximum and minimum scores, yielding a vector X . In a similar manner, we gen-erate a vector of the ranked scaled approximated scores Y , from a distribution Y . We compute the  X  r for this pair of vectors, and repeat this process 100,000 times.

We study three different distributions, but others could have been considered as well. In the uniform distribution ( U [ 0 , between different systems are equal (in expectation). In the normal each end of the ranked list, while the majority of the systems would be located around the middle of the score distribution. With Zipf X  X  to the head of the ranked list, while most of the systems would be clustered towards its tail. Each scatter plot in Figure 3 corresponds to the median  X  r , for a pair of distributions, out of the 100,000 pairs of score vectors corresponding to 50 simulated systems. We also indicate in that figure the different quartiles of the values of  X 
We first observe that  X  r is more likely to have a high value when both of the reference and approximated scores follow the uniform distribution (Figure 3(a)). In fact, the differences between the scores are equal in expectation. The next highest  X  r values appear to be those of the normal distribution (Figure 3(b)). The gaps there are bigger (in expectation) closer to either end. Thus, the penalty (which is weighted towards the head of the list) is expected to be larger than that of the uniform distribution. Figure 3(c) shows lower values of Pearson Rank. In this case, the reference scores are drawn from the normal distribution, which means there is (in expectation) a high gap towards either end of the reference scores. However, these gaps are lost in the approximated scoring space, as succes-sive scores are expected to be equally distant.

We now turn to the bottom row of Figure 3, where at least one of the axes follows Zipf X  X  distribution. In this distribution, we ex-pect that large gaps would appear only close to the head of the ranked list. Figure 3(d) shows that  X  r can have a low value of 0 and a median of 0.95, even when both the reference and approx-imated scores are drawn from an identical distribution. This can be explained by the high variance of the gaps near the head of the scores. In Figure 3(e) we observe even lower  X  r scores (e.g., the median value is 0.91). Although, both of the normal and Zipf X  X  distributions have a tendency for high gaps close to the head of the score list, those of the Zipf X  X  are (in expectation) much larger. Thus, the gaps appear to get lost when the Zipf X  X  scores are  X  X on-verted X  into normal ones. Finally, the worst  X  r values are shown in Figure 3(f), when very large gaps are expected to appear at the head of the reference scores, while those of the approximated scores are expected to be much smaller. In this case, we observe the lowest  X  value of 0.51, and a median of 0.87.
We have proposed a novel head-weighted gap-sensitive score-based correlation coefficient  X  r . By construction,  X  r gives more weight to the items with higher reference scores. We have also shown that  X  r more severely penalizes the swaps occurring near the head of the list, and those with larger reference score gaps. Sim-ulation experiments illustrate the sensitivity of  X  r to score values, even when (as happened by construction in our simulation) rank-based metrics fail to detect any difference between reference and approximated scores due to the consistent ranking of all systems. This work was made possible by NSF award 1065250, and NPRP grant # NPRP 6-1377-1-257 from the Qatar National Research Fund (a member of Qatar Foundation). The statements made herein are solely the responsibility of the authors.
 [1] B. Carterette. Robust test collections for retrieval evaluation. [2] N. Gao and D. Oard. A head-weighted gap-sensitive [3] M. G. Kendall. A new measure of rank correlation.
 [4] K. Pearson. Note on regression and inheritance in the case of [5] E. M. Voorhees. Variations in relevance judgments and the [6] E. Yilmaz et al. A new rank correlation coefficient for
