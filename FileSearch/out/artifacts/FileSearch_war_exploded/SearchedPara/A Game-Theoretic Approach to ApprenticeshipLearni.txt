 mon approach is to model the situation using a Markov Decisio n Process. An MDP consists of reward collected by the agent.
 approach often taken is to observe and follow the behavior of an expert in the same environment. Learning how to behave by observing an expert has been called apprenticeship learning , with the agent in the role of the apprentice.
 Abbeel and Ng [1] proposed a novel and appealing framework fo r apprenticeship learning. In this function depends. With this setting in mind, Abbeel and Ng [1] described an effic ient algorithm that, given enough respect to the unknown reward function. The number of exampl es their algorithm requires from the expert depends only moderately on the number of features.
 same will hold for the apprentice.
 function, while at the same time are guaranteed to be no worse .
 Our approach is based on a multiplicative weights algorithm for solving two-player zero-sum games can be viewed as a game with this property.
 Our results represent a strict improvement over those of Abb eel and Ng [1] in that our algorithm the expert, and removes the upper bound on the apprentice X  X  p erformance. Moreover, our algorithm the MDP X  X  transition function  X  is unknown. We conducted experiments from a small car drivin g simulation that illustrate some of our theoretical findings .
 work, mimicking the expert was an explicit goal of their appr oach.  X   X   X  : S  X  R k .
 hold.
 where the initial state s  X  and  X  . We also define a k -length feature expectations vector, we have V (  X  ) = w  X   X   X  (  X  ) , by linearity of expectation.
 policy for M , i.e.  X   X  = arg max We also assume that there is a policy  X  executing in M . Following Abbeel and Ng [1], our goal is to find a policy  X  such that V (  X  )  X  a policy that is optimal in a certain conservative sense.
 in definitions of value and feature expectations apply to mixed policies as well: V (  X  ) = E and  X  (  X  ) = E policy, then V (  X   X  ) = V (  X   X  ) .
 The observations from the expert X  X  policy  X  expert: s i We compute an estimate  X   X  We compare our approach to the  X  X rojection algorithm X  of Abb eel and Ng [1], which finds a policy B algorithm runs for T iterations. It returns a mixed policy  X  such that k  X  (  X  )  X   X  T expectations. The value of  X  will necessarily be close to that of the expert X  X  policy, sin ce where in Eq. (1) we used the Cauchy-Schwartz inequality and k w  X  k The following theorem is the main result in Abbeel and Ng [1]. However, some aspects of their immediately below.
 Theorem 1 (Abbeel and Ng [1]). Given an MDP \ R, and m independent trajectories from an ex-pert X  X  policy  X  policy returned by the algorithm. Then in order for to hold with probability at least 1  X   X  , it suffices that and only two steps that are computationally expensive: The algorithm we present in Section 5 performs these same exp ensive tasks in each iteration, but is trivial.
 that each sample trajectory has length H  X  (1 / (1  X   X  )) ln(1 / ( policy is found in each iteration of the projection algorith m (see Step 1 above). Also let [1] comment at various points in their paper that of our algorithm, and also incorporate an in a similar way as the projection algorithm. expert X  X  feature expectations.
 some w  X   X  S k , where S k = { w  X  R k : k w k in
M . Now consider the optimization case is appropriate. person zero-sum game . Indeed, this is the motivation for redefining the domain of w as we did. the game matrix is the k  X  |  X  | matrix tions for the j th deterministic policy  X  j . Now Eq. (3) can be rewritten in the form However, the well-known minmax theorem of von Neumann says that we can swap the min and max operators in Eq. (5) without affecting the game value. In oth er words, the maximization over  X  is done after w  X  and hence the reward function  X  has been fixed. So the maximum is achieved by the best policy in  X  with respect to this fixed reward function. Note there always exists a deterministic optimal policy. Hence v  X   X  0 .
 In fact, we may have v  X  &gt; 0 . Suppose it happens that  X  (  X   X  )  X  (  X  things being equal, a larger value for each feature implies a larger reward. that explore this aspect of our approach in Section 7. for solving this optimization problem.
 the max player. Also, a strategy  X  w is called pure if  X  w ( i ) = 1 for some i . MDP \ R.
 a game matrix G , it suffices to be able to efficiently perform the following tw o steps: 3. Step 1 amounts to finding the optimal policy in a standard MD P with a known reward function. the size of the game matrix G .
 Our Multiplicative Weights for Apprenticeship Learning (M WAL) algorithm is described below. Lines 7 and 8 of the algorithm correspond to Steps 1 and 2 direc tly above. The algorithm is es-sentially the MW algorithm of Freund and Schapire [2], appli ed to a game matrix very similar to G exactly.
 Algorithm 1 The MWAL algorithm 1: Given: An MDP \ R M and an estimate of the expert X  X  feature expectations  X   X  2: Let  X  = 1 + 3: Define e G ( i,  X  ) , ((1  X   X  )(  X  ( i )  X   X   X  4: Initialize W (1) ( i ) = 1 for i = 1 , . . . , k . 5: for t = 1 , . . . , T do 9: W ( t +1) ( i ) = W ( t ) ( i )  X  exp(ln(  X  )  X  e G ( i,  X   X  ( t ) )) for i = 1 , . . . , k . 10: end for 11: Post-processing: Return the mixed policy  X  that assigns probability 1 Theorem 2 below provides a performance guarantee for the mix ed policy  X  returned by the MWAL based on the main result in Freund and Schapire [2]. A proof is available in the supplement [4]. Theorem 2. Given an MDP \ R M , and m independent trajectories from an expert X  X  policy  X  by the algorithm. Let gorithm. Let H  X  (1 / (1  X   X  )) ln(1 / ( Let v to hold with probability at least 1  X   X  , it suffices that where Note the differences between Theorem 1 and Theorem 2. Becaus e v  X   X  0 , the guarantee of the output by the MWAL algorithm consists of fewer stationary po licies. And if a purely stationary stationary polices in the mixed policy (this is also true of t he projection algorithm [1]). ysis applies to the projection algorithm as well [Abbeel, pe rsonal communication], so the MWAL algorithm does not represent an improvement in this respect . 5.1 When no expert is available Our game-playing approach can be very naturally and easily e xtended to the case where we do not that maximizes trivially adapted to find this policy just by setting  X  The following corollary follows straightforwardly from th e proof of Theorem 2. algorithm for T iterations. Let  X  be the mixed policy returned by the algorithm. Let defined as in Theorem 2. Let v  X  = max if where 5.2 Representation error Then for each s , if  X   X   X  ( s ) = ( f  X   X   X  sure that the representation error game value v  X  to be zero, ensuring that the MWAL algorithm, like the projec tion algorithm, will the MWAL algorithm: both the original and negated version of a feature should be used if we are uncertain how that feature is correlated with reward. applying the MWAL algorithm to this setting can be informall y described as follows: Let M = be an accurate estimate of  X  on Z . We form a pessimistic estimate c M Singh [5], who used a very similar idea in their analyis of the E 3 algorithm, we call c M MDP \ R on Z .
 expert at least O ( |S| 2 including a precise procedure for constructing c M discussed below are available in the supplement [4].
 collisions and off-roads are bad.
 cases, the MWAL algorithm leverages information encoded in the features to produce a policy that is significantly better than the expert X  X  policy.
 our features, this is indeed the best policy for the worst-ca se choice of reward function. Acknowledgments We thank Pieter Abbeel for his helpful explanatory comments regarding his proofs. We also thank the anonymous reviewers for their suggestions for addition al experiments and other improvements. This work was supported by the NSF under grant IIS-0325500.

