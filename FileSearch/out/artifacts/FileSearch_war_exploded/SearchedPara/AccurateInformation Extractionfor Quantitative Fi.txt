 In this paper, we present a novel financial event extraction system that achieves very high extraction quality by combin-ing the outcome of statistical classifiers with a set of rules. Using expert-annotated press releases as training data, and novel feature generation schemes, our system learns multiple binary classifiers for each  X  X lot X  in a financial event. At run-time, common parsing and search indexing methods are used to normalize incoming press releases and to identify candi-date event  X  X lots X . Rules are applied on candidates that sat-isfy a combination of classifiers, and the system confidence on extracted events is estimated using a unique confidence model learned from training data.

We present results of experiments performed on European corporate press releases for extracting dividend events, and show that our system achieves a precision of 96% and a recall of 79%.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models ; H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Question-answering (fact retrieval) systems Algorithms, Experimentation, Performance
Quantitative financial events such as corporate earnings announcements and revenue forecasts often serve as the pri-mary driver for significant changes in asset prices in the financial markets worldwide. These events are typically de-livered as unstructured text along with other information in corporate press releases over realtime news feeds. Structured delivery of important information pertaining to the events is preferred by professional investors and is also essential for algorithmic trading systems.

Considering that the precision of exisiting statistical sys-tems is insufficient for practical use in the finance indus-try, financial information providers primarily rely on teams of professional analysts that constantly monitor news, and manually extract and publish events in a structured format.
Some highly customized, rule-based systems have also been developed to partially automate financial event extraction. However, these systems take a prolonged period of time to develop and require constant maintenance to accommodate for slight changes in the language of press releases.
In this paper, we present a novel event extraction system that achieves very high extraction quality by augmenting the outcome of multiple statistical classifiers with a set of rules. Since the bulk of the event extraction task is handled by the classifiers, and rules are merely used to handle excep-tions, our approach significantly reduces the time needed to support new event types or to extend support of existing event types to new geographical markets. We also present a unique confidence estimation approach that classifies each extracted event into a set of predefined confidence categories.
Quantitative event extraction is an information extraction task that uses methods from information retrieval, machine learning, statistics and natural language processing along with other related research areas. It involves specialized pre-processing and normalization, entity recognition, rela-tionship identification and confidence estimation. We refer the reader to [4] for a comprehensive survey.
 Existing systems closest to our task are JASPER [1] and SCISOR [3]. Both of these systems are mainly rule-driven and use template-based pattern matching and other heuris-tic techniques. Our system uses a hybrid approach that combines statistical and rule-based methods and also assigns a confidence value to the extracted events. To the best of our knowledge, our system is the first such attempt in the domain of quantitative event extraction.
We refer to the basic unit of information that is extracted from the unstructured press releases as a  X  X act X . A fact, as we intend it, is a relevant financial event that our system is trained to detect and report. Common quantitative financial facts include corporate earnings, revenue, dividends, etc.
We may also think of fact extraction as a template filling ta sk. i.e., each fact may be considered as a template with a set of fields, some of which may be optional, and the task is to find suitable values for these fields from the unstructured content. We refer to each field in such template as a slot.
During the learning phase, a set of existing press releases are selected by expert analysts as training data for all tar-get fact types. These press releases are first pre-processed to identify candidate facts and slots in each press release, and then presented to domain experts for annotation in a tool that was specifically developed for this purpose (Sec-tion 4.2). The annotated press releases are successively split in training and test sets used to construct the classification and confidence estimation models (Sections 4.3, 4.4 and 4.6). The resulting models and expert-written rules (Section 4.5) are then applied on the press releases in the test set in a way similar to the production phase. The results are sub-sequently evaluated by experts and additional iterations of this annotation/training cycle are made as needed.
Our production system is organized as a multi-stage pipeline that receives XML requests containing the text of financial news enriched with some metadata. Each instance of the production system runs within a Jetty server, and scalability is achieved by concurrent stateless processing of multiple re-quests within each instance. Multiple production instances are used behind a hardware load balancer for fault-tolerance.
The first step in the production pipeline consists of pre-processing each press release with LUCENE and ANTLR for paragraph selection and tokenization respectively. The selected paragraphs are then analyzed in one pass over the data to identify candidate facts and slots, and features are extracted for each candidate to prepare instances for classi-fication . The candidates are then classified using the statis-tical models obtained during the learning phase, and corre-sponding rules are applied on any positively identified can-didates to handle known-errors, and to identify candidates that the classifiers are known to miss.
Given a set F of target fact types, a set T of training documents, and a set S of unique slot identifiers that con-tains all slots for facts in F , the training phase of our system begins with normalizing training documents.
Each document in T is normalized using an ANTLR-based tagging engine that uses a BNF grammar to identify syn-onyms such as X  X SD 0.05 X  X nd X 5 US cents X , and tokens that belong to the same category. For example Jan, January, and March all belong to the MONTH category. The normalized documents are then presented to analysts for annotation.
The training data was annotated using a custom-developed tool that presents tagged documents to a human expert in a graphical user interface, and allows the user to identify and annotate the positive tokens for any slot in S from the set of candidates highlighted by the tagging engine (or to create tags that were missed by the tagging engine), while preserving the hierarchical nature of facts.
The set of annotated facts is used to identify positive and negative examples for each slot where all slots of the same type but for a different kind of fact serve as negative exam-ples.

For each example we then apply feature generation schemes on the marked up text that surrounds the target candidate in a window, the size of which is configurable. Our initial experiments have focused on extracting dividend and profit facts. For these fact types, we have experimented with many existing and novel feature generation schemes as shown be-low, and for each slot, selected a subset of these schemes based on our empirical evaluation (Section 6.1). a ) Bag of Words (unigrams) b ) Delimiter-Present : indicates delimiters occurring in the window c ) Figure-Value threshold : indicates if the numerical value of the slot is greater than pre-defined threshold d ) Figure-Value-Log : logarithm of the figure e ) N-Grams : bi/tri grams occurring in the window f ) Distance-Farthest/Distance-Closest : These schemes add a feature for each tag (word, phrase or normalized text) from a list of pre-defined tags for each slot type (selected based on domain knowledge) that occurs in the window. The feature value represents the spatial dis-tance between the candidate slot and the matched tag.
The Distance-Farthest scheme uses distance of the far-thest instance of each matching tag as the feature value whereas the Distance-Closest scheme uses distance of the closest instance of each matching tag as the feature value. g ) Before-Or-After : This scheme adds a feature for each token/tag that occurs in a list of pre-defined tokens/tags.
The feature is assigned a value of 1, 0 or -1 if the to-ken/tag occurs after the candidate slot in the window, does not exist in the window, or exists before the candi-date slot in the window, respectively. h ) Period-in-Context : This scheme applies to time-period-dependent fact types, and adds a feature with value = 1 if the time-period obtained from the document context (such as document title or metadata) matches the period specified in the window. i ) Closest Single Matching Tag on Left / Right : This scheme adds a feature indicating the single matching tag that occurs closest to the candidate slot, on its left or right, where the tag is taken from the same list as the Distance-Closest scheme.

After feature extraction we normalize values of all real valued textual features to the same scale by applying z -score standardization, and the resulting instances are then used to train the classification models for each slot.
We have used two different classification algorithms in our system, i.e., Linear SVMs and the Feature Weighting Clas-sifier (FWC) [2]. Both of these algorithms are trained in linear-time and have been successfully applied to a variety of text classification problems.

Since the raw classification scores assigned to a partic-ular sample does not accurately reflect the probability of the sample belonging to a particular class, we re-scale the cl assifier scores using isotonic regression, which have been successfully used to obtain accurate class membership prob-ability estimates for binary and multiclass problems [5]. We then combine the re-scaled scores of SVM and FWC using a weighted linear combination, where the weights were de-termined empirically, for each fact and slot-type.
In addition to using a combination of statistical classifiers, our system also incorporates a rule engine. Unlike existing rule-based systems, our system does not use rules as primary means for extracting the desired information from unstruc-tured text, but instead uses rules to handle exceptions and to improve the overall system precision. In particular, the rules in our system aim to cover the following cases: a ) Handle rarely used verbiage or reporting standards, i.e., situations where high-precision classifiers could not be practically trained because of a lack of training examples. b ) Presence of outlier cases that are almost always incor-rectly classified by the statistical classifiers. c ) Pruning certain types of samples from being classified.
For example, our system uses a rule to exclude valid, but previously declared dividends (e.g., dividend for the same period last year) from being reported, to satisfy a business requirement.
We also train a confidence model that is used to esti-mate and report the system confidence on each extracted fact. Our confidence estimation scheme focuses on measur-ing the textual similarity of an unseen press release against the training corpus. The confidence classes used in our sys-tem include HIGH, GOOD, MODERATE, and LOW. Con-fidence estimation allows users to act on the automatically extracted facts based on their tolerance to risks associated with acting on potentially incorrect information.
Our confidence model consists of a bi-gram corpus con-structed from the annotated training set. All bi-grams that occur in windows surrounding each fact instance in the train-ing set are added to this corpus, maintaining their frequency counts. This corpus is then used in the production phase to estimate the system confidence on extracted facts.
The real-time fact finding process consists of receiving a new document D as input, preprocessing the document and identifying candidates for each fact from a list of unique facts F and a unique slots S for each fact, classifying the candi-dates for each slot in S , applying relevant rules, assigning confidence and reporting the extracted fact. We now explain these steps in detail. a ) Preprocessing The incoming document is first indexed using Apache Lucene. Then for each fact in F , the doc-ument is queried with relevant keywords (identified by domain experts) to retrieve paragraphs that may poten-tially contain the fact. b ) Candidate Selection and Feature Extraction The selected paragraphs are normalized and tagged using the process explained in Section 4.1 c ) Classification and Classifier Combination The can-didate instances are then classified using the models trained Con dition Co nfidence If score &gt; + 2 + -ot herwise T able 1: Thresholds for confidence assessment, is mean and is standard deviation of the training corpus scores in Section 4.4. Raw scores from SVM and FWC classifiers are normalized using isotonic regression. The normalized scores are then combined using the method explained in
Section 4.4 and the resulting score is used to classify the candidate instance as positive or negative. d ) Applying Rules Depending on the fact and slot type of a positively classified instance, the rule engine is op-tionally invoked in order to prune common errors, and to handle the other situations explained in Section 4.5. e ) Computing Confidence Scores We finally estimate confidence on each extracted fact.
We use the normalized window text to create a corpus of bigrams B . The confidence score is then calculated as follows:
Conf idenceScore = wh ere counts ( b ) indicates the number of times the bi-gram b appears in the training corpus (section 4.6). Var-ious thresholds that use mean and standard deviation of window scores in the training corpus are then applied to map the confidence score to a confidence class (Table 1).
Our dataset consisted of English financial press releases for European companies from January 2006 to May 2010. Our experiments focused on Dividend and Profit facts; se-lected based on business priorities in our organization.
We first compare various feature generation schemes and classifier combination methods on the main slots for each fact type, i.e., the dividend and profit figure slots, using a 10-fold cross-validation on the annotated data. Note that these experiments are limited to the main slots, and no rules are applied at this stage. Additional slots must be obtained before the fact is considered to be complete and publishable. Section 6.3 evaluates our system on complete facts.
In this section we compare the feature generation schemes discussed in Section 4.3. We used Bag-of-words as our base-line scheme and measured the incremental improvement achieved by each feature generation scheme, when combined with bag of words. Tables 2 and 3 present the results of this exper-iment for the dividend and profit figures, respectively. We observe that not all schemes are effective for all slots. There-fore, the final classifiers for each slot were constructed using a subset of feature generation schemes that yielded at-least some improvement over the bag-of-words baseline. For ex-ample, the precision and recall results in the last row of Table 3 were obtained by using all schemes to generate features, except  X  X eriod In Context X .
In this section we compare the performance of SVM against that of FWC. We performed a 5-fold cross validation on the T able 2: The performance of feature generation schemes on the Dividend-Figure T able 3: The performance of feature generation schemes on the Profit-Figure dividend-figure slot. From Table 4, we observe that SVM outperforms FWC in terms of precision, whereas FWC out-performs SVM in terms of recall, thus motivating us to com-bine the outcome of these methods.
 T able 4: Comparing SVM and FWC classifiers (for the Dividend-Figure)
As we have explained in Section 4.4, our system computes the final classification score as a weighted linear combina-tion of normalized individual classifier scores. To determine weights for the classification methods, we applied 10-fold cross validation on training data for dividend and profit fig-ures, and evaluated three different weight combinations. Ta-ble 5 presents the results of this experiment. We observe that the best performance is achieved when SVM and FWC are assigned 70% and 30% weight, respectively. Therefore, we used these weights in the rest of our experiments.
All the results presented so far cover a single slot. It is therefore important to evaluate the overall system perfor-mance when a fact is obtained by combining several slots, each with its own classifier and rules.

For this purpose, we applied our system on a new set of 604 press releases from a period of 16 months, not all of Com bination Method Div idend Pro fit Lin ear SVM 70 FWC 30 0. 98 0 .97 0.9 7 0. 96 Lin ear SVM 50 FWC 50 0. 97 0 .95 0.9 8 0. 97 Lin ear SVM 30 FWC 70 0. 94 0 .95 0.9 1 0. 90 T able 5: Different classifier combination methods for dividend and profit
Ext racted Go od/High Conf. Corre ct Ac tual Mi ssed T able 6: Overall system performance for Dividend Facts which contained facts. Our team of annotators manually verified the system output and inspected the press releases for additional unreported facts. Table 6 presents the results of this experiment. Our system achieved a precision of 96% and a recall of 79%, when the facts were classified as high or good confidence. It is important to note that the system identified many additional facts in lower confidence cate-gories but these facts are not included in the system output because of high-precision requirements in our domain.
We have presented a novel event extraction system that uses a combination of multiple binary classifiers and manu-ally written rules, where the bulk of the extraction task is handled by the statistical classifiers and rules are used to handle exceptions. We also present a unique approach to estimate system confidence on each extracted fact.
In the future, we plan to extend our system to support additional quantitative fact-types such as revenue and earn-ings per share, and non-quantitative events such as mergers and acquisitions. [1] P. M. Andersen, P. J. Hayes, A. K. Huettner, L. M. [2] H. Malik, D. Fradkin, and F. Moerchen. Single pass [3] L. F. Rau and P. S. Jacobs. Integrating top-down and [4] S. Sarawagi. Information extraction. Found. Trends [5] B. Zadrozny and C. Elkan. Transforming classifier
