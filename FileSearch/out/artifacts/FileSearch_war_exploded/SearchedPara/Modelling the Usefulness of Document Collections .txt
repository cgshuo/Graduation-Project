 Dealing with the medical terminology is a challenge when searching for patients based on the relevance of their med-ical records towards a given query. Existing work used query expansion (QE) to extract expansion terms from dif-ferent document collections to improve query representation. However, the usefulness of particular document collections for QE was not measured and taken into account during retrieval. In this work, we investigate two automatic ap-proaches that measure and leverage the usefulness of doc-ument collections when exploiting multiple document col-lections to improve query representation. These two ap-proaches are based on resource selection and learning to rank techniques, respectively. We evaluate our approaches using the TREC Medical Records track X  X  test collection. Our re-sults show the potential of the proposed approaches, since they can effectively exploit 14 different document collections, including both domain-specific (e.g. MEDLINE abstracts) and generic (e.g. blogs and webpages) collections, and signif-icantly outperform existing effective baselines, including the best systems participating at the TREC Medical Records track. Our analysis shows that the different collections are not equally useful for QE, while our two approaches can automatically weight the usefulness of expansion terms ex-tracted from different document collections effectively. Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval Keywords: Patient Search, Query Expansion
To improve the healthcare service quality, electronic medi-cal records (EMRs) are used to document the medical condi-tions (e.g. symptom, treatment) of patients visiting a hospi-tal. These EMRs can also be leveraged in medical research. For example, using a patient search system, EMRs can help to identify cohorts of patients suitable for particular clini-cal trials. In this paper, we tackle the patient search task. In this task, healthcare practitioners describe the medical conditions of patients of interest as a query and the search c system retrieves patients based on their relevance towards the stated medical conditions.

Existing work showed that the complexity of medical ter-minology makes patient search a challenging task [7, 10, 18]. For instance, patients whose medical records state that they are suffering from X  X eafness X  X ay not be retrieved for a query to find patients with  X  X earing loss X , even though  X  X eafness X  and  X  X earing loss X  share the same meaning. Limsopatham et al. [9, 11], as well as Qi and Laquerre [18] dealt with this problem by representing queries and medical records in the forms of medical concepts. As a result,  X  X eafness X  and  X  X earing loss X  were represented with the same concept. On the other hand, several approaches exploited various doc-ument collections to improve query representation (e.g. [5, 10, 25]). For example, Limsopatham et al. [10] used the relationships between medical concepts found in medical re-sources, such as UMLS 1 and MeSH 2 , to improve the rep-resentation of medical conditions in the queries. King et al. [5], whose system achieved the best retrieval performance at the TREC 2011 Medical Records track [21], expanded the queries with terms extracted from the medical record collec-tion itself. Later, Zhu et al. [25] used a query expansion (QE) technique to extract expansion terms from four differ-ent document collections, including web documents, medical records and two sets of medical articles. In particular, Zhu et al. [25] suggested that using expansion terms extracted from all of those four collections to expand a query is more effective than using expansion terms from each collection in-dividually. In contrast, we hypothesise that the usefulness of different document collections for QE could be estimated and more effectively exploited when ranking patients based on the relevance of their medical records.

In this work, we investigate two approaches for modelling the likelihood that expansion terms extracted from each document collection are effective for improving query rep-resentation. The first approach adapts a resource selection technique [3] to measure the likelihood that a document col-lection can provide good expansion terms, and then uses the measured likelihood to weight the expansion terms ex-tracted from that collection. On the other hand, the second approach combines the relevance scores computed from the expanded query using each of the document collections by using a learning to rank (LTR) technique (e.g. [15, 23]) to learn an effective combination.

Using the TREC 2011 and 2012 Medical Records track X  X  test collection [20, 21], we evaluate our two proposed ap-proaches when applied with a QE technique to exploit 14 different document collections that are either generic or domain-specific. Our results show that both approaches are effec-h ttp://www.nlm.nih.gov/research/umls/ http://www.ncbi.nlm.nih.gov/mesh/ tive, as they could significantly outperform existing effecti ve baselines with and without QE. In particular, the resource selection-based approach outperforms the approach of Zhu et al. [25] by up to 32%.
We first describe the retrieval models used for ranking pa-tients based on the relevance of their medical records (Sec-tion 2.1), and for extracting expansion terms from a partic-ular document collection (Section 2.2).
Existing work (e.g. [5, 10, 25]) used either a patient model or a two-stage model to rank patients based on the relevance of their medical records [12]. The patient model ranks pa-tients based on the relevance scores of the concatenations of medical records associated to particular patients, while the two-stage model aggregates the relevance scores of the medical records using an aggregate function (e.g. expComb-SUM [14]) to rank their associated patients. In this work, we use the expCombSUM voting technique [14] to rank pa-tients, as it has been shown to be effective in previous works (e.g. [12, 13]). In particular, expCombSUM calculates the relevance score of patient p towards query Q , as follows: where R ( Q ) is a ranking of the medical records retrieved us-ing query Q , R ( Q )  X  profile ( p ) is the set of medical records in ranking R ( Q ) that are also associated to patient p ; score ( d, Q ) is the relevance score of medical record d given the query Q . We follow [13] and use DFR DPH [2] to calculate score ( d, Q ), and limit the number of voting records ( | R ( Q ) | ) to 5,000.
Query expansion (QE) models (e.g. DFR Bo1 [1] or a rel-evance model [8]) extract expansion terms from a document collection by firstly retrieving the top N ranked documents, and then extract the top k most informative terms from those documents as expansion terms. Traditional QE uses the targeted collection to extract the expansion terms, while external QE (e.g. [4]) extracts the expansion terms from the documents retrieved from an external collection. In this work, we use both the targeted and external collections to expand a query. We deploy the DFR Bo1 model to extract the top k expansion terms from each collection, since it has been shown to be effective for this patient search task [10]. In particular, we follow [10] and extract the 10 most infor-mative terms, as well as their weight, from the top 3 ranked documents in each collection (i.e. N = 3 and k = 10).
Sections 3.1 and 3.2 introduce our two proposed approaches to model the usefulness of different document collections when exploiting their extracted expansion terms.
Resource selection techniques have been used to select col-lections that are likely to contain relevant documents, so that a retrieval system can focus on those collections during retrieval [3]. Our first approach adapts a resource selection technique to measure the likelihood that expansion terms extracted from a particular collection are effective for QE. In particular, it is intuitive that expansion terms extracted from collections that are related to the original query are more likely to be useful than those extracted from unrelated collections. When taking into account the likelihood that ex-pansion term t k extracted from collection c i is effective for QE, the relevance score of medical record d towards query Q (i.e. score ( d, Q )) can be calculated as follows: where score ( d, t j ) and score ( d, t k ) can be calculated using any retrieval function (e.g. DFR DPH [2]), C is the set of used collections, w c ( c i ) is a collection weight, Qc i of expansion terms extracted from collection c i , and w t is the weight of expansion term t k , which can be computed from each document collection using a QE model. Note that in the approach of Zhu et al. [25], w c ( c i ) is equally set to 1 for all collections c i .

We use the CORI resource selection algorithm [3] to cal-culate the collection weight w c ( c i ) in Equation (2). Indeed, CORI is calculated as a combination of the score p ( t j | c of each term t j in query Q , using a probabilistic operator. Using the operator AND, OR or SUM, the CORI score (i.e. w ( c i )) of collection c i can be calculated as follows [3]: where p ( t j | c i ) is calculated as follows [3]: df is the number of documents in collection c i that contain term t j , cw is the number of terms in collection c i , avg is the average number of terms in each collection, | C | is the number of used collections, cf is the number of collections containing term t j , and b is the default belief, which is set to 0.4 as recommended in [3].
Our second approach uses an LTR technique to combine rankings produced from the query expanded using each col-lection separately. In general, the LTR techniques aim to learn an effective ranking model from a set of features us-ing training data. As we aim to leverage QE from multiple collections to improve the patient search performance, our features are the relevance scores computed from the query expanded using each of the used document collections. For a linear LTR technique, such as Automatic Feature Selection (AFS) [15], the weights of these features can be viewed as the usefulness for QE of the associated document collections. We deploy four existing effective LTR techniques (namely, AdaRank [23], AFS [15], Coordinate Ascent (CA) [16] and LambdaMART [22]) that aim to optimise a targeted evalua-tion measure (e.g. MAP) during training. Indeed, these four Table 1: List of document collections used for QE. techniques deploy different types of algorithms to learn a s uitable ranking model from a set of features. AdaRank [23] applies a boosting technique to optimise the targeted evalu-ation measure by considering each feature as a weak ranker. AFS [15] and CA [16] apply a greedy algorithm to learn an effective linear combination of features by using differ-ent underlying optimisation techniques. Our used imple-mentations of AFS and CA deploy simulated annealing [6] and coordinate ascent, respectively, when learning the fea-ture weights. LambdaMART [22] deploys boosted regression trees to find an effective combination of features that opti-mises a targeted evaluation measure. Test Collection: To evaluate the two proposed approaches, we use the test collection provided by the TREC Medical Records track [20, 21]. The task is to retrieve patient visits relevant to a given query. A patient visit contains medi-cal records related to a particular visit to the hospital by a patient. Due to privacy concerns, a patient visit is used to represent a patient [20, 21]. The topic set includes 34 and 47 queries from TREC 2011 and 2012, respectively. We report the retrieval performance in terms of the track pri-mary measures, which are bpref and infNDCG for TREC 2011 and 2012, respectively.
 Expansion Collections: For reproducibility, we leverage 14 widely available document collections for QE, which in-clude both generic and domain-specific collections, as de-scribed in Table 1.
 Retrieval Toolkits: We conduct experiments using the Terrier platform [17] 4 , applying Porter X  X  English stemmer and removing stopwords. We use the models discussed in Sections 2.1 and 2.2 to rank patients and to extract expan-sion terms, respectively. For the LTR techniques, we use the implementation of RankLib 5 with the default setting. In ad-dition, as the LTR techniques require a set of training data to learn a model, we use the queries from TREC 2011 to train an LTR model when testing on TREC 2012, and vice versa. In addition, as the targeted measure of TREC 2011 and 2012 are different, we follow [24] and train the learned model to optimise the MAP measure. tails about each document collection. 4 http://terrier.org http://www.lemurproject.org/ranklib.php
We compare the retrieval performances of our proposed approaches with four categories of baselines. The first two categories of baselines are basic baselines that either do not apply QE or use expansion terms from only one of the 14 used collections. The third and the fourth categories of baselines are more advanced. In particular, the third cat-egory of baselines uses expansion terms extracted from all 14 collections (as in [25]). The fourth category of baselines firstly retrieves patients for the query that is expanded using each of the 14 collections, and then uses a data fusion tech-nique [19] (i.e. CombSUM, CombMNZ, expCombSUM or ex-pCombMNZ) to combine the relevance scores of the patients. Note that the CombSUM data fusion technique is in some sense similar to the approach of [25] in that the relevance scores of expansion terms extracted from different collections are equally combined to rank patients. However, CombSUM is arguably more robust to topic drift, since the focus of each expanded query is typically still on the original query.
Table 2 shows our experimental results in terms of bpref and infNDCG for TREC 2011 and 2012, respectively. First, we discuss the performance of the baselines that apply QE on each collection individually. From Table 2, we observe that, for TREC 2011, 12 out of 14 collections are useful for improving the query representation, as expansion terms extracted from these collections improve the retrieval perfor-mance over the baseline that does not apply QE (i.e.  X  X o-QE X ). Meanwhile, 10 out of 14 collections are effective for TREC 2012. In general, the expansion terms extracted from all medical-related collections (e.g. EMRs11, Genomics04 and Genomics06) effectively improve the retrieval perfor-mance for both TREC 2011 and 2012. However, surprisingly ClueWeb09B and WT10G, which are generic collections, are the most effective collections for QE for TREC 2011 and 2012, respectively, and even outperform traditional QE, which expands the queries using the targeted collection (i.e. EMRs11). In particular, leveraging expansion terms from ClueWeb09B is overall the most effective and significantly (paired t-test, p &lt; 0 . 05) outperforms the  X  X o-QE X  baseline for both TREC 2011 and 2012 (bpref 0.5323 vs. 0.4871 and infNDCG 0.4446 vs. 0.4167). Nevertheless, we observe that expanding queries with terms extracted from all collections (i.e.  X  X ll Collections X ) is not effective. This differs from the conclusion reported in [25]. However, note that our exper-iments consider more document collections both in number and variety. Meanwhile, the data fusion baselines are more effective than the  X  X o-QE X  and  X  X ll Collections X  baselines.
When considering the performance of our two proposed approaches, we observe that they significantly (paired t-test, p &lt; 0 . 05) outperform both the  X  X o-QE X  and  X  X ll Collections X  baselines. Indeed, our adaptation of CORI (i.e. CORI SUM and CORI OR ) to weight the expansion terms from each collection (i.e. Equation (2)) outperforms all of the eval-uated approaches, including when applying QE with the ClueWeb09B collection. This shows that expansion terms extracted from different document collections are not equally useful, and our adaptation of CORI could estimate the effec-tiveness of expansion terms extracted from different collec-tions. However, CORI AND is less effective than CORI SUM and CORI OR since it applies the stronger constraint that a collection should contain all of the query terms. It is of note that both CORI SUM and CORI OR perform better than the best TREC 2011 system [5], and comparably to the top performing systems in TREC 2012 [20], without deploying other well-known performance boosting techniques for pa-Table 2: Retrieval performances on TREC 2011 a nd 2012 Medical Records track of different QE ap-proaches. Statistical significance (paired t-test) at p &lt; 0 . 05 over the baseline that does not apply QE and the baseline that uses all expansion terms ex-tracted from the 14 used document collections are tient search such as negation handling [20, 21]. Meanwhile, o ur approach that uses LTR techniques could not outper-form the ClueWeb09B baseline. In particular, the most ef-fective LTR technique is CA, which achieves bpref 0.5298 and infNDCG 0.4475. The other LTR techniques, e.g. Lamb-daMART, are less effective. This could be due to the small number of available queries for training the learned models.
When analysing the expansion terms extracted from dif-ferent collections, we observe that the expansion terms from some collections are not related to the query medical con-ditions. For instance, for the query  X  X earing loss X , our ap-proach gives low weights to W3C and CERC, from which the extracted expansion terms, such as  X  X ndividual X ,  X  X rgument X ,  X  X lan X  and  X  X trategy X  were off-topic. Meanwhile, collections such as ClueWeb09B and Wiki09, which obtain high weights, provide expansion terms that are more related to the query e.g.  X  X arwax X ,  X  X ensorineural X  and  X  X eaf X . In contrast, ex-pansion terms extracted from EMRs11, which received a medium weight from our approach, also include an off-topic term (i.e.  X  X oman X ), after a list of related terms (e.g.  X  X eru-men X ,  X  X ar X ,  X  X anal X ). This might be the reason why QE using ClueWeb09B or Wiki09 is more effective than when applied to EMRs11, since highly ranked documents from the web might contain more effective related terms to  X  X earing loss X  than highly ranked medical records, as the latter could also mention other health information.
We introduced the use of resource selection and learning to rank techniques to weight the expansion terms extracted from particular document collections differently. Our exper-imental results conducted on the TREC Medical Records track X  X  test collection showed that the two approaches were more effective than an existing approach that used all the expansion terms extracted from all of the used collections equally. Specifically, our approach based on a resource se-lection technique (i.e. CORI SUM and CORI OR ) was shown to be the most effective and significantly outperformed the aforementioned baseline by up to 32% (infNDCG 0.4689 vs. 0.3551). This shows that expansion terms extracted from different document collections are not equally useful and that our proposed approaches can automatically and effec-tively weight these expansion terms during retrieval.
