 1. Introduction
The current process of scientific communication has been severely criticized in the last decades, given the current needs of the scientific community and its inability to support them ( King &amp; Tenopir, 1998 ). Many are the factors that contribute to this, including: (1) the imbalance between publishers X  prices for journal licenses, which grow much faster than inflation, and the low budget of research and university libraries; (2) the impossibility of authors to promote and share their own work with peers and therefore obtain the neces-sary scientific acknowledgment, due to copyright transfers to the publishers; and (3) the enormous delays between submission and the actual publication of the work in the literature.

In this context, digital libraries (DLs) and repositories have emerged as a valid alternative to solve some of the aforementioned problems. These problems have also led to the growth of the  X  X  X prints X  X  movement ( Hanard, 2001 ) and subsequent important developments such as the Open Archives Initiative for interopera-bility ( Lagoze &amp; de Sompel, 2001 ).
Due mainly to issues of economic sustainability, many of these DLs and repositories support  X  X  X elf-archiv-ing X  X  services that allow researchers to easily archive their own work and share results and publications with their peers, thus saving costs by involving the interested community. In fact, according to Harnad ( Hanard, and useful for any interested user with an Internet access. The term self-archiving is used since it is considered that the author herself is responsible for inserting her own work in the repositories as well as describing it by filling out any pertinent metadata description. However, the term can also be used for submissions coming from third-parties, as long as this is allowed by the author.

In this article, we present an overview of a self-archiving service for the Brazilian Digital Library of Com-puting (BDBComp 1 )( Laender, Gonc  X alves, &amp; Roberto, 2004 ) and describe a user experiment conducted to evaluate it. The goals of such a service are two fold: (1) to help expand the BDBComp collections by allowing
Brazilian Computer Science (CS) researchers to submit new types of work beyond those already supported by the DL, and (2) to provide a means to guarantee the BDBComp sustainability by actively engaging its target community in its maintenance, quality control, and administration ( Veiga e Silva, 2004 ). We acknowledge the fact that we could use similar systems such as EPrints ( Gutteridge, 2002 ), DSpace ( Tansley et al., 2003 ), Kep-ler ( Maly et al., 2004 ), and Fedora ( Staples, Wayland, Payette, &amp; April, 2003 ) to provide this service and to conduct such study. In this work, however, we chose to develop our own self-archiving service for a number of reasons, including: (1) Interoperability with the existing version of the BDBComp.
 (2) Easier customization for the reality and necessities of the Brazilian CS community.
 (3) Easier instrumentation for analysis and experimentation of the service.
 In the remainder of this article, we will describe in details and evaluate many of the aspects described above.
This article is organized as follows. Section 2 explores related work. Section 3 describes the BDBComp self-archiving service, including background, design and some implementation details. Section 4 is the core of the article and describes the user experiments conducted to evaluate the service, their results and analyses. Section 4 concludes the article with some glimpses into future work. 2. Related work
Several current DL archival systems provide some kind of support for self-archiving services on top of their repositories, the most prominent ones being DSpace ( Tansley et al., 2003 ), EPrints ( Gutteridge, 2002 ), and
Kepler ( Maly et al., 2004 ). The first two are more focused on institutional submissions while the last one is more focused on individual/personal collections. Table 1 shows a comparative analysis of these three systems and the BDBComp X  X  self-archiving service under a number of dimensions. These dimensions were chose based on Open Society Institute (2003) . That work intended to be a guide to help institutions in the planning of their repositories and to help them choose the best software based on their needs.

A few interesting observations can be seen from Table 1 . Arc vesting and information retrieval service for multiple data providers. Also in Kepler, users are authenticated by the central servers using URLs and IP addresses. The use of MySQL tables to store information about login names and passwords, used in EPrints and BDBComp, facilitates implementation but may cause security problems, mainly in BDBComp which does not provides a secure password generation. By far, the most secure system is DSpace which uses a X.509, a public key infrastructure in its authentication sub-system. Also, only
DSpace supports some kind of access restrictions regarding which users are allowed to use the self-archiving service. Since our goal was to attract the maximum number of users possible, in our implementation we decided not to introduce any type of artificial constraints on who could submit material to the service. DSpace and BDBComp are also the only systems that support different stages in the digital object life cycle. This is extremely important to support the reviewing process implemented in the self-archiving service. The notifica-tion system in the BDBComp is centered around the reviewer role, a feature that tries to improve quality and speed up the process of approval. All systems support multiple and different types of collection (e.g., disci-plined-based) and groups of users (e.g., per institution). Kepler and BDBComp do not support incomplete submissions, an interesting feature that facilitates the submission process but introduces complications in the implementation, since partial submission workflows need to be stored. Finally, DSpace is the only one with special support to complex objects, supporting the METS (Metadata Encoding and Transmission Stan-dard) standard proposed by the Library of Congress (2003) .

Besides the ones mentioned above, the motivations for creating our own self-archiving were already high-lighted in the previous section. As we will see in the next sections, and it is illustrated in the last column of
Table 1 , our service covers most of the features supported by the previously mentioned systems. In special, our service supports different stages in the object life cycle as DSpace and explicit notifications to reviewers.
Other user-based studies have been reported in the literature, but those are more concentrated on usage issues of information-satisfaction services, i.e., the way patrons find, use, or interact with material found in
DLs (e.g. Blandford, Keith, Connell, &amp; Edwards, 2004; Blandford, Stelmaszewska, &amp; Ryan Kinns, 2001 ), mainly in educational settings. For example, Borgman et al. ( Borgman, Leazer, Gilliland-Swetland, &amp; Gazan, 2001, Borgman et al., 2004 ) have studied the design and evaluated the Alexandria Digital Earth ProtoType (ADEPT), a digital library of geo-referenced information resources, for use in undergraduate education. Inter-views indicated that geographical resources in the project were better organized for research not for educa-tional purposes, which brought implications for the design of new services. Online surveys were used in
McMartin and Terada (2002 ) to conducted interviews with educators and students using NEEDS X  X  US national engineering education digital library. It was found that digital libraries need to develop and imple-ment more effective services to bring authors, essential for their survival, into their communities; identified ones include review and feedback services and online discussions. Focus groups were conducted with practic-ing teachers, pre-service teachers, and science librarians, drawn from different educational contexts, to under-stand their perceptions of quality in digital libraries ( Sumner, Khoo, Recker, &amp; Marlino, 2003 ). Adams and
Blandford present an interesting study about digital library support for the user X  X   X  X nformation journey X  in the health and academia domains ( Adams &amp; Blandford, 2005 ) through in-depth analysis of findings from inter-views, focus groups and observations of 150 users. In comparison, to the best of our knowledge, ours is the first work that reports a comprehensive user experimental evaluation study of a self-archiving service fol-lowing sound statistical principles.

Notice that our user study was not objectified to be a controlled, lab-based formative usability study, such those presented by Heath et al. (1995) and Zhu et al. (2003) . Instead, it is a user experiment intended to cap-ture qualitative properties of our solution for the problems described above. If the system did not have the intended properties (e.g., being usable, easy to use, efficient, etc.) we would not satisfy our users and the solu-tion would have failed. Specifically, questionnaires with potential users are well-established techniques for sys-tem evaluation ( Preece, Rogers, &amp; Sharp, 2002 ) and have been used in other similar studies such as the one described by Kengeri, Seals, Harley, Reddy, and Fox (1999) in which the user interface of several DLs are contrasted and evaluated. Again, that study did not make an effort to present a statistical analysis of the gath-ered data. 3. The BDBComp self-archiving service: background, design and implementation
The BDBComp is an effort aimed at providing a platform for archiving, indexing, disseminating, and pre-serving the wealth of scientific knowledge produced by the Brazilian CS community ( Laender et al., 2004 ). It has been designed to be OAI compliant and adopts Dublin Core (DC) as its metadata standard. There are several services and user interfaces that serve as a  X  X lue X  that binds all services together (see Fig. 1 ). These diverse interfaces are specially tailored to the needs of different communities of users, among them: general users (e.g., educators, apprentices, researchers), contributors, reviewers, and administrators. The services pro-vided for general users are those usually available in any DL, such as searching, browsing, and linking. The self-archiving service, which was implemented as an additional service and incorporated to the BDB-
Comp original set of services, allows contributors to submit metadata of different types of work (conference papers, journal articles, books, and book chapters) to the central metadata repository. Other types, such as thesis, dissertation, and technical report, will be supported in the future. Reviewers play an important role in this service since they are responsible for approving the metadata submissions. This service works together with the DL collection management facilities that use the OAI protocol to harvest approved entries to the main metadata repository. In addition to the self-archiving service, we have adopted two alternative ways to collect metadata for the BDBComp repository: (1) by extracting them from existing web sites, for instance, by using tools such as the Web-DL environment ( Calado et al., 2003 ), and (2) by harvesting other OAI com-plaint repositories. The former deals with a large number of sources of legacy data (e.g., conference and insti-tutional web sites) already existing on the Web but it is time-consuming and usually generates incomplete DC records, while the latter supplements the BDBComp information, for example, by including data from works of Brazilian authors published in international conferences and journals (e.g., harvested from DLs such as CITIDEL 3 ), but requires additional processing to solve problems such as deduping and name ambiguity.
The BDBComp self-archiving service has been designed to enforce the quality and consistency of metadata stored in the central repository and to serve as a mechanism to promote the involvement of its target commu-nity, therefore guaranteeing the sustainability of all the other services. The service has been specifically tailored to the CS community and allows only the submission of post-print publications. As we can see from Fig. 1 , the
BDBComp self-archiving service comprises three basic sub-services: registration, submission, and review. The registration service allows users to register themselves as contributors so that they can submit the metadata of their own works. A contributor may also be someone who makes submissions on behalf of other persons (for instance, a librarian who works as an archivist for a research project). Notice that this submission service is restricted to the scope of the self-archiving service and it is not responsible for managing user information for
BDBComp as a whole. The submission service provides a different interface for each type of work supported by the repository. Finally, the review service guarantees the quality of the submitted data. By means of this service, a reviewer evaluates the submitted data with respect to authenticity, relevance, and quality, and decides whether they should be accepted or rejected. All the three services communicate with the metadata repository. Also, the BDBComp self-archiving service has been designed to accept submissions from any CS research area. Therefore, each submitted work must be classified as belonging to a specific CS area, such as databases, computer networks, software engineering, etc., and conform to one of the currently supported types. The CS research areas were derived from the set of special interest groups of the Brazilian Computing
Society and are also used to help assigning reviewers who are experts in the respective fields. The whole appli-cation has been implemented in Java, using JSP (Java Server Pages). Communication between the services and the repository is supported by JDBC (Java DataBase Connectivity).

BDBComp has currently about 4500 entries. This number is expected to increase to about 30,000 in the next few months, after an experiment that collected and integrated relevant data from other web sites. It receives an average of 20,000 accesses and 330 queries per week. 3.1. The BDBComp metadata repository
The BDBComp metadata repository is a relational database implemented using the MySQL DBMS. In order to support the self-archiving service, the original BDBComp repository ( Laender et al., 2004 ), which was designed to accomodate the Dublin Core metadata standard and the OAI Protocol for Metadata Harvest-ing (OAI-PMH) 4 infrastructure, has been expanded to include additional data required to represent: (1) new user communities (e.g., reviewers, contributors); (2) the main CS research areas according to which the sub-mitted works must be classified; and (3) CS publication venues, such as journals and conferences. Also, the BDBComp repository supports the Qualified Dublin Core metadata format as proposed by the Dublin Core
Metadata Initiative Citation Working Group. 5 This format adopts a bibliographicCitation qualifier that extends the usual DC fields to store additional data required to represent a complete bibliographic citation for journal articles and conference papers. Fig. 2 shows an OAI record from the BDBComp repository. Notice the bibliographicCitation qualifier included to cope with the additional citation information. 3.2. User interfaces
Fig. 3 shows the BDBComp main page which is used to access its basic services, searching ( Pesquisar ) and user must first register herself as a contributor by filling in a registration form. When filling in this form, a user informs not only some personal data (name, e-mail, and institution), but also a login name and a password which will be required to full access to the submission service.

After logging into the submission service, a user gets access to her contributor X  X  area . In this area, she can access all her previously submitted works, classified according to their current status (submitted, accepted or rejected), and update her personal data. She can also visualize the metadata of these works and revise or delete those that hold the submitted or rejected status. When submitting a work X  X  metadata, a contributor must first select its type (journal article, conference paper, book or book chapter) and then enter the data as required.
Fig. 4 shows an example of a journal article submission. The submission form fields correspond to the DC fields and follow the order proposed by the MARC format. We notice that, having finished a submission, the contributor may visualize its result and then, if needed, modify or add any data (see Fig. 5 ).
Likewise, there is also a reviewer X  X  area for each reviewer registered by the BDBComp administrator. In this area, a reviewer can access and visualize any submission assigned to her, and then make the decision whether to accept or reject it. 4. Experimental evaluation In order to evaluate the proposed self-archiving service, we conducted the user experiment described below.
The statistical techniques used to analyze the answers in our experiments are described in Triola (2004) . 4.1. Subjects
Twenty one subjects participated in this experiment, chosen among potential users of the service. The sub-jects were equally divided into three groups: (1) experts, which corresponded to librarians, archivists and
Information Science professors; (2) Computer Science professors; and (3) Computer Science graduate stu-dents. The last two groups had certain experience with self-archiving, since all Brazilian researchers are required by the Federal Government funding agencies to post their resumes in a central database (called the CNPq Lattes Curriculum Platform 6 ), including publications. For the first group, we chose subjects who had some experience in archiving scientific publications. Notice that despite the small number of users, our results, as will be seen, were statistically sound and consistent. 4.2. Tasks, measures, and procedures
Each subject, independent of the group, was supposed to submit the metadata of at least three publications of any of the supported types. It was suggested that they mixed types and national and international publica-tions, so that we could evaluate several of the features of our service. In the case of the experts, they were given printed material taken from Brazilian CS researchers X  publications, with all the necessary metadata, as well as digital versions of their abstracts and keywords.

Each subject had to perform the same tasks which included: (1) Access the system through a browser of choice. (2) Register herself as a contributor in the registration service. (3) Login into the submission service with her login name and a given password. (4) Click in the submission link and fill in the solicited data in the form for each type of work to be submit-(5) Check if the submission was correctly performed, by entering in the contributor X  X  area. In case of any (6) Fill in an evaluation questionnaire by the end of the third submission.

The questionnaire served to measure the overall user experience with the service. Each of the eight ques-tions of the questionnaire was designed to measure different facets of the interface as well as to assess some of the users X  background. Each question was associated to a 1 X 7 scale point system. The questions were: (1) How would you grade your knowledge of bibliographic self-archiving before the experiment? (2) How would you grade your knowledge about the Dublin Core metadata standard? (3) How would you grade your knowledge of bibliographic self-archiving after using the BDBComp service? (4) How would you grade the interface in terms of easiness to use? (5) How comfortable did you feel using the service X  X  interface? (6) How useful do you think the interface is to correctly collect bibliographic data of scientific work? (7) When compared to the Lattes Curriculum X  X  user interface, how would you grade the BDBComp user (8) How would you grade the interface in terms of the order of the fields to be filled?
The first two questions measure some previous user knowledge. The third question measures the impact of the use of the tool on the users knowledge, a measure of learnability. The other questions measure the effec-tiveness of the service in terms of easiness to use, user comfort, and usefulness for the task at hand. In par-ticular, the seventh question was specific for the Brazilian reality, since it compares our system to the
CNPq Lattes Curriculum Platform. For this, and only this question, the lower the value the better our inter-face was considered in the comparison. Finally, in the questionnaire, there was an option and space for users to register free comments about difficulties, suggestions, opinions, etc.

Instructions to conduct the experiments were group-oriented, trying to emulate real situations. For profes-sors and students instructions were emailed with no further personal instructions or help. Subjects performed the experiments on their own with their personal computers. For the experts, since they had to submit material with which they were not familiar, a previous study and some help from an instructor about the material to be archived before the experiment was allowed. Experiments with experts were performed individually with a minimum level of interference by the instructor. Users requested confirmation that the task was correctly per-formed in a couple of cases which may reveal some lack of confidence. We credit this again to the unfamiliarity with the material. 4.3. Results
To evaluate our results, we use two types of analysis. The first one, the Kruskal X  X allis test, serves to deter-mine whether three or more populations have the same statistical averages for the values of the answered ques-tions ( Triola, 2004 ). This method is used to check if subjects within a same group have a similar behavior, i.e., if they belong to a same population sample, so that, in the positive case, it would make possible to perform an analysis of the sample averages. Differently from, for example, the analysis of variance of one criteria (ANOVA) method, the Kruskal X  X allis test does not require that the samples have a normal distribution, given that each sample has at least five data points. The second method is based on the dependencies among the questions in the questionnaire. For this second analysis, we used a technique called  X  X eorderable Matrix X  ( Bertin, 1981 ). This analysis is the subject of the next (sub-)sections.

Tables 2 and 3 show the answers for the eight questions in the questionnaire by group. Table 2 deals with the first three questions related to the users knowledge while Table 3 shows the answers to the other questions related to the user interface of the service.

In Table 3 it can be noticed that two subjects were not familiar with the Lattes Curriculum X  X  interface. For the others, the smaller the value, the simpler our interface was considered when compared to that of the Lattes
Curriculum. It can also be noticed that all subjects considered the order of the fields in the form adequate so we will not consider this issue in the subsequent analysis.

The time spent by each subject with the respective averages is shown in Table 5 in minutes. Since we did not specify a priori the types of document that should be submitted, we obtained different values for each type. For the students, conference papers constituted the vast majority of entries, since they publish more in this type of venue than in journals. For the same reason, students did not submit any books and only one book chapter. 4.3.1. Analysis of averages
The analysis in this section is based on a comparison of the average values for each answer in the question-naire. We use the Kruskal X  X allis method to test whether the three populations had the same statistical aver-ages regarding their answers to the questions. According to Triola (2004 ), this method checks whether the group averages have values within a specified margin and populational variances with close values. In positive case, the group averages are comparable. In this analysis we only used the questions and answers relative to the service user interface (as in Table 3 ). Accordingly, we tested the hypothesis that the averages of answers for all three groups are statistically similar for each question (for example, the hypothesis that the averages for the usefulness questions are the same, i.e., H 0 = l E = l P = l at least one average is different . We used a degree of confidence of 95% and a significance level a = 0.05. We used the MiniTab software to calculate the P value, which represents the descriptive level of the test. If
P is not greater than a = 0.05, we reject the null hypothesis. The P value for each of the analyzed questions is shown in Table 4 . According to Table 4 there is enough evidence to accept the hypothesis that, for the second and third questions, the groups come from populations with the same average, or in other words, have similar characteristics. In case of the first and fourth entries, there is strong evidence to reject the null hypothesis, meaning that the populations have different opinions about this feature. In some sense, however, it was expected that experts would find such a type of interface easier than the other groups, given their familiarity with this type of system and task. For the fourth question, it can be seen that the averages for the group of students and experts are more similar than the one for the group of professors, which means that the latter has a different opinion from the first two. This means that some features of the Lattes platform may be appreciated by some users and these features may be used or incorporated in our interface.

Accordingly, assuming that the groups come from similar populations regarding most features of the ser-vice and the answers of the questionnaire, we present Fig. 6 (a) that shows a comparison of the average values of answers for each group per question and Fig. 6 (b) that shows average values for all users.

It can be seen from Fig. 6 (a) and (b) that average values for the group of experts is higher than that of the other two groups, as expected, for the questions about previous knowledge of self-archiving services and the Dublin Core format. This may account for the better performance of this group in terms of submission time.
For the question about usefulness, the three groups had similar averages, around the value of 6 (for a max-imum of 7). With regard to the comparison with the Lattes Curriculum X  X  interface the average values for the group of professors was around 3 while for both, experts and students, the value was close to 2. We recall that, for this last question, the lower the value the better our interface was considered in the comparison, which means that in average all groups of users considered our interface better that the Lattes Curriculum X  X  one.
To compare the submission time we did not use the Kruskal X  X allis method, given the small quantity of submissions of some types of work such as books and book chapters. Fig. 7 (a) and (b) show the average group time per type of work and the total average time for all users respectively. It can be seen from these figures that the average time of submission was around 6 min. In average, the experts were faster than the other two groups. We attribute this difference to the fact that this group had all the data to be submitted a priori and also their previous knowledge in the Dublin Core and self-archiving services. When compared with the group of professors, students were also a bit faster. We accredit this difference to the high level of blank non-man-datory fields by the students when compared with the professors.

Conference papers took in average more time than journal articles. According to some comments provided by the users, 7 this was due to the fact that some users did not know the information to fill in some of the man-datory fields by the time of the submission, such as the ISSN for example. Another factor that might have increased the submission time for conference papers and journal articles was the time spent to identify the cor-rect conference or journal from a previously defined list of options in the interface. It was observed that most users did not explicitly type the abstract in the form but copied and pasted it from another available electronic version, therefore speeding up the overall submission process. This is in part a result of the initial focus in post-prints and may change for pre-prints and other types of publication.

Finally, it was also observed that generally users spent more time to submitting the first work than the sub-sequent ones, indicating that the more experienced with the interface the easier the submission process became, therefore reducing the submission time. Fig. 8 shows the average submission time for all users in the first, sec-ond, and third submission. 4.3.2. Dependencies among features
To visualize the dependencies among the several features of the interface, we used the  X  X eorderable matrix X  technique (also known as the permutation matrix) to provide a graphical treatment of the questionnaire data ( Bertin, 1981 ). This technique allows easy visualization and analysis of the data and is commonly used with cartographic data.

Fig. 9 shows the reorderable matrix with all questions. The leftmost column represents each question in the questionnaire, the lines represent the range of values, and columns 1 X 21 represent each individual user, the first seven being the experts, the next seven are the professors, and the rest, the students. For each value selected by a user, a black mark is inserted in the corresponding space. The goal of the reorderable matrix is graphically depict a cross-reference of multiple types of information whose visual perception of blanks and black marks allows an easy understanding of the whole distribution of values.

Some interesting results of the experiments are revealed from an analysis of the reorderable matrix. Among all users, 81% found the interface very easy to use, with values between 6 and 7. For these users, 76.5% had a high degree of knowledge about self-archiving, 41% came from the group of experts, 82% classified the level of comfort as very high and 82.4% also considered the interface of our service easier to use than the Lattes Cur-riculum X  X  interface. The users who had little experience with self-archiving spent more time during submission, with an average time 67.5% greater than the average time for all submissions. Moreover, 75% of these users classified the service X  X  interface with a high level of comfort and easiness to use, and all of them felt the inter-face useful for self-archiving, half with a value of 7 and the other half with a value of 5. 4.4. Discussion
Our experiments indicate that we have achieved many of our goals: to build an easy, comfortable, and use-ful self-archiving service for BDBComp. Moreover, the experiments seem to suggest that the service is learn-able (indicated by the decrease in submission time) and efficient (indicated by the short submission time).
These results are consistent among the three communities and within the communities themselves (as indicated by the statistical analysis). These results, however, can not indicate if our potential users will be motivated to utilize the system. This remains to be seen in the long term with the actual use of the service.
A few observations are worth making. Professors were always a slightly slower than the other two types of user and had the lowest previous knowledge of self-archiving services. This was somehow surprising but may be explained by some cultural and local issues. Most of the CS departments in Brazil do not maintain a col-lection of technical reports and the culture of self-archiving is not well-established among faculty members. It was expected that they had some familiarity with the Lattes Curriculum Platform, but most of the data is full-filled in this system by secretaries not by the professors themselves, specifically in our department where the experiment was conducted. This is substantiated by the larger difference of their average values for the question about the comparison with the Lattes Curriculum Platform and the averages for the other two com-munities. Since these professors are the ones producing content and preparing the next generation of scholars, this may have impact in the sustainability of BDBComp, which will rely on self-archiving as one of its major sources of content. A similar experiment in other CS departments needs to be performed to investigate this fact further.

The results also have some direct and indirect implications for other self-archiving services. Overall usabil-ity of the Lattes platform was considered very poor when compared with the BDBComp X  X  service. Despite the fact that the specific source of dissatisfaction of the users in this platform needs to be found, some of our design choices may help guide improvements in that system. For other self-archiving services, it was interest-ing to notice that the low previous knowledge of Dublin Core did not have a large impact in the evaluation measures. This may indicate that the choice of specific metadata standard may not be a major factor in the usability of the system. Despite small differences among the three groups in all evaluation measures (easiness, comfort, usefulness, and efficiency (time)), the differences were not sufficient to justify having different user interfaces for different communities. Further studies with other similar services, however, need to be con-ducted to confirm the last two observations above.

The most troublesome issue was the aspect of mandatory fields. Non-mandatory fields caused a lot of miss-ing information. Mandatory fields with controlled vocabulary (e.g., conference names and journal titles) caused most of the delays. Better ways to help users easily find the correct values for these fields and to keep the vocabulary updated need to be investigated. One solution for the second issue is, for example, to share information among users. For example, once a user enters the name or title for a new conference or journal which was not previously registered in the system, those values are shared with all other users. This, however, brings the problems of non-standardization and ambiguity. On the other hand, the fact that students were responsible for most of the blank non-mandatory fields may indicate that they were not motivated for filling the entries (performing the experiment only because  X  X he professor asked X ) or were not aware of the meaning or importance of those fields. Further experiments need to be conducted to test these hypothesis.

Finally, since ours is one of the first evaluations of this type of service, this study may serve as a basis and provide guidelines to conduct new and better experiments with other similar self-archiving services and systems. 5. Conclusions and future work In this article, we described a statistically sound evaluation of a new self-archiving service for the Brazilian
Digital Library of Computing. Our experiments indicated that the users found the service easy to learn, com-fortable, and useful, which we expect to play a key role in the sustainability of this DL. While several services exist with similar characteristics, few, if any, similar studies of such services, as the one presented here, have been reported in the DL literature. We believe that our experience in tailoring the service to the specific needs of the Brazilian community also should be helpful to others facing similar issues.

As future work, we intend to support the submission of new types of publication (e.g., electronic theses and dissertations) and to evaluate the impact of the service in the expansion of the BDBComp content in the long term as well as on its sustainability. We have also plans to perform more experiments to test some of the incon-clusive aspects of the study, raised in Section 4.4 , including  X  X hink-aloud X  and more controlled experiments as well as experiments outside of the original environment in which our experiments were conducted. We have also plans to perform log-based studies in the long run, once the service is put in production. Acknowledgements
This work was partially supported by the I3DL project (MCT/CNPq/ProTeM-CC Grant 680154/01-9) and the 5S-QV project (MCT/CNPq/CT-INFO Grant 55.1013/2005-2). The first author was supported by a schol-arship from CAPES. The second and third authors are currently supported by CNPq. The authors would like to thank Dr. Marcelo Azevedo, from the UFMG Statistics Department, for his assistance in the statistical analysis of the questionnaire data.
 References
