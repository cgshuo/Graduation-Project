 Mladen Kolar mladenk@cs.cmu.edu Eric P. Xing epxing@cs.cmu.edu Covariance matrices and their inverses, precision ma-trices, arise in a number of applications including prin-cipal component analysis, classification by linear and quadratic discriminant analysis, and the identification of conditional independence assumptions in the con-text of Gaussian graphical models. As a result, obtain-ing good estimators of covariance and precision matri-ces under various contexts is of essential importance in statistics and machine learning research. In the con-text of Gaussian Markov Random Fields (MRFs), the graph structure encodes certain conditional indepen-dence assumptions; if variables corresponding to nodes a and b are conditionally independent given the re-maining variables, then there is no edge between nodes a and b . As a precision matrix parametrizes a Gaus-sian MRF and a zero element in the precision ma-trix implies that two variables are conditionally inde-pendent, the problem of estimating precision matrices commonly arises in the context of learning the struc-ture and parameters of Gaussian MRFs. The availabil-ity of high-dimensional data, where the sample size n can be small relative to the dimension p , has pushed the focus of research towards methods for estimating sparse precision matrices under proper regularizations. See, for example, Meinshausen &amp; B  X uhlmann (2006), Peng et al. (2009), Cai et al. (2010), Ravikumar et al. (2008), Rothman et al. (2008), and Yuan &amp; Lin (2007). Many theoretical results have been obtained for the high-dimensional problems, including consistency and rate of convergence results under a variety of assump-tions, as well as efficient algorithms to numerically find estimates. However, all of the above approaches have been devised to deal with the case where all data are fully observed.
 In practice, we often have to analyze data that contains missing values (Little &amp; Rubin, 1987). Missing values may occur due to a number of reasons, for example, faulty machinery that collects data, subjects not be-ing available in subsequent experiments (longitudinal studies), limits from experimental design, etc. When missing values are present, they are usually imputed to obtain a complete data set on which standard methods can be applied. However, methods that directly per-form statistical inference, without imputing missing values, are preferred. A systematic approach to miss-ing values problem is based on likelihoods of observed values. However, with an arbitrary pattern of miss-ing values, no explicit maximization of the likelihood is possible even for the mean values and covariance matrices (Little &amp; Rubin, 1987). Expectation maxi-mization algorithms, which are iterative methods, are commonly used in cases where explicit maximization of the likelihood is not possible; however, providing theoretical guarantees for such procedures is difficult. This approach was employed in St  X adler &amp; B  X uhlmann (2009) to estimate sparse inverse covariance matrices, which we will review in the following section. In re-cent work, Lounici (2012) deals with the estimation of covariance matrices from data with missing values under the assumption that the true covariance matrix is approximately low rank. Loh &amp; Wainwright (2011) recently studied high-dimensional regression problems when data contains missing values. Casting the esti-mation of a precision matrix as a sequence of regres-sion problems, they obtain an estimator of the pre-cision matrix without maximizing partially observed likelihood function using an EM algorithm.
 In this work, we present a simple, principled method that directly estimates a large dimensional precision matrix from data with missing values. We form an un-biased estimator of the covariance matrix from avail-able data, which is then plugged into the penalized maximum likelihood objective for a multivariate Nor-mal distribution to obtain a sparse estimator of the precision matrix. Even though the initial estimator of the covariance matrix is not necessarily positive-definite, we can show that the final estimator of the precision matrix is positive definite. Furthermore, un-like the EM algorithm, which is only guaranteed to converge to a local maximum, we prove consistency and convergence rate for our estimator in the Frobe-nius norm, spectral norm and `  X  norm. Our results have important practical consequences as they allow practitioners to use existing tools for penalized covari-ance selection (see, e.g., Friedman et al., 2008), which are very efficient in high-dimensions for data sets with missing values without changing the algorithm or re-sorting to the iterative EM algorithm.
 Throughout the paper we assume that the missing values are missing at random in the sense of Rubin servations with samples organized into rows, and let served values, that is, r ij = 1 if the value x ij observed and r ij = 0 otherwise. We assume that the data is missing completely at random (MCAR), which means that P [ R | X , X  ] = P [ R |  X  ] for all X and  X  , where  X  denotes unknown parameters. The MCAR assumption implies that the missingness does not de-pend on the observed values, e.g., in a distributed en-vironment, each sensor may fail independently from other sensors. This assumption is relaxed in the ex-perimental section where we test the robustness of our procedure when the missing data mechanism departs from the MCAR assumption. Another more realistic assumption is called missing at random (MAR), which assumes P [ R | X , X  ] = P [ R | X obs , X  ] for all X mis  X  , where X obs denotes the observed components of X and X mis denotes the missing components. The MAR assumes that the distribution of R depends on the ob-served values of X , but not on the missing values, e.g., cholesterol level may be measured only if patient has high blood pressure. Finally, the missing data mech-anism is called not-missing at random (NMAR) if the distribution of R depends on the non-observed values of X . Estimation under NMAR is a hard problem, as one needs to make assumptions on the model for miss-ing values. The method presented in this paper can, in theory, be extended to handle the MAR case. Let { x i } n i =1 be an i.i.d. sample from the multivariate Normal distribution with the mean  X   X  R p and the matrix of missing values indicators with r ij = 1 if x ij is observed and 0 otherwise. The goal is to estimate the sparse precision matrix  X  =  X   X  1 from the data with missing values.
 Estimating covariance matrices from data with missing values is quite an old problem. See, for example, Afifi &amp; Elashoff (1966), Wilks (1932), Anderson (1957), Hocking &amp; Smith (1968), and Hartley &amp; Hocking (1971). However, literature on high-dimensional es-timation of covariance matrices from incomplete data is missing. Recently St  X adler &amp; B  X uhlmann (2009) pro-posed to use an EM algorithm, called MissGlasso, to estimate sparse precision matrices, which we review below.
 Yuan &amp; Lin (2007) proposed to estimate the sparse precision matrix by solving the following ` 1 -norm pe-nalized maximization problem where  X  S is the empirical covariance matrix,  X   X  :=  X   X  diag(  X  ) and || A || 1 = P ij | A ij | . The tuning pa-rameter  X  &gt; 0 controls the sparsity of the solution and hence the complexity of the solution. The opti-mization problem in (1) can be solved efficiently using a number of procedures (e.g., Friedman et al., 2008; Hsieh et al., 2011).
 When the data are fully observed, Yuan &amp; Lin (2007) arrived at the optimization procedure in (1) from the penalized maximum likelihood approach, with  X  S = n contains missing values, the log-likelihood of observed data takes the following form ` (  X  ,  X  ; { x i, obs } i ) =  X  where for a sample point x i we write x i = ( x i, obs , x i, mis ) to denote observed and missing compo-nents, and  X  i, obs and  X  i, obs are the mean and precision matrix of the observed components of x i . MissGlasso is an EM algorithm that finds a local maximum (  X   X  ,  X   X  ) of the ` 1 penalized observed log-likelihood. In the E-step, MissGLasso imputes the missing values by condi-tional means of the distribution. That is, imputation  X  obs ), where  X   X  and parameters. In the M-step, the optimization problem (1) is solved using the GLasso on data with imputed missing values. The procedure iterates between the E-step and the M-step until convergence to a local op-timum of the penalized observed log-likelihood. We will denote  X   X  EM , the final estimator of the precision matrix obtained by the EM algorithm. As the objec-tive is non-convex, it is difficult to establish theoretical guarantees on the solution produced by the EM. Next, we present our estimator. In this section, we propose a simple procedure based on the plug-in estimator of the covariance matrix from available data that can be used together with existing procedures for estimating precision matrices from fully observed data. Specifically, we will use the penalized likelihood approach, which was introduced in the pre-vious section in (1). From (1) it is obvious that we only need a sample estimate of the covariance matrix, which is plugged into a convex program that produces an estimate of the precision matrix.
 We form a sample covariance matrix from the available samples containing missing values as follows. Let  X  S = [ X   X  ab ] ab be the sample covariance matrix with elements where  X   X  = ( X   X  a ) is the sample mean defined as  X   X  a values in X are taken into account naturally and that the mean and covariance elements are estimated only from the observed sample. Under the MCAR assump-tion, it is simple to show that  X  S is an unbiased estima-tor of  X  , that is, E [  X  S ] =  X  .
 Our estimator is formed by plugging  X  S into the objec-tive in (1), which we will denote as  X   X  mGLasso . Note that  X  S is not necessarily a positive definite matrix, however, the minimization problem in (1) is still con-vex and the resulting estimator  X   X  mGLasso will be posi-tive definite and unique. In the next section, we lever-age the analysis of Ravikumar et al. (2008) to establish a number of good statistical properties of the estima-3.1. Selecting tuning parameters The procedure described in the previous section re-quires selection of the tuning parameters  X  , which con-trols the sparsity of the solution and balances it to the fit to data. A common approach is to form a grid of candidate values for the tuning parameter  X  and choose one that minimizes a modified BIC criterion BIC(  X  ) =  X  2 ` (  X   X  ,  X   X  ; { x i, obs } i )+log( n ) X Here (  X   X  ,  X   X  ) are estimates obtained using the tun-log-likelihood. Yuan &amp; Lin (2007) proposed to use P a  X  b 1I {  X   X  ab 6 = 0 } to measure the degrees of freedom. Performing cross-validation is another possibility for finding the optimal parameter  X  . In the V-fold cross-validation, samples are divided into V disjoint folds, say D v for v = 1 ,...,V , and the score is computed as CV(  X  ) = where (  X   X  v ,  X   X  v ) denote estimates obtained from the sample { x i } n i =1 \D v . The optimal tuning parameter is the one that minimizes CV(  X  ). The final estimates (  X   X  ,  X   X  ) are constructed using the optimization proce-dure with the tuning parameter  X   X  on all the data. 3.2. Related procedures Lounici (2012) and Loh &amp; Wainwright (2011) have recently proposed procedures for estimating approxi-mately low-rank covariance matrices and sparse pre-cision matrices, respectively, from high-dimensional data with missing values. In both works, a sample covariance estimator is formed, which is then plugged into an optimization procedure. The sample covari-ance estimator they consider, assuming ( r ia ) ia iid Bern(  X  ) with  X   X  (0 , 1] known, is defined as The estimator  X   X  is an unbiased estimator of the co-variance matrix, however, it requires knowledge of the parameter  X  .
 Procedure of Lounici (2012) is focused on estimating a covariance matrix under the assumption that the true covariance matrix is approximately low rank and hence is not comparable to our procedure. Loh &amp; Wainwright (2011) used a projected gradient descent method to obtain a solution to a high-dimensional re-gression problem when data contains missing values. A sparse precision matrix can be obtained by max-imizing an ` 1 penalized pseudo-likelihood, which re-duces to a sequence of regression problems. We note that the estimator  X   X  mGLasso can be obtained using any convex program solver that can solve (1), while the re-sults of Loh &amp; Wainwright (2011) rely on the usage of projected gradient descent. In this section, we provide theoretical analysis of the estimates  X   X  mGLasso , which we denote  X   X  throughout the section for notational simplicity, under the MCAR assumption. We start by analyzing the sample covari-ance matrix  X  S in (2). We will assume that each element of the missing values indicator matrix R is indepen-dently distributed as r ia  X  Bern(  X  ), i = 1 ,...,n, a = 1 ,...,p . Furthermore, we assume that a distribution of X has sub-Gaussian tails, that is, there exists a con-stant  X   X  (0 ,  X  ) such that A multivariate Gaussian distribution satisfies this con-dition. We define the function f ( n, X , X  ), which will be useful for characterizing probabilistic deviation of dif-ferent quantities, as Our first result characterizes the deviation of the sam-ple covariance matrix from the true covariance matrix. Lemma 1. Assume that X a / with parameter  X  2 . Fix  X  &gt; 0 and assume that n is big enough so that f ( n, X , X  )  X  1 / 2 . Then for any fixed ( a,b )  X  { 1 ,...,p } 2 , a 6 = b , with probability at least 1  X   X  , we have that |  X   X  ab  X   X  ab | X  C  X  p f ( n, X , X  ) where C  X  = 16 Similarly, we can obtain that for any diagonal elements of  X 
S the statement |  X   X  aa  X   X  aa | X  C  X  p f ( n, with probability 1  X   X  .
 We use Lemma 1 to prove properties of the estimate  X   X  mGLasso . We start by introducing some additional notation and assumptions. Following Ravikumar et al. (2008), we introduce the irrepresentable condition : where  X  =  X   X   X  , S := { ( a,b ) :  X  ab 6 = 0 } is sup-port of  X  and S C := { ( a,b ) :  X  ab = 0 } , and ||| X ||| is the `  X  /`  X  -operator norm. Furthermore, we define K
 X  := |||  X  |||  X  and K  X  := ||| (  X  SS )  X  1 |||  X  . The maxi-mum number of non-zero elements in a row of  X  is denoted d := max a =1 ,...,p |{ b :  X  ab 6 = 0 }| . The rate of convergence will depend on these quantities.
 Theorem 2. Suppose that the distribution of X sat-isfies the irrepresentable condition in (3) with param-eter  X   X  (0 , 1] and that the missing values indicator matrix R has i.i.d. Bern(  X  ) elements, that is, the data is missing completely at random with probability 1  X   X  . Furthermore, assume that the conditions of Lemma 1 hold. Let  X   X  be the unique solution for the regular-ization parameter  X  = 8  X  C  X  p f ( n, X ,p  X   X  ) with some  X  &gt; 2 and C  X  = 16 ple size satisfies n &gt; where C 1 = 6 C  X  max { K  X  K  X  ,K 3  X  K 2  X  } then with prob-ability at least 1  X  p 2  X   X  max where  X   X  = [ X   X  ab ] ab and  X  = [  X  ab ] ab . The result follows from application of Theorem 1 in Ravikumar et al. (2008) to the tail bound in Lemma 1 and some algebra. A simple consequence of Theorem 2 is that the support  X  S of  X   X  consistently estimates the support S of  X  if all the elements of  X  are large enough in absolute values.
 Corollary 3. Under the same assumptions as in The-orem 2, we have that P [  X  S = S ]  X  1  X  p 2  X   X  if Proof follows by straightforward algebra from Theo-rem 2. Using the element-wise `  X  bound on deviation of  X 
 X  from  X  established in Theorem 2, we can simply establish the bounds on the convergence in the Frobe-nius and spectral norms.
 Corollary 4. Under the same assumptions as in The-orem 2, we have that with probability at least 1  X  p 2  X   X  where K = 2(1 + 8  X   X  1 ) K  X  C  X  .
 Proof follows by straightforward algebra from Theo-rem 2. We can compare the established results for  X   X  under the MCAR assumption to results of Ravikumar et al. (2008) for the fully observed case. We observe that the sample size increases by a factor of O (  X   X  2 ), while the rate of convergence in the element-wise `  X  norm is slower by a factor of O (  X   X  1 ). The parameter  X  which controls the rate of missing data is commonly considered a constant, however, it is clear from Theo-rem 2 that we could let  X   X  0 slowly as a function of n and p , while maintaining the convergence properties of the procedure. In this section, we perform a set of simulation studies to illustrate finite sample performance of our proce-dure. First, we show that the scalings predicted by the theory are sharp. Next, we compare our proce-dure to the EM algorithm, MissGLasso (St  X adler &amp; B  X uhlmann, 2009) and the projected gradient method (Loh &amp; Wainwright, 2011), PGLasso. Furthermore, we can explore robustness of our method when the data generating process departs from the one assumed in Section 4. 5.1. Verifying theoretical scalings Theoretical results given in Section 4 predict behav-ior of the error when estimating the precision ma-trix. In particular, Corollary 3 suggests that we need O ( d 2 log( p )) samples to estimate the graph structure consistently and Corollary 4 states that the error in the operator norm decreases as O ( d p log( p ) /n ). There-fore, if we plot the error curves against appropriately rescaled sample size, we expect them to align for dif-ferent problem sizes. To verify this, we create a chain-structured Gaussian graphical model (following Loh &amp; Wainwright (2011)), so that d = 2 and the precision matrix  X  is created as follows. Each diagonal element is set to 1, and all the entries corresponding to the chain are set equal to 0 . 1. The precision matrix is rescaled so that |||  X  ||| 2 = 1 and  X  = 0 . 8. Figure 1 shows the hamming distance between the sup-port of  X   X  and  X  plotted against the rescaled sample size. Vertical line marks a threshold in scaled sample size after which the pattern of non-zero element of the precision matrix is consistently recovered. Figure 2 shows that the error curves align when the sample size is rescaled, as predicted by the theory. 5.2. Data missing completely at random Our first simulation explores the MCAR assumption. We use models from St  X adler &amp; B  X uhlmann (2009): Model 1 :  X  ab = 0 . 7 | a  X  b | , so that the elements of the covariance matrix decay exponentially.
 Model 2 : where the symbol 1I represents the indicator function which is 1 if a = b and 0 otherwise.
 Model 3 :  X  = B +  X  I , where each off-diagonal entry of B is generated independently and equals 0 . 5 with probability  X  = 0 . 1 or 0 with probability 1  X   X  . Diag-onal entries of B are zero, and  X  is chosen so that the condition number of  X  is p .
 We report convergence results in the operator norm. We also report precision and recall for the perfor-mance on recovering the sparsity structure of  X  , where Section 3.1, the tuning parameter  X  is selected by min-imizing the BIC criterion. We observed that using the tuning parameters that minimize the cross-validation loss result in complex estimates with many falsely se-lected edges (results not reported).
 We set the sample size and number of dimen-sions ( n,p ) = (100 , 100) , (150 , 200) , (200 , 500) for each model and report results averaged over 50 independent runs for each setting. For each generated data set, we remove completely at random 10%, 20% and 30% en-tries. Results on recall and precision for different de-grees of missingness are reported in Table 1, while the operator norm convergence results are reported in Ta-ble 2. From the simulations, we observe that mGLasso performs better than the EM algorithm on the task of recovering the sparsity pattern of the precision matrix. PGLasso does well on Model 1, but does not perform so well under Model 2 and 3. Model 2 is a difficult one for recovering non-zero patterns, as the true pre-cision matrix contains many small non-zero elements. The EM algorithm performs better than mGLasso and PGLasso measured by |||  X   X   X   X  ||| 2 , with mGLasso doing better than PGLasso. However, on average the EM al-gorithm requires 20 iterations for convergence, which makes mGLasso about 20 times faster on average. 5.3. Data missing at random In the previous section, we have simulated data with missing values completely at random, under which consistency of the estimator  X   X  mGLasso given in Sec-tion 3 can be proven. When the missing values are pro-duced at random (MAR), the EM algorithm described is still valid, however, the estimator  X   X  mGLasso is not. Little (1988) provided a statistical test for checking whether missing values are missing completely at ran-dom, however, no such tests exist for high-dimensional data. In this section, we will observe how robust our estimator is when the data generating mechanism de-parts from the MCAR assumption. When the missing data mechanism is NMAR, then neither the EM al-gorithm, nor the procedures described Section 3 are valid.
 We will use the model considered in St  X adler &amp; B  X uhlmann (2009) in Section 4.1.2. The model is a Gaussian with p = 30, n = 100 and the covariance matrix is block-diagonal,  X  = diag( B , B ,..., B ) with B  X  R 3  X  3 , b ab = 0 . 7 | a  X  b | . Missing values are created using the following three mechanisms: 1. For all j = 1 ,..., b p/ 3 c and i = 1 ,...,n : x i, 3  X  j 2. For all j = 1 ,..., b p/ 3 c and i = 1 ,...,n : x i, 3  X  j 3. For all j = 1 ,..., b p/ 3 c and i = 1 ,...,n : x i, 3  X  j The threshold value T determines the percentage of missing values. We consider three settings: 1)  X  = 0 . 25 and T =  X   X  1 (0 . 25), 2)  X  = 0 . 5 and T =  X   X  1 (0 . 5). and 3)  X  = 0 . 75 and T =  X   X  1 (0 . 75) where  X (  X  ) is the stan-dard Normal cumulative distribution function. The first missing data mechanism is MCAR as the miss-ing values do not depend on the observed values. The second missing data mechanism is MAR as the miss-ing value indicators depend on the observed values of other variables. Finally, the third missing data mech-anism is NMAR as the missing data indicators depend on the unobserved values.
 Results of the simulation, averaged over 50 indepen-dent runs, are summarized in Table 3 and Table 4. We first observe that when the missing values are not missing at random, performance of all procedures de-grades. Furthermore, the EM algorithm performs bet-ter than the other two methods when the data is gen-erated under MAR. This is expected, since our pro-posed procedure is not valid under this assumption. Note, however, that mGLasso performs better than PGLasso under this simulation scenario. We have proposed a simple estimator for the precision matrix in high-dimensions from data with missing val-ues. The estimator is based on a convex program that can be solved efficiently. In particular, from our simu-lation studies, we observed that the algorithm runs on average 20 times faster than the EM algorithm. Fur-thermore, the estimator does not require imputation of the missing values and can be found using exist-ing numerical procedures. As such, we believe that it represents a viable alternative to the iterative EM algorithm.
 From the analysis in Section 4, it is clear that other procedures for estimating precision matrices from fully observed data, such as the Clime estimator (Cai et al., 2011), could be easily extended to handle data with missing values. Theoretical properties of those proce-dures would be established using the tail bounds on the sample covariance matrix given in Lemma 1. There are two directions in which this work should be extended. First, the MCAR assumption is very strong and it is hard to check whether it holds in practice. However, we have observed in our simulation studies that under the MAR assumption, which is a more re-alistic assumption than MCAR, performance of the estimators does not degrade dramatically when esti-mating the support of the precision matrix. However, estimated parameters are quite far from the true pa-rameters. This could be improved by using a weighted estimator for the sample covariance matrix (see, e.g., Robins et al., 1994). Second, it is important to es-tablish sharp lower bounds for the estimation problem from data with missing values, which should reflect de-pendence on the proportion of observed entries  X  (see Lounici, 2012).
 Acknowledgments : This work is supported in part
