 Time-aware retrieval models exploit one of two time dimen-sions, namely, (a) publication time or (b) content time (tem-poral expressions mentioned in documents). We show that the effectiveness for a temporal query (e.g., illinois earthquake 1968 ) depends significantly on which time dimension is fac-tored into ranking results. Motivated by this, we propose a machine learning approach to select the most suitable time-aware retrieval model for a given temporal query. Our method uses three classes of features obtained from analyz-ing distributions over two time dimensions, a distribution over terms, and retrieval scores within top-k result docu-ments. Experiments on real-world data with crowdsourced relevance assessments show the potential of our approach.
Previous work [1, 4] has shown that the retrieval effec-tiveness of temporal queries can be significantly improved by modeling and taking into account publication time (i.e., when a document was published) or content time (i.e., what time a document refers to). The right choice of time-aware retrieval model can make a huge difference for a given tem-poral query, as we observe empirically. Thus, the model based on publication time proposed in [4] (labeled PT-Rank ) performs best for temporal queries like iraq 2001 and mac os x 24 march 2001 , whereas the model based on content time from [1] (labeled CT-Rank ) performs best for temporal queries like sound of music 1960s and michael jackson 1982 .
Our contribution in this work is a novel machine learn-ing approach to select the most suitable time-aware retrieval model for a given temporal query  X  to the best of our knowl-edge the first approach tackling this objective. It uses three classes of features obtained from analyzing distributions over two time dimensions, a distribution over terms, and retrieval scores within top-k result documents. We further present ex-perimental results, showing the significance of the problem addressed and the effectiveness of our approach. A document d consists of a textual part d text (a bag of words) and a temporal part d time composed of its publica-tion time PubTime ( d ), and temporal expressions { t 1 ,...,t mentioned in d , denoted ContentTime ( d ). A temporal query q consists of keywords q text , and temporal expressions q PT-Rank and CT-Rank both employ a mixture model that linearly combines textual similarity and temporal similarity between q and d as S ( q,d ) = (1  X   X  )  X  S 0 ( q text ,d S ( q time ,d time ) using a mixing parameter  X  . The textual similarity S 0 can be determined using any existing term-based retrieval model (e.g., tf.idf or a unigram language model). The temporal similarity S 00 is determined assum-ing that temporal expressions in the query are generated independently from a two-step generative model, i.e.: For CT-Rank [1] the probability P ( t q | t d ) is estimated ac-cording to the L MT U method based on content time . For PT-Rank [4] P ( t q | t d ) is estimated based on publication time using an exponential decay function. For both methods Jelinek-Mercer smoothing eliminates zero probabilities.
Given a temporal query q , we will predict which time-aware retrieval model achieves the best effectiveness by learn-ing a prediction model using three classes of features:
Temporal KL-divergence , originally proposed in [3], measures the difference between the distribution of publi-cation time within a set of top-k result documents D q and their distribution in the overall document collection C . This definition thus only considers publication time, and we fur-ther refer to it as KL P T . While it gives a strong signal, for instance, when all relevant documents were published around the occurrence of an important real-world event (e.g., a sports tournament), it does not capture when they all refer to a common time period (e.g., the 19th century). We there-fore adapt temporal KL-divergence to also consider content time as KL CT ( D q || C,q ) = P t  X  T T
C is a set of all temporal expressions in C . P ( t | T C probability of a temporal expression t in C . P ( t | q ) is the probability of generating a temporal expression t given q : P ( t | q ) = P d  X  D retrieval score of d wrt. a particular retrieval model. Since a document can contain more than one temporal expression, rences of t 0 in d . Again, we employ Jelinek-Mercer smooth-ing when estimating P ( t | d ) to avoid zero probabilities.
As suggested in [3], temporal features alone could not achieve high accuracy for query classification. Thus, we also employ a clarity score [2] for measuring the KL-divergence between the distribution of terms within top-k results D and their distribution in the overall document collection C . A clarity score can be computed as Clarity = P w  X  V P ( w | q )  X  distinct terms in C . P ( w | q ) is the probability of generating w given q and P ( w | C ) is the probability of w in C .
Retrieval scores can also be exploited to select a re-trieval model, as proposed in [5]. We employ different fea-tures obtained from analyzing/comparing the retrieval scores of a term-based baseline model that is not time-aware, PT-Rank , and CT-Rank , namely: 1) average score of the base-line ( AVG base ), 2) average score of PT-Rank ( AVG PT-Rank 3) average score of CT-Rank ( AVG CT-Rank ), and 4) the di-vergence of retrieval scores according to PT-Rank and CT-Rank from those produced by the baseline ( JS PT-Rank and JS
CT-Rank ). We employ Jensen-Shannon divergence to mea-sure the extent to which the time-aware models alter the scores of the baseline retrieval model, formally: JS ( S b || S r ,q ) = X where S b ( q,d ) is the retrieval score of d according to the baseline S b . S r ( q,d ) is the score of d when ranked using a time-aware retrieval model S r  X  X  PT-Rank , CT-Rank } .
We conducted two sets of experiments: 1) evaluate our prediction model as classification accuracy, and 2) demon-strate how an accurate choice of the retrieval model can im-prove retrieval effectiveness. We used the New York Times Annotated Corpus containing 1.8M documents published between 1987 and 2007, and the 40 temporal queries and relevance assessments from [1]. Temporal expressions were extracted using the TARSQI Toolkit. Documents were in-dexed and retrieved with Apache Lucene 2.9.3 using its de-fault similarity function as a baseline retrieval model. We consider both inclusive and exclusive modes of evaluating queries, described in [1], that differ in whether temporal expressions are also treated as textual query terms. The mixture parameter  X  was determined empirically:  X  = 0 . 5 for PT-Rank and  X  = 0 . 6 for CT-Rank in inclusive , and  X  = 0 . 5 for PT-Rank and  X  = 0 . 1 for CT-Rank in exclusive . The parameters for TSU were: DecayRate = 0 . 5,  X  = 0 . 5, and  X  = 6 months. For L MT U, smoothing  X  was 0 . 75. For temporal KL-divergence, smoothing was set to 0 . 1.
For classification, each query was labeled according to whether PT-Rank or CT-Rank performs best on it. More precisely, we assumed the model with the best MAP as a query label . We excluded queries with a small difference in MAP of two time-aware models. We learned a prediction model using several algorithms: decision tree, Na  X   X ve Bayes, neural network and SVM, using 10-fold cross-validation with 10 repetitions. We measured statistical significance using a t -test with p &lt; 0 . 05. In the tables, bold face indicates sta-tistically significant difference from the respective baseline.
Classification results. The baseline method for query classification is the majority classifier. The accuracy of the baseline is 0 . 54 for exclusive and 0 . 60 for inclusive . Ta-ble 1 shows the accuracy of the best-performing classifiers, i.e., SVM for exclusive and decision tree for inclusive . The Table 2: Effectiveness of different retrieval models. results show that prediction accuracy tends to be better when using k = 100 rather than k = 500. One reason for this is that with the larger number of top-k documents, more irrelevant documents are introduced into the analysis. The performance among different feature classes shows that JS
PT-Rank performs well in most case. For exclusive , using a small number of top-k documents is better than a large number of top-k documents. For top-100, JS PT-Rank outper-forms the baseline classifier and other features significantly (accuracy=0.72). For top-500, all single features perform worse compared to the baseline classifier. For inclusive , the performance of top-100 is better than top-500. For top-100, the best performing feature is the combination of Clarity , JS PT-Rank and JS CT-Rank , which achieves an accuracy of 0.75.
Retrieval results. For each query, we determined re-trieval results using a model chosen according to the best prediction model determined in the previous experiment, such as, 1) JS PT-Rank for retrieval in exclusive , and 2) Clar-ity + JS PT-Rank + JS CT-Rank for retrieval in inclusive . Table 2 shows the effectiveness of different retrieval models, where PR is the retrieval model based on our prediction model. MAX is the maximum (or optimal) effectiveness that can be achieved, that is, if a prediction model performs accurately 100%. The retrieval results are compared with the baseline method CT-Rank . The results show that our prediction-based retrieval model ( PR ) outperforms the baseline signifi-cantly in P@1 and MAP. However, we note that it is difficult for PR to achieve the optimal effectiveness because of the classification accuracy as explained above.
We have demonstrated that selecting the right time-aware retrieval model can have a significant impact on the retrieval effectiveness of temporal queries. We proposed a novel ma-chine learning approach to do so automatically and demon-strated its effectiveness through extensive experiments.
