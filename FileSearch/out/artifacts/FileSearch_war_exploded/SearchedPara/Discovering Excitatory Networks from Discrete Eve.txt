
Discrete event streams are prevalent in many applications, such as neuronal spike train analysis, physical plants, and human-computer interaction modeling. In all these domains, we are given occurrences of events of interest over a time course and the goal is to identify trends and behaviors that serve discriminatory or descriptive purposes.

A Multi-Electrode Array (MEA) records spiking action potentials from an ensemble of neurons which after various pre-processing steps, yields a spike train dataset providing real-time, dynamic, perspectives into brain function (see Fig. 1). Identifying sequences (e.g., cascades) of firing neurons, determining their characteristic delays, and recon-structing the functional connectivity of neuronal circuits are key problems of interest. This provides critical insights into the cellular activity recorded in the neuronal tissue.
Similar motivations arise in other domains as well. In physical plants the discrete event stream denotes diagnostic and prognostic codes from stations in an assembly line and the goal is to uncover temporal connections between codes emitted from different stations. In human-computer inter-action modeling, the event stream denotes actions taken by users over a period of time and the goal is to capture aspects such as user intent and interaction strategy by understanding causative chains of connections between actions.

Beyond uncovering structural patterns from discrete generative temporal process model for the data. In particular, our aim is to infer dynamic Bayesian networks (DBNs) which encode conditional independencies as well as tem-poral influences and which are also interpretable patterns in their own right. We focus exclusively on excitatory networks where the connections are stimulative rather than inhibitory in nature (e.g.,  X  X vent A stimulates the occurrence of event B 5ms later which goes on to stimulate event C 3ms beyond. X ) This constitutes a large class of networks with relevance in multiple domains, including neuroscience.

Our main contributions are three-fold: 1) New model class of excitatory networks : Learning 2) New methods for learning DBNs : We demonstrate 3) New applications to spike train analysis : We demon-
The paper is organized as follows. Sec. II gives a brief overview of DBNs and Sec. III presents our formalism for modeling event streams using DBNs. Sec. IV defines excitatory networks and develops the theoretical basis for efficiently learning such networks. Sec. V introduces fixed-delay episodes and relates frequencies of such episodes with marginal probabilities of a DBN. Our learning algorithm is presented in Sec. VI, experimental results in Sec. VII and conclusions in Sec. VIII.

Formal mathematical notions are presented in the next section, but here we wish to provide some background context to past research in Bayesian networks (BNs). As is well known, BNs use directed acyclic graphs to encode prob-abilistic notions of conditional independence, such as that a node is conditionally independent of its non-descendants given its parents (for more details, see [1]). The earliest known work for learning BNs is the Chow-Liu algorithm [2]. It showed that, if we restricted the structure of the BN to a tree, then the optimal BN can be computed using a minimum spanning tree algorithm. It also established the tractability of BN inference for this class of graphs.
 More recent work, by Williamson [3], generalizes the Chow-Liu algorithm to show how (discrete) distributions can be approximated using the same general ingredients as the Chow-Liu approach, namely mutual information quantities between random variables. Meila [4] presents an acceler-ated algorithm that is targeted toward sparse datasets of high dimensionality. The approximation thread for general BN inference is perhaps best exemplified by Friedman X  X  sparse candidate algorithm [5] that presents various greedy approaches to learn (suboptimal) BNs.

DBNs are a relatively newer development and best ex-amples of them can be found in specific state space and dynamic modeling contexts, such as HMMs. In contrast to their static counterparts, exact and efficient inference for general classes of DBNs has not been studied well.

Consider a finite alphabet, E = { A 1 ,...,A M } , of event-types (or symbols). Let s =  X  ( E 1 , X  1 ) , ( ..., ( E n , X  n )  X  denote a data stream of n events over E . Each E ,i = 1 ,...,n , is a symbol from E . Each  X  i ,i = 1 ,...,n , takes values from the set of positive integers. The events in s are ordered according to their times of occurrence,  X  last event in s , is denoted by  X  n = T . We model the data stream, s , as a realization of a discrete-time random process X ( t ) ,t = 1 ,...,T ; X ( t ) = [ X X type, A j  X  E , at time t . Thus, for j = 1 ,...,M and t = 1 ,...,T , we will have X X indicator random variable for event-type, A j , at time t .
Example 1: The following is an example event sequence of n = 7 events over an alphabet, E = { A,B,C,...,Z } , of M = 26 event-types:  X  ( A, 2) , ( B, 3) , ( D, 3) , ( B, 5) , ( C, 9) , ( A, 10) The maximum time tick is given by T = 12 . Each X ( t ) , t 1 ,..., 12 , is a vector of M = 26 indicator random variables. Since there are no events at time t = 0 in the example sequence (1), we have X (1) = 0 . At time t = 2 , we will have X (2) = [1000  X  X  X  0] 0 . Similarly, X (3) = [0101  X  X  X  and so on.

A DBN [6] is a DAG with nodes representing random variables and arcs representing conditional dependency re-lationships. We model the random process X ( t ) (or equiv-alently, the event stream s ), as the output of a DBN. Each event-indicator, X j ( t ) , t = 1 ,...,T and j = 1 ,...M , corresponds to a node in the network, and is assigned a set of parents, which is denoted as  X  ( X j ( t )) (or simply  X  ( t ) ). A parent-child relationship is represented by an arc (from parent to child) in the DAG. In a DBN, nodes are conditionally independent of their non-descendants given their parents. The joint probability distribution of X under the DBN model, can be factorized as a product of P [
X j ( t ) |  X  j ( t )] for various j,t . In this paper we restrict the class of DBNs using the following two constraints:
A1 [Time-bounded causality] For user-defined param-
A2 [Translation invariance] If  X  j ( t ) = { X j 1 ( t While A1 limits the range-of-influence of a random variable, X  X  , A2 is a structural constraint that allows parent-child re-lationships to depend only on relative (rather than absolute) time-stamps of random variables. Further, we also assume that the underlying data generation model is stationary, so that joint-statistics can be estimated using frequency counts of suitably defined temporal patterns in the data.
 A3 [Stationarity] For every set of event-indicators, Learning network structure involves learning the map,  X  ( t ) , for each X I [ X j ( t ) ;  X  j ( t )] denotes the mutual information between X posed as a problem of approximating the data distribution, P [  X  ] , by a DBN distribution, Q [  X  ] . Let D KL ( P || Q the KL divergence between P [  X  ] and Q [  X  ] . Using A1 , A2 and A3 , and following the lines of [2], [3], it is possible to show that for parent sets with sufficiently high mutual information to X j ( t ) , D KL ( P || Q ) will be concomitantly lower [7]. In other words, a good DBN-based approximation of the underlying stochastics (i.e. one with small KL divergence) can be achieved by picking, for each node, a parent-set whose corresponding mutual information exceeds a user-defined threshold.

However, picking such sets with high mutual information (while yields a good approximation) falls short of unearthing useful dependencies among the random variables. This is because mutual information is non-decreasing as more ran-dom variables are added to a parent-set (leading to a fully-connected network always being optimal). For a parent-set to be interesting, it should not only exhibit sufficient correlation (or mutual information) with the corresponding child-node, but should also successfully encode the conditional inde-pendencies among random variables in the system. This can be done by checking if, conditioned on a candidate parent-set, the mutual information between the corresponding child-node and all its non-descendants is always close to zero. (We provide more details later in Sec. VI-C).

The structure-learning approach described in Sec. III is applicable to any general DBN that satisfies A1 and A2 . In this paper, we focus on a further specialized class of net-works, called excitatory networks , where only certain kinds of conditional dependencies among nodes are permitted. In general, each event-type has some baseline propensity in the data which is small and less than 0.5 (This corresponds to a sparse-data assumption). A collection,  X  , of random variables is said to have an excitatory influence on an event-type, A  X  E , if occurrence of events corresponding to the variables in  X  , increases the propensity of A to greater than 0.5 . We define an excitatory network as one in which nodes can only exert excitatory influences on one another. For example, in an excitatory network it is possible that  X  X f B , C and D occur (say) 2 time-ticks apart, the probability of A increases. X  By contrast, in excitatory networks, it is not possible to model relationships like  X  X hen A does not occur, the probability of B occurring 3 time-ticks later increases. X  Similarly, excitatory networks cannot model inhibitory relationships like  X  X hen A occurs, the probability of B occurring 3 time-ticks later decreases. X  Excitatory networks are natural in neuroscience, where one is interested in unearthing conditional dependency relationships among neuron spiking patterns. Several regions in the brain are known to exhibit predominantly excitatory relationships [8] and our model is targeted toward unearthing these.

The excitatory assumption manifests as a set of constraints on the conditional probability tables associated with the DBN. Consider a node 1 X A (an indicator variable for event-type A  X  X  ) and let  X  denote a parent-set for X A . In excita-tory networks, the probability that A occurs, conditioned on the occurrence of all the events associated with  X  , should be at least as high as the corresponding conditional probability when only some (though not all) of the events of  X  occur. Further, the probability of A is less than 0.5 when none of the events of  X  occur and greater than 0.5 when all the events of  X  occur. For example, let  X  = { X B ,X C ,X The conditional probability table for A given  X  is shown in Table I. The different conditioning contexts come about by the occurrence or otherwise of each of the events in  X  . These are denoted by a j , j = 0 ,..., 7 . So while a 0 represents the all-zero assignment (i.e. none of the events B , C or D occur), a 7 (or a  X  ) denotes the all-ones assignment (i.e. all the events B , C and D occur). The last column of the table lists the corresponding conditional probabilities along with the associated excitatory constraints. The baseline propensity of A is denoted by and the only constraint on it is that it must be less than 1 2 . Conditioned on the occurrence of any event of  X  , the propensity of A can only increase, and hence, j  X   X  j and  X   X  . Similarly, when both C and D occur, the probability must be at least as high as that when either C or D occurred alone (i.e. we must have 3  X  1 and  X  2 ). Finally, for the all-ones case, denoted by  X  = a  X  the conditional probability must be greater than 1 2 and must also satisfy  X   X  j  X  j .

In the context of DBN structure learning, excitatory networks ensure that event-types will occur frequently after their respective parents (with suitable delays). This will allow us to estimate DBN structure using frequent pattern discovery algorithms (which have been a mainstay in data mining for many years). We now have a simple necessary condition on the probability (or frequency) of parent-sets in an excitatory network.
 Theorem 4.1: Let X A denote a node in the Dynamic Bayesian Network corresponding to the event-type A  X  E . Let  X  denote a parent-set with excitatory influence on A . Let  X  be an upper-bound for conditional probabilities P [
X A = 1 |  X  = a ] for all a 6 = a  X  (i.e. for all but the all-ones assignment in  X  ). If the mutual information I [
X A ;  X ] exceeds  X  ( &gt; 0) , then the joint probability of an occurrence of A along with all events of  X  satisfies P [ X A = 1 ,  X  = a  X  ]  X  P min  X  min , where and where h (  X  ) denotes the binary entropy function h (  X  q log q  X  (1  X  q ) log(1  X  q ) , 0 &lt; q &lt; 1 and h  X  1 [ its pre-image greater than 1 2 .
 Proof: Under the excitatory model we have P [ X A = 1 |  X  = a  X  ] &gt; P [ X A = 1 |  X  = a ]  X  a 6 = a  X  . First we apply terms in the expression for P [ X A = 1] : This gives us P [ X  = a  X  ]  X  P min (see Fig. 2). Next, since we are given that mutual information I [ X A ;  X ] exceeds  X  , the corresponding conditional entropy must satisfy:
H [ X A |  X ] = P [ X  = a  X  ] h ( P [ X A = 1 |  X  = a  X  ]) Every term in the expression for H [ X A |  X ] is non-negative, and hence, each term (including the first one) must be less than ( h ( P [ X A = 1])  X   X  ) . Using ( P [ X  = a  X  ]  X  P the above inequality and observing that P [ X A = 1 |  X  = must be greater than 0.5 for an excitatory network, we now get ( P [ X A = 1 |  X  = a  X  ] &gt;  X  min ) . This completes the proof.

In the framework of frequent episode discovery [9] the data is a single long stream of events over a finite alphabet (cf. Sec. III and Example 1 ). An ` -node (serial) episode,  X  , is defined as a tuple, ( V  X  ,&lt;  X  ,g  X  ) , where V  X  = denotes a collection of nodes, &lt;  X  denotes a total order that v i &lt;  X  v i +1 , i = 1 ,..., ( `  X  1) . If g  X  ( v E ,j = 1 ,...,` , we use the graphical notation ( A i A s =  X  ( E { 1 ,...,n } such that (i) E h ( v i th and j th events in the occurrence satisfy  X  h ( v in s .

Example 2: Consider a 3-node episode  X  = ( V  X  ,&lt;  X  g ) , such that, V and v 1 &lt;  X  v 3 , and g  X  ( v 1 ) = A , g  X  ( v 2 ) = g ( v  X  = ( A  X  B  X  C ) , indicating that in every occurrence of  X  , an event of type A must appear before an event of type B , and the B must appear before an event of type C . For exam-ple, in sequence (1), the subsequence  X  ( A, 1) , ( B, 3) constitutes an occurrence of ( A  X  B  X  C ) . For this occurrence, the corresponding h -map is given by, h ( v 1 h ( v 2 ) = 2 and h ( v 3 ) = 5 .

There are many ways to incorporate explicit time con-straints in episode occurrences like the windows-width con-straint of [9]. Episodes with inter-event gap constraints were introduced in [10]. For example, the framework of [10] can express the temporal pattern  X  B must follow A within 5 time-ticks and C must follow B within 10 time-ticks. X  Such a pattern is represented using the graphical notation, ( sub-case of the inter-event gap constraints, in the form of fixed inter-event time-delays. For example, ( A 5  X  B 10 represents a fixed-delay episode, every occurrence of which must comprise an A , followed by a B exactly 5 time-ticks later, which in-turn is followed by a C exactly 10 time-ticks later.

Definition 5.1: An ` -node fixed-delay episode is defined (serial) episode of [9], and D = (  X  1 ,..., X  `  X  1 ) is a sequence of ( `  X  1) non-negative delays. Every occurrence, h , of the fixed-delay episode in an event sequence s must satisfy the inter-event constraints,  X  i = (  X  h ( v 1 cal notation for inter-event episode, (  X , D ) , where A g ( v
Definition 5.2: Two occurrences, h 1 and h 2 , of a fixed-delay episode, (  X , D ) , are said to be distinct , if they do not share any events in the data stream, s . Given a user-defined, W &gt; 0 , frequency of (  X , D ) in s , denoted f s (  X , D ,W defined as the total number of distinct occurrences of (  X , D in s that terminate strictly after W .
 In general, counting distinct occurrences of episodes suffers from computational inefficiencies [11]. (Each occurrence of an episode ( A  X  B  X  C ) is a substring that looks like A  X  B  X  C , where  X  denotes a variable-length don X  X -care, and hence, counting all distinct occurrences in the data stream can require memory of the same order as the data sequence which typically runs very long). However, in case of fixed-delay episodes, it is easy to track distinct occur-rences efficiently. For example, when counting frequency of ( A 3  X  X  X  B 5  X  X  X  C ) , if we encounter an A at time t , to recognize an occurrence involving this A we only need to check for a B at time ( t + 3) and for a C at time ( t + 8) . In addition to being attractive from an efficiency point-of-view, we show next in Sec. V-A that the distinct occurrences-based frequency count for fixed-delay episodes will allow us to interpret relative frequencies as probabilities of DBN marginals. (Note that the W in Definition 5.2 is same as length of the history window used in the constraint A1 . Skipping occurrences terminating in the first W time-ticks makes it easy to normalize the frequency count into a probability measure).
 A. Marginals from episode frequencies
In this section, we describe how to compute mutual infor-mation from the frequency counts of fixed-delay episodes. For this, every subset of event-indicators in the network is associated with a fixed-delay episode.

Definition 5.3: Let { X j ( t ) : j = 1 ,...,M ; t = 1 ,...,T } denote the collection of event-indicators used to model event stream, s =  X  ( E 1 , X  1 ) ,... ( E n , X  alphabet, E = { A 1 , ...,A M } . Consider an ` -size subset, X = { X j without loss of generality, assume t 1  X   X  X  X   X  t ` . Define the ( `  X  1) inter-event delays in X as follows:  X  j = ( t (  X  ( X ) , D ( X )) , that is associated with the subset, X , of event-indicators is defined by  X  ( X ) = ( A j 1  X  X  X  X  X  X  A j and D ( X ) = {  X  1 ,..., X  `  X  1 } . In graphical notation, the fixed-delay episode associated with X can be represented as follows: For computing mutual information we need the marginals of various subsets of event-indicators in the network. Given mates for probabilities of the form, P [ X j 1 ( t 1 X given by Definition 5.3 and its frequency in the data stream, where W denotes length of history window as per A1 . Since an occurrence of the fixed-delay episode, (  X  ( X ) , D can terminate in each of the ( T  X  W ) time-ticks in s , the probability of an all-ones assignment for the random variables in X is given by:
P [ X j 1 ( t 1 ) = 1 ,...,X j For all other assignments (i.e. for assignments that are not all-ones) we use inclusion-exclusion to obtain corre-sponding probabilities. Inclusion-exclusion has been used before in data mining, e.g., in [12], to obtain exact or approximate frequency counts for arbitrary boolean queries using only counts of frequent itemsets in the data. In our case, counting distinct occurrences of fixed-delay episodes facilitates use of the inclusion-exclusion formula for ob-taining the probabilities needed for computing mutual in-formation of different candidate parent-sets. Consider the let A = ( a 1 ,...,a ` ) , a j  X  { 0 , 1 } , j = 1 ,...,` , be an assignment for the event-indicators in X . Let U  X  X  denote the subset of indicators out of X for which corresponding assignments (in A ) are 1 X  X , i. e. U = { X j k  X  X : Procedure 1 Overall Procedure Input: Alphabet E , event stream s =  X  ( E 1 , X  1 ) ,..., Output: DBN structure (parent-set for each node in the 1: for all A  X  X  do 2: X A := event-indicator of A at any time t &gt; W 3: Set f min = ( T  X  W ) P min  X  min , using Eqs. (2)-(3) 4: Obtain set, C , of fixed-delay episodes ending in A , 5: for all fixed-delay episodes (  X , D )  X  X  do 6: X (  X , D ) := event-indicators corresponding to (  X , D 7: Compute mutual information I [ X A ; X (  X , D ) ] 8: Remove (  X , D ) from C if I [ X A ; X (  X , D ) ] &lt;  X  9: Prune C using conditional mutual information cri-10: Return (as parent-set for X A ) event-indicators corre-a compute the probabilities as follows: where f s ( Y ) is short-hand for f s (  X  ( Y ) , D ( frequency (cf. Definition 5.2 ) of the fixed-delay episode, (  X  ( Y ) , D ( Y )) .
 A. Overall approach
In Secs. III-V, we developed the formalism for learning an optimal DBN structure from event streams by using distinct occurrences-based counts of fixed-delay episodes to compute the DBN marginal probabilities. The top-level algorithm (cf. Sec. III) for discovering the network is to fix any time t &gt; W , to consider each X j ( t ) , j = 1 ,...,M , in-turn, and to find its set of parents in the network. Due to the translation invariance assumption A2 , we need to do this only once for each event-type in the alphabet.

The algorithm is outlined in Procedure 1 . For each A  X  X  , we first compute the minimum frequency for episodes ending in A based on the relationship between mutual information and joint probabilities as per Theorem 4.1 (line 3, Procedure 1 ). Then we use a pattern-growth approach (see Procedure 2 ) to discover all patterns terminating in A Procedure 2 pattern grow (  X , D , L (  X , D ) ) 1:  X  = W  X  span (  X , D ) 2: for all A  X  X  do 3: for  X  = 0 to  X  do 4: if  X  = 0 and ( A j 1 &gt; A or ` = 1) then 5: continue 8: if  X  ( E j , X  j ) such that E j = A and  X  i  X   X  j =  X  9: Increment f s (  X  0 , D 0 ) 12: Add (  X  0 , D 0 ) to output set C 13: if span (  X  0 , D 0 )  X  W then (line 4, Procedure 1 ). Each frequent pattern corresponds to a set of event-indicators (line 6, Procedure 1 ). The mutual information between this set of indicators and the node X is computed using exclusion-exclusion formula and only sets for which this mutual information exceeds  X  are retained as candidate parent-sets (lines 5-8, Procedure 1 ). Finally, we prune out candidate parent-sets which have only indirect influences on A and return the final parent-sets for nodes corresponding to event-type A (lines 9-10, Procedure 1 ). This pruning step is based on some conditional mutual information criteria (to be described later in Sec. VI-C). B. Discovering fixed-delay episodes
We employ a pattern-growth algorithm (Procedure 2) for mining frequent fixed-delay episodes because, unlike Apriori -style algorithms, pattern-growth procedures allow use of different frequency thresholds for episodes ending in different alphabets. This is needed in our case, since, in general, Theorem 4.1 prescribes different frequency thresh-olds for nodes in the network corresponding to different alphabets. The recursive procedure is invoked with (  X , D (
A, X  ) and frequency threshold f min = ( T  X  W ) P min  X  min (Recall that in the main loop of Procedure 1 , we look for parents of nodes corresponding to event-type A  X  X  ).
The pattern-growth algorithm listed in Procedure 2 takes as input, an episode (  X , D ) , a set of start times L (  X , D ) the event sequence s . L (  X , D ) is a set of time stamps  X  that there is an occurrence of (  X , D ) starting at  X  i in s . For example, if at level 1 we have (  X , D ) = ( C, X  ) , then L = { 1 , 4 , 5 , 8 , 9 } in the event sequence s shown in Fig 3. The algorithm obtains counts for all episodes like generated by extending (  X , D ) e.g. B 1  X  C , ... , A 5  X  C etc. For an episode say (  X  0 , D 0 ) = B 2  X  C , the count is obtained by looking for occurrences of event B at times  X  j =  X  i  X  where  X  i  X  X  ( C, X  ) . In the example such B  X  X  at  X  j  X  X  = { 2 , 3 , 6 } . The number of such occurrences (= 3) gives the count of B 2  X  C . At every step the algorithm tries to grow an episode with count f s &gt; f min otherwise stops. C. Conditional MI criteria The final step in determining the parents of a node X involves testing of some conditional mutual information criteria (cf. line 10, Procedure 1 ). The input to the step is a set C of (frequent) fixed-delay episodes ending in A . Each episode in C is associated with a set of event-indicators whose mutual information with X A exceeds  X  . Consider two such sets Y and Z , each having sufficient mutual information with X A . Our conditional mutual information criterion is: remove Y from the set of candidate parents (of X A ), if I [
X A ; Y |Z ] = 0 3 . We repeat this test for every pair of episodes in C .

To understand the utility of this criterion, there are two cases to consider: (i) either Y  X  X  or Z  X  X  , and (ii) both Y 6 X  X  and Z 6 X  X  . In the first case, our conditional mutual criterion will ensure that we pick the larger set as a parent only if it brings more information about X A than the smaller set. In the second case, we are interested in eliminating sets which have a high mutual information with X A because of indirect influences. For example, if the network were such that C excites B and B excites A , then X C can have high mutual information with X A , but we do not want to report X C as a parent of X A , since C influences A only through B . Our conditional mutual information criterion will detect this and eliminate X C from the set of candidate parents (of X
A ) because it will detect I
We present results on data gathered from both mathemat-ical models of spiking neurons as well as real neuroscience datasets.
 A. Neuronal network model
The approach here is to model each neuron as an inho-mogeneous Poisson process whose firing rate is a function of the input received by the neuron is recent past [13]: Eq. (7) gives the firing rate of the i th neuron at time t . The network inter-connect allowed by this model gives it the amount of sophistication required for simulating higher-order interactions. More importantly, the model allows for variable delays which mimic the delays in conduction path-ways of real neurons.
 I ( t ) = X parameters for the interactions. The higher order terms in the input contribute to the firing rate only when the i neuron received inputs from all the neurons in the term with corresponding delays. With suitable choices of parameters  X  ( . ) , one can simulate a wide range of networks.
 B. Types of Networks
In this section we demonstrate the effectiveness of our approach in unearthing different types of networks. Each of these networks was simulated by setting up the appropriate inter-connections, of suitable order, in our mathematical model.

Causative chains and higher order structures : A higher-order chain is one where parent sets are not restricted to be of cardinality one. In the example network of Fig. 4(a), there are four disconnected components with two of them having cycles. (Recall this would be  X  X llegal X  in a static Bayesian network formulation.) Also the component consisting of nodes 18 , 19 , 20 , 21 exhibits higher-order interactions. The node 20 fires with high probability when node 18 has fired 4 ms before and node 18 has fired 5 ms before. Similarly node 21 is activated by node 18 , 19 , 20 firing at respective delays. The complete network consists of 100 nodes (with the remainder of the nodes firing independently). Spike train data is generated for runs of 60 sec using the multi-neuronal simulator. The base firing rate for neurons is set at 20Hz and the activation probability of a child node (i.e. the conditional probability that the child node fires given its parents) is varied form 0.6 to 0.8 (by suitably selecting  X  ij algorithm reports good precision and recall over a range of  X  ( 0 . 05  X   X   X  0 . 5 ) and  X  ( 0 . 02  X   X   X  0 . 04 ). For lower values of (simulation) conditional probability, recall gradually drops but precision remains high (100%). Details are shown in Table II.
 Overlapping causative chains : The graph shown in Fig. 4(a) (b) has two chains 0  X  1  X  2  X  3 and 12  X  1  X  13  X  14  X  15  X  which share the node 1 .
 Here 1 can be independently excited by 0 or 12 . Also is activated by 0 , 1 together and 13 is activated by Thus a firing event on 0 excites the chain 0  X  1  X  2  X  3 where as a firing event on 12 excites the other chain. This shows one possible way in which neurons can participate in several different circuits at the same time (e.g. polychronous circuits [14]). Depending on the stimulus sequence, the same neurons can participate in different cascade firing events (en-coding completely unrelated pieces of information). For each node, our formulation reports multiple sets of nodes that satisfy the minimum mutual information  X  , thus unearthing 0 and 12 as two sets activating 1 . 0 , 1 and 12 , 1 are also found to be the parent sets of 2 and 3 respectively (with high precision and recall).

Syn-fire chains : Another important pattern often reported in neuronal spike train data is that of synfire chains. This consists of groups of synchronously firing neurons strung together repeating over time. In [10], it was noted that discovering such patterns required a combination of serial and parallel episode mining. But the DBN approach applies more naturally to mining such network structures. Again we are able to find the structure for a wide range of parameters. For larger histories or influence windows W , many combinations of nodes are frequent, slowing down the mining process. (For instance, network 4(c) takes 180 sec to mine as compared to 60 sec for network 4(a), on a dual core 3GHz Windows Vista computer with 3GB RAM.)
Polychronous circuits : Groups of neurons that fire in a time-locked manner with respect to each other are referred to as polychronous groups. This notion was introduced in [14] and gives rise to an important class of patterns. Once again, our DBN formulation is a natural fit for discovering such groups from spike train data. A polychronous circuit is shown in Fig 4(d). We are also able to discover overlapping polychronous circuits (where different sets of nodes can excite the same node). For relatively deep networks (having nodes with long ancestry) recall drops mainly because the nodes lower down in the graph are not excited sufficiently often (and hence do not meet the mutual information thresh-old). Detailed results are listed in Table III.
 C. Scalability
The scalability of our approach with respect to data length and number of variables is shown in Fig 5(a). Here four different networks with 50, 75, 100 and 125 variables respectively were simulated for time durations ranging from was fixed at 20 Hz. In each network 40% of the nodes were chosen to have upto three parents. The parameters of the DBN mining algorithm were chosen such that recall and precision are both high ( &gt; 80% ). It can be seen in the figures that for a network with 125 variables, the total run-time is of the order of few minutes along with recall &gt; 80% and precision at almost 100%.

Another way to study scalability is w.r.t. the density of the network, defined as the ratio of the number of nodes that are descendants for some other node to the total number of nodes in the network. Fig 5(b) shows the time taken for mining DBNs when the density is varied from 0.1 to 0.6, averaged over 36 datasets. We observe near linear growth in time taken and the absolute figures can be improved using native implementation (currently our algorithms are implemented in Python). D. Sensitivity
Finally, we discuss the sensitivity of the DBN mining algorithm to the parameters  X ,  X  and cond. mutual infor-mation threshold (used to check for MI=zero). To obtain precision-recall curves for our algorithm applied to data se-quences with different characteristics, we vary the parameter  X  in the range [0.04-0.06] and repeat for different cond. mutual information threshold values. The data sequence for this experiment is generated from the multi-neuronal simu-lator using different settings of base firing rate, conditional probability, number of nodes in the network, and the density of the network.
 The set of precision-recall curves are shown in Fig 6. The general trends observed here show that for a range of settings of the conditional probability, base firing rates, and network topology, both high precision and high recall can be obtained. As the stringency of the conditional mutual information threshold is increased (compare Fig. 6 (top) to Fig. 6 (bottom), we observe a deterioration of performance only w.r.t. the base rate threshold.
 E. Cortical cultures
Multi-electrode arrays provide high throughput recordings of the spiking activity in neuronal tissue and are hence rich sources of event data where events correspond to specific neurons being activated. We use data from dissociated corti-cal cultures gathered by Steve Potter X  X  laboratory at Georgia Tech [15] which gathered data over several days. The mining is done with mutual information threshold  X  = 0 . 001 DBN search parameter  X  = 0 . 02 .

In order to establish the significance of the networks discovered we run our algorithm on several surrogate spike trains generated by replacing the neuron labels of spikes in the real data with randomly chosen labels. These surrogates break the temporal correlations in the data and yet preserve the overall summary statistics. No network structure was found in such surrogate sequences. We are currently in the process of characterizing and interpreting the usefulness of such networks found in real data. An example network is shown in Fig. 7, reflecting the sustained bursts observed in this culture by Wagenaar et al [15].

Our work marries frequent pattern mining with probabilis-tic modeling for analyzing discrete event stream datasets. DBNs provide a formal probabilistic basis to model rela-tionships between time-indexed random variables but are intractable to learn in the general case. Conversely, frequent episode mining is scalable to large datasets but does not exhibit the rigorous probabilistic interpretations that are the mainstay of the graphical models literature. We have pre-sented the beginnings of research to relate these two diverse threads and demonstrated its potential to mine excitatory networks with applications in spike train analysis.
 Two key directions of future work are being explored. The excitatory assumption as modeled here posits an order over the entries of the conditional probability table but does not impose strict distinctions of magnitude over these entries. This suggests that, besides the conditional indepen-dencies inferred by our approach, there could potentially be additional  X  X tructural X  constraints masquerading inside the conditional probability tables. We seek to tease out these relationships further. A second, more open, question is whether there are other useful classes of DBNs that have both practical relevance (like excitatory circuits) and which also can be tractably inferred using sufficient statistics of the form studied here.

This work is supported in part by General Motors Re-search, NSF grant CNS-0615181, and ICTAS, Virginia Tech. Authors thank V. Raajay and P. S. Sastry of Indian Institute of Science, Bangalore, for many useful discussions and for access to the neuronal spike train simulator of [13].
