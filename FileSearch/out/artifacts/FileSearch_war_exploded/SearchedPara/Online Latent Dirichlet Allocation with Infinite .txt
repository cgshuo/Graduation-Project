 Ke Zhai zhaike@cs.umd.edu Jordan Boyd-Graber jbg@umiacs.umd.edu iSchool and UMIACS, University of Maryland, College Park, MD USA Latent Dirichlet allocation (LDA) is a probabilistic approach for exploring topics in document collec-tions (Blei et al., 2003). Topic models offer a formalism for exposing a collection X  X  themes and have been used to aid information retrieval (Wei &amp; Croft, 2006), un-derstand academic literature (Dietz et al., 2007), and discover political perspectives (Paul &amp; Girju, 2010). As hackneyed as the term  X  X ig data X  has become, re-searchers and industry alike require algorithms that are scalable and efficient. Topic modeling is no differ-ent. A common scalability strategy is converting batch algorithms into streaming algorithms that only make one pass over the data. In topic modeling, Hoffman et al. (2010) extended LDA to online settings. However, this and later online topic models (Wang et al., 2011; Mimno et al., 2012) make the same lim-iting assumption. The namesake topics, distributions over words that evince thematic coherence, are always modeled as a multinomial drawn from a finite Dirich-let distribution. This assumption precludes additional words being added over time.
 Particularly for streaming algorithms, this is neither reasonable nor appealing. There are many reasons immutable vocabularies do not make sense: words are invented ( X  X rowdsourcing X ), words cross languages ( X  X angnam X ), or words common in one context become prominent elsewhere ( X  X uvuzelas X  moving from music to sports in the 2010 World Cup). To be flexible, topic models must be able to capture the addition, invention, and increased prominence of new terms.
 Allowing models to expand topics to include additional words requires changing the underlying statistical for-malism. Instead of assuming that topics come from a finite Dirichlet distribution, we assume that it comes from a Dirichlet process (Ferguson, 1973) with a base distribution over all possible words, of which there are an infinite number. Bayesian nonparametric tools like the Dirichlet process allow us to reason about distri-butions over infinite supports. We review both topic models and Bayesian nonparametrics in Section 2. In Section 3, we present the infinite vocabulary topic model , which uses Bayesian nonparametrics to go beyond fixed vocabularies.
 In Section 4, we derive approximate inference for our model. Since emerging vocabulary are most important in non-batch settings, in Section 5, we extend inference to streaming settings. We compare the coherence and effectiveness of our infinite vocabulary topic model against models with fixed vocabulary in Section 6. Figure 1 shows a topic evolving during inference. The algorithm processes documents in sets we call mini-batches ; after each minibatch, online variational infer-ence updates our model X  X  parameters. This shows that out of vocabulary words can enter topics and eventually become high probability words .
 Latent Dirichlet allocation (Blei et al., 2003) assumes a simple generative process. The K topics, drawn from a symmetric Dirichlet distribution,  X  k  X  Dir (  X  ) ,k = { 1 ,...,K } generate a corpus of observed words: 1: for each document d in a corpus D do 2: Choose a distribution  X  d over topics from a 3: for each of the n = 1 ,...,N d word indexes do 4: Choose a topic z n from the document X  X  distri-5: Choose a word w n from the appropriate topic X  X  Implicit in this model is a finite number of words in the vocabulary because the support of the Dirichlet distribution Dir (  X  ) is fixed. Moreover, it fixes a priori which words we can observe, a patently false assump-tion (Algeo, 1980). 2.1. Bayesian Nonparametrics Bayesian nonparametrics is an appealing solution; it models arbitrary distributions with an unbounded and possibly countably infinite support. While Bayesian nonparametrics is a broad field, we focus on the Dirich-let process (DP, Ferguson 1973).
 The Dirichlet process is a two-parameter distribution with scale parameter  X   X  and base distribution G 0 . A draw G from DP(  X   X  ,G 0 ) is modeled as b 1 ,...,b i ,...  X  Beta (1 , X   X  ) ,  X  1 ,..., X  i ,...  X  G Individual draws from a Beta distribution are the foundation for the stick-breaking construction of the DP (Sethuraman, 1994). Each break point b i mod-els how much probability mass remains. These break points combine to form an infinite multinomial, where the weights  X  i give the probability of selecting any particular atom  X  i from the base distribution. The model we develop in Section 3 uses a base distri-bution over all possible words, and each topic is a draw from the Dirichlet process. This approach is inspired by unsupervised models that induce parts-of-speech. 2.2. N-gram Models in Latent Variable Models A strength of the probabilistic formalism is the abil-ity to embed specialized models inside more general models. The problem of part-of-speech (POS) induc-tion (Goldwater &amp; Griffiths, 2007) uses morphological regularity within part of speech classes (e.g., verbs in English often end with  X  X d X ) to learn a character n-gram model for parts of speech (Clark, 2003). This has been combined within the latent variable HMM via a Chinese restaurant process (Blunsom &amp; Cohn, 2011). We also view latent clusters of words (topics) as a non-parametric distribution with a character n-gram base distribution, but to better support streaming data sets, we use online variational inference; previous approaches used Monte Carlo methods (Neal, 1993). Variational inference is easier to distribute (Zhai et al., 2012) and amenable to online updates (Hoffman et al., 2010). Within the topic modeling community, there are dif-ferent approaches to deal with changing word use. Dynamic topic models (Blei &amp; Lafferty, 2006) dis-cover evolving topics by viewing word distributions as n -dimensional points undergoing Brownian motion. These models reveal compelling topical evolution; e.g., physics moving from studies of the  X ther to relativity to quantum mechanics. However, the models assume fixed vocabularies ; we show that our infinite vocabu-lary model discovers more coherent topics (Section 6.2). An elegant solution for large vocabularies is the  X  X ash-ing trick X  (Weinberger et al., 2009), which maps strings into a restricted set of integers via a hash function. These integers become the topic model X  X  vocabulary. While elegant, words are no longer identifiable. How-ever, our infinite vocabulary topic model retains iden-tifiability and better models datasets (Section 6.3). Our generative process is identical to LDA X  X  (Section 2) except that topics are not drawn from a finite Dirich-let. Instead, topics are drawn from a DP with base distribution G 0 over all possible words: 1: for each topic k do 2: Draw words  X  kt , ( t = { 1 , 2 ,... } ) from G 0 . 3: Draw b kt  X  Beta (1 , X   X  ) , ( t = { 1 , 2 ,... } ). The rest is identical to LDA. 3.1. A Distribution over Words An intuitive choice for G 0 is a conventional character language model. However, such a na  X  X ve approach is unrealistic and is biased to shorter words; preliminary experiments yielded poor results. Instead, we define G 0 as the following distribution over strings 1: Choose a length l  X  Mult(  X  ). This is similar to the classic n -gram language model, except that the length is first chosen from a multinomial distribution over all lengths. Estimating conditional n -gram probabilities is well-studied in natural language processing (Jelinek &amp; Mercer, 1985).
 The full expression for the probability of a word  X  consisting of the characters c 1 ,c 2 ,... under G 0 is where |  X  | is the length of the word. To avoid length bias, we chose the multinomial  X  that minimizes the average discrepancy between word corpus probabilities p
C and the probability in our word model  X   X  arg min  X  P  X  | p C (  X  )  X  p WM (  X  |  X  ) | 2 , s.t. P The n -gram statistics are estimated from an English dictionary which need not be very large, since it is a language model over characters, not words. Inference in probabilistic inference uncovers the latent variables that best reconstruct observed data. The quality of this reconstruction is measured by log like-lihood. For a corpus of D documents where the d -th document contains N d words, the joint distribution is p ( W ,  X  ,  X  ,  X  , z ) = Q K k =1 Q  X  t =1 p (  X  kt | G 0 h Directly optimizing the latent variables Z  X { corpus-level stick proportions  X  , document topic distributions  X  and word topic assignments z } is intractable, so we use variational inference (Blei et al., 2003). To use variational inference, we select a simpler family of distributions over the latent variables Z . We call these distributions q . This family of distributions allows us to optimize a lower bound of the likelihood called the evidence lower bound (ELBO) L , Maximizing L is equivalent to minimizing the Kullback-Leibler (KL) divergence between the true distribution and the variational distribution.
 Unlike mean-field approaches (Blei et al., 2003), which assume q is a fully factorized distribution, we integrate out the word-level topic distribution vector  X  : q ( z d |  X  ) figurations rather than a product of N d multinomial distributions over K topics. Combined with a beta variational distribution q is q ( Z )  X  q (  X  , z ) = Q D q ( z d |  X  ) Q K q ( b k |  X  1 However, we cannot explicitly represent a distribution over all possible strings, so we truncate our variational stick-breaking distribution q ( b |  X  ) to a finite set. 4.1. Truncation Ordered Set Variational methods typically cope with infinite dimen-sionality of nonparametric models by truncating the distribution to a finite subset of all possible atoms that nonparametric distributions consider (Blei &amp; Jordan, 2005; Kurihara et al., 2006; Boyd-Graber &amp; Blei, 2009). This is done by selecting a relatively large truncation index T k , and then stipulating that the variational dis-tribution uses the rest of the available stick at that index, i.e., q ( b T in expectation under q beyond that index.
 However, directly applying such a technique is not feasible here, as truncation is not just a search over dimensionality but also over atom strings and their ordering. This is often a problem in for nonparametric models, and the truncation that solves the problem matches the underlying probabilistic model: for mix-ture models, it is the number of components (Blei &amp; Jordan, 2005); for hierarchical topic models, it is a tree (Wang &amp; Blei, 2009); for natural language gram-mars, it is grammatons (Cohen et al., 2010). Similarly, our truncation is not just a fixed vocabulary size; it is a truncation ordered set (TOS). The ordering is important because the Dirichlet process is a size-biased distribution; words with lower indices are likely to have a higher probability than words with higher indices. Each topic has a unique TOS T k of limited size that maps every word type w to an integer t ; thus t = T k ( w ) is the index of the atom  X  kt that corresponds to w . We defer how we choose this mapping until Section 4.3. More pressing is how we compute the two variational distributions of interest. For q ( z |  X  ), we use local col-lapsed MCMC sampling (Mimno et al., 2012) and for q ( b |  X  ) we use stochastic variational inference (Hoffman et al., 2010). We describe both in turn. 4.2. Stochastic Inference Recall that the variational distribution q ( z d |  X  ) is a single distribution over the N d vectors of length K . While this removes the tight coupling between  X  and z that often complicates mean-field variational inference, it is no longer as simple to determine the variational distribution q ( z d |  X  ) that optimizes Eqn. (2) . However, Mimno et al. (2012) showed that Gibbs sampling in-stantiations of z  X  dn from the distribution conditioned on other topic assignments results in a sparse, effi-cient empirical estimate of the variation distribution. In our model, the conditional distribution of a topic assignment of a word with TOS index t = T k ( w dn ) is We iteratively sample from this conditional distribution to obtain the empirical distribution  X  dn  X   X  q ( z dn ) for latent variable z dn , which is fundamentally different from mean-field approach (Blei et al., 2003). There are two cases to consider for computing Eqn. (4)  X  whether a word w dn is in the TOS for topic k or not. First, we look up the word X  X  index t = T k ( w dn ). If this word is in the TOS, i.e., t  X  T k , the expectations are straightforward (Mimno et al., 2012) It is more complicated when a word is not in the TOS. Wang &amp; Blei (2012) proposed a truncation-free stochastic variational approach for DPs. It provides more flexible truncation schemes than split-merge tech-niques (Wang &amp; Blei, 2009). The algorithm resem-bles a collapsed Gibbs sampler; it does not represent all components explicitly. For our infinite vocabu-lary topic model, we do not ignore out of vocabulary (OOV) words; we assign these unseen words proba-bility 1  X  P t  X  T distribution of an unseen word ( t &gt; T k ) is then This is different from finite vocabulary topic models that set vocabulary a priori and ignore OOV words. 4.3. Refining the Truncation Ordered Set In this section, we describe heuristics to update the TOS inspired by MCMC conditional equations, a com-mon practice for updating truncations. One component of a good TOS is that more frequent words should come first in the ordering. This is reasonable because the stick-breaking prior induces a size-biased ordering of the clusters. This has previously been used for trun-cation optimization for Dirichlet process mixtures and admixtures (Kurihara et al., 2007).
 Another component of a good TOS is that words con-sistent with the underlying base distribution should be ranked higher than those not consistent with the base distribution. This intuition is also consistent with the conditional sampling equations for MCMC infer-ence (M  X uller &amp; Quintana, 2004); the probability of creating a new table with dish  X  is proportional to  X  G 0 (  X  ) in the Chinese restaurant process.
 Thus, to update the TOS, we define the ranking score of word t in topic k as sort all words by the scores within that topic, and then use those positions as the new TOS. In Section 5.1, we present online updates for the TOS. Online variational inference seeks to optimize the ELBO L according to Eqn. (2) by stochastic gradi-ent optimization. Because gradients estimated from a single observation are noisy, stochastic inference for topic models typically uses  X  X inibatches X  of S docu-ments out of D total documents (Hoffman et al., 2010). An approximation of the natural gradient of L with respect to  X  is the product of the inverse Fisher infor-mation and its first derivative (Sato, 2001) which leads to an update of  X  , where i = (  X  0 + i )  X   X  defines the step size of the algo-rithm in minibatch i . The learning rate  X  controls how quickly new parameter estimates replace the old;  X   X  (0 . 5 , 1] is required for convergence. The learn-ing inertia  X  0 prevents premature convergence. We recover the batch setting if S = D and  X  = 0. 5.1. Updating the Truncation Ordered Set A nonparametric streaming model should allow the vocabulary to dynamically expand as new words ap-pear (e.g., introducing  X  X uvuzelas X  for the 2010 World Cup), and contract as needed to best model the data (e.g., removing  X  X uvuzelas X  after the craze passes). We describe three components of this process, expanding the truncation, refining the ordering of TOS, and con-tracting the vocabulary.
 Determining the TOS Ordering This process de-pends on the ranking score of a word in topic k at minibatch i , R i,k (  X  ). Ideally, we would compute R from all data. However, only a single minibatch is accessible. We have a per-minibatch rank estimate which we interpolate with our previous ranking We introduce an additional algorithm parameter, the reordering delay U . We found that reordering after every minibatch ( U = 1) was not effective; we explore the role of reordering delay in Section 6. After U minibatches have been observed, we reorder the TOS for each topic according to the words X  ranking score R in Eqn. (10) ; T k ( w ) becomes the rank position of w according to the latest R ik .
 Expanding the Vocabulary Each minibatch con-tains words we have not seen before. When we see them, we must determine their relative rank position in the TOS, their rank scores, and their associated variational parameters. The latter two issues are rele-vant for online inference because both are computed via interpolations from previous values in Eqn. (10) and (9) . For an unseen word  X  , previous values are undefined. Thus, we set R i  X  1 ,k for unobserved words to be 0,  X  to be 1, and T k (  X  ) is T k + 1 (i.e., increase truncation and append to the TOS).
 Contracting the Vocabulary To ensure tractabil-ity we must periodically prune the words in the TOS. When we reorder the TOS (after every U minibatches), we only keep the top T terms, where T is a user-defined integer. A word type  X  will be removed from T k if its in-dex T k (  X  ) &gt; T and its previous information (e.g., rank and variational parameters) is discarded. In a later minibatch, if a previously discarded word reappears, it is treated as a new word.
 In this section, we evaluate the performance of our infinite vocabulary topic model ( infvoc ) on two cor-pora: de-news 1 and 20 newsgroups . 2 Both corpora were parsed by the same tokenizer and stemmer with a common English stopword list (Bird et al., 2009). First, we examine its sensitivity to both model param-eters and online learning rates. Having chosen those parameters, we then compare our model with other topic models with fixed vocabularies.
 Evaluation Metric Typical evaluation of topic mod-els is based on held-out likelihood or perplexity. How-ever, creating a strictly fair comparison for our model against existing topic model algorithms is difficult, as traditional topic model algorithms must discard words that have not previously been observed. Moreover, held-out likelihood is a flawed proxy for how topic models are used in the real world (Chang et al., 2009). Instead, we use two evaluation metrics: topic coherence and classification accuracy.
 Pointwise mutual information (PMI), which correlates with human perceptions of topic coherence, measures how words fit together within a topic. Following New-man et al. (2009), we extract document co-occurence statistics from Wikipedia and score a topic X  X  coherence by averaging the pairwise PMI score (w.r.t. Wikipedia co-occurence) of the topic X  X  ten highest ranked words. Higher average PMI implies a more coherent topic. Classification accuracy is the accuracy of a classifier learned from the topic distribution of training docu-ments applied to test documents (the topic model sees both sets). A higher accuracy means the unsupervised topic model better captures the underlying structure of the corpus. To better simulate real-world situations, 20-newsgroup X  X  test/train split is by date (test documents appeared after training documents).
 Comparisons We evaluate the performance of our model ( infvoc ) against three other models with fixed vocabularies: online variational Bayes LDA ( fixvoc-vb , Hoffman et al. 2010), online hybrid LDA ( fixvoc-hybrid , Mimno et al. 2012), and dynamic topic models ( dtm , Blei &amp; Lafferty 2006). Including dynamic topic models is not a fair comparison, as its inferences requires access to all of the documents in the dataset; unlike the other algorithms, it is not online.
 Vocabulary For fixed vocabulary models, we must decide on a vocabulary a priori . We consider two different vocabulary methods: use the first minibatch to define a vocabulary ( null ) or use a comprehensive dictionary 3 ( dict ). We use the same dictionary to train infvoc  X  X  base distribution.
 Experiment Configuration For all models, we use the same symmetric document Dirichlet prior with  X   X  = 1 /K , where K is the number of topics. Online models see exactly the same minibatches. For dtm , which is not an online algorithm but instead partitions its input into  X  X pochs X , we combine documents in ten consecutive minibatches into an epoch (longer epochs tended to have worse performance; this was the shortest epoch that had reasonable runtime).
 For online hybrid approaches ( infvoc and fixvoc-hybrid ), we collect 10 samples empirically from the variational distribution in E-step with 5 burn-in sweeps. For fixvoc-vb , we run 50 iterations for local parameter updates. 6.1. Sensitivity to Parameters Figure 2 shows how the PMI score is affected by the DP scale parameter  X   X  , the truncation level T , and the reordering delay U . The relatively high values of  X   X  may be surprising to readers used to seeing a DP that instantiates dozens of atoms, but when vocabularies are in tens of thousands, such scale parameters are necessary to support the long tail. Although we did not investigate such approaches, this suggests that more advanced nonparametric distributions (Teh, 2006) or explicitly optimizing  X   X  may be useful. Relatively large values of U suggest that accurate estimates of the rank order are important for maintaining coherent topics. While infvoc is sensitive to parameters related to the vocabulary, once suitable values of those parameters are chosen, it is no more sensitive to learning-specific parameters than other online LDA algorithms (Fig-ure 3), and values used for other online topic models also work well here. 6.2. Comparing Algorithms: Coherence Now that we have some idea of how we should set parameters for infvoc , we compare it against other topic modeling techniques. We used grid search to select parameters for each of the models 4 and plotted the topic coherence averaged over all topics in Figure 4. While infvoc initially holds its own against other mod-els, it does better and better in later minibatches, since it has managed to gain a good estimate of the vocabu-lary and the topic distributions have stabilized. Most of the gains in topic coherence come from highly spe-cific proper nouns which are missing from vocabularies of the fixed-vocabulary topic models. This advantage holds even against dtm , which uses batch inference. 6.3. Comparing Algorithms: Classification For the classification comparison, we consider addi-tional topic models. While we need the most probable topic strings for PMI calculations, classification exper-iments only need a document X  X  topic vector. Thus, we consider hashed vocabulary schemes. The first, which we call dict-hashing , uses a dictionary for the known words and hashes any other words into the same set of integers. The second, full-hash , used in Vowpal Wabbit, 5 hashes all words into a set of T integers. We train 50 topics for all models on the entire dataset and collect the document level topic distribution for ev-ery article. We treat such statistics as features and train a SVM classifier on all training data using Weka (Hall et al., 2009) with default parameters. We then use the classifier to label testing documents with one of the 20 newsgroup labels. A higher accuracy means the model is better capturing the underlying content.
 Our model infvoc captures better topic features than online LDA fixvoc (Table 1) under all settings. 6 This suggests that in a streaming setting, infvoc can better categorize documents. However, the batch algorithm dtm , which has access to the entire dataset performs better because it can use later documents to retrospec-tively improve its understanding of earlier ones. Unlike dtm , infvoc only sees early minibatches once and cannot revise its model when it is tested on later minibatches. 6.4. Qualitative Example Figure 1 shows the evolution of a topic in 20 news-groups about comics as new vocabulary words enter from new minibatches. While topics improve over time (e.g., relevant words like  X  X eri(es) X ,  X  X ssu(e) X ,  X  X orc(e) X  are ranked higher), interesting words are being added throughout training and become prominent after later minibatches are processed (e.g.,  X  X aptain X ,  X  X omic-strip X ,  X  X utant X ). This is not the case for standard online LDA X  X hese words are ignored and the model does not capture such information. In addition, only about 60% of the word types appeared in the SIL En-glish dictionary. Even with a comprehensive English dictionary, online LDA could not capture all the word types in the corpus, especially named entities. We proposed an online topic model that, instead of assuming vocabulary is known a priori , adds and sheds words over time. While our model is better able to create coherent topics, it does not outperform dynamic topic models (Blei &amp; Lafferty, 2006; Wang et al., 2008) that explicitly model how topics change. It would be interesting to allow such models to X  X n addition to modeling the change of topics X  X lso change the underlying dimensionality of the vocabulary.
 In addition to explicitly modeling the change of topics over time, it is also possible to model additional struc-ture within topic. Rather than a fixed, immutable base distribution, modeling each topic with a hierarchical character n-gram model would capture regularities in the corpus that would, for example, allow certain top-ics to favor different orthographies (e.g., a technology topic might prefer words that start with  X  X  X ). While some topic models have attempted to capture orthogra-phy for multilingual applications (Boyd-Graber &amp; Blei, 2009), our approach is more robust and incorporating the our approach with models of transliteration (Knight &amp; Graehl, 1997) might allow concepts expressed in one language better capture concepts in another, further improving the ability of algorithms to capture the evolv-ing themes and topics in large, streaming datasets. The authors thank Chong Wang, Dave Blei, and Matt Hoffman for answering questions and sharing code. We thank Jimmy Lin and the anonymous reviewers for helpful suggestions. Research supported by NSF grant #1018625. Any opinions, conclusions, or recommenda-tions are the authors X  and not those of the sponsors.
