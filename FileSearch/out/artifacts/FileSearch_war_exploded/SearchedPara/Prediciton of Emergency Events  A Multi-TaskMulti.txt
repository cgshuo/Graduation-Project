 Resource allocation at hospital emergency departments is critical. To facilitate such allocation, accurate prediction of several patient outcomes is crucial -emer-gency presentation, readmission or length of stay are some examples. Routinely collected Electronic Medical Data (EMR) offers opportunity to make such prog-nosis. This data is longitudinal, containing information about evolving risk, cap-turing disease progression and health conditions amongst other factors. Multiple patient outcomes are related to underlying patient health and, therefore, their joint modeling can potentially help in building better prediction models. Previous research efforts to build prediction models for longitudinal data have attempted to model the data as a time series [ 1 ] and use mixture distributions as data generative models. A set of latent states are learnt using mixture com-ponents and the change is modeled using the transitions over these states. Other works use random effect models for longitudinal measurements assuming that the risk over time remains nearly constant [ 2 ]. These approaches are insufficient to handle evolving risk. ments, each of which can be considered as a task . We build one prediction model MTL ensures that the data from different tasks are appropriately combined to exploit their common relatedness whilst the distribution of risk for each segment can still differ. To assist joint modeling, MTL techniques employ various con-straints on the task parameters e.g. sampling the task-parameters from a shared prior [ 5 , 6 ], modeling the task-parameters through a common low-dimensional subspace [ 4 , 7 ], or combining the tasks in proportion to their relatedness learnt using a task-to-task covariance matrix [ 8 ]. Focusing on our problem, we employ a MTL framework to jointly model different segments of the longitudinal data, and additionally exploit the relationship amongst multiple outcomes. Existing MTL models focus on multiple outcomes of a single task. Therefore , the problem of developing a multi-task, multi-label prediction model remains open. or labels prediction in a MTL paradigm. For each task-label pair, we learn a prediction function. The prediction functions of the task-label pairs interact through two subspaces. The first subspace is used to impose sharing across all tasks for a given label .The second subspace, specific to a task, is used to allow task-specific variations and is shared across all the labels for the task. We term this model multi-task multi-label (MTML) learning. The proposed MTML is formulated as an iterative optimization problem and solved using a scalable and efficient block co-ordinate descent method. We empirically demonstrate both the scalability and convergence. We apply the proposed MTML models on two real-world cohorts -a Cancer Electronic Medical Records (EMR) with 3000 patients collected over two years involving 11 different cancer types, and an Acute Myocardial Infarction (AMI) cohort with 2652 patients collected over the same two year period. We predict multiple emergency related outcomes -future emergency attendances, admissions and length-of-stay-over a 3 month, 6 month and twelve month period. We show that the predictive performance of MTML main contributions are:  X  X  novel multi-task-multi-label learning (MTML) model that extends the  X  Solution of the optimization problem using a scalable and efficient Block  X  Empirical validation of the model through experiments using two real world The significance of our approach is in providing solutions to classification prob-lems in data that contain multiple tasks wherein examples of each task have multiple labels. Our solution helps in building accurate prediction system for better upfront planning of resources at hospital emergency departments. We briefly review prediction models for longitudinal data analysis, multi-task and multi-label learning methods as following: Prediction Models for Longitudinal Data: Longitudinal data has two cru-cial challenges: (1) uneven distributions of data points in temporal intervals and (2) evolving risk factors. Instead of modeling the temporal data using a time series method, MTL methods are used at multiple time-points of the data. For example, Zhang et.al. [ 12 ] proposed a MTL model to capture the disease pro-used longitudinal phenotype markers for Alzheimer X  X  disease (AD) progression prediction. A similar type of multi-task modeling of longitudinal data can be found at [ 1 ]. These models are not sutiable for evolving risk factors and multiple outcomes.
 Multi-task Learning (MTL) Methods: Assume we have T supervised learn-ing tasks and each task is associated with a predictive model f usually expressed as f t ( x )= u T t x . Here, u t is a task-parameter. The MTL framework aims to improve predictive performance of a task by learning mul-tiple related tasks simultaneously. The predictive functions jointly by minimizing the following regularized empirical risk where L i is a loss function and R is a regularization function on u ization parameter  X  . Given the tasks are related, MTL techniques employ various from a shared prior [ 6 ], modeling the task-parameters through a common low-relatedness learnt using task-to-task covariance matrix [ 8 ].
 Multi-label Learning (MLL) Methods: MLL models deal with examples with multiple labels. They express a task with multiple labels into multiple independent binary classification problem. Representative methods are as fol-lows: Schapire et.al .[ 15 ] proposed a boosting technique for MLL problem, Chen et.al. [ 16 ] presented a semi-supervised MLL model, Zhang et.al. [ 17 ] extended k -nearest neighborhood method ( k NN) to solve MLL problems. The major draw-back of these methods is that they do not exploit the correlations amongst the labels. Representative methods which consider correlations amongst labels are as Sun et.al. [ 19 ] presented a hypergraph spectral learning model and Hariharan et.al. [ 20 ] used a user specified prior matrix that encodes the correlation among the labels. A similar formulation as mentioned in equation ( 1 ) can be used for MLL problems. The parameter vector of label , i.e. u can be expressed as u = w +  X  T p , where w encodes the information of the feature space and a low-diemensional subspace capturing the correlations amongst the labels. This model originally proposed by Ando et.al. with an iterative solution [ 10 ] was later form solution. Assume we are given T supervised learning tasks wherein each task has N examples with M labels. Training data of the task t is expressed as ( X ( x 1 t , y 1 t ) ... ( x N t t , y N t t ) , where the i with labels y i t  X  X  1 ,  X  1 } M . The th element of label vector y if the th label has been assigned to the example i and -1 otherwise. Overall, Y f t, ( x )= u T t, x where u t, is a prediction function of task t with label .Welearn the prediction function u t, by minimizing following regularized empirical risk function where L i ( u t,l , x t, ,y i t, ) is a loss function and of u t, with a penalizing parameter  X  . 3.1 Multi-Task Multi-Label (MTML) Formulation We propose a formulation for the multi-task multi-label problem inspired by the multi-label framework in [ 11 ]. We decompose the prediction function u in equation ( 2 ) into three components: The first component is derived from the original feature space, the second component learns a subspace shared across tasks and the third component is a shared subspace spanning across labels. We express the prediction function u t, as where w t,l  X  X  D encodes the information of the original feature space, tasks of a given label .  X  t  X  X  D T  X  D with weight vector v the subspace across all labels of a given task t . The dimension of the shared sub-spaces i.e D L and D T are estimated by solving a generalized eigenvalue problem (detailed in Appendix 5 ). The prediction function u t, and other task and label related parameters can be obtained from following formulation: where the first regularization component on w t, with regularization coefficient  X  controls the amount of information of task-label pairs ( t , ), the second reg-where W = w 1 , ,..., w T, and P = p 1 , ,..., p T, controls the amount of information in the shared subspace of tasks of a given label . Similarly, the third regularization component controls the amount of information in the shared subspace of labels of a given task t .The  X  and  X  t are assumed to be orthogonal to reduce label and task specific redundant information. 3.2 BCD Solution of MTML Formulation By combining equations ( 3 ) and ( 4 ), we have where the regularization function R is defined as R ( u  X  u sidering L i to be a hinge loss function, by change of variables, equation ( 5 )can be decomposed into two separate equations as follows where, b t, = u t,  X   X  T t v t, , and  X  r i =1  X  y i t, v the formulation is decoupled over T tasks. where we can run T sub-problems in parallel. Each sub-problem is convex with respect to parameters ( a respectively. We consider an iterative Block coordinate descent method (BCD) [ 21 ] to find an optimal solution for each sub-problem. A concise summary of MTML framework is provided in Algorithm 1 with an analysis of the optimiza-tion steps is in Appendix 5 .
 Algorithm 1. Multi-Task Multi-Label Method We perform several experiments to evaluate the predictive performance of the proposed MTML models and compare them with state-of-the-art baseline meth-ods. For evaluation, we use two real-world hospital cohorts: Cancer and AMI patients, and predict events at hospital emergency. 4.1 Healthcare Datasets Cancer Electronic Medical Records: This dataset consists of electronic med-ical records (EMR) of the patients visiting the hospital from 2010-2012 are 11 different types of cancers in the dataset, for example, Breast, Skin, Central Nervous System (CNS), Colorectal, Lung etc. The feature set has information about medical conditions relating to the previous visits of each patient including past diagnosis and procedure codes (ICD-10) 2 , diagnosis related group codes (DRG), conditions relating to emergency admissions and cancer specific details. 3000 patients are reported in the dataset and the feature length is 531. The whole dataset is divided into non-overlapping segments where each segment is considered a task. It contains patient records from the past 3 months, past 3-6 months, past 6-12 months and past 12-24 months respectively. Our focus is on prediction of emergency events for patient where each patient is associated with 6 emergency labels: emergency attendances (E-ATND) and emergency admissions (E-ADM) in the future 3, 6, and 12 months. The dataset is detailed in [ 22 ]. Acute Myocardial Infarction (AMI) EMR: AMI is the medical term for heart attack. The AMI electronic medical records (EMR) is recorded over a period of two years. The data contains diverse information such as patient demo-graphics, state of the emergency admissions, personal history of other diseases ( e.g. nervous and musculoskeletal systems). 2652 patients are reported in the dataset and the feature length is 431. The pre-processing of the dataset and the outcome variables are similar to the Cancer EMR. 4.2 Baselines We compare the proposed MTML models against following baseline multi-task learning and multi-label learning methods: Multi-Task Learning (MTL) baselines : RMFL [ 9 ] : Robust multi-task feature learning method learns a task-parameter that has two components. The first component has the common feature set across related tasks, whereas the other component detects outlier tasks. The convex formulation is optimized by the accelerated gradient optimization method (AGM) [ 23 ].
 MTFL [ 4 ] : MTFL model learns a common set of features across tasks by constraining the task-parameters with a mixed 2 / 1 norm. The formulation is non-convex and translated to a convex optimization problem with an iterative solution.
 DM [ 24 ] : The formulation of the Dirty model (DM) and RMFL is similar, however DM used a group-sparse 1 /  X  norm to learn the common features and a norm for detecting outlier tasks. The proposed formulation is convex and is solved using AGM [ 23 ] method.
 Multi-Label Learning (MLL) baselines : ASOM [ 10 ] : The proposed multi-label learning (MLL) framework computes a common subspace that captures the correlations amongst labels. The formulation is non-convex and the optimization technique is iterative.
 ML-LS [ 11 ] : The proposed framework is similar to the ASO method. How-ever the formulation has a least-square loss function and provides a closed form solution.
 M3L [ 20 ] : The proposed framework has a max-margin formulation for multi-matrix that represents the correlation amongst the labels. 4.3 Experimental Analysis Performance Evaluation Measures. For multi-task learning (MTL) meth-ods, the performance of the learning system is evaluated by a measure that com-puted on each test example separately and computes the average value across the test dataset. Examples ar eaccuracy (AC) [ 7 ] and area under ROC curve (AUC). The evaluation measures specific to multi-label learning (MLL) models are micro-F 1 and macro-F 1 [ 25 ], the metrics are defined as where Microprecision and Microrecall denotes the precision and recall aver-aged over all example/label pair. Macroprecison and Macrorecall are defined as M acroprecision = 1 M M =1 Tp Tp + Fp and M acrorecall = where Tp , Fp and Fn are numbre of true positives, false positives and false negatives for label the .
 proposed MTML models and the baselines. Higher the value of AC, AUC, micro -F 1 and macro-F 1 , the better is the classifier. As our dataset has T tasks and each task has M labels, we apply the MTL baseline models for each label separately and then average the performance across labels. Similarly, we use MLL baseline models for each task and compute the average performance over all tasks. Experimental Setting and Results. We randomly select 500 examples from the cancer EMR dataset for training and use the rest of the examples for testing. We repeat this experiment over 10 randomly chosen training/test sets and the mean of the performances are reported in Table 1 . The parameters of the MTML models are chosen by 5-fold cross-validation.The proposed MTML framework outperform all the baselines. Specifically, for the cancer EMR, the difference in performances of MTML and the closest contending MTL baseline model is 19% on accuracy, 10% on AUC, 10% on micro-F 1 and 7% on macro-F tively. Similarly, the difference in performance with the closest contending MLL model is 6% on accuracy, 4.2% on AUC, 2.4% on micro-F 1 and 6% on macro-F respectively. Similar improvements exist for the AMI cohort. 4.4 Sensitivity Analysis Effect of the Regularization Parameters: We randomly sample 20% exam-ples from the Cancer EMR and the rest of the data is used as the test data. Fixing  X  , X  3 =10 itor the effect of  X  1 on the classification performance of the MTML model. We performance of the MTML model with respect to the parameters  X  The best AUC is obtained for  X  1 =10  X  4 ,  X  2 =10  X  2 and  X  micro-F 1 is obtained for  X  1 =10  X  4 ,  X  2 =10  X  2 and  X  Similar plots are also provided for the AMI data.
 Effect of Training Set Size: We vary the length of the training set by randomly sampling 500, 1000, 1500, 2000 and 2500 examples from the Cancer cohort, whereas the rest of the data is used for testing only. Figure 2a presents the classification performance of each training sets. The plot shows the length of the training sets vs. AUC. The performance of MTML improves with increasing size of the training set. This observation is according to our expectation that more training data leads to improved performance. 4.5 Computational and Convergence Analysis We randomly select training sets with 500, 1000, 1500, 2000 and 2500 exam-ples respectively and the rest of the data is used for testing. From each training dataset, we compute the mean computational time and the number of iterations for convergence required by the MTML model. Figure 2b presents the compu-tational time taken by each training set averged over 10 repreated trials.We notice that the computation time is nearly linear in proportion to the length of the training set. Figure 2c present the convergence plot of the proposed MTML framework. The plot is between the cost value of equation ( 4 ) and number of iterations taken by the MTML model when length of the training set is fixed to 1000. The MTML framework (Algorithm 1 ) is considered to converge when the change in cost function (equation ( 4 )) between two consecutive iterations is less than 10  X  4 . As we see, BCD method for MTML requires only 10 iterations for converging to an optimal point. We have presented a framework to predict the events at a hospital emergency using electronic medical records (EMR) data. As the data is longitudinal and the risk of attending emergency varies over time, the whole data is partitioned into segments where each segment is considered as a task. As hospital emergency pre-diction involves predicting multiple outcomes, the problem is posed as learning from multiple tasks where each example (patient) in a task is having multiple labels. We propose a novel multi-task-multi-label learning (MTML) model jointly modeling multiple tasks with multiple labels simultaneously. MTML model has two components (1) a subspace that spans across all the tasks (2) another sub-space that spans across all the labels of a specific task. The objective function of the proposed model is convex and guarantees an optimal solution. The opti-mal point is computed using an efficient and scalable Block Coordinate Descent method. The proposed formulation has been applied on two hospital emergency cohorts and performs better than many existing baselines. Although we apply our framework for classification tasks, it can easily be adapted for regression. Appendix: BCD for optimizing equation ( 6 ) : We can write equation ( 6 ) as where we dropped subscript t for notational brevity. The three optimization steps are as following: (1) Minimization of v :fixing a and by minimizing the formulation as min { v } M =1 a  X   X  T v optimal solution is given by v =  X  a ,  X  =[1 ,...,M ]. (2) Minimization of a : By fixing (  X  , v ) and the problem in equation ( 8 ) becomes min We solve the dual formulation of equation ( 9 ) by scalable SVM solving package LIBSVM [ 26 ]. As we can write,  X  1 a  X   X  T v 2 +  X  2 a 2  X   X  T  X  ] a , the dual problem of this formulation becomes min where  X   X  X  N is a dual variable and Z = (  X  1 +  X  2 ) I  X  variable  X  , we can compute the primal variable a . For r lation converges to a similar formulation of multi-task learning (MTL) method as mentioned in [ 27 ]. (3) Minimization of  X  : Fixing A =[ a the optimization formulation for  X  is as follows: where the solution of this problem is given by the eigenvalue decomposition of matrix C 1 where C 1 = AA T and the largest eigenvectors corresponding to the largest D T nonzero eigenvalues are optimal solution for  X  (1) and (2) to compute p t, and b t, respectively. The transformation matrix is computed from the eigenvalue decomposition of matrix C B = b 1 , ,..., b T, . The optimal solution  X  is given by the largest eigenvectors of C 2 corresponding to the largest D C nonzero eigenvalues .

