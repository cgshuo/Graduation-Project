 Statistical machine translation (SMT) systems have made great strides in translation quality. However, high quality translation output is dependent on the availability of massive amounts of parallel text in the source and target language. However, there are a large number of languages that are considered  X  X ow-density X , either because the population speaking the language is not very large, or even if millions of peo-ple speak the language, insufficient amounts of par-allel text are available in that language.

A statistical translation system can be improved or adapted by incorporating new training data in the form of parallel text. In this paper, we propose sev-eral novel active learning (AL) strategies for statis-tical machine translation in order to attack this prob-lem. Conventional techniques for AL of classifiers are problematic in the SMT setting. Selective sam-pling of sentences for AL may lead to a parallel cor-pus where each sentence does not share any phrase pairs with the others. Thus, new sentences cannot be translated since we lack evidence for how phrase pairs combine to form novel translations. In this pa-per, we take the approach of exploration vs. exploita-tion: where in some cases we pick sentences that are not entirely novel to improve translation statis-tics, while also injecting novel translation pairs to improve coverage.

There may be evidence to show that AL is use-ful even when we have massive amounts of parallel training data. (Turchi et al., 2008) presents a com-prehensive learning curve analysis of a phrase-based SMT system, and one of the conclusions they draw is,  X  X he first obvious approach is an effort to iden-tify or produce data sets on demand (active learning, where the learning system can request translations of specific sentences, to satisfy its information needs). X 
Despite the promise of active learning for SMT there has been very little experimental work pub-lished on this issue (see Sec. 5). In this paper, we make several novel contributions to the area of ac-tive learning for SMT:  X 
We use a novel framework for AL, which to our knowledge has not been used in AL experiments be-fore. We assume a small amount of parallel text and a large amount of monolingual source language text. Using these resources, we create a large noisy par-allel text which we then iteratively improve using small injections of human translations.  X 
We provide many useful and novel features use-ful for AL in SMT. In translation, we can leverage a whole new set of features that were out of reach for classification systems: we devise features that look at the source language, but also devise features that make an estimate of the potential utility of transla-tions from the source, e.g. phrase pairs that could be extracted.  X 
We show that AL can be useful in domain adapta-tion. We provide the first experimental evidence in SMT that active learning can be used to inject care-fully selected translations in order to improve SMT output in a new domain.  X 
We compare our proposed features to a random se-lection baseline in a simulated setting for three lan-guage pairs. We also use a realistic setting: using hu-man expert annotations in our AL system we create an improved SMT system to translate from Bangla to English, a language pair with very few resources. Starting from an SMT model trained initially on bilingual data, the problem is to minimize the hu-man effort in translating new sentences which will be added to the training data to make the retrained SMT model achieves a certain level of performance. Thus, given a bitext L := { ( f lingual source text U := { f a subset of highly informative sentences from U to present to a human expert for translation. Highly in-formative sentences are those which, together with their translations, help the retrained SMT system quickly reach a certain level of translation quality. This learning scenario is known as active learning with Selective Sampling (Cohn et al., 1994).
Algorithm 1 describes the experimental setup we propose for active learning. We train our initial MT system on the bilingual corpus L , and use it to trans-late all monolingual sentences in U . We denote sen-tences in U together with their translations as U + (line 4 of Algorithm 1). Then we retrain the SMT code the test set. Afterwards, we select and remove a subset of highly informative sentences from U , and add those sentences together with their human-provided translations to L . This process is continued iteratively until a certain level of translation quality, which in our case is measured by the BLEU score, is met. In the baseline, against which we compare our sentence selection methods, the sentences are cho-sen randomly .

When (re-)training the model, two phrase tables are learned: one from L and the other one from U as a new feature function in the log-linear trans-lation model. The alternative is to ignore U + as in a conventional AL setting, however, in our ex-periments we have found that using more bilingual data, even noisy data, results in better translations. Algorithm 1 AL-SMT 1: Given bilingual corpus L , and monolingual cor-2: M F  X  E = train ( L,  X  ) 3: for t = 1 , 2 ,... do 4: U + = translate ( U,M F  X  E ) 5: Select k sentence pairs from U + , and ask a 6: Remove the k sentences from U , and add the 8: Monitor the performance on the test set T 9: end for Phrase tables from U + will get a 0 score in mini-mum error rate training if they are not useful, so our method is more general. Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U + , or (2) com-bining the two sets of data and training from the bi-text L  X  U + .

The setup in Algorithm 1 helps us to investigate how to maximally take advantage of human effort (for sentence translation) when learning an SMT model from the available data, that includes bilin-gual and monolingual text. Our sentence selection strategies can be divided into two categories: (1) those which are independent of the target language and just look into the source lan-guage, and (2) those which also take into account the target language. From the description of the meth-ods, it will be clear to which category they belong to. We will see in Sec. 4 that the most promising sen-tence selection strategies belong to the second cate-gory. 3.1 The Utility of Translation Units Phrases are basic units of translation in phrase-based SMT models. The phrases potentially extracted from a sentence indicate its informativeness. The more new phrases a sentence can offer, the more informative it is. Additionally phrase translation probabilities need to be estimated accurately, which means sentences that contain rare phrases are also informative. When selecting new sentences for hu-man translation, we need to pay attention to this tradeoff between exploration and exploitation , i.e. selecting sentences to discover new phrases vs es-timating accurately the phrase translation probabil-ities. A similar argument can be made that empha-sizes the importance of words rather than phrases for any SMT model. Also we should take into account that smoothing is a means for accurate estimation of translation probabilities when events are rare. In our work, we focus on methods that effectively expand the lexicon or set of phrases of the model.
The more frequent a phrase is in the unlabeled data, the more important it is to know its translation; since it is more likely to occur in the test data (es-pecially when the test data is in-domain with respect to unlabeled data). The more frequent a phrase is in the labeled data, the more unimportant it is; since probably we have observed most of its translations.
Based on the above observations, we measure the importance score of a sentence as: where X p s can offer, and P ( x |D ) is the probability of observ-The score (1) is the averaged probability ratio of the set of candidate phrases, i.e. the probability of the candidate phrases under a probabilistic phrase model based on U divided by that based on L . In ad-dition to the geometric average in (1), we may also consider the arithmetic average score: Note that (1) can be re-written as which is similar to (2) with the difference of additional log .

In parallel data L , phrases are the ones which are extracted by the usual phrase extraction algorithm; but what are the candidate phrases in the unlabeled data? Considering the k -best list of translations can tell us the possible phrases the input sentence may offer. For each translation, we have access to the phrases used by the decoder to produce that output. However, there may be islands of out-of-vocabulary (OOV) words that were not in the phrase table and not translated by the decoder as a phrase. We group together such groups of OOV words to form an OOV phrase. The set of possible phrases we extract from the decoder output contain those coming from the phrase table (from labeled data L ) and those coming from OOVs. OOV phrases are also used in our com-putation, where P ( x | L ) for an OOV phrase x is the uniform probability over all OOV phrases. 3.1.2 n -grams (Geom n -gram, Arith n -gram)
As an alternative to phrases, we consider n -grams as basic units of generalization. The resulting score is the weighted combination of the n -gram based scores: where X n P ( x |D ,n ) is the probability of x in the set of n grams in D . The weights w of the scores of n -grams with different lengths. In addition to taking geometric average, we also con-sider the arithmetic average: As a special case when N = 1 , the score motivates selecting sentences which increase the number of unique words with new words appearing with higher frequency in U than L . 3.2 Similarity to the Bilingual Training Data The simplest way to expand the lexicon set is to choose sentences from U which are as dissimilar as possible to L . We measure the similarity using weighted n -gram coverage (Ueffing et al., 2007b). 3.3 Confidence of Translations (Confidence) The decoder produces an output translation e using the probability p ( e | f ) . This probability can be treated as a confidence score for the translation. To make the confidence score for sentences with dif-ferent lengths comparable, we normalize using the sentence length (Ueffing et al., 2007b). 3.4 Feature Combination (Combined) The idea is to take into account the information from several simpler methods, e.g. those mentioned in Sec. 3.1 X 3.3, when producing the final ranking of sentences. We can either merge the output rankings ated by them as input features for a higher level ranking model. We use a linear model: where  X  the feature functions from Sections 3.1 X 3.3, e.g. confidence score, similarity to L, and score for the utility of translation units. Using 20K of Spanish unlabeled text we compared the r 2 correlation co-efficient between each of these scores which, apart from the arithmetic and geometric versions of the same score, showed low correlation. And so the in-formation they provide should be complementary to each other.

We train the parameters in (5) using two bilingual development sets dev1 and dev2, the sentences in dev1 can be ranked with respect to the amount by which each particular sentence improves the BLEU this ranking, we look for the weight vector which produces the same ordering of sentences. As an al-ternative to this method (or its computationally de-manding generalization in which instead of a single sentence, several sets of sentences of size k are se-lected and ranked) we use a hill climbing search on the surface of dev2 X  X  BLEU score. For a fixed value of the weight vector, dev1 sentences are ranked and then the top-k output is selected and the amount of improvement the retrained SMT system gives on dev2 X  X  BLEU score is measured. Starting from a random initial value for  X  mension at a time and traverse the discrete grid placed on the values of the weight vector. Starting with a coarse grid, we make it finer when we get stuck in local optima during hill climbing. 3.5 Hierarchical Adaptive Sampling (HAS) (Dasgupta and Hsu, 2008) propose a technique for sample selection that, under certain settings, is guar-anteed to be no worse than random sampling. Their method exploits the cluster structure (if there is any) in the unlabeled data. Ideally, querying the label of only one of the data points in a cluster would be enough to determine the label of the other data points in that cluster. Their method requires that the data set is provided in the form of a tree represent-ing a hierarchical clustering of the data. In AL for SMT, such a unique clustering of the unlabeled data would be inappropriate or ad-hoc. For this reason, we present a new algorithm inspired by the ratio-nale provided in (Dasgupta and Hsu, 2008) that can be used in our setting, where we construct a tree-namic tree construction allows us to extend the HAS algorithm from classifiers to the SMT task.
 The algorithm adaptively samples sentences from U while building a hierarchical clustering of the sen-tences in U (see Fig. 1 and Algorithm 2). At any it-eration, first we retrain the SMT model and translate all monolingual sentences. At this point one mono-lingual set of sentences represented by one of the tree leaves is chosen for further partitioning: a leaf H is chosen which has the lowest average decoder confidence score for its sentence translations. We then rank all sentences in H based on their similar-ity to L and put the top  X  | H | sentences in H the rest in H sample  X K sentences from H tences from H tions. 3.6 Reverse Model (Reverse) While a translation system M guage F to language E , we also build a translation system in the reverse direction M sure how informative a monolingual sentence f is, we translate it to English by M Algorithm 2 Hierarchical-Adaptive-Sampling the translation back to French using M this reconstructed version of the original French sentence by  X  f . Comparing f with  X  f using BLEU (or other measures) can tell us how much information has been lost due to our direct and/or reverse transla-tion systems. The sentences with higher information loss are selected for translation by a human. The SMT system we applied in our experiments is PORTAGE (Ueffing et al., 2007a). The models (or features) which are employed by the decoder are: (a) one or several phrase table(s), which model the translation direction p ( f | e ) , (b) one or several n -gram language model(s) trained with the SRILM toolkit (Stolcke, 2002); in the experiments reported here, we used 4-gram models on the NIST data, and a trigram model on EuroParl, (c) a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase, and (d) a word penalty. These different models are combined log-linearly. Their weights are optimized w.r.t. BLEU score using the algorithm described in (Och, 2003). This is done on a development corpus which we will call dev1 in this paper.

The weight vectors in n -gram and similarity methods are set to ( . 15 ,. 2 ,. 3 ,. 35) to emphasize longer n -grams. We set  X  =  X  = . 35 for HAS, and use the 100-best list of translations when identi-fying candidate phrases while setting the maximum phrase length to 10 . We set  X  = . 5 to smooth proba-bilities when computing scores based on translation units. 4.1 Simulated Low Density Language Pairs We use three language pairs (French-English, German-English, Spanish-English) to compare all of the proposed sentence selection strategies in a simu-lated AL setting. The training data comes from Eu-roParl corpus as distributed for the shared task in the NAACL 2006 workshop on statistical machine translation (WSMT06). For each language pair, the first 5K sentences from its bilingual corpus consti-tute L , and the next 20K sentences serve as U where the target side translation is ignored. The size of L was taken to be 5K in order to be close to a real-istic setting in SMT. We use the first 2K sentences from the test sets provided for WSMT06, which are in-domain, as our test sets. The corpus statistics are summarized in Table 1. The results are shown in Fig. 2. After building the initial MT systems, we se-lect and remove 200 sentence from U in each itera-tion and add them together with translations to L for 25 iterations. Each experiment which involves ran-domness, such as random sentence selection base-line and HAS, is averaged over three independent runs. Selecting sentences based on the phrase-based utility score outperforms the strong random sentence selection baseline and other methods (Table 2). De-coder confidence performs poorly as a criterion for sentence selection in this setting, and HAS which is built on top of confidence and similarity scores outperforms both of them. Although choosing sen-tences based on their n -gram score ignores the re-lationship between source and target languages, this methods outperforms random sentence selection. 4.2 Realistic Low Density Language Pair We apply active learning to the Bangla-English ma-chine translation task. Bangla is the official lan-guage of Bangladesh and second most spoken lan-guage in India. It has more than 200 million speak-ers around the world. However, Bangla has few available language resources, and lacks resources for machine translation. In our experiments, we use training data provided by the Linguistic Data Con-sortium 5 containing  X  11k sentences. It contains newswire text from the BBC Asian Network and some other South Asian news websites. A bilingual Bangla-English dictionary collected from different websites was also used as part of the training set which contains around 85k words. Our monolingual corpus 6 is built by collecting text from the Prothom Alo newspaper, and contains all the news available for the year of 2005  X  including magazines and pe-riodicals. The corpus has 18,067,470 word tokens and 386,639 word types. For our language model we used data from the English section of EuroParl. The development set used to optimize the model weights in the decoder, and test set used for evaluation was taken from the same LDC corpus mentioned above.
We applied our active learning framework to the problem of creating a larger Bangla-English parallel text resource. The second author is a native speaker of Bangla and participated in the active learning loop, translating 100 sentences in each iteration. We compared a smaller number of alternative methods to keep the annotation cost down. The results are shown in Fig. 3. Unlike the simulated setting, in this realistic setting for AL, adding more human transla-tion does not always result in better translation per-features that prove most useful in extracting useful sentences for the human expert to translate. 4.3 Domain Adaptation In this section, we investigate the behavior of the proposed methods when unlabeled data U and test data T are in-domain and parallel training text L is out-of-domain.

We report experiments for French to English translation task where T and development sets are the same as those in section 4.1 but the bilingual main is similar to EuroParl, but the vocabulary is very different. The results are shown in Fig. 4, and summarized in Table 3. As expected, unigram based sentence selection performs well in this scenario since it quickly expands the lexicon set of the bilin-gual data in an effective manner (Fig 5). By ignor-Lang. Geom Phrase Random (baseline) Pair bleu% per% wer% bleu% per% wer% Fr-En 22.49 27.99 38.45 21.97 28.31 38.80 Gr-En 17.54 31.51 44.28 17.25 31.63 44.41 Sp-En 23.03 28.86 39.17 23.00 28.97 39.21 ing sentences for which the translations are already known based on L , it does not waste resources. On the other hand, it raises the importance of high fre-quency words in U . Interestingly, decoder confi-dence is also a good criterion for sentence selection in this particular case. Despite the promise of active learning for SMT for domain adaptation and low-density/low-resource languages, there has been very little work published on this issue. A Ph.D. proposal by Chris Callison-Burch (Callison-burch, 2003) lays out the promise of AL for SMT and proposes some algorithms. However, the lack of experimental results means that performance and feasibility of those methods can-not be compared to ours. (Mohit and Hwa, 2007) provide a technique to classify phrases as difficult to translate (DTP), and incorporate human transla-tions for these phrases. Their approach is differ-ent from AL: they use human translations for DTPs in order to improve translation output in the de-coder. There is work on sampling sentence pairs for SMT (Kauchak, 2006; Eck et al., 2005) but the goal BLEU score has been to limit the amount of training data in order to reduce the memory footprint of the SMT decoder. To compute this score, (Eck et al., 2005) use n -gram features very different from the n -gram features pro-posed in this paper. (Kato and Barnard, 2007) imple-ment an AL system for SMT for language pairs with limited resources (En-Xhosa, En-Zulu, En-Setswana and En-Afrikaans), but the experiments are on a very small simulated data set. The only feature used is the confidence score of the SMT system, which we showed in our experiments is not a reliable feature. We provided a novel active learning framework for SMT which utilizes both labeled and unlabeled data. Several sentence selection strategies were proposed and comprehensively compared across three simu-lated language pairs and a realistic setting of Bangla-English translation with scarce resources. Based on our experiments, we conclude that paying atten-tion to units of translations, i.e. words and candi-date phrases in particular, is essential to sentence se-lection in AL-SMT. Increasing the coverage of the bilingual training data is important but is not the only factor (see Table 4 and Fig. 5). For exam-ple, decoder confidence for sentence selection has low coverage (in terms of new words), but performs well in the domain adaptation scenario and performs poorly otherwise. In future work, we plan to ex-plore selection methods based on potential phrases, adaptive sampling using features other than decoder confidence and the use of features from confidence estimation in MT (Ueffing and Ney, 2007).
