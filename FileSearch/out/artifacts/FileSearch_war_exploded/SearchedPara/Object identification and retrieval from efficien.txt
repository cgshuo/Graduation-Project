 1. Introduction
The primary difference between text and non-text IR is that text IR attempts to retrieve relevant documents based on  X  X  X emantic X  X  content whereas traditional non-text IR (e.g., content based image retrieval (CBIR)) attempts to retrieve images based on  X  X  X yntactic X  X  (i.e., low-level) features. If we considered, for the sake of illustration, that an image is analogous to a printed page of a document, on such an analogy, traditional CBIR which is feature based, would roughly be similar to retrieving text documents based on their font size, their layout, the colour of the ink, etc. (i.e., physical characteristics of the document) rather than on their meaning-ful content.
In general, image retrieval systems are only of the following types: using image to retrieve images, using non-image (usually text) to retrieve images, or using image to retrieve non-images (information in general).
Traditional CBIR is of the first variety, annotated image banks and Google are of the second, and there are only a few efforts in the third. In this paper we explore the possibility of non-image retrieval based on the content of images. Such an approach roughly requires an object identification phase to generate semantic content followed by whatever suitable actions based on those semantics.

There are many applications for successful object identification systems. We are particularly interested in two of them. The first is for homeland security or image monitoring in general. In our discussions with intel-ligence agencies, they tell us that they need a way to filter images in the same way that text is filtered. Tradi-tional CBIR does not work for their scenario which needs to work at the semantic object level and not at the syntactic feature level. Another strong reason is that it is easy to give a codeword to replace a name or definite description. So terrorists may use FOO in email and chat to refer to, say, the Subic Bay Naval Base thus defeating keyword spotting algorithms, but if they were to exchange an image, it would be very difficult to code it. Note that they cannot just encrypt their conversations or images since encrypted data is a red flag in monitoring scenarios.

The second application follows an important trend in mobility; this is the increasing prevalence of cameras on mobile phones. During an industry panel at the Consumer Electronics Show in January 2005 mated that 700 million mobile handsets will be sold in 2005 and 2/3 of them will have cameras. A significant number of pictures taken on such cameras are likely to be  X  X  X hrow-away X  X  images, i.e., pictures taken to serve a function and which has no value once that function is served. Scenarios mooted include taking a picture of a dress to get an opinion from a friend, or as an illustration to a message. But the scenario which we are inter-ested in, is taking a picture to find out more information. So a tourist takes a picture of an unknown land-mark, sends it to a server, and gets back useful information. Or a health-conscious consumer takes a picture of his dinner, sends it to a server and gets back nutritional information.

In Section 2 , we emphasize the significance of a few aspects in our work including the task, the indexing and retrieval paradigm, a unique image dataset, and the requirement for fast query processing. Then we describe important applications related to the object identification task followed by related approaches. Section 4 is devoted to the description of the current prototype on mobile image-based tourist directory. Our experimental evaluation on our unique STOIC dataset is given in Section 5 . 2. Significance
There are four key aspects in our work. First, we look at object identification as an important genre of image search. Second, images are used as intermediate means to retrieve information about an object or loca-tion. Third, we introduce a new type of image dataset with associated queries and relevance judgments. Con-ventional image datasets are not designed or evaluated at the semantic level. Last but not least, we show that simple image feature matching is sufficient for good object identification if we provide a sufficient image set for object description. Such efficient techniques are necessary for very large scale critical applications such as homeland security monitoring or the limited processing capacity of the ubiquitous camera phones. 2.1. Object identification
The object identification task may be described as follows: given an image, determine the referent most salient object in the image. For example, all three images in Fig. 1 are of the same object, albeit from different perspectives, scales, and colour. The referent is the Merlion statue in One Fullerton in Singapore.
The most salient object in the image may be the image in its entirety, e.g., the skyline image in Fig. 2 but which also includes the Merlion in it.
Once the object has been successfully identified, any appropriate follow-up based on the referent of the object can be activated. For example, you may be a tourist looking at the Merlion but not knowing what it is. You snap a picture using your mobile phone and send to a Tourist Information server (via multimedia messaging). The system identifies the object as the Merlion and sends back information about it. The link to the information is through what the object is and not what the object looks like . We describe such a system prototype known as Snap2Tell below.

Incidentally, face recognition in an image is a special sub-genre of object detection in images. This is a more constrained task as there is an implicit normalization of the face. However there are also clear limitations, e.g., a picture of the back of a person X  X  heard is not a candidate for face recognition. More generally for an object, however, we should be able to identify it from any angle. This paper will not be considering face recognition but landmarks recognition. 2.2. Image set as index
In our image-to-information task, images that capture varying appearances of an object are used as an intermediate means in indexing and matching for retrieving the final information for a given query image.
That is, in our mobile tourist information application, we index text and audio description of landmarks by a set of images of this landmark. We know that content access using image is still a difficult task for a com-puter, because low level features are often not enough to describe the intrinsic semantics depicted in an image content. Some meaningful image annotation can be automatically obtained like in Fan, Gao, Luo, and Xu (2004) , but they are only useful for categorizing a set of images by a few image content description (sand, sea, sky) and not very useful for the selection of one particular building, or sculpture. To overcome this prob-lem, we propose to describe one item image using a set of representative images that are taken with different viewing perspective, distances, and lighting conditions.

As in text retrieval, a short text is hard to be retrieved as one needs additional information like synonyms or ontology or linguistic features. (Ex: in answering definitional question Xu, Weischedel, &amp; Licuanan (2004) , one can use prototypal linguistic expressions). Long texts are better retrieved because of augmented probabil-ity of world concurrency related to the query. In our case, we think that increasing the amount of related relevant images to an object in the database will improve the chance of identifying this object. 2.3. The STOIC dataset
Traditional CBIR efforts have often started from fairly arbitrary collection of images, i.e., those which are easily available to the researchers, rather than designed specifically for research. The Singapore Tourist Object
Identification Collection (STOIC) dataset is designed to explore new possibilities in image search, specifically in the genre of object identification from images.

The STOIC database consists of thousands of outdoor scene images, typically of tourist locations, in Sin-gapore. It was first created to test the image-based mobile tourist information retrieval system known as
Snap2Tell, but can be used for other benchmarking. This data set is freely available. The images were taken using several cameras (including camera phones and PDA) in different resolutions with high variance in light-ing, distances, and angles. Occlusions and cluttered background are also ensured by the authors. The wide spectrum of imaging conditions is to simulate unconstrained images taken by a casual tourist in real situations.
Example in Fig. 3 shows 2 picture of 12 different locations. In these examples, one can see (top down, left to right) light variation, orientation, colour shift due to camera device, back light condition, part of the landmark (bridge, theatre, temple, sculpture, buildings), same sculpture (Merlion), but in different locations and a small sculpture with a strong background variation.
 STOIC is a work in progress and currently 4 comprises about 3000 pictures taken at or of tourist spots in
Singapore. There are 120 different spots 5 . In this paper, we have used several version of this database collec-tion. So results presented here are not comparable, otherwise stated. Fig. 4 shows a distribution of number of spots for a given set size of index images. We can see that a majority of scenes (43) have an image index set between 5 and 9, the rest have various amount of images up to 190. Images are taken with eight different imag-ing devices, with different resolutions, from different perspectives, at different times, under different weather conditions, and by different people. Imaging devices include low and high end digital cameras, PDA cameras and phone cameras. In particular, the images taken by the PDA and phones are of much lower resolution, often colour shifted, with poor contrast and uneven saturation (see Fig. 3 ). Each image is tagged with meta-data to identify the referent and to provide context information for experimentation. Context information includes location (GPS and phone cell id), author, date, and device.

The dataset can support many types of queries. For the object detection task, the query set are images of the tourist spots or objects in the tourist spots. Each image in the query set has an unambiguous referent.
Since the task is object detection, there are various ways of evaluating the success of a retrieval or a match of the query image. In particular, the idea of a ranked list (with traditional precision and recall measures) does not help at all (see 5.1 ). Instead, we have to use the following evaluations:
Strict for a given query image, if the retrieved first image has the same referent, then it is successful. Other-wise not. This corresponds to successful object identification.

Loose for a given query image and a ranked list of retrieved images, what is the position of the first image with the same referent as the query image. While this is meaningless to the object identification task since any result other than first position is not successful, it provides an artificial non-binary measure of perfor-mance which is useful for training and evaluating changes.

Assessments of relevance, i.e., that an image has the same referent as a query image, were done by several assessors. The operational criteria for saliency was according to the intuitions of assessors. Thus if a Merlion image was the query, it would match the image in Fig. 1 but not Fig. 2 even if few pixels represent it in Fig. 2 .
The complete test collection, comprising the STOIC dataset, queries and relevance judgments, are freely avail-able from: http://ipal.imag.fr/snap2tell/ . 2.4. Efficient image processing
We are motivated in this research by the applications mentioned above. They have the same requirement for efficient image processing. There is still a limitation on the carrier bandwidth to transfer images taken on mobile phones. Ideally, the image processing should take place on the phone and only a proxy (e.g., image feature vectors) be sent to the server. Given the low compute capability on the phone, this requires that only simple image processing techniques can be applied if we are to maintain a realistic response time. Thus a pro-gram which can do scene/object identification very efficiently is needed.
 In these initial experiments, we are using simple algorithms because they are more computationally efficient.
We compensate for the simplicity by loading the server side (the matching engine) with many more examples of the objects to be recognized. Our hypothesis is that this compensation works sufficiently well for the object identification task. 3. Related work
The touring machine Feiner, MacIntyre, Hllerer, and Webster (1997) is an example of augmented reality in urban environment. This is an early version of digital help to orientation and access to information with mobile devices. The system Infoscope Haritaoglu, 2001 is a good example of what can be multimedia mobile information retrieval. This is mainly an information augmentation system for foreign travelers, which super-imposes new information like text translation into an actual picture of a scene. The system is composed by a
PDA client associated with GPS for location, liked with a phone for communication. The communication with a server is required due to processing power limitations of small devices like PDA. The second application they proposed, information augmentation in the city , add information to an actual picture such as details about a flat for rental house hunting.
 From the technological point of view, obtaining location-based information is already possible with the
GPS devices or the GSM cellular network infra-structure. However, knowing the location of a mobile phone user is not sufficient to determine what he or she is interested in (or looking at). The location-based informa-tion certainly helps to refine the user X  X  context, but fails to capture his or her intention. Hence, image-based query is complementary to the context localization information.

Another mobile tourist information system is described in Mai, Dodds, and Tweed, 2003 . The client device used is a PDA system connected to internet through WLAN. It supposes that this wireless access point is installed in the area in which the system is going to work. The system includes an iPAQ 3870, a NexiCam
PDA camera, an orientation sensor, and a GPS receiver. The position detection is ensured by a GPS attached to the PDA. However, the direction and tilt sensor is connected to the PDA via a laptop computer due to technical difficulty. The camera is integrated into the communication device and localization is provided by the telecommunication operator.

In the PDA prototype Mai et al. (2003) , the image taken from the connected camera together with GPS and orientation data are sent to a server. The server then runs the 3DMax program to generate a reference image from the same position and angle in a 3D model built in advance based on the GPS and orientation data. The matching is performed using detected line features. Only one building model has been constructed and tested in the paper though color segmentation has been explored for future experimentation.

We believe that a camera phone is a better choice for communication than a PDA. In Snap2Tell, we have chosen a camera mobile phone which is a lighter and more ubiquitous device. The camera is integrated into the communication device and localization is provided by the telecommunication operator. Moreover, our approach of scene recognition is different. Instead of unnatural matching between a real image and a synthe-sized image from 3D model, our server will match the query image with different images of a scene, taken using different angles and positions. We think that 3D model construction is costly and not applicable to all kinds of scenes. The PDA system Mai et al. (2003) requires a GPS device, orientation sensor, and WLAN connection. We think this solution is not realistic.
 The IDeixis system Tollmar, Yeh, and Darrell (2004) also adopted camera-phone as the query device and
MMS as the communication infrastructure. However the image database was constructed from 12,000 web-crawled images where the qualities are difficult to control. The 50 test query images were centered around only three selected locations. The evaluation was still based on conventional image retrieval paradigm using the percentage of attempts their test subjects found at least one similar image among the first 16 retrieved images.
In Hare and Lewis, 2005 authors present a system based on PDA connected wireless (not by phone) able to recognise visual query from a painting database (National Gallery image collection). This system uses salient image region as descriptors, the Scale Invariant Feature Transform Lowe, 2004 which is effective for detecting similar images in different positions. The difference with our approach resides in the use of picture of existing strong variation of the image content, less change in lightning condition (weather, time of the day), no occlu-sions. Moreover, the background is not important in this application. Problems that remain concern part of image identification, low quality query image.

The MUVIS system Ahmad, Abdullah, Kiranyaz, and Gabbouj, 2005 uses classic image descriptors (color histogram, dominant color, gray level co-occurrence matrix for texture, Gabor analysis), for image indexing.
There is no image processing perform on the phone. This system is evaluated on the quantitative aspect only (query timing) on different mobile hand sets. On user aspect, one can also find some studies about the problem of User Interface for mobile Information Retrieval Schofield and Kubin (2002) .

In Snap2Tell, we consider a more comprehensive set of locations for both database and queries. The eval-uation was carried out using object identification paradigm with the use of contextual cues such as location priming and with investigation into the effect of poor quality query images produced by mobile devices. This notion of context has been also extended for other application, like helping localization from a vehicle Ram-nath, Lim, Chevallet, and Zhang (2005) . A preliminary version of this system has been presented in Lim, Che-vallet, and Merah (2004) . 4. Snap2Tell: a mobile image-based tourist directory
Imagine you are at a tourist spot looking at a beautiful lake or interesting monument. Instead of searching through your travel guide books to learn more about the scene, you snap a picture of the scene using your camera phone and send it to a service provider. Short time after, you receive an audio clip or a text message that provides you more information about the scene. You can continue to enjoy the scene while your fingers carry out this information retrieval task. As the saying goes  X  X  X  picture is worth thousand words X  X , a tourist can forget about the hassle of looking up scene description in a travel guide that distracts him/her from enjoy-ing the scene or recalling the right name for the scene (assuming he/she knows what the scene is) to access a text-driven information directory. Moreover the charging of the on-demand service is more fine-grained and hence can tailor to the need of each tourist. Service providers can charge a fee for using this fun, easy-to-use and convenient picture-driven information directory, independent of the MMS charges. 4.1. System architecture
The Snap2Tell framework is realized as a typical three-tier client/server architecture (see Fig. 5 ). The cli-ent is a mobile phone with built-in camera that supports MMS, and GPRS such as the Nokia 7650 model used in our development and test. With the camera phone, a user can launch the Snap2Tell application to send a request to the application server. The request is a picture of a real scene or object that information is sought.

After receiving the query, the application server obtains the location information from the mobile network operator. With the location identified, the Snap2Tell server sends a SQL query to the database to retrieve the image meta-data for the scenes related to the location. The image meta-data of the query image is extracted and compared with image meta-data of the scenes by image matching algorithm. If the best matching score is above a certain threshold, scene descriptions of this best matched image is extracted from the scene database. Otherwise, a no match situation has occurred.

The Nokia 7650 mobile phone is used with the Nokia Series 60 platform (powered by Symbian OS v6.1), and is one of the earliest all-in-one device that combines mobile phone, digital camera and PDA functions.
Fig. 6 displays a sequence of screen shots for a running Snap2Tell client which is written in C++ programming language. Following a top X  X own, left-to-right order, the first three screen shots shows the invocation of the
Snap2Tell application on Nokia 7650 phone. After the Snap2Tell application is active, the user can start the camera to take a picture or open an existing image stored on the phone to used as the query as shown in the fourth screen shot. In this illustration, the user has chosen to select a stored image  X  X  X upremeCourt-16.jpg X  X  as query (fifth screen shot on the second row) which is displayed in the sixth screen shot. Note that if the user has decided to take a picture instead, the video camera will be turned on to allow the user see what the camera is focused at.

Once the user has selected or created a query image, he or she can scroll to the  X  X  X et Description X  X  option to initiate a query. As described above, the query will be sent as a MMS to the Snap2Tell application server.
Once a MMS reply is received from the Snap2Tell application server, the user can play the MMS. As illus-trated in the last screen shot in Fig. 6 , the description is shown as text or/and audio. The Snap2Tell application server is the functional core of the system and is developed in Java. 4.2. Scene database
Using Singapore in our tests, we have set up an original data set of image and descriptions which is a subset of the STOIC dataset. We have divided the map into zones. A zone includes several locations, each of which may contain a number of scenes. A scene is characterized by images taken from different view-points, distances, and possibly lighting conditions. Besides a location ID and image examples, a scene is asso-ciated with a text description, an audio description which is send to the user as answer to his query. Fig. 7 shows relationships among zone, location, scene, and category with examples. For Location 11: Chinatown in Zone 4, two scenes  X  X  X hinatown X  X  and  X  X  X hian Hock Keng Temple X  X  are shown. Three scenes labeled as  X  X  X ndian National Monument X  X ,  X  X  X upreme Court X  X , and  X  X  X ir Raffles Statue X  X  are located in Location 14 of Zone 5.
 5. Experimental evaluation
As the STOIC Dataset is an ongoing task, we have set up a first experimentation with only 535 images, two devices (cameras) and 103 locations. This first version of STIOC gives us the opportunity to test the usefulness of location based context and impact of simple image structure using blocks. For this initial study, we have adopted color histograms Swain and Ballard (1991) to characterize and index the images. They are known to be invariant to translation and rotation about the viewing axis and change only slowly under change of angle of view, change of scale, and occlusion.

We have experimented with both global and local color histograms. There is a trade-off between content symmetry and spatial specificity. If we want images of similar semantics with different spatial arrangement (e.g., mirror images) to be treated as similar, we can have histograms of larger blocks (i.e., the extreme case will be a single block that covers the entire image, similar to the effect of a global histogram). However, spatial locations are sometimes important for discriminating more localized objects. Then local histograms will pro-vide good sensitivity to spatial specificity. Furthermore, we can attach different weights to the blocks to emphasize the focus of attention: in our case we have emphasized the center. That is, the similarity k between a query q (with m local blocks Z j ) and an image x (with m local blocks X where x j are weights, and k ( Z j , X j ) is the similarity between two image blocks defined as
Note that this similarity measure is equivalent to histogram intersection Swain and Ballard, 1991 between his-tograms H i ( Z j ) and H i ( X j ).

We use color histograms in the Hue, Saturation, and Value (HSV) Smith (1978) color space as it is found to be perceptually more uniform than the standard RGB color space Paschos (2001) . The number of bins of a color histogram is b 3 where b is the number of equal intervals in each of the H, S, and V dimensions. We have tested from 2 to 14 bins. We have also partitioned an image in identical blocks in both X  X  Y dimensions (i.e.,
K  X  K grid). When two images are compared, we only compare the two local histograms of the corresponding blocks (Eq. (2) ) with equal weights (Eq. (1) ). We have tested a maximum of K = 8 in each dimension. That is, images are split into 64 blocks, and 64 histograms are computed in this case. Note that K = 1 refers to global color histogram.

For experimentations with the dataset, we have adopted the leave-one-out methodology for evaluation. In all tests, each image of the test collection is considered as a query and is removed from the collection. This image is tested for histogram similarity matching against the rest of the collection: we compute histogram similarity between this query image and all other images, and we sort the similarities in descending order. The strict mean precision is the percentage of success in object identification for the whole collection. On the followings tests, we want to look at the following points:
Influence of bins : Our goal is to find a balance between the number of bins of histogram send by the phone and the precision of scene recognition. Of course, we have to minimize the data size to be sent via the phone.

Influence of image composition : We make the assumption that image composition, i.e., the position of the object of interest, plays a role in image matching. This position information is lost if we use histogram.
Spilling image into fixed or overlapping blocs is a simple way to take into account image structure. Blocks can be set regularly, randomly, of manually. Explicit manual image cropping should input explicit human knowledge expertise into the system. We also can based on the assumption according to which normal users tends to move the interesting object in the center of his image. In that case, it we can emphasis information that belongs to the center of an image.

Influence of context : The position of the input device can be used to reduce the number of possible building to recognize. Hence this information should improve system results.

Influence of data size : Having more scenes to recognize should reduce system precision, but augmenting the number of image per scene should increase the probability to recognize this scene. This issue of scalability is important and there is a trade-off to find between these two values.

Influence of device : The device used as image input, or for the construction of the object image index, may have an influence in the final result. We can expect low quality devices, especially those with low color con-sistency, to perform poorly compare to better one. 5.1. Influence of bins and blocks
Results of this first experimentation is shown in Fig. 8 . In this figure we see the percentage of strict iden-tification. A strict identification arises when the most similar image to the query image belongs to the same scene. In that case, the system has recognized the correct scene.

When the number of histogram bins increases, the discrimination power increases and hence the quality of the results increases too. At some point, more histogram bins may result in mismatch of the bins when slight change in the color distribution can cause shifts of pixel counts in adjacent bins. Without using any location information, the overall best precision is found at 11 bins using 3  X  3 blocks with a mean of 73.4% precision. Using block provides some improvement which shows the importance of image structure. We can see that 3  X  3 block seems to be the minimum for a noticeable improvement. More blocks leads to more computation and not much improvement. Also, after 6 bins, the improvement is to very noticeable.

We have computed the precision at 100% of recall as if we were considering an image retrieval task like in traditional CBIR. It is in fact the ratio of images in the correct scene, on the total of images retrieved when all images of this scene are retrieved. The distribution of best performance is not the same: figures are much lower. Excluding low bins and low blocks number, we obtain precision between 18% and 25%. The distribu-tion of the results depends on the content of actual images. For some scenes, the image set is very homoge-neous, and the whole set is retrieved at the top of the list. As for other scenes, the set is very heterogeneous because the pictures are taken differently with varying distances or view angles, hence this image set is less consistent and reduce the precision at full recall. Clearly in these situations, the color histo-gram approach is not discriminative enough. But, even if this measure is very common in Information Retrie-val, in our case, it is not important at all because we only need that the first image retrieved to be correct to have a correct answer. Moreover, if we want our system to work in a lot of situations, the picture set that describes an object has to cover as much different situations as possible. For example, we can plan to take 4 or 8 pictures in circle around one landmark, and select also different circle sizes: the more different images we have of the object, the more chances we have to be close to an image query. In that sense, a good indexing quality. We really expect in the image database, to have a lot of different images for the same object in order to maximize the chance to find the correct image and then to recognize the landmark. 5.2. Influence of context
In this empirical study, we also want to investigate the effect of context as location information, or the posi-tion of the device, for reducing the search space and performance improvement.

Fig. 9 shows the strict precision when using context information. That is, we select the best matching image from the images that share the same context as the query image. Contexts are: zone or location information (see 5.2). This figure shows only the mean precision at 3  X  3 blocks. Clearly, we notice the enhancement when context information is used. We reach 82.4%, and this follows intuition. As before, partitioning the images into smaller blocks for matching is useful, and we notice little variation with context information. Using zone the overall best precision it at 79.6% with 5  X  5 blocks and 14 bins; using location, the overall best precision is at 82.6% with 4  X  4 blocks and 11 bins. 5.3. Influence of image composition
The role of the query image is to indicate the information need. Hence we make the hypothesis the center of the image should be more relevant than the edges. Thus we propose to weight blocks according to their posi-tion at the matching process.

The Fig. 10 shows the results using a linear block weighting from the center to the edge of the pictures, using two weightings: from 1 (edge) to 2 (center) and from 1 to 5. The results shows that for our collection, doubling the center with 3  X  3 blocs with 11 bins, is the best choice (74.9%). We have the same behavior using contextual information. Hence, we have a small improvement using weighted blocks.

We have also tests manual cropping area. We have added to our prototype, a simple tool that allows user to define a crop zone by using the navigation phone button (see Fig. 11 ). Users, if they want, can set up this zone in order to tell the system with portion of the image is more relevant.
 We have test the use of this cropping area on an enlarged version of the test collection. This latest version of
STOIC includes 2829 images and 101 scenes. Each scene contains at least 5 images. In this test, every image has been manually cropped. Then only the histogram of the cropping area is used for comparing one query image to the rest of the collection. During the test, every image of the collection can be used as a query. Hence we make the assumption that the cropping zone is always available and correct. It means that it roughly isolate the building from the surrounding background. For some images, the cropping zone is the image it self because the picture is a close up. Results in Fig. 12 show again that starting 11 bins seems to have interesting precision. Again in this experiment, focussing on a special area of the image improve results. This is not sur-prising as manual cropping is an explicit meaningful introduction of human knowledge; even if this knowledge is very simple.

The combination of both cropped zone with the rest of the image (i.e., background), produces an enhance-ment when the cropping part count for 80% (i.e., a = 0.8) of a linear combination of the histogram distance between the cropping zone and the background. The new image similarity k ping par x c of the image x (rep. cropping part q c of the query q ), and the background x ground q b ), where k ( q , x ) is computed using equation ( 1 ):
This combination provides better precision than total image and cropping zone only. This shows that this little human knowledge is of great benefice to enhance image matching using this simple image comparison. Finally, in order to use the shape information provided by the cropping zone, we have set up a shape filter
F . The goal of this filter is to discard from matching two images in witch cropped objects does not have the same shape. The filter is equal to 1 for identical shapes between the cropped image x q , and tend to zero for less identical shapes. The new matching function k image x . We combine the difference of this ration between two images using an exponential:
The results provided in Figs. 13 and 14 show that introduction of this shape filter on the best results obtained using cropping zone filter, has a positive effect on precision since the value 0.6 of the b parameter.
This results shows that we can exploit the cropping information two times: first be reducing the matching area, and second by exploiting the raw shape of object to match by using this shape ration formula. This shape information has a little more positive effect than the use of the background. 5.4. Influence of set size
For this test, we enlarge the STOIC collection to reach 1650 images with 68 scenes and 8 cameras. The new set of cameras has been chosen so to reflect diversity of camera quality in term of pixel size, sharpness and color consistency. With this new set we imposed a minimum of 5 images for each scene, so we can have at least 4 images do describe an object, during the leave one out process. We notice a positive change in Fig. 15 . The the variety of the set associated to objects.

Moreover, the precision at full recall is now only 8% compared with the previous value of 18 X 25%. Thus, it is worth to mention that the full recall is reduced when the diversity of the image set associated to the object increases. Of course, this reduction depends on the matching algorithm itself, but even with a very good image matching process, if the side of one landmark is very different for one other side, it could be impossible to link semantically these two images to the same object only on a visual basis. 5.5. Influence of top section
We can view our section of the top first image as a k -NN (nearest neighbour) classifier with k = 1, and examine the behaviour of our collection when we increase the k value. Except for the worth situation (no blocs and only 2 bins), we never notice any improvement when increasing this k value. Fig. 16 shows the best and worst situation with every k from 1 to 30. Notice that in case of identical value for two scenes, we choose the set of images with the overall closer total added distance.

Usual behaviour of k -NN classifier is first an increase with k and then a decrease. One possible explanation of our results is the too small size of image set per scene.
 5.6. Influence of device quality
With 8 different cameras, we can test the influence of the device itself. The results by eliminating from the dataset all images that has been taken by the same device. In that way we test the compatibility of one device among all other devices on the whole image collection.

This results shows a clear drop (from 80% to maximum 60% and 15% minimum) in the strict precision. This can be explained by shift of device X  X  color characteristics. The size of the CCD in mega pixel or the year of marketing, does not seem to be very important. When one looks at actual images, we can clearly notice that the Pocket PC and the phone produce very low quality images. Hence we are surprised by the relative good results of the hand phone compared to those of the Pocket PC. It is also surprising that the best results belong to a chip camera (camera 1). We can conclude that we will need either device dependent color calibration or low lever feature extraction less sensitive to camera characteristics. 6. Conclusion
In the Snap2Tell prototype, our approach deals with real situation and real access device in order to measure the feasibility of such a system. It turns out that we have stretched the limit of currently available wearable technology, but we are convinced that ubiquitous computing is going to have rapid development in the very near future. We are also convinced that mobile information access, in particular context-aware image-based information access, will be a hot research and development topic. For example, some simple tech-nology based on barcode 7 readable from the phone camera could be available soon in order to provide extra content to existing mobile information (advertising on the street, journal, etc).

The results we obtained using our original STOIC image database has shown us that simple matching, based on color histograms, combined with localization information, seems powerful enough to solve this object identification problem thought image matching mainly, because of task we have: retrieving among a set of images describing one object, the only one that is closed to image query. It is not a usual IR querying task, and the poor value of the precision at full recall is not that significant in this case. We also know that even if the prototype is functional and that these results are encouraging, results are not good and stable enough to be used for a real commercial product and relying only on color distribution is for sure too weak in many other real situations as it is shown by our test on the 8 different cameras. We are about to investigate this aspect and adding more discriminative image features extractions. The complete test collection is freely available from: http://ipal.imag.fr/snap2tell/ . This work has been done under a IPAL CNRS France and I2R A-STAR Singapore common funded research action.
 References
