 We explore the potential of using users click-through logs where no editorial judgment is available to improve the rank-ing function of a vertical search engine. We base our anal-ysis on the Cumulated Relevance Model , a user behavior model recently proposed as a way to extract relevance sig-nal from click-through logs. We propose a novel way of directly learning the ranking function, effectively by-passing the need to have explicit editorial relevance label for each query-document pair. This approach potentially adjusts more closely the ranking function to a variety of user be-haviors both at the individual and at the aggregate levels. We investigate two ways of using behavioral model; First, we consider the parametric approach where we learn the es-timates of document relevance and use them as targets for the machine learned ranking schemes. In the second, func-tional approach, we learn a function that maximizes the be-havioral model likelihood, effectively by-passing the need to estimate a substitute for document labels. Experiments us-ing user session data collected from a commercial vertical search engine demonstrate the potential of our approach. While in terms of DCG the editorial model out-performs the behavioral one, online experiments show that the behavioral model is on par  X  X f not superior X  to the editorial model. To our knowledge, this is the first report in the Literature of a competitive behavioral model in a commercial setting. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6 [ Articial Intelligence ]: Learn-ing Algorithms, Experimentation  X  This work was done while the authors were with Yahoo! Labs.
 Click-Through Data, User Behavior, Search Engines, Prob-abilistic Model
It is quite natural to assume that users X  interactions with the result pages of a search engine convey information about the degree of relevance of the web pages. This idea has inspired many researchers to use click-through logs to im-prove user search experience. The main approach was to show how useful an appropriate interpretation of user in-teractions  X  X licks and query re formulations, in particular X  can help in constructing more performant relevance func-tions. On the other hand, little work has been published on the problem of learning a ranking solely based on user interactions, entirely by-passing editorial judgment of edi-tors. More precisely, we formulate the problem as follows; Start with a reasonable Information Retrieval ranking func-tion like BM25 or a good language model based approach, collect clicks and use this information to bootstrap a full fledge, state-of-the-art function able to compete with the best learning to rank algorithms based on tens of thousands of query-document judgment pairs.

State of the art search engines use various algorithms that extend the supervised machine learning framework to rank-ing. These have become popular approaches because they have both reasonable complexities and good scalability for very large size of Web data. One caveat of learning to rank methods, however, is that they require expensive manual la-beling of documents to generate training data. The problem is multifold: one is that the labels needs to be updated and expanded regularly in order to maintain a large size of train-ing data and cope with the time-varying, dynamic nature of the Web. The other is that those labor intensive relevance judgments may contain errors and be different from what the real users on the Web require. The problem is made worse, as the internet expands, by the need for ever more specialized search engines  X  X alled verticals for which it is impractical, time consuming, expensive and even infeasible to gather enough editorial judgment. For example, in this work we carry the numerical experiments on the data of a local search engine where it is hard for an editor to judge if such business is a good recommendation for a user living in a remote neighborhood he is unfamiliar with.
 Contribution. In this paper, we explore two related ways of exploiting the record of user interactions with the search engine. Namely, the methods we propose attempt to learn a ranking function directly from the user behavior data, in-stead of editorial judgment data. The basic assumption is that the user behavior is influenced by the overall relevance of the search result sets, and such behaviors can be captured by probabilistic models, treating the relevance of the search result sets as hidden variables. Once a probabilistic model has been decided upon, we have two possibilities: The second approach is more novel and we describe the gen-eral functional learning framework it belongs to in Section 3. The rest of the paper is organized as follows. In Section 2, we review some prior work. Then, in Section 4, we summa-rize a particular user behavior model, the Cumulated Rele-vance model , which was recently introduced in [3] and that serves as a basis for further experiments. Section 4.1 will explain the exact procedure for using the Cumulated Rel-evance model and functional gradient boosting method to obtain user behavior driven ranking functions. Section 5 re-ports experimental results, and Section 6 concludes with a discussion and future research directions.
Over the past decade, a large number of learning to rank algorithms, which require human editorial judgment labels, have been proposed, see [7] and references therein. They mainly focus on devising sensible loss functions as surrogates to the true ranking loss so that minimizing such loss func-tions would lead to a good ranking performance. The user behavior modeling has been primarily centered around mod-eling click-through logs of search engines. We can divide in essentially four categories according to how click information has been used in the literature: gaining insight into typical user behaviors [6], estimating the relevance feature or target values for documents from the user clicks [9, 2], deriving a metric based on clicks to compare ranking functions [1], and directly re-ranking the top documents retrieved from search engine [5]. By no means, these lists are exhaustive.
In this section, we describe the general framework for learning a ranking function both in parameter and functional space. We use user sessions as the basic data structure to record user behavior as well as the content of the result set. To this end, we use S to denote a user session, and it con-sists of three components: a sequence of queries Q = { q a set of documents D = { d ij } presented in response to the queries, and user interactions with those documents in the form of clicks C = { c ij } ,where c ij is 1 if the j -th document of query i was clicked and 0 otherwise. Namely, we can ex-press the session S as S { Q, D, C } . Once a user session S is given, we can associate the collection of events E that are generated from the session to describe the content of the result sets and user behaviors. In addition to associating the observable events with the session S , we also assume a set of hidden variables H that capture aspects that are not directly observable. In this work, our set of hidden vari-ables only includes document relevance, but the framework we propose is by no means limited to this choice.
The user behavior models (UBM) can then be derived from the session data S by imposing a joint probabilistic model on the event E and the hidden variables H , param-eterized by the parameter set U , i.e., P ( E ,H | U ). In exist-ing work, the main usage of such models was to take ad-vantage of abundant user session data S N = { S 1 ,...,S N and estimate the parameters of the model by maximizing the likelihood of the observations E N = {E 1 ,..., E N } takes the form of finding the set of parameters U that maxi-mize the following expression with respect to the parameters in U : U were used as relevance features for ranking functions.
We can go one step further in this line of reasoning and instead of learning the values of the parameters in U ,we consider these as functions of a set of relevant features. To fix ideas, we postulate for a query and document pair ( q ,d ij ) the existence of a corresponding parameter u ij  X  that represents the degree of relevance of document d ij to query q i . We further assume that this relevance can be estimated as a function of features that characterizes the query document pair: u ij = f ( x ij ), where x ij  X  R d rep-resents a d -dimensional feature vector for ( q i ,d ij ), and f is a function that maps R d into R . Then, we can directly learn the relevance function by finding f that maximizes Q cording to the values of f for a given query. We try to show the effectiveness of such obtained ranking function via ex-periments.

In the next sections, we illustrate the framework both in parametric and functional space on the Cumulated Relevance Model , a particularly simple model of the influence of docu-ment relevance on user behavior.
Here, we briefly summarize th e original cumulated rele-vance model. For simplicity, we only consider the case where each session consists of a single query q . Then, without loss of generality, we can express D = { d i } and C = { c i } the size of both sets n .Wethendenote I ( C ) { i : c i  X  C, c i =1 } as the set of clicked document indices in C and, for all i  X  X  ( C ), denote I ( C ) i { j : c j  X  C, c j =1 ,j i } X  X  ( C ) as the set of clicked document indices up to i -th document. Given these notations, for given N sessions, the cumulated relevance model tries to predict the session success and estimates the document utilities by minimizing L ( u 0 ,u 1 ,...,u n )= P fer to [3]. They also partially solve the sparsity problem by adding a Gaussian priors with mean  X  u and variance  X  2 u the utilities; they introduce regularization terms and modi-fies the original loss function as which becomes a maximum a posteriori estimation.
Once the loss function is defined as in the previous section, we can use the step described in Section 3 to express it in a functional form. That is, by associating the utility variable as u i = f ( x i ), where x i is the feature vector for i -th example, we convert the loss function into L ( u 0 ,f ), which equals to
X and try to minimize it in a functional space to obtain the ranking function. We can also add the regularization term as in (1). Since the loss function (2) takes into account the whole document list to compute the loss, it can be seen as a listwise loss function. The optimization of the loss func-tions can be carried out by using functional gradient de-scent method [4] with using regression trees as weak learners, which results in the ranking function.
We now present the experimental results on a commer-cial local search engine. , i.e. a specialized search engine dedicated to find business of different kinds like restaurant, hardware, or flower shops, etc. in the neighborhood of the user. First, we describe how we collected data, then describe the performance metric we use. Finally, we present some re-sults to demonstrate the effectiveness of the user behavior driven ranking functions. Click Session Data: We collected 1.8 million sessions with at least on click from the local search engine, which resulted in 3 million query-document pairs. Each query-document pair ( q, d ) is represented with a feature vector x ,ofwhich dimension is more than 500.
 Editorial Judgment Data: To evaluate the performance of ranking functions derived from the different approaches, we also gathered a set of editorial judgment data. We ran-domly sampled 13,916 queries from the query log, and for each query up to the top 15 documents are retrieved. This results in 170,431 query-document pairs. We then requested human editors to label each query-document pair to be { Per-fect,Excellent,Good,Fair,Bad } according to the degree of relevance of the document with respect to the query. Since we need to train the editorial models, the data is further divided randomly into training set and test set, with 11,583 queries (or 141,238 query-document pairs) in training set and 2,333 queries (or 29,193 query-document pairs) in the test set.

We adopt Discounted Cumulative Gain ( DCG )asoured-itorial metric because of its popularity and its general ac-ceptance. The DCG at position 5 ( DCG @5) of a ranked list of documents with respect to a query q is defined as a weighted average of the scores of the documents in the rank-ing: DCG@5 := the relevance score of the r -th document in the ranked re-sult set. g r = 4 corresponds to a  X  X erfect X  label, and g corresponds to a  X  X ad X  label. Note that this metric would be naturally biased toward the editorial judgment based rank-ing function. However, we still use this metric for our offline experiments as a sanity check.
We now move on to demonstrate the performances of the user behavior driven learning. We compare various behav-ioral and editorial model implementations. We compared the following six different ranking functions: Scheme 1 : Baseline. This is a hand-tuned ranking function Scheme 2 : State-of-the-art learning to rank algorithms that Scheme 3 : 1-step implementation of user behavior driven Scheme 4 : 2-step implementation of user behavior driven Scheme 5 : Pair-wise learning based on  X  X kip Above X  and
Figure 1 provides an overall performance comparison in terms of DCG . The different curves denote the best perfor-mances we achieve with different parameter sets in the gradi-ent boosting trees method and in the model. All algorithms are limited to a maximum of 800 trees, which is enough for all of them to converge. The editorial models training and test sets are the product of randomly sampling the queries with editorial judgments. Training sample size is varied and the effect on performance on the test set is shown in Figure 1 where the x-axis reports the proportion of the total set used for training.

It is demonstrated in Figure 1 that
Overall, both the 1 and 2-step methods improve over the baseline, which shows their viability. The 1-step imple-mentation and 2-step GBDT optimize similar log-likelihood functions (2) and (1), respectively, we would expect simi-lar performance. An important difference between the two is that the utilities of the documents are learned indepen-dently for each query in the 2-step method while they are learned together in the 1-step one. This entails that they are constrained to be on the same scale in the 1-step method, Figure 1: Performance comparison among different algorithms. but not in parametric space. When learning the paramet-ric utilities, GBDT, being a point-wise method, enforce the scores to share a common scale, while GBRank, being a pair-wise method, is sensitive only to the score differences. This means that GBRank is more flexible and this might explain why it performs better. This also suggests that we should revise the Cumulated Relevance model to work on difference in utility rather than absolute values.
Ranking functions can be compared in terms of DCG , but they can also be compared on how users interact with them. To gain more insight, we select the editorial model and the behavioral model with highest DCG (2-step GBRank) and di-verted part of the live traffic from the production function towards the two functions we want to test. Figure 2 reports the click-through rate (CTR) of both models. We observe that the behavioral model experiences a slightly larger, sta-tistically significant, CTR at all positions. The average click rank of the behavioral model is 3.85 against 3.94 for the ed-itorial model. The average CTR at positions 1 and 2 are larger by 3.4% and 1.8% respectively. Although simply com-paring CTR is a tricky comparison, we believe improving clicks on all positions clearly is a positive signal for ranking quality. As far as we know, this is the first record in the Literature of a purely user behavior model showing perfor-mance on par or better than an editorial model in comercial settings. This also demonstrates the limitations of DCG .
We approached the problem of using the click-through logs of user interactions with a search engine to construct a new ranking function that adapts to users. The goal is ambitious as we want this new function to show comparable performance to a state-of-the-art ranking algorithm trained on tens of thousands query-document of editorial judgments. Wesetupageneralframeworkandusedthecumulatedrele-vance model as a concrete example to carry out experiments. Although the DCG of the editorial based model was higher than the user behavior model in the offline experiment, we observed that in the online experiments, users seem to prefer it. To our knowledge, this is the first report in the Literature of a ranking function trained exclusively on click data that outperforms or is at least on par with the state-of-the-art, Figure 2: Click-through rate for Editorial and Be-havioral models in online bucket tests. Values are normalized by the largest CTR. heavily trained editor based ranking function. As for future work, we plan to relax some assumptions of the cumulated relevance models to further improve the performance of the user behavior driven ranking models. [1] B. Carterette and R. Jones. Evaluating search engines [2] O. Chapelle and Y. Zhang. A dynamic bayesian [3] G. Dupret and C. Liao. Cumulated relevance: A model [4] J. Friedman. Greedy function approximation: a [5] S. Ji, K. Zhao, C. Liao, Z. Zheng, G. Xue, O. Chapelle, [6] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [7] T. Y. Liu. Learning to Rank for Information Retrieval . [8] F. Radlinski and T. Joachims. Evaluating the [9] Z. Zheng, H. Zha, K. Chen, and G. Sun. A regression
