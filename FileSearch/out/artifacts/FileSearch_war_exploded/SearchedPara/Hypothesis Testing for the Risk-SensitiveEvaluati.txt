 The aim of risk-sensitive evaluation is to measure when a given information retrieval (IR) system does not perform worse than a corresponding baseline system for any topic. This paper argues that risk-sensitive evaluation is akin to the underlying methodology of the Student X  X  t test for matched pairs. Hence, we introduce a risk-reward tradeoff measure T
Risk that generalises the existing U Risk measure (as used in the TREC 2013 Web track X  X  risk-sensitive task) while be-ing theoretically grounded in statistical hypothesis testing and easily interpretable. In particular, we show that T Risk is a linear transformation of the t statistic, which is the test statistic used in the Student X  X  t test. This inherent relation-ship between T Risk and the t statistic, turns risk-sensitive evaluation from a descriptive analysis to a fully-fledged in-ferential analysis. Specifically, we demonstrate using past TREC data, that by using the inferential analysis techniques introduced in this paper, we can (1) decide whether an ob-served level of risk for an IR system is statistically signifi-cant, and thereby infer whether the system exhibits a real risk, and (2) determine the topics that individually lead to a significant level of risk. Indeed, we show that the latter permits a state-of-the-art learning to rank algorithm (Lamb-daMART) to focus on those topics in order to learn effective yet risk-averse ranking systems.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval; G3.3 [Probability and Statistics]: Experimental design Keywords: Risk-Sensitive Evaluation, Student X  X  t Test
Various paradigms for the evaluation of information re-trieval (IR) systems rely on many topics to produce reliable estimates of their effectiveness. For instance, in the TREC series of evaluation forums, 50 topics is generally seen as the minimum for producing a reliable test collection [2, 25]. However, in more recent times, the evaluation of systems has increasingly focused upon their robustness -ensuring that a given IR system performs well on difficult topics (as investigated by the TREC Robust track [24]), or at least as well as a baseline system (which is known as risk-sensitive evaluation [26]). Recently, the TREC 2013 Web track in-troduced a risk-sensitive task, which assessed how systems could perform effectively yet without exhibiting large losses compared to a pre-determined baseline system [10].
In such a risk-sensitive evaluation, the risk associated with an IR system is defined as the risk of performing a given par-ticular topic less effectively than a given baseline system [8, 9, 26]. In particular, the U Risk risk-sensitive evaluation mea-sure [26] calculates the absolute difference of an effectiveness measure (e.g. NDCG) between a given retrieval system and the baseline system, in a manner that more strongly empha-sises decreases with respect to the baseline (known as risk) than gains (reward). A parameter  X   X  0 controls the risk-reward tradeoff towards losses in effectiveness compared to the baseline, where  X  = 0 weights risk and rewards equally.
In this paper, we argue that in the current practice of risk-sensitive evaluation based on U Risk , any amount of loss in an IR system X  X  average effectiveness, observed on a par-ticular set of topics, is considered enough in magnitude to infer that the system exhibits a  X  real risk X . However, from a statistical viewpoint, such an inferential decision may be said to be valid only if the observed amount of loss cannot be attributed to chance fluctuation. Otherwise, it will be equally likely that the corresponding system may or may not be under a real risk, meaning that it is possible that the system can perform every topic with a score higher than that of the baseline system on another set of topics that could be drawn from the population of topics. On the other hand, it is also possible that the observed amount of loss in a par-ticular system X  X  average effectiveness can be attributed to a chance fluctuation, while the corresponding performance losses for some individual topics are statistically significant in magnitude. In other words, significant performance losses for a few topics may not result in a significant total loss on average, given a relatively large set of topics.

Hence, we advocate that risk-sensitive evaluation can ac-tually provide the necessary basis for (i) testing the signifi-cance of the observed amount of loss in a given IR system X  X  average effectiveness, called inferential risk analysis in this paper, and (ii) testing the significance of the corresponding losses for individual topics, called exploratory risk analysis .
Indeed, we show that the U Risk risk-reward tradeoff mea-sure is actually a linear transformation of the t statistic, as used in the Student X  X  t test. Therefore, using this statistical interpretation of U Risk based upon hypothesis testing, this paper proposes a new risk-reward tradeoff measure, T Risk which is a linear transformation of the existing U Risk mea-sure, yet is theoretically grounded upon the Student X  X  t test for testing the significance of the observed amount of loss in a given IR system X  X  average effectiveness. For  X  = 0, T Risk is equivalent to the standard t statistic used typically in the Student X  X  t test for testing the null hypothesis of equality in the population mean effectiveness for two IR systems. How-ever, for  X  &gt; 0, the U Risk measure emphasises performance losses compared to the baseline effectiveness. This raises challenges in the estimation of the standard error of the cal-culated U Risk scores. For this reason, we propose the use of the Jackknife technique (or leave-one-out) [11], which is a re-sampling technique for estimating the bias and the stan-dard error of any estimate. The Jackknife technique serves two purposes: firstly, to allow the empirical verification of the estimation of the standard error of U Risk as valid; and secondly, for testing the significance of the corresponding performance losses for individual topics.

From a practical perspective, a risk-sensitive evaluation serves two objectives: firstly, as a step further than the clas-sical evaluation of IR systems, which takes into account the stability or variance of retrieval results across queries as well as for the average retrieval effectiveness [8, 9]; and secondly, as a technique for jointly optimising the retrieval effective-ness and robustness of retrieval frameworks such as learning to rank [26]. Indeed, compared to the existing U Risk mea-sure, this paper contributes to both objectives, by exploit-ing the theory of statistical hypothesis testing for allowing meaningful interpretation of risk-sensitive evaluation scores, and also by allowing a learning to rank technique, namely LambdaMART, to focus on those topics that lead to a sig-nificant level of risk, in order to learn effective yet risk-averse ranking systems. The remainder of this paper is structured as follows: Section 2 provides an overview of risk-sensitive evaluation practices, including U Risk ; Section 3 relates the U
Risk measure to the t statistic, and hence proposes the new T Risk risk-sensitive evaluation measure, and discusses the estimation of the standard error. Section 4 and Section 5 describe new forms of analysis, inferential and exploratory respectively, that arise from the T Risk measure, and demon-strate their application upon the TREC 2012 Web track. Next, Section 6 shows how T Risk can improve the robust-ness of the LambdaMART state-of-the-art learning to rank technique. Finally, we review some related work and provide concluding remarks in Sections 7 &amp; 8, respectively.
Different approaches in IR such as query expansion [1, 5] and learning to rank [17] behave differently across topics, often improving the effectiveness for some of the topics while degrading performance for others. This results in a high variation in effectiveness across the topics. To address such variation, there has been an increasing focus on the effective tackling of difficult topics in particular (e.g. through the TREC Robust track [23]), or more recently, on the risk-sensitive evaluation of systems across many topics [8, 9, 26].
Originally, the aim of risk-sensitive evaluation [9] was to provide new analysis techniques for quantifying and visual-ising the risk-reward tradeoff of any retrieval strategy that requires a balance between risk and reward. Hence, it facili-tates the quest for ranking strategies that are more robust in retrieval effectiveness compared to a baseline retrieval strat-egy  X  robust in the sense of the stability or variance of the retrieval results across topics, while achieving good average performance over all topics.

The variance with respect to a given baseline system b over a given set of topics Q with c topics can then be measured as a risk function F Risk , which takes into account the downside-risk of a new system r (i.e. performing a topic worse than the baseline) is defined in [26] as follows: where r i and b i are respectively the score of the system r and the score of the baseline system b on topic i , as mea-sured by a retrieval effectiveness measure (e.g. NDCG@20, ERR@20 [6]). Similarly, a reward function F Reward , which takes into account the upside-risk (i.e. performing a topic better than the baseline) is defined as: Thereby, the overall gain in the retrieval effectiveness of r with respect to b can be expressed as:
Next, a single measure, U Risk [26], which allows the risk-reward tradeoff to be adjusted, was defined: where  X  q = r q  X  b q . The left summand in the square brack-ets, which is the sum of the score differences  X  q for all q where r q &gt; b q (i.e., q  X  Q + ), gives the total win (or upside-risk) with respect to the baseline. Orthogonally, the right summand, which is the sum of the score differences  X  q for all q where r q &lt; b q , gives the total loss (or downside-risk). The risk sensitivity parameter  X   X  0 controls the tradeoff between reward and risk (or win and loss):  X  = 0 results in a pure gain model, while for higher  X  , the penalty for under-performing with respect to the baseline is increased: typically  X  = 1 ; 5 ; 10 [10].

In this paper, we extend the original aforementioned aim of risk-sensitive evaluation with the following contributions: 1. A well-established statistical hypothesis testing theory for risk-sensitive evaluations from which arises a new risk measure T Risk (Section 3), to turn risk-sensitive evaluation from a descriptive analysis to a fully-fledged inferential anal-ysis (Section 4). 2. A method for exploratory risk analysis that can identify the topics that commit real levels of risk (Section 5). 3. Adaptations of the proposed T Risk measure that can en-hance the robustness of the state-of-the-art LambdaMART learning to rank technique, compared to U Risk , without degradations in overall effectiveness, where the learned model adaptively adjusts with respect to the risk level committed by individual topics (Section 6).
Without loss of generality, at  X  = 0, the risk-reward trade-off measure U Risk reduces to the U Gain formula in Eq. (3), which can be expressed as the average gain over c topics: In the context of statistics, U Gain refers to the sample mean of paired score differences,  X  d , for two IR systems (the system under evaluation r and the baseline system b ): and in the context of evaluating IR systems, this refers to the difference in average effectiveness between two IR systems, of system r and the average effectiveness of the baseline system b over c topics.

On the other hand, the Student X  X  t statistic for matched pairs, as is commonly applied when testing the significance of results between two systems, can be expressed as: Within Eq. (7), the standard error of paired sample mean, SE (  X  d ), can be estimated as follows: where s d = p c  X  1 P (  X  i  X   X  d ) 2 i s the paired sample standard deviation. Hence, we argue that the Student X  X  t statistic of Eq. (7) is actually a linear transformation of U Gain from Eq. (3), which we call T Gain : This transformation can be referred to as studentisation (c.f., t -scores) [14], which in fact is a type of standardisa-tion (i.e., z -scores). Standardisation is a monotonic linear transformation, which transforms any given set of data to a set with zero mean and unit variance, while preserving the original data distribution in shape.

The t -score of a raw U Gain measurement, T Gain , differs from the raw measurement in two important aspects. First, given a set of IR systems, a test collection, and a baseline system, the systems X  ranking to be obtained on the basis of T
Gain will not necessarily be concordant with the systems X  ranking to be obtained on the basis of U Gain , since the t statistic takes into account the inherent variation in the ob-served paired score differences r i  X  b i across the topics, i.e., SE (U Gain ). Second, given a particular baseline system, the two T Gain scores to be obtained on two different test col-lections for the same IR system are comparable with each other in magnitude, at least in theory [7], while the two U
Gain scores are not, as typical in the case of the two raw effectiveness scores to be yielded from a standard effective-ness measure, such as mean average precision [28].
Having shown how T Gain can be defined as a linear trans-formation of U Gain , based upon the t statistic, we now exam-ine U Risk , which allows the risk-reward tradeoff to be con-trolled by the  X  parameter. For  X   X  0, the t statistic based on U Risk , which we call T Risk , can be expressed as follows:
Although both the T Gain formula in Eq. (9) and the T Risk formula in Eq. (10) stem from the classical t statistic in Eq. (7), the estimation of the standard error in U Risk , the estimation of SE (U Risk ) within T Risk , is not as straight-forward as in the case of SE (U Gain ), for the reason that the U Risk formula reweighs the score differences  X  i in aver-aging, proportionally to  X  , for each topic i where r i &lt; b as opposed to U Gain . Hence, in the remainder of this sec-tion, we propose two methods to estimate SE (U Risk ): A speculative parametric estimator SE  X  x that is an analogy to the paired sample standard deviation s d (Section 3.1); and a nonparametric Jackknife Estimator SE J , based on the leave-one-out Jackknife technique (Section 3.2). Indeed, later in Section 3.3, we use the Jackknife Estimator SE J to show the validity of the speculative SE  X  x estimator. On the other hand, T Risk has several advantages over U
Risk . Firstly, it can be easily interpreted for an inferen-tial analysis of risk. Indeed, we will later show in Section 4 that in order to test the significance of an observed risk-reward tradeoff score between a particular IR system and a provided baseline system, one can use T Risk as the test statistic of the Student X  X  t test for matched pairs.
Secondly, T Risk permits the identification of topics that commit significant risk or not  X  we call this exploratory risk analysis  X  which we present later in Section 5.

Finally, this exploratory risk analysis leads to new risk-sensitive measures that can be directly integrated into the LambdaMART learning to rank technique, to produce learned models that exhibit less risk than those obtained from U Risk whilst not degrading effectiveness, as explained in Section 6.
Let the random variable X i denote the risk-reward trade-off score between system r and baseline b for topic i : for i = 1 ; 2 ; : : : ; c and a predefined value of  X   X  0. Then, the standard error of U Risk , SE (U Risk ) can be approximated by the standard error of the sample mean  X  x : where s 2 x = c  X  1 P ( x i  X   X  x ) 2 . Here, the sample mean  X  x corre-sponds to the U Risk score considered as the arithmetic mean of the sample of the observed individual topic risk-reward tradeoff scores x 1 ; x 2 ; : : : ; x c at a predefined value of  X  :
This parametric estimator of SE (U Risk ), SE  X  x , is specula-tive and hence its validity might be compromised to some ex-tent. Therefore, we empirically verify the validity of SE estimating SE (U Risk ) by means of comparing it with a non-parametric re-sampling technique, called the Jackknife [21], which we present in Section 3.2. Indeed, by comparing the two estimates of SE (U Risk ) (i.e., the parametric estimate SE  X  x of Eq. (12) and the nonparametric Jackknife estimate of SE (U Risk )), one can decide whether an inference to be made on the basis of the T Risk statistic is valid. If the two estimates agree with each other, such an inference may be said to be valid, otherwise its validity is compromised.
In this paper, the Jackknife technique is employed for a purpose which serves two different aims: 1) as a mechanism of the empirical verification of the validity of an inference to be made based on the T Risk statistic in Eq. (10), and 2) as a mechanism for exploratory risk analysis.
 Jackknife, which is also known as the Quenouille-Tukey Jackknife or leave-one-out, was first introduced by Que-nouille [18] and then developed by Tukey [21]. Tukey used the Jackknife technique to determine how an estimate is af-fected by the subsets of observations when discordant values (i.e., outlier data) are present. In the presence of discorda nt values, it is expected that the Jackknife technique could re-duce the bias in the estimate. Although the original ob-jective of Jackknife is to detect outliers, in principle it is a re-sampling technique for estimating the bias and the stan-dard error of any estimate [11]. In Jackknife, the same test is repeated by leaving one subject out each time: this explains why this technique is also referred to as leave-one-out.
Let the random variables X 1 ; X 2 ; : : : ; X c denote a random sample of size c , such that X i is drawn identically and inde-pendently from a distribution F for i = 1 ; 2 ; : : : ; c . Suppose that the goal is to estimate an unknown parameter  X  of F . It can be shown that  X  can be estimated by a statistic  X   X  , which is derived from an observed sample x 1 ; x 2 : : : ; x F , with a measurable amount of sampling error [15].
An unbiased estimator  X   X  is a statistic whose expected value E (  X   X  ) is equal to the true value of the population pa-rameter of interest  X  , i.e., E (  X   X  ) =  X  . The amount of bias associated with an estimator is therefore given by: We denote as X ( i ) the sub-sample without the datum X i There are in total c sub-samples of size c  X  1 for i = 1 ; 2 ; : : : ; c : X
Next, let the estimate derived from the i th sub-sample X be denoted as  X   X  ( i ) , and the mean over c sub-samples be: The Jackknife estimate of bias , which is actually a nonpara-metric estimate of E (  X   X   X   X  ), is defined as follows [21]: and, in accordance, the bias-reduced Jackknife estimate of  X  is defined as  X   X  =  X   X   X  bias J (  X   X  ) = c  X   X   X  ( c  X  1)
Tukey [21] showed that the Jackknife technique can also be used to estimate the variance of  X   X  by introducing the so-called pseudo-values ,  X   X  ( i ) = c  X   X   X  ( c  X  1)  X  var J (  X   X  ) = 1
This nonparametric Jackknife estimate of variance gives the empirical estimate of the standard error of  X   X  : For the T Risk statistic in Eq. (10), the standard error of U Risk , SE (U Risk ), can hence be estimated by substituting U Risk into Eq. (16) as  X   X  :
The nonparametric estimator SE J is an alternative to the parametric estimator SE  X  x (Eq. (12)). In this section, we em-pirically compare these estimates of SE (U Risk ) with each other, to assess the validity of the result of a hypothesis test to be performed using T Risk as the test statistic. In general, if the two estimates agree, the test result may be said to be valid, and otherwise its validity will be compromised. As a result, nonparametric methods can help to alleviate doubts about the validity of the analysis performed [14].
In the following, we compare the estimates using the sub-mitted runs to the TREC Web track. In particular, the provided baseline run for the TREC 2013 Web track risk-sensitive task is based on the Indri retrieval platform. How-ever, as the submitted runs and results for the TREC 2013 campaign were not yet publicly available at the time of writ-ing, in the following we perform an empirical study based on runs submitted to the TREC 2012 Web track. Indeed, the 2013 track coordinators have made available a set of Indri runs on the TREC 2012 Web track topics 1 that correspond to the TREC 2013 baseline runs -in our results, we use the 2012 equivalent run to the 2013 pre-determined baseline, the so-called indriCASP . We report the U Risk values obtained using the official TREC 2012 evaluation measure, ERR@20.
Table 1 reports the parametric estimates ( SE  X  x ) and the nonparametric Jackknife estimates ( SE J ) of the standard er-rors associated with the average risk-reward tradeoff scores (U
Risk ), calculated for each of the TREC 2012 Web track top 8 ad-hoc runs over c = 50 topics, with respect to the in-driCASP baseline, applying several risk-sensitivity param-eter values of  X  = 0 ; 1 ; 5 ; 10. From the results, it can be observed that the two estimates, SE  X  x and SE J agree with each other for each of the 8 runs. In fact, over all of the 48 runs submitted to the TREC 2012 Web track, we observe a Root Mean Square Error (RMSE) of 0 : 000 between SE  X  x and SE J . Thus, we conclude that it is highly likely that it would be valid to conduct an inferential risk analysis upon those TREC 2012 runs based on the new risk-reward trade-off measure T Risk (Eq. (10)), regardless of how SE (U Risk is estimated. An example of inferential risk analysis based on T Risk follows in the next section.
The goal of the classical evaluation of IR systems is to de-cide whether one IR system is better in retrieval effectiveness than another on the population of topics. This goal can be formulated into a (two-sided) null hypothesis, as given by: against the alternative hypothesis H 1 :  X  r 6 =  X  b , where  X  and  X  b represent respectively the population mean perfor-mance of the system r and the population mean performance of the baseline system b . The test statistic for this null hy-pothesis is the t statistic (Eq. (7)), since the larger values of t are evidence against the null hypothesis H 0 :  X  r  X   X  b Below, we describe the hypothesis testing of H 0 in abstract terms, before explaining how it can be applied to T Risk (Sec-tion 4.1) and illustrating its application upon the TREC 2012 Web track runs (Section 4.2).

In order to decide how much difference between the two sample means  X  r and  X  b is assumed to be large enough to re-ject the null hypothesis, we should first determine how much difference can be attributed to a chance fluctuation . It can be shown that, under the null hypothesis H 0 , the sampling (or null) distribution of the test statistic t can be approxi-mated by a Student X  X  t distribution with df = c  X  1 degrees of freedom for any population distribution with finite mean  X  and variance  X  2 &gt; 0, because of the central limit theo-rem [12]. Thus, at a predefined significance level of  X  (typi-cally  X  = 0 : 05 for 95% confidence), two standard deviations (  X  t (  X = 2 ;df )  X  SE(  X  d )) determine the maximum difference that can be attributed to chance fluctuation, where in between the critical values  X  t (  X = 2 ;df ) the area under the Student X  X  t h ttps://github.com/trec-web/trec-web-2013 metric Jackknife estimates SE J of the associated standard errors SE (U d istribution sums up to (1  X   X  ). If an observed t -score is H 0 with 100%(1  X   X  ) confidence, denoted as the p-value.
The above protocol of hypothesis testing is referred to as the Student X  X  t test for matched pairs, or paired t test for short, in statistics. Hence, in the context of risk-sensitive evaluation, the T Gain formula in Eq. (9) stands for the test statistic t . In fact, at  X  = 0, testing the significance of an observed risk-reward tradeoff score between r and b (i.e. an observed U Gain score) is akin to testing the significance of the observed difference between  X  r and  X  b .

To test the significance of an observed U Gain score, one can therefore compare the corresponding T Gain score with the two-sided critical  X  t (  X = 2 ;df ) values at a desired level of served U Gain score can be attributed to chance fluctuation, meaning that the observed gain in the performance of the system r with respect to the baseline system b is not sta-tistically significant. In such a case, it is equally likely that the observed U Gain score may or may not occur on another topic sample drawn from the population. Other-however be sure that a U Gain score at least as extreme as the observed score would occur on 100(1  X   X  )% of the topic samples that could be drawn from the population.

Both T Gain and T Risk stem from the t statistic. Indeed, for  X  = 0, T Gain = T Risk , while for  X  &gt; 0, SE (U Risk shown to be valid in Section 3.3. Hence, we argue that an equivalent inferential analysis can be conducted upon the T
Risk scores that have been calculated based on U Risk . In the following, we provide an illustration of such inferential analysis upon runs submitted to the TREC 2012 Web track, but the same inferential analysis methodology could be ap-plied for any risk-sensitive evaluation scenario.
Given a particular IR system, a baseline system, and a set of c topics, one can use the paired t test for testing the signif-icance of the calculated average tradeoff score between risk and reward over the c topics, U Risk , by comparing the corre-sponding t -score, T Risk , with the critical values  X  t (  X = 2 ;df ) a desired level of significance  X  . To illustrate such an anal-ysis, Table 2 reports the U Risk risk-reward tradeoff scores based on ERR@20, and the corresponding T Risk scores for the 8 highest performing TREC 2012 ad-hoc runs, given the baseline run indriCASP (we omit other submitted runs for brevity, however the following analysis would be equally ap-plicable to them). As the TREC 2012 Web track has 50 topics, for a significance level of  X  = 0 : 05, the critical values
In Table 2, the U Risk scores to which a two-sided paired t test gives significance are those that have a corresponding T
Risk score less than  X  2 or greater than +2. For example, at  X  = 0, the calculated U Risk scores of the top 4 runs are significant with a p -value less than 0.05. This means that, under the null hypothesis H 0 :  X  r =  X  b , given another sample of 50 topics from the population, the probability of observing a risk-reward tradeoff score, between any one of these 4 runs and the baseline run indriCASP , that is as extreme or more extreme than the one that was observed is less than 0.05, i.e. the associated p -values. Since T Risk for those runs, the declared significance counts in favour of  X  X eward X  against  X  X isk X . Thus, one can conclude, with 95% confidence, that the expected per topic effectiveness of each of the top 4 runs is, on average, higher than the expected per topic effectiveness of the baseline run indriCASP on the population of topics. In other words, given a topic from the population, it is highly likely that any one of the top 4 runs will not perform worse for that topic than indriCASP . This suggests, as a result, that those top runs do not exhibit a real risk that is generalisable to the population of topics.
On the other hand, a run with T Risk &lt;  X  2 at  X  = 0 will be under a real risk, though among the shown top 8 TREC 2012 runs there is no such run. For those runs with  X  2  X  T Risk &lt; +2, such as utw2012fc1 and qutwb , the risk analysis performed here is inconclusive, since the associated U
Risk scores can be attributed to chance fluctuation, i.e. it is equally likely that they may or may not be under a real risk.
Next, we observe from Table 2 that as  X  increases, the observed tradeoffs between risk and reward for each run changes in favour of risk compared to reward, hence the runs exhibiting significant U Risk scores change. For exam-ple, each of the runs with significant U Risk scores at  X  = 0 (i.e., the top 4 runs) have a U Risk score that can be at-tributed to a chance fluctuation at  X  = 10, while, in con-trast, those runs whose U Risk scores can be attributed to chance fluctuation at  X  = 0 (i.e., the last 4 runs) have a significant U Risk score at  X  = 10.
 Figure 1 shows the change in the T Risk scores of the TREC 2012 top 8 ad-hoc runs for several risk-sensitivity  X  parameter values from 0 to 15. From the figure, we ob-serve that for  X  &gt; 5 the T Risk scores for all runs are negative in sign, and for the last 4 runs the calculated U Risk scores can be considered statistically significant (i.e., T Risk for  X  &gt; 5). It is also observed that, even for  X  = 15, the calculated U Risk scores of the top 4 TREC runs can still be attributed to chance fluctuation.

As a result, the inferential analysis performed so far sug-gests that, in general, none of the 8 top TREC 2012 ad-hoc where the baseline is indriCASP . The underlined U Risk scores are those for which a two-tailed paired t test gives significance with p &lt; 0 : 05 -i.e. exhibit a T Risk F igure 1: The change in standardised T Risk scores for the top TREC 2012 ad-hoc runs for 0  X   X   X  15 . runs are under a real risk of performing any given topic from t he population worse than the baseline run indriCASP , on average. In particular, there can be no significant reduction in risk that could be attained for the top 4 systems, given a baseline system with the average retrieval effectiveness of indriCASP . On the other hand, a significant reduction in risk could be attained, on average, for the last 4 systems, particularly for  X  &gt; 5.

Lastly, in Table 2, it is notable that the high U Risk scores do not necessarily imply high T Risk scores, because of the fact that each system would in general have a different in-herent variation in r i  X  b i across topics (i.e. SE (U Risk that of the other systems. For example, consider the runs uogTrA44xi and srchvrs12c09 . At  X  = 0, uogTrA44xi has a U
Risk score (0.1185) higher than the U Risk score (0.1102) of srchvrs12c09 , while srchvrs12c09 has a higher T Risk score than uogTrA44xi , i.e. 2.3034 vs. 2.2440. This shows that a ranking of retrieval systems obtained based on T Risk will not necessarily be concordant with the ranking of systems obtained based on U Risk .
In the previous section, the risk analysis that we per-formed could hide significant performance losses on individ-ual topics. Nevertheless, one can perform an exploratory risk analysis to determine those individual topics on which the observed risk-reward tradeoff score between a given IR system and the baseline system (i.e., x i ) is statistically sig-nificant. In the following, we provide a definition for ex-ploratory risk analysis (Section 5.1), which we later illustrate upon the TREC 2012 Web track runs (Section 5.2).
The T Risk measure permits the topic-by-topic analysis of risk-reward tradeoff measurements, which we refer to as ex-ploratory risk analysis. Such an analysis is implicitly sug-gested by the t statistic itself. The t statistic in Eq. (7) can be rewritten as follows: In here, each component of the sum t i = r i  X  b i s standardised score of the observed difference in effectiveness between the system r and the baseline system b on topic i , for i = 1 ; 2 ; : : : ; c .

In analogy, the T Risk measure, which stems from the t statistic, can be rewritten as: where each component of the sum, in this case, gives the standardised score of the individual topic risk-reward trade-off measurements x 1 ; x 2 ; : : : ; x c :
In a similar manner that we compare the calculated T Risk score of a given IR system with the two-sided critical values  X  t (  X = 2 ;df ) to decide whether the system exhibits a significant level of risk on average (Section 4), to decide whether an observed loss (or gain) on a particular topic i is significant, we can compare the component T R i score with the same critical values  X  t (  X = 2 ;df ) , at a desired significance level of  X  . can be attributed to chance fluctuation, and otherwise it can be considered statistically significant.

Indeed, this is one of the typical methods of outlier detec-tion in statistics [14]. Recall that the original objective of Jackknife is to detect outliers [21]. The T Risk measure can also be expressed in terms of the Jackknife estimate of bias, following Wu [29]: Here, each component of the sum: gives the standardised Jackknife estimate of bias in U Risk due to leaving the topic risk-reward score x i out of the sam-ple x 1 ; x 2 ; : : : ; x c , where  X  x = U Risk and  X  x score to be obtained when the i th topic is leaved out of the topic set in use, for i = 1 ; 2 ; : : : ; c .
 In general, both the T R i statistic in Eq. (21) and the T i statistic in Eq. (23) can be used for the purpose of ex-ploratory risk analysis. However, there is a certain difference between them in theory. Using T R i , we can decide whether an observed performance loss on topic i is significant, by comparing the topic risk-reward score x i with the maximum score that can be attributed to chance fluctuation, but as if the single datum x i is the whole sample. In contrast, using T i , we can make the same decision by comparing the ob-served difference between two U Risk scores,  X  x ( i )  X   X  x , with the maximum difference that can be attributed to chance fluctuation. Since we showed in Section 3.3 that the two estimates of the standard error for each TREC run are in perfect agreement (i.e. SE  X  x  X  SE J ), we argue that this the-oretical difference has no practical consequences. Hence, in the following, we provide an illustration of exploratory risk analysis on the TREC 2012 Web track runs, based on T J i alone. However, initial experiments showed no differences between T R i and T J i .
Figure 2 shows the standardised Jackknife estimate of bias in the U Risk scores calculated for two TREC runs, namely uogTrA44xi and qutwb at  X  = 0 ; 5 ; 10 ; 15 for the 50 TREC 2012 Web track topics, where indriCASP is the baseline. This standardised Jackknife estimate of bias, T is estimated by leaving one TREC 2012 Web track topic out of the set of topics { 151 ; 152 ; : : : ; 200 } in turn. In the figure, the topics that result in a significant performance loss (gain) for the corresponding systems with respect to indriCASP , at the significance level of  X  = 0 : 05, are those which have a T score less than  X  2 (greater than 2, respectively). Horizontal lines at  X  2 and +2 are shown to aid clarity.
 From Figure 2, at  X  = 0 it can be observed that uog-TrA44xi has more significant wins in number than qutwb , and less significant losses. This shows in detail why the declared significance for uogTrA44xi in Section 4 counts in favour of reward against risk, while the observed tradeoff between risk and reward can be attributed to chance fluctu-ation for qutwb , with respect to the baseline indriCASP .
In general, both of the runs uogTrA44xi and qutwb exhibit considerable performance losses with respect to indriCASP on the same topics, including 166, 172, 174, 175, and 191, out of which 2 are significant for uogTrA44xi (i.e., 166 and 175) and 4 are significant for qutwb (i.e., 166, 172, 175, and 191), at  X  = 0. In particular, consider the topic 166, on which the magnitude of the T J i score is nearly the same for both runs. It is notable here that, as  X  increases, the significance of that topic relatively doubles for uogTrA44xi , while for qutwb it nearly remains the same. The situation is also similar for topic 175, though the T J i score of uogTrA44xi at  X  = 0 is small in magnitude compared to that of qutwb .

This is one of the important differences between T Risk and U Risk in assessing the risk associated with IR systems. Given a particular topic i , the same amount of performance loss with respect to a provided baseline effectiveness can lead to different T J i (and T R i = x i =SE  X  x ) scores for different IR systems, depending on the variation in the observed risk-reward tradeoff across the topics (i.e., different SE  X  x ferent systems), while leading to the same topic risk-reward score, x i , for i = 1 ; 2 ; : : : ; c . As  X  increases, the topic risk-reward score x i increases proportionally for both of the runs uogTrA44xi and qutwb . However, the tradeoff counts, on av-erage, significantly in favour of reward against risk for uog-TrA44xi , whereas, it counts neither in favour of reward nor against risk for qutwb , as shown in Section 4. Thus, the same margin of increase in topic risk-reward tradeoff score x i favour of risk should lead to a relatively higher level of risk for uogTrA44xi than that for qutwb , in a way that T J i did. Assessing the level of risk that a topic commits for a given IR system relative to the level of risk associated with the sys-tem on average is a property unique to the measures T J i T
R i . Besides the use of these measures for exploratory risk analysis, this property also enables adaptive risk-sensitive optimisation within a learning to rank technique, as we ex-plain in the next section.
In this section, we describe how to exploit the new risk-reward tradeoff measure T Risk (Eq. (10)) in learning robust ranking models that maximises average retrieval effective-ness while minimising risk-reward ratio, in the context of the state-of-the-art LambdaMART learning to rank tech-nique [30]. As discussed below, Wang et al. [26] proposed to integrate U Risk (Eq.(4)) within LambdaMART to achieve risk sensitive optimisation , by using  X  to penalise risk dur-ing the learning process. However, U Risk considers top-ics equally regardless of the level of risk they commit. In contrast, we propose to adaptively change the level of risk-sensitivity, so that the total risk-sensitivity is distributed across the topics proportionally to the level of risk each topic commits. In the following: Section 6.1 provides an overview of the LambdaMART objective function, while Section 6.3 describes the integration of U Risk within LambdaMART; Section 6.3 explains our proposed adaptive risk-sensitive op-timisation approaches, with the experimental setup &amp; re-sults following in Sections 6.4 &amp; 6.5, respectively.
LambdaMART [30] is a state-of-the-art learning to rank technique, which won the 2011 Yahoo! learning to rank chal-lenge. It can be described as a tree-based technique, in that its resulting learned model takes the form of an ensemble of regression trees, which is used to predict the score of each document given the document X  X  feature values. Dur-ing learning, LambdaMART creates a sequence of gradient boosted regression trees that improve an effectiveness met-ric. In general, for our purposes 2 , it is sufficient to state that LambdaMART X  X  objective function is based upon the product of two components: (i) the derivative of a cross-entropy that originates from the RankNet learning to rank technique [3] calculated between the scores of two documents a and b , and (ii) the absolute change  X  M in an evaluation measure M due to the swapping of documents a and b [4]. Therefore the final gradient  X  new a of a document a within the objective function is obtained over all pairs of documents that a participates in for query q : where  X  ab is RankNet X  X  cross-entropy derivative, and  X  M is the change in an evaluation measure M by swapping doc-uments a and b . Various IR evaluation measures are suitable for use as M , including NDCG and MAP, as they have been shown to satisfy a consistency property [4]: for a pair of documents a and b where a is ranked higher than b , if the relevance label of a is higher than b , then a X  X egrading X  X wap of a and b must result in a decrease in M (i.e.  X  M  X  0), and orthogonally  X  M  X  0 for  X  X mproving X  swaps.
F urther details on LambdaMART can be found in [4, 26]. and qutwb at  X  = 0 ; 5 ; 10 ; 15 , where indriCASP is the baseline.
W ang et al. [26] demonstrated that a more robust learned model could be obtained from LambdaMART if the  X  M is replaced by the difference in U Risk for a given swap of two documents, denoted  X  T . In doing so, their implementation weights the value of  X  M by  X  + 1 only for the topics with down-side risk, while for the topics with up-side risk it leaves  X  M as is,  X  T =  X  M .  X  T was shown to exhibit the consis-tency property iff the underlying evaluation measure  X  M is consistent (e.g. as obtained from NDCG).
Compared to U Risk , T Risk is grounded in the theory of hypothesis testing and produces values that are easily inter-pretable  X  as shown in Section 4. However, as a linear trans-formation of U Risk , the direct application of T Risk as  X  T within LambdaMART to attain risk-sensitive optimisation cannot offer marked improvements on the resulting learned models. On the other hand, the exploratory risk analysis of Section 5 offers a promising direction, as it permits the learn-ing to rank process to adaptively focus on topics depending upon the level of risk that they commit. In this section, we propose two new models of adaptive risk-sensitive optimisa-tion that exploit the standardised topic risk-reward tradeoff scores (T R i , Eq. (21)), but which differ on which individual topics they operate on. In particular, the first model, S emi-A daptive R isk-sensitive O ptimisation (SARO), focuses only o n the topics with down-side risk and augments only the corresponding  X  M values. In contrast, the F ully A daptive R isk-sensitive O ptimisation (FARO) model operates on all t opics and augments every  X  M value. Hence, compared to U Risk as used in [26], FARO and SARO both alter the importance of riskier topics within the learning process.
In U Risk ,  X  M is multiplied by  X  + 1 if the topic commits a downside risk 3 . This amounts to a static level of sensitiv-ity for each topic, irrespective of the level of risk that the topic commits. In contrast, based on the standardised topic
T his follows directly from the definition of Eq. (4), however the consistency proof in Section 4.3.2 of [26] defines  X  T for different scenarios. risk-reward tradeoff scores, T R i (Eq. (21)), we propose to adaptively adjust  X  so that the total level of sensitivity can be distributed across the topics proportional to the levels of risk that they commit. In order to achieve this, for each topic we must estimate the probability of observing a risk-reward score greater than the actual observed T R i score. Technically speaking, we need to estimate the cumulative probability P r ( Z  X  T R i ), where T R i is the observed risk-reward tradeoff score and Z is the corresponding standard normal variable of T R i for all topics i = 1 ; 2 ; ::; c . For large sample sizes (generally agreed to be  X  30), the distribu-tion of the t statistic in Eq. (7) can be approximated by the standard normal probability distribution function, with zero mean and unit variance [15]. Thus, the probability P r ( Z  X  T R i ), which is the probability of a topic risk-reward score greater than T R i , can be estimated by the standard normal cumulative distribution function  X (  X  ), as follows: for i = 1 ; 2 ; : : : ; c .  X ( Z ) is a monotonically increasing func-tion of the standard normal random variable Z , where 0  X   X ( Z = z )  X  1 for  X  X  X  X  z  X  X  X  , and at Z = 0,  X ( Z ) = 0 : 5.
Hence, we can replace the original  X  in  X  T as  X   X  as follows: where 0  X   X   X   X   X  . As the level of risk T R i committed by topic i increases,  X   X  also increases. By substituting  X   X   X  T (as defined by Wang et al. [26]), this augments the  X  M values for every topic with a weight proportional to the level of risk that each topic commits.

The application of  X   X  differs between the SARO and FARO models. In particular, SARO only addresses the down-side risk, as in the case of U Risk . Indeed, under the null hy-pothesis H 0 :  X  r =  X  b , the higher the level of down-side risk (i.e. the larger the size of the difference r i  X  b i higher the probability of observing a topic risk-reward trade-off score greater than the observed score ( P r ( Z  X  T R Hence, SARO varies  X   X  from 0 to  X  , according to the down-side risk of each topic.

On the other hand, FARO operates on all topics. Indeed, for the topics with up-side risk, FARO gives lower weights to the topics that more strongly outperform the baseline s ystem (i.e. as the difference r i  X  b i &gt; 0 increases). At the extreme, if topic i exhibits maximal improvements over the baseline (i.e. r i  X  b i = 1), then  X (T R i ) = 1, and hence topic i has minimal emphasis on the learner. In other words, the learner focuses on improving the riskier topics. FARO operates on all topics, by redefining  X  T as follows: Moreover, for  X  = 0,  X   X  = 0, hence  X  T  X  =  X  M , i.e. the gain-only LambdaMART, as for U Risk .

Finally, we informally comment on the consistency of SARO and FARO: For both models, we calculate SE ( U Risk ) after the first iteration of boosting within LambdaMART, and not for each considered swap  X  we found this to be sufficient to obtain accurate estimates of SE ( U Risk ); Next, the con-sistency of SARO follows from U Risk , as our replacement of  X  with  X   X  , as 0  X   X   X   X   X  . For FARO,  X  T  X  only changes sign with  X  M , again as 0  X   X   X   X   X  . Hence, as long as  X  M is consistent, both SARO and FARO are also consistent.
We implement the U Risk , SARO and FARO models within the Jforests implementation [13] of LambdaMART 4 . Experi-ments are conducted using the large MSLR-Web10k learning to rank dataset 5 , as used by Wang et al. [26]. This dataset encompasses 9,685 queries with labelled documents obtained from a commercial web search engine. For each ranked doc-ument for each query, a range of 136 typical query-inde-pendent, query-dependent and query features are provided.
We use identical hyper-parameters for LambdaMART to those described by Wang et al. [26], namely: the minimum number of documents in each leaf m = 500 ; 1000, the num-ber of leaves l = 50, the number of trees in the ensemble nt = 800 and the learning rate r = 0 : 075. The best m value is chosen for each of the five folds using the valida-tion topic set, based on the NDCG@10 performance of the original LambdaMART algorithm, and used for all experi-ments for that fold thereafter. For the calculation of risk measures, like [26], we use the ranking obtained from the BM25.whole.document feature as the baseline system. The NDCG@10 performance of this baseline is 0.309.
 The performances obtained for LambdaMART upon the MSLR-Web10k in terms of NDCG@1 and NDCG@10 are similar in magnitude to those reported by Wang et al. [26], however we note some differences in the risk profile. Such differences are expected given the different implementations: Wang et al. [26] used a private implementation of Lamb-daMART, while we use and adapt an open source machine learning toolkit for U Risk , SARO and FARO. Nevertheless, the reported results allow valid conclusions to be drawn, in-cluding identical conclusions to [26] on the impact of using U
Risk within LambdaMART.
Table 3 reports the effectiveness and robustness results for FARO and SARO along with U Risk , for  X  = 1 ; 5 ; 10 ; 20 In the table, the gain over the baseline effectiveness is ex-
A ll of our code has been integrated to Jforests, available at https://code.google.com/p/jforests/ http://research.microsoft.com/en-us/projects/mslr/  X  =0 is equivalent to the normal LambdaMART algorithm.
Table 3: Results for SARO, FARO and U Risk . p ressed as the risk (Eq. (1)) to reward (Eq. (2)) ratio (i.e., the  X  X isk/Reward X  rows). Similarly, the number of topics that the risk-sensitive optimisation contributed to reward against risk is expressed as the loss to win ratio (i.e., the  X  X oss/Win X  rows). Raw numbers of losses and wins asso-ciated with each  X  value for each model are also shown. Finally the  X  X oss &gt; 20% X  rows show, for each model, the number of topics on which the relative loss in performance over the BM25 baseline was higher than 20% 7 .

As expected, since the semi-adaptive risk-sensitive opti-misation (SARO) and the risk-sensitive optimisation based on U Risk focus on only those topics with down-side risk, there is a steady decrease in average retrieval effectiveness (i.e., NDCG@1 and NDCG@10), as the risk-sensitivity pa-rameter value of  X  increases. Nevertheless, SARO results in a decrease in average retrieval effectiveness that is less than U Risk , for all  X  values. In contrast, the fully adaptive risk-sensitive optimisation (FARO) maintains the average retrieval effectiveness nearly constant across all  X  values, as well as the values of the quality and robustness measures, namely the risk-reward ratio and the loss-win ratio.
For SARO, the observed values of the two quality and ro-bustness metrics (risk-reward ratio and loss-win ratio) are better than for U Risk across the  X  values. For the met-ric  X  X oss &gt; 20% X , they are comparable between SARO and U Risk , given a topic sample as large as 9685 in size.
Next, for FARO, the observed values of the two quality and robustness metrics are comparable with that of the risk-sensitive optimisation based on U Risk across  X  values, and for the metric,  X  X oss &gt; 20% X  the observed values for FARO are slightly worse than that of both U Risk and SARO.
To summarise, the empirical evidence in Table 3 suggest that (i) FARO is best suited for retrieval tasks that are not tolerant to any loss in average effectiveness but also require robustness in effectiveness across the topics, and (ii) SARO suits retrieval tasks that require primarily robustness but are tolerant to some loss in the achievable average effectiveness.
S imilar measures are reported in [26]. With 9685 topics, all NDCG differences are statistically significant.
T o the best of our knowledge, this paper is the first work examining risk-sensitive evaluation from the perspective of statistical inference. Indeed, while there has been some in-vestigation into measures of robustness in the literature, such as Geometric-Mean Average Precision [24], developed within the context of the TREC 2004 Robust track, this pa-per advances upon the U Risk measure, first proposed in [26] in 2012. The T Risk measure is the test statistic counterpart of U Risk , which enables hypothesis testing on the level of risk associated with a given IR system. As a result, it facilitates adaptive risk-sensitive optimisation within learning to rank.
Outside of risk-sensitive evaluation, statistical hypothesis testing has a long history within IR. Van Rijsbergen [22] noted that  X  X here are no known statistical tests applicable to IR X  . However, later, Hull [32] recommended various hypoth-esis tests for the evaluation of retrieval experiments, includ-ing the Student X  X  t test for matched pairs. Zobel [31] was the first to apply re-sampling techniques in IR, by using a leave-one-out technique for assessing the effect of pooling on the effectiveness measurements and the significance of hypothe-sis tests, including the paired t test and the Wilcoxon signed rank test. Later, Smucker et al. [19, 20] and also Urbano et al. [27] investigated nonparametric re-sampling techniques, such as the bootstrapping and permutation tests, for the purposes of the evaluation of retrieval experiments.
Finally, much work in developing effective learning to rank techniques has occurred in the last few years, as reviewed by Liu [16]. Macdonald et al. [17] examined how the choice of evaluation measure encoded within their loss functions impacted upon the effectiveness of various learning to rank techniques. In particular, it is notable that the AdaRank technique [16, Ch. 4] focuses on hard queries using boost-ing. Taking a different approach, Wang et al. [26] proposed a risk-sensitive optimisation for the state-of-the-art Lamb-daMART technique, based on their U Risk measure. We fur-ther extend U Risk to the new T Risk measure within this paper, which is both theoretically founded, and results in more effective and less risky learning to rank.
This paper proposed the new T Risk measure for risk-sensitive evaluation, which is theoretically grounded within hypothe-sis testing. It easily allows inferential hypothesis testing of risk, as well as the exploratory identification of topics that commit significant levels of risk. In particular, we showed how T Risk could be integrated within the state-of-the-art LambdaMART learning to rank technique, to permit effec-tive yet risk-averse retrieval. Indeed, compared to the exist-ing U Risk measure, we attain higher effectiveness with com-parable or better risk/reward tradeoffs. For future work, we believe that there is a huge scope to build further effective and risk-averse adaptations for learning to rank upon T Risk other than SARO and FARO, and beyond LambdaMART.
