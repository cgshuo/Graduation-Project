 Support Vector Machines (SVMs) have been very successful in text data has been ignored by standard kernels commonly used in multinomial manifold, which is the simplex of multinomial models furnished with the Riemannian structure induced by the Distance (NGD) on the mu ltinomial manifold is conditionally Experiments show the NGD kernel on the mu ltinomial manifold standard kernels on the ambient Euclidean space. H.3.1. [ Content Analysis and Indexing ]; H.3.3 [ Information Search and Retrieval ]; I.2.6 [ Artificial Intelligence ]: Learning; I.5.2 [ Pattern Recognition ]: Design Methodology  X  classifier design and evaluation .
 Algorithms, Experimentation, Theory. Text Classification, Machine Learning, Support Vector Machine, Kernels, Manifolds, Differential Geometry. Machine (SVM) as one of the most powerful and promising  X  X he crucial ingredient of SVMs and other kernel methods is the so-called kernel trick, which permits the computation of dot functions defined on pairs of input patterns. This trick allows the prominent example. X  [32] However, standard kernels commonly used in SVMs have document feature vectors as points in a Riemannian manifold, rather than in the much larger Euclidean space. This paper studies kernels on the multinomial manifold that enable SVMs to effectively exploit the intrinsic geometric structure of text data to improve text classification accuracy. In the rest of this paper, we first examine the multinomial demonstrate its effectiveness ( X 4), later review related works ( X 5), finally make concluding remarks ( X 6). how documents can be naturally embedded in it. Let { } (| ) p  X  x  X  is C  X  at each point in the interior of  X  . Let denote information metric [1, 19, 21] at  X  X   X  is defined in terms of the matrix given by or equivalently as Note that ()  X   X  , giving S the structure of an n -dimensional Riemannian manifold. information a single data point supplies with respect to the problem of estimating the parameter  X  . The choice of the Fisher theory and good performances in practice [21, 23, 24]. independent events =  X  should be on the n -simplex defined by The probability that times is given by where 1 multinomial distribution equipped with the Fisher information metric, which can be shown to be isometric to the positive portion of the n -sphere of radius 2 [18] Therefore the geodesic distance between , n  X   X  () F  X  the geodesic distance between , n  X   X   X  X  P is given by independent draws from a multinomial distribution vocabulary { } modeled by a multinomial distribution, which may change from document to document. Given the feature representation of a document, d tf w D = which means the term frequency (TF) of word in document D , i.e., how many times embedding that corresponds to the TF representation is the multinomial distribution [15, 21, 24]. The popular TF X IDF representation [2] of a document D sets d tf w D idf w = X  , where the TF component (,) weighted by () word and word i df m df w = . The embedding that corresponds to the TF X IDF representation can be interpreted as a pullback metric of the Fisher information through the transformation with How to find the optimal embedding is an interesting research even better than using the TF X IDF weighting [23]. later) between two documents d and  X  d means ( )  X  X  wise similarity/dissimilarity in the domain. The motivation of Euclidean space [2]. The geometric interpretation of the dot vectors provided they are normalized to unit length. When no longer available on general manifolds, because the concept of dissimilarity measure on general manifolds: geodesic distance . discusses the Negative Euclidean Distance kernel ( X 3.2) and the Negative Geodesic Distance kernel ( X 3.3) in detail. valued symmetric function : k  X  X  XX is called a positive definite (pd) kernel if for all m  X  and all induced mm  X  Gram (kernel) matrix K with elements kernel if K satisfies the above inequa lity for any vector m  X  c with T = c1 0 .
 As a direct consequence of Definition 1, we have Lemma 1 ([32]). (i) Any pd kernel is also a cpd kernel. cpd kernel. (iii) If kk  X  X  + is a pd (resp. cpd) kernel. (iv) If Lemma 2 (Connection of PD and CPD Kernels [4, 29, 32]). Let k be real-valued symmetric function defined on  X  Then we have and only if k is cpd; (ii) exp( ) t k is pd for all 0 t &gt; if and only if k is cpd. Theorem 1 (Hilbert Space Representation of PD Kernels [32]). Let k be a real-valued pd kernel on X . Then there exists :  X  X  X H such that Theorem 2 (Hilbert Space Representation of CPD Kernels [32]). Let k be a real-valued cpd kernel on X exists a Hilbert space of real-valued functions on mapping :  X  X  XH such that 32].
 Standard (commonly used) kernels [32] include: Linear ( ,) , Polynomial () (, ) , d Gaussian Sigmoid ( ) (, ) tanh , The former three are pd but the last one is not. Lemma 3 ([31, 32]). The negative squared Euclidean distance Lemma 4 (Fractional Powers and Logs of CPD Kernels [4, 01  X  &lt;&lt; and ln(1 ) k  X  X  X  .
 Proposition 1. The Negative Euclidean Distance (NED) function is a cpd kernel. Proof. It follows directly from Lemma 3 and 4 with 12  X  = . Theorem 3 (Dot Product Kernels in Finite Dimensions [30, a finite n dimensional Hilbert space is a pd kernel if and only if its Legendre polynomial expansion has only nonnegative coefficients, i.e., Theorem 4 (Dot Product Kernels in Infinite Dimensions [30, an infinite dimensional Hilbert space is a pd kernel if and only if its Taylor series expansion has only nonnegative coe fficients, i.e., Since (19) is more stringent than (18), in order to prove positive definiteness for arbitrary dimensional dot product kernels it suffices to show that condition (19) holds [32]. Proposition 2. The Negative Geodesic Distance (NGD) function on the multinomial manifold k  X   X  +  X  X  is a pd kernel.
 Proof. Plugging the formula (8) into (20), we have Denote respectively. It is obvious that  X  and  X   X  are both on the unit sphere. Let Then we can rewrite (, ) about 0) for the inverse cosine function with 11 t  X  X  X   X  is where () x  X  is the gamma function. Hence and Since () 0 x  X &gt; for all 0 x &gt; , we have 0 By Theorem 3 and 4, the dot product kernel (, ) f  X   X  X  is pd. Thus the NGD kernel (, ) The shifted NGD kernel (, ) which has been proved to be pd. Definition 2 (Support Vector Machine, Dual Form [32]) Given a set of m labeled examples (SVM) is optimization problem: subject to and yyk  X   X   X  xx over all training examples with * 0 Proposition 3. Let k be a valid kernel, and kk  X  =+ where  X   X  is a constant. Then k and k lead to the identical SVM , given the same training data. Proof. Denote the SVMs with kernel k and k learned from the training data optimization problem for training () S VM k is ( )(, ) Wyyk  X  X  X  yyk yyk  X  X   X  Furthermore, the decision function of () S VM k is () sgn (, ) m fykb  X  functions for classification are identical. Proposition 2 and Proposition 3 reveal that although the NGD kernel (, ) as a pd kernel (, ) the NGD kernel. pd kernels can be constructed based on the geodesic distance on the multinomial manifold using Lemma 1, Lemma 2, Lemma 4, will be discussed later in the section of related works. Proposition 4. The kernel with 0 t &gt; for , n  X   X   X  X  P is pd. Proof. It is not hard to see that (, ) 4 exp ( , ) trivial consequence of Proposition 2, Lemma 1 and Lemma 2. We have conducted experiments on two real-world datasets, proposed NGD kernel for text classification using SVMs. The WebKB dataset contains manually classified Web pages that were collected from the computer science departments of four universities (Cornell, Texas, Washington and Wisconsin) and All pages were pre-processed using the Rainbow toolkit [25] training on three of the universities plus the misc. collection, and testing on the pages from a fourth, held-out university. This way of train/test split is recommended because each university X  X  pages have their idiosyncrasies. The performance measure is the multi-class classification accuracy averaged over these four splits. approximately 20,000 documents that were collected from 20 different newsgroups [22]. Each newsgroup corresponds to a pre-processed using the Rainbow toolkit [25] with the option  X  1 http://www.cs.cmu.edu/afs/cs.cmu.edu/projec t/theo-20/ www/ data/ the following considerations: duplicates and newsgroup-identifying headers have been removed; there is no randomness time is more realistic. The performance measure is the multi-class classification accuracy. LIBSVM [5] was employed as the implementation of SVM, with because of its effectiveness and efficiency [12]. We have tried standard kernels, the NED kernel and the NGD kernel. The linear (LIN) kernel worked better than or as well as other standard kernels (such as the Gaussian RBF kernel) in our experiments, which is consistent with previous observations that reported here. The text data represented as TF or TF X IDF vectors can be assume Euclidean geometry (including the LIN and NED kernel) often perform better with L 2 normalization, we report such when using it. NED and NGD kernels are shown in Table 1 and 2. superiority of the NGD kernel unaffected. should not be directly compared with most published results because of the difference in experimental settings and performance measures. representation normalization kernel accuracy The NED kernel worked comparably to the LIN kernel under L observation has not been reported before. The NGD kernel consistently outperformed its Euclidean All improvements made by the NGD kernel over the LIN or NED attributed to its ability to exploit the intrinsic geometric structure of text data. It is very attractive to design kernels that can combine the merits of generative modeling and discriminative learning. This paper lies along the line of research towards this direction. An influential work on this topic is the Fisher kernel proposed by Jaakkola and Haussler [13]. For any suitably regular probability model (|) p x  X  with parameters  X  , the Fisher kernel is based on brought by the NGD kernel are statistically significant. the Fisher score log ( | ) Up = X  parameter space: where [] T I EUU = the full geometry of statistical models. kernel proposed by Jebara et al. [14]. Let (| ) p  X  probability product kernel is defined as assuming that such that probability product kernel is called the Bhattacharyya kernel Bhattacharyya X  X  affinity between probability distributions. When applied to the multinomial manifold, the Bhattacharyya kernel of  X  X  P turns out to be which is closely related to the NGD kernel given by (21) through though they are proposed from different angles. The idea of assuming text data as points on the multinomial manifold for constructing new classification methods has been investigated by Lebanon and Lafferty [21, 24]. In particular, they heat equation on the Riemannian manifold defined by the Fisher information diffusion kernel can be approximated by It is identical to the pd kernel (, ) component is squared. Since 1 2 looks not cpd, the kernel (, ) Euclidean space, the NGD kernel generalizes the NED kernel and provides more insights on this issue. information diffusion kernel on the multinomial manifold, in the 
Table 3, Statistical significance tests about the differences between the NGD kernel (under L normalization) and the LIN/NEG kernel (under L normalization). 
NGD vs. LIN 
NGD vs. NED representation normalization kernel accuracy mysterious wisdom that preprocessing term frequencies by taking classification [14]. Although the NGD kernel is not restricted to the mu ltinomial manifold, it may be hard to have a closed-form formula to compute geodesic distances on manifolds with complex structure. One possibility to overcome this obstacle is to use manifold learning techniques [28, 33, 35]. For example, given a set of data Tenenbaum et al. [35] estimates the geodesic distance between a pair of points by the length of the shortest path connecting them metric, the distance between nearby points (distributions) of can be approximated in terms of the Kullback-Leibler divergence via the following relation. When  X   X  =+  X  X  X  with  X  the density X  X  Fisher information [7, 20] Another relevant line of research is to incorporate problem-specific distance measures into SVMs. One may simply represent examples, or embed the problem-specific distances in a sparsity, consequently they are not suitable for large-scale dataset. Another kind of approach directly uses kernels constructed based on the problem-specific distance, such as the Gaussian RBF kernel with the problem-specific distance measure plugged in [3, 6, 10, 11, 26]. Our proposed NGD kernel on the mu ltinomial theoretically justified (cpd) and practically effective. The main contribution of this paper is to prove that the Negative Geodesic Distance (NGD) on the mu ltinomial manifold is a conditionally positive definite (cpd) kernel, and it leads to geometry for text classification. Clustering, etc.) [34]. We thank the anonymous reviewers for their helpful comments. [1] Amari, S., Nagaoka, H. and Amari, S.-I. Methods of [2] Baeza-Yates, R. and Ribeiro-Neto, B. Modern Information [3] Bahlmann, C., Haasdonk, B. and Burkhardt, H. On-Line [4] Berg, C., Christensen, J.P.R. and Ressel, P. Harmonic [5] Chang, C.-C. and Lin, C.-J. LIBSVM: a Library for Support [6] Chapelle, O., Haffner, P. and Vapnik, V.N. SVMs for [7] Dabak, A.G. and Johnson, D.H. Relations between [8] Dumais, S., Platt, J., Heckerman, D. and Sahami, M. [9] Graepel, T., Herbrich, R., Bollmann-Sdorra, P. and [10] Haasdonk, B. and Bahlmann, C. Learning with Distance [11] Haasdonk, B. and Keysers, D. Tangent Distance Kernels for [12] Hsu, C.-W. and Lin, C.-J. A Comparison of Methods for [13] Jaakkola, T. and Haussler, D. Exploiting Generative Models [14] Jebara, T., Kondor, R. and Howard, A. Probability Product [15] Joachims, T. Learning to Classify Text using Support [16] Joachims, T. Text Categorization with Support Vector [17] Joachims, T., Cristianini, N. and Shawe-Taylor, J. [18] Kass, R.E. The Geometry of Asymptotic Inference. [19] Kass, R.E. and Vos, P.W. Geometrical Foundations of [20] Kullback, S. Information Theory and Statistics . Wiley, [21] Lafferty, J.D. and Lebanon, G. Information Diffusion [22] Lang, K. NewsWeeder: Learning to Filter Netnews. in [23] Lebanon, G. Learning Riemannian Metrics. in Proceedings [24] Lebanon, G. and Lafferty, J.D. Hyperplane Margin [25] McCallum, A.K. Bow: A Toolkit for Statistical Language [26] Moreno, P.J., Ho, P. and Vasconcelos, N. A Kullback-[27] Pekalska, E., Paclik, P. and Duin, R.P.W. A Generalized [28] Roweis, S.T. and Saul, L.K. Nonlinear Dimensionality [29] Schoenberg, I.J. Metric Spaces and Positive Definite [30] Schoenberg, I.J. Positive Definite Functions on Spheres. [31] Scholkopf, B. The Kernel Trick for Distances. in Advances [32] Scholkopf, B. and Smola, A.J. Learning with Kernels . MIT [33] Seung, H.S. and Lee, D.D. The Manifold Ways of [34] Shawe-Taylor, J. and Cristianini, N. Kernel Methods for [35] Tenenbaum, J.B., Silva, V.d. and Langford, J.C. A Global [36] Yang, Y. and Liu, X. A Re-examination of Text 
