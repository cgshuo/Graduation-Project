 NATTHAWUT KERTKEIDKACHORN, PROADPRAN PUNYABUKKANA, In tonal languages such as Mandarin, Cantonese, and Thai, tone information not only expresses prosody, as in any language, but also provides an implicit character of a word. For example the word /ka:/ in Thai has five different meanings, including  X  X  kind of grass, X   X  X alangal, X   X  X o kill, X   X  X o trade, X  and  X  X  leg. X  These five meanings directly correspond to five tones, consisting of the mid-tone, the low tone, the high tone, the rising tone, and the falling tone. Nevertheless, tone information typically is omitted. Generally, when constructing automatic speech recognition (ASR) systems, statistical machine learning X  X ased approaches with spectral-based features are employed. Even though proposed approaches are successful and provide reasonable results [Rabiner 1989; Glass 2003; Sung and Jurafsky 2009], the results are still not satisfying for some applications, for example, the spelling speech recognition application [Kertkeidkachorn et al. 2012a, 2014a]. Without considering tone information, performances of ASR sys-tems degrade significantly due to lexical ambiguity of words. In tonal languages, an ASR system generates candidate hypotheses of possible words by considering spectral cues in the speech signal; however, some candidate hypotheses do not have a unique meaning, such as the word /ka:/ in the example. Consequently, it leads to a lexical ambiguity problem. Tone classification, which provides tone information to differenti-ate such words, is necessary in order to improve performances of the ASR system, as reported by Wei [2008] and Nguyen [2008a]. However, in Thai, tone classification is still a challenging task. This article aims to report our empirical study on Thai tone classification.

Although there are many studies exploring a tone classification task in Thai [Thubthong et al. 2002; Thubthong and Kijsirikul 2002; Tan et al. 2004; Maleerat et al. 2009; Tungthangthum 1998], they still present drawbacks due to their machine-learning approaches for constructing statistical models.

Most rely on a nonsequential discriminative classifier approach, especially an Ar-tificial Neural Network (ANN) X  X ased approach, in which acoustic feature vectors for learning and optimizing statistical models are extracted independently from differ-ent time positions of speech signals. The drawback of this this approach is in regard to which individual time positions in speech signals are appropriate for constructing tone models. For example, acoustic feature vectors are generally represented by fun-damental frequency (F 0 ) values and their derivatives. According to numerous studies [Thubthong et al. 2002a, 2002b; Tan 2004; Maleerat et al. 2009], time positions for extracting F 0 values in speech segments are diverse. Therefore, the nonsequential dis-criminative classifier approach is difficult to use to select suitable time positions in speech signals for extracting acoustic feature vectors [Xing et al. 2010].
 On the other hand, there is a study that applied a sequential-based approach, the Hidden Markov Model (HMM) X  X ased approach, in order to avoid the selecting time po-sitions problem when extracting acoustic features in speech segment [Tungthangthum 1998]. The sequential-based approach can effectively support a sequence of feature vec-tors because it does not consider which individual time positions in speech signals are appropriate to extract acoustic features. Nevertheless, there is still a disadvantage for the HMM-based approach because it assumes the strong independence assumption as reported in Gunawardana et al. [2005]. The strong independence assumption assumes that the relation of each observation is independent and the current state of the model replied merely on a previous state. This assumption is made in order to reduce a com-plexity of an HMM-based model so that it can be trained and inferred more feasibly. In fact, speech signals are continuous sequences and there are overlapping regions be-tween speech segments when acoustic feature vectors are extracted. Consequently, the strong independence assumption was wrong [Gunawardana et al. 2005]. As a result, an HMM-based system would not be proper for a tone classification task.

In our study [Kertkeidkachorn et al. 2014b], we adopted a Hidden Conditional Ran-dom Field (HCRF)-based approach for Thai tone classification. There are three ad-vantages for selecting the HCRF-based approach: (1) an HCRF-based classifier is a sequential model supporting sequential data properly; (2) the HCRF-based classifier does not assume the strong independence assumption since feature functions are used to capture relations among observations instead of defining the independence assump-tion, as reported by Gunawardana et al. [2005]; and (3) the HCRF-based classifier contains hidden states, which can capture an intrinsic substructure of acoustic cues that swiftly and thoroughly changes in each syllable. Even though our previous work showed reasonable results, there still is room for improvement.

In Thai, the studies of Thai tone classification [Thubthong et al. 2002; Thubthong and Kijsirikul 2002; Tan et al. 2004; Maleerat et al. 2009; Tungthangthum 1998], in-cluding our previous work [Kertkeidkachorn et al. 2014b], mostly focused on acoustic cues limited to F 0 values and their derivatives in the acoustic feature extraction pro-cess. Nonetheless, other acoustic cues could also contribute to Thai tone classification. Preliminary experiments in our study [Kertkeidkachorn et al. 2012b] show concrete evidence that there are other acoustic cues that can enhance Thai tone perception of Thai native speakers. The study indicates that the tone perception of Thai native speakers does not rely alone on F 0 contours. The experiments also show that spectral information within the first formant frequency plays a significant role in human lis-teners identifying Thai tones, while temporal changes in syllable energy represents improvement of capabilities for Thai native speakers only in the case of monosyllabic stimuli. The findings encourage further investigations into the roles of other acoustic cues that can be used to represent acoustic feature vectors. We then hypothesize that those cues, especially energy movements and spectral shapes, can also improve the per-formance of a Thai tone classification system. In this article, we therefore investigated other acoustic features that were contributed by Thai tone perceptual experiments, then proposed an HCRF-based Thai tone classification system with suitable acoustic feature sets in both an isolated word scenario and continuous speech scenario.
The rest of this article is organized as follows. First, we briefly describe the overview of Thai tone in Section 2, then present related works in Section 3. In Section 4, data preparation is introduced. In Section 5, the HCRF-based approach for Thai tone classi-fication is presented. In Section 6, analyses of acoustic features for Thai tone classifica-tion are reported. In Section 7, we describe the experiments and the results. Section 8 includes our discussion. We present our conclusions in Section 9. In Thai, the sound system consists of three sound units: (1) the consonantal sound unit, (2) the vowel sound unit, and (3) the tonal sound unit, which could be composed to be a syllabic unit, as illustrated in Figure 1. In Figure 1, the formula of the syllable structure in Thai is shown as follows: (C i (C i )V(:)(C f ))T, where C i represents an initial consonant, C f is a final consonant, V stands for a vowel, : is a length of vowel, and T represents a tone.

According to the linguistic definition, consonants in Thai consist of 21 noncluster consonants (C i ) and 12 cluster consonants (C i C i ). Those 33 consonants can appear at the initial consonant position, while only 9 possible consonants occur in the final conso-nant position. For vowels in Thai, two categories X  X onophthongs and diphthongs X  X re linguistically defined. The monopthongs are comprised of 9 different qualities, each in-cluding two different quantities of duration. The two quantity differences regarding vowels are referred to lax and tense. In the case of diphthongs, there are 6 vowels similar to monopthongs that also have two quantitative differences. However, there are only 3 differences in qualities.

Apart from the consonantal sound unit and the vowel sound unit, in tonal language such as Thai, there is still the tonal sound unit that plays a key role in characterizing meanings of words. As an example, the word /l-a-w  X  / in Thai has five different meanings, including  X  X oughly, X   X  X roup, X   X  X o tell, X   X  X table, X  and  X  X o sharpen. X  Those five meanings correspond to five Thai tones: the mid-tone, the low tone, the high tone, the rising tone, and the falling tone, respectively.

The five tones in Thai generally are linguistically defined by their F 0 movements and sound level. According to the study by Luksaneeyanawin [1998], tones in Thai could be roughly divided into two groups, consisting of the static tone group and the dynamic tone group. There are three tones: the mid-tone, the low tone, and the high tone included in the static-tone group. F 0 movements of tones in the static group are relatively stable, whereas F 0 movements of the dynamic-tone group consisting of the rest tones are relatively dynamic, as shown in Figures 2 and 3. Figure 2 shows the average of F 0 movements of five Thai tones in citation form normalized by duration of the syllable, collected from a male Thai native speaker, as reported in Kertkeidkachorn et al. [2012b], while Figure 3 also illustrated F 0 movements of the five tones in the continuous-speech scenario uttered by the same speaker. In the continuous-speech scenario, we can clearly notice that the shapes of the average of F 0 movements of each Thai tone are shortened because of influences of adjacent syllables, in particular, the preceding syllable and the succeeding syllable. The shortened deformation of F 0 movements leads to a difficult problem for tone discrimination in the continuous-speech scenario, because the shapes of F 0 movements of each Thai tone are almost similar to each other. Considering the differences between F 0 movements in the isolated-speech scenario and the continuous-speech scenario, we therefore intend to tackle both aspects of the Thai tone-classification problem. In this section, we review and discuss research works related to the tone classification task and tone recognition task in various tonal languages.

In Chinese, many works are proposed for tackling the tone classification task and tone recognition task. Lee et al. [1995] introduced a tone recognition system for Can-tonese, one of Chinese dialects, relying on three acoustic features: (1) the relative pitch level between the initial position of the syllable and the final position of the sylla-ble, (2) the temporal variation among syllables computed by the average change of F 0 values along syllables, and (3) the energy drop rate of the syllable. In their ex-periments, three tone acoustic features with the ANN-based approach were selected in order to construct a tone recognition system for identifying Cantonese tones in the isolated-syllable scenario. Their system yielded an accuracy of 89% in a speaker-dependent case, while obtaining an accuracy of 87.6% in the speaker-independent case. In Taiwanese, another Chinese dialect, Jian [1998] studied the relation between energy movements in a syllable and Taiwanese tones. Their investigation indicated that some Taiwanese tones, in particular, the short falling tone and the long falling tone, could not be differentiated by F 0 movements alone due to similarities in their F 0 movement pat-terns. Consequently, they explored energy movements in syllables to help identify these tones. They therefore proposed the Attack, Decay Sustain, and Release (ADSR) model obtained from characters of energy movements. Their experiment succeeded in signif-icantly improving the performance of tone classification in Taiwanese. For Mandarin, the official language of Chinese, Xu et al. [2006] represented tone acoustic features by using F 0 values at the center of each segment, which were acquired from dividing the considered tone utterance into small segments, and then applying an ANN-based approach to recognize Mandarin tones. Another study in Mandarin tone recognition was proposed by Tian et al. [2004]. They presented the fractionalized model of F 0 val-ues and their derivative for an HMM-based approach and a Gaussian Mixture Model (GMM) X  X ased approach in the Mandarin tone recognition task. Their results reported that a GMM-based approach with the fractionalized model estimated from four sub-sections of tone segment yielded the best result. They also introduced another finding: that some parts of F 0 movements are sufficient to identify Mandarin tones. Later, Dong and Li [2011] also investigated the setting for the Mandarin tone recognition task re-garding the algorithm for the F 0 value detection, the F 0 value extraction process, and classifiers in order to fine-tune the results of Mandarin tone recognition. Their results indicated that the autocorrelation algorithm gave the best results for the F 0 detection, while the z-score normalization technique is the most suitable for the F 0 extraction process, and the SVM with the RBF kernel had been reported as the best result for their experiment. Furthermore, Kristine and Hiu [2013] studied the contribution of the acoustic cues, called the phonation cues, to lexical tone perception in Cantonese. Their study presented concrete evidence showing that, apart from F 0 values and pitch, difference in voice quality affects tone perception of humans.

In Vietnamese, Nguyen et al. [2008b] proposed an HMM-based approach using F 0 values and their derivative and energy of speech signals for constructing the tone recognition system. They also investigated normalization techniques for reducing dif-ferences between speakers in their experiments. Their system achieved accuracy of 70.44% for gender independence, while they obtained an accuracy of over 72% in the case of gender dependence. Later, Nguyen et al. [2013] investigated how speaker inten-tion affects the realization of the glottalization of a tone 3 and a tone 6 in Vietnamese. Their finding indicated that significant ranges of intonation of interested tones could be observed in the realization of the glottalization.

In Burmese, one of tonal languages, James [2013] reported the contribution of per-ceptual cues to Burmese tones. Their results suggested that multiple aspects of acoustic cues are involved in Burmese tone perception. Their study also reported that one of the important acoustic cues was phonation type.

In White Hmong, another tonal language, Garellek et al. [2013] investigated the importance of phonation in White Hmong tone perception. The results of their tone perception experiment indicated that phonation plays a key role in identifying White Hmong tones.

In Thai, the HMM-based tone recognition system representing acoustic features by F 0 values was first introduced by Tungthangthum [1998]. As shown in Tungth-angthum X  X  experiment, the system achieved high accuracy at 91% since the experiment was conducted on the isolated word scenario with the speaker-dependence condition. In this study, the relation between vowels and tones in Thai was also investigated, and tonal sound and vowel sound were found to be independent; in consequence, it leads to Tungthangthum X  X  conclusion that a tone in one vowel can identify tone in another vowel. Later, Thubthong et al. [2002] continued investigation of the Thai tone recogni-tion task by studying not only the relation between a vowel and a tone but examining the relation between a consonant and a tone. They had reported in their study that F 0 values and F 0 movements between monophthongs and diphthongs are slightly differ-ent, while the relation between final consonants and tones is crucial due to effects of the consonant structure that directly influence F 0 contours. Moreover, they also proposed an ANN-based approach for Thai tone recognition together with three acoustic feature sets extracted from F 0 values and their derivative in the isolated-word scenario. Their experimental results indicated that the acoustic feature set obtained from F 0 values at the initial position and the final position of syllable segments and F 0 values changes with time; delta F 0 (dF 0 ) at 20%, 40%, 60%, and 80% of syllabic duration gave the best performance. Thubthong and Kijsirikul [2002] conducted experiments on the empirical study relating to a Thai tone-recognition task on a continuous-speech scenario. Three acoustic feature sets with the ANN-based approach also were observed, as in their previous work [Thubthong et al. 2002]. However, in their experiments in Thubthong and Kijsirikul [2002], the acoustic feature sets were not similar, especially selected positions of syllabic duration, since they changed positions for extracting F 0 values and their derivative to 0%, 25%, 50%, 75%, and 100% of syllabic duration. Moreover, in their experiments, they also considered the configuration concerning the frequency scales and the normalization techniques. The results had shown that the acoustic fea-ture set consisting of five F 0 values and five dF 0 values at the new positions with changing frequency scale to the ERB-rate scale and the Z-score normalization tech-nique outperformed other settings. After that, Tan et al. [2004] conducted experiments on the Thai tone classification task similar to the study by Thubthong and Kijsirikul [2002]. Most of the detail in their study was similar to the work of Thubthong and Kijsirikul [2002]. Nevertheless, the speech corpus used in the experiments was differ-ent. It led to different results, that Tan X  X  best setting was obtained from the acoustic feature set consisting of five F 0 values and five dF 0 , as in Thubthong X  X  investigation, while the frequency scale and the normalization technique of the best setting were the semitone scale and the mean normalization technique, respectively. Another of Tan X  X  finding was that the final consonant information could significantly help im-prove performances of tone classification. When they manually constructed tone mod-els relying on the final consonant information and then conducted a tone classification task, they found that the performance of their system increased from an accuracy of 72.21% to 77.13%. Their finding conformed the findings of Thubthong et al. [2002], that the final consonants held the critical information for identifying Thai tones. Maleerat et al. [2009] also proposed experiments on Thai tone classification in the isolated-word scenario. They selected a Multilayer Perceptron (MLP) X  X ased approach with similar acoustic feature sets as Thubthong et al. [2002]. Instead of experimenting on each acoustic feature set, they combined three acoustic feature sets proposed by Thubthong et al. [2002] into one acoustic feature set. Maleerat et al. reported that they achieved an accuracy of 91.4% for Thai tone recognition on the isolated-word scenario. Recently, we introduced the novel approach for Thai tone classification on both the isolated-word scenario and the continuous-speech scenario [Kertkeidkachorn et al. 2014b]. Instead of using conventional approaches, such as an ANN-based approach or an HMM-based approach, we selected the HCRF-based approach for Thai tone classification because it has three advantages that overcome other approaches: (1) the HCRF-based approach can support sequential data, especially time series of F 0 values and their derivative; (2) the HCRF-based approach does not have the strong independence assumption, as does the HMM-based approach; and (3) the HCRF-based approach can capture an in-trinsic substructure of acoustic features that dramatically and thoroughly changes in each syllable by their hidden states. Although our HCRF-based approach in our pre-vious work [Kertkeidkachorn et al. 2014b] outperforms other approaches, especially an ANN-based approach, which had been reported as the best approach for Thai tone classification, our acoustic features are limited to F 0 values and their derivatives.
Based on reviews mentioned earlier, we found that most approaches proposed a sta-tistical machine learning X  X ased approach with acoustic features represented by F 0 values and their derivatives. Although there is much implicit evidence showing that other acoustic cues would help to identify tones more correctly, most studies in Thai generally omitted those cues and considered only F 0 values and their derivatives. Ac-cording to studies in other tonal languages, Cantonese [Lee et al. 1995], Taiwanese [Jian 1998] and Vietnamese [Nguyen et al. 2008b], they evidently showed that energy movements of a syllable can also be incorporated into acoustic feature vectors used for identifying tones. However, in Thai, the energy of a syllable still does not take into consideration constructing acoustic features. As a result, the performance of the Thai tone recognition system had limitations. In addition, studies [Thubthong et al. 2002; Tan et. al. 2004] reported that the final consonant information carried useful information for identifying Thai tones. Even though final consonant information can improve performance of Thai tone classification systems, their approaches still need prior knowledge of what final consonants were considered. Consequently, they cannot be used for practical situations. In our preliminary experiments in Thai tone perception study [Kertkeidkachorn et al. 2012b], we conducted the tone perception experiments on Thai native speakers in order to investigate acoustic cues that they used for iden-tifying Thai tones. Our experiments clearly showed that energy can help humans to classify Thai tone in the isolated-word scenario more correctly, while spectral informa-tion greatly contributes to Thai tone perception in both the isolated-word scenario and the continuous-speech scenario.

In this article, other acoustic cues apart from F 0 values and their derivative are investigated and integrated into the HCRF-based approach for Thai tone classification in order to improve classification performance. Acoustic cues investigated in this article are energy value and spectral information. Before the experiment, data preparation was set up in order to acquire datasets for the experiments. First, the speech corpora for conducting experiments were selected and the syllable segmentation process was performed in order to divide speech utterances into a syllable unit so that we could conduct tone classification on each syllable. After that, the feature extraction process for extracting F 0 values and their derivatives was executed. The setting for extracting F 0 values and their derivatives is considered in this section, while the setting for extracting other acoustic features is described in a later section. In this study, we aimed to conduct experiments on two speech scenarios: the isolated-word scenario and the continuous-speech scenario. Two speech corpora were taken into account in order to satisfy each acoustic condition. 4.1.1. Isolated Word Speech Corpus. For the isolated-word scenario, we prepared and constructed the speech corpus consisting of 22 Thai word forms uttered in five differ-ent ways according to Thai tones, as suggested in Kertkeidkachorn et al. [2012b]. To prepare the environment for the recording procedure, the recording room was set up and the Shure SM58 Unidirectional Dynamic Microphone was used to record all speech audio by setting the sampling rate at 44.1kHz with 16b PCM, and all files were saved in the MS wav format. Before recording, two criteria were assigned to be the standard for the recording audio. The first criterion was that Signal to Noise Ratio (SNR) of recorded audio had to be more than 30dB; the second criterion was that the maximum ampli-tude of audio was not more than the maximum setting. If the recorded audio had not satisfied the recording conditions, the speaker would have been asked to re-record until the recorded audio would follow the recording conditions. Twelve Thai native speakers, six males and six females, whose age ranged from 21 to 22 years old, were recruited to record audios under this setting condition. As a result, the isolated-word speech corpus was comprised of 1,320 samples, each tone containing 264 samples. Note that, during the recording process, the microphone was expected to be placed approximately 15cm from the speaker X  X  mouth.

After collecting the speech audio, we manually segmented and phonetically labeled the speech audio. The segmenting and labeling processes were done manually by us-ing an analysis speech tool, WaveSurfer [WaveSurfer 2012], to show speech signal in spectrogram formant; then, a speech specialist would consider the boundaries of words. 4.1.2. Continuous Syllable Speech Corpus. In Thai, there had already been a speech cor-pus for the continuous speech scenario called LOTUS [Kasuriya et al. 2003], which is the large vocabulary Thai continuous-speech recognition corpus. Since LOTUS has been widely used in many applications [Wutiwiwatchai and Furui 2007; Wutiwiwatchai et al. 2012], we selected LOTUS for our experiments. In LOTUS, speech utterances were recorded in two recording environment settings: the clean-speech environment and the office environment. In our experiments, the clean-speech environment was selected since the recording condition was close to our isolated-word speech corpus. In the clean-speech environment, SNR of recorded audios was set around 30dB and a high-quality headset (Sennheiser HMD-410 close-talk) was used to record speech audio. Although there are four sets of speech data X  X honetically distributed (PD) set, Training (TR) set, Development Test (DT) set, and Evaluation Test (ET) set X  X ncluded in LOTUS, only the PD set, whose purpose aimed to train acoustic models, was selected in our study.
 The segmenting and labeling processes have already been done by expert linguists. However, even though the transcription of speech data obtained from the segment-ing and labeling processes was highly correct, there were still some mistakes in some sentences. As a consequence, there is no transcription of tones since the expert lin-guists labeled phonemes relating only to consonants and vowels. We thus verified and corrected transcriptions of speech data again, then labeled tone symbols into transcrip-tions as well. After correcting and labeling in the PD set, we found that it contained 25,257 syllables consisting of 8,662 mid-tones, 5,277 low tones, 4,966 high tones, 4,202 falling tones, and 2,150 rising tones. Note that only the PD set recorded at the National Electronics and Computer Technology Center (NECTEC) had been chosen for the ex-periments because its quality of speech audio was acceptable and the PD set recorded at the NECTEC was the only set whose transcription was done carefully by expert linguists. The F 0 extraction process plays a significant role in developing tone classification or tone recognition systems. Consequently, in our experiments, F 0 values and their derivatives were carefully extracted from speech signals. In order to extract reliable F 0 values and their derivatives from speech signals, the well-known phonetic analysis tool, Praat [Boersma and Weenink 2011], was used because F 0 values and their derivatives extracted from Praat were highly accurate due to a modified autocorrelation method as reported by Boersma [1993]. Other studies [Dong and Li 2011; Nguyen et al. 2008b] also utilized Praat to extract F 0 values and their derivatives.

Nevertheless, there was a problem regarding how parameters for extracting F 0 val-ues and their derivatives should be configured. Consequently, we needed to consider the parameters for Praat. In doing literature reviews, we found that there are two studies that proposed Praat X  X  parameters. For the first study, Nguyen et al. [2008b] in-vestigated the function for extracting F 0 values and their derivatives. In their analysis, they reported that the  X  X itch (ac) X  function, which used the modified autocorrelation method [Boersma 1993], gave better results than other methods that Praat supported. The second study, by Dong and Li [2011], reports results that are also consistent with Nguyen X  X  finding. We therefore selected the  X  X itch (ac) X  function for extracting F 0 val-ues and their derivatives. Other parameters were also configured relying on the studies in Dong and Li [2011], Nguyen et al. [2008b], and Boersma [1993], as shown in Table I.
After extracting raw F 0 values and their derivatives, there were still influences of formants X  structure that directly affected those values and made some data extracted incorrect. Therefore, we aimed to smooth F 0 values and their derivatives in order to reduce the effect of the formants X  structure. The Smoothing Method was also provided by Praat X  X  function, which attempted to convolute the Gaussian curve with the raw F 0 values in order to decrease errors from some incorrect data. The parameter for range bandwidth of the Gaussian curve was set as the default value. In our work [Kertkeidkachorn et al. 2014b], we adopted the HCRF-based approach for Thai tone classification in the isolated-word scenario and the continuous-speech scenario. There were many configurations involved in the experiments, including tone feature sets, frequency scaling, and normalization techniques. Before continuing study of acoustic features for Thai tone classification, a brief review of our previous work is necessary in order to understand what we have done and how we plan to improve the performance of a Thai tone classification system. In this section, we therefore briefly describe our previous work, then present analyses of results so that we can utilize these analyses as the suggestions for investigating acoustic features that would improve the performance of Thai tone classification in the later section.

In our previous study, the HCRF-based approach is compared with two baseline approaches, an HMM-based approach and an ANN-based approach that had been reported as the best result for Thai tone classification. There are three configurations for each approach: tone feature set, frequency scaling, and normalization techniques.
There are five tone feature sets investigated in our experiment. The first tone feature set, PCR, is four coefficients of 3 rd -degree polynomial ( ax 3 + bx 2 + cx + d ) computed by fitting a tone contour of F 0 values through a considered syllable segment. The polynomial regression technique is used to calculate those four coefficients, a, b, c, and d. The second tone feature set, F2_dF5, is a set of F 0 values and dF 0 values at different time positions of the syllable segment. For the F 0 values, only an initial position and a final position of the syllable segment are taken into account, while dF 0 values at 0%, 25%, 50%, 75%, and 100% of the duration in the voiced part of the considered syllable acquired from fitting F 0 values in the syllable segment in 3 rd -degree polynomial are considered. The second feature set consists of two F 0 values and five dF 0 values. For the third set, it has been improved from the F2_dF5 by also considering F 0 values at 25%, 50%, and 75% of the duration in the voiced part of the syllable, and is given the name F5_dF5. Three tone feature sets are chosen for a nonsequential-based classifier, such as an ANN-based classifier. For our HCRF-based approach and the HMM-based approach, which is a sequential-based classifier, we need to consider the sequence of data for the input feature vectors. Therefore, the fourth and fifth tone feature sets for the sequence of data are defined. For the fourth set, the sequence of F 0 values and dF 0 values is used to represent tone features, while the fifth set determines not only the sequence of F 0 values and dF 0 values, but also considers acceleration of F 0 values (aF 0 ) for constructing the tone feature set. The summary of the tone feature sets is shown in Table II.

For frequency scaling, three frequency scales, consisting of Hertz (Hz) scale, Semitone scale, and Equivalent Rectangular Bandwidth rate (ERB-rate) scale, have been taken into consideration in the experiments, as suggested from earlier studies [Thubthong and Kijsirikul 2002; Tan et al. 2004].

The mean normalization technique and the z-score normalization technique are ap-plied to each tone feature set in order to reduce the dynamic range of F 0 values due to speakers X  variation, especially between male speakers and female speakers. The z-score normalization is computed by normalizing F 0 values of each speaker by subtracting F 0 values by their mean and then divided by the standard deviation of F 0 values; the mean normalization is calculated by considering the mean of F 0 values only. In our previous work, we reported that our proposed HCRF-based approach for Thai tone classification with the appropriate setting outperformed other approaches, especially the approach that had been reported as the best approach for Tone classifi-cation in Thai [Thubthong et al. 2002]. Results of the experiments for each scenario are presented in Table III and Table IV for the isolated-word scenario and the continuous-speech scenario, respectively.

The results in Table III and Table IV show that the HCRF-based approach with the tone feature F_dF_aF in ERB-rate frequency scale and normalizing by the z-score normalization technique outperforms other settings in both scenarios. Therefore, in this study, we initialized the study from the HCRF-based approach with such settings and assigned it as baseline . Before investigating acoustic features in the next section, analyses of the experimental results at baseline were conducted in order to determine how to enhance the performance of our approach in the next section.
 The confusion matrixes for the HCRF-based approach with the tone feature set, F_dF_aF, plus changing frequency scale to ERB-rate scale and normalizing by the z-score normalization technique, which is the best result of both scenarios, were de-termined as shown in Table V and Table VI for the isolated-word scenario and the continuous-speech scenario, respectively.

In Table V, it indicates that the low tone is frequently confused with the mid-tone and the rising tone. Due to the confusion between the low tone and mid-tone and the confusion between the low tone and rising tone, accuracy results of our HCRF-based approach for classifying the low tone in the isolated-word scenario have been affected. As a consequence, the accuracy result of the low-tone classification is significantly lower than other tones. This phenomenon also happens in the continuous-speech sce-nario. Comparing the accuracy results between the isolated-word scenario and the continuous-speech scenario in Table V and Table VI, we found that accuracy results of the high tone and rising tone in the continuous-speech scenario are smaller than accuracy results of other tones, while it does not happen in the isolated-word scenario. This situation occurs because the confusion between the mid-tone and high tone and the confusion between the low tone and rising tone increase due to the characteristic of the continuous-speech scenario, in which the context variation will directly affect tone contours of the syllable (see Section 8.3).

Therefore, to improve the performance of Thai tone classification in both scenarios, we intend to reduce the error caused by some confusion pairs that we introduced and increase the accuracy results of tones that had low accuracy results, as mentioned. Acoustical feature analyses were taken into consideration in order to explain the rea-sons why we selected energy values and spectral cues as extended features to im-prove the HCRF-based approach for Thai tone classification. Thai tone perception experiments were conducted [Kertkeidkachorn et al. 2012b]; then, based on Thai tone perception, the analyses in energy and spectrum of speech signal were conducted to identify necessities of both energy and spectral information for Thai tone classification. Although tones in tonal languages basically are defined according to the character-istics of fundamental frequency movements, there are many studies reporting that fundamental frequency movements alone do not lead to good tone identification. In the study by Jian [1998], an energy value of each speech frame was added to the acoustic feature vector for Taiwanese tone recognition; some significant improvements based on augmented acoustic features represented by the energy of each speech frame could be seen. Liu and Samuel [2004] studied a Mandarin tone perception experiment, re-porting that subjects who participated in the tone perception experiment were still able to distinguish tones in many cases, even though the fundamental frequencies were completely removed from the speech. Lv and Zhao [2010] and Chen and Zhao [2008] conducted an experiment on identifying tones in a whispered Mandarin Chi-nese scenario, in which the fundamental frequencies were absent at the beginning. Their results showed that formant structures played a key role in discriminating tones instead. These studies were our motivation to further investigate acoustic cues that potentially carried information about tones in Thai. We conducted a study on Thai tone perception by Thai native speakers in order to investigate evidence of possible additional acoustic cues (except for F 0 movements) that might contribute to tone dis-crimination [Kertkeidkachorn et al. 2012b].

Two experiments were conducted to reveal whether fundamental frequencies alone were sufficient for Thai native speakers to discriminate Thai tones, whether temporal changes in energy envelope might affect the perception of Thai tones, and whether spectral shapes during the interval of a syllable contribute to the tone perception of that syllable [Kertkeidkachorn et al. 2012b]. There were six stimulus sets prepared for experiments. Each set of stimuli consisted of three syllabic types: monosyllabic, bisyllabic, and trisyllabic. Each syllable of the stimuli sets was the /la:/ syllable, pro-nounced similarly to the English word  X  X a. X  For the first stimulus set, it was referred as the original stimuli set because their speech stimuli were produced by a Thai native speaker. In the second stimulus set, F 0 tracks were extracted from the speech signals in the original stimulus set, then were used for creating sinusoidal signals, of which F 0 values were dominated by F 0 tracks of the speech signals in the original stimulus set. Since the second set considered only fundamental frequency tracks, it was called the F 0 set. For the F 0 set, the amplitudes of the sinusoidal signals were set as a constant rate throughout each stimulus; in the third set, the amplitudes of the sinusoidal sig-nals were varied, relying on the envelope of their corresponding stimuli in the original stimulus set. Since stimuli in the third not only varied the F 0  X  X  contours of the original stimuli but also followed their original stimuli X  X  energy envelope, the third stimulus set therefore was given the name F 0 + E set. In the fourth stimulus set, Lo_F1, the orig-inal stimuli were passed through a low-pass filter with the cut-off frequency setting at the local minimum of the spectrum between the fundamental frequency and the first formant. The local minimum of the spectrum between the fundamental frequency and the first formant was estimated at half the average frequency of the first formants com-puted among all stimulus tokens. For the fifth and the sixth stimulus sets, the original speech signals were also low-pass filtered, but with two different cut-off frequencies. In the fifth stimulus set, the cut-off frequency was roughly set at the local minimum of the spectrum between the first and the second formant, while the cut-off frequency was approximately at the local minimum of the spectrum between the second and the third formant frequencies for the sixth stimulus set. Therefore, the fifth and sixth stimulus sets were named the Lo_F1/F2 set and the Lo_F2/F3 set, respectively. Note that, since only the syllable /la:/ was considered and it was uttered by only one speaker recorded in a single recording session, we assumed that the variations of the formant structures were small.

In the first experiment, 15 Thai native speakers, who did not have any hearing problems and passed qualifications in distinguishing the five Thai tones, were recruited to participate in a tone perception test. Three stimulus sets X  X ncluding the original stimulus set, the F 0 + Eset,andtheF 0 set X  X ere presented to the participants in random order. Then, the participants were asked to identify what tones they perceived. Results are reported in Table VII.

In the second experiment, 11 Thai native speakers, who did not have any hearing problems and also passed qualifications in distinguishing the five Thai tones, were invited to participate in a perception test in which the stimulus sets comprising the original stimulus set, the Lo_F1, set the Lo_F1/F2 set and the Lo_F2/F3 set. Results were presented in Table VIII. Note that, results of each experiment in Table VII and Table VIII were calculated from a number of correct responses from the participants in the experiment. Percentage of correctness indicated number of times a participant was able to identify a given stimulus.

In Table VII, the results of the tone perception experiment revealed that the F0 set and the F0 + E set presented a similar pattern; however, for the monosyllabic case, the correctness of perception significantly increased. These results motivated us to investigate the energy envelopes in the isolated-word scenario. In addition, we could observe a huge gap between the original stimulus set and the F0 set. This could be implied that there must be further acoustic cues that contribute to Thai tone perception. In Table VIII, we observe that the Lo_F1 set gives the lowest correctness. With this result, we imply that some acoustic cues that help to identify Thai tones are missing. This finding therefore motivated us to study in detail spectral information in both the isolated-word scenario and continuous-speech scenario for Thai tone classification. In the first experiment on Thai tone perception, we found that energy envelopes of speech signal enhanced Thai tone perception of Thai native speakers significantly in the monosyllabic case; in the bisyllabic and trisyllabic cases, there were no improve-ments. Furthermore, in other tonal languages, Vietnamese [Nguyen et al. 2008b] and Taiwanese [Jian 1998], energy values and their derivatives contributed to improving tone classification in Vietnamese and Taiwanese, respectively. Based on our finding and review lists, we assume that energy values and their derivatives can help to iden-tify Thai tones with greater precision. We therefore analyze energy values and their derivatives in order to clarify whether they are definitely appropriate for Thai tone classification. Energy values of speech signal in both the isolated-word scenario and the continuous-speech scenario and their derivatives are plotted here. Figure 4 shows energy contours of each tone, which was normalized by the z-score normalization tech-nique, whereas Figures 5 and 6 present contours X  derivatives of energy values, delta of energy values, and acceleration of energy values, respectively.

In Figure 4, we can observe that energy values probably help to identify Thai tones more correctly since levels of energy value contours have some characteristics that are helpful to classify tones, especially in the isolated-word scenario. Considering levels of energy value contours in the isolated-word scenario, we found that energy value contours could be separated into three groups according to similarities between levels of energy value contours. In the first group, levels of energy values are the highest level; thus, the first group, referred to the Highest Energy group, consists of three tones: the mid-tone, the falling tone, and the high tone. The second group is the Middle Energy group, consisting of the low tone. The third group, called the Lowest Energy group, comprising the rising tone, has the lowest level of energy values. As we have already mentioned in Section 5, the confusion matrix in Table V shows that the accuracy results of classifying the low tone in the isolated-word scenario is the lowest because the low tone is frequently confused with the mid-tone and rising tone. Thanks to characteristics X  levels of energy values in the isolated-word scenario, those three tones are separated into different groups. Consequently, we assume that energy values can reduce confusion among the low tone, mid-tone, and rising tone effectively as well as enhance the overall performance of the HCRF-based approach for Thai tone classification in the isolated-word scenario. In the same way, levels of energy value contours in the continuous-speech scenario have some characteristics that probably help to identify Thai tones; however, characteristics in the continuous-speech scenario are not prominent when compared with the isolated-word scenario. Considering levels of energy value contours in the continuous-speech scenario, we found that there are some slight differences among energy value levels of each tone. Therefore, we also assumed that energy values might improve the performance of the HCRF-based approach for Thai tone classification in the continuous-speech scenario as well.

Although the study by Nguyen et al. [2008b] showed the contribution of derivatives of energy values for Vietnamese tone classification, we can see from Figures 5 and 6 that there are no significant differences among energy value levels of tones in both scenarios. Based on the figures, we assume that adding a derivative of energy values into a tone feature set is not reasonable because a derivative of energy values cannot find any difference among tones. For our Thai tone classification in both scenarios, we therefore ignore derivatives of energy values and merely consider energy values of speech signals. Thubthong and Kijsirikul [2002] revealed that final consonants of a syllable hold critical information for identifying Thai tones. Tan et al. [2004] therefore tried to exploit infor-mation of final consonants in order to improve accuracies of Thai tone classification. In the experiment by Tan et al. [2004], tone data is categorized into many groups by types of final consonants corresponding to tones and classifiers, then built to classify Thai tones in each group independently. By separating tone data into tiny groups relying on final consonant information, they can successfully achieve higher accuracy results than accuracy results of tone data, which is not grouped by final consonant information. Nevertheless, the approach by Tan et al. [2004] still has limitations for classifying Thai tones because types of final consonants usually are unknown in a practical situation, thus separating tone data into small groups based on final consonant information is not feasible. The workaround is to investigate other acoustic information that highly relates to the type of final consonants and can represent consonant information effec-tively. Based on the fact that consonant information explicitly corresponds to spectral information, we therefore investigated spectral information instead. Furthermore, in the second experiment on Thai tone perception, we found that spectral information makes significant contributions to enhancing tone perception of Thai native speakers and contributions of spectral information become more prominent when syllables are uttered continuously. Based on this discussion, it is worthwhile to investigate spectral information for Thai tone classification.
 Analyses of consonantal information were conducted for assuring its contributions. In Figure 7, F 0 contours of each tone separated by final consonants are presented. Despite the same tone but different final consonants, some F 0 contours are slightly different. Considering Figures 7(a) and 7(c), we can observe that F 0 contours of the final consonant, l  X  , are different from other tones due to levels of F 0 values. In Figure 7(b), we also found that F 0 values X  levels of the final consonant z  X  are higher than F 0 values X  level of other final consonants and F 0 contours of final consonants, k  X  ,p  X  ,andt  X  have slightly different forms compared with F 0 contours of other final consonants. F 0 contours of the final consonant, ch  X  , are also slightly different from F 0 contours of other final consonants owing to the level of F 0 values and shape of F 0 contours, as shown in Figure 7(d). Due to variations of tones relying on final consonants, F 0 contours and F 0 values of each tone are widely varied. Consequently, we cannot easily differentiate tones by considering F 0 values alone. We show the critical case of tone variations of final consonants in Figure 8.

In Figure 8, we can see that the F 0 contour of the mid-tone, whose final consonant is w  X  ,andtheF shapes even though their tones are different. Based on our analyses, we presumed that F 0 values alone could not differentiate some variations of some tones effectively. We therefore introduced spectral-based features, which successfully captured consonant information in ASR systems, for Thai tone classification. In this section, we present an empirical study of Thai tone classification experiments and report their results. First, the experiment setup is described, then experiments are conducted to uncover the contribution of energy and spectral cues for Thai tone classification. The best setting of the HCRF-based approach for Thai tone classification X  X he HCRF-based approach with the tone feature set, F_dF_aF, with the ERB-rate scale and the z-score normalization technique, in our previous work [Kertkeidkachorn et al. 2014b] X  was a baseline for our experiments. Therefore, to fairly evaluate acoustic features, en-ergy of speech frame, and spectral information, the setup of the HCRF-based approach is similar to our previous study [Kertkeidkachorn et al. 2014b]. The HCRF Library [Morency et al. 2010] was selected to construct the HCRF-based tone models by the following configurations. The model of the HCRF Library was set as the Gaussian Hidden Conditional Random Field (GHCRF) toolbox implemented from Gunawardana et al. [2005] with three hidden states. Three hidden states were configured due to the characteristic of Thai tones, as shown in Figure 1 and Figure 2, where contours of Thai tones could be approximately divided into three parts: the early part, mid-part, and late part. In the early part, which lies in the start of the tone contours to the initial middle of tone contours, and in the late part, which stays in the final middle of the tone contours to the end of tone contours, F 0 movements rapidly change. During the middle period of tone contours, F 0 movements are stable or slightly fluctuated. Since our previous experiments do not consider the neighbor frame, the window size of the HCRF-based approach for the experiments was set at zero. An initial weight for the HCRF-based model was computed by the means and variances of the train-ing dataset, and the maximum weight and minimum weight were limited between 1.0 and -1.0, respectively. Eventually, the Quasi-Newton method X  X imited-memory Broyden X  X letcher X  X oldfarb X  X hanno (L-BFGS) with L2 cache X  X as configured as the optimization technique for training HCRF-based tone models.

Based on our analyses of energy values and their derivatives, we therefore con-structed another tone feature set by appending the tone feature set, F_dF_aF, with energy values. The tone feature set, F_dF_aF, which was appended by energy values, is referred to the tone feature set F_dF_aF_E. In order to obtain energy values for the tone feature set F_dF_aF_E, energy values were extracted from speech signals every 10ms by Praat with its default configuration. There is variation among interspeakers and intraspeakers as well. Hence, we normalize energy values of each utterance by the utterance X  X  mean and variance in order to reduce variations due to the speaker characteristic.

According to our discussion in spectral analyses, we planned to increase perfor-mances of the HCRF-based approach with the tone feature set, F_dF_aF, by utiliz-ing spectral information. In order to represent spectral information effectively, three spectral-based features, Linear Prediction Coefficients (LPC), Perceptual Linear Pre-dictive (PLP) and Mel-Frequency Spectral Coefficients (MFCC), were investigated. Generally, each spectral-based feature was widely chosen for representing acoustic characteristic of speech signals in order to construct ASR systems. In the experiment, spectral-based features were extracted by HTK [Young et al. 2009] with the follow-ing configurations. Three parameters of HTK for selecting spectral-based features are LPC_D_A_Z for the LPC-based feature, PLP_D_A_Z for the PLP-based feature, and MFCC_D_A_Z for the MFCC-based feature. The order for presenting the spectral-based feature is set at 12. The Hamming window size 25ms with 10ms time shifting in each frame was configured. After obtaining three spectral-based features X  X he LPC-based feature, the PLP-based feature, and the MFCC-based feature X  X hey were then appended into the tone feature set F_dF_aF one by one in order to construct other tone feature sets. Therefore, three tone feature sets X  X  X _dF_aF_LPC, X   X  X _dF_aF_PLP, X  and  X  X _dF_aF_MFCC X  X  X ere constructed relying on the LPC-based feature, the PLP-based feature, and the MFCC-based feature, respectively.

In both speech scenarios, k-fold cross-validation technique was applied in order to evaluate the accuracies of each tone feature set. For the isolated word scenario, the k value was set at six so that speech data in the isolated word scenario was randomly divided into six groups and each group comprised of one male speaker and one female speaker. In the 6-fold cross-validation, five groups were used to train and fine-tune tone models, and the evaluation was done on the other group, which was the test data group. The test data group was rotated until all groups were tested. In the continuous speech scenario, the k value was configured at four. The evaluation process is similar to the isolated-word scenario, but in the continuous-speech scenario, the speech data was randomly separated into four groups, each group consisting of six male speakers and six female speakers. In the experiment, the HCRF-based approach with five different tone feature sets X  F_dF_aF, F_dF_aF_E, F_dF_aF_LPC, F_dF_aF_PLP, and F_dF_aF_MFCC X  X ere com-pared. Table IX shows accuracies of Thai tone classification results in both speech scenarios.
 Considering results in Table IX, the tone feature set F_dF_aF_E provides better results than the tone feature set F_dF_aF in both scenarios, especially in the isolated-word scenario. In the isolated-word scenario, an error rate reduction of 22.40% is achieved; an error rate reduction of 1.66% is obtained in the continuous-speech scenario. The t-test is conducted to evaluate the statistical difference between the tone feature set F_dF_aF_E and the tone feature set F_dF_aF in both scenarios. The significance level is set at  X  = 0 . 05. The statistical testing indicates that there is a statistical difference between performances of the tone feature set F_dF_aF_E and those of the tone feature set F_dF_aF in the isolated-word scenario, while there is no significance in the continuous-speech scenario. We therefore conclude that energy values signifi-cantly increase performances of Thai tone classification in the isolated-word scenario, while showing small improvements in the continuous-speech scenario without signif-icance. This finding conforms to our work in tone perception of Thai native speakers [Kertkeidkachorn et al. 2012b], which reported that energy envelopes could numer-ously enhance tone perception of Thai native speakers in the monosyllabic case, while there are no improvements for the bisyllabic and trisyllabic cases, in which syllables are uttered continuously.

To analyze and discuss the effect of energy in detail, the confusion matrixes of the results in both scenarios are listed. Table X presents the confusion matrix of tone clas-sification relying on the HCRF-based approach with the tone feature set, F_dF_aF_E, in the isolated-word scenario. Table XI shows the confusion matrix of tone classification in the continuous-speech scenario.

Comparing confusion matrixes for the isolated-word scenario in Table X with Table V, we observe that the accuracy of the low tone is improved from an accuracy of 90.91% to 93.56% and the accuracy of the mid-tone also improved, from an accuracy of 93.18% to 94.70%. The accuracy of the rising tone improved from an accuracy of 91.29% to 93.94%. This complies with our hypothesis that energy values can reduce confusion among the low tone, the mid-tone, and the rising tone. The accuracy results of these three tones therefore are increased. Nevertheless, the accuracy result of the falling tone slightly decreased. Considering levels of energy values in Figure 4(a), we can see that the mid-tone, the falling tone, and the high tone are in the same group, the Highest Energy group. As a consequence, levels of energy values of those three tones are quite similar. It leads to a reason why accuracy results of the falling tone and high tone slightly dropped, because confusion between the falling tone and high tone increased. By the way, the overall accuracy result of tone classification in the isolated-word scenario still significantly improved when energy values were considered. Considering the confusion matrix of the continuous-speech scenario in Table XI and Table VI, we can see that improvements, when energy values are applied, are minor without significance. Although the finding is consistent with our assumption, that energy values probably help to identify Thai tones more correctly, the overall accuracy result is slightly increased, thus there is still a lot of room for improving Thai tone classification in the continuous-speech scenario. Analyzing accuracy results of tone feature sets, F_dF_aF_LPC, F_dF_aF_PLP and F_dF_aF_MFCC, we found that their results are higher than the accuracy result of the baseline tone feature set F_dF_aF and the tone feature set F_dF_aF_E in the continuous-speech scenario. The accuracy results of those feature sets in the isolated-word scenario were lower than the accuracy result of the baseline tone feature set F_dF_aF and the tone feature F_dF_aF_E.

In the isolated-word scenario, the experiment shows that the spectral-based feature could not provide much of a contribution to performance of Thai tone classification. Still, the best result of Thai tone classification was obtained from the tone feature set F_dF_aF_E. Giving spectral information to the tone feature set F_dF_aF, the accuracy performance of Thai tone classification became worse than the accuracy performance of the tone feature set F_dF_aF alone. The reason why performance of Thai tone classification got worse is the limitation of the data in the isolated-word speech corpus. Since there were approximately 10 samples for each consonant in the isolated-word speech corpus, the training data was very limited. As a result, the spectral-based feature could not represent a distribution of the acoustic spaces by a few data; that is why it caused poorer results on Thai tone classification.

In the continuous-speech scenario, comparing accuracy results between tone feature sets consisting of spectral-based features, we observe that the LPC-based tone fea-ture set slightly enhances performances of Thai tone classification from the baseline, while the PLP-based tone feature set and the MFCC-based tone feature set signifi-cantly improve performance. The PLP-based tone feature set also outperforms other spectral-based tone feature sets and yields an error-rate reduction of 13.90% when com-pared with the baseline. The error-rate reduction obtained from the tone feature set F_dF_aF_PLP numerously increases from an error-rate reduction of 1.66%, acquiring from the tone feature set F_dF_aF_E. The results from the experiment also conform to our previous study [Kertkeidkachorn et al. 2012b] and our assumption that spectral information plays a critical role in improving Thai tone classification in the continuous-speech scenario. In Table XII, the confusion matrix of the HCRF-based approach with the PLP-based feature in only the continuous-speech scenario is listed.

Comparing results in Table XII with results in Table VI and Table XI, which are also the confusion matrices of the HCRF-based approach in the continuous-speech scenario, we found that not only overall accuracy results substantially improve but accuracy results of each tone also improve, except in the low tone. The accuracy result of the tone feature set F_dF_aF_PLP slightly drops from the accuracy result of the tone feature set F_dF_aF in the case of the low tone but increases from the accuracy result of the tone feature F_dF_aF_E. Still, the experiment indicates interesting improvements of Thai tone classification results in the continuous-speech scenario based on the PLP-based tone feature. Analyzing the experimental results, the accuracy results between the isolated-word scenario and the continuous-speech scenario are obviously different. The important factor affecting accuracy results in both scenarios is an effect of contextual variation. An effect of contextual variation is caused by preceding and following tones. A preceding syllable and a following syllable directly influence the F 0 contour of the target syllable. Generally, the F 0 contour of the preceding syllable influences the F 0 contour of the target syllable in the beginning period, while the F 0 contour of the following syllable affects the F 0 contour of the target syllable in the ending period.

To show the influence of the contextual syllable, F 0 contours of target syllables with variations of F 0 contours of preceding syllables and following syllables in the continuous speech scenario are plotted in Figure 9. There are five target tones: the mid-tone, low tone, falling tone, high tone, and rising tone, abbreviated as M, L, F, H, and R, respectively. Each target tone in Figures 9(a) through 9(e) is plotted together with six possible preceding tones and following tones. The six possible tone patterns are the mid-tone, low tone, falling tone, high tone, and rising tone and no tone, occurred when there was no preceding syllable or following syllable. Therefore, there are 36 tone contours in each figure. Note that, in Figure 9, an asterisk (  X  ) could also represent any tone patterns.

Considering F 0 contours of target syllables in Figure 9, influences of F 0 contours of preceding syllables and following syllables on F 0 contours of the target syllables could be clearly seen. In our experiment, effects of contextual variation in continuous speech scenario caused by preceding syllables and following syllables are ignored since our study focus on Thai tone classification in case of considering on a single target segment. As a result, accuracy results between the isolated word scenario and the continuous speech scenario are obviously different.

According to tone contours in Figure 9, an effect of contextual variations from pre-ceding tones extremely affects the beginning period of the tone of the target syllable, while its effect gradually decreases over time of the syllable. In the case that the pre-ceding tones are the high tone and the rising tone, the F 0 contours lift up in the ending period of the preceding syllable. Consequently, F 0 contours in the beginning period of target tones X  X specially the static tones, mid-tone, low tone, and high tone X  X ave a higher level than other patterns due to the lifting-up effect of the preceding syllable. In case of the non-preceding syllable, F 0 contours of target tones in the beginning period are in high level since generally the nonpreceding syllable occurred at the beginning of sentences, at which capabilities of sound production are higher than other parts of sentences. F 0 contours of the target syllable with the nonpreceding syllable in starting period therefore have a high level. The contrary effect occurred in the case of the nonfol-lowing syllable. As shown in Figure 9, F 0 contour levels of target tones that are followed by the nonfollowing syllable are lower than other patterns in the ending period. Since such syllables usually happen at the end of sentences, capabilities of sound production are lower than other parts. Consequently, F 0 contour levels of target tones followed by the nonfollowing syllable are low at the ending period. In addition, we observe that F 0 contours of target tones followed by the falling tone lifts up at the ending period in order to support the shape of the falling tone of the following syllable. Consequently, F 0 contours of target tones at the ending period followed by the falling tone are higher than other patterns.
Based on the effect of contextual variation described earlier, Thai tone classification in the continuous-speech scenario could still be improved. This article introduces acoustic features apart from F 0 values and their derivatives in order to incorporate such acoustic features into tone features for Thai tone classi-fication. Energy values and spectral information have been investigated in the exper-iment. In our experiment, we found that energy values significantly help to improve performance of Thai tone classification in the isolated-word scenario but slightly in-crease performance in the continuous-speech scenario. For spectral information, three spectral-based features X  X he LPC-based feature, PLP-based feature, and MFCC-based feature X  X re chosen for representing spectral information. The results indicate that spectral information can actually improve Thai tone classification performance in the continuous-speech scenario, while spectral information in our experiment degrades performance of Thai tone classification in the isolated-word scenario. Furthermore, in our experiment, the highest result for Thai tone classification in the continuous-speech scenario was obtained from an HCRF-based approach with the tone feature set F_dF_aF_PLP, whereas the best result for Thai tone classification in the isolated-word scenario was obtained from an HCRF-based approach with the tone feature set F_dF_aF_E. As a result, the findings in this article are that energy could considerably provide information for identifying Thai tone more correctly in both the isolated-word scenario and the continuous-speech scenario, and spectral information greatly con-tributes to Thai tone classification in the case of the continuous-speech scenario.
For our future direction, in the isolated-word scenario, the result is satisfied. We therefore plan to apply the HCRF-based Thai tone classification to Thai ASR systems in the isolated-word scenario in order to enhance their performance for identifying Thai words that do not have unique meaning, such as the word /ka:/ in the example. In the continuous-speech scenario, we also plan to investigate in detail contextual variation and employ the syllabic context in order to improve performance of Thai tone classification in the continuous-speech scenario.

