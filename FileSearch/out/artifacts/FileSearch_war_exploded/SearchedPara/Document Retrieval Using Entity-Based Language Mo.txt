 We address the ad hoc document retrieval task by devising novel types of entity-based language models. The models utilize information about single terms in the query and doc-uments as well as term sequences marked as entities by some entity-linking tool. The key principle of the language models is accounting, simultaneously, for the uncertainty inherent in the entity-markup process and the balance between using entity-based and term-based information. Empirical evalu-ation demonstrates the merits of using the language models for retrieval. For example, the performance transcends that of a state-of-the-art term proximity method. We also show that the language models can be effectively used for cluster-based document retrieval and query expansion.

Most ad hoc document retrieval methods compare query and document representations. To address the potential vo-cabulary mismatch between a short query and documents relevant to the query, various semantic document-query sim-ilarity measures have been proposed [28].

Specifically, there is a growing body of work on retrieval methods that utilize information about entities in a repos-itory (e.g., Wikipedia or Freebase) which appear in queries and documents (e.g., [46, 35, 39, 7, 13, 31, 45, 29, 33, 44]). Most of these methods expand the query with terms or en-tities related to those appearing (or marked) in it [46, 35, 39, 7, 13, 31, 45, 29]; other methods project queries and documents onto a latent or explicit entity space [14, 33, 44].
In this paper we take a step back, and address a more fundamental challenge regarding the use of entity-based in-formation for document retrieval. We study whether using surface level entity-based query and document representa-tions can help to improve retrieval effectiveness. By  X  X ur-face level X  we refer to representations based only on terms in the text and markups of entities in it, along with raw corpus-based occurrence statistics. This is in contrast to expansion-based and projection-based representations that utilize also terms and entities related to those (marked) in the text and which often use auxiliary information about entities from the entity repository; e.g., textual descriptions of entities, entities X  categories and inter-entity relations [46, 35, 39, 7, 13, 31, 45, 29, 33, 44]. Put in simpler words, the question we address is whether the markup of entities in a query and documents is, by itself, sufficient information for improving retrieval effectiveness .

The reason for addressing the question just posed is two fold. First, it will shed light on the effectiveness of using entities in their most basic capacity; that is, special tokens marked in queries and documents. Indeed, findings in past work on ad hoc retrieval regarding the merits of using sur-face level entity-based representations are inconclusive [16, 42, 47, 3, 14]. Second, such representations can be naturally used in existing retrieval approaches and tasks to improve performance; e.g., query expansion and cluster-based docu-ment retrieval as we show in this paper.

There are various potential merits in using surface level entity-based representations. For example, these can help to cope with the vocabulary mismatch problem; e.g., the en-tity United States of America can have different expressions in the text, including,  X  X .S. X ,  X  X SA X ,  X  X nited States X  and more. Furthermore, expressions of entities in the text are variable-length n -grams that bear semantic meaning. Thus, entities can be used for effective modeling of term proximity information which goes beyond using fixed-length n -grams.
An important challenge in inducing entity-based repre-sentations is accounting for the uncertainty inherent in the entity-markup process (a.k.a. entity linking); that is, asso-ciating term sequences with entities in a repository. Specif-ically, a term sequence can potentially be associated with multiple entities; e.g., the term  X  X incoln X  can be associated with the U.S. president, the car, the 2012 movie, etc. The uncertainty in entity linking has significant impact on re-trieval effectiveness as we show in this paper.

We present novel types of entity-based language models which consider both single terms in the text as well as term sequences marked as entities by an existing entity-linking tool. These language models are induced from the query and documents in the corpus and serve for retrieval in the language modeling framework. The main novelty of these language models is accounting, simultaneously, for (i) the uncertainty in entity linking  X  specifically, the confidence levels of entity markups; and, (ii) the balance between using term-based and entity-based information. We demonstrate t he importance of accounting for the mutual effects of these two aspects. For example, we show that using high recall entity markup, which is quite noisy, can help to significantly improve retrieval effectiveness if the noise is  X  X alanced X  by sufficient utilization of term-based information.
Empirical evaluation demonstrates the merits of using our entity-based language models for retrieval. The performance significantly transcends that of a state-of-the-art term prox-imity method: the sequential dependence model (SDM) [36, 19]. Integrating the language models with SDM yields fur-ther performance improvements. The language models are also effective for two additional retrieval paradigms: cluster-based document retrieval and query expansion.
The work most related to ours is that on devising sur-face level entity-based document and query representations for document retrieval [21, 16, 42, 47, 3, 41, 14]. The find-ings about the merits of these representations have been inconclusive. The few cases where the representations were shown to be somewhat effective for retrieval were when en-tity markups were devised in extreme care and were of very high quality [47, 3, 14]. In contrast to this past work that fo-cused on vector space models, we demonstrate the clear mer-its of using our entity-based language models for retrieval. Also, in contrast to previously proposed representations [21, 16, 42, 47, 3, 41, 14], our language models account simul-taneously for the uncertainty in the entity-markup process, and the balance between using term-based and entity-based information. Consequently, a highly important aspect that further differentiates our approach from past work is the effective utilization of high recall, noisy, entity markups.
There is work on query expansion using entity-based in-formation [43, 34, 30, 40, 8, 10, 18, 46, 6, 20, 7, 35, 39, 13, 31, 29, 45] and on projecting queries and documents onto an entity space to compare them [14, 33, 44]. There are two fundamental differences between all this past work and ours which focuses on surface level entity-based query and docu-ment representations. First, in these past methods, queries and documents are represented by external terms and en-tities which they do not contain 1 . Our surface level repre-sentations do not utilize such expansions. Second, auxiliary information about entities from the entity repository (e.g., textual descriptions of entities and their interrelations) is utilized in this past work, but not in our representations
We show that our entity-based language models can be used to create effective expanded query forms by  X  X lugging X  them into an existing query expansion method: the relevance model [26, 1]. The resultant approach, which simultaneously expands the query with both terms and entities, is concep-tually reminiscent of some methods recently proposed by Dalton et al. [13]. In their work, queries are expanded, inde-pendently, using terms and entities. The retrieval scores at-
X iong and Callan [44] found that representing queries using only entities marked in them is of merit for their learning-to-rank approach. However, features describing the query-entities relations rely on auxiliary information from the en-tity repository that is not used by our methods.
The entity-linking process could use auxiliary information from the entity repository. However, our proposed repre-sentations utilize the entity markups simply as tokens with confidence levels, and do not use auxiliary information. tained by using multiple term-only and entity-only expanded query forms are fused using a learning-to-rank method [13]. We show that our language models can be used to further im-prove the effectiveness of such expansion-based approaches by improving the quality of the pseudo relevant document list used for query expansion.

We also demonstrate the merits of using our language models for cluster-based document retrieval. Using entity-based representations for this task is novel to this study.
In some studies, concepts (entities) in verbose queries were automatically weighted [2, 22, 4, 5]. In contrast to our ap-proach, weights (confidence levels) of entities in documents were not accounted for. We demonstrate the importance of accounting for the confidence level of entity markups in both queries and documents. Further tuning of entities X  weights in our proposed language models, using some of these ap-proaches [2, 22, 4, 5], is interesting future work.
There are language models that integrate word phrases and named entities based on their association with prede-fined classes [27, 23]. In contrast to our language models, which are not based on such classes, these language models were not designed and used for document retrieval.
In what follows we present ad hoc document retrieval methods that rank documents in a corpus D in response to query q . The methods utilize information about entities mentioned in the query and in documents.

To mark entities in texts, we use some entity-linking tool that utilizes a repository (e.g., Wikipedia or Freebase) where entities have unique IDs. The entity-linking tool takes as in-put a text, query or document in our case, and marks vari-able length sequences of terms as potential entities in the repository. The entity markup of a term sequence is com-posed of entity ID and a confidence level in [0 , 1]. The con-fidence level reflects the likelihood that the term sequence corresponds to the entity. The confidence level relies on the term sequence and its context; e.g., its neighboring terms or other term sequences marked as entities [15, 38]. Using high confidence level results in high precision entity markup while low confidence level results in high recall.
We assume that each position in a given text can be part of at most a single term sequence that is marked as an entity; i.e., the entity markups do not overlap. A specific occurrence of a term sequence in a text cannot be marked with more than one entity. Yet, a term sequence can appear several times in a text with different entity markups as the markups depend on the context of the sequence. Details of the entity linking tools we use are provided in Section 4.1.
The retrieval methods we present in Section 3.2 use entity-based query and document language models. We now turn to define these language models.
We define unigram entity-based language models over a token space T ; i.e., tokens are generated by the language model independently of each other. The token space, is composed of the set V of all terms in the corpus D and the set E of entities in the entity repository which were marked at least once in a document in D with any confidence level.
The language models we devise rely on a definition of p seudo counts for tokens. Two definitions of pseudo counts will be presented in Sections 3.1.1 and 3.1.2. Let pc ( t,x ) be the pseudo count of token t (  X  X  ) in the text or text collection x . We define the pseudo length of x as:
The maximum likelihood estimate (MLE) of token t (  X  X  ) with respect to x is: The MLE can be smoothed using Dirichlet priors [49]:  X  is a smoothing parameter.

We next describe two types of language models defined over T and induced using Equations 2 and 3. The language models differ by the definition of pseudo counts for tokens. The hard confidence-level thresholding language model, HTLM in short, is based on fixing a threshold  X  (  X  [0 , 1]) for entity markups. Entity-based information is used only for entity markups with confidence level  X   X  . In contrast, every term occurrence in a text, including those in entity markups with a confidence level &lt;  X  , is accounted for.
To formally define a HTLM using Equations 2 and 3, we have to define pseudo counts for tokens from T in a text or text collection x . To that end, we lay down a few defini-tions. If t (  X  X  ) is a term, then c term ( t,x ) is the number of occurrences of t in x . Let M ( x ) denote the set of all en-tity markups in x ; i.e., all occurrences of term sequences in x that were marked as entities with some confidence level. For a markup m (  X  X  ( x )), E ( m ) is the entity and  X  ( m ) is the confidence level. The equivalence relation t 1  X  t 2 holds iff the entity tokens t 1 and t 2 are identical (i.e., have the same ID). The pseudo count of t (  X  X  ) in x is based on (i) the raw count of t in x if t is a term; and, (ii) the number of entity markups of t in x with a confidence level  X   X  if t is an entity. Formally,  X  (  X  [0 , 1]) is a free parameter which controls the relative importance attributed to term and entity tokens;  X  is Kro-necker X  X  delta function: for statement s ,  X  [ s ] = 1 if s is true and  X  [ s ] = 0 otherwise.

We note that using a Dirichlet smoothed HTLM (i.e., us-ing Equation 4 in Equation 3) can still result in assigning zero probability to some tokens in T . These are entities with no corresponding markup of a term sequence in the corpus with confidence level  X   X  . We re-visit this point below.
If we set  X  = 1 in Equation 4, then the resultant HTLM reduces to a standard unigram term-based language model. Setting  X  = 0 results in HTEntLM which is a unigram language model that assigns non-zero probability only to entities: if the MLE from Equation 2 is used, then these are the entities with at least one markup in x with a confidence level  X   X  ; if the Dirichlet smoothed language model is used (Equation 3), then these are the entities with at least one markup in the corpus with a confidence level  X   X  .
A potential drawback of HTLM is committing to a specific threshold  X  for entity markups. That is, information about entity markups with confidence level lower than  X  is ignored. Furthermore, all entity markups with confidence level  X   X  are counted equally as their confidence levels are ignored.
Thus, we now turn to present a soft confidence-level thresh-olding language model, STLM . STLM accounts for all mark-ups of an entity and weighs them by the corresponding con-fidence levels. Specifically, the pseudo count of t (  X  X  ) in the text or text collection x is defined as: pc  X  (  X  [0 , 1]) is a free parameter that, as in HTLM, controls the relative importance attributed to term and entity to-kens. Thus, STLM addresses the uncertainty inherent in the entity linking process by using expected entity occur-rence counts; the corresponding confidence levels serve for occurrence probabilities. These expected counts are then integrated with deterministic term counts.

If we set  X  = 1 in Equation 5, then STLM reduces to a standard unigram term-based language model as was the case for HTLM. Setting  X  = 0 results in STEntLM . This language model assigns a non-zero probability only to en-tities that have at least one markup (with any confidence level) in x when using the MLE (Equation 2) or in the corpus when using the Dirichlet smoothed language model (Equa-tion 3). We note that in contrast to the case for HTLM, there is no token in T that is assigned a zero probability by a Dirichlet smoothed STLM.
We rank document d by the cross entropy between the language models induced from the query ( q ) and d [25]: higher values correspond to decreased similarity.
Equation 6 is instantiated using the entity-based language models described in Section 3.1. Following common practice [48], we use an unsmoothed maximum likelihood estimate for the query language model (Equation 2) and a Dirich-let smoothed document language model (Equation 3). We obtain four retrieval methods : HT 3 , HTOEnt , ST and STOEnt 4 , which utilize the HTLM, HTEntLM, STLM and
I n HT, the same confidence-level threshold,  X  d , is used for all documents; the query threshold,  X  q , can be different from  X  . Hence, an entity token assigned a non-zero probability by  X  q could be assigned a zero probability by  X  d ; e.g., an entity marked in q with a confidence level  X   X  q but with no markup in the corpus with confidence level  X   X  d . In these cases, we zero the probability assigned to the entity token by  X  to avoid a log 0 in Equation 6. This is common practice in addressing term tokens that appear in a query but not in any document in the corpus.
HTOEnt and STOEnt rely only on entity tokens. If all en-tities in E are assigned a zero probability by the unsmoothed STEntLM language models, respectively. HT and ST utilize e ntity and term tokens, while HTOEnt and STOEnt utilize only entity tokens, hence the  X  X  X  in the methods names.
The HTLM and STLM language models integrate term-based and entity-based information at the language model level . Hence, the query-document comparison in Equation 6 simultaneously accounts for the appearance of the query terms and entities in a document.

An alternative approach is integrating term and entity in-formation at the retrieval score level . Inspired by approaches in the vector-space model [42], and in work on using a latent entity space [33], we consider a method that fuses document retrieval scores produced by utilizing, independently , term-only (  X  term x ) and entity-only (  X  ent x ) language models induced from text x . Document d is scored by: the  X  parameter balances the score fusion 5 . The query language models are unsmoothed maximum likelihood es-timates (Equation 2) and the document language models are Dirichlet smoothed (Equation 3).
 Instantiating Equation 7 with an entity-only language model, HTEntLM or STEntLM, and with a standard unigram term-based language model yields the F-HT and F-ST methods, respectively. These are conceptually highly similar to the HT and ST methods which integrate term-based and entity-based information at the language-model level. However, HT and ST use a single smoothing parameter for both term and entity tokens (see Equation 3) while F-HT and F-ST can use a different smoothing parameter for each as they utilize separately term-only and entity-only language mod-els. We could have used different smoothing parameters for entity and term tokens under the same language model, e.g., by applying term-specific smoothing [17], but we leave this exploration for future work.
Experiments were conducted using the TREC datasets specified in Table 1. AP and ROBUST are mostly composed of news articles. WT10G is a small, noisy, Web collection. GOV2 is a much larger Web collection composed of high quality pages crawled from the .gov domain. ClueB is the query language model, then no documents are retrieved. T his can happen for example when inducing HTEntLM from the query with a high confidence-level threshold or inducing a STEntLM from a query which has no entity markups.
The  X  in the score-based fusion model has a conceptually similar role to that of  X  in STLM and HTLM: balancing the use of term-based and entity-based information.
 English part of the Category B of the ClueWeb 2009 Web collection. ClueBF was created from ClueB by filtering from rankings suspected spam documents: those assigned a score below 50 by Waterloo X  X  spam classifier [11].
 Tokenization and Porter stemming were applied using the Lucene toolkit (lucene.apache.org) which was used for ex-periments. Stopwords on the INQUERY list were removed from queries but not from documents.

Unless otherwise specified, the TagMe entity-linking tool (tagme.di.unipi.it) is used to annotate queries and docu-ments. TagMe uses Wikipedia (a July 2014 dump) as the en-tity repository, and was shown to be highly effective and effi-cient in comparison to other publicly available entity-linking systems [12]. In Section 4.2.1 we also show the effective-ness of our methods using the Wikifier entity-linking tool [9, 12]. Wikifier was applied with an efficient configuration claimed to yield baseline entity linking effectiveness.
TagMe and Wikifier cannot process very long texts. Thus, we split documents into non-overlapping term-window pas-sages. We terminate a passage at the first space that appears at least 500 characters after the beginning of the previous passage. We let the tools mark the passages independently. The tools are applied on the non-stemmed and non-stopped queries and documents. Entity markup of a term sequence includes an entity ID and a confidence level (in [0 , 1]). We scan each text left to right and remove overlapping entity markups so that each position can be part of at most a single markup. If two markups overlap, we select the one with the higher confidence level. We break ties of confidence levels by selecting the markup which starts at the leftmost position. model retrieval [25], denoted TermsLM , for reference. This is a special case of the HT, ST, F-HT and F-ST methods with  X  = 1. Documents are ranked by the cross entropy between the unsmoothed (MLE) query language model and Dirichlet smoothed document language models.

The HTCon method is a special case of HT with  X  = 0 . 5 and  X  q =  X  d = 0 (  X  q and  X  d are the query and document thresholds, respectively). HTCon accounts uniformly for all entity mentions, and attributes the same importance to term and entity tokens. HTCon is conceptually reminiscent of methods representing documents and queries using concepts (e.g., from Wordnet) by concatenating with equal weights term-based and concept-based vector-space representations [41, 16, 42]. Accordingly, we consider F-HTCon : a special case of F-HT with  X  = 0 . 5 and  X  q =  X  d = 0.

Additional baseline is the state-of-the-art sequential de-pendence model, SDM , from the Markov Random Field framework which utilizes term proximities [36, 19]. The comparison with SDM, and its integration with our STLM is presented in Section 4.2.3.
 age precision at cutoff 1000 (MAP), precision of the top 10 documents (p@10) and NDCG@10 (NDCG) serve as evalu-ation measures. Statistically significant performance differ-ences are determined using the two-tailed paired t-test with a 95% confidence level. c ogcomp.cs.illinois.edu/page/demo view/Wikifier Table 2: Comparison of methods instantiated from E quation 6 using term-only (TermsLM) and entity-based language models. Bold: the best result in a row.  X  t  X ,  X  h  X ,  X  o  X ,  X  c  X  and  X  s  X  mark statistically sig-nificant differences with TermsLM, HT, HTOEnt, HTCon and ST, respectively.

The free parameter values of a ll retrieval methods are set using 10-fold cross validation performed over the queries in a dataset. Query IDs are used to create the folds. The optimal parameter values for each of the 10 train sets are determined using a simple grid search applied to optimize MAP. The learned parameter values are then used for the queries in the corresponding test fold.

The value of the Dirichlet smoothing parameter,  X  , is se-lected from { 100 , 500 , 1000 , 1500 , 2000 , 2500 , 3000 } . The pa-rameter  X  , used in HTLM, STLM, F-HT and F-ST, is set to values in { 0 , 0 . 1 ,..., 1 } . The document (  X  d ) and query (  X  ) entity-markup confidence level thresholds, used in HT, HTOEnt and F-HT, are set to values in { 0 , 0 . 1 ,..., 0 . 9 } .
Table 2 presents the performance of the methods that use entity-based language models to instantiate Equation 6. Our first observation is that the HT and ST methods out-perform the standard term-based language-model retrieval, TermsLM, in all relevant comparisons (6 corpora  X  3 eval-uation measures); most improvements are substantial and statistically significant. Furthermore, HT and ST outper-form to a substantial and statistically significant degree their special cases which use only entity tokens: HTOEnt and STOEnt, respectively. These findings attest to the merits of using our proposed language models, HTLM and STLM, which integrate term-based and entity-based information. We also see in Table 2 that HT and ST outperform HT-Con in most relevant comparisons; most MAP improvements for ST are statistically significant. Recall from Section 4.1 that HTCon represents past practice of concept-based repre-sentations: accounting uniformly for all entity mentions and attributing equal importance to entity and term tokens. Be-low we further study the importance of accounting for the Figure 1: The effect of varying  X  o n the MAP of HT and ST. For  X  = 1 , the methods amount to TermsLM (term-based language model retrieval). For  X  = 0 , the methods use only entity tokens. The perfor-mance is reported for the test folds (i.e., all queries in a dataset) when fixing the value of  X  and using cross validation to set the values of all other free pa-rameters. Note: figures are not to the same scale. confidence level of entity markups, and attributing different weights to term and entity tokens as in HT and ST.
Table 2 shows that ST outperforms HT in most relevant comparisons, although rarely to a statistically significant de-gree. In addition, ST posts more statistically significant im-provements over HTCon than HT. We note that HT depends on four free parameters (  X  ,  X  q ,  X  d and  X  ) while ST depends only on two (  X  and  X  ). Furthermore, the values learned for  X  and  X  d in HT using the training folds are very low, attest-ing to the merits of using high recall entity markup. (We revisit this point below.) Overall, these findings attest to the potential merits of using a soft-thresholding approach for the confidence level of entity markups (STLM) with respect to a hard-thresholding approach (HTLM); i.e., accounting for all entity markups in a text and weighing their impact by their confidence levels is superior to accounting, uniformly, for entity markups with a confidence level above a threshold. of HT and ST as a function of  X  . Low and high values of  X  result in more importance attributed to entity-based and term-based information, respectively. For  X  = 1, the two methods amount to TermsLM  X  i.e., standard term-based language model retrieval. For  X  = 0, the methods use only entity-based information; specifically, HT reduces to HTOEnt and ST reduces to STOEnt.

We see in Figure 1 that optimal performance is always attained for  X  6 X  X  0 , 1 } . This finding echoes those based on Table 2. That is, HT and ST outperform TermsLM, Figure 2: The effect of varying  X  q a nd  X  d on the MAP performance of HT. The values of free parameters, except for that in the x -axis, are set using cross val-idation as in Figure 1. and HTOEnt and STOEnt, respectively. Thus, we find that there is much merit in integrating term-based and entity-based information for representing queries and documents.
Figure 1 shows that the optimal value of  X  for HT is often higher than for ST. This can be attributed to the fact that HTLM, used to represent the query and documents in HT, uses a single confidence-level threshold for entity markups. Thus, potentially valuable information about entities is not utilized. As a result, HT calls for more reliance on term-based information to  X  X ompensate X  for this potential infor-mation loss. In contrast, ST accounts for all entity markups, weighing their impact by their confidence levels. Hence, the  X  X isk X  in relying on entity-based information is lower 7 .
To further explore the effect of using a hard threshold for the confidence level of entity markups in HT, we present in Figure 2 its MAP performance as a function of  X  q and  X  d  X  the query and document thresholds, respectively. Recall that low threshold corresponds to high recall markup. Fig-ure 2 shows that low values of  X  q and  X  d lead to improved performance. This finding can be attributed to the fact that increasing the confidence-level threshold amounts to loosing potentially valuable information about appearances of en-tities in the query and documents. To compensate for the lower precision (i.e., noisier) markup caused by using a low threshold, more weight is put on term-based information as is evident in the relatively high optimal values of  X  presented in Figure 1. Specifically, we note that the learned values of  X  ,  X  , and  X  q , averaged over the train folds, for AP, ROBUST, WT10G, GOV2, ClueB and ClueBF are (0 . 6, 0 . 01, 0 . 11), (0 . 81, 0 . 17, 0) respectively; namely, relatively high values of  X  and low values of  X  d and  X  q lead to improved performance. for entity linking. In Table 3 we compare the retrieval perfor-mance when using the entity markups of TagMe and Wik-ifier. Having Wikifier annotate large-scale collections is a challenging computational task. Thus, we present results only for AP, ROBUST and WT10G. We report MAP and NDCG; the performance patterns for p@10 are the same.
Table 3 shows that using ST, our best performing method from above, with Wikifier, results in performance that tran-scends (often, significantly) that of the standard term-based language model (TermsLM) when using all queries in a dataset
S etting  X  on a per-query basis, in the spirit of work on fus-ing term-only-based and latent-entity-space-based retrieval scores [33], is a future direction we intend to explore. Table 3: Comparing entity-linking tools. Either all queries in a dataset are used ( X  X ll Queries X ), or only those marked with at least one entity by both TagMe and Wikifier ( X  X arked Queries X ). Bold: best result in a column in a block;  X  t  X ,  X  s  X ,  X  w  X  and  X  e  X : statisti-cally significant differences with TermsLM, TagMe-ST, Wikifier-ST and TagMe-STOEnt, respectively.
 (the  X  X ll Queries X  block). However, the performance of us-i ng TagMe is consistently better.
 TagMe marks more queries with at least one entity than Wikifier: for AP, ROBUST and WT10G, Wikifier marked no entities in 17, 34 and 26 queries, respectively; TagMe did not mark entities in 0, 1 and 3 queries. (For GOV2 TagMe marked all queries with entities and for ClueB/ClueBF all queries except for one.) Recall that for queries with no marked entities, ST relies only on term-based information.
To refine the comparison of TagMe and Wikifier, we report the performance of ST and STOEnt 8  X  the latter relies only on entity tokens  X  with these two tools over only queries in which both marked at least one entity. As can be seen in the  X  X arked Queries X  block in Table 3, TagMe still outperforms Wikifier in almost all relevant comparisons; for STOEnt, several improvements are statistically significant.
TagMe X  X  superiority can be partially attributed to mark-ing more entities (with confidence level &gt; 0) on average than Wikifier: (2 . 4, 1 . 8, 2 . 0) with respect to (1 . 7, 1 . 2, 1 . 0) in queries over AP, ROBUST and WT10G; and, (157 . 2, 158 . 7, 207 . 0) with respect to (58 . 4, 50 . 5, 61 . 7) in documents.
To conclude, our methods are effective with both TagMe and Wikifier. Using TagMe yields better performance that can be partially attributed to higher recall entity markup. Table 4 presents the performance of the F-HT and F-ST methods from Section 3.2.1 that perform score fusion of term-only-based and entity-only-based retrieval scores. The performance of TermsLM (term-only language model), HT and ST that integrate term and entity information at the language model level, and that of F-HTCon which is a spe-cial case of F-HT (see Section 4.1), is presented for refer-ence. We see that F-HT and F-ST substantially outperform TermsLM. (F-ST posts the best performance in most rele-vant comparisons in Table 4.) Both methods also outper-form F-HTCon in most relevant comparisons.
F or queries for which a tool does not mark any entities, no documents are retrieved with STOEnt. Thus, we do not report the performance of STOEnt using all queries as the results are inherently biased in favor of TagMe which marks many more queries with entities than Wikifier. Table 4: Score-based fusion ( X  X - X  methods). Bold: b est result in a row;  X  t  X ,  X  h  X ,  X  s  X ,  X  f  X  and  X  c  X : statisti-cally significant differences with TermsLM, HT, ST, F-HT and F-HTCon, respectively.
 In most relevant comparisons, F-HT outperforms HT and F -ST outperforms ST, although most performance differ-ences are not statistically significant. The improvements can be attributed to the fact that F-HT and F-ST use a different smoothing parameter value for terms and entities while HT and ST use a joint one. (See Section 3.2.1 for details.)
The potential effectiveness of using different smoothing parameters for term and entity tokens stems from the dif-ferent number of terms and entity markups in a document. The average number of terms in a document for AP, RO-BUST, WT10G, GOV2, and ClueB (ClueBF) is 455 . 4, 474 . 8, 588 . 2, 904 . 7 and 813 . 6, respectively. The average number of entity markups with a confidence level &gt; 0 is much lower: 157 . 2, 158 . 7, 207 . 0, 291 . 9 and 307 . 8.
We next compare our entity-based approach with the se-quential dependence model (SDM) [36] which scores d by: S
SDM ( d ; q ) the sum of the  X  S ,  X  O and  X  U parameters is 1; Sim S ( d,q ), Sim O ( d,q ) and Sim U ( d,q ) are cross-entropy based similar-ity estimates of the document to the query, utilizing informa-tion about occurrences of unigram, ordered bigrams, and un-ordered bigrams, respectively, of q  X  X  terms in d ; un-ordered bigrams are confined to 8-terms windows in documents.
Using entity tokens in our methods amounts to utiliz-ing information about the occurrences of only some ordered variable-length n -grams of query terms in documents  X  i.e., n -grams which constitute entities. Thus, in contrast to SDM, our methods do not utilize proximity information for query terms which are not in entity markups nor proximity information for unordered n -grams of query terms.
In addition, we study the merit of integrating entity-based information, specifically, our soft-thresholding language model STLM, with SDM. To that end, we augment the SDM scor-ing function with an entity-based document-query similar-Table 5: Comparison and integration with SDM [36]. Bold: the best result in a row.  X  t  X ,  X  s  X ,  X  f  X  and  X  m  X  mark statistically significant differences with TermsLM, ST, F-ST and SDM, respectively.
 ity estimate, S im E ( d,q ). For this estimate, we use the score assigned to d by the STOEnt method; i.e., we use an entity-only language model since term-based informa-tion is accounted for in Sim S ( d,q ). The resultant method, SDM+STLM , scores d by (  X  S +  X  O +  X  U +  X  E = 1): S SDM+STLM can be viewed as a novel instantiation of a weighted dependence model (WSDM) [4] with a novel con-cept type (i.e., entity). If  X  O =  X  U = 0, SDM+STLM amounts to our F-ST method (see Section 3.2.1).

All free parameters of SDM and SDM+STLM:  X  S ,  X  O ,  X  ,  X  E and the Dirichlet smoothing parameter,  X  , are set using cross validation as described in Section 4.1;  X  S ,  X   X  , and  X  E are selected from { 0 , 0 . 1 ,..., 1 } and  X  is set to values in { 100 , 500 , 1000 , 1500 , 2000 , 2500 , 3000 } .
Table 5 shows that ST and F-ST outperform SDM, of-ten statistically significantly, in most relevant comparisons (6 corpora  X  3 evaluation measures). This implies that us-ing variable length n -grams which potentially bear semantic meaning (entities) can yield better performance than us-ing ordered and unordered bigrams which do not necessarily have semantic meaning. Recall that in contrast to SDM, ST and F-ST do not account for proximities between terms which do not constitute entities and for unordered bigrams. In most relevant comparisons, SDM+STLM outperforms SDM and ST (which utilizes STLM) and is as effective as, and often posts statistically significant improvements over, F-ST  X  its special case that fuses unigram term-only and entity-only retrieval scores. The few cases where F-ST out-performs SDM+STLM could be attributed to potential over-fitting effects due to the high number of free parameters of SDM+STLM and the relatively low number of queries.
We also found that effective weights assigned to entity-only similarities in SDM+STLM (  X  E ) are much higher than those assigned to ordered (  X  O ) and un-ordered (  X  U ) bigram Table 6: Robustness analysis. Number of queries f or which ST hurts (-) and improves (+) AP perfor-mance with respect to TermsLM and SDM.
 term-based similarities. Furthermore, effective values of  X  an d  X  U are lower and higher, respectively, for SDM+STLM than for SDM. These findings further attest to the merits of using entity-based similarities with respect to (ordered and un-ordered) bigram similarities, and show that un-ordered bigram, in contrast to ordered bigram, similarities could be complementary to entity-based similarities.
We now turn to further analyze merits, and shortcomings, of using entity-based query and document representations. To that end, we focus on the ST method that utilizes STLM.
Table 6 presents performance robustness analysis: the number of queries for which ST improves or hurts average precision (AP) over TermsLM and SDM. In both cases, ST improves AP for more queries than it hurts; naturally, the differences with SDM are smaller than those with TermsLM.
One advantage of STLM is that it represents the query and documents using entities which constitute variable length n -grams with semantic meaning. A case in point, query #41 in ClueWeb,  X  X range county convention center X , refers to the primary public convention center for the Central Florida re-gion. TermsLM, SDM and ST ranked the Web home page for this entity second. However, at the third rank in the lists retrieved by TermsLM and SDM appears a Wikipedia page titled  X  X ist of convention and exhibition centers X , which is not specific to the entity of concern. The average preci-sion (AP) of TermsLM, SDM and ST for the query in the ClueB dataset was 9, 13, and 30, respectively, attesting to the merit of the correct identification of the entity in the query and its utilization by ST.

The ST method can suffer from incorrect entity identi-fication in queries. For example, query #407 in ROBUST,  X  X oaching, wildlife preserves X , targets information about the impact of poaching on the world X  X  various wildlife preserves. The entities identified by TagMe are  X  X oaching X ,  X  X ildlife X  and  X  X reserves X ; the latter refers to fruit preserves instead of nature preserves. Such erroneous entity identification can be attributed to the little context short queries provide. Con-sequently, the AP of ST for this query is only 8 while that of TermsLM and SDM is 31 . 4 and 30 . 0, respectively.
We next explore the effectiveness of using our entity-based language models in two additional retrieval paradigms: cluster-based document retrieval and query expansion. Let D init denote the list of top-n documents retrieved by TermsLM (standard language-model-based retrieval). Fol-lowing common practice in work on cluster-based document retrieval [32, 24], we re-rank D init using information induced from nearest-neighbor clusters of documents in D init . Table 7: Cluster-based document re-ranking. Bold: the best result in a row;  X  t  X ,  X  s  X ,  X   X   X  and  X   X   X  mark sta-tistically significant differences with TermsLM, ST, C-Term-Term and C-Term-Ent, respectively.

We use S im ( x,y ) def = exp (  X  CE `  X  MLE x ||  X  Dir y sure the similarity between texts x and y [24];  X  MLE x is an unsmoothed MLE induced from x and  X  Dir y is a Dirichlet smoothed language model induced from y . Each document d (  X  D init ) and the k  X  1 documents d  X  ( d  X  6 = d ) in D that yield the highest Sim ( d,d  X  ) constitute a cluster.
We rank the (overlapping) clusters c , each contains k doc-uments, by: k q Q d  X  c S im ( q,d ) [32]. This is a highly effective simple cluster ranking method [24]. To induce document ranking, each cluster is replaced with its constituent docu-ments omitting repeats; documents in a cluster are ordered by their query similarity: Sim ( q,d ).

The document (re-)ranking procedure just described re-lies on the choice of the document language models used to induce clusters (i.e., in Sim ( d,d  X  )) and the choice of docu-ment and query language models used to induce document-query similarities ( Sim ( q,d )); the latter are used for rank-ing both clusters and documents within the clusters. We use C-Term-Term to denote the standard method that uses term-only language models for inducing clusters and document-query similarities [32, 24]. The C-Term-Ent method utilizes the same clusters used by C-Term-Term, but uses our entity-based language model, STLM, for inducing document-query similarities to rank clusters and documents in them. In the C-Ent-Ent method, STLM is used to both create clusters and induce document-query similarities. As a reference comparison, we re-rank D init using the ST method that uses STLM but does not utilize clusters.

As the main goal of cluster-based re-ranking is improv-ing precision at top ranks [32, 24], we report p@10 and NDCG@10 (NDCG). Free-parameter values are set using cross validation; NDCG is the optimization criterion. Specif-ically, n is selected from { 50 , 100 } ; k is in { 5 , 10 } ; and,  X  (used in STLM) is in { 0 , 0 . 1 ,..., 1 } ; the Dirichlet smooth-ing parameter is set to 1000. Table 7 presents the results.
We see that all cluster-based methods (denoted  X  X -X-Y X ) almost always outperform the initial term-based document ranking, TermsLM. C-Term-Ent substantially outperforms C-Term-Term. This attests to the merits of using STLM for inducing cluster ranking and within cluster document ranking. In most relevant comparisons, C-Ent-Ent outper-forms (and is never statistically significantly outperformed by) C-Term-Ent, attesting to the potential merits of using Table 8: Query expansion. Bold: the best result i n a row.  X  t  X ,  X  s  X ,  X  r  X ,  X  w  X ,  X  m  X  and  X  n  X  mark sta-tistically significant differences with TermsLM, ST, RM3, WikiRM, SDM-RM and RMST, respectively.
 entity-based information to also create clusters. However, on ly two improvements are statistically significant.
Finally, Table 7 shows that in almost all relevant compar-isons, ST outperforms TermsLM (often, statistically signifi-cantly) and C-Term-Term and is outperformed by C-Term-Ent and C-Ent-Ent. This shows that while there is merit in using STLM for direct ranking of documents as shown in Section 4.2.1, the performance can be further improved by using STLM for cluster-based document ranking.
As noted in Section 2, there is much work on expanding queries with terms and entities using entity-based informa-tion. In contrast, our entity-based language models, when induced from the query, utilize only query terms and entities marked in the query. Hence, we study the effectiveness of using our language models to perform query expansion.
We use the relevance model (RM3) [1] as a basis for in-stantiating expanded query forms. The probability assigned to token t by a relevance model RM is:
RM ( t ) def =  X  X  MLE q ( t ) + (1  X   X  ) X  X  is a free parameter; L is a list of top-retrieved documents used to construct RM ; S ( d ; q ) is d  X  X  score. Due to computa-tional considerations, as in work on entity-based query ex-pansion [13, 45] we use RM to re-rank an initially retrieved document list; CE ` RM ||  X  Dir d  X  serves for re-ranking.
Using only terms as tokens, and applying standard language-model-based retrieval (TermsLM) over the corpus to create L , yields the standard RM3 [1]. Creating L by applying TermsLM over Wikipedia results in WikiRM [46], an ex-ternal corpus expansion approach also used in [13, 45]. RM3 and WikiRM re-rank a document list retrieved by TermsLM. (WikiRM is the only model where the list from which RM is constructed, L , is not a sub-set of the list to be re-ranked.) In both methods, S ( d ; q ) def = exp(  X  CE `  X  MLE q ||  X 
The SDM-RM model [13] is constructed from, and used to re-rank, lists retrieved by the sequential dependence model (SDM) [36].  X  Dir d , and the resultant relevance model con-structed by setting  X  = 0 in Equation 8, are term-based unigram language models; S ( d ; q ) is the exponent of the score assigned to d by SDM. Re-ranking is performed by linear interpolation of the SDM score assigned to d and CE ` RM ||  X  Dir d  X  , using a parameter  X  . SDM-RM is, in fact, the highly effective Latent Concept Expansion method [37] without IDF-based weighting of expansion terms.
The next two relevance models, defined over T (the term-entity token space from Equation 1), are novel to this study. They utilize our STLM language model which integrates terms and entities at the language model level. RMST is inspired by methods proposed by Dalton et al. [13] 9 by the virtue of using both terms and entities for query expansion.  X  q and  X  Dir d are our STLM language models. S ( d ; q ) exp(  X  CE `  X  MLE q ||  X  Dir d  X  ). The TermsLM method is ap-plied over the corpus to create the initial list to be re-ranked (cf. [45]) and from which L is derived.

RMST-ST is constructed as RMST using STLM. The difference is that our entity-based ST method, rather than TermsLM, is used to create the initial list to be re-ranked and from which L is derived. The formal ease of using STLM in the relevance model (Equation 8), yielding RMST and RMST-ST, attests to the merits of using a single language model defined over terms and entities with respect to the alternative score-based fusion approach from Section 3.2.1.
The free parameters of all methods are set using cross val-idation. The number of expansion terms (i.e., those assigned the highest probability by RM ), the number of documents in L , and  X  are set to values in { 10 , 30 , 50 , 100 } , { 50 , 100 } and { 0 , 0 . 1 ,..., 1 } , respectively. (Only for WikiRM, the num-ber of documents in L is selected from { 1 , 5 , 10 , 30 , 50 , 100 } following [46].) All lists that are re-ranked contain 1000 doc-uments. The values of the free parameters of ST and SDM are selected from the ranges specified in Section 4.1. The Dirichlet smoothing parameter,  X  , is selected from { 100 , 500 , 1000 , 1500 , 2000 , 2500 , 3000 } ; for relevance model con-struction (Equation 8) the value 0 is also used (yielding un-smoothed MLE). To reduce the number of free-parameter values configurations, we use the same value of  X  for cre-ating L , for re-ranking and for constructing the relevance model, unless 0 is used for relevance model construction.
Table 8 presents the performance. Our ST method, which does not perform query expansion, is competitive with the term-based relevance model (RM3). We also see that RMST is an effective expansion method which often outperforms RM3 and SDM-RM. This finding echoes those from past work [13, 45] about the merits of using both terms and en-tities for query expansion. The best performing method in most relevant comparisons is RMST-ST which uses STLM to (i) create an effective initial list for re-ranking; (ii) create an effective list, L , for relevance model construction; and, (iii) induce ranking using the entity-based relevance model as in RMST. We conclude that our STLM language model can play different important roles in query expansion.
Table 8 shows that expansion using Wikipedia as an exter-nal corpus (WikiRM) is effective. Our RMST and RMST-ST expansion methods (as well as ST) utilize entity tokens marked by TagMe (i.e., Wikipedia concepts), but do no use
V arious expansion methods, which utilize also auxiliary in-formation about entities from the entity repository, were in-tegrated in [13]. We do not use such auxiliary information. the text on their Wikipedia pages in contrast to WikiRM. T hus, integrating WikiRM with our methods, e.g., using score-based integration [13], is interesting future direction.
We presented novel entity-based language models induced using an entity linking tool. The models simultaneously ac-count for the uncertainty in the entity-linking process and the balance between using term-based and entity-based in-formation. We showed the merits of using the language mod-els for document retrieval in several retrieval paradigms. Acknowledgments. We thank the reviewers for their com-ments. This paper is based upon work supported in part by a Yahoo! faculty research and engagement award.
