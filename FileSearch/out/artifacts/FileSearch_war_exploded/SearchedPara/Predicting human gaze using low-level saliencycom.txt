 Although understanding attention is interesting purely from a scientific perspective, there are nu-merous applications in engineering, marketing and even art that can benefit from the understanding of both attention per se , and the allocation of resources for attention and eye movements. One ac-cessible correlate of human attention is the fixation pattern in scanpaths [1], which has long been of interest to the vision community [2]. Commonalities between different individuals X  fixation patterns allow computational models to predict where people look, and in which order [3]. There are several models for predicting observers X  fixations [4], some of which are inspired by putative neural mech-anisms. A frequently referenced model for fixation prediction is the Itti et al. saliency map model (SM) [5]. This  X  X ottom-up X  approach is based on contrasts of intrinsic images features such as color, orientation, intensity, flicker, motion and so on, without any explicit information about higher order scene structure, semantics, context or task-related ( X  X op-down X ) factors, which may be cru-cial for attentional allocation [6]. Such a bottom-up saliency model works well when higher order semantics are reflected in low-level features (as is often the case for isolated objects, and even for reasonably cluttered scenes), but tends to fail if other factors dominate: e.g., in search tasks [7, 8], strong contextual effects [9], or in free-viewing of images without clearly isolated objects, such as forest scenes or foliage [10]. Here, we test how images containing faces -ecologically highly rel-evant objects -influence variability of scanpaths across subjects. In a second step, we improve the standard saliency model by adding a  X  X ace channel X  based on an established face detector algorithm. Although there is an ongoing debate regarding the exact mechanisms which underlie face detec-tion, there is no argument that a normal subject (in contrast to autistic patients) will not interpret a face purely as a reddish blob with four lines, but as a much more significant entity ([11, 12]. In fact, there is mounting evidence of infants X  preference for face-like patterns before they can even consciously perceive the category of faces [13], which is crucial for emotion and social processing ([13, 14, 15, 16]).
 Face detection is a well investigated area of machine vision. There are numerous computer-vision models for face detection with good results ([17, 18, 19, 20]). One widely used model for face recognition is the Viola &amp; Jones [21] feature-based template matching algorithm (VJ). There have been previous attempts to incorporate face detection into a saliency model. However, they have either relied on biasing a color channel toward skin hue [22] -and thus being ineffective in many cases nor being face-selective per se -or they have suffered from lack of generality [23]. We here propose a system which combines the bottom-up saliency map model of Itti et al. [5] with the Viola &amp; Jones face detector.
 The contributions of this study are: (1) Experimental data showing that subjects exhibit significantly less variable scanpaths when viewing natural images containing faces, marked by a strong tendency to fixate on faces early. (2) A novel saliency model which combines a face detector with intensity, color, and orientation information. (3) Quantitative results on two versions of this saliency model, including one extended from a recent graph-based approach, which show that, compared to previous approaches, it better predicts subjects X  fixations on images with faces, and predicts as well otherwise. 2.1 Experimental procedures Seven subjects viewed a set of 250 images ( 1024  X  768 pixels) in a three phase experiment. 200 of the images included frontal faces of various people; 50 images contained no faces but were otherwise identical, allowing a comparison of viewing a particular scene with and without a face. In the first ( X  X ree-viewing X ) phase of the experiment, 200 of these images (the same subset for each subject) were presented to subjects for 2 s, after which they were instructed to answer  X  X ow interesting was the image? X  using a scale of 1-9 (9 being the most interesting). Subjects were not instructed to look at anything in particular; their only task was to rate the entire image. In the second ( X  X earch X ) phase, subjects viewed another 200 image subset in the same setup, only this time they were initially presented with a probe image (either a face, or an object in the scene: banana, cell phone, toy car, etc.) for 600 ms after which one of the 200 images appeared for 2 s. They were then asked to indicate whether that image contained the probe. Half of the trials had the target probe present. In half of those the probe was a face. Early studies suggest that there should be a difference between free-viewing of a scene, and task-dependent viewing of it [2, 4, 6, 7, 24]. We used the second task to test if there are any differences in the fixation orders and viewing patterns between free-viewing and task-dependent viewing of images with faces. In the third phase, subjects performed a 100 images recognition memory task where they had to answer with y/n whether they had seen the image before. 50 of the images were taken from the experimental set and 50 were new. Subjects X  mean performance was 97.5% correct, verifying that they were indeed alert during the experiment. The images were introduced as  X  X egular images that one can expect to find in an everyday personal photo album X . Scenes were indoors and outdoors still images (see examples in Fig. 1). Images included faces in various skin colors, age groups, and positions (no image had the face at the center as this was the starting fixation location in all trials). A few images had face-like objects (see balloon in Fig. 1, panel 3), animal faces, and objects that had irregular faces in them (masks, the Egyptian sphinx face, etc.). Faces also vary in size (percentage of the entire image). The average face was 5%  X  1% (mean  X  s.d.) of the entire image -between 1  X  to 5  X  of the visual field; we also varied the number of faces in the image between 1-6, with a mean of 1 . 1  X  0 . 48 . Image order was randomized throughout, and subjects were na  X   X ve to the purpose of the experiment. Subjects fixated on a cross in the center before each image onset. Eye-position data were acquired at 1000 Hz using an Eyelink 1000 (SR Research, Osgoode, Canada) eye-tracking device. The images were presented on a CRT screen (120 Hz), using Matlab X  X  Psychophysics and eyelink toolbox extensions ([25, 26]). Stimulus luminance was linear in pixel values. The distance between the screen and the subject was 80 cm, giving a total visual angle for each image of 28  X   X  21  X  . Subjects used a chin-rest to stabilize their head. Data were acquired from the right eye alone. All subjects had uncorrected normal eyesight. Figure 1: Examples of stimuli during the  X  X ree-viewinng X  phase. Notice that faces have neu-tral expressions. Upper 3 panels include scanpaths of one individual. The red triangle marks the subsequent fixations. Lower panels show scanpaths of all 7 subjects. The trend of visiting the faces first -typically within the 1 st or 2 nd fixation -is evident. All images are available at http://www.klab.caltech.edu/  X moran/db/faces/ . 2.2 Combining face detection with various saliency algorithms We tried to predict the attentional allocation via fixation patterns of the subjects using various saliency maps. In particular, we computed four different saliency maps for each of the images in our data set: (1) a saliency map based on the model of [5] (SM), (2) a graph-based saliency map according to [27] (GBSM), (3) a map which combines SM with face-detection via VJ (SM+VJ), and (4) a saliency map combining the outputs of GBSM and VJ (GBSM+VJ). Each saliency map was represented as a positive valued heat map over the image plane.
 SM is based on computing feature maps, followed by center-surround operations which highlight lo-cal gradients, followed by a normalization step prior to combining the feature channels. We used the  X  X axnorm X  normalization scheme which is a spatial competition mechanism based on the squared ratio of global maximum over average local maximum. This promotes feature maps with one con-spicuous location to the detriment of maps presenting numerous conspicuous locations. The graph-based saliency map model (GBSM) employs spectral techniques in lieu of center surround subtrac-tion and  X  X axnorm X  normalization, using only local computations. GBSM has shown more robust correlation with human fixation data compared with standard SM [27].
 For face detection, we used the Intel Open Source Computer Vision Library ( X  X penCV X ) [28] im-plementation of [21]. This implementation rapidly processes images while achieving high detection rates. An efficient classifier built using the Ada-Boost learning algorithm is used to select a small number of critical visual features from a large set of potential candidates. Combining classifiers in a cascade allows background regions of the image to be quickly discarded, so that more cycles process promising face-like regions using a template matching scheme. The detection is done by applying a classifier to a sliding search window of 24x24 pixels. The detectors are made of three joined black and white rectangles, either up-right or rotated by 45  X  . The values at each point are calculated as a weighted sum of two components: the pixel sum over the black rectangles and the sum over the whole detector area. The classifiers are combined to make a boosted cascade with classifiers going from simple to more complex, each possibly rejecting the candidate window as  X  X ot a face X  [28]. This implementation of the facedetect module was used with the standard default training set of the original model. We used it to form a  X  X aces conspicuity map X , or  X  X ace channel X  by convolving delta functions at the (x,y) detected facial centers with 2D Gaussians having standard deviation equal to estimated facial radius. The values of this map were normalized to a fixed range. For both SM and GBSM, we computed the combined saliency map as the mean of the normalized color (C), orientation (O), and intensity (I) maps [5]: And for SM+VJ and GBSM+VJ, we incorporated the normalized face conspicuity map (F) into this mean (see Fig 2): This is our combined face detector/saliency model. Although we could have explored the space of combinations which would optimize predictive performance, we chose to use this simplest possible combination, since it is the least complicated to analyze, and also provides us with first intuition for further studies.
 Figure 2: Modified saliency model. An image is processed through standard [5] color, orientation and intensity multi-scale channels, as well as through a trained template-matching face detection mechanism. Face coordinates and radius from the face detector are used to form a face conspicuity map (F), with peaks at facial centers. All four maps are normalized to the same dynamic range, and added with equal weights to a final saliency map (SM+VJ, or GBSM+VJ). This is compared to a saliency map which only uses the three bottom-up features maps (SM or GBSM). 3.1 Psychophysical results To evaluate the results of the 7 subjects X  viewing of the images, we manually defined minimally sized rectangular regions-of-interest (ROIs) around each face in the entire image collection. We first assessed, in the  X  X ree-viewing X  phase, how many of the first fixations went to a face, how many of the second, third fixations and so forth. In 972 out of the 1050 (7 subjects x 150 images with faces) trials (92.6%), the subject fixated on a face at least once. In 645/1050 (61.4%) trials, a face was fixated on within the first fixation, and of the remaining 405 trials, a face was fixated on in the second fixation in 71.1% (288/405), i.e. after two fixations a face was fixated on in 88.9% (933/1050) of trials (Fig. 3). Given that the face ROIs were chosen very conservatively ( i.e. fixations just next to a face do not count as fixations on the face), this shows that faces, if present, are typically fixated on within the first two fixations ( 327 ms  X  95 ms on average). Furthermore, in addition to finding early fixations on faces, we found that inter-subject scanpath consistency on images with faces was higher. For the free-viewing task, the mean minimum distance to another X  X  subject X  X  fixation (averaged over fixations and subjects) was 29.47 pixels on images with faces, and a greater 34.24 pixels on images without faces (different with p &lt; 10  X  6 ). We found similar results using a variety of different metrics (ROC, Earth Mover X  X  Distance, Normalized Scanpath Saliency, etc.). To verify that the double spatial bias of photographer and observer ([29] for discussion of this issue) did not artificially result in high fractions of early fixations on faces, we compared our results to an unbiased baseline: for each subject, the fraction of fixations from all images which fell in the ROIs of one particular image. The null hypothesis that we would see the same fraction of first fixations on a face at random is rejected at p &lt; 10  X  20 (t-test).
 To test for the hypothesis that face saliency is not due to top-down preference for faces in the absence of other interesting things, we examined the results of the  X  X earch X  task, in which subjects were presented with a non-face target probe in 50% of the trials. Provided the short amount of time for the search (2 s), subjects should have attempted to tune their internal saliency weights to adjust color, intensity, and orientation optimally for the searched target [30]. Nevertheless, subjects still tended to fixate on the faces early. A face was fixated on within the first fixation in 24% of trials, within the first two fixations in 52% of trials, and within the three fixations in 77% of the trials. While this is weaker than in free-viewing, where 88.9% was achieved after just two fixations, the difference from what would be expected for random fixation selection (unbiased baseline as above) is still highly significant ( p &lt; 10  X  8 ).
 Overall, we found that in both experimental conditions ( X  X ree-viewing X  and  X  X earch X ), faces were powerful attractors of attention, accounting for a strong majority of early fixations when present. This trend allowed us to easily improve standard saliency models, as discussed below.
 Figure 3: Extent of fixation on face regions-of-interest (ROIs) during the  X  X ree-viewing X  phase . Left: image with all fixations (7 subjects) superimposed. First fixation marked in blue, second in cyan, remaining fixations in red. Right: Bars depict percentage of trials, which reach a face the first time in the first, second, third, . . . fixation. The solid curve depicts the integral, i.e. the fraction of trials in which faces were fixated on at least once up to and including the n th fixation. 3.2 Assessing the saliency map models We ran VJ on each of the 200 images used in the free viewing task, and found at least one face detection on 176 of these images, 148 of which actually contained faces (only two images with faces were missed). For each of these 176 images, we computed four saliency maps (SM, GBSM, SM+VJ, GBSM+VJ) as discussed above, and quantified the compatibility of each with our scan-path recordings, in particular fixations, using the area under an ROC curve. The ROC curves were generated by sweeping over saliency value thresholds, and treating the fraction of non-fixated pixels on a map above threshold as false alarms, and the fraction of fixated pixels above threshold as hits [29, 31]. According to this ROC fixation  X  X rediction X  metric, for the example image in Fig. 4, all models predict above chance (50%): SM performs worst, and GBSM+VJ best, since including the face detector substantially improves performance in both cases.
 Figure 4: Comparison of the area-under-the-curve (AUC) for an image (chosen arbitrarily. Subjects X  scanpaths shown on the left panels of figure 1). Top panel: image with the 49 fixations of the 7 subjects (red). First central fixations for each subject were excluded. From left to right, saliency map model of Itti et al. (SM), saliency map with the VJ face detection map (SM+VJ), the graph-based saliency map (GBSM), and the graph-based saliency map with face detection channel (GBSM+VJ). Red dots correspond to fixations. Lower panels depict ROC curves corresponding to each map. Here, GBSM+VJ predicts fixations best, as quantified by the highest AUC.
 Across all 176 images, this trend prevails (Fig. 5): first, all models perform better than chance, even over the 28 images without faces. The SM+VJ model performed better than the SM model for 154/176 images. The null hypothesis to get this result by chance can be rejected at p &lt; 10  X  22 (using a coin-toss sign-test for which model does better, with uniform null-hypothesis, neglecting the size of effects). Similarly, the GBSM+VJ model performed better than the GBSM model for 142/176 images, a comparably vast majority ( p &lt; 10  X  15 ) (see Fig. 5, right). For the 148/176 images with faces, SM+VJ was better than SM alone for 144/148 images ( p &lt; 10  X  29 ), whereas VJ alone (equal to the face conspicuity map) was better than SM alone for 83/148 images, a fraction that fails to reach significance. Thus, although the face conspicuity map was surprisingly predictive on its own, fixation predictions were much better when it was combined with the full saliency model. For the 28 images without faces, SM (better than SM+VJ for 18) and SM+VJ (better than SM for 10) did not show a significant difference, nor did GBSM vs. GBSM+VJ (better on 15/28 compared to 13/28, respectively. However, in a recent follow-up study with more non-face images, we found preliminary results indicating that the mean ROC score of VJ-enhanced saliency maps is higher on such non-face images, although the median is slightly lower, i.e. performance is much improved when improved at all indicating that VJ false positives can sometimes enhance saliency maps. In summary, we found that adding a face detector channel improves fixation prediction in images with faces dramatically, while it does not impair prediction in images without faces, even though the face detector has false alarms in those cases. First, we demonstrated that in natural scenes containing frontal shots of people, faces were fixated on within the first few fixations, whether subjects had to grade an image on interest value or search it for a specific possibly non-face target. This powerful trend motivated the introduction of a new saliency Figure 5: Performance of SM compared to SM+VJ and GBSM compared to GBSM+VJ. Scatter-plots depict the area under ROC curves (AUC) for the 176 images in which VJ found a face. Each point represents a single image. Points above the diagonal indicate better prediction of the model including face detection compared to the models without face channel. Blue markers denote images with faces; red markers images without faces ( i.e. false positives of the VJ face detector). His-tograms of the SM and SM+VJ (GBSM and GBSM+VJ) are depicted to the top and left (binning: 0.05); colorcode as in scatterplots. model, which combined the  X  X ottom-up X  feature channels of color, orientation, and intensity, with a special face-detection channel, based on the Viola &amp; Jones algorithm. The combination was linear in nature with uniform weight distribution for maximum simplicity. In attempting to predict the fixations of human subjects, we found that this additional face channel improved the performance of both a standard and a more recent graph-based saliency model (almost all blue points in Fig. 5 are above the diagonal) in images with faces. In the few images without faces, we found that the false positives represented in the face-detection channel did not significantly alter the performance of the saliency maps  X  although in a preliminary follow-up on a larger image pool we found that they boost mean performance. Together, these findings point towards a specialized  X  X ace channel X  in our vision system, which is subject to current debate in the attention literature [11, 12, 32].
 In conclusion, inspired by biological understanding of human attentional allocation to meaningful objects -faces -we presented a new model for computing an improved saliency map which is more consistent with gaze deployment in natural images containing faces than previously studied models, even though the face detector was trained on standard sets. This suggests that faces always attract attention and gaze, relatively independent of the task. They should therefore be considered as part of the bottom-up saliency pathway.

