 attracting much attention lately due to the rapid growth of World Wide Web. We can consider dimension reduction algorithms in two categories: feature extraction and feature selection. An import ant problem remains: it has been difficult to integrate these two algorithm categories into a single framework, making it difficult to reap the benefit of both. In this paper, we formulate the two algorithm categories through a unified optimization framework . Under this framework, we develop a novel feature selection algorithm called Trace Oriented Feature Analysis (TOFA). The novel objective function of TOFA is a unified framework that integrates many prominent feature extraction algorithms such as unsupervised Principal Component Analysis and supervised Maximum Margin Criterion are special cases of it. Thus TOFA can process not only supervi sed problem but also unsupervised and semi-supervised problems. Experimental results on real text dataset s demonstrate the effectiveness and efficiency of TOF A. important role in the area of information retrieval and Web search. To process the Web scale text data, dimension reduction algorithms [17] are attracting more and more attention. One reason is that dimensi on reduction algorithms can significantly decrease the computational cost of text categorization and, at t he same time, preserve or increase the classification performance. However, many traditional dimension reduction algorithms are difficult to apply on toda y X  X  large-scale text data due to their high complexity. classified into feature extraction and selection algorithms [3]. The feature extraction algorithms s uch as Latent Semantic Indexing (LSI) [2] reduce data dimension by projecting the high dimensional data i nto lower space through algebraic transformations. However, the complexity of feature extraction algorithms is often too high to be applied on large -scale text processing tasks. On the other hand, fea ture selection algorithms such as Information Gain (IG) and CHI [20] are widely used in the area of text proces sing due to their efficiency. However, finding the globa l optimal solutions is an NP problem and as a result, greedy strategies are often used to search a subopt imal feature group. In this paper, we propose a novel integrated feature selection algorithm, which is ca lled as Trace Oriented Feature Analysis (TOFA), based on a unified framework for both the feature selection and extraction algorithms. We show that the objective function of TOFA integrates many previously diversion solutions for feature extraction problems and its solution is optimal according to its objective function. extraction algorithms as an optimization problem in a continuous solution space. We show that different feature extraction approaches are determined by different objective functions in this optimization framework. Under this framework, we propose a novel unified objective function. We prove that the commonly used trace-oriented feature extraction algorithms such as Principal Component Analysis (PCA) [8] and Maximum Margin Criterion (MMC) [10] are special cases of our proposed unified objective function. An important consequence is that under th is unified objective function, the problem of selectin g a good algorithm for finding lower dimensional featur e representation is transformed into the problem of tuning a weight in the objective function. Through this unified objective function, TOFA can process not on ly supervised but also unsupervised and semi-supervise d dimension reduction problems. that it allows an efficient way to find optimal sol utions for feature selection. Traditionally, feature extra ction algorithms optimize objective function in the continuous solution space using Singular Value Decomposition (SVD) [16] incurs a computational complexity of O ( d 3 ), where d is the data dimension. This high complexity algorithm is not applicable on large-scale text data. Note that the feature select ion problem can also be treated as an optimization prob lem for some objective functions in a discrete solution space. Motivated by this intuition, we propose to optimize our unified objective function in the disc rete solution space. Thus by optimizing the objective function of the corresponding feature extraction algorithms in the solution spaces of feature select ion algorithms, we obtain a novel feature selection algorithm, TOFA, which is both of optimal quality a nd high efficiency. formulating the feature extraction algorithms as optimization problem in continuous solution space, we give a unified feature extraction objective functio n, whereby many commonly used previous work are special cases of this unified objective function; ( 2) by formulating the feature selection algorithms as the optimization problem in a discrete solution space, we propose a novel feature selection algorithm by optimizing our proposed unified objective function in the discrete solution space; and (3) through integr ating the objective function of feature extraction and so lution space of feature selection, our proposed feature selection algorithm can find the optimal solution according to the unified objective function. Experimental results on real text datasets show the effectiveness and efficiency of TOFA for text categorization. Section 2, we introduce some related work for dimension reduction. In Section 3, we formulate the state of the art feature extraction and selection algorithms and unify them under the same optimizati on framework. In Section 4, we propose the unified feature extraction objective function and formally formulate our problem under an optimization framework. In Section 5, we give develop algorithm for optimizing the unified objective function in th e discrete solution space. In Section 6, we show the experimental results. In Section 7, we give the conclusion and future work. algorithms are generally classified into Feature Extraction and Feature Selection algorithms [3]. Feature extraction algorithms aim to reduce data by projecting the high dimensional data to lower dimensional space through algebraic transformations . The classical feature extraction algorithms could b e classified into linear and nonlinear algorithms. Th e linear algorithms, such as Latent Semantic Indexing (LSI) [2], Principal Component Analysis (PCA) [8], Linear Discriminant Analysis (LDA) [3], Maximum Margin Criterion (MMC) [10] and Orthogonal Centroid Algorithm (OCA) [7], project the data by linear transformations according to some criterion. Note the traditional LDA has the singularity proble m and its subspace dimension is limited by the class number. LSI is mathematically similar to PCA. Thus in this work we mainly study PCA, MMC and OCA instead of LDA and LSI. On the other hand, nonlinea r algorithms, such as Locally Linear Embedding (LLE) [13], ISOMAP and Laplacian Eigenmaps, etc [14] project the data by nonlinear transformations while preserving certain local information according to s ome criterion. It is hard to use the nonlinear algorith ms on text data due to their very high computational complexity. Though the linear approaches could be computed much more efficient than nonlinear ones, they still cannot endure the high complexity lead b y large data scale nowadays. feature selection algorithms are generally used on large scale text data [20]. They aim to search a subset o f the most representative features according to some criterion. In other words, the features subsets are ranked according to their predictive power. The fea ture selection algorithms are consisted of two algorithm categories: the supervised algorithms and unsupervi sed algorithms. The supervised feature selection for te xt categorization is widely studied. For instance, Information Gain (IG), CHI, and Mutual Information (MI) [20] are all commonly used ones. The comparative study [20] tells us that IG is one of t he most effective supervised text feature selection algorithms. However, the feature subset selection problem is always treated as a search in a hypothes is space which is a NP problem. Thus IG is generally computed by greedy strategy. The Orthogonal Centroi d Feature Selection (OCFS) [18] can provide optimal solution according to its objective function. We wi ll show that OCFS is a special case of our proposed TOFA algorithm in this work. Though the supervised feature selection algorithms have been deeply studi ed, the unsupervised algorithms [11] are not attracting as much attention as supervised ones. We will show tha t through tuning a weight, TOFA is also applicable on unsupervised case, semi-supervised problems and supervised cases with insufficient training data. framework for dimension reduction, under which to review some commonly used feature extraction and feature selection algorithms. Mathematically, a tex t document is treated as a vector using the bag of wo rds (BOW) model [1], where all the documents are transformed into numerical vectors by the Term Frequency Inversed Document Frequency (TFIDF) indexing [1]. Mathematically, a corpus of documents is represented by a d n  X  real matrix d n X R  X   X  in the BOW model, where n is the number of text documents and d is the number of terms appeared in this corpus. Each document is denoted by a column vector , 1, 2, , x i n = L in X. The th j entry of i x is denoted by x j d = L . linear projection , 1, 2, , T p i i y W x R i n =  X  = L can reduce the dimension of text vectors from d to p , where projection matrix W is obtained by optimizing an objective function ( ) J W subject to the constraint that W H  X  , where { | } d p T fe H W R W W I  X  =  X  = is the solution space of the feature extraction problem. Here the matrix p p I R  X   X  is an identity matrix. Note that the entries of W can be any real value, thus fe H is a continuous solution space . Let ( ) fe J W stands for any feature extraction algorithm X  X  objective function, using which the feature extraction algorithms can be formulated as, used feature extraction algorithms, PCA, MMC and OCA using the above-mentioned optimization framework in formula (1). 3.1.1. Principal Component Analysis. PCA, which is an unsupervised feature extraction algorithm, aims at finding a p -dimensional subspace whose basis vectors correspond to the directions with maximal variances. documents, the covariance matrix of these text documents is function of PCA can be formulated as
J W tr W CW = , where 1 { } n i ii tr C c = =  X  means the trace of the matrix. The computational cost of PCA mainly comes from Singular Value Decomposition Thus the solution of PCA is, 3.1.2. Maximum Margin Criterion. Maximum Margin Criterion (MMC) [10] is a recent proposed supervised algorithm. Suppose there are k different classes in the data collection, using k j and n represent class j and the number of sample data in class j respectively, j =1,2,... k , then the centroid vector of the j th class is Similar to LDA, the objective function of MMC consists of an Inter-class scatter matrix and an Intra-class scatter matrix In the projected low-dimensional space, MMC aims at making the documents of different classes as far as possible and those in the same class as close as possible. The objective function of MMC is J W tr W S S W =  X  and thus the solution of MMC is, 3.1.3. Orthogonal Centroid Algorithm. The Orthogonal Centroid Algorithm (OCA) is also a supervised algorithm. Tests showed that it is an effective algorithm for text classification problem [7] using a QR matrix decomposition. However, the time and space cost of QR decomposition cannot meet the efficiency requirements of large-scale data process ing. Lemma 1 [6] below shows that the OC algorithm can also be formulated using the optimization framework . algorithm equals to the solution of the following optimization problem, [6]. From (2), (3) and (4) we can see that PCA, MMC and OCA are all aiming at maximizing the trace of different matrices. Thus we can call these linear f eature extraction algorithms the trace-oriented approaches . feature selection algorithms have been widely used on large scale data due to their efficiency. Unlike f eature extraction algorithms which aim to find a combinati on of original features, feature selection algorithms aim at finding out a subset of the most representative features according to some objective functions. In this sect ion, we formulate the feature selection problem as an optimization problem as well, but in a discrete sol ution space. dimensional vector. Suppose that we want to select the second and the fourth features to reduce i x from five to two dimension, we can select the features throug h multiplying a binary matrix 5 2 W R  X   X  . Let we have T T 2 4 ( , ) i i i x x W x = . From this example we can see that for any d dimensional feature vectors, if we have determined that a certain group of p features are to be selected, we can reduce the d dimensional data to p dimensional space by multiplying a binary matrix  X  . The binary projection matrix W satisfies two constraints: (a) each column of W has one and only one non-zero element; (b) each row of W has at most one non-zero element. We define the solution space of feature selection in Definition 1. selection problem is, H W B B W  X  =  X  = . changed projection matrix will select the same grou p of features. In the above example, if we exchange samples are exchanged, this corresponds to a rotati on of the Euclidean space which will not affect the distance between vectors. In such cases, we can tre at the column exchanged projection matrix in the same way in this work. Under this condition, a group of selected features will determine a unique projectio n matrix. Similarly, a projection matrix will determi ne a unique group of features. In other words, selecting the optimal group of features is equivalent to finding the optimal projection matrix in fs H . which group of features should be selected. This is always achieved by optimizing some objective problem can be formulated as finding an optimal binary projection matrix, [20], Information Gain (IG) has been shown to be on e of the most effective feature selection algorithms for text classification. Suppose we select a group of features as T , the information gain of T is, corresponding conditional probability. The aim is finding a group of features T among all the d original features which can maximize the information gain ( ) IG T . Since there has one to one map between T and W H  X  , we can write ( ) IG T as ( ) IG W or directly rewrite it as ( ) IG J W . Thus the Information Gain (IG) based feature-selection algorithm can be formulated as, is NP problem. Thus, many work have utilized greedy approaches to find suboptimal solutions on large sc ale text data. solved by proposing a novel unified objective funct ion. As demonstrated by Formula (1), the feature extract ion algorithms aim to maximize some objective functions in continuous solution space fe H . Alternatively, Formula (5) shows that feature selection algorithms aim to maximize some objective functions in discret e (binary) solution space fs H . These two sets of algorithms are complementary. An interesting question we try to answer is how to integrate these solutions together. been proved to be very effective in different real applications. However, they are not suitable to tex t data due to their high complexity. To make the objective functions ( ) fe J W usable for the large-scale data, we propose to optimize the objective function
J W in the discrete solution space fs H . This corresponds to a novel feature selection algorithm. Table 1 explains the intuition and distribution of the problems and solutions for the two spaces. 
Table 1. Novel feature selection algorithm in 
Algorithm category 
Objective function 
Solution space 
J W separately, in this work we treat all three as the same optimization problem. Lemma 2 below informs us of the relationship among these three algorithms. The detailed proof of this lemma is in [5]. the objective functions of linear feature extractio n algorithms are, Matrix traces, we call them the trace-oriented feature extraction algorithms . We will show that they can easily achieve the optimal solution in the discrete solution space. Formula (8) is the integrated formu la, clear that if 0  X  = , ( ) ( )
J W J W = . In the past, many work argued that supervised dimension reduction is more suitabl e to classification problem. However, some researchers [ 12] argued for many cases where the unsupervised dimension reduction algorithms are more suitable fo r classification problems. In our proposed objective function (8), all these historical arguments can be put to rest by adjusting the parameter  X  . In addition, most previous work aim at studying the three special cas es, when 0,1  X  = and 2, separately. We integrate them to the same objective function which is suitable to no t only supervised but also unsupervised and semi-supervised learning. As a conclusion, in this work we aim to solve the problem, in discrete solution space fs H , the optimization algorithm should satisfy the following requirements : (a) the solution is globally optimal according to the objective function (8); (b) the algorithm is one pa ss, i.e. it only need to scan each data sample once such tha t the solution could be computed incrementally; and ( c) the complexity should be linear with both the dimension of data and the number of data samples. T he detailed algorithm is derivate below. Suppose the objective function (8) is transformed to, Defining a score for each l w , 1, 2, , l p = L , 
Score w w m m w x m The objective function is reformulated as, means maximize Considering a binary vector set % % L with d elements. Using represent the th j entry of i w % , we constrain the vector set W % such that, vectors for constructing fs W H  X  . Any fs W H  X  is constructed by different vectors in W % . 
W H  X  , all column vectors within W must belong to W % . On the other hand, (11) tells us that each vector in W % has one and only one nonzero entry, thus any % can be used to construct set of all possible vectors for constructing fs W H  X  . Note the constraint (b) of definition 1, each row o f W has at most one none-zero entry. This requires that each can appear in the same W at most once. This ends the proof. (9) is to select p different elements in W % which can maximize (10). We compute scores for all l
Score w w m m w x m Theorem 1 tells us that in order to maximize (10), what we should do is just to select the p vectors with largest scores in W % . formula (10) is the projection matrix fs W H  X  , which consists of p largest-scored vectors in W % . of W are vectors in W % and each vector in W % can appear in the same W at most once. Without loss of generality, suppose that Thus, function (10) will decrease its value. In other wor ds, possible cases under Assumption (12). This complete s the proof. discrete solution space fs H , we only need to compute d scores ( ), l Score w % 1, 2, , l d = L . Then we can select the p vectors with largest scores to construct the proje ction matrix fs W H  X  . Based on Theorem 1, this projection matrix is optimal according to objective function J W . automatically is still a remaining issue for many s tate-of-the-art feature selection algorithms. The featur e extraction algorithms such as PCA use an energy function to suggest the optimal features number. Motivated by this, we define the energy function of
S . The energy of a matrix is defined by the summation of its eigenvalues [8]. Note a commonly used lemma in algebra can be used here:  X  , suppose that its eigenvalues are 1 2 , , , d  X   X   X  L , the summation of eigenvalues equals to the trace of matrix A , i.e. 1 { } d i i tr A  X  = =  X  . 
S as sorting the features in a decreasing score order, t he energy function is defined through the following procedure. threshold value such as T =60% which corresponds to the proportion of energy to be preserved after the feature selection procedure is applied. We can get the optimal number of features * p by: features will be selected. To utilize this strategy , we require the matrix to be positive semi-definite, th us it only suitable to process the case in which 1  X   X  . 1. Note that for the online computation considerati on, we summarize the algorithm in a one pass incremental manner. The algorithm in the simple batch manner is ignored in this paper since it is easy to be summari zed according to Section 5.1. algorithm in the incremental manner, i.e., it only need to scan the data corpus once. This property allows TOFA to be applied on streaming data. The main computational cost of this algorithm is the calcula tion of feature scores. We need to compute d scores and thus the computational cost is linear with the orig inal dimension of training data. Each feature score is calculated by the sum of n squared values. Thus, the cost is also linear with the number of training sam ples. It has been proved that OCFS, which is a special ca se of our proposed algorithm by selecting 1  X  = , is more efficient than both IG and CHI. Though the TOFA algorithm is slower than OCFS, its complexity is th e same to the greedy algorithms. In other words, TOFA can achieve the optimal solution by an algorithm whose complexity is the same as greedy algorithms. In addition, when the training data are not sufficient , the supervised IG and OCFS all will decrease their performance. TOFA will get much better performance since it can handle both supervised-, unsupervised-and semi-supervised cases by adjusting  X  . with eight 2GHz Intel Pentium Processor and 10Gb Memory. The dimension reduction algorithms are written by C# (Visual Studio 2005). The traditional IG is also used for comparative study. The first is the bydate version 20 Newsgroups data 1 . It consists of Usenet articles Lang collected from 20 different newsgroups and includes an overall number of 20000 documents.  X  X xcept for a small fraction of the articles, each document belongs to exactly one newsgroup. X  In the "bydate" version of data, the training and testing data are split previously by t he data provider. There are totally about 60% training documents and 40% testing documents. The second dataset is Reuters Corpus Volume 1 (RCV1) [9] dataset. We choose the data samples with the highes t four topic codes in the  X  X opic Codes X  hierarchy, i. e. CCAT, ECAT, GCAT, and MCAT. 2 . There are 23,149 documents in the training set and 781,256 documents in the testing set. The last dataset is the first l ayer Open Directory Project (ODP) 3 . It includes 1,733,500 documents and 7,655,415 features. We use 5 folds cross validation for the evaluation of ODP. In this paper all datasets are represented by the TFIDF indexing in Bag of Words model. [4]. Muti-labeled training data are copied for all corresponding clas ses. features for dimension reduction by training data. We use the reduced training data to train a Support Ve ctor Machine (SVM) classifier. In this work, we modify t he SVMlight classifier [15] to process the multiple cl ass classification problem. After that we select the sa me group of features to reduce the dimension of testin g data. Finally, we use the trained SVM classifier to classify the reduced testing data. The performance of feature selection algorithms for text categorizatio n is evaluated by F1 which is widely used for classific ation evaluation. since it shows similar results and can draw the sam e conclusion with Macro F1 in our experiments. The Micro F1 is represented by Precision (Pre) and Reca ll (Rec), where classes. 
Input: data samples for training 
Initialize global data centroid m and class centroid m j= 1,2,...,k by d dimensional zero vectors. 
Initialize class sample numbers j n to zeros, j = 1, 2, ..., k . for i = 1, 2, ..., n end for 
Sort the feature scores in a decreasing order and find the p largest ones , 1, 2, , l w l p = L Construct the projection matrix 1 2 ( , , , ) p W w w w =
Output: 1 2 ( , , , ) p W w w w = L parameters, we first set 0,1  X  = and 2 respectively in TOFA X  X  objective function. To save space, we only show the results of 20NG data in Figures 1 since th e same conclusion can be drawn from RCV1 and ODP datasets. In this figure, the x-axis stands for the number of selected features and the y-axis stands for the Micro-F1. The numbers 0, 1 and 2 in the legend stand for Figure 1. Approach evaluation on 20NG data. TOFA, involves the intra-class information, its performance is incomparable; In other words, only considering the inter-class scatter by selecting 1  X  = is better than considering both inter-and intra-clas s scatters averagely ( 2  X  = ) on these text datasets. (2) did not use any label information for training. Its performance is comparable when the selected feature number is large enough; (3), similar to [18] report ed, the performance of IG can approximate TOFA when is not optimal for TOFA. Through tuning the paramet er  X  , we can easily achieve additional improvements for F1 measurement. To show this, we reduce all dataset s to the same dimension (100-d) by TOFA with differen t parameter values. The results are shown in Table 2. 20NG 0.42 0.72 0.63 0.72 0.75 RCV1 0.68 0.75 0.72 0.74 0.77 ODP 0.31 0.41 0.11 0.33 0.49 cases and IG, the 1  X  = is always the best. However, through finding the best  X  , we can give 3% to 9% improvements on these datasets. To find the best  X  , we plot the F1 on all  X  values in Figure 2. The number of selected features in Figure 2 is 100. It can be see n that for 20NG, the best  X  is 1.1; for RCV1 it is 1.2; and for ODP, it is 0.2. 
Figure 2. Parameter effectiveness of different both unsupervised algorithm and supervised algorith ms, TOFA is not only used to improve F1 value for gener al text categorization problems, but also used to some special cases. In many real tasks we may face the condition that only a small percentage of data are labeled due to the highly cost of data labeling. We first consider an insufficient-labeled training data case . To meet this condition, in the training data of RCV1, we randomly select 10% data as labeled data and the la bels of other training data are deleted. We re-do the sa me experiments for 5 times and the results in Figure 3 are the average of the five runs. features. From this figure we can see that the supervised IG and TOFA with 1  X  = can not work as well as the general cases with sufficient training data. In contrast, the unsupervised TOFA with 0  X  = can improve about 5% F1 and 0.2  X  = can improve about 7% F1. In real tasks, there may have even worse condit ions. For instance, the training data maybe unbalanced. I n other words, some classes have sufficient training data while others have only a few or even no training da ta. In the top four classes of RCV1 training data, we preserve 5%, 10%, 30%, 55% labels for each class respectively. Other training data are treated as unlabeled data. TOFA treat this as a semi-supervise d learning problem. Here the semi-supervised learning means we use labeled training data to compute use all training data to compute C .The classification results are shown in Figure 4. and 0.2  X  = can improve the F1 from 13% to 25% in classification. Note this result is also average of five runs. 
Figure 3. TOFA for insufficient training data 
Figure 4. TOFA for unbalanced training data give the average CPU runtime for feature selection o n the three sets of training data. If 1  X   X  , the time cost of TOFA with different parameters is very similar in o ur experiments. Thus we give three groups of average time in Table 3: the time used by IG, by TOFA 1  X  = and by average TOFA 1  X   X  . From this table we can see that the TOFA with 1  X  = , which only needs to compute the trace of inter-class scatter matrix, is the most efficient one. Though the TOFA with other parameter values is not as efficient as 1  X  = , it is still faster than IG. Algorithm IG TOFA 20NG 20.13 6.01 19.23 RCV1 126.33 38.58 103.13 ODP 3,201.91 613.13 2,654.66 number of selected features and the feature selecti on Energy (Equation (14) in Section 5.1), we still tak e the RCV1 data, whose size is between 20NG and ODP, as example. In Figure 5, the x-axis shows the preserve d energy after feature selection and the y-axis shows the number of selected features. We can see that most o f the energy is in the leading features for both 1  X  = and preserve the same energy value by selecting much fewer features. This is one of the reasons why 1  X  = can outperform 0  X  = in low dimensional space (See Figure 1). To preserve 60% energy, in the RCV1 data , only about 2000 features for 0  X  = and about 500 features for 1  X  = will be sufficient. Through our experiments, 60% energy can make the F1 value large r than 0.8 on RCV1 data. Figure 5. Energy versus feature number for effective framework. Through tuning  X  , it can take advantage from both the supervised algorithm and th e unsupervised algorithm. 1  X  = is always the most efficient one among various algorithms involved in this paper. Through our observation, 0  X  = , which needs no class label information for training, can be as effective as supervised algorithms for classificati on when its selected feature number can make the energy beyond 60%. The small  X  is suitable to semi-supervised learning which can solve the insufficien t-labeled training data and unbalance-labeled trainin g data problems effectively. We will study how to evaluate the best  X  from the data distribution in our future work. consist of feature extraction algorithms and featur e selection algorithms. In this work, we integrated t he two algorithm categories by formulate them in an optimization framework. We proposed a novel feature -selection algorithm by optimizing the feature extraction objective function in the solution space of feature selection algorithms. Instead of optimizing the objective functions of the feature extraction algor ithms separately, we developed a unified objective functi on. We proved that many commonly used algorithms in several previous work are special cases of our proposed unified objective function. In contrast to many of the previous greedy feature selection algorithms, our novel proposed feature selection algorithm can achieve the optimal solution accordin g to its objective function. As demonstrated by our analysis and experiments, TOFA is also very efficie nt and flexible. In other words, TOFA is suitable for large scale text data and it can handle the unsupervised and semi-supervised cases which cannot be solved by traditional algorithms such as IG. In the future, w e will analyze the relationship between data distribution and parameter  X  . And then we will give a way to select  X  , i.e. select an optimal objective function according to the data distribution without cross validation. 
