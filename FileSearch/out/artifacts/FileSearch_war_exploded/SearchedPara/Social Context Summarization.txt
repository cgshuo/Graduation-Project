 We study a novel problem of social context summarization for Web documents. Traditional summarization research has focused on ex-tracting informative sentences from standard documents. With the rapid growth of online social networks, abundant user generated content (e.g., comments) associated with the standard documents is available. Which parts in a document are social users really caring about? How can we generate summaries for standard documents by considering both the informativeness of sentences and interests of social users? This paper explores such an approach by model-ing Web documents and social contexts into a unified framework. We propose a dual wing factor graph (DWFG) model, which uti-lizes the mutual reinforcement between Web documents and their associated social contexts to generate summaries. An efficient al-gorithm is designed to learn the proposed factor graph model. Ex-perimental results on a Twitter data set validate the effectiveness of the proposed model. By leveraging the social context information, our approach obtains significant improvement (averagely +5.0%-17.3%) over several alternative methods (CRF, SVM, LR, PR, and DocLead) on the performance of summarization.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; H.2.8 [ Database Management ]: Data Mining; J.4 [ Computer Applications ]: Social and Behavioral Sciences Algorithms, Experimentation Document summarization, Social context, Factor graph, Twitter
Web document summarization has been widely studied for many years. Existing methods mainly use statistical or linguistic infor-mation such as term distribution, sentence position, and topics to extract the most informative sentences from standard (Web) docu-ments. However, these methods only consider the document X  X  con-tent information, but ignore how users (readers) think about the document. With the rapid growth of online social networks, users can freely express what they are thinking about any Web document. For example, many news websites allow the users to directly add comments to each news document. On Twitter 1 , many users post the URL address of a news document onto their microblogs, fol-lowed by some personal comments. The comments imply the im-portance of different sentences and can be used to help improve the quality of document summarization. More importantly, the users X  comments essentially reflect which part of the document that they are interested in.

In this work, we present a novel problem of social context sum-marization . The question we intend to answer is: how to generate a summary for Web documents by considering both the informa-tiveness of sentences and interests of social users? The concept of  X  X ontext X  for summarization has been previously studied and vari-ous approaches have been proposed based on different kinds of con-text, such as hyperlinks [1, 9], click-through data [29], comments [13, 19], or opinionated text [26, 11, 15]. Most of these methods directly integrate the context information into the target webpage to help estimate the informativeness of sentences. However, in this way, the context information is only considered as textual informa-tion. Many important information has been ignored. For example, if a user is an opinion leader, his comments should be more im-portant than others. From a comment X  X  perspective, if a comment has been forwarded or replied by many other users, the comment should be more important than others. One goal of this work is to consider the social influence and the information propagation for document summarization. The problem is referred to as social con-text summarization. The problem is clearly different from existing research and poses a set of unique challenges: h ttp://www.twitter.com, a microblogging system.
In this paper, we try to systematically investigate the problem of social context summarization for Web documents. We formu-late the problem of social context summarization and propose a dual-wing factor graph (DWFG) model. The DWFG model incor-porates the summarization task and the social network analysis into a unified framework. In this way, the two tasks can be mutually re-inforced. We employ the Microblogging as an example to quanti-tatively study the social context summarization problem. In partic-ular, we crawl a data set from Twitter. The user generated content is the tweet posted by the user. The retweeting (forwarding) and replying relationships between tweets form an implicit information network, and the following relationships between users form the user network. Some tweets have the links pointing to standard Web documents (such as news documents). The problem then becomes how to leverage both the information network and the user network to generate high-quality summaries for standard Web documents.
The overview of the proposed method is a supervised framework (as shown in Figure 1). In training, we estimate the importance of defined features and strength of dependencies for identifying key sentences and microblogs in the social contexts. In test, given a new Web document with its social context, we perform collec-tive inference for the importance of each sentence and microblog and select a subset of sentences as the summary according to the trained models. We validate the proposed approach on the Twit-ter data set. The experiment results show that by leveraging the social context information, our approach can significantly improve (on average +10.8%) the performance of summarization. We also compare with a set of alternative methods (i.e., CRF, SVM, LR, PR, and DocLead), and our approach clearly outperforms (aver-agely +5.0%-17.3%) the baseline methods.

The paper is organized as follows: in Section 2, we introduce some notations and formally define the problem. In Section 3, we propose the factor graph model to address the problem, and in Sec-tion 4, we conduct the experiments on the Twitter data set. Finally, in Section 5 and 6, we summarize related works and conclude.
In this section, we introduce some notations and representations necessary for the problem, and then define the problem of social context summarization.

Definition 1 (Social context). Given a Web document d , its social context C d is defined as h M d , U d i , where M d comments on d written by users U d in a social network.
In the context of Web 2.0, Web documents, e.g. news or blogs, are freely discussed and commented by users. These comments again spread (e.g., by forwarding between friends) in the social network. The users X  activities implicitly reflect the importance of different parts (e.g., sentences) in the document from the user X  X  per-spective. We believe that the social context, thus, integrating the document content information and the social context information can disclose a more thorough view of the document. In this paper, we employ Twitter as the basis for our study. Specifically, given a Web document 2 d and its associated social context (tweets M taining the URL address of document d and users U d who posted those tweets), we give the following definition of social context augmented network.
 Definition 2 (Social Context Augmented Network, SCAN).
 Social Context Augmented Network G d = ( S d , C d , E d ) is defined as a network that is built upon the sentence set S d of document d and its social context C d , where the edge set E d contains three types of edges: E s d , E m d , and E u d . E s d = { ( s represents the relationships between document sentences, E { ( m i , m j ) | m i , m j  X  M d } represents the relationships between tionships between users.

Compared with traditional contexts that are defined based on textual information, social context need incorporate various dy-namic social relationships, such as the follower-followee relation-ships between users, retweeting relationships and replying relation-ships between tweets. An example of SCAN is shown in Figure 2(a). In this figure, the upper layer includes two documents d and d 2 , and d 1 contains four sentences s 1 , s 2 , s 3 documents are respectively associated with two sets of messages M 1 = { m 5 , m 6 , m 7 , m 8 } and M 2 = { m 9 , m 10 } in the middle layer. The lower layer refers to the user layer consisting of users u , u 2 and u 3 , who are also associated with the messages M M 2 . Besides external relationships between the objects across dif-ferent layers, SCAN also describes internal relationships between objects within the same layer (as shown in Figure 2(a)). Given this, we can formally define our problem of Social Context Summarization .

Definition 3 (Social Context Summarization). Given a social context augmented network G d , the goal of social context summa-rization is to generate a summary which consists of two pieces of information: the most important sentences S  X  d  X  S d and the most representative messages M  X  d  X  M d .

The problem of social context summarization contains two sub-problems: Key Sentence Extraction and Tweet Summarization . In the former subproblem, we aim to identify the most important sen-tences from document d  X  X  content, while in the latter subproblem, we intend to find the most representative tweets from the social context C d of document d . Social context C d contains rich infor-mation about the document d , which is helpful for the Key Sentence Extraction problem, while the important sentences in a document can equally help Tweet Summarization in the social context. The mutual reinforcement between the two subproblems can facilitate generating a high-quality summary. Moreover, social context sum-marization could also answer a number of related questions, e.g., who are the most experienced users of a specific topic or a fact mentioned in a document.
O n Twitter, a Web document (e.g., news document) is often pointed out by a URL address, which might be in some forms of encoded shortened URLs such as by tinyurl.com and bit.ly. be included in the tweet summary.
In this section, we propose a dual wing factor graph (DWFG) model, which formulates the social context summarization problem in a unified learning framework. The DWFG model simultaneously incorporates all resources in social context to generate high-quality summaries for Web documents.
In our Twitter data set, each Web document is associated with a social context. To generate summaries for Web documents, a straightforward method is to define a set of features to characterize the importance of each sentence, and then use a classification model to identify which sentences should be included into the summary [16, 25, 36]. To further consider the correlation between sentences, we can consider a sequential labeling approach such as conditional random field. Such a method has been studied by [8, 28]. Both of them consider the sentence local features and similarities (correla-tions) between sentences, and model the sentence extraction task with a linear-chain conditional random field. An example of the graphical representation is shown in Figure 2(b). The method only considers the correlation between sentences (the document layer in Figure 2(a)), but ignores the social context information resided in the microblog and user layers.

To model the tweet network, we design another similar graphical model with structures reflecting the information propagation. Fig-ure 2(c) presents an example. Each gray circle indicates a tweet, the arrow represents the replying/retweeting relationship between two tweets. Based on such a formulation, we can define local fea-tures (content-based features) for each tweet, as well as edge fea-tures for each replying/retweeting relationship. By learning such a graphical model, we can classify which tweets are important (or informative). Obviously, this model only considers the information from the tweet side and does not consider the Web documents. An ideal way is to incorporate the two tasks together so that they can reinforce each other.

Based on these considerations, we propose a novel dual wing factor graph (DWFG) model. The graphical representation is shown in Figure 2(d). In the DWFG model, the upper layer is used to model the key sentence extraction task and the bottom layer is designed to model the tweet summarization problem. In the middle layer, we design a set of correlation factor functions h to bridge the two tasks. By carefully designing the correlation factor function h , we can elegantly combine the two tasks of key sentence extraction and tweet summarization into a unified framework. In the rest of this section, we will explain in details how we design and learn the dual wing factor graph model.
We model the social context summarization problem in the dual wing factor graph (DWFG) model. Each sentence s i  X  S d or tweet m i  X  M d is associated with a binary value y i indicating the im-portance of the sentence or tweet (1 representing important, and 0 representing unimportant).

We first collect a set of labeled SCANs (training set) T = { G d } n T d =1 , i.e., each sentence s i  X  S d or tweet m i social context C d is associated with a known binary label y over, we also collect the test set S of unlabeled instances, which consists of all the sentences and tweets not yet judged. Our goal is then to learn a DWFG model from the training set and apply it to predict which sentences and tweets are important in the test set S , i.e., to infer the value (label) of y , and then generate a summary for the social context.

We define three types of factor functions associated with indi-vidual instances or instance groups: local attribute factor , intra-domain dependency factor , and inter-domain dependency factor . Local attribute factor. The probability that a sentence or tweet is important could be estimated by some local attributes (represented as x ), which refer to features that are inherent to the sentence or tweet itself. In general, we define similar features for sentences and tweets. The features include the average TF-IDF score over words and the log likelihood generated by the context, the position of the sentence in the document, the author X  X  authoritativeness, etc. Details of the defined local features for sentences and tweets are given in Section 4.

To estimate the significance of each feature, we introduce a weight variable  X  c for each feature c , and we define a local at-tribute factor f i,c for the feature c of each sentence s Formally, a factor could be defined as the local entropy: where x i,c is the value of the c -th feature extracted from sentence s or tweet m i .
 Intra-domain dependency factor. As described in Section 3.1, we introduce factors that are capable of handling multiple instances in either sentence level or tweet level, to characterize the dependen-cies between sentences and tweets respectively. Intra-domain in-teraction may promote some sentences to become more important while inhibit others from becoming important. We associate each type of interaction with a weight  X  c indicating the confidence of the corresponding interaction. The interaction has a positive influence only if the weight  X  c is greater than 0. We introduce factor g capture the dependency between sentence pair s i and s j or tweet pair m i and m j . g ij,c (  X  c , y i , y j ) = exp  X  c if some condition holds
A document can be regarded as a sequence of sentences, and thus key sentence extraction could be viewed as a sequence label-ing process [28], i.e., the judgment on a certain sentence is affected by the nearby sentences to avoid both sentences of high similarity are chosen simultaneously. Hence, the dependency conditions in Eq. 2 for a sentence pair s i and s j can be formalized as follow: the factor takes value exp  X  c if y i 6 = 1 or y j 6 = 1 . To avoid high computational complexity, we constrain only consecutive and sim-ilar sentences, i.e., establish sentence relation for sentence s s i +1 whose mutual similarity (e.g., cosine similarity) exceeds the threshold  X  g .

Moreover, we consider the two interactions between tweets: re-plying and retweeting. If tweet m i replies or retweets tweet m then m j successfully excites and attracts attentions from others, and it is reasonable that m j is more important than its succeeding tweets in the thread. Formally, for such a tweet pair m i the factor takes value exp  X  c if y i  X  y j .
 Inter-domain dependency factor . By leveraging knowledge from both domains, the inter-domain relationships may benefit to the identification of social context summarization. We introduce a set of factors defined on variables across domains, which are able to coordinate the labels of sentences and tweets simultaneously. Specifically, if tweet m j is considered as a representative tweet, i.e., y j = 1 , then a sentence s i highly similar to m j larity more than a threshold  X  h ) should be biased towards the same label, i.e., y i = 1 . Formally, for each sentence-tweet pair ( s of high similarity, we define where  X  is the weight variable that represents the significance of inter-domain dependency factor.
 Objective function. Finally, the objective function can be defined as the normalized product of Eqs. 1 -3 for all the instances. We denote Z as the normalization factor, which sums up the condi-tional likelihood P ( Y | X,  X ) over all the possible labels of all the instances, where Y contains all the undetermined labels for sen-tences and tweets, i.e., Y = { y i } i , and  X  is the collection of weights, i.e.,  X  = {  X  c } c  X  X   X  c } c  X  X   X  } .

We first estimate the parameters  X  with a maximum likelihood procedure on the training instances, i.e., max
We use L-BFGS, a quasi-Newton method for solving the non-linear optimization problem (i.e., Eq. 4). To avoid overfitting, we add a penalty term  X  1 2 | |  X  || 2 / X  2 , a spherical Gaussian prior, into the objective function, which is a regularization method commonly used in maximum entropy and conditional random fields [6, 27, 28].

Calculating the marginal distribution for each factor (in deriving the log-gradient of the objective function) requires a loopy sum-product inference algorithm. With the learned parameter  X  , we may summarize an unlabeled social context for a Web document in the test set by extracting important sentences, which are also identified by a similar max-sum inference procedure. The inference algorithm is introduced in the next section.
 Connection with existing models. We note that the proposed DWFG model can also be viewed as a model generalized from existing models. In Eq. 4, if parameter  X  is fixed as 0, i.e., all factors { h ij } i,j take constant values of 1, and factors { f and { g ij,c } i,j,c are only defined for sentences, then the simplified model only incorporates sentence local factors and sentence rela-tion factors, and DWFG model is degenerated to a special case: the summarization approach based on linear-chain CRF [28]. More-over, if all parameters {  X  c } c are also set as 0, i.e., only the local factors { i,c } i,c are non-trivial, then DWFG is turned into the logis-tic regression classifier [25].
Since the graphical model DWFG proposed for summarization (cf. Figure 2(d)) contains cycles, we cannot directly employ a forward-backward algorithm like in [28] for exactly inferring the optimal labeling for a test instance. We then propose an approxi-mate inference approach based on the loopy sum-product or max-sum algorithm.

To achieve an approximate inference for predicting labels, the algorithm contains multiple iterations for updating the beliefs, and each iteration is comprised of two phases. Here, we denote the up-date variables for delivering beliefs between variables and factors by { p ij } i,j and { q ij } i,j , where p ij represents the message propa-gating from variable (e.g., y i ) to factor (e.g., g ij,c represents the message propagating from factor to variable. The messages can be formulated as follows: where r i corresponds to the logarithmic value of the local factor, i.e., Analogously, t ij ( y i , y j ) is the logarithmic value of the dependency factor, i.e., Specific to a particular dependency factors, f i,j , g ij,c to 3), the message q ij has a more succinct expression, e.g., the sen-tence dependency factor q ij = max { p ji  X   X  c , 0 } X  max { p
We can obtain the label for each sentence s i or tweet m i the variables calculated in the two phases for the last iteration as follows:
Algorithm 1 : Social context summarization with DWFG input : A document d with social context C d and SCAN G d output : A summary for social context C d : important // initialization calculate { r i } i according to Eq. 7; // update message values for i  X  1 to I do // output result f oreach s i  X  S d and m i  X  M d do
S  X  d  X  { s i  X  S d | y i = 1 } ;
M  X  d  X  X  m i  X  M d | y i = 1 } ;
The learning algorithm is depicted in Algorithm 1. Initially , we calculate all local variables { r i } i according to Eq. 7, and initialize all update variables { q ij } i,j as 0 (Line 1 and 2). Next, we compute new values for all the update variables { p ij } i,j according to Eq. 5. Then we estimate the new values for all { q ij } i,j according to Eq. 6. We continue to update the variables for a number of iterations until some termination condition is satisfied (Line 3 -5). Finally, the summary for the social context is generated according the update variables (Line 6 -9).
 Complexity analysis. If we denote the number of iterations for the inference algorithm as I , then the computational complexity of the algorithm is proportional to I ( | E s | + | E m | + | E | E s | , | E m | , and | E c | correspond to the number of sentence re-lationships, tweet relationships, and inter-domain relationships re-spectively. They can be varied from zero to many when we tune the thresholds  X  g and  X  h , which is further discussed in Section 4 . 2 . 2 . In fact, the inference algorithm can be easily parallelized or dis-tributed onto clusters to handle large-scale data set, and the design of distributed algorithm will be reported elsewhere. In this section, we evaluate the proposed summarization method DWFG with manually labeled documents. We firstly introduce the data set, baseline methods that do not incorporate the relationship between the Web document domain and tweet thread domain, the evaluation metrics, and then we give the detailed discussion of the experimental results with the comparison of other approaches. Re-lated support materials (data description and software) of this work can be found at http://arnetminer.org/socialcontext/.
We aim to find a collection of Web documents with their asso-ciated social contexts, and labeled summaries for each document and its social context, so that we can use the data set as the gold-standard to evaluate different approaches for social context sum-marization. To begin the collection process, we selected the first and also one of the most popular users on Twitter, Jack Dorsey (@jack) 3 , and collected his followers. We took these users as seed t he creator, co-founder and executive chairman of Twitter.
Figure 3: The distribution of URLs carried by the tweets u sers and used a crawler to collect all followers of these users by traversing following edges. We continue the traversing process, which produced in total 4,874,389 users. The crawler monitored tweets posted by these users from January 1st 2010 to July 17th 2010. We extracted all tweets posted by these users and in total there are 404,544,462 tweets.

We use explicit URLs (starting with  X  X ttp:// X  or  X  X ttps:// X ) to identify Web documents from the Twitter data. In some cases, users may shorten the URL addresses via services such as tinyurl.com or bit.ly. We implemented a shorten URL decoder based on a HTTP client to obtain the decoded URLs and finally aligned the obtained URLs. 4 Finally, we use the most frequent (200,000) URLs to con-stitute the collection of Web documents and employ their associ-ated (12,964,166) tweets to construct the social context. The dis-tribution of frequency for these Web documents (URLs) is plotted in Figure 3 in log-logarithmic scale. We see that the distribution of frequency follows the power law. According to the selected URLs, we crawled the pointed webpages, and then constructed two kinds of data sets (webpages and their corresponding social tweets). The Web documents were then segmented into a set of sentences with the jTokeniser Toolkit 5 . Our summarization algorithm was then performed on both domains.

A preliminary analysis shows that many frequent URLs are ad-vertisement pages. To guarantee the quality of the data set, we re-strict ourselves to a few high-quality news websites such as CNN, BBC, Mashable etc., and selected a subset of URLs related to these websites to perform a manual annotation for the summarization. Descriptions of the five selected domains are given in Table 1.
To reduce the possible noise in the manual annotation data, we further manually validated the informativeness of all the selected Web documents by posting both the Web documents and tweets on Amazon Mechanical Turk 6 . We totally issued 1,145 HITs on Mechanical Turk, and for each HIT we asked at least two differ-ent workers to read both Web documents and their corresponding tweets. All the HITs were divided into 12 batches with each assign-ment entitled  X  X ey sentences and tweets extraction from news and related tweets X . We gave a detailed description on how to label the sentences and tweets, and also emphasized that the workers should
A Web document might be referred by different URLs, e.g., http://news.bbc.co.uk/1/hi/england/8604663.stm, http://news.bbc.co.uk/2/hi/uk_news/england/8604663.stm, http://news.bbc.co.uk/2/hi/8604663.stm correspond to the same document. We further group such Web doc-uments according to the unique document ID: 8604663. http://code.google.com/p/jtokeniser/ http://mturk.com, an Internet marketplace to use human intelli-gence to solve various kinds of problems  X  X xtract several sentences from news that attract them mostl y X , and  X  X fter reading the news, extract the most interesting tweets that ap-peal you mostly X . We required the workers to label no less than 5 tweets and 10 Web document sentences. Finally, 158 different users have participated in annotating the benchmark for social con-text summarization task. The labeled sets of sentences and tweets formed the benchmark for evaluation.

In this paper, two performance metrics applied in [29] were adopted to evaluate the proposed approach DWFG. The first is Pre-cision, Recall and F-measure. In the following section, we will report the evaluation on F 1 measure, which is defined as: w here S cand and S ref denote the sentences contained in the candidate summary and the reference summary respectively.

Another performance metric is ROUGE [18], which measures summarization quality according to the overlap between the units, such as n-gram (referred to as ROUGE-N) etc, of machine gener-ated summary and human generated summary. ROUGE-N is de-fined as follows: where n is the length of the n-gram, Count match ( gram n imum number of n-grams co-occurring in a candidate summary and the reference summaries, Count ( gram n ) is the number of n-grams in the reference summaries.

We employ the ROUGE evaluation methods implemented in the Dragon Toolkit 7 , and report the experimental results in terms of ROUGE-1 and ROUGE-2 with stop words filtered out. Since ROUGE is a recall-oriented metric, we keep the number of sen-tences extracted be equal with that of the human summary for fair comparison. Specifically, we select the sentences and tweets with the greatest positive beliefs according to p ( y i = 1 | X ) (cf. Eq. 9).
Many features have been designed for document summarization in prior literatures. In this paper, we only extract 11 basic and straight-forward features from both domains. Besides some fea-tures that are widely used in traditional summarization methods, we also utilize several features extracted from users X  online social behaviors, e.g., the number of users following the tweet X  X  author and the PageRank score of the author. Table 2 gives the brief defi-nitions of these features applied in this paper, where some features were represented by nominal values, e.g., Feature 1 will take value 4 if the sentence was extracted from the title of the document, 3 if h ttp://dragon.ischool.drexel.edu/ Figure 4: Comparison of feature values for sentence domain on fi ve domains it was extracted from the subtitle, 2 if the sentence was located in the first paragraph of the original document, 1 if the sentence was located in the last paragraph, and 0 otherwise.

The feature values extracted from sentence domain and tweet domain are summarized in Figure 4 and 5. Since different features take values in diverse ranges, e.g., the maximum value of Feature 4 is 15, while the maximum value of Feature 6 is 1.495, we normal-ize the feature values by the mean value of corresponding feature. From Figure 4, we can see that Web documents from different do-mains exhibit differently. For example, articles in CNN, BBC, and ESPN have smaller values of Feature 1 but greater values of Fea-ture 2 than MTV, which indicates that news websites CNN, BBC, and ESPN have longer articles consisting of a greater number of shorter paragraphs. Therefore, we trained an individual model on each domain respectively to capture the distinctiveness.
We compare the proposed DWFG model with six supervised baselines methods. SVM classifiers (SVM) and logistic regression classifiers (LR) are performed for each sentence and tweet only with its local features. Linear-chain and tree-structured CRF mod-els (LC-/TS-CRF) are respectively trained and tested on documents and tweet threads, i.e., inter-domain relationships are considered as a supplement to the basic local features. The linear-chain CRF baseline model employed in the sentence summarizaiton is equiva-lent to the method proposed in [28].

We also extend the feature list for each sentence and tweet by considering the features of related sentences or tweets extracted from both domains (denoted as SVM+, LR+). Specifically, for each sentence s i in a document, we append 11 features ( x s i, 7 where each of x s i, 7 , . . . , x s i, 12 adds up the corresponding feature values of its similar sentences, and each of x s i, 13 , . . . , x up the corresponding feature values of its related tweets. Similarly, for each tweet in the thread, we append 11 features, which are the sums of feature values of its relevant sentences or tweets.
In addition, we also compare DWFG with several unsuper-Figure 5: Comparison of feature values for tweet domain on fi ve domains vised summarization algorithms, i.e., the importance sentences and tweets are selected according to a metric or score. First, we ran-domly select sentences or tweets (Random) as the basic unsuper-vised method. Another baseline method for summarization is to select the sentences according to their positions in the document or paragraph (DocLead and ParaLead). Finally, we apply PageRank algorithm for summarization on the whole graph consisting of three types of relationships (PR) [24].
All experiments were conducted in the 10-fold cross validation procedure, where one fold is for test and the other nine folds for training. The performance results are shown in Table 3 and 4, and the best performances in the comparisons are highlighted in bold. In the following results, we set the similarity threshold for sentence dependency  X  g = 0 . 1 , and the similarity threshold for inter-domain dependency  X  h = 0 . 8 . We will further discuss the variation of per-formance with different assignment of thresholds in Section 4.2.2.
From Table 3, we see that DWFG clearly outperforms the base-line methods in most cases in terms of both F 1 and ROUGE-N for document summarization. Moreover, we discover that the per-formances are statistically significantly improved on the MTV and ESPN domains by conducting sign test on the results, where the p values are much smaller than 0.01. In fact, we collect relatively fewer documents and corresponding tweets from MTV and ESPN compared with other domains, and thus, additional dependencies, especially cross-domain dependencies boost the performance by leveraging additional information.

In contrast to the improvements in Web document summariza-tion, DWFG performs comparably to the simpler CRF-based meth-ods for tweet summarization. In fact, the ground truth data are manually annotated from the perspective of readers X  interests and focuses, which naturally reveals the users X  motivations for writing tweets. Therefore, the identification of important sentences from the Web document domain rarely influences the results for identi-fying important tweets.
In this section, we discuss the impact of thresholds  X  g and  X  our proposed approach. Although the proposed approach within a supervised framework can automatically learn the optimal model parameters  X  based on the training instances, we still need to pre-define the thresholds  X  g and  X  h to control the number of inter-domain and intra-domain dependencies in the factor graph model. Specifically, with larger  X  g or  X  h , we obtain fewer dependencies, and if  X  g = 0 , each pair of consecutive sentences will be connected by a inter-domain factor, or if  X  h = 0 , all the sentences will be connected with all the tweets. To evaluate the impact of thres h-olds to DWFG and baseline methods (e.g., LC-CRF), we varied  X  or  X  h from 0 to 1 with step length 0.1 respectively with the other threshold fixed. Due to space limitation, we only report the impact to the performance of DWFG in Figure 6(a) and (b) in terms of F ROUGE-1, and ROUGE-2, and the performances of the baseline methods follow similar trends with different thresholds. We also plot the percentage of consecutive sentence pairs with similarity more than  X  g in Figure 6(a), and the percentage of sentence-tweet relation pairs with similarity more than  X  h in Figure 6(b).
From Figure 6(a), we can see that when  X  g increases from 0.0 to 0.5, the performance drops by 5%  X  16% in terms of F 1 ROUGE, which can be attributed to the lack of a complete view of sentence relations within the document. While with  X  g is 0.7, the performance reaches a local maximum when the retained sentence relations have a relatively higher quality. As shown in Figure 6(b), the performance of sentence identification reaches the global max-imum when  X  h is set between 0.5 and 0.6. With smaller or greater  X  , the extracted relation pairs between sentences and tweets may contain more low-quality relations or lack of high-quality relations. Generally speaking, the performance of important tweet extraction is relatively stable.
We further analyze the contribution or significance of each fac-tor. We show the estimated weights for sentence-level local factors  X  , . . . ,  X  6 on five domains respectively and calculate their aver-ages in Figure 7, and show the estimated weights with their aver-ages for tweet-level local factors  X  7 , . . . ,  X  11 in Figure 8.
From Figure 7, we see that most of the local factors have posi-tive contributions to our task except for Feature 4 (the number of common words to the title). On average, it seems that Feature 5 (sentence length) and Feature 2 (sentence position in paragraph) Figure 6: The impact of  X  g a nd  X  h to the performance of DWFG are the most important local factors for identifying the important sentences. From Figure 8, we see the two most important local fac-tors for identifying the representative tweets are Feature 8 (tweet length) and Feature 7 (average TF-IDF). In fact, we find that long tweets tend to cover both the main ideas of the Web documents and the personal comments towards them.
In this section, we demonstrate an example of the inference step for a specific Web document, an article entitled  X  X omen try to take body on plane at Liverpool airport X  8 , with its social con-text. In Figure 9, the left column lists a portion of sentences of the Web document, and the right column lists a portion of tweets containing URLs (or shortened URLs) directing to the article (the selected texts are indicated by bold font). The established inter-domain and intra-domain dependencies are shown in arrows. Fur-thermore, beliefs propagated from local factors and pair-wise de-pendency factors in the last iteration of our inference algorithm are partly shown with the associated variables taking values of 1 (in rectangles, rounded rectangles or diamonds). Beliefs taking values h ttp://news.bbc.co.uk/2/hi/8604663.stm Figure 7: Parameter estimation results for sentence-level l ocal factors on five domains Figure 8: Parameter estimation results for tweet-level loca l fac-tors on five domains of 0.5 indicate that the corresponding factors have no preference on whether the sentences are regarded as part of the summary or not. Beliefs taking values greater than 0.5 convey positive attitudes, and the greater the belief values, the stronger the confidence that the as-sociated variables should take values of 1. According to the calcu-lated beliefs, the summary for the social context is generated based on the selected sentences and tweets (in bold).

We can see that the local features, e.g., statistical features, still play a major role for social context summarization. For example, since the most common words or phrases in the Web documents in-clude  X  X omen X ,  X  X ead person X ,  X  X ody X ,  X  X iverpool Airport X , and those in tweet threads include  X  X iverpool airport X ,  X  X eekend At Bernie X  X  X , texts that cover these words or phrases are more likely chosen, and the probability that the relevant sentence-tweet pairs are simultaneously selected is boosted. Moreover, various types of relations also come into play. For example, since the last two tweets shown in the right column form a retweet pair, the content is evaluated more important, and thus the related sentence (the fourth sentence) in the document then receives a higher belief (0.51) of taking a positive decision. As we suggested, in the social context summarization task, the tweet thread contributes additional infor-mation (e.g., Weekend At Bernie X  X  9 ) to the original document con-tent, which unveils the users X  interests from an alternative angle.
Web-page summarization techniques have been widely studied for many years and various approaches have been developed. There are two major types of approaches for web-page summarization, i.e., supervised and unsupervised. The supervised summarization approach treats the summarization task as a two-class classifica-tion problem [16, 25, 36] or as a sequence labeling problem [8, 28] at the sentence level, where each sentence is represented by a 1989 American motion picture comedy, which has a similar plot as the news story. a vector of features. Comparably, the unsupervised approach re-lies on a set of heuristic rules to develop the summarization. Web-page summarization can also be either generic or query-dependent. Generic summarization targets to cover the main idea of the page while query-oriented summary is to present the information that is most relevant to the given queries [4, 31].

Without consideration of context, the extracted summary is com-posed of sentences from the Web documents, and thus features from local content of a document is the key to summarization. Tra-ditional document-oriented features can be defined either from lin-guistic, such as rhetorical structure [22], lexical chains [2] or sta-tistical perspectives, such as term significance [20], sentences sim-ilarity [24] and topic detection [12]. Although document-oriented features can disclose most of the basic characteristics of summary sentences, as stated in [29], the textual information of a Web docu-ment may be scarce and diverse in topics and, moreover, contain a lot of noise.

Document-oriented features cannot fully capture the main idea of a Web document. In the past few years, some work starts to utilize various kinds of context to assist document summarization, such as external documents or cited articles [23]. User requirement is one of the most important kind of context [10, 32]. In the study of [21], user X  X  needs come from a set of documents selected by user, where the top content words are extracted according to their G score and then treated as users X  interests. Hyperlinks among web-pages are another kind of context. Based on the text surrounding the hyperlink, summarization of the target webpage can be real-ized either by extracting the related sentences in surrounding text [1] or by extracting significant sentences from the linked webpage [9]. Similar to the hyperlink context, Sun et al. [29] utilize search-engine clickthrough data to construct the extra knowledge. In their work, webpage and query terms collected from the clickthrough data work together to decide the significance of each word in sen-tences for summarization. With the rapid growth of social web-sites, comments-oriented approach is also studied, where the most important comments are selected and leveraged into sentence se-lection for summarization. Traditional feature-based methods and graph-based methods for sentence extraction have been applied for commented sentence selection [13, 33, 19], or opinionated text [26, 11, 15].

Different from previous works, we study to leverage multifaceted social media information for Web document summarization, es-pecially social influence among users [30] and retweeting rela-tions among messages [35]. However, we adopt a totally differ-ent approach to not only incorporate the extra knowledge extracted from microblogs, but also take full advantage of conventional tech-niques in single document summarization. In recent years, the rapid growth of microblogging services provides an efficient way for in-formation communication. Here, people can freely issue various comments on any topic they interested in. Compared with tra-ditional tightly-coupled relationship between Web document and comments, messages from microblogs can provide more valuable information beneficial for summarization. Microblog has been widely studied in recent years. Some work focuses on investigating the characteristics of Twitter, e.g.,[17], [7], [14], while some work analyzes the patterns of retweets on Twitter, influential twitter and the routines of changes of hashtags, etc., e.g., [34], [5], [3], [35]. To the best of our knowledge, little work in the literature has tried to use microblog data for Web-page summarization.
In this paper, we explore a novel problem of social context sum-marization and aim to utilize the mutual reinforcement between Web document and its associated social data to extract high-quality summaries. In our study, the importance of each document sen-tence is firstly predicted by considering a series of local features of a document. At the same time, the social context relating to the Web document is associated with it, in which the significant sentences are also identified by taking advantage of various social factors. We formally define the concept of social context for Web document and propose a unified summarization approach through factor graph model. Our experiments are performed on a set of Web documents and associated microblog messages. The experiment re-sults prove that the proposed summarization method shows signif-icant improvement over the baseline approaches on social context summarization task.
To systematically combine the content analysis and social be -haviors represents a new and interesting direction for information retrieval. There are many future directions of this work. For ex-ample, due to the fact that not only tweets are highly associated with other tweets, users are also connected by the friendship re-lations, we can extend this work by establishing the connection among users and adding the dependencies between users and their tweets. Intuitively, the influence among users will also affect the identification of important tweets, and subsequently influence the importance of sentences in Web documents. The work is supported by the Natural Science Foundation of China (No. 61073073, No. 60703059, No. 60973102), Chinese National Key Foundation Research (No. 60933013, No.61035004), National High-tech R&amp;D Program (No. 2009AA01Z138). [1] E. Amitay. Automatically summarising web sites -is there a [2] R. Barzilay and M. Elhadad. Using lexical chains for text [3] d. boyd, S. Golder, and G. Lotan. Tweet, tweet, retweet: [4] J. Carbonell and J. Goldstein. The use of mmr, [5] M. Cha, H. Haddadi, F. Benevenuto, and K. P. Gummadi. [6] S. F. Chen and R. Rosenfeld. A gaussian prior for smoothing [7] M. Cheong and V. Lee. Integrating web-based intelligence [8] J. M. Conroy and D. P. O X  X eary. Text summarization via [9] J.-Y. Delort, B. Bouchon-Meunier, and M. Rifqi. Enhanced [10] A. D X az and P. Gerv X s. User-model based personalized [11] K. Ganesan, C. Zhai, and J. Han. Opinosis: a graph-based [12] Y. Gong and X. Liu. Generic text summarization using [13] M. Hu, A. Sun, and E.-P. Lim. Comments-oriented document [14] A. Java, X. Song, T. Finin, and B. Tseng. Why we twitter: [15] H. D. Kim and C. Zhai. Generating comparative summaries [16] J. Kupiec, J. Pedersen, and F. Chen. A trainable document [17] H. Kwak, C. Lee, H. Park, and S. Moon. What is twitter, a [18] C.-Y. Lin and E. Hovy. Automatic evaluation of summaries [19] Y. Lu, C. Zhai, and N. Sundaresan. Rated aspect [20] H. P. Luhn. The automatic creation of literature abstracts. [21] I. Mani and E. Bloedorn. Machine learning of generic and [22] D. Marcu. From discourse structures to text summaries. In [23] Q. Mei and C. Zhai. Generating impact-cased summaries for [24] R. Mihalcea. Language independent extractive [25] M. Osborne. Using maximum entropy for sentence [26] M. J. Paul, C. Zhai, and R. Girju. Summarizing contrastive [27] F. Sha and F. Pereira. Shallow parsing with conditional [28] D. Shen, J. tao Sun, H. Li, Q. Yang, and Z. Chen. Document [29] J.-T. Sun, D. Shen, H.-J. Zeng, Q. Yang, Y. Lu, and Z. Chen. [30] J. Tang, J. Sun, C. Wang, and Z. Yang. Social influence [31] J. Tang, L. Yao, and D. Chen. Multi-topic based [32] C. Teng, N. Xiong, Y. He, L. T. Yang, and D. Liu. A [33] X. Wan and J. Yang. Multi-document summarization using [34] J. Weng, E.-P. Lim, J. Jiang, and Q. He. Twitterrank: Finding [35] Z. Yang, J. Guo, K. Cai, J. Tang, J. Li, L. Zhang, and Z. Su. [36] J.-Y. Yeh, H.-R. Ke, W.-P. Yang, and I.-H. Meng. Text
