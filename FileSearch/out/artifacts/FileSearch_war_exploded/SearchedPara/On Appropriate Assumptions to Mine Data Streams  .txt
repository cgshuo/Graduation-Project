
Recent years have witnessed an increasing number of studies in stream mining, which aim at building an accurate model for continuously arriving data. Somehow most ex-isting work makes the implicit assumption that the training data and the yet-to-come testing data are always sampled from the  X  X ame distribution X , and yet this  X  X ame distribu-tion X  evolves over time. We demonstrate that this may not be true, and one actually may never know either  X  X ow X  or  X  X hen X  the distribution changes. Thus, a model that fits well on the observed distribution can have unsatisfactory accuracy on the incoming data. Practically, one can just assume the bare minimum that learning from observed data is better than both random guessing and always predicting exactly the same class label. Importantly, we formally and experimentally demonstrate the robustness of a model aver-aging and simple voting-based framework for data streams, particularly when incoming data  X  X ontinuously follows sig-nificantly different X  distributions. On a real streaming data, this framework reduces the expected error of baseline mod-els by 60%, and remains the most accurate compared to those baseline models.
Classification on stream data has been extensively stud-ied in recent years with many important algorithms devel-oped. Much of the previous work focuses on how to ef-fectively update the classification model when stream data flows in [1, 4, 6]. The old examples can be either thrown away after some period of time or smoothly faded out by de-creasing their weights as time elapses. Alternatively, other researchers explore some sophisticated methods to select old examples to help train a better model rather than just using the most recent data alone [10, 2, 9, 7, 8]. These al-gorithms select either old examples or old models with re-spect to how well they match the current data. Hence, they also implicitly make the assumption that the current train-ing distribution is considerably close to the unknown dis-tribution that produces future data. Among these methods, the weighted ensemble approaches [2, 7, 8, 9] were demon-strated to be highly accurate, when the  X  X tationary distribu-tion assumption X  holds true. Formally, we denote the fea-ture vector and class label as x and y respectively. Data stream could be defined as an infinite sequence of ( x i ,y Training set D and test set T are two sets of sequentially ad-jacent examples drawn from the data stream. The labels in T are not known during classification process and will only be provided after some period of time. The assumption held by existing algorithms is stated as follows: Assumption 1 (Shared Distribution -Stationary Distribution) Training D and test data T are assumed to be generated by the same distribution P ( x ,y )= P ( y | x )  X  P ( x ) no matter how P ( x ,y ) evolves as time elapses.
 Given this assumption, one would ask:  X  X hat is the differ-ence between stream mining and traditional mining prob-lems? X  The most significant difference from traditional  X  X tatic X  learning scenarios is that this shared distribution between training and testing data (abbreviated as  X  X hared distribution X  in the rest of paper) evolves from time to time in three different ways [3]: (1) feature changes , i.e., the changes of the probability P ( x ) to encounter an ex-ample with feature vector x ; (2) conditional changes , i.e., the changes of the conditional probability P ( y | x ) to assign class label y to feature vector x ; and (3) dual changes , i.e., the changes in both P ( x ) and P ( y | x ) . An illustration with a real-world intrusion dataset can be found later in this sec-tion.

Under the  X  X hared distribution assumption X , the funda-mental problems that previous works on stream mining fo-cus on are mainly the following areas: 1) How often the shared distribution changes? It could be continuous or pe-riodical, and fast or slow; 2) How much data is collected to mine the  X  X hared distribution X ? It could be sufficient, insuf-ficient or  X  X ust don X  X  know X ; 3) What is this  X  X hared distri-bution X ? It could be balanced or skewed, binary or multi-class, and etc.; 4) How the shared distribution evolves? There could be conditional change, feature change, or dual change; and 5) How to detect the changes in shared dis-tribution? Some methods do not detect them at all and al-ways keep the models up-to-date whereas others only trig-ger model reconstruction if a change is suspected. Obvi-ously, the validity of some of these problems relies on the  X  X hared distribution assumption X , which we challenge be-low. Interestingly, given  X  X tationary distribution assump-tion X , stream learning would still be effectively the same as traditional learning if the set of training examples col-lected to mine the  X  X hared distribution X  is sufficiently large so that additional examples cannot construct a more accu-rate model [2].
 Realistic Assumption The implicitly held assumption (Assumption 1) may not always be true for data streams. As an example, let us consider the KDDCUP X 99  X  X ntrusion detection X  dataset that is widely used in the stream mining literature. We plot the evolution on the percentage of intru-sions using  X  X veraged shifted histogram (ASH) X  in Figure 1. The true probability P ( y ) to encounter an intrusion is shown in thick solid line. Obviously, P ( y ) is very volatile. As time elapses, P ( y ) continues to change and fluctuate. At some period, the change is more significant than others. Except for the flat area between time stamps 2  X  10 5 and 3  X  10 5 , P ( y ) from the past is always different from that of the future examples. Under  X  X hared distribution X  assump-tion, the training distribution ought to be accurately mod-eled as the ultimate target. However, it may not precisely match future testing distribution due to continuous change.
The fluctuation in P ( y ) comes from changes in P ( y | x or P ( x ) . Let + denote intrusions. By definition, P ( y = +) = P then P ( y )  X  P ( x ,y )= P ( y | x )  X  P ( x ) . Thus, the change in P ( y ) has to come from P ( y | x ) ,or P ( x ) , or possibly both P ( y | x ) and P ( x ) . Unless the dataset is synthesized, one normally does not know which of these three cases is true, either before or after mining. Because of this, a model con-structed from the training data may not be highly accurate on the incoming data. This can particularly be an issue if the changes are attributed to conditional probability P ( y | x follows, we illustrate how P ( x ) and P ( y | x ) change using the same intrusion detection example.

Figure 2 shows the histograms of the percentage of intrusions and normal connections given the feature  X  X rv diff host rate X  in three different time periods, where gray represents intrusions and black indicates normal con-nections. The range of this feature, or the percentage of connections to different hosts, remains within [0,1]. Due to the space limit, we only show the histograms between 0 and 0.25. Most bars between 0.25 and 1 have heights close to 0 and do not reveal much useful information. It is obvious that the distribution of this feature, or visually the relative height of each bar in the histogram represent-ing the percentage of connections, is different among these three time periods. This obviously indicates the change in P ( x ) as data flows in. In addition, the probability distri-bution to observe intrusions given this feature is quite dif-ferent among these three periods. For example, in the first time period, P ( y =+ | x  X  [0 . 095 , 0 . 105]) = 0 but it later jumps to around 0.7 at the last time stamp. In the following, we will discuss how the  X  X hared distribution X  assumption affects learning when the actual data evolves in the manner described above. It is worth noting that some stream mining algorithms [10, 6, 2, 9, 7, 12, 8] discuss about the concept drifts in streams and recognize the changes in the distribu-tion that generates the data. However, they still make some assumptions about the forms of concept drifts. For exam-ple, most of them assume that the most recent training data is drawn from the distribution which is considerably close to that generates the test data [6, 2, 9, 7, 8].

Depending on when labeled training data becomes avail-able, existing stream classification algorithms belong to two main categories. The first group [1, 4] updates the training distribution as soon as labeled example becomes available and flows in, and at the same time, obsolete examples are either discarded or  X  X eighted X  out. Under the  X  X hared dis-tribution X  assumption, such method obviously assumes that the distribution of the next moment is the same as those ob-served data in memory. Visually, it assumes a  X  X hifted X  or  X  X elayed X  P ( y ) as the distribution of the future, as shown by the  X  X eal Time Update X  curve in Figure 1. To be pre-cise, when either the number of examples kept in mem-ory is not sufficiently large or the fading weights are not set properly, P ( y ) may not only be shifted but also carry a  X  X ifferent shape X  from the plot constructed by average shifted histogram. The second family of stream classifica-tion algorithms [2, 9, 7, 8] normally receives labeled data in  X  X hunks X , and assumes that the most recent chunk is the closest to the future distribution. Thus, they concentrate on learning from the most recent data accurately as well as some old examples that are similar to the current distribu-tion. Due to the changing P ( y ) , we observe both  X  X hift X  and  X  X lattening X  of the assumed future distribution, shown in the  X  X atch Update X  curve in Figure 1.  X  X lattening X  is due to chunking and is hard to avoid since labeled data may ar-rive in chunks. As a summary, for both families of methods,  X  X hifting X  is not desirable and ought to be resolved.
In fact,  X  X hift X  or  X  X elay X  is inevitable under the  X  X hared distribution assumption X , since the culprit is the assump-tion itself: the future data is not known and can change in different ways from the current data, but they are im-plicitly assumed to be the same. In order to overcome the  X  X elaying X  problem, the main question is how one should judiciously use what is known in order to optimally match the unknown future, with the least surprise and disappoint-ment. Existing algorithms have obviously taken the road to accurately match the training distribution with the hope that it will perform well on the future data. However, from the above example as well as detailed experiments on this example in Section 3, they could perform poorly when the future is quite different from the current. By this token, we could see that the commonly held  X  X hared distribution as-sumption X  may not be appropriate, and stream classification algorithms ought to consider situations where training and testing distributions are different. Thus, we take this differ-ence into consideration and suggest a relaxed and realistic assumption as follows: Assumption 2 (Learnable Assumption) The training and testing distributions are similar to the degree that the model trained from the training set D has higher accuracy on the test set T than both random guessing and predicting the same class label.
 The core of this new assumption is that it does not assume to know any exact relationship between current training and future test distribution, but simply assume that they are sim-ilar in the sense that learning is still useful. As commonly understood, this is the bare minimum for learning. It should be noted that this assumption is made concerning the induc-tive learning problem. Mining data streams from other per-spectives, such as clustering, association mining, may re-quire other appropriate assumptions. All the notations and assumptions we made in the paper are summarized in Fig-ure 3. With the relaxed assumption, we first elaborate the idea that one should only match the training distribution to a certain degree, then we shall provide a straightforward framework that can maximize the chance for models to suc-ceed on future data with different distributions.
In Section 1, we illustrate that when learning from stream data, it is unlikely that training and testing data al-ways come from the same distribution. This phenomenon hurts existing algorithms that are based upon such an as-sumption. Some stream mining work has investigated the change detection problem [5] or utilized the concept change in model construction [12]. However, since there are only unlabeled examples available in the test data set, the  X  X hange detection X  could at most detect feature change. It is rather difficult to detect the change in P ( y | x ) before class labels are given. The moral of the relaxed assump-tion (Assumption 2) ought to be understood in the way that  X  X trong assumptions are no good for stream mining X . To carry this understanding one step further, any single learn-ing method on data streams also makes assumptions one way or the other on how to match the training distribution effectively and still perform well on testing distribution, and these assumptions can also fail for a continuously chang-ing data stream. Instead, we use a naive model averaging based approach that does not depend specifically on any single technique but combines multiple techniques wher-ever and whenever available. Formally, suppose k models {
M 1 ,M 2 ,...,M k } are trained (e.g. using different learn-ing algorithms) and each of them outputs an estimated pos-terior probability P ( y | x ,M i ) for each test example use simple averaging to combine the probability outputs, thus f A ( x )= 1 k k i =1 P ( y | x ,M i ) , and its optimality is discussed below.
 Performance Guarantee As described above, we gener-ate k models and each model M i outputs an estimated prob-ability P ( y | x ,M i ) for x . For the sake of simplicity, we use M to denote any of the k models M i and use  X  M to rep-resent the collection of the k models. Then any base model M  X  X  expected mean squared error is the difference between its predicted probability and the true probability integrated over all test examples: Err M = = E P ( x ,y ) [ P ( y | x ) 2  X  2 P ( y | x ) P ( y | x ,M )+ P ( y Suppose each model M has probability P ( M ) on the test set, then the expected error incurred by randomly choosing a base streaming model to do prediction is the above error Err S = ] It should be noted that the above equation only evaluates the general performances of base streaming models, but the pre-dictions of test examples are not averaged. Now, we come to the analysis of ensemble where the predictions are aver-aged. As introduced before, we make the following  X  X odel averaging X  prediction: f A ( x )= E P ( M ) [ P ( y | x ,M )] . Then the expected error of this ensemble should be the error inte-grated over the universe of test examples:
Err A = The inequality holds since E [ f ( x ))] 2  X  E ( f ( x ) 2 case, E P ( M ) [ P ( y | x ,M )] 2  X  E P ( M ) [ P ( y | x fore, Err A  X  Err M , i.e., probability averaging of multiple models is superior to any base streaming model chosen at random with respect to reduction in expected errors on all possible examples.

We are not claiming that model averaging is more accu-rate than any single model at any given time. As a simple illustration, Figure 4 shows the errors of three models at time A and time B. At a specific time stamp, a single model M that fits current distribution well could have much bet-ter performances on test data than other models, e.g., M 2 time A and M 1 at time B. At this same time stamp, the prob-ability averaging of three models (shown as AP) may not necessarily be more accurate than using a specific model. However, in stream learning problems, it is hard to find a single model that works well on all possible training-test pairs drawn independently from continuously changing dis-tributions. Since it is unknown which single model could be optimal at each and every time stamp, the current practice is to select a method and hope it will perform the best at any time stamp. However, this could be risky. In the above ex-ample, the most accurate model M 2 at time stamp A turns out to be the least accurate at time stamp B. On the other hand, the model averaging approach could reduce the prob-ability of surprises and guarantee the most reliable perfor-mance. The above analysis formally proves the expected er-ror incurred by randomly choosing a single model is greater than model averaging. Therefore, unless we know exactly which model is always the best, unrealistic in a constantly changing stream environment, we could expect model aver-aging to have the best expected performance.
 Optimality of Uniform Weights The next question is how to decide P ( M ) , or the probability of model M be-ing optimal. The simplest way is to set P ( M  X  )=1 where M  X  is the most accurate model and set other model X  X  prob-ability as 0. This is one of the common practice adopted by some stream mining algorithms where the model itself is fixed but its parameters are re-estimated as labeled data flows in. As discussed above, the expected performance of a single model could be low, when the distribution is con-tinuously evolving. Another more sophisticated approach is introduced in [9], where each model is assigned a weight that is reversely proportional to its error estimated using training data. That is to say, P ( M ) is higher if the model M incurs less errors when cross-validated using the train-ing data. This weighting scheme is problematic because: 1) the training examples may be insufficient to reflect the true accuracy of model M , thus the weights may not represent the true P ( M ) ; and 2) more importantly, the training and testing distributions may not be the same as previous meth-ods have assumed, thus the weights derived from the train-ing data would be essentially inappropriate for the test data. As illustrated in Figure 4, when training and test data have different distributions, P ( M ) calculated using training data may be off from its true value, thus leading to the unsatis-factory performance of weighted ensemble (denoted as WE) as compared with the simple model averaging (AP). As fol-lows, we formally illustrate why simple averaging with uni-form weights beats other non-uniform weighting schemes.
Suppose the weights of k models are { w 1 ,w 2 ,...,w k } each of which is from [0,1] and satisfies the constraint i =1 w i =1 . Ideally, the weight of model M i (1 ought approximate its true probability P ( M i ) as well as possible. We use the following measure to evaluate the dif-ference between the assigned weights and the true weights: Let  X  i be the hypothesis space where M i is drawn, which has a uniform distribution with a constant density C i .In other words, we don X  X  have any prior knowledge about the optimality of a model for a constantly changing stream. This is a valid assumption since the choice of optimal model is changing with the evolving distribution. The test distribu-tion is somewhat revealed by the training distribution, how-ever, which model fits the distribution the best remains un-known. Another clarification is that P ( M i ) = P ( M j )( i = j ) on a specific pair of training and test sets given in time. This means that we cannot have preference for some model over others, since the preference needs to change continu-ously considering all possible training and test sets in time. The constraint k i =1 P ( M i )=1 should also be satisfied. As an example, suppose there are two models, M 1 and M 2 . Then P ( M 1 ) and P ( M 2 ) are both uniformly distributed within [0,1]. At one evaluation point, P ( M 1 )=0 . 8 and P ( M 2 )=0 . 2 , but at another time, P ( M 1 )=0 . 3 and P ( M 2 )=0 . 7 . It is clear that both M 1 and M 2 would be preferred at some time but it is unknown when and how this preference is changing. As another example, look at Figure 4 again, it is clearly shown that M 2 and M 1 are the best models with lowest test errors at time A and B respectively. However, since the labels of test examples are not known in advance, we could never know this changing preference before mining.

Integrating the distance measure in Eq. 1 over all possi-ble M i , we could obtain the expected distance as:
E [ D ( w )]= The aim is to minimize E [ D ( w )] w.r.t w . Eliminating irrel-evant items, the above could be simplified to: where { C 1 ,C 2 ,C 3 } are positive constants. Since i =1 w i =1 , the problem is transformed to:
Minimize The closed form solution to this constrained optimization problem is: w i = 1 k (1  X  i  X  k ) . Therefore, when we have no prior knowledge about each model, equal weights are expected to be the closest to true model probabilities on the test data over some period of time, thus giving the best performances on average. This is particularly true in the stream environment where the distribution is continuously changing. As shown in the following experiments, the best model on current data may have bad performances on future data, in other words, P ( M ) is changing and we could never estimate the true P ( M ) and when and how it would change. Hence non-uniform weights could easily incur over-fitting, and relying on a particular model should be avoided. Under such circumstances, uniform weights for the models are the best approximate of the true P ( M ) .
We conduct an extensive performance study using both synthetic and real data sets, where training and testing dis-tributions are explicitly generated differently, to demon-strate the effectiveness of the averaging ensemble against change. As discussed below in detail, this empirical study validates the following claims: 1) ensemble based on model averaging would reduce expected errors compared with sin-gle models, thus is more accurate and stable; and 2) previ-ous weighted ensemble approach is less effective than en-semble based on simple voting or probability averaging. Synthetic Data Generation We describe how to generate synthetic data and simulate its concept changes, rephrased from [3]. Form of P ( x ) . x follows a Gaussian distribu-tion, i.e., P ( x )  X  N (  X ,  X  ) , where  X  is the mean vec-tor and  X  is the covariance matrix. The feature change is simulated through the change of the mean vector where  X  i is changed to  X  i s i (1 + t ) for each data chunk. t is between 0 to 1 , representing the magnitude of changes, and s i  X  X  X  1 , 1 } specifies the direction of changes and could be reversed with a probability of 10% . Form of P ( y | x ) in deterministic problems . In binary problems, the boundary between two classes is defined using function g ( x )= d tor. Then the examples satisfying g ( x ) &lt; 0 are labeled positive, whereas other examples are labeled negative. a i initialized by a random value in the range of [0,1] and the value of a 0 is set based on the values of r and { a 1 ,...,a In multi-class problems, suppose there are l classes and the count of examples in each class is { C 1 ,C 2 ,...,C l } .We calculate the value of g ( x ) for each x using the definition given in binary problems. All the examples are ranked in ascending order of g ( x ) . Then the top C 1 examples are given class label 1 , examples with ranks C 1 +1 to C 1 + C are assigned to class 2 , and so on. In both problems, the concept change is represented by the change in weight a i which is changed to a i s i (1 + t ) for every data chunk. The parameters t and s i are defined in the same way as in the feature change. Form of P ( y | x ) in stochastic problems . We use a sigmoid function to model the posterior distribu-tion of positive class: P (+ | x )=1 / (1 + exp( g ( x ))) . The concept changes are also realized by the changes of weights as illustrated in the deterministic scenario.

The distribution within a data chunk is unchanged whereas between data chunks, the following changes may occur: 1) each data chunk could either be deterministic or stochastic (in binary problem); 2) in each chunk, the Gaus-sian distribution of the feature values may either have diago-nal variance matrix or non-diagonal one; 3) either one of the three concept changes (feature change, conditional change and dual change) may occur; 4) the number of dimensions involved in the concept change is a random number from 2 to 6; and 5) the magnitude of change in each dimension is randomly sampled from { 10% , 20% ,..., 50% } . Since lots of random factors are incorporated into the simulated con-cept change, it is guaranteed that training and testing distri-butions are different and evolving quickly.
 Real-World Data Sets We test several models on KDD Cup X 99 intrusion detection data set, which forms a real data stream. This data set consists of a series of TCP connec-tion records, each of which can either correspond to a nor-mal connection or an intrusion. We construct three data streams from the 10% subset of this data set: Shuffling . Randomly shuffle the data and partition it into 50 chunks with varying chunk size from 5000 to 10000. Stratified Sampling . Put the data into class buckets: One for normal connections and one for intrusions. Generate 50 chunks as follows: 1) choose an initial P ( y ) , 2) sample without re-placement from each bucket to form a chunk that satisfies P ( y ) , 3) evolve P ( y ) and sample from the remaining data in the buckets as the next chunk, and finally, 4) put data sampled in step 2 back to the buckets and repeat steps 2 and 3. The chunk size is also varied from 5000 to 10000. Merg-ing . Partition the data into chunks and maintain its original order. Both normal connections and intrusions should ac-count for at least 5% in each chunk, if this is not satisfied, merge the chunk with the next chunk until the percentage is above the threshold. In the experiments, we discard one large data chunk with only normal examples and there are altogether 22 chunks with chunk size varying from 1069 to 32122. We construct these three data sets because the origi-nal data set does not have explicit time stamps and there are chunks of intrusions followed by chunks of normal exam-ples. Each chunk may only contain examples of one class and this is a non-learnable problem. On the other hand, the three data streams we constructed have the following properties: 1) each chunk is ensured to have examples from both classes; and 2) the distributions of two consecutive data chunks are different and evolve in different ways in three streams. Upon such datasets, we could validate our claims of stream mining upon the relaxed assumption.
 Measures and Baseline Methods For a data stream with chunks T 1 ,T 2 ,...,T N , we use T i as the training set to clas-sify T i +1 and the distribution of the test set T i +1 is not nec-essarily the same as that of T i . We evaluate the accuracy of each model. For the classifier having posterior probability as the output, the predicted class label is the class with the highest posterior probability under zero-one loss function. Another measure is mean squared error (MSE), defined as the averaged difference between estimated probability and true posterior probability P ( y | x ) . In problems where we are only exposed to the class labels but do not know the true probability, we set P ( y | x )=1 if y is x  X  X  true class label, otherwise P ( y | x )=0 . We are comparing the fol-lowing algorithms: single models built using Decision Tree (DT), SVM, Logistic Regression (LR) and ensemble ap-proaches including Weighted Ensemble (WE), Simple Vot-ing (SV) and Averaging Probability (AP). Different from averaging ensemble framework, the weighted ensemble ap-proach assigns a weight to each base model which reflects its predictive accuracy on the training data (obtained by ten-fold cross validation) and the final prediction outputs are combined through weighted averaging. In previous work, such weighted ensembles are shown to be effective when the  X  X hared distribution X  assumption holds true. In our ex-periments, we evaluate its performances upon the relaxed assumption. For all the base learning algorithms, we use the implementation in Weka package [11] with parameters set to be the default values. In the averaging ensemble frame-work, either SV or AP, the base streaming models could be chosen arbitrarily. We test the framework where base mod-els are constructed from either different learning algorithms or different samples of the training sets.
 For a learning algorithm A h , we build a model based on T i and evaluate it on T i +1 to obtain its accuracy p ih and MSE e ih . There are altogether N  X  1 models and we re-port its average accuracy (Aacc) and average MSE (Amse). Furthermore, in each of the N  X  1 runs, we compare the per-formances of all algorithms and decide the winner and loser in the following way: if p ih is within m %of max h p ih , al-gorithm A h is a winner in that run, similarly, if p ih is within m %of min h p ih , it is a loser. In other words, we tolerate some small difference between two algorithms, if their ac-curacies are the same with respect to the  X  X argin tolerance rate X  m , we regard their performances as the same. We re-port the number of wins and loses for each algorithm (#W and #L). With winners ranking the first, losers ranking the third and all other algorithms occupying the second posi-tion, we give N  X  1 ranks to each algorithm and obtain the mean and standard deviation of the ranks (AR and SR). A good algorithm will have a higher accuracy, a lower MSE and average rank closer to 1. If it has a lower standard de-viation in the ranks, the learning algorithm is more stable.
We report the experimental results comparing the two ensemble approaches (SV, AP) with single model algo-rithms (DT, SVM, LR) as well as weighted ensemble method (WE). As discussed below in detail, the results clearly demonstrate that on the stream data where training and testing distributions are different and fast evolving, the two ensemble approaches have the best performances on av-erage with higher accuracy and lower variations. Therefore, when facing unknown future, the ensemble framework is the best choice to minimize the number of bad predictions. Test on Concept-Drifting Stream Data We generate four synthetic data streams, each of which is either binary or multi-class and has chunk size 100 or 2000. Each data set has 10 dimensions and 100 data chunks. The margin tolerance rate is set to be 0.01. From Table 1, it is clear that the two ensemble approaches (SV and AP) have better performances (best are highlighted in bold font) regardless of the measures we are using, the problem type (binary or multi-class) and the chunk size. Take the binary problem with chunk size 100 as an example. AP proves to be the most accurate and stable classifier with the highest accu-racy (0.7690), lowest MSE (0.1752), 53 wins and only 2 loses. SV is quite comparable to AP with 50 wins and 2 loses. The best single classifier SVM wins 47 times and loses 12 times and WE approach seems to suffer from its training set-based weights with only 34 wins but 23 loses. These results suggest the following: when the  X  X ame distri-bution X  between training and testing data does not exist: 1) there are no uniformly best single classifiers, even decision tree, which has the worst average performance, still wins 30 times among all 99 competitions. The large variabilities of single models result in their high expected errors; 2) on average, ensemble approaches, simple voting or probability averaging, are the most capable of predicting on future data with unknown distributions; 3) assigning a weight to each base learner even hurts the predictive performances on test-ing data since the distribution it tries to match is different from the true one.

For binary streams, we also record the results on the first 40 chunks to see how the concept evolution affects the classification performances. The results indicate that even within the same data stream, the best single classifier for the first 40 chunks is different from the best one on the whole data set. Take the stream data with chunk size 100 as an example. At first, LR has 18 wins, compared with DT (4 wins) and SVM (14 wins), it appears to be the best on av-erage. However, later, SVM takes the first place with 47 wins (DT 30 and LR 28). This clearly indicates that in a stream whose distribution evolves, a model which performs well on current data may have poor performances on future data. Since we never know when and how the distribution changes, depending on one single classifier is rather risky. On the other hand, ensemble based on averaged probability is more robust and accurate, which is the winner for classi-fying data streams with regard to the average performance (ranks around 1.5 while others rank more than 2 on aver-age). Ensemble based on simple voting (SV) produces re-sults similar to that of AP in binary stream problems, but is not that competitive in multi-class problems. The rea-son may be that two class problems are easier for predic-tion tasks, so the probability outputs of a classifier may be rather skewed, greater than 0.9 or less than 0.1. So there isn X  X  much difference between simple voting and averag-ing probability in this case. However, when the number of classes grows large, it is quite unlikely that the predicted probability is skewed. The strengths of probability averag-ing over simple voting is therefore demonstrated on multi-class problems. As for the weighted ensemble approach, it sometimes increases the predictive accuracy, but sometimes gives even worse predictions compared with single models. Whether it performs good or not is dependent on how the training and testing distributions match. In this sense, the other two simple ensemble methods are more robust since they are not based on the assumption that training and test-ing data come from the same distribution.

We also compare all the methods on data streams where training and testing distributions are identical, as assumed by previous stream mining algorithms. Two data streams are generated, where the first one is used as the training set and the second one is the testing data. They have  X  X volv-ing shared distribution X  in the sense that the corresponding chunks of training and testing streams are sampled from the same distribution, but this shared distribution is evolving in the way we have described in Section 3.1. Each stream has 100 data chunks with 100 examples in each chunk. Since the training examples may be far from sufficient due to the small training size, it may not obtain an accurate model even if the training and testing distributions are the same. As indicated in Table 2, again, ensemble could help reduce the classification errors on such data sets (from around 0.2 to 0.12). AP has obtained the highest accuracy, the low-est mean squared error and the highest rank on average. Weighted ensemble wins 40 times, which appears to be competitive, but worse than AP in terms of loses (19 ver-sus 2). Therefore, even if the  X  X hared distribution X  assump-tion holds true, simple averaging or simple voting are still more effective than weighted ensemble because the train-ing samples may be insufficient and training errors derived from such data sets are unreliable. We could safely con-clude that the averaging ensemble framework could maxi-mize the chance of matching the true distribution, thus re-turn the most satisfactory prediction results in general. Test on KDD Cup X 99 Data In Section 3.1, we describe the three data streams we generate from the KDD Cup X 99 intrusion detection data set and how the training and testing distributions are made different explicitly. Also, as illus-trated in Section 1, both P ( x ) and P ( y | x ) undergo contin-uous and significant changes in this stream data. Results of various methods on streams generated by  X  X huffling X  and  X  X tratified Sampling X  are summarized in Table 3 where margin tolerance rate is set to be 0.001. Similar to earlier results on simulated streams, the advantage of the ensem-ble framework is clearly demonstrated. The two ensemble approaches not only increase the accuracy of single models but also occupy the first place in most of the evaluations. The most significant improvements could be observed on the data set generated by shuffling, where accuracy goes up from 0.9961 to 0.9975 and the number of wins increases from 18 to 49 after combining outputs of multiple models. The performances of SV and AP are almost the same for these two data sets. As discussed in the synthetic data exper-iments, SV and AP are expected to have similar predictions when the estimated probabilities of each class are skewed in binary problems. Another observation is that the weighted ensemble approach could improve over a single model but the improvements are less significant compared with simple averaging. This phenomenon again shows that the weight-ing scheme cannot survive the relaxed assumption where training and testing distributions could be different since it fits the training data too  X  X ightly X .

Figure 5 reveals some detailed information about the evaluation results (Accuracy and MSE w.r.t Chunk ID) on the first data set where data records are randomly shuffled. To exclude the effects of different scales, we normalize the measures by the maximal value. It is obvious that the prob-ability averaging ensemble (AP) is the most accurate clas-sifier in general with normalized accuracy close to 1 and mean squared error below 0.5. Also, as shown in both plots, as measures of single models fluctuate within a wide range, the performances of probability averaging ensemble are much more stable. This clearly shows the benefits of us-ing our ensemble framework when the testing distribution is unknown and departed from the training distribution. On average, the ensemble would approximate the true distribu-tion more accurately than single models, with least number of loses. The weighted ensemble could achieve higher accu-racy than single-model classifier but still has larger variance and worse average performance compared with AP. For ex-ample, the highest normalized MSE of AP is only around 0.6, but over 0.8 for weighted ensemble approach.
In Table 4, the results on the third data stream where the original order is maintained are reported. It seems that logistic regression is consistently worse than the other two base learners, which wins only once, whereas DT and SVM win 7 and 10 times respectively. In general, the ensemble approaches (SV and AP) still predict well, win 9 times and lose twice. Unlike the synthetic stream where distribution evolves quickly and no uniformly best classifier could be derived, this data set may have certain characteristics that favor one classifier over another. This property could be observed and analyzed from the data set and regarded as prior knowledge. We could incorporate such knowledge into each base learner and further improve a single model. In this case, if we know that logistic regression is not suit-able for this data and expected to perform bad, we could construct an ensemble based on multiple samples from the training data using decision tree and SVM as the base learn-ers. In this experiment, we build 10 base models from 10 samples of the original data and combine their outputs by averaging the probabilities. As shown in Table 5, the en-semble based on multiple samples (denoted as APS), boosts the accuracy from 0.8508 (accuracy of the best single clas-sifier) to 0.8579 and reduces the MSE from 0.1492 down to 0.1301. When counting the number of wins and loses, APS is the best compared with single models and other ensem-ble approaches. It should be noted that we are not against the ensemble based on multiple learning algorithms. Logis-tic regression is not working on this data set but may per-form good on others, when we know nothing about the data set, ensemble based on multiple learning algorithms is the safest way for prediction. The key point here is that ensem-ble demonstrates its strengths no matter we know nothing or something about the stream data. Regardless of the base-line models, the averaging ensemble framework is expected to generate the best estimate of the true target function from the limited information conveyed by training examples.
Sample selection bias [13] investigates the effect on learning accuracy when the training data is a  X  X iased X  sam-ple of the true distribution. Although the true target function to be modeled, P ( y | x ) , does not  X  X xplicitly X  change, its value can be wrong in various ways in the biased training data. Previously, decision tree based model averaging has been shown to be helpful to correct feature bias or the bias where the chance to sample an example into the training set is independent on y given x [13]. The most important difference of our work from these previous studies is: (1) P ( y | x ) in our problem is allowed to explicitly change and can change significantly, (2) changes in P ( y | x ) are com-bined with changes in P ( x ) . To consider the significance of our work under sample selection bias formulation, our com-prehensive results significantly extend the previous work and demonstrate that model averaging can reliably correct sample selection bias where biased conditional probability is quite different from unbiased testing data.
We demonstrate that assuming training and testing data follow the same distribution, as commonly held by much existing work, is inappropriate for practical streaming sys-tems. On the contrary, the distributions on both feature vec-tor and class label given feature vector can evolve in some unknown manner, and models matching training distribu-tion well may perform poorly in continuously changing dis-tributions. As a result, the difference between training and testing distributions needs to be taken into account. We also argue that, contrary to common practice, in order to de-sign robust and effective stream mining algorithms against changes, an appropriate methodology is not to overly match the training distribution, such as by weighted voting or weighed averaging where the weights are assigned accord-ing to training distribution. On these basis, we use both model averaging of conditional probability estimators and simple voting of class labels as a robust framework  X  X gainst change X  and argue that weighted averaging/voting are inap-propriate. We demonstrate both formally and empirically such a framework can reduce expected errors and give the best performance on average when the test data does not follow the same distribution as the training data. Among many experiments, in a test on KDDCup X 99 intrusion de-tection dataset, the framework X  X  predictions are the most ac-curate in 49 out of 49 competitions, whereas the best base-line model is the most accurate only 18 times. The base-line models are not limited to those used in the empirical evaluations and other more sophisticated methods can be plugged in. Since the property of expected error reduction is proved formally, the framework is expected to have ro-bust and better performance regardless of chosen baseline models. Nonetheless, for stream mining research in gen-eral, practitioners and researchers ought to clearly specify the training and testing assumptions made in the algorithms, and evaluation benchmarks ought to be designed with these assumptions clearly considered.

