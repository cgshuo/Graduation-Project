 ORIGINAL PAPER Gheith A. Abandah  X  Alex Graves  X  Balkees Al-Shagoor  X  Alaa Arabiyat  X  Fuad Jamour  X  Majid Al-Taee Abstract This paper presents a sequence transcription approach for the automatic diacritization of Arabic text. A recurrent neural network is trained to transcribe undiacritized Arabic text with fully diacritized sentences. We use a deep bidirectional long short-term memory network that builds high-level linguistic abstractions of text and exploits long-range context in both input directions. This approach differs from previous approaches in that no lexical, morphological, or syntactical analysis is performed on the data before being processed by the net. Nonetheless, when the network is post-processed with our error correction techniques, it achieves state-of-the-art performance, yielding an average diacritic and word error rates of 2.09 and 5.82%, respectively, on samples from 11 books. For the LDC ATB3 benchmark, this approach reduces the diacritic error rate by 25%, the word error rate by 20%, and the last-letter diacritization error rate by 33% over the best published results.
 Keywords Automatic diacritization  X  Arabic text  X  Machine learning  X  Sequence transcription  X  Recurrent neural networks  X  Deep neural networks  X  Long short-term memory 1 Introduction The Arabic alphabet is the base alphabet used in around 27 languages, including Arabic, Persian, Kurdish, Urdu, and Jawi [ 26 ]. The Arabic language has 28 basic letters and eight basic diacritics that are encoded in the Unicode code block 0600 X 06FF. Figure 1 shows where these letters and diacrit-ics are encoded in this code block. Note that this code block includes 36 variants of the 28 letters having hexadecimal codes 0621 X 063A and 0641 X 064A; the diacritics have codes 064B X 0652 [ 1 ].

The modern standard Arabic (MSA) is commonly written without diacritics as the example shown in Fig. 2 a (note that Arabic is written from right to left). Classical Arabic (CA), on the other hand, is written with diacritics as in Fig. 2 b. The Holy Quran and classical books are usually diacritized to specify the accurate pronunciation and the intended meaning.
Although the MSA writing is faster, undiacritized words often cannot be pronounced correctly from their orthographic representation only. Educated native readers can generally infer the correct pronunciation of undiacritized words from the context and from their knowledge of the grammar and lexicon. However, the lack of diacritics causes ambiguity for children and non-native speakers, who have not mastered the language X  X  rich derivation and inflection mechanisms. In Arabic, many stem words can be derived from the finite Arabic root consonant combinations into known patterns [ 32 ]. Different words, with different meanings and pronun-ciations, often differ only by their diacritics [ 7 ].
For example, the first word in the previous example is the verb  X  X rote X  and is pronounced  X  X ataba. X  The same three letters can mean  X  X ooks X  and are pronounced  X  X utub X  the first word is the verb  X  X ataba X  . As the second and third words are the nouns  X  X he student X  and  X  X etter X  is a verb-subject-object sentence.

Inflection changes are phonological changes a word undergoes as it is being used in context. In Arabic, there are rules to inflect a stem word according to its tense/aspect, person, voice, mood, gender, number, case, and definiteness. In many cases, different inflections differ only by the diacritic of the last letter [ 22 ]. For example, the verb  X  X ataba X  is inflected for masculine second person as  X  X atabta X  (with Fatha diacritic at the end) and is inflected for femi-nine second person as  X  X atabti X  (with Kasra). Another example: the noun is inflected as  X  X isalatun X  (with Dammatan) in the subject case and as  X  X isalatan X  (with Fathatan) in the object case.

To correctly diacritize the words of a sentence, it is often required to analyze the entire sentence. Figure 3 shows two different diacritizations of four words based on the fifth (last) word. The four words are diacritized as shown in (a)  X  X ooks of Ahmad and Ali and Omar are numerous X  when followed by the adjective  X  X umerous X  or as shown in (b)  X  X hmad and Ali and Omar wrote the lesson X  when followed by the noun  X  X he lesson X  . As the last word in (a) is an adjective, the word must be the noun  X  X ooks X  , whereas the last word in (b) is a noun; so it is more likely that the word
This example illustrates that it is not possible to automat-ically diacritize a word based only on its past context. The successor words must often be considered to reach correct analysis and diacritization. Moreover, it is often necessary to do long-range context analysis. For example, checking only one, two, or three successor words of the word is not sufficient in this example.

Adding diacritics to undiacritized or partially diacritized text is very helpful for children and non-native speakers who are trying to learn Arabic. It is a necessary step in text-to-speech (TTS) software, as otherwise pronunciation is ambiguous, and is also useful for training automatic speech recognition (ASR) engines [ 25 ]. Moreover, indexing dia-critized text instead of undiacritized text enables search engines to better exclude unwanted matches.

The best methods to date for diacritization generally ana-lyze the input text and derive extensive morphological and contextual information, and then use pattern classification techniques to select the analysis that best matches the con-text.

This paper proposes a novel approach, where diacritiza-tion is solved as a sequence transcription problem. We use a recurrent neural network (RNN) to transcribe raw Ara-bic sentences, without relying on any prior morphological or contextual analysis, into fully diacritized sentences. We use a deep bidirectional long short-term memory (LSTM) network that builds high-level linguistic abstractions of text and exploits long-range context in both input directions. We describe how Arabic sentences are encoded into sequences suitable for sequence transcription by an RNN. Moreover, we describe some post-processing corrections developed to improve the results of the sequence transcription stage. This approach, which builds on recent advances in sequence tran-scription using RNNs [ 17 ], yields higher accuracy than the best published results.

The problem that our approach solves is adding dia-critics to undiacritized sentences. Given an input sequence x = ( x 1 ,..., x T ) that represents an Arabic sentence of T unicode letters of the set U in the range 0621 X 064A and x  X  U T , transcribe this sequence into the output sequence y = ( y 1 ,..., y U ) that represents the diacritized sequence and consists of U unicode letters and diacritics of the set D in the range 0621 X 0652 and y  X  D U . Gen-erally, adding diacritics increases the sequence length and U  X  T .

The rest of this section reviews the related work, Sect. 2 describes the RNN used in this work for sequence tran-scription, Sect. 3 describes our experimental setup, Sect. 4 describes the experiments conducted and presents their results, Sect. 5 discusses the results of this approach and com-pares its results with other leading work, and finally Sect. 6 presents the conclusions and suggestions for future work. 1.1 Literature review Past approaches to automatic diacritization of Arabic text can be roughly divided into rule-based, statistical and hybrid methods [ 5 ]. The earliest approaches were rule-based [ 3 , 11 ], and relied on morphological analyzers, dictionaries, and grammar modules. The major drawbacks of rule-based dia-critization are the high development cost and the reliance on parsed corpora that are difficult to create. Additionally, the rules must be continuously maintained as new words and terms are generated in living languages.

More recently, there have been several statistical , machine-learning approaches. Gal used hidden Markov mod-els (HMMs) to capture the contextual correlation between words [ 12 ]. His approach restores only short vowels (a sub-set of all diacritics) and achieves a test-set word accuracy of 86% on the text of the Quran.

Hifny used statistical n-gram language modeling of a large corpus [ 23 ]. He used the Tashkila diacritized Arabic text cor-pus described in Sect. 3.1 . The possible diacritized word sequences of an undiacritized input sentence are assigned probability scores using the n-gram models. Then, using a dynamic programming algorithm, the most likely sequence is found. Smoothing techniques were used to handle unseen n-grams in the training data. On Tashkila, this approach achieved diacritic and word error rates of 3.4 and 8.9%, respectively. The accuracy of n-gram models depends on the order n . Larger order gives higher accuracy as it incorporates longer linguistic dependencies. However, larger order results in larger models and requires larger training data. Hifny used n-gram models of order three ( trigram ), thus, does not exploit long-range context dependencies.

To the best of our knowledge, the most accurate system reported also uses a statistical approach and is due to Azim et al. [ 4 ]. However, this system requires the availability of speech input as it combines acoustic information from the speech input to complement text-based conditional random fields model. The diacritic and word error rates on the Lin-guistic Data Consortium X  X  Part 3 of the Arabic Treebank of diacritized news stories (LDC ATB3) [ 27 ] are 1.6 and 5.2%, respectively.

Most current work in the area relies on hybrid approaches that combine rule-based and statistical modules. Vergyri and Kirchhoffinvestigatedtheeffectofcombiningseveralknowl-edge sources (acoustic, morphological, and contextual) to automatically diacritize Arabic text [ 35 ]. They treated dia-critization as an unsupervised tagging problem where each word is tagged as one of the many possible forms provided by the Buckwalter Arabic morphological analyzer (BAMA) [ 8 ]. They also investigated the use of Arabic dialectal speech. In this study, they used two different corpora: the For-eign Broadcast Information Service (FBIS) corpus of MSA speech and the LDC CallHome Egyptian Colloquial Arabic (ECA) corpus. They did not model the Shadda diacritic and achieved diacritic and word error rates of 11.5 and 27.3%, respectively.

Nelken et al. [ 30 ] proposed a weighted finite state machine algorithm to restore the missing diacritics. Their basic mod-ule consists of a standard trigram language model that chooses the most probable diacritized word sequence that could have generated the undiacritized text. They also used several other transducers to improve the diacritization process. This system was trained and tested on LDC X  X  Ara-bic Treebank of diacritized news stories (Part 2) and achieved diacritic and word error rates of 12.79 and 23.61%, respec-tively.

Zitouni et al. [ 38 ] followed a statistical model based on the framework of maximum entropy where several sources of information were used, including lexical, segment-based, and part-of-speech (POS) features. Their system was trained and evaluated on the LDC ATB3. They described their exper-imental setup in great detail, and the LDC treebank subse-quently became a benchmark in the area. They reported that the hybrid approach improves the diacritic and word error ratesto5.5and18.0%fromthepurestatisticalratesof8.2and 25.1%, respectively. Their maximum entropy model makes a decision for each state independent of other states. There-fore, it does not exploit context information to achieve high diacritization accuracy.

Habash and Rambow [ 22 ] extended the use of their mor-phological analysis and disambiguation of Arabic system (MADA) for diacritization. MADA consults BAMA to get a list of possible diacritized analyses for a word. To narrow this list to a small number, fourteen SVM predictors are used to predict morphological features of the possible analyses and to rank them. Finally, one solution is selected from the narrowed list using n-gram language models. The system achieved diacritic and word error rates of 4.8 and 14.9%, respectively, on LDC ATB3. The n-gram models used do not exploit long-range context dependencies as they are limited to three words.

The stochastic Arabic diacritizer by Rashwan et al. [ 31 ]is a recent approach with excellent results. The system uses two stochastic layers, the first of which predicts the most likely diacritics by choosing the sequence of unfactorized full-form Arabic word diacritizations with maximum marginal proba-bility via A* lattice search algorithm and n-gram probability estimation. When full-form words are not found, the system falls back on the second layer, which factorizes each Ara-bic word into its possible morphological components (prefix, root, pattern, and suffix), and then uses n-gram probability estimation and A* lattice search algorithm to select among the possible factorizations to get the most likely diacritiza-tion sequence. The system also uses LDC ATB3 and achieves diacritic and word error rates of 3.8 and 12.5%, respectively. Similar to Habash and Rambow X  X  approach, this approach is limited by relying on trigram language models to select most probable diacritization options. However, it achieves better results through its elaborate factorization and dictio-nary techniques.

Said et al. [ 33 ] developed a hybrid system that achieves the best results prior to this paper on LDC ATB3 without speech input. The system relies on automatic correction, morpho-logical analysis, POS tagging, and out of vocabulary dia-critization and yields diacritic and word error rates of 3.6 and 11.4%, respectively. This system is similar to Rashwan et al. X  X  system, but uses HMMs for morphological analy-ses disambiguation and resolving the syntactic ambiguity to restore the syntactic diacritic. The HMM estimates the most probable tag sequence of the alternative POS tag sequences using Viterbi algorithm. However, this estimation is based on two local probabilities: the word likelihoods and adjacent word transition probabilities.

Bahanshal and Al-Khalifa [ 6 ] evaluated the accuracy of three available diacritization systems using fully diacritized text from the Quran and short poems. This study demon-strates that the available systems X  accuracy is still not suffi-cient and there is still ample room for improvement.
Our pure statistical approach of sequence transcription is based on the deep bidirectional LSTM architecture [ 24 ]. This architecture has been successfully applied to many sequence transcription tasks, including automatic speech and hand-writing recognition [ 15 , 20 ], which is currently state-of-the-art in offline Arabic handwriting recognition [ 2 , 28 ]. To the best of our knowledge, this work is the first to use RNN sequence transcription to automatically add diacritics to Ara-bic text. Our experiments were carried out with the open-source software library RNNLIB [ 19 ]. 2 Sequence transcription The basic RNN used in this work is deep bidirectional LSTM [ 20 ]. This architecture combines LSTM [ 24 ] with bidirectional RNNs [ 34 ] and the stacked hidden layers seen in deep feedforward neural networks.

Deep LSTM networks have been previously applied to character-level text prediction and have proved capable of modeling long-range linguistic dependencies [ 18 ].
Given an input sequence x = ( x 1 ,..., x T ) , a stan-dard RNN computes the hidden vector sequence h = ( h by iterating the following equations from t =1to T : h = H ( W y = W where the W terms denote weight matrices (e.g., W ih is the input-hidden weight matrix), the b terms denote bias vectors (e.g., b h is hidden bias vector), and H is the hidden layer activation function. Note the recurrence in Eq. 1 where ht depends on the previous vector h t  X  1 . 2.1 Long short-term memory H is usually an elementwise application of a sigmoid func-tion. However, the LSTM architecture [ 24 ], which uses purpose-built memory cells to store information, is better at finding and exploiting long-range context. Figure 4 illustrates a single LSTM memory cell.

For the version of LSTM used in this paper [ 13 ], H is implemented by the following composite function: i =  X  ( W f =  X  W c = f o =  X  ( W h = o where  X  is the logistic sigmoid function and i , f , c , and o , respectively, are the input gate , forget gate , cell activa-tion, and output gate vectors, all of which are the same size as the hidden vector h . The weight matrix subscripts have the obvious meaning, for example, W hi is the hidden-input gate matrix, W xo is the input X  X utput gate matrix. The weight matrices from the cell to gate vectors (e.g., W ci ) are diag-onal, so element m in each gate vector only receives input from element m of the cell vector. The bias terms (which are added to i , f , c , and o ) have been omitted for clarity. 2.2 Bidirectional RNNs One shortcoming of conventional RNNs is that they are only able to make use of previous context. In automatic diacritiza-tion, where whole sentences are transcribed at once, there is no reason not to exploit future context as well. Bidirectional RNNs (BRNNs) [ 34 ] do this by processing the data in both directions with two separate hidden layers, which are then fed forward to the same output layer. A BRNN computes the forward hidden sequence  X   X  h , and the output sequence y by iterating the backward layer from t = T to 1, the forward layer from t = 1to T , and then updating the output layer:  X   X  h  X   X  h y
Combing BRNNs with LSTM gives bidirectional LSTM, which can access long-range context in both input direc-tions [ 21 ]. 2.3 Deep recurrent neural network A crucial element of the recent success of hybrid systems is the use of deep architectures, which are able to build up pro-gressively higher-level representations of text. Deep RNNs can be created by stacking multiple RNN hidden layers on top of each other, with the output sequence of one layer form-ing the input sequence for the next. Assuming that the same hidden layer function is used for all N layers in the stack, the hidden vector sequences h n are iteratively computed from n = 1to N and t = 1to T : h where h 0 = x . The network outputs y t are y = W
Deep bidirectional RNNs can be implemented by replac-ing each hidden sequence h n with the forward and backward sequences receives input from both the forward and backward layers at the level below. If LSTM is used for the hidden layers, the complete architecture is referred to as deep bidirectional LSTM [ 20 ].

Two different methods were used to train the RNN to tran-scribe sequences of undiacritized Arabic letters with their diacritized counterparts. These two methods are described in the following two subsections. 2.4 One-to-one network Inthefirstmethod,the X  X ne-to-one X  X etterencodingdescribed in Sect. 3.2 was used, ensuring that the target sequences had a one-to-one correspondence with the inputs sequences. Thus, the lengths of sequences x and y are equal as implied by Eqs. 1 and 2 above.

The network was trained to individually classify each input letter with the corresponding diacritized version. As is standard for classification tasks, a softmax output layer was used to define a probability distribution over the output labels and the network was trained to minimize the cross-entropy of this distribution with the target labels. That is, given a length T input and target sequence pair ( x , y  X  network outputs at time t are interpreted as the probabil-the loss function minimized by the network is defined as L ( x , y  X  ) = X  T t = 1 log Pr ( y  X  t | t , x ) .

The network is trained to minimize the loss function L using online gradient descent algorithm with momentum. A similar approach was previously used for bidirectional LSTM networks applied to framewise phoneme classifica-tion [ 21 ] X  X he main difference being that the networks in this work had more than one hidden layer. From now on, we will refer to this configuration as the  X  X ne-to-one X  network. 2.5 One-to-many network The second network transcribes undiacritized input encoded in Unicode to diacritized output using the  X  X ne-to-many X  encoding described in Sect. 3.2 . In this encoding, the output sequence is usually longer than the input sequence ( U  X  T ).
The second training method used an additional, single-layer LSTM network to predict the  X  X ne-to-many X  output symbols. This approach was based on the  X  X equence trans-ducer X  recently applied to phoneme recognition [ 16 ]; again, the main modification in the current work was that the bidi-rectional network contained multiple hidden layers.
The sequence transducer is able to emit zero, one, or many output labels for every input label, making it possible to train with input and target sequences of different lengths. Further-more, the transducer automatically learns to align the two sequences, so there is no need to pre-define which diacritic marks correspond to which letters. We will refer to this as the  X  X ne-to-many X  network.

The sequence transducer that aligns the input and target sequences consists of two separate RNNs X  X he input net-work, which is typically bidirectional, and the prediction network, which must be unidirectional X  X long with a feed-forward output network used to combine the outputs of the two RNNs. The two networks are used to determine a sep-arate distribution Pr ( k | t , u ) for every combination of input timestep t and output timestep u . Each distribution covers the K possible labels in the task plus a blank . Intuitively, the net-work chooses what to output, depending on both where it is in the input sequence and the outputs it has already emitted. For a length U target sequence y  X  , the complete set of TU deci-sions jointly determines a distribution over all possible align-mentsbetween x and y  X  ,fromwhichlog Pr ( y  X  | x ) canbeeffi-ciently determined with a forward X  X ackward algorithm [ 16 ].
Denote by ward hidden sequences of the input network and by p the hidden sequence of the prediction network. At each t , u ,the output network is implemented by feeding a linear layer to generate the vector l t and then feeding l p u to a tanh hidden layer to yield h t , u , and finally feeding h u toasize K l h y Pr ( k | t , u ) = exp where y t , u [ k ] is the k th element of the length K + malized output vector. For simplicity, we constrained all non-output layers to be the same size ( | | l |=| h More detail about this network and its training is in Ref. [ 16 ]. 3 Experimental setup Figure 5 summarizes our experimental setup. For evaluation purposes, the diacritized Arabic sentence is first encoded in a record suitable for RNN sequence transcription. This record consists of the diacritized target sequence and the undia-critized input sequence. The RNN transcribes the input with fully diacritized output sequence. We apply post-processing corrections to overcome some transcription errors. Finally, the corrected output is compared with the target to find the diacritization accuracy.

During training the RNN, both the input and the target sequences are presented to the network. For diacritizing an undiacritized sentence, the target field is not available and, consequently, no comparison is made. The following subsec-tions describe this method in more detail. 3.1 Data Our experimental data were drawn from ten books of the Tashkila collection of Islamic religious heritage books [ 37 ], along with the simple version of the Holy Quran [ 36 ]. These 11 books, summarized in Table 1 , are written in CA with full diacritization provided in HTML format. However, book 1 is partially diacritized and is selected to study the effect of using partially diacritized input on the diacritization accuracy.
We also used LDC X  X  Arabic Treebank (#LDC2010T08) of diacritized news stories, Part 3, v3.2 [ 27 ]. The LDC ATB3 is an example of the MSA and consists of 599 distinct newswire stories from the Lebanese publication An-Nahar that were published in 2002. This treebank is used in this work in the final experiments to facilitate comparisons with previous sys-tems.

Some of these classical books are large, multi-volume texts that take a long time to process, e.g., book 11 is 1306K words long (punctuation marks are not counted). As the table indicates, we randomly selected subsets of each book X  X  sen-tences in our experiments. To have varying data set sizes, we selected about 3% of book 1 on one end and 100% of book 4 and the ATB3 on the second end.

These books have wide range of sizes and sentence length styles. They widely differ in the utilization of the punctua-tion marks. However, they share similarities, particular to the Arabic language, such as the average number of letters per word, and the average percentages of letters with no, one, and two diacritics. Book 1 and the ATB3 have higher values of the first percentage because they are partially diacritized.
The Tashkila books are encoded in Code Page 1256 Win-dows Arabic and are organized in paragraphs that each con-tains one or more sentence. We prepare them for training and testing purposes by first converting them from HTML to plain text files that have one sentence per line. A paragraph is split into more than one sentence when it contains sentence-ending punctuation marks such as  X . X ,  X   X ,  X   X ,  X : X ,  X   X ,  X   X , and  X   X . However, the punctuation marks used for interjection do not break the sentence, e.g.,  X ( X  and  X ) X .

Note that the simple version of the Quran comes in a UTF-8 encoded text file. This file does not have punctuation marks and every Quranic verse (sentence) is in a separate line.
WeextractedthediacritizedversionoftheATB3sentences fromtheATB3integratedformat,everysentenceinaseparate line. The diacritized words in this format are available in the unsplit vocalization field which is encoded in Buckwalter Arabic transliteration [ 8 ]. As some words in the ATB3 are not available in the diacritized version, we use the source undiacritized version instead for these words.
 All data lines are then converted to Unicode encoding. Each line has a diacritized Arabic sentence. 3.2 Data encoding This section describes how the sentences are encoded for sequence transcription. These sentences are converted to sen-tence records that have each sentence in two versions: (1) input sequence, without diacritics, and (2) target sequence, with diacritics, comma separated. The non-diacritized ver-sion is generated from the original sentence after removing all diacritics.
Diacritics are represented in Unicode as additional char-acters. For example, the word  X  X humma X  has the two-field record  X   X ,  X   X  and is encoded as  X 062B 0645 X ,  X 062B 064F 0645 0651 064E X . Therefore, the diacritized target sequences are in general longer than the non-diacritized input sequences. This  X  X ne-to-many X  encoding is used with the one-to-many network.

For the one-to-one network, we encode every possible diacritization of every letter with a single symbol. Thus, the input and target sequences have same length. We use the following formula to obtain a unique code L for the letter with Unicode value l and possible diacritics d 1 and d 2 : L =
The most significant eight bits of the letter X  X  Unicode code are cleared: the masked code is shifted four bit positions to the left, and then the shifted code is ORed with the bit code(s) of its diacritics d 1 and d 2 that are shown in Table 2 ,ifany. The previous example is therefore encoded as  X 02B0 0450 X ,  X 02B5 045C X , where each input code is mapped to one target code. Note that an Arabic letter cannot have more than two diacritics, and if it has two diacritics, then one of them must be Shadda (bit code 1000). 3.3 Training parameters The RNNs are generally trained in this work using 88% of the available sentences. The remaining 12% are randomly selected from the available data and are used for testing pur-poses. For the ATB3, we use the same split of data between training and testing as in previous work [ 22 , 31 , 33 , 38 ]. The first, in chronological order, 509 newswire stories are used for training, and the last 90 newswire stories are used in testing.
We follow previous researchers in having a single set for development and testing, rather than separate development and test sets (as is common). Zitouni et al. [ 38 ]havepro-posed this split when they started the LDC ATB3 bench-mark, and this split is reluctantly followed by all following researchers [ 22 ]. This adoption allows us to compare our results to theirs.

However, using the test set as the validation set in training a neural network would generally give a net that is optimized for the test set. Therefore, we also experimented with using separate development and test sets. In the separate case, the training sentences described above are randomly split into 70% training set and 30% validation test. The resulting net is then used to find the diacritization accuracy on the held-aside test set.

All networks were trained with online gradient descent (weight updates after every sequence) using a learning rate of 10  X  3 , a momentum of 0.9, and random initial weights drawn uniformly from [ X  0 . 1 , 0 . 1 ] . The training algorithm uses an error gradient calculated with a combination of real time recurrent learning (RTRL) and back propagation through time (BPTT) [ 16 , 21 ]. The one-to-one network was stopped at the point of lowest label error rate on the validation set. The one-to-many network was stopped at the point of lowest log-loss.

InordertotraintheRNNtoachievehighaccuracy,wehave experimented with several training options. These options include transcription method, net parameters and size, and size of the training data. The results are presented in Sect. 4 . 3.4 Post-processing We use the following post-processing techniques to improve the output of the sequence transcription stage.  X  Letter correction : As the input letter sequences should  X  Sukun correction : The Sukun diacritic is used as an indi- X  Fatha correction : The letter that precedes the letters Alef  X  Dictionary correction : A dictionary is consulted to check 3.5 Accuracy evaluation The output of the post-processing stage is analyzed for accu-racy against the target sequence using the following two met-rics.  X  Diacritization error rate (DER), which is the proportion  X  Word error rate (WER), which is the percentage of incor-
We calculate these two metrics as is done in previous related work [ 31 , 33 ]: (1) all words are counted including numbers and punctuators, (2) each letter or digit in a word is a potential host for a set of diacritics, and (3) all diacritics on a single letter are counted as a single binary (True or False) choice. Moreover, the target letter that is not diacritized is skipped as there is no reference to compare the output let-ter X  X  diacritics with. 4 Experiments and results The following subsections present the experiments, and their results, that we carried out to train and tune the RNNs to automatically add diacritics on Arabic sentences. 4.1 One-to-many versus one-to-one We have evaluated the accuracy of the two sequence tran-scription networks: one-to-many and one-to-one. Figure 6 shows the diacritization error rates of these two networks for books of four representative sizes. The hidden layer size in both networks is 250 nodes, and the one-to-many network has a 250-node prediction network. For the four books, the one-to-one network has lower error. We have also noticed that this advantage holds even when we change the network size and the training options. Therefore, we adopt the one-to-one network and use it in the following experiments. 4.2 Weight noise distortion All networks were found to overfit on the training data, and we therefore used  X  X eight noise X  regularization [ 14 , 29 ]to improve generalization.

Following previous work [ 14 ], the standard deviation of the noise was fixed at 0.075 for all network weights. Figure 7 shows the effect of training the RNN with and without weight noise. The figure shows the DER for the four representative books. These experiments were conducted using the one-to-one network of one hidden layer. In all the experiments that we have conducted, weight noise gave considerably superior results. Therefore, we adopt this option in all the following experiments.

In fact, best results are obtained when the network is trained in two steps. First, the network is trained without weight noise. Then, it is retrained with weight noise, starting from the weight values found in the first step. 4.3 Network size As the neural network size and topology have a great impact on its performance, we conducted experiments to optimize these hyperparameters. Figure 8 shows the effect of changing the number of RNN hidden layers on the DER. Each hidden layer used here has 250 nodes. We have adopted the 250-node size because we noticed that using fewer nodes decreases the accuracy and using more nodes does not significantly improve it. This figure shows that the error decreases as we go from one layer to two layers. But the error increases slightly when we go from two to three. Additionally, three layers are slower than two layers. Therefore, we adopt the two-layer structure in the final system.

Similar results have been recorded in the speech recogni-tion literature, with additional neural network hidden layers rapidly decreasing the error up to a certain point, after which performance either levels out or slightly degrades as more layers are added [ 9 , 20 ]. 4.4 Data size We have conducted several experiments to study the effect of changing the training data size on performance. Figure 9 shows the DER for 11 books (the ten Tashkila books and the Quran) and the weighted average of their errors. This figure shows the error rates of two schemes: We have several observations about these experiments. Themainobservationisthattheerrorrategenerallydecreases as the training set size increases. The weighted average DER when using all training sets is 2.48 versus 2.88% for the indi-vidual training. Moreover, the error rate generally decreases as the book size increases. Note that the books are ordered in increasing size as we go from book 1 to book 11, and the DER generally decreases as we go from left to right in Fig. 9 .
For nine out of the 11 books, the  X  X ll training X  scheme gives lower DER than the  X  X ndividual training X  scheme. Only books 3 and 4 have higher error rates. These two books are Sahyh Muslim and the Quran, and they seem to benefit more from the specific individual training than the  X  X ll training. X  It seems that their characteristics are, to some degree, different form the other books. For example, the Quran has the largest letters per word average among the used samples and book 3 has the second smallest average (see Table 1 ).
The training time for the RNNs was in general very long, especially for the larger datasets. For example, it took more than three months to train the RNN using all 11 training sets (a total of 2009 K words). However, activating a trained RNN is relatively fast.

These experiments use the one-to-one network, two 250-node hidden layers, and the weight noise distortion training option.
 This RNN setup is also used with the ATB3 samples. Table 3 shows the results of three experiments where three different training data are used: the training sets of the 11 books only, the 11 books and the ATB3, and the ATB3 only, respectively. The table shows that the worst result is got when the training is done using the classical 11 books. The results are improved when the MSA ATB3 train data are added to the 11 books. However, best DER at 2.74% and WER at 9.31% are obtained when training is done using the ATB3 train data only. This indicates that there are large differences between CA and MSA and different networks should be used with each type.
 For the separate training and test sets case described in Sect. 3.3 , we noticed that the DER is higher by less than 5% compared with the single set case. For the ATB3 samples, the DER is 2.87% (vs. 2.74%) and for the  X  X ll training X  samples, it is 2.57% (vs. 2.48%). This increase is expected as separate training does not tune the net for the test set. 4.5 Influence of post-processing The error rates reported in the previous subsections include the improvements due to Sukun and Fatha corrections, but not dictionary correction. Table 4 details the impact of all three post-processing techniques for the  X  X ll training X  and ATB3 experiments.

The Sukun correction has more effect on the ATB3 than  X  X ll training X  (6.3 vs. 2.5%), presumably because the ATB3 is partially diacritized.

Thedictionaryusedinthe X  X lltraining X  X aseisconstructed from the training sets of the 11 books, and the dictionary used with the ATB3 is constructed from its training set (first 509 newswire stories). The dictionary correction gives significant error reduction of 19.3% in the  X  X ll training X  case. However, it gives smaller reduction of only 1.3% with ATB3. The rea-son for this smaller improvement is the smaller ATB3 dic-tionary that is constructed from the earlier newswire stories. The last 90 stories used in the test include many new words not in the earlier stories.

In fact, dictionary correction in the  X  X ll training X  case reduces the error rates for all 11 books, as shown in Fig. 10 , with the average DER dropping from 2.48 to 2.09%. In the ATB3 case, the DER is marginally reduced from 2.74 to 2.72%.

To conclude presenting results, we present the final WER for the  X  X ll training X  case in Fig. 11 . The average WER at 5.82% is naturally higher than DER, but the relative stand-ings of the 11 books is maintained. In the ATB3 case, the WER is 9.07%. 5 Discussion Table 5 shows the diacritization results of best published sys-tems and our system. For each system, the table shows its publication year, the database used in evaluating it, and its DERandWER.Thetableshowsthatoverthelast8years,dia-critization accuracy has generally improved and our system continues this trend with a significant boost over its prede-cessors.

The table shows two sets of results reported by Zitouni et al. who defined the ATB3 benchmark. The first set was obtained using a rich collection of input features, including POS tags, while the second set used only the raw undia-critized sentences, the same as our system.

Said et al. X  X  system, to the best of our knowledge, is the system that has the best reported accuracy on the ATB3. For this database, our system provides 25% DER improvement and 20% WER improvement compared with Said et al. X  X  sys-tem. And this improvement is achieved with our approach that uses the raw undiacritized sentences only, without uti-lizing a hybrid approach combining statistical and rule-based techniques.
 In evaluating his system, Hifny also used books from the Tashkila collection [ 37 ], similar to our work. Compared with the ATB3, Tashkila is much larger and is freely available. For the Tashkila samples, our system provides 38% DER improvement and 35% WER improvement over Hifny X  X  sys-tem.
 The results shown for the KAD system were reported by Bahanshal and Al-Khalifa [ 6 ]. Among three available auto-matic diacritization systems evaluated in their study (KAD, Sakhr, and Mishkal), KAD has the best results for diacritizing verses from the Holy Quran. However, our system has much better accuracy in diacritizing Quran verses, especially when it is trained only on some of the Quran verses. For Quran, our system provides 45% DER improvement compared with KAD. This table shows the Quran results for the  X  X ndividual training X  case followed by the  X  X ll training X  case.
In addition to showing the error rates when all diacritiza-tion errors are counted, Table 5 shows the error rates when the errors in diacritizing the last letter of each word are ignored. Although the proportion of last letters to all letters is one-to-4.64, the error rates are significantly lower when these errors are ignored. The diacritic of the last letter is generally determined by the syntax and is usually harder to get right compared with the diacritics of other letters.

The last column in Table 5 shows the difference between the all-diacritics DER and ignore-last DER. This difference is the proportion of last-letter diacritization errors of all letters. This column demonstrates that our system is more successful than previous work in reducing this error. This error rate (1.34) is 33% lower than Said et al. X  X  system on the ATB3.
Now, we discuss the errors of our system. Table 6 shows the distribution of word errors according to the number of diacritic errors per word and whether there is an error in the diacritic of the last letter or not. The table shows that more than three quarters of the word errors (77.5%) have one diacritic error, two errors per word comprise 18.2%, and three errors or more per word are not frequent (4.3%).
The table also shows that about half of the word errors (49.4%) have an error in the last letter. As there are an average of about four letters per word, one would expect to have smaller error rate in the last letter had the errors been uniformly distributed. However, as other researchers have noticed, the diacritic of the last letter highly depends on the context and is hard to predict in the Arabic language that has rich inflection rules [ 38 ]. Moreover, last-letter errors are hard to correct using dictionaries.

We have manually inspected 200 error samples. Table 7 shows six sample sequences that have errors. The words that have errors are underlined and the table shows the target (test) sequence and the output sequence.

We have noticed that, in about 55% of the samples, the word that has diacritic error is a valid Arabic verb or noun. Sample 1, for example, shows that target stem verb  X  X affa X  (command form of the verb describe). Sample 2 shows that the noun  X  X ubbu X  (love) is output as  X  X abbu X  (seeds). Sample 3 shows a counter example where the output word
Sample 4 shows an example where the output word is missing the diacritic of the last letter  X  X khiyk X  (your brother). This word has the possessive pronoun suffix  X  X a X  . We have traced the reason of this error to the training data used. We have noticed that this pronoun suffix is not diacritized in many cases in the training data.

We also noticed that a significant fraction of the error words (23%) have prefixes or suffixes or both, as in sam-ples 1, 4, and 5. The error word in sample 5 has both; it has the prefix  X  X a X  , the stem verb  X  X arawunna X  , and the pronoun suffix  X  X aa X  . These complex words are the hard-est words to be correctly diacritized, especially because the inflection dicritic often goes to the last letter in the stem and not to the last letter of the suffix.

We have estimated that about 3% of the errors are due to diacritization errors in the test samples used. For exam-ple, sample 6 shows that the Fatha in the word  X  X aa X  was mistakenly entered after the last letter ( ), not after the first letter.

It seems that the Shadda diacritic is harder to restore than the average diacritic; although the samples have about 6.5% Shadda diacritic relative to all diacritics, Shadda errors are about 9.8% of the DER. Note that the problematic words in samples 1, 2, 3, and 5 have Shadda diacritic in the target or the output word. 6 Conclusions In this paper, we have tackled the problem of adding diacrit-ics to Arabic text as a sequence transcription problem using deep bidirectional LSTM network. Our approach is a pure machine-learning approach. The network is trained on tran-scribing raw undiacritized Arabic text sequences into fully diacritized sequences.

We have experimentally selected the type and configura-tion of the used network. Best accuracy is obtained when using the one-to-one network, training with weight noise distortion, and using two hidden layers each of 250-node size.

We have used samples from ten books of the Tashkila col-lection and the Quran to test this approach. The experimental evidence indicates that the accuracy improves as the training set size increases. Best average results are obtained when training the network using the training sets of the 11 books.
Inordertoimproveaccuracy,weusedfourpost-processing correction techniques: letter, Sukun, Fatha, and dictionary. These techniques reduce the average DER by 23.8% for the 11 books. The dictionary correction is responsible of 19.3% of this reduction.
We have also tested this approach on the LDC ATB3 that is widely used in related work. Although some words in the ATB3 are not, or partially, diacritized and the ATB3 is smaller than the 11 books samples, this approach achieves high accuracy on the ATB3 as well. The achieved DER of 2.72% and WER of 9.07% are better than the best published results by 25 and 20%, respectively. Also the last letter dia-critization error rate is 33% lower than the best published results.

These results are mainly due to the sequence transcrib-ing capabilities of the network used and its ability to exploit long-range context dependencies in the forward and back-warddirections.Evenwithoutthepost-processingtechniques used, this approach has a DER better than the best published results by 18%.
 This approach outperforms leading hybrid approaches. It gives better results without utilizing available rule-based techniques such as morphological analysis. However, we think that integrating such techniques as pre-processing stages before the sequence transcription stage could provide higher accuracy.

We intend to experiment with adding such techniques to this approach. This future work is motivated by the observa-tion that significant fraction of the errors is in complex words that have prefixes, suffixes, or both. We expect that provid-ing the morphological analysis of such words to the RNN would provide it with better information to achieve higher accuracy.
 References
