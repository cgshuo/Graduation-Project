 Statistical machine learning techniques for data classifica-tion usually assume that all entities are i.i.d. (independent and identically distributed). However, real-world entities of-ten interconnect with each other through explicit or implicit relationships to form a complex network. Although some graph-based classification methods have emerged in recent years, they are not really suitable for complex networks as they do not take the degree distribution of network into consideration. In this paper, we propose a new technique, Modularity Kernel, that can effectively exploit the latent community structure of networked entities for their classi-fication. A number of experiments on hypertext datasets show that our proposed approach leads to excellent classifi-cation performance in comparison with the state-of-the-art methods.
 H.2.8 [ Database Management ]: Database Applications X  data mining ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelli-gence ]: Learning; I.5.2 [ Pattern Recognition ]: Design Methodology X  classifier design and evaluation Algorithms, Performance, Experimentation Graph Mining, Community Discovery, Semi-Supervised Learn-ing, Kernel Methods, Hypertext Classification.
We are in a connected age: real-world entities often in-terconnect with each other through explicit or implicit re-lationships to form a complex network [34], such as social networks, information networks, technological networks and biological networks. One of the  X 10 challenging problems in data mining research X  1 [47] highlighted in ICDM-2005 is  X  X ata mining in a network setting X .

In this paper, we address the problem of classifying net-worked entities (or within-network classification). Given a set of both labelled and unlabelled entities that are intercon-nected with each other, our goal is to assign class labels to the unlabelled entities, as shown in Figure 1. For example, in the application of web spam filtering 2 , the web pages are connected via hyperlinks in a complex network where some web pages are known to be  X  X pam X  while some web pages are known to be  X  X on-spam X , and the task is to classify the other web pages into the  X  X pam X  or  X  X on-spam X  class.

Statistical machine learning [5] techniques for data classi-fication usually assume that all entities are i.i.d. (indepen-dent and identically distributed). Although such techniques could be applied straightforwardly to the problem of classi-fying networked entities, we anticipate that a better classifi-cation performance would be achieved by taking advantage of the additional link information because entities tend to connect to other members of their own class.

Most complex networks in the real-world are large but sparse, and they are found to have some common statis-tical properties such as the small-world phenomenon and the scale-free (power-law) degree distribution [34, 33, 9, 16], which distinguish them with simple (regular or random) graphs. Due to the sparsity of network, two entities in the same class may appear to be dissimilar on surface when they are not directly connected, but such a pair of entities are likely to be connected indirectly through some paths with a number of intermediate entities. Therefore it is usually not promising to use links directly as additional features. In other words, it is important to exploit not only local link in-formation but also global structure information for effective classification of networked entities. We think the underlying community structure [35] of network contains valuable clues http://www.cs.uvm.edu/  X  icdm/10Problems/ http://webspam.lip6.fr/ Figure 2: The community structure of the Cora-HA network. about the right classification of its entities. For example, the community structure of the Cora-HA network (see Section 5) is visualised in Figure 2. It is clear that the nodes in the same class (with the same colour) tend to group together in a community.

Some graph-based classification methods have emerged in recent years (see Section 2). However, they are not really suitable for complex networks because they do not take the degree distribution of network into consideration. For exam-ple, the degree distribution of the above Cora-HA network roughly obeys the power law (as shown in Figure 3), which is very different with that of a simple graph.
 In this paper, we proposes a new technique, Modularity Kernel, that can effectively exploit the latent community structure [35] of networked entities for their classification. A number of experiments on hypertext datasets show that our proposed approach leads to excellent classification per-formance in comparison with the state-of-the-art methods.
The rest of this paper is organised as follows. In Section 2, we review the related work. In Section 3, we formally define the problem of classifying networked entities. In Section 4, we present our approach in details. In Section 5, we show the experimental results. In Section 6, we make conclusions.
Recently many approaches to the problem of classifying networked entities (such as web pages) have been proposed from different perspectives.

One possibility is to do network-aware feature engineer-ing or dimensionality reduction first and then employ stan-dard classification methods. Previous research studies have shown that using links directly as features does not work well in practice [12, 53]. Another simple method is to ex-pand each entity X  X  feature vector by incorporating the fea-Figure 3: The degree distribution ( Pr[ k ]  X  k )ofthe Cora-HA network. tures of its neighbours (directly-linked entities) [48], but this method seems to have difficulties with parameter tuning and often does not provide a robust solution [12, 1]. Inspired by the success of PageRank [10] and HITS [30] in web search and mining, people have tried to apply link analysis tech-niques to extract network structure features. Gy  X  ongyi et al. propose a technique similar to Personalised PageRank for large-scale web page classification [24]. Cohn and Hofmann propose to address hypertext classification by constructing a latent space of both content and link information [18] where probabilistic LSI (PLSI) is used for content analysis [26] and probabilistic HITS (PHITS) is used for link analysis [17]. Joachims et al. propose to address hypertext classification in the kernel methods [38] framework by combining bag-of-words kernels and bag-of-links kernels [29]. Recently Zhu et al. propose to use the technique Matrix Factorization (MF) to find latent factors from both content matrix and link ma-trix for the task of hypertext classification, and they also extend it to Supervised Matrix Factorization (SupMF) by taking label information into account as well [53].
An alternative way is to use relational learning [27] meth-ods such as Markov Random Fields [12, 11, 1], Inductive Logic Programming [43, 19], Probabilistic Relational Mod-els [21], Relational Markov Networks [45], Dependency Net-works [32] and Lazy Associative Classification [46]. Some comprehensive studies in this area can be found in [27, 31]. However, such relational learning methods do not necessar-ily converge, or only converge to a local optimal solution.
Yet another (probably more principled) way is to use semi-supervised learning methods [13, 54] based on graph par-titioning [15, 40], such as MinCut ( st -Cut) [6, 7], t -step Markov Random Walk [44], Gaussian Random Fields and Harmonic Functions [55], Spectral Graph Transducer [28], Tikhonov Regularisation [3], Manifold Regularisation [42, 41, 4], Cluster Kernel [14], Local and Global Consistency [50], Directed Graph Regularisation (DGR) [52, 51] and Laplacian Kernel [25, 2]. However, while these methods work well for simple graphs (like k -nearest-neighbours graphs), they are not really suitable for complex networks as they to-tally ignore the degree distribution of network.
We define the problem of classifying networked entities formally as follows.
Given a complex network (graph) G that consists of n entities (nodes) and m links (edges), we can describe the network structure using its n  X  n adjacency matrix A with elements A ij  X  0 representing the number or weight of edges between node x i and node x j . Since real-world complex networks are usually sparse, most elements in A should be 0. In this paper we focus on undirected networks, so A is symmetric. The degree of a node x i , i.e., the number of edges connected to a node x i ,isgivenby k i = j A ij .
Without Loss of generality, suppose that there are two classes of entities in the network: the entities in one class are labelled with +1 and the entities in the other class are labelled with  X  1. In the given set of networked entities X = { x i } n i =1 ,thereare l entities X l := { x i } l i =1 the labels Y l := { y i } l i =1 are provided, and u entities X { x j } l + u j = l +1 for which the labels are absent. Obviously X = X l X u and n = l + u . Our goal is to learn a classification function f so that the class of any networked entity x can be accurately predicted by the sign of f ( x ). This is actually semi-supervised learning , i.e., learning from both labelled and unlabelled data [13, 54].
Let us consider the problem of classifying networked en-tities in the framework of kernel methods [38, 39, 49]. A prominent advantage of kernel methods is that they typi-cally lead to a convex optimisation problem so the global optimal solution can be computed efficiently.
 For a Mercer kernel K : X  X  X  X  R , there is an associated Reproducing Kernel Hilbert Space (RKHS) H K of functions X  X  R with the corresponding norm K . Given a set of labelled examples X l as well as a set of unlabelled examples X u , typical kernel methods for semi-supervised learning es-timate the classification function to be where the first term V is a loss function defined on X l ,the second term f 2 K is a regulariser defined on X l X u and the parameter C controls its relative weight. The regulariser f 2 K imposes smoothness conditions on f to avoid over-fitting.

Different choices of the loss function V in the above opti-misation problem lead to different learning algorithms. For example, substituting the logistic loss ln(1 + exp(  X  y i for V we get Regularised Logistic Regression (RLR); substi-tuting the hinge loss (1  X  y i f ( x i )) + =max(0 , 1  X  y for V we get Support Vector Machine (SVM).

A straightforward extension of the classic Representer The-orem [38] states that the solution to the above optimisation problem exists in H K and can be written as an expansion in terms of both labelled and unlabelled examples: This can be proved by a variation of the simple orthogo-nality argument [38], as shown in [4]. Therefore the above learning problem can be reduced to optimising over the fi-nite dimensional space of coefficients {  X  i } l + u i =1 algorithmic basis of kernel methods.
In real-world complex networks, entities (nodes) tend to group into communities, with a high density of links (edges) within communities and a low density of links (edges) be-tween them [35]. The latent community structure of a com-plex network must contain valuable clues about the right classification of entities (nodes) in the network, because en-tities are likely to connect to other members of their own class. A  X  X ood X  classification function f for networked en-tities should align well not only with the labelled data but also with the community structure  X  entities in the same community should have a high probability to be classified into the same class.

In the ideal situation, each class of entities in the complex network would group into a separate community. So if the value of f is restricted to be either +1 or  X  1, then f = ( f ( x 1 ) ,...,f ( x n )) T can be regarded as the binary indication vector for a division of the network into communities, and f f = n . Furthermore, let g i denote the group to which vertex x i belongs. The function  X  ( g i ,g j )=1if g i = g  X  ( g i ,g j ) = 0 otherwise. It is easy to see that  X  ( g (1 + f ( x i ) f ( x j )).
Most existing graph-based classification methods are rooted in graph partitioning [40] that divides the network into groups by minimising the cut-size , i.e., the number of edges running between different groups of nodes:
We can rewrite the cut-size for the division of network f in matrix form as where L is the Laplacian matrix [15] defined as with D =diag( k 1 ,...,k n ).

It is well-known that all eigenvalues of L are non-negative, and the graph G has z connected components if and only if L has z zero eigenvalues with corresponding eigenvec-tors being piece-wise constant on the connected components [15]. Without loss of generality, assume that the eigenval-ues of L in increasing order are 0 =  X  1 = ... =  X  z  X  z +1  X  ...  X   X  n and the corresponding eigenvectors of L are u 1 ,..., u z , u z +1 ,..., u n .

If we allow the the elements of f to take any real value but just keep the constraint f T f = n ,a non-trivial division of network that minimises the cut-size S is given by f = u z +1 the eigenvector of L corresponding to the smallest non-zero eigenvalue [15]. The sign of u z +1  X  X  i -th element indicates the class to which x i belongs to and the value of of u z +1 i -th element indicates the strength of membership.
People have tried to use the cut-size S as the regulariser for learning and found that it is equivalent to taking the pseudo-inverse of Laplacian matrix, L + ,asthekernelma-trix, which is called Laplacian Kernel (LapKer) [25, 2]. It has been shown that the Laplacian Kernel is interestingly connected to the resistance distance (the total resistance be-tween two nodes) and the commute time (the average length of a random walk between two nodes) over the graph.
The normalised Laplacian [15] that has many attractive theoretical properties can also be used in the above formulae. Since using  X  L + stead of L + as Laplacian Kernel consistently provides bet-ter classification performance , the reported Laplacian Ker-nel experimental results (in Section 5) are all based on  X 
Despite its success on simple graphs (such as k -nearest-neighbours graphs), Laplacian based graph partitioning is poor in detecting natural communities in real-world com-plex networks, because the degree distribution of network has been totally ignored. The fundamental problem of us-ing this technique for community discovery is that cut sizes are not really the right thing to optimise since they don X  X  accurately reflect our intuitive concept of network commu-nities. A good division of a network into communities is not merely one in which the number of edges running across communities is small. Rather, it is one in which the num-ber of edges across communities is smaller than expected .It has been reported that Laplacian based graph partitioning often fails to find the right division of a complex network [37, 36]. Consequently the effectiveness of semi-supervised learning methods based on graph partitioning for classifying networked entities would be limited.
One proven-effective approach to community discovery is maximising the quality function known as modularity [37, 36] over the possible divisions of a network: where P ij =( k i k j ) / (2 m ). In fact, P ij is the expected number of edges between node x i and node x j in the  X  X ull model X   X  a random graph with the same degree distribution as the given network. Optimising modularity reflects our intuition that the number of edges within communities should be higher than expected by chance. Only if the number of within-community edges 1 2 ij A ij  X  ( g i ,g j ) is significantly higher than it would be expected purely by chance 1 2 ij P ij  X  ( g can we justifiably claim to have found significant community structure. Maximising modularity has been shown to pro-duce excellent community discovery results in practice [20, 23].

We can rewrite the modularity for the division of network f in matrix form as where M is the modularity matrix 3 [36] defined as
Unlike the graph Laplacian, the modularity matrix M is not guaranteed to be positive semi-definite, i.e., it may have negative eigenvalues. Without loss of generality, as-sume that the eigenvalues of M in decreasing order are  X  1  X  ...  X   X  n and the corresponding eigenvectors of M are u 1 ,..., u n .
We have extended the original definition of modularity ma-trix [36] to weighted networks here.

If we allow the elements of f to take any real value but just keep the constraint f T f = n , the optimal division of network that maximises the modularity Q is given by f = u 1 ,the eigenvector of M corresponding to the largest eigenvalue [36]. The sign of u 1  X  X  i -th element indicates the class to which x i belongs to and the value of of u 1  X  X  i -th element indicates the strength of membership.
Motivated by modularity based community discovery, we propose to use the following matrix as the kernel matrix: where  X  1  X  ...  X   X  p &gt; 0arethe p positive eigenvalues of M and u 1 ,..., u p are the p corresponding eigenvectors. Since the original modularity matrix M is not guaranteed to be positive definite, it could not be used straightforwardly as the kernel matrix otherwise the generated kernel func-tion would be invalid and the resulted optimisation problem would no longer be convex. According to linear algebra [22], M is the positive definite matrix that best approximates the modularity matrix M (in terms of M  X  M F ), therefore we use it instead 4 and name this technique Modularity Kernel (ModKer).
 The Modularity Kernel could be justified as follows. Let H ( G ) be the linear space of real-valued functions defined on the graph, i.e., an n -dimensional vector space whose el-ements are the real-valued vector g =( g 1 ,...,g n ) T .On H ( G ) we introduce the inner-product This inner-product function is well-defined since is symmetric and positive definite. Moreover, the function g = g , g , g  X  X  ( G ) is a norm that measures the function smoothness or complexity. With a little derivation, it is easy to see that minimising the above defined norm of function f leads to f = u 1 which gives the optimal division of network into communities (according to the analysis in the above section). Therefore it is reasonable to use f 2 f , f = f T M  X  1 f as the regulariser in the kernel methods learning framework. The reproducing kernel K of H ( G ) should be an n  X  n matrix such that for every g  X  X  ( G )the reproducing kernel property g i = K i , g holds, where K i the i -thcolumnof K .Infact, K = M ,becauseif g  X  X  ( G ) then M  X  1 Mg = g , which implies that g i = e i M M  X  1 g = K
M  X  1 g = K i , g where e i is the i -th coordinate vector, i.e., the reproducing kernel property holds. Hence K = M is the reproducing kernel of H ( G ).

Modularity Kernel can effectively exploit the latent com-munity structure of networked entities for their classifica-tion. It is particularly suitable for complex networks because it takes the degree distribution into account.
We have found that in practice using only a small number (about 30 X 50) of largest positive eigenvalues and eigenvec-tors would achieve almost as good classification performance as using all p positive eigenvalues and eigenvectors.
The proposed Modularity Kernel technique can be used straightforwardly for classifying networked entities. How-ever, it should be beneficial to exploit both entity content information and entity link information [53]. We tried the following three linear combination methods with a parame-ter 0  X   X   X  1. To distinguish the usage of Modularity Kernel based on only entity link information and that based on both entity content and entity link information, we denote the former ModKer (link) and the latter ModKer (content+link) .
We conduct our experiments on two collections of real-world datasets: WebKB 5 and Cora 6 .

The WebKB datasets consist of about 6,000 web pages from the computer science departments of four universities: Cornell, Texas, Washington and Wisconsin . The web pages are classified into categories such as  X  X ourse X ,  X  X aculty X  and  X  X tudent X . For each dataset, we use the co-citation graph derived from the original directed hyperlinks (citations) as http://www.cs.cmu.edu/  X  webkb/ http://www.cs.umass.edu/  X  mccallum/code-data.html the link-based entity network for our algorithms, i.e., two pages (nodes) are connected by an edge if there is a third page linking to or being linked to both of them, and multiple edges are allowed between one pair of nodes. This is because for web data communities formed by co-citations are usually more reliable than by hyperlinks, which has been reported in past research studies [49] and also confirmed by our ex-periments. The characteristics of the WebKB datasets are shown in Table 1.

The Cora datasets consist of the abstracts and references of about 34,000 computer science papers. In our experi-ments, we use only the papers in four research areas, Data Structure (DS), Hardware and Architecture (HA), Machine Learning (ML) and Programming Language (PL) ,andwe discard the papers without reference to other papers in the same area. The papers are classified according to their sub-fields in the research area. For each dataset, we use the undirected graph derived from the original directed refer-ences as the link-based entity network for our algorithms, i.e., two papers (nodes) are connected by an edge if one of them cites the other or vice versa, and multiple edges are allowed between one pair of nodes. The characteristics of the Cora datasets are shown in Table 2.
We perform our hypertext classification experiments on each of the above datasets using Regularised Logistic Re-gression (RLR) and Support Vector Machine (SVM) [38, 39] with the proposed Modularity Kernel 7 .

We measure classification performance by 5-fold cross-validation accuracy (mean  X  std %), i.e., the dataset is randomly split into five equal-size folds and the classifica-tion experiment is repeated for five times, each time with one fold for test and four other folds for training. For exper-iments with our proposed Modularity Kernel technique, we do not tune the RLR or SVM parameters but simply take their default values. For experiments using other methods, the parameters are tuned through cross-validation on the training-folds data, as in [53].
We first compare the classification performance of RLR and SVM with ModKer (link). The experimental results on the WebKB datasets and the Cora datasets are shown in Ta-ble 3 and 4 respectively. It is clear that SVM with ModKer outperforms RLR with ModKer on all the datasets. There-fore SVM is used in all the following ModKer experiments.
We then compare the effectiveness of three combination methods (see Section 4.4) using SVM with ModKer (con-tent+link). The experimental results on the WebKB datasets and the Cora datasets are shown in Table 5 and 6 respec-tively, where the combination parameter  X  is simply set to its default value 0.5. It is clear that Graph Combination out-performs Regulariser Combination and Kernel Combination on all the datasets. Therefore Graph Combination is used in all the following ModKer experiments.
Each kernel matrix is normalised [38, 39] in our experi-ments. Figure 4: Effect of the combination parameter  X  on the WebKB datasets.
 Figure 5: Effect of the combination parameter  X  on the Cora datasets.
Furthermore, we study the effect of the combination pa-rameter  X  using SVM with ModKer (content+link). The experimental results on the WebKB datasets and the Cora datasets are shown in Figure 4 and 5 respectively, where the combination parameter  X  is varied from 0 to 1. The opti-mal  X  on the WebKB datasets is about 0.1, which implies that the correct classification is mainly determined by the link information. The optimal  X  on the Cora datasets is around 0.5, which implies that the content information and the link information have roughly equal influences on the correct classification. Nevertheless, we do not tune the com-bination parameter but simply set  X  =0 . 5 when comparing ModKer with other approaches in the next section.
We finally compare our proposed approach with the state-of-the-arts methods for classifying networked entities, some using only entity link information while some using both entity content and entity link information. When using en-tity content information, each entity is regarded as a bag-of-words and pre-processed by TF  X  IDF weighting. The meth-ods being compared are briefly described as follows.
The significance of performance difference is assessed us-ing McNemar X  X  test . When we use the term  X  X ignificant X  in the following text, we mean a P -value  X  0 . 05.

We show the experimental results of competing methods on the WebKB datasets in Table 7 and Figure 6. When using only entity link information, Modularity Kernel works sig-nificantly better than Laplacian Kernel on all four datasets, and outperforms Directed Graph Regularization on three of the four datasets. When using both entity content and en-tity link information, Modularity Kernel works significantly better than PLSI+PHITS, and slightly better than (Super-vised) Matrix Factorization.

We show the experimental results of competing methods on the Cora datasets in Table 8 and Figure 7. When using only entity link information, Modularity Kernel works sig-nificantly better than Laplacian Kernel and Directed Graph Regularization on all four datasets. When using both en-tity content and entity link information, Modularity Kernel works significantly better than PLSI+PHITS, Matrix Fac-torization and Supervised Matrix Factorization.
 In summary, Modularity Kernel is superior to Laplacian Kernel for real-world complex networks, and it can provide excellent classification performance in comparison with the state-of-the-art methods. It has also been shown that Mod-ularity Kernel works well no matter whether the link-based network is derived from co-citat ions or direct-references and whether it is combined with the content-based network.
We propose a new technique, Modularity Kernel, that can effectively exploit the latent community structure of net-worked entities for their classification. A number of exper-iments on hypertext datasets show that the proposed ap-proach leads to excellent classification performance in com-parison with the state-of-the-art methods.

So far we have focused on classification within undirected networks. It would be interesting to create a directed ver-sion of Modularity Kernel for classification within directed networks. We also plan to use Modularity Kernel for other applications such as clustering and ranking of networked en-tities, and improve the scalability of the algorithm. We would like to thank Dr. Shenghuo Zhu for sharing his pre-processed datasets. Thanks also to the anonymous re-viewers for their helpful comments. [1] R. Angelova and G. Weikum. Graph-based text [2] A. Argyriou, M. Herbster, and M. Pontil. Combining [3] M. Belkin, I. Matveeva, and P. Niyogi. Regularization [4] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold [5] C.M.Bishop. Pattern Recognition and Machine [6] A. Blum and S. Chawla. Learning from labeled and [7] A. Blum, J. D. Lafferty, M. R. Rwebangira, and [8] A. Blum and T. Mitchell. Combining labeled and [9] S. Boccaletti, V. Latora, Y. Moreno, M. Chavez, and [10] S. Brin and L. Page. The anatomy of a large-scale [11] S. Chakrabarti. Mining the Web: Discovering [12] S. Chakrabarti, B. Dom, and P. Indyk. Enhanced [13] O. Chapelle, B. Scholkopf, and A. Zien, editors. [14] O. Chapelle, J. Weston, and B. Scholkopf. Cluster [15] F. R. K. Chung. Spectral Graph Theory .American [16] F. R. K. Chung and L. Lu. Complex Graphs and [17] D. Cohn and H. Chang. Learning to probabilistically [18] D. A. Cohn and T. Hofmann. The missing link -a [19] M. Craven and S. Slattery. Relational learning with [20] L. Danon, J. Duch, A. Diaz-Guilera, and A. Arenas. [21] L. Getoor, N. Friedman, D. Koller, and B. Taskar. [22] G. H. Golub and C. F. V. Loan. Matrix Computations . [23] M. Gustafsson, A. Lombardi, and M. Hornquist. [24] Z. Gy  X  ongyi, H. Garcia-Molina, and J. Pedersen. Web [25] M. Herbster, M. Pontil, and L. Wainer. Online [26] T. Hofmann. Probabilistic latent semantic indexing. In [27] D. Jensen, J. Neville, and B. Gallagher. Why collective [28] T. Joachims. Transductive learning via spectral graph [29] T. Joachims, N. Cristianini, and J. Shawe-Taylor. [30] J. M. Kleinberg. Authoritative sources in a [31] S. A. Macskassy and F. Provost. Classification in [32] J. Neville and D. Jensen. Dependency networks for [33] M. Newman, A.-L. Barabasi, and D. Watts. The [34] M. E. J. Newman. The structure and function of [35] M. E. J. Newman. Detecting community structure in [36] M. E. J. Newman. Finding community structure in [37] M. E. J. Newman. Modularity and community [38] B. Scholkopf and A. J. Smola. Learning with Kernels . [39] J. Shawe-Taylor and N. Cristianini. Kernel Methods [40] J. Shi and J. Malik. Normalized cuts and image [41] V. Sindhwani, M. Belkin, and P. Niyogi. The [42] V. Sindhwani, P. Niyogi, and M. Belkin. Beyond the [43] S. Slattery and M. Craven. Combining statistical and [44] M. Szummer and T. Jaakkola. Partially labeled [45] B. Taskar, P. Abbeel, and D. Koller. Discriminative [46] A. Veloso, W. M. Jr., and M. J. Zaki. Lazy associative [47] Q. Yang and X. Wu. 10 challenging problems in data [48] Y. Yang, S. Slattery, and R. Ghani. A study of [49] T. Zhang, A. Popescul, and B. Dom. Linear prediction [50] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and [51] D. Zhou, J. Huang, and B. Scholkopf. Learning from [52] D. Zhou, B. Scholkopf, and T. Hofmann.
 [53] S. Zhu, K. Yu, Y. Chi, and Y. Gong. Combining [54] X. Zhu. Semi-supervised learning literature survey. [55] X. Zhu, Z. Ghahraman, and J. D. Lafferty.

