 Arun Tejasvi Chaganty CHAGANTY @ CS . STANFORD . EDU Stanford University, Stanford, CA, USA Latent-variable graphical models provide compact repre-sentations of data and have been employed across many fields (Ghahramani &amp; Beal, 1999; Jaakkola &amp; Jordan, 1999; Blei et al., 2003; Quattoni et al., 2004; Haghighi &amp; Klein, 2006). However, learning these models remains a difficult problem due to the non-convexity of the negative log-likelihood. Local methods such as expectation maxi-mization (EM) are the norm, but are susceptible to local optima.
 Recently, unsupervised learning techniques based on the spectral method of moments have offered a refreshing per-spective on this learning problem (Mossel &amp; Roch, 2005; Anandkumar et al., 2011; 2012b;a; Hsu et al., 2012; Balle &amp; Mohri, 2012). These methods exploit the linear algebraic properties of the model to factorize moments of the ob-served data distribution into parameters, providing strong theoretical guarantees. However, they apply to a limited set of models, and are thus not as broadly applicable as EM. In this paper, we show that a much broader class of dis-crete directed and undirected graphical models can be con-sistently estimated: specifically those in which each hidden variable has three conditionally independent observed vari-ables ( X  X iews X ). Our key idea is to leverage the method of moments, not to directly provide a consistent parame-likelihood-based objective. Notably, our method applies to latent undirected log-linear models with high treewidth. The essence of our approach is illustrated in Figure 1, which contains three steps. First, we identify three views for each hidden variable h i (for example, x a are conditionally independent given h 1 ) and use the tensor factorization algorithm of Anandkumar et al. (2013) to es-timate the conditional moments P ( x a for each i (Section 3). Second, we optimize a compos-ite marginal likelihood to recover the marginals over sub-sets of hidden nodes (e.g., P ( h 2 ,h 3 ,h 4 ) ). Normally, such a marginal likelihood objective would be non-convex, but given the conditional moments, we obtain a convex objec-tive, which can be globally optimized using EM (see Sec-tions 4 and 4.2). So far, our method has relied only on the conditional independence structure of the model and ap-plies generically to both directed and undirected models. The final step of turning hidden marginals into model pa-rameters requires some specialization. In the directed case, this is simple normalization; in the undirected case, we need to solve another convex optimization problem (Sec-tion 5). Let G be a discrete graphical model with observed variables x = ( x 1 ,...,x L ) and hidden variables h = ( h 1 ,...,h We assume that the domains of the variables are x v  X  [ d ] for all v  X  [ L ] and h i  X  [ k ] for all i  X  [ M ] , where { 1 ,...,n } . Let X , [ d ] L and H , [ k ] M be the joint domains of x and h , respectively.
 For undirected models G , let G denote a set of cliques, where each clique C  X  x  X  h is a subset of nodes. The joint distribution is given by an exponential family: p  X  ( x , h )  X  Q tor, and  X  C ( x C , h C ) is the local feature vector which only depends on the observed ( x C ) and hidden ( h C ) variables in clique C . Also define N ( a ) = { b 6 = a :  X  X   X  X  a,b }} the neighbors of variable a .
 For directed models G , define p  X  ( x , h ) = Q Pa ( a )) , where Pa ( a )  X  x  X  h are the parents of a variable a . The parameters  X  are the conditional probability tables of each variable, and the cliques are G = {{ a } X  Pa ( a ) : a  X  x  X  h } .
 Problem statement This paper focuses on the problem of parameter estimation: We are given n i.i.d. examples of the observed variables D = ( x (1) ,..., x ( n ) ) , where each x produce a parameter estimate  X   X  that approximates  X   X  . The standard estimation procedure is maximum likelihood: Maximum likelihood is statistically efficient, but in general computationally intractable because marginalizing over hidden variables h yields a non-convex objective. In prac-tice, one uses local optimization procedures (e.g., EM or L-BFGS) on the marginal likelihood, but these can get stuck in local optima. We will later return to likelihoods, but let us first describe a method of moments approach for param-eter estimation. To do this, let X  X  introduce some notation. Notation We use the notation [  X  ] to indicate indexing; for example, M [ i ] is the i -th row of a matrix M and M [ i,j ] is the ( i,j ) -th element of M . For a tensor T  X  R d  X  X  X  X  X  d and a vector i = ( i 1 ,...,i ` ) , define the projection T [ i 1 ,...,i ` ] .
 We use  X  to denote the tensor product: if u  X  R d ,v  X  R k , and vectors v 1 ,  X  X  X  ,v `  X  R d , define the application: Analogously, for matrices M 1  X  R d  X  k ,  X  X  X  ,M `  X  R d  X  k : T ( M 1 ,  X  X  X  ,M ` )[ j ] = We will use P (  X  ) to denote various moment tensors con-structed from the true data distribution p  X   X  ( x , h ) : M Here, M i ,M ij ,M ijk are tensors of orders 1 , 2 , 3
Z i , P ( h i ) , Z ij , P ( h i ,h j ) , Z ijk , P ( h Finally, we define conditional moments O ( v | i ) , P ( x h )  X  R d  X  k for each v  X  [ L ] and i  X  [ M ] . 2.1. Assumptions In this section, we state technical assumptions that hold for the rest of the paper, but that we feel are not central to our each hidden variable are possible: Assumption 1 (Non-degeneracy) . The marginal distribu-tion of each hidden variable h i has full support: P ( h i Next, we assume the graphical model only has conditional independences given by the graph: Assumption 2 (Faithful) . For any hidden variables a,b,c  X  h such that an active trail 1 connects a and b con-ditioned on c , we have that a and b are dependent given c .
 Finally, we assume the graphical model is in a canonical form in which all observed variables are leaves: Assumption 3 (Canonical form) . For each observed vari-able x v , there exists exactly one C  X  G such that C = { x v ,h i } for some hidden node h i .
 The following lemma shows that this is not a real assump-tion (see the appendix for the proof): Lemma 1 (Reduction to canonical form) . Every graphical model can be transformed into canonical form. There is a one-to-one correspondence between the parameters of the transformed and original models.
 Finally, for clarity, we will derive our algorithms using ex-act moments of the true distribution p  X   X  . In practice, we would use moments estimated from data D . We start by trying to reveal some information about the hidden variables that will be used by subsequent sec-tions. Specifically, we review how the tensor factorization method of Anandkumar et al. (2013) can be used to recover tion is that of a bottleneck: Definition 1 (Bottleneck) . A hidden variable h i is said to be a bottleneck if (i) there exists three observed variables (views), x v given h i (Figure 2(a)), and (ii) each O ( v | i ) , P ( x say that a subset of hidden variables S  X  h is bottlenecked if every h  X  S is a bottleneck. We say that a graphical model G is bottlenecked if all its hidden variables are bot-tlenecks.
 For example, in Figure 1, x a tleneck h 1 , and x a Therefore, the clique { h 1 ,h 2 } is bottlenecked. Note that views are allowed to overlap.
 The full rank assumption on the conditional moments O one state cannot be a mixture of that of other states. Anandkumar et al. (2012a) provide an efficient tensor fac-torization algorithm for estimating P ( x v | h i ) : Theorem 1 (Tensor factorization) . Let h i  X  h be a bot-tleneck with views x v gorithm G ET C ONDITIONALS that returns consistent esti-of the hidden variables.
 To simplify notation, consider the example in Figure 2(a) where h 1 = 1 ,v 1 = 1 ,v 2 = 2 ,v 3 = 3 . The observed moments M 12 ,M 23 ,M 13 and M 123 can be factorized as follows: M M The G ET C ONDITIONALS algorithm first computes a I k  X  k , and uses W to transform M 123 into a symmetric orthogonal tensor. Then a robust tensor power method is used to extract the eigenvectors of the whitened M 123 ; un-The other conditional moments can be recovered similarly. The resulting estimate of O ( v | i ) based on n data points con-verges at a rate of n  X  1 2 with a constant that depends poly-quite small if h i and x v are connected via many intermedi-The tensor factorization method attacks the heart of the non-convexity in latent-variable models, providing some information about the hidden variables in the form of the conditional moments O ( v | i ) = P ( x v | h i ) . Note that G ET C ONDITIONALS only examines the conditional independence structure of the graphical model, not its parametrization.
 If i is the single parent of v (e.g., P ( x a 1 | h 1 ) in Figure 1), then this conditional moment is a parameter of the model, but this is in general not the case (e.g., P ( x a thermore, there are other parameters (e.g., P ( h 4 | h 2 which we do not have a handle on yet. In general, there is a gap between the conditional moments and the model parameters, which we will address in the next two sections. Having recovered conditional moments O ( v | i ) , P ( x v h ) , we now seek to compute the marginal distribution of sets of hidden variables Z S , P ( h S ) .
 Example To gain some intuition, consider the directed grid model from Figure 1. We can express the observed marginals M 12 , P ( x a the hidden marginals Z 12 , P ( h 1 ,h 2 )  X  R k  X  k , where the linear coefficients are based on the conditional moments O We can then solve for Z 12 by matrix inversion: 4.1. Exclusive views For which subsets of hidden nodes can we recover the marginals? The following definition offers a characteriza-tion: Definition 2 (Exclusive views) . Let S  X  h be a subset of hidden variables. We say h i  X  S has an exclusive view x v if the two conditions hold: (i) there exists some observed variable x v which is conditionally independent of the oth-ers S \{ h i } given h i (Figure 2(b)), and (ii) the conditional moment matrix O ( v | i ) , P ( x v | h i ) has full column rank k and can be recovered. We say that S has the exclusive views property if every h i  X  S has an exclusive view. Estimating hidden marginals We now show that if a subset of hidden variables S has the exclusive views prop-erty, then we can recover the marginal distribution P ( h Consider any S = { h i property. Let x v fine V = { x v the marginal over the observed variables P ( x V ) factorizes according to the marginal over the hidden variables P ( h times the conditional moments: of all the conditional moments. Vectorizing, we have that Z O column rank k m . Succinctly, M V (which can be estimated directly from data) is a linear function of Z S (what we seek to recover). We can solve for the hidden marginals Z S sim-ply by multiplying M V by the pseudoinverse of O : Algorithm 1 summarizes the procedure, G ET M ARGINALS . Given Z S , the conditional probability tables for S can eas-ily be obtained via renormalization.
 Theorem 2 (Hidden marginals from exclusive views) . If S  X  x is a subset of hidden variables with the ex-clusive views property, then Algorithm 1 recovers the marginals Z S = P ( h S ) up to a global relabeling of the hidden variables determined by the labeling from G Relationship to bottlenecks The bottleneck property al-lows recovery of conditional moments, and the exclusive Algorithm 1 G ET M ARGINALS (pseudoinverse) Input: Hidden subset S = { h i views V = { x v Output: Marginals Z S = P ( h S ) .
 views property allows recovery of hidden marginals. But we will now show that the latter property is in fact implied by the former property for special sets of hidden variables, which we call bidependent sets (in analogy with bicon-nected components), in which conditioning on one variable does not break the set apart: Definition 3 (Bidependent set) . We say that a subset of nodes S is bidependent if conditioned on any a  X  S , there is an active trail between any other two nodes b,c  X  S . Note that all cliques are bidependent, but bidepen-dent sets can have more conditional independences (e.g., { h 1 ,h 2 ,h 3 } in Figure 2(b)). This will be important in Sec-tion 5.1.
 Bidependent sets are significant because they guarantee ex-clusive views if they are bottlenecked: Lemma 2 (Bottlenecked implies exclusive views) . Let S  X  h be a bidependent subset of hidden variables. If S is bot-tlenecked, then S has the exclusive views property. Proof. Let S be a bidependent subset and fix any h 0  X  S . Since h 0 is a bottleneck, it has three conditionally indepen-dent views, say x 1 ,x 2 ,x 3 without loss of generality. For condition (i), we will show that at least one of the views is conditionally independent of S \{ h 0 } given h 0 . For the sake of contradiction, suppose that each observed variable x is conditionally dependent on some h i  X  S \{ h 0 } given h , for i  X  { 1 , 2 , 3 } . Then conditioned on h 0 , there is an active trail between h 1 and h 2 because S is biconnected. This means there is also an active trail x 1  X  h 1  X  h 2  X  x conditioned on h 0 . Since the graphical model is faithful by assumption, we have x 1 6 X  x 2 | h 0 , contradicting the fact that x 1 and x 2 are conditionally independent given h . To show condition (ii), assume, without loss of gen-erality, that x 1 is an exclusive view. Then we can recover O Remarks. Note that having only two independent views for each h i  X  S is sufficient for condition (i) of the exclu-sive views property, while three is needed for condition (ii). The bottleneck property (Definition 1) can also be relaxed if some cliques share parameters (see examples below). Our method extends naturally to the case in which the ob-served variables are real-valued ( x v  X  R d ), as long as the hidden variables remain discrete. In this setting, the con-longer be distributions but general rank k matrices. Example: hidden Markov model. In the HMM (Figure 3(a)), h 2 is a bottleneck, so we can recover O , P ( x 2 h ) . While the first hidden variable h 1 is not a bottle-neck, it still has an exclusive view x 1 with respect to the clique { h 1 ,h 2 } , assuming parameter sharing across emis-sions ( P ( x 1 | h 1 ) = O ).
 Example: latent tree model. In the latent tree model (Figure 3(b)), h 1 is not directly connected to an observed variable, but it is still a bottleneck, with views x a for example. The clique { h 1 ,h 2 } has exclusive views Non-example In Figure 3(c), h 1 does not have exclusive views. Without parameter sharing, the techniques in this paper are insufficient. In the special case where the graphi-cal model represents a binary-valued noisy-or network, we can use the algorithm of Halpern &amp; Sontag (2013), which first learns h 2 and subtracts off its influence, thereby mak-ing h 1 a bottleneck. 4.2. Composite likelihood So far, we have provided a method of moments estima-tor which used (i) tensor decomposition to recover condi-tional moments and (ii) matrix pseudoinversion to recover the hidden marginals. We will now improve statistical ef-ficiency by replacing (ii) with a convex likelihood-based objective.
 Of course, optimizing the original marginal likelihood (Equation 1) is subject to local optima. However, we make two changes to circumvent non-convexity: The first is that we already have the conditional moments from tensor de-composition, so effectively a subset of the parameters are fixed. However, this alone is not enough, for the full likeli-hood is still non-convex. The second change is that we will optimize a composite likelihood objective (Lindsay, 1988) rather than the full likelihood.
 Consider a subset of hidden nodes S = { h i with exclusive views V = { x v composite log-likelihood over x V given parameters Z S , P ( h S ) with respect to the true distribution M V can be writ-ten as follows:
L cl ( Z S ) , E [log P ( x V )] The final expression is an expectation over the log of a lin-ear function of Z S , which is concave in Z S . Unlike maxi-mum likelihood in fully-observed settings, we do not have a closed-form solution, so we use EM to optimize it. How-ever, since the function is concave, EM is guaranteed to converge to the global maximum. Algorithm 2 summarizes our algorithm.
 Algorithm 2 G ET M ARGINALS (composite likelihood) Input: Hidden subset S = { h i views V = { x v Output: Marginals Z S = P ( h S ) .
 Return Z S = arg max Z 4.3. Statistical efficiency We have proposed two methods for estimating the hidden marginals Z S given the conditional moments O , one based on computing a simple pseudoinverse, and the other based on composite likelihood. Let  X  Z pi estimator and  X  Z cl The Cram  X  er-Rao lower bound tells us that maximum like-lihood yields the most statistically efficient composite es-timator for Z S given access to only samples of x V . 4 Let us go one step further and quantify the relative efficiency of the pseudoinverse estimator compared to the composite likelihood estimator.
 Abusing notation slightly, think of M V as just a flat multi-nomial over d m outcomes and Z S as a multinomial over k m outcomes, where the two are related by O  X  R d m  X  k m . We will not need to access the internal tensor structure of M and Z S , so to simplify the notation, let m = 1 and define  X  = M V  X  R d , z = Z S  X  R k , and O = O  X  R d  X  k . The hidden marginals z and observed marginals  X  are related via  X  = Oz .
 Note that z and  X  are constrained to lie on simplexes  X  k  X  1 and  X  d  X  1 , respectively. To avoid constraints, we reparam-eterize z and  X  using e z  X  R k  X  1 and e  X   X  R d  X  1 : In this representation, e  X  and e z are related as follows, The pseudoinverse estimator is defined as b  X  z pi = e O O  X  d,k ) , and the composite likelihood estimator is given by b  X  z the log-likelihood function.
 First, we compute the asymptotic variances of the two esti-mators.
 Lemma 3 (Asymptotic variances) . The asymptotic vari-ances of the pseudoinverse estimator b  X  z pi and composite likelihood estimator b  X  z cl are: where e D , diag( e  X  ) and e d , 1  X  1 &gt; e  X  . Next, let us compare the relative efficiencies of the two es-timators: e pi , 1 bound (van der Vaart, 1998), we know that  X  cl  X  pi . This implies that the relative efficiency, e pi , lies between and when e pi = 1 , the pseudoinverse estimator is said to be (asymptotically) efficient. To gain intuition, let us explore two special cases: Lemma 4 (Relative efficiency when e O is invertible) . When e O is invertible, the asymptotic variances of the pseudoin-verse and composite likelihood estimators are equal,  X  cl  X  Lemma 5 (Relative efficiency with uniform observed marginals) . Let the observed marginals  X  be uniform:  X  = 1 . The efficiency of the pseudoinverse estimator is: where 1 U , e O e O  X  1 , the projection of 1 onto the column space of e O . Note that 0  X k 1 U k 2 When k 1 U k 2 = 0 , the pseudoinverse estimator is efficient: e pi = 1 . When k 1 estimator is strictly inefficient. In particular, if k 1 k  X  1 , and we get: Based on Equation 3 and Equation 4, we see that the pseu-doinverse gets progressively worse compared to the com-posite likelihood as the gap between k and d increases for the special case wherein the observed moments are uni-formly distributed. For instance, when k = 2 and d  X   X  , the efficency of the pseudolikelihood estimator is half that of the composite likelihood estimator. Empirically, we ob-serve that the composite likelihood estimator also leads to more accurate estimates in general non-asymptotic regimes (see Figure 4). We have thus far shown how to recover the conditional mo-ments O ( v | i ) = P ( x v | h i ) for each exclusive view each hidden variable h i , as well as the hidden marginals Z
S = P ( h S ) for each bidependent subset of hidden vari-ables S . Now all that remains to be done is to recover the parameters.
 Since our graphical model is in canonical form (Assump-tion 3), all cliques C  X  G either consist of hidden vari-ables h C or are of the form { x v ,h i } . The key observation tics of the model p  X  . How we turn these clique marginals {
P ( x C , h C ) } C X  X  into parameters  X  depends on the exact model parametrization.
 For directed models, the parameters are simply the local conditional tables p  X  ( a | Pa ( a )) for each clique C = { a } X  Pa ( a ) . These conditional distributions can be obtained by simply normalizing Z C for each assignment of Pa ( a ) . For undirected log-linear models, the canonical parameters  X  cannot be obtained locally, but we can construct a global convex optimization problem to solve for  X  . Suppose we were able to observe h . Then we could optimize the super-vised likelihood, which is concave: Of course we don X  X  have supervised data, but we do have the marginals P ( x C , h C ) , from which we can easily com-pute the expected features:  X  C , E [  X  ( x C , h C )] = Therefore, we can optimize the supervised likelihood ob-jective without actually having any supervised data! In the finite data regime, the method of moments yields the esti-mate  X   X  mom learning, we obtain a different estimate  X   X  sup on an empirical average over data points. In the limit of infinite data, both estimators converge to  X  C .
 Algorithm 3 G ET P ARAMETERS Input: Conditional moments O ( v | i ) = P ( x v | h i ) and hid-den marginals Z S = P ( h S ) .
 Output: Parameters  X  . if G is directed then else if G is undirected with low treewidth then else if G is undirected with high treewidth then end if Remark If we have exclusive views for only a subset of the cliques, we can still obtain the expected features  X  C those cliques and use posterior regularization (Grac  X a et al., 2008), measurements (Liang et al., 2009), or generalized expectation criteria (Mann &amp; McCallum, 2008) to encour-age E p functions would be non-convex, but we expect local optima to be less of an issue. 5.1. Pseudolikelihood While we now have a complete algorithm for estimating directed and undirected models, optimizing the full likeli-hood (Equation 5) can still be computationally intractable for undirected models with high treewidth due to the in-employ various variational approximations of A (  X  ) (Wain-wright &amp; Jordan, 2008), but these generally lead to incon-pseudolikelihood (Besag, 1975). The pseudolikelihood ob-jective is a sum over the log-probability of each variable given its neighbors N ( a ) : In the fully-supervised setting, it is well-known that pseu-dolikelihood provides consistent estimates which are com-over cliques C that contain a ; note that  X  a, N ( a ) only de-pends on a and its neighbors N ( a ) . We can write each conditional log-likelihood from Equation 7 as: p ( a |N ( a )) = exp(  X  &gt;  X  a, N ( a ) ( a, N ( a ))  X  A where the conditional log-partition function A a (  X  ; N ( a )) = log involves marginalizing only over the single variable a . If we knew the marginals for each neighborhood, then we would be able to optimize the pseudolikelihood objective again without having access to any labeled data. Unfortunately, { a }  X  N ( a ) does not always have exclu-sive views. For example, consider a = h 1 and N ( a ) = { h 2 ,h 3 ,h 4 } in Figure 3(b).
 However, we can decompose { a }  X  N ( a ) as fol-lows: conditioning on a partitions N ( a ) into indepen-dent subsets; let B ( a ) be the collection of these sub-sets, which we will call sub-neighborhoods . For exam-neighborhood in Figure 5.
 A key observation is that for each sub-neighborhood B  X  B ( a ) , each { a } X  B is bidependent: conditioning on a not introduce new independencies within B by construc-tion of B ( a ) , and conditioning on any b  X  B does not either since every other b 0  X  B \{ b } is connected to a . Assum-ing G is bottlenecked, by Lemma 2 we have that { a } X  B has exclusive views. Hence, we can recover P ( a,B ) for each a and B  X  B ( a ) . Based on conditional indepen-dence of the sub-neighborhoods B given a , we have that P ( a, N ( a )) = P ( a ) compute the expected features  X  a, N ( a ) and use them in the optimization of the pseudolikelihood objective.
 Note that our pseudolikelihood-based approach does de-pend exponentially on the size of the sub-neighborhoods, which could be exceed the largest clique size. Therefore, each node essentially should have low degree or locally ex-hibit a lot of conditional independence. On the positive side, we can handle graphical models with high treewidth; neither sample nor computational complexity necessarily depends on the treewidth. For example, an n  X  n grid model has a treewidth of n , but the degree is at most 4 . For latent-variable models, there has been tension between local optimization of likelihood, which is broadly appli-cable but offers no global theoretical guarantees, and the spectral method of moments, which provides consistent es-timators but are limited to models with special structure. The purpose of this work is to show that the two methods can be used synergistically to produce consistent estimates for a broader class of directed and undirected models. Our approach provides consistent estimates for a family of models in which each hidden variable is a bottleneck  X  X hat is, it has three conditionally independent observations. This bottleneck property of Anandkumar et al. (2013) has been exploited in many other contexts, including latent Dirichlet allocation (Anandkumar et al., 2012b), mixture of spherical Gaussians (Hsu &amp; Kakade, 2013), probabilistic grammars (Hsu et al., 2012), noisy-or Bayesian networks (Halpern &amp; Sontag, 2013), mixture of linear regressions (Chaganty be viewed as  X  X reprocessing X  the given model into a form that exposes the bottleneck or tensor factorization structure. The model parameters correspond directly to the solution of the factorization.
 In contrast, the bottlenecks in our graphical models are given by assumption, but the conditional distribution of the observations given the bottleneck can be quite com-plex. Our work can therefore be viewed as  X  X ostprocess-ing X , where the conditional moments recovered from tensor factorization are used to further obtain the hidden marginals and eventually the parameters. Along the way, we devel-oped the notion of exclusive views and bidependent sets, which characterize conditions under which the conditional moments can reveal the dependency structure between hid-den variables. We also made use of custom likelihood func-tions which were constructed to be easy to optimize. Another prominent line of work in the method of moments community has focused on recovering observable opera-tor representations (Jaeger, 2000; Hsu et al., 2009; Bailly et al., 2010; Balle &amp; Mohri, 2012). These methods allow prediction of new observations, but do not recover the ac-tual parameters of the model, making them difficult to use in conjunction with likelihood-based models. Song et al. (2011) proposed an algorithm to learn observable opera-tor representations for latent tree graphical models, like the one in Figure 3(b), assuming the graph is bottlenecked. Their approach is similar to our first step of learning condi-tional moments, but they only consider trees. Parikh et al. (2012) extended this approach to general graphical models which are bottlenecked using a latent junction tree repre-sentation. Consequently, the size of the observable repre-sentations is exponential in the treewidth. In contrast, our algorithm only constructs moments of the order of size of the cliques (and sub-neighborhoods for pseudolikelihood), which can be much smaller.
 An interesting direction is to examine the necessity of the bottleneck property. Certainly, three views is in general needed to ensure identifiability (Kruskal, 1977), but re-quiring each hidden variable to be a bottleneck is stronger than what we would like. We hope that by judiciously leveraging likelihood-based methods in conjunction with the method of moments, we can generate new hybrid tech-niques for estimating even richer classes of latent-variable models.
 Anandkumar, A., Chaudhuri, K., Hsu, D., Kakade, S. M.,
Song, L., and Zhang, T. Spectral methods for learning multivariate latent tree structure. In Advances in Neural Information Processing Systems (NIPS) , 2011.
 Anandkumar, A., Hsu, D., and Kakade, S. M. A method of moments for mixture models and hidden Markov mod-els. In Conference on Learning Theory (COLT) , 2012a. Anandkumar, A., Liu, Y., Hsu, D., Foster, D. P., and
Kakade, S. M. A spectral algorithm for latent dirichlet allocation. In Advances in Neural Information Process-ing Systems (NIPS) , pp. 917 X 925, 2012b.
 Anandkumar, A., Ge, R., Hsu, D., Kakade, S. M., and Tel-garsky, M. Tensor decompositions for learning latent variable models. Technical report, ArXiv, 2013.
 Bailly, R., Habrard, A., and Denis, F. A spectral approach for probabilistic grammatical inference on trees. In Al-gorithmic Learning Theory , pp. 74 X 88. Springer, 2010. Balle, B. and Mohri, M. Spectral learning of general weighted automata via constrained matrix completion.
In Advances in Neural Information Processing Systems (NIPS) , pp. 2159 X 2167, 2012.
 Besag, J. The analysis of non-lattice data. The Statistician , 24:179 X 195, 1975.
 Blei, D., Ng, A., and Jordan, M. I. Latent Dirichlet allo-cation. Journal of Machine Learning Research , 3:993 X  1022, 2003.
 Chaganty, A. and Liang, P. Spectral experts for estimating mixtures of linear regressions. In International Confer-ence on Machine Learning (ICML) , 2013.
 Ghahramani, Z. and Beal, M. J. Variational inference for Bayesian mixtures of factor analysers. In Advances in Neural Information Processing Systems , 1999.
 Grac  X a, J., Ganchev, K., and Taskar, B. Expectation maxi-mization and posterior constraints. In Advances in Neu-ral Information Processing Systems (NIPS) , 2008. Haghighi, A. and Klein, D. Prototype-driven learning for sequence models. In North American Association for Computational Linguistics (NAACL) , 2006.
 Halpern, Y. and Sontag, D. Unsupervised learning of noisy-or Bayesian networks. In Uncertainty in Artificial Intel-ligence (UAI) , pp. 272 X 281, 2013.
 Hsu, D. and Kakade, S. M. Learning mixtures of spherical
Gaussians: Moment methods and spectral decomposi-tions. In Innovations in Theoretical Computer Science (ITCS) , 2013.
 Hsu, D., Kakade, S. M., and Zhang, T. A spectral algorithm for learning hidden Markov models. In Conference on Learning Theory (COLT) , 2009.
 Hsu, D., Kakade, S. M., and Liang, P. Identifiability and unmixing of latent parse trees. In Advances in Neural Information Processing Systems (NIPS) , 2012.
 Jaakkola, T. S and Jordan, M. I. Variational probabilistic inference and the QMR-DT network. Journal of Artifi-cial Intelligence Research , 10:291 X 322, 1999.
 Jaeger, H. Observable operator models for discrete stochas-tic time series. Neural Computation , 2000.
 Koller, D. and Friedman, N. Probabilistic graphical mod-els: principles and techniques . MIT press, 2009. Kruskal, J. B. Three-way arrays: Rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics. Linear Algebra and Applica-tions , 18:95 X 138, 1977.
 Liang, P., Jordan, M. I., and Klein, D. Learning from mea-surements in exponential families. In International Con-ference on Machine Learning (ICML) , 2009.
 Lindsay, B. Composite likelihood methods. Contemporary Mathematics , 80:221 X 239, 1988.
 Mann, G. and McCallum, A. Generalized expectation cri-teria for semi-supervised learning of conditional random fields. In Human Language Technology and Association for Computational Linguistics (HLT/ACL) , pp. 870 X 878, 2008.
 Mossel, E. and Roch, S. Learning nonsingular phylogenies and hidden markov models. In Theory of computing , pp. 366 X 375. ACM, 2005.
 Parikh, A., Song, L., Ishteva, M., Teodoru, G., and Xing,
E. A spectral algorithm for latent junction trees. In Un-certainty in Artificial Intelligence (UAI) , 2012. Quattoni, A., Collins, M., and Darrell, T. Conditional ran-dom fields for object recognition. In Advances in Neural Information Processing Systems (NIPS) , 2004.
 Song, Le, Xing, E. P, and Parikh, A. P. A spectral algo-rithm for latent tree graphical models. In International
Conference on Machine Learning (ICML) , 2011. van der Vaart, A. W. Asymptotic statistics . Cambridge University Press, 1998.
 Wainwright, M. and Jordan, M. I. Graphical models, expo-nential families, and variational inference. Foundations
