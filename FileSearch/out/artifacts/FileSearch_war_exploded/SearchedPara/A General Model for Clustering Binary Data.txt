
The problem of clustering data arises in many disciplines an d has a wide range of applications. Intuitively, clustering is th e problem of partitioning a finite set of points in a multi-dimensional space into classes (called clusters) so that (i) the points belong ing to the same class are similar and (ii) the points belonging to different classes are dissimilar . The clustering problem has been studied extensively in machine learning, databases, and statistic s from var-ious perspectives and with various approaches and focuses.
In this paper, we focus our attention on binary datasets. Bin ary data have been occupying a special place in the domain of data analysis. Typical applications for binary data clustering include Copyright 2005 ACM 1-59593-135-X/05/0008 ... $ 5.00. market basket data clustering and document clustering. For market basket data, each data transaction can be represented as a bi nary vector where each element indicates whether or not any of the cor-responding item/product was purchased. For document clust ering, each document can be represented as a binary vector where eac h element indicates whether a given word/term was present or n ot.
The first contribution of the paper is the introduction of a ge n-eral model for binary clustering. A distinctive characteri stic of the same nature as the data they intend to account for: both are bi nary. The characteristic suggests a new clustering model where th e data and features are treated equally. The new clustering model, ex-plicitly describes the data assignments (assigning data po ints into clusters) as well as feature assignments (assigning featur es into clusters). The clustering problem is then formulated as a ma trix approximation problem where the clustering objective is to mini-mize the approximation error between the original data matr ix and the reconstructed matrix based on the cluster structures. I n gen-eral, the approximation can be solved via an iterative alter nating least-squares optimization procedure. The optimization p rocedure simultaneously performs two tasks: data reduction (assign ing data points into clusters) and feature identification (identify ing features associated with each cluster). By explicit feature assignm ents, the clustering model produces interpretable descriptions of t he result-ing clusters. In addition, by iterative feature identificat ion, the clus-tering model performs an implicit adaptive feature selecti on at each iteration and flexibly measures the distances between data p oints. Therefore it works well for high-dimensional data [18]. For many cases, there is usually a symmetric association relations b etween the data and features in binary clustering: if the set of data points is associated to the set of features, then the set of attribut es is as-sociated to the set of data points and vice versa. This symmet ric association motivates a block diagonal variant of the gener al model.
The second contribution of this paper is the presentation of a unified view for binary data clustering. In particular, we sh ow that our new clustering model provides a general framework for bi nary data clustering based on matrix approximation. Many previo usly known clustering algorithms can be viewed as different vari ations derived from the general framework with different constrai nts and relaxations. Thus our general model provides an elegant bas is to es-tablish the connections between various methods while high light-ing their differences. In addition, we also examine the rela tions between our clustering model with other binary clustering m odels. As a third contribution, we examine the problem of determini ng the number of clusters with our binary clustering model.

The rest of the paper is organized as follows: Section 2 intro -duces the general clustering model and describes the genera l opti-mization procedure, Section 3 presents the block diagonal v ariants of the general model, Section 4 provides the unified view on bi nary clustering. Section 5 discusses the method for deciding the number of clusters; Section 6 shows our experimental results. Sect ion 7 surveys the related work. Finally, Section 8 presents our co nclu-sions. The notations used in the paper are introduced in Table 1.
We first present a general model for binary clustering proble m The model is formally specified as follows: where matrix E denotes the error component. The first term AX B characterizes the information of W that can be described by the cluster structures. A and B explicitly designate the cluster member-ships for data points and features, respectively. X specifies clus-ter representation. Let  X  W denote the approximation AX B goal of clustering is to minimize the approximation error (o r sum-of-squared-error ) Note that the Frobenius norm, k M k F , of a matrix M = ( given by k M k F = q  X  i , j M 2 i j .
In general, the model leads to the formulation of two-side cl us-tering, i.e., the problem of simultaneously clustering bot h data points (rows) and features (columns) of a data matrix [7, 17].

Suppose A = ( a ik ) , a ik  X  X  0 , 1 } ,  X  K k { 0 , 1 } ,  X  C c = 1 b jc = 1 (i.e., A and B denote the data and feature mem-
It should be noted that: although the clustering model prese nted here is motivated from the characteristics of binary data, t he model can be generalized to other data types as well. berships, respectively). Thus, based on Equation 3, we obta in
For fixed P k and Q c , it is easy to check that the optimum X is obtained by In other words, X can be thought as the matrix of centroids for the two-side clustering problem and it represents the assoc iations between the data clusters and the feature clusters [6]. O can then be minimized via an iterative procedure of the follo wing steps 1. Given X and B , then the feature partition Q is fixed, O 2. Similarly, Given X and A , then the data partition P is fixed, 3. Given A and B , X can be computed using Equation 5.

This is a natural extensions of the K-means type algorithm fo r two-side case [3, 6, 27]. The clustering procedure is shown i n Al-gorithm 1.
 Algorithm 1 General Clustering Procedure Input: ( W n  X  m , K and C ) Output: A : cluster assignment; begin 1 Initialize A and B ; 2 Compute X based on Equation 5. 3. Iteration: Do while the stop criterion is not met 3.1 Update A based on Equation 6 3.2 Update B based on Equation 7 2.3 Compute X based on Equation 5 3. Output A and B end
As mentioned in Section 2, in general, X represents the associa-tions between the data clusters and the feature clusters. Fo r binary data clustering, in many cases, there is usually a symmetric asso-ciation relations between the data and features: if the set o f data points is associated to the set of features, then the set of at tributes is associated to the set of data points and vice versa. This sy mmet-ric association motivates a variant of the general model whe re X is an identity matrix. Then in the general model, we have C i.e., both data points and features have the same number of cl usters. The assumption also implies that, after appropriate permut ation of the rows and columns, the approximation data take the form of a block diagonal matrix [15].

In this case, AB T can then be interpreted as the approximation of the original data W . The goal of clustering is then to find a that minimizes the squared error between W and its approximation AB The objective criterion can be expressed as where y k j = 1 n to denote the entry of B T .). The objective function can be min-imized via an alternating least-squares procedure by alter natively optimizing one of A or B while fixing the other.

Given an estimate of B , new least-squares estimates of the entries of A can be determined by assigning each data point to the closest cluster as follows:
When A is fixed, O A , B can be minimized with respect to B by minimizing the second part of Equation 9:
Note that y k j can be thought of as the probability that the j -th feature is present in the k -th cluster. Since each b k j either 0 or 1, O  X  ( B ) is minimized by: then it is viewed as an outlier at the current stage. The optim ization procedure for minimizing Equation 9 alternates between upd ating A based on Equation 10 and assigning features using Equation 1 1. After each iteration, we compute the value of the objective c rite-rion O ( A , B ) . If the value is decreased, we then repeat the process; otherwise, the process has arrived at a local minimum. Since the
If the entries of B are arbitrary, then the optimization here can be performed via singular value decomposition. procedure monotonically decreases the objective criterio n, it con-verges to a local optimum. The clustering procedure is shown in Algorithm 2. A preliminary report of the block diagonal clus tering is presented in [25].
 Algorithm 2 Block Diagonal Clustering Procedure Input: (data points: W n  X  m , # of classes: K ) Output: A : cluster assignment; begin 1. Initialization: 1.1 Initialize A 1.2 Compute B based on Equation 11 1.3 Compute O 0 = O ( A , B ) 2. Iteration: 2.1 Update A given B (via Equation 10) 2.2 Compute B given A (via Equation 11) 2.3 Compute the value of O 1 = O ( A , B ) ; 2.4 if O 1 &lt; O 0 2.4.1 O 0 = O 1 2.4.2 Repeat from 2.1 2.5 else 2.5.1 break; (Converges) 3. Return A , B ; end
In this section, we present a unified view for binary data clus ter-ing. In particular, we show that our new clustering model pro vides a general framework for binary data clustering based on matr ix ap-proximation and illustrate several variations that can be d erived from the general model in Section 4.1, and examine the relati ons among between our clustering model with other binary cluste ring models in Section 4.2. The discussions are summarized in Fig ure 1. Figure 1: A Unified View on Binary Clustering. The lines r ep-resent relations. Note that the relations between maximum likelihood principle and minimum description length (MDL) , shown as the dotted line, are well-known facts in machine lea rn-ing literature.
Consider the case when C = m , then each feature is a cluster by itself and B = I m  X  m . The model thus reduces to popular one-side clustering, i.e., grouping the data points into cluste rs. Here we only discuss the one-side clustering for data points. It sho uld be note that, similarly, we can derive one-side feature cluste ring when K = n , A = I .

Suppose A = ( a ik ) , a ik  X  X  0 , 1 } ,  X  K k data membership), then the model reduces to
O ( A , X ) = k W  X  AX k 2 F Given A , the objective criterion O is minimized by setting x y rows belong to a particular cluster are contiguous, so that a ll data points belonging to the first cluster appear first and the seco nd clus-ter next, etc 3 . Then A can be represented as A = Note that A T A = the cluster size on the diagonal. The inverse of A T A serves as a weight matrix to compute the centroids. Thus we have the foll ow-ing equation for representing centroids
On the other hand, given X , O ( A , X ) is minimized by The alternative minimization leads to traditional the K-me ans clus-tering procedure [19].

In fact, there are some other variations that can be derived f or the one-side clustering. For example, if we prefer a low-dimens ional
This can be achieved by multiplying W with a permutation matrix if necessary. representation of the cluster structure by restricting Rank t , t &lt; = min ( K  X  1 , m ) , the general model can lead to the one-Side low dimensional clustering [35]. We can also put non-negati ve con-straints on both A and X for other variations [38, 32, 11].
When X is identity matrix and if we allow entries of A and B to be any positive values, this leads to the cluster model des cribed in [23]. The objective function can be rewritten as O ( A , X , B ) = k W  X  AB T ) k 2 F = Trace (( W  X  AB T )( Note that if we relax A and B and let them be arbitrary matrices, then based on we would get the optimization rules A = W B ( B T B ) W
T A ( A T A )  X  1 . By imposing orthogonal requirements, we could obtain two simplified updating rules which has a natural inte rpreta-tion analogous to the HITS ranking algorithm [21].
 Basically, the optimizing rules show a mutually reinforcin g rela-tionship between the data and the features for binary datase t which can be naturally expressed as follows: if a feature f (or, data point d ) is shared by many points (or, features) that have high weigh ts associated with a cluster c , then feature f (or, data point d ) has a high weight associated with c . The clustering approach also share some commonalities with non-negative matrix factorizatio n [22] and concept factorization [11, 37]. In general, if A and B denote the cluster membership, then A diag ( p 1 , , p K ) and B T B = diag ( q 1 , , q C ) are two diagonal ma-trices. If we relax the conditions on A and B , requiring A and B T B = I C , we would obtain a new variation of two-side clus-tering algorithm. Note that O ( A , X , B ) = k W  X  AX B T k 2 F = Trace (( W  X  AX B T )( Since Trace ( WW T ) is constant, hence minimizing O ( equivalent to minimizing The minimum of Equation 17 is achieved where X = A Plugging X = A T W B into Equation 17, we have ( A , X , B ) = Trace ( X X T )  X  2Trace ( AX B T W T ) Since the first term Trace ( W W T ) is constant, minimizing O  X  is thus equivalent to maximizing Trace ( A T W BB T W T A nore the special structure of A , B and let them be arbitrary orthonor-mal matrices, the clustering problem then reduced to the tra ce max-imization problem which can be solved by eigenvalue decompo si-tion [39].

A summary of different variations of the general model is lis ted in Table 2. associated constraints.
In this section, we examine the relations among between our clustering model with other binary clustering models.
Recently, an information-theoretic clustering framework appli-cable to empirical joint probability distributions was dev eloped for two-dimensional contingency table or co-occurrence matri x [10]. In this framework, the (scaled) data matrix W is viewed as a joint probability distribution between row and column random var iables taking values over the rows and columns. The clustering obje ctive is to seek a hard-clustering of both dimensions such that los s in mu-matrix, is minimized [34].

Here we explore the relations between our general clusterin g model and the information-theoretic framework. If we view e n-tries of W as values of a joint probability distribution between row and column random variables, then I ( W ) =  X  n i where w i . =  X  m j
Once we have a simplified K  X  C matrix  X  W , we can construct an n  X  m matrix  X  W as the approximation of original matrix W by where i  X  P k , j  X  Q c and  X  w k . =  X  C c the approximation preserves marginal probability [10], it can easily check that Hence we have I (  X 
W i j ) = So
I ( W )  X  I (  X  W ) = I ( W )  X  I (  X  W ) (Based on Equation 24) The last step from the above derivation is based on power seri es approximation of logarithm. The approximation is valid if t he ab-The right side of Equation 25 can be thought as a weighted ver-sion of the right side of Equation 2. Thus minimizing the crit erion O (
A , X , B ) is conceptually consistent with the loss of mutual infor-mation , i.e., I ( W )  X  I (  X  W ) .
In this section, we show the relations between the block diag o-nal model with the binary dissimilarity coefficients 4 . A popular partition-based criterion (within-cluster) for one-side clustering is to minimize the summation of distances/dissimilarities in side the cluster. The within-cluster criterion can be described as m inimiz-ing or where  X  ( w i , w i  X  ) is the distance measure between w use w i as a point variable and we write i  X  P k to mean that the i -th vector belongs to the k -th data class. For binary clustering, the dissimilarity coefficients are popular measures of the dist ances.
Various Coefficients: Given two binary data points, w and w  X  , there are four fundamental quantities that can be used to defi ne sim-ilarity between the two [4]: a = k{ j | w j = w  X  j = 1 }k
More discussions on binary dissimilarity coefficients can b e found in [24].
Equation 26 computes the weighted sum using the cluster size s. w j = 1  X  w  X  j = 0 }k , c = k{ j | w j = 0  X  w  X  j = 1 }k , and d w j = w  X  j = 0 }k , where 1  X  j  X  r . It has been shown in [4] that the presence/absence based dissimilarity measure can be ge nerally written as D ( a , b , c , d ) = b + c  X  a ble 3 shows several common dissimilarity coefficients and th e cor-responding similarity coefficients.
 Table 3: Binary dissimilarity and similarity coefficients. The  X  X etric X  column indicates whether the given dissimilarity co-efficient is metric or not. A  X  X  X  stands for  X  X ES X  while an  X  X  X  stands for  X  X o X .

In cluster applications, the rankings based on a dissimilar ity co-efficient is often of more interest than the actual value of th e dissim-ilarity coefficient. It has been shown that [4], if the paired absences are ignored in the calculation of dissimilarity values, the n there is only one single dissimilarity coefficient modulo the global order equivalence: b + c a single dissimilarity coefficient.

Relation With Dissimilarity Coefficients: For block diagonal clustering model, given representation ( A , B ) , basically, A denotes the assignments of data points associated into clusters and B indi-cates the feature representations of clusters. Observe tha t
O ( A , B ) = || W  X  AB T || 2 F = r  X  where e k = ( b k 1 , , b km ) , i = 1 , , K is the cluster  X  X epresenta-tive X  of cluster P i . Thus minimizing Equation 28 is the same as minimizing Equation 27 where the distance is defined as d  X  w i j and ( e k ) i j are all binary.) In fact, given two binary vectors X and Y ,  X  i | X i  X  Y i | calculates their mismatches, which is the numer-ator of their dissimilarity coefficients.
Minimum Description length(MDL) aims at searching for a mod el that provides the most compact encoding for data transmissi on [31]. As described in Section 2, in block diagonal clustering, the origi-nal matrix W can be approximated by the matrix product of AB Instead of encoding the elements of W alone, we then encode the model, A , B , and the data given the model, ( W | AB T ) code length is thus can be expressed as In the Bayesian framework, L ( A ) and L ( B ) are negative log priors for A and B and L ( W | AB T ) is a negative log likelihood of W given A and B . If we assume that the prior probabilities of all the element s of A and B are uniform (i.e., 1 2 ), then L ( A ) and L ( the dataset W . In other words, we need to use one bit to represent each element of A and B irrespective of the number of 1 X  X  and 0 X  X . Hence, minimizing L ( W , A , B ) reduces to minimizing L
P ROPOSIT ION 1. minimizing L ( W | AB T ) is equivalent to mini-mizing O ( A , B ) = 1 2 || W  X  AB T || 2 F .

Proposition 1 establishes the connections between MDL and o ur clustering model. The proof of the proposition is presented in Ap-pendix. Figure 2: Visualization of the original document-data matr ix and the reordered document-data matrix. The shaded region represents non-zero entries.
We have seen in the previous section the equivalence among va r-ious clustering criteria. In this section, we investigate t he problem of determining the number of clusters for binary clustering with our general model. The symmetric association relationship bet ween the data and features provides a novel method for determining th e num-ber of clusters for binary data.
Given a binary dataset, W , how many possible clusters in the dataset? Let X  X  look at the case where the approximation data take the form of a block diagonal matrix. Based on the symmetric re -lations between the data and features in binary clustering, the data points in a cluster share many features and vice versa. Hence , if we arrange rows and columns of W based on the cluster assignments (that is, the points and features in the first cluster appear fi rst, the points and features in the last cluster appear at the end), th en we would get a block diagonal structure.

An example is given in Figure 2 based on CSTR dataset 6 , which Each abstract is represented using a 1000-dimension binary vec-tor. For this example, due to large number of terms, it is natu ral to constrain the features so a term may belong to one cluster onl y. Figure 2(a) shows the original word-document matrix of CSTR and Figure 2(b) shows the reordered matrix obtained by arran g-ing rows and columns based on the cluster assignments. The fo ur block diagonals in Figure 2(b) correspond to the four cluste rs and the dense region at the bottom of the figure identifies the feat ure outliers (which are distributed uniformly across the techn ical re-ports. The rough block diagonal sub-structures observed in dicate the cluster structure relation between documents and words .
Without loss of generality, we assume that the rows belong to a particular cluster are contiguous, so that all data points b elonging to the first cluster appear first and the second cluster next, etc ilarly, we also ordered the features in W according to which cluster they are in, so that all features belonging to the first cluste r appear first and the second cluster next, etc. Hence B has a similar format as A . Note that AB T is a block diagonal matrix. Assume W has k clusters. Since W = AB T + E , W can be regarded as the addition of two matrices: W = L + E where L = R with a small value in each entry, i.e., E = O (  X  ) .

The following theorem gives a way to deciding the number of clusters. The proof of the theorem follows from the standard results in matrix computations (see [14, Theorem 8.3.4. on page 429] ).
T HE ORE M 2. Let M = L + E where L and E are matrices de-scribed above, then M has dominant k singular values.

Since the permutation does not change the spectral properti es, we then could decide the number of clusters based on the singu lar values of W . In essential, we try to look for a large gap between the singular values  X  k and  X  k + 1 of the related matrices. We note here that the above conclusion can also be derived from matrix per tur-bation theory [9, 20].
The detailed description of the dataset can be found in Secti on 6.
This can be achieved by multiplying W with a permutation matrix if necessary.
There are many ways to measure how accurately clustering al-gorithm performs. One is the confusion matrix [1]. Entry a confusion matrix is the number of data points assigned to ou tput class o and generated from input class i . The purity [40] that mea-sures the extend to which each cluster contained data points from primarily one class is also a good metric for cluster quality . The purity of a clustering solution is obtained as a weighted sum of indi-vidual cluster purities and is given by Purity =  X  K i max j ( n j i ) where S i is a particular cluster of size n ber of documents of the i -th input class that were assigned to the j -th cluster, K is the number of clusters and n is the total number of points 8 . A high purity value implies that the clusters are  X  X ure X  purity, the better the clustering solution is.
In this section, we evaluate the performance of the general o p-timization procedure in Section 2.2 on the zoo database avai lable at the UC Irvine Machine Learning Repository. The database c on-tains 100 animals, each of which has 15 boolean attributes an d 1 categorical attribute 9 . We translate the numeric attribute,  X  X egs X , respectively. Table 4 shows the confusion matrix of this exp eri-ment. In the confusion matrix, we find that the clusters with a large number of animals are likely to be correctly clustered. Ther e are 7 different types in zoo dataset and the animal numbers for ea ch type are 41,20,5,13,3,8,10 respectively. Our procedure, Algorithm 1 , identifies 5 clusters in the datasets and doesn X  X  identify t ype 3 (with 5 animals) and type 5 (with 3 animals) due to the limited number of samples. Figure 3 shows the original zoo dataset an d the reordered dataset by arranging rows based on their cluster m em-berships. We can observe the associations between the data a nd features. For instance, in Figure 3(b), feature 1 is a discri minative feature for cluster 1, feature 8 is a discriminative feature for both cluster 1 and cluster 3 and feature 7 is an outlier feature as i t dis-tributes uniformly across all the clusters. Our algorithm e xplicitly explore the association relationship between data and feat ures and tends to yield better clustering solution. The purity value of our approach, obtained by averaging the results of 10 trials, is 0.94. In comparison, the value is 0.76 for K-means approach.

In this section, we apply our clustering algorithm to cluste r docu-ments and compare its performance with other standard clust ering
P ( S i ) is also called the individual cluster purity.
The original data set has 101 data points but one animal,  X  X ro g, X  appears twice. So we eliminated one of them. We also eliminat ed two attributes,  X  X nimal name X  and  X  X ype. X  algorithms. In our experiments, documents are represented using binary vector-space model where each document is a binary ve ctor in the term space and each element of the vector indicates the pres-ence of the corresponding term. Since there is usually a symm etric association between the documents and words, we use the bloc k di-agonal clustering model described in Section 3 in our experi ments. Figure 3: Visualization of the zoo dataset and the reordered dataset after clustering. X-axis indicates the animals and Y-axis indicates the features. The shaded region represents non-z ero entries.
We use the following datasets in our experiments and Table 5 summarizes the characteristics of the datasets.

CSTR: This is the dataset of the abstracts of technical reports (TRs) published in the Department of Computer Science at the Uni-versity of Rochester between 1991 and 2002. The TRs are avail able at http://www.cs.rochester.edu/trs . The dataset contained 476 ab-stracts, which were divided into four research areas: Natur al Lan-guage Processing(NLP), Robotics/Vision, Systems, and The ory.
WebKB: The WebKB dataset contains webpages gathered from university computer science departments. There are about 8 280 documents and they are divided into 7 categories: student, f ac-ulty, staff, course, project, department and other. The raw text is about 27MB. Among these 7 categories, student, faculty, cou rse and project are four most populous entity-representing cat egories. The associated subset is typically called WebKB4 . In this paper, we did experiments on both 7-category and 4-category datase ts.
Reuters: The Reuters-21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1 987. It is a standard text categorization benchmark and contains 135 cat-egories. In our experiments, we use a subsets of the data coll ection which include the 10 most frequent categories among the 135 t op-ics and we call it Reuters-top 10 .

K-dataset: The K-dataset was from WebACE project [16] and it was used in [5] for document clustering. The K-dataset con tains 2340 documents consisting news articles from Reuters new se rvice via the Web in October 1997. These documents are divided into 20 classes.

To pre-process the datasets, we remove the stop words use a st an-dard stop list, all HTML tags are skipped and all header fields ex-cept subject and organization of the posted article are igno red. In all our experiments, we first select the top 1000 words by mutu al information with class labels. The feature selection is don e with the rainbow package [28].

In our experiments, we compare the performance of our approa ch on the datasets with the algorithms provided in CLUTO packag e [40]. Figure 4 shows the performance comparison. Each value is the pu-rity of the corresponding column algorithm on the row datase t. P 1 is a multi-level partitioning method which tries to maximiz e the cosine similarity between each document and the cluster cen troid. The criterion of P 2 is similar to minimizing the intra-scatter matrix in discriminant analysis. Hierarchical column shows the re sults of hierarchical clustering algorithms 10 . We observe that the perfor-mance of Algorithm 2 is always either the winner or very close to the winner. The comparison shows that, although there is no s ingle winner on all the datasets, our clustering approach is a viab le and competitive algorithm for binary clustering.
Figure 4: Purity comparisons on various document datasets.
In this section, we review the clustering methods that are cl osely related to our approach (see Figure 5 for a summary).
The results reported are the largest values of three differe nt hierar-chical clustering algorithms using single-linkage, compl ete-linkage and UPGMA aggregating policies.
First of all, our clustering model can be regarded as an integ ra-alternating optimization procedure common to K-means type algo-rithms. The procedure for feature assignment can be thought as an approximation of boolean factor analysis.

Our clustering model is also loosely related to additive clu stering where the similarities among the data points are being consi dered instead of the content of the features [8, 33]. In additive cl ustering, a similarity model is postulated that expresses the observe d similar-ities of data points in terms of their underlying features. F ormally, additive clustering tries to preforms the following matrix decom-position: S = PW P T , where S is the similarity matrix, W is a di-agonal matrix indicating the weights of each clusters, and P is a binary matrix indicating the cluster membership of the data points. The matrix P in additive clustering corresponds to the matrix A in our model. However, additive clustering works on similarit y ma-trix instead of the original data matrix and does not admit fe ature assignments.

Our clustering model performs binary matrix decomposition and can be thought as a special case of positive matrix decomposi -tion [22, 30, 38]. The positive matrix factorization techni ques place non-negativity constraints on the data model for optimizat ion. The binary constraints in our model optimization is a special ca se of non-negativity. The clustering model also shares some comm on characteristics with probability matrix decomposition mo dels in [26].

Our clustering model is similar to the data and feature maps i n-troduced in [41]. In [41], data and feature maps are two funct ions from the data and feature sets to the number of clusters and th e clustering algorithm is based on Maximum Likelihood Principle via co-learning between feature and data maps.

The simultaneous optimization in both directions of data an d fea-ture used in our clustering model is similar to the optimizat ion pro-cedure in co-clustering. Govaert [15] studies simultaneou s block clustering of the rows and columns of contingency tables. Dh illon et al. [10] propose an information-theoretic co-clusterin g method for two-dimensional contingency table. The relation betwe en our cluster model with the information-theoretic co-clusteri ng is dis-cussed in section 4.2.1.
 Figure 5: Summary of related work. The arrows show connec-tions.

Since our clustering method explicitly models the cluster s truc-ture at each iteration, it is viewed as an adaptive subspace c luster-ing. CLIQUE [2] is an automatic subspace clustering algorit hm. It uses equal-size cells and cell density to find dense regions i n each subspace in a high dimensional space, where cell size and the den-sity threshold are given as a part of the input. Aggarwal et al . [1] introduce projected clustering and present algorithms for discover-ing interesting patterns in subspaces of high dimensional s paces. The core idea is a generalization of feature selection which enables selecting different sets of dimensions for different subse ts of the data sets. Our clustering method adaptively computes the di stance measures and the number of dimensions for each class. It also does not require all projected classes to have the same number of d imen-sions.

By iteratively updating, our clustering method performs an im-plicit adaptive feature selection at each iteration and has some com-mon ideas with adaptive feature selection methods. Ding et a l. [12] propose an adaptive dimension reduction clustering algori thm. The basic idea is to adaptively update the initial feature selec tion based on intermediate results during the clustering process and t he pro-et al. [13] use a Chi-squared distance analysis to compute a fl exi-ble metric for producing neighborhoods that are highly adap tive to query locations. Neighborhoods are elongated along less re levant feature dimensions and constricted along most influential o nes.
As discussed in Section 4.2.1, our clustering model can be th ought of as an approximate iterative information bottleneck meth od. The information bottleneck (IB) framework is first introduced f or one-sided clustering [36]. The core idea of IB is as follows: give n the empirical joint distribution of two variables ( X , Y ) , one variable is compressed so that the mutual information about the other is pre-served as much as possible. The IB algorithm in [36] tries to m ini-mize the quantity I ( X ;  X  X ) while maximizing I (  X  X ; Y mutual information and  X  X is the cluster representation of X .
In this paper, we introduce a general binary clustering mode l that allows explicit modeling of the feature structure associat ed with each cluster. An alternating optimization procedure is emp loyed to perform two tasks: optimization of the cluster structure and up-dating of the clusters. We provide several variants of the ge neral clustering model using different characterizations. We al so provide a unified view on binary clustering by establishing the conne ctions among various clustering approaches. Experimental result s on doc-ument datasets suggest the effectiveness of our approach. The author is grateful to Dr. Shenghuo Zhu for his insightful sug-gestions. The author also wants to thank the anonymous revie wers for their invaluable comments.
