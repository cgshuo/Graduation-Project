 The emerging interest in Massively Parallel Stream Pro-cessing Engines (MPSPEs), which are able to process long-standing computations over data streams with ever-growing velocity at a large-scale cluster, calls for efficient dynamic resource management techniques to avoid any waste of re-sources and/or excessive processing latency. In this paper, we propose an approach to integrate dynamic resource man-agement with passive fault-tolerance mechanisms in a MP-SPE so that we can harvest the checkpoints prepared for failure recovery to enhance the efficiency of dynamic load migrations. To maximize the opportunity of reusing check-points for fast load migration, we formally define a check-point allocation problem and provide a pragmatic algorithm to solve it. We implement all the proposed techniques on top of Apache Storm, an open-source MPSPE, and conduct ex-tensive experiments using a real dataset to examine various aspects of our techniques. The results show that our tech-niques can greatly improve the efficiency of dynamic resource reconfiguration without imposing significant overhead or la-tency to the normal job execution.
 H.2.4 [ Database Management ]: Systems Fault-Tolerance; Elasticity; Resource Management Recently, a new generation of Massively Parallel Stream Processing Engines have emerged, such as Apache Storm [19], Apache S4 [13] and Muppet [11]. These systems are tuned towards low-latency processing and massive scale-out to achieve high throughput. With such systems comes a plethora of issues to be handled, such as hardware and soft-ware failures, load skewness, overload and underload. c  X  2015 ACM. ISBN 978-1-4503-3794-6/15/10. . . $15.00.
As the main task of stream processing systems is to exe-cute continuous and long-standing queries, it is easy to en-counter failures especially when running on large-scale clus-ters. Therefore, it is widely recognized that fault-tolerance is an essential requirement for a large-scale MPSPE. A typ-ical approach to achieve fault-tolerance is to make periodic checkpoints of the computation state of each task and store them on other processing nodes. A failed task can be re-covered by loading the latest checkpoint and replaying the data that arrived after the checkpoint was made. To achieve low recovery latency, i.e. the time it takes to completely re-cover a failed task, the frequency of checkpointing should be reasonably high. Frequent checkpointing incurs significant overhead, especially when the state is large and subject to frequent updates. However, this price has to be paid for fault-tolerance and fast recovery.

Another important requirement of an MPSPE is the abil-ity to perform dynamic resource reconfiguration to adapt to runtime changes of system workload incurred by, for exam-ple, the fluctuation of data input rates, the change of data distributions, etc. Resource reconfigurations include rebal-ancing load distribution among the processing nodes, adap-tively increasing and decreasing the number of processing nodes. An essential operation in dynamic reconfiguration is to migrate tasks among the processing nodes, which often incurs significant latency and overhead and hence hinder the system from performing frequent reconfigurations.

In this paper, we propose an approach to integrate check-point-based fault-tolerance and dynamic resource reconfig-uration. Such an integration can achieve low-cost and fast state migration by exploiting the checkpoints made for fault-tolerance. This is because checkpoints contain the compu-tation states of the tasks and migrating a task is essentially equivalent to recovering the task at another location. Since the price of maintaining checkpoints have to be paid any-way, this benefit comes almost for free. In summary, the main contributions of this paper are as follows:
In this section, we present related work and relevant back-ground on some of the problems faced by MPSPEs, such as fault-tolerance and dynamic resource management.
The basic fault-tolerance techniques in distributed stream processing systems can be categorized into upstream backup , passive backup and active backup [9, 6, 17, 3].

Upstream backup requires that every tuple included in the output buffer on an upstream operator instance, until the processing state of o i no longer depends on it. In case of a fault, the buffered tuples are replayed to restore the failed state. Upstream backup has a significant problem when used alone, as the size of the upstream buffers are not bounded.
Passive backup relies on checkpoints, which are basically copies of the current processing state. When a checkpoint has been stored on a separate node, the buffers can safely be trimmed. In case of a fault, a checkpoint can be used to install an  X  X ld X  state, which can then be made up-to-date, by replaying the buffered tuples, from the output buffers on the upstream operator instances.

The recovery latency of upstream and passive backup can be in the order of tens of seconds. For some applications such a recovery latency can be unacceptable, in which case active backup is often used. Active backup works by executing a duplicate computation on a separate location, such that if either the primary or a duplicate fails, the other can take over. The recovery latency, in this case, is to synchronize the computation. However, active backup is very expensive and basically needs double resources.

There is much work on trying to improve the performance of these basic techniques, such as [10, 16, 6]. Some of these defines the benefits of making checkpoints more fine-grained, some discusses proper checkpoint schedules and some even investigates the tradeoff between all of these techniques.
A recent article [22] presented a tweaked passive backup strategy. The idea is to treat the streaming computation as a series of small deterministic batch computations, and store the history (lineage) of computations. One of the ben-efits this provides is that parallel recovery can be easily sup-ported, such that if one node fails, multiple nodes can work together to calculate the failed state.
Supporting intra-operator data partitioning at runtime has been investigated in [18]. As their work focuses strictly on load-balancing and not on scaling, it can be considered as part of the solution towards dynamic resource management.
Multiple papers [20, 23, 24] have studied how to allocate operators to achieve load-balancing. In their models, a query is composed of an operator network and each operator does not need to be partitioned. In other words, their models cannot handle dynamic scaling of individual operators.
The authors in [15] studied how to provide an abstrac-tion to decouple the number of threads and the number of processing units to achieve dynamic intra-operator scaling, i.e. changing the number of threads used for processing on runtime. While their work mainly focused on multi-core systems, our system also provide such an abstraction in the context of a large-scale cluster.

To support scaling, one of the key operations is to change the parallelization degree of state-partitioned operators at runtime. This unfortunately poses problems, one of which is that the partitioning often depends on the number of in-stances. Changing the number of instances thus requires repartitioning some state. The authors in [2], discusses how to avoid this, by adding another operator to perform aggre-gation of the partial results of the scaled reduce operators.
There is a recent research work [5], which focuses on how to design an adaptation algorithm to achieve elasticity in a distributed stream processing system. They rely on two simple metrics (congestion and throughput), to determine how/when to scale. Their algorithms can be adopted in our system to make scaling decisions.

There are some efforts on developing systems to support both load-balancing and elastic scaling. ESC [14] is a dis-tributed stream processing platform, which supports elas-ticity and load-balancing by using an Autonomic Manager, which has access to information about the workload of each node, and the queue lengths of the worker processes, to make global decisions. StreamCloud [7] is another recent dis-tributed stream processing engine that supports both elas-ticity and load-balancing. Load-balancing is supported by defining a point in time p , such that all tuples of a bucket (group) earlier than p , are processed by the old  X  X wner X , and tuples later than p are processed by the new one. This sim-ple technique has a problem for stateful operators, as one tuple might contribute to multiple windows, which means there will be tuples that needs to be processed by both the old and new owners (and thus duplicated).

While the above work mostly focuses on how to make better adaptation decisions, our work differs by focusing on how to make adaptations, more specifically state migrations, more efficient and less intrusive to the system X  X  normal exe-cution.

A recent article [4] studied how to integrate the operations of fault-tolerance and scaling out. Our work can be consid-ered as another step in this direction. In their model, the checkpoints of an operator are stored on its upstream op-erators and hence there are limited opportunities to exploit the fast state migration strategies introduced in this paper. To solve the problem, we design a system that allows check-points be placed at any node, and we provide an algorithm to derive a good checkpoint allocation plan.
Data is modelled as a number of continuous streams of tuples in the form of  X  id,key,value,ts  X  . The id is a unique identifier of the tuple, composed by the unique id of the stream that the tuple comes from and the per-stream unique sequence number of the tuple. The key is the key of the tuple, as a blob in arbitrary form, that is opaque to the system. It is the value of the key that decides how the tuple is routed, which is similar to other systems, e.g. Hadoop. The value is the value of the tuple, also as a blob in an arbitrary form that is opaque to the system. The ts is a timestamp associated with the tuple, which can be used to indicate the time that the tuple is generated and to define time-based window computations.
The programming model consists of one function: which takes a key-tuple pair and returns a list of key-tuples pairs. The function is associated with a local storage to keep track of the current state of the computation. This programming model can be considered similar to other typ-ical programming models used in MPSPEs. For example, both the Update and the Map function in the MapUpdate framework [11] are equivalent to our compute function with and without using the local storage to store computation state respectively.
A job can be considered as a set of continuous queries represented as an operator network. An operator network is a directed acyclic graph  X  O,E  X  , where each vertex is an operator O i , and each edge E i is a stream where the direc-tion represents the direction of data flow. Figure 1 shows an example with five operators.
An operator has a set of input streams In i , and a set of output streams Out i . On each incoming tuple, a func-tion F i is invoked, which updates processing-state  X  i and a map m key  X  seq , which, for each key processed by the oper-ator, contains the id of the latest tuple contributing to  X  The operator adds tuples to a set of output streams Out i specified by the function. An operator O i is defined by func-tion F i : (  X  ( In i ) , X  i ,m key  X  seq )  X  (  X  ( Out i where  X  ( In i ) returns the next incoming tuple, and  X  ( Out returns the set of resulting tuples.
An operator O i in the operator network can be parallelized to several instances o i j , where i is the id of the operator and j is the id of the operator instance. Formally, an operator O , is modelled as where n describes the degree of parallelization of O i .
Each input tuple of O i is associated with a key from a set of independent keys K i . The keys K i are partitioned into a set of key groups, such that each group contains a non-overlapping subset of K i . The set of key groups of K is modelled as
Every operator instance o i j will handle a non-overlapping subset of G i . There is a trade-off between avoiding exces-sive overhead and maintaining sufficient fine-grained control when partitioning state, e.g. if using | G i | = 1, the system would need to maintain one output queue per key, which would incur excessive overhead, while using | G i | = | K would hinder useful state-migration.

Based on the model above, the main assumption is that the processing of key groups are independent of one another. Each key group g i j  X  G i has an independent processing state  X  j and a corresponding set of checkpoints
A cluster has a set of nodes N = { n 1 ,...,n n } . Each node executes exactly one instance of every operator, which means each node has the capability to process any logic. Figure 2 shows how the operator network shown in figure 1 is executed in a cluster with two nodes.

Every node n processes a non-overlapping subset of key groups  X  n from all key groups, defined in the operator net-work. Furthermore each node n stores a set of checkpoints  X  . Any node n can thus process any subset of key groups and store a subset of checkpoints. From the users X  perspec-tive, the execution graph becomes similar to Figure 3.
Proceessing Order. Given that an operator has several input sources, obtaining a reproducible ordering of input tu-ples is costly [3]. We assume out-of-order processing, which means that operators eventually produce the same result, given the same data as long as the unorderedness is within some bound [12]. If the computation relies on a strict or-dering of the input, then one has to order the data using an additional function before feeding the data to the ac-tual computation, e.g. the SUnion function proposed in [3]. Therefore, the assumption of out-of-order processing does not exclude applications that require a strict input order. Symbol Explanation
O i Operator i o j Instance j of operator i
G i Set of key groups of operator i g j Key group j of operator i  X  j Processing state of key group j of operator i
CP i j Set of checkpoints of key group j of operator i cp i j,m Checkpoint m of key group j of operator i  X  n Set of key groups processed at node n  X  n Set of checkpoints stored at node n
Our system is built on top of Apache Storm [19], an open-source MPSPE. In Storm, a job is specified as an instance of a topology, which defines how the job is executed, such as how the operators are parallelized, how the data are trans-ferred between the operators, etc. Storm comes with a ba-sic topology which implements some essential functionalities of parallel stream processing, and allows a user to build a stream processing job by simply creating a topology object, and adding operators to it. Most of our functionalities are implemented as an advanced topology on top of the basic one, and is thus very loosely coupled with Storm. Figure 4 gives an overview of how our system is built on top of Storm.
The Controller is a system level operator, used to make global decisions. The controller executes the Fault-Tolerance Manager and the Adaptation Manager. In addition, it is re-sponsible for periodically requesting statistics from all oper-ator instances in the job and preparing it such that the infor-mation is easily accessible. Each operator keeps statistics on the utilizations of CPU, memory and network bandwidths and the input rate of each key group.

The Adaptation Manager is responsible for making global decisions on scale-in/out, and load-balancing. It executes the adaptation algorithm, which bases its decisions on the statistics collected by the controller.

The Elastic Scheduler is the key component used to fa-cilitate dynamic horizontal scaling. Storm provides a basic scheduler that is responsible for managing the assignment of operator instances to the cluster nodes. Each time the basic scheduler is invoked, it will look for any operator in-stances that have not been allocated to any node. This happens when the job is initially submitted or some faults occur. The scheduler will then distribute the non-allocated operator instances to the nodes as evenly as possible. This basic scheduler is insufficient for our framework as it always attempts to allocate all the operator instances. Note that in Storm, a topology is static in the sense that the number of instances of each operator is fixed, and cannot be increased and decreased dynamically at runtime. To dynamically in-crease or decrease the instances, one has to pause the whole job, rebuild, and redeploy the complete topology. This will incur prohibitive latency to runtime adaptation.

In order to achieve low-cost runtime reconfiguration while keeping the simple implementation of static topology, we implement a customized elastic scheduler in Storm by em-ploying the following technique. Given the operators of a submitted job, a pre-defined set of instances for each oper-ator is prepared. Initially, only a subset of instances of each operator is scheduled over the initial cluster. The scheduler will maintain that each node has one instance of each op-erator in the topology. This allows us to flexibly allocate the key groups of each operator to any node. If an instance is not assigned any key group, then it will not be run, and hence will consume negligible resources on its host node.
The Fault-Tolerance Manager runs in a separate thread as part of the controller. It detects failures by reading Storm heartbeats directly from Zookeeper [8]. If a failure is de-tected, it stops the processing of all the failed operator in-stances, and waits until the scheduler has reallocated the failed instances. Then it invokes the recovery process to restore the computing state and recreate checkpoints.
The system uses a combination of upstream backup and passive backup, as this combination requires fewer compu-tation resources compared to active backup, and has the po-tential of using checkpoints to achieve faster state-migration. The technique was proposed in previous papers as a general technique which can provide at-least-once or exactly-once processing [10].

To speed up the recovery, a checkpoint will be periodically created for each key group and upon failure of a key group, its latest checkpoint will be loaded and the input data which arrived after its latest checkpoint, will be replayed.
Dynamic partitioning of data is necessary to support migrating key groups from one node to another. To support this kind of operation, the system uses dynamic partitioners between all operators. It works by first calculating which key group a tuple belongs to by a simple hash function. Then it looks up in a routing table to see, which operator instance is currently responsible for processing the key group, and then sends the tuple there. The routing table is updated directly by the controller as part of the state-migration policies.
Messages. There is one FIFO message stream between each operator instance, such that all messages (data and control messages) are sent on the same stream. When re-ceiving a message, the corresponding key group g x is cal-culated from the key of the message. A message can be fast , in which case it is processed immediately without go-ing through the FIFO queue. Alternatively, a message can also be normal , in which case it is put into the input queue for the calculated key group, and will be processed in a FIFO manner. Furthermore, a message can be stateful , if it con-tains data. Supporting two different priorities of messages is especially useful for control messages, as these can often skip dataqueues, leading to a system which can be controlled more efficiently.
In this section, we explore different techniques to obtain dynamic resource management in a general stream process-ing system. The crucial operation is to move a subset of key groups from  X  n to  X  m . In this section, we present our state-migration protocols, which support this operation for partitioned stateful operators.

In order to analyse the different protocols X  completion time, and the data processing latency incurred by them, we first need to model the time that it takes for a message to be Actions Cost Fast Message 1 Normal Message 1 + C Fast Stateful Message msgSize Normal Stateful Message msgSize + C
Processing one message at o i b  X  o i
Size of output buffer from o i a to g j x  X  o i transferred over the network. The time from sending a fast message to being seen at the receiver is defined as 1, while the time from sending a normal message, until being seen at the receiver depends on the input queue at the receiver. For the sake of our analysis we assume the size of input queues is bounded by a constant and we define the time of sending a normal message as 1 + C , where C is to account for the queuing time of the message. Lastly in case a fast message is stateful , we define the time as msgSize , and in case a normal message is stateful , we define the time as msgSize + C . To explain the protocols, we make heavy use of the shorthand O i  X  1 , which is the set of upstream operator instances of O
Moving g i x from o i a to o i b using direct state-migration, could potentially incur a large overhead, if the processing ing and deserializing  X  g i time and the latency of the operation, as will be shown later and verified by our experiments. It is possible to avoid this cost, if migrating to an operator instance, which already has the newest checkpoint of g i x , by converting the checkpoint to state and replaying the upstream buffers. The technique is described by Algorithm 1.

Algorithm 1: CP-Assisted Migration (abbreviated: CPA)
Controller informs O i  X  1 : g i x moves from o i a to o i when o in O i  X  1 receives information do 3 Send toState to o i b . contains output buffer 4 Redirect data for g i x to o i b 5 Send toCheckpoint to o i a when o i a gets toCheckpoint from all o in O i  X  1 do 7 Convert state  X  g x to checkpoint when o i b gets toState from any o in O i  X  1 do 9 if  X  g x is null : convert checkpoint to state  X  g x 10 Process replayed data
Since this state-migration protocol (CPA) relies on the system X  X  fault-tolerance, it naturally inherits the processing guarantee of the fault-tolerance. For example if the system is currently providing a processing guarantee of exactly-once, then when using this technique, the duplicate tuples can automatically be discarded. In case the system provides at-most-once or at-least-once, it is assumed, as this guarantee is sufficient for fault-tolerance, it is also sufficient for state-migration.

Description. First each operator instance o , from the set of upstream operator instances of O i , sends a toState message to o i b . The toState message contains the output buffer, and is sent as a fast message, as o i b must process the output buffers before processing other data. After having sent the toState message, new data for g i x is redirected from o to o i b . Lastly a toCheckpoint message is sent as a normal message to o i a . The algorithm now proceeds in parallel at o and o i b , where o i a handles the toCheckpoint messages, and o handles the toState messages.

When receiving all normal toCheckpoint messages at o i a , it is known that no more data will ever be received. For this reason it is safe to convert the state to a checkpoint.
When receiving a toState message at o i b , it is known there is already a checkpoint at o i b , which can be converted to state, after which the output buffer contained within the toState message is processed. Notice that the checkpoint is only converted to state, when receiving the first toState message.

Analysis. The cost-model captures the cost of moving g , from o i a to o i b , when o i b has a checkpoint from the set of newest checkpoints of  X  g i considered, as this step exists in all the state-migration al-gorithms.

The completion time is defined, as the time from the mi-gration was ordered at the controller until effectuated.
The latency is defined, as the amount of time the process-ing of g i x is paused. For this protocol it equals the maxi-mum time it takes to send, and the sum of time to process all output buffers (done sequentially). The completion time and latency is thus equal.
When moving key group g i x from o i a to o i b , using the cp-assisted state-migration protocol (CPA), the latency and completion time heavily depends on  X  o j tocol, as described in algorithm 2, we show how to achieve a significantly lower latency, which is independent on  X  o j
Algorithm 2: CP-Assisted Low-Latency Migration (abbreviated: CPAL)
Controller informs O i  X  1 : g i x moves from o i a to o i when o in O i  X  1 receives information do 3 Send toState to o i b . contains output buffer 4 Duplicate data for g i x to o i a and o i b when o i b gets toState from any o in O i  X  1 do 6 if  X  g x is null : convert checkpoint to state  X  g x 7 Process replayed data 8 if o i b has toState from all o in O i  X  1 : 9 Send processedAllReplay to all o in O i  X  1 when o in O i  X  1 gets processedAllReplay do 11 Send syncGroup to o i b 12 Stop sending data for g i x to o i a 13 Send toCheckpoint to o i a when o i a gets toCheckpoint from all o in O i  X  1 do 15 Convert state  X  g i x to checkpoint when o i b gets syncGroup from any o in O i  X  1 do 17 Begin buffering incoming data to g i x from o when o i b has syncGroup from all o in O i  X  1 do 19 Process all buffered data for g i x and stop buffer
Description. First each operator instance o in O i  X  1 sends a normal message called toState, which contains the output buffer for g i x , to o i b , and o begins duplicating new data for g i x to both o i a and o i b .

When o i b gets the toState message, it converts any check-point to state for the key group g i x , and processes all data contained within the toState message. This idea is similar to the previously described technique, with the exception that data is currently being duplicated to both o i a and o which means the processing at o i a is not paused. When o has received and processed all toState messages, the state of o i a and o i b must be synchronized, such that o i tinue the computation exactly where o i a stops. This is done by sending a fast message from o i b to all o in O i  X  1 , called processedAllReplay.
 When a operator instance o in O i  X  1 , gets a processedAll-Replay message, it knows o i b has processed all replayed data, and that synchronization between o i a and o i b should be done. First a normal message called syncGroup is sent from o to o , and then o stops duplicating data for g i x to o i a . Lastly a normal message called toCheckpoint is sent to o i algorithm now proceeds in parallel at o i a and o i b , where o handles the toCheckpoint messages and o i b handles the sync-Group messages.

When receiving a toCheckpoint at o i a from all o in O i  X  1 it is known that no more data will be received. It is thus safe to convert the state to a checkpoint.

When receving a syncGroup from o in O i  X  1 at o i b , the op-erator instance must buffer all new incoming data from o to g . The synchronization is complete, when having received a syncGroup from all o in O i  X  1 , and all buffered data can be processed.

Analysis. The cost-model captures the cost of moving g , from o i a to o i b , when o i b has a checkpoint from the set of newest checkpoints of  X  g i considered, as this step exists in all the state-migration al-gorithms.

The completion time is defined, as the time from the mi-gration was ordered at the controller until effectuated.
The latency is defined as the time from buffering begins, until all buffered data is processed:
Using direct state-migration to move g i x from o i a is done by requesting o i a to create a new checkpoint of g and send it to o i b . After o i b acknowledges the checkpoint, o a requests all operator instances o in O i  X  1 to trim output buffers. When the trimming is done, algorithm 1 is executed.
For analyzing the cost of direct state-migration, we sim-plify our analysis by assuming the amount of data, which needs to be replayed from the upstream operator instances are negligible, due to the recent trim operation. By consid-ering the completion time and latency costs from table 3, it can be seen that if  X  o j the completion time and latency of CPA is also negligible. The completion time and latency of direct state-migration becomes  X  g i
In this section, we show how to allocate checkpoints, such that the probability they can be used with the cp-assisted state-migration protocols (CPA or CPAL), is maximized. This is important, because the latency and in many cases the completion time of state-migration is significantly less for the cp-assisted state-migration protocols (verified in the evaluation section). Checkpoint allocation can therefore be considered as proactive load-balancing.

In this work we consider partial checkpoints [16], where each new checkpoint only contains the difference between the current computation state and the last checkpoint. Com-pared to other checkpointing techniques, partial checkpoint-ing has low overhead and is widely used. Partial checkpoint-ing has been been implemented in well-known systems such as Google MillWheel [1] and Storm Trident [19]. For par-tial checkpoints, the checkpoint allocation is only updated periodically, as it is costly, and the migration of key groups can only use cp-assisted strategies opportunistically. There-fore, a good checkpoint allocation algorithm is needed to maximize the opportunity.

Algorithm -Baseline. The simplest algorithm for al-locating checkpoints to nodes, which works by ensuring a checkpoint, cannot be on the same node, as an identical checkpoint, or the key group it is checkpointing.

Algorithm -Even Distribution. Even Distribution is an extension of the baseline algorithm and we believe it represents most existing systems X  behavior. The algorithm imposes two constraints the allocation. Constraint 1 states that a checkpoint cannot be on the same node as an identical checkpoint, or the key group it is checkpointing. Constraint 2 states that checkpoints must be divided fairly on all nodes, such that no node has more checkpoints than d # cps # nodes
Intuition -Correlated Distribution. A set of nodes can become overloaded, if the load of the key groups allo-cated to them increases. The change in workload of the key groups could affect the workload of other key groups, which can be measured by correlations. Assume a set of key groups A gets an increased workload, and introduces overload on the set of nodes, where it is allocated. Now the adaptation algorithm would try to migrate a subset of the key groups to a set of nodes, which are not overloaded. In order to do so efficiently, it is beneficial, if as many checkpoints as possible for key groups in A are available on the nodes that are not overloaded. To increase the probability of this, we allocate the checkpoints of the key groups in A to the nodes having key groups that are as negatively as possible correlated with the key groups of A.
In this section we define the optimal solution to the prob-lem of allocating partial checkpoints, as a minimization prob-lem. The solution consists of a mapping from each check-point to a node. In the formulation below each checkpoint is allocated to node i . min X where G is the set of all key groups, N is the set of nodes in the cluster, G i is the set of key groups at node i, such that 1. A node can contain one cp i j , if it doesn X  X  contain g
Constraint 1 ensures no node can contain both a key group and a corresponding checkpoint. This is needed to ensure fault-tolerance. Also in case multiple checkpoints are defined for one key group, these cannot be allocated at the same node either.

Constraint 2 guarantees the load of the key groups be-ing checkpointed, are distributed fairly even among all the nodes. It can be important to spread out checkpoints, as it increases the amount of unused processing capacity, which can be  X  X eached X  by the cp-assisted state-migration policies from any node.

The minimization can be proved to be NP-hard by a polynomial reduction from the NP-hard problem called The Multi-Resource Generalized Assignment Problem [21]. Due to space issues, we omit the proof.
As the problem formulation is NP-hard, we simplify the problem in order to obtain a solution with polynomial com-plexity. We simplify the problem by assuming that once a checkpoint is allocated to a node, its allocation does not need to be changed again to achieve a satisfactory solution. This reduces the problem to finding a good order of allocat-ing checkpoints. Our proposed algorithm 3, is polynomial with a complexity of O ( | N | X  # checkpoints ).

Heuristic. We aim to design an algorithm which allo-cates the checkpoints in a order, such that the largest ben-efit in terms of the minimization problem can be obtained. For each checkpoint we calculate the potential impact on the minimization problem, of allocating it to any possible node. Then we allocate checkpoints in turn, such that the check-point with the largest decrease in benefit if not placed at the current best node, is to be allocated next. The reasoning be-hind this heuristic is to prioritize allocating the checkpoints, which harms the value of the minimization problem most, if they cannot be assigned to their current best nodes. In this way the algorithm allocates the most  X  X ensitive X  checkpoints first.

Explanation. If the checkpoint to allocate next, satisfies constraints 1 and 2 at the best node, then it is simply allo-cated. Otherwise, the calculation is updated to consider the difference in benefit between the 2nd and the 3rd best node assignment for the checkpoint, and the next checkpoint to al-locate is recalculated. Sometimes multiple nodes M achieve the same correlation value for a given checkpoint, and sat-isfy both constraints 1 and 2. This happens often, when Algorithm 3: Partial Checkpoint Allocation Input : The set of checkpoints and nodes
Output : Map with checkpoint to node allocation
Procedure calcOptimizedAllocation 2 cps  X  new List 3 for each cp in checkpoints do 4 for each node in nodes do 5 cp.vals(node, corr ( cp,node )  X  load ( cp )) 6 cps.add(cp) 7 sort cps in desc order of best -2 X  X h best vals 8 while cps.size &gt; 0 do 9 cp  X  cps.peek 10 bNodes  X  getBestNodes(cp) 11 if hasAllNodesBeenConsidered(cp) then 12 assign cp to node with min correlation 13 value, secondly to node with min cp load 14 continue 15 else if bNodes is null then 16 // no node currently satisfies constraint 1 17 updateSortValue(cp) 18 sort cps in desc order by sortValue 19 continue 20 cpPlaced  X  false 21 for each node in bNodes do 22 if satisfies2(cp, node, assign) then 23 assign.put(cps.poll, node) 24 cpPlaced  X  true 25 break 26 if !cpPlaced then 27 updateSortValue(cp) 28 sort cps in desc order by Sortvalue 29 return assign
Procedure getBestNodes (cp) 31 safeNodes  X  nodes satisfying constraint 1 for cp 32 IthBest  X  i X  X h best nodes for placing cp, 33 return safeNodes  X  IthBest
Procedure updateSortValue (cp) 35 cp.sortIndex++ 36 cp.sortVal  X  cp.atIthBestNode(cp.sortIndex) there is no correlation, i.e. the value is 0. When this hap-pens, we allocate the checkpoint to the node in M, which has the lowest checkpoint load, i.e. the minimum sum of loads for each key group corresponding to the checkpoints on the node. The reasoning is to divide the checkpoints evenly over all the nodes in the set, because each node is equally good.
Temporal changes. The  X  X uality X  of a checkpoint allo-cation may deteriorate over time, as each checkpoint-assisted state-migration operation changes the location of at least one checkpoint, and thus introduces noise. The quality of a given checkpoint allocation is measured by the value of the minimization problem. The lower the score, the better the allocation.

Each time the checkpoint algorithm is executed, the sys-tem calculates the score of the current checkpoint allocation and also an approximate solution to the minimization prob-lem using algorithm 3. If the difference is greater than a user-definable threshold  X  , the checkpoint allocation is up-dated, by moving the checkpoints with improves the score maximally, until the difference is lower or equal to the user-defined threshold  X  . In this way, the checkpoint allocation is updated to ensure it remains useable for maximizing the number of checkpoint assisted state-migrations.

By setting  X  to a low value, the deterioration of the check-point allocation can be prevented by frequent migration of key groups. On the other hand, setting  X  to a high value, can result in significant deterioration of the checkpoint allo-cation over time, which will decrease the possibility of using the checkpoint-assisted state-migration policies. The lower the value of  X  , the higher the overhead, due to the need for more state-migrations.
All experiments are carried out on Amazon EC2. Spouts are executed on instance type m3.xlarge . All other nodes are of type m1.medium . Spouts are executed on faster instances to ensure they can output enough tuples to stress the sys-tem if needed. The controller, master and zookeeper are co-located on a dedicated node of type m1.medium , called the control node . There is always exactly one checkpoint per key group.

The Airline On-Time dataset is provided by the Re-search and Innovative Technology Administration, United States Department of Transportation 1 .

We use data for the period from January 2004 to Decem-ber 2013, which contains information about airplanes, such as departure, arrival, expected time of arrival and so on. The data contains 67,328,899 tuples, and is approximately 28gb uncompressed. The dataset is rich with more than 100 attributes per tuple. We split the data into multiple slices, and stream it in parallel from each of the inputs.
The proposed state-migration protocols are evaluated on latency , i.e. the additional processing latency of the input tuples incurred by the migration protocol and completion time , i.e. the time to complete the whole migration.
The job processes air-flight data and extracts delays, which are then stored per plane over a window of a year. To achieve this functionality, the job consists of one input and two ordinary operators. The input operator parses the data, and sends to operator 1 . Operator 1 extracts delays, and sends events (delays) along to operator 2 . Operator 2 sums delays by airplane per year, and stores all events with delay in a key-value store.

The job is parallelized over 25 nodes, with 1 control node and 4 inputnodes. The remaining 20 nodes are used to pro-cessing operator 1 and 2. Each operator is divided into 20 key groups (evenly allocated the 20 nodes used to process the operators). The input rate is 8000 tuples per second, which guarantees no node is overloaded. The job was left running for 500 seconds, before a key group of operator 2 was migrated from node 2 to node 4.

Figure 5 and 6 shows the latency and completion time of the state-migration operations with varying number of tuples to replay (achieved by adjusting the trimming). The results show that the latency of CPA depends on the amount of data to replay. CPAL does not depend on the replay size, but instead on the time to synchronize, which is shown to be very short for this experiment. Both CPA and CPAL have http://www.rita.dot.gov/bts/ similar completion time, which depends on the size of data to replay, as expected.

In summary we conclude that both CPA and CPAL out-perform direct state-migration in terms of latency and com-pletion time for this experimental setting, especially, when state size is large, and replay size is small. Furthermore, we have shown that by using the CPAL technique, the system incurs negligible latency to tuple processing.

Obtaining negligible latency is good from the users point of view, but the system also gets an easily overlooked benefit. Consider that latency during state-migration, comes from the periods where the processing is paused. When the pro-cessing resumes, all the buffered data must be processed. If the system is processing high-volume input, then the amount of buffered data might be very significant. This means it can take a long time to process, leading to bad performance and even to severe problems such as timeout of a node, due to extreme load for an extended period. The low-latency state-migration technique, thus represents a very significant improvement over existing methods.
The following set of experiments requires an adaptation algorithm. As this paper mainly focuses on how to increase the efficiency of adaptations, we adopt a simple adaptation algorithm that supports load-balancing, scale-in and scale-out. Scale-in is done by migrating key groups away from the resources to be freed, and scale-out is done by acquir-ing new resources followed by allocating key groups there. Load-balancing is done through active repartitioning of key groups, which is similar to the method in [18].

We use  X  =  X  (see constraint 2 in problem formulation) for the optimized checkpoint allocation, because we want to explore the limits of what can be obtained from considering only the correlations.
The optimum checkpoint allocation is defined by our prob-lem formulation, and can be calculated for small problem sizes, using exhaustive search. Unfortunately the problem is so expensive to calculate using this technique, that we are unable to do so for any reasonably sized job. Solving this problem for 20 nodes, with only 7 checkpoints, took more than 1 . 5 hour (we stopped it), on a desktop with an Intel Core i7 processor (3,4 GHz) and 8GB RAM.
The job consists of one input and two operators each with 10 key groups. The input is synthetic, and the calculation is a simple count. Initially the input emits all data to operator 1, but each second it begins redirecting one percent of data more to operator 2, such that all data is being sent to opera-tor 2 after 100 seconds. Operator 1 and operator 2 are thus negatively correlated with a value of -1.

The job is parallelized over 4 nodes. Node 0 is dedicated to process inputs and nodes 1 and 2 are worker nodes, dedi-cated to processing the two operators. All the key groups of operator 1 are assigned to node 1, and all the key groups of operator 2 are assigned to node 2. Furthermore, both node 0 and node 2 are under an external load that consumes 70% of their CPU. Finally, node 3 is the control node, and is not considered by the adaptation algorithm.

For this job, the worst, average and best checkpoint allo-cation can be deduced. The optimal checkpoint allocation is to place all checkpoints of operator 2 on node 1, and all checkpoints of operator 1 on node 2. To see why, consider that operator 2 is assigned to node 2, and node 2 is already loaded to the overload threshold. As more and more data is sent to operator 2 from the input, this workload must be transferred to a node, which is not overloaded (node 1). Placing the checkpoints of operator 2 on node 1 enables CPA or CPAL to perform fast migration. On the contrary, if all checkpoints of operator 2 are placed on node 0, only direct state-migration can be done, which is the worst-case assignment. The even distribution approach, will result in 50% of the checkpoints being available for cp-assisted state-migration, because it randomly spreads out the checkpoints evenly over all the available nodes. Our checkpoint allocation algorithm, achieves the best possible checkpoint allocation for this experiment. It provides twice the amount of available checkpoints, compared to the even distribution approach .
Having considered a simple job, we now want to evaluate the checkpoint allocation algorithm for a more realistic job.
We extend the aforementioned sophisticated job (in Sec-tion 7.1) with two operators (3 and 4). Operator 3 subscribes to tuples from operator 1 and calculates the percentage of time each airplane has been in the air. Operator 4 receives a random data tuple from each instance of operator 1 for ev-ery 200 milliseconds, and calculates the percentage of delayed flights on randomly sampled data to reduce the computation cost.

Calculating a result on randomly sampled data to reduce computation cost, is typical for real-life jobs. Operator 4 is independent on the input and hence theoretically completely uncorrelated with the rest of the operators in the job. The correlation between operator 1 and operator 2 should be close to one, because the percentage of delays is stable in the input data. The correlation between operator 1 and operator 3 should also be close to one. When increasing the load of the system, the load of operator 1, 2 and 3 will increase almost linearly, whereas the load of Operator 4 will remain fixed.

The job is parallelized over 25 nodes. 1 node is used as control node, and 4 are dedicated to parse input. We use 16 nodes to process operator 1, 2 and 3 and 4 nodes to process Operator 4. In addition, we vary the number of key groups from 16 per operator to 48 per operator.

Each experiment is run for 48 minutes, to investigate if the checkpoint allocation impacts the system for more than a short period. The first 15 minutes is used to collect statistics about the system, after which, the checkpoints are allocated with either our checkpoint allocation algorithm, the even distribution or the baseline approach.

Figure 7 compares the percentage of checkpoint assisted state-migrations which were effectuated when using different techniques to allocate checkpoints. The lazy-move technique is intended for full checkpoints, and works as described in previous sections, by lazily updating the checkpoint alloca-tion, then doing CPA afterwards.

The figure shows that both even and baseline follow a similar trend and give similar results. This is because the expected number of checkpoints at any node is equal in the average case. When increasing the number of groups, the percentage of checkpoint assisted state-migrations are shown to increase, which is simply explained by the fact that there are more key groups in total.

Our proposed checkpoint allocation technique outperforms the even and baseline approaches with a factor of approxi-mately two. In order to understand why our optimized ap-proach is only improving slightly with an increasing number of key groups, consider that when the number of key groups are doubled, then the load of each key group is halved in av-erage. Remember that one checkpoint per key group is used, which means that when doubling the number of key groups, the potential load of each checkpoint is halved in average. This means the number of key groups have no impact on the  X  threshold, which in turn tells us that the optimized technique will not spread out the extra checkpoints. The reason for the improvement, is that more key groups leads to more potential to use the correlations in order to guide the placement of checkpoints.

Figure 8 compares the percentage of checkpoints which were  X  X seable X  over the period. The calculation is done as follows: for each key group on an overloaded node, con-sider if the corresponding checkpoint could be used for state-migration, without introducing overload at the node host-ing the checkpoint. The percentage of  X  X seable X  checkpoints are important, because it allows the adaptation to make better decisions, i.e. there are more potential places where checkpoint-assisted state-migration can be done.
The figure shows that the percentage of useable check-points for baseline and even are fairly independent on the number of key groups, which is reasonable, as a random al-location like baseline and even, should not benefit from an increased number of key groups in percent. The optimized technique on the contrary benefits from an increased num-ber of key groups, because this increases the possibility of using correlations to guide placing of checkpoints. Our pro-posed checkpoint allocation technique outperforms the even and baseline approaches with a factor of 2  X  3.

Figure 9 compares the accumulated processing latencies incurred by all state-migration operations. For the consid-ered job, the latency of doing direct and checkpoint assisted state-migrations were fairly similar. The reason, is that the state-size of the sophisticated job is fairly small. Still, even in this case, our approach is beneficial.

The reason the saved latency increases with the number of key groups, is because using more key groups, leads to a larger number of state-migrations. This happens as the adaptation algorithm will be able to handle overload more finegrained.
In this paper, we have presented an approach to integrate passive fault-tolerance and dynamic resource management in a general MPSPE. The integration allows us to employ more efficient runtime state-migration strategies to achieve dynamic load-balancing and scaling. The techniques are im-plemented as a significant extension over Apache Storm X  X  static execution framework. The experiments shows, that our CPAL strategy would only incur negligible latency to the normal computation, while its completion time only de-pends on the size of the data that needs to be replayed. This is a very significant improvement over the traditional techniques, which usually exhibits much longer latencies.
Finally, we have proposed a new checkpoint allocation technique, and the experimental results show that it can significantly increase the opportunities to make use of our fast and more efficient state migration strategies, and hence minimize the latency incurred to the normal computations. [1] T. Akidau, A. Balikov, Bekiro  X glu, et al. Millwheel: [2] D. Alves, P. Bizarro, and P. Marques. Flood: elastic [3] M. Balazinska, H. Balakrishnan, S. R. Madden, and [4] R. Castro Fernandez, M. Migliavacca, E. Kalyvianaki, [5] B. Gedik, S. Schneider, M. Hirzel, and K.-L. Wu. [6] Y. Gu et al. An empirical study of high availability in [7] V. Gulisano, R. Jimenez-Peris, M. Patino-Martinez, [8] P. Hunt, M. Konar, F. P. Junqueira, and B. Reed. [9] J.-H. Hwang, M. Balazinska, A. Rasin, U. Cetintemel, [10] J. hyon Hwang, Y. Xing, and S. Zdonik. A [11] W. Lam, L. Liu, S. Prasad, A. Rajaraman, Z. Vacheri, [12] J. Li, K. Tufte, V. Shkapenyuk, V. Papadimos, [13] L. Neumeyer, B. Robbins, A. Nair, and A. Kesari. S4: [14] B. Satzger, W. Hummer, P. Leitner, and S. Dustdar. [15] S. Schneider, H. Andrade, B. Gedik, A. Biem, and [16] Z. Sebepou and K. Magoutis. Cec: Continuous [17] M. A. Shah, J. M. Hellerstein, and E. Brewer. Highly [18] M. A. Shah, J. M. Hellerstein, S. Chandrasekaran, and [19] A. Toshniwal, S. Taneja, A. Shukla, K. Ramasamy, [20] Y. Xing, S. Zdonik, and J.-H. Hwang. Dynamic load [21] M. Yagiura, S. Iwasaki, T. Ibaraki, and F. Glover. A [22] M. Zaharia, T. Das, H. Li, S. Shenker, and I. Stoica. [23] Y. Zhou, K. Aberer, and K.-L. Tan. Toward massive [24] Y. Zhou, B. C. Ooi, K. Tan, and J. Wu. Efficient
