 In crowdsourced relevance judging, each crowd worker typ-ically judges only a small number of examples, yielding a sparse and imbalanced set of judgments in which relatively few workers influence output consensus labels, particularly with simple consensus methods like majority voting. We show how probabilistic matrix factorization, a standard ap-proach in collaborative filtering, can be used to infer missing worker judgments such that all workers influence output la-bels. Given complete worker judgments inferred by PMF, we evaluate impact in unsupervised and supervised scenar-ios. In the supervised case, we consider both weighted voting and worker selection strategies based on worker accuracy. Experiments on crowd judgments from the 2010 TREC Rel-evance Feedback Track show promise of the PMF approach merits further investigation and analysis.
 I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Design, Experimentation, Performance Crowdsourcing, label aggregation, matrix Factorization
Crowdsourced relevance judging offers potential to reduce time, cost, and effort of relevance judging [1] and benefit from greater diversity of crowd judges. However, quality of judgments from non-workers continues to be a concern, motivating continuing work in quality assurance methods based on statistical label aggregation methods or greater attention to human factors. A common approach is to collect multiple, redundant judgments from workers and aggregate them via methods like majority voting (MV) or expectation maximization (EM) to produce consensus labels [4].
Because each crowd worker typically judges only a small number of examples, collected judgments are typically sparse and imbalanced, with relatively few workers influencing out-put consensus labels. MV is completely susceptible to this problem. EM addresses this indirectly: while only workers Figure 1: Crowdsourcing workers judgments (Left) are copied to a sparse worker-task matrix (Middle). Missing judgments are inferred via PMF (Right). labeling an example vote on it, global worker judgments are used to infer class priors and worker confusion matrices.
We propose to tackle this issue directly by adopting a col-laborative filtering approach, which routinely deals with the issue of each user rating only a small number of items (e.g., movies, books, etc.) vs. the complete set. In particular, we employ probabilistic matrix factorization (PMF), which induces a latent feature vector for each worker and exam-ple [6] in order to infers unobserved worker judgments for all examples. Figure 1 depicts our approach graphically. We are not familiar with any prior work investigating PMF, or collaborative filtering approaches more generally, toward crowdsourcing quality assurance. Related prior work has investigated other ways to infer bias corrected labels in place of raw labels [4], as well as inference of missing labels by estimating a unique classifier for each worker [3].
Probabilistic Matrix Factorization (PMF). Suppose we have M tasks (examples to be labeled), N workers, and a label matrix R in which R ij indicates the label of worker i for task j . Let U  X  R D  X  M and V  X  R D  X  N be latent fea-ture matrices for workers and tasks, with column vectors U and V j representing D-dimensional worker-specific and task-specific latent feature vectors, respectively. The conditional probability distribution over the observed labels R  X  R N  X  M is given by Equation 1. Indicator I ij equals 1 iff worker i la-beled task j . We place zero-mean spherical Gaussian priors on worker and task feature vectors (Equations 2 and 3). To estimate model parameters, we maximize the log-posterior over task and worker features with fixed hyper-parameters. Maximizing the posterior with respect to U and V is equiv-alent to minimizing squared error with L2 regularization: 1 2
X where  X  U =  X  U / X , X  V =  X  V / X  , and k k 2 F denotes the Frobe-nius Norm. We use gradient descent to find a local mini-mum of the objective for U and V . Finally, we infer missing worker judgments in the worker-task matrix R by taking the scalar product of U and V . Note that as in [4], we also replace actual labels with bias-corrected inferred labels.
Label Aggregation. Given the complete set of inferred worker relevance judgments in matrix R , we next aggregate worker judgments to induce consensus labels. We consider both unsupervised supervised scenarios. In the former, we consider majority voting with raw (sparse) labels (Method 1), expectation maximization with raw labels (Method 2), and PMF-based MV (Method 3). In the supervised case, we measure each worker X  X  accuracy based on expert judgments, with labels of anti-correlated workers flipped such that ac-curacy is always  X  50%. We use supervision in two distinct ways: weighted voting (WV) and worker filtering, in which only workers with accuracy  X   X  participate in voting.
Experiments are performed on crowd judgments collected in the 2010 TREC Relevance Feedback Track [2] from Ama-zon Mechanical Turk. 762 crowd workers judged 19033 query-document tasks (examples), and 89624 judgments were col-lected. Our worker-task matrix thus has 762 columns (work-ers) and 19,033 rows (tasks); only 89,624 out of 14,503,146 labels (0.6%) are observed, so data is extremely sparse. 3,275 expert relevance judgments by NIST are partitioned into training (2,275) and test (1,000) sets. The test set is evenly-balanced between relevant and non-relevant classes.
Parameters . For dimensionality of task and worker la-tent feature vectors, we consider D  X  10 , 30 , 50 and select D = 30 based on cross-validation on the entire set of labels (unsupervised). We similarly tune regularization parameter worker filtering threshold  X   X  [0 . 6 , 0 . 99] by cross-validation on the training set using a linear sweep with step-size 0.01. Metrics and Results. Table 1 reports accuracy (ACC), RMSE, and specificity achieved by each method.

Unsupervised Methods . Method 2 of PMF with ma-jority voting (MV) outperforms the MV baseline (Method 1) and performs equivalently to EM (Method 2).

Supervised vs. Unsupervised Methods . While su-pervised methods tend to dominate, unsupervised EM and PMF both match performance of the supervised weighted voting (WV) method without filtering or PMF (Method 4).
Supervised Methods . Worker filtering is clearly seen to provide the greatest benefit, and surprisingly performs bet-ter without PMF than with PMF (Methods 6 vs. 7). When filtering is used, use of WV is not seen to further improve performance (Methods 5 vs. 6). We do see PMF-based mod-eling outperform non-PMF modeling when worker filtering is not employed (Methods 7 vs. 4).
While unsupervised consensus labeling accuracy with PMF only matched EM performance, PMF is advantageous in that once complete worker judgments are inferred, they might be used for a variety of other purposes, such as better rout-ing or recommending appropriate tasks to workers.
Intuitively, an accurate worker X  X  empirical label distribu-tion should resemble the actual class prior. This suggests an alternative, more weakly supervised scenario to consider in which class priors are known while example labels are not. In the unsupervised case, we might instead simply examine the distribution of empirical priors for each worker and de-tect outliers [5]. In future work, we plan to investigate these ideas further in combination with those described here. [1] O. Alonso, D. Rose, and B. Stewart. Crowdsourcing for [2] C. Buckley, M. Lease, and M. D. Smucker. Overview of [3] S. Chen, J. Zhang, G. Chen, and C. Zhang. What if the [4] P. Ipeirotis, F. Provost, and J. Wang. Quality [5] H. J. Jung and M. Lease. Improving Consensus [6] R. Salakhutdinov and et al. Probabilistic matrix
