 In games, the challenge of ascertaining one player or team X  X  advantage over their opponents continues to be an open r esearch problem. In particular, the rise of online multi-player games has put the task of skill assessment front and center for game developers, wherein the long-term success or failure of a title is linked, in part, to the ability of players to find similarly-skilled teammates and opponents to play against.  X  X atchmaking X , an automated process used to match players together for an online game, depends on accurate estimations of player skill at all times in order to reduce the likelihood of imbalanced matches. If one player or team is far superior to their opposition, the resulting game can frustrate less-skilled players and potentially lead to customer churn.
For games which focus on the online multi-player experience, including pop-ular titles such as Halo, Call of Duty, and StarCraft 2, the task of appropriately matching up millions of players and teams of roughly equal skill is crucial -and daunting. With such large player populations, batch learning methods become impractical, neccesitating an online sk ill assessment process in which adjust-ments to a player X  X  skill rating happen one game at a time, depending only on their existing rating and the outcome of the game. This task is made more diffi-cult in titles centered around team-based competition, where interaction effects between teammates can be difficult to model and integrate into the assessment process.

Our work is concerned with this particular variant of the skill estimation prob-lem. Although many approaches exist for skill estimation, such as the well-known Elo rating system [1] and the Glicko rating system [2], [3], they were primarily designed for one versus one competition settings (in games such as Chess or tennis) instead of team-based play. They can be altered to a ccomodate competi-tions involving teams, but, problematically, assume the performances of players in teams are independent from one another, thereby excluding potentially useful information regarding a team X  X  collect ive  X  X hemistry X . More recent approaches [4] have explicitly modeled teams, but still assume player independence within teams, summing individual player ratings to produce an overall team rating.  X  X eam chemistry X  is a widely-held notio n in team sports [5] and is often cited as a key differentiating factor, particula rly at the highest levels of competition. In the context of skill assessment in an online setting, however, less attention has been given to situations in which team chemistry would be expected to play a significant role, such as the case where the player population is highly-skilled individually, instead using data from a general population of players for evaluation [4].

Our previous work in this area [6] described several methods for capturing elements of  X  X eam chemistry X  in the a ssessment process by maintaining skill ratings for subsets of teams as well as individuals, aggregating these ratings together for an overall team skill rating. One of the methods, TeamSkill-AllK-EV (hereafter referred to as EV), performed especially well in our evaluation. One drawback of EV, however, was that it weighted each aggregate n-sized subgroup skill rating uniformly in the final summation, leaving open the possibility that further improvements might be made through an adaptive weighting process.
In this paper, we build on our previous work by introducing five algorithms which address this drawback in various ways, TeamSkill-AllK-Ev-OL1 (OL1), TeamSkill-AllK-Ev-OL2 (OL2), TeamSkill-AllK-Ev-OL3 (OL3), TeamSkill-AllK-EVGen (EVGen), and TeamSkill-AllK-EVMixed (EVMixed). The first three -OL1, OL2, and OL3 -employ adaptive weighting frameworks to adjust the summation weights for each n-sized group skill rating and limit their feature set to data common across all team games: the players, team assignments, and the outcome of the game. For EVGen an d EVMixed, however, we explore the use of EV X  X  final prediction, the label of the winning team, as a feature to be included along with a set of game-specific pe rformance metrics in a variety of on-line classification settings [7], [8], [9]. For EVMixed, a threshold based on EV X  X  prior probability of one team defeating another is used to determine whether or not to include the metrics as features and, if not, the algorithm defers to EV X  X  predicted label. EVGen, in contrast, always includes the metrics during classification.

Evaluation is carried out on a carefully-compiled dataset consisting of tourna-ment and scrimmage games between professional Halo 3 teams over the course of two years. Halo 3 is a first-person shooter (FPS) game which was played competitively in Major League Gaming (MLG), the largest professional video game league in the world, from 2008 through 2010. With MLG tournaments regularly featuring 250+ Halo teams vying for top placings, heavy emphasis is placed on teamwork, making this dataset ideal for the evaluation of interaction effects among teammates.

We find that EVMixed outperforms all other approaches in most cases, often by a significant margin. It performs particularly well in cases of limited game history and in  X  X lose X  games where teams are almost evenly-matched. These re-sults suggest that while game-specific features can play a role in skill assessment, their utility is limited to contexts in which the skill ratings of teams are similar. When they are not, the inclusion of gam e-specific information effectively adds noise to the dataset since their values aren X  X  conditioned on the strength of their opponents.

The outline of this paper follows. Section 2 briefly describes some of the work related to the problem of skill assessment. In Section 3, we introduce our pro-posed approaches -OL1, OL2, OL3, EVGen, and EVMixed. In Section 4, we describe some of the key features of the dataset, our evaluation testbed, and share the results of our evaluation in terms of game outcome prediction accu-racy. We then conclude with Section 5, discussing the results and future work. The foundations of modern skill assessment approaches date back to the work of Louis Leon Thurstone [10] who, in 1927, proposed the  X  X aw of comparitive judgement X , a method by which the mean distance between two physical stimuli, such as perceived loudness, can be computed in terms of the standard deviation when the stimuli processes are normally-distributed. In 1952, Bradley-Terry-Luce (BTL) models [11] introduced a logistic variant of Thurstone X  X  model, using taste preference measurements for evaluation. This work in turn led to the creation of the Elo rating system, introduced by Arpad Elo in 1959 [1], a professor and master chess player who sought to replace the US Chess Feder-ation X  X  Harkness rating system with one more theoretically sound. Similar to Thurstone, the Elo rating system assumes the process underlying each player X  X  skill is normally-distributed with a constant skill variance parameter  X  2 across all players, simplifying skill updates after each game.

However, this simplification was also Elo X  X  biggest drawback since the  X  X elia-bilty X  of a rating was unknown from player to player. To address this, the Glicko rating system [3], a Bayesian approach introduced in 1993 by Mark Glickman, allowed for player-specific skill varian ce, making it possible to determine the confidence in a player X  X  rating over time and produce more conservative skill estimates.

With the release of the online gaming service Xbox Live in 2002, whose player population quickly grew into the millions, there was a need for a more generalized rating system incorporating the notion of teams as well as individual players. TrueSkill [4], published in 2006 by Ralf Herbrich and Thore Graepel, used a factor graph-based approach to meet this need. In TrueSkill, skill variance is also maintained for each player, but in contrast to Glicko, TrueSkill samples an expected performance given a player X  X  skill rating which is then summed over all members of a team to produce an estimate of the collective skill of a team. Be-cause the summation is over individual players, player performances are assumed to be independent from one another, leaving out potentially useful group-level interaction information. For team-based games in which highly-skilled players may coordinate their strategies, this lost interaction information can make the estimation of a team X  X  advantage over another difficult, especially as players change teams.

Several other variants of the aforementioned approaches have also been in-troduced, including BTL models [12], [13], [14] and expectation propagation techniques for the analysis of paired comparison data [15]. In our previous work [6], we sought to explicitly model group-level interac-tion effects during the skill assessment process, introducing four methods which took varying approaches to addressing this issue -TeamSkill-K, TeamSkill-AllK, TeamSkill-AllK-EV, and TeamSkill-AllK-LS. These approaches had in common the idea that ratings themselves need not be limited to individual players, but subsets of teams as well. Here, we modified the Elo, Glicko, and TrueSkill rating systems to be used as generic learners which maintained skill ratings for groups of players. In doing so, both group and player-level skill could be captured, pro-ducing a clearer picture of a team X  X  collective skill. The key differences between these approaches was the amount of subgroup rating information used and the ways in which aggregate group skill ratings were weighted during the summation to produce a team X  X  skill rating.

One of the approaches, EV, performed especially well during evaluation, im-proving on the unaltered versions of Glicko and TrueSkill, and, in most test cases, the other TeamSkill approaches as well. The main idea behind EV is to use all available group-level history, from groups of size k = 1 (individual play-ers) to k = K (the size of the team), and sum together the expected skill rating corresponding to each set of k -sized group ratings, weighting each uniformly: In this notation, s  X  i is the estimated skill of team i and h i ( k ) is a function returning the set of skill ratings for player groups of size k in team i , including the empty set  X  if none exist. When h i ( k )  X  X  X  ,welet E [ h i ( k )] = 0.
Despite its excellent results, EV is a  X  X aive X  approach, lacking a means of updating the summation weights, potentially leading to suboptimal performance. To that end, we introduce three adaptive frameworks which allow the summation weights to vary over time -TeamSkill-AllK-Ev-OL1 (OL1), TeamSkill-AllK-Ev-OL2 (OL2), and TeamSkill-AllK-Ev-OL3 (OL3). 3.1 TeamSkill-AllK-Ev-OL1 When attempting to construct an overall team skill rating, one key challenge to overcome is the fact that the amount of group history can vary over time. Consider figure 1: after the first game is played, history is available for all possible groups of players. Later, player 4 leaves the team and is replaced by player 5, who has never played with players 1, 2, or 3, leaving only a subset of history available and none for the team as a whole. Then in the final step, player 2 leaves and is replaced by player 6, who has played with player 3 and 5 before, but never both on the same team, resulting in yet another variant of the team X  X  collective group-level history. The feature space is constantly expanding and contracting over time, making it difficult to know how best to combine the group-level ratings together. In OL1, we address this issue by maintaining a weight w k for each aggregate group skill rating of size k , contracting w during summation by uniformly redistributing the weights from indicies in the weight vector not present in the available aggregate group skill rating history. Given the winning team i , w k is updated by computing to what extent each of the aggregate rating X  X  pri or probability of team i defeating some team j according to TeamSkill-K [6], P k ( i&gt;j ), is better than random, increasing the weight of w k for a correctly-predicted outcome.
 1  X   X   X  X  X  ,w 0 k = The main drawback of this approach is that the weight for k = 1 eventually dominates the weight vector as it is the element of group history present in every game and, therefore, the weight mos t frequently increased relative to the weights of k&gt; 1. Given enough game history, this classifier will converge to exactly k = 1 -the classifier corresponding the an unmodified version of the general learner (Elo, Glicko, or TrueSkill) it employs. 3.2 TeamSkill-AllK-Ev-OL2 OL2 attempts to remedy this by maintaining a weight matrix corresponding to the lower triangular of a K x K grid, or one weight vector w for each of the K possible summation situations given a team X  X  group-level game history. This ameliorates the issue of the k = 1 weight increasing faster relative to the weights of k&gt; 1 since each row in the K x K grid pertains to a situation where the length of the non-zero row elements equals K (as defined previously). 3.3 TeamSkill-AllK-Ev-OL3 OL3 works similarly to OL1 in most respects, but instead uses a predefined window of the d most recent games in which k -sized group history was available to compute its updates. In this way, the weights  X  X ollow X  the most confidently-correct aggregate skill ratings for each window d . In the following, let L d,k be the number of games in the window d in which, for some k , TeamSkill-K incorrectly predicted the outcome of a game. 3.4 Using Game-Specific Data during Classification OL1, OL2, and OL3 -like the other TeamSkill approaches -only use data avail-able in all team-based games, namely the players, their team associations, and game outcome history. One natural question to ask is how well could we do if we included game-specific data during the step in which the label of the win-ning team is predicted. Though not ideal from a general implementation per-spective, it is reasonable to assume that a carefully-chosen set of game-specific performance metrics might help produc e a more accurate pr ediction. Here, we introduce two such methods -TeamSkill-AllK-EVGen (EVGen) and TeamSkill-AllK-EVMixed (EVMixed). 3.5 TeamSkill-AllK-EVGen In EVGen, we create a feature set x t from a combination of EV X  X  predicted label { +1 ,  X  1 } of the winning team,  X  EV t ,andasetof n game-specific metrics m . For Halo 3, several logical metrics are available, such as kill/death ratio and assist/death ratio (an assist is given to a player when they do more than half of the damage to a player who is eventually killed by another player), and act as rough measures of a team X  X  in-game efficiency since players respawn after each death throughout the duration of a game. After compiling these metrics for each team, we take the difference between them for use in x t , adding in  X  EV t as the final feature. EV was chosen because of its superior performance in previous evaluations [6] as well as results from preliminary testing for this work, drawing from the pool of all previous approaches (including OL1, OL2, and OL3). Having constructed the feature set x t , we use a more traditional online clas-sification framework to predict the label of the winning team  X  y t , such as the perceptron [7], online Passive-Aggressive algorithms [8], or Confidence-Weighted learning [9] (Note: substitute  X  t for w t in the latter): After classification, the weight vector over the feature set is then updated according to the chosen learning framework.
 3.6 TeamSkill-AllK-EVMixed EVMixed introduces a slight variant to E VGen X  X  overall strategy by selecting a classification approach based on whether or not both teams are considered relatively evenly-matched (that is, if a team X  X  prior probability of winning ac-cording to EV, P t EV ( i&gt;j ), is close to 0.5). Here, if the prior probability of one team winning is within some of 0.5, we use the EVGen model for prediction. Otherwise we simply use EV X  X  label. The approach is simple, as is the intuition behind it: if EV is sufficiently confident in its predicted label, then there is no need for additional feature information. 4.1 Dataset We evaluate our proposed approaches using a dataset of 7,568 Halo 3 multiplayer games between professional teams. Each was played over the Internet on Mi-crosoft X  X  Xbox Live service in custom games (known as scrimmages) or on a local area network at an MLG tournament and includes information such as the play-ers and teams competing, the date of the game, the map and game type, the result (win/loss) and score, and per-player statistics such as kills, deaths, and assists.
Characteristics unique to this dataset make it ideal for our evaluation pur-poses. First, it is common for players to change teams between tournaments, each of which is held roughly every 1-2 months, thereby allowing us to study the effects of  X  X eam chemistry X  on performance without the assumption of degraded individual skill. Second, because every player is competing at such a high level, their individual skill isn X  X  considered as important a factor in winning or losing a game as their ability to work together as a team. 4.2 Overall Results The prediction accuracy of OL1, OL2, OL3, EVGen, and EVMixed were evalu-ated using a number of different subsets of the Halo 3 dataset:  X  Games played in tournaments only, scrimmage games only, and both tour- X  All of the games, or just those games considered  X  X lose X  (i.e., prior proba-For comparison, we include results from the previous TeamSkill approaches as well. To compute the prior probability of t 1 defeating t 2 , we use the negative CDF evaluated at 0 for the distribution c orresponding to the difference between two independent, normally-distributed random variables (as in [6]). Games were labeled as  X  X lose X  using a variant of the  X  X hallenge X  method [4] in which the top 20% closest games for one rating system ar e identified and presented to the other. Because we are interested in performance beyond that of unmodified general learners (i.e., k = 1), the closest games from k = 1 were presented to the other TeamSkill approaches while EV X  X  closest games were presented to k = 1 (due to its evaluated performance in [6]). The following defaults were used for Elo (  X  = 0.07,  X  = 193.4364,  X  0 = 1500,  X  2 0 =  X  2 ), Glicko ( q = log (10) / 400,  X  0 = 1500,  X  0 = 100 to [4] and [3]. For OL1/OL2,  X  =1 . 1, OL3, d = 20. For EVGen/EVMixed ( =0 . 03), the Passive-Aggressive II algorithm [8] was used for classification (  X  = 0 . 1, C =0 . 001,  X  =0 . 9). The final feature set was comprised of cumulative and windowed (10 games of history) versions of team differences in average team and player-level kill/death ratio, assist/d eath ratio, kills/game, and assists/game.
From the results in table 1, it is clear that EVMixed performs the best overall, and in the widest array of evaluation conditions. It has the best performance in 10 of the 18 test cases and 16 of 18 in which it was at least second best, a testament to its consistency. EVGen X  X  overall performance, however, is roughly 7-10% lower on average over all games, e xceeding EVMixed X  X  results only in 3 of the  X  X lose X  game test cases. 4.3 Results over Time Next we explore how these approaches perform over time by predicting the outcomes of games occuring prior to 10 tournaments which took place during 2008 and 2009, using tournament data only in order to isolate conditions in which we expect teamwork to be strongest. From figures 2 and 3, EVMixed X  X  superior peformance is readily apparent. Of particular note, however, is how well EVMixed does when little history is available, having a roughly 64% accuracy just prior to the first tournament for all three learner cases. For close games, both EVGen and EVMixed show strong results, eventually tapering off and approaching the other competing methods as more game history is observed. 4.4 Online Classification Variants For EVGen and EVMixed, we investigated a number of different online classi-fication frameworks -the perceptron [7], Passive-Aggressive algorithms [8], and Confidence-Weighted learning [9] -and evaluated them using a subset of the testbed from section 4.2. The results are shown in table 2. Though similar, the PA-II ap-proach appears to be the most consistent overall (with CW-diag not far behind). In sum, the results show EVMixed consistently outperforming competing ap-proaches in a multitude of scenarios, often by great margins. Initially, we found the subpar performance of EVGen somewhat surprising given that the only dif-ference between it and EVMixed is the classifier choice according to a given . Upon closer examination, the reason fo r this discrepency becomes clear: the game-specific data used to supplement the feature set was not weighted accord-ing to the strength of their opposition in each game, effectively adding  X  X oise X  in cases where the games were not considered close. Only the skill rating is a function of opposition skill, and as such, when the ratings of two teams are suffi-ciently divergent, the additional features are not necessary, nor desired. It follows that this is also the reason why both EVGen and EVMixed perform well in close games. Here, because the difference in skill ratings is small, the supplemental feature information tells us something about how two otherwise evenly-matched teams might perform if they competed. This is also why EVGen and EVMixed have excellent results when little game history has been observed -nearly all games are considered  X  X lose X  early in the rating process.

Turning our attention back to OL1, OL2, and OL3, it X  X  clear that little im-provement was made relative to EV X  X  results for any of these approaches. In fact, while the weights for OL1 even tually converge to the classifier k =1,OL2 X  X  weights largely mimic EV X  X , suggesting there are more subtle group-level dy-namics we need to pay attention to as this would only arise if the classifiers corresponding to 1  X  k  X  K have somewhat similar ratings. OL3 also produces results similar to EV (even moreso than OL2), adding to the previous observa-tion. While the results for OL1, OL2, and OL3 are unfortunate, the naive means by which EV weights each of the aggregated group-level skill ratings leaves the door open for improvement.

Our future work takes two directions. The first is to more fully explore what can be done to enhance the EVMixed model, perhaps by introducing a mecha-nism by which canvaryovertimeorweightingpl ayer performances in-game by the strength of their opponents. The second is to derive an adaptive weighting framework which does improve on EV X  X  results significantly, and then integrate it into EVGen and EVMixed. In this paper, we extended our previous work by introducing three methods in which various strategies are used to maintain a set of weights over aggre-gate group-level skill rating information. Additionally, we explored the utility of incorporating game-specific data as features during the prediction process, describing two such approaches: EVGen and EVMixed. EVMixed outperformed all previous efforts in the vast majority of cases, leading to the conclusion that game-specific data is best included when teams are relatively evenly-matched, and disregarded otherwise.
 Acknowledgments. We would like to thank members of the Data Mining Research Group for their feedback and suggestions. We would also like to thank Major League Gaming for making their 2008-2009 tournament data available.
