 In this paper, we provide an overview of the MSR-Bing Web Scale Speller Challenge of 2011. We describe the motivation and outline the algorithmic and engineering challenges posed by this activity. The design and the evalua tion methods are also reviewed, and the online resources that will remain publicly available to the community are also described. The Challenge will culminate in a workshop after the time of the writing where the top prize winners will publish their approaches. The main findings and the lessons learned will be summarized and shared in the Industry Track presentation accompanying this paper. I.2.7 [ Artificial Intelligence ]: Natural Language Processing  X  Language models. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  query formulation Algorithm, Measurement, Experimentation. Web search, Language model, Spelling Alteration, MSR-Bing Speller Challenge. As a follow-up to the invited talk in SIGIR 2010 on the critical role of query understanding for web search [2], Microsoft Research (MSR) and Bing jointly announced a public contest in December 2010 [3] to promote the algorithmic and engineering research in this area. The focus for the MSR-Bing Web Scale Speller Challenge is on an early stage of query understanding: spelling alteration. Spelling alteration has traditionally been studied in the field of natural language processing (NLP) [1]. As the web has dramatically democratized the cont ent publishing business, it has been noted [4] that traditional NLP techniques that excel at handling edito rial quality contents are no longer sufficient for processing the web contents. Specifically for web search, one often observes that search querie s sometimes consist of segments from different languages that are difficult to detect using the conventional language identification techniques, and the term usages and the lexicon for th e web are rapidly growing and changing over time. Popularized by the short message services and the conflated domain name s commonly seen for online marketing, many terms are a mixture of alphabetical of numeral characters that do not follow conventional word boundaries, and even individual terms are well constructed the search queries are often expressed in a  X  X idgin X  language that does not abide by normal linguistic rules. The notion of a  X  X orrect X  spelling can vary with contexts, and it is not unusua l that some relevant documents can themselves contain spelling errors that an overzealous speller in the query processing stage would prevent these documents from being retrieved and ranked high. In addition to these algorithmic ch allenges, applications for web search typically need to process a large amount of data and complete the NLP task in a split second. Engineering quality plays as critical a role as algorithmic excellency. As such, a peculiar setting of the Speller Challenge is to require the participants to implement their entries as publicly accessible web services with a REST interface defined in [3], based on which the engineering quality, such as reliability and latency, can also be evaluated by the on-demand, web-based tools de veloped by MSR (Sec. 2). An intended side effect is the participating teams can easily replicate the results and learn from the submitted spellers among one another. The contest rules allow and encourage teams to pool their resources online and utilize social networks or other collaborative means to improve their spellers for a period of five months with a final entry deadline at noon on May 31, 2010. After the Challenge results are finalized, all the web-based tools and data resources will be reopened to the public so that future researchers can benchmark their systems against th e participating entries at high fidelity with minimum effort. The objective of spelling alteration is to produce plausible alternatives for a given query. As laid out in [2], the problem can be formulated as the noisy channel problem in information theory where the optimal solution is known to be where h and q denote a hypothesized alternative and the received queries, respectively. A realization of (1) is a spelling alternation task when the hypothesized alternatives are limited to lexical manipulations on the received quer y and no semantic revisions are considered. In practice, spelling alteration is merely a subcomponent of the query understanding process where multiple spelling alternatives are often desirable and the re spective posterior probabilities  X   X   X  |  X   X  serve as an invaluable cue for the optimization of the overall query processing. The Challenge therefore requires the submissions to not only produce a list of alternatives for each query but also estimate their posterior probabilities. To take into account the posteri or probabilities, the Challenge modified slightly the conventional definition of F1 measure for evaluating the the algorithmic performance of a speller. First, the Expected Precision ( EP ) is defined as where  X . X  X  is the Boolean indicator function,  X  is the full query speller and the set of the desirable alterations for query  X  X  X  , respectively. The design is to favor spellers that do not over-generate alteration hypotheses because the total posterior probability must be distribut ed among all the hypotheses and summed up to 1. The corresponding Expected Recall ( ER ) is defined as For this Challenge, we only use manually annotated data that does not estimate the relative importance of each plausible alteration proper distribution can be estimated for  X   X   X  |  X   X  either through the search behavioral data or cr oss-validations among a diverse collection of multiple spellers. Finally, the harmonic mean of EP and ER , called the Expected F1 ( EF1 ), is used as the main metric for the Challenge. MSR maintains a web site [3] to accept the URI of a speller for evaluation. The evaluator sets a 60 second timeout for the speller to respond to each query, and treats the response as  X   X   X   X   X  X  if an HTTP 500 level error or timeout expiration is encountered. In addition to EF1 , the average latency for each query is also computed as the potential tie-breaker for the Challenge. The Challenge uses search queries received by Bing in March 2010 as the final test set  X  with |  X  |  X 1500 . The queries are uniformly sampled from the tail s ection of the query log so that the uniform frequency assumptions intrinsically in both (2) and (3) are valid. The queries are manually annotated with spelling alterations, and are tokenized in th e same fashion as described in [5]. After the award contest phase of the Challenge is completed, this dataset will continue to be used by the evaluator (Sec. 2.1) for researchers to benchmark future spelling alteration systems. Three datasets are made available for the purpose of developing spelling alteration systems. Firs t, the query set from TREC 2008 Million Query Track (1MQ) is annotated with spelling alterations using the same guidelines as th e Bing test set and is made available for download [3]. The TREC dataset, with a size of more than 5500 queries, consists of search queries seeking public information published by US gove rnment websites. Roughly 10% of the queries contain typographical errors and, although the queries are collected in the EN-US region, the dataset contains a subset of queries in ES-US Sp anish. These non-English queries are deliberately preserved beca use having rudimentary multi-lingual capability is a necessity fo r web search query processing. The frequencies of having non-English or spelling errors in TREC query set, however, are lower than the Bing dataset for the final evaluation of the Challenge. The second dataset is the Microsoft Web N-gram service [5], available exclusively in the web service format. The dataset includes multiple statistical language models created from text resources on the web, including queries received by Bing over a 10 month period. In addition, the dataset includes language models built from the body, title, and the anchor text of the web documents indexed by Bing. Two snapshots from the Bing index are available: one from June 30, 2009 and April 30, 2010, respectively. These language models are provided as a reference resource for the speller developers to estimate the contextual probabilities of the alterations. As misspelling is common in the web documents and the cut-off is set low in processing these snapshots, the web N-gram dataset appears to be a valuable resour ce to discover common patterns of misspelling in a large scale. To that end, the Challenge organizer made available a service to identify tokens appearing in the similar N-gram context that can be used as a supplement material to the TREC data in developing a speller. At the time of writing, the prize competition phase of the Challenge is still ongoing and th ere are 339 teams producing over 400 prototypes vying for the top 5 cash reward positions. The Challenge will consummate in July, 2011 when a workshop is held for the Challenge participants to discuss their work and exchange experience. The papers and discussion will be published in an electronic workshop proceedings, with summary shared in the SIGIR-2011 industry track presentation. The dataset and the evaluation tools will continue to be available as web services at the Challenge web site [3]. It is the hope that researchers can utilize these re sources to benchmark future improvements against the competition results. The authors would like to thank Mr. Christopher Thrasher, Nikolas Gloy and Amit Koul for their technical assistance in maintaining the online infrastructure for the Challenge. Drs. Bo-Jun  X  X aul X  Hsu, Evelyne Viegas and Jianfeng Gao contributed to the design of the Challenge. [1] Gao, J., Li, X., Micol, D., Quirk, C., Sun, X. 2010. A large [2] Pedersen, J. 2010. Query Understanding at Bing. Invited talk [3] Speller Challenge, http://www.sp ellerchallenge.com, also at [4] Wang, K., Thrasher, C., Hsu, B.-J. 2011. Web scale NLP: A [5] Wang, K., Thrasher, C., Viegas , E., Li, X., Hsu, B.-J. 2010. 
