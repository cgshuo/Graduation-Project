 Kaizhu Huang kzhuang@cse.cuhk.edu.hk Haiqin Yang hqyang@cse.cuhk.edu.hk Irwin King king@cse.cuhk.edu.hk MichaelR.Lyu lyu@cse.cuhk.edu.hk Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong Recently, learning large margin classifiers (Smola et al., 2000) has become an active research topic. Sup-port Vector Machine (SVM) (Vapnik, 2000), the most famous one of them, achieves a great success in ma-chine learning and pattern recognition. SVM aims to find a hyperplane, which can separate two classes of data with the maximal margin. However, this mar-gin is defined in a  X  X ocal X  way, i.e., the margin is exclusively determined by some critical points, which are called support vectors, whereas all other points are totally irrelevant to the decision hyperplane. Al-though this scheme is both theoretically and empiri-cally demonstrated to be powerful, it actually discards the global information of data. An illustration exam-ple can be seen in Figure 1. In this figure, the classi-fication boundary is intuitively observed to be mainly determined by the dotted axis, i.e., the long axis of the y data (represented by  X  X ) or the short axis of the x data (represented by o X  X ). Moreover, along this axis, the y data are more possible to scatter than the x data, since y contains a relatively larger variance in this direction. Noting this  X  X lobal X  fact, a good deci-sion hyperplane seems reasonable to lie closer to the x side (see the dash-dot line). However, SVM ignores this kind of  X  X lobal X  information, i.e., the statistical trend of data occurrence: the derived SVM decision hyperplane (the solid line) lies unbiasedly right in the middle of two  X  X ocal X  points (the support vectors). Motivated from this important observation, we pro-pose Maxi-Min Margin Machine (M 4 ) to simultane-ously consider data in both a local and a global fash-ion. Interestingly, as we show later, M 4 is actu-ally a unified model of SVM and another recently-proposed promising model Minimax Probability Ma-chine (MPM) (Lanckriet et al., 2002). Moreover, based on our proposed local and global view of data, an-other popular model, Linear Discriminant Analysis (LDA) (Fukunaga, 1990) can easily be interpreted and extended as well. Another critical feature of M 4 is that, it can be cast as a sequential conic program-ming problem (Pruessner, 2003), or more specifically, a sequential Second Order Cone Programming (SOCP) problem (Lobo et al., 1998), which can be solved effi-ciently.
 This paper is organized as follows. In the next sec-tion, we introduce the M 4 model in detail, including its model definition, the geometrical interpretation, con-nections with other models, and the associated solving methods. Following that, we evaluate this novel model in Section 4. Finally, we conclude this paper in Sec-tion 5. In the following, we first introduce the notations used in this paper. We then, for the purpose of clarity, divide M 4 into separable and nonseparable categories, and introduce the corresponding models sequently. 2.1. Notations We only consider two-category classification tasks. As-suming a training data set contains two classes of sam-ples, represented by x i  X  R n and y j  X  R n respectively, where i =1 , 2 ,... ,N x , j =1 , 2 ,... ,N y . The basic task here can be informally described to find a suit-able hyperplane f ( z )= w T z + b separating two classes of data as robustly as possible ( w  X  R n \{ 0 } , b  X  R , and w T is the transpose of w ). Future data points z for which f ( z )  X  0 are then classified as the class x ; otherwise, they are classified as the class y . 2.2. Separable Case Assuming the classification samples are separable, we first introduce the model definition and the geometri-cal interpretation. We then transform the model op-timization problem into a sequential SOCP problem and discuss the detailed optimization method. 2.2.1. Model Definition The formulation for M 4 can be written as: where  X  x and  X  y refer to the covariance matrices of the x and the y data, respectively.
 This model tries to maximize the margin defined as the minimum Mahalanobis distance for all training sam-ples, 1 while simultaneously classifying all the data cor-rectly. Compared to SVM, M 4 incorporates the data information in a global way; namely, the covariance information of data or the statistical trend of data oc-currence is considered, while SVMs, including l 1 -SVM, l -SVM, and l  X  -SVM ( l p -SVM means the  X  p -norm X  distance-based SVM) (Smola et al., 2000), simply dis-card this information or consider the same covariance for each class.
 A geometrical interpretation of M 4 can be seen in Fig-ure 2. In this figure, the x data are represented by the inner ellipsoid on the left side with its center as x , while the y data are represented by the inner el-lipsoid on the right side with its center as y 0 .Itis observed that these two ellipsoids contain unequal co-variances or risks of data occurrence. However, SVM does not consider this global information: its decision hyperplane (the dotted blue line) locates unbiasedly in the middle of two support vectors (filled points). In comparison, M 4 defines the margin as a Maxi-Min Ma-halanobis distance, which thus constructs a decision plane (the solid magenta line) with considerations of both the local and global information: the M 4 hyper-plane corresponds to the tangent line of two dashed ellipsoids centered at the support vectors (the local information) and shaped by the corresponding covari-ances (the global information). 2.2.2. Optimization Method We will in the following show how the above problem can be cast as a sequential conic programming prob-lem, or more specifically, a sequential SOCP problem. Our strategy is based on the  X  X ivide and Conquer X  technique. One may note that in the optimization problem of M 4 ,if  X  is fixed to a constant  X  n , the prob-lem is exactly changed to  X  X onquer X  the problem of checking whether the constraints of (2) and (3) can be satisfied. Moreover, as will be demonstrated shortly, this  X  X hecking X  procedure can be stated as an SOCP problem. Thus the problem now becomes how  X  is set, which we can use  X  X ivide X  to handle: if the con-straints are satisfied, we can increase  X  n accordingly; otherwise, we decrease  X  n .
 We detail this solving technique in the following two steps: 1. Divide: Set  X  n =(  X  0 +  X  m ) / 2, where  X  0 is a 2. Conquer: Call the Modified Second Order Cone In the above, if a  X  satisfies the constraints of (2) and (3), we call it a feasible  X  ; otherwise, we call it an infeasible  X  . These two steps are iterated until |  X  0  X   X  is less than a small positive value.
 The MSOCP procedure is introduced in the following. We reformulate the constraints of (2) and (3) as fol-lows: Our task here is to check whether there exist a w and a b satisfying the above two constraints, which are ob-viously the forms of the second order cones (here  X  n is a constant). Actually, many SOCP programs, e.g., Se-dumi (Sturm, 1999), provide schemes to directly han-dle the above checking procedure. However, to make it clear, we elaborate in the following how this checking problem can be transformed as an SOCP optimization problem.
 Introducing dummy variables  X  , we rewrite the above checking problem into an equivalent optimization problem: where i =1 ,... ,N x and j =1 ,... ,N y .
 By checking whether the minimum  X  k at the optimum point is positive, we can know whether the constraints of (2) and (3) can be satisfied.
 We can further introduce another dummy variable and transform the above problem into an SOCP problem: where i =1 ,... ,N x , j =1 ,... ,N y ,and k = 1 ,... ,N x + N y . By checking whether the optimal  X  is greater than 0, we can immediately know whether there exist a w and a b satisfying the constraints of (2) and (3). Moreover, the above optimization is eas-ily verified to be the standard SOCP form, since the optimization function is a linear form and the con-straints are either linear or the typical second order conic constraints.
 We now analyze the time complexity of M 4 .Asin-dicated in (Lobo et al., 1998), if the SOCP is solved based on interior-point methods, it contains a worst-case complexity of O ( n 3 ). If we denote the range of feasible  X   X  X  as L =  X  max  X   X  min and the required pre-cision as  X  , then the number of iterations for M 4 is log( L/ X  ) in the worst case. Adding the cost of form-ing the system matrix (constraint matrix), which is O (
Nn 3 )( N represents the number of training points), the total complexity would be O (log( L/ X  ) n 3 + Nn 3 )  X  O (
Nn 3 ), which is relatively large but can still be solved in polynomial time. 2 2.3. Connections with Other Models In this section, we establish connections between M 4 and other models. We show that SVM and MPM are actually special cases of our model. Moreover, LDA can be interpreted and extended according to our local and global views of data. 2.3.1. Connection with Minimax Probability If one expands the constraints of (2) and add all of them together, one can immediately obtain the follow-ing: where x denotes the mean of the x training data. Similarly, from (3) one can obtain: where y denotes the mean of the y training data. Adding (4) and (5), one can obtain: The above optimization is exactly the MPM optimiza-tion (Lanckriet et al., 2002). Note, however, that the above procedure cannot be reversed. This means the MPM is a special case of M 4 .
 Remarks : In MPM, since the decision is com-pletely determined by the global information, namely, the mean and covariance matrices (Lanckriet et al., 2002), 3 to assure an accurate performance, the esti-mates of mean and covariance matrices need to be re-liable. However, it cannot always be the case in real world tasks. On the other hand, M 4 seems to solve this problem in a natural way, because the impact caused by inaccurately estimated mean and covariance matri-ces can be neutralized by utilizing the local informa-tion, namely by satisfying those constraints of (2) and (3) for each local data point. This is also demonstrated in the later experiment. 2.3.2. Connection with Support Vector If one assumes  X  x = X  y =  X , the optimization of M 4 can be changed as: where i =1 ,... ,N x and j =1 ,... ,N y .
 Observing that the magnitude of w will not influence the optimization, without loss of generality, one can further assume  X  mization can be changed as: where i =1 ,... ,N x and j =1 ,... ,N y . A special case of the above with  X  = I is precisely the optimization of SVM, where I is the identity matrix. Remarks: In the above, two assumptions are implic-itly made by SVM: One is the assumption on data  X  X rientation X  or data shape, i.e.,  X  x = X  y = X ,and the other is the assumption on data  X  X cattering magni-tude X  or data compactness, i.e.,  X  = I . However, these two assumptions are inappropriate. We demonstrate this in Figure 3(a) and Figure 3(b). We assume the orientation and the magnitude of each ellipsoid repre-sent the data shape and compactness, respectively, in these figures.
 Figure 3(a) plots two types of data with the same data orientations but different data scattering magnitudes. It is obvious that, by ignoring data scattering, SVM is improper to locate unbiasedly in the middle of the sup-port vectors (filled points), since x is more possible to scatter in the horizontal axis. Instead, M 4 is more rea-sonable (see the solid line in this figure). Furthermore, Figure 3(b) plots the case with the same data scatter-ing magnitudes but different data orientations. Simi-larly, SVM does not capture the orientation informa-tion. In comparison, M 4 grasps this information and demonstrates a more suitable decision plane: M 4 rep-resents the tangent line between two small dashed el-lipsoids centered at the support vectors (filled points). Note that SVM and M 4 do not need to achieve the same support vectors. In Figure 3(b), M 4 contains the above two filled points as support vectors, whereas SVM has all the three filled points as support vectors. 2.3.3. Link with Linear Discriminant Analysis LDA, an important and popular method, is used widely in constructing decision hyperplanes and reduc-ing the feature dimensionality. In the following dis-cussion, we mainly consider its application as a clas-sifier. LDA involves solving the following optimiza-MPM, LDA also focuses on using the global infor-mation rather than considering data both locally and globally. We now show that LDA can be modified to consider data both locally and globally.
 If one changes the denominators in (2) and (3) as w T  X  x w + w T  X  y w , the optimization can be changed as: where i =1 ,... ,N x and j =1 ,... ,N y . The above optimization is actually a generalized case of LDA, which considers data locally and globally. This is ver-ified as follows.
 If one performs the procedure similar to that of Sec-tion 2.3.1, the above optimization problem is easily verified to be the following optimization: is exactly the optimization of the LDA ( w T ( x  X  y )is implicitly implied as a positive value from (11) and (12)).
 Remarks: The extended LDA optimization actu-ally focuses on considering the data orientation, while omitting the data scattering magnitude information. Using the analysis similar to that of Section 2.3.2, we can know that the extended LDA lacks the considera-tion on the data scattering magnitude. Its decision hy-perplane in the example of Figure 3(a) coincides with that of SVM. With respect to the data orientation, it actually uses the average of covariances for two types of data. As illustrated in Figure 3(c), the extended LDA corresponds to the line lying exactly in the mid-dle of the long axes of the x and y data. This shows that the extended LDA considers the data orientation partially yet incompletely . 2.4. Nonseparable Case In this section, we modify the M 4 model to handle the nonseparable case. We need to introduce slack vari-ables in this case. The optimization of M 4 is changed as: where i =1 ,... ,N x , j =1 ,... ,N y ,and k = 1 ,... ,N x + N y . C is the positive penalty parameter and  X  k is the slack variable, which can be considered as the extent how the training point z k disobeys the  X  margin ( z k = x k when 1  X  k  X  N x ; z k = y k  X  N y can be conceptually regarded as the training error. In other words, the above optimization achieves maximiz-ing the minimum margin while minimizing the total training error. The above optimization is easily veri-fied to be an SOCP problem if we fix  X  . We can then update  X  sequently. This is again a sequential SOCP problem and thus can be solved practically. One may note that in the above, the classifier derived from M 4 is provided in a linear configuration. In or-der to handle nonlinear classification problems, in this section, we seek to use the kernelization trick to map the n -dimensional data points into a high-dimensional feature space R f , where a linear classifier corresponds to a nonlinear hyperplane in the original space. The kernel mapping can be formulated as: x i  X   X  ( x i ), y  X  : R n  X  R f is a mapping function. The correspond-ing linear classifier in R f is  X  T  X  ( z )= b , where  X  ,  X  R f ,and b  X  R .
 The optimization of M 4 in the feature space can be written as: However, to make the kernel work, we need to repre-sent the optimization and the final decision hyperplane an inner product form of the mapping data points. 3.1. Kernelization Theory for M 4 In the following, we demonstrate that the kernelization trick indeed works in M 4 , provided suitable estimates of means and covariance matrices are applied therein. Corollary 1 If the estimates of means and covariance matrices are given in M 4 as then the optimal  X  in (15-17) lies in the space spanned by the training points.
 Proof We write  X  =  X  p +  X  d , where  X  p is the pro-jection of  X  in the vector space spanned by all the training data points and  X  d is the orthogonal compo-nent to this span space. By using  X  T d  X  ( x i )=0and  X  tion (15-17) changes to: where i =1 ,... ,N x , j =1 ,... ,N y . Since we intend to maximize the margin  X  , the denominators in the constraints of (18) and (19) need to be as small as possible. This would lead to  X  d = 0 . In other words, the optimal  X  lies in the vector space spanned by all the training data points.
 According to Corollary 1, we can write  X  as  X  =  X  ,  X  j  X  R ,i =1 ,... ,N x , j =1 ,... ,N y . By simply substituting the above formula into (15-17), we can obtain the kernel form of the optimization of M 4 in the feature space. We present the main result as the following Kernelization Theorem.
 Kernelization Theorem of M 4 The optimal deci-sion hyperplane for M 4 involves solving the following optimization problem:
The notations in the above are defined in Table 1. In this section, we report the evaluation results. The SOCP problem is solved based on the popular SOCP software Sedumi (Sturm, 1999). 4.1. Evaluations on a Synthetic Toy Data Set We demonstrate the advantages of our approach in comparison with SVM and MPM in the following syn-thetic toy data set first.
 As illustrated in Figure 4, the data set is gener-ated under two Gaussian distributions: the x data are randomly sampled from the Gaussian distribu-tion with the mean as [  X  3 , 0] T and the covariance as [0 . 5 , 0; 0 , 8], while the y data are randomly sampled from another distribution with the mean and the co-variance as [4 , 0] T and [6 , 0; 0 , 1] respectively. Train-ing (test) data, consisting of 20 (60) data points for each class, are presented as o X  X  (+ X  X ) and  X   X  X  (  X  X ) for x and y respectively. Figure 4(a) illustrates the cor-responding derived decision hyperplanes from training data, while Figure 4(b) illustrates the performance of these hyerplanes on the test set.
 From Figure 4, M 4 achieves the ideal decision bound-ary, which considers data both locally and globally; whereas SVM obtains the local boundary just in the middle of the support vectors, which discards the global information, namely the statistical  X  X rend X  of data occurrence. For MPM, its decision hyperplane is exclusively dependent on the mean and covariance matrices. Thus we can see that this hyperplane coin-cides with the data shape, i.e., the long axis of training data of x is nearly in the same direction as the MPM decision hypeplane. However, the estimated mean and covariance may be inaccurate. This results in a rela-tively lower test accuracy as illustrated in Figure 4(b). In comparison, M 4 incorporates the information of the local points to neutralize the effect caused by inaccu-rate estimations. The test accuracies are respectively 98 . 3%, 97 . 5%, and 95 . 8% for M 4 , SVM, and MPM, which also demonstrates the advantages of M 4 . 4.2. Evaluations on Other Data Sets We perform evaluations on seven standard data sets. Data for Twonorm problem were synthetically gener-ated according to (Breiman, 1998). The remaining six data sets were real world data obtained from the UCI machine learning repository. We compared M 4 with SVM and MPM engaging both the linear and Gaus-sian kernels. The parameter C for both M 4 and SVM was tuned via cross validations, so were the width pa-rameter in the Gaussian kernel for all three models. The final performance results were obtained via the 10  X  fold cross validation. Table 2 summarizes the eval-uation results.
 From the results, we observe that M 4 achieves the best overall performance. In comparison with SVM and MPM, M 4 wins five cases in the linear kernel and four cases in the Gaussian kernel. The evaluations on these standard bench-mark data sets demonstrate that it is worth considering data both locally and globally, which is emphasized in M 4 . Inspecting the differences between M 4 with SVM, the kernelized M 4 appears marginally better than the kernelized SVM, while the linear M 4 demonstrates a distinctive advantage over the linear SVM. Due to the sparsity of data points in the kernelized space or feature space (compared with the infinite dimension in the Gaussian kernel), this is reasonable, since the plug-in estimation of the covari-ance matrices may not accurately represent the data information in this case. Further investigations on this topic is highly worthy in the future. We propose a novel large margin classifier, called Maxi-Min Margin Machine. This model learns the de-cision boundary in both a local and a global fashion. In comparison, other large margin classifiers construct classifiers either locally or globally. For example, a state-of-the-art large margin classifier, Support Vec-tor Machine considers data locally; while another sig-nificant model Minimax Probability Machine focuses on building the decision hyperplane exclusively based on the global information. As a critical contribution, we show that M 4 actually presents a unified frame-work of Support Vector Machine and Minimax Prob-ability Machine. This establishes a bridge between these two important models and provides potentials to exploit the properties of both models in a common way. Moreover, based on our proposed local and global view of data, another popular model, Linear Discrim-inant Analysis can easily be interpreted and extended as well. The experimental results have also demon-strated the advantages of our new model.
 Two important issues are worthy of future investiga-tions. First, due to the sparsity of M 4 (with support vectors as well), it would be highly valuable to develop methods to reduce those redundant data points so as to reduce the time complexity of M 4 .Moreover,We believe that there is much to gain from both exploiting analogies to SVM and developing specialized optimiza-tion procedures for the M 4 model. Second, both SVM and MPM contain a generation error bound. There-fore, exploring the bound of their superset, M 4 ,isan interesting subject. Third, the current M 4 can only handle binary classifications. How to extent its appli-cation into multi-way classifications is also an impor-tant topic.
 The work described in this paper was fully supported by two grants from the Research Grants Council of the Hong Kong SAR, China (Project No. CUHK4182/03E and Project No. CUHK4351/02).
 Bertsekas, D. P. (1999). Nonlinear programming . Bel-mont, Massashusetts: Athena Scientific. 2nd edi-tion.
 Breiman, L. (1998). Arcing classifiers. Annals of Statistics , 26(3) , 801 X 849.
 Fukunaga, K. (1990). Introduction to statistical pat-tern recognition . Boston, MA: Academic Press. Lanckriet, G. R. G., Ghaoui, L. E., Bhattacharyya, C., &amp; Jordan, M. I. (2002). A robust minimax approach to classification. Journal of Machine Learning Re-search , 3 , 555 X 582.
 Lobo, M., Vandenberghe, L., , Boyd, S., &amp; Lebret,
H. (1998). Applications of second order cone pro-gramming. Linear Algebra and its Applications , 284 , 193 X 228.
 Pruessner, A. (2003). Conic programming in GAMS.
In Optimization software -the state of the art . http://www.gamsworld.org/cone/links.htm: IN-FORMS Atlanta.
 Smola, A. J., Bartlett, P. L., Scholkopf, B., &amp; Schuur-mans, D. (2000). Advances in large margin classi-fiers . MIT Press.
 Sturm, J. (1999). Using Sedumi 1.02, a matlab toolbox for optimization over symmetric cones. Optimization Methods and Software , 11 , 625 X 653.
 Vapnik, V. N. (2000). The nature of statistical learning
