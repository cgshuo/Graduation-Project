 Monolingual sentence rewriting encompasses a va-riety of tasks for which the goal is to generate an output sentence with similar meaning to an input sentence, in the same language. The generated sen-tences can be called sentential paraphrases . Some tasks that generate sentential paraphrases include sentence simplification, compression, grammatical error correction, or expanding multiple reference sets for machine translation. For researchers not fo-cused on these tasks, it can be difficult to develop a one-off system due to resource requirements.
To address this need, we are releasing a black box for generating sentential paraphrases: machine translation language packs. The language packs consist of prepackaged models for the Joshua 6 decoder (Post et al., 2015) and a monolingual  X  X ranslation X  grammar derived from the Paraphrase Database (PPDB) 2.0 (Pavlick et al., 2015). The PPDB provides tremendous coverage over English text, containing more than 200 million paraphrases extracted from 100 million sentences (Ganitkevitch et al., 2013). For the first time, any researcher with Java 7 and Unix (there are no other dependencies) can generate sentential paraphrases without devel-oping their own system. Additionally, the language packs include a web tool for interactively paraphras-ing sentences and adjusting the parameters.

The language packs contain everything needed to generate sentential paraphrases in English:  X  a monolingual synchronous grammar,  X  a language model,  X  a ready-to-use configuration file,  X  the Joshua 6 runtime, so that no compilation is  X  a shell script to invoke the Joshua decoder, and  X  a web tool for interactive decoding and param-The system is invoked by a single command, either on a batch of sentences or as an interactive server.
Users can choose which size grammar to include in the language pack, corresponding to the PPDB pack sizes (S through XXXL).

In the rest of the paper, we will describe the trans-lation model and grammar, provide examples of out-put, and explain how the configuration can be ad-justed for specific needs. Several different size language packs are available packs are described below. Grammar Our approach to sentential paraphras-ing is analogous to machine translation. As a trans-lation grammar, we use PPDB 2.0, which contains 170-million lexical, phrasal, and syntactic para-phrases (Pavlick et al., 2015). Each language pack contains a PPDB grammar that has been packed into a binary form for faster computation (Ganitkevitch et al., 2012), and users can select which size gram-mar to use. The rules present in each grammar are determined by the PPDB 2.0 score, which indicates the paraphrase quality (as given by a supervised re-gression model) and correlates strongly with human judgments of paraphrase appropriateness (Pavlick et al., 2015). Grammars of different sizes are cre-ated by changing the paraphrase score thresholds; larger grammars therefore contain a wider diversity of paraphrases, but with lower confidences.
 Features Each paraphrase in PPDB 2.0 contains 44 features, described in Ganitkevitch and Callison-Burch (2014) and Pavlick et al. (2015). For each paraphrase pair, we call the input the original and the new phrase the candidate . Features can reflect just the candidate phrase or a relationship between the original and candidate phrases. Each of these features is assigned a weight, which guides the de-coder X  X  choice of paraphrases to apply to generate the final candidate sentence. All feature values are pre-calculated in PPDB 2.0.
 Decoding The language packs include a compiled Joshua runtime for decoding, a script to invoke it, and configuration files for different tuned mod-els. There is also a web-based tool for interactively querying a server version of the decoder for para-phrases. We include a 5-gram Gigaword v.5 lan-guage model for decoding. One or more language-model scores are used to rank translation candidates during decoding. The decoder outputs the n -best candidate paraphrases, ranked by model score. Each language pack has three pre-configured mod-els to use either out of the box or as a starting point for further customization. There are tuned mod-els for (1) sentence compression, (2) text simplifi-cation, and (3) a general-purpose model with hand-tuned weights. These models are distinguished only by the different weight vectors, and are selected by point the Joshua invocation script to the correspond-ing configuration file. 3.1 Tuned models We include two models that were tuned for (1) sen-tence compression and (2) simplification. The com-pression model is based on the work of Ganitkevitch et al. (2011), and uses the same features, tuning data, and objective function, P R  X  ECIS . The simplification model is described in Xu et al. (2016), and is opti-mized to the SARI metric. The system was tuned using the parallel data described therein as well as the Newsela corpus (Xu et al., 2015). There is no specialized grammar for these models; instead, the parameters were tuned to choose appropriate para-phrases from the PPDB.

Sample output generated with these models is shown in Table 1. 3.2 Hand-derived weights To configure the general-purpose model, which gen-erates paraphrases for no specific task, we examined the output of 100 sentences randomly selected from each of three different domains: newswire (WSJ 0 X  1 (Marcus et al., 1993)),  X  X imple X  English (the Bri-tannica Elementary corpus (Barzilay and Elhadad, 2003)), and general text (the WaCky corpus (Baroni et al., 2009)). We systematically varied the weights of the Gigaword LM and the PPDB 2.0 score fea-tures and selected values that yielded the best output as judged by the authors. The parameters selected for the generic language packs are weight lm = 10 and weight ppdb2 = 15 , with all other weights are set to zero. Example output is shown in Table 1. The language packs include configuration files with pre-determined weights that can be used on their own or as a jumping-off point for custom configura-tions. There are weights for each of the 44 PPDB 2.0 features as well as for the language model(s) used by the decoder. We encourage researchers to ex-plore modifications to the model to suit their specific tasks, and we have clearly identified five aspects of the language packs that can be modified: 1. Alternate language models. The decoder can accept multiple LMs, and the packs include LMs es-
Gen: partisanship is regarded as a crime timated over newswire text and  X  X imple X  English. Other user-provided LMs can be used for tasks tar-geting different domains of text. 2. Rank output with a custom metric. The n-best candidate sentences are chosen by their score according to a given metric (LM score for the generic model, and P R  X  ECIS and SARI for the tuned models), however other metrics can be used instead. 3. Manually adjust parameters. The weights of the features discussed in Section 3 can be adjusted, as well as other PPDB feature weights. The web tool (Figure 1) allows users to select the weights for all of the features and see the top-5 candidates generated with those weights. Some of the more in-terpretable features to target include the length dif-ference and entailment relations between the phrase original and candidate, as well as formality and com-plexity scores of the candidate paraphrase. 4. Optimize parameters with parallel data. For tailoring machine translation to a specific task, the weights given to each feature can be optimized to a given metric over a tuning set of parallel data. This metric is commonly BLEU in machine trans-lation, but it can be a custom metric for a specific task, such as P R  X  ECIS for compression (Ganitkevitch et al., 2011) or SARI for simplification (Xu et al., 2016). The user needs to provide a parallel dataset for tuning, ideally with about 2,000 thousand sen-tences. The pipeline scripts in the Joshua decoder have options for optimization, with the user specify-ing the language pack grammar and parallel tuning data. The configuration file included in the language pack can be used as a template for tuning. Finally, we include a web tool that lets users interact with the decoder and choose custom weights (Fig-ure 1). Once users have downloaded the tool kit, an included script lets them run the decoder as a server, and through the web interface they can type indi-vidual sentences and adjust model parameters. The interface includes an input text box (one sentence at a time), and slider bars to change the weights of any of the features used for decoding. Since this model has not been manually evaluated, we favor precision over recall and maintain a relatively con-servative level of paraphrasing. The user is shown the top 10 outputs, as ranked by the sentence score. For each output sentence, we report the Translation Edit Rate (TER), which is the number of changes needed to transform the output sentence into the in-put (Snover et al., 2006).

This tool can be used to demonstrate and test a model or to hand-tune the model in order to de-termine the parameters for a configuration file to paraphrase a large batch of sentences. Detailed instructions for using the tool and shell scripts, as well as a detailed description of the config-uration file, are available at the language pack home page: http://joshua-decoder.com/ language-packs/paraphrase/ Previous work has applied machine translation tech-niques to monolingual sentence rewriting tasks. The most closely related works used a monolingual para-phrase grammar for sentence compression (Gan-itkevitch et al., 2011) and sentence simplification (Xu et al., 2016), both of which developed custom metrics and task-specific features. Various other MT approaches have been used for generating sen-tence simplifications, however none of these used a general-purpose paraphrase grammar (Narayan and Gardent, 2014; Wubben et al., 2012, among others). Another application of sentential paraphrases is to expand multiple reference sets for machine transla-tion (Madnani and Dorr, 2010).

PPDB has been used for many tasks, including recognizing textual entailment, question generation, and measuring semantic similarity.

These language packs were inspired by the for-eign language packs released with Joshua 6 (Post et al., 2015). We have presented a black box for generating sen-tential paraphrases: PPDB language packs. The lan-guage packs include everything necessary for gener-ation, so that they can be downloaded and invoked with a single command. This toolkit can be used for a variety of tasks: as a helpful tool for writing (what is another way to express a sentence?); generating additional training or tuning data, such as multiple-references for machine translation or other text-to-text rewriting tasks; or for changing the style or tone of a text. We hope their ease-of-use will facilitate future work on text-to-text rewriting tasks.
