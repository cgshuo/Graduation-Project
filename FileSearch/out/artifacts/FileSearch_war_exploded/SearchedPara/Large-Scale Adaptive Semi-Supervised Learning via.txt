 Most semi-supervised learning models propagate the labels over the Laplacian graph, where the graph should be built beforehand. However, the computational cost of constructing the Laplacian graph matrix is very high. On the other hand, when we do classification, data points lying around the decision boundary (boundary points) are noisy for learning the correct classifier and deteriorate the clas-sification performance. To address these two challenges, in this paper, we propose an adaptive semi-supervised learning model. Different from previous semi-supervised learning approaches, our new model needn X  X  construct the graph Laplacian matrix. Thus, our method avoids the huge computational cost required by pre-vious methods, and achieves a computational complexity linear to the number of data points. Therefore, our method is scalable to large-scale data. Moreover, the proposed model adaptively sup-presses the weights of boundary points, such that our new model is robust to the boundary points. An efficient algorithm is derived to alternatively optimize the model parameter and class probability distribution of the unlabeled data, such that the induction of clas-sifier and the transduction of labels are adaptively unified into one framework. Extensive experimental results on six real-world data sets show that the proposed semi-supervised learning model out-performs other related methods in most cases.
 H.2.8 [ Database Management ]: Database Applications-Data Min-ing Algorithms Semi-supervised learning; Large-scale semi-supervised learning; Unified inductive and transductive model
In most data mining applications, the data are generally abun-dant, however, labeled data is often scarce. Labeling data is a te-dious work, and costs huge amount of time and money. In this situation, how to fully utilize the abundant unlabeled data becomes very important.

Semi-supervised learning is a learning paradigm that suits for this situation, where both the labeled data and unlabeled data are used to learn the prediction model. There are two types of semi-supervised learning models: transductive learning models and in-ductive learning models. Transductive semi-supervised learning methods learn the labels of unlabeled data by propagating the la-bel from labeled data to unlabeled data. The drawback of this kind of methods is that they can not be used for out-of-sample testing ( i.e. , new testing data from not included in the unlabeled data). So when a new testing data arrives, such methods need to merge those new testing data into the previous data we have, and then recon-struct the whole model based on the merged data. Obviously, such a way is very inefficient for the testing of new out-of-sample data.
Inductive semi-supervised learning methods learn a classifier us-ing both labeled data and unlabeled data. Then the learned classifier can be used for the classification of both unlabeled data using for training and also new out-of-sample testing data. In view of the convenience of out-of-sample testing, inductive semi-supervised learning methods are more attractive in practice.

Many graph based learning approaches have been proposed in recent years [1, 3, 5, 6, 13, 14, 15, 20]. Some of the most repre-sentative graph based semi-supervised learning models are: local and global consistency (LGC) [18], random walk (RW) [19], and gaussian field harmonic function (GFHF) [21], Laplacian regres-sion [12], and semi-supervised discriminant analysis [2]. All of these models utilize the Laplacian graph and propagate the labels over the graph. Therefore, in order to use these models, an n graph Laplacian matrix has to be built beforehand.
 However, the computational cost of building the n  X  n graph Laplacian matrix is at least O ( n 2 ) . Such computational cost is daunting in the circumstance of large-scale data, where the number of data can easily reach billion level. So such graph based algo-rithms are not scalable to large-scale data. Figure 1: Data samples with boundary points (the black cir-cles). The red circle point and the blue circle point are the la-beled data points. All the rest data are unlabeled.

On the other hand, when we do classification, there are many data points lie around the decision boundary, which we call bound-ary points in the paper. These data points are very noisy for learn-ing the correct classifier, and thus will deteriorate the classification performance of the learned classifier.

To address the above two challenges, a large-scale adaptive semi-supervised learning model is proposed in this paper. The proposed semi-supervised learning model has many good characteristics which will be discussed in detail after we introduce the model.
Most semi-supervised learning methods are based on graph Lapla-cian matrix, like in the following works: local and global consis-tency (LGC) [18], random walk (RW) [19], and gaussian field har-monic function (GFHF) [21], Laplacian regression [12], and flexi-ble manifold embedding (FME) [8].

One major drawback of such kind of methods is that the com-putational cost of constructing the n  X  n graph Laplacian matrix is very high, which is at least O ( n 2 ) . What X  X  more, if we use the gaussian kernel to construct the graph, the bandwidth parameter  X  should be tuned carefully in order to achieve good performance. This makes the Laplacian graph based semi-supervised learning methods impractical to solve large-scale applications in which the number of samples n is often more than billion. If we can develop semi-supervised learning methods without constructing the graph Laplacian matrix, most of the computational cost can be avoided, such that the method can be applied to large-scale data.
On the other hand, when we do classification, there exist many bad data points that lie around the decision boundary. Consider the situation demonstrated in Figure 1. The red circle point and the blue circle point are the labeled data points. Considering the distri-bution of the data, the ideal decision boundary to classify these data points into two classes should be close to the vertical line x =0 . However, there are many data points lie around the decision bound-ary (the black circles in the figure). We call these points boundary points in the following. These boundary points will blur the clear distribution of the whole data, and are very noisy for learning the correct classifier. If these boundary points dominated the loss func-tion, the learned classifier maybe distorted far way from the ground truth. In the example of Figure 1, if we consider too much of these boundary points, the learned classifier may become a horizontal line close to y =25 . If we do not consider these boundary points, and consider only the remaining clearly classified points, the cor-rect vertical decision boundary can be easily discovered. There-fore, in order to learn the correct classifier, boundary points should be considered less. In fact, boundary points widely existed in all data sets. Especially when the class number of the data is large, boundary points existed around every decision boundary of each two classes. Therefore, it is very important to develop algorithms that is adaptive to boundary points. By adaptive, we mean that the algorithm can automatically distinguish the boundary points and clearly classified points, and pay less attention to boundary points while learning the classifier.

To address the above two important challenges, in this paper, we propose a new semi-supervised learning model.
In [7], the label is obtained by the label propagation procedure and then used in a regression model. Inspired by [7], in this paper, we propose a new adaptive semi-supervised learning model where the label matrix Y is used as weights and optimized simultaneously with W . Our new model aims to solve the following objective function: min where X l  X  d  X  nl is the labeled data set, nl is the number of labeled data, d is the number of feature, 1 nl is a column vector of size nl whose elements are all one, W  X  d  X  c is the model parameter matrix that need to be learned, c is the number of classes, b  X  c  X  1 is the regression bias, Y l  X  nl  X  c is the label of labeled data; y ik is the probability of the i -th unlabeled data belongs to the k -th class, which should be in a value between [0,1], Y  X  n  X  c is the matrix formed by y ik , which should also be learned along with W , n is the number of unlabeled data, x i  X  d  X  1 is the i -th unlabeled data, t k  X  c  X  1 is a class indicator vector for k -th class, where the k -th element of t j : t jk =1 , and the rest elements are zeros. r is an adaptive parameter that need to be tuned, and r
The first term in the objective function is the total loss of labeled data, the second term is the loss of unlabeled data, weighted by the probability distribution matrix Y . There is only one parameter, i.e. r , in our proposed ASL model. In our further analysis in the experiment section, we will show that the range of r can be fixed, and a reasonable interval is suggested.

It is interesting to note that the parameter actually serves for mul-tiple purposes: from the macro level, r balance the two terms in the objective function, decides how much unsupervised infor-mation is used; from the micro level, after r is fixed, the weights of boundary points will be suppressed to make the model robust to boundary points. Following are some more detailed analysis for the conclusion.

From a macro point of view, r serves as a tradeoff between the first term (supervised part with label information available) and the second term (unsupervised part without label information). Note that we do not need another tradeoff parameter before the second term because r is able to balance the two terms. When r becomes large, y r ik will become small since it is a probability which is defi-nitely less than 1, thus the weight of the second term become small. In the extreme case, when r approaches infinity, the second term vanishes to zero, so only the first term counts, which means the objective function reduces to supervised learning.

From a micro point of view, r automatically adjust the impor-tance (weight: y r ik in the objective function Eq. (1)) of each data instance. Consider the i -th data instance: if it is clearly classi-fied, y ik ( k =1 , ..., c , the probability for x i belongs to differ-ent class) will show obvious magnitude difference. For boundary points, however, y ik ( k =1 , ..., c ) will be more likely equal to one another. Without loss of generality, assuming a binary classifica-tion problem, and r =2 . For clearly classified points, y would be one large and one small, say y i 1 =0 . 9 and y i classified points still have large weights in total and contribute a lot to the second term in the objective function. For boundary points, however, y i 1 and y i 2 would be more likely equal. Assum-ing y i 1 = y i 2 =0 . 5 , then y r i 1 = y r i 2 =0 . 5 2 boundary points will have small weights and contribute much less to the second term in the objective function. The above analysis is based on binary classification. Actually, in multi-class problems, for boundary points, y ik ( k =1 , ..., c ) may tend to be fore, when c becomes larger, y ik becomes smaller, and y r much smaller than the case in binary classification.

From the above analysis, we can see that: when r increases, y ik will decrease for both clearly classified points and boundary points. However, the weights of boundary points are suppressed much more faster than clearly classified points. In this way, our model will relatively pay more attention to the clearly classified points and pay less attention to the boundary points. This makes our model adaptive and robust to boundary points, therefore, a bet-ter classifier can be learned.

It is innovative to use the class probability matrix of unlabeled data to do inductive learning. In our model, the induction of the classifier W is dependent on the class probability matrix Y of un-labeled data, and the transduction of labels to unlabeled data is de-pendent on the classifier W . The two steps are unified together by simultaneously optimize W and Y . This adaptive procedure is expected to benefit both the induction of classifier and also the transduction of labels to unlabeled data. However, in previous in-ductive semi-supervised learning methods, only the feature matrix X of unlabeled data is utilized to learn the classifier. And in those previous methods, an additional step is needed after the model is learned in order to predict the label of unlabeled data.
We summarize the characteristics of our model as following: (1) Computational efficient : different from common graph-based semi-supervised learning methods, our model avoids the computa-tional expensive step of constructing the graph Laplacian matrix. (2) Adaptive and robust to boundary points : our model can adaptively adjust the weights of each data point. Boundary points will get much smaller weights than other points. This makes our model pay less attention to boundary points, and thus robust to boundary points. (3) Adaptive optimization procedure : we simultaneously op-timize the model parameter W and the class probability matrix Y of unlabeled data. This adaptive procedure is expected to benefit both the induction of classifier and also the transduction of labels to unlabeled data. (4) Only one parameter : there is only one parameter to be tuned in our model, and the single parameter serves for multiple purposes. A reasonable range of the parameter can be given to facilitate the tuning procedure.

In the following section, an efficient iterative algorithm will be derived to solve the proposed objective function.
In this section, we derive an efficient iterative algorithm that al-ternatively optimize over the model parameter W, b and the class probability matrix Y . (1) When the model parameters W and b are fixed, we solve the class probability matrix Y . Since the first term becomes a constant, the objective function is reduced to: Denote p ik = x T i W + b T  X  t T k 2 2 , Eq. (2) can be written as: Obviously, Eq. (3) can be decoupled between samples. So it is equivalent to solving: where y i  X  represents the i -th row of Y .

When r =1 , obviously, the optimal solution of Eq. (4) is: where k  X  = arg min
When r&gt; 1 , we solve Eq. (4) in the following way. The La-grangian function of Eq. (4) is: where  X  is the Lagrangian multiplier. In order to get the optimal solution of the subproblem, we set the derivative of Eq. (6) with respect to y ik to zero. Thus, we get:
Substituting Eq. (7) into the constraint closed form solution of Y as following: (2) When the class probability matrix Y is fixed, we solve the model parameter W and b . Note that the second term in the objec-tive function in Eq. (1) sums over the number of unlabeled points n and the number of classes c . If we directly taking the derivative of the second term with respect to W , the resulting algorithm would need to iterate over n and c , which would be slow.

It is interesting to note that we can write the objective function in Eq. (1) into compact matrix representation in the following way: min where F = Y r  X  n  X  c (here Y r denotes to perform power operation on each element in Y ), S  X  n  X  n is a diagonal matrix with the i -th diagonal element equal to s ii =
Setting the derivative of Eq. (9) with respect to b to zero, we get: where q is a scalar, q = 1 nl + 1 T
Setting the derivative of Eq. (9) with respect to W to zero, and plug in Eq. (10), we get: [ X where Q 1 = q 1 nl 1 T nl , Q 2 = qS 1 n 1 T n , Q 3 = q 1 q 1
Denote C = X l ( I nl  X  Q 1 ) X T l + X ( I n  X  Q 2 ) SX T XQ 4 X T l , and A = X l ( I nl  X  Q 1 ) Y + X ( I n  X  Q 2 XQ 4 Y , then the optimal solution of W is:
The optimization procedure is summarized in Algorithm 1. The two step optimization procedure alternatively optimize the model parameter W and the class probability matrix Y of unlabeled data. The induction of the classifier W is dependent on the class prob-ability matrix Y of unlabeled data, and the transduction of labels to unlabeled data is dependent on the classifier W . This adaptive procedure is expected to benefit both the induction of classifier and also the transduction of labels to unlabeled data. It is novel to use the class probability matrix of unlabeled data to do inductive learn-ing.

Because the algorithm get the minimum in each updates of W, b and Y , so the objective value decreases in each updates. What X  X  more, the objective function is lower bounded by zero. Therefore, it is obvious that our algorithm will converge. We will show in the experiment section that the algorithm actually converges quite fast on all data sets.
 Algorithm 1 Algorithm to solve the problem (1).

Initialize W . repeat until Converges
The major computational cost lies in our algorithm lies in the updating of W = C  X  1 A , where c is a d  X  d matrix. The computa-tional cost of matrix inverting is O ( d 3 ) . In fact, the computation of matrix inverse can be avoided. Note that we are aiming to compute C  X  1 A , which is the solution of the following problem: The solution of this minimization problem can be get iteratively us-ing gradient descent method using the updating formula: W W t  X   X  ( CW t  X  A ) , with a computational cost of O ( Td T is the number of iterations, c is the number of columns in A .
In addition, getting the d  X  d matrix C cost O ( nd 2 ) , getting the d  X  c matrix A cost O ( ndc ) . Therefore, the total computational cost of our algorithm is upper bounded by O ( nd 2 )+ O ( ndc )+ O ( Td 2 c ) . Consider in real situation c is always smaller in mag-nitude compared to d and n , the total computational cost can also be written as O ( nd 2 )+ O ( Td 2 ) . This shows that the computa-tional cost of our algorithm is linear with respect to the number of data samples n . Therefore our algorithm is able to scale to large-scale data.

However, for those graph based semi-supervised learning meth-ods, without taking into consideration of the huge computation cost for tuning the gaussian kernel bandwidth parameter and the run-ning time of the algorithms themselves, constructing the Laplacian graph matrix already takes at least O ( n 2 ) . In the big data applica-tions, the number of data n can easily reach billion level. So such graph based algorithms are not scalable to large-scale data.
In order to show the effectiveness of the proposed adaptive semi-supervised learning method, experiments are conducted on six real world data sets: AR [4], YALE-B [17], MSRC-V1 [16], PIE [11], FERET [10] and ORL [9]. These six data sets are all comprising of human faces, represented using gray scale pixel values. Figure 2 demonstrate some sample images from each data set. Important statistics are summarized in Table 1.
 In order to evaluate the effectiveness of the proposed Adaptive Semi-supervised Learning (ASL) method, we compare it with some most representative state-of-the-art semi-supervised learning meth-ods. Since our method is a unified model which can simultaneously perform transduction and induction, we compare our methods with both transductive semi-supervised learning methods and inductive semi-supervised learning methods.

In this paper, we compare our method with three representative transductive semi-supervised learning methods: local and global consistency (LGC) [18], random walk (RW) [19], and gaussian field harmonic function (GFHF) [21].

Inductive semi-supervised learning methods learn a classifier us-ing both labeled data and unlabeled data. Then the learned classifier can be used for the classification of both unlabeled data using for training and also new out-of-sample testing data. Compared with our method which can directly learn the labels of unlabeled data, common inductive semi-supervised learning methods need an ad-ditional step to get the labels for unlabeled data used in training.
Two representative inductive semi-supervised learning methods are compared with our method: flexible manifold embedding (FME) [8], and Laplacian regression (LapReg) [12]. LapReg aims to solve the manifold regularized problem, which has the following objec-tive function: min where Y l , X l are the labels and feature matrix of labeled data, re-spectively, X is formed by both labeled data and unlabeled data, L is the graph Laplacian matrix constructed using X ,  X  1 and  X  regularization parameters to balance the three terms.

Semi-supervised discriminant analysis (SDA) [2] is also a com-monly used semi-supervised learning method. It aims to learn a projection matrix to map the data in high dimension to a lower dimension subspace. Since our method aims to learn a classifier rather than projection matrix for dimension reduction, we do not compare with SDA in this paper.

In our experiment, we repeat every methods 20 times to compute the average classification accuracy and standard deviation. Each data are first preprocessed using PCA such that 95% of the en-ergy are retained. Different number of labeled points are used for training to study the sensitivity of those methods to the number of labeled data. We randomly choose 1,3,5 labeled points from each class as labeled data, and the remaining as unlabeled data. For transductive methods, since they can not perform out-sample testing, only the accuracy for unlabeled data is computed. For in-ductive methods, 33% of data are leaved out for out-of-sample test-ing, then the remaining training data is splited into labeled data and unlabeled data. The accuracy for both unlabeled data and out-of-sample testing are computed.

For our method, the parameter r is tuned from 1 to 2 with a step-size of 0.1. For LGC and RW, the tradeoff parameter  X  is tuned in [0.1:0.1:0.9,0.99] ( i.e. from 0 to 0.99 with a step-size of 0.1, 0.99 is also included). GFHF is parameter-free. For LapReg and FME, both of them have two regularization parameters, and are tuned in { best tuned parameter are recorded.
In this section, we show the working mechanism of the proposed adaptive semi-supervised learning model on a synthetic data. The synthetic data (demonstrated in Figure 3(a)) is generated as follow-ing: 50 data points (corresponding to the samples in the left part of the figure) are sampled from a gaussian distribution with a mean of -15, and a standard deviation of 5 ; Another 50 data points (corre-sponding to the samples in the right part of the figure) are sampled from a gaussian distribution with a mean of 15, and also a standard deviation of 5. Then 20 boundary points are generated around the ideal decision boundary ( i.e. the vertical line x =0 ) with a mean of 0 and a standard deviation of 12. All the above data points are unlabeled data. After that, we add only one labeled point to each part ( i.e. , red circle box in the left, blue circle box in the right). Note that we intentionally place the labeled point in the upper left corner and lower right corner. Compared to randomly label one point from each part, this labeling strategy will make it harder for a model to recover the correct decision boundary.

Figure 3 (c) shows the weights/importance of each unlabeled data point learned by our model . The weights of the i -th data point is tribute more to the learning of the classifier. The last 20 samples are boundary points (points in the middle part of Figure 3 (a)). We can see that the weights of those boundary points are very small com-pared to other point. Therefore, our model can adaptively adjust the weights of each point, and thus robust to boundary points.
Figure 3 (b) shows the data after classification using our adap-tive semi-supervised learning model. The size of each point is pro-portional to their weights. Boundary points in the middle part get smaller weights. The correct vertical decision boundary (the green line in the figure) is perfectly recovered, even with only one se-lected labeled point from each class, and the one labeled point is intentionally set to make it harder to recover the correct decision boundary points in Figure (a) boundary. This shows that our model is able to learn the correct classifier with the presence of boundary points. There is only one parameter, i.e. r , in our proposed ASL model. As discussed before, the parameter actually serves for multiple pur-poses. In order to achieve the best performance, a proper r should be used to balance the supervised part and unsupervised part in the objective function, and in the meantime, make our model robust to boundary points.

If r is too large, the y r ik will tend to be zero, and the unsupervised information is lost. So r can not be too large. That X  X  why r is suggested to vary in [1,2] (Consider the class number is often larger than 10, r =2 is large enough to suppress; when the class number is small, the range of r can be extended correspondingly. Generally, [1,4] is totally sufficient for all small class number problems.)
In the following, we will show how the change of parameter r influence the weights of data points , and the influence on classifi-cation performance.
When r varies, the contribution of each data instance to the model also changes. We define a data point as an important point if the sum of weights over all classes exceeds an threshold value, i.e. it satisfies the following condition: where t is a pre-specified threshold. In our experiment, t is set as 0.25 since it is reasonable to assume that an important point should belong to one certain class with a probability larger than 0.5, so y r ik &gt; 0 . 5 2 =0 . 25 when r&lt; 2 .

Figure 4 shows the number of important points using different r values. From the figure we can see that: (1)When r =1 , all points are regarded as important points be-cause the loss function since their sum of weights over all class are the same. (2)When r increases, the number of important points will de-crease. In this process, boundary points will become unimportant (contribute less to the objective function) with a small r value. This makes our model robust to boundary points. Some non boundary points may become unimportant with a relatively large r value. In the early stage of the increasing of r (say r&lt; 1 . 4 ), the decreas-ing of important points is mainly because boundary points become unimportant. In the later stage of the increasing of r (say r&gt; 1 . 4 ), the decrease of number of important points may also be due to that some non boundary points also become unimportant. Note that the number of important points on some data sets become zero in the later stage, but this does not mean unlabeled data points are not used in our model. Remember that the threshold we are us-ing is 0.25, so the unlabeled data information is still utilized in the model with a relatively small weight. In the extreme case, when r approaches to infinity, all points will become totally unimportant with weight 0. In this case, unlabeled data points are not used in the model, which reduces to supervised learning.

Therefore, in order for the model to utilize the unlabeled data to the best degree, a proper r value should be chosen. The proper r value should: in the macro level, balance the supervised term and unsupervised term in the objective function; in the micro level, automatically adjust the weights of data points, so that the model become robust to boundary points, and thus learn a better classifier. (3) On some data sets with large number of classes (FERET with 200 classes, AR with 120 classes), the number of important points will decrease drastically to zero. In the contrary, on some data sets with small number of classes (MSRC-V1 with 12 classes), the number of important points will decrease much slower, and still has many important points when r =2 . This is because when the class number is small, there tend to be less boundary points, and the probability y ik will be relatively larger, so even with a large r value, there are still many important points. However, when the class number is large, there tend to be much more boundary points whose y ik are small (tend to be 1 c in the worst case). So even with small r value, those boundary points become unimportant. In this case, the benefits of our proposed model, which suppresses the weights of boundary points, will be more obvious.
After the above analysis, we know that r will influence the learned model from both macro level (the weight of unsupervised term in the objective function) and micro level (suppress the weights of boundary points). Different model will lead to different classifica-tion results. In this section, we show how the r value will influence the classification performance.

Figure 5 shows the classification accuracy using different r value on the six data sets. From the figure we can see that: the classifi-cation accuracy changes with the parameter r , and the best clas-sification results is achieved by different r value on different data sets.

On all the data sets, the best classfication accuracy is not achieved when r =1 , which is the common practice in many papers. This is because: by setting y r ik as data weights, our model is more ro-bust to boundary points by suppressing the weights of boundary points. Therefore, by setting the weights of data as y r ik the commonly used y ik is more meaningful.
In this section, we present extensive empirical study of the clas-sification performance on six real world data sets. The classifica-tion accuracy and standard deviation for running different semi-supervised learning methods are reported in Table 2 to Table 7. From these tables, we can conclude that: (1) The ASL method outperforms other comparing inductive semi-supervised learning methods (LapReg and FME) in most cases, and is significantly better than other transductive methods (LGC, RW, and GFHF) on all the six data sets. This justifies the effectiveness of the proposed ASL method. Because the ASL method is able to automatically balance the supervised part and unsupervised part in macro level. What X  X  more, in the micro level, the ASL method can suppress the weights of boundary points, which makes the model robust to boundary points, and thus, learn a better classifier. (2) In general, the inductive semi-supervised learning methods perform better than transductive methods. (3) The performance of the three transductive methods are com-parable to each other. The performance of these transductive meth-ods is pretty good on the MSRC-V1 and ORL data sets. The rea-son maybe that the number of classes is small on these two data sets compared to other data sets. It seems that those transductive methods can not perform well when the data has large number of classes. (4) The performance of LapReg and FME are comparable to each other. On the AR data set, the performance of FME is slightly better than the ASL method. This shows the effectiveness of using the manifold regularization to utilize the unlabeled data information. However, the drawback of LapReg and FME is that both of them have two regularization parameters, and the parameters X  range is pretty large and not fixed (vary in [10  X  5 , 10 5 ] or even larger). This makes it hard to tune the parameter. (5) When the number of labeled data points from each class( i.e. kl) increases, the performance also become better on all data sets. Especially when number of labeled data points increase from 1 to 3, the classification accuracy improved significantly. Therefore, when the labeled information is scarce, to acquire more labeled data can be very helpful in semi-supervised learning.
In this section, we show the converge speed of the proposed al-gorithm empirically. Figure 6 shows the objective function value versus number of iterations. We can see that the proposed iterative algorithm converges in less than 20 iterations on all data sets.
In our experiment, running our algorithm 20 iterations only takes about 1 second on a laptop with 2.70GHz double core Intel Core i7 cpu, 16GB memory. Since the major computational cost in our al-Data Set.
 2.42 NA 68.16  X  1.48 NA 2.38 NA 68.69  X  1.42 NA 1.71 NA 70.32  X  2.12 NA 1.73 86.51  X  1.92 94.90  X  0.82 95.02  X  1.07 2.55 85.54  X  2.92 94.61  X  2.03 94.83  X  2.11 1.86 96.13  X  1.92 99.14  X  0.54 99.00  X  0.85 V1 Data Set.
 5.38 NA 95.19  X  3.58 NA 5.29 NA 96.88  X  2.94 NA 4.39 NA 97.07  X  2.72 NA 2.08 97.55  X  1.90 98.69  X  1.69 98.61  X  1.61 2.64 97.28  X  2.57 99.29  X  0.88 99.22  X  0.98 1.92 98.45  X  1.82 99.36  X  1.25 99.42  X  1.15 PIE Data Set.
 0.85 NA 54.67  X  1.36 NA 1.15 NA 54.28  X  1.36 NA 1.72 NA 55.58  X  1.17 NA 1.56 86.26  X  1.23 91.44  X  0.81 91.45  X  0.96 1.78 85.36  X  1.66 91.35  X  1.05 91.31  X  1.42 1.73 90.70  X  1.64 93.53  X  0.84 93.38  X  0.90 Data Set.
 Data Set.
 gorithm lies in the inverse of a d by d matrix, when the algorithm is used on data with high dimensionality, the computational cost can be reduced by using PCA for dimensionality reduction beforehand.
In this paper, we propose an adaptive semi-supervised learning model. Different from previous semi-supervised learning, our pro-posed model needn X  X  construct the graph Laplacian matrix. Thus, our method avoids the huge computational cost required by previ-ous methods, and achieves a computational complexity linear to the number of data points. Therefore, our method is scalable to large-scale data. Moreover, the proposed model adaptively suppresses the weights of boundary points. This makes our model robust to boundary points. An efficient algorithm is derived to alternatively optimize the model parameter and class probability distribution of unlabeled data, such that the induction of classifier and the trans-duction of labels are adaptively unified in one framework. Our model only has one parameter need to be tuned, and a fixed range is also suggested. Extensive experimental results show that the pro-posed semi-supervised learning model outperforms other state-of-the-art methods in most cases. This research was partially supported by NSF-IIS 1117965, NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628.
 [1] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold [2] D. Cai, X. He, and J. Han. Semi-supervised discriminant [3] X. Chang, F. Nie, Y. Yang, and H. Huang. A convex [4] A. M. Martinez. The ar face database. CVC Technical [5] F. Nie, S. Xiang, Y. Jia, and C. Zhang. Semi-supervised [6] F. Nie, S. Xiang, Y. Liu, and C. Zhang. A general [7] F. Nie, D. Xu, X. Li, and S. Xiang. Semisupervised [8] F. Nie, D. Xu, I. W.-H. Tsang, and C. Zhang. Flexible [9] ORL Face Database, 2007. http: [10] P. J. Phillips, H. Moon, S. A. Rizvi, and P. J. Rauss. The feret [11] T. Sim, S. Baker, and M. Bsat. The cmu pose, illumination, [12] V. Sindhwani, P. Niyogi, M. Belkin, and S. Keerthi. Linear [13] D. Wang, Y. Wang, F. Nie, J. Yan, W. Cai, A. Saykin, [14] H. Wang, C. Ding, and H. Huang. Directed Graph Learning [15] H. Wang, H. Huang, and C. Ding. Image annotation using [16] J. Winn and N. Jojic. Locus: Learning object classes with [17] Yale Univ. Face Database, 2002. http://cvc.yale. [18] D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Sch X lkopf. [19] D. Zhou and B. Sch X lkopf. Learning from labeled and [20] X. Zhu. Semi-supervised learning literature survey. [21] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised
