 Currently, the Internet has witnessed a proliferation of Web pages. Although users access Web pages in order to obtain useful information, it is difficult to search for the Web pages that contain such information. When users perform a search, they get a ranked list of Web p ages with their summaries, which are called result snippets [1]. Using the result snippets, users can select the pages that contain relevant information. However , users are sometimes unable to recognize the contents of Web pages because the snip pets consist of a combination of text strings and query keywords; these strings are not sufficiently long to determine whether the corresponding Web page contains relevant information.

If the contents of Web pages are represented using summaries that are more comprehensible than the snippets, users will be able to identify the Web pages that contain relevant information. Thus, we propose a method for extracting a relevant text string based on the content-density distribution of a Web page; this method provides the user with an alternative snippet of the Web page, thereby enabling him/her to grasp its content. Then, the content can be regarded as a set of words in a text string. The content-density distribution helps users to understand the position and influence of the content of the Web page. Ercan et al. extracted nouns from a Web page by using lexical chains for its summarization [3]; this approach is a type of text segmentation [4,5]. Text seg-mentation is a method for separating text into blocks; it is adopted in several research fields including query extraction [6,10]. A lexical chain is a sequence of words in each sentence of a Web page; each lexical chain has a lexical chain occurrence vector. When we regard a lexical chain as a type of content, the its occurrence vector is regarded as the position and the influence of the content. However, it is difficult to grasp the content if there is a sudden in-text content change; this is because a lexical chain is defined for each sentence.
Another related concept is p assage retrieval [7], which involves the extraction of parts of a Web text related to query key words. Consequently, users can obtain these parts or the entire Web pages. In other words, they can search for a part of a Web text in the vicinity of the query keywords. In contrast, our objective is to determine the position and influence of the content of a Web text; we calculate the content related to the keywords.

Lv et al. constructed the positional language model by extending the infor-mation retrieval model on the basis of the language model [8]. They carried out passage retrieval without determining the size of passages, and they determined the position and proximity of query keywords. They described the construction of the positional language model and its application to the passage retrieval score. In addition, they calculated the values representing the estimated word count at each position in a Web text. Thus, they did not use the model to deter-mine the position and influence of the content of the Web text. They evaluate their study not on the basis of the content but passage retrieval. Moreover, if the estimated word count is regarded as the influence of the word in a Web text, the influence does not take the value of zero at any position in their method. In our method, zero value of the influence of the word indicates no content of the word in that position. Therefore, we cannot compare their method to our method. In this section, we describe the construction of the content-density distribution, and we extract a text as a summary of the W eb page. As described in Section 1, the content-density distribution denotes the positions of the words and their influence in a Web text. Thus, the content-density distribution reflects both the position and the influence of the content. We construct it in the following steps: 1. Calculation of word-density distribution in a Web text 2. Construction of content-density dis tributioninaWebtextandextractionof Before calculating the word-density distribution, we should extract words in a text string of a Web page (Web text, in s hort). Therefore, we search for Web pages using a search engine, extract the Web text, and specify the POS (part of speech) of each word in the Web text. If the word is not in dictionary form, we convert the word into its dictionary form.
 3.1 Calculation of Word-Density Distribution in a Web Text We calculate the word-density distribution of each word from the extracted Web text. The Web text is composed of a set of words, and we can recognize the appearance position of each word in it. In addition, if we define the influence of the word to be the highest at the appearance position, we can briefly survey both the position and the influence of the word, and calculate the word-density distribution. We explain its calculation using Fig. 1.
If we extract the Web text s m , t m i denotes the word t i in s m .Whenwecount the words in the Web text s m , hw [ t m.j i ]( k ) denotes a value of the word-density distribution, which is taken by the j -th word of t m i at the k -th word in the Web text s m ( j =1 , 2 ,  X  X  X  )(SeeFig.1).The j -th word t m i appears at the position l [ t i ], so that hw [ t of the content may suddenly change at the end of a sentence; this is known as a sentence separator 1 . We define a [ t m.j i ] as the sentence separator just before l [ t i ], and b [ t the weighted Hanning window function; it has been suggested that this function is the most useful function for extracting the influence of words [9] 2 : We use the window function because we assume that the word has an effect on other words that precede and follow it. Th erefore, we assume its effect in the range | k  X  l [ t m.j i ] | X  W 2 . S is a weighting parameter whose value lies between zero and one, and we define this parameter in order to consider the change in the content of a Web text. Although the Hamming window function was proposed as an improved variant of the Hanning window function, the Hamming window function does not take the value of zero at the tail. The Hamming window function is not suitable for treating this influence because we assume that the influence of the word gradually tends to zero.

We present the following example in order to provide readers with a better understanding of word-density distribution. Fig. 2 shows the calculation of the word-density distribution of t m 1 . We assume three t 1 in a Web text, and the words up and normalizing hw [ t m. 1 1 ]( k ), hw [ t m. 2 1 ]( k ), and hw [ t m. 3 1 ]( k ). There are several methods for integrating multiple values into one value [2]. We select the summation of all hw [ t m.j i ]( k )ateachposition k because summation is an intuitive approach. Here, the word t m i may appear many times, in which case we have to combine hw [ t m.j i ]( k )toconstructtheentirecontent-density distribution. However, if we treat the summation as the word-density distribution Web text s m . To solve this problem, we normalize hw [ t m.j i ]( k ) by dividing each 3.2 Construction of Content-Density Distribution in a Web Text After calculating the word-density distribution, we use it to construct the content-density distribution. In this paper, we denote the content-density distribution of s m as hw [ Q m ]( k ), where Q m is a query composed of word t m i ( i =1 , 2 , in s m . If the query has only one word, the word-density distribution and the content-density distribution are equivalent.

Because the query is usually compos ed of two or more keywords, we need to judge whether a set of query keyword s forms content. We believe that a set of keywords forms content if each keyword is closely located in a Web text. We judge the existence of the content as the overlap between the word-density distributions of keywords. In this study, we assume the presence of a range in which the content exists if the word-density distributions overlap with each other. Fig. 3 shows the construction of the content-density distribution of s m . The two-headed arrows in Fig. 3 repre sent content related with the query Q m , comprising t m 1 and t m 2 , because two word-density distributions overlap in s m .
Here, we construct the content-density distribution from the word-density distributions. In much the same fashion as the construction of word-density dis-tributions, we use the summation of each word-density distribution related to each query keyword. If we calculate the c ontent-density dist ribution related to query keywords Q m to sum up hw [ t m i ]( k )ateach k , the content-density distri-bution relies on the number of query keywords at the same position; thus, we divide their summation by the number of query keywords to get hw [ Q m ]( k )as: We illustrate the following example to facilitate the understanding of word den-sity. In Fig. 4, if we judge whether t m 1 and t m 2 form content, we check whether two word-density distributions overlap in s m . According to Fig. 4, these words in the query Q m have the range of content. Then, we c an construct the content-density distribution related to Q m . We can regard these word-density distributions as content-density distributions because t m 1 and t m 2 overlap with each other. Fi-nally, we can extract three contents re lated to the query keywords in the Web hw [ t m 2 ]( k ), and hw [ Q m ]( k ).

Finally, we extract a part of a Web text including the content by using the content-density distribution. If the value of the content-density distribution at a position is larger than a threshold, we can say that the content related to the query Q m exists at this position. In this paper, we regard this threshold as zero because we believe that the content will exist, no matter how small the value of the content-density distribution is, unless the value is zero. The content extracted by the content-density distribution has two aspects. One is the range of the content; the other is the influence of the content. If we evaluate these aspects precisely, we must confirm w hether our method is appropriate with regard to these aspects. In this section, we evaluate a Web text extracted by the content-density distribution and the position containing the highest value of the content-density distribution in each Web text. Next, we assume that these aspects are suitable for the content users consider useful. Then, they use the Google AJAX Search API to obtain Japa nese Web texts [12]. We describe the evaluation of our method and the object ives of these experiments as follows:  X  Evaluating the extracted Web text  X  Evaluating the influence of the extracted Web text In order to evaluate the aspects mention ed above, we create two indicators, and in order to conduct the experiments from the viewpoint of these aspects, we have to create a data set for evaluating our method. We request an external individual (collaborator) to create it in the following steps: 1. The collaborators of the experiments generate queries composed of two fa-2. In order to construct the data set, the extracted Web texts should be parsed 3. The collaborators choose a word that they believe to be the most important Fig. 5 shows the answer data in the Web text s m . The shaded positions in Fig. 5 represent the answer part of the word  X  X eople. X  In addition, the square shows the answer position.

In this experiment, we used 89 Web texts f or constructing the data set because some Web pages do not include the Web text. Moreover, we conduct prelimi-nary experiments in order to obtain good parameters. As a result, we set some parameters as W and S . W equals 0.6, and S is three times the average length of sentences in each Web text. 4.1 Evaluating the Extracted Web Text In this experiment, we evaluate whether the part of the Web text, which is extracted on the basis of a threshold of the content-density distribution, coincides with the answer part of the Web text. Thus, we compare the Web text extracted by our method with that extracted by the Google Snippet using the answer parts in the data set. For this comparison, we use an indicator called the concordance rate of an answer. Equation (4) represents the indicator used for the comparison. This indicator shows the concordance rates of an answer, the part calculated by the numbers of words in both the answer part and the positions of the values of the content-density distribution.
 This indicator shows how much of the answer part can be extracted for evaluating all extracted Web texts by using each method. We utilize the answer part for the evaluation. In this equation, N a indicates the number of words in the range of the answer extracted by all Web texts, N a  X  e indicates the number of words in the answer part and the part of all Web texts extracted by our method.
When we utilize the number of words to evaluate the extracted Web text, the number of words in the result snippet is smaller than the number of words in our method. At this time, we are unable to compare these methods directly. To solve this problem, we adopt the proposed method, which can extract the Web text containing the same number of words as the proposed method.

We calculate the concordance rate of an answer to compare this rate, based on each method. According to the results, this value based on the proposed method is 0.1801. In contrast, this value based on the conventional method is 0.1645. Therefore, the proposed method coincides with the answer more than the conventional method in this evaluation. This is because we consider not only the proximity of the query keywords but also the position where the content may change. As a result, we can precisely extract the position where the value of the influence is large. Thus, we can say that the Web text extracted by the content-density distribution is more appropriate for a comprehensible summary of a Web page than a result snippet. 4.2 Evaluating the Influence of the Extracted Web Text In the other experiment, we determine whether the influence of the content-density distribution is suitable for c ans . In the paper, we denote the collaborators believe that the influence of the content is the highest at c ans , and the value of the content-density distribution at c max is the highest. When c ans coincides with c max , we can say that we can extract the influence of the content using the content-density distribution. Even though the gap between c ans and c max is small, the value of the content-density distribution at c ans is not always small. Consequently, we cannot evaluate it using only the distance between c ans and c max . Thus, we also define another indicator to evaluate the proximity between the values of the content-density distribution at c ans and c max .
 First, we evaluate our method on the based on the gap between c ans and c max . When we define the number of words in each Web text s m as N s m , we utilize these positions to define this indicator as Equation (5) represents the normalized distance between c ans and c max .There-fore, if the value calculated by Equation (5) is smaller, c ans is closer to c max ,we can assume that the content-d ensity distribution grasps c ans .

We also evaluate our method based on the proximity between the value of the content-density distribution at c ans and the highest value of the content-density distribution. We define the equation to calculate the indicator as In Equation (6), hw [ Q m ]( c ans ) describes the value of the content-density dis-tribution at c ans ,and hw [ Q m ]( c max ) describes the value of the content-density distribution at c max for a query Q m in the Web text s m .If IV s m is large, we can say that our method can precisely extract c ans from the Web text. As a result, we can assume that the gap between hw [ Q m ]( c max )and hw [ Q m ]( c max ) is small.
We evaluate the influence of extracted Web text, whether c max can grasp the answer position of the content by combining these two indicators, and plot the data for evaluations on a scatter di-agram. When IP s m is small, the content-density distribution of the Web text s m can grasp the answer position c ans . On the other hand, when IV s m is large, the value of the content-density distribution at c ans is similar to the highest value of the content-density distribution. Thus, if the Web text produces a good result by the two in-dicators, the content-density distribution of the Web text can grasp the content by evaluation of the position as well as by evaluation of the influence.
 Fig. 6 shows the result calculated for each Web text. The vertical axis in Fig. 6 denotes the indicator IP s m and the horizontal axis in Fig. 6 denotes the other one IV s m In this experiment, the plots on the lower-right indicate the Web text precisely extracted the center o f the content by the content-density distribution. This is because the result of the evaluation will be better, when IP s m is smaller and IV s m is larger. According to Fig. 6, 43 plot appear on the lower-right; we can precisely extract the important part of 48% texts related to the content from the viewpoint of both the position and the influence. When we use the summary extracted by the content-density distribution, we can recognize whether the content exists in one of the two Web pages. Therefore, we can show that the range and influence of the content extracted by the content-density distribution are appropriate. In this paper, we proposed a method for grasping the content of text on a Web page in order to construct the content-density distribution related to query keywords. For the sake of evaluation, we compared the range of the content-density distributions and result snippets on the basis of the average concordance rate of an answer. Consequently, the p roposed method produces much better results than conventional one. According to discussions on the influence of the content-density distributions, users believe that the center of content in a Web text has a larger value of the content-density distribution; moreover, we also have to evaluate our method from the the viewpoint of time complexity. Acknowledgments. This work is partly supported by JSPS Grant-in-aid for Scientific Research (A) #22240005 and for Young Scientists (B) #22700248.
