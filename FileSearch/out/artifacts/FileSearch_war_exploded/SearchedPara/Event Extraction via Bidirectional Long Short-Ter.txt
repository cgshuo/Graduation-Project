 Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments. The task is quite difficult because a larger field of view is often needed to understand how facts tie together. To capture valuable clues for event extraction, current state-of-the-art methods [16,17,11,18,13] often use a set of elaborately designed features that are extracted by textual analysis. For example, consider the following sentences: Attack , which is more common than type End-Position . Because of the ambiguity, a traditional approach may mislabel fired in S1 as a trigger of Attack . However, if we know that the context words  X  air defense chief  X  is a job title, we have ample evidence to predict that fired in S1 is a trigger of type End-position . To capture these seman-tics, traditional methods often use the part-of-speech tags (POS), entity information, and morphology features (e.g., token,lemma,etc.). However, these features for words are a kind of one-hot representation, which may suffer from the data sparsity problem and overlook the impact of the context words. Furthermore, these methods often rely on human ingenuity for designing such elaborate features, which is a time-consuming process and lacks generalization. [22].
 understand semantic relations between words. Previous methods often use the syntac-tic features to capture these semantics. For example, in S3, there are two events that share one argument as shown in Figure 1, from the dependency relation of nsubjpass between the argument Peterson and trigger arrested , we can induce a Person role to Pe-terson in the Arrest-Jail event. However, for the Die event, the argument word Peterson and its trigger word murder are in different clauses, and there is no direct dependency path between them. Thus it is difficult to find the Agent role between them using tradi-tional dependency features. In addition, extracting such features depends heavily on the performance of NLP systems, which could suffer from error propagation.
 understand the semantics of each word but also need to exploit internal semantic rela-tion over the entire sentence such that the Die event results in Arrest-Jail event. Recent improvements of Recurrent Neural Networks (RNNs) have been proven to be efficient for learning semantics of words, which take the impact of the context into consider-ation [12,14]. Unfortunately, the recurrent structure of RNNs also make it endure the  X  X anishing gradients X  problem, which makes it difficult for the RNN model to learn long-distance correlations in a sequence [8,9]. So we use a RNN with Bidirectional Long Short-Term Memory (BLSTM) unit [10,6] to addresses this problem.
 representation of RNNs for each word directly. While in the task of event extraction, to capture the most useful information within a sentence, a representation of the entire sentence is needed, which can be acquired by applying a max-pooling layer over the RNNs [15]. However, in event extraction, one sentence may contain two or more events, and these events may share the argument with different roles, as shown in Figure 1. We apply a dynamic multi-pooling layer in our LSTM-based framework, which can capture the valuable semantics of a whole sentence automatically and reserve information more comprehensively to extract events [4].
 neural network based approaches [20,4] did not model the interaction between the can-didate arguments and predict them separately. However, these interactions are impor-tant to predict arguments. For example, in Figure 1, if we know  X  X is wife X  and  X  X nborn son X  are paralleled in the sentence, we are easy to predict they play same role in the corresponding event. Thus we propose a tensor layer to explore the interaction between candidate arguments automatically.
 Pooling Long Short-Term Memory Tensor Neural Networks (BDLSTM-TNNs) for event extraction, which can automatically induce valuable clues for event extraction without complicated NLP preprocessing and predict candidate arguments simultane-ously. We propose a Bidirectional Long Short-Term Memory Network with Dynamic Multi-Pooling (BDLSTM) to extract event triggers and arguments separately, which can capture meaningful semantics of words with taking the context words into consid-eration and capture more valuable information for event extraction within a sentence automatically. And we devise a tensor layer, which aims to explore interaction between candidate arguments and predict them jointly. We conduct experiments on a widely used ACE2005 event extraction dataset, and the experimental results show that our approach outperforms other state-of-the-art methods. In this paper, we focus on the event extraction task defined in Automatic Content Ex-traction 1 (ACE) evaluation, where an event is defined as a specific occurrence involving participants. First, we introduce some ACE terminology to understand this task more easily:  X  Event mention : a phrase or sentence within which an event is described, including  X  Event trigger : the word that most clearly expresses the occurrence of an event.  X  Event argument : an entity mention, temporal expression or value (e.g. Job-Title)  X  Argument role : the relationship between an argument to the event in which it par-with specific subtypes and their arguments. The upper side of Figure 1 depicts the event triggers and their arguments for S3 in Section 1. ACE defines 8 event types and 33 subtypes, such as Arrest-Jail or Die . In this paper, event extraction is formulated as a two-stage, multi-class classification via Bidirectional Dynamic Multi-pooling Long Short-Term Memory Networks (BDL-STM) with the automatically learned valuable clues. The first stage is called trigger classification , in which we use a BDLSTM to classify each word in a sentence to iden-tify trigger words. If one sentence has triggers, the second stage is conducted, which applies a similar BDLSTM to assign arguments to triggers and align the roles of the arguments. We call this argument classification . To explore interaction between candi-date arguments and predict all candidate arguments simultaneously, we design a tensor more complicated, we first describe the methodology of how predict each candidate argument separately by BDLSTM in Section 3.1  X  3.4 and then illustrate the difference between the BDLSTMs that are used for trigger classification and those used for ar-gument classification in Section 3.5. Finally, we introduce the Bidirectional Dynamic Multi-Pooling Long Short-Term Memory Tensor Neural Networks (BDLSTM-TNN) and illustrate how it predict all candidate arguments jointly in Section 3.6  X  3.7 . marily involves the following three components: (i) context-aware word representation; (ii) dynamic multi-pooling layer; (iii) argument classifier output layer. 3.1 Context-aware Word Representation Using LSTM Networks The semantics of words and relations between words serve as important clues for event extraction [11,17]. RNNs with LSTM unit is a good choice to learn the context-aware semantics [15,12]. In event extraction, both forward and backward words are important for understanding a word. Thus, we propose to use a Bidirectional Long Short-Term Memory (BLSTM) architecture for representing words.
 Context-aware Word Representation This subsection illustrates the components of the context-aware word representation. As shown in Figure 2, it primarily involves the following four parts: (i) Word embedding, which aims to capture the background of a word; (ii) left context and right context, which help to disambiguate the semantics of the current word and reveal the relations between the words; (iii) position information, which aims to specify which words are the predicted trigger or candidate argument; (iv) predicted event type, which aims to embed the predicted event type from the stage of trigger classification into the argument classification. the predicted trigger word t and the candidate argument a , which are transformed by looking up word embeddings. The word embeddings are trained by a Skip-gram model [2] from a significant amount of unlabeled data. w context c r ( w i ) . As shown in Figure 2, the context of each word is different. The left-Function f is the operation of LSTM, which we will illustrate in the next section. The right-side context c r ( w i ) is calculated in a similar manner, as shown in equation (2). tance of the current word to the predicted trigger or candidate argument as [4] did. in the position information.
 catenation operator. our model may be better to disambiguate the meaning of the word w i and the semantic relations to others words, especially the interactions to the predicted trigger and the candidate argument. We apply a linear transformation together with the tanh activation function to x i and send the result to the next layer. matrix y (1)  X  R n  X  m and m is the dimension of y (1) i .
 Long Short-Term Memory Networks Traditional RNNs (Figure 3) are able to process input sequences of arbitrary length via the recurrent application of a transition function on a hidden state vector h t . Commonly, the RNN unit calculates the hidden states via the recurrence equations 5. where  X  is an element-wise non-linearity, such as a sigmoid or hyperbolic tangent, x t is the input, and h t is the hidden state at time t . to learn long-distance correlations in a sequence, because components of gradient vector can decay exponentially over long sequences [8,3]. The LSTM architecture [10] provide a solution by incorporating a memory unit that allows the network to learn when to forget previous hidden states and when to update hidden states given new information. In this paper, we use the LSTM unit as described in [23]. As shown in Figure 3, the modulation gate u t , a forget gate f t , an output gate o t , a memory cell c t and a hidden state h t . Where x t is the input at the current time step, see [23] for details. As shown in Figure 2, we use a BLSTM, which consists of two LSTMs that are run in parallel. One on the input sequence and the other on the reverse of the input sequence. At each time step, the forward hidden state aligns to c l ( w t ) , and the backward hidden state aligns to c ( w t ) . This setup allows the model to capture both past and future information. 3.2 Dynamic Multi-Pooling The size of the layer y (1)  X  R n  X  m depends on the number of tokens in the input sen-tence. In order to apply subsequent layers, traditional RNNs [15] apply a max-pooling operation, which take each dimension of y (1) as a pool and get one max value for each dimension. However, single max-pooling is not sufficient for event extraction. Because one sentence may contain two or more events, and one argument candidate may play a different role with a different trigger [4]. To solve this problem, [4] devise a dynamic multi-pooling layer for Convolutional Neural Networks (CNNs). We apply a similar layer to our BLSTM. We split each dimension of y (1) into three parts according to the candidate argument and predicted trigger in the argument classification stage. expressed as Eq. 6, where 1  X  i  X  3 and 1  X  j  X  m . considered as valuable clues and contains the key semantics of the whole sentence to classify argument precisely. 3.3 Output To compute the confidence of each argument role, the vector y (2) is fed into a classifier. output of the network, where n 1 is equal to the number of the argument role including the  X  X one role X  label for the candidate argument which don X  X  play any role in the event. For regularization, we also employ dropout[7] on the penultimate layer. 3.4 Training We define all of the parameters for the stage of argument classification to be trained as predicted event type embeddings pe , transformation matrixes parameters W 1 ,b 1 ,W 2 ,b 2 , and the parameters of forward-LSTM lf and backward-LSTM lb .
 the i -th component O i contains the score for argument role i . To obtain the conditional probability p ( i | x, X  ) , we apply a softmax operation over all argument role types: objective function as follows: stochastic gradient descent over shuffled mini-batches with the Adadelta [24] rule. 3.5 Model for Trigger Classification The method proposed above is also suitable for trigger classification, but trigger clas-sification only need to find triggers in the sentence, which is less complicated than ar-gument classification. Thus we can used a simplified version of BDLSTM. In context-aware word representation of trigger classification, we do not use the position of the candidate argument. Furthermore, instead of splitting the sentence into three parts, the sentence is split into two parts by a candidate trigger. Except for the above change, we classify a trigger as the classification of an argument. 3.6 BDLSTM-TNNs Though BDLSTM can extract events from plain texts with automatically generate fea-tures, it cannot explore the interaction between candidate arguments, which is impor-rectional Dynamic Multi-Pooling Long Short-Term Memory Tensor Neural Networks (BDLSTM-TNNs) to solve this problem in argument classification and we use a same BDLSTM in trigger classification as illustrate above. To model the interaction between candidate arguments, we devise a tensor layer, which approved to be effective for cap-turing multiple interactions among words [21]. As shown in Figure 4, we predict all candidate argument simultaneously. For each candidate argument, we use y (2)  X  R n f get from BDLSTM as input. n f is the dimension of y (2) . If there are n c candidates, the layer, where n t is the length of the interaction vector. The internal relation I between candidate arguments is calculated as follows: between candidate arguments. To make the model independent of the number of can-didate arguments, we apply a max-pooling layer to capture the most valuable internal relation as follows: date arguments A and the interaction I max as a whole feature F  X  R n c  X  ( n t + n f ) . Each candidate argument has a corresponding feature F i . Finally the output O i is computed as follows: for argument i to be assign as role j . 3.7 BDLSTM-TNNs Training We define all of the parameters for the stage of argument classification to be trained as are 3-way tensor T , transformation matrixes parameters W 3 ,b 3 , and corresponding pa-rameters of BDLSTM. Given an input instance x i of sentence s i with the trigger word tw the predicted role sequence and Y ( x for x i . Thus as Eq. 12 shown, O  X  y ij ( k ) is the score of j -th candidate argument to be assigned with role k for x i , and we compute a sentence level score as follows: loss  X  ( y i ,  X  y i ) as follows: ( x i ,y i ) , we can then define the objective function as follows: To increase the score of the correct role sequence y i and decrease the highest score of incorrect sequence  X  y , we minimizing the object sated above.
 4.1 Dataset and Evaluation Metric We utilized the ACE 2005 corpus as our dataset. For comparison, as the same as velopment set with 30 other documents randomly selected from different genres and the rest 529 documents are used for training. Similar to previous work [4,17,11,18], we use the following criteria to judge the correctness of each predicted event mention:  X  A trigger is correct if its event subtype and offsets match those of a reference trigger.  X  An argument is correctly identified if its event subtype and offsets match those of  X  An argument is correctly classified if its event subtype, offsets and argument role Finally we use Precision ( P ) , Recall ( R ) and F measure ( F ) as the evaluation metrics. 4.2 Our Method vs. State-of-the-art Methods We select the following state-of-the-art methods for comparison. 1) Hong X  X  baseline is the system proposed by [11], which only employs basic human-designed features; 2) Liao X  X  cross-event is the method proposed by [18], which uses document-level information to improve the performance of ACE event extraction; 3) Hong X  X  cross-entity is the method proposed by [11], which extracts event by using based system; 4) Li X  X  structure is the method proposed by [17], which extracts events based on structure prediction. It is the best-reported structure-based system; 5) Chen X  X  DMCNN is the method proposed by [4], which extracts events based on convolutional neural networks. It is the best-reported neural-based system.
 search. Specifically, in the trigger classification, we set the batch size as 150 , n f as 100, and the dimension of the pi as 5. In the argument classification, we set the batch size as 50, n f as 150, n t as 180 and the dimension of the pi and pe as 5. We train the word embedding using the Skip-gram algorithm 2 on the NYT corpus 3 . can see that the BDLSTM-TNNs model achieves the best performance among all of the compared methods in the stage of argument classification and get the best performance among all of methods expect for DMCNN method. BDLSTM-TNNs can improve the best F 1 [4] in the state-of-the-arts for argument classification by 0 . 6% and competitive result for trigger classification. Moreover, we get larger gain compared with traditional methods [17,11,18]. This demonstrates the effectiveness of the proposed method. We believe the reason is that the clues we automatically learned can capture more mean-ingful semantic regularities of words.
 4.3 RNN vs. LSTM vs. BLSTM vs. DLSTM vs. BDLSTM This subsection studies the effectiveness of our context-aware word representation mod-els. As shown in Table 2. BLSTM is described in section 3.1 which use both forward LSTM and Backward LSTM with max pooling layer. LSTM only use one forward LSTM. RNN use a traditional RNN instead of LSTM and BDLSTM apply a dynamic multi-pooling instead max pooling in BLSTM. As shown in results, the methods based on LSTM(LSTM, BLSTM, DLSTM and BDLSTM) makes significant improvements compared with the RNN baseline in the classification of both the trigger and argument. It demonstrated that LSTM is more powerful than RNN for event extraction. Moreover, a comparison of BLSTM with LSTM illustrates that BLSTM achieves a better perfor-mance. We can make a same observation when comparing BDLSTM with BLSTM. It proves that both forward and backward of words are important to understand the se-mantic of a word, and the dynamic multi-pooling layer is effective for capture more valuable information to extract an event when using recurrent neural networks. 4.4 Effect of Tensor layer In this subsection, we prove the effectiveness of the tensor layer for exploring the in-teraction between candidate arguments. Because we used the same BDLSTM in trigger classification, we only evaluate its performance for argument classification. Results are shown in Table 3. It proves that tensor layer is useful to capture the internal relation between candidate arguments. Specifically, BDLSTM-TNNs gain a 5 . 5% improvement for argument identification and 3 . 8% improvement for argument classification. Event extraction is one of important topics in NLP. Many approaches have been ex-plored for event extraction. Nearly all of the ACE event extraction use supervised paradigm. We further divide supervised approaches into feature-based methods, structure-based methods and neural-based methods.
 classification clues into feature vectors. [1] uses a set of traditional features (e.g., full dence from related documents with local decisions for the event extraction. To capture more clues from the texts, [5,18,11] proposed the cross-event and cross-entity infer-ence for the ACE event task. Although these approaches achieve high performance, feature-based methods suffer from the problem of selecting a suitable feature set when converting the classification clues into feature vectors.
 dicting the structure of the event in a sentence. [19] casted the problem of biomedical event extraction as a dependency parsing problem. [17] presented a joint framework for ACE event extraction based on structured perceptron. These methods yield relatively high performance. However, the performance of these methods depend strongly on the quality of the designed features and endure the errors in the existing NLP tools. automatically generating features without using complicated NLP tools [4,20]. But they did not consider interaction between candidate arguments and they predicted all candi-date arguments separately. This paper proposes a novel event extraction method (BDLSTM-TNNs), which can automatically extract valuable clues from plain texts without complicated NLP prepro-cessing and predict candidate arguments simultaneously. A context-aware word repre-sentation model based on BDLSTM is introduced to capture semantics of words. In addition, a tensor layer is devised to capture interaction between candidate arguments and predict them jointly. The experimental results prove the effectiveness of the pro-posed method.
 This work was supported by the Natural Science Foundation of China (No. 61533018), the National Basic Research Program of China (No. 2014CB340503) and the National Natural Science Foundation of China (No. 61272332). And this work was also sup-ported by Google through focused research awards program.

