 Wikipedia is the largest monolithic repository of human knowledge. In addition to its sheer size, it represents a new encyclopedic paradigm by interconnecting articles through hyperlinks. However, since these links are created by hu-man authors, links one would expect to see are often miss-ing. The goal of this work is to detect such gaps automat-ically. In this paper, we propose a novel method for aug-menting the structure of hyperlinked document collections such as Wikipedia. It does not require the extraction of any manually defined features from the article to be aug-mented. Instead, it is based on principal component anal-ysis, a well-founded mathematical generalization technique, and predicts new links purely based on the statistical struc-ture of the graph formed by the existing links. Our method does not rely on the textual content of articles; we are ex-ploiting only hyperlinks. A user evaluation of our technique shows that it improves the quality of top link suggestions over the state of the art and that the best predicted links are significantly more valuable than the  X  X verage X  link al-ready present in Wikipedia. Beyond link prediction, our algorithm can potentially be used to point out topics an article misses to cover and to cluster articles semantically. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  linguistic processing ; I.5.4 [ Pattern Recognition ]: Applications X  text processing Algorithms, Experimentation Data Mining, Link Mining, Graph Mining, Wikipedia, Prin-cipal Component Analysis
In 1407, in times of the Ming dynasty, two thousand Chi-nese scholars assembled the Yongle Encyclopedia, a corpus that was to hold the record as the largest coherent repository of human knowledge for 600 years. Things changed when the English version of Wikipedia crossed the two million ar-ticle mark in 2007. Not only is it now the record-holder in terms of size, it also constitutes a structural paradigm shift. Whereas traditional encyclopedias are sequential, i.e. ordered along alphabetical, topical, or historical lines, Wi-kipedia has a hypertextual graph structure, in which every article is connected to hundreds or even thousands of other articles by means of hyperlinks placed directly on the words they strive to explain (the so-called anchors ).

The daunting number of two thousand authors working on the Yongle Encyclopedia is also surpassed by Wikipedia, as every Internet user can potentially contribute. To maintain a consistent degree of quality, authors are encouraged to adhere to a Manual of Style [19, 18], which stipulates, among many other things, the following: However, since humans are not flawless and the experience level varies widely among contributors, articles deviate fre-quently from these rules, which affects the textual content of articles as well as the hyperlinks they comprise. Conse-quently, human authors often forget to add links that should be there according to the editing guidelines. It would be de-sirable to detect such missing links automatically because it could enhance the browsing experience significantly. Ar-tificial intelligence and data mining programs that exploit Wikipedia X  X  link structure would equally profit from a data set that has been improved this way.

In this paper we present an algorithm that has the capa-bility of finding missing links in Wikipedia. As an example, consider the article about Karl Marx . It misses essential connections to other relevant articles, for instance it con-tains no links to Soviet Union or Proletarian revolu-tion . Our method is capable of predicting these links, as Suggested link target Anchors Gain
Socialism socialist, 1032.9 Soviet Union Soviet Union 939.7 Democracy democratic 892.4 Social democracy Social-Democratic 826.2 Jew Jewish, Jews 774.9 State state, states 734.4 Slavery slavery 726.3 Politics political, politics 702.3
Proletarian revolution proletarian 667.8
Property property, 663.4 Table 1: Top 10 suggestions for missing links to be added to the article about Karl Marx. Anchors are phrases on which the link can be placed.  X  X ain X  is the score for the suggestion (cf. Section 4). well as others. The top 10 suggestions are listed in Table 1. (The link to Socialism has actually been added to the online Karl Marx article since March 2009, the date of our local working copy of Wikipedia.)
We use a well-known technique called principal compo-nent analysis (PCA) in order to enrich existing articles with new links. Our approach can be viewed as using generaliza-tion from existing data in order to align articles to a more uniform linking policy. The intuition underlying our work can be described as cumulative analogy . Consider for in-stance Chuuk, Kosrae, Pohnpei, and Yap, the four states forming the Federated States of Micronesia. If most articles that link to Chuuk , Kosrae and Pohnpei also link to Ya p then another article that already links to Chuuk , Kosrae and Pohnpei but not to Ya p should probably be modified by adding that missing link X  X rovided the word  X  X ap X  occurs in the article.
 The remainder of this paper is structured as follows. In Section 2 we summarize previous related work in terms of problem domain and methodology. Section 3 introduces Wi-kipedia X  X  adjacency matrix, the data structure serving as input to our algorithm. Then, in Section 4, we describe di-mensionality reduction, and PCA in particular, and give an intuitive explanation of how and why it works in our setting. Section 5 provides the algorithm and describes how we make it computationally tractable given the sheer size of the input matrix. We demonstrate the quality of our approach in Sec-tion 6, by showing that it outperforms the previous state of the art in a human user evaluation. Section 7 discusses the differences between our algorithm and previous work, and points out synergetic effects that might result from combin-ing them; we also discuss implications of our work that go beyond the specific problem of link suggestion. In Section 8 we conclude and discuss future research directions.
There have been several attempts to tackle the problem of suggesting links for Wikipedia.

The closest to our approach is the one developed by Adafre and de Rijke [1]. Like ours, it can enrich articles that already contain some outgoing links and is based on the structure of the Wikipedia link graph. The method consists of two steps. First, it identifies a set of articles which are similar to the input article. Then, the outgoing links that are present in the similar articles but not in the input article are suggested to be added to the input article. A link is only suggested if its anchor text in the similar article is also found in the input article.
 In step one, similarity is defined in terms of incoming links. Intuitively, given two articles, if it is often the case that the same page refers to both articles, then the two articles will be considered similar. The actual implementation is more complicated, consisting of several steps harnessing the indexing feature of the custom search engine Lucene [3].
Another, more recent method was proposed by Mihalcea and Csomai [7]. It differs from [1] and the work presented here in that its input is a piece of plain text (the raw content of a Wikipedia article or any other document). It operates in two stages: detection and disambiguation. First, the algo-rithm decides which phrases should be used as link anchors, then it finds the most appropriate target articles for the link candidates.

To detect link candidates, the best method they tried com-putes the link probability of candidate phrases and selects the top 6% of them. The link probability of an n -gram T is defined as the number of Wikipedia articles containing T as a link anchor divided by the number of articles contain-ing T . It is the prior probability of T being used as a link anchor given that it appears in an article. For instance, the n -gram  X  X ig truck X  has a link probability of 0%, whereas  X  X n-ternet X  has link probability 20%, i.e. every fifth article that mentions the Internet contains a link to its article. In this approach,  X  X nternet X  is likely to be linked again, while  X  X ig truck X  is considered to not be a useful link anchorage.
Once the anchors have been chosen, disambiguation is key, since many phrases have several potential meanings. For in-stance, the phrase  X  X onk X  will refer most of the time to a male nun and should point to the article Monk ,whereas in a jazz-related article, it should probably link to Thelo-nious Monk . To decide the best sense of a phrase, Mihalcea and Csomai extract local features from surrounding text and train a machine learning classifier from Wikipedia articles, which can serve as labeled examples since the links they contain are already disambiguated. The features are a set of words occurring frequently in the document, as well as the three words to the left of the candidate, the three words to its right, and their parts of speech.

A third method, proposed by Milne and Witten [10], con-sists of the same steps, but in swapped order. They first find the best sense of each phrase and only then decide which phrase to use as a link anchor.

To disambiguate a term, they look up the articles to which it points when it occurs as a link anchor in Wikipedia. They call the frequency of each potential target article (or sense) its  X  X ommonness X . Then they find all the unambiguous terms in the document; these are the terms that link to the same target article regardless where they occur as anchors in Wi-kipedia. Then they compute the average semantic  X  X elat-edness X  between the candidate term and the unambiguous terms. While any relatedness measure could be plugged in, theirs is itself derived from Wikipedia [9]. Finally, they train a machine learning classifier to combine commonness and relatedness and predict the most appropriate sense of each phrase.

After all phrases have been disambiguated, Milne and Witten decide which of them to use as link anchors, based on several features of the input article. These include, among others, the link probability of the candidate, its semantic relatedness to the context, how often it appears in the doc-ument, and in what positions. Again, all features are com-bined to train a machine learning classifier. This approach outperforms the predecessor due to Mihalcea and Csomai.
Our technique is different from the two outlined last in that it strives to complete documents that already contain some links to Wikipedia. The input document may be a Wikipedia article or, alternatively, a piece of text that has been  X  X reprocessed X  by one of the above methods. In this paper, we concentrate on the former case.

While our paper and the approaches just summarized deal with very similar problems, the methodology we propose is rather different. We draw o n work done by the common-sense reasoning community, most notably the paradigm of cumulative analogy explained in the introduction, which is due to Chklovski [5]. There, the goal is to infer new com-monsense facts by analogy. A typical example would be:  X  X  know many things with feathers, wings, and a beak. So a new thing I find that has feathers and wings probably has a beak as well. X  Chklovski proposed  X  X and-coded X  rules for the cumulative analogy heuristic. Speer et al. [13] implemented this type of reasoning automatically by doing principal com-ponent analysis. Our technique is inspired by theirs, but we transfer it to the novel domain of link mining.
The hyperlink structure of Wikipedia is captured com-pletely in its adjacency matrix. Let N be the number of articles. Then the adjacency matrix has N rows and N columns. The entry at position ( i, j )is1ifarticle i has a link to article j and 0 otherwise.
 In this work we modify the adjacency matrix in two ways. First, we weight columns according to how many articles link to the respective article. This is useful because links pointing to an article that is rarely linked are more informative than links to articles that are linked from nearly everywhere else. For instance, around 320,000 articles link to United States of America , while only 500 link to Federated States of Micronesia . The fact that an article links to Federated States of Micronesia is much more characteristic than that it links to United States of America .Inparticular, we use the weighting scheme of [9], as follows. Let W be the weighted adjacency matrix. Its value at position ( i, j )is where d j is the number of articles containing a link to j . Thus, the term  X  log( d j /N ) is the information content of the event  X  X icking an article that links to j  X  X henwedraw a Wikipedia article uniformly at random.

After weighting columns this way, we center W around the mean by subtracting the respective column mean from each column. This results in the weighted, mean-centered adja-cency matrix A , whose columns all have mean 0, a technical requirement of the mathematical methods we are using.
In this section we will show how the reasoning scheme of cumulative analogy, which we presented in the introduction, is elegantly implemented by principal component analysis (PCA) [11], without any need for coding it explicitly. In order to make the intuition clear, we will first give a brief summary of how PCA works in the context of the Wikipedia adjacency matrix.

In PCA, the rows of the input matrix A are considered data points, while the columns are taken as features. There-fore, an article (a row in A ) is characterized by its links to all the other articles, which makes the outlinks the features of the input matrix. The articles form a cloud of points in an N -dimensional vector space (let us call it article space ). Since A is mean-centered, the cloud is centered around the average article sitting in the origin.

This cloud is not uniformly distributed but rather sprawl-ing in certain directions and squished in others. This is due to correlations among the points: articles that all link to one specific article often have other particular outlinks in com-mon. For example, articles with links to Adam will often link to Eve as well. PCA finds the directions along which the point cloud is spread out most, i.e. along which articles tend to differ most from the average article. Those directions are called principal components . They are vectors in the N -dimensional article space pointing away from the average article in the origin; by convention, they are normalized to alengthof1.

The principal components found are orthogonal. Hence, an appealing geometric way of thinking about PCA is as a rotation of the axes of the coordinate system such that the spread (more formally, the variance) of the data is k -th largest along dimension k ; it then computes the co-ordinates of each point in the new basis formed by the principal com-ponents. The principal components themselves can be con-sidered  X  X antasized X  articles (since they are points in the N -dimensional article space).

Mathematically, the principal components are the eigen-vectors of the data covariance matrix. Hence, we call them eigenarticles, to emphasize that they are eigenvectors and points in article space. The new space resulting from the rotation is called eigenspace .

Computing the coordinates of an article a i (a row vector of A ) in eigenspace amounts to projecting it onto the eigen-space basis vectors, i.e. onto the eigenarticles e k . Then the vector p i of projections is the eigenspace representation of a : In matrix notation this can be written succinctly as where projection vector p i is the i -th row of P and eigenar-ticle e k is the k -th row of E .

Since PCA performs a rotation, E T is a rotation matrix, i.e. E T E is the identity matrix. Thus, the reverse projec-tion from eigenspace back into article space (the so-called reconstruction of A )is
Expanding (4), a single entry of A is computed in the reconstruction as follows: Entry a ij is large when there are many eigenarticles e k (a) are important components of a i in eigenspace (result-ing in large p ik ) and that (b) link themselves to article j (resulting in large e kj ). Remember that eigenarticles live in article space and have  X  X antasized X  outlinks to  X  X eal X  arti-cles, e kj being the weight of the k -th eigenarticle X  X  link to article j .

The reconstruction of A as PE = AE T E is exact. How-ever, getting an exact reconstruction is not useful from our point of view. What we would like, intuitively, is to find first the articles similar to the article of interest. Then, we would like to propose new links , which are not present in the origi-nal matrix A , but which are suggested because they appear in similar articles. This is an important difference compared to more traditional applications of PCA, in which one wants to obtain a reconstruction that is as exact as possible. We actually want to obtain a reconstruction that enriches the original data.

In order to achieve this goal, we must first ensure that we  X  X orget X  some information while dwelling in eigenspace, just like in the case of traditional dimensionality reduction. First, we project an article a i from article space into eigen-space, obtaining its eigenspace representation p i (cf. (2)). Now we  X  X hrink X  p i by setting to zero all components p ik with k&gt;K for some fixed K . These were the projections onto eigenarticles along whose direction the variation in the data is small, so it can be considered noise. By shrinking p we eliminate that noise. Now we can reconstruct a i approx-imately by projecting it back into article space (cf. (4)): where P K consists of the first K columns of P and E K of the first K rows of E .Matrix A K still has the same dimensions as A , but its entries have changed values, since (6) amounts to replacing N with K in (5):
After the back-projection we compare a ij to a K ij .Ifthere was no link between articles i and j originally, but a K ij then our method predicts that the link should be added.
To see how this algorithm naturally incorporates the cu-mulative analogy scheme, let us look at the system in action. Consider an article i which should link to article j but does not. Now consider also a set of articles A which are similar to article i , in terms of the other outgoing links. These arti-cles will reside in a part of article space similar to i ,sothey will project similarly onto eigenarticles (because a rotation will preserve the neighborhood structure of these articles). If many of the articles in A contain j as an outlink, the eige-narticles on which they cause a significant projection will also link to j . These eigenarticles will cause the value a to increase, compared to a ij , so article j will be suggested as a link from i as well. Its absence in the original article is, in this case, directly attributed by our method to noise, caused by projecting onto insignificant eigenarticles.
Note that no heuristic is involved in our method. It sim-ply exploits the statistical properties of the set of already existing links. We emphasize again the particular flavor of the use of PCA here (as also in [13]). Typical PCA ap-plications strive to minimize the reconstruction error while compressing the data through dimensionality reduction (e.g. Algorithm 1 Wikipedia link suggestion Input: Article i , represented by its outlinks a i ; minimum link probability  X  Output: Link suggestions for article i , in order of decreas-ing quality Static: Eigenarticle matrix E K p i  X  a i E T K (projection into reduced eigenspace) a K i  X  p i E K (projection back into article space) g i  X  a K i  X  a i (the reconstruction gain vector)
S X  X  X  (set of link candidates) for n -grams T of text of article i do end for for j  X  X  , in order of descending g ij do end for [14]). In our paradigm, the  X  X rror X  is exactly what we want! To underline this, we should speak of reconstruction gain or generalization gain rather than reconstruction error.
Pseudocode for the method we just described is provided in Algorithm 1. The steps laid out in Section 4 are followed directly. The article to be augmented is projected into the reduced eigenspace, then back into article space. The output is a list of link suggestions, ordered by the reconstruction gain of the links, i.e. by how much more weight they have after the projections than before.

Of course, a link can be suggested only if the appropriate anchor term occurs in the text of the source article. In order to prune away nonsense terms and stopwords from the beginning, and thus speed up the algorithm, we consider as potential anchors only n -grams whose link probability (cf. Section 2) is above a specified threshold  X  .Avalue of  X  = 6.5% was found to afford optimal performance [10], which is why we use this threshold in our implementation.
We ran our algorithm on two versions of Wikipedia: one consisting of a carefully selected small subset of important articles, the other being a recent data dump of the entire Wi-kipedia. The complete version is three orders of magnitude larger than the small selection, so we had to apply additional tricks in order to make PCA computationally tractable on the former. We now describe both our implementations.
The  X 2008/9 Wikipedia DVD Selection is a free, hand-checked, non-commercial selection from Wikipedia, targeted around the UK National Curriculum and useful for much of the English speaking world. X  [16] It is edited by SOS Chil-dren X  X  Villages UK and contains 5,503 articles (so N = 5,503) that can serve as a free alternative to costly encyclopedias. As most Wikipedia articles are not present in it, the major-ity of links had to be removed, too. All links pointing to articles included in the collection were kept; redirects were resolved (e.g. links to M  X  unchen were changed to Munich since the two are different titles of the same article) [4].
Using Matlab X  X  built-in functions, computing the eigenar-ticles and implementing Algorithm 1 was straighforward. For this data set we chose an eigenspace dimensionality of K = 256. We will describe the results in Section 6.2.
While the Wikipedia Selection for schools serves well as a proof of concept and for evaluating the potential of the technique, the full version of Wikipedia is certainly more interesting, for several reasons.
 First, Wikipedia X  X  live online version is consulted by many Internet users on a daily basis. So, if our method can im-prove full Wikipedia, it will have much more traction than if it worked only on a small subset of articles.

Second, live Wikipedia is evolving constantly, articles be-ing added or modified constantly. Thus, if our method is applicable to full Wikipedia, then it can be used by authors every day to find links they have probably forgotten to in-clude in the articles they are writing.

Third, Wikipedia contains over two million articles (three orders of magnitude more than the school selection). In order to cope with such a challenging amount of information, our algorithm really has to scale well.

Fourth, previous methods used full Wikipedia as a data set, and we want to compare the performance of our tech-nique directly to them.

We downloaded the Wikipedia snapshot of March 6, 2009 [17], and indexed it in a database to facilitate quick look-up of basic information such as the set of links contained in an article or pointing to it. The database contains 20 GB of information. To create, fill and access it, we used a Java toolkit called WikipediaMiner [8], written by David Milne.
The data dump contains over six million pages, 2,697,268 of which are actual articles (the rest are, among others, cate-gory, redirect, or disambiguation pages). Now N = 2,697,268 and consequently the N  X  N adjacency matrix A would oc-cupy 29 terabytes of memory (assuming 32-bit floating point precision); a sparse representation is useless as well, because mean-centering turns most zeros of the sparse original adja-cency matrix into negative numbers.

So, in order to make PCA and thus our method tractable on full Wikipedia, we have to apply some further tricks.
First, we reduce the size of W ,theweighted,non-mean-centered adjacency matrix. In terms of columns, we keep only those associated with articles that have at least 15 in-coming and 15 outgoing links. This way we eliminate ar-ticles about the most obscure topics X  X eemingly a majority of Wikipedia X , reducing the width of the data matrix to w = 468,510 (17% of the original width). The same method of constraining the set of articles is used by [6]. Remember that columns are the features of the data matrix, so dis-carding 83% of the columns could be described as feature selection.

To compress the height of W , we keep a row only if the article it represents is about a topic for which the schools selection contains an article as well. This reduces the height of the data matrix to h = 5,503 (0.2% of the original height). Recall that rows are the data points of the data matrix, so discarding 99.8% of the rows amounts to shrinking the set of training samples for our algorithm aggressively, to only the most important articles (as determined by this other source of information). We will show in Section 6.1 that restricting the set of training articles that drastically does not impede our ability to suggest links for new articles that have not been encountered during training.

After decreasing the size of W , we mean-center it and obtain the h  X  w matrix  X  A . This matrix has a lot more columns than rows, which makes it amenable to a trick used in a seminal image processing paper on  X  X igenfaces X  [14]. As mentioned in Section 4, the eigenarticles are the eigenvectors of the data covariance matrix, which can be written as  X  A By definition, this means for an eigenarticle e k with associated eigenvalue  X  k .
Now consider the eigenvectors of another matrix,  X  A  X  A T Eigenvector v k fulfills for eigenvalue  X  k . Left-multiplying by  X  A T yields
So each  X  A T v k is an eigenvector of  X  A T  X  A .Moreprecisely
The crucial observation is that  X  A  X  A T is h  X  h , i.e. 5,503 5,503 in our case, which means it fits into memory, making it possible to compute the eigenvectors v k efficiently. Sub-sequently, we can find eigenarticle e k simply as  X  A T v
Once the eigenarticles have been computed (we used ei-genspace dimensionality K = 1,000 for the full Wikipedia data set), Algorithm 1 can be deployed just as for the small schools selection.
We evaluated the performance of our link suggestion al-gorithm by querying human raters on Amazon Mechanical Turk [2]. Mechanical Turk is an online platform on which  X  X e-questers X  can post questionnaires (among many other types of tasks), which are subsequently completed for a typically small amount of money by  X  X orkers X , regular Internet users who have registered with the system. It has recently been shown that non-expert labels obtained through Mechanical Turk agree very well with gold-standard expert annotations for natural language tasks [12], which justifies using it for our purpose.

In each rating task we presented the human contributor with the text of a randomly selected Wikipedia article about atopic T . The article text still contained the original out-going links. The task description read as follows:
In order to be able to compare our algorithm to Milne and Witten X  X , the definition of a useful link is directly copied from their instructions to human raters [10], which in turn capture Wikipedia X  X  linking policy [19].

The four outgoing links between which raters had to choose were the following. 1. The top link suggestion S made by our method, using 2. The top link suggestion S mw made by Milne and Wit-3. A pre-existing link S pre already present in article T , 4. A link S rnd to an article that is not linked from T but
The order of the four choices was randomized, to prevent any bias.

We evaluated the performance on a set of 181 articles ran-domly picked from the set of articles not used in computing the eigenarticles, to avoid overfitting and test whether our algorithm generalizes well to unseen data; call this set the test set . We constrained our random selection to articles with at least 100 incoming and at least 100 outgoing links. The reasoning is similar to that behind our choice of the columns of  X  A (cf. Section 5.2): we wanted to ensure that the articles were not about very obscure topics, so human raters would not have to read the article text in depth to be able to make an informed decision.

To facilitate the performance analysis, we considered only articles on which our method and that of Milne and Witten did not agree. (Out of the 200 articles we initially tried, the methods agreed on 8%.)
Each task was completed by six different raters, so we gathered 6  X  181 = 1,086 votes. As a safeguard against participants who might potentially have clicked randomly rather than made an informed decision, we implemented a voting scheme that counts a vote only if it agrees with at least two others on the same task, which resulted in a set of 660 effective votes. This heuristic is justified a posteriori by the low performance of the random baseline, which is according to our expectations.

The results are summarized in Figure 1 (all gathered data can be found online [15]). Our method won most votes (36%), followed by Milne and Witten (27%), the random pre-existing links (25%), and finally the baseline of random n -grams (11%). Figure 1: Results of the human user evaluation, in terms of percentages of votes won by the different link types (explained in Section 6.1). The error bars show the 95% confidence intervals.

Thus, our method outperforms the previous state of the art. Our top suggestion is considered best 9% more often than theirs. A difference of at least 4% is statistically signif-icant at the p&lt; 0 . 05 level (estimated by bootstrap resam-pling).

Also, the fact that our suggestions won significantly more votes than the randomly picked pre-existing links (11% dif-ference; at least 6% is significant at the p&lt; 0 . 05 level) implies that the top links our method finds are better than the average human-added link: we do not just find minor links that happen to have some relevance for the article be-ing augmented; instead, we find important links that the human authors forgot to include.

This quality of suggestions is reached on a set of test ar-ticles that did not partake in the eigenarticle calculation, which implies that our algorithm generalizes well to articles it was not trained from. This is crucial because it justifies selecting only a small subset of all Wikipedia articles as rows of  X 
A (cf. Section 5.2), a restriction without which PCA on the enormous adjacency matrix would be computationally infeasible.
To illustrate the effect of our technique on more than a few hand-picked examples, we augmented a complete local copy of the 2008/9 Wikipedia Selection for schools by adding the 17,000 highest ranking links suggested by Algorithm 1 (an average of approximately three new links per article). The result can be browsed online [15].

Since we have already shown that our algorithm performs very well on the full version of Wikipedia, we do not evaluate the quality of link suggestions formally on the small selection as well. Instead, we focus on a more qualitative analysis. In particular, a desirable property of the algorithm, which was not measured in the previous set of results, would be a de-cay in quality with the ranking of the suggestion; i.e. we would expect links with small reconstruction gain to be less useful. Figure 2 suggests that this is the case. To make the argument clear, we point out that the first for loop of Algo-rithm 1 considers only link candidates that could potentially be accepted because an appropriate anchor appears in the text of the source article. This is exclusively for reasons of Figure 2: Running average of the number of sug-gestions that are acceptable because an appropriate anchor for the target appears in the source article. Note that the maximum is deceptively low because the running average is taken over 10,000 consecutive ranks. efficiency. We might just as well loop over all potential tar-get articles, regardless of whether there is an apt anchor or not. This way we can first collect all link predictions and calculate later for how many of them a fitting anchor exists.
Figure 2 plots the running average of this quantity as a function of rank in the list of link suggestions. The r -axis shows the rank; the f -axis shows the fraction of suggestions for which an anchor exists in the source article, among the 10,000 suggestions up to rank r . The fact that this fraction decays as we descend in the list of suggestions means that fewer and fewer of the predicted links have an anchor in the source article, which in turn implies that the quality of suggestions decays as well: the less likely a target article is to have an anchor in the source article, the less likely it is to be a valuable suggestion. Our algorithm does not just roughly separate good from bad suggestions, it also ranks them continuously in a sensible way.

Note that the probability of a random article name ap-pearing in the text of another random article is only 1.5% (estimated from 10,000 randomly selected article pairs), sig-nificantly lower than the 14% that Figure 2 shows for sugges-tions 90,001 to 100,000. This means that not only our top suggestions are much better than random ones (as shown in Section 6.1) but that this is true even far down in our ranking.
To highlight the contributions of this research, we will now contrast it with the existing methods referenced in Section 2.
As mentioned, the technique coming closest to ours is that of Adafre and de Rijke [1], since it is based on the links rather than the text that articles contain. However, there are several important differences.

Adafre and de Rijke gauge the similarity of two articles in terms of how many incoming links they share. To augment an input article with new links, they copy links from any single article that is sufficiently similar to the input article according to this measure. Our method represents articles in terms of their outgoing links. This makes it possible to apply the cumulative analogy paradigm:  X  X f there are many articles sharing a lot of features (outlinks) among each other and with the input article, and if these articles also share a certain single feature (outlink), then the input article should have that feature (outlink), too. X  The fact that many similar articles, rather than just a single one, are required makes the method more robust to noise.

In addition to this robustness concerning where to copy from, our technique is also more careful regarding what to copy. If an article is similar enough to the input article, Adafre and de Rijke copy any of its outgoing links, as long as the appropriate anchor text occurs in the input article. On the contrary, our method works with numerical values and can thus weight outlinks with importance values (recon-struction gain).

Also, the approach we propose naturally incorporates the two steps (picking the similar articles and ranking the can-didate links before suggesting them for the input article) into one simple mathematical operation: PCA. Adafre and de Rijke X  X  first step alone seems considerably more compli-cated, involving a scheme of several rounds of querying the search engine that indexes the incoming links of each article.
Before we compare our method to [7] and [10], we will first summarize their principal properties (for more details, see Section 2) in a concise list: 1. Both methods consist of two separate phases , link 2. They rely heavily on several hand-picked features , 3. These features strive to capture the textual content
We demonstrated that we can outperform the state of the art [10] with an algorithm that elegantly integrates detection and disambiguation in one single phase . To illustrate this, it is worthwhile to point out a subtlety we have glossed over in the pseudocode of Algorithm 1. We wrote  X  X opic T  X  X n the first loop, while in fact T is an n -gram, i.e. a sequence of words, which could be ambiguous. However, mapping the n -gram to the most appropriate target article is easy: given a source article i , the PCA will already have computed a score (the reconstruction gain) for every other Wikipedia article, so to retrieve the most appropriate sense of the n -gram T in article i , we simply look at all possible senses (all articles the anchor T ever links to in all of Wikipedia) and define  X  X opic T  X  as the one with highest reconstruction gain for source article i .

Even if the features used in the two approaches make sense intuitively and turn out to work well, they still had to be defined  X  X anually X  by experts. On the contrary, our method is featureless . It merely completes the hyperlink struc-ture of a document collection by means of a mathematically sound and proven generalization technique. There is no need to  X  X orce X  the algorithm to follow Wikipedia X  X  linking policy [19] by hand-crafting features that more or less encode those rules. Our technique starts from whatever linking policy is in place X  X ost articles abide by it very closely to begin with! X  X nd enforces it where it is infringed, by eliminiating the noise such a deviation represents.

The algorithm we propose works on the hypertextual content of an article (the set of outgoing links), not on its raw text. No advanced scanning or even parsing is nec-essary, as in the two text-based methods (e.g. Milne and Witten [10] need to know at what position in the article a phrase occurs; Mihalcea and Csomai [7] even require part-of-speech tagging). We only ever inspect the article content in one trivial way, to see which n -grams it contains (and once, offline, to calculate link probabilities; but as explained in Section 5, this is not even integral to our approach but just a means of speeding up the algorithm). Our technique is based entirely on the link structure of the document col-lection. This rich source of information is left completely untapped by Mihalcea and Csomai. Milne and Witten do use link structure, but more indirectly, to compute their se-mantic relatedness measure. However, since it is used as a black box, this component could be replaced with any such measure and is not an integral ingredient of their approach.
It should be mentioned that the memory requirements of our algorithm can be rather high, depending on how one chooses the eigenspace dimensionality K , since the eigenarti-cles have to be stored in RAM. Memory usage grows linearly in K .
Content-based methods have the advantage of being able to add links to raw text rather than documents that already come with a set of Wikipedia links. This is why it is im-portant to point out that in the big picture our algorithm is not so much a rival of [7] and [10] as rather a tool to exploit dimensions of Wikipedia unaccounted for by those prede-cessors. Consequently, we conjecture that a combination of textual and hypertextual methods might have a synergetic effect: while in this paper we restricted ourselves to showing that our technique works well for suggesting links within Wi-kipedia, the method is applicable, without any changes, to any input document containing a basic set of links to Wiki-pedia. It could thus employ a text-based link suggester such as [7] or [10] as a preprocessor and fill in links those methods have missed. We conducted preliminary experiments with this approach but do not report results, for the lack of a formal evaluation.

One could even go further and couple a textual technique with our hypertextual one, in order to link a complete plain-text document collection (such as a large news story archive) to Wikipedia in three steps: first, add a basic set of links to each document by means of a text-based technique; sec-ond, compute the eigenarticles for this document collection; third, run our method on all articles to complete the link structure. Step two only serves the purpose of fine-tuning the method to the characteristics of the document collection at hand. Alternatively, the eigenarticles computed from Wi-kipedia can be used.

A synergetic effect may also be expected when our method is deployed in a feedback loop. As Wikipedia authors accept (or reject) an increasing number of link suggestions, Wiki-pedia will comply ever closer to its own linking policy, which in turn means more accurate training data for the next gen-eration of suggestions. A similar argument could be made for the text-based methods, yet it is more immediate for our approach, since it takes its own output X  X ink structure X  Table 2: Top 15 suggestions of Algorithm 1 for links to be added to the article about Statistics in the 2008/9 Wikipedia Selection for schools.  X  X ain X  refers to reconstruction gain. Links marked with a star could actually be added because the appropri-ate anchor text occurred in the source article. directly as input.
While link suggestion is useful in its own right, the reach of our technique goes beyond. Recall that, unlike the exist-ing approaches, our algorithm computes scores not only for phrases appearing in the input article but for every Wikipe-dia article. Let us take Table 2 as an example. It shows the top 15 suggestions of Algorithm 1 for the Statistics article of the Wikipedia Selection for schools. Note that many links (those not marked with a star) could not be suggested for the sole reason that there was no appropriate anchor text in the source article. It is interesting to see that, more often than not, it would be desirable if the article about Statis-tics did in fact cover the target topic. For instance, it is well possible that the author simply forgot to properly in-troduce the concepts Random variable and Probability distribution or to mention that Statistics is of foremost importance to modern Physics .

Consequently, our method can be deployed not only to suggest missing links but also to suggest missing topics. This feature, too, distinguishes our method significantly from pre-vious link suggestion methods [7, 10]. They constrain their suggestions to topics that are present in the source article in the first place, and are thus unable to predict which topics should be present. They can only decide whether a term that already appears in the article text should be used as a link anchor. Previous methods are topic detectors, ours is at heart a topic suggester. 1
The central computation of our algorithm is the projec-tion of an article onto the eigenarticles. To understand the effect of this operation graphically, let us take a quick peek into eigenspace. Figure 3 plots 200 articles selected ran-
Although Adafre and de Rijke do not mention it, we conjec-ture that their technique, too, is in principle able to suggest topics. domly from the full Wikipedia version, neglecting all higher dimensions and showing only the projections onto the two most important eigenarticles. In the notation of Section 4, the axes of the plot are e 1 and e 2 ,andarticle i has co-ordinates ( p i 1 ,p i 2 ). The dashed line shows that the plane spanned by e 1 and e 2 is  X  X emantically separable X : articles below the line are nearly exclusively about science-related topics, whereas those above the line live in the realm of the arts and humanities (history, culture, etc.).

This is a consequence of the fact that PCA finds the di-rections of largest variance in the data. Since a data point is defined by the outgoing links of an article and since articles about science topics typically have a very different set of outlinks from articles about the arts and humanities, these two classes are far apart in the subspace spanned by the first principal components of the data.

These observations suggest that our method may also be used to cluster concepts into semantic classes. Milne and Witten [9] use Wikipedia X  X  link structure to compute the semantic relatedness of pairs of concepts, but they do not include the crucial step of dimensionality reduction, which makes clustering possible by grouping related concepts in a low-dimensional subspace of the original data.

Both the detection of missing topics and semantic concept clustering are currently investigated by our group.
In this paper we present a novel approach to find missing links in document collections such as Wikipedia. We use exclusively the structure of Wikipedia X  X  hyperlink graph, in a featureless approach based on principal component anal-ysis, a mathematically sound generalization technique. It enforces the linking policy that is implicit in the entirety of Wikipedia X  X  hyperlink structure by putting additional links into those articles that contravene the linking guidelines. The method is conceptually clean, yet its simplicity does not keep it from outperforming the state of the art.
Our method draws on work done by the commonsense reasoning community, and we strive to give an intuitive ex-planation of how and why it implements the paradigm of cumulative analogy by performing dimensionality reduction. We point out implications of the approach beyond link com-pletion: It can detect topics a given Wikipedia article fails to cover, and cluster articles along semantic lines. We hope this work will inspire the application of similar techniques to other problems in graph and especially Wikipedia mining.
The Natural Sciences and Engineering Research Council of Canada (NSERC) supported this research financially. We would also like to thank David Milne for making the Wiki-pediaMiner code publicly available. [1] S. F. Adafre and M. de Rijke. Discovering missing [2] Amazon. Amazon Mechanical Turk. Website, 2009. [3] Apache. Lucene. Website, 2009. [4] A. Cates. SOS Children X  X  Villages UK. Personal [5] T. Chklovski. Learner: A system for acquiring [6] E. Gabrilovich and S. Markovitch. Computing [7] R. Mihalcea and A. Csomai. Wikify! Linking [8] D. Milne. WikipediaMiner toolkit. Website, 2009. [9] D. Milne and I. H. Witten. An effective, low-cost [10] D. Milne and I. H. Witten. Learning to link with [11] K. Pearson. On lines and planes of closest fit to [12] R. Snow, B. O X  X onnor, D. Jurafsky, and A. Y. Ng. [13] R. Speer, C. Havasi, and H. Lieberman.
 [14] M. Turk and A. Pentland. Eigenfaces for recognition. [15] R. West. Project website, 2009. [16] Wikipedia. 2008/9 Wikipedia Selection for schools. [17] Wikipedia. Data dump of March 6, 2009. Website, [18] Wikipedia. Wikipedia:Linking. Website, 2009. [19] Wikipedia. Wikipedia:Manual of Style. Website, 2009.
