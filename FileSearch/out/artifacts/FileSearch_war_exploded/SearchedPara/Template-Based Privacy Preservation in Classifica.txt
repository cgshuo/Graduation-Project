
In this paper, we present a template-based privacy preservation to protect against the threats caused by data mining abilities. The problem has dual goals: preserve the information for a wanted classification analysis and limit the usefulness of unwanted sensitive inferences that may be derived from the data. Sensitive inferences are specified by a set of  X  X rivacy templates X . Each template specifies the sensitive information to be protected, a set of identifying at-tributes, and the maximum association between the two. We show that suppressing the domain values is an effective way to eliminate sensitive inferences. For a large data set, find-ing an optimal suppression is hard, since it requires opti-mization over all suppressions. We present an approximate but scalable solution. We demonstrate the effectiveness of this approach on real life data sets.
Knowledge Discovery in Databases (KDD) or data min-ing aims at finding out new knowledge about an applica-tion domain using collected data on the domain, typically data on individual entities like persons, companies, trans-actions. Naturally, the general concerns over data security and individual privacy are relevant for data mining. The first concern relates to the input of data mining methods due to data access. Many techniques have been proposed [4, 6, 8, 10, 12, 15, 18] to address this problem while pre-serving the benefits of data mining. The second concern is related to the output of data mining methods. Although the output of data mining methods are aggregate patterns, not intended to identify single individuals, they can be used to infer sensitive information about individuals. In this paper, we consider the privacy threats caused by such  X  X ata mining abilities X . Let us first consider an example. Example 1 (Running Example). Table 1 contains records about bank customers. After removing irrelevant attributes, each row represents the duplicate records and the count. The class attribute Rating contains the class frequency of credit rating. For example, 0G/4B represents 0 Good and 4 Bad. Suppose that the bank (the data owner) wants to release the data to a data mining firm for classification anal-ysis on Rating , but does not want the data mining firm to infer the bankruptcy state Discharged using the attributes Job and Country . For example, out of the 5 individu-als with Job = T rader and Country = UK , 4 has the Discharged status. Therefore, the rule { T rader, U K } X  Discharged has support 5 and confidence 80%. If the data owner tolerates no more than 75% confidence for this infer-ence, the data is not safe for release. In general, currently bankrupted customers have a bad rating and simply remov-ing the Bankruptcy column loses too much information for the classification analysis.

The private information illustrated in this example has the form  X  X f x then y  X , where x identifies a group of indi-viduals and y is a sensitive value. We consider this infer-ence  X  X rivate X  if its confidence is high, in which case an individual in the group identified by x tends to be linked to y . The higher the confidence, the stronger the linking. In the context of data mining, association or classification rules [1, 14] are used to capture general patterns of large populations for summarization and prediction, where a low support means the lack of statistical significance. In the context of privacy protection, however, inference rules are used to infer sensitive information about the existing indi-viduals , and it is important to eliminate sensitive inferences of any support, large or small. In fact, a sensitive inference in a small group could present even more threats than in a large group because individuals in a small group are more identifiable [16].

The problem considered in this paper can be described as follow. The data owner wants to release a version of data in the format to achieve two goals. The classification goal is to preserve as much information as possible for modeling the class at-tribute  X  . The privacy goal is to limit the ability of data mining tools to derive inferences about sensitive attributes  X  ,...,  X  n . This requirement is specified using one or more templates of the form, IC  X   X , h , where  X  is a value from some  X  i , inference channel IC is a set of at-tributes not containing  X  i , and h is a threshold on confi-dence. The data satisfies IC  X   X , h if every matching in-ference has a confidence no more than h . The privacy goal can be achieved by suppressing some values on masking at-tributes M 1 ,...,M m . We are interested in a suppression of values for M 1 ,...,M m that achieves both goals. In Example 1, the inference { T rader, U K } X  Discharged violates the template To eliminate this inference, we can suppress Trader and Clerk to a special value  X  Job , and suppress UK and Canada to a special value  X  Country , see Table 2. Now, the new in-ference { X  Job ,  X  Country } X  Discharged has confidence 50%, less than the specified 75%. No information is lost since Rating does not depend on the distinction of the sup-pressed values.

The use of templates provides several flexibilities for specifying the notion of privacy: selectively protecting cer-tain values  X  while not protecting other values; specifying a different threshold h for a different template IC  X   X  ; spec-ifying multiple inference channels IC (even for the same  X  ); specifying templates for multiple sensitive attributes These flexibilities provide not only a powerful representa-tion of privacy requirements, but also a way to focus on the problem area in the data to minimize unnecessary informa-tion loss.

Given that the classification task is known in advance, one may ask why not releasing a classifier, which likely contains less information, instead of the data. This could be an option if the data owner knows exactly how the data re-cipient may analyze the data. Often, this information is un-known. For example, in visual data mining, the data miner has to visualize data records in a certain way to guide the search, and in this case releasing data records is essential. Some classifiers such as the k-nearest neighbor are actually the data itself, and some are better in accuracy whereas oth-ers are better in interpretability. The data owner may not have the expertise to make such decisions because sophisti-cated data analysis is not part of its normal business.
The contributions of this work can be summarized as fol-lows. First, we formulate a template-based privacy preser-vation problem. Second, we show that suppression is an effective way to eliminate sensitive inferences. However, finding an optimal suppression is a hard problem since it requires optimization over all possible suppressions. For a table with a total of q distinct values on masking attributes, there are 2 q possible suppressed tables. We present an ap-proximate solution based on a search that iteratively im-proves the solution and prunes the search whenever no bet-ter solution is possible. We evaluate this method on real life data sets.
Most works on privacy preservation address the concern related to the input of data mining tools where private infor-mation is revealed directly by inspection of the data without sophisticated analysis [4, 6, 8, 10, 12, 15, 18]. Our work is more related to the concern over the output of data mining methods, where the threats are caused by what data mining tools can discover. We focus on this group of works.
Kloesgen [13] pointed out the problem of group discrim-ination where the discovered group behavior is attached to all members in a group, which is a form of inferences. Clifton [3] suggested to eliminate sensitive inferences by limiting the data size. Recently, Kantarcioglu et al. [11] defined an evaluation method to measure the loss of pri-vacy due to releasing data mining results. However, they did not propose a solution to prevent the adversary from getting data mining results that violate privacy.
Verykios et al. [17] proposed several algorithms for hid-ing association rules in a transaction database with minimal modification to the data. The general idea is to hide one rule at a time by either decreasing its support or its confidence; this is achieved by removing items from transactions. They need to assume that frequent itemsets of rules are disjoint in order to avoid high time complexity. We consider the use of the data for classification analysis. Instead of min-imizing pure syntax changes to the data, we minimize the information lose for this analysis and eliminate all sensi-tive inferences including those with a low support. We can efficiently handle overlapping inference rules.

Suppression of domain values was employed in [2, 10, 15] for achieving k -anonymity. The k -anonymity require-ment states that no identifying attributes identify a group of size smaller than k . Therefore, if such identifying attributes are used to link the data to an external table, all the records in each group (at least k ) will behave in the same way and are difficult to be distinguished. However, this notion does not address sensitive inference on the identified groups.
In database security, Farkas et al. [7] conducted a sur-vey on inference control. In multilevel secure databases, the focus is detecting and removing inference channels by combining meta-data with data. For example, it is possible for a user to use a series of unsuspicious queries to infer sen-sitive data in the database. [19] proposed a method to detect such queries using functional dependencies. This type of inferences is different from ours.

In statistical databases, the focus is limiting the ability of inferring confidential information by correlating differ-ent statistics. For example, Cox [5] proposed the k % -dom-inance rule which suppresses a sensitive cell if the attribute values of two or three entities in the cell contribute more than k % of the corresponding SUM statistic. Such  X  X ell suppression X  suppresses the count or other statistics stored in a cell of a statistical table, which is very different from the  X  X alue suppression X  considered in our work.
Consider a table T ( M 1 ,...,M m ,  X  1 ,...,  X  n ,  X ) . M are called masking attributes .  X  i are called sensitive at-tributes .  X  is called the class attribute . All attributes have a categorical domain. For each M j , we add the special value  X  j to its domain. For a domain value v , att ( v ) denotes the attribute of v . M j and  X  i are disjoint.

The data owner specifies sensitive inferences using tem-plates. A template has the form IC  X   X , h .  X  is sensi-tive value from some  X  i . IC , called an inference channel , is some set of attributes not containing  X  i . h is a confi-dence threshold. An inference for IC  X   X , h has the form ic  X   X  , where ic contains values from the attributes in IC . The confidence of ic  X   X  , written conf ( ic  X   X  ) is the percentage of the records that contain  X  among those that contain the values in ic , that is, s ( ic,  X  ) /s ( s ( V ) denotes the number of records containing the values in V . Conf ( IC  X   X  ) denotes the maximum conf ( ic  X   X  ) for all ic over IC .
 Definition 3.1 (Privacy Templates). T satisfies a template
IC  X   X , h if Conf ( IC  X   X  )  X  h . T satisfies a set of templates if T satisfies every template in the set.
Some template may be  X  X edundant X  once we have some other template. The next lemma considers one such case, which can be used to remove  X  X edundant X  templates. Lemma 3.1. Consider two templates IC  X   X , h and
IC  X   X  ,h .If  X  =  X  , h  X  h , and IC  X  IC , then (1) Conf ( IC  X   X  )  X  Conf ( IC  X   X  ) , and (2) if T satisfies IC  X   X  ,h , T satisfies IC  X   X , h .

To see (1), consider the partition { P 1 ,  X  X  X  ,P k } of the records matching ic according to the values x 1 ,  X  X  X  ,x k X = IC  X  IC . If the partitioning decreases the confi-dence of ic  X   X  in some P i , it must increase the confi-dence in some other P j . Therefore, the partitioning does not decrease the maximum confidence in P i  X  X . We omit the detailed proof. (2) follows immediately from (1).
If T violates a set of templates, (under certain condi-tions) we can suppress some values on masking attributes M j to make it satisfy the templates. The suppression of a value on M j means replacing all occurrences of the value with the special value  X  j . Thus, all suppressed values on M j are represented by the same  X  j . One question is what makes us believe that suppression can reduce the confidence of sensitive inference. Indeed, if suppression actually in-creases the confidence, we are not getting any closer to the privacy goal but losing information for the classifica-tion goal. Below, we show that suppression never increases Conf ( IC  X   X  ) .

Consider suppressing a value v in M j to  X  j . The sup-pression affects only the records that contain v or  X  j before the suppression. Let  X  j and  X  j denote  X  j before and after the suppression. The difference is that  X  j covers v but  X  does not. After the suppression, two inferences { ic, v } X  and { ic,  X  j } X   X  become one inference { ic,  X  j } X   X  . We have the next lemma, which has a similar proof as that of Lemma 3.1.
 Lemma 3.2. max { conf ( ic, v  X   X  ) ,conf ( ic,  X  j  X   X  ) }  X  conf ( ic,  X  j  X   X  ) .
 The lemma says that, by suppressing a value, Conf ( IC  X   X  ) does not go up. This property provides the basis for employing suppression to reduce Conf ( IC  X   X  ) Corollary 3.1. Conf ( IC  X   X  ) is non-increasing with re-spect to suppression.

From Corollary 3.1, the most suppressed T , where all values for M j are suppressed to  X  j for every M j in  X  IC , has the minimum Conf ( IC  X   X  ) . Therefore, if this table does not satisfy the templates, no suppressed T does. Lemma 3.3. Given a set of templates, there exists a sup-pressed table T that satisfies the templates if and only if the most suppressed T satisfies the templates.
 Definition 3.2 (Inference Problem). Given a table T and a set of templates, the inference problem is to (1) decide whether there exists a suppressed T that satisfies the set of templates, and if yes, (2) produce a satisfying suppressed T that preserves as much information as possible for modeling the class attribute.
Given a table T and a set of templates { IC  X   X , h } , our algorithm iteratively  X  X iscloses X  the domain values starting from the most suppressed T in which each mask-ing attribute M j in  X  IC contains only  X  j , i.e., the set of suppressed values, Sup j , contains all domain values in M At any time, we have a set of suppressed records , with du-plicates being collapsed into a single record with a count. In each iteration, we disclose one value from some Sup j .To disclose a value v from Sup j , we do exactly the opposite of suppressing v , i.e., replace  X  j with v in all suppressed records that currently contain  X  j and originally contain v before suppression. This process repeats until no disclo-sure is possible without violating the set of templates. From Corollary 3.1, any further disclosure leads to no solution. Algorithm 1 Progressive Disclosure Algorithm (PDA) 1: suppress every value of M j to  X  j where M j  X  X  X  IC ; 2: every Sup j contains all domain values of M j  X  X  X  IC ; 3: while there is a valid/beneficial candidate in  X  Sup j 4: find the winner w of highest Score ( w ) from  X  Sup j ; 5: disclose w on T and remove w from  X  Sup j ; 6: update Score ( x ) and the valid/beneficial status for x 7: end while 8: output the suppressed T and  X  Sup j ;
The above algorithm, called Progressive Disclosure Al-gorithm (PDA) , is presented in Algorithm 1. At each it-eration, if some Sup j contains a  X  X alid X  and  X  X eneficial X  candidate for disclosure, the algorithm chooses the win-ner candidate w that maximizes the score function denoted Score . A disclosure is valid if it leads to a table satisfying the set of templates. A disclosure from Sup j is beneficial if more than one class is involved in the records contain-ing  X  j . Next, the algorithm discloses w , and updates the Score and status of every affected candidate. We focus on the three key steps (Lines 4 to 6) in the rest of this section. Example 2. Consider the templates: { Job , Country } X  Discharged, 50% , { Job , Child } X  Discharged, 50% .
 Initially, the values of Job , Country and Child in Table 1 are tains all domain values in Job , Country , and Child . This is the most suppressed, or the least disclosed, state.
This step finds the winner w , i.e., the valid and beneficial candidate from  X  Sup j that has the highest Score . Since disclosing a value v gains information and loses privacy, Score ( v ) measures the information gain [14] per unit of privacy loss, defined as Consider the set of suppressed records that currently con-tain  X  j , denoted T [  X  j ] . Disclosing v from Sup j means replacing  X  j with v in all records in T [  X  j ] that originally contain v . Let T v denote the set of such records, and let T denote T v after replacing  X  j with v . The disclosure of v is to replace T [  X  j ] with T [ v ] and T [  X  j ]  X  T v . InfoGain is the information gain of this replacement. InfoGain ( v depends only on the class frequency and count statistics on the single attribute att ( v ) in T [  X  j ] , T [ v ] and T
PrivLoss ( v ) measures the privacy loss, defined as the average increase of Conf ( IC  X   X  ) over all affected IC  X   X  , i.e., those IC such that att ( v ) is contained in IC , where Conf and Conf v represent the confidence before and after disclosing v .

Computing Conf v efficiently is a challenge since it may involve count statistics on a combination of several at-tributes. It is inefficient to perform the disclosure of v in order to compute Conf v . The key to the scalability of our algorithm is incrementally updating Score ( v ) in each iter-ation for valid/benefical candidates v in  X  Sup j . We will present this update algorithm in Section 4.3.
This step discloses the winner w and replaces  X  j with w in the suppressed records in T [  X  j ] that originally contain w . It requires accessing the raw records of these suppressed records. The following data structure facilitates the direct access to such raw records. The idea is to partition raw records according to their suppressed records on  X  IC . Definition 4.1 (VIP). Value Indexed Partitions (VIP) con-tains the set of suppressed records over  X  IC . Each sup-pressed record represents the set of raw records from which it comes, called a partition . Each raw record is in exactly one partition. For each disclosed value x (including  X  )on an attribute in  X  IC , P [ x ] denotes a partition represented by a suppressed record containing x . Link [ x ] links up all P [ x ]  X  X , with the head stored with the value x .

To disclose the winner w , we follow the link Link [  X  w ] and find all suppressed records that contain  X  w , and through these suppressed records, access the represented raw records. Let  X  w denote the special  X  value for the attribute of w . The following example illustrates the proce-dure of disclosing w in VIP.
 Example 3. Consider the templates in Example 2. In Figure 1, the left-most VIP has the most suppressed record  X  Job ,  X  Country ,  X  Child on three links: Link [  X  Link [  X  Country ] , Link [  X  Child ] . The shaded fields  X  X otal X  and  X   X   X  contain the number of raw records suppressed (i.e., | P | ) and the number of those records containing Discharged .

Suppose the winner is Clerk . We create a new sup-pressed record Clerk ,  X  Country ,  X  Child , as shown in the middle VIP, to represent 4 raw records. We add this new suppressed record to Link [  X  Country ] , Link [  X  Child ] the new Link [ Clerk ] . Finally, we remove Clerk from Sup The next winner, Canada , refines the two partitions on Link [  X  Country ] , resulting in the right-most VIP. The over-head of maintaining these links is proportional to the length of Link [  X  w ] and is negligible.

For the purpose of updating Score ( x ) efficiently, we also maintain the following count statistics for each parti-tion P in the VIP: for every class  X  and sensitive value  X  , (1) | P | , s (  X  ) and s (  X  ) , (2) for each masking attribute M on which P has the value  X  j , for every value v in Sup j s ( P . These count statistics are stored together with the par-tition P and, on disclosing w , are updated as we scan the partitions on Link [  X  w ] .

We should mention that this step (Line 5) is the only time that raw records are accessed in our algorithm.
This step updates Score ( x ) and the valid/beneficial sta-tus for x in  X  Sup j . InfoGain ( x ) is affected only if x and w are from the same attribute. InfoGain ( x ) can be up-dated using the count statistics stored at the partitions on Link [  X  w ] , in the same scan as maintaining the count statis-tics in the previous step. Mark x as beneficial if there is more than one class in these partitions.

To update PrivLoss ( x ) , for every IC  X   X  , we first update Conf ( IC  X   X  ) using Conf w ( IC  X   X  ) that was computed in the previous iteration. Next, we update Conf x ( IC  X   X  ) for x in  X  Sup j . Observe that if att ( is not in IC , Conf x ( IC  X   X  )= Conf ( IC  X   X  ) ; if att ( w ) is not in IC , Conf x ( IC  X   X  ) is not affected by the disclosure of w . Therefore, we need to update Conf x ( IC  X   X  ) only if both att ( x ) and att ( w ) are con-tained in IC . We propose the following IC -tree structure to maintain Conf ( IC  X   X  ) .
 Definition 4.2 ( IC -trees). For each IC = { A 1 ,...,A u the IC -tree is a tree of u levels, where level i&gt; 0 represents the values for A j . A root-to-leaf path represents an existing ic on IC in the suppressed T , with s ( ic ) and s ( ic,  X  at the leaf node.
 Recall that conf ( ic  X   X  )= s ( ic,  X  ) /s ( ic ) . Conf ( IC  X   X  ) is given by max { conf ( ic  X   X  ) } for all ic in the IC -tree. All templates IC  X   X , h with the same IC can share a single IC -tree by keeping s ( ic,  X  ) arately for different  X  . We update IC -trees on disclosing w . Here is an example.
 Example 4. Figure 2 shows the initial IC 1 -tree and IC 2 tree on the left, where IC 1 = { Job , Country } and IC 2 {
Job , Child } . On disclosing Clerk , { Clerk,  X  Country } {
Clerk,  X  Child } are created in IC 1 -tree and IC 2 -tree. Next, on disclosing Canada , { Clerk,  X  Country } is refined into {
Clerk , Canada } in IC 1 -tree, and a new { X  Job , Canada is split from { X  Job ,  X  Country } . To compute s ( ic ) s ( ic,  X  ) for these ic  X  X , we access all partitions P [ Canada in one scan of Link [ Canada ] in the VIP:
As discussed above, for x  X  X  X  Sup j , if both att ( x ) and att ( w ) are in IC , we need to update Conf x ( IC  X   X  ) . Recall that Conf x ( IC  X   X  ) is the maximum conf ( ic  X   X  ) after disclosing x . Therefore, we can treat x as if it were disclosed, and computing s ( ic, x ) , s ( ic, x,  X  ) , s s ( ic,  X  x , X  ) as we did for w . The only difference is that we perform these computations on a copy because we do not actually update the VIP and IC -trees for x . Conf x ( IC  X  ) is the new maximum conf ( ic  X   X  ) in the IC -tree. If Conf x ( IC  X   X  )  X  h , mark x as valid.
The cost at each iteration can be summarized as two operations. The first operation scans the partitions on Link [  X  w ] for disclosing the winner w in VIP and main-taining some count statistics. The second operation simply makes use of the count statistics to update the score and status of every affected candidate without accessing data records. Thus, each iteration accesses only the records sup-pressed to  X  w . The number of iterations is bounded by the number of distinct values in the masking attributes.
We evaluated how well the proposed method can pre-serve the usefulness for classification for some highly re-strictive limiting requirements. We also evaluated the effi-ciency of this method. We adopted two widely used bench-marks from the UCI repository [9]: Japanese Credit Screen-ing and Adult . We removed all continuous attributes since our method focuses on only categorical attributes. We used the C4.5 classifier [14] for classification modeling. All ex-periments were conducted on an Intel Pentium IV 3GHz PC with 1GB RAM.

Templates . We chose the best N attributes for the clas-sification analysis, denoted TopN , as the sensitive attributes  X  ,...,  X  N . Simply removing such sensitive attributes will compromise the classification goal. The top most attribute is the attribute at the top of the C4.5 decision tree. Then we removed this attribute and repeated this process to deter-mine the rank of other attributes. The remaining attributes were chosen as the masking attributes M 1 ,...,M m .For each  X  i , we choose the 50% least frequent values as sen-sitive values. The rationale is that less frequent values are more vulnerable to inference attacks. Let {  X  1 ,..., X  k } note the union of such values for all  X  i . The template is
IC  X  X   X  1 ,..., X  k } ,h , where IC contains all masking attributes. From Lemma 3.1, this template is more restric-tive than a set of multiple templates with each being a subset of IC (for the same threshold h ).

Errors to measure . The base error ( BE ) refers to the error for the original data without suppression. The sup-pression error ( SE ) refers to the error for the data sup-pressed by our method. The suppression was performed before splitting the data into the training set and the testing set. SE  X  BE measures the quality loss due to suppression, the smaller the better. We also compared with the error caused by simply removing all sensitive attributes, which is denoted by removal error ( RE ). RE  X  SE measures the benefit of suppression over this simple method, and the larger the better. Finally, RE  X  BE measures the impor-tance of sensitive attributes on classification. All errors are collected on the testing set. The Japanese Credit Screening data set, also known as CRX , is based on credit card application. There are 9 cat-egorical attributes and a binary class attribute representing the application status succeeded or failed . After removing records with missing values, there are 465 and 188 records for the pre-split training and testing respectively. In the UCI repository, all values and attribute names in CRX have been changed to meaningless symbols, e.g., A 1 ...A 15 . We con-sider the four template requirements: Top1 , Top2 , Top3 and Top4 . BE =15 . 4% . Table 3 shows the number of inferences above different confidence thresholds h in the original data. For example, the number of inferences that have a confidence larger than 90% is 6 in CRX for Top4 . Threshold h 10% 30% 50% 70% 90% CRX ( Top4 ) 40 27 15 8 6 Adult ( Top4 ) 1333 786 365 324 318
Figure 3a depicts SE and RE for TopN averaged over h = 50%, 70%, 90%. The dashed line represents BE .We summarize the results as follows: 1. SE spans narrowly between 15.4% and 16.5% across 2. The minimum RE  X  SE is 10 . 1% for Top1 , and the 3. For all templates tested, the variance of SE is less than 4. Having more sensitive attributes (i.e., a larger N in 5. The algorithm took less than 2 seconds, including disk
Let us take a closer look at the suppressed data for Top4 with h = 70% . Some values of attributes A 4 and A 5 are suppressed, and the entire A 13 is suppressed. Despite such vigorous suppression, SE =15 . 4% is equal to BE .In fact, there exist multiple classification structures in the data. When suppression eliminates some of them, other structures emerge to take over the classification. Our method makes use of such  X  X ooms X  to eliminate sensitive inferences while preserving the quality of classification.
The Adult data set is a census data previously used in [2, 8, 10, 18]. There are 8 categorical attributes and a bi-nary class attribute representing the income levels  X  50K or &gt; 50K. There are 30,162 and 15,060 records without miss-ing values for the pre-split training and testing respectively. Table 4 describes each categorical attribute. Top4 attributes are M,Re,E,S in that order. BE =17 . 6% .
 Attribute #of Attribute #of Education (E) 16 Marital-status (M) 7 Occupation (O) 14 Native-country (N) 40 Race (Ra) 5 Relationship (Re) 6 Sex (S) 2 Work-class (W) 8
Figure 3b shows the errors for TopN , averaged over h = 10%, 30%, 50%, 70%, 90%. We summarize the results as follows: (1) SE  X  BE is less than 0.8% in all cases. This is amazing considering that hundreds of inferences were elim-inated according to Table 3. (2) The largest RE  X  SE is ap-proximately 6% for Top4 . (3) The difference between max-imum and minimum SE is less than 1%. (4) For Top1 , RE is slightly lower than SE , implying that removing the top attribute does not affect the classification. However, as more sensitive attributes were removed (i.e., Top2 , Top3 , Top4 RE picked up. (5) The algorithm spent at most 14 seconds for all experiments on Adult , of which approximately 10 seconds were spent on suppressing the 45,222 data records.
The purpose of this experiment is to see how scalable our method is for large data sets. We evaluated the scalabil-ity on an expanded version of Adult . We first combined the training and testing sets, giving 45,222 records. Then for each original record r in the combined set, we created  X   X  1  X  X ariations X  of r , where  X &gt; 1 is the expansion scale .For each variation of r , we randomly and uniformly selected y attributes from  X  IC , selected some random values for these y attributes, and inherited the values of r on the remain-ing attributes, including the class and sensitive attributes. Together with original records, the expanded data set has  X   X  45 , 222 records.

Figure 3c depicts the runtime of our suppression method for 200K to 1M data records based on the templates IC  X  {  X  1 ,..., X  k } , 90% , where {  X  1 ,..., X  k } is the set of 50% least frequent values in the Top1 attribute M , and IC con-tains the other 7 attributes. This is one of the most time consuming settings because of the largest number of disclo-sure candidates to consider at each iteration, and a larger h requires more iterations to reach a solution. Our method spent 192 seconds to suppress 1M records, of which 150 seconds were spent on suppression, and the rest was spent on disk I/O operations.
We studied the problem of eliminating the sensitive in-ferences that are made possible by data mining tools, while preserving the classification value of the data. A sensitive inference has a high confidence in linking a group of indi-viduals to sensitive values. We eliminated sensitive infer-ences by letting the user specify the templates and max-imum confidence for such inferences. We used suppres-sion of domain values as a way to achieve this goal. We presented a progressive disclosure algorithm that iteratively searches for a better suppression and prunes the search whenever no better alternative is possible. Experiments on real life data sets showed that the proposed approach pre-serves the information for classification modeling even for very restrictive privacy requirements.

