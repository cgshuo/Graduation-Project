 Bernardino Romera-Paredes bernardino.paredes.09@ucl.ac.uk Min Hane Aung m.aung@ucl.ac.uk Nadia Bianchi-Berthouze n.berthouze@ucl.ac.uk Massimiliano Pontil m.pontil@cs.ucl.ac.uk College London, UK The principal assertion in Multitask Learning (MTL) is that the combined learning of multiple related tasks can outperform learning each task in isolation, see for example ( Argyriou et al. , 2008a ; Baxter , 2000 ; Caru-ana , 1997 ; Romera-Paredes et al. , 2012 )andreferences therein. In doing this, MTL allows for common infor-mation shared between the tasks to be used in the learning process, which leads to better generalization if the tasks are related ( Ando and Zhang , 2005 ; Baxter , 2000 ; Maurer , 2009 ; Maurer and Pontil , 2012 ; Maurer et. al , 2013 ). For example, consider a task to be a regression problem in predicting restaurant ratings by aspecificrestaurantcritic,givenarestaurantasan input query. If we then include ratings from N critics, this will lead to N tasks. MTL methods will learn all of the regression functions that model all N tasks to-gether by exploiting the common trends among all of the restaurant raters as well as the individual prefer-ences.
 Traditional MTL methods do not consider any addi-tional inherent structure in the dataset and therefore the referencing of the tasks is simplified to a single in-dex i ;intheaboveexamplethiswouldrefertothe raters ranging from 1 to N .However,itisclearthata loss of information would arise if these methods were applied to datasets that are defined by multiple in-dices. For example, if our restaurant critics rated M separate aspects of each restaurant, this would give rise to a second index j ranging from 1 to M . This 2 -dimensional indexing information would be lost in a traditional MTL approach. In this paper, we pro-pose to extend the method developed in ( Argyriou et al. , 2008a )toconsidertheinherentstructuresin such datasets by preserving multi dimensional indices (or modes) which are associated with the tasks. To this end, we propose the use of multilinear models as anaturalunderpinningtorepresentthisstructuralin-formation. We will refer to our proposed framework as Multilinear Multitask Learning (MLMTL).
 The use of multilinear models in previous applications has been shown to be e ff ective in determining separate underlying factors in data. In ( Tenenbaum &amp; Free-man , 2000 )theauthorsintroducedabilinearmodel as a basis to represent the relationships in data with two modes. A common application for this bilinear model is to decouple two mode data for classification purposes; for example in ( Mpiperis et al. , 2008 )the authors separate facial identity factors and expression factors to classify an emotional state and subject iden-tity. By using bilinear models, the bi-factor interac-tions were more accurately represented. By extension, multilinear models in general have been used to sepa-rate more than two modes in similar datasets. For ex-ample ( Vasilescu &amp; Terzopoulos , 2002 )decomposenat-ural facial images according to four modes: identity, expression, head pose and lighting condition. Mul-tilinear models have also been extended to account for di ff erent assumptions in the data. In ( Vasilescu &amp;Terzopoulos , 2005 ), the authors propose multilinear independent component analysis, where the factors ex-tracted for each mode are not only uncorrelated but also statistically independent.
 In this paper we form a multilinear model by struc-turing the weight parameters of all tasks into a tensor. This is a departure from the aforementioned studies, where the multilinear decomposition was applied di-rectly to the input data, obtaining unsupervised learn-ing models which can be seen as higher-order general-izations of principal (or independent) component anal-ysis. The tensor representation allows us to account for the multi-modal interactions between the tasks. In addition, our approach allows one to make predictions even in absence of training data for one or more of the tasks, thereby providing a useful tool for transfer learning, see e.g. ( Argyriou et al. , 2008b ). For exam-ple, the vector of parameters for the ( i, j ) -task can still be estimated provided that training data are available for at least one task ( i, k ) , k 6 = j and one other task ( ` , j ) , ` 6 = i .
 In order to formulate MLMTL, we follow a complex-ity regularization approach which encourages low rank matricizations ( Kolda &amp; Bader , 2009 )oftheweight tensor. However, the regularization term will cause a non convex minimization problem. Therefore, the first of our learning approaches involves a convex relaxation of the original minimization problem. This solution is based on several recent studies which have showed that the use of the trace norm of tensors provides close con-vex approximations of similar minimization problems ( Gandy et al. , 2011 ; Liu et al. , 2009 ; Signoretto et al. , 2011 ; Tomioka , 2010 ). For our second approach we in-vestigate a di ff erent strategy that makes use of an al-ternating minimization scheme for Tucker decomposed components of the original weight tensor. In summary, the main contributions of this paper are:  X  The extension of multitask learning to account for  X  The introduction of an alternating minimization  X  A framework for transfer learning with multilinear The remainder of this paper is organized as follows. In Section 2 ,wedescribethekeyconceptsfrommultilin-ear algebra that are needed to formulate our learning problem. In the following two sections, we describe two alternative ways to obtain solutions to the pro-posed problem: Section 3 describes a convex relaxation of the learning problem, whereas Section 4 presents an alternating minimization algorithm for Tucker decom-position. In Section 5 we compare the proposed ten-sor based methods with respect to their matrix based MTL counterparts. Additionally, non MTL baseline models are also compared. Finally, in Section 6 we conclude with a discussion of the results obtained. We begin by introducing some notation. We let N be the set of natural numbers and, for every k 2 N ,we use [ k ] to denote the set of integers up to and including k .Let N 2 N and choose p 1 ,...,p N 2 N .An N -order tensor, X 2 R p 1  X   X  X  X   X  p N ,isacollectionofrealnumbers ( X i 1 ,...,i N : i n 2 [ p n ] ,n 2 [ N ]) .Vectorsare 1 tensors and will be denoted by lower case letters, e.g. a or b ;matricesare 2 -order tensors and will be denoted by upper case letters, e.g. A or B .Boldfaceletters, e.g. X , W , will be used to denote tensors of order higher than two. We use the symbol  X  :  X  X nthelower-script indices to denote sub-arrays within a matrix or atensor,e.g. if A 2 R p 1  X  p 2 ,then A : ,i 2 R p 1 denotes the i -th column of A ,forevery i 2 [ p 1 ] .Forthesake of clarity, colons at the beginning of superscripts are omitted so that A i := A : ,i 2 R p 1 .
 Mode-n fiber is a vector composed of the elements of a tensor obtained by fixing all indices but one, cor-responding to the n -th mode. This notion is a higher order analogue of columns (mode-1 fibers) and rows (mode-2 fibers) in matrices. For example, in a 3 -order tensor X 2 R p 1  X  p 2  X  p 3 ,thesetofvectorsoftheform of tensor X .
 Mode-n matricization is the process of rearranging all the elements of a tensor X 2 R p 1  X   X  X  X   X  p N into a ma-trix. Specifically, if n 2 [ N ] ,themode-n matricization, denoted as X ( n ) ,isobtainedbyarrangingthemode-n fibers of the tensor so that each of them is a column of Mode-n product is the product of a tensor X 2 A . The result is a new tensor of size p 1  X   X  X  X   X  p n 1  X  J  X  p n +1  X   X  X  X   X  p N , where each mode-n fiber is multiplied by A ,thatis ( X  X  n A ) i Rank n is the dimension of the space spanned by the mode-n fibers, that is the rank of the mode-n matri-cization of the tensor, rank n ( X ) := rank( X ( n ) ) . If is a matrix, this is the usual definition of rank, since rank 1 ( X ) = rank 2 ( X ) = rank( X ) .Forhigherorder tensors, however, rank n ( X ) varies with n . We are now ready to describe the learning problem. We consider a set of T linear regression tasks, each of which is represented by a vector w t 2 R d , t 2 [ T ] Each task is associated with two or more modes (recall the restaurant rating example described in the intro-duction). We regard the d  X  T matrix [ w 1 ,...,w T ] as the mode-1 matricization, W (1) ,ofthetensor W 2 and the index t can be identified by the multi-index ( i ,...,i N ) 2 [ p 2 ]  X   X  X  X   X  [ p N ] .Foreachtask t we sample the underlying regression problem m t times, obtaining a set of input/output observations, where ( x ,y t i ) 2 R d  X  R , i =1 ,...,m t .Wealsousethe shorthand notation for the data term where h  X  ,  X  i is the inner product in R d , L is a prescribed loss function, e.g. the square error L ( z, y )=( z y ) 2 . We estimate the regression vectors as the solution of the joint optimization problem where is a positive parameter which may be cho-sen by cross validation. The regularizer R encourages common structure between the tasks. In particular, our goal is to encourage the tensors W which have a simple structure in the sense that they involves a small number of  X  X egree of freedoms X . To this end, a natu-ral choice is to consider the sum of the ranks of the matricizations of the tensors. Specifically, we let Finding a convex relaxation of R (  X  ) has been the ob-jective of recent works ( Gandy et al. , 2011 ; Liu et al. , 2009 ; Signoretto et al. , 2011 ). All of them agree to use the trace norm for tensors as a good convex proxy. This is defined as the average of the trace norm of each matricization of W , where k  X  k 1 is the trace norm of a matrix, namely the ` -norm of the singular values. Note that in the par-ticular case of 2 -order tensor, ( 4 ) coincides with the usual notion of trace norm of a matrix. The above ob-servation motivates us to consider the convex problem When N =2 problem ( 5 )isequivalenttotheone proposed in ( Argyriou et al. , 2008a ). However, if N&gt; 2 ,problem( 5 )ismoredi ffi culttosolveduetothe composite nature of the regularizer ( 4 ). To explain this observation, we introduce N auxiliary tensors B n 2 1  X   X  X  X   X  p N , n 2 [ N ] , each of which represents a version of the original tensor W .Withthisnotation,problem ( 5 )canbereformulatedas where all the trace norm regularizers on the auxiliary matrix are related through the equality constraints. As noted by Gandy et al. ( 2011 )and Signoretto ( 2013 ) problem ( 6 )canbesolvedbythealternatingdirec-tion method of multipliers (ADM) (see e.g. Bertsekas &amp;Tsitsiklis , 1989 ). This optimization strategy allows problem 6 to be decoupled into independent subprob-lems which no longer have interdependent trace norm constraints. This decoupling is achieved by introduc-ing a set of Lagrange multipliers C n , 8 n 2 { 1 ,...,N } The resultant augmented Lagrangian function is ( Bert-sekas &amp; Tsitsiklis , 1989 ) L ( W , C , B )= F ( W )+ for some &gt; 0 , where the inner product between ten-sors is defined as the regular inner product between the vectorized form of the tensors. Appendix A describes an algorithm to solve problem ( 7 ).
 The main advantage of this approach is that it al-ways obtains the global solution of problem ( 4 ). How-ever the fact that the outputs of the algorithm are the weight vectors themselves leads to two important drawbacks. First, transfer learning is not possible straight from the model. This is because the fac-tors (see equation ( 8 ) below) are learned implicitly but one cannot have access to them under this ap-proach. Therefore, if we want to add a new entity (e.g. a new restaurant in the example described in the introduction), the whole algorithm needs to be run again from scratch. The second drawback is related to memory necessities. In some problems, dealing with the whole weight tensor W can be problematic since this can be very large. Furthermore, this approach needs to keep N +1 versions of the tensor in mem-ory so the total memory needed to run the algorithm is O ( N + 1) many problems. Finally, note that this approach is not optimizing the original problem but a convex ap-proximation of it. To overcome these shortcomings, we propose a new method in the following section. In this section, we describe an alternative method which encourages low rank representations of the ten-sor using the Tucker decomposition, (see e.g. Kolda &amp; Bader , 2009 ). It is defined as where W 2 R p 1  X   X  X  X   X  p N is the tensor containing all weight vectors, A ( n ) 2 R p n  X  k n , n 2 [ N ] ,arethefactor matrices and G 2 R k 1  X   X  X  X   X  k N , which is called the core tensor, models the interaction between factors. Figure 1 depicts the Tucker decomposition of a 3 mode ten-sor. We can also express this decomposition in a more compact way using the matricization and product op-erators, We would like to minimize the error term F ( W ) in equation ( 1 ), over tensors of the form ( 8 ). Note that the Tucker decomposition is invariant under multipli-cation and division of di ff erent factors by the same scalar. With the aim of avoiding this issue and reduc-ing overfitting, we add Frobenius norm regularization terms to the components. The resultant problem is where we defined and  X  is a regularization parameter. Although the regularization term is heuristic in nature, we will argue in Section 5 that it helps avoiding overfitting. We attempt to solve problem ( 9 )byalternatingmin-imization, where in each step we fix all components but one and solve the resultant convex problem. We distinguish three di ff erent cases: minimizing over G , and over A ( n ) for any n 2 { 2 ,...,N } .
 Minimizing over G . Equation ( 9 )canbemini-mized over G by noticing that where  X  denotes the Kronecker product and e t 2 R T is a vector such that e t t =1 and e t s =0 for s 6 = t .Here, we express the weight vector estimators in terms of the product of the first matricization of G with the other factor matrices. This leads to the convex problem min  X  k G k 2 Fr which we can solve by gradient descent if L is di ff er-entiable. The gradient of H w.r.t G (1) is given by where L 0 spect to its first argument evaluated at x Finally, in order to obtain the tensor G ,weonlyneed to invert the matricization operation.
 Minimizing over A (1) . In this case, we can reuse the equality ( 10 )tominimizeover A (1) . This can be solved by gradient descent, where the gradient of H w.r.t. to A (1) is given by Minimizing over A ( n ) ,n 2 { 2 ,...,N } . This set of cases is more di ffi cult to describe. In order to simplify the presentation we assume that N =3 and n =2 ,but the generalization to larger values is straightforward. First of all, it is useful to note that the 2 -mode splits all tasks into p 2 sets, each of which has p 3 tasks. For every  X  2 [ p 2 ] we let S  X  be the set of tasks indexed by (  X  ,  X  ) .Werearrangetheinputdatabelongingtothose tasks as e X ber of instances of all tasks belonging to group S  X  ,that is, = Notice that, unlike the previous cases, the columns of A pler problems. The corresponding gradient of H with The local approach has a set of advantages derived from the explicit calculation of the factors. First, it al-lows for adding new factors in the setting without the necessity of relearning the previous factors, thereby al-lowing for transfer learning in a natural way. Second, the memory needed is O which can be much smaller than that of the convex ap-proach, particularly if k n  X  p n for some n 2 [ N ] . The main drawback of this approach is that the solution obtained is a local optimum and there is no guarantee about how far this is from the global optimum. We have conducted a set of experiments on one syn-thetic dataset and two real world datasets. In this section, we present and analyze the results obtained. The predictive performances of the five methods out-lined below are compared:  X  Ridge Regression (RR): this model, chosen as a  X  Multitask Feature Learning (MTL-C): a convex  X  Matrix Factorization (MTL-NC): a non convex  X  Convex Multilinear Multitask Learning  X  Non-convex Multilinear Multitask Learning The last two methods have been implemented us-ing the Tensor Toolbox ( Bader &amp; Kolda , 2006 ). All methods have one hyper-parameter which needs to be tuned. This is always done by means of a validation set. The range of hyper-parameter values tried are 10 s ,for s = 3 , 2 ,..., 5 , 6 .Preliminaryexperiments show that this range for s empirically contains the best solution for all approaches. 5.1. Synthetic Data In order to test the correctness of the implementa-tion of the algorithms proposed and to investigate the performance, we create a synthetic dataset where the weight tensor is decomposable as described in equa-tion ( 8 ). This dataset is generated as follows: we cre-ate a set of T = 100 tasks, organized in an p 2  X  p 3 grid where p 2 = p 3 = 10 and the input data has di-mensionality p 1 = 10 . The tasks weight vectors can consequently be organized in an N =3 mode tensor W 2 R p 1  X  p 2  X  p 3 .Furthermore,thistensorhasbeen generated so that rank n ( W )=3 , 8 n 2 [ N ] .Particu-larly, any element in the tensor has been generated as W i 8 i 1 2 [ p 1 ] ,i 2 2 [ p 2 ] ,i 3 2 [ p 3 ] , where all elements A dom sampling from a Gaussian distribution N (0 , 1) . For each task t =( i 2 ,i 3 ) ,asetof m training in-stances x t 1 ,...,x t m 2 R d are sampled from N (0 , 1) and the labels are generated by the linear regression y pled i.i.d. from N (0 , 0 . 1) .Similarly,asetof m val and m test instances and their corresponding labels are generated for each task for validation and testing pur-poses. The validation set is used to tune the regular-ization parameter for all approaches. Additionally for the factorization techniques (MTL-NC and MLMTL-NC), the number of factors for each mode has been fixed to the known values of the ranks.
 The described experiment has been done for several values of m in order to investigate the e ff ect of the number of training samples given. 20 trials have been executed for each value of m . The average results are shown in Figure 2 , where we see that all MTL ap-proaches perform better than ridge regression as ex-pected. Furthermore, we see that among the convex approaches, MLMTL-C is slightly better than its ma-trix counterpart MTL-C although these di ff erences are only significant for m&lt; 60 .Regardingthenon-convex approaches, we see that MLMTL-NC obtains the best performance with a clear improvement with respect to all remaining approaches. Nevertheless, in the current setting, the non-convex approaches have advantage in that the ground truth ranks of the tensor are known for the synthetic dataset. To see how sensitive MLMTL-NC approach is with respect to incorrect values of the ranks, we have carried out a similar experiment where we compare MLMTL-C and three versions of MLMTL-NC, taking each one di ff erent values for the ranks. The results are shown in Figure 3 . The MLMTL-NC approaches with ranks =(1 , 1 , 1) and ranks =(2 , 2 , 2) which have ranks smaller than the true values, are not the best approach is MLMTL-NC (3 , 3 , 3) since in this case the ranks coincides with the actual ranks of the underlying tensor. However, we see that MLMTL-NC approaches with higher values of ranks perform quite similarly and in all of these cases there is an im-provement with respect to MLMTL-C approach. This supports the hypothesis that MLMTL-NC approach is quite insensitive to the values of ranks, as long as they are an overestimation of the actual ones.
 Finally, we empirically assess the computational e ffi -ciency of the approaches for di ff erent tensor dimen-sions. The results can be seen in Table 1 .Both MLMTL approaches require more time for the learn-ing process. Furthermore, we see that MLMTL-NC scales better than the convex counterpart as the size of the tensor increases. 5.2. Real Data In this section, we test the described approaches with two real world datasets. For both datasets we want to infer the weight tensor W 2 R p 1  X  p 2  X  p 3 , where p 1 the number of attributes, p 3 is the number of subjects involved in the data and p 2 is the number of tasks we want to learn for each subject.
 In these experiments, we also compare with a version of each non-MLMTL approach that ignores the subject identifier index and groups all of the instances. This leads to only one generic impersonal predictor for each p task. This is done with the objective of appraising the e ff ect of discarding the information provided by one mode. The resultant approaches are denoted as GRR, GMTL-C and GMTL-NC.
 Regarding the multilinear approaches, the value of each rank n for MLMTL-NC has been set to min(10 ,p n ) for both experiments. This value is deemed to be a safe overestimate of the true rank on these data. The results of the previous experiments, presented in Figure 3 ,showthatoverestimateshavea minimal e ff ect on the final performance. 5.2.1. Restaurant &amp; Consumer Dataset The Restaurant &amp; Consumer Dataset ( Vargas-Govea et al. , 2011 )containsdatatobuildarestaurantrec-ommender system where the objective is to predict consumer ratings given to di ff erent restaurants. Each of the p 3 = 138 consumers gave p 2 =3 scores for food quality, service quality and overall quality. The dataset also contains p 1 = 44 various descriptive at-tributes of the restaurants (such as geographical posi-tion, cuisine type and price band). We consider this to be a regression problem where the objective is to predict the scores given the attributes of a restaurant as an input query. Since there are 138 consumers, this leads to a multitask problem composed of 138  X  3 re-gression tasks.
 This experiment was conducted in a similar way to the synthetic dataset, so that the training, validation and test sets were randomly selected for each task. The process was repeated 20 times for each value of m and the average results are shown in Figure 4 . We observe that both MLMTL approaches outperform the remaining methods for m 750 .Asetofpaired t-test conducted between each pair of MLMTL and non-MLMTL shows that this improvement in the per-formance is significant obtaining always p -values be-low 0 . 01 . This fact supports our hypothesis about the multi-modal relation among tasks and how MLMTL can take advantage of this over conventional MTL methods. We also checked the significance of the im-provement observed between both MLMTL methods for m 750 ,obtaining p -values below 0 . 025 . 5.2.2. Shoulder Pain Dataset In the second real world experiment we use the Shoul-der Pain dataset ( Lucey et al. , 2011 ), which contains video clips of the faces of people who su ff er from shoul-der pain while performing active and passive exercises. For each frame of the video, the facial expression is described by a set of p 1 = 132 attributes (2D posi-tions of 66 anatomical points). Each video is labelled frame by frame according to the physical activation of di ff erent set of facial muscles, encoded by the Facial Action Coding System ( Ekman et al. , 1978 ). This sys-tem defines a set of Action Units (AU) which refer to acontractionorrelaxationofadeterminedsetofmus-cles, e.g. AU6 is defined as the raising of a cheek. In this dataset, the intensity of an AU is expressed as a degree which ranges between 0 and 5 .Ourobjectiveis to recognise the AU intensity level of p 2 =5 di ff erent AUs for each of the p 3 =5 di ff erent patients. One common problem on this kind of data is that some subjects may not have shown any intensity for some AUs in the training set. For such AU/patient tasks traditional supervised learning approaches will not be e ff ective. In contrast, MLMTL methods can naturally handle this scenario. Therefore, in this dataset we focus on assessing the performances of the methods in situations where no instances are provided to learn some of the tasks. The performances of the approaches are measured only on those tasks with no training in-stances, which we will refer to as target tasks hereafter. Asetof T target =2 tasks are selected at random and the instances available for them are not used in the training process. Similarly, another set of tasks T val =2 are selected randomly for tuning the hy-perparameters so that at the training stage, no in-stances from these tasks are used. Finally, m instances are used for the learning process of the remaining tasks and constitutes all of the information provided to the models to produce good estimators for the tar-get tasks. Note that classic supervised learning ap-proaches cannot learn predictors for tasks where there are no training instances. Therefore, we only com-pare with the grouped approaches (GRR, GMTL-C and GMTL-NC). 30 trials for di ff erent values of m were run, the aver-aged results are shown in Figure 5 . The approaches based on tensors outperform their matrix based coun-terparts. A paired t-test shows that the improvement between MLMTL-NC and any other matrix approach is significant ( p&lt; 0 . 01 )forall m .Alsoweseethat MLMTL-NC generally outperforms MLMTL-C. In this paper, we have investigated two approaches for multilinear multitask learning. One being an adap-tation of the low-rank tensor recovery strategy which employs a convex relaxation of the tensor decompo-sition problem. The second is based on an alternat-ing minimization algorithm which optimizes the origi-nal non-convex problem together with a set of Frobe-nius norm regularizers to avoid overfitting. The sets of experiments carried out on both synthetic and real data support the hypothesis that employing multilin-ear methods in the described MTL scenarios is advan-tageous.
 These approaches are useful in a multitask learning scenario where there is a priori information about how tasks are related among them, being these relations expressed as combinations of prescribed factors. This is the case for many real world datasets that contain multiple modalities. Even though such datasets are now commonplace, it is often seen that inter-task rela-tionships are only exploited in one modality such as in stantard MTL. Furthermore, we have seen that mul-tilinear models can obtain predictors for tasks which have no training instances, so long as there are enough training instances for other related tasks. This could potentially be of significant value in scenarios where specific instances in the data are missing or more dif-ficult to gather.
 The proposed methods use one hyperparameter to con-trol the regularization over all matricizations of the tensor ( in MLMTL-C and  X  in MLMTL-NC). An avenue for further study would be to assign a hyper-parameter to each matricization regularizer, in order to trade-o ff the regularizing e ff ect on each matriciza-tion. An interesting goal would be to find a way to tune these hyperparameters without any significant in-crease in computational expense.
 This work was supported by EPSRC Grants EP/H016988/1, EP/H027203/1, and Royal Society In-ternational Joint Project Grant 2012/R2.
 Ando, R.K. and Zhang, T. A framework for learning predictive structures from multiple tasks and unla-beled data. J. Machine Learning Research ,6:1817 X  1853, 2005.
 Argyriou, A., Evgeniou, T., and Pontil, M. Con-vex multi-task feature learning. Machine Learning , 73(3):243 X 272, 2008.
 Argyriou, A., Maurer, A., and Pontil, M. An algo-rithm for transfer learning in a heterogeneous envi-ronment. Proc. European Conf. Machine Learning , pages 71 X 85, 2008.
 Bader, B. W. and Kolda, T. G. Algorithm 862:
MATLAB tensor classes for fast algorithm prototyp-ing ACM Transactions on Mathematical Software , 32(4):635 X 653, 2006.
 Baxter, J. A model for inductive bias learning. J. of Artificial Intelligence Research ,12:149 X 198,2000. Bertsekas, D.P. and Tsitsiklis, J.N. Parallel and Distributed Computation: Numerical Methods , Prentice-Hall, 1989.
 Caruana, R. Multi-task learning. Machine Learning , 28:41 X 75, 1997.
 Ekman, P., Friesen, W. Facial Action Coding System:
ATechniquefortheMeasurementofFacialMove-ment. Consulting Psychologists Press ,1978.
 Gandy, S., Recht, B., and Yamada, I. Tensor com-pletion and low-n-rank tensor recovery via convex optimization. Inverse Problems ,27,2011.
 Kolda, T. G. and Bader, B. W. Tensor decomposi-tions and applications. SIAM Review ,51(3):455 X  500, 2009.
 Kumar, A. and Daum X  III, H. Learning task group-ing and overlap in multitask learning. International Conference on Machine Learning (ICML) ,2012.
 Liu, J., Musialski, P., Wonka, P., and Ye, J. Tensor completion for estimating missing values in visual data. Proc. 12th International Conference on Com-puter Vision (ICCV), pages 2114 X 2121, 2009.
 Lucey, P. and Cohn, J.F. and Prkachin, K.M. and Solomon, P.E., and Matthews, I. PAINFUL DATA: The UNBC-McMaster Shoulder Pain Expression
Archive Database. IEEE Facial and Gesture (FG) , pages 57 X 64, 2011. van der Maaten, L. Audio-Visual Emotion Challenge 2012: A Simple Approach. Workshop ICMI 12 , 2012.
 Maurer, A. Transfer bounds for linear feature learning. Machine Learning ,75(3):327 X 350,2009.
 Maurer, A. and Pontil, M. Excess risk bounds for multitask learning with trace norm regularization. arXiv:1212.1496, 2012.
 Maurer, A., Pontil, M., Romera-Paredes, B. Sparse coding for multitask and transfer learning. Inter-national Conference on Machine Learning (ICML) , 2013.
 Mpiperis, I., Malassiotis, S., and Strintzis, M.G. Bi-linear elastically deformable models with applica-tion to 3D face and facial expression recognition. Proc. 8th International Conference on Automatic Face and Gesture Recognition ,pages1 X 8,2008.
 Romera-Paredes, B., Argyriou A., Bianchi-Berthouze,
N., Pontil, M. Exploiting unrelated tasks in multi-task learning. JMLR -Proceedings Track ,22:951 X  959, 2012.
 Signoretto, M., De Lathauwer, L., Suykens, J.A.K.
Nuclear norms for tensors and their use for convex multilinear estimation, Technical Report, 2012. Signoretto, M., Tran Dinh, Q., De Lathauwer, L.,
Suykens, J.A.K. Learning with tensors: a framework based on convex optimization and spectral regular-ization. Machine Learning ,toappear.
 Signoretto, M., Van de Plas, R., De Moor, B., and
Suykens, J.A.K. Tensor versus matrix completion: a comparison with application to spectral data. IEEE Signal Processing Letters ,18(7):403 X 406,2011. Tenenbaum, J.B. and Freeman, W.T. Separating style and content with bilinear models. Neural Computa-tion ,12(6):1247 X 1283,2000.
 Tomioka, R., Hayashi, K., Kashima, H., Presto, J.S.T.
Estimation of Low-Rank Tensors via Convex Opti-mization. 2010.
 Vargas-Govea, B. and Gonz X lez-Serna, G. and Ponce-
Medell X n, R. E ff ects of relevant contextual features in the performance of a restaurant recommender sys-tem. RecSys 11: Workshop on Context Aware Rec-ommender Systems (CARS-2011) ,2011.
 Vasilescu, M. A. O. and Terzopoulos, D. Multilin-ear image analysis for facial recognition. Proc. 16th International Conference on Pattern Recogni-tion (ICPR), pages 511 X 514, 2002.
 Vasilescu, M. A. O. and Terzopoulos, D. Multilinear independent components analysis. Proc. 2005 Con-ference on Computer Vision and Pattern Recogni-
