 The increasing heterogeneity of client's device, browsing preference, and network connectivity results in a big challenge of pervasive Internet access. The traditional approach is to offer multi-version web pages, each version for a specific type of client devices. This approach is inflexible and inefficient. Changing the web pages is quite troublesome since any change on a particular version must be replicated to the other versions. It is also costly, as more disk sp ace is required to store the multiple versions. In order to reduce the disk space usage, often the number of versions is restricted. Hence, the given web service becomes inflexible. 
Transcoding (sometimes called distillation ), the recent proposal to address client heterogeneity, is defined as the process to convert a representation of a data object into another representation [6]. A typical transcoding system works as follows: the transcoding system  X  usually embedded in a web intermediary  X  retrieves a web object on client X  X  behalf, transcodes the object accordingly, and then sends the transcoded object to the client. Although transcoding offers flexibility and may not need much disk space, it still exhibits many weaknesses, some of the most important ones are server to proxy bandwidth usage, transcoding overhead, and data reusability among transcoded versions of the same object. 
The introduction of a new image standard, JPEG2000 [7], inspired us to propose a new technique to serve pervasive web access more efficiently. Besides superior low bit-rate performance and robustness to bit-errors, JPEG2000 supports progressive transmission [3], in which an image is transferred packet by packet resulting in a gradual display of the image. The gradual display can be performed in pixel accuracy (blurred to clear presentation) and/or in resolution (small to large presentation). In the JPEG2000 standard, an image consists of several layers of quality and several levels of decomposition. Based on the quality layers, the decomposition levels, and two other parameters (color components and precincts), the image is partitioned into packets. Dropping packets representing high quality layers from a JPEG2000 image will give us a low quality (blurred) image. Similarly, dropping packets representing high decomposition levels will result in a low resolution (small) image. The quality and resolution reduction can be performed gradually depending on the number of quality layers and decomposition levels, respectively. Of one image, different representations can be generated without complex computations. Moreover, a low quality or low resolution representation can be improved by adding the necessary packets. If all these features are applied to the Web, we will get a new transcoding approach with the following characteristics: able to work with partial objects (i.e., packets), no complex computations, and high data reuse. 
In this paper, we propose a new approach to perform real-time transcoding of images for pervasive Internet access. Taking full advantages of the trend of new image format and structure, this new transcoding mechanism, which we call it modulation, achieve the followings: (a) negligible overhead in real-time transcoding, (b) on-demand bandwidth consumption between the web server and the client/proxy, (c) maximum reuse of cached image variations, independent of whether it is from lower quality to higher quality or vice versa. Many transcoding systems have been built to meet the demand for pervasive Internet access. The representative ones are proxy-based GloMop [4] and Mowser, and server-based system InfoPyramid [11]. Most of them use implicit information, such as the HTTP Content-Type header, to control when and how they convert between representations. Mogul et al. [8] [10] argued th at such approach could risk the loss of important information since the proxy did not know the semantics of the content. They proposed a server-directed transcoding (SDT) to preserve end-to-end semantics while supporting aggressive content transformations. Han et al. [6] presented an analytical framework for determining whether to transcode and how much to transcode an image for the cases of store-and-forward transcoding as well as streamed transcoding. Chandra and Ellis [1] a quality aware image transcoding based on JPEG Quality Factor. Integration of transcoding and caching systems was done by TranSquid [9]. TranSquid maintains separate caches for different categories of clients (high capability, medium capability, and limited capability). JPEG2000 to provide scalable image presentation. Various representations can be generated only by virtue of requesting necessary parts of a JPEG2000 image. This shows the advanced features of the JPEG2000 standard, compared to the other image standards. While the other two studies focused on fulfilling a client X  X  needs, our previous work tried to cover a wider client base with the help of the caching system. Our work can achieve high data reuse while keeping only a single variant in the caching system. However, due to its ignorance about the JPEG2000 data format, it suffers some drawbacks. Sometimes the image presentation is split into two regions, one with low quality and the other with high quality. In addition, our previous work can only exploit the image scalability in quality, whereas JPEG2000 also support scalability in resolution. The Oxford English Dictionary 1 defines modulation as the action of forming, regulating, or varying according to due measure and proportion. In this paper, we define modulation as the process to obtain an object X  X  representation by means of adjusting (eliminating and/or adding) the building blocks of the object. The building blocks of an object could be layers, packets, fragments, and so forth (for simplicity, we use fragments to represent the object X  X  building blocks for the rest of this section). A typical modulation system works as follows: 1. Receiving a client request, a modulating proxy adapts the request to fit the client 2. The web server modulates the requested object according to the (adapted) client 3.1 Characteristics Modulation is expected to deliver proper object X  X  representations to heterogeneous clients fast and efficiently. Based on the general steps described above, modulation has the following characteristics:  X  Modulation is basically collaboration between web servers and caching proxies.  X  Modulation is an exclusive process. It is an object transformation within a data  X  Unlike transcoding, which only transforms a high fidelity variant to a low fidelity  X  Since modulation involves only elimination and addition of an object X  X  
Preserving the end-to-end semantics should not be an issue in modulation. During specified. The essential semantics of the object should be retained in all presentation levels. So, when the object is modulated, the modulating system will not care about the object semantics anymore. If necessary, the object may be accompanied with additional hints to modulate the object appropriately. SDT [10] can be a good choice to deliver the hints. 3.2 Conceptual Framework We present the conceptual framework of modulation to bring clarity of the transformations within it. The framework is centered on the data model since it is the most important element of modulation. The efficiency of data delivery achieved in modulation is mainly due to the data model. On the other hand, modulating processes are relatively simple, albeit essential, too. The last element of modulation is metadata, whose information can help to modulate the associated object properly. The data model, the modulating processes, and the metadata are elaborated in the following subsections. 3.2.1 Data Model As defined earlier, modulation is an eliminating and/or adding process applied to the building blocks (fragments) of an object, in order to obtain the appropriate object X  X  representation. The fragments can be differentiated one another because each of them has distinct attribute values. The attributes (or properties) can be anything, depending on the object X  X  data format. An image, for instance, may specify color component and quality degree as the attributes of its fragments. Color images in almost all known image formats have at least 3 (three) color components, and the data supporting each color component are usually easy to spot. So , we can gather data fragments of an image based on the color component. In a multi-scan image format, like progressive JPEG and interlaced GIF, each scan adds to the image clarity (or quality). The data associated with each scan can also be id entified, and therefore, retrieving data fragments based on the quality degree is possible, too. If some data fragments, either based on color component or quality degree, can be dropped from an image without the need for recoding and the resulted image is still presentable  X  although it is of low quality  X , that would be ideal for modulation. 
The fragments of an object in the data model are segments and atoms . An object is composed of segments. As pointed out before, the division of an object into segments is based on one or more specified attributes. Thus, each segment carries certain attribute values, which can distinctly identify the segment. Each segment is further composed of atoms. An atom is the smallest entity, which is indivisible. More attributes are required to identify an atom uniquely. We define two types of fragments  X  segment and atom  X  in the data model since an object may be decomposed (fragmented) more than one alternative. The segments are to deal with one decomposing alternative, while the atoms are to support all alternatives. Referring to the image example above, the collection of segments could be based on either color component or quality degree, while the collection of atoms would be based on the combination of the two. The other reason of defining segments is to balance fine granularity and fast modulation. The granularity of atoms is very small. Performing modulation on atoms could take a longer time than that on segments. and atom. The presentation layer is where an object and its representations (variants) layer have specific characteristics. An entity of the presentation layer (in the form of an object) is heterogeneous since it contains fragments with diverse attribute values. As indicated by the data layer X  X  name, the entity is presentable. On the contrary, an entity of the segment layer (in the form of a segment) is unpresentable. However, homogeneity starts to appear on the segment-layer entity; that is, the segment X  X  fragments support one or more similar attribute values (some attributes still vary in value). An entity of the atom layer (in the form of an atom) is homogeneous and has a unique combination of attribute values. Clearly , the atom-layer entity is very primitive and far from presentable. In general, the entity X  X  granularity becomes finer and the entity X  X  fragments become more homogeneous as we go from the presentation layer to the atom layer. 
In the data model (see Figure 1), an object (OBJ) can be decomposed into segments can be divided into atoms A 1 , A 2 , A 3 , A 4 , A 5 , A 6 , A 7 , and A 8 . These are two-way transformations. We can regard them as top-down transformations (from the presentation layer to the atom layer). Conversely, we can also view them as bottom-up transformations (from the atom layer to the presentation layer). Hence, we can say that segments S 1.2 , S 3.4 , S 5.6 , and S 7.8 are constructed by collecting the atoms with the same values for the set of attributes attr 1 . Since there may be more than one decomposing alternative, different sets of segments may be produced. Of the object in the illustration, three sets of segments  X  SET 1 , SET 2 , and SET 3  X  can be formed, each respectively. 
Continuing the previous illustration, Figure 2 shows how various object X  X  representations are constructed. An object X  X  representation is constructed from one or more segments. For example (based on SET 1 ), representation P 1.2 is made up of segments S 1.2 , while representation P 1.2.3.4 is made up of segments S 1.2 and S 3.4 . Note that segment S 1.2 itself is unpresentable. Some headers (metadata) have to be added to segment S 1.2 to become representation P 1.2 . Adding one or more segments to a representation can also result in a more sophisticated representation (e.g., Likewise, eliminating one or more segments from a representation can result in a less sophisticated representation (e.g., removing segment S 3.4 from representation P 1.2.3.4 yields representation P 1.2 ). When we combine the entire set of segments, the yielded representation is indeed the original object. Besides manipulating segments of one set (in this case, SET 1 ), we can also work on segments of the other set (SET 3 , for example) to gather the object X  X  representations, exploiting the other properties of the object. Referring back o the image example at the beginning of this subsection, we could generate varieties of representations by exploiting the color component as well as the quality degree. 3.2.2 Modulating Processes Processes in modulation are quite straightforward, and closely related to the data model. We begin by introducing two modulating aspects (refer to Figure 2): scalability and variability . As explained in the data model, an object may be decomposed in more than one alternative, using different sets of attributes and resulting in different sets of segments. The number of sets determines the number of modulating alternatives. The variability aspect of modulation (horizontal dimension in Figure 2) deals with the modulating alternatives (variations) supported by a representation. Within a particular set, the segments can construct representations representations can have different degrees in quality, or resolution, or the number of color components, or anything else. The scalability aspect of modulation (vertical dimension in Figure 2) deals with the degree (scale) retained by a representation with by modulating an object, we can determine specifically what the variations are (i.e., attributes that can be exploited) and what the scale of each variation is (i.e., the degree of that variation). For example, an image X  X  representation has quality as its variation, with 7 out of 10 as the quality scale. Note that several variations  X  with several corresponding scales  X  may be attached to an object X  X  representation. 
Modulating processes are performed on an object along the two modulating aspects. Consequently, there are two types of modulating processes. The first type, which consists of scaling processes, is to decrease or increase the number of segments in a representation to produce another representation of the same set of segments. It exploits the scalability aspect of an object. The processes to be performed depend on modulating process to reduce the quality of an image. The second type of processes, which includes many varying processes, is to change the target variation, so that a representation can be transformed into another representation of a different set of segments. It exploits the variability aspect of an object. Remind that changing the target variation causes rearrangement of atoms to a different set of segments. A varying process should not work alone, thus it must work together with some scaling processes. In addition, it is usually applied to a representation rather than a full object. For instance, transforming a low quality, color image into a high quality, grayscale image involves a varying process (to change the target variation from quality to color component) and two scaling processes (to decrease the number of color components and to increase the quality). 
Depending on the object X  X  data format, some modulating processes may perform concurrently. Using the last example, the low quality, color image can be transformed first to a low quality, grayscale image (changing the target variation while reducing the number of color components), and then transformed again to a high quality, grayscale image (adding the intersection of high quality and grayscale segments). If the modulating processes cannot perform concurrently, we have to transform the low quality, color image to a high quality, color image (adding the quality segments). After that, we change the target variation (from quality to color component) and reduce the number of color components, resulting in a high quality, grayscale image. The latter method is less efficient than the former since a larger amount of data is wasted. 3.2.3 Metadata The modulating processes cannot modulate an object properly if they do not know what variations are supported by the object and what the current scale is for each variation. All of this supported information should accompany the object. The role of metadata (the common term to label data describing the main data) is quite important in modulation. From modulation X  X  point of view, there are three types of metadata, which are useful for fulfilling the modulation X  X  goals. Those three types of metadata are as follows:  X  Direct support: information directly related to the modulating processes. It  X  Indirect support: information not directly related to the modulating processes but  X  Hints/instructions: guidance from the server or other network elements, also 
Some metadata is embedded on the corresponding object, but some has to be provided separately. Direct support and some indirect support are often attached to the object, either on the object X  X  header or scattered in the object, depending on the data metadata file, in HTML tags, or in HTTP headers. Some metadata is available explicitly, whereas some is implicit and has to be derived from other information. Most direct support is explicit information, while indirect support and hints sometimes have to be derived from, calculated, or estimated from other information. 3.3 Data Format's Requirements So far, we have elaborated the concept of modulation in details, involving its characteristics, conceptual framework, and formulation. Now let us assess how to of modulation and the data model is related to data formats in the real world, we need to examine what kind of data format can support modulation. Firstly, the data format must support scalable presentation, which modulation can exploit. There is no use to apply modulation to a data format if it only supports single presentation. The progressive multimedia formats are often scalable, so they are potential candidates for modulation. Secondly, the scalable presentation specified by the data format should be constructed of fragments, which can be easily identified in and fetched from the data. Lastly, exploiting the scalable presentation, a representation can be built without recoding (decoding and encoding) the data. Apart from the first condition, the specification for the modulation X  X  data format may be loosened here and there. In this paper, we have described a new transcoding approach called modulation. Different from transcoding, which involves a lot of complex factors, modulation is straightforward and fast. Basically, it is an eliminating and/or adding process applied to the building blocks (fragments) of an object. We have outlined the conceptual framework of modulation. The most important requirement for modulation is the scalable presentation supported by the object X  X  data format. Due to its potential benefits and observing the trend of object fragmentation presently, we believe that modulation can proliferate broadly in many types of web objects. 
