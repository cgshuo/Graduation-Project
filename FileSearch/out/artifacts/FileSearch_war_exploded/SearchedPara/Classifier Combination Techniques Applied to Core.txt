 Coreference resolution is the task of partitioning a set of mentions ( i.e. person, organization and loca-tion) into entities. A mention is an instance of textual reference to an object, which can be either named ( e.g. Barack Obama), nominal ( e.g. the president) or pronominal ( e.g. he, his, it). An entity is an aggre-gate of all the mentions (of any level) which refer to one conceptual entity. For example, in the following sentence:
John said Mary was his sister . there are four mentions: John , Mary , his , and sister .

John and his belong to the one entity since they refer to the same person; Mary and sister both refer to another person entity. Furthermore, John and Mary are named mentions, sister is a nomi-nal mention and his is a pronominal mention.
In this paper, we present a potential approach for improving the performance of coreference resolu-tion by using classifier combination techniques such as bagging and boosting. To the best of our knowl-edge, this is the first effort that utilizes classifier combination for improving coreference resolution.
Combination methods have been applied to many problems in natural-language processing (NLP). Ex-amples include the ROVER system (Fiscus, 1997) for speech recognition, the Multi-Engine Machine Translation (MEMT) system (Jayaraman and Lavie, 2005), and part-of-speech tagging (Brill and Wu, 1998; Halteren et al. , 2001). Most of these tech-niques have shown a considerable improvement over the performance of a single classifier and, therefore, lead us to consider implementing such a multiple-classifier system for coreference resolution as well.
Using classifier combination techniques one can potentially achieve a classification accuracy that is superior to that of the single best classifier. This is based on the assumption that the errors made by each of the classifiers are not identical, and there-fore if we intelligently combine multiple classifier outputs, we may be able to correct some of these er-rors.

The main contributions of this paper are:  X  Demonstrating the potential for improvement in  X  Adapting traditional bagging techniques  X  Mul- X  Implementing a document-level boosting algo- X  Addressing the problem of entity alignment  X 
The baseline coreference system we use is sim-ilar to the one described by Luo et al. (Luo et al. , 2004). In such a system, mentions are processed sequentially, and at each step, a mention is either linked to one of existing entities, or used to create a new entity. At the end of this process, each possible partition of the mentions corresponds to a unique se-quence of link or creation actions, each of which is scored by a statistical model. The one with the high-est score is output as the final coreference result. 2.1 Bagging One way to obtain multiple classifiers is via bagging or bootstrap aggregating (Breiman, 1996). These classifiers, obtained using randomly-sampled train-ing sets, may be combined to improve classification.
We generated several classifiers by two tech-niques. In the first technique, we randomly sample the set of documents (training set) to generate a few classifiers. In the second technique, we need to re-duce the feature set and this is not done in a random fashion. Instead, we use our understanding of the in-dividual features and also their relation to other fea-tures to decide which features may be dropped. 2.2 Oracle In this paper, we refer to an oracle system which uses knowledge of the truth. Here, truth, called the gold standard henceforth, refers to mention detec-tion and coreference resolution done by a human for each document. It is possible that the gold standard may have errors and is not perfect truth, but, as in most NLP systems, it is considered the reference for evaluating computer-based coreference resolution.
To understand the oracle, consider an example in which the outputs of two classifiers for the same in-put document are C 1 and C 2 , as shown in Figure 1. The number of entities in C 1 and C 2 may not be the same and even in cases where they are, the number of mentions in corresponding entities may not be the same. In fact, even finding the corresponding entity in the other classifier output or in the gold standard output G is not a trivial problem and requires us to be able to align any two classifier outputs.
The alignment between any two coreference la-belings, say C 1 and G , for a document is the best one-to-one map (Luo, 2005) between the entities of C 1 and G . To align the entities of C 1 with those of G , under the assumption that an entity in C 1 may be aligned with at most only one entity in G and vice versa, we need to generate a bipartite graph between the entities of C 1 and G . Now the align-ment task is a maximum bipartite matching prob-lem. This is solved by using the Kuhn-Munkres al-gorithm (Kuhn, 1955; Munkres, 1957). The weights of the edges of the graph are entity-level alignment measures. The metric we use is a relative mea-sure of the similarity between the two entities. To compute the similarity metric  X  (Luo, 2005) for the entity pair ( R,S ) , we use the formula shown in Equation 1, where (  X  ) represents the commonal-ity with attribute-weighted partial scores. Attributes are things such as (ACE) entity type, subtype, entity class, etc.
The oracle output is a combination of the entities in
C 1 and C 2 with the highest entity-pair alignment measures with the entities in G . 1 We can see in Fig-ure 1 that the entity G-E1 is aligned with entities C1-EA and C2-EP. We pick the entity with the highest entity-pair alignment measure (highlighted in gray) which, in this case, is C1-EA. This is repeated for every entity in G . The oracle output can be seen in the right-hand side of Figure 1. This technique can be scaled up to work for any number of classifiers. 2.3 Preliminary Combination Approaches Imitating the oracle. Making use of the existing framework of the oracle, we implement a combina-tion technique that imitates the oracle except that in this case, we do not have the gold standard. If we have N classifiers C i , i = 1 to N , then we replace the gold standard by each of the N classifiers in suc-cession, to get N outputs Comb i , i = 1 to N .
The task of generating multiple classifier combi-nation outputs that have a higher accuracy than the original classifiers is often considered to be easier than the task of determining the best of these out-puts. We used the formulas in Equations 2, 3 and 4 to assign a score S i to each of the N combination outputs Comb i , and then we pick the one with the highest score. The function Sc (which corresponds to the function  X  in Equation 1) gives the similarity between the entities in the pair ( R,S ) .
 Entity-level sum-rule. We implemented a basic sum-rule at the entity level, where we generate only one combination classifier output by aligning the entities in the N classifiers and picking only one entity at each level of alignment. In the oracle, the reference for entity-alignment was the gold standard. Here, we use the baseline/full system (generated using the entire training and feature set) to do this. The entity-level alignment is represented as a table in Figure 2.
Let A i , i = 1 to M be the aligned entities in one row of the table in Figure 2. Here, M  X  N if we exclude the baseline from the combination and M  X  N + 1 if we include it. To pick one entity out of these M entities, we use the traditional sum rule (Tulyakov et al. , 2008), shown in Equation 5, to compute the S ( A i ) for each A i and pick the entity with the highest S ( A i ) value.
 2.4 Mention-level Majority Voting In the previous techniques, entities are either picked or rejected as a whole but never broken down fur-ther. In the mention-level majority voting technique, we work at the mention level, so the entities created after combination may be different from the entities of all the classifiers that are being combined.
In the entity-level alignment table (shown in Fig-ure 3), A, B, C and D refer to the entities in the base-line system and A1, A2, ..., D4 represent the enti-ties of the input classifiers that are aligned with each of the baseline classifier entities. Majority voting is done by counting the number of times a mention is found in a set of aligned entities. So for every row in the table, we have a mention count. The row with the highest mention count is assigned the mention in the output. This is repeated for each mention in the document. In Figure 3, we are voting for the men-tion m1, which is found to have a voting count of 3 (the majority vote) at the entity-level A and a count of 1 at the entity-level C, so the mention is assigned to the entity A. It is important to note that some clas-sifier entities may not align with any baseline clas-sifier entity as we allow only a one-to-one mapping during alignment. Such entities will not be a part of the alignment table. If this number is large, it may have a considerable effect on the combination. 2.5 Document-level Boosting Boosting techniques (Schapire, 1999) combine mul-tiple classifiers, built iteratively and trained on re-weighted data, to improve classification accu-racy. Since coreference resolution is done for a whole document, we can not split a document fur-ther. So when we re-weight the training set, we are actually re-weighting the documents (hence the name document-level boosting). Figure 4 shows an overview of this technique.

The decision of which documents to boost is made using two thresholds: percentile threshold P thresh and the F-measure threshold F thresh . Doc-uments in the test set that are in the lowest P thresh percentile and that have a document F-measure less than F thresh will be boosted in the training set for the next iteration. We shuffle the training set to cre-ate some randomness and then divide it into groups of training and test sets in a round-robin fashion such that a predetermined ratio of the number of training documents to the number of test documents is main-tained. In Figure 4, the light gray regions refer to training documents and the dark gray regions refer to test documents. Another important consideration is that it is difficult to achieve good coreference res-olution performance on documents of some genres compared to others, even if they are boosted signif-icantly. In an iterative process, it is likely that doc-uments of such genres will get repeatedly boosted. Also our training set has more documents of some genres and fewer of others. So we try to maintain, to some extent, the ratio of documents from different genres in the training set while splitting this training set further into groups of training and test sets. This section describes the general setup used to con-duct the experiments and presents an evaluation of the combination techniques that were implemented. Experimental setup. The coreference resolution system used in our experiments makes use of a Max-imum Entropy model which has lexical, syntacti-cal, semantic and discourse features (Luo et al. , 2004). Experiments are conducted on ACE 2005 data (NIST, 2005), which consists of 599 documents from rich and diversified sources. We reserve the last 16% documents of each source as the test set, and use the rest of the documents as the training set. The ACE 2005 data split is tabulated in Table 1. Bagging A total of 15 classifiers ( C generated, 12 of which were obtained by sampling the training set and the remaining 3 by sampling the feature set. We also make use of the base-line classifier C 0 . The accuracy of C 0 to C 15 has been summarized in Table 2. The agreement be-tween the classifiers X  output was found to be in the range of 93% to 95% . In this paper, the metric used to compute the accuracy of the coreference resolu-tion is the Constrained Entity-Alignment F-Measure (CEAF) (Luo, 2005) with the entity-pair similarity measure in Equation 1.
 Oracle. To conduct the oracle experiment, we train 1 to 15 classifiers and align their output to the gold standard. For all entities aligned with a gold entity, we pick the one with the highest score as the output. We measure the performance for varying number of classifiers, and the result is plotted in Figure 5.
First, we observe a steady and significant increase in CEAF for every additional classifier, because ad-ditional classifiers can only improve the alignment score. Second, we note that the oracle accuracy is 87.58% for a single input classifier C 1 , i.e. an abso-lute gain of 9% compared to C 0 . This is because the availability of gold entities makes it possible to re-move many false-alarm entities. Finally, the oracle accuracy when all 15 classifiers are used as input is 94.59%, a 16.06% absolute improvement.

This experiment helps us to understand the perfor-mance bound of combining multiple classifiers and the contribution of every additional classifier. Preliminary combination approaches. While the oracle results are encouraging, a natural question is how much performance gain can be attained if the gold standard is not available. To answer this ques-tion, we replace the gold standard with one of the classifiers C 1 to C 15 , and align the classifiers. This is done in a round robin fashion as described in Sec-tion 2.3. The best performance of this procedure is 77.93%. The sum-rule combination output had an accuracy of 78.65% with a slightly different base-line of 78.81%. These techniques do not yield a sta-tistically significant increase in CEAF but this is not surprising as C 1 to C 15 are highly correlated. Mention-level majority voting. This experiment is conducted to evaluate the mention-level majority voting technique. The results are not statistically better than the baseline, but they give us valuable insight into the working of the combination tech-nique. The example in Figure 6 shows a single entity-alignment level for the baseline C 0 and 3 clas-sifiers C 1 ,C 2 , and C 3 and the combination output by mention-level majority voting. The mentions are denoted by the notation  X  X ntityID -MentionID X  , for example 7-10 is the mention with EntityID=7 and MentionID=10 . Here, we use the EntityID in the gold file. The mentions with EntityID=7 are  X  X or-rect X  i.e. they belong in this entity, and the others are  X  X rong X  i.e. they do not belong in this entity.
The aligned mentions are of four types:  X  Type I mentions  X  These mentions have a highest  X  Type II mentions  X  These mentions have a high- X  Type III mentions  X  There is only one mention  X  Type IV mentions  X  These false-alarm mentions
In summary, the current implementation of this technique has a limited ability to distinguish correct mentions from wrong ones due to the noisy nature of
C 0 which is used for alignment. We also observe that mentions spread across different alignments of-ten have low-count and they are often tied in count. Therefore, it is important to set a minimum thresh-old for accepting these low-count majority votes and also investigate better tie-breaking techniques. Document-level Boosting This experiment is con-ducted to evaluate the document-level boosting tech-nique. Table 3 shows the results with the ratio of the number of training documents to the num-ber of test documents equal to 80:20, F-measure threshold F thresh = 74% and percentile threshold P thresh = 25%. The accuracy increases by 0.7%, relative to the baseline. Due to computational com-plexity considerations, we used fixed values for the parameters. Therefore, these values may be sub-optimal and may not correspond to the best possible increase in accuracy. A large body of literature related to statistical meth-ods for coreference resolution is available (Ng and Cardie, 2003; Yang et al. , 2003; Ng, 2008; Poon and Domingos, 2008; McCallum and Wellner, 2003). Poon and Domingos (Poon and Domingos, 2008) use an unsupervised technique based on joint infer-ence across mentions and Markov logic as a repre-sentation language for their system on both MUC and ACE data. Ng (Ng, 2008) proposed a genera-tive model for unsupervised coreference resolution that views coreference as an EM clustering process. In this paper, we make use of a coreference engine similar to the one described by Luo et al. (Luo et al. , 2004), where a Bell tree representation and a Maxi-mum entropy framework are used to provide a natu-rally incremental framework for coreference resolu-tion. To the best of our knowledge, this is the first ef-fort that utilizes classifier combination techniques to improve coreference resolution. Combination tech-niques have earlier been applied to various applica-tions including machine translation (Jayaraman and Lavie, 2005), part-of-speech tagging (Brill and Wu, 1998) and base noun phrase identification (Sang et al. , 2000). However, the use of these techniques for coreference resolution presents a unique set of chal-lenges, such as the issue of entity alignment between the multiple classifier outputs. In this paper, we examined and evaluated the ap-plicability of bagging and boosting techniques to coreference resolution. We also provided empir-ical evidence that coreference resolution accuracy can potentially be improved by using multiple clas-sifiers. In future, we plan to improve (1) the entity-alignment strategy, (2) the majority voting technique by setting a minimum threshold for the majority-vote and better tie-breaking, and (3) the boosting algorithm to automatically optimize the parameters that have been manually set in this paper. Another possible avenue for future work would be to test these combination techniques with other coreference resolution systems.
 The authors would like to acknowledge Ganesh N. Ramaswamy for his guidance and support in con-ducting the research presented in this paper.
