 This study investigates the impact of different search feature designs in DLs on user search experience. The results indicate that the impact is significant in terms of the number of queries issued, search steps, zero-hits pages returned, and search errors.
 H.3.7. [Digital libraries] User issues Design, Experimentation search feature design, digital libraries, user search experience This study compares different interface designs for search features in DLs and their impacts on user searching experience. Three DLs tested in this study are the ACM digital library (ACM), the IEEE Xplore (Xplore) digital library, and the IEEE computer society (IEEE CS) digital library. User search experience is operationalized as queries users issued, their search time, search three DLs. All three DLs cover computer science and computer engineering. Xplore also covers other subjects in engineering. IEEE CS and Xplore have overlaps, e.g, part of their proceedings. All three DLs support both fielded search and full-text search and provide basic and advanced search levels. However, they have different design features for the search function. ACM supports both fielded and full text search as the default mode. IEEE CS also supports both fielded search and full text search. However, it provides only four fields. Full text search is also the default search mode. Although Xplore supports both fielded search and full-text basic and advanced search modes, and only in advanced search mode the user can choose full-text search. In terms of results display, ACM presents citation and abstract. IEEE CS presents citation and a few words of the abstract. However, Xplore examine how these differences affect users X  search experience. In order to inform interface design in IR systems, Shneiderman and Plaisant [5] clarify searching and propose a five-phase user-interface framework for text searches. Kengeri, et al. [3] compare the four DLs: ACM, IEEE-CS, NCSTRL, and NDLTD and detect some usability problems of these DLs, including confused link of  X  X rowse X  and  X  X earch X  in NDLTD; too many constraints in simple search and advanced search, and so on. Another case study [2] on usability inspection of DLs calls for reconsidering the interaction design of search and browsing functions. Shiri and Molberg [6] investigate the interfaces of 33 digital collections but without report of user experience. Rao et al. [4] present different interface designs supporting searching or browsing or both for DLs. Belkin et al. [1] demonstrate that the box mode and asking searchers to describe their information problems at length result in longer queries. However, few studies investigate how existing search feature designs in DLs affect users X  experience. Participants : Thirty-five engineering and MLIS students from Rutgers University participated in the study. Task: One search task to locate relevant documents on protecting the on-line repository from fraudulent activity by watermarking was assigned to the participants for searching the three DLs. They were asked to save the first ten results from the most satisfactory items.
 Experimental design: To avoid the learning effects, a Latin-square design was employed to randomize the order of the three systems tested in the experiment. Procedure: The experiments were conducted in a usability lab. An entry questionnaire and a pre-search questionnaire were administered before the experiment. A post-search questionnaire was filled out after the search, and an exit interview after the experiment was conducted. Think-aloud was required. The whole session of the experiment was recorded. Over 94% of participants rated themselves above the medium level of computer experience, and nearly 50% considered themselves experts in using computers. About 83% considered themselves very experienced with searching on the internet. The search performance of the three systems was compared based on the Average Precision (AP) of the participants X  saved results. One-way ANOVA test showed no statistical significance between the three systems in terms of this measure. User searching experience was measured using the following objective metrics: 1) Average query length: The mean number of words in all queries issued. 2) The number of queries issued. 3) Amount of search time. 4) The number of search steps: It refers to how many steps the subjects move before they finally get the results. 5) The number of zero-hits ( X  X o results X ) pages returned. 6) The number of user errors related to searching. *Significant difference was found at p&lt;.01, tested by ANOVA ** Significant difference was found at p&lt;.01, tested by Chi-square Table 1 shows that the queries issued to Xplore were relatively shorter than the other two systems, but not significantly. This means that the subjects issued similar lengthy queries to these three systems. Comparing to ACM and IEEE CS, the participants had to issue significantly more search queries in Xplore ( F (2, 103)=9.025, p&lt;.01). The subjects spent much more time when searching Xplore, though the differences are not statistically significant. Xplore also required the participants to move significantly more steps than ACM and IEEE CS did for completing the search task ( F (2, 103) = 7.167, p&lt;.01). These results indicate that the subjects had to exert more effort to accomplish the search task in Xplore, which may be related to the issue of zero-hits pages. On average, the participants obtained significantly more zero-hits pages from Xplore ( F (2, 103) = had to spend more time, move more steps, and issue more queries in Xplore. In terms of user errors, a Chi-square test found a significant difference in users X  total number of search errors (  X  2 (2, N=27) = 11.56, p &lt; .01). The participants made most errors with the Xplore and least with ACM. Table 2 shows users X  satisfaction ratings on the three systems X  search features. The ratings were based on 7-point Likert scales. Except the statement of  X  X ade great effort to accomplish the task, X  for other statements, the higher the rating, the better the feature. There were no statistical significances among the ratings for the three systems. However, the ratings for Xplore were lower than ACM and IEEE CS in most cases. Apparently, many zero hits returned and poor system support and feedback for query construction and refinement in the search feature design affected the participants X  satisfaction with Xplore. Our results indicate that given similar users X  familiarity with information searching, different system X  X  default search methods ( X  X ll field X  in Xplore;  X  X ll information (including full-text) X  in ACM, and  X  X ull-text X  in IEEE CS) and display formats of search result may lead to significant differences in the number of query issued, mean number of search steps and zero-hits pages returned, as well as the users X  satisfaction ratings on search features. However, this paper only reports part of user searching experience by examining some objective measures and user satisfaction ratings. In the next step of data analysis, how the participants used frustrations will be analyzed. Only after that can a holistic picture of how search interface design impacts user experience be presented. Our thanks to IEEE, Inc. for sponsoring this project. [1] Belkin, N. J., Cool, C., Kelly, D., Kim, G., Kim, J.-Y., Lee, [2] Hartson, H. R., Shivakumar, P., and Perez-Quinones, M. A. [3] Kengeri, R., Seals, C. D., Harley, H. D., Reddy, H. R., &amp; [4] Rao, R., Pedersen, J. O., Hearst, M. A., Mackinlay, J. D., [5] Shenierderman, B., and Plainsant, C. Designing the user [6] Shiri, A. and Molberg, K. Interfaces to knowledge Easy to get start (searching) 5.46 5.66 4.97 
Made great effort to accomplish the task 4.09 3.94 
Satisfaction with final search results 4.77 4.77 4.71 
Satisfaction with overall ease of search 5.37 5.26 4.83 
Satisfaction with overall search feature 5.06 4.89 4.97 Average query length Mean # of queries issued Mean search time (Second) Mean # of search steps Mean # of  X  X ero-hits X  pages Total # of search errors** 
