 ORIGINAL PAPER Xiaofan Lin  X  Ya n X i o n g Abstract As a special type of table understanding, the de-tection and analysis of tables of contents (TOCs) play an important role in the digitization of multi-page documents. Most previous TOC analysis methods only concentrate on the TOC itself without taking into account the other pages in the same document. Besides, they often require manual coding or at least machine learning of document-specific models. This paper introduces a new method to detect and analyze TOCs based on content association. It fully lever-ages the text information throughout the whole multi-page document and can be directly applied to a wide range of doc-uments without the need to build or learn the models for in-dividual documents. In addition, the associations of general text and page numbers are combined to make the TOC anal-ysis more accurate. Natural language processing and layout analysis are integrated to improve the TOC functional tag-ging. The applications of the proposed method in a large-scale digital library project are also discussed.
 Keywords Table of contents  X  Document structure analysis  X  Table recognition  X  Optical character recognition  X  Algorithm combination 1 Introduction There is a high demand for the automatic conversion of printed multi-page documents into electronic format. For ex-ample, the initial steps in most digital library projects in-volve the digitization and recognition of large collections of multi-page documents such as journals, magazines, and books. Besides the recognition of individual pages using OCR software, it is usually desirable to organize the logical units (for example, articles in journals and chapters in books) of a document into a systematic structure to facilitate fu-ture information retrieval. Fortunately, most multi-page doc-uments come with a built-in table of contents (TOC), which naturally reflects the logical structure of the entire document. The TOC describes each constituent article in a concise for-mat: the title, author(s), and start page number. Thus, an ef-ficient approach to multi-page document structure analysis is detecting and analyzing TOC pages.
 been reported in the literature. As the following summary indicates, those methods usually require the creation of spe-cific models for the documents to be processed. The sim-plest approach is to hard-code this apriori knowledge as rules into a system. For example, layout patterns or spe-cial text sequences can be used to segment article reference blocks and to extract fields related to each article. In order to analyze the logical structure of books in Japanese, Lin et al. [ 1 ] introduced a system of TOC page analysis and logical structure extraction by layout modeling and head-line matching. They listed four patterns of text lines on the TOC pages. He et al. [ 2 ] combined geometrical rules (in-dentations) and semantic rules (typical text sequences iden-tifying chapters and sections) to extract hierarchical logi-cal structure in Chinese books. Mandal and Chowdhury [ 3 ] used layout rules to detect and segment the TOC pages from the University of Washington document database. Tsuruoka et al. [ 4 ] constructed models based on indentation features to extract structural elements such as chapters and sections in a book. The RightPages system developed by Story et al. [ 5 ] depended on layout models handcrafted for individual jour-nal titles. Instead of building large number of layout models needed for various TOC pages, Bela  X   X d [ 6 ] formulated several POS tagging rules to delimit articles. Realizing the heavy burden of hard-coding the rules, some researchers adopted machine learning to automate the model generation process. In the CyberMagazine project, Satoh et al. [ 7 ] proposed a system where TOC pages of academic journals were con-verted into bibliographic database by image segmentation. They used training data to learn decision trees for differ-ent kinds of journals. Bourgeois et al. [ 8 ] proposed a proba-bilistic relaxation method to analyze the structure of periodi-cals through training on the layouts of several representative samples.
 riously limit the practical value of the system. As shown in Fig. 1 , there are so many possible layouts for TOC pages that it is almost impossible to exhaustively model all of them. Thus, it is quite likely that the TOC understanding system created for one document collection cannot be used on an-other collection. Learning from examples only provides a partial solution for certain applications such as journal pro-cessing, in which a large number of documents in the same style are expected. Even so, it still requires the careful selec-tion of good samples and manual ground truthing, and thus can become a bottleneck in the whole workflow. Using POS tagging alone also carries limitations. It is language depen-dent and thus different POS models are required for different languages. In addition, the presence of OCR errors can dis-tort the targeted POS patterns.
 analyze the TOC pages in a wide range of multi-page docu-ments without explicit modeling or training. For the TOC detection and article linking stage (in this paper  X  X rticle X  means both an article in a journal/magazine and a chapter in a book), we only make one very universal assumption: Some contents (title, author name(s), and start page number, etc.) of an article will repeat on the title page (the first page of an article). In fact, this method is rooted in the fundamen-tal nature of TOC: A TOC is simply a collection of refer-ences to individual articles of a document no matter what layout it follows. By taking full advantage of this charac-teristic, it is possible to find a widely applicable TOC de-tection/analysis solution. Along this direction, an efficient algorithm is designed to associate both the general text con-tents and the page numbers on TOC pages with those on the body (non-TOC) pages. Such association simultaneously solves the TOC detection and article linking problems. Be-sides, layout information and POS information are fused in the TOC functional tagging stage to make the system more robust.
 Section 3 discusses the core TOC detection and article link-ing algorithms. Section 4 introduces the article separation algorithm. Section 5 presents the TOC functional tagging algorithm. Sections 4 and 5 also cover the experimental re-sults. Section 6 contains a summary and points to the direc-tions of future work. 2 Overall workflow The document processing system is developed for the Dig-ital Content Re-Mastering (DCRM) research project under collaboration between HP Labs and MIT Press [ 9 ]. The goal is to convert MIT Press X  three thousand out-of-print books, journals, and magazines from paper to high-quality elec-tronic version to enable new services such as on-line read-ing and print-on-demand (POD). This depicts the overall workflow of our system as well as the interaction between the TOC Detection and Analysis System (TOCDAS) and other components. The first step is to scan all pages of a doc-ument into color/gray-scale TIFF files. Commercial OCR software development kit (SDK) is used to recognize the images and to generate the text and word bounding boxes. Meanwhile, through a proprietary zoning analysis method [ 10 ], the images are converted into a compressed high-quality PDF that also embeds the recognized text (Fig. 2 ). passed into TOCDAS, which accomplishes several tasks:  X  Automatic detection of TOC pages: In this step, TOC- X  Article linking: TOCDAS detects the title pages and  X  Article segmentation: In this step, TOCDAS segments  X  TOC functional tagging: Each article reference obtained 3 Content association The most unique characteristic of the proposed method lies in content association, through which TOC detection and a significant part of TOC analysis are conducted simultane-ously, helping to verify each other. As shown in Fig. 3 ,aset of candidate TOC pages are selected. The more candidate TOC pages are included, the less likely it is that some true TOC pages will be overlooked by the system, and the more computation will be involved. We have adopted an adaptive strategy. In the initial phase, the first 20 pages are chosen as TOC candidates. If the last few candidate pages are con-firmed to be real TOC pages, the candidate set will be ex-panded by the next 20 pages. The proposed algorithm al-lows TOC pages not to be one after another, but have body pages in between. Actually, the back cover pages also serv-ing as TOC pages in journals such as Pattern Recognition and IEEE PARMI .
 The contents on TOC pages are associated with those on the body pages in two ways. First, general text informa-tion about an article such as the article title, chapter/section name, and author name(s) will be present on both the TOC page and the title page of the article. Second, the start page number of an article will be listed on the TOC page. Due to the distinct natures of the two kinds of information, two algorithms, general text mining and page number match-ing, are designed to handle them separately. A confidence score is then assigned to the candidate TOC page by adding together the general text mining score and page number matching score. If a candidate X  X  score is above a certain threshold, it will be marked as a real TOC page and the body pages linked to it will be marked as title pages.
 TOC page or normal body page, the TOC pages can contain other miscellaneous information besides article references, such as the copyright and subscription information shown at the bottom of Fig. 1 b. 3.1 General text mining Although the idea of associating the text on the TOC pages with the text on the body pages sounds straightforward, it is a very hard technical problem accompanied by several chal-lenges:  X  How to retrieve only the  X  X alid X  matches? Only matches  X  How to search efficiently? We do not know in advance mining algorithm has been designed (Fig. 4 ). 3.1.1 Construct a tree-structured dictionary for each candidate TOC page In order to accelerate the string matching, a dynamic tree-structured dictionary is created for each candidate content page. Once such a dictionary (see Fig. 5 ) is built, it takes much less time to find out if a word is on the candidate TOC page through nonlinear tree search than through se-quential linear search. The experiment has shown a three-fold speedup using this technique.
 3.1.2 Represent the candidate TOC page as a directional graph The next question is how to describe the candidate content pages. The simplest approach is to use a linked list to thread together all of the words. The drawback is that repeated words will appear more than once in the list. So we model the candidate content page as a directional graph with each node as a unique word. On such a graph, each node uniquely corresponds to a leaf element of the tree-structured dictio-nary resulting from the earlier step. The edge between two nodes has a state to indicate the reading order. Figure 6 rep-resents the sentence  X  X  student is a person who studies in a school. X  The biggest advantage of this graph representation is its high efficiency. For example, in order to find out all the phrases starting with the word  X  X , X  we first look up the ver-tex in the tree dictionary. Then we can immediately locate the phrases starting from that word ( X  X  student, X   X  X  school, X  and  X  X  person X ) by traversing all of the edges from that ver-tex. Additionally, if both the start state and the end state are given, one phrase can be uniquely decided. We define such a pair (start state, end state) as a Range . For example, as shown in Fig. 6 Range (4,6) corresponds to the phrase  X  X erson who studies. X  3.1.3 Divide the candidate TOC page into text chunks Many text chunks in the TOC page should have good matches in the title pages. The following algorithm is used to detect the matched Ranges:
RangeList : ={} foreach &lt; Wo rd &gt;  X  BodyPages endfor words in the same rectangle are put together by text chunk-ing. It can be seen that most article titles and author names are properly grouped. Some phrases fall outside of the groups due to OCR errors. It should be emphasized that the grouping result comes exclusively from the earlier text chunking process without using geometric layout informa-tion. 3.1.4 Score the candidate TOC page based on text mining As an initial result from the previous steps, it is possible that one Range will be linked to multiple body pages. Of course, we should keep only one link for each Range. So  X  X inner-take-all X  is employed in the evaluation stage. All of the links to a Range are scored based on several factors, including the match length, the ordering of body pages, the font size, etc. The best link is kept and the other links are deleted. The link scores on a candidate TOC page will be summed up as this page X  X  general text mining score. 3.2 Page number matching Page number matching also contributes to TOC detection and article linking, especially when the text contains OCR errors or general text mining cannot provide enough evi-dence (for example, in books, the chapter titles sometimes can be very short and usually there are no author names for individual chapters). 3.2.1 Score TOC page candidates based on page numbers Page numbers have several distinct characteristics: They are roman numerical strings, usually located at the start or end of each article reference, vertically aligned, and incremental from column to column, from line to line, and from page to page. Thus, the numerical strings located at the beginning or end of lines or vertically aligned are extracted as page number candidates. Through depth-first search, biggest in-cremental subset of the page number candidates is selected. The page number candidates that are not in this incremental subset are removed. Then a candidate TOC page is assigned a confidence score based on the quantity (measured as the number of detected page numbers) and the quality (mea-sured in terms of the incremental pattern, alignment, etc.) of detected page numbers. The sum of this score and the candi-date X  X  general text mining score (described in Sect. 3.1) are used to decide if this candidate is really a TOC page or not. 3.2.2 Link page numbers to articles If the candidate TOC page is confirmed, the page numbers on that TOC page are linked to individual articles. There are two types of page numbers. The page numbers printed on the TOC pages are the logical page numbers (LPNs), which are different from the physical page numbers (PPNs). PPNs are imposed by the scanning process. For example, if all the pages in a book are scanned, the front cover page will be the first physical page. Since it is the PPN that can uniquely identify a page, we have to convert LPNs to PPNs in order to map LPNs to individual articles. Although this conversion depends on the LPNs printed on the body pages, it is not trivial for several reasons. First, many journals do not actu-ally have the page numbers printed on the article title pages. Second, the locations of page numbers on body pages are very different from journal to journal. Third, page numbers on body pages are typically in small font sizes and are more likely to be misrecognized by OCR software than other text. Consequently, solely relying on the page number of a single page can be very fragile. We have designed a more robust weighted histogram filter method, which first decides the offset between the LPN and PPN through statistics across multiple pages and then infers the LPN from the PPN and the offset.
 Step 1 : Extract a set of LPN candidates on Page n :
Step 2 : Calculate the set of differences between the candi-
Step 3 : Run the weighted histogram filter operation to re-for a window that spans the w preceding pages, Page n ,and w following pages. The weight for each page in the window is assigned as: c ( x , n ) = 2 The accumulated strength of offset value diff is calculated as: where conut ( diff , n ) = 1 Then the diff with the maximal strength, denoted as DIFF ( n ) , is selected, and the LPN is inferred from DIFF and n : DIFF ( n ) = arg diff max ( strength ( diff , n )) LPN ( n ) = n + DIFF ( n ) (5) It is worth explaining why we calculate an offset for each page instead of a common offset for the whole document. If all of the pages in a book or journal are sequentially scanned without any omissions or mistakes, the offset will be the same for every page. However, in the real-world operations, it is inevitable that some pages will be skipped either in-tentionally or by mistake. For example, the operator may choose to skip blank pages to increase efficiency. In such situations, the offset is no longer a constant throughout the whole document. The choice of window width w reflects the tradeoff between the statistical accuracy and the frequency of local variations. If w is large, more pages will fall into the window and the statistics can be more accurate, but the averaging effect may overwhelm real local variations of the offsets. In our experimental results, when w is set to 6, this algorithm works very well on a wide variety of body page layouts, in the absence of page numbers on title pages, or with moderate degree of OCR errors. With the LPN to PPN conversion, each page number x on the confirmed TOC page is linked to Page PPN( x ). 3.3 Selection of title pages and links Based on each confirmed TOC page, a non-TOC page P is assigned a title-page confidence score that is the sum of the general text mining score and the page number matching score, and the maximum is P  X  X  final title-pagescore. Title page score ( P ) = max where T is the number of confirmed TOC pages.
 be confirmed as a title page. After the title pages are decided, all links established in general text mining and page number matching are revisited. Only those that connect a confirmed TOC page and a confirmed title page are kept. 3.4 Applications of content association We have explored several useful applications built on top of the core content association algorithms: document naviga-tion [ 16 ], journal splitting [ 17 ], and scanning quality assur-ance.
 pages are identified, navigation capabilities can be embed-ded into electronic documents. The navigation is added to the PDF files in two ways. First, a bookmark is automatically generated for each article (left panel of Fig. 8 ). The text de-scription of a bookmark comes from the Range(s) pointing to an article. When the user clicks on a bookmark, the right panel will display the linked article. Second, the Ranges on the TOC pages are marked out and linked to individual arti-cles (see the right panel of Fig. 8 ). When the user clicks on a text region on the TOC page, the PDF viewer will display the linked title page.
 is straightforward to split the whole document into smaller files, each of which contains an article or the TOC. The PDF file for a whole book/journal can have a size ranging from several megabytes to tens of megabytes. It can take a long time to download the file through low-speed Internet access, such as the dial-up service. Accordingly, if the book/journal is already split into smaller units such as individual articles, the user can simply download the contents he/she is inter-ested in. In addition, by splitting a book/journal, the content provider can increase sales by selling the contents in finer granularity. The content association algorithm has been used to split 256 books in MIT Press X  Classic Series into individ-ual chapters, which are available from MIT X  X  Cognet web-site, a brain science online community [ 18 ].
 benefit scanning quality assurance (QA). The program auto-matically pinpoints the pages where the offset between PPN and LNP changes. A human operator can then inspect those pages to make sure that no pages have been left out by mis-take. This tool has been incorporated into our re-mastering system X  X  QA process. 3.5 Experimental results The content association algorithm has been tested on jour-nals and books from MIT Press. Table 1 shows the statis-tics of the testing samples. Since content association ac-complishes two tasks, TOC page detection and article link-ing, we measure them separately. TOC page detection can be evaluated by the detection error rate (the sum of dele-tion and insertion rate). Article linking can be measured in a number of ways. We can count the percentage of words on the TOC pages correctly linked to title pages. However, this measurement requires a considerable amount of manual ground truthing and is sometimes ambiguous. For example, thor names is linked to a title page because of OCR errors or even the documents themselves. On the other hand, when title pages are incorrectly inserted or deleted, they will defi-nitely have big impact on the downstream applications, such as document navigation or document splitting. So we use the title page detection error rate (the sum of deletion and inser-tion rate) as the metric of article linking. Table 2 shows the testing results. We can see that all of the TOC pages are cor-rectly detected and the title page detection error rate is 2%. Besides, if we turn off general text mining and only use page number matching, the title page detection error rate would increase to 3%. This demonstrates the benefit of combining general text mining and page number matching approaches. First, the test is a strictly open test. None of documents we have seen in the algorithm design stage have been used in testing. Second, the second column of Table 1 shows that the documents come from nice journal titles. As can be seen in Fig. 1 , different journal titles can have completely different layout styles. These two facts strongly support our opening statement: The content association method can be directly applied to a wide range of documents without the need to build or learn the models for individual documents. book and one journal. The computer is a 733 MHz Pentium III workstation running Windows 2000. The time spent on content association is only a fraction of the time spent on the re-mastering process of converting TIFF files to PDF files. The re-mastering of the 518-page book takes about 15 min and the re-mastering of the journal takes about 5 min (Table 3 ).
 engines affects the linking algorithm. The results based on two commercial OCR engines (see [ 19 ] for more details on the engines) are compared. Engine A has a character error rate of 0.46%, while Engine B has an error rate of 1.1%. More words on the TOC page are linked to articles by us-ing Engine A. In addition, some articles that can be detected with Engine A are missed when Engine B is used. In conclu-sion, a superior OCR engine can still help the content associ-ation algorithm, although the proposed algorithm is already not very sensitive to OCR errors. 4 Article separation Figure 9 displays the hierarchical logic structure of a TOC. On the top level, a TOC is composed of a number of article references. Each article reference consists of several logi-cal elements: author (this element may be absent in books), title, and page number. Words are the most basic objects. Word groups sit between articles and words. They are col-lections of words belonging to the same line and the same logical element. Through the content association process, many words on the TOC pages have already been linked to individual articles. Guided by the word X  X rticle linkages, the article separation algorithm works in a bottom X  X p manner. It first lines up words into word groups and then clusters word groups into article reference blocks. This bottom X  X p approach does not assume any TOC page layout templates or rules, which are generally required by many existing top X  down article separation methods and can limit the applica-bility of those methods. On the other hand, it is also dif-ferent from common bottom X  X p layout analysis algorithms by fully leveraging the word X  X rticle relationship obtained in content association. 4.1 Word group generation The first step in article separation is to generate word groups. Each physical line is traversed from left to right. If the gap between the current word and the previous one is larger than 1.5 times the median word gap of this page, a new word group is created to include the current word. Otherwise, the current word is added to current word group.
 page) of a word group is the dominant article ID label of its constituent words. 4.2 Clustering word groups into article reference blocks Under the assumption that no two articles will start on the same page, the word groups are then clustered into blocks based on geometrical proximity and the article IDs of the word groups. The following sequential merge algorithm is introduced:
Step 1 : Find an unexpanded word group g x = g 0 (the seed)
Step 2 : Mark current word group g x as expanded and add
Step 3 : Evaluate all unexpanded word groups g j in the
Step 4 :Every g j selected in Step 3 becomes the current Step 5 : Increase i by 1 and go back to Step 1.
 blocks generated by the article separation algorithm on one TOC page. The statistical result will be presented together with TOC functional tagging in Sect. 5. 5 TOC functional tagging Functional tagging divides an article reference block, as identified by the article separation algorithm, into seman-tic elements: author names, articles, and page numbers. Al-though POS tagging is very useful for this purpose [ 6 ], it is still a great challenge to design a TOC functional tagging al-gorithm that can work reasonably well across a wide range of documents. First, author names and titles sometimes can-not be differentiated from each other by POS tagging alone. Second, POS tagging itself is still an open problem and will make mistakes. Third, OCR errors will adversely affect POS tagging [ 20 ]. On the other hand, the physical layout of a doc-ument provides another imperfect source for TOC functional tagging. Thus, we have introduced a method that combines POS tagging and layout.
 ence block consists of a set of word groups. For example, the article reference block in Fig. 11 consists of five word groups shown in Fig. 12 . Each word group has four possible number ( n ). So this block can potentially have 4 ferent tagging combinations, such as ( t , t , t , a , a , plied to reduce the search space. For example, numerical g1: Spanish and Nahuatl Views on g2: Smallpox and Demographic g3: Catastrophe in Mexico g4: Robert McCaa g5: 397 g1: CN CC PN CN PREP g2: CN CC PN g3: CN PREP PN g4: PN PN g5: NUM strings should never be tagged as an author name. The re-maining combinations are then evaluated in terms of both POS tagging and layout analysis. Each tagging combination is assigned a total cost that is the sum of the linguistic cost and the layout cost. The tagging combination with the least cost is then selected as the final result.
 Figure 13 shows the POS tagging results of Fig. 12 .Then the linguistic costs for different logical elements are as-signed based on the factors listed in Table 4 .
 in the same logical element. A straightforward measurement is to calculate the direct distance between any two groups. However, this direct distance does not reflect the hidden con-nections that play an important role in human judgment. In Fig. 11 , although the direct distance between Groups 1 and 3 is not small, the two groups do not look far away from each other due to the bridge effect of Group 2. Based on this observation, we have adopted Floyd X  X  all-pairs shortest-path algorithm [ 22 ], which calculates the shortest distance between every pair of points in a graph. We first extract the distance matrix: D =[ d d ij measures the direct distance between g i and g j .Ifthetwo word groups are on the same line, the distance is determined by their horizontal distance. If they are on the neighboring two lines, the distance is determined by the degree of their horizontal overlap.
 tance matrix D is then converted to the shortest-path matrix S , which measures the connectivity among all word groups in the block. The key difference between S and D is that S takes into account the indirect path between two points in or-der to find the global shortest path. Then the layout cost from this title region is calculated as the sum of the elements in S . Similarly, the costs for the author element, page number element, and section element are also calculated. The total layout cost for a block is a sum of them.
 tional tagging methods on 19 journals from MIT Press. They cover 10 different journal titles and consist of 306 arti-cles. The accuracy for article separation is 94.0%. The tag-ging accuracy is 100% for page number elements, 94.4% for title elements, and 90.5% for author elements. The accuracy is defined as the percentage of correctly iden-tified article references or logical elements. We have not counted the accuracy for section elements, since they of-fer little value for cataloging purpose. Most confusion be-tween title elements and author elements is caused by OCR errors and non-English author names, which can gen-erate out-of-vocabulary words and result in POS tagging errors. 6 Conclusions After a comprehensive survey of existing work on TOC detection and analysis, we have pinpointed the fundamen-tal characteristic distinguishing TOCs from ordinary tables: The contents in the TOC are literally associated with those on the body pages. Taking advantage of this characteristic, we have designed and implemented a complete TOC pro-cessing solution. Through content association, document-specific models are avoided and the system can thus directly process a wide range of documents without tuning or train-ing. The results of content association also provide guidance to later processing steps. For example, the article separation algorithm incorporates the word X  X rticle linkage knowledge into the bottom X  X p clustering. In addition, significant effort has been devoted to making the system more robust against OCR errors and other variations that are inevitable in a se-rious high-volume digital library project. First, information fusion is widely employed. Content association combines general text mining and page number matching, and TOC functional tagging considers both the linguistic cost and the layout cost. Second, the method is mostly statistical rather than rule based. In intermediate steps, quantitative scoring instead of hard decision is used as much as possible. We have also enumerated several interesting applications, such as automatically embedding navigation capabilities into re-mastered documents and scanning quality assurance. similarity to a seemingly unrelated but hot topic: Web link mining. Most modern Internet search engines, including Google, take advantage of the fact that the authoritative Web pages are the ones frequently pointed to by other pages. In this paper, the TOC pages are the hub pages referenced by other body pages. However, there is an important dif-ference between this work and Web link mining. The link-ing information is explicitly marked with special HTML tags in Web link mining, while the links are only implied in the form of repeated sentences or phrases in the con-text of TOC analysis. In order to discover those hidden links, we have designed an efficient algorithm based on dy-namic tree dictionary, TOC graph representation, and text chunking.
 ing of non-English or multi-lingual TOC pages. The ba-sic operation of content association is text string match-ing, which works on all languages. The article separa-tion algorithm operates on top of content association and physical layout, and thus it is independent of the lan-guages as well. However, TOC functional tagging uti-lizes on the language-dependent POS tagging and currently only supports English. We plan to add support of other languages.
 References
