 We present an algorithm for language identification, in particular of short documents, for the case of an Internet domain with sites in multiple countries with differing languages. The algorithm is significantly faster than standa rd language identification methods, while providing state-of-the-art identification. We bootstrap the algorithm based on the language identification based on the site alone, a methodology suitable for any supervised language identification algorithm. We de monstrate the bootstrapping and algorithm on eBay email data and on Twitter status updates data. The algorithm is deployed at eBay as part of the back-office development data repository. I.2.7 [ Computing Methodologies ]: Artificial Intelligence  X  Natural Language Processing.
 Algorithms, Performance, Design. Language identification, large da ta, statistical model, boosting. In today X  X  interconnected global Internet community it is quite common that Internet-based businesses (e.g., eBay, Twitter) operate sites in different langua ges (e.g., ebay.com, ebay.de, ebay.fr, etc., or twitter.com, twitter.com/?lang=de, twitter.com/? lang=fr, etc.). Many such sites ha ve user-contributed content data, private or public, such as web pos tings, reviews, bl ogs, or emails. Usually, but not always, either the site which the user employs to generate the content or more often the site on which it is to appear (or which the recipient uses in case of a message) dictates the language used by the creator of the content. Typically one may expect that knowing the site language alone allows language identification of user-provided co ntent with high precision. On the other hand this approach is not e xpected to be pe rfect, and one can employ standard language identifi cation algorithms, which even if not specifically trained for the corpus at hand, also achieve usually still high precision. The task is to improve over either approach to determine the language of the user-created documents. The general idea, of course, is to combine the language identification based on the site language or language from a user profile with some other supervised algorithm. In particular in a web setting one usually has lots of data, however obtaining suitable accurate labels constitutes the bottleneck. Our approach is to use data labeled by the site or the user profile alone for training, and we build a custom model usi ng a supervised algorithm. Then we combine the language identification based on the site or user profile language with the newly trained supervised model to create a final model. The met hod of combination could be as simple as a linear model. Most algorithms for language iden tification, and in particular most modern ones, employ a sta tistical approach, often not word (dictionary) based, but rather character or n-gram based [1,2,3]. However, these algorithms often perform not optimally if the documents to be classified are short [4]. The archetypical example of shorts text on the web nowadays are Twitter status updates (so-called tweets), which by design are limited to 140 characters. On the other hand, tweets are not the only short prevalent documents on the web. This research grew out of the need to accurately identify the language of user-to-user email communications on eBay. We found that these often also are rather short. This may well be due to the limited setting of what the messages are usually about (e.g.,  X  X o you have this also in a different color? X  or  X  X ould you ship the item to Europe? X , or the complete email could also just be the single word  X  X es X ). It turns out that on these user-to-user email messages a dictionary-based approach does indeed perform very well. We then also apply our methodology to Twitter status updates, which besides being short, are also well known to be difficult to classify due to the vast amount of Twitter-spe cific slang and abbreviations. Our specific approach is to look onl y at the first two and the last two words of the documents (emails , tweets), and use the site or user profile determined language to build frequency tables on a training set. In the email settin g, the intuition of looking at the opening word of a message was the observation that many users start their text with one of onl y relatively few and very language-specific terms, such as hello, bo njour or goedendag. Then again, starting an email with the name of the recipient is also very frequent, which leads to a diluti on of the signal captured by the first word. This is why we also consider the second word, which then in turn tends to be language-specific. An analogous reasoning applies to the end of the messa ges, common endings are strongly indicative of the language, such as good-bye, ciao, or Tsch X  X . However, if the email is ended with the name of the sender, then the second-to-last word carries this signal. This approach also works quite well for tweets, which start often with common opening words or abbreviations and often finish with common ending words or abbreviations. For example in English common opening words or abbreviations are I, lol, you, my, ..., while common ending words or abbreviations are you, now, me, smh, ..., and similarly for other languages. Normalizing these frequency tables across the languages for each word separately leads to estimated language probabilities, and summing up these probabilities with suitable weights creates a dictionary-based language identifica tion model, with weights on the opening and closing words only. Combination with the site or user profile language identification is easy, the indicator variable is just another (binary) probability weighted into the sum. Because in the end only the first two and the last two words of a document need to be investigated, the algorithm is computationally very lightweight during scoring, and correspondingly very fast. Additionally our method needs no human-labeled training data, rather it bootstraps itself from the (hopefully relatively strong) learner given by language identification based on the site or user profile alone. This is also where the nature of today X  X  high-volume web sites helps our approach: the hosting domain generally captures vast amounts of such trivially machine-labeled data for free. Furthermore, as a consequence of being built on large amounts of data, the resulting models are very stable. On the eBay data where we have access to a longer period of data, the model still performs exceptionally well one year after fitting. The algorithm is deployed at eBay as part of the back-office development data repository. With the investigation of the Tw itter data, which has the relatively weaker language identifier given by the user profile language, we conclude that the (a-priori) precision of the site or user profile language determination is a key ingredient to our approach: the stronger the site or user profile language learner, the better the algorithm and bootstrapping. Applications of language identification are not subject of this paper, but of course could include the o bvious ones, from spam filtering, fraud detection, sentiment analysis, to customer service queue routing, among others. The specific data leading to the development of the algorithm were the (non-public) emails between se llers and buyers (or interested members, in case of a pre-sale communication). These are all called member-to-member emails (or messages), in short, M2M emails. The eBay sites that are part of this study use one of the following seven languages: English, German, It alian, French, Spanish, Dutch, or Polish. This glosses over subtle differences, for example, British English, Australian English, and American English are simply identified as English, similarly for French being spoken in France, parts of Canada, parts of Switzer land, and parts of Belgium, the same holds for German being used in Germany, Austria, and parts of Switzerland, and finally Dutch for the Netherlands and parts of Belgium. M2M messages on eBay usually are generated within the MyMessages facility, which wraps each message with a template. What the template precisely contains is not important, with the exception that the template header before the message ends on  X  X ear recipient-user-name X , and the footer template after the message starts with a signature of the form  X -sender-user-name X . Hence many eBay members do not include the recipient name as part of their message, and neither do they include their own sender name as a signature. This is important for our algorithm, as otherwise if one looks just at the beginning and the ending of a typical email message one would generally have to discard (and identify!) user names in order to derive a generalizable model. On the other hand, as explained in the introduction, we consider the leading two words and the ending tw o words, just in case a greeting by name or a signature user name is still present in the message. A key feature of the eBay M2M data is that it contains many short documents; one-and two-word documents are not uncommon. Obviously, the strongest indicator of the email language is the site. In our setting we extract the site from the email template, thus avoiding a user database lookup to find the registered site of the user. However, in particular because of a small percentage of template-free emails this results in missing values for some of the records. Also, some users write em ails in a language different from their site language, which comb ined with the missing values ultimately gives a site-language accuracy of about 96% in identifying the language of emails. More precisely, in case the sites of the sender and the recipient differ, this is based on the site of the recipient, which has proven to give slightly better language identification performance. This percentage was established on a random un-biased sample of about 15K emails from June 2010 which were human-labeled into the seven languages considered. For the eBay data, our goal is to improve on this 96% accuracy rate. As the obvious source of short web documents in multiple languages, we also consider Twitte r status updates. We used the streaming API to capture a 1% sample of public status updates made freely available by Twitter, commonly known as the  X  X prinkler X  data feed [6]. For this study we excluded several languages that could easily be separately handled from the Twitter data feed because these languages use a different font enc oding: Japanese (ISO 639-1 code  X  X a X ), Chinese (both  X  X h-cn X  an d  X  X h-tw X ), Korean ( X  X o X ) and Russian ( X  X u X ). Similarly to how we grouped all different flavors of English from eBay X  X  data into simply one English language, we mapped the intricately related macro-languages Malay ( X  X sa X ) and Indonesian ( X  X d X ) simply into Indonesian ( X  X d X ). The Twitter user profile languages (in the following often called  X  X ite languages X ) that are part of this study were determined on the training set and comprise the following eleven languages: English, Spanish, Portuguese, French, Dutch, Turkish, Indonesian, German, Italian, Filipino, and Hindi. In the end there actually was not a single Twitter status update in Hindi in the test sets, however all code used all eleven languages. An important point is that while th ere are no missing values in the language field in the Twitter user profiles, the language specified in the user profile is only about 80% accurate if used to identify the language used for status updates. However, the public user profile distributed by Twitter as part of the streaming API also contains a location field. This field is populated for about three quarters of all status updates, but of these about absolute 15% of the values contain not even a single letter (e.g., contain a  X  X miley face X ), leaving about 60% of locations with potentially identifiable values. Not all of these values map to an actual physical location on a map, but some contain other whimsical entries (e.g .,  X  X n my kitchen X ). In the end, we prepared a simple parsing model on the location field by using both the geographic location and the language used to specify the location to assign a language to the location (e.g. both  X  X hicago X  and  X  X n my kitchen X  would be a ssigned  X  X nglish X ). We managed to map about 50% of status updates to a location language (from an upper limit of 60%). Finally, we assigned this location language (if present) as the site-language, t hus overriding the user profile language. As it turns out, the majority of Twitter users in Indonesia and in the Philippines post in English, and hence for these two locations we did not override the user language from the profile with the location language. With this improved site language from using both the language field and the location field from the user profile, the combined prediction accuracy if using it to predict the language used for the status updates is about 87%, comparing favorably to the 80% accuracy if using the user profile language alone. In the rest of this study we use both of these Twitter site languages (user profile language alone, or user profile language and location language combined), and we list the results in separate tables. The algorithm expects only absolutely minimal data cleaning. Generally, digits and letters including umlauts and accented letters make up the words, anything else is a word break. For the eBay message data, after breaking the document into words, all words containing a digit were removed, which removed many user-selected user ids and names. For the Twitter data, on the other hand, numbers are often used as abbrev iations (e.g.,  X  X r8 X  for  X  X reat X ), and hence these words were allowed. As a further concession to the abbreviated nature of Twitter stat us updates, we also allowed apostrophes (e.g., in this study  X  X   X  X  X  is considered one word in the Twitter data, but two words in the eBay data). Similar to the eBay data we removed Twitter user names from the status updates (e.g.,  X  X Example X ) and we saw a slight improvement in the language identification. Finally all the lette rs are transformed into a uniform case (lower case, for example). On the other hand, no stemming or aliasing is performed. After pre-processing, we collect s ite or user language counts for each of the first two words and each of the last two words of each document. For the eBay M2M data we found during our experimentations that it helps slightly to combine the frequencies from the last two words into a single table. Hence for the eBay M2M data we created only three frequency tables, one each for the first words, the second words, and one for the combined collection of second-to-last and last words. The likely reason for this improvement is that in the combined frequency table stray entries from signature user names will obtain relatively less dominance as they would in a last-word-only table. The Twitter status update data contains no signatures at the end of tweets, and we observed no such improvement by combining the last two words into a single frequency table. We kept those tables separate. The frequency tables can be both pr e-and post-processed. The most obvious post-processing is to remove entries with insufficient total frequency. For both the eBay M2 M data and the Twitter status updates data, we collected the frequency tables on a training set of several million documents, and we set the minimum frequency to 50 for the individual (first, second, second-to-last, last) word tables, and to 100 for the combined last words table in the case of eBay M2M data. Additional pre-processing may include removal of undesirable words (e.g., words which are known to carry no signal) from the document before extraction of the leading and terminating words for the frequency tables. For example, for the eBay M2M data we excluded the words ebay and paypal, while for the Twitter data we excluded the abbreviation RT (short for Re-Tweet). Assigning a language to a given doc ument is highly efficient. It amounts to extracting th e first two and the last two words of the document, and for each of these four words w k ( k = 1,...,4) and for each of the predetermined set of languages L looking up the corresponding frequency f kL (w k ) in the count tables k ( k = 1,...,4), and computing a linear weighted sum. Recall that for the eBay M2M data we created one combined table for the last two words ( k = 3,4), which is to say in this case f 3L languages L and all words w . Let f k (w) denote the sum of the frequencies for a word w in table k across all languages. Given weights W k ( k = 1,...,4), the language score is computed for each language L as W 3 f 3L ( w Lacking any prior, we chose all weights W k = 1. The final language assigned is the language w ith the highest score. In case of a score tie, a predetermined preference order based on the expected distribution of languages could be used. We chose a simple approach and combined the above language identification scores s L in a linear fashion with the site language. Let  X  ( L ) be the indicator function for the site language (value = 1 if L is the site language, 0 otherwise), and let W S be a pre-chosen site weight, then the combined site and algorithm score is For both the eBay M2M and the Twitter status data, we chose W = 1.75, with the idea that the site or user profile language is by itself already very accurate, and this way at least two of the features in the weighted sum s L have to trigger at values close to 1 to override the site or user profile language. As a first evaluation of our appr oach, we fitted the models and tested the algorithm on proprietary eBay M2M data. Specifically we used the samples listed in the table below. Time Period Volume Filter Labeled By Sept 30, 2010 20K Cleaned Human In the table above,  X  X leaned X  means that records that were using more than one language, or that were ambivalent were removed (e.g., a message consisting of di gits only could be any language).  X  X iased X  means that the give n distribution of languages was ignored, and rather a specific num ber of documents was extracted for all languages in order to generate a reference set; these were also filtered so that each sender was allowed only one message in order to capture as much different user behavior as possible. The data set listed in row 1 is a superset of those in rows 2 and 3. No attempt has been made to keep the data sets from rows 2 and 3 disjoint. We compared our algorithm to two industry-standard algorithms, which we also combined with th e site language determination for further competitiveness. We prepared a set of reference documents for each of the seven languages, these are the 1,000 documents each from June 2010 in row 2 of Table 1. We used Gzip as the compression tool, and pruned the 1K reference document files so that for each of the seven languages the resulting gzi pped reference set had roughly the same byte count (about 64KB each). We also used a much smaller reference set (about 2K B each) as a second classifier. Classification is as follows: Give n a document, for each of the language reference sets add the given document and compress the resulting document set using gzip. Among the seven resulting document sets that one reference set that grew relatively the least must have had similar statistical byte patter ns, and its language is selected [4]. One immediate drawback of this algorithm is that even for a one-word message the algorithm needs to compress seven 64KB (or 2KB, respectively) data sets, which is a relatively huge computational effort. T hus the expected run-time performance is expected to be very poor if compared to other methods. For the Zipping algorithm, a voting scheme determines whether to use the gzip-determined language or the site language. The voting scheme is as follows. For any language L , let r L be the ratio of the size of the gzipped reference set with the document attached to the size of the gzipped reference set without the document attached. Let r L (for the given document), and let W g be a pre-determined weight. The voting scheme is testing W g &lt; r L language, otherwise the gzip language. The ratio of the two smallest ratios is thus taken as a measurement of how sure the gzip determination is of its result. For the 64K (2K) sized reference sets we experimentally fit W g = 0.999925 (0.997, respectively) using the human-labeled June 2010 data (row 3 of Table 1). Note that this combina tion is not bootstrapped but uses a human-labeled dataset to obtain the optimal combination weight. It is included here for comparison purposes only, and not to illustrate the methods described in this paper. We used the publicly available imp lementation from Cybozu Labs [2]. This Java software comes out-of-the box pre-trained for 49 languages, the training being based on Wikipedia documents. We used the tool as it is, but al so used it with the 49 languages restricted to just the seven la nguages under consideration, and finally, we also retrained the un derlying classifier using a sample of the eBay M2M data, using the same reference sets as for the Zipping classifier from June 2010. The classifier software drops short documents during training, in order to avoid this, we repeated any short message in side the eBay M2M reference training set until it exceeded that drop cut-off (a simple study showed this repetition did improve the performance because it increased the training data size). Note that this custom language profile generation follows the same bootstrapping philosophy as we used for our algorithm, no human-labeled data was needed. The software described in th e previous section outputs a probability score for the assigned language. If this score exceeds the prior precision known of the s ite language (0.96 for the eBay M2M data), then the language a ssigned by the Bayesian model was used, otherwise the si te language was used. Bayes49WSite Na X ve Bayes 49 languages with site, Wikipedia profile 
Bayes7WSite Na X ve Bayes 7 languages with site, Wikipedia Bayes7MSite Na X ve Bayes 7 languages with site, M2M profile Based on the discussions and explanations of the previous sections, Table 2 on the previous page lists the models we compared. Below are various result tables. Note that the June 2010 data is the training data, while the other data sets are unseen test data (out of sample, out of time). Runtimes of the  X  X ithout site X  models are essentially equivalent. Gzip64Site 0m 0.5s 30m 0s (not performed) Bayes7MSite 0m 0.3s 0m 3.5s 0m 19s As a second cross-domain applic ation, we determined the language profiles and tested the algorithm on public Twitter status update data. Specifica lly we used the following samples listed in the Table 6. In this table,  X  X leaned X  means that records that were using more than one language, or that were ambivalent were removed (e.g., a message consisti ng only of a smiley face could be any language). Additionally all languages not part of the set of eleven Twitter languages considered were removed. The data from October 2011 was used to generate the location to language mapping explained at the end of Section 3. It was not otherwise used. The November 2011 data was used in several ways. First, it was used to generate the word frequency tables. Second, a subset was used to generate custom profile s for the Character n-gram Na X ve Bayesian classifier. The December 2011 data was used as a near-term small test set, in particular as a sanity check, whil e the January 2012 data was used as a longer-term test set. Oct 29-31, 2011 750K No Not labeled 
Nov 13-19, 2011 
Dec 11-17, 2011 
Jan 8-10, 2012 We followed our approach to evaluating our method on the eBay M2M data, and compared our algorithm to the industry-standard Standalone Character n-gram Na X ve Bayesian classifier, which as for the eBay data we also combined with the site language (in the user profile sense, see Section 3) for further competitiveness. We did not evaluate the Zipping algorithm on the Twitter data as it had proved to be non-competitive on the eBay data. As before, we used the publicly available implementation from Cybozu Labs [2], and as for the eBay data used it with all 49 languages, the training being based on Wikipedia documents, and also restricted to just the subset of the considered eleven Twitter site-languages. Similar to the eB ay data we also retrained the underlying classifier using a sample of the Twitter data. For this we generated two sets of language profiles, once using the user profile language as the label, and once using the language determined from the location and language from the user profile as the label (see Section 3). The language profiles were generated on the November 2011 data, limited to at most 100K records per language (some less-used languages stayed significantly below this). As before for the eBay M2M data we appended to itself any short message inside the Twitter reference training set until it exceeded the software X  X  internal minimum message-length cut-off, and as for the eBay data a simple study showed this repetition did improve the performance. Once again this implements the same bootstrapping philosophy as we used for our algorithm, no human-labeled data was needed. As previously explained, the so ftware described in the section above outputs a probability score for the assigned language, and if this score exceeds the prior precision known of the pre-assigned site language, then the language assigned by the Bayesian model was used, otherwise the site lan guage was used.  X  X re-assigned site language X  in the previous sentence means either (a) the language from the user profile with an accuracy estimate of 80%, or (b) the language determined from the combination of user location and profile language with an accuracy estimate of 87%. Grouped True Bayes49WSite Na X ve Bayes 49 languages with site, Wikipedia profile Bayes11WSite Na X ve Bayes 11 languages with site, Wikipedia profile 
Bayes11TSite Na X ve Bayes 11 languages with site, Twitter Based on the discussions and explanations of the previous sections, Table 7 above lists the models we compared, and below are the result tables. There are two sets of all results (Tables 8a and b), corresponding to the two site-language strategies: (a) the site language is the language from the user profile, or (b) the site language is the language determined from the combination of language and location from the user profile. Table 8a. Precision on Human-labeled Twitter Data with Table 8b. Precision on Human-labeled Twitter Data with and Language from User Profile Table 9. Precision by Langu age on Jan 2012 Twitter Data As is apparent from comparing Tables 4, 8a and b, there are significant differences between the eBay M2M data, and the Twitter status data. On the positive side, on all data sets all models / approaches are very stable, a nd show no over-fitting. Scoring eBay M2M data one year out from the training period achieves the same precision, showing that the underlying patterns are well captured by all approaches. The Twitter data was evaluated on a shorter timeframe, but the models also appear stable. On the eBay M2M data the Bayesian model shows, as expected, that a custom model (Bayes7M) performs better than a generic model (Bayes7W). On the other hand, this is not the case on the Twitter status data, where the generic profile (Bayes11W) outperforms the custom profile (Bayes11T). The email messages in the eBay M2M data are fairly clean text with highly accurate site language identification labels, certainly if compared to Twitter status updates, where abbreviations and creative slang are the norm and where the user profile language (and location) information is less accurate in identifying the language of status updates. In other words, the major differences between the eBay M2M and the Twitter status update data sets seem to be the cleanliness of the data and the vast difference in the labeling accuracy by the site language. It stands to reason that this noisy signal confuses both the Bayesian model X  X  language profiles when built on the Twitter data, and also our dictionary-based model X  X  frequency tables. Indeed, on the Twitter status update data labeled by the user profile language, which has the lowest accuracy of all site-languages considered in this study, both modeling approaches perform poorly when evaluated stand-alone. For the Bayesian model using an off-the-shelf pr ofile avoids this pitfall. From a pure run-time considerati on (Table 3), the Zipping method is not competitive in a real-time high-volume application. While the Bayesian model has a shorter load time as compared to our model, which must load the more extensive count tables, once the model is loaded, scoring with the Bayesian model is more than an order of magnitude slower as compared to scoring with our model. Our observation of the poor run-time performance of the Zipping method along with its also relatively poor language identification performance validates a previous observation from the literature dated shortly after the original publication of the Zipping algorithm [6]. Maybe surprisingly, our simple custom model performed in the same ballpark as all the other mode ls, in an overall sense, at least if the underlying site language cl assifier was strong. Its natural role thus is as a boost to an already existing strong language detector. A more detailed breakdown (Tables 5 and 9) shows that it roughly matched or outperformed the best competing model on the popular languages, while havi ng a (slightly) higher error rate on the less popular languages. This imbalance was more pronounced on the more noisy Tw itter data. As to specific languages, the main root cause of the poor language identification performance of our algorithm on the Twitter data, even when combined with the site, in particular on the Filipino language (Table 9) is that all users tweetin g in Filipino had English as their site language from their user prof ile. Because of the rarity of the Filipino language there was insufficient evidence in the frequency tables to override this dominant site language. The same explanation applies on the Twitter data to the lower identification rate of the Dutch language where more than half of the users have English as their site language, and also to German, where just under half of the users have En glish as their site language. The performance on the Twitter data indicates that our word-frequency based approach has a strong dependency on the accuracy of the classifier based on the site-language. When this accuracy is lower, this lack of accuracy transports itself into the frequency tables, and the al gorithm X  X  performance suffers significantly (Table 8a). On th e other hand, the boot-strapping procedure using an existing str ong classifier based on the site-language works well, for all of th e algorithms considered (Tables 4 and 8b). Indeed, even on noisy Twitter data the bootstrapping procedure boosts the precision to roughly the 90% range (Tables 8a and 8b), which is significantly better than the roughly 80% accuracy one obtains from the user profile language alone. One key point is that both our algorithm and our bootstrapping are completely unsupervised in the classical sense that no human-labeled data is needed, instead the site provides the necessary labeling. The weights in our algorithm including the weight for the combination with the site language have not been optimized but were set based only on insight into the model structure, and could likely be improve d. The combination of the Bayesian model with the site-language classifier requires a single a-priori estimate on the precision of the site-languag e classifier, but nothing else in form of labeled data, and it boosts the performance measurably. In summary, we presented a simple, stable, and highly runtime-efficient algorithm for language identification, which does not need any human-labeled training data, provided there already exists a strong language identifier based on a site language or user language. While it is at heart a statistical method based on frequency tables on a machine-gene rated dictionary, the decision of building the dictionary only on the opening and closing words of a message was driven by human insight into the problem structure. The method shines on web-scale data, validating once more that bigger data is better data, and that, when combined with human understanding of the problem domain, bigger data is even better data. Our approach in particular works well for short documents, and its precision is on-par with state-of-the-art language identification. Our thanks go to Cybozu Labs, Inc. for publicly posting their implementation of a character n-gram Bayesian language classifier, thus allowing us to easily compare our method to a state-of-the-art classifier. [1] Dunning, Ted. 1994. Statistical Identification of Language. [2] Shuyo, Nakatani. 2010. Language Identification Library . [3] Benedetto, Dario, Caglioti Emanuele and Loreto, Vittorio. [4]  X  eh  X  X  ek, Radim and Kolkus, Milan. 2009. Language [5] Twitter, Streaming API. Online documentation at [6] Goodman, Joshua, 2002. Extended comment on "Language 
