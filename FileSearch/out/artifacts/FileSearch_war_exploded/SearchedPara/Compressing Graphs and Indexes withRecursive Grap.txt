 Graph reordering is a powerful technique to increase the lo-cality of the representations of graphs, which can be helpful in several applications. We study how the technique can be used to improve compression of graphs and inverted indexes.
We extend the recent theoretical model of Chierichetti et al. (KDD 2009) for graph compression, and show how it can be employed for compression-friendly reordering of social net-works and web graphs and for assigning document identi-fiers in inverted indexes. We design and implement a novel theoretically sound reordering algorithm that is based on recursive graph bisection.

Our experiments show a significant improvement of the compression rate of graph and indexes over existing heuris-tics. The new method is relatively simple and allows efficient parallel and distributed implementations, which is demon-strated on graphs with billions of vertices and hundreds of billions of edges.
Many real-world systems and applications use in-memory representation of indexes for serving adjacency information in a graph. A popular example is social networks in which the list of friends is stored for every user. Another example is an inverted index for a collection of documents that stores, for every term, the list of documents where the term occurs. Maintaining these indexes requires a compact, yet efficient, representation of graphs.

How to represent and compress such information? Many techniques for graph and index compression have been stud-ied in the literature [23,37]. Most techniques first sort vertex identifiers in an adjacency list, and then replace the identi-fiers (except the first) with differences between consecutive ones. The resulting gaps are encoded using some integer compression algorithm. Note that using gaps instead of orig-inal identifiers decreases the values needed to be compressed and results in a higher compression ratio. We stress that the success of applying a particular encoding algorithm strongly depends on the distribution of gaps in an adjacency list: a se-quence of small and regular gaps is more compressible than a sequence of large and random ones.

This observation has motivated the approach of assigning identifiers in a way that optimizes compression. Graph re-ordering has been successfully applied for social networks [7, 12]. In that scenario, placing similar social actors nearby in the resulting order yields a significant compression improve-ment. Similarly, lexicographic locality is utilized for com-pressing the Web graph: when pages are ordered by URL, proximal pages have similar sets of neighbors, which re-sults in an increased compression ratio of the graph, when compared with the compression obtained using the original graph [8, 28]. In the context of index compression, the corre-sponding approach is called the document identifier assign-ment problem. Prior work shows that for many collections, index compression can be significantly improved by assign-ing close identifiers to similar documents [5, 6, 14, 31, 33].
In this paper, we study the problem of finding the best  X  X ompression-friendly X  order for a graph or an inverted in-dex. While graph reordering and document identifier assign-ment are often studied independently, we propose a unified model that generalizes both of the problems. Although a number of heuristics for the problems exists, none of them provides any guarantees on the resulting quality. In con-trast, our algorithm is inspired by a theoretical approach with provable guarantees on the final quality, and it is de-signed to directly optimize the resulting compression ratio. Our main contributions are the following.
The paper is organized as follows. We first discuss existing approaches for graph reordering, assigning document iden-tifiers, and the most popular encoding schemes for graph and index representation (Section 2). Then we consider al-gorithmic aspects of the underlying optimization problem. We analyze the models for graph compression suggested by Chierichetti et al. [12] and suggest our generalization in Sec-tion 3.1. Next, in Section 3.2, we examine existing theoreti-cal techniques for the graph reordering problem and use the ideas to design a practical algorithm. A detailed description of the algorithm along with the implementation details is presented in Section 4, which is followed by experimental Section 5. We conclude the paper with the most promising future directions in Section 6.
There exists a rich literature on graph and index com-pression, that can be roughly divided into three categories: (1) structural approaches that find and merge repeating graph patterns (e.g., cliques), (2) encoding adjacency data represented by a list of integers given some vertex/document order, and (3) finding a suitable order of graph vertices. Our focus is on the ordering techniques. We discuss the existing approaches for graph reordering, followed by an overview of techniques for document identifier assignment. Since many integer encoding algorithms can benefit from such a reorder-ing, we also outline the most popular encoding schemes.
Among the first approaches for compressing large-scale graphs is a work by Boldi and Vigna [8], who compress web graphs using a lexicographical order of the URLs. Their com-pression method relies on two properties: locality (most links lead to pages within the same host) and similarity (pages on the same host often share the same links). Later Apos-tolico and Drovandi [2] suggest one of the first ways to com-press a graph assuming no a priori knowledge of the graph. The technique is based on a breadth-first traversal of the graph vertices and achieves a better compression rate using an entropy-based encoding.

Chierichetti et al. [12] consider the theoretical aspect of the reordering problem motivated by compressing social net-works. They develop a simple but practical heuristic for the problem, called shingle ordering . The heuristic is based on obtaining a fingerprint of the neighbors of a vertex and po-sitioning vertices with identical fingerprints close to each other. If the fingerprint can capture locality and similarity of the vertices, then it can be effective for compression. This approach is also called minwise hashing and is originally ap-plied by Broder [9] for finding duplicate web pages.
Boldi et al. [7] suggest a reordering algorithm, called Lay-ered Label Propagation , to compress social networks. The algorithm is built on a scalable graph clustering technique by label propagation [27]. The idea is to assign a label for every vertex of a graph based on the labels of its neighbors. The process is executed in rounds until no more updates take place. Since the standard label propagation described in [27] tends to produce a giant cluster, the authors of [7] construct a hierarchy of clusters. The vertices of the same cluster are then placed together in the final order.
The three-step multiscale paradigm is often employed for the graph ordering problems. First, a sequence of coarsened graphs, each approximating the original graph but having a smaller size, is created. Then the problem is solved on the coarsest level by an exhaustive search. Finally, the process is reverted by an uncoarsening procedure so that a solution for every graph in the sequence is based on the solution for a previous smaller graph. Safro and Temkin [30] employ the algebraic multigrid methodology in which the sequence of coarsened graphs is constructed using a projection of graph Laplacians into a lower-dimensional space.

Spectral methods have also been successfully applied to graph ordering problems [19]. Sequencing the vertices is done by sorting them according to corresponding elements of the second smallest eigenvector of graph Laplacian (also called the Fiedler vector). It is known that the order yields the best non-trivial solution to a relaxation of the quadratic graph ordering problem , and hence, it is a good heuristic for com-puting linear arrangements.
 Recently Lim et al. [22] present another technique, called SlashBurn . Their method constructs a permutation of graph vertices so that its adjacency matrix is comprised of a few nonzero blocks. Such dense blocks are easier to encode, which is beneficial for compression.

In our experiments, we compare our new algorithm with all of the methods, which are either easy to implement, or come with the source code provided by the authors.
Several papers study how to assign document identifiers in a document collection for better compression of an inverted index. A popular idea is to perform a clustering on the collec-tion and assign close identifiers to similar documents. Shieh et al. [31] propose a reassignment heuristic motivated by the maximum travelling salesman problem (TSP). The heuristic computes a pairwise similarity between every pairs of doc-uments (proportional to the number of shared terms), and then finds the longest path traversing the documents in the graph. An alternative algorithm calculating cosine similari-ties between documents is suggested by Blandford and Blel-loch [6]. Both methods are computationally expensive and are limited to fairly small datasets. The similarity-based ap-proach is later improved by Blanco and Barreiro [5] and by Ding et al. [14], who make it scalable by reducing the size of the similarity graph, respectively through dimensionality reduction and locality sensitive hashing.

The approach by Silvestri [33] simply sorts the collection of web pages by their URLs and then assigns document iden-tifiers according to the order. The method performs very well in practice and is highly scalable but it does not generalize to document collections that do not have URL-like identifiers.
Our algorithm is not tailored specifically for an encod-ing scheme; any method that can take advantage of lists with higher local density (clustering) should benefit from the reordering. For our experiment we choose a few encod-ing schemes that represent the state-of-the-art.
Most graph compression schemes build on delta-encoding , that is, sorting the adjacency lists ( posting lists in the in-verted indexes case) so that the gaps between consecutive elements are positive, and then encoding these gaps using a variable-length integer code. The WebGraph framework adds the ability to copy portions of the adjacency lists from other vertices, and has special cases for runs of consecutive integers. Introduced in 2004 by Boldi and Vigna [8], it is still widely used to compress web graphs and social networks.
Inverted indexes are usually compressed with more spe-cialized techniques in order to enable fast skipping, which en-ables efficient list intersection. We perform our experiments with Partitioned Elias-Fano and Binary Interpolative Cod-ing. The former, introduced by Ottaviano and Venturini [25], provides one of the best compromise between decoding speed and compression ratio. The latter, introduced by Moffat and Stuiver [24], has the highest compression ratio in the litera-ture, with the trade-off of slower decoding by several times. Both techniques directly encode monotone lists, without go-ing through delta-encoding.
Graph reordering is a combinatorial optimization problem with a goal to find a linear layout of an input graph so that a certain objective function (referred to as a cost function or just cost ) is optimized. A linear layout of a graph G = ( V,E ) with n = | V | vertices is a bijection  X  : V  X  { 1 ,...,n } . A layout is also called an order, an arrangement, or a number-ing of the vertices. In practice it is desirable that  X  X imilar X  vertices of the graph are  X  X lose X  in  X  . This leads to a number of problems that we define next.

The minimum linear arrangement ( MLA ) problem is to find a layout  X  so that is minimized. This is a classical NP -hard problem [17], which is known to be APX -hard under Unique Games Conjecture [13], that is, it is unlikely that an efficient approximation algo-rithm exists. See [26] for a survey of results on MLA .
A closely related problem is minimum logarithmic arrange-ment ( MLogA ) in which the goal is to minimize Here and in the following we denote log( x ) = 1 + b log 2 that is, the number of bits needed to represent an integer x . The problem is also NP -hard, and one can show that the op-timal solutions of MLA and MLogA are different on some graphs [12]. In practice a graph is represented as an ad-jacency list using an encoding scheme; hence, the gaps in-duced by consecutive neighbors of a vertex are important for compression. For this reason, the minimum logarithmic gap arrangement ( MLogGapA ) problem is introduced [12]. For a vertex v  X  V of degree k and an order  X  , consider the neighbors out ( v ) = ( v 1 ,...,v k ) of v such that  X  ( v  X  X  X  &lt;  X  ( v k ). Then the cost compressing the list out ( v ) un-der  X  is related to f  X  ( v,out ( v )) = P k  X  1 i =1 log |  X  ( v MLogGapA consists in finding an order  X  , which minimizes To the best of our knowledge, MLogA and MLogGapA are introduced quite recently by Chierichetti et al. [12]. They show that MLogA is NP -hard but left the computational complexity of MLogGapA open. Since the latter problem is arguably more important for applications, we address the open question of complexity of the problem.
 Theorem 1. MLogGapA is NP -hard.
 Proof. We prove the theorem by using the hardness of MLogA , which is known to be NP -hard [12]. Let G = ( V,E ) be an instance of MLogA . We build a bipartite graph G 0 = ( V 0 ,E 0 ) by splitting every edge of E by a degree-2 vertex. Formally, we add | E | new vertices so that V 0 = V  X  U , where V = { v 1 ,...,v n } and U = { u 1 ,...,u m } . For every edge ( a,b )  X  E , we have two edges in E 0 , that is, ( a,u ( b,u i ) for some 1  X  i  X  m . Next we show that an optimal solution for MLogGapA on G 0 yields an optimal solution for MLogA on G , which proves the claim of the theorem.
Let R be an optimal order of V 0 for MLogGapA . Ob-serve that without loss of generality, the vertices of V and U are separated in R , that is, R = ( v i 1 ,...,v i n ,u j Otherwise, the vertices can be reordered so that the total objective is not increased. To this end, we  X  X ove X  all the vertices of V to the left of R by preserving their relative order. It is easy to see that the gaps between vertices of V and the gaps between vertices of U can only decrease. Now the cost of MLogGapA on G is where  X  u = ( u j 1 ,...,u j m ) and  X  v = ( v i 1 ,...,v that the second term of the sum depends only on the order  X  v of the vertices in V , and it equals to the cost of MLogA for graph G . Since R is optimal for MLogGapA , the order  X  v = ( v i 1 ,...,v i n ) is also optimal for MLogA .
Most of the previous works consider the MLogA problem for graph compression, and the algorithms are not directly suitable for index compression. Contrarily, an inverted in-dex is generally represented by a directed graph (with edges from terms to documents), which is not captured by the MLogGapA problem, which is introduced for undirected graphs. In the following, we suggest a model, which gener-alizes both MLogA and MLogGapA and better expresses graph and index compression.
Intuitively, our new model is a bipartite graph comprising of query and data vertices. A query vertex might correspond to an actor in a social network or to a term in an inverted index. Data vertices are an actor X  X  friends or documents con-taining the term, respectively. The goal is to find a layout of data vertices.

Formally, let G = ( Q X  X  ,E ) be an undirected unweighted bipartite graph with disjoint sets of vertices Q and D . We denote |D| = n and | E | = m . The goal is to find a permuta-tion,  X  , of data vertices, D , so that the following objective is minimized: where deg q is the degree of query vertex q  X  Q , and q  X  X  neighbors are { u 1 ,...,u deg q } with  X  ( u 1 ) &lt;  X  X  X  &lt;  X  ( u Note that the objective is closely related to minimizing the number of bits needed to store a graph or an index rep-resented using the delta-encoding scheme. We call the op-timization problem bipartite minimum logarithmic arrange-ment ( BiMLogA ), and the corresponding cost averaged over the number of gaps LogGap.

Note that BiMLogA is different from MLogGapA in that the latter does not differentiate between data and query vertices (that is, every vertex is query and data in MLogGapA which is unrealistic in some applications. It is easy to see that the new problem generalizes both MLogA and MLogGapA : to model MLogA , we add a query vertex for every edge of the input graph, as in the proof of Theorem 1; to model MLogGapA , we add a query for every vertex of the input graph; see Figure 1. Moreover, the new approach can be naturally applied for compressing directed graphs; to this end, we only consider gaps induced by outgoing edges of a vertex. Clearly, given an algorithm for BiMLogA , we can easily solve both MLogA and MLogGapA . Therefore, we focus on this new problem in the next sections. 2 Figure 1: Modeling of MLogA and MLogGapA with a bi-partite graph with query (red) and data (blue) vertices.
How can one solve the above ordering problems? Next we discuss the existing theoretical approaches for solving graph ordering problems. We focus on approximation algorithms, that is, efficient algorithms for NP -hard problems that pro-duce sub-optimal solutions with provable quality.
To the best of our knowledge, no approximation algo-rithms exist for the new variants of the graph ordering prob-lem. However, a simple observation shows that every algo-rithm has approximation factor O (log n ). Note that the cost of a gap between u  X  D and v  X  D in BiMLogA cannot exceed log n , as |  X  ( v )  X   X  ( u ) |  X  n for every permutation  X  . On the other hand, the cost of a gap is at least 1. Therefore, an arbitrary order of vertices yields a solution with the cost, which is at most log n times greater than the optimum.
In contrast, the well-studied MLA does not admit such a simple approximation and requires more involved algo-rithms. We observe that most of the existing algorithms for MLA and related ordering problems employ the divide-and-conquer approach; see Algorithm 1. Such algorithms parti-tion the vertex set into two sets of roughly equal size, com-pute recursively an order of each part, and  X  X lue X  the order-ings of the parts together. The crucial part is graph bisection or more generally balanced graph partitioning , if the graph is split into more than two parts.

The first non-trivial approximation algorithm for MLA follows the above approach. Hancen [18] proves that Algo-rithm 1 yields an O (  X  log n )-approximation for MLA , where  X  indicates how close is the solution of the first step (bi-section of G ) to the optimum. Later, Charikar et al. [11] shows that a tighter analysis is possible, and the algorithm
Input : graph G 1. Find a bisection ( G 1 ,G 2 ) of G ; 2. Recursively find linear arrangements for G 1 and G 2 ; 3. Concatenate the resulting orderings;
Algorithm 1: Graph Reordering using Graph Bisection is in fact O (  X  )-approximation for d -dimensional MLA . Cur-rently,  X  = O ( sequently, the idea of Algorithm 1 is employed by Even et al. [15], Rao and Richa [29], and Charikar et al. [10] for com-posing approximation algorithms for MLA . The techniques use the recursive divide-and-conquer approach and utilize a spreading metric by solving a linear program with an expo-nential number of constraints.

Inspired by the algorithms, we design a practical approach for the BiMLogA problem. While solving a linear program is not feasible for large graphs, we utilize recursive graph partitioning in designing the algorithm. Next we describe all the steps and provide implementation-specific details. Assume that the input is an undirected bipartite graph G = ( Q X  X  ,E ), and the goal is to compute an order of D . On a high level, our algorithm is quite simple; see Algorithm 1.
The reordering method is based on the graph bisection problem, which asks for a partition of graph vertices into two sets of equal cardinality so as to minimize an objective func-tion. Given an input graph G with |D| = n , we apply the bi-section algorithm to obtain two disjoint sets V 1 ,V 2  X  X  with | V 1 | = b n/ 2 c and | V 2 | = d n/ 2 e . We shall lay out V set { 1 ,..., b n/ 2 c} and lay out V 2 on the set {d n/ 2 e ,...,n } . Thus, we have divided the problem into two problems of half the size, and we recursively compute good layouts for the graphs induced by V 1 and V 2 , which we call G 1 and G respectively. Of course, when there is only one vertex in G , the order is trivial.

How to bisect the vertices of the graph? We use a graph bi-section method, similar to the popular Kernighan-Lin heuris-tic [20]; see Algorithm 2. Initially we split D into two sets, V 1 and V 2 , and define a computational cost of the partition, which indicates how  X  X ompression-friendly X  the partition is. Next we exchange pairs of vertices in V 1 and V 2 trying to improve the cost. To this end we compute, for every vertex v  X  D , the move gain , that is, the difference of the cost af-ter moving v from its current set to another one. Then the vertices of V 1 ( V 2 ) are sorted in the decreasing order of the gains to produce list S 1 ( S 2 ). Finally, we traverse the lists S 1 and S 2 in the order and exchange the pairs of vertices, if the sum of their move gains is positive. Note that unlike classical graph bisection heuristics [16,20], we do not update move gains after every swap. The process is repeated until the convergence criterion is met (no swapped vertices) or the maximum number of iterations is reached.

To initialize the bisection, we consider the following two alternatives. A simple approach is to arbitrarily split D into two equal-sized sets. Another approach is based on shingle ordering (minwise hashing) suggested in [12]. To this end, we order the vertices as described in [12] and assign the first b n/ 2 c vertices to V 1 and the last d n/ 2 e to V 2 . Input : graph G = ( Q X  X  ,E )
Output : graphs G 1 = ( Q X  V 1 ,E 1 ) ,G 2 = ( Q X  V 2 ,E 2 determine an initial partition of D into V 1 and V 2 ; repeat until converged or iteration limit exceeded ; return graphs induced by Q X  V 1 and Q X  V 2
Algorithm 2 tries to minimize the following objective func-tion of the sets V 1 and V 2 , which is motivated by BiMLogA For every vertex q  X  X  , let deg 1 ( q ) = |{ ( q,v ) : v  X  V is, the number of adjacent vertices in set V 1 ; define deg similarly. Then the cost of the partition is X where n 1 = | V 1 | and n 2 = | V 2 | . The cost estimates the re-quired number of bits needed to represent G using delta-encoding. If the neighbors of q  X  X  are uniformly distributed in the final arrangement of V 1 and V 2 , then the the average gap between consecutive numbers in the q  X  X  adjacency list is gap 1 : = n 1 / (deg 1 ( q ) + 1) and gap 2 : = n 2 / (deg V and V 2 , respectively; see Figure 2. There are (deg 1 ( q )  X  1) gaps between vertices in V 1 and (deg 2 ( q )  X  1) gaps between vertices in V 2 . Hence, we need approximately (deg 1) log(gap 1 ) + (deg 2 ( q )  X  1) log(gap 2 ) bits to compress the within-group gaps. In addition, we have to account for the average gap between the last vertex of V 1 and the first vertex of V 2 , which is (gap 1 + gap 2 ). Assuming that n 1 = n have log(gap 1 + gap 2 ) = log(gap 1 ) + log(gap 2 ) + C , where C is a constant with respect to the data vertex assignment, and hence, it can be ignored in the optimization. Adding this between-group contribution to the within-group contri-butions gives the above expression. Figure 2: Partitioning D into V 1 and V 2 for a query q  X  Q with deg 1 ( q ) = 3 and deg 2 ( q ) = 2.

Note that using the cost function, it is straightforward to implement ComputeMoveGain ( v ) function from Algo-rithm 2 by traversing all the edges ( q,v )  X  E for v  X  X  and summing up the cost differences of moving v to another set.
Combining all the steps of Algorithm 1 and Algorithm 2, we have the following claim.
 Theorem 2. The algorithm produces a vertex order in O ( m log n + n log 2 n ) time.

Proof. There are d log n e levels of recursion. Each call of graph bisection requires computing move gains and sort-ing of n elements. The former can be done in O ( m ) steps, while the latter requires O ( n log n ) steps. Summing over all subproblems, we get the claim of the theorem.
Due to the simplicity of the algorithm, it can be efficiently implemented in parallel or distributed manner. For the for-mer, we notice that two different recursive calls of Algo-rithm 1 are independent, and thus, can be executed in par-allel. Analogously, a single bisection procedure can easily be parallelized, as each of its steps computes independent val-ues for every vertex, and a parallel implementation of sorting can be used. In our implementation, we employ the fork-join computation model in which small enough graphs are pro-cessed sequentially, while larger graphs which occur on the first few levels of recursion are solved in parallel manner.
Our distributed implementation relies on the vertex-centric programming model and runs in the Giraph framework In Giraph, a computation is split into supersteps that con-sists of processing steps: (i) a vertex executes a user-defined function based on local vertex data and on data from adja-cent vertices, (ii) the resulting output is sent along outgoing edges. Supersteps end with a synchronization barrier, which guarantees that messages sent in a given superstep are re-ceived at the beginning of the next superstep. The whole computation is executed iteratively for a certain number of rounds, or until a convergence property is met.

Algorithm 2 is implemented in the vertex-centric model with a simple modification. The first two supersteps com-pute move gains for all data vertices. To this end, every query vertex calculates the differences of the cost function when its neighbor moves from a set to another one. Then, every data vertex sums up the differences over its query neighbors. Given the move gains, we exchange the vertices as follows. Instead of sorting the move gains, we construct, for both sets, an approximate histogram of the gain values. Since the size of the histograms is small enough, we collect the data on a dedicated host, and decide how many vertices from each bin should exchange its set. On the last superstep, this information is propagated over all data vertices and the corresponding swaps take effect.
We design our experiments to answer two primary ques-tions: (i) How well does our algorithm compress graphs and indexes in comparison with existing techniques? (ii) How do various parameters of the algorithm contribute to the solu-tion, and what are the best parameters?
For our experiments, we use several publicly available web graphs, social networks, and inverted document indexes; see Table 1. In addition, we run evaluation on two large sub-graphs of the Facebook friendship graph and a sample of the Facebook search index. These private datasets serve to demonstrate scalability of our approach. We do not release http://giraph.apache.org the datasets and our source code due to corporate restric-tions. Before running the tests, all the graphs are made un-weighted and converted to bipartite graphs as described in Section 3.1. Our dataset is as follows.
To build the inverted indexes for Gov2 and ClueWeb09 the body text was extracted using Apache Tika 2 and the words lowercased and stemmed using the Porter2 stemmer; no stopwords were removed. We consider only long posting lists containing more than 4096 elements.
 Graph |Q| |D| | E | Enron 9 , 660 9 , 660 224 , 896 AS-Oregon 13 , 579 13 , 579 74 , 896
FB-NewOrlean 63 , 392 63 , 392 1 , 633 , 662 web-Google 356 , 648 356 , 648 5 , 186 , 648 LiveJournal 4 , 847 , 571 4 , 847 , 571 85 , 702 , 474 Twitter 41 , 652 , 230 41 , 652 , 230 2 , 405 , 026 , 092 Gov2 39 , 187 24 , 618 , 755 5 , 322 , 924 , 226 ClueWeb09 96 , 741 50 , 130 , 884 14 , 858 , 911 , 083 FB-Posts-1B 60  X  10 3 1  X  10 9 20  X  10 9 FB-300M 300  X  10 6 300  X  10 6 90  X  10 9 FB-1B 1  X  10 9 1  X  10 9 300  X  10 9
We compare our new algorithm (referred to as BP ) with the following competitors. http://tika.apache.org
BP has a number of parameters that can affect its quality and performance. In the following we discuss some of the parameters and explain our choice of their default values.
An important aspect of BP is how two sets, V 1 and V are initialized in Algorithm 2. Arguably the initialization procedure might affect the quality of the final vertex order. To verify the hypothesis, we implemented four initialization techniques that bisect a given graph: Random , Natural , BFS and Minhash . The techniques order the data vertices, D , us-ing the corresponding algorithm, and then split the order into two sets of equal size. In the experiment, we intention-ally consider only the simplest and most efficient bisection techniques so as to keep the complexity of the BP algorithm low. Figure 3 illustrates the effect of the initialization meth-ods for graph bisection. Note that initialization plays a role to some extent, and there is no consistent winner. the best initialization for three of the graphs but does not produce an improvement on the indexes. One explanation is that the indexes contain high-degree query vertices, that make the BFS order essentially random. Overall, the differ-ence between the final results is not substantial, and even the worst initialization yields better orders than the alternative algorithms do. Therefore, we utilize the simplest approach, Random , for graphs and Minhash for indexes as the default technique for bisection initialization.

Is it always necessary to perform log n levels of recursion to get a reasonable solution? Figure 4 shows the quality of the resulting vertex order after a fixed number, i , of recur-sion splits. For every i (that is, when there are 2 i disjoint sets), we stop the algorithm and measure the quality of the order induced by the random assignment of vertices respect-ing the partition. It turns out that graph bisection is benefi-cial only when D contains more than a few tens of vertices. In our implementation, we set (log n  X  5) for the depth of recursion, which slightly reduces the overall running time. It might be possible to improve the final quality by finding an optimal solution (e.g., using an exhaustive search or a Figure 3: LogGap cost of the resulting order produced with different initialization approaches for graph bisection. Figure 4: LogGap cost of the resulting order produced with a fixed depth of recursion. Note that the last few splits make insignificant contributions to the final quality. linear program) for small subgraphs on the lowest levels of the recursion. We leave the investigation for future research.
Figure 5 illustrates the speed of convergence of our opti-mization procedure utilized for improving graph partitioning in Algorithm 2. The two sets approach a locally optimal state within a few iterations. The number of required iterations increases, as the depth of recursion gets larger. Generally, the number of moved vertices per iteration does not exceed 1% after 20 iterations, even for the deepest recursion levels. Therefore, we use 20 as the default number of iterations in all our experiments.
Table 2 presents a comparison of various reordering meth-ods on social networks and web graphs. We evaluate the following measures: (i) the cost of the BiMLogA problem (LogGap), (ii) the cost of the MLogA problem (the log-arithmic difference averaged over the edges, Log), (iii) the average number of bits per edge needed to encode the graph with WebGraph [8] (referred to as BV ). The results suggest that BP yields the best compression on all but one instance, providing an 5  X  20% improvement over the best alterna-tive. An average gain over a non-reordered solution reaches impressive 50%. The runner-up approaches, TSP , LLP , and Multiscale , also significantly outperform the natural or-der. However, their straightforward implementations are not scalable for large graphs (none of them is able to process Twitter within a few hours), while efficient implementations are arguably more complicated than BP . Figure 5: The average percentage of moved vertices on an iteration of Algorithm 2 for various levels of recursion. The data is computed for LiveJournal . Figure 6: Adjacency matrices of FB-NewOrlean after applying various reordering algorithms; nonzero elements are blue.
The computed results for FB-300M and FB-1B demonstrate that the new reordering technique is beneficial for very large graphs, too. Unfortunately, we were not able to calculate the compression rate for the graphs, as WebGraph [8] does not provide distributed implementation. However, the ex-periment indicates that BP outperforms Natural by around 50% and outperforms Minhash by around 30%.
 The compression ratio of inverted indexes is illustrated in Table 3, where we evaluate the Partitioned Elias-Fano [25] encoding and Binary Interpolative Coding [24] (respectively PEF and BIC ). Here the results are reported in average bits per edge. Again, our new algorithm largely outperforms existing approaches in terms of both LogGap cost and com-pression rate. BP has a large impact on the indexes, achieving a 22% and a 15% compression improvement over alterna-Enron Natural 5 . 01 9 . 82 7 . 80 AS-Oregon Natural 7 . 88 12 . 06 13 . 34 FB-NewOrlean Natural 9 . 74 14 . 29 14 . 64 web-Google Natural 13 . 39 16 . 74 20 . 08 LiveJournal Natural 10 . 43 17 . 44 14 . 61 Twitter Natural 15 . 23 23 . 65 21 . 56 FB-300M Natural 17 . 65 25 . 34 FB-1B Natural 19 . 63 27 . 22 Table 2: Reordering results of various algorithms on graphs: the costs of MLogA , BiMLogA , and the number of bits per edge required by BV . The best results in every column are highlighted. We present the results that completed the computation within a few hours.
 Table 3: Reordering results of various algorithms on inverted indexes with highlighted best results. tives; these gains are almost identical for PEF and BIC .
An interesting question is why does the new algorithm perform best on most of the tested graphs. In Figure 7 we analyze the number of gaps between consecutive numbers of graph adjacency lists. It turns out that BP and LLP have quite similar gap distributions, having notably more shorter gaps than the alternative methods. Note that the number of edges that the BV encoding is able to copy is related to the number of consecutive integers in the adjacency lists; hence short gaps strongly influences its performance. At the same time, BP is slightly better at longer gaps, which is a reason why the new algorithm yields a higher compression ratio.
We point out that the cost of BiMLogA , LogGap, is more relevant for the compression rate than the cost of MLogA ; see Figure 8. The observation agrees with the previous evalu-ation of Boldi et al. [7] and motivates our research on the for-mer problem. The Pearson correlation coefficients between the LogGap cost and the average number of bit per edge us-ing BV , PEF , and BIC encoding schemes are 0 . 9853, 0 . 8487, and 0 . 8436, respectively. While the high correlation between LogGap and BV is observed earlier [8, 12], the relation be-tween LogGap and PEF or BIC is a new phenomenon. A possible explanation is that the schemes encode a sequence of k integers in the range [1 ..n ] using close to the information-theoretic minimum of k (1 + b log 2 ( n/k ) c ) bits [25], which is equivalent to our optimization function utilized in Algo-rithm 2. It might be possible to construct a better model for the two encoding schemes, where the cost of the op-timization problem has a higher correlation with the final compression ratio. For example, this can be achieved by in-creasing the weights of  X  X hort X  gaps that generally require more than log(gap) bits. We leave the question for future investigation.

Figure 6 presents an alternative comparison of the impact of the reordering algorithms on the FB-NewOrlean graph. Note that only BP and LLP are able to find communities in the graph (dense subgraphs), that can be compressed effi-ciently. The recursive nature of BP is also clearly visible.
We created and tested two implementations of our algo-rithm, parallel and distributed. The parallel version is im-plemented in C++11 and compiled with the highest opti-mization settings. The tests are performed on a machine with Intel(R) Xeon(R) CPU E5-2660 @ 2.20GHz (32 cores) Figure 8: LogGap cost against the average number of bits per edge using various encoding schemes. with 128GB RAM. Our algorithm is highly scalable; the largest instances of our dataset, Gov2 , ClueWeb09 and Posts-1B , are processed with BP within 29, 129, and 163 minutes, respectively. In contrast, even the simplest hash takes 14, 42, and 70 minutes for the indexes. Natural and BFS also have comparable running times on the graphs. Our largest graphs, Twitter and LiveJournal , require 149 and 3 minutes; all the smaller graphs are processed within a few seconds. In comparison, the author X  X  implementation of LLP with the default settings takes 23 minutes on LiveJour-nal and is not able to process Twitter within a reasonable time. The other alternative methods are less efficient; for in-stance, Multiscale runs 12 minutes and TSP runs 3 minutes on web-Google . The single-machine implementation of BP is also memory-efficient, utilizing less than twice the space required to store the graph edges.

The distributed version of BP is implemented in Java. We run experiments using the distributed implementation only on
FB-300M and FB-1B graphs, using a cluster of a few tens of machines. FB-300M is processed within 350 machine-hours, while the computation on FB-1B takes around 2800 machine-hours. In comparison, the running time of the Minhash al-gorithm is 20 and 60 machine-hours on the same cluster configuration, respectively. Despite the fact that our imple-mentation is a part of a general graph partitioning frame-work [1], which is not specifically optimized for the problem, BP scales almost linearly with the size of the utilized cluster and processes huge graphs within a few hours.
We presented a new theoretically sound algorithm for graph reordering problem and experimentally proved that the re-sulting vertex orders allow to compress graphs and indexes more efficiently than the existing approaches. The method is highly scalable, which is demonstrated via evaluation on sev-eral graphs with billions of vertices and edges. While we see impressive gains in the compression ratio, we believe there is still much room for further improvement. In particular, our graph bisection technique ignore the freedom of orienting the decomposition tree. An interesting question is whether a postprocessing step that  X  X lips X  left and right children of tree nodes can be helpful. It is shown in [4] that there is an O ( n 2 . 2 )-time algorithm that computes an optimal tree orientation for the MLA problem. Whether there exists a similar algorithm for MLogA or BiMLogA , is open.

While our primary motivation is compression, graph re-ordering plays an important role in a number of applications. In particular, various graph traversal algorithms can be ac-celerated if the in-memory graph layout takes advantage of the cache architecture. Improving vertex and edge locality is important for fast node/link access operations, and thus can be beneficial for generic graph algorithms and applica-tions [32]. We are currently working on exploring this area and investigating how reordering of graph vertices can im-prove cache and memory utilization.

From the theoretical point of view, it is interesting to de-vise better approximation algorithms for the MLogA and BiMLogA problems. It is likely that relaxing the balance condition of the bisection step yields a better approxima-tion, similarly to the recursive algorithm for the MLA prob-lem [34]. Finally, optimal algorithms for special cases of the problem are also of interest, for example, ordering of certain classes of graphs that occur in practical applications. We would like to thank Yaroslav Akhremtsev, Mayank Pundir, and Arun Sharma for fruitful discussions of the problem, Ilya Safro for helping with running experiments with the Multiscale method, and Sebastiano Vigna for help-ing with the WebGraph library. [1] A, B. Shalita, I. K. Karrer, A. Sharma, A. Presta, [2] A. Apostolico and G. Drovandi. Graph compression by [3] S. Arora, S. Rao, and U. Vazirani. Expander flows, [4] R. Bar-Yehuda, G. Even, J. Feldman, and J. Naor. [5] R. Blanco and  X  A. Barreiro. Document identifier [6] D. Blandford and G. Blelloch. Index compression [7] P. Boldi, M. Rosa, M. Santini, and S. Vigna. Layered [8] P. Boldi and S. Vigna. The WebGraph framework I: [9] A. Z. Broder. On the resemblance and containment of [10] M. Charikar, M. T. Hajiaghayi, H. Karloff, and [11] M. Charikar, K. Makarychev, and Y. Makarychev. A [12] F. Chierichetti, R. Kumar, S. Lattanzi, [13] N. R. Devanur, S. A. Khot, R. Saket, and N. K. [14] S. Ding, J. Attenberg, and T. Suel. Scalable techniques [15] G. Even, J. S. Naor, S. Rao, and B. Schieber. [16] C. M. Fiduccia and R. M. Mattheyses. A linear-time [17] M. R. Garey and D. S. Johnson. Computers and [18] M. D. Hansen. Approximation algorithms for [19] M. Juvan and B. Mohar. Optimal linear labelings and [20] B. W. Kernighan and S. Lin. An efficient heuristic [21] H. Kwak, C. Lee, H. Park, and S. Moon. What is [22] Y. Lim, U. Kang, and C. Faloutsos. SlashBurn: Graph [23] S. Maneth and F. Peternek. A survey on methods and [24] A. Moffat and L. Stuiver. Binary interpolative coding [25] G. Ottaviano and R. Venturini. Partitioned Elias-Fano [26] J. Petit. Addenda to the survey of layout problems. [27] U. N. Raghavan, R. Albert, and S. Kumara. Near [28] K. H. Randall, R. Stata, R. G. Wickremesinghe, and [29] S. Rao and A. W. Richa. New approximation [30] I. Safro and B. Temkin. Multiscale approach for the [31] W.-Y. Shieh, T.-F. Chen, J. J.-J. Shann, and C.-P. [32] J. Shun, L. Dhulipala, and G. E. Blelloch. Smaller and [33] F. Silvestri. Sorting out the document identifier [34] H. D. Simon and S.-H. Teng. How good is recursive [35] J. Ugander and L. Backstrom. Balanced label [36] B. Viswanath, A. Mislove, M. Cha, and K. P.
 [37] I. H. Witten, A. Moffat, and T. C. Bell. Managing
