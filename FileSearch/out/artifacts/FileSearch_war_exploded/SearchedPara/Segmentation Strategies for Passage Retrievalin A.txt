 The importance of Information Retrieval (IR) in audio-visual recordings has been increasing with steeply growing numbers of audio-visual documents available on-line. Compared to traditional IR methods, this task requires specific techniques, such as Passage Retrieval which can accelerate the search process by retrieving the exact relevant passage of a record-ing instead of the full document. In Passage Retrieval, full recordings are divided into shorter segments which serve as individual documents for the further IR setup. This tech-nique also allows normalizing document length and applying positional information. It was shown that it can even improve retrieval results (e.g. [3]).

In this work, we examine two general strategies for Pas-sage Retrieval: blind segmentation into overlapping regular-length passages and segmentation into variable-length pas-sages based on semantics of their content.

Time-based segmentation was already shown to improve retrieval of textual documents and audio-visual recordings (e.g. [3, 5]). Our experiments performed on the test collection used in the Search subtask of the Search and Hyperlinking Task in MediaEval Benchmarking 2012 1 confirm those find-ings and show that parameters (segment length and shift) tuning for a specific test collection can further improve the results. Our best results on this collection were achieved by using 45-second long segments with 15-second shifts.
Semantic-based segmentation can be divided into three types: similarity-based (producing segments with high intra-similarity and low inter-similarity), lexical-chain-based (pro-ducing segments with frequent lexically connected words), and feature-based (combining various features which signalize a segment break in a machine-learning setting) [4]. In this work, we mainly focus on feature-based segmentation which allows exploiting various features from all modalities of the data (including segment length) in a single trainable model and produces segments which can eventually overlap. http://www.multimediaeval.org/
