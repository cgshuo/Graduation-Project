 Recent research has demonstrated beyond doubts the bene-fits of compressing natural language texts using word-based statistical semistatic compression. Not only it achieves ex-tremely competitive compression rates, but also direct search on the compressed text can be carried out faster than on the original text; indexing based on inverted lists benefits from compression as well.

Such compression methods assign a variable-length code-word to each different text word. Some coding methods (Plain Huffman and Restricted Prefix Byte Codes) do not clearly mark codeword boundaries, and hence cannot be ac-cessed at random positions nor searched with the fastest text search algorithms. Other coding methods (Tagged Huffman, End-Tagged Dense Code, or ( s, c )-Dense Code) do mark codeword boundaries, achieving a self-synchronization prop-erty that enables fast search and random access, in exchange for some loss in compression effectiveness.

In this paper, we show that by just performing a sim-ple reordering of the target symbols in the compressed text (more precisely, reorganizing the bytes into a wavelet-tree-like shape) and using little additional space, searching ca-pabilities are greatly improved without a drastic impact in compression and decompression times. With this approach, all the codes achieve synchronism and can be searched fast and accessed at arbitrary poin ts. Moreover, the reordered compressed text becomes an implicitly indexed representa-tion of the text, which can be searched for words in time independent of the text length. That is, we achieve not only fast sequential search time, but indexed search time, for al-most no extra space cost.

We experiment with three well-known word-based com-pression techniques with different characteristics (Plain Huff-man, End-Tagged Dense Code and Restricted Prefix Byte  X 
Funded in part by ACEI gra nt A/8065/07 (Spain); for the Spanish group by MEC grant TIN2006-15071-C03-03 (Spain); and for the fourth author by Yahoo! Research grant  X  X ompact Data Structures X .
 Codes), and show the searching capabilities achieved by re-ordering the compressed representation on several corpora. We show that the reordered versions are not only much more efficient than their classical counterparts, but also more ef-ficient than explicit inverted indexes built on the collection, when using the same amount of space.
 E.4 [ Coding and Information Theory ]: Data Compaction and Compression; H.3.3 [ Information Storage and Re-trieval ]: Information Search and Retrieval X  search process Algorithms Word-based compression, sea rching compressed text, com-pressed indexing.
Text compression is useful not only to save disk space, but more importantly, to save processing, transmission and disk transfer time. Compression techniques especially designed for natural language texts permit searching the compressed text much faster (up to 8 times) than the original text [17, 10], in addition to their proven effectiveness (with compres-sion ratios around 25%-35%).

Those ratios are obtained using a word-based model [9], where words are encoded instead of characters. Words present a more biased distribution of frequencies than characters, following a Zipf Law [18, 1]. Thus the text (regarded as a sequence of words) is highly co mpressible with a zero-order encoder such as Huffman code [8]. With the optimal Huff-man coding, compression ratios approach 25%.

Although necessarily inferior to Huffman code in compres-sion effectiveness, different coding methods, such as Plain Huffman [11] or Restricted Prefix Byte Codes [4], try to ap-proach the performance of classical Huffman while encoding the source symbols as sequences of bytes instead of bits. This degrades compression ratios to around 30%, yet allows much faster decompression.

Still other encoding methods, such as Tagged Huffman codes [11], End-Tagged Dense Codes, and ( s, c )-Dense Codes [3], worsen the compression ratios a bit more (up to 35%) in exchange for being self-synchronized . This means that code-word boundaries can be distinguished starting from any-where in the encoded sequence, which enables random access to the compressed text, as well as very fast Boyer-Moore-like direct search of the compressed text.

In this paper, we propose a reordering of the bytes in the codewords of the compressed text following a wavelet-tree-like strategy. We show that this simple variation obtains a compressed text that is always self-synchronized, despite building on encodings which are not. That is, the reorga-nized compressed text can be accessed at any point, even if Plain Huffman coding is used, for example. This encourages using the most efficient bytewise encodings with no penalty.
What is even more striking is that the reorganized text turns out to have some implicit indexing properties. That is, with very little extra space, it is possible to search it in time that is not proportional to the text length (as any sequential search method) but logarithmic on it (as typical indexed techniques). Indeed, we compare our reorganized codes against the original techniques armed with an explicit inverted index, and show that the former are more efficient when using the same amount of space. Within that little al-lowed space, block-addressing compressed inverted indexes are the best choice as far as we know [14, 19]. We imple-ment such a compressed block-addressing index following the most recent algorithms for list intersections [5]. Our results demonstrate that it is more convenient to use reor-ganized codes than trying to use very space-efficient inverted indexes; only if one is willing to pay a significant extra space do inverted indexes pay off.

We note that our technique is tailored to main memory due to its random access pattern. Therefore, it can only compete with inverted indexes in main memory. There has been a lot of recent interest on inverted indexes that oper-ate in main memory [15, 16, 5], mainly motivated by the possibility of distributing a large collection among the main memories of several interconnected processors. By using less space for those in-memory indexes (as our technique allows) more text could be cached in the main memory of each pro-cessor and fewer processors (and less communication) would be required.

The paper is organized as follows. The next section de-scribes the coding schemes used as the basis for our research. Section 3 describes wavelet trees and how they can be used. Section 4 presents our reorganizing strategy in detail. Fi-nally, Sections 5 and 6 present our empirical results, conclu-sions and future work.
We cover the byte-oriented encoding methods we will use in this paper; many others exist, e.g. [11, 3].

The basic byte-oriented variant of the original Huffman code is called Plain Huffman (PH) [11]. Plain Huffman does not modify the basic Huffman code except by using bytes as the symbols of the target alphabet. This worsens the compression ratios to 30%, compared to the 25% achieved by the original Huffman coding on natural language and using words as symbols [9]. In exchange, decompression and searching are much faster with Plain Huffman code because no bit manipulations are necessary.

End-Tagged Dense Code (ETDC) [3] is also a word-based byte-oriented compression technique where the first bit of each byte is reserved to flag whether the byte is the last one of its codeword. The flag bit is enough to ensure that the code is a prefix code regardless of the content of the other 7 bits of each byte, so there is no need at all to use Huffman coding in order to maintain a prefix code. Therefore, all pos-sible combinations are used over the remaining 7 bits of each byte, producing a dense encoding. ETDC is easier to build and faster in both compression and decompression. While searching Plain Huffman compressed text requires inspect-ing all its bytes from the beginning, the tag bit in ETDC permits Boyer-Moore-type searching [2] (that is, skipping bytes) by simply compressing the pattern and then running the string matching algorithm. On Plain Huffman this does not work, as the pattern could occur in the text not aligned to any codeword [11]. Moreover, it is possible to start de-compression at any point of the compressed text, because the 7th bit gives ETDC the self-synchronization property: one can easily determine the codeword boundaries.
In general, ETDC can be defined over symbols of b bits, although in this paper we focus on the byte-oriented ver-sion where b = 8. Given source symbols with decreasing probabilities { p i } 0  X  i&lt;n the corresponding codeword using the ETDC is formed by a sequence of symbols of b bits, all of them representing digits in base 2 b  X  1 (that is, from 0 to 2  X  1  X  1), except the last one which has a value between 2  X  1 and 2 b  X  1, and the assignment is done sequentially.
Note that the code depends on the rank of the words, not on their actual frequency. As a result, only the sorted vo-cabulary must be stored with the compressed text for the decompressor to rebuild the model. Therefore, the vocab-ulary will be slightly smaller than in the case of Huffman codes, where some information about the shape of the Huff-man tree must be stored (even for canonical Huffman trees).
As it can be seen, the computation of codes is extremely simple: It is only necessary to sort the source symbols by decreasing frequency and then sequentially assign the code-words. But not only the sequential procedure is available to assign codewords to the words. There are simple en-code and decode procedures that can be efficiently imple-mented, because the codeword corresponding to symbol i is obtained as the number x written in base 2 b  X  1 ,where
In Restricted Prefix Byte Codes (RPBC) [4] the first byte of each codeword completely specifies its length. The en-coding scheme is determined by a 4-tuple ( v 1 ,v 2 ,v 3 ,v isfying v 1 + v 2 + v 3 + v 4  X  R . The code has v 1 one-byte codewords, Rv 2 two-byte codewords, R 2 v 3 three-byte code-words and R 3 v 4 four-byte ones. They require v 1 + v 2 v
R 2 + v 4 R 3  X  n where R is the radix, typically 256. This method improves the compression ratio of ETDC as it adds more flexibility to the codeword lengths, it maintains the ef-ficiency with simple encode and decode procedures (it is also a dense code) but it loses the self-synchronization property. If we seek to a random position in the text, it is not possi-ble to determine the beginning of the current codeword. It is possible to adapt Boyer-Moore searching over text com-pressed with this technique, but it is slower than searching overtextcompressedwithETDC.
A wavelet tree is a succint data structure. It was proposed in [7] for solving rank and select queries over sequences on large alphabets. Given a sequence of symbols B , rank b ( B,i )= y if the symbol b appears y times in the prefix B 1 ,i ,and select b ( B, j )= x if the j th occurrence of the symbol b in the sequence B appears at position x .
The original wavelet tree is a balanced binary tree that divides the alphabet into two halves at each node, and stores bitmaps in the nodes to mark which side was chosen by each symbol in the sequence. Each child handles recursively the part of the sequence formed by its symbols. Solving rank and select queries over bit sequences in constant time is well-known [12, 13]. The wavelet tree reduces rank and select operations on a sequence S to rank and select operations over the bitmaps stored at the nodes. For rank, the tree is traversed top-down, and bottom-up for select.

Multi-ary wavelet trees are introduced in [6], where sym-bol rank and select operations are needed within the nodes. Huffman shaped wavelet trees have also been considered [7, 13]. Our wavelet trees in this paper are in some sense in-spired by these.
Our method can be applied to any word-based, byte-oriented semistatic statistical prefix-free compression technique (as all those mentioned in Section 2). Basically the idea is to re-organize the different bytes of each codeword, placing them in different nodes of a tree that we call wavelet tree for its similarity with the wavelet trees used in [7]. That is, instead of representing the compressed text as a concatenated se-quence of codewords (composed of one or more bytes), each one replacing the original word at that position in the text, we represent the compressed text as a wavelet tree where the different bytes of each codeword are placed at different nodes.

The root of the wavelet tree contains the first byte of all the codewords, following the same order as the words in the original text. That is, at position i intherootweplace the first byte of the codeword that encodes the i th word in the source text. The root has as many children as different bytes can be the first byte of a codeword. For instance, in ETDC the root has always 128 children and in RPBC it will typically have 256  X  v 1 . The node x in the second level (taking the root as the first level) stores the second byte of those codewords whose first byte is x . Hence each node handles a subset of the text words, in the same order they have in the original text. That is, the byte at position i in node x is the second byte of the i th text codeword that starts with byte x . The same arrangement is done to create the lower levels of the tree. That is, node x has as many children as different second bytes exist in codewords with more than 2 bytes having x as their first byte.

Formally, let us represent the text words 1 as w 1 ,w 2 ...w Lets call cw i the codeword representing word w i .Notice that two codewords cw i and cw j can be the same if the i and j th words in the text coincide. The bytes of codeword cw i are denoted as c cw i . The root node of the tree is formed by the following as many bytes as words has the text. As explained, the root has a child for each byte value that can be the first in a codeword. Assume there are r words in the source text en-coded by codewords (longer than 1 byte) starting with the byte x : cw i 1 ...cw i r . Then the node x will store the sequence c
We speak of words to simplify the discussion. In practice both words and separators are encoded as atomic entities in word-based compression. codeword, yet others would correspond to codewords with more than two bytes.

Therefore, node x would have in turn children as explained before. Assume node xy is a child of node x .Itstores of codewords cw j 1 ...cw j k starting with xy , in their original text order. Our wavelet tree is not balanced because some codewords are longer than others. The number of levels will be equal to the number of bytes of the longer codewords.
Figure 1 shows an example of a wavelet tree 2 , built from the text LONG TIME AGO IN A GALAXY FAR FAR AWAY ,and the alphabet  X  = { A , AGO , AWAY , FAR , GALAXY , IN , LONG After obtaining the codewords for all the words in the text, using a known compressor, we reorganize their bytes in the wavelet tree following the arrangement explained. The first byte of each codeword is in the root node. The next bytes are contained in the corresponding child nodes. For exam-ple, the second byte of the word  X  X WAY X  is the third byte of node B 2, because it is the third word in the root node having b 2 as first byte. Its third byte is in node B 2 B 4asits two first codeword bytes are b 2 and b 4 .
 Assume we want to know which is the 6 th word in the text. Starting at the root node in Figure 1, we read the byte at po-sition 6 of the root node: Root [6] = b 4 . The encoding scheme indicates that the codeword is not complete yet, so we move to the second level of the tree. The second byte is contained in the node B 4, which is the child node of the root where the second bytes of all codewords starting by byte b 4 are stored. Usingabyte rank operation we obtain rank b 4 ( Root, 6) = 2. This means that the second byte of the codeword starting in the byte at position 6 in the root node will be the 2 nd byte in the node B 4. In the next level, B 4[2] = b 5 , therefore b 5 is the second byte of the codeword we are looking for. Again the encoding scheme indicates that the codeword is still not complete, and rank b 5 ( B 4 , 1) = 1 tells us that the 3 rd byte of that word will be in the node B 4 B 5 at position 1. One level down, we obtain B 4 B 5[1] = b 2 , and now the obtained sequence b 4 b 5 b 2 is a complete codeword according to the encoding scheme. It corresponds to  X  X ALAXY X  ,which therefore is the 6 th word in the source text.

This process can be used to recover any word. Notice that this mechanism gives direct access and random decom-pression capabilities to encoding methods that do not mark boundaries in the codewords. With the proposed arrange-ment, those boundaries become automatically defined (each byte in the root corresponds to a new codeword).

If we want to search for the first occurrence of  X  X WAY X  in the example of Figure 1, we start by finding out its code-word, which is b 2 b 4 b 3 . Therefore the search will start at the node B 2 B 4, which holds all the codewords starting with b b 4 . In this leaf node we find out where the first byte b occurs, because b 3 is the third byte of the codeword sought. Operation select b 3 ( B 2 B 4 , 1) = 1 tell us that the first occur-rence of our codeword is the first of all codewords starting with b 2 b 4 , thus in the node B 2 the first occurrence of byte b is the one encoding the first occurrence of the word  X  X WAY X  in the text. Again, to know where in the node B 2 is the first byte b 4 we perform select b 4 ( B 2 , 1) = 3. Now we know that in the root node the 3 rd byte b 2 will be the one corresponding to the first byte of our codeword. To know where in the root node is that 3 rd byte b 2 we compute select b 2 ( Root, 3) = 9.
Note that only the shaded byte sequences are stored in the nodes; the text is shown only for clarity. B2 B4
LONG TIME AGO IN A GALAXY FAR FAR AWAY 123456789 123 Finally the result is that the word  X  X WAY X  appears for the first time as the 9 th word of the text. Notice that it would be easy to obtain a snippet of an arbitrary number of words around this occurrence, just by using the explained decom-pression mechanism, on any encoding.

The sum of the space needed for the byte sequences stored at all nodes of the tree is exactly the same as the size of the compressed text. Just a reordering has taken place. Yet, a minimum of extra space is necessary in order to maintain the tree shape information with a few pointers. Actually, the shape of the tree is determined by the compression tech-nique, so it is not necessary to store those pointers, but only the length of the sequence at each node.
We now detail the algorithms for compression, decompres-sion, and searching.
The compression algorithm makes two passes on the source text. In the first pass we obtain the vocabulary and the model (frequencies), and then assign codewords using any prefix-free semistatic encoding scheme. In the second pass the source text is processed again and each word is trans-lated into its codeword. Instead of storing those codewords sequentially, as a classical compressor, the codeword bytes are spread along the different nodes in the wavelet tree. The node where a byte of a codeword is stored depends on the previous bytes of that codeword, as explained.

It is possible to precalculate how many nodes will form the tree and the sizes of each node before the second pass starts, so they can be allocated and filled with the codeword bytes as the second pass takes place. We maintain an array of markers that point to the current writing position at each node, so that they can be filled sequentially following the order of the words in the text.

Finally, we generate the compressed text as the concate-nation of the sequences of all the nodes in the wavelet tree, and add a header with the words  X  codewords assignment, plus the length of the sequence at each tree node.
To decompress from a random text word j , we access the j -th byte of the root node sequence to obtain the first byte of Algorithm 1 Construction of WTDC //input: t ,sourcetext //output: compressed text with shape of wavelet tree voc  X  first -pass ( t ) sort ( voc ) totalNodes  X  calculateNumberNodes () for all node  X  totalNodes do end for for all word  X  t do end for return concatenation of node sequences, vocabulary, and length of node sequences the codeword. If the codeword has just one byte, we finish at this point. If the byte read b i is not the last one of a codeword, we have to go down in the tree to obtain the rest of the bytes. As explained, the next byte of the codeword is stored in the child node Bi , the one reached from the first byte b i . All the codewords starting with that byte b i stored in Bi ,sowecountthenumberofoccurrencesofthe byte b i in the root node before position j by using the rank operation, rank b i ( root, j )= k .Thus k is the position in the child node Bi of the second byte of the codeword. We repeat this procedure as many times as the length of the codeword.
If we need to decompress the previous or the next word we follow the same algorithm starting with the previous or the next entry of the root node.

The complexity of this algorithm is ( l  X  1) times the com-plexityofrankoperation,where l is the length of the code-word. Therefore, its performance depends on the implemen-tation of the rank operation. Algorithm 2 Display x //input: x ,positioninthecompressedtext //output: p , word at position x in the compressed text currentnode  X  rootnode c  X  wt [ currentnode ][ x ] cw  X  [ c ] while cw is not completed do end while p  X  decode ( cw ) return p
After loading the vocabulary and rebuilding the wavelet tree, the full decompression of the compressed text consists of decoding sequentially each entry of the root. All the nodes of the tree will be also processed sequentially, so to gain effi-ciency we maintain pointers to the current first unprocessed entry of each node. Once we obtain the child node where the codeword of the current word continues, we can avoid un-necessary rank operations because that byte will be the next one to process in the corresponding node. Except for this improvement, the algorithm is the same as that explained in Section 4.1.2.
To count the occurrences of a given word, we compute how many times the last byte of the codeword assigned to that word appears in the corresponding leaf node. That leaf node is the one identified by all the bytes of the codeword except the last one. The pseudocode is presented in Algo-rithm 3.
 Algorithm 3 Count operation //input: w ,aword //output: n , number of occurrences of w cw  X  code ( w )
Let cw = cw || c ,being c the last byte currentnode  X  node corresponding to code cw n  X  rank c ( currentnode, length [ currentnode ]) return n
To locate all the occurrences of a given word, we start looking for the last byte of the corresponding codeword cw in the associated leaf node using operation select .Ifthe last symbol of the codeword, cw | cw | , occurs at position j in the leaf node, then the previous byte cw | cw | X  1 of that codeword will be the j th oneoccurringintheparentnode. We proceded in the same way up in the tree until reaching the position x of the first byte cw 1 in the root. Thus x is the position of the first occurrence of the word searched for. To find all the occurrences of a word we proceed in the same way, yet we can use pointers to the already found positions in the nodes to speed up the select operations (this might be relevant depending on the select algorithm used).
It is also possible to search a phrase pattern .Welocateall the occurrences of the least frequent word in the root node, and then check if all the first bytes of each codeword of the Algorithm 4 Locate j th occurrence of word w operation //input: w ,word //input: j , integer //output: position of the j -th occurrence of w cw  X  code ( w )
Let cw = cw || c ,being c the last byte currentnode  X  node corresponding to code cw for i  X  X  cw | to 1 do end for return j pattern match with the previous and next entries of the root node. If those first bytes match, we verify their complete codewords around the candidate occurrence found. Our ex-periments show this is a very efficient method in practice.
As it was mentioned before, the efficiency of the search and random decompression algorithms depends on the im-plementation of rank and select operations.

A baseline solution is to carry out those operations by brute force, that is, by sequentially counting all the occur-rences of the byte we are interested in, from the beginning of the node sequence. This simple option does not require any extra structure. Interestingly enough, it already allows that operations count and locate are carried out more effi-ciently than in classically compressed files. In both cases we do sequential searches, but in the reorganized version these searches are done over a reduced portion of the file. Like-wise, it is possible to access the compressed text at random, even using non-synchronized codes such as PH and RPBC, faster than scanning the file from the beginning.
However, it is possible to drastically improve the perfor-mance of rank and select operations at a very moderate extra space cost, by adapting well-known theoretical techniques [6]. Given a sequence of bytes B [1 ,n ], we use a two-level di-rectory structure, dividing the sequence into sb superblocks and each superblock into b blocks of size n/ ( sb  X  b ). The first level stores the number of occurrences of each byte from the beginning of the sequence to the start of each superblock. The second level stores the number of occurrences of each byte up to the start of each block from the beginning of the superblock it belongs to. The second-level values cannot be larger than sb  X  b , and hence can be represented with fewer bits.

With this approach, rank b i ( B, j ) is obtained by counting the number of occurrences of b i from the beginning of the last block before j up to the position j , and adding to that the values stored in the corresponding block and superblock for byte b i . Instead of O ( n ), this structure answers rank in time O ( n/ ( sb  X  b )). To compute select b i ( B, j )webinary search for the first value x such that rank b i ( B, x )= j .We first binary search the stored values in the superblocks, then those in the blocks inside the right superblock, and finally complete the search with a sequential scanning in the right block. Thetimeis O (log sb +log b + n/ ( sb  X  b )).
An interesting property is tha t this structure is parame-terizable. That is, there is a space/time tradeoff associated to parameters sb and b . The shorter the blocks, the faster the sequential counting of occurrences of byte b i . CR 31.06 31.94 31.06 31.06 31.95 31.07 ZIFF 32.88 33.77 32.88 32.88 33.77 32.89
ALL 32.83 33.66 32.85 32.83 33.66 32.85 We used some large text collections from trec-2 :AP Newswire 1988 (AP) and Ziff Data 1989-1990 (ZIFF), as well as trec-4 , namely Congressional Record 1993 (CR) and Financial Times 1991 to 1994 (FT91 to FT94) to create a large corpora (ALL) by aggregating them all. We also used CR and ZIFF corpus individually. Table 5 presents the main characteristics of the corpora used. The first column indicates the name of the corpus, the second its size (in bytes). The third column in that table indicates the number of words that compose the corpus and finally the fourth column shows the number of different words in the text.
We used the spaceless word model [10] to create the vo-cabulary, that is, if a word was followed by a space, we just encoded the word, otherwise both the word and the separa-tor were encoded.
 An isolated Intel R  X  Pentium R  X  -IV 3.00 GHz system (16Kb L1 + 1024Kb L2 cache), with 4 GB dual-channel DDR-400Mhz RAM was used in our tests. It ran Debian GNU/Linux (kernel version 2.4.27). The compiler used was gcc version 3.3.5 and -O9 compiler optimizations were set. Time results measure cpu user time in seconds.
We measure how our reorganization of codeword bytes af-fects the main compression parameters, such as compression ratio and compression and decompression times.

We use our reorganization method over the compressed texts obtained using three well-known compression tech-niques explained in Section 2. We call WPH, WTDC, and WRPBC to the wavelet-tree reorganization applied over Plain Huffman, End-Tagged Dense Code, and Restricted Prefix Byte Codes, respectively.

Table 2 shows that compression ratio is essentially not affected. There is a very slight loss of compression (close to 0.01%), due to the storage of the tree shape.

Tables 3 and 4 show the compression and decompression time obtained using the wavelet trees. Whereas the com-pression time is almost the same (2%-4% worse), there are larger differences in decompression time (20%-25% slower). With the wavelet tree, decompression is not just a sequen-tial process. For each word of the text, a top-down traver-sal is carried out on the tree, so the benefits of cache and spatial locality disappear. This is more noticeable than at compression, where the overhea d of parsing the source text blurs those time differences.
 CR 2.886 2.870 2.905 3.025 2.954 2.985 ZIFF 11.033 10.968 11.020 11.469 11.197 11.387 ALL 71.317 71.452 71.614 74.631 73.392 74.811 CR 0.574 0.582 0.583 0.692 0.697 0.702 ZIFF 2.309 2.254 2.289 2.661 2.692 2.840
ALL 14.191 13.943 14.131 16.978 17.484 17.576
We show now the efficiency achieved by the reordering technique for pattern searching and random decompression.
Table 6 summarizes the performance, measuring user time, of the main search operations: count all the occurrences of a pattern (in milliseconds), locate the position of the first oc-currence (in milliseconds), locate all (in seconds) and extract all the snippets of 10 words centered around the occurrences of a pattern (in seconds). We run our experiments over the largest corpus, ALL, and show the average time of searching for 100 distinct words randomly chosen from the vocabulary, of frequency up to 50,000 (to avoid searching for stopwords, which is meaningless). We present the results obtained by the compression methods PH, ETDC, and RPBC; by the wavelet trees implemented without blocks and superblocks (WPH, WTDC, and WRPBC); and also by the wavelet trees using those structures (covering 21,000 bytes per block, and 10 blocks per superblock) with a waste of 1% of extra space, to speed up rank and select operations (WPH+, WTDC+, and WRPBC+). Table 5 shows the loading time, so that the compressed text becomes ready for querying, and the inter-nal memory usage to solve queries needed for each method. In the case of rank structures, it takes more than 1 second to create the two-level directory. This time is not as important as search times, because this loading is paid only once.
Even without using the extra space for the blocks and superblock structures, wavelet trees improve all searching capabilities except for extracting all the snippets, as shown in Table 6. This is because snippets require decompress-ing several codewords around each occurrence, and random decompression is very slow in the wavelet trees if one has no extra support for the rank operations that track random codewords down.

By just spending 1% extra space in block and superblock data structures to obtain rank values faster, all the oper-ations are dramatically improved, including the extraction of all the snippets. Only the self-synchronized ETDC is still faster than its corresponding wavelet tree (WTDC+) for extracting snippets. This is because extracting a snip-pet around a word in a non self-synchronized code implies extra operations to permit the decompression of the previ-ous words, while ETDC can easily move backwards in the compressed text.

By raising the extra space allocated to blocks and su-perblocks to 5%, WTDC+ finally takes over ETDC in ex-tracting snippets as well. It is important to remark that our proposal improves all searching capabilities when a compres-sion technique is not self-synchronized.
As explained, our reorganization brings some (implicit) indexed search capabilities into the compressed file. In this section we compare the searc h performance of WTDC+ with a block-addressing compressed inverted index (II) in the style of [14], over text compressed with ETDC, and work-ing completely in main memory 3 . Basically, II is a block-grained inverted index: It assumes that the indexed text is partitioned into blocks of size b , and for each term it keeps a list of occurrences that stores all the block-ids in which that term occurs. To reduce its size, the lists of occurrences were compacted using the index+bc strategy in [5]; that is, an ab-solute sample is kept every s values, and the remaining val-ues are kept with a d-gap strategy combined with byte-codes (bc). Moreover, the text is compressed with ETDC. There-fore, the list of occurrences of a term t points actually to the blocks that contain the beginning of the codeword ETDC ( t ) associated by ETDC to t . Searches for a word t are done by obtaining the block-ids of the blocks in which t appears and then searching for ETDC ( t ) (using Horspool X  X  algorithm) in the pointed blocks. Searches for a phrase t 1 ...t m imply intersecting the lists of occurrences of all the terms t 1 and finally applying Horspool X  X  algorithm to search for the sequence of their codewords p = ETDC ( t 1 ) ...ETDC ( t m in such blocks 4 . As recommended in [5], the intersection of lists is done using set-vs-set , which begins with the shortest list as a pivot and intersects it against the others in increas-ing order of length, using binary search on the larger set to carry out each intersection.

II uses two parameters: the block size ( b )measuredin kilobytes, and the sample value ( s ), such that given a post-ing list of size n , n/s absolute samples will be chosen at regular intervals of size s . Different space-time trade-offs are obtained depending on such parameters. To make a fair
ETDC is used for the II because it permits the fastest text scanning, which is essential to II X  X  performance. For fairness with the space ETDC costs to II, we compared to WTDC+ instead of WTPH+, which was better in all aspects.
Note that we miss phrases that span over consecutive blocks, so a complete II solution would pay some kind of additional penalty to handle this properly. comparison between II and WTDC+ we chose two differ-ent configurations for II ( II s 8 ,b 16 and II s 32 ,b 256 b = 16, s = 32; and b = 256, s = 32, respectively. Then, we chose two settings of WTDC+ ( WT 1 and WT 2 )thatre-quired almost the same amount of memory. More precisely, WT 1 uses one block per each 2,000 bytes in the compressed text, and 1 superblock per each 8 blocks. In WT 2 ,weused 1 superblock per each 20 blocks and 1 block per 7,000 bytes. The sizes of the resulting four structures, as well as their compression ratios, are shown in Table 7.
 Table 7: Sizes of the compared WTDC+ and II structures.
 C. ratio(%) 44.37 38.61 45.54 39.07
Table 8 shows the time (in seconds) needed to locate all the text occurrences of 100 words randomly chosen from the vocabulary, and to extract all the snippets around such oc-currences. We show results for 4 groups of words depending on their number of occurrences (frequency f ): i) f  X  100, ii) 100 &lt;f  X  1 , 000, iii) 1 , 000 &lt;f  X  10 , 000 , and iv) f&gt; 10 , 000. The snippets were obtained by decompress-ing 20 words, starting at an offset 10 words before an oc-currence. Both locate and the extraction of snippets are faster in WTDC+ than in II, as the former directly jumps to the next occurrence whereas the latter has to scan the text. Only when we are extracting the snippets of very fre-quent words can II beat WTDC+, by profiting from the locality of its text decompression process.
 Table 8: Searching for words: WTDC+ Vs Block-addressing Inverted Index. Times in seconds.

Locate 101-1,000 0.134 0.580 1.343 7.260
Snippet 101-1,000 0.771 0.640 2.845 7.300
Table 9 shows the time (in seconds) needed for perform-ing locate and extract-snippet operations over 100 distinct phrase patterns randomly chosen from the text (and dis-carding those where all are stopwords). Results are given depending on the number of words (2 , 4 , 6 , 8) in the phrase pattern. In all cases WTDC+ behaves better than II, and the gap widens as less space is used in both structures. In particular the difference in locate times is always notorious, whereas those for snippet extraction tend to reduce as more snippets have to be extracted, just as for one-word queries. Table 9: Searching for phrases: WTDC+ Vs Block-addressing Inverted Index. Times in seconds.
Locate 6 1.300 2.020 3.050 10.990
Snippet 6 1.300 2.020 3.060 11.000
We remark that our good results essentially owe to the fact that we are not sequentially scanning any significant portion of the file, whereas a block addressing inverted in-dex must sequentially scan (sometimes a significant number of) blocks. As more space is given to both structures, both improve in time but the II eventually takes over WTDC+ (this occurs when both use around 55% of the text X  X  space and a block size of 256 bytes is set for II). If sufficient space is given (70%-80%), the II can directly point to occurrences instead of blocks and needs no scanning. Yet, as explained in the Introduction, using little space is very relevant for the current trend of maintaining the index distributed among the main memory of several processors. What our exper-iments show is that the WTDC+ makes better use of the available space when there is not much to spend.
It has been long established that semistatic word-based byte-oriented compressors such as those considered in this paper are useful not only to save space and time, but also to speed up sequential search for words and phrases. However, the more efficient compressors such as PH and RPBC are not that fast at searching or random decompression, because they are not self-synchronizing. In this paper we have shown how a simple reorganization of the bytes of the codewords obtained when a text is being compressed, can produce clear codewords boundaries for those compressors. This gives bet-ter search capabilities and random access than all the byte-oriented compressors, even those that pay some compression degradation to mark codeword boundaries (TH, ETDC).
As our reorganization permits carrying out all those op-erations efficiently over PH, the most space-efficient byte-oriented compressor, the usefulness of looking for coding variants that sacrifice compression ratio for search or decod-ing performance is questioned: A reorganized Plain Huffman (WPH) will do better in almost all aspects.

This reorganization has also surprising consequences re-lated to implicit indexing of the compressed text. Block-addressing indexes over compressed text have been long con-sidered the best low-space structure to index a text for ef-ficient word and phrase searches. They can trade space for speed by varying the block size. We have shown that the reorganized codewords provide a powerful alternative to these inverted indexes. By adding a small extra structure to the wavelet trees, the search operations are speeded up so sharply that the structure competes successfully with block-addressing inverted indexes that take the same space on top of the compressed text. Especially, our structure is supe-rior when little extra space on top of the compressed text is permitted. More experiments are required to compare more exhaustively our wavelet trees against not only inverted in-dexes but also other reduced-space structures. [1] R. A. Baeza-Yates and B. Ribeiro-Neto. Modern [2] R.S.BoyerandJ.S.Moore.Afaststringsearching [3] N. Brisaboa, A. Fari  X  na,G.Navarro,andJ.Param  X  a. [4] J. S. Culpepper and A. Moffat. Enhanced byte codes [5] J. S. Culpepper and A. Moffat. Compact set [6] P.Ferragina,G.Manzini,V.M  X  akinen, and [7] R. Grossi, A. Gupta, and J. Vitter. High-order [8] D. A. Huffman. A method for the construction of [9] A. Moffat. Word-based text compression. Software [10] E. Moura, G. Navarro, N. Ziviani, and R. Baeza-Yates. [11] E. Moura, G. Navarro, N. Ziviani, and [12] I. Munro. Tables. In Proc. 16th Conference on [13] G. Navarro and V. M  X  akinen. Compressed full-text [14] G. Navarro, E. Moura, M. Neubert, N. Ziviani, and [15] P. Sanders and F. Transier. Intersection in integer [16] T. Strohman and B. Croft. Efficient document [17] A. Turpin and A. Moffat. Fast file search using text [18] G. K. Zipf. Human Behavior and the Principle of [19] J. Zobel, A. Moffat, and K. Ramamohanarao. Inverted
