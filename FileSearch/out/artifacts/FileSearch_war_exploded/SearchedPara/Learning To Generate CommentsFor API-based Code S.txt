 Code comments are an important part of software projects. Previous research has shown that adding comments to code snippets could help developers understand the functionality of code more effectively, and improve the readability and main-tainability of software projects[26, 22, 21]. Furthermore, comments are usually viewed as significant resource for software reuse tasks, such as code retrieval and bug report generation[14, 18]. However, writing comments is a labor-intensive operation which is often delayed or ignored by developers[19, 12]. Therefore, the lack of code comments is a common problem in software development.
 to generate code comments automatically. Early research uses rule-based ap-proaches[5, 10, 20, 27, 1], which usually define heuristic rules to extract the con-tent from code snippets and combine them with predefined templates to gain comments. They often require additional domain knowledge and can only gen-erate restricted comments with templates. Then researchers proposed keyword-based approaches[8, 13] which leverage latent semantic analysis techniques to extract keywords from code snippets. But the final outputs of these approaches are actually keyword lists rather than common comment sentences. After that, retrieval-based approaches[25, 24] leverage existed code snippets and comments in Q&amp;A websites and open source projects, then use code clone and informa-tion retrieval to retrieve comments of code snippets that implementing similar functions as final comments. However, they heavily rely on the efficiency and accuracy of code clone detection.
 approaches are proposed. They learn language models from corpora of code snippets and corresponding comments by probabilistic models[15]or deep neural networks[2, 3, 11], then predict comments word by word based on the learnt lan-guage models. On this basis, machine translation models are further introduced into automatic comment generation recently, which take comment generation as a translation task from code snippets to comment sentences. Related preliminary work[16, 9] has been done to generate pseudocode from code snippets. They show that end-to-end comment generation approaches of machine translation ideas are feasible and effective.
 snippets. With the growth of software projects X  scale, more and more developers rely on API libraries to implement expected requirements[23]. Generating com-ments for API-based code snippets could help developers understand, modify and reuse existed code resource, which is beneficial to development activities relied on API libraries.
 code snippets, which extracts method-level API sequences and applies machine translation neural networks. Research work on API reuse[28, 18], like API usage recommendation, treats API sequences as abstract functionality representations of method-level API-based code snippets. Our approach follows this assump-tion and generates comments for API-based code snippets by translating API sequences to comment sentences with deep neural networks. We leverage open source Java projects data from Github, extract pairs of method-level API se-quences and comments as training and test datasets, and learn end-to-end neural networks of comment generation for API-based code snippets. The evaluation result of 36 . 48% BLEU-4 score and the case study on 24 K generated comments show the effectiveness of our approach. To the best of our knowledge, our work is the first to generate comments for API-based code snippets with extracted API sequences via end-to-end neural networks.
 proach of comment generation for API-based code snippets with API sequences. Section 3 introduces the dataset information, quantitative evaluation results and qualitative case studies. Section 4 gives the conclusion and discusses future work. 2.1 Overview In this paper, we extract API sequences from API-based code snippets and train generative translation models to encode functional semantics implicated in API sequences and translate them for predicting corresponding comments word by word. Thus our approach contains two stages: the offline training stage and online test stage.
 composed of corpus crawling, data preprocessing and model training. We first crawl Java software projects updated before April 2016 from Github 1 and keep source code files from projects of which stars are at top-5 K ranking. Then we use AST toolkits to extract API sequences and comments from method-level code snippets of crawled projects. After text preprocessing on comments and filtering operations based on statistical information of corpus, we finally get the parallel corpus of API sequences and comments (Details are presented in Section 3.1). Table 1 shows a pair of API sequence and comment extracted from a method-level API-based code snippet. It can be seen that the extracted API sequence keeps the core functional semantics of this code snippet.
 generative neural models based on the idea of machine translation. Figure 1 shows the basic components  X  an encoder and a decoder. The encoder accepts API sequences as inputs and learn to understand their implicated semantics. The decoder predicts comments word by word based on context information provided by the encoder. With the trained encoder and decoder, comments can be generated word by word from API sequences of API-based code snippets in the online test stage. 2.2 Models of Generating Comments from API Sequences In this paper, we treat comment generation from API-based code snippets as a translation task from API sequences to comment sentences, and leverage two sequence-to-sequence neural networks to generate comments from API se-quences. Sequence-to-sequence neural networks remove the aligning process in traditional statistical machine translation models and implement the end-to-end translation.
 comment generation from API sequences (annotated as  X  X eq2seq X ). As shown in Figure 2(a), the seq2seq model consists of two recurrent neural networks as the encoder and decoder separately. Given an API sequence ( x 1 ,x 2 ,...,x m ), the encoder accepts it as the input and encodes the whole sequence into a fix-sized context vector c . The decoder accepts c and predicts the comment output ( y nal symbol  X  &lt; EOS &gt;  X . The prediction objective is to maximize the conditional seq2seq, it can be computed as follows: To train the seq2seq model on the whole corpus D , the objective function is to maximize the average log-likelihood of generating the correct comment S com from the API sequence S api : with API methods in the API sequences. For example,  X  X isplay X  aligns to the API method  X  X ndroid.view.WindowManager.getDefaultDisplay X  and  X  X ize X  to  X  X ndroid.view.Display.getSize X  in Table 1. Besides, we find that API sequences in the corpus may be long (detailed in Section 3.1). Existed experimental study on machine translation using seq2seq models shows that evaluation results will decrease with the growth of input length when the length of the test set is larger than the length of the training set[6]. To capture the latent alignment relations between API sequences and comments and improve the encoding effect on long sequences, we further introduce the attention-based sequence-to-sequence model in the literature[4] into our task (annotated as  X  X eq2seq-att X ).
 between the encoder and the decoder. The encoder transfers the input API a context vector in the seq2seq model. When the decoder predicts the word at i th position of the comment, the corresponding hidden state s i is computed by the previous hidden state s i  X  1 , the previous predicted word y i  X  1 and a specific context vector c i as follows: The specific context vector c i is gained from the attention module based on the intermediate sequence ( e 1 ,e 2 ,...,e m ) of the encoder and the predicted sequence of the decoder. The process can be described by the following equations. In the above equations, the function a plays the role of aligning API methods in the API sequence and words in the comment. The weight  X  ij can be seen as the probability of translating the comment word y j from the API method x i . The attention module is usually implemented with a multilayer perceptron so that the whole seq2seq-att model could be trained by stochastic gradient descent. Finally, the objective function of the seq2seq-att model is the same as the one of seq2seq model (Equation 2). 3.1 Dataset We build our dataset of parallel API sequences and comments according to the following steps: on the star ranking of projects, we keep source code files of the top-5 K projects as the preliminary dataset.
 trees and extract API sequences and corresponding comments. First sentences under  X  X avaDoc Comment X  nodes of AST trees are used as comments. We ignore method code without comments. As for API sequences, we mainly extract APIs in JDK and Android libraries. Because we do statistics on projects X  tags and API library frequency with  X  X mport X  statements, then find that most crawled projects are related to mobile developement and API libraries of high frequencies are JDK and Android. Eclipse JDT could extract JDK APIs from code snippets directly from AST trees, but fail on Android APIs. Thus we combine  X  X mport X  statements and the package list of Android library to identify Android method calls in AST trees. Then API sequences of JDK and Android APIs are extracted as the order of method calls in code snippets.
  X  X code X , remove HTML tags, split sentences into word sequences by whites-paces, decompose words in the CamelCase format and finally do the lowercase transformation. The longest length of API sequences and comments is 6 , 325 and 389 respectively, and the average lengths are 4 . 42 and 9 . 81 respectively. The distributions of their lengths are shown in Figure 3. So we set the length scopes of API sequences and comments both as [3, 20], then we remove samples with too long or too short lengths from the dataset.
 after the length filtering operation. The 217 K samples are split as the ratio of 8 : 1 : 1 for training, validation and test. According to the training set, we get the vocabularies of API methods and words in comments, of which the size is 24 , 511 and 22 , 491 respectively. We cut 24 K API methods and 22 K words of high frequencies as the final vocabularies. 3.2 Experimental Settings We implement the two generative models by tensorflow 1.0.1 2 . All the encoders and decoders use LSTM as hidden layers. The unit number of LSTM is 512. We make experiments of 2 and 3 layers for each model. The iterative number of training epochs is set to 50 for all the experiments. We train the models on servers with NVIDIA GeForce GTX 1080 GPU and 16GB DDR4 memory.
 It takes about 7 to 8 hours to train each model. The attention-based models need more training time than models without attention under the same epoch number. 3.3 Results Here we use two metrics, BLEU and Accuracy, to evaluate generated comments in the test set.
 flects how close the generated translation are in words and length to human-expected references. The computation of BLEU is fast and independent of spe-cific languages. BLEU is effected by the n-gram overlap ratio and a penalty weight of length differences between candidate translations and reference trans-lations. Here we use the nltk 3.2.1 3 toolkit to compute BLEU scores under 1-gram to 4-gram (annotated as BLEU-1 to BLEU-4).
 that is the same as the original sentences. Then accuracy is used to evaluate this aspect, computed by the ratio of correctly generated comments in the whole test set.
 API-based snippets with API sequences. Currently we haven X  X  found appropri-ate baselines from previous work. So we first compare results between groups of different experimental settings. Table 2 shows the BLEU and accuracy scores. It can been seen that the attention mechanism could improve the scores of both BLEU and accuracy under the same number of hidden units and layers. The growth of hidden layers is also beneficial to the results. Then we take the BLEU scores in natural language machine translation[7, 4] as an indirect reference. The 34.90%  X  36.48% BLEU-4 scores have achieved the numerical level of current machine translation tasks, which shows the feasibility and effectiveness of gen-erating comments from API sequences with our approach.
 We split generated results into three groups: 1) totally correct, that is as same as the original comments; 2) with high BLEU scores: results of which BLEU-1 scores are higher than 50%, that is sharing more than half of words with original comments; 3) with low BLEU scores: results left after filtering by 1) and 2). present the API sequence, the original comment and the generated comment (in bold). By analyzing cases of generated results, we can find the that:  X  Our approach could generate exactly correct comments from API sequences  X  As for the results of high BLEU scores, they are different from original com- X  The results of low BLEU scores could be divided into meaningless sentences In this paper, we propose an end-to-end approach to generate comments for API-based code snippets automatically. It leverages API sequences to represent the core semantics of code snippets and generates comments from API sequences with sequence-to-sequence neural networks. Our approach trains generative neu-ral models by parallel API sequences and comments extracted from open source Java projects. The experiments achieve 36.48% BLEU-4 score (machine trans-lation metric) and 9.90% accuracy (exactly correct generation). Further case studies on results of correct, high and low BLEU scores show the feasibility and effectiveness of our approach. To the best of our knowledge, we are the first to generate comments for API-based code snippets from API sequences in a way of translation.
 the following aspects can be explored in future work: 1) Make fine-grained filter-ing on the corpus, such as refining comments with POS tagging and removing continuously repetitive APIs in API sequences; 2) Leverage information of AST trees. We discard the structural information and other identifiers here. If we could combine them with current API sequences and use a recursive neural net-work in the encoder, comment generation results may be improved with more context.

