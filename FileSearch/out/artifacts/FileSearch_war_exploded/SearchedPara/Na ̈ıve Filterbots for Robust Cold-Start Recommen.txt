 The goal of a recommender system is to suggest items of interest to a user based on historical behavior of a com-munity of users. Given detailed enough history, item-based collaborative filtering (CF) often performs as well or better than almost any other recommendation method. However, in cold-start situations X  X here a user, an item, or the entire system is new X  X imple non-personalized recommendations often fare better. We improve the scalability and perfor-mance of a previous approach to handling cold-start situa-tions that uses filterbots, or surrogate users that rate items based only on user or item attributes. We show that intro-ducing a very small number of simple filterbots helps make CF algorithms more robust. In particular, adding just seven global filterbots improves both user-based and item-based CF in cold-start user, cold-start item, and cold-start system settings. Performance is better when data is scarce, per-formance is no worse when data is plentiful, and algorithm efficiency is negligibly affected. We systematically compare a non-personalized baseline, user-based CF, item-based CF, and our bot-augmented user-and item-based CF algorithms using three data sets (Yahoo! Movies, MovieLens, and Each-Movie) with the normalized MAE metric in three types of cold-start situations. The advantage of our  X  X a  X   X ve filter-bot X  approach is most pronounced for the Yahoo! data, the sparsest of the three data sets.
 Categories and Subject Descriptors: I.2.6 [Artificial Intelligence]: Learning General Terms: Algorithms, Experimentation, Measure-ment, Performance Keywords: Performance analysis, cold start, collaborative filtering, recommender systems, robustness, hybrid content and collaborative filtering, na  X   X ve filterbots
A recommender system uses algorithmic means to churn through the available data of user preferences in order to Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. suggest items of interest. For example, a recommender sys-tem for books would use the user X  X  past ratings on books to find similar users, and determine algorithmically how much that user would like a book he/she has not read before. Rec-ommender systems have been used for all types of informa-tion, from websites to CD-Roms, books, etc. Recommender systems are widely deployed on the web, appearing on ev-erything from independent community-driven web sites to e-commerce powerhouses.

Once a substantial amount of preference data has been gathered about a particular user and his community, collab-orative filtering (CF) algorithms are among the most effec-tive recommendation algorithms. In particular, item-based CF [29], a simple variant of the very first user-based CF algorithms, is hard to beat when enough data is available. For example, many studies including [29, 13, 7, 22] demon-strate that item-based CF is as good or better than other approaches across a variety of settings in the movie domain.
However one situation when CF algorithms are less ef-fective is when data is sparse, either because the target user is new to the system, an item is new, or both. In fact, in extreme cases, when data is very scarce, simple non-personalized recommendations based on global averages can perform better than CF algorithms. We study the behav-ior of a number of CF algorithms as data availability on users and items grows, examining three types of cold-start situations: cold-start user, cold-start item, and cold-start system. Our algorithm is a simple variation of the filter-bot algorithm proposed by Good et al. [9], which we call the na  X   X ve filterbot algorithm. A filterbot algorithm injects pseudo users or bots into the system. These bots rate items algorithmically according to attributes of items or users, for example according to who acted in a movie, or according to the average of some demographic of users. Ratings gener-ated by the bots are injected into the user-item matrix along with actual user ratings. Then standard CF algorithms are applied to generate recommendations.

In this paper, we examine how simple filterbots effect the performance of standard user-based (UB) and item-based (IB) collaborative filtering algorithms. Our approach im-proves on the scalability of Good et al. X  X  original filterbot approach. In previous work Good et al. saw improvement in performance from a set of 2N personal filterbots for N users and 20 genrebots. In our paper, we demonstrate im-provement in performance with a smaller set of only a total of seven global filterbots. In addition to changing the num-ber of filterbots, we changed the scale on which the filterbots rated movies from binary (high and low in Good et al.) to average ratings. We also demonstrate that switching from Good et al. X  X  UB baseline to IB significantly improves ac-curacy. We then show that our na  X   X ve filterbots improve ac-curacy further, especially in cold-start settings, when users, items, or both are relatively new to the system, and thus have little data associated with them. The computational cost of employing 5-7 na  X   X ve filterbots is almost negligible compared to the baseline.

We analyze the performance of a non-personalized base-line recommender (AVG), traditional user-based (UB) and item-based (IB) CF algorithms, and our na  X   X ve filterbot al-gorithms in three cold start situations with three large data sets in the movie domain: Yahoo! Movies, 1 MovieLens, and EachMovie. 3 We use normalized mean absolute error (NMAE) as our performance metric, as employed in [15]. Among the non-filterbot approaches, IB appears best across a wide range of data availability settings, confirming previ-ous results. We find that AVG does outperform IB when the user-item matrix is extremely sparse. In general, our na  X   X ve filterbot algorithms either equal or beat their non-filterbot counterparts over all datasets and cold-start settings. The benefit of na  X   X ve filterbots grows as the training matrix be-comes more sparse. The benefit of filterbots is most clear with the Yahoo! data set, which also happens to be the sparsest data set.
Collaborative filtering (CF) systems use community rat-ings to determine recommendations for users. CF systems typically work by associating a user with a group of like-minded users, called neighborhoods, then recommending items enjoyed by others in the group. The movie domain is proba-bly the most widely studied application area for CF systems [9, 28, 29, 25, 17, 32], though the same methodology is ap-plicable across a number of domains, including books, music [30], web pages [6], jokes [8], news, newsgroups [27, 14, 19], advertisements, and more.

CF algorithms range from simple nearest-neighbor meth-ods [6, 27, 29] (Breese et al. [6] call these  X  X emory-based X  methods) to more elaborate model-based methods which first train or  X  X ompile X  a model X  X or example, a Bayesian network [6], a classifier [5, 20], a co-clustered matrix [31], or a truncated singular value decomposition matrix [5, 8, 28] X  X ased on historical data, then use the trained model to generate recommendations. Hundreds of CF algorithms have been proposed and studied, including machine learning methods [5, 4, 15, 26], graph-based methods [1, 12], linear algebra based methods [5, 8, 28], and probabilistic meth-ods [11, 23, 24]. A number of hybrid methods combining information filtering (IF) and CF techniques have also been proposed [3, 2, 4, 24, 18, 9], which are especially useful when data is sparse, for example in cold-start situations. In fact, in the extreme cold-start setting, CF methods cannot provide recommendations at all, and IF methods or hybrid IF/CF methods are needed.

In general, given enough data, a straightforward item-based (IB) nearest-neighbor method [29, 13, 7] is hard to
Yahoo! Movies, http://movies.yahoo.com/
MovieLens, http://movielens.org/
The data was available at http://www.research.compaq.com/src/eachmovie/ un-til October 2004 when it was finally retired. beat decisively. Park et al. [22] showed that the Fast MMMF algorithm, which has recently been cited as one of the best CF algorithms [26], does not beat IB. We base our na  X   X ve filterbot approach on Good et al. X  X  filterbot algorithm [9], a hybrid CF/IF approach. A filterbot is an automated agent that rates all or most items using IF techniques. These fil-terbots are then treated as additional users in a CF system. We use three movie ratings datasets: Yahoo! Movies, Movie-Lens and EachMovie. We use movie content information (e.g., cast, genre, synopsis, etc.) from Yahoo! Movies. All movie-ids in MovieLens and EachMovie were converted to the Yahoo! movie-ids to index the same movie content infor-mation. User ratings in the Yahoo!, MovieLens and Each-Movie data sets are integers ranging from 1 (F) to 13 (A+), from 1 to 5, and from 0 to 5, respectively, where higher ratings mean  X  X ore preferred X .

Due to the large number of experiments we run in this study, we choose to use smaller datasets. We use the 100K user rating set, which is sampled and provided by Movie-Lens. Then, we randomly select 10% of ratings for testing and the rest for training. For the EachMovie dataset, we first remove all of the  X  X ounds awful X  ratings since these ratings (which have a weight less than one) described users X  impres-sions of the item but not their actual experience. Next, we sample 10% of users randomly, and randomly assign 10% of their ratings into test and the rest into training set. In our dataset, we choose to keep users who have rated less than 20 movies in the EachMovie data set. We believe that many users in real systems provide only a few ratings to test the system, and then never come back. In order to keep these users, increasing the initial accuracy seems very important. TheYahoo! dataset consists of two files in chronological or-der, where the test data was gathered after the training data. Because of this, the Yahoo! training and testing data sets have slightly different rating-frequency distributions. Table 1 displays summary statistics of the three datasets. For example, the Yahoo! training data contains 211,327 rat-ings from 7,642 users for 11,916 movies. The average user rating ( R u = (
R ings of user u and item i . The average number of ratings ( X  X otes X ) per user, denoted V u , is 27.65. The matrix den-meaning that only 0 . 23% entries in the user-item matrix are filled. The Yahoo! dataset is more biased toward positive user preferences than the other data sets, with a greater fre-quency of top ratings and a higher average rating. Yahoo! users are also more likely express extreme preferences at ei-ther ends of the scale (13=A+ or 1=F), showing a bimodal ratings distribution. The Yahoo! dataset has the lowest den-sity (  X  =0 . 23) while the MovieLens has the highest density (  X  =6 . 3).
We test three CF algorithms: user-based (UB), item-based (IB), and our na  X   X ve filterbot modifications of UB and IB. In this section, we briefly explain the first two algo-(
R u ), the average item rating ( R i ) and matrix density (  X  ).
 rithms. Our modifications will be explained with more de-tail in the Section 5.
UB first calculates the similarities between user ratings using the pearson correlation coefficient [6, 14, 27]. where r u,i is the rating of user u for item i and r u is user average rating for items rated by both u and v . I u is the set of items that user u has rated. It helps to penalize similarity scores that are based on a small number of overlapping items in order to reflect a lack a confidence, yielding a modified similarity score sim ( u, v ) as follows [10]: We set  X  = 50. Then, the predicted rating of item j for user u is calculated as: where r u is user u  X  X  average rating.
IB is very similar to UB. Instead of calculating similarities between users, it calculates similarities between items using adjusted cosine similarity [29, 16]. sim ( i, j )= Note that when r u is calculated, all items which user u has rated are considered. Similarities based on a small number of common items are penalized similarly to (2). The predicted rating of the item i for user u is: where r i denotes the average rating of item i .

Note that IB algorithm can be considered as a model based approach and it consists of two parts; item similarity computations (or model learning stage) and neighbor selec-tion (or prediction calculation stage) using the the model. For example, IB first calculates item similarities and it can be done at offline. Then when a new user-item pair comes to the system, IB selects the top k nearest neighbors of the target item in the candidate set  X  items the target user has rated so far  X  by using the item similarities matrix. Then the prediction of the target item for the target user is given by the sum of the average rating of the target item and the weighted average of its neighbors.
We use  X  X ormalized MAE X  (NMAE), proposed in [15], as a performance metric. MAE [19, 6] can be calculated in two ways: macro-averaged and micro-averaged .Macro-average first calculates the mean absolute error of each user and averaging them over all users. Micro-averaged MAE averages errors over all ratings. From a system-wide per-spective, a recommender service may want to reward users who provide more ratings. If so, micro-averaged metrics which put more weight on high-volume raters are more ap-propriate. For this reason, we use micro-averaged MAE for our cold-start system experiments. For our cold-start user and item experiments, we use macro-averaged MAE in or-der to better measure the average incremental effect of each new user/item. Then MAE is normalized by MAE random , which denotes the expected MAE where predictions are ran-domly selected. If NMAE is smaller than 1, the algorithm works better than random. We used 4.824, 1.6, 1.944 as the MAE random for Yahoo!, MovieLens and EachMovie. 4 Note that NMAE provides the similar performance scale over three different datasets and make comparison easier.
We measure prediction accuracy of AVG, UB, IB and our na  X   X ve filterbot variations of UB and IB in three cold-start situations: cold-start system , where a new service starts and the user-item matrix is extremely sparse; cold-start user , where a new user comes to the system and the system has little knowledge about the user; and cold-start item ,where a new item comes to the system and the system has little knowledge about the item.
To analyze performance variations of CF algorithms in cold-start settings, we first generate training sets of varying degrees of sparsity. We randomly select ratings from the original training data with probability of  X  and use them for training algorithms. We increase  X  from 0.1 to 1. We
We empirically calculate MAE random for Yahoo!, but use the same MAE random in [15] for MovieLens and EachMovie. Table 2: Average density ratio (  X  ) of sparse training data in cold-start system setting. generate 10 different sparse training data for each  X  with 10 different seed numbers. Table 2 shows the average density ratio and the average number of users/items in the sparse training data while  X  increases. For each test, bots use only user ratings in the sparse training data. The same user demographics, content information of items and test data are used for all test. All results in this environment are averaged over 10 runs, each with a different collection of randomly selected ratings used for training.
To study the effect of a new user on CF algorithms, we select users who have rated more than 40 items in the train-ing data and at least 1 item in the test data. We select 432 users from Yahoo!, 612 users from MovieLens, and 1,845 users from EachMovie. Then, we generate 5 different test-user sets, each of which includes 20% of those users. We remove all ratings of test users in the training data, then filterbot ratings and item similarities are calculated based on the given matrix. In UB, we keep ratings of non-test users and one test user in the memory and user similarities between the target and only non-test users arer calculated. Then, in each successive round of testing, we randomly add two more of the target user X  X  ratings back into the training matrix. Each algorithm computes predictions for all of the target user X  X  test ratings. The process repeats, restoring two more training ratings to the training set at each step. The results shown in this setting are the average of five different test-user data.
Similar to cold-start user, we select items that have been rated by more than 40 users in the training data and by at least 1 user in the test data as test items. We select 803 test items from Yahoo!, 651 items from MovieLens, and 702 items for EachMovie. Then, we divide items into 5 test sets. We remove all ratings of test items in the training data and ratings of content-based bots are calculated based on the given matrix. In IB, when 2 more ratings are added, item similarities between the target and other items are re-calculated. The system keep ratings of only non-test items for each user, thus the size of the candidate sets will not change. In UB, user similarities are calculated with only training data and will not be recalculated during the test. Similar to cold-start user, in each successive round of test-ing, we randomly add two more of the target item X  X  ratings back into the training matrix. Each algorithm computes predictions for all of the target item X  X  test ratings. We do not use award and average critic rating information in this experiment since this information is generally not available for a new movie. The results shown in this setting are the average of five different test-item data.
Our algorithm is a variation of Good et al. X  X  [9] filterbot algorithm discussed briefly in Section 2. We were interested in applying filter-bots to the item-based algorithm, which seems to be the best state of the art CF algorithm. In this section, we discuss the problems and limitations of the previous approaches and describe our solution to address these problems. Please refer [22] for more details. A bot can be generated as either an artificial user or item. One example of user-bots are RipperBots in [9]. RipperBots generated personalized ratings of items based on the item features and user profiles. An ActionBot, on the other hand (which rates  X  X ction X  movies) can be considered as an item which generates ratings of all users based on their ratings of action movies. Once the filterbots are defined, we inject their ratings into the system by treating them just like any other users or items, applying either UB or IB to calculate predictions. When any CF algorithm fails to generate a prediction for item i , we use the item X  X  average rating as a default prediction. Note that user and item-bots affect IB and UB differently. In IB, user-bots only have an effect on the learning model  X  the item similarity matrix  X  but do not increase the size of the candidate set. Since more item similarities can be defined due to addition of  X  X seudo X  users, more neighbors for the target item can be chosen from the candidate set. On the other hand, item-bots affect on the size of the candidate set rather than the learning model itself. In UB, user-bots increase the size of the candidate set  X  users who have rated the target item  X  and the item-bots effect on user similarities.
The first bots we consider are the critic ratings. We select 42 critics 5 who have rated more than 10 movies and in-sert their ratings directly to the user-item matrix. We find that even though 42 critics seems to be useful for Yahoo! they cause significant performance degradation on Movie-Lens and EachMovie. Since we consider a media as a critic, ratings of a critic may not be consistent.

We also consider feature-bots, which generate item ratings based on the features of items such as genre and casting in-formation. We inject various feature-bots into the user-item matrix and test their performance. Unfortunately, the per-formance of this approach is often worse. For example, when we inject many user feature-bots into the matrix, they are often useful for IB when the items have been rated by very few users. In this case, item similarities are mainly calcu-lated based on pseudo ratings generated by bots. However, when items have been rated by many users, it can cause a problem. Note that bots generate ratings of most items and those pseudo ratings becomes major factor for the item sim-ilarities even though the item has been rated by many users.
Note that we consider a media, e.g. New York Times ,as a critic rather than an individual reviewer, e.g. Stephen Holden , to increase the coverage of each critic. Thus, item similarities often overfit contents too much. We observe that item feature-bots are useful for the IB and user-feature bots are useful for the UB in the cold-start setting where the number of bots is large. In both cases, bots in-crease the size of the candidate sets rather than changing the similarity model. More detailed results are shown in the [22].
Even though user-bots are useful for UB and item-bots are useful for IB, their limited scalability can be a critical barrier to adoption by many real world systems, as users and item-bots significantly increase the size of the user-item matrix. Thus, we define three criteria, which our bot-augmented ap-proach should meet.
Based on three criteria, we reject personal-bots because of the increased computational complexity. Similarly we re-ject user and item feature bots. Instead of them, we pro-pose 7 global user-bots (GBots) based on aggregate rat-ing information and item content information. AVGBot generates ratings based on average item ratings over all users. VTBot generates the rating of an item i accord-ing to: r i =log  X  V i ,where V i is the number of users who have rated item i .  X  is a normalization factor that caps ratings at the maximum available rating (13 for Yahoo!; 5 for MovieLens and EachMovie). We set  X  = 2 for Yahoo! and  X  = 4 for MovieLens and EachMovie. Critic-bot (CR-Bot) generates item ratings based on their average critic ratings. Award-bot (AWBot) first partitions items based on how many awards they have won ( won i ) or nominated ( nomi i )suchas Cl i = int (( won i +0 . 5  X  nomi i ) / 3). Then, the rating of each item is generated based on the average rating of the items in the partition that the item i belongs to. Actor-bot (ATBot) first calculates ratings of actors over all users. The rating of an actor is the average rating of movies in which the actor has starred. Then, it generates the rating of movie i based on the average of actor ratings who starred in the movie i . Here, we only consider the first five featured actors among the movie X  X  cast. Director-bot (DRBot) and Genre-bot (GRBot) generate ratings of items similarly to ATBot.

To meet scalability criterium, we try to keep the number of bots as small as possible. If we only inject a few bots into the user-item matrix, additional computation complexity is almost negligible. Also, it will minimal effect on the item similarities when enough ratings are available. For user-based CF algorithm, a user can have only a few  X  X seudo X  neighbors among the top 50. Thus, it provides at least com-petitive performance with the original IB and UB when the user-item matrix is dense.
Figure 1 shows NMAE changes of five different recom-mendation algorithms while the density of the training ma-trix increases. We do not compare the performance of the original filter-bot algorithm[9] with others due to its limited scalability. Note that we need to run each algorithm 273 times (10 for each . 1  X   X   X  . 9and1for  X  = 1 with three different data) and it is almost impossible to conduct this experiment with the original algorithm.

Where the matrix is extremely sparse, AVG is more ro-bust than IB and UB. For example, AVG provides better average prediction than IB and UB when  X   X  0 . 5 for Ya-hoo! and  X   X  0 . 2 for MovieLens and EachMovie. Also, where the training matrix is very sparse (i.e.,  X  =0 . 1for Yahoo! and MovieLens and  X   X  0 . 3 for EachMovie), UB provides better predictions than IB. However, the advan-tage of UB disappears rapidly in all three data sets as the density of the training data increases. Our bot-augmented algorithms with seven GBots (IB+7G and UB+7G) always improve the performance of both IB and UB in cold-start. In general, performance improvements of our na  X   X ve filter-bot algorithms become larger as  X  decreases and IB+7G outperforms UB+7G. The only exception is when Yahoo! is used, which is the most sparse data among the three. When  X   X  0 . 7, UB+7G is slightly better than IB+7G on Yahoo! data. When a new user comes to the system, the user affects IB and UB differently. The ratings of a single user do not effect IB X  X  item similarities, since the model is already built offline. If a new user rates more items, it increases the size of candidate set for both IB and IB+7G. Note that additional bots only effect the model but do not increase the size of the candidate set. Thus, when the matrix is sparse such as with the Yahoo! dataset, IB+7G shows better performance than IB due to better similarity computation. The improvements will continue while the user rates more items. However, where the matrix is dense, the performance improvements of IB+7G may not be clear because item similarities have not changed signifigantly. The results shown in the Figure 2 confirm our reasoning. Note that among the three datasets, items in EachMovie have been rated by 174 users while items in Yahoo! have been rated by 18 users on average.
On the other hand, ratings of a new user significantly effect user similarities in UB and UB+7G. If the user rates more items, the system can calculate more accurately user similarities because of the additional information. Addition of bots in UB slightly increase the size of the candidate set. We expect that UB+7G shows increased performance improvement when the user has rated very few items. When the user have rated more items, the user similarities in UB and UB+7G will improve, and as the candidate set increases and the effect of bots will be negligible. The result shown in the Figure 2 confirms our hypothesis. With Yahoo!, UB+7G shows performance improvements over all periods due to the matrix sparsity. In this case, the additional candidates, who rate most of items, are chosen as neighbors in most cases and those bots actually help to generate more accurate predictions for the given user-item pairs. With MovieLens and EachMovie, UB+7G shows performance improvements only when a user has rated very few items. One interesting observation is that when a user has voted only 2 items, all algorithm show similar performance due to the size of the candidate set being very small.
When a new item has received more ratings, item similari-ties in IB will improve. When the item has been rated by few users and the matrix is dense as in EachMovie, the addition of bots may cause inaccurate item similarities because the similarities largely depend on  X  X seudo X  ratings rather than users X   X  X eal X  ratings . However, when the new item has re-ceived enough ratings, the effect of bots will disappear or even improve the quality of predictions. With Yahoo! where the matrix is very sparse, the system often ends up failing to find neighbors in the candidate set due to lack of infor-mation. In cases where there datasets are very sparse and there are no neighbors to correlate with, item similarities will be based solely off of the bot ratings.

In UB algorithm, more ratings of new items will increase the size of the candidate set. Since additional bots only provide a small number of additional candidates, the effect of bots will be clear only when the item has been rated by a few users. When more users rate the item, the size of the candidate set will be larger and the effect of the bots will disappear. Similar to the cold-user setting, the effect of the bots is lessoned or worse when an item has been rated by very few users. It seems that  X  X seudo X  ratings are more useful when they are used as supplements of user ratings rather than by themselves. We conduct performance analyses of five CF algorithms: AVG, IB, UB and our  X  X a  X   X ve filterbot X  variations of IB and UB in three different cold-start environments. We use three data sets from Yahoo!, MovieLens and EachMovie and the performances of algorithms are measured by NMAE. Our filterbot algorithms clearly demonstrate better robustness than UB and IB in all three cold-start situations. The ad-vantage of our algorithm is more clear if we use Yahoo! data, which is the most sparse among the three data sets. We see our main contribution as a detailed study of a number of different filter-bot generation methods and demonstration that a very few number of simple ( X  X a  X   X ve X ) filterbots help collaborative filtering algorithms work better in cold-start situation, with negligible impact on non-cold-start recom-mendation accuracy and system efficiency. In the future, we plan to develop a new algorithm which exploits user implicit data such as user pageview history and search logs. Also, we plan to study how our filterbot algorithm reacts when at-tacks are introduced to the system. We plan to deploy our algorithm within MAD6 [21], a personalized movie search engine developed at Yahoo! Research. We plan to make our Yahoo! data available to academic researchers in the near future. We thank Yahoo! Movies, GroupLens (MovieLens), and Digital Equipment Corporation (EachMovie) for providing valuable movie ratings and content data.
