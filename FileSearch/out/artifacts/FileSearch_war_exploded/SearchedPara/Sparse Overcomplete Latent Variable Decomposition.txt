 shashanka@cns.bu.edu A frequently encountered problem in many fields is the analys is of histogram data to extract mean-ingful latent factors from it. For text analysis where the data represent c ounts of word occurrences from a collection of documents, popular techniques availab le include Probabilistic Latent Semantic Analysis (PLSA; [6]) and Latent Dirichlet Allocation (LDA; [2]). These methods extract compo-nents that can be interpreted as topics characterizing the corpus of documents. Although they are primarily motivated by the analysis of text, these methods c an be applied to analyze arbitrary count data. For example, images can be interpreted as histograms o f multiple draws of pixels, where each draw corresponds to a  X  X uantum of intensity X . PLSA allows us to express distributions that underlie such count data as mixtures of latent components. Extension s to PLSA include methods that attempt to model how these components co-occur (eg. LDA, Correlated Topic Model [1]).
 One of the main limitations of these models is related to the n umber of components they can extract. Realistically, it may be expected that the number of latent c omponents in the process underlying any dataset is unrestricted. However, the number of compone nts that can be discovered by LDA or PLSA is restricted by the cardinality of the data, e.g. by the vocabulary of the documents, or the number of pixels of the image analyzed. Any analysis that attempts to find an overcomplete set of a larger number of components encounters the problem o f indeterminacy and is liable to  X  X xpressiveness X  of the extracted components i.e. the information content in them. Although the methods aim to find  X  X eaningful X  latent components, they do not actually provide any control over the information content in the components.
 In this paper, we present a learning formulation that addres ses both these limitations by employing the notion of sparsity . Sparse coding refers to a representational scheme where, of a set of compo-nents that may be combined to compose data, only a small numbe r are combined to represent any particular instance of the data (although the specific set of components may change from instance to number of latent components, but requiring that only a small number of them contribute to the com-position of the histogram represented by any data instance. In other words, the latent components must be learned such that the mixture weights with which they are combined to generate any data have low entropy  X  a set with low entropy implies that only a fe w mixture weight terms are signifi-cant. This addresses both the limitations. Firstly, it larg ely eliminates the problem of indeterminacy permitting us to learn an unrestricted number of latent comp onents. Secondly, estimation of low entropy mixture weights forces more information on to the la tent components, thereby making them more expressive.
 The basic formulation we use to extract latent components is similar to PLSA. We use an entropic prior to manipulate the entropy of the mixture weights. We formula te the problem in a maximum a posteriori framework and derive inference algorithms. We use an artific ial dataset to illustrate the effects of sparsity on the model. We show through simulation s that sparsity can lead to components that are more representative of the true nature of the data co mpared to conventional maximum like-lihood learning. We demonstrate through experiments on ima ges that the latent components learned in this manner are more informative enabling us to predict un observed data. We also demonstrate that they are more discriminative than those learned using r egular maximum likelihood methods. We then present conclusions and avenues for future work. Consider an F  X  N count matrix V . We will consider each column of V to be the histogram of an independent set of draws from an underlying multinomial dis tribution over F discrete values. Each column of V thus represents counts in a unique data set. V column of V , represents the count of f (or the f th discrete symbol that may be generated by the multinomial) in the n th data set. For example, if the columns of V represent word count vectors for a collection of documents, V document in the collection.
 We model all data as having been generated by a process that is characterized by a set of latent probability distributions that, although not directly obs erved, combine to compose the distribution proportions that are specific to that set. Thus, each histogr am (column) in V is the outcome of draws underlying the n th column of V as where P mixing proportion signifying the contribution of P ( f | z ) towards P Equation 1 is functionally identical to that used for Probab ilistic Latent Semantic Analysis of text the z th latent topic in the documents. Analogous interpretations may be propose d for other types of data as well. For example, if each column of V represents one of a collection of images (each to P ( f | z ) as the basis distributions for the process.
 Geometrically, the normalized columns of V (obtained by scaling the entries of V  X  V vectors in the same simplex. The model expresses P Figure 1: Illustration of the latent variable model. Panels show 3-dimensional data distributions as points within the Standard 2-Simplex given by { (001) , (010) , (100) } . The left panel shows a set of 2 Basis distributions ( compact code ) derived from the 400 data points. The right panel shows a set of 3 Basis distributions ( complete code ). The model approximates data distributions as points lyin g within the convex hull formed by the basis distributions. Al so shown are two data points (marked by + and  X  ) and their approximations by the model (respectively shown by  X  and ).
 P the simplex defined by P ( f | z ) , it can only model  X  V hull. Any  X  V illustrated in Figure 1 for a synthetic data set of 400 3-dime nsional data distributions. 2.1 Parameter Estimation Given count matrix V , we estimate P ( f | z ) and P done through iterations of equations derived using the Expe ctation Maximization (EM) algorithm: Detailed derivation is shown in supplemental material. The EM algorithm guarantees that the above multiplicative updates converge to a local optimum. 2.2 Latent Variable Model as Matrix Factorization We can write the model given by equation (1) in matrix form as p vector indicating P th element corresponding to P ( f | z ) . If we characterize V by R basis distributions, W is an F  X  R matrix. Concatenating all column vectors p write the model as P = WG , where G is an R  X  N matrix. It is easy to show (as demonstrated in the supplementary material) that the maximum likelihood es timator for P ( f | z ) and P to minimize the Kullback-Leibler (KL) distance between the normalized data distribution V P represents the decomposition where D is an N  X  N diagonal matrix, whose n th diagonal element is the total number of counts in
V n and H = GD . The astute reader might recognize the decomposition of equ ation (4) as Non-negative matrix factorization (NMF; [8]). In fact equation s (2) and (3) can be shown to be equivalent to one of the standard update rules for NMF.
 Representing the decomposition in matrix form immediately reveals one of the shortcomings of the that achieves perfect decomposition: W = I ; H = V , where I is the identity matrix (although the algorithm may not always arrive at this solution). However, this solution is no longer of any utility to us since our aim is to derive basis distributions that are c haracteristic of the data, whereas the Figure 2: Illustration of the effect of sparsifying H on the dataset shown in Figure 1. A-G represent any set of three or more bases that form an enclosing polygon a nd there are many such polygons. However, if we restrict the number of bases used to enclose  X  +  X  to be minimized, only the 7 enclosing triangles shown remain as valid solutions. By further impos ing the restriction that the entropy of the mixture weights with which the bases (corners) must be co mbined to represent  X  +  X  must be minimum, only one triangle is obtained as the unique optimal enclosure. columns of W in this trivial solution are not specific to any data, but repr esent the dimensions of the space the data lie in. For overcomplete decompositions where R &gt; F , the solution becomes indeterminate  X  multiple perfect decompositions are possi ble.
 The indeterminacy of the overcomplete decomposition can, h owever, be greatly reduced by im-posing a restriction that the approximation for any  X  V distributions required. By further imposing the constrain t that the entropy of g the indeterminacy of the solution can often be eliminated as illustrated by Figure 2. This principle, which is related to the concept of sparse coding [5], is what we will use to derive overcomplete sets of basis distributions for the data. Sparse coding refers to a representational scheme where, of a set of components that may be com-bined to compose data, only a small number are combined to rep resent any particular input. In the context of basis decompositions, the goal of sparse coding i s to find a set of bases for any data set such that the mixture weights with which the bases are combin ed to compose any data are sparse. Different metrics have been used to quantify the sparsity of the mixture weights in the literature. Some approaches minimize variants of the L approaches minimize various approximations of the entropy of the mixture weights.
 In our approach, we use entropy as a measure of sparsity. We us e the entropic prior , which has been used in the maximum entropy literature (see [9]) to manipulate entropy. Given a probabi lity distribution  X  , the entropic prior is defined as P entropy of the distribution and  X  is a weighting factor. Positive values of  X  favor distributions with lower entropies while negative values of  X  favor distributions with higher entropies. Imposing this prior during maximum a posteriori estimation is a way to manipulate the entropy of the distribu tion. The distribution  X  could correspond to the basis distributions P ( f | z ) or the mixture weights P or both. A sparse code would correspond to having the entropi c prior on P value for  X  . Below, we consider the case where both the basis vectors and mixture weights have the entropic prior to keep the exposition general. 3.1 Parameter Estimation We use the EM algorithm to derive the update equations. Let us examine the case where both priors. We can write log P ( X ) , the log-prior, as Figure 3: Illustration of the effect of sparsity on the synth etic data set from Figure 1. For visual clarity, we do not display the data points.
 Top panels: Decomposition without sparsity . Sets of 3 (left), 7 (center), and 10 (right) basis dis-tributions were obtained from the data without employing sp arsity. In each case, 20 runs of the estimation algorithm were performed from different initia l values. The convex hulls formed by the bases from each of these runs are shown in the panels from left to right. Notice that increasing the number of bases enlarges the sizes of convex hulls, none of wh ich characterize the distribution of the data well.
 Bottom panels: Decomposition with sparsity . The panels from left to right show the 20 sets of estimates of 7 basis distributions, for increasing values o f the sparsity parameter for the mixture weights. The convex hulls quickly shrink to compactly enclo se the distribution of the data. where  X  and  X  are parameters indicating the degree of sparsity desired in P ( f | z ) and P tively. As before, we can write the E-step as The M-step reduces to the equations where we have let  X  represent grange multipliers. The above M-step equations are systems of simultaneous transcendental equa-tions for P ( f | z ) and P W function [4]. It can be shown that P ( f | z ) and P n ( z ) can be estimated as Equations (7), (8) form a set of fixed-point iterations that t ypically converge in 2-5 iterations [3]. The final update equations are given by equation (6), and the fi xed-point equation-pairs (7), (8). De-tails of the derivation are provided in supplemental materi al. Notice that the above equations reduce to the maximum likelihood updates of equations (2) and (3) wh en  X  and  X  are set to zero. More generally, the EM algorithm aims to minimize the KL distance between the true distribution of the subject to the a priori constraints. Consequently, reducing entropy of the mixtur e weights P obtain a sparse code results in increased entropy (informat ion) of basis distributions P ( f | z ) . 3.2 Illustration of the Effect of Sparsity The effect and utility of sparse overcomplete representati ons is demonstrated by Figure 3. In this example, the data (from Figure 1) have four distinct quadril aterally located clusters. This structure Figure 4: Application of latent variable decomposition for reconstructing faces from occluded im-ages ( CBCL Database ). (A). Example of a random subset of 36 occluded test images. Four 6  X  6 patches were removed from the images in several randomly cho sen configurations (corresponding to the rows). (B). Reconstructed faces from a sparse-overco mplete basis set of 1000 learned compo-nents (sparsity parameter = 0.1). (C). Original test images shown for comparison. a triangular simplex, as demonstrated by the top left panel i n the figure. Simply increasing the num-ber of bases without constraining the sparsity of the mixtur e weights does not provide meaningful solutions. However, increasing the sparsity quickly resul ts in solutions that accurately characterize the distribution of the data.
 A clearer intuition is obtained when we consider the matrix f orm of the decomposition in Equation 4. The goal of the decomposition is often to identify a set of l atent distributions that characterize the underlying process that generated the data V . When no sparsity is enforced on the solution, the trivial solution W = I , H = V is obtained at R = F . In this solution, the entire information in V is borne by H and the bases W becomes uninformative, i.e. they no longer contain information about the underlying process.
 However, by enforcing sparsity on H the information V is transferred back to W , and non-trivial solutions are possible for R &gt; F . As R increases, however, W become more and more data-like. At R = N another trivial solution is obtained: W = V , and H = D ( i.e. G = I ). The columns of W now simply represent (scaled versions) of the specific data V rather than the underlying process. For R &gt; N the solutions will now become indeterminate. By enforcing s parsity, we have thus increased the implicit limit on the number of bases that can b e estimated without indeterminacy from the smaller dimension of V to the larger one. We hypothesize that if the learned basis distribution are ch aracteristic of the process that generates the data, they must not only generalize to explain new data fr om the process, but also enable predic-tion of components of the data that were not observed. Second ly, the bases for a given process must be worse at explaining data that have been generated by any ot her process. We test both these hy-potheses below. In both experiments we utilize images, whic h we interpret as histograms of repeated draws of pixels, where each draw corresponds to a quantum of i ntensity. 4.1 Face Reconstruction In this experiment we evaluate the ability of the overcomple te bases to explain new data and predict the values of unobserved components of the data. Specificall y, we use it to reconstruct occluded portions of images. We used the CBCL database consisting of 2429 frontal view face images hand-aligned in a 19  X  19 grid. We preprocessed the images by linearly scaling the gra yscale intensities 2000 images were randomly chosen as the training set. 100 ima ges from the remaining 429 were randomly chosen as the test set. To create occluded test imag es, we removed 6  X  6 grids in ten random configurations for 10 test faces each, resulting in 10 0 occluded images. We created 4 sets of test images, where each set had one, two, three or four 6  X  6 patches removed. Figure 4A represents the case where 4 patches were removed from each face.
 In a training stage, we learned sets of K  X  X  50 , 200 , 500 , 750 , 1000 } basis distributions from the training data. Sparsity was not used in the compact ( R &lt; F ) case (50 and 200 bases) and sparsity Figure 5: 25 Basis distributions (represented as images) ex tracted for class  X 2 X  from training data without sparsity on mixture weights (Left Panel, sparsity p arameter = 0) and with sparsity on mixture weights (Right Panel, sparsity parameter = 0.2). Basis imag es combine in proportion to the mixture weights shown to result in the pixel images shown. Figure 6: 25 basis distributions learned from training data for class  X 3 X  with increasing sparsity parameters on the mixture weights. The sparsity parameter w as set to 0, 0.2 and 0.5 respectively. In-creasing the sparsity parameter of mixture weights produce s bases which are holistic representations of the input (histogram) data instead of parts-like feature s. was imposed (parameter = 0.1) on the mixture weights in the ov ercomplete cases (500, 750 and 1000 basis vectors).
 we estimate the distribution underlying the image as a linea r combination of the basis distributions. This is done by iterations of Equations 2 and 3 to estimate P known, stay fixed) based only on the pixels that are observed ( i.e. we marginalize out the occluded pixels). The combination of the bases P ( f | z ) and the estimated P P ( f ) for the image. The occluded pixel values at any pixel f is estimated as the expected number of counts at the pixels, given by P the value of the image at the f th pixel and { F reconstructed faces for the sparse-overcomplete case of 10 00 basis vectors. Figure 7A summarizes the results for all cases. Performance is measured by mean Si gnal-to-Noise-Ratio (SNR), where SNR for an image was computed as the ratio of the sum of squared pixel intensities of the original image to the sum of squared error between the original image p ixels and the reconstruction. 4.2 Handwritten Digit Classification In this experiment we evaluate the specificity of the bases to the process represented by the training data set, through a simple example of handwritten digit clas sification. We used the USPS Handwrit-ten Digits database which has 1100 examples for each digit cl ass. We randomly chose 100 examples from each class and separated them as the test set. The remain ing examples were used for training. represents the index of the class. Figure 5 shows 25 bases ima ges extracted for the digit  X 2 X . To classify any test image v , we attempted to compute the distribution underlying the im age using the bases for each class (by estimating the mixture weights P k The  X  X atch X  of the bases to the test instance was indicated by the likelihood L k of the image com-puted using P k ( f ) = the true class of the image to best compose it, we expect the li kelihood for the correct class to be maximum. Hence, the image v was assigned to the class for which likelihood was the highes t. Figure 7: (A). Results of the face Reconstruction experimen t. Mean SNR of the reconstructions is shown as a function of the number of basis vectors and the test case (number of deleted patches, shown in the legend). Notice that the sparse-overcomplete c odes consistently perform better than the compact codes. (B). Results of the classification experi ment. The legend shows number of basis distributions used. Notice that imposing sparsity al most always leads to better classification performance. In the case of 100 bases, error rate comes down b y almost 50% when a sparsity parameter of 0.3 is imposed.
 Results are shown in Figure 7B. As one can see, imposing spars ity improves classification perfor-mance in almost all cases. Figure 6 shows three sets of basis d istributions learned for class  X 3 X  with different sparsity values on the mixture weights. As the spa rsity parameter is increased, bases tend performance -as the representation of basis distributions gets more holistic, the more unlike they become when compared to bases of other classes. Thus, there i s a lesser chance that the bases of one class can compose an image in another class, thereby impr oving performance. In this paper, we have presented an algorithm for sparse extr action of overcomplete sets of latent distributions from histogram data. We have used entropy as a measure of sparsity and employed the entropic prior to manipulate the entropy of the estimate d parameters. We showed that sparse-overcomplete components can lead to an improved characteri zation of data and can be used in appli-cations such as classification and inference of missing data . We believe further improved characteri-zation may be achieved by the imposition of additional prior s that represent known or hypothesized structure in the data, and will be the focus of future researc h.
 [1] DM Blei and JD Lafferty. Correlated Topic Models. In NIPS , 2006. [2] DM Blei, AY Ng, and MI Jordan. Latent Dirichlet Allocatio n. Journal of Machine Learning [3] ME Brand. Pattern Discovery via Entropy Minimization. I n Uncertainty 99: AISTATS 99 , 1999. [4] RM Corless, GH Gonnet, DEG Hare, DJ Jeffrey, and DE Knuth. On the Lambert W Function. [5] DJ Field. What is the Goal of Sensory Coding? Neural Computation , 1994. [6] T Hofmann. Unsupervised Learning by Probabilistic Late nt Semantic Analysis. Machine Learn-[7] PO Hoyer. Non-negative Matrix Factorization with Spars eness Constraints. Journal of Machine [8] DD Lee and HS Seung. Algorithms for Non-negative Matrix F actorization. In NIPS , 2001. [9] J Skilling. Classic Maximum Entropy. In J Skilling, edit or, Maximum Entropy and Bayesian
