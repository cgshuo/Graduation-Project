 With the increase of available public data sources and the interest for analyzing them, privacy issues are becoming the eye of the storm in many applications. The vast amount of data collected on human beings and organizations as a result of cyberinfrastructure advances, or that collected by statisti-cal agencies, for instance, has made traditional ways of pro-tecting social science data obsolete [5, 15]. This has given rise to different techniques aimed at tackling this problem and at the analysis of limitations in such environments, such as the seminal study by Aggarwal of anonymization tech-niques and their dependency on data dimensionality [1]. The growing accessibility to high-capacity storage devices allows keeping more detailed information from many areas. While this enriches the information and conclusions extracted from this data, it poses a serious problem for most of the previ-ous work presented up to now regarding privacy, focused on quality and paying little attention to performance aspects. In this workshop, we want to gather researchers in the areas of data privacy and anonymization together with researchers in the area of high performance and very large data volumes management. We seek to collect the most recent advances in data privacy and anonymization ( i.e. anonymization tech-niques, statistic disclosure techniques, privacy in machine learning algorithms, privacy in graphs or social networks, etc) and those in High Performance and Data Management ( i.e. algorithms and structures for efficient data manage-ment, parallel or distributed systems, etc).
 Categories and Subject Descriptors: K.4.0 Comput-ers and Society:General;H.2.8 Database Applications:Data Mining, Statistical Databases General Terms: Security, Performance Keywords: Efficient Privacy-Enhancing Technologies, Effi-cient Statistical Disclosure Control, Privacy Preserving Data Mining for Large Datasets
As privacy requirements grow, the complexity of the tech-niques required to guarantee them also grows, sometimes exponentially. There are several examples that make it ev-ident that performance is becoming essential for guarantee-ing privacy, ranging from the complexity of anonymization the exception of some recent work, privacy concerns asso-ciated with graph-data analysis and management, and the performance problems derived from the complexity of the first proposals presented in the literature, have been largely ignored. Zhou and Pei [18], for instance, define privacy so that each node must have k others with the same (one-step) neighborhood characteristics. Zheleva and Getoor [17] study the effectiveness of machine learning techniques to infer sen-sitive links which have been erased, given a graph in which non-sensitive links have been anonymized. Liu et al. [8] ask for the k-degree anonymous graph that stems from a graph with the minimum number of graph-modification op-erations. These first proposals require the execution of com-plex algorithms on large graph-like data, posing interesting challenges in terms of performance.
Ensuring privacy usually requires from auxiliary processes, as for example the preprocessing of data using data cleansing techniques, or the use of record linkage to test the quality of anonymization techniques. Record linkage techniques are designed to establish relationships among different entities and, thus, they are a common method for an intruder to re-veal anonymized data by relinking confidental information to the real entities to which it belongs. There are differ-ent classical approaches for improving performance during a Record Linkage process, like standard blocking and slid-ing window methods, that intend to reduce the number of record comparisons. Other like RAR are aimed at reducing the number of attribute comparisons [14]. Other authors have focused on finding other efficient data structures to speed-up the record linkage processes [2], or to exploit the architecture potential for parallelizing these processes [4].
In this brief overview, we have presented three different scenarios where privacy issues on very large data sets require from considering computational aspects in order to improve performance. Parallel computation, a smarter use of multi-core systems, the reduction of the complexity of anonymiza-tion techniques, new indexing structures, etc. are just some of the topics that might feed this workshop.
 [1] C. C. Aggarwal. On k-anonymity and the curse of [2] P. Christen. Automatic record linkage using seeded [3] J. Domingo-Ferrer and J. M. Mateo-Sanz. Practical
