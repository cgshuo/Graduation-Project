 Battista Biggio battista.biggio@diee.unica.it Blaine Nelson blaine.nelson@wsii.uni-tuebingen.de Pavel Laskov pavel.laskov@uni-tuebingen.de Machine learning techniques are rapidly emerging as a vital tool in a variety of networking and large-scale system applications because they can infer hidden pat-terns in large complicated datasets, adapt to new be-haviors, and provide statistical soundness to decision-making processes. Application developers thus can employ learning to help solve so-called big-data prob-lems and these include a number of security-related problems particularly focusing on identifying malicious or irregular behavior. In fact, learning approaches have already been used or proposed as solutions to a number of such security-sensitive tasks including spam, worm, intrusion and fraud detection (Meyer &amp; Whateley, 2004; Biggio et al., 2010; Stolfo et al., 2003; Forrest et al., 1996; Bolton &amp; Hand, 2002; Cova et al., 2010; Rieck et al., 2010; Curtsinger et al., 2011; Laskov &amp;  X  Srndi  X c, 2011). Unfortunately, in these do-mains, data is generally not only non-stationary but may also have an adversarial component, and the flex-ibility afforded by learning techniques can be exploited by an adversary to achieve his goals. For instance, in spam-detection, adversaries regularly adapt their ap-proaches based on the popular spam detectors, and generally a clever adversary will change his behavior either to evade or mislead learning.
 In response to the threat of adversarial data manip-ulation, several proposed learning methods explicitly account for certain types of corrupted data (Globerson &amp; Roweis, 2006; Teo et al., 2008; Br  X uckner &amp; Schef-fer, 2009; Dekel et al., 2010). Attacks against learning algorithms can be classified, among other categories (c.f. Barreno et al., 2010), into causative (manipula-tion of training data) and exploratory (exploitation of the classifier). Poisoning refers to a causative attack in which specially crafted attack points are injected into the training data. This attack is especially important from the practical point of view, as an attacker usually cannot directly access an existing training database but may provide new training data; e.g. , web-based repositories and honeypots often collect malware ex-amples for training, which provides an opportunity for the adversary to poison the training data. Poisoning attacks have been previously studied only for simple anomaly detection methods (Barreno et al., 2006; Ru-binstein et al., 2009; Kloft &amp; Laskov, 2010). In this paper, we examine a family of poisoning attacks against Support Vector Machines (SVM). Following the general security analysis methodology for machine learning, we assume that the attacker knows the learn-ing algorithm and can draw data from the underlying data distribution. Further, we assume that our at-tacker knows the training data used by the learner; generally, an unrealistic assumption, but in real-world settings, an attacker could instead use a surrogate training set drawn from the same distribution ( e.g. , Nelson et al., 2008) and our approach yields a worst-case analysis of the attacker X  X  capabilities. Under these assumptions, we present a method that an attacker can use to construct a data point that significantly decreases the SVM X  X  classification accuracy.
 The proposed method is based on the properties of the optimal solution of the SVM training problem. As was first shown in an incremental learning tech-nique (Cauwenberghs &amp; Poggio, 2001), this solution depends smoothly on the parameters of the respective quadratic programming problem and on the geometry of the data points. Hence, an attacker can manipu-late the optimal SVM solution by inserting specially crafted attack points. We demonstrate that finding such an attack point can be formulated as optimiza-tion with respect to a performance measure, subject to the condition that an optimal solution of the SVM training problem is retained. Although the test er-ror surface is generally nonconvex, the gradient ascent procedure used in our method reliably identifies good local maxima of the test error surface.
 The proposed method only depends on the gradients of the dot products between points in the input space, and hence can be kernelized . This contrasts previous work involving construction of special attack points ( e.g. , Br  X uckner &amp; Scheffer, 2009; Kloft &amp; Laskov, 2010) in which attacks could only be constructed in the fea-ture space for the nonlinear case. The latter is a strong disadvantage for the attacker, since he must construct data in the input space and has no practi-cal means to access the feature space. Hence, the pro-posed method breaks new ground in optimizing the im-pact of data-driven attacks against kernel-based learn-ing algorithms and emphasizes the need to consider resistance against adversarial training data as an im-portant factor in the design of learning algorithms. We assume the SVM has been trained on a data set D tr = { x i ,y i } n i =1 , x i  X  R d . Following the standard no-tation, K denotes the matrix of kernel values between two sets of points, Q = yy &gt; K denotes the label-annotated version of K , and  X  denotes the SVM X  X  dual variables corresponding to each training point. Depending on the value of  X  i , the training points are referred to as margin support vectors (0 &lt;  X  i &lt; C , set S ), error support vectors (  X  i = C , set E ) and reserve points (  X  i = 0, set R ). In the sequel, the lower-case letters s,e,r are used to index the corresponding parts of vectors or matrices; e.g. , Q ss denotes the margin support vector submatrix of Q . 2.1. Main derivation For a poisoning attack, the attacker X  X  goal is to find a point ( x c ,y c ), whose addition to D tr maximally de-creases the SVM X  X  classification accuracy. The choice of the attack point X  X  label, y c , is arbitrary but fixed. We refer to the class of this chosen label as attacking class and the other as the attacked class.
 The attacker proceeds by drawing a validation data set D val = { x k ,y k } m k =1 and maximizing the hinge loss incurred on D val by the SVM trained on D tr  X  ( x c ,y c max In this section, we assume the role of the attacker and develop a method for optimizing x c with this objective. First, we explicitly account for all terms in the margin conditions g k that are affected by x c : g It is not difficult to see from the above equations that L ( x c ) is a non-convex objective function. Thus, we exploit a gradient ascent technique to iteratively op-timize it. We assume that an initial location of the attack point x (0) c has been chosen. Our goal is to up-date the attack point as x ( p ) c = x ( p  X  1) c + tu where p is the current iteration, u is a norm-1 vector representing the attack direction, and t is the step size. Clearly, to maximize our objective, the attack direction u aligns to the gradient of L with respect to u , which has to be computed at each iteration.
 Although the hinge loss is not everywhere differen-tiable, this can be overcome by only considering point indices k with non-zero contributions to L ; i.e. , those for which  X  g k &gt; 0. Contributions of such points to the gradient of L can be computed by differentiating Eq. (2) with respect to u using the product rule: where The expressions for the gradient can be further re-fined using the fact that the step taken in direction u should maintain the optimal SVM solution. This can expressed as an adiabatic update condition using the technique introduced in (Cauwenberghs &amp; Poggio, 2001). Observe that for the i -th point in the training set, the KKT conditions for the optimal solution of the SVM training problem can be expressed as: The equality in condition (4) and (5) implies that an infinitesimal change in the attack point x c causes a smooth change in the optimal solution of the SVM, under the restriction that the composition of the sets S , E and R remain intact. This equilibrium allows us to predict the response of the SVM solution to the variation of x c , as shown below.
 By differentiation of the x c -dependent terms in Eqs. (4) X (5) with respect to each component u l (1  X  l  X  d ), we obtain, for any i  X  X  , which can be rewritten as The first matrix can be inverted using the Sherman-Morrison-Woodbury formula (L  X utkepohl, 1996): (8) into (7) and observing that all components of the inverted matrix are independent of x c , we obtain: Substituting (9) into (3) and further into (1), we obtain the desired gradient used for optimizing our attack: where 2.2. Kernelization From Eq. (10), we see that the gradient of the objec-tive function at iteration k may depend on the attack point x ( p ) c = x ( p  X  1) c + tu only through the gradients of the matrix Q . In particular, this depends on the cho-sen kernel. We report below the expressions of these gradients for three common kernels.  X  Linear kernel:  X  Polynomial kernel:  X  RBF kernel: The dependence on x ( p ) c (and, thus, on u ) in the gra-dients of non-linear kernels can be avoided by substi-tuting x ( p ) c with x ( p  X  1) c , provided that t is sufficiently small. This approximation enables a straightforward extension of our method to arbitrary kernels. 2.3. Poisoning Attack Algorithm The algorithmic details of the method described in Section 2.1 are presented in Algorithm 1.
 In this algorithm, the attack vector x (0) c is initialized by cloning an arbitrary point from the attacked class and flipping its label. In principle, any point suffi-ciently deep within the attacking class X  X  margin can Algorithm 1 Poisoning attack against SVM Input: D tr , the training data; D val , the validation data; y c , the class label of the attack point; x (0) c , the initial attack point; t , the step size.
 Output: x c , the final attack point. 1: {  X  i ,b } X  learn an SVM on D tr . 2: k  X  0. 3: repeat 4: Re-compute the SVM solution on D tr  X  X  x ( p ) c ,y c } 5: Compute  X  X   X  X  on D val according to Eq. (10). 6: Set u to a unit vector aligned with  X  X   X  X  . be used as a starting point. However, if this point is too close to the boundary of the attacking class, the iteratively adjusted attack point may become a reserve point, which halts further progress.
 The computation of the gradient of the validation error crucially depends on the assumption that the structure of the sets S , E and R does not change during the up-date. In general, it is difficult to determine the largest step t along an arbitrary direction u , which preserves this structure. The classical line search strategy used in gradient ascent methods is not suitable for our case, since the update to the optimal solution for large steps may be prohibitively expensive. Hence, the step t is fixed to a small constant value in our algorithm. After each update of the attack point x ( p ) c , the optimal solu-tion is efficiently recomputed from the solution on D tr , using the incremental SVM machinery ( e.g. , Cauwen-berghs &amp; Poggio, 2001).
 The algorithm terminates when the change in the vali-dation error is smaller than a predefined threshold. For kernels including the linear kernel, the surface of the validation error is unbounded, hence the algorithm is halted when the attack vector deviates too much from the training data; i.e. , we bound the size of our attack points. The experimental evaluation presented in the follow-ing sections demonstrates the behavior of our pro-posed method on an artificial two-dimensional dataset and evaluates its effectiveness on the classical MNIST handwritten digit recognition dataset. 3.1. Artificial data We first consider a two-dimensional data generation model in which each class follows a Gaussian distri-bution with mean and covariance matrices given by  X   X  = [  X  1 . 5 , 0],  X  + = [1 . 5 , 0],  X   X  =  X  + = 0 . 6 I . The points from the negative distribution are assigned the label  X  1 (shown as red in the subsequent figures) and otherwise +1 (shown as blue). The training and the validation sets, D tr and D val (consisting of 25 and 500 points per class, respectively) are randomly drawn from this distribution.
 In the experiment presented below, the red class is the attacking class. To this end, a random point of the blue class is selected and its label is flipped to serve as the starting point for our method. Our gradient ascent method is then used to refine this attack un-til its termination condition is satisfied. The attack X  X  trajectory is traced as the black line in Fig. 1 for both the linear kernel (upper two plots) and the RBF ker-nel (lower two plots). The background in each plot represents the error surface explicitly computed for all points within the box x  X  [  X  5 , 5] 2 . The leftmost plots in each pair show the hinge loss computed on a vali-dation set while the rightmost plots in each pair show the classification error for the area of interest. For the linear kernel, the range of attack points is limited to the box x  X  [  X  4 , 4] 2 shown as a dashed line. For both kernels, these plots show that our gradient ascent algorithm finds a reasonably good local maxi-mum of the non-convex error surface. For the linear kernel, it terminates at the corner of the bounded re-gion, since the error surface is unbounded. For the RBF kernel, it also finds a good local maximum of the hinge loss which, incidentally, is the maximum classi-fication error within this area of interest. 3.2. Real data We now quantitatively validate the effectiveness of the proposed attack strategy on a well-known MNIST handwritten digit classification task (LeCun et al., 1995). Similarly to Globerson &amp; Roweis (2006), we focus on two-class sub-problems of discriminating be-tween two distinct digits. 1 In particular, we consider the following two-class problems: 7 vs. 1; 9 vs. 8; 4 vs. 0. The visual nature of the handwritten digit data provides us with a semantic meaning for an attack. Each digit in the MNIST data set is properly normal-ized and represented as a grayscale image of 28  X  28 pixels. In particular, each pixel is ordered in a raster-scan and its value is directly considered as a feature. The overall number of features is d = 28  X  28 = 768. We normalized each feature (pixel value) x  X  [0 , 1] d by dividing its value by 255.
 In this experiment only the linear kernel is considered, and the regularization parameter of the SVM is fixed to C = 1. We randomly sample a training and a vali-dation data of 100 and 500 samples, respectively, and retain the complete testing data given by MNIST for D ts . Although it varies for each digit, the size of the testing data is about 2000 samples per class (digit). The results of the experiment are presented in Fig. 2. The leftmost plots of each row show the example of the attacked class taken as starting points in our algo-rithm. The middle plots show the final attack point. The rightmost plots displays the increase in the vali-dation and testing errors as the attack progresses. The visual appearance of the attack point reveals that the attack blurs the initial prototype toward the ap-pearance of examples of the attacking class. Compar-ing the initial and final attack points, we see this effect: the bottom segment of the 7 straightens to resemble a 1, the lower segment of the 9 becomes more round thus mimicking an 8, and round noise is added to the outer boundary of the 4 to make it similar to a 0. The increase in error over the course of attack is es-pecially striking, as shown in the rightmost plots. In general, the validation error overestimates the classifi-cation error due to a smaller sample size. Nonetheless, in the exemplary runs reported in this experiment, a single attack data point caused the classification error to rise from the initial error rates of 2 X 5% to 15 X 20%. Since our initial attack point is obtained by flipping the label of a point in the attacked class, the errors in the first iteration of the rightmost plots of Fig. 2 are caused by single random label flips. This confirms that our attack can achieve significantly higher error rates than random label flips, and underscores the vul-nerability of the SVM to poisoning attacks.
 The latter point is further illustrated in a multiple point, multiple run experiment presented in Fig. 3. For this experiment, the attack was extended by in-jecting additional points into the same class and av-eraging results over multiple runs on randomly cho-sen training and validation sets of the same size (100 and 500 samples, respectively). One can clearly see a steady growth of the attack effectiveness with the in-creasing percentage of the attack points in the training set. The variance of the error is quite high, which can be explained by relatively small sizes of the training and validation data sets. The poisoning attack presented in this paper is the first step toward the security analysis of SVM against training data attacks. Although our gradient ascent method is arguably a crude algorithmic procedure, it attains a surprisingly large impact on the SVM X  X  empirical classification accuracy. The presented at-tack method also reveals the possibility for assessing the impact of transformations carried out in the input space on the functions defined in the Reproducing Ker-nel Hilbert Spaces by means of differential operators. Compared to previous work on evasion of learning al-Laskov, 2010), such influence may facilitate the prac-tical realization of various evasion strategies. These implications need to be further investigated.
 Several potential improvements to the presented method remain to be explored in future work. The first would be to address our optimization method X  X  restriction to small changes in order to maintain the SVM X  X  structural constraints. We solved this by tak-ing many tiny gradient steps. It would be interesting to investigate a more accurate and efficient computa-tion of the largest possible step that does not alter the structure of the optimal solution.
 Another direction for research is the simultaneous opti-mization of multi-point attacks, which we successfully approached with sequential single-point attacks. The first question is how to optimally perturb a subset of the training data; that is, instead of individually opti-mizing each attack point, one could derive simultane-ous steps for every attack point to better optimize their overall effect. The second question is how to choose the best subset of points to use as a starting point for the attack. Generally, the latter is a subset selec-tion problem but heuristics may allow for improved ap-proximations. Regardless, we demonstrate that even non-optimal multi-point attack strategies significantly degrade the SVM X  X  performance.
 An important practical limitation of the proposed method is the assumption that the attacker controls the labels of the injected points. Such assumptions may not hold when the labels are only assigned by trusted sources such as humans. For instance, a spam filter uses its users X  labeling of messages as its ground truth. Thus, although an attacker can send arbitrary messages, he cannot guarantee that they will have the labels necessary for his attack. This imposes an ad-ditional requirement that the attack data must satisfy certain side constraints to fool the labeling oracle. Fur-ther work is needed to understand these potential side constraints and to incorporate them into attacks. The final extension would be to incorporate the real-world inverse feature-mapping problem; that is, the problem of finding real-world attack data that can achieve the desired result in the learner X  X  input space. For data like handwritten digits, there is a direct map-ping between the real-world image data and the input features used for learning. In many other problems ( e.g. , spam filtering) the mapping is more complex and may involve various non-smooth operations and nor-malizations. Solving these inverse mapping problems for attacks against learning remains open.
 This work was supported by a grant awarded to B. Biggio by Regione Autonoma della Sardegna, PO Sardegna FSE 2007-2013, L.R. 7/2007  X  X romotion of the scientific research and technological innovation in Sardinia X . The authors also wish to acknowledge the Alexander von Humboldt Foundation and the Heisen-berg Fellowship of the Deutsche Forschungsgemein-schaft (DFG) for providing financial support to carry out this research. The opinions expressed in this paper are solely those of the authors and do not necessarily reflect the opinions of any sponsor.

