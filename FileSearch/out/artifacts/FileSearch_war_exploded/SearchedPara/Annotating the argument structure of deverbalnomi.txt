 Abstract Over recent years, there has been a growing interest in the computa-tional treatment of nominalized Noun Phrases due to the rich semantic information they contain. These Noun Phrases can be understood as verbal paraphrases and, just like them, they can also denote argument and thematic-role relations. This paper presents the methodology followed to annotate the argument structure of deverbal nominalizations in the Spanish AnCora-Es corpus. We focus on the automated annotation process that is mostly based on the semantic information specified in a verbal lexicon but also on the syntactic and semantic information annotated in the corpus. The heuristic rules that make use of this information rely on linguistic assumptions that are also evaluated as we evaluate the reliability of the automated process. The automated annotation was manually checked in order to ensure the accuracy of the final resource. We demonstrate its feasibility (77% F-measure) and show that it facilitates corpus annotation, which is always a time-consuming and costly process. The result is the enrichment of the AnCora-Es corpus with the argument structure and thematic roles of deverbal nominalizations. It is the first Spanish corpus with this kind of information that is freely available.
 Keywords Nominalization Argument structure Semantic corpus annotation Heuristic rules 1 Introduction This paper presents the methodology followed to annotate the argument structure of deverbal nominalizations in the Spanish AnCora-Es corpus (Taule  X  et al. 2008 ; Recasens and Mart X   X  2010 ), focusing on the automated annotation process. We mainly describe the set of heuristic rules, and the underlying linguistic knowledge, developed for assigning the argument position (arg0, arg1, arg2, etc.) and thematic role (agent, patient, cause, etc.) to each argument of a deverbal nominalization. By deverbal nominalization we refer to those nouns that are derived from verbs, i.e. construction ,  X  X onstruction X . Departing from the initial assumption that deverbal nominalizations inherit the argument structure of the base verb, these rules rely mostly on the semantic information contained in the AnCora-Verb lexicon (Aparicio et al. 2008 ), and partially on the linguistic information encoded in the AnCora-Es corpus. This paper also aims to evaluate the reliability of the automated annotation process and the linguistic assumptions underlying it. We demonstrate that it facilitates corpus annotation, which is always a time-consuming and costly process. Moreover, the results achieved prove that reusing the verbal information specified in existing linguistic resources is a feasible and promising strategy for annotating deverbal nominalization argument structure, as was also shown by Pado  X  et al. ( 2008 ) and Gurevich and Waterman ( 2009 ). The automated annotation of arguments and thematic roles is manually validated in order to ensure the accuracy of the definitive resource: the Spanish AnCora-Es corpus enriched with the argument structure of deverbal nominalizations. This is a new layer of semantic annotation. Previously, the corpus was only semantically annotated with the argument structure of verbs (56,590 verbal tokens). More precisely, we annotate a total of 23,439 tokens belonging to 1,655 different types of deverbal nominaliza-tions, which represent 30% of the total of nominal and verbal predicates in the corpus. This resource will be used as a gold-standard to evaluate the automated process. Identifying this information can be very useful for many tasks and applications in Natural Language Processing (NLP), especially for Information Extraction (IE) and Question-Answering (QA) systems. It can also be very helpful for dealing with parsing decisions like PP (prepositional phrase) attachment, and as training and test data for nominal semantic role labeling (Surdeanu et al. 2008 ). Although there have been similar projects for English such as NomBank (Meyers 2007 ), to our knowledge, this is the only Spanish corpus that is annotated with the argument structure of nominalizations. 1 In addition, this type of corpus can provide real evidence for the linguistic analysis of nominalizations, such as the discourse factors that influence the choice of either a sentence or a nominalization to express an event.
 The paper is organized as follows. In Section 2 , related work is presented. Section 3 describes the method used to annotate argument structures automatically. Section 4 reports the evaluation of the automated annotation process by comparing its results to those of the manual validation. In Section 5, we discuss the findings and compare our results to those of the related work. Finally, main conclusions are drawn in Section 6 . 2 Related work In recent years, there has been a growing interest in the treatment of deverbal nominalizations. In the NomBank project, the argument structure of about 5,000 common nouns appearing in the Penn Treebank corpus (Meyers 2007 ) was manually annotated. To annotate deverbal nominalizations, they generalize the rich set of PropBank verb frames (Palmer et al. 2005 ) in order to use them to annotate a large number of nominalization arguments with numbered arguments. In contrast, we annotate AnCora-Es in a semiautomatic rather than fully manual way and we include thematic roles in the argument annotation. NomBank annotation has been used as training data to develop nominal semantic role labeling (SRL) systems such as the ones described in Surdeanu et al. ( 2008 ).

In the FrameNet project (Ruppenhofer et al. 2006 ) the nominalizations are also represented. Pado  X  et al. ( 2008 ) address the task of SRL for event nominalizations using only verbal data for training, that is, they borrow existing verbal annotations from FrameNet to carry out SRL for unseen but similar nominal predicates. They experiment with four model types for assigning argument labels to event nominalizations. A hybrid model that combines syntactic information with distributional semantics achieves the best results. The approach used by Pado  X  et al. ( 2008 ) is very interesting but intrinsically different from ours since they use previously annotated verbal information focusing on the development of a nominal SRL system.

The idea of taking advantage of verbal data to annotate the argument structure of deverbal nominalizations has also been proposed in other works such as Hull and Gomez ( 2000 ), Gurevich et al. ( 2006 ), and Gurevich and Waterman ( 2009 ). Hull and Gomez ( 2000 ) is one of the earliest proposals for assigning argument structure to nominalizations. They designed three algorithms that use verbal information (verbal senses and subcategorization frames) and take into account some noun constraints (e.g., the order of arguments, constituent requirements for argument realization, the preposition in Prepositional Phrase (PP) complements) to interpret the nominalized Noun Phrase (nominalized NP).

Gurevich et al. ( 2006 ) present a method for mapping the arguments of verbs to those of their corresponding deverbal nouns, relying on a rich verbal lexicon and on a series of heuristics for knowledge representation purposes. A more recent work by Gurevich and Waterman ( 2009 ) focuses on deverbal nominalizations derived from transitive verbs. They examine three different models for assigning syntactic preferences ( ? Subj, ? Obj) to deverbal complements. That is, they analyze whether deverbal complements are interpretable as subject-like ( ? Subj) or object-like ( ? Obj) complements. They show that the best results are obtained with the model that takes into account lexically specific role preferences for each argument of a deverbal noun. This is possibly the most similar study to ours in that they also designed a series of heuristics for the annotation of deverbal nouns using a verbal lexicon. Furthermore, the test sample is rather large, in fact, in order to determine to what extent the size of the sample influences the results, they work with a sample of two million documents and with a subset of ten thousand documents. 3 Semantic annotation of nominalizations in AnCora-Es The semantic annotation of deverbal nominalizations consists of identifying nominal arguments and assigning to them the corresponding argument position (arg0, arg1, arg2, etc.) and thematic role (agent, theme, cause, etc). 2 By deverbal nominalization we mean a noun morphologically derived from a verb (e.g., and the so-called cousins (Meyers 2007 ), which is a noun that gives rise to a verb semantically related to a verb (e.g., victoria ,  X  X ictory X , is semantically related to the verb vencer ,  X  X o win/to beat X ). The deverbal notion used in this context is purely semantic, that is, we understand that a deverbal nominalization is semantically related to a verb, regardless of whether it is morphologically derived from a verb or not. In Spanish, arguments can be incorporated in the noun itself (1), or they can appear either inside (2) or outside (3) the NP. (1) [El invento de Juan ] NP tuvo mucho e  X  xito. (2) [La construccio  X  n de la casa por parte de Juan ] NP duro  X  dos an  X  os. (3) Juan tomo  X  [la decisio  X  n ma  X  s acertada] NP .
In (1), invento ,  X  X nvention X , has an argument patient incorporated in the noun itself, whereas the agent is realized by the PP de Juan ,  X  X y Juan X . This NP can be paraphrased as  X  X uan invented an invention X . In (2), both arguments are realized by PPs inside the NP: de la casa ,  X  X f the house X , is the argument patient of construccio  X  n ,  X  X onstruction X , and por parte de Juan ,  X  X y Juan X , realizes the agent .In (3), Juan is the agent of decisio  X  n ,  X  X ecision X , but this agent is realized outside the NP headed by decisio  X  n . Although the automated annotation of arguments focused on the internal arguments in the NP, the incorporated arguments were also annotated in the manual check process (see Sect. 3.3 ). The external arguments of the NP will be considered in the near future. By external argument we mean those arguments that are outside the nominalized NP, either within the sentence or in other sentences of the same document. Our notion includes the external arguments in NomBank terminology (Meyers et al. 2004 ) as well as the implicit arguments to use the terminology of Gerber and Chai ( 2010 ). The constituents that can be arguments inside a nominalized NP are: Prepositional Phrases (PPs), relational Adjective Phrases (APs), Genitive Relative Pronouns (GRels) and Possessive Determiners (Poss). Other types of constituents such as the remaining APs, Noun Phrases (NPs), Adverbial Phrases (AdvPs) or Sentences (Ss) cannot be arguments inside a nominalized NP (Badia 2002 ; Meyers 2007 ; Picallo 1999 ). 3.1 Annotation scheme We use the same annotation scheme as the one followed to annotate the argument structure of verbs in AnCora-Es (Taule  X  et al. 2008), which is in turn based on PropBank for argument annotation (Palmer et al. 2005 ) and VerbNet for thematic role annotation (Kipper et al. 2000 , 2006 ). In this way, we ensure the consistency of the argument annotation of different predicates (nouns and verbs) and we make our resources compatible with the English ones. As in PropBank, the selected arguments are numbered incrementally X  X rg0, arg1, arg2, arg3, arg4 X  X xpressing their degree of proximity in relation to their predicate, and the adjuncts are labeled as argM. However, since PropBank roles are abstract labels and are defined on a per-lexeme basis (they are specified for each verb individually), we add finer-grained roles, such as those provided by VerbNet, in order to generalize role labels across predicates, being these roles more class specific. In fact, our annotation scheme is similar to the combination of PropBank and VerbNet semantic roles proposed in the SemLink project (Loper et al. 2007 ; Yi et al. 2007 ). 3 Our list of thematic roles includes 19 different labels widely used in linguistics: agt (agent), cau (cause), exp (experi-encer), scr (source), pat (patient), tem (theme), atr (attribute), ben (beneficiary), ext (extension), ins (instrument), loc (locative), tmp (time), mnr (manner), ori (origin), des (goal), fin (purpose), ein (initial state), efi (final state), and adv (adverbial). We use these thematic roles because they provide richer semantic information than would be achieved by the numbered argument tags alone. Our proposal is based on the 23 VerbNet thematic roles because they are specific enough for our purposes but more general than the large number of thematic roles proposed in FrameNet (Baker et al. 1998 ; Ruppenhofer et al. 2006 ). In this database the thematic roles (frame elements) are organized hierarchically and their interpretation is specific for a frame. However, the thematic roles we adopt are compatible with those of FrameNet. The SemLink project (Palmer 2009 ) has shown that FrameNet and VerbNet thematic roles can be mapped.

As well as the combination of these numbered arguments (6 in total) and thematic role tags (19 in total), we use the semantic label RefMod for those constituents that cannot be arguments, and which cannot, therefore, receive a thematic role (i.e., non relational APs, NPs, AdvPs or Ss). This label indicates that these constituents modify the noun they are complementing. The final tagset consists of 36 possible semantic tags, of which only 26 can be filled by the automated annotation of nominalizations.
We use the same annotation scheme for nouns and verbs because we consider that their arguments are of the same type. This becomes evident in deverbal nominalizations in which we rely on the information specified in the verbal lexicon AnCora-Verb (Aparicio et al. 2008 ) to assign their argument position and thematic role. Given that NomBank takes its annotation scheme from PropBank, we can say that the AnCora-Es corpus is a resource comparable to its English counterpart. 3.2 Annotation process The annotation process was carried out in two steps, combining automated and manual processes (Fig. 1 ). In the first step, the corpus was automatically annotated from a set of manually developed heuristic rules that encode linguistic knowledge. The aim of these rules is to map the argument structure of verbs declared in the AnCora-Verb lexicon to the arguments of deverbal nominalizations. To this end, the rules also use information extracted from the AnCora-Es corpus and from a list of relational adjectives (see Sect. 3.2.1 ). In the second step, the annotation resulting from the automated process was manually validated. This also allowed us to evaluate the linguistic assumptions underlying the heuristic rules. In order to ensure the quality of the results, annotation guidelines with accurate criteria were defined 4 and inter-annotator agreement tests were conducted (see Sect. 3.3.1 ).

Candidate nominalizations to be annotated were manually selected from a list o  X  n, -da/-do, -dura/-ura, -e, -ido, -miento/-mento, -ncia/-nza, -o/-eo ) that take verbal stems and have an action-result meaning (Santiago and Bustos 1999 ). This resulted in a list of nominalizations with a potential deverbal meaning. This list also specifies the relationship between the nominalization and the base verb. 3.2.1 Linguistic resources This section presents briefly the linguistic resources used in the automated process: the AnCora-Es corpus, the AnCora-Verb lexicon, and the list of relational adjectives.

AnCora-Es is a 500,000-word Spanish corpus consisting of newspaper texts annotated at different linguistic levels: morphology (Part of Speech (PoS) and lemmas), syntax (constituents and functions), semantics (verbal argument structure, thematic roles, semantic verb classes, named entities, and WordNet nominal senses), and pragmatics (coreference). 5 Notice that AnCora-Es plays a twofold role: (i) it is our target of annotation, and (ii) it is a linguistic resource for the automated annotation process. For instance, the types of constituents and the tags of named entities are information used in the heuristic rules (see Sect. 3.2.2 ).

AnCora-Verb-Es is a lexicon that contains 2,830 verbs, corresponding to those which appear in the corpus. The lexicon specifies the mapping between syntactic functions, arguments and thematic roles taking into account the verbal semantic class and the diatheses alternations in which verbs participate. A verb can have different senses, each related to one or more semantic classes. They differ in the four event classes X  X ccomplishments, achievements, states, activities (Vendler 1967 ; Dowty 1979 ), and the diathesis alternations in which they occur (Va  X  zquez et al. 2000 ). We consider a total of twelve semantic classes, which are summarized in Table 1 together with their corresponding event structure.

Two types of information are taken into consideration in the automated process: (i) the preposition heading the PPs that are verbal arguments, which can be mapped to their corresponding nominalizations arguments; and (ii) the verbal semantic class, which provides a rationale for assigning an argument and a thematic role to nominalized arguments.

Finally, given that relational adjectives are the only ones that can be interpreted as arguments in deverbal nominalizations (Picallo 1999 ; Grimshaw 1990 ; Bosque and Picallo 1996 ), we built a list of relational adjectives by automatically extracting from AnCora-Es the adjectives ending with a suffix from the list: -al, -ario, -es, -ico, -ista, -stico (Rainer 1999 ). Relational adjectives are characterized by expressing a relationship between the involved noun (e.g., actuacio  X  n ) and the noun from which conduct X ), as well as by their position next to the noun. Afterwards, we manually selected the 331 true relational adjectives from the 746 automatically extracted adjective lemmas. 6 3.2.2 Heuristic rules To annotate the argument structure of deverbal nominalizations in AnCora-Es, we manually built 107 heuristic rules whose aim is to map a nominalized constituent to its argument and thematic role using the above-described resources. These rules incorporate linguistic knowledge from a previous corpus study (Peris and Taule  X  sequentially until the first one is successfully applied. The target of the application of these rules is a nominalized NP which is constituted by a nominalization (N) and a particular CONTEXT that may be one, two or three constituents. Each rule satisfies a condition, a logical combination of predicates over N or CONTEXT, and therefore, a semantic tag is assigned. An example of the syntax of the rules is the following:  X  X  X ithin(X, CONTEXT) and (typeof(X,NP) or typeof(X,PP)) and namedentity(X) and typeofnamedentity(X,Location) &gt; argM-loc X  X 
We distinguish between two types of rules: (i) fourteen general rules based on linguistic information from AnCora-Es, and (ii) ninety-three specific rules that also take into account the information in the AnCora-Verb lexicon. These rules and the linguistic hypotheses they are based on are described in this section. Most of the hypotheses reflect proposals in the linguistic literature that were validated in a previous study of a 100k-word subset of AnCora-Es.

General rules . These are rules that are applied first and that rely on the semantic, morphosyntactic and lexical annotations existing in the AnCora-Es corpus. This information allows us to assign unequivocally an argument and a thematic role to a nominalized NP constituent. We consider three types of general rules depending on the information they involve (see Table 2 ). (a) Named entity rules : The first type of rules takes into account the semantic information contained in named entities (NE) of  X  X  X ocation X  X  or  X  X  X ate X  X  type. We assume that the NPs and PPs containing them correspond to locative and temporal adjunct arguments: argM-loc (4) and argM-tmp (5), respectively. (4) [La concentracio  X  n de la produccio  X  n [en Europa NE-location ] PP-argM-loc (6) Presentaron el acto con momentos emblema  X  ticos y con [ anuncios [ (La
Thus, only NPs constituting a named entity of  X  X  X ocation X  X  or  X  X  X ate X  X  type are annotated as arguments. The remaining NPs are not considered to be arguments of the deverbal nominalization (Meyers 2007 ) and so they do not receive a thematic role either. In these cases, the NPs are annotated as RefMod indicating that they modify the reference of the noun (6). (b) Preposition rules : The second type of general rules takes lexical information into account, namely the type of preposition heading prepositional phrases (PPs), as some select a specific thematic role. For instance, the preposition hacia ,  X  X oward X , usually introduces a PP denoting a destination (7), while desde ,  X  X rom X , indicates an origin (8). In the same way, the preposition para ,  X  X or X , usually introduces a PP denoting a purpose (9), and durante  X  X uring X  usually means time (10). (7) En [la marcha [ hacia Bruselas] PP-arg4-des ] NP fue cortando cabezas. (8) Ha sido muy importante la recuperacio  X  n de Hierro, un hombre vital en [la (9) Es preciso aplicar [ remedios [serios] AP-RefMod [ para restablecer la compe-(10) [El incremento [del nu  X  mero de desempleados] PP-arg1-tem [ durante el
However, not all these assumptions always hold true. PPs introduced by desde ,  X  X rom X , for instance, do not always indicate an origin (8) but can also refer to time (11) (see Sect. 4 ). (11) Ha crecido el gasto en los hogares, [el primer incremento [ desde hace siete (c) RefMod rules : The third and last type of rules takes morphosyntactic information into account, namely the type of constituent that modifies deverbal nominalizations: Ss, AdvPs, NPs not constituting a named entity and APs with a non relational adjective head. Following Badia ( 2002 ) and Meyers ( 2007 ), relative Ss are not considered to be argumental nor are most AdvPs. In these cases, the rules assign the RefMod tag to them (12), (13). Although some AdvPs can be adjunct arguments of deverbal nominalizations (14), we do not have sufficient information to distinguish them automatically from AdvP-RefMod complements. Therefore, we assign the RefMod tag to all AdvPs by default. (12) Pod X   X  a estar tras [las amenazas [ que he recibido ] S-RefMod ] NP (13) Quieren [una investigacio  X  n [ complementaria ] AP-RefMod [dentro del sumar-(14) Protagonizo  X  [un recorrido [ a pie ] AdvP-argM-mnr [por la Rambla] PP-
Regarding APs, it is generally accepted that only relational adjectives (15) can be interpreted as arguments of deverbal nominalizations (Picallo 1999 ; Grimshaw 1990 ; Bosque and Picallo 1996 ). Relational adjectives differ from attributive adjectives in that only the latter express a quality of the noun and can appear either next to or in front of the noun ( complementaria in (13) versus (16)). Only the listed relational adjectives are annotated as arguments of deverbal nominalizations. The remaining are tagged as RefMod. (15) El tema de conversacio  X  n era [la actuacio  X  n [ policial ] AP-arg0-agt ] NP . (16) Hoy, tras [una [ maratoniana ] AP-RefMod negociacio  X  n [de trece horas] PP-
Specific rules . These rules were designed to be applied after the general ones, so they do not take into consideration the constituents already matched by the general rules. They are based on the information specified in the AnCora-Verb lexicon, from which the verbal semantic class and the kind of preposition introducing verbal PP arguments are taken. The verbal semantic class allows us to assign an argument and a thematic role to nominalized arguments, whereas the preposition allows us to map verbal PP arguments to their corresponding nominalized arguments. Recall that we consider a total of twelve semantic classes that are organized around the four event classes X  X ccomplishments, achievements, states, activities (Vendler 1967 ;Dowty 1979 ): classes A correspond to accomplishments, classes B correspond to achievements, classes C correspond to states and classes D correspond to activities. These twelve semantic classes are summarized in Table 1 together with their corresponding event structure.

It is important to note that the correct mapping between the arguments of deverbal nominalizations and verbal arguments is ensured by the list of candidate nominalizations, which establishes the relationship between the noun and its base verb. However, we need to consider whether the corresponding verb has one or more meanings. If the verb is monosemic (i.e., it has only one verbal semantic class), then the rules take the information from this particular sense. If the verb is polysemic, then the sense corresponding to the semantic class with the largest number of arguments is automatically chosen and the rules take the information from this sense. In this way, a larger number of arguments is available for mapping.
The specific rules also take the number and type of nominalized constituent (PP, AP, GRel, Poss) into account. Depending on how many constituents the NP has, the verbal arguments that are projected may vary. The information about the type of constituent is also important since some verbal arguments seem to prefer to map to a specific type of nominal constituent. For instance, possessive arguments tend to be interpreted as agents. We consider two types of specific rules depending on the number of nominalized constituents that are candidates to be an argument: (a) rules of one constituent, and (b) rules of two or more constituents. Table 3 summarizes the specific rules involving one nominalized constituent. For the sake of readability the table summarizing the specific rules involving two or more nominalized constituents (Table 10 ) have been included in the Appendix .

Rules involving one nominalized constituent : For the sake of simplicity, each type of rule is briefly presented and we focus on PPs in more detail as they are the most frequent type of constituent. (a1) The rules for PPs are based on two assumptions. First, we hypothesize that a PP modifying a nominalization takes the same argument and thematic role as the PP introduced by the same preposition modifying the base verb. For instance, combatir ,  X  X o fight X , has a PP arg2 instrument generally introduced by the preposition con  X  X ith X  (17). In the corresponding nominalization, combate ,  X  X ight X , the same interpretation holds for the PP headed by con ,  X  X ith X , (18). (17) Necesitaban combatirlo [ con las armas] PP-arg2-ins . (18) [El combate [ con la espada] PP-arg2-ins ] NP siempre es ma  X  s elegante.
Secondly, we observed that PPs headed by de ,  X  X f X , the least marked preposition, show a preference for the arg1 interpretation (19) when possible (semantic classes A, B and C, but not D). In the case of nominalizations corresponding to verbs of class D, the PPs can only be interpreted as arg0, since this is the only possible argument for verbs of this class (20). (19) Pujol dio un toque de alerta sobre [el aumento [ de los accidentes (20) La gran novedad en la lista es [el regreso [ de Richard Dutruel] PP-arg0-
Therefore, in the case of PPs, the rules consider first the preposition and then the semantic class of the verb. For instance, if the preposition is de ,  X  X f X , the argument and thematic role is arg1-tem (if the verb belongs to the verb class A1, B, or C), arg1-pat (if the verb belongs to class A2 or A3), and arg0-agt/exp/src (if the verb belongs to classes D1, D2, and D3, respectively). For instance, in (19) the verb base of aumento ,  X  X ncrease X , belongs to the B1 verbal class, so the argument and thematic role corresponding to the PP is arg1-tem. In contrast, in (20) the verb base of regreso ,  X  X eturn X , belongs to the D1 verbal class, and the argument and thematic role label corresponding to the PP is therefore arg0-agt. If the preposition is not de , the rules look for a verbal argument introduced by the same preposition in the entry of the base verb. If one is found, the complement of the nominalization is assigned the same argument and thematic role (17) and (18). If none is found, the least marked argM-label is associated. (a2) APs (21) and GRels (22) follow the same rules as de -PPs for being assigned an argument and a thematic role. It is understood that the AP referred to here has a head that belongs to the list of relational adjectives and appears next to the nominalization, that is, it is potentially argumental.
 (21) Se esta  X  creando un entorno propicio para [la innovacio  X  n [ empresarial ] AP-(22) Ma  X  s de 1.200 candidatos se presentara  X  n a las elecciones [[ cuya ] -arg1-pat (a3) Possessive determiners prefer to be interpreted as arguments corresponding to verbal subjects. Gurevich and Waterman ( 2009 ) also propose this interpretation as the default one. Therefore, possessive determiners are automatically assigned arg0 when they are constituents of nominalized NPs whose head noun belongs to either the A or D verbal semantic class (23), and arg1 when the head noun belongs to either the verbal semantic class B or C (24). The thematic role depends on the verbal semantic class (see Table 4 ). (23) [[ Su ] -arg0-agt informe ] NP es correcto. (24) Decidieron esperar [[ su ] -arg1-tem salida ] NP .
Table 4 presents the correlation between verbal semantic classes and their arguments and thematic roles.

Rules involving two or more nominalized constituents : We present the rules concerning different combinations of constituents in nominalized NPs and their linguistic motivation. It is worth noting that GRels and Poss can appear only once in an NP and cannot be combined since they appear in the same position, that is, in the specifier position. In Fig. 2 the frequency of different combinations of argument constituents are presented. Since the combinations of two PPs (59%) and a Poss and a PP (24%) constitute the 83% of the total of possible combinations, we present the rules concerning these two combinations in Table 5 . The rules corresponding to other combinations are summed up in the Appendix .

Next, we describe with some detail the rules involving PP ? PP and a Poss ? PP for being the most frequent ones and we briefly describe the others. (b1) For two PPs (PP ? PP), we follow the same criteria as for one PP. We also hypothesize that a PP modifying a nominalization takes the same argument and thematic role as the PP introduced by the same preposition modifying the base verb. However, the rules differ in the case when no match is possible between the PP complements of the nominalization and the base verb. They assign arg1 to the first PP (mostly headed by de ,  X  X f X ) for the semantic classes A, B, and C; and, an arg0 to the second PP for class A (25), or arg2 for classes B and C (26). In the case of class D, the first PP is annotated as arg0 since this is the only possible argument for this type of verbs. The second PP is annotated as argM with no thematic role (27). In these cases, the order of the constituents is significant. The thematic roles depend on the semantic class of the verb (see Table 4 ). (25) Fue [un lanzamiento [ de falta ] PP-arg1-pat [ a cargo de Alonso ] PP-arg0-(26) Los inversores comenzara  X  n a tomar posiciones por [la entrada [ de Terra ] PP-(27) [El regreso [ del Real Madrid ] PP-arg0-agt [ el jueves ] PP-argM-tmp ] NP . (b2) A Poss shows a strong preference for being interpreted as the verbal subject, therefore, the PP Poss is interpreted as the first verbal object in the verbal argument structure (see Table 4 ). The rules that reveal this preference for each verbal class are shown in Table 5 . They assign arg0 to the Possessive for nominalizations deriving from A and D verbal classes. The difference is that in nominalizations from A verbal classes the PP Poss is associated with an arg1 interpretation (28) while in nominalizations from D verbal classes the PP Poss is annotated as argM, since arg0 is the only possible argument for this type of verbs (29). In nominalizations deriving from B and C verbal classes the Possessive is annotated as arg1 and the PP Poss as arg2 (30). In the case of Poss ? AP, the rules are equivalent (see the Appendix ). (28) Presentaron al juez [[su] -arg0-agt propuesta [ de solucio  X  n judicial ] PP-(29) [[Su] -arg0-agt paso [ por Madrid ] PP-argM-loc ] NP ha dejado huella. (30) Justifico  X  [[su] -arg1-tem salida [ del pa X   X  s ] PP-arg2-loc ] NP . (b3) For a PP and an AP , the PP seems to prefer the arg1 interpretation (for the semantic verbal classes A, B, C) when no match is possible between the PP complement of the nominalization and the verb. The AP receives an arg0 interpretation for class A, and an arg2 interpretation for classes B and C. However, if the match is possible, the PP takes the argument and thematic role from the verbal complement. In the case of class D, we prioritize the PP as an argument constituent (arg0), whereas the AP is assigned the value of an adjunct argument, namely the argM label with no thematic role (see the Appendix ). (b4) For two relational APs (AP ? AP), no clear preference for a specific argument interpretation was observed. We assumed that they would behave similarly to PPs and we apply the same criteria as set out in the above rules (see the Appendix ). (b5) Since a GRel is semantically equivalent to the first de -PP, we annotate it with arg1 for the semantic verbal classes A, B, and C, and with arg0 for class D. The other constituent (either a PP2 or an AP2 ) is interpreted as the remaining argument in the verbal argument structure (see the Appendix ). It should be pointed out that we have not found any examples for the combination Grel ? AP and only ten for the combination Grel ? PP in the whole AnCora-Es corpus.

In general, nominalized NPs with more than two constituents are rare because, in contrast to verbs, they do not contain a large number of arguments. Nominalized NPs are structures that tend to condense the information and so the number of arguments is usually low: mostly from zero to two, and up to three or four arguments in a few cases. In these few cases, if there are constituents that remain unannotated after the above rules have been applied, they are annotated with the default label argM. A thematic role is assigned in the manual check process.
This automated process was followed by a manual check of the annotated arguments and thematic roles that is described in the next subsection. 3.3 Manual check In order to evaluate the reliability of the automated process and the linguistic hypotheses underlying it, we carried out a manual validation of the data. Three kinds of information are checked in the manual process: (1) Whether the noun constitutes a deverbal nominalization, i.e., whether it exhibits verbal properties. For example, the nominalization cura means both  X  X he process of curing X  and  X  X riest X . Since all forms of cura are annotated regardless of their meaning in the automated process, it is required that the reviewers make sure that the annotated noun is a real deverbal nominalization. (2) The verbal frame from which the nominalization is derived (from a transitive, unnacusative, intransitive, etc.). And (3) the annotation of arguments, which includes argument positions, thematic roles, the label RefMod (non-argumental complements) and the incorporated arguments that were not taken into account in the automated process. To ensure the quality of this manual process, an inter-annotator agreement test was conducted. We provided the annotators with guidelines and required them to consult the information specified in AnCora-Verb lexicon. The annotations were performed using the AnCoraPipe annotation tool (Bertran et al. 2008 ). 3.3.1 Inter-annotator agreement The inter-annotator agreement test was conducted on a sample of one hundred sentences from the AnCora-Es corpus, each sentence containing a true deverbal nominalization with at least one candidate argumental constituent of the nominal-ization. A total of 131 constituents were included. Three Linguistics graduate students at the University of Barcelona participated in the test. All of them had experience in annotating argument and thematic roles for verbs in the AnCora-Es corpus, so no training was needed. The test consisted of deciding, for each constituent, (1) whether it was an argument and, if so, (2) which argument position (arg0, arg1, arg2, etc.) and thematic role (agent, theme, patient, etc.) they should be assigned (out of a total of 36 possible tags). To this end, they had to take into account the information specified in the AnCora-Verb lexicon about the verbal sense from which they decided the deverbal nominalization came. This was important since we measured inter-annotator agreement taking into account whether the annotators agreed on the verbal class from which the nominalization derived. Disagreements on the argument and thematic role are expected when the verbal class taken as starting point is not the same. For this reason, we calculated observed agreement (Scott 1955 ) and kappa 11 (Siegel and Castellan 1988 ) measures increasing the penalization rate for disagreements when the annotators picked the same verbal class, and reducing the penalization rate for disagreements when the annotators picked different verbal classes. The weighting schema for measuring agreement was empirically set to 40% for the former, and to 60% for the latter.
Total agreement  X  X  0 : 4 agreement on examples having the same VerbalClass  X 
Table 6 presents the results of the inter-annotator agreement test. The columns show the results for each pair of annotators and the average result. The rows present observed agreements and kappa coefficients according to the above formula.
We focus on the average result (last column). As expected, when the annotators did not agree on the verbal class, the agreement decreases approximately 20% both in observed (71.6%) and kappa (66%) agreement scores with respect to when the annotators agreed on the verbal class (90.6 and 88.6%, respectively). As mentioned above, it is very difficult to reach agreement when the verbal class is different since the arguments and thematic roles to be mapped vary. According to the above-presented measure, the mean of inter-annotator agreement reaches 75% kappa, which translates to 79.2% observed agreement. Standard guidelines characterize a kappa over 75% as excellent, 40 X 75% as fair to good, and below 40% as poor (Fleiss, 1981). According to this, 75% is an acceptable degree of agreement. It is even more satisfactory given that there are 36 possible tag combinations, which largely increases the opportunities for disagreement. Furthermore, this agreement score ensures the quality and reliability of the manual annotation process. 4 Evaluation of the automated process In this section, we evaluate the reliability of the automated annotation process. This evaluation allows us to assess the validity of the linguistic hypotheses that our heuristic rules are based on. The global performance of the automated process achieved an F-measure (F1) of 77%, which was computed as the harmonic mean of precision and recall of the automated assigned labels (Table 8 ). This result shows that this automated process is a valid strategy for deverbal nominalization annotation that reduces time and cost by 37% compared to a completely manual annotation.

Table 7 presents the results obtained broken down into constituent types (first column). The second column shows the frequency of each constituent in the corpus. The third and fourth columns indicate whether the constituent is automatically annotated according to a specific or a general rule. Columns 5, 6 and 7 present the Precision, Recall and F1 of the automatic process taking into account all the possible tags assigned. The last three columns show the Precision, Recall and F1 of the automatic process but taking into account only the argumental labels assigned, that is, excluding the RefMod label.

Table 8 presents the results obtained in more detail. These results are shown in the rows broken down into the different labels X  X he combination of arguments and thematic roles X (column 1). In the second column the frequency of each label in the corpus is presented. In each of the remaining columns, the F1 score of the corresponding semantic label is given for each constituent type. The last three columns present the Precision, Recall and F1 for each label independently of the type of constituent. The last two rows present the results for each constituent and for the global performance (the last three columns) with and without taking into account the RefMod label.

As Tables 7 and 8 show, when taking into account all the possible labels, the best results are achieved in the automated annotation of S (97%) and Poss (82%). The results for the former were to be expected since sentences inside nominalized NPs can only be automatically annotated with the non-argumental tag RefMod. Sentences that are complements of nominalizations that do not exhibit deverbal properties account for the remaining 3% error rate. The results for Poss confirm our hypothesis that they are mostly interpreted as arguments corresponding to verbal subjects.
 The mean score for APs is 76% F1. There is a significant difference between the AP non-argumental tag RefMod X  X hich achieves a rate of up to 89% F1 X  X nd the AP argumental tags X  X hich obtain an average F1 score of 28% (Table 8 ). This means that our rule for the detection of non-argumental APs performed considerably better than the one developed to detect argumental APs. We account for this result as follows. Firstly, the ambiguity of relational APs in terms of their argumental or non-argumental nature is a generalized problem. Nearly half of the APs annotated as argumental are considered non-argumental in the validation process. Therefore, our assumption that relational adjectives are argumental does not always hold true. In fact, 213 adjective lemmas from our list of adjectives (331 lemmas) are annotated as argumental or non-argumental depending on the noun they are complementing. This is widely known in Linguistics as lexical co-ocurrence. For instance, an adjective such as constitucional ,  X  X onstitutional X , is interpreted as the arg1-pat of a noun such as reforma ,  X  X eform X , since the underlying meaning is the reform of the Constitution. However, constitucional cannot be interpreted as an argument of a noun such as acusacio  X  n ,  X  X ccusation X , since it is not the arg1-pat nor the arg0-agt, the accusation of the Constitution (arg1-pat) is not a permitted interpretation and the accusation by the Constitution (arg0-agt) is also forbidden. In this case, the adjective constituc-ional is specifying the meaning of acusacio  X  n , it is a type of accusation that infringes the Constitution. This phenomenon of lexical co-ocurrence is very common among relational adjectives that combine with nominalizations, and, therefore, explains why not all the occurrences of relational adjectives behave as noun arguments. Moreover, there are 90 lemmas of adjectives that were not in our list of relational adjectives but have been revised as argumental APs. This is due to the ambiguity of some adjectives. For example, an adjective such as popular ,  X  X opular, famous X , was not included in the list of relational adjectives because it is not formed with any of the corresponding suffixes (see Sect. 3.2.1 ), however, when its meaning is not  X  X amous X  but  X  X elated to people X , then it can be interpreted as an argument. Thus, popular in movilizacio  X  n popular ,  X  X opular mobilization X , is arg0-agt but in el toque popular ,  X  X he popular touch X , it is a RefMod because popular does not mean  X  X elated to people X . Therefore, in order to consider an AP to be argumental, the noun the AP is complementing is more important than whether the adjective heading the AP is relational or not. Secondly, in APs, the order hypothesized by the projection rules does not always hold true. On the one hand, arg1 does not always appear as the first complement as we expected; and on the other hand, arg0 of class A and arg2 of classes B and C do not always appear as the second complement. The results show that there is no fixed order of AP complements that are analogous to the verbal objects and subjects. Thirdly, our intuition that most third and fourth nominal AP complements would be argM is not confirmed by the results, which show only 6% F1 (Table 8 ).

Finally, there are two main types of unpredictable errors: (i) those caused by linking the nominalization to the wrong verbal frame, and (ii) those due to the fact that the nominalization does not exhibit deverbal properties.

An F1 of 76% was achieved for AdvPs , corresponding to 61% precision and 100% recall (Table 8 ). Such a low precision rate was to be expected: we were aware that some AdvPs are adjunct arguments (argM), however, since we did not have enough information to automatically distinguish them from AdvPs-RefMod complements, we assigned the RefMod tag to all AdvPs by default. During the manual check, most of the false positive cases were modified to different types of argM.

We achieved an F1 of 75% for NPs complements (see Tables 7 and 8 ). NPs can be associated with three different tags: argM-loc, argM-tmp, or the non-argumental RefMod. Recall that the two first tags apply if the NP contains a named entity of the  X  X  X ocation X  X  or the  X  X  X ate X  X  type, respectively, and that the third tag applies if the NP does not contain any named entity. Obviously, better results were achieved for the RefMod tag X 83% F1(Table 8 ) X  X ince its rule is straightforward to apply. The assumption that named entities of the  X  X  X ate X  X  type correspond to argM-tmp turns out to be largely true, achieving an F1 of 71% (Table 8 ). However, this F1 corresponds to high precision and mid recall rates, meaning a limited coverage of the rule: nearly half of the argM-tmp NPs were automatically annotated as RefMod. Against our expectations, the rule translating our assumption that named entities of the  X  X  X ocation X  X  type correspond to argM-loc only achieved an F1 of 50% (Table 8 ) because some false positives were reannotated as arg1-pat.

The automated annotation of GRel achieved an F1 of 64%, with a lower rate of precision (53%) than recall (79%) (Table 7 ). However, note that the corpus contains only 28 occurrences of this constituent type inside nominalized NPs, which is a too small a sample for meaningful interpretation. Overall, we can conclude that GRels do not always map to arg1 for classes A, B and C, but that they always map to arg0 for class D. On average, PPs achieved an F1 of 52%, which is a good result taking into account that PPs are highly ambiguous: they can be assigned to 26 different tags. Several reasons account for this result. An important explanatory factor, like in APs, is that the order hypothesized by the projection rules does not always hold: arg1 does not always appear as the first complement and arg0 and arg2 do not always appear as the second complement. This suggests that there is no fixed order of nominal complements being similar to the verbal objects and subjects; 12 and, secondly, that not all PPs in the second position are arg0 for nouns deriving from the A verbal semantic class; actually an important percentage are argM (adjuncts). Thus, it emerges that arg0 is not usually syntactically realized in nominalized NPs. Another assumption that does not always hold is that PPs are necessarily argumental, although the majority are. Up to 24% of PPs turn out to be non argumental. The results for PPs are also affected by the fact that the prepositions of verbal complements and their nominal counterparts are not always the same. This is shown by arg1- X  and arg2- X  tags, which are the ones that prototypically correspond to verbal PP complements. They achieve mid F1 rates X 51% and 52%, respectively X (Table 8 ), exhibiting high precision rates, which means that the argument and thematic role are correct most of the time when the preposition is shared between the verbal and the nominal complement. However, they show a low recall which indicates that there is a large number of arg1- X  and arg2- X  nominal PP complements that cannot be detected because the preposition is not shared between the verbal and the nominal complement. The rules that take into account the type of preposition introducing a PP work quite well (Table 9 ). In general, high recall rates (about 90%) but only mid precision rates (about 50%) are achieved. This suggests that we overgenerated the tags assigned by these rules, covering all or most of the relevant cases but we were less accurate in terms of precision.

The most striking results are those of arg4-des and arg3-ori. In the case of hacia ,  X  X oward X , both recall (66%) and precision (42%) are quite low, and precision is even lower (9%) for desde ,  X  X rom X . The reason is that 40% of PPs headed by desde ,  X  X rom X , are reannotated as argM-tmp. Our intuition regarding the semantic relatedness of the preposition desde with the concept of  X  X rigin X  was not accurate. In fact, if we had associated the preposition desde with the temporal tag argM-tmp, the recall would have been much higher (100%) and the precision would also have increased (36%), therefore, we would have obtained an F1 of 53%, instead of the 15% we obtained associating the preposition desde with the concept of  X  X rigin X . If we have taken into account this result the global performance would have achieved up to 78% F1. Two types of errors account for the results of PPs: partial errors and uncontrollable errors. By partial errors we mean those cases in which there is only one change X  X ither the argument or the thematic role X  X etween the automated and the manually checked annotation. For instance, 19% of argM-loc constituents to which the name-entity-location rule is applied were reannotated as arg1-loc or arg2-loc. Similarly, the default tag argM (with no thematic role) for third and fourth nominal complements, needed to be modified in many cases, although 40% of the cases only required specifying a thematic role. Thus, our intuition that most of third and fourth nominal complements are argM is partially confirmed in contrast to the case of APs. Unforeseeable errors fall into three groups: (i) tags that are missing as the complement was considered not to exhibit verbal properties (2%); (ii) automated annotations that are manually reannotated with erroneous tags (6%); and (iii) errors that are caused by picking the wrong verbal frame from which the nominalization is derived.

If the label RefMod is not included in the evaluation, the results decrease by 22% (see Table 8 ). This is because in the constituents where this label is applied (S, AdvP, NP and AP), the rules for detecting RefMod performed considerably well. For instance, in S and AdvP, since it is the only label that can be applied the results are very good (97% and 76% respectively). Therefore, if we do not take into account these results the global performance of the automated process decreases. Likewise, in NPs and APs, though not being the only possible tag, RefMod is the one whose automated assignation performed best. It should be pointed out that the RefMod rule for adjectives obtained an F1 of 89%, while the rules for assigning argumental adjectives only achieved an F1 of 28% due to the problems mentioned above. This also holds true for NPs, though the decrease is not so important: the rules for the detection of argumental NPs only achieved an F1 of 50% compared to the 83% (see Table 8 ) achieved in the assignation of RefMod tag to NPs. All of these factors explain the decrease in the global performance for these constituents, and, in consequence the global performance of the automated process. That said, it is worth noting that we strongly believe that the RefMod label should be taken into account in the evaluation for two reasons: first, from a linguistic point of view it is important to have the constituents that cannot be arguments of nominalizations also annotated and that is our goal with the RefMod tag. Therefore, the automated rules for assigning this tag have saved a lot of time in the corpus annotation, which is our main purpose. Secondly, if we compare these results with those of nominal SRL systems, it can be said that the rules for identifying RefMod constituents are similar to the arguments identification task in nominal SRL systems, where the constituents that can be arguments are identified and the ones that cannot are discarded. Since our RefMod rules identify the non-argumental tags, we strongly believe that they have to be evaluated as the identification subtask in SRL. 5 Discussion We have mainly described and evaluated the set of heuristic rules that allows us to automatically annotate the argument structure of deverbal nominalizations in the Spanish AnCora-Es corpus. The initial assumption underlying this automated process is that deverbal nominalizations inherit the argument structure of the base verb, which seems to be confirmed since this set of rules, which mostly relies on the semantic information contained in AnCora-Verb lexicon, achieves a global performance of 77% F1 taking into account the RefMod label. Nevertheless, not all the specific linguistic assumptions underpinning the different heuristic rules are validated to the same degree, as the previous analysis has shown. Next we detail which rules perform better and, therefore, which linguistic assumptions are corroborated. In terms of the general rules, it can be said that they perform quite well. Rules for the detection of RefMod complements achieve an F1 of 94% (Table 8 ), which means that the hypothesis in the literature positing that non relational APs, AdvPs, NPs and Ss cannot be arguments inside a nominalized NP (Badia 2002 ; Meyers 2007 ; Picallo 1999 ) is largely confirmed. However, in the case of NPs we claim that those containing a named entity  X  X  X ocation X  X  or  X  X  X ate X  X  will be a locative (argM-loc) or temporal (argM-tmp) adjunct argument, respectively. This is partially confirmed for argM-loc assignation in NPs (50% F1) and strongly confirmed in argM-tmp assignation in NPs (71% F1) (Table 8 ). The rules that take into account the type of preposition introducing a PP also achieve a good performance (see Table 9 ), corroborating that certain prepositions point to specific argM tags.

One of the outcomes of the analysis of the performance of the specific rules, those that take into account the information from the base verb, is that there is no fixed order in the realization of nominal arguments that corresponds to verbal arguments. In PPs and APs, arg1 does not always appear as the first complement and arg0 (for verbal class A) and arg2 (for class B and C) do not always appear as the second complement. However, the inverse order X  X rg1 associated to the second complement X  X ould not have achieved better results. The cases analyzed from the corpus show that the order of nominalized constituents is freer than that of verb arguments and, to a certain extent, depends on the context. For the same reason, a specific syntactic-semantic pattern would not be accurate enough since the context is what provides enough information to associate the arg1 to the first or second nominalized constituent in a nominalized NP. Moreover, this freer order is also motivated by a greater degree of optionality. In fact, we observed that arg0 is an optional argument that is almost never syntactically realized in nominalizations, which is an interesting linguistic finding. Specifically for PPs, it is worth noting that governed prepositions are not always shared between the nominal and the verbal complements: arg1- X  and arg2- X  tags prototypically correspond to verbal PP complements. The rules for their assignation exhibit high precision, meaning that they are correct most of the time when the preposition is shared between the verbal and the nominal complements. However, the low recall indicates that there is a large number of arg1- X  and arg2- X  nominal PP complements that cannot be detected because the preposition is not shared between the verbal and the nominal complements. In the case of APs, we found that some relational adjectives (45%) are non-argumental, thus calling into question our initial hypothesis based on the literature (Picallo 1999 ; Grimshaw 1990 ) that these types of constituent are argumental. What emerges from the analysis of these cases is that relational adjectives are subject to the phenomenon of lexical co-ocurrence, that is, they are annotated as argumental or non-argumental depending on the noun they are complementing. Regarding possessive determiners, our initial hypothesis [shared for English by Gurevich and Waterman ( 2009 )] that this type of constituents are mostly interpreted as arguments corresponding to verbal subjects is largely confirmed. Its automatic assignation achieves an F1 of 82%. Regarding GRel, the sample contained in the corpus is too small for meaningful interpretation (only 28 occurrences) of this constituent type inside nominalized NPs. Finally, the default rule assigning the argM tag to the third or fourth AP or PP is a good choice for the latter constituent but not for the former, which is mostly corrected as RefMod. This confirms that in Spanish PPs are more likely to be argM arguments (adjuncts) of nominalizations while APs tend to modify the nominalization. 5.1 Results comparison Now we will compare the results obtained with those presented in previous works (Sect. 2 ), all devoted to English, this being the first approach that deals with the argument structure of Spanish deverbal nominalizations. The system proposed by Hull and Gomez ( 2000 ) achieves very good results in the interpretation of genitives (93% accuracy), PPs (96%) and APs (71%). Nevertheless, the evaluation was only carried out in a subset of ten different nominalizations (corresponding to 1,247 tokens). It is too small a sample to provide a precise idea of how the algorithm would perform in a much wider variety of nominalizations. Therefore, it is difficult to compare these results with those presented in this paper, which are manually evaluated from a sample of 1,655 different deverbal nominalizations, corresponding to 23,439 tokens.

NomBank has been used as a training corpus for SRL systems based on supervised Machine Learning approaches such as Che et al. ( 2008 ), Johansson and Nugues ( 2008 ), Zhao and Kit ( 2008 ) and Ciaramita et al. ( 2008 ) presented in the CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies (Surdeanu et al. 2008 ). 13 In the close challenge task consisting of the assignation of arguments to deverbal nominalizations (not thematic roles), the best result was achieved by Che et al. ( 2008 ) obtaining an F1 of 76.64%. In this case, the participants only had 14 different tags (arg n ) to assign, while in our case there are 26 possible tags (almost twice as many as in NomBank), because we take into account not only the argument position but also the thematic roles.

Another way of addressing the task of nominal SRL is the unsupervised approach presented by Pado  X  et al. ( 2008 ). They borrow existing verbal annotations from FrameNet to carry out SRL for unseen but similar nominal predicates. A hybrid model that combines syntactic information with distributional semantics achieves the best result (56.42%). Since this model does not learn from nominal but verbal annotation and FrameNet roles are more fine-grained, we consider that this is also a good result that is comparable to our global performance result without taking into account the RefMod label (Table 8 ). Nevertheless, the problem with supervised and unsupervised SRL systems based on Machine Learning techniques is that they do not provide specific evaluations for arguments or constituents in nominalized NPs that give rise to linguistic findings; therefore, the only possible comparison is the global performance, where our set of rules outperforms SRL systems.

The most similar work to ours is Gurevich and Waterman ( 2009 ), who also designed a series of heuristics for annotating deverbal nouns using a verbal lexicon. The test sample is also rather large, two million documents and a subset of ten thousand documents. However, there are two differences: (1) they analyze whether deverbal complements are interpretable as subject-like ( ? Subj) or object-like ( ? Obj) complements, that is, they only assign two possible tags, whereas we work with 26 possible automated tags, which represents an increase in difficulty with respect to the two tags ( ? Subj, ? Obj) used by Gurevich and Waterman. (2) They only deal with  X  X f X -PPs and possessive determiners (Poss), whereas we deal with any constituent type that occurs in a nominalized NP (PPs, APs, GRel, Poss, NPs, AdvP, and Ss).

Therefore, in order to compare the results we focus on PPs and Poss. The F1 performance achieved in deverbal nominalizations with one  X  X f X -PP argument is 82% in Gurevich and Waterman ( 2009 ). This figure beats our PP performance by 30%. However, three important facts have to be mentioned: first, our PP rules include all type of PPs (taking into account all kinds of prepositions) and not only de -PPs X  X quivalent to  X  X f X -PP in Spanish. Secondly, our PP rules assign 26 possible tags which represents an increase in difficulty with respect to the two tags ( ? Subj, ? Obj) used in Gurevich and Waterman. Finally, our evaluation of PPs includes those that appear as the only nominalized constituent as well as PPs that appear in combination with other constituents in nominalized NPs. These three factors explain the difference in F1 obtained in both works. Regarding possessives, the best result is an F1 of 85% in Gurevich and Waterman, which is a 3% improvement over our Poss result (82%). Nevertheless, it is worth noting two important facts: (1) Our Poss rules assign a larger number of labels than the two assigned by them and (2) our Poss evaluation includes possessives that appear as the only nominalized constituent as well as possessives that appear in combination with other constituents in nominalized NPs. These two factors lead us to conclude that our 82% is, in fact, a better result for Possessives than their 85%. 6 Conclusions In this article, we present the methodology followed to annotate the argument structure of deverbal nominalizations in the Spanish AnCora-Es corpus, with special emphasis on the automated process that we used for this annotation. The results achieved an F1 of 77%, thus showing that reusing the verbal information specified in existing linguistic resources is a good approach for the annotation of deverbal nominalization argument structure. Therefore, this automated process facilitates corpus annotation, which is always a time-consuming and costly process (with a time saving of 37%). In order to evaluate the reliability of the automated process, the automatically annotated arguments and thematic roles were manually checked in order to ensure the accuracy of the final resource. This manual validation allows us to evaluate the linguistic assumptions that underlie the automated process. The final outcome, the AnCora-Es corpus, is the only Spanish corpus annotated with the argument structure of deverbal nominaliza-tions, adding to the resources developed for English by the NomBank project (Meyers 2007 ). More precisely, a total of 23,439 tokens belonging to 1,655 different types of deverbal nominalizations were annotated in AnCora-Es. A corpus annotated with this information can be very useful for many NLP tasks and applications, especially for information extraction, question answering, and nominal semantic role labelling systems for Spanish. Furthermore, such a resource can provide real evidence for the linguistic analysis of nominalizations. Our work pointed to several interesting findings regarding the interface between syntax and semantics in nominalized NPs, such as the optionality of arg0 arguments that map to agents, the non-fixed order of nominalizations with respect to their counterparts in a verbal environment, and the change of preposition of nominal PP complements in relation to verbal PP complements. Some of these findings imply that our linguistic assumptions were not totally valid and, therefore, that our automated process could obtain even better results. Future work will focus on applying these improved heuristic rules to Catalan and studying the transportability of these rules to similar Romance languages. Another line of future work will consist of the (automatic) annotation of external arguments in line with the proposals of Gerber and Chai ( 2010 ). This will complete the argument structure annotation of deverbal nominalizations in the AnCora-Es corpus.
 Appendix See Table 10 .
 References
