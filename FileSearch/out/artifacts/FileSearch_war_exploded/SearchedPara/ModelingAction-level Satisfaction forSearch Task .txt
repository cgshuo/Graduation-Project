 Search satisfaction is a property of a user X  X  search process. Understanding it is critical for search providers to evalu-ate the performance and improve the effectiveness of search engines. Existing methods model search satisfaction holisti-cally at the search-task level, ignoring important dependen-cies between action-level satisfaction and overall task satis-faction. We hypothesize that searchers X  latent action-level satisfaction (i.e., whether they believe they were satisfied with the results of a query or click) influences their observed search behaviors and contributes to overall search satisfac-tion. We conjecture that by modeling search satisfaction at the action level, we can build more complete and more accurate predictors of search-task satisfaction. To do this, we develop a latent structural learning method, whereby rich structured features and dependency relations unique to search satisfaction prediction are explored. Using in-situ search satisfaction judgments provided by searchers, we show that there is significant value in modeling action-level satisfaction in search-task satisfaction prediction. In addi-tion, experimental results on large-scale logs from Bing.com demonstrate clear benefit from using inferred action satisfac-tion labels for other applications such as document relevance estimation and query suggestion.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Search-task satisfaction modeling, action-level satisfaction
Measuring search engine performance via behavioral indi-cators of search satisfaction has recently received consider-able attention [1, 11, 12, 15]. In comparison with traditional relevance-based evaluations [4], such methods enable evalua-tion using real user populations, in naturalistic settings, and across a diverse set of information needs. It has been shown that users X  search behaviors provide more accurate signals of search satisfaction than query-document relevance [12, 15].
The core problem in search-task satisfaction modeling is to understand whether users are satisfied with their search actions (i.e., whether they believe they were satisfied with the search results for a particular information need) when performing the task [1, 3, 9, 15]. Unfortunately, searchers X  detailed action satisfaction labels are unobservable in search log data; and they are difficult to obtain at scale from the searchers or reliably from third-party assessors. As a result, most prior search satisfaction models do not directly con-sider user satisfaction at the action level , or elect to only approximate that with specific assumptions. For example, most of existing methods consider search-task as the unit, and extract holistic measures, such as total dwell time [11, 31] and search result clicks [12], to perform search satisfac-tion prediction. Other methods that consider action-level behaviors do not predict users X  detailed satisfaction over those actions [1, 15, 16]. Instead they assume that all ac-tions are satisfying in a satisfying task, and all actions are unsatisfying in an unsatisfying task. This masks the com-plex relationship between action-level satisfaction and over-all search-task satisfaction: e.g., searchers can be ultimately satisfied by the search task, but most of their search actions might be quite unsatisfying [11]. Therefore, such a modeling assumption expropriates the model X  X  ability to discriminate between different actions, i.e., satisfying vs. unsatisfying.
In this work, we hypothesize that users X  perceived action-level satisfaction, even though unobservable in search logs, influences their observed search behaviors and contributes to overall search-task satisfaction. We conjecture that by mod-eling satisfaction at the individual action level, we can build more complete and more accurate predictors of search sat-isfaction. To achieve this, we consider the action-level user satisfaction as latent variables, and explicitly model their relationship to overall task satisfaction in a latent structural learning framework. By introducing the latent variables, ex-pressive features and dependencies unique to the search sat-isfaction problem can be incorporated to depict searchers X  complex behavioral patterns. Knowledge about users X  in-task search behaviors, e.g., consistency between action-level and overall task satisfaction, is naturally modeled in the pro-posed learning framework to guide satisfaction modeling.
Our research contributions can be summarized as follows:
Recent advances in retrieval evaluation have focused on modeling search behaviors and exploiting implicit feedback [2, 19]. Qualitative studies showed that users X  search be-haviors are good indicators of retrieval system performance [27] and search-task difficulty [3]. Smith and Kantor found that users adapted their search behaviors to the deliberately degraded retrieval systems, e.g., increase the rate of query entry and decrease the occurrence of repeated queries [27]. Aula et al. reported that when facing with difficult search tasks, users tended to use more diverse queries and more advanced operations, and spend longer time on the search result pages [3]. Such studies shed lights on the potential of evaluating search performance via searchers X  behaviors.
Satisfaction has been studied extensively in a number of areas such as psychology [24] and commerce [26]. In the IR literature, search satisfaction is generally defined as the ful-fillment of a user X  X  information need [12, 15]. Fox et al. [12] used an instrumented browser to collect search activities and compared them against explicit user satisfaction judgments of full search sessions. They identified a strong association between users X  search patterns and their explicit satisfaction ratings. Hassan et al. [15, 17] utilized a user X  X  search action sequence to predict search satisfaction. Feild et al. [11] fo-cused on the behavioral clues to detect search frustration, where various signals from query logs and physiological sen-sors were explored. In [21], Kim et al. introduced more sophisticated signals to calibrate click dwell time for better estimating click satisfaction.

Despite the wealth of research in this area, most prior studies regard search-tasks as the basic modeling unit, from which holistic measures, e.g, total dwell-time [31] and query-click ratio [11], are extracted for predicting search satisfac-tion. However, users X  detailed action-level satisfaction was largely ignored in prior work, though it conveys important information about searchers X  overall search satisfaction [3, 11]: searchers can be ultimately satisfied by the search task, but most of their search actions might be quite unsatisfy-ing. Thus, methods that fail to consider satisfaction at the action-level may not be optimal for this prediction problem. To the best of our knowledge, Ageev et al. X  X  work in [1] was the first attempt to consider users X  action-level satisfaction for search-task satisfaction prediction. In their work, a con-trolled lab experiment is performed to track users X  search ac-tivities during predefined search tasks. They approximated users X  action-level satisfaction by using manual relevance judgments, and they identified distinct search paths among the satisfying/unsatisfying actions in the satisfying versus unsatisfying search tasks. Their study confirms our claim that it is necessary to distinguish and model users X  action-level satisfaction in search-task satisfaction prediction. As their solution, a CRF model was adopted to predict search-task satisfaction based on a set of behavior features. How-ever, because they asserted that all action labels equaled the task label, discrimination between different search actions was not possible. Therefore, their method is still unable to distinguish action-level satisfaction, as we do in this paper.
In this section, we formally define the problem of search-task satisfaction prediction. A search task is defined to be an atomic information need, which results in a series of search actions [20]. Various methods have been proposed to ex-tract search tasks from users X  search logs [5, 29], and we will assume such segmentation is given a priori in our problem.
Specifically, the input of a search-task satisfaction pre-diction problem is a sequence of user u  X  X  search actions in a particular search task t u , in which the actions are chronolog-ically ordered, i.e., A t u = { a t u 1 , . . . , a t u n tion type definition in [15], and consider the following types of search actions in this work: Each action a t u i has an attribute a t u i .ref pointing to the previous action which leads to the current action.
The above action types cover most of the search actions a user typically performs during Web search. Additionally, to be consistent with our later description of the proposed method, we add two dummy actions into every search task, i.e., a t u 0 = START and a t u n +1 = END, indicating the start and end of a search task respectively. In particular, we de-note Q = { Q , SERP , PAGN , RL , SP } as query-related ac-tions, and C = { SR , BR } as click-related actions.
The output of a search-task satisfaction prediction prob-lem is an overall satisfaction label y t u indicating whether the user u has been satisfied in the search task t u . In this work, we follow Aula et al. X  X  criterion [3] to define search-task satisfaction as, Definition (Search-Task Satisfaction) Given a user u  X  X  search task t u , search-task satisfaction is a binary label y y thus resulting a satisfying search task; otherwise y t u = 0.
In literature, there are different terms, e.g.,  X  X earch suc-cess X  [1, 15] and  X  X rustration X  [11], and perspectives, e.g., subjective [3, 15] or objective [8], used for defining a similar concept. We want to emphasize that our definition charac-terizes search satisfaction from a user X  X  subjective perspec-tive: a search-task is considered as satisfying, if, and only if, the searcher is satisfied with the search results and believes that they has found the answer (but the answer could be factually incorrect).

As a result, the problem of search-task satisfaction pre-diction is to estimate a function f (  X  ) from the given search such that the predicted satisfaction label agrees with users X  belief on whether they have satisfied their information need.
Most of the previous approaches for search-task satisfac-tion prediction [1, 11, 12, 14, 15] fall into the above for-malism. However, one important factor that has not yet been explicitly defined and explored in prior works is user u  X  X  satisfaction label h t u i related to a specific action a Intuitively, h t u i characterizes the contribution of action a towards user u  X  X  overall satisfaction of task t u . Formally, we define a user X  X  action-level satisfaction as, De finition (Action-level Satisfaction) Action-level sat-isfaction h t u i is a binary outcome of a search action a task t u , such that h t u i = 1, if user u is satisfied with action a i , e.g., found helpful information after clicking on a docu-ment; otherwise h t u i = 0, e.g., a query action does not lead to any useful document.

It is worthwhile to note that despite defining h t u i as binary in the above definition, the potential label space for the vari-able h t u i is quite flexible, e.g., encoding it with multi-level ordinal labels to reflect users X  complex information seeking behaviors (e.g., query refinement [3], exploring related infor-mation [30]). Our proposed method can be easily extended to the multi-label setting. In this work, we will follow this binary definition for simplicity and explicability.
In this section, we describe the proposed latent structural model for search-task satisfaction prediction. We start with a real search task example to illustrate the necessity of mod-eling searchers X  action-level satisfaction. Then we discuss our hypothesis about users X  search behaviors with respect to action-level satisfaction. And based on it, rich structured features and dependency relations unique to search-task sat-isfaction modeling are devised. In the end, we discuss how to incorporate domain-knowledge to guide the proposed model in learning the latent structures effectively.
Table 1 presents a real example of a satisfying search task extracted from Ageev et al. X  X  public search data set [1]. We applied several state-of-the-art search satisfaction mod-els, including the Markov Model Likelihood (MML) method [15], logistic regression (LogiReg) model [11] and session-CRF model [1], and our proposed method on this case. In particular, the MML and LogiReg take a holistic view to directly predict the task-level satisfaction, while the session-CRF and our method consider action-level satisfaction in the task. Due to space limitations, we only showed the domain of clicked documents in the table. The action-level predic-tion results from session-CRF model (denoted as  X  X RF X ) and our method (denoted as  X  X urs X ) are illustrated in the last two columns of the table.

In this example, the searcher sought information on metals that can float on water. She rated this task as satisfying because she claimed the answer had been found after search. But it does not mean that she was satisfied with every action in the task. As we can observe, she first attempted three queries on Google, but was not satisfied with the search results: she kept reformulating the queries, spent a very short time on the clicked documents, and switched to Bing with the same query. After spending quite some time on Bing X  X  search result page, she issued a very specific query to Google and reached the correct answer (the answer was verified by a human editor).

Models based on task-level implicit measures, i.e., MML and LogiReg, mistakenly predicted that the searcher was unsatisfied with the task: dwell times on the clicked docu-ments were generally short, along with a number of query reformulations and search engine switches. Due to the re-strictive assumption in Ageev et al. X  X  session-CRF model, i.e., all actions have to be satisfying in a satisfying task, it made a wrong prediction for this task as well, since most ac-tions were unsatisfying. But once we consider the searcher X  X  action-level satisfaction, as predicted in our method X  X  out-put, we could reach the correct conclusion that the task is Table 1: Example of a satisfying search task.  X + X / X - X  indicates a predicted satisfying/unsatisfying action. Q: metals float on water Google 10s --SR: wiki.answers.com 2s --BR: blog.sciseek.com 3s --Q: which metals float on water Google 31s --Q: metals floating on water Google 16s --SR: www.blurtit.com 5s --Q: metals floating on water Bing 53s --
Q: lithium sodium potassium float on water
SR: www.docbrown.info 15s -+ sa tisfying. From this example, we can clearly realize the im-portance of recognizing a user X  X  fine-grained satisfaction at action level for search-task satisfaction prediction.
As was discussed in our motivating example in Table 1, the action-level satisfaction labels H 1 convey informative clues about overall search-task satisfaction. If H is known, sophis-ticated features about users X  perceived satisfaction of search activities can be extracted, e.g., examining if the task ends with a satisfying action or measuring the ratio of time spent on satisfying actions versus unsatisfying ones, for better pre-dicting the overall task satisfaction label y . Unfortunately, H is hidden in search log data; and it is also quite chal-lenging to be manually annotated at scale. This prevents previous works from directly utilizing such information for search-task satisfaction prediction.

To address this challenge, we devise a basic hypothesis about users X  search behaviors: Hypothesis. The desire for satisfaction drives users' inter-action with search engines and that the satisfaction attained during the search-task contributes to the overall satisfaction.
This hypothesis makes two assumptions. First, users X  overall search-task satisfaction depends on their satisfaction with the performed search actions, e.g., if all actions were satisfying, it is very likely that the user would end up with a satisfying search task. Second, users X  search actions are mutually dependent via the latent action satisfaction labels. For example, if a query is unsatisfying, e.g., it is later reis-sued to another search engine [13], the result clicks in the first search engine X  X  result page can hardly be satisfying.
We consider H as latent variables and realize our hypoth-esis about a user X  X  search behaviors in a structured predic-tion model. We name the proposed method as Action-aware Task Satisfaction (AcTS) model, and describe the structural dependencies imposed in the AcTS model in Figure 1.
To formally encode the dependency assumptions in our hypothesis, we define a feature vector for the task satis-faction label y specified by the search action sequence A and corresponding hidden action satisfaction labels H as  X ( A, H, y ). Based on this feature representation, AcTS pre-dicts the search-task satisfaction at testing time by, In Eq (1), Y and H represent the sets of all possible values of y and configurations of H respectively. w is the parameter vector in our AcTS model, and it reflects the relative im-portance of features in predicting search-task satisfaction.
Wh en no ambiguity is invoked, we will discard the user index u and task index t u to simplify the notations. Fi gure 1: Structural dependency assumptions about a user X  X  search behaviors postulated in AcTS model. Light circles represent latent variables and shadow circles represent observable variables. Lines indicate possible dependencies between the variables (the de-pendency between y and a is not shown to make the representation concise). In AcTS, a joint mapping of f ( A )  X  H  X  y is estimated.
 In this paper, we refer to solving Eq (1) as the inference problem. In the solution of our inference problem,  X  y be-comes the output for the task-level satisfaction prediction and  X  H is the inferred action-level satisfaction labels for the input search action sequence.

The inference problem of Eq (1) clearly distinguishes the proposed AcTS model from all the prior search satisfaction models. In order to make a prediction of the overall search satisfaction label y , we need to determine the latent action satisfaction labels H as well, which are mostly consistent with the observations in the input search actions A and sup-port the predicted overall satisfaction label y in task t . For-mally, we are estimating a joint mapping from input search action sequence A to task satisfaction label y and latent ac-tion satisfaction labels H , i.e., f ( A )  X  H  X  y ; while most prior works only estimate a binary mapping of f ( A )  X  y . Moreover, in the proposed AcTS model, a user X  X  search ac-tions A are no longer treated as independent, but instead, they are modeled as being correlated with each other via the latent action satisfaction labels H . Expressive features about a user X  X  search behaviors can thus be designed, such as measuring the transition between a user X  X  satisfying and unsatisfying search actions and examining whether a user is satisfied with all the query actions.

More importantly, the inferred action-level satisfaction la-bels H not only provide informative signals for determining overall search satisfaction, but also reveal the utility of those actions towards a user X  X  information need. For example, based on the identified labels in H , we can easily recog-nize which clicked document is helpful in satisfying a user X  X  information need, and which query leads to the helpful doc-uments. The estimated utilities are beneficial for a variety of search applications, e.g., document relevance estimation and query suggestion. Nevertheless, such information is not available in any of the existing search satisfaction models.
In the following, we will discuss in detail about our design of the structured features  X ( A, H, y ) in Section 4.3, and the use of domain knowledge for learning the optimal feature weights w in Section 4.4.
Previous work has developed a wide variety of behavioral features for search satisfaction prediction [1, 11, 12, 13, 31]. All of those features can be flexibly applied in our AcTS model. However, since most prior research only estimates a holistic mapping of f ( A )  X  y , their employed features (e.g., total dwell time [31] and number of result clicks [12]) cannot capture a user X  X  action-level satisfaction. In this section, we focus on the newly developed structured features for AcTS, in which expressive signals about the dependency among search actions A , action-level satisfaction labels H and task-level satisfaction label y is explicitly explored via the latent variables. The devised features can be categorized into two classes: short-range features (specifying satisfaction label for a single action in task t ) and long-range features (specifying satisfaction labels for a set of actions in task t ).  X  Short-range features: As shown in Figure 1, in our AcTS model, the features extracted from action a i are di-rectly used to predict the corresponding satisfaction label h and overall task satisfaction label y (i.e., f ( A )  X  H  X  This is distinct from the features explored in most existing search satisfaction models, where the action-level observa-tions are aggregated to determine the task-level satisfaction label y [11, 12, 13].

In a user X  X  query-related actions, although not especially common, search engine switching (i.e., the voluntary tran-sition between different search engines) usually indicates searcher frustration [13]. We encode this as  X  switch ( y, A, h =  X  ( y, a i , h i )  X  ( a i .Engine  X  = a j .Engine ), where a next query action following the current query action a i and  X  (  X  ) is the indicator function. Similarly, query reformulation also indicates the user is not satisfied with the search results of the current query [3]. We formalize this by measuring the similarity between two consecutive queries:  X  reform ( y, A, h =  X  ( y, a i , h i ) sim ( a i .Query, a j .Query ), where a action following the current query action a i , and sim ( X, Y ) is the edit distance between query string X and Y . Be-sides, we also examine if the query is in a question form by  X  question ( y, A, h i ) and calculate the proportion of stopwords in the query by  X  stopword ( y, A, h i ) to estimate satisfaction for the query-related actions.

Among a user X  X  click-related actions, the relevance qual-ity of a clicked document to the given query can be an important criterion to measure user satisfaction [18]. Be-cause we do not assume the availability of document con-tent in our problem (it is usually unavailable in search log data), we can only measure relevance of the clicked docu-ments according to their URL strings. In particular, we de-fine  X  rel ( y, A, h i ) =  X  ( y, a i , h i ) c ( a i .U RL, a c ( U RL, Query ) counts the number of query terms occurred in the URL string, and a k is the query action that leads to the current click action a i . In addition, the original rank position of the clicked URL in search-result page is also a good indicator of its relevance quality [18]. We encode it as  X  pos ( y, a i , h i ) =  X  ( y, a i , h i ) a i .P os .
Besides, previous studies have demonstrated that a user X  X  last search action is closely related to her search-task sat-isfaction [12]. We encode this as  X  last ( y, h n ) =  X  ( y = h i.e., examine whether the satisfaction label of the user X  X  last action agrees with her overall task satisfaction.  X 
Long-range features: We devise the first order tran-sequential search behaviors with respect to the latent ac-tion satisfaction labels. For example, in a satisfying search task, an unsatisfying query is more likely to be reformu-lated into a satisfying query rather than another unsatisfying one. In particular, we define  X  trans ( y, h i , h i +1 , a  X  ( y = y  X  , h i = h  X  , h i +1 = h  X  X  , a i = a  X  , a i +1 ( y faction label, action satisfaction labels and action types. We should note that our transition features are different from those introduced in [14, 15, 17]: in those works, only the transitions between different action types are modeled, e.g., from Q to SR; while in our model, we distinguish search ac-ti on transitions with respect to the latent action satisfaction labels, e.g., from satisfying Q to satisfying SR.

Beyond exploring the behaviorial patterns within adja-cent search actions, a set of features are introduced to cap-ture dependency at the whole task level by examing: I. if all the query-related actions are satisfying:  X  allQ ( y, H, A ) =  X  (  X  action:  X  existQ ( y, H, A ) =  X  ( click-related actions are satisfying:  X  allC ( y, H, A ) =  X  ( h  X  existC ( y, H, A ) =  X  (
We need to emphasize that the above long-range features can only be exploited by our AcTS model, since it explicitly models the users X  action-level satisfaction across different actions in a search task. None of existing methods can utilize such information for search-task satisfaction prediction.
In addition to the above newly introduced structured fea-tures, we also included the action-level and task-level be-havior features from [1] and [12] in our AcTS model, such as action dwell time and query-click ratio. The list of features used in this work appears in Table 2.
Because the ground-truth labels for a user X  X  action-level satisfaction are unobservable in the search log data, we have no direct supervision to guide the model in learning about such latent structures. Fortunately, there is plenty of work in cognitive science and information science exploring users X  search behaviors and strategies in performing a successful search task [3, 25, 30]. Such studies shed light on the insights of users X  detailed in-task search behavior patterns. In this section, we propose the use of structured loss functions [7] to inject such domain knowledge as weak supervision for AcTS training (i.e., learning the weight vector w in Eq (1)).
To regularize the training of AcTS model with domain knowledge, we derive our learning algorithm for the AcTS model from the latent structural SVMs framework [7]. For a given set of search tasks with only task-level search sat-isfaction labels, i.e., { ( A m , y m ) } M m =1 , AcTS model training can be formalized as the following optimization problem: min Deta ils of features from [1, 12] are not listed in the table.
In Eq (2),  X ( y m ,  X  y,  X  H, A ) measures the distance between the predicted labels ( X  y,  X  H ) and the ground-truth ( y where H  X  m is the unobservable ground-truth of action-level satisfaction labels. {  X  m } M m =1 is a set of slack variables to allow errors in the training data, and C controls the trade-off between empirical training loss and model complexity.  X ( y m ,  X  y,  X  H, A m ) indicates the prediction error between ( X  y,  X  H ) and ( y m , H  X  m ); and thus it drives model learning. As H m is unknown in the training data, we have no supervision to guide the AcTS model in learning about such latent struc-tures. As our solution, weak supervision about users X  search behaviors is injected via the design of  X ( y m ,  X  y,  X  H, A tuitively, we should increase  X ( y m ,  X  y,  X  H, A m ), i.e., penalize the prediction, when the inferred  X  H contradicts our knowl-edge about a legitimate configuration of H . In this work, we define a set of structured loss functions  X  ( X  y,  X  H, A ) to realize the knowledge about  X  H in  X ( y m ,  X  y,  X  H, A m ) from different perspectives.

First, a good configuration of  X  H has to be consistent with the predicted overall search-task satisfaction label  X  y . We measure this by: i.e., all the actions should not be unsatisfying in a satisfying task. And, vice versa, Second, the configuration of  X  H itself should be consistent. For example, an unsatisfying query cannot result in any sat-isfying search-result clicks [1], i.e.,  X  And when the user performs duplicated actions in the same task, e.g., submit the same query twice to the same search engine, their inferred satisfaction labels should be the same, The suggested query from a search engine X  X  spelling correc-tion, e.g., correcting the misspelt query  X  X mazone X  into its correct form  X  X mazon, X  should not hurt user satisfaction,  X  sp (  X  H, A )= Ba sed on the above estimated distance between  X  H and H m , we can define the margin in Eq (2) as,  X ( y m ,  X  y,  X  H, A m ) =  X  ( y m =  X  y ) + where  X  i is a trade-off parameter between task-level 0/1 loss and action-level loss defined by the structured loss functions  X  ( X  y,  X  H, A ) as described in Eq (3) to Eq (7).
The margin function defined above encodes the knowledge about a user X  X  latent action-level satisfaction labels within a search task as weak supervision for latent structure learn-ing [7]. It bridges the qualitative studies of users X  search behaviors [3, 25, 30] and quantitative modeling approaches. We should note that the structured loss functions  X  ( X  y, might be violated in a particular user X  X  real search actions, and  X  i controls our confidence of such loss functions.
The optimization problem in Eq (2) can be efficiently solved by the iterative algorithm proposed in [7]. One thing we should note is that due to the long-range dependency in-troduced by the structured features proposed in Section 4.3, e.g.,  X  allQ ( y, H, A ) and  X  existQ ( y, H, A ), the inference prob-lems defined in Eq (1) and Eq (2) become computationally intractable. We address these inference problems via inte-ger linear programming (ILP), and more details about this inference method can be found in [22].
In this section, we first quantitatively evaluate the effec-tiveness of the proposed AcTS model that models users X  action-level satisfaction as latent variables, whereby several state-of-the-art search satisfaction models are compared over the in-situ task satisfaction labels from previous studies [1, 16]. Then we assess the quality of the inferred action-level satisfaction labels via their utilities in facilitating other in-formation retrieval studies, where understanding users X  de-tailed action-level satisfaction is important.
Hassan et al. [16] developed a toolbar plugin for the Inter-net Explorer browser to collect search activities and explicit search satisfaction ratings from the searchers. The authors explicitly asked the searchers to rate their search tasks im-mediately upon termination. This data set provides reliable first-hand annotation of search-task satisfaction. We refer to this data set as  X  X oolbar data X  in our experiments.
Ageev et al. [1] designed a game-like online contest for crowdsourcing search behavior studies. In their study, users were required to perform several predefined informational tasks via a Web search interface and submit the answers they found to the system. All users X  search behaviors were logged and annotated by the authors. According to our search-task satisfaction definition described in Section 3, we treat the tasks in which the user has submitted an answer as satisfying (the answer might be incorrect with respect to the predefined information need). We refer to this data set as  X  X ontest data X  in our experiments.

To investigate the utility of the proposed method in pre-dicting search satisfaction in real-world search engine logs, we extracted large-scale query logs sampled from the Mi-crosoft Bing Web search engine. In a four-month period, from December 2012 to March 2013, a subset of users were randomly selected. The search logs recorded their search activities, including the anonymized user ID, query string, timestamp, returned URL sets and the corresponding user clicks. These logs were segmented into search tasks by the method developed in [29]. This data set does not contain Table 3: Basic statistics of evaluation data sets. search log 2.4M 7.6M 7.1(  X  11.8) -ta sk-level nor action-level satisfaction labels. We refer to it as  X  X earch log data, X  and describe its usage in Section 5.3.
Basic statistics of these data sets appear in Table 3.
To investigate the effectiveness of modeling users X  action-level satisfaction as latent variables in AcTS model, we first quantitatively compare the performance of the proposed model with several state-of-the-art methods in predicting overall search-task satisfaction.
Several methods have been proposed to predict search sat-isfaction based on users X  search behaviors [1, 11, 14, 15]. We adopt several best-performing models from the previous works as our baseline methods.

Hassan et al. [15] proposed a Markov Model Likelihood (MML) method to predict search satisfaction. In MML, two sets of first order transition probabilities are estimated from the search action trails in satisfying and unsatisfying tasks. At testing time, MML calculates the likelihood ratio of an input search action sequence between the satisfying and unsatisfying models to determine the task satisfaction label. We followed the specification of MML in [14] to imple-ment the model (they used the same set of action types as ours). Maximum a Posteriori estimator with Dirichlet pri-ors is used to estimate the transition probabilities in MML. To model click dwell time in MML, we add two new ac-tion types, SR l ong and BR l ong , which represent the click actions ( SR and BR ) with dwell time longer than 30s.
Feild et al. [11] used a logistic regression model to predict search frustration, where features extracted from both query logs and physiological sensors are employed. We built a logistic regression model based on the features described in Section 4.3. The short-range features are aggregated in each task by action type, e.g., average the click position features  X  pos ( y, a i , h i ) over all SR actions in the same task. The long-range features, e.g.,  X  t allQ ( y, H, A ), are not included, since logistic regression cannot handle latent variables. We refer to this method as  X  X ogiReg. X 
Ageev et al. proposed a session-CRF model [1] to predict search-task satisfaction. Although search actions were ex-plicitly modeled, they asserted that action-level satisfaction labels equaled to the task-level label. Mathematically, this assumption makes their session-CRF degenerate to a logis-tic regression model. This obscures the complex dependency between task satisfaction and detailed action satisfactions in session-CRF. As a result, it cannot as effectively model the action-level user satisfaction as our model does. We adopted the same implementation of session-CRF as used in [1].
As illustrated in Table 3, the distribution of task satisfac-tion labels in both toolbar and contest data are highly unbal-anced: about 85% of the tasks are labeled as satisfying. In such an unbalanced data set, accuracy alone is inadequate to compare the performance of different methods. In our eval-uation, we compute the f 1 scores for both satisfying ( T and unsatisfying tasks ( T  X  -f 1 ). Following the metric used T able 4: Search task success prediction performance on the toolbar data set. session-CRF 0.728 0.910 0.545 0.850 Table 5: Search task success prediction performance on the contest data set. labeled-AcTS 0.649 0.945 0.352 0.899 in [1], we also report the average f 1 between T + -f 1 and T f . In order to avoid bias introduced by training/testing split, we performed five-fold cross-validation in each method by sampling tasks into different folds, and repeated it three times with different random seeds. As a result, we report the average performance of all methods from 15 different trials on the toolbar and contest data sets in Table 4 and Table 5. Paired two sample t-test is performed to validate the statis-tical significance of the improvement from the AcTS model against the best-performing baseline, LogiReg, under each of the performance metrics. In particular, we set the trade-off parameters  X  i to one in Eq (8) for AcTS model in all our experiments.

We can clearly observe the significant improvement from the proposed AcTS model over all baselines in both data sets. MML, which only models the sequential patterns in a user X  X  search actions, performed the worst among all the methods. This indicates that a user X  X  sequential search be-haviors alone are insufficient to capture the overall search satisfaction. session-CRF behaved similarly as LogiReg. Al-though action-level labels are explicitly modeled in session-CRF, its restrictive assumption about the labels degrades the model X  X  capability in distinguishing the action-level sat-isfaction labels, e.g., unsatisfying actions will not be allowed in a satisfying task in session-CRF. We accredit the en-couraging performance improvement of the proposed AcTS model to its unique capability of modeling the action satis-faction labels as latent variables. By explicitly modeling a user X  X  action-level satisfaction, AcTS can naturally include all the signals used in the baseline methods and explore richer structured information, as specified in our long-range features, which cannot be handled in any baseline method.
Beside exploring more expressive structured features for search-task satisfaction prediction, another unique advan-tage of modeling the action-level satisfaction labels as latent variables in AcTS is to incorporate domain-knowledge for model training via the structured loss functions. To investi-gate this aspect, we test a special setting of AcTS, in which we set the trade-off parameters  X  i to zero in Eq (9). As a result, we are training the AcTS model with only task-level supervision. We name this model as AcTS 0 and include its performance on both data sets in Table 4 and Table 5.
Without the structured loss functions, AcTS X  X  performance dropped significantly: it performed similarly as the LogiReg Fi gure 2: Case study of two manually annotated search sequences in the contest data set. Red labels on top of each action are the editor X  X  annotations from [1], and green labels at the bottom are AcTS X  X  predicted labels. T + and T  X  indicate the task satis-faction labels provided by the users. and session-CRF baselines. The reason is that the task-level satisfaction label alone cannot guide the model in learning the latent structures of H . As a result, the inferred labels of H in AcTS 0 becomes arbitrary, and provides little help in predicting task satisfaction. This result confirms the need to explicitly model the dependency between action-level and task-level satisfaction in search satisfaction modeling.
In addition, since action-level manual annotations are avail-able in the contest data set, we can treat those labels as  X  X round-truth X  X ction satisfaction labels, and train our AcTS model with a known structure. To incorporate these labels into AcTS training, we define a new margin for Eq (2) as, i.e., we are computing the Hamming distance between two labeled search sequences. We name this new model as labeled-AcTS, and list its performance in Table 5.

Surprisingly, the labeled-AcTS model did not outperform the original AcTS model with latent structures; and it per-formed significantly worse when predicting the unsatisfy-ing tasks. To analyze the degraded performance, we exam-ined the annotated search tasks in this data set and found many disagreements between the editor X  X  judgements and searchers X  actual behaviors. The discrepancy mainly stems from the inconsistent criteria between third-party editors and real users; and to understand it, we illustrate two typi-cal inconsistent search sequences in Figure 2.

In Figure 2(a), all the clicked documents are judged to be irrelevant for this task. However the searcher still rated the task as satisfying. A reasonable explanation is that the searcher believed that she had found the correct answer, so was satisfied. While in Figure 2(b), according to the edi-tor X  X  judgment, the searcher has already issued several good queries and found the relevant documents for the question. However, the user rated it as unsatisfying in the end. The reason might be that the user did not notice the relevant passage(s) in the clicked documents, so was not satisfied with all of her search actions. The main reason for such dis-agreements is the annotation criterion devised in this data set [1]: Ageev et al. labeled a URL as a good URL if it con-tains the correct answer to the predefined question in the search task; and a query is judged to be good if it leads to a good URL in its search result page. Nevertheless, from a real searcher X  X  perspective, because she may not have any knowledge about the questions beforehand, she cannot fully judge the helpfulness of search results in an objective way. As a result, the editor X  X  objective judgments in this data set cannot precisely reflect a user X  X  perceived satisfaction dur-ing search, which, however, is the goal of prediction in our task satisfaction prediction problem. Such discrepancy ex-plains the degraded performance of labeled-AcTS: in Eq (9), we overly penalized the predictions in model training due to the inappropriate manual annotations.
Mea nwhile, as shown in Figure 2, the inferred action la-bels from our AcTS model better aligned with the final task satisfaction labels in both cases: in Figure 2(a), the last query and last click action are predicted as satisfying; and in Figure 2(b), most of the actions are predicted as unsat-isfying. Those inferred labels are more consistent with our above hypothetical analysis of users X  search behaviors.
As an output of our structured inference problem defined in Eq (1), the inferred action-level satisfaction labels H have shown their ability in helping to predict the overall search-task satisfaction. Nevertheless, because the ground-truth labels of such output are not available in our evaluation data sets, we cannot directly evaluate the quality of the predicted action-level labels. In this section, we assess the quality of the inferred action satisfaction labels via their utilities in facilitating other information retrieval applications, where understanding users X  action-level satisfaction is important.
Accurately interpreting users X  clickthroughs and extract-ing relevance signals for search engine optimization is an important topic in IR studies [2, 9]. The action-level satis-faction labels from AcTS can serve as a proxy to estimate the utility of clicked documents. In this section, we evalu-ate how the estimated document relevance can be used to improve the training of general learning-to-rank algorithms.
We chose LambdaMART [6] as our base learning-to-rank algorithm, and evaluate its ranking performance improve-ment by adding the features derived from AcTS model X  X  output. A large set of manually annotated query-URL pairs are collected to create the evaluation data set. In this anno-tation set, each query-URL pair is labeled with a five-point relevance score, i.e., from 0 for  X  X ad X  to 4 for  X  X erfect. X  And each pair is represented by a set of 398 standard ranking features, e.g., BM25, language model score and PageRank. We refer to this collection as the  X  X nnotation set. X 
In this experiment, we train an AcTS model based on all the search tasks in the toolbar data, and apply the learned model on the four-month search log data. We group the in-ferred satisfaction labels under each unique query-URL pair, and calculate the corresponding median, mean and stan-dard deviation as the additional relevance features derived from AcTS. To reduce noise in this estimation, we ignore the query-URL pairs occurred less than five times in this corpus. In the end, we joined the query-URL pairs extracted from the search log data with those in the annotation set, and obtained 3,311 annotated queries and 128,120 query-URL pairs with additional features derived from AcTS.

The same feature generation strategy is applied to the session-CRF X  X  output. However, the MML and LogiReg baselines are not directly applicable in this evaluation, since they cannot make predictions of individual search actions. To compare with them, we followed Hassan et al. X  X  method [16] to estimate document utility based on the predicted overall task satisfaction labels. In their method, the utility of a clicked document is assumed to be proportional to its dwell time. To distinguish document utilities between satis-fying and unsatisfying tasks, they separated such scores into  X  X tility X  (for satisfying tasks) and  X  X espair X  (for unsatisfying tasks), which were used as two different relevance features. To make their relevance feature representation consistent with that from our AcTS model, i.e., one utility score per query-URL pair, we unified X  X tility X  X nd X  X espair X  X y simply treating  X  X espair X  as negative  X  X tility. X  As a result, we can Table 6: Ranking performance improvements of LambdaMART with additional document relevance features estimated from different methods. session-CRF +4.752 +3.402 +3.896 +2.616 apply the same aggregation strategy over all the query-URL pairs based on MML X  X  and LogiReg X  X  output to generate new relevance features from those methods.

In addition, we also include a session-based click model, i.e., Session Utility Model (SUM) [9], as a baseline in this experiment. SUM aims to extract the intrinsic relevance of documents to the given query from users X  click behaviors in search sessions (tasks). However, it assumes all the search tasks are satisfying when modeling clicks. Thus, it is nec-essary to investigate if modeling search-task satisfaction is needed for estimating document relevance from user clicks. In our experiment, we fixed the total number of trees in LambdaMART to 100, each of which has 15 nodes. The learning rate was set to 0.1. Five-fold cross-validation was used, where we used one fold of data for testing, one fold for validation and the reminder for training. We computed four standard IR evaluation metrics. By treating all the labels above X  X air X  X s relevant, we calculated P@1, MAP and MRR. NDCG@5 was also computed based on the five-point rele-vance scale. The improvements of LambdaMART X  X  rank-ing performance with different additional relevance features against the original features are listed in Table 6.
The new relevance features from AcTS significantly im-proved LambdaMART X  X  performance against the original features under all the metrics ( p -value &lt; 0.01). We exam-ined the learned tree models in LambdaMART and found all the features generated by AcTS model are selected as important splitting factors (i.e., among the top 10 impor-tant features). The features from AcTS also significantly improved the MAP and NDCG@5 metrics ( p -value &lt; 0.05) comparing to those from MML and SUM, which are the second best methods under these two metrics respectively. Since no baseline search satisfaction models can distinguish the fine-grained action-level satisfaction, their estimated rel-evance features are not as accurate as those from the inferred action-level labels of our AcTS model. Comparing to SUM, although it distinguishes the utility of different clicked doc-uments, it does not consider overall task satisfaction when modeling user clicks, and thus the relevance features from AcTS led to better improved ranking performance. This re-sult validates the need to distinguish overall task satisfaction in modeling clicks for document relevance estimation.
Search tasks provide rich context for performing log-based query suggestion [10, 23]. Liao et al. [23] reported that the Log Likelihood Ratio (LLR) based query similarity metric achieved the best performance in their task-based query sug-gestion experiment. In this section, we investigate how the identified action-level user satisfaction labels can be used to further improve LLR in task-based query suggestion.
Given two queries q a and q b , assuming q b is issued after q , LLR makes the null hypothesis H 0 as: P ( q b | q a ) = p P ( q b | X  q a ), i.e., q a and q b are independent; and the alterna-ti ve hypothesis H 1 as: P ( q b | q a ) = p 1  X  = p 2 = P ( q q a and q b are dependent. Likelihood ratio test is used, in which the test statistic is defined as  X  2 ln max p 0 L ( H determine the dependency between q a and q b . If the value of test statistic is larger than a predefined threshold, the null hypothesis is rejected, i.e., q a and q b are determined to be dependent, and q b will be selected as a suggestion for q
In Liao et al. X  X  work, the probabilities of p 1 , p 2 were esti-mated by the occurrences of consecutive queries in the same task, without considering the quality of query reformula-tions. For example, if a satisfying query q a is frequently re-formulated into an unsatisfying query q b , even though they are strongly correlated according to LLR, we should not sug-gest q b for q a to users. To take the inferred action-level sat-isfaction into account, we weight the consecutive query pairs according to their inferred satisfaction labels by, i.e., we emphasize the pair of queries in LLR calculation, where the follow-up query improved user satisfaction; and downgrade the reformulations that hurt user satisfaction. Based on this weighting scheme, the same LLR test statistics are computed for measuring correlation between queries.
The LLR test statistics for all consecutive query pairs in the identified search tasks are computed based on the first three-month search logs. The same threshold, 100 as used in [23], is applied to filter the suggestion candidates. The fourth-month search logs are used as the testing set to ex-amine whether the suggested queries will be issued by users after the target query [28]. Such evaluation measures utility of the suggested queries in real usage context. In particular, the next three consecutive queries following the target query in the same search task are regarded as relevant in com-puting the evaluation metrics of P@3, MAP and MRR. To make the evaluation results comparable between the baseline (LLR without query weighting) and our method (LLR with query weighting), we only evaluated the overlapped target queries in both methods.

Beside this automatic evaluation method, we also col-lected a set of manual annotations to assess the quality of the suggested queries. We ordered all the target queries from the first three-month search logs according to their frequency, and treated the first third of queries as high fre-quency queries, the second third as medium, and the rest as low. 100 queries were randomly selected from each category. For each selected target query, the top five suggestions from both methods were selected and interleaved before being pre-sented to the annotators, in order to reduce annotation bias. Six human annotators were recruited to label the suggestion results. They were instructed to judge if the suggestions are relevant to the given target query with binary labels. An-notators were separated into two groups, each of which was required to annotate 150 target queries selected evenly from the above three categories. The final relevance judgment was obtained by majority vote. We list the improvements of the LLR-based query suggestion performance from the new query weighting scheme on these two testing sets in Table 7.
As shown in the results, the new query weighting scheme greatly improved the original co-occurrence based query sug-gestion performance. In the log-based automatic evaluation, all the performance metrics are significantly improved. Ac-cording to the manual judgments, the major improvement is derived from the low frequency queries: P@3 and MAP are improved by 14.8% and 15.1% accordingly ( p -value &lt; 0.01). In Liao et al. X  X  reported result, their task-based LLR per-Table 7: Query suggestion performance improve-ments with query reformulation quality estimated by AcTS. formed poorly on this category, which they attributed to the sparsity of query co-occurrence. Therefore, we can find clear benefit of distinguishing the quality of the reformu-lated queries when performing query suggestion for those low frequency queries. And the inferred action satisfaction labels from AcTS provide such a reliable quality estimator for further improving the query suggestion performance.
Analyzing users X  search behavior patterns is important, since it helps us understand how people use search engines to solve search problems. Ageev et al. [1] analyzed search paths in different types of users and tasks, and identified distinct users X  in-task behavioral patterns. In particular, they approximated the search paths based on the manually annotated search actions. However, such manual judgments are not generally available and expensive to acquire at scale. In contrast, our model is capable of performing such analysis of user search activities without manual annotations .
We performed this analysis on the toolbar data, where we do not have action-level annotations. The first-order transi-tion probabilities between different action types with respect to the inferred action satisfaction labels are estimated. In Figure 3, we demonstrated two subgraphs of the identified search paths in satisfying and unsatisfying tasks. We ignored the edges with transition probability less than 0.05 and used bold font to highlight the outgoing edges with the highest transition probability from each node in the figure. Since we only showed a sub-graph from the original graph, the illustrated outgoing transition probabilities of some nodes may not sum up to one (e.g., p (SERP  X  | SR  X  ) = 0 . 558 is not included in Figure 3(b)).

According to the search paths estimated from the inferred action satisfaction labels, users exhibit quite different behav-ior patterns in the satisfying and unsatisfying search tasks. In a satisfying task (as shown in Figure 3(a)), users usually start with a satisfying query ( p (Q + | START)=0.854), which will very likely result in a satisfying click ( p (SR + | while in an unsatisfying task (as shown in Figure 3(b)), users are more likely to begin with an unsatisfying query ( p (Q  X  | START)=0.544), and move to some unhelpful doc-uments. An interesting search pattern we observed in the estimated search paths is that in a satisfying task, once users issue an unsatisfying query, they can quickly correct it and reformulate a satisfying one ( p (Q + | Q  X  )=0.331); while in an unsatisfying task, users tend to get stuck in a sequence of unsatisfying queries ( p (Q  X  | Q  X  )=0.232) and end up with a failed search task ( p (END | Q  X  )=0.334). These are exam-ples of the types of insights that our automated method can yield, without having to apply expensive manual labeling.
In this work, we explicitly modeled searchers X  satisfaction at the action level for search-task satisfaction prediction. A latent structural learning framework was developed to model the unobservable action-level satisfaction labels, which en-abled us to explore rich structured features and dependency relations unique to search satisfaction modeling. Significant performance improvements in extensive experimental com-parisons against several state-of-the-art search satisfaction models confirmed the value of modeling action-level satis-faction in search-task satisfaction prediction. Moreover, we demonstrated the clear benefit of the inferred action satis-faction labels in other search applications such as document relevance estimation and query suggestion.

As future work, we will investigate how to apply the de-veloped framework for predicting search-task satisfaction in real time, action-by-action. If we can detect task failure early in the search process, search engines can adjust their ranking strategies or search support offered, before users abandon their searches. In addition, exploring the applica-tions of action-level satisfaction labels in additional contexts is also an interesting direction to pursue.
