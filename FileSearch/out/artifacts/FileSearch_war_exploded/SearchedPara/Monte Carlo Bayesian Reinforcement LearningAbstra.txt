 A major obstacle in reinforcement learning is slow conver-gence, requiring many trials to learn an effective policy. Model-based Bayesian reinforcement learning (BRL) pro-vides a principled framework to tackle this difficulty. To speed up convergence, BRL encodes prior knowledge of the world in a model. It explicitly represents uncertainty in model parameters by maintaining a probability distribution over them and chooses actions that maximize the expected long-term reward with respect to this distribution. One ap-proach to BRL is to cast it as a partially observable Markov decision process (POMDP) P (Duff, 2002). The state of P is a pair ( s, X  ) , where s is the discrete world state for the re-inforcement learning task and  X  is the unknown continuous model parameter. POMDP policy computation automati-cally analyzes both aspects of each action: its reward and its contribution towards inferring unknown model parame-ters, thus achieving optimal trade-off between exploration and exploitation.
 Despite its elegance, this approach is not easy to use in practice. Since model parameters are continuous in gen-eral, P has a hybrid state space and requires the restrictive assumption of conjugate distributions to represent beliefs during the policy computation (Duff, 2002; Poupart et al., 2006; Ross et al., 2007; Poupart &amp; Vlassis, 2008). We propose Monte Carlo Bayesian Reinforcement Learn-ing (MC-BRL), a simpler and more general approach to BRL, based on the following observation: although there are infinitely many parameter values, it may be possible to compute an approximately optimal policy without consid-ering all of them, if the objective is good average perfor-mance with respect to a prior distribution b 0 P of model pa-rameters. We sample a finite set of values from b 0 P and form a discrete POMDP  X  P whose state is ( s,  X   X  ) , with  X   X  taking values from the sampled set only. This discrete POMDP  X  P approximates the hybrid POMDP P .  X  P does not require conjugate distributions for belief representation and can be solved much more easily with existing point-based approx-imation algorithms, e.g. , (Kurniawati et al., 2008). MC-BRL also naturally handles both fully and partially observ-able worlds.
 We show that MC-BRL is approximately Bayes-optimal with a bounded error in the average case. The output-sensitive bound indicates that if a small approximately op-timal policy exists, then a small number of samples is suf-ficient for  X  P to approximate P well. In other words, if we treat P as a generalization of  X  P with a richer model param-eter space, a small policy results in better generalization. This nicely mirrors similar results in learning theory. We also provide experimental results evaluating MC-BRL on four distinct domains, including one from an application in autonomous vehicle navigation. 2.1. MDP and POMDP An MDP is a tuple  X  S,A,T,R, X   X  , where S is a set of world states, A is a set of actions, T ( s,a,s 0 ) specifies the transi-tion probability of reaching state s 0 when taking action a in state s , R ( s,a,s 0 ) specifies the reward received when taking action a in state s and reaching state s 0 , and  X  is a discount factor.
 A policy  X  : S  X  A for an MDP is a function that specifies which action to take in each state s  X  S . The value of a policy  X  is defined as the expected cumulative discounted reward where the expectation is with respect to the random vari-able s t , the state at step t . The aim of the MDP is to find an optimal policy  X  ? with maximum value.
 MDPs assume that the agent can directly observe the world state. POMDPs generalize MDPs by allowing par-tially observable states. Formally, a POMDP is a tuple  X  S,A,O,T,Z,R, X   X  , where S , A , T , R ,  X  are as defined in the case of MDP, O is a set of observations, and Z ( s 0 ,a,o ) is the observation function that specifies the probability of observing o when action a was taken in the previous step and the current state is s 0 .
 In a POMDP, the agent does not know for sure its state. Instead, it maintains a probability distribution or belief b ( s ) over the state space S . A policy  X  : B  X  A for a POMDP is a mapping from the belief space to actions. The value of  X  at a belief b is defined as where the expectation is with respect to the random vari-able b t , the belief at step t . Given an initial belief b aim of the POMDP is to find an optimal policy  X  ? maximum value at b 0 . 2.2. Related Works One common approach to BRL adopts the proposal in (Duff, 2002) and casts BRL as a POMDP P with a hy-brid state space (Wang et al., 2005; Poupart et al., 2006; Ross et al., 2007; Castro &amp; Precup, 2007; Poupart &amp; Vlas-sis, 2008; Ross &amp; Pineau, 2008). To maintain the posterior belief of continuous model parameters, it requires either a closed-form representation or effective approximate infer-ence techniques. Instead of solving P directly, MC-BRL approximates it with a discrete POMDP  X  P by sampling from the prior distribution and takes advantage of the re-cent advances in point-based discrete POMDP algorithms. This way, we avoid the restrictive assumption of close-form belief representation and obtain a simpler and more general approach.
 Sampling has been used extensively in BRL (Castro &amp; Pre-cup, 2007; Ross et al., 2007; Poupart &amp; Vlassis, 2008; Ross &amp; Pineau, 2008; Asmuth et al., 2009). However, the ear-lier works draw samples from the posterior distributions to speed up planning for P or to maintain beliefs efficiently. This is conceptually different from our approach, which samples hypotheses from the model parameter space a pri-ori to form  X  P and works exclusively with the sampled hy-potheses afterwards.
 Our theoretical result shares a similar idea with that for the (PO)MDP algorithm PEGASUS. (Ng &amp; Jordan, 2000). The PEGASUS analysis bounds the number of samples re-quired to find a good policy in a policy class with finite VC-dimension. Our result does not assume such a policy class. It provides an output-sensitive bound that depends on the size of the policy actually computed, instead of a worst-case bound for all policies in a class. 3.1. BRL as POMDP To simplify the presentation, let us first consider BRL of an MDP. Given an MDP  X  S,A,T,R, X   X  , the task of BRL is to find an optimal policy when the transition function T is unknown. Let  X  = {  X  sas 0 | s,s 0  X  S,a  X  A } de-note the collection of unknown parameters of the MDP, where  X  sas 0 = T ( s,a,s 0 ) . It has been shown that the BRL problem can be formulated as a POMDP P =  X  S
P ,A P ,O P ,T P ,Z P ,R P , X ,b 0 P  X  (Duff, 2002). The state space S P = S  X   X  is the cross product of the MDP states S and the parameter space  X  . A state ( s, X  ) consists of a world state s of the MDP and a hypothesized value  X  of the unknown parameter. The actions A P are identical to the actions A in the MDP. Assuming the parameter  X  does not change over time, the transition function is defined as where  X   X  X  0 is the Kronecker delta that takes value 1 if  X  =  X  0 and value 0 otherwise. The observation of the POMDP P indicates the current MDP state. Therefore, we define O P = S and Z P ( s 0 , X  0 ,a,o ) =  X  s 0 o . The re-ward does not depend on the parameter  X  , so we have R
P ( s, X ,a,s 0 , X  0 ) = R ( s,a,s 0 ) . Finally, we put a prior distribution b 0 P (  X  ) over  X  , which reflects our initial belief of the unknown parameter.
 This formulation explicitly represents the uncertainty in the unknown parameter. The parameter  X  forms a com-ponent of the POMDP state, which is partially observable and can be inferred based on the history of the observed MDP state/action pairs. By solving the POMDP P , one plans against both the uncertainty in the dynamics and the uncertainty in the model parameter. An optimal policy for P thus yields an optimal strategy for action selection that balances exploration with exploitation.
 Since the parameter  X  sas 0 takes continuous value, P has a hybrid state space. Two difficulties arise as a result. The first is how to efficiently maintain a belief for the contin-uous state variable. In order to attain a closed-form rep-resentation, most existing work assumes a conjugate prior b
P over the parameter  X  , such as the Dirichlet distribution (Dearden et al., 1999; Duff, 2002; Poupart et al., 2006; Ross et al., 2007; Poupart &amp; Vlassis, 2008). The sec-ond difficulty is how to solve the hybrid POMDP P effi-ciently. Although several approximate algorithms based on function approximation and online planning have been pro-posed (Duff, 2002; Poupart et al., 2006; Ross et al., 2007), there is no satisfactory answer in general. 3.2. Algorithm MC-BRL is motivated by the following observation. Al-though there are infinitely many possible values for the pa-rameter  X  , it may be possible to compute an approximately optimal policy without considering all of them. MC-BRL consists of two phases, offline and online. Given a prior distribution b 0 (  X  ) and a sample size K , the offline phase of the algorithm works in three steps. 1. Sample K hypotheses  X   X  1 ,  X   X  2 ,...,  X   X  K indepen-2. Form a discrete POMDP  X  P = 3. Solve the POMDP  X  P and output a policy  X   X  . In the online phase, the agent then follows the policy  X   X  to select actions.
 MC-BRL sidesteps the two technical obstacles of the exist-ing approach based on the hybrid POMDP P . The discrete POMDP  X  P can be readily solved with point-based approxi-mation algorithms (Pineau et al., 2003; Smith &amp; Simmons, 2005; Kurniawati et al., 2008). There is also no restrictive assumption on the form of the prior distribution b 0 (  X  ) . The only requirement is that it is easy to sample from. We further note that  X  P falls into the class of mixed observ-ability MDPs (MOMDPs). Its state ( s,k ) has mixed ob-servability. While the second component k is hidden, the first component s is fully observable. It has been shown that MOMDPs admit a compact factored representation of the state space, which can be exploited to speed up POMDP planning (Ong et al., 2010). In this paper, we use SARSOP (Ong et al., 2010) to solve  X  P which readily takes advantage of the MOMDP representation.
 MC-BRL takes a prior distribution b 0 (  X  ) as input. In prac-tice, if we know nothing about the true parameter, we use a non-informative prior such as uniform distribution. When there is prior knowledge about the true parameter, more in-formative prior can be used to bias the hypotheses towards the ground truth. 3.3. Generalization to Partially Observable MC-BRL can be readily generalized to BRL problems un-der partially observable environments. Suppose we are given a POMDP  X  S,A,O,T,Z,R, X   X  , and we aim to find an optimal policy when both the transition function T and the observation function Z are unknown. The unknown parameters can be denoted as a pair (  X , X  ) , where  X  is as defined before, while  X  = {  X  s 0 ao | s 0  X  S,a  X  A,o  X  O } denotes the observation function and  X  s 0 ao = Z ( s 0 ,a,o ) . MC-BRL can be naturally adapted to address this problem with two modifications to the offline phase. First, it sam-ples the hypotheses from a joint prior distribution b 0 (  X , X  ) instead of b 0 (  X  ) . Second, the POMDP  X  P is modified by set-observation function Z  X  P now incorporates the uncertainty in the unknown parameter  X  of the underlying POMDP. MC-BRL uses the discrete POMDP  X  P to approximate the hybrid POMDP P . To analyze the quality of this approxi-mation, we derive a probably approximately correct (PAC) bound on the regret of MC-BRL X  X  solution, compared with the optimal solution to P .
 We assume that a POMDP policy  X  is represented as a pol-icy graph G , which is a directed graph with labeled nodes and edges. Each node of G is labeled with an action a  X  A and has | O | outgoing edges, each labeled with a distinct observation o  X  O . The size of the policy  X  , denoted as |  X  | , is the number of nodes in G . To execute the policy, the agent first picks a node in G according to the initial belief. It then takes the action associated with the node, receives an observation, and transits to the next node by following the edge labeled with that observation. The process then repeats.
 The policy graph representation allows us to establish the correspondence between policies for P and  X  P . If  X  is a pol-icy for P , then it is also a valid policy for  X  P , and vice versa, as P and  X  P share the same action space A and observation space O .
 Suppose that MC-BRL forms the discrete POMDP  X  P by taking K samples from the initial belief b 0 P of P . There are three policies of interest: an optimal policy  X  ? for P , an optimal policy  X   X  ? for  X  P , and the policy  X   X  that MC-BRL ac-tually computes. We want to bound the regret of  X   X  against  X  . Define V  X  as the value of a policy  X  for P with initial belief b 0 P , and  X  V  X  as the value of  X  for  X  P with initial be-lief b 0  X  result. The proof is given in the supplementary material 1 Theorem 1. Suppose that  X  ? is an optimal policy for P and  X   X  is the policy that MC-BRL computes by taking K samples to form a discrete POMDP  X  P . Let R max =  X   X  (0 , 1) ,
V with probability at least 1  X   X  .
 The theorem says that MC-BRL with a small set of samples produces a good approximate solution  X   X  to P with high probability, provided that there exists a simple approximate solution  X   X  to  X  P . It is interesting to observe that although we formulate and solve the underlying reinforcement learning task as a planning problem, this analysis closely mirrors similar results in learning: if we think of P as a general-ization of  X  P with a richer model parameter space, then the theorem implies that a small policy results in better gener-alization.
 The error bound consists of two terms. The first term de-cays at the rate O (1 / more hypotheses from the prior, but at the cost of poten-tially increasing the complexity of the discrete POMDP  X  P and the resulting policy  X   X  . The second term  X  bounds the error in the approximate solution to the discrete POMDP  X  P . Algorithms such as HSVI (Smith &amp; Simmons, 2005) and SARSOP (Kurniawati et al., 2008) output such bounds as a by-product of POMDP policy computation. We can reduce  X  by running these algorithms longer towards con-vergence.
 It is also important to observe that the approximate Bayes-optimality of  X   X  , quantified by V  X   X  , guarantees the average performance of  X   X  with respect to the prior distribution b of models. It does not guarantee the performance of  X   X  on any particular model.
 Our analysis assumes a policy graph representation of POMDP policies. In practice, point-based discrete POMDP algorithms, such as HSVI and SARSOP, typically output policies represented as a set of  X  -vectors, which in principle can be converted to policy graphs. We now experiment with MC-BRL on both fully observ-able and partially observable reinforcement learning tasks. First, we evaluate MC-BRL on two small synthetic do-mains widely used in the existing work on BRL (Sec-tions 5.1 and 5.2). Here the standard setup requires us to measure the performance of an algorithm on particu-lar model parameter values rather than the average perfor-mance with respect to a prior distribution of model param-eters. Therefore the bound in Theorem 1 is not applicable here. Next, we test MC-BRL on two more realistic domains (Sections 5.3 and 5.4), where we measure the average per-formance of MC-BRL and show that it performs well in this sense, as our theoretical result guarantees. All the experiments are conducted on a 16-core Intel Xeon 2.4GHz server. 5.1. Chain We start with the Chain problem used in (Dearden et al., 1998; Poupart et al., 2006). This problem consists of a chain of 5 states and 2 actions { a,b } . The actions cause the transitions between states and receive corresponding re-wards, as shown in Figure 1. The actions are noisy. They slip with probability 0.2 and cause the opposite effect. The optimal policy of this problem is to always perform action a .
 We consider two versions of the Chain problem. In the semi-tied version, we assume that the structure of transi-tions between states in Figure 1 are given. The only un-known parameters are the 2 slipping probabilities, one for each action. In the full version, we assume that the tran-sition function T ( s,a,s 0 ) is completely unspecified. This leads to 40 unknown parameters.
 We evaluate MC-BRL algorithm using 500 simulations with 1000 steps in each simulation. We test K = 10 , 100 , and 1000 , and use the uniform prior to sample hypotheses. Since it is a stochastic algorithm, we rerun the offline phase of MC-BRL before each simulation, obtain a policy, and then execute that policy online. We run the offline phase up to 180 seconds. The online time is negligible.
 Table 1 reports the average (undiscounted) total rewards of MC-BRL. For comparison, we also report an upper bound on the reward that could be achieved only if we had known the true model parameters, as well as the rewards of three alternatives: the Beetle algorithm (Poupart et al., 2006), the Exploit heuristic, which never explores but takes the opti-mal action with respect to the expected MDP under the cur-rent belief, and Q-learning with -greedy exploration and linear learning rate. For Q-learning, we test a wide range of values from 0 to 0 . 5 . The reward for the optimal value is reported.
 MC-BRL achieves good performance in the semi-tied ver-sion. It obtains near-optimal reward with 1000 samples and is comparable to Beetle. It outperforms Exploit and Q-learning. In the full version, MC-BRL is still better than Q-learning. However, it performs slightly worse than Bee-tle and is unable to improve the performance substantially with increased number of samples. Exploit performs much better than both MC-BRL and Beetle. However, Exploit relies on a myopic heuristic and does not explore well in general. For example, it performs much more poorly than MC-BRL and Beetle in the semi-tied version.
 MC-BRL X  X  performance degrades in the full version, be-cause the sample size is too small to cover the neighbor-hood of the true parameters within the 40-dimensional pa-rameter space using the uniform prior. To verify this, we conduct another experiment by inserting the true param-eter values as one of the samples of MC-BRL. The re-sults, denoted as MC-BRL + in Table 1, show that MC-BRL achieves good performance in this case. Constructing effective sampling strategies is an important direction for future research. 5.2. Tiger We next test MC-BRL on the Tiger problem (Kaelbling et al., 1998) with partial observability. In this problem, the agent must decide whether to open one of two doors or to listen for the position of the tiger at each time step. Opening the wrong door will cause the agent to be eaten by a tiger with a penalty of  X  100 , while opening the correct door will give a reward of 10 . Listening costs  X  1 and gives the true position of the tiger with 15% error. We assume that the transition and reward functions are given, but the observation error rates are unknown.
 We evaluate MC-BRL using 1000 simulations. Each sim-ulation consists of 100 episodes. In each episode, the agent takes actions and receives observation sequentially. The episode ends when the agent opens a door and the position of the tiger is reset. We test MC-BRL with K = 10 and 100 . Following (Ross et al., 2007), we use Dirichlet (3 , 5) as the prior distribution to sample the unknown parameters. This prior corresponds to an ex-pected error rate 37 . 5% . We run the offline phase of MC-BRL up to 300 seconds.
 Table 2 shows the total reward gained by MC-BRL in 100 episodes, averaged over the 1000 simulations. For refer-ence, we also include the upper bound induced by the true model, and the reward of the prior model in which the ob-servation error rate is set to the prior expectation 37 . 5% . With K = 100 , MC-BRL achieves performance close to the upper bound, and is far better than the prior model. We further look into the evolution of the reward over episodes. Figure 2 shows the reward gained by MC-BRL per episode, averaged over the 1000 simulations. As we do not have the exact settings used in (Ross et al., 2007), we cannot directly compare with their experimental results. However, we can see that MC-BRL quickly learns the un-known parameters and improves over the prior model. It achieves near-optimal performance after about 20 episodes. 5.3. Iterated Prisoner X  X  Dilemma The Prisoner X  X  Dilemma (Poundstone, 1992) is a well known one-shot two-player game in which each player tries to maximize his own reward by cooperating with or be-traying the other. In this section, we studied its repeated version, the Iterated Prisoner X  X  Dilemma (IPD) (Axelrod, 1984), and show that MC-BRL can achieve excellent per-formance on this problem.
 In IPD, the game is played repeatedly and each player knows the history of his opponents moves. A key factor for an agent to gain high reward is the capability to model the opponent X  X  behaviour based on history. It has been shown that any memoryless and one-stage memory opponent can be modeled using 4 parameters  X  P S ,P T ,P R ,P P  X  , which are the probabilities that the opponent will cooperate in the next step, given the 4 possible situations of the current step: (1) the agent cooperates while the opponent defects (de-noted by S ); (2) the agent defects while the opponent co-operates ( T ); (3) mutual cooperation ( R ); and (4) mutual defection ( P ) (Kraines &amp; Kraines, 1995).
 Suppose the agent knows the parameters of its opponent. Then the IPD can be naturally formulated as an MDP. The state of the MDP is the current move of the two players, which takes values from { S,T,R,P } . The agent needs to select between cooperating or defecting for the next move. The transition function is defined based on the parameters of the opponent. The reward depends on the next state, and is set to 0 , 5 , 3 , 1 for S,T,R,P respectively, following the setting commonly used in IPD tournaments.
 In reality, the parameters of the opponent are unknown. The agent needs to explore the opponent X  X  strategy and at the same time maximize its reward. This leads to a RL problem and we apply MC-BRL to solve it.
 We are interested in the average performance of MC-BRL when facing various opponents. Therefore, we randomly select 1000 test opponents by uniformly sampling their pa-rameters. For each opponent, we run the offline phase of MC-BRL for 180 seconds and obtain a policy. We then use the policy to play against the opponent for 300 steps and collect the total reward. This is repeated for 20 times to account for the stochastic behaviour of the opponent. For MC-BRL, we test K = 250 and 1000 , and use the uniform prior to sample the parameters. We set the discount factor  X  = 0 . 95 .
 Table 3 shows the total rewards averaged over the 1000 op-ponents. With K = 250 , MC-BRL already achieves good rewards. With K = 1000 , it approaches the upper bound, which is achieved by solving the underlying MDP with the true parameters of the opponents.
 For reference, we also compare MC-BRL with two classic hand-crafted strategies, Tit-for-Tat (TFT) (Axelrod, 1984) and Pavlov (Nowak &amp; Sigmund, 1993), and the two win-ning entries of the 2005 IPD tournament, Adaptive Pavlov (AP) (Li, 2007) and Omega Tit-for-Tat (OTFT) (Slany &amp; Kienreich, 2007). These four strategies are used to play against the same 1000 test opponents under the same set-ting as MC-BRL. The results are summarized in Table 3. MC-BRL achieves comparable reward to OTFT, and sig-nificantly outperforms all the others. It is interesting to note that AP, the tournament winner, performs very poorly. TFT, Pavlov, AP, and OTFT are all specially designed to win the IPD tournaments, while MC-BRL is a general al-gorithm for BRL and is not optimized for competitions. On the other hand, one should not directly translate the good performance of MC-BRL here to the IPD tournaments, as it is unlikely to face random opponents. However, MC-BRL can use more informative priors to exploit domain knowl-edge on the opponents, as the other algorithms do. We further compare MC-BRL with Q-learning. We fol-low the setting suggested by (Littman &amp; Stone, 2001). The result is shown in Table 3. We can see that MC-BRL sig-nificantly outperforms Q-learning on this task.
 While MC-BRL achieves good average performance, as our theorem guarantees, it can perform worse than other algorithms when faced with particular oppo-nents. For instance, for the opponent parameterized by  X  0 . 806 , 0 . 108 , 0 . 596 , 0 . 185  X  , MC-BRL obtains a much lower reward than that of Q-learning: 596 . 05 versus 659 . 1 . 5.4. Intersection Navigation This problem is motivated by an accident in the 2007 DARPA Urban Challenge (Leonard et al., 2008). In that event, two autonomous vehicles, R and A , approached an uncontrolled traffic intersection as shown in Figure 3. R had the right-of-way and proceeded. However, possi-bly due to sensor failure or imperfect driving strategy, A did not yield to R and caused a near-miss. This situa-tion is quite common and occurs frequently even with hu-man drivers. Crossing the intersection safely and efficiently without knowing the driving strategy of A poses a signifi-cant challenge.
 We formulate the problem as a RL problem. The underly-ing model is a POMDP. The state consists of the positions and velocities of R and A . For simplicity, we discretize the environment into a uniform grid. In each step, the agent R can take three actions: accelerate, maintain speed, and decelerate. It then receives an observation on its own state and the state of A . Both actions and observations are noisy. The transition function is defined based on the driving strat-egy of A , which is unknown to the agent R . The agent re-ceives a reward for crossing the intersection safely, and a large penalty for collision with A . A small penalty is given in each step to expedite the agent to cross the intersection faster. Due to space limitation, we give the detailed settings in the supplementary material.
 The driving strategy of A is unknown to the agent. We parameterize the driving strategy using 4 parameters: (1) driver imperfection,  X   X  [0 , 1] , (2) driver reaction time,  X   X  [0 . 5 , 2] s , (3) acceleration, a  X  [0 . 5 , 3] m / s deceleration, d  X  [  X  3 ,  X  0 . 5] m / s 2 . A preliminary study shows that this parameterization can cover a variety of drivers such as a reckless driver who never slows down at the intersection and an impatient driver who performs a rolling stop near the intersection. The agent needs to learn the parameters of A and cross the intersection at the same time.
 We test MC-BRL on this RL problem. We test a range of K values and sample the parameters from the uniform distribution. Similar to the IPD problem, we are interested in the average performance of MC-BRL with respect to dif-ferent drivers A . Therefore, we uniformly sampled 250 test drivers. For each driver, we run the offline phase of MC-BRL for 1.5 hours and obtain a policy. We then evaluate the policy against that test driver using 200 simulations with 40 steps in each simulation.
 Figure 4 shows the average discounted total rewards with discount factor  X  = 0 . 99 . We can see that, as the sample size K increases, the performance of MC-BRL improves quickly. With K = 300 , it gets close to the upper bound, which is achieved when the true parameters of the driver A are known.
 We also compare MC-BRL to a hand-crafted intersection policy that is commonly used in the traffic modeling com-munity (Liu &amp; Ozguner, 2007). With K = 150 and above, MC-BRL significantly outperforms that policy. While the hand-crafted policy is not designed to handle noisy ob-servations, we think that the performance gap between the hand-crafted policy and MC-BRL is more likely to be caused by insufficient adaptivity of the hand-crafted policy in learning the driving strategy of A .
 As a final remark, this problem gives an example where it is more natural to define the prior over the physical properties of the environment. MC-BRL handles such priors easily, although they are challenging to specify using methods that rely on conjugate distributions. We have presented MC-BRL, a simple and general ap-proach to Bayesian reinforcement learning. We prove that by sampling a finite set of hypotheses from the model parameter space, MC-BRL generates a discrete POMDP that approximates the underlying BRL problem well with guaranteed performance. We provide experimental results demonstrating strong performance of the approach in prac-tice. Furthermore, MC-BRL naturally handles both fully and partially observable worlds.
 One important issue for MC-BRL is to sample the model parameter space effectively. A naive method is to dis-cretize the parameter space uniformly and treat the fixed grid points as samples. This method, however, suffers from the  X  X urse of dimensionality X  and is difficult to scale up as the number of parameters increases (Poupart et al., 2006). MC-BRL takes one step further and samples a set of hy-potheses independently from a given prior distribution. The promising results obtained in this work open up many pos-sibilities for future investigation, e.g. , constructing better informed prior distributions by exploiting domain knowl-edge and adaptive sampling.
 Y. Wang and D. Hsu are supported in part by MoE AcRF grant 2010-T2-2-071 and MDA GAMBIT grant R-252-000-398-490. K.S. Won is supported by an NUS Presi-dent X  X  Fellowship. W.S. Lee is supported in part by the Air Force Research Laboratory, under agreement number FA2386-12-1-4031. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorse-ments, either expressed or implied, of the Air Force Re-search Laboratory or the U.S. Government.

