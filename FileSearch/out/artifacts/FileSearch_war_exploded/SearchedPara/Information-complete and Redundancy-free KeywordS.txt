 Keyword search over graphs has a wide array of applica-tions in querying structured, semi-structured and unstruc-tured data. Existing models typically use minimal trees or bounded subgraphs as query answers. While such models emphasize relevancy, they would suffer from incompleteness of information and redundancy among answers, making it difficult for users to effectively explore query answers. To overcome these drawbacks, we propose a novel cluster-based model, where query answers are relevancy-connected clus-ters . A cluster is a subgraph induced from a maximal set of relevancy-connected nodes. Such clusters are coherent and relevant, yet complete and redundancy free. They can be of arbitrary shape in contrast to the sphere-shaped bounded subgraphs in existing models. We also propose an efficient search algorithm and a corresponding graph index for large, disk-resident data graphs.
 Categories and Subject Descriptors: H.2.4 [Database Management]: Systems X  Query processing Keywords: Keyword search over graphs, Information com-plete, Redundancy free, Indexing
Keyword search over data graphs has received increasing attention in recent years from the fields of databases, data mining, information retrieval (IR) and Web search [10, 2, 1, 6, 7, 5, 12, 8, 3]. On one hand, IR-style keyword search is very user-friendly and has been universally accepted. On the other hand, graphs are a convenient generic data model for structured, semi-structured, and unstructured data. For structured data such as relational databases, nodes repre-sent tuples and edges represent foreign key references. For semi-structured data such as XML databases, nodes repre-sent elements and edges represent containment relationships. For unstructured data such as HTML data, nodes represents Web pages and edges represent hyperlinks.
 Existing models. Over the years, various models and algo-rithms have been proposed for keyword search over graphs. As a natural way of prioritizing relevancy, existing approaches focus on the problem of finding minimal trees or bounded subgraphs as query answers. A typical representative is the minimum group Steiner tree problem [10, 2], which mini-mizes the total edge weights of a tree containing keyword nodes and Steiner nodes that connect keyword nodes. The Steiner tree problem is hard. [5] proposed a simpler distinct rooted tree model, which minimizes the total edge weights of a set of root-to-keyword node paths.

With the majority of answer models being tree-based, [9] proposed the r -radius Steiner graph model, where each an-swer is a subgraph, not necessarily a tree, with radius of r containing keyword nodes and Steiner nodes. [11] proposed the multi-center induced graph model, in contrast to single-center (root) tree-based answers. Under this model, for a given query with l input keywords, each answer is referred to as a community whose node set contains up to l keyword nodes, multiple center nodes each connecting to every key-word node within r radius, and the structural nodes on the paths from center nodes to keyword nodes. Both models have their answer subgraphs bounded to retain relevancy and meaningfulness [9].
 Drawbacks. These answer models, especially the tree-based ones, would suffer from two major drawbacks of in-completeness of information and redundancy among answers. As first argued in [11], an answer tree typically shows only partial information about how those keyword nodes are con-nected. They proposed the community-based multi-center model to alleviate this drawback.

We observe that the two drawbacks are well related. In tree-based models, many tightly connected trees sharing nodes and edges in common have to be presented separately as dif-ferent answers. This causes not only loss of structural infor-mation among these answer trees, but also tremendous re-dundancy among them. Similarly, in the community-based model, each community is bounded containing up to l key-word nodes. Then, multiple overlapping communities have to be presented separately and redundantly, causing loss of structural information among these communities. Note that it is hard for users to manually reconstruct the structural information among answers once they are separated.
Example 1. Figure 1 shows a subgraph for a query with input keywords of k 1 , k 2 and k 3 . The subgraph contains many overlapping Steiner trees and multiple overlapping communities. By separating them, we lose the information how, e.g., the k 1 node 2 interacts with both of its neighbor-ing k 2 nodes 1 and 3.
Moreover, the bounded subgraph models would suffer from additional drawbacks. Firstly, such models favor sphere-shaped subgraphs. However, in practice legitimate query answers can be of arbitrary shape. Secondly, due to the constraint of r , they fail to work for queries that involve  X  X engthy X  (long path) relationships among keyword nodes. Our contributions: (1) We propose a completely different relevancy-connected cluster answer model that uses maximal , unbounded , arbitrary-shaped subgraphs to provide rich structural information and eliminate redundancy while retaining relevancy. (2) We propose a novel kernel-based scoring strategy for the purpose of ranking maximal answers. (3) We design an efficient search algorithm and a corre-sponding graph index for large, disk-resident data graphs.
Under our model, a query answer is a relevancy-connected cluster, cluster for short, that is maximal, unbounded, and arbitrary-shaped. The model is described as follows.  X  A node in a cluster can be a keyword node that maps to one or more keywords, or structural node that connects keyword nodes.  X  A node in a cluster, keyword or structural, is either a core node or a border node .  X  Each core node must satisfy a user-specified core condition with respect to relevancy. Core condition can be defined in different ways depending on the application. A reasonable example would be: N r ( v ) must cover  X  min ( l, l 0 ) different input keywords, where l is the number of input keywords, l 0 is a threshold, and N r ( v ) denotes the set of nodes within the r -radius neighborhood of v .  X  A border node is not a core node. It can be a keyword node v 0  X  N r ( v ) for some core node v , or a structural node on a path of length up to r from v to v 0 .  X  A node v 0 is directly relevancy-reachable from a node v if v is a core node and v 0  X  N r ( v ).  X  A node v 0 is relevancy-reachable from a node v if there is a chain of nodes v 1 ,  X  X  X  , v n such that v 1 = v , v n = v v i +1 is directly relevancy-reachable from v i for 1  X  i  X  n .  X  Nodes v and v 0 are relevancy-connected if there exist a node o such that v and v 0 are relevancy-reachable from o .  X  A relevancy-connected cluster (cluster for short) is a sub-graph of the data graph induced from a maximal set of relevancy-connected nodes.

Example 2. In Figure 1, let r = 2 and the core condi-tion be that N r ( v ) must contain all the three input key-words. Then, the dark nodes indicate the core nodes and other nodes indicate the border nodes. Node 1 is directly relevancy-reachable from node 2. Nodes 1 and 4 are relevancy-reachable from, e.g., node 3, thus they are relevancy-connected. The subgraph, assuming maximal, represents a relevancy-connected cluster and a query answer.

Based on the definition, each cluster must have at least one core node. It is possible that a cluster only contains core nodes. Note that the definition resembles the one in the DB-SCAN clustering algorithm [4] that finds density-connected arbitrary-shaped clusters. For queries with lengthy relation-ships, tree-based models will find the long paths under no conditions. Our cluster model will include such paths only if they are relevancy-connected. Although we assume undi-rected and unweighted data graphs in this study, the idea can be easily extended to directed and weighted data graphs. Relaxed cluster model. Based on the definition, two relevancy-connected clusters cannot share core nodes, but they could share border nodes in rare cases. In Figure 2, the two clusters (dashed ovals) share the same border node k . When r is small, e.g., 1 or 2, merging such overlapping clusters into one relaxed cluster would not cause loss of co-hesion and discontinuation of relevancy. The benefit of the relaxed cluster model is two fold: (1) complete elimination of overlapping and redundancy, (2) simple search algorithm. Scoring. Query answers need to be scored with respect to relevancy for ranking purposes. Scoring minimal answers is straightforward, where we can simply favor answers that are small in size or total edge weight.
 We propose a novel approach to score maximal answers. We define the kernel of an answer C , C kernel , to be the subgraph of C that receives the best score with respect to a given scoring function F . We use the score of the kernel of a cluster C as the score of C , i.e., F ( C ) = F ( C kernel
Based on this idea, F can be defined in a similar manner as those for minimal graphs. In our study, since we do not require an answer to cover all the input keywords, our F favors kernels that (1) cover more different input keywords, and (2) have small total number of edges (total edge weights if edges are weighted).
 Finding kernels can be challenging when clusters are big. In this study we use a heuristic that restrains a kernel to be the r -radius neighborhood of some single core node. This way, we only need to individually examine the core nodes in a cluster.
Let K = { k 1 ,  X  X  X  , k l } be a query represented by a set of keywords. Let G ( V, E ) be the data graph, which may not fit into memory. Our clustering-based search algorithm returns a ranked list of relevancy-connected clusters in linear time, taking advantage of a r -radius graph index I for G that extends the conventional inverted index.
 Two observations. We start with introducing two obser-vations that motivate the way our graph index is constructed and used in the search (clustering) algorithm.

Let G r ( v ) denote the r -radius neighborhood of node v , which is the induced subgraph of G from N r ( v ). The fol-lowing observation states that the union of all clusters is a subgraph of G 0 ( V 0 , E 0 ), the union of the r -radius neighbor-hoods of all keyword nodes in G .

Observation 1. Let v 1 ,  X  X  X  , v n be the keyword nodes in V with respect to query K . Let C 1 ,  X  X  X  , C m be the relevancy-connected clusters. S m i =1 C i is a subgraph of G 0 ( V S
Proof. We give the sketch of a proof. Based on the definition of core condition, each core node must be in V Each border node v can be a keyword node or a structural node. In the former case, v  X  V 0 trivially. In the latter case, v is on a path of length  X  r from some core node to some keyword node, thus v  X  V 0 .

G 0 ( V 0 , E 0 ) contains some nodes and edges that are not on any keyword-to-keyword path. The following observa-tion states that such nodes and edges cannot be part of any cluster.

Observation 2. Let P h denote the set of keyword node to keyword node paths of length h in G 0 ( V 0 , E 0 ). Let C be the relevancy-connected clusters. S m i =1 C i is a subgraph
Obviously, G 00 is a subgraph of G 0 . Note that this obser-vation is valid not only for our relevancy-connected cluster model, but also for other bounded-graph models. This is because all such models share a common interest in paths of bounded length connecting keyword nodes, and Steiner or structural nodes are on such paths.

Based on Observation 1, we construct a graph index I that allows us to retrieve G 0 with minimal IO. Based on Observation 2, we derive G 00 from G 0 by applying simple in-tersections. Then, we run a linear-time clustering algorithm on G 00 , a much, much smaller portion of the data graph G . Retrieving G 0 from graph index I . Basically, I stores the r -radius neighborhood G r ( v ) for each v  X  V . For a keyword k i , we use I h ( k i ) to denote the set of nodes in V that are h -hops away from some node that maps to k i . In particular, I 0 ( k i ) denotes the set of nodes in V that map to k . Let I ( k i ) denote the postings list for keyword k i . For the conventional inverted index, I ( k i ) = I 0 ( k i ) for each keyword in the dictionary. Our graph index extends the inverted index, where I ( k i ) = I 0 ( k i )  X  I 1 ( k i )  X  I 2 ( k addition, the edges from the nodes in I h +1 ( k i ) to the nodes in I h ( k i ) are also stored in the form of pointers.
For a query K = { k 1 ,  X  X  X  , k l } , V 0 = S l i =1 I ( k retrieve G 0 , we can simply read I ( k i ) for each keyword in K into memory, together with the corresponding edges. G 0 can well fit into memory, and the retrieval takes minimal IO. Obtaining G 00 from G 0 . Let P h ( k i , k j ) denote a set of paths of length h connecting a k i keyword node and a k j keyword node. G 00 contains keyword node to keyword node paths of lengths up to 2 r . Any such path belongs to P h for some h, i, j where 0  X  h  X  2 r and 1  X  i, j,  X  l . It is pos-sible that k i = k j , where the paths in P h ( k i , k j nodes that map to the same keyword. In case h = 0 and k 6 = k j , P h ( k i , k j ) corresponds to a degenerate set of paths, each being a single node that maps to both keywords k i and k .

To obtain P h ( k i , k j ), we can perform an intersection oper-ation on I h 1 ( k i ) and I h 2 ( k j ), where h 1 + h 2 I 1 ( k i )  X  I h 2 ( k j ) contains a set of nodes, each connecting to some k i node in h 1 hops and some k j node in h 2 hopes. Note that there can be different combinations of h 1 and h 2 that add up to h . We choose the pair the has the smallest section time.

How to obtain the actual paths in P h ( k i , k j ) from I I 2 ( k j )? Each node v  X  I h 1 ( k i )  X  I h 2 ( k j ) must connect to some k i node v 1  X  I 0 ( k i ) in h 1 hops and some k j node v I ( k j ) in h 2 hops. We only need to establish the sub-path of length h 1 from v to v 1 and the sub-path of length h 2 from v to v 2 . The two sub-paths make up the path v 1 to v 2 of length h . Note that there can be multiple such paths in P ( k i , k j ) that pass node v . To get the sub-paths, e.g., v to v , follow a pointer (may have multiple, need to follow each of them) from v to a node v 0  X  I h 1  X  1 ( k i ) recursively until reaching I 0 ( k i ).

Example 3. Let r = 2 and K = { k 1 , k 2 } . G 00 contains the following sets of paths: P h ( k 1 , k 1 ), P h ( k 2 , k I ( k 1)  X  I 2 ( k 2), or I 2 ( k 1)  X  I 0 ( k 2), or I 1 ( k 1)  X  I one of the first two, depending on the size of I 0 ( k 1) and I ( k 1), whichever is smaller.

To obtain P 4 ( k 1 , k 2 ), we only have one option, I 2 I ( k 2). This is because r = 2 and 0  X  h 1 , h 2  X  r . To obtain the actual paths in P 4 ( k 1 , k 2 ), for each v in I ( k 1)  X  I 2 ( k 2), follow each pointer from v to node v I ( k 1), and then follow each pointer from v 0 to node v 1 I ( k 1), to establish the v to v 1 type of sub-paths. The v to v type of sub-paths can be established in the same manner, in particular, follow each pointer from v to node v 00  X  I and then follow each pointer from v 00 to node v 2  X  I 0 ( k 2). Observation 2 is based on a reasonable core condition that N ( v ) must cover  X  min ( l, l 0 ) different input keywords. In this case, the maximum minimum length of paths between keyword nodes is 2 r . With a more restrictive core condition, e.g., requiring each core node must also be a keyword node, then the maximum minimum path length is r . Then, G 00 is even smaller and can be obtained by fewer number of intersections.
 The graph index I can be prohibitively large if r is big. To give an example, suppose the average degree is d in G , then I needs to index a graph that is d r times as large as G . However, reasonable core conditions require small r values to achieve high relevancy. Thus, in most cases the size of I would not cause practical problems. Depending on the application, good choices of r can be 1 or 2. Additional index compression techniques can also help to alleviate the problem when a big r has to be used.
 Clustering on G 00 . As we have established previously, the union of all clusters, G 00 , G 0 and G are in turn subgraphs. Algorithm 1 Query processing 1: for each i from 1 to l 3: for each h from 0 to 2 r 6: if ( v is a core node) then 8: recursively add to V C each node that is (1) directly 11: score each cluster C with the kernel heuristic, then rank and We only need to search G 00 for the clusters. Since our an-swer model eliminates redundancy, in general there exist a relatively small number clusters. Thus in this study, our search (clustering) algorithm does not focus on finding top k answers (clusters) in a progressive manner.

Our clustering algorithm finds all the relevancy-connected clusters in G 00 ( V 00 , E 00 ) in linear time. Note that DBSCAN runs in O ( n log n ) time because it needs an R  X  -tree index, whereas we do not.

The sketch of the algorithm is as follows. For each v  X  V we check if v satisfies the core condition. If yes, a new cluster C ( V C , E C ) is created. Then we recursively collect the set of core nodes that are directly relevancy-reachable from v . Then, we add to V C the border nodes. Each border node can be a keyword node v 0  X  N r ( v ) for some core node v  X  V or a structural node on a path of length up to r from v to v . After collecting all the core nodes and border nodes for cluster C , we add to E C the edges in E 00 induced from V
For the relaxed model, we can have a conceptually sim-pler search algorithm. We first identify all the core nodes individually and create a cluster for each of them. Then, we add the border nodes to each cluster. Then, we merge the connected clusters into one.

Algorithm 1 summarizes the main procedures of our frame-work for processing query K .
To validate the utility of our framework, we have im-plemented it on DBLP data graph (834,609 authors and 585,468 papers) with researcher profile pages as nodes and co-authorships as edges. The core condition is that N 1 ( v ) must cover  X  min ( l, l 0 ) different input keywords. Tuning l adjusts the degree of relevancy requirement. Although more comprehensive evaluations need to be done, an initial user study confirmed that our approach returns relevant answers just like the Steiner tree and bounded graph models, yet our answers are much more complete and redundancy free. The following is a typical example.

Example 4. Suppose we want to search for the profile page of either Jiawei , Edward or David who has coauthored with other two. We may issue a query  X  X iawei, Edward, David X . One search result is shown in Figure 3. From the result we can see that, although these author names are not selective (numerous different Jiawei X  X , Edward X  X  and David X  X ), we can still quickly locate the author page we want because there are a very selective number of answers that conform to the required coauthor relationship. The result is very coherent and relevant. In addition, the result is also very complete and redundancy free. If the Steiner tree model is used, the result will be split into six trees, each containing Jiawei, Edward, and one of the six David X  X .
