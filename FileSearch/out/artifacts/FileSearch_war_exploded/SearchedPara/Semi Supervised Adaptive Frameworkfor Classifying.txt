 Data streams have inherent properties which make it difficult for the traditional data mining techniques to classify stream data. Some of the most challenging properties of data streams include but not limited to infinite length, concept drift, concept evolution, limited labeled data and delayed labeling. Since data stream is an infinite stream of data, it cannot be stored into any storage for process. Concept drift occurs when the target class or concept evolves within the feature space such that, the class encroaches or crosses previously defined context of data streams need to be updated to cope up with changing concepts. Concept evolution occurs when a new class emerges in the data stream [ 2 , 3 ]. To address infinite length and concept drift problems, most of the approaches these approaches fail to adapt to the change of concepts immediately. If the chunk size is too small, classification method may end up with frequent training during stable period when there is no concept drift, causing performance drawback due to unnecessary update of the classifier. On the contrary, if the chunk size is too large, the classifier may remain outdated for a long period of time. Some other approaches [ 5 , 6 ]use gradual forgetting to address infinite length and concept drift problems. These approaches use various decay functions to assign weight to the instances based on their age. This strategy also suffers from similar trade-off while choosing the decay rate to match unknown rate of change. To solve the problems due to fixed chunk size or decay rate, a dynamic sliding window is maintained in [ 7 , 8 ] by tracking any major change in error rate of the classifier. These approaches mostly assume that, true labels of the data instances will be available to calculate the error rate as soon as it is tested. However, in the real world, labeled data is scarce since labeling data instances manually is costly and time consuming [ 9 ]. In data streams, where data arrives very quickly, it may not be possible to label all the data instances as soon as they arrive. So, a good classifier in streaming context should be able to defer the training until true labels become available yet continuing labeling newly arrived instances using the current classifier. Moreover, it should be able to use partially labeled training data [ 9 ].
 In this paper, we present a complete framework which addresses all of the and novel class detection. However, unlike [ 2 ], our proposed framework divides the data stream into dynamically determined chunks using change detection technique. To avoid the use of true labels of data instances for change detection, our proposed framework calculates a confidence value while predicting label of each data instance. It then uses a change point detection technique to detect any significant change in the classifier confidence scores and determines the chunk size dynamically. If a significant change is detected, the classifier is updated using only recent labeled training data instances. To address the concept drift problem, our framework maintains an ensemble of classifier models, each trained on different dynamically determined chunks. Both of classification and confi-dence value calculation in our framework are semi supervised. So, the proposed framework can work with delayed labeling and partially labeled training data. To the best of our knowledge, our framework is the first semi supervised approach which addresses both of concept drift and concept evolution using dynamically determined chunk boundaries.
 The primary contributions of our work are as follows: 1) We present a technique to estimate classifier confidences in predicting labels of stream data instances. 2) We propose a change detection approach which takes classifier confi-confidence. It detects the chunk boundary dynamically if there is a significant change, which triggers updating of the existing classifier on recent labeled train-ing data. Unlike other adaptive sliding window techniques which detect changes in error rates of the classifier, our approach does not need true labels of all the data instances for determining the chunk boundary dynamically. 3) We present a semi supervised framework by integrating classification and novel class detec-tion technique with the change detection approach to address both of concept drift and concept evolution using dynamic chunk sizes. 4) We implement and evaluate our proposed framework on several benchmark and synthetic data sets. Results from the experiments show that, our framework outperforms other state of the art approaches for data stream classification and novel class detection. some related works. Section 3 describes our approach in detail. We describe the data sets, evaluation metrics and present experiment results in Section 4 . Finally, Section 5 concludes the paper. Typically, existing data stream classification approaches address infinite length ing the perfect decay function for gradual forgetting are challenging tasks if the information on time-scale of change is not available [ 8 ]. Unlike these approaches, we determine the chunk size dynamically based on a significant change in clas-sifier confidence in predicting labels of test data instances.
 ing change in the posterior distribution of the classes given the features another one is detecting change in the generating distribution literature, several methods[ 11 , 12 ] exist to deal with change of mensional data. However, detecting change in P ( X ) is a hard problem especially in case of multi-dimensional data and does not work well to detect change of con-cept in multi-dimensional multi-class data streaming context [ 13 ]. In this paper, we focus on detecting changes in one dimensional classifier confidence values. 8 , 13 ], which are mostly based on loss estimation of a predictor performance. These approaches track any significant change in classifier error rate over time which requires the true labels of the data instances. Instead of using the error rate, we calculate classifier confidence in the prediction. Monteith et al. propose a method to estimate classifier confidence in [ 14 ], but they use confidence scores only for weighted voting. Unlike this approach, we use confidence scores both for weighted voting and for determining chunk boundaries dynamically. We use two sample t-test for one sided right tail hypothesis testing to detect changes in classifier confidence scores. Our proposed framework uses this change detec-tion technique to address both of concept drift and concept evolution problems where [ 7 , 8 , 13 ] address only the concept drift problem. Unlike most of the above approaches, both of the classification and change detection of our approach are semi supervised in nature. is a non trivial task without prior knowledge on time-scale of change [ 8 , 10 ]. Moreover, approaches which use dynamic sliding window using change detection techniques [ 7 , 8 , 13 ] are based on loss estimation of predictor performance. This estimation needs true labels of the data instances to calculate the predictive per-formance. However, in the real world data streams, labeled data is scarce and not readily available. The above mentioned approaches might suffer in these scenario.
 In this paper, we present a complete framework SCDMiner (Adaptive S emi supervised C oncept D rift Miner with novel class detection and delayed labeling) for classifying evolving data streams with novel class detection. It predicts the label of a data instance along with a confidence behind this prediction. Moreover, we also propose a change detection technique which takes these confidence values as input and detects any significant change in the classifier confidence over the time. If a significant change is detected, chunk boundary is determined and the classifier is updated using only the recent labeled data. In this way, SCDMiner addresses different challenges of data stream mining discussed in Section 1 . Figure 1 depicts the high level workflow of SCDMiner . It maintains an ensemble of L classification models and a dynamic window W sifier confidence scores in predicting labels of data instances in the stream. Let { M , ..., M data stream arrives, label for this instance is predicted by the current ensem-ble along with a confidence score which is inserted into W change detection technique is executed on W . If it detects a significant change in the confidence scores, i.e., values stored in W , SCDMiner determines the chunk boundary which contains all the instances corresponding to the values stored in
W . A new model is trained on the instances which are already labeled in this chunk, and the ensemble is updated by including the newly trained model. On the other hand, if the change detector finds no significant change in the confidence scores, the current ensemble is retained and W keeps growing. Since, SCDMiner tracks changes in the confidence values instead of the predictive performance, so it does not need all the true labels immediately after the prediction. 3.1 Classification and Novel Class Detection SCDMiner uses similar techniques as ECSMiner [ 2 ] for classification and novel class detection. A k -NN based classifier is trained with the training data. Rather than storing the raw training data, K clusters are built using a semi-supervised K -means clustering, and the cluster summaries (mentioned as pseudopoint s) of each cluster are saved. These pseudopoints constitute the classification model. The summary contains the centroid , radius ,and frequencies of data points be-longing to each class. The radius of a pseudopoint is equal to the distance between the centroid and the farthest data point in the cluster. The raw data points are discarded after creating the summary. Therefore, each model K pseudopoints. A test instance x be the pseudopoint whose centroid is nearest from x j . The predicted class of is the class that has the highest frequency in h . A confidence score between 0 to 1 is calculated based on certain criteria (will be discussed shortly) which is used as the weight of this prediction. The data point x j ensemble M by taking a weighted majority vote among all the classifiers. corresponding centroid and radius. The decision boundary of a model fore the union of the feature spaces encompassed by all pseudopoints decision boundary of the ensemble M is the union of the decision boundaries of all models M i  X  M . If a test instance is outside of the ensemble decision bound-ary, it is declared as an F-outlier , or filtered outlier. These are potential novel class instances, and are temporarily stored in a buffer buf they are close to each other ( cohesion ) and farther apart from the data points of other classes ( separation )[ 2 ]. A new class is declared if there are sufficient number of F-outlier s fulfilling these conditions. 3.2 Calculation of Confidence Scores instance to calculate confidence of each individual model. Finally, we combine all these individual model confidences to calculate the overall confidence of the ensemble classifier. Assuming h is the closest pseudopoint from labeled data instance x in model M i , our proposed confidence estimators are as follows:  X  Association is calculated by R h  X  D i ( x ), where R h  X  Purity is calculated by N m /N s , where N s is the sum of all the frequencies and N m is the highest frequency in h .  X  Representativeness is calculated by N s /N t , where N t training instances used to build M i and N s is the sum of all the frequencies in h .
 Association , Purity and Representativeness of the model M contribute to the final confidence in prediction of a model according to their estimation capability. We measure this capability by calculating the correlation coefficient between confidence estimator values and classification accuracy for each model M i using the labeled training instances as follows.
 confidence estimator values for each of the labeled training instances. Let the value of j th confidence estimator in M i  X  X  classification of instance we use three confidence estimators, j  X  X  1 , 2 , 3 } . Let  X  on instance k and y k be the true label of that instance. Let containing v k i values indicating whether the classification of instance M is correct or not. In other words, v k i =1if  X  y k i = y Finally, correlation vector r i is calculated for model M which are pearson  X  X  correlation coefficients between h ij Correlation coefficients calculated in the training phase are used for classifica-tion and confidence estimation during testing phase as follows. First, SCDMiner calculates confidence estimator values h x i for a test instance confidence value of model M i in predicting test instance taking the dot product of h x i and v i , i.e., c x i = h culates confidence value of each of the models in the ensemble along with the prediction for each test instance. Each confidence value is normalized between 0 and 1. Normalized confidence value is treated as the weight of the prediction  X  y by model M i . Finally, to estimate confidence of the entire ensemble denoted by c , SCDMiner takes the average confidence of the models in the ensemble towards the predicted class. 3.3 Change Detection and Updating the Ensemble As discussed earlier, SCDMiner maintains a variable size window itor confidence scores of the ensemble classifier on recent data instances. The expectation is, the size of W will increase during stable period, and will decrease when there is a concept drift. The basic intuition behind this is, concept drift or concept evolution causes change of class boundaries which worsens performance of the classifier if not updated timely [ 15 ]. Confidence estimators are chosen in such a way that estimator values are expected to be decreased if class bound-aries are changed. For example, if class boundaries are changed due to concept drift, more recent instances will be nearby the decision boundary or outside of the decision boundary of the ensemble classifier. So, in case of these instances, classifier models will have low association values. A change detection algorithm is therefore applied on the confidence values stored in W change in classifier confidence scores. If a change is detected, the base learning algorithm is invoked to build a new model on data instances corresponding to the current window W and subsequently W is shrinked. On the contrary, if no change is detected, W keeps growing indicating a stable period.
 Algorithm 1.. Change detection algorithm size window W is maintained as follows. After inserting each confidence value of the ensemble classifier, our change detection technique divides windows. Let W b and W a are two sub windows within W , where performance values on more recent data instances than W b algorithm detects any significant change of statistical properties between con-tents of the sub windows for all possible combinations of sufficiently large and W b (Lines 6 to 15 ). By mentioning sufficiently large , we mean that each of the sub windows must have atleast number of values. We use one tenth of the size of W as in our experiments.
 erty  X  between elements of W b and W a . In this paper, we use the mean of the population as  X  .Let  X  a and  X  b be the mean of population of distribution in W a and windows, i.e., D =  X  b  X   X  a . Since we want to detect the case where greater average confidence value than W a , i.e., decreasing classifier confidence, we perform a one sided right tail hypothesis testing. In this hypothesis testing, Null Hypothesis is D  X   X  ; in other words  X  b is at most  X  contrary, Alternative Hypothesis is D&gt; X  ; in other words and difference between them is more than  X  . Here,  X  is a small real number and an user defined parameter.
  X  X a and According to Central Limit Theorem, if sample size n is large, distribution with expectation  X  and variance  X  2 /n , where the population. Since we expect each of the sub windows W sufficiently large number of values and true variance of the distribution of these values are unknown, we perform a two-sample t-test for the hypothesis testing. Let n and m be the number of values in W for our case is the following-Where s a 2 and s b 2 are the sample variances of elements in tively. Since the samples in W a and W b are not paired, i.e., independent samples, the test statistic in Equation 1 follows a Student X  X  t-distribution with degree of freedom  X  , which is the Null Distribution in our case. The value imated using the following Satterthwaite X  X  Formula. To limit the possibility of false positives, we use a small value  X  as the level of significance so that Pr [ Reject H t  X  t confidence values stored in W and drop the sub window W b a new model is trained, it replaces the oldest model in the ensemble classifier. This ensures that we have exactly L models in the ensemble at any given point of time. We evaluate our proposed approach SCDMiner both on several benchmark real world and synthetic data sets. In this section, we present and analyze the exper-iment results. 4.1 Data Sets We use three real and four different types of synthetic data sets to test perfor-mance of SCDMiner along with some other baseline approaches. Table 1 depicts the characteristics of the data sets.
 ForestCover [ 16 ] contains geospatial descriptions of different types of forests. We normalize the data set, and arrange the data in order to prepare it for novel class detection so that in any chunk at most three and at least two classes co-occur, and new classes appear randomly. In PAMAP [ 17 ], nine persons were equipped with sensors that gathered a total of 52 streaming metrics features whilst they performed activities. Electricity [ 16 ] data set contains data collected from the Australian New South Wales Electricity Market.
 SynCN (Synthetic Data with Concept-Drift and Novel Class) is a synthetic data set generated using the following equation: d i =1 a in [ 2 ]. SynRBF@X are synthetic data sets generated using RandomRBFGener-atorDrift of MOA [ 18 ] framework where X is the Speed of change of centroids in the model. We generate three such data sets using different efficiently different approaches can adapt to a concept drift.
 cept drift and novel classes. On the contrary, rest of the data sets are used to test only concept drift capturing ability of different approaches. 4.2 Experiment Setup We implement SCDMiner in Java version 1.7.0.51. To evaluate performance, we use a virtual machine which is configured with 8 cores and 16 GB of RAM. The clock speed of each virtual core is 2.4 GHZ .
 roach SCDMiner with ECSMiner [ 2 ]. We choose ECSMiner since it is one of the most robust and efficient frameworks available in the literature for classi-fying data streams having both concept drift and concept evolution. However, ECSMiner uses fixed chunk size where our proposed approach uses variable chunk size based on the change in classifier confidence.
 (OBA) and Adaptive Hoeffding Tree (AHT) implemented in MOA [ 18 ] frame-work, since these approaches seem to have superior performance than others on the data sets used in the experiments. Both of OBA and AHT use ADWIN [ 8 ]as the change detector. These approaches do not have novel class detection feature. So, we compare these approaches with SCDMiner only in terms of classification performance.
 ing. To evaluate SCDMiner and ECSMiner , we use 50 pseudopoints, ensemble size 6, and 95% of labeled training data as suggested in [ 2 ]. On the contrary, we use 100% labeled training data in case of OzaBagAdwin (OBA) and Adaptive Hoeffding Tree (AHT) , since training and updating of these approaches are fully supervised. 4.3 Performance Metrics Let FN = total number of novel class instances misclassified as existing class, FP = total number of existing class instances misclassified as novel class, total number of novel class instances correctly classified as novel class, total number of existing class instances misclassified (other than number of novel class instances in the stream, N = total number of instances the stream. We use the following performance metrics to evaluate our technique: 1. ERR : Total misclassification error (percent), i.e., ( FP + FN + Fe ) 2.

M 3.

F 4.

F 4.4 Classification Performance As discussed earlier, SCDMiner avoids unnecessary training during stable period and frequently updates the classifier when needed using dynamically deter-mined chunks. As an instance, with increasing speed of change of centroids X in SynRBF@X data sets, our change detection technique helps SCDMiner to update the ensemble classifier more frequently to cope up with increasing concept drift. SCDMiner creates 111 and 160 number of chunks while classify-ing SynRBF@0.001 and SynRBF@0.003 data sets respectively where ECSMiner creates same 69 number of chunks in both of the cases. We do not report the number of chunks for all the data sets due to limited space in this paper. Table 2 summarizes the classification error of the techniques on each data set described in Section 4.1 . In almost all the cases, our proposed approach SCD-Miner clearly outperforms all the other approaches by large margin in terms of classification accuracy. For example, in case of ForestCover data set, SCDMiner shows around 38% , 89% and 85% better performance than ECSMiner , AHT and OBA respectively. Only in case of SynCN data set, ECSMiner shows slightly better performance than SCDMiner . It can be observed that, in case of data set, all the approaches show comparatively better result than the other data sets which indicates that SynCN data set contains less frequent concept drift. Since, ECSMiner uses fixed chunk size, it updates the model more frequently during stable time period comparing with SCDMiner . From the experiment, we know that SCDMiner updates the ensemble classifier only 7 times comparing with 44 number of updates by ECSMiner .So, ECSMiner gains slightly better accuracy in expense of more frequent training and updating the ensemble. 4.5 Novel Class Detection Table 3 summarizes novel class detection performance of SCDMiner and ECS-Miner on different data sets. From the experiment data, it is clear that SCD-Miner outperforms ECSMiner by a large margin based on F 2 the data sets except SynCN . For example, in case of ForestCover SCDMiner shows 8% better performance than ECSMiner in terms of sure. In case of SynCN data set, SCDMiner shows competitive performance. ECSMiner gains slightly better performance due to more frequent updates as discussed above. In this paper, we present a framework SCDMiner which addresses most of the challenges of classifying evolving data streams. It exploits a change detection technique to determine the chunk boundaries dynamically. As a result, SCD-Miner determines number of training based on the frequency and intensity of concept drift. Results from the experiments show that, SCDMiner outperforms other approaches in the stream mining domain in terms of both classification and novel class detection accuracy.

