 The primary purpose of news articles is to convey infor-mation about who, what, when and where. But learning and summarizing these relationships for collections of thou-sands to millions of articles is difficult. While statistical topic models have been highly successful at topically sum-marizing huge collections of text documents, they do not ex-plicitly address the textual interactions between who/where, i.e. named entities (persons, organizations, locations) and what, i.e. the topics. We present new graphical models that directly learn the relationship between topics discussed in news articles and entities mentioned in each article. We show how these entity-topic models, through a better un-derstanding of the entity-topic relationships, are better at making predictions about entities.
 G.3 [ Probability and Statistics ]: probabilistic algorithms; H.4 [ Information Systems Applications ]: Miscellaneous Algorithms Text Modeling, Topic Modeling, Entity Recognition
News articles aim to convey information about who, what, when and where. Statistical topic models can not distin-guish between these different categories and produce top-ical descriptions that are mixtures of whos, whats, whens and wheres. But in many applications it is important for these different concepts to be explicitly modeled. In this  X  Corresponding author Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. paper we consider the problem of modeling text corpora where documents contain, in addition to ordinary words, additional classes of words (referred to as entities). For our study of news articles, the entities can be persons (e.g.  X  X eorge Bush X ), organizations (e.g.  X  X FL X ), and locations (e.g.  X  X ondon X ). Our focus is on modeling entities and making predictions about entities based on learning that uses entities and words.

Many statistical topic models are based on the simple idea that individual documents are made up of one or more top-ics, where each topic is a distribution over words (e.g. Blei et al. X  X  Latent Dirichlet Allocation (LDA) model [3]). These models can efficiently describe a collection by a set of topics, retrieve information, classify documents and be used in pre-diction tasks. There have been several applications showing the power of these models on a wide variety of text collec-tions (e.g. Enron emails, CiteSeer abstracts, Web pages).
In language modeling and information extraction, there is growing interest in finding and analyzing entities mentioned in text. These entities are usually proper nouns, i.e. persons, organizations or locations. We take advantage of recent de-velopments in named entity recognition systems to identify and extract entities mentioned in news articles. Rather than build on research in named entity recognition and entity res-olution, we take this line of work in a different direction. We are primarily interested in modeling and making predictions on entities, once they have been identified in the text.
In this paper, we review a series of graphical models that extend LDA to explicitly treat and model entities mentioned in text. We introduce two new models, and compare a total of five different entity-topic models that have fundamentally different generative processes. We demonstrate two primary results: (i) our proposed CorrLDA2 model has better ability to predict entities than LDA, and (ii) words can be leveraged to improve predictions about entities by up to 30%.
Researchers have furthered Blei et al. X  X  original LDA model in the directions of algorithm development, applications, and model extensions. In algorithms, Griffiths and Steyvers [8] proposed the now popular Gibbs sampling method for in-ference. There has been a wide variety of extensions to the original LDA model including Steyvers et al. X  X  author-topic model [13]; McCallum et al. X  X  author-topic-recipient model [11]; Griffith et al. X  X  hidden-Markov topic models for sep-arating semantic and syntactic topics [9]; Blei X  X  correlated topic model [2]; and Buntine X  X  PCA models [5].

In this paper we are specifically interested in the inter-section of topic modeling and entity modeling, and in this context there are two branches of closely related work. In topic modeling, several researchers have extended basic topic models to include other information (beyond the words) contained in individual text documents. Steyvers et al. X  X  author-topic model uses a document X  X  authorship informa-tion together with the words to learn models that relate authors, topics and documents [13]. Using Gibbs sampling, they applied their Author-Topic model to a collection of CiteSeer abstracts to infer relations between authors, topics and words. Blei and Jordan [1] modeled collections of images and their captions. In this case the caption contained the words, and the image (represented as a set of real-valued fea-tures) was the other information. Using the Corel database of captioned images, they used variational EM to estimate parameters, and learn relations between images and text. Erosheva et al. [7] combined paper abstracts with bibliogra-phy information to create a mixed-membership model that identifies categories of publications. Also using variational EM, they identified categories for a collection of PNAS bio-logical sciences publications. While this work models words and other objects, it does not specifically address entities.
Different aspects of entity analysis include named entity recognition; entity resolution; and social networks built on entities. Probabilistic modeling approaches have been ap-plied to all of these. For example, McCallum et al. [12] use conditional random fields for noun co-references. Zhu et al. use non-probabilistic latent semantic indexing to recognize named entities and find relationships between named enti-ties in Web pages [14]. Our focus in the present work is to use simple and effective named entity recognition tech-niques to extract entities as a preprocessing step prior to our primary goal or relating entities, topics and words.
To analyze entities and topics, we require text datasets that are rich in entities including persons, organizations and locations. News articles are ideal because they convey infor-mation about who, what, when and where. Our first data set is a collection of New York Times news articles taken from the Linguistic Data Consortium X  X  English Gigaword Second Edition corpus (www.ldc.upenn.edu). We used all articles of type  X  X tory X  from 2000 through 2002. These include articles from the NY Times daily newspaper publication as well as a sample of news from other urban and regional US news-papers. Our second data set are articles from the Foreign Broadcast Information Service (FBIS), which comes from www.fbis.gov or wnc.dialog.com. FBIS articles come from around the globe, and include English translations of a va-riety of foreign (and open source) news. We used a set of FBIS articles spanning Feb 1999 to Nov 2000.

We automatically extracted named entities (i.e. proper nouns) from each article using existing named entity recog-nition tools. We evaluated two tools including GATE X  X  Information Extraction system ANNIE (gate.ac.uk), and Coburn X  X  Perl Tagger (search.cpan.org/  X  acoburn/Lingua-EN-Tagger). ANNIE is rules-based and makes extensive use of gazetteers, while Coburn X  X  tagger is based on Brill X  X  HMM part-of-speech tagger [4]. ANNIE tends to be more conser-vative in identifying proper nouns. The negative effect of imperfect entity recognition is vastly reduced by the topic models X  ability to handle synonymy and polysemy.
 For the NY Times 1 data, entities were extracted using Coburn X  X  tagger. For this 2000-2002 period, the most fre-quently mentioned people were: George Bush; Al Gore; Bill Clinton; Yasser Arafat; Dick Cheney and John McCain. For the FBIS data, only people entities were extracted using GATE X  X  ANNIE. Locations were omitted because long lists of countries were included at the top of every FBIS arti-cle. For this 1999-2000 period, the most frequently men-tioned people were: Bill Clinton, Vladimir Putin, Jiang Zemin, Boris Yeltsin, Slobodan Milosevic, Keizo Obuchi, Boris Berezovskiy, and Ehud Barak.

After tokenization and removal of stopwords, the vocab-ulary of unique words was filtered by requiring that a word occur in at least ten different news articles. We produced a large NY Times data set containing 330,000 documents, a vocabulary of 44,000 unique words, a list of 59,000 entities, and a total of 112 million word and entity tokens. After this processing, entities occur at the rate of 1 in 8 words (not counting stopwords). For experiments, we used 29,000 docs from July-Sept in 2000 and 2001 for training, and 11,000 docs from July-Sept 2002 for testing, creating an challeng-ing temporal gap between training and testing. The final FBIS dataset was similarly processed, resulting in a total of 11 million word tokens. In FBIS, entities are far sparser, only occurring at the rate of 1 in 40 words.
In this section we describe five graphical models for entity-topic modeling. We start with LDA, and follow with more complex models that aim to better fit our multi-class text data of words and entities. Three of the models have been proposed by other researchers; we introduce two new mod-els, namely SwitchLDA and CorrLDA2. Here we introduce some notation: D is the number of documents, T is the number of topics, N d is the total number of tokens in doc-ument d (with N d = N w d + N  X  w d , the sum of all the words plus entities),  X  and  X  are Dirichlet smoothing parameters,  X  is the topic-document distribution,  X  is the word-topic dis-tribution, z i is a topic, v i is a word or entity, while w word and  X  w i is an entity. A tilde is used to denote the entity version of that variable.
To introduce the notation and explain the differences in the five graphical models, let us start with the LDA model, whose generative process is: 1. For all d docs sample  X  d  X  Dir(  X  ) 2. For all t topics sample  X  t  X  Dir(  X  ) 3. For each of the N d words v i in document d :
The learning algorithm for LDA follows the Gibbs sam-pling approach described in [8]. The learning algorithm for the other four models uses an analogous Gibbs sampling ap-proach. Note that LDA does not distinguish between words and entities, this distinction is made post-hoc (i.e. not dur-ing learning), when we make predictions about entities.
The conditionally-independent LDA model (CI-LDA) ex-plicitly makes an a priori distinction between words and en-tities during learning. It is an obvious modification to the LDA model to handle multiple classes of word tokens, in our case words and entities . Cohn and Hofmann described a similar model to relate Web pages and their links [6]. The generative process is very similar to LDA X  X , except that we generate N w d words and N  X  w d entities in document d .CI-LDA X  X  generative process is: 1. For all d docs sample  X  d  X  Dir(  X  ) 2. For all t topics sample  X  t  X  Dir(  X  )and  X   X  t  X  Dir( 3. For each of the N w d words w i in document d : 4. For each of the N  X  w d entities  X  w i in document d :
One issue with CI-LDA is that it is not truly generative; every document d contains some arbitrary number of words N w d and entities N  X  w d . It is more natural to have this distri-bution of entities in a document be part of the process itself. In our proposed SwitchLDA model we include an additional Binomial distribution  X  (with a Beta prior of  X  ) which con-trols the fraction of entities in topics. The generative process for SwitchLDA is: 1. For all d docs sample  X  d  X  Dir(  X  ) 2. For all t topics sample  X  t  X  Dir(  X  ),  X   X  t  X  Dir(  X  3. For each of the N d words v i in document d
One appealing feature about CI-LDA and SwitchLDA is that they are independent of ordering of the word-token classes. They are also easily generalized to handle n -classes of word tokens (for example, we may explicitly model words, people, organizations and locations).
We have found in practice that CI-LDA X  X  word topics and entity topics can be too decoupled. To force a greater de-gree of correspondence between word and entity topics we use the CorrLDA1 model. This model first generates word topics for a document. Then only the topics associated with the words in the document are used to generate entities, resulting in a more direct corre lation between entities and words. This CorrLDA1 model is essentially the same as Blei and Jordan X  X  Corr-LDA model used for Image/Caption data [1]. The generative process for CorrLDA1 is: 1. For all d docs sample  X  d  X  Dir(  X  ) 2. For all t topics sample  X  t  X  Dir(  X  )and  X   X  t  X  Dir( 3. For each of the N w d words w i in document d : 4. For each of the N  X  w d entities  X  w i in document d : Finally we introduce the CorrLDA2 model, which is like CorrLDA1 but with word topics including a mixture of en-tity topics (not individual entities). The intuition is that word topics often relate to different groups of entities; say a word topic of sports may contain entity topics of NFL teams, NBA teams, and Baseball teams. Another key difference is that CorrLDA2 allows different numbers of word topics, T , and entity topics,  X  T . The generative process for CorrLDA2 is: 1. For all d docs sample  X  d  X  Dir(  X  ) 2. For all t =1 ...T word topics sample  X  t  X  Dir(  X  )and 3. For all t =1 ...  X  T entity topics sample  X   X  t  X  Dir( 4. For each of the N w d words w i in document d : 5. For each of the N  X  w d entities  X  w i in document d :
We point out that for CorrLDA1 and CorrLDA2 there is an ordering to the classes of word tokens (for our applica-tions using news articles we treat words as the primary class of tokens). Again, both CorrLDA1 and CorrLDA2 can be generalized to handle more than two classes of tokens.
We can use entity-topic models for multiple purposes. Be-yond learning topics, they can infer and explain (latent) rela-tionships between entities mentioned throughout a collection of text documents. They can also make predictions about entities outside a collection, based on varying amounts of additional information. In the entity prediction task, the models are first trained on words and entities. The models then make predictions about entities in the test set using the words in the test set. In the entity-pair classification task, models are trained on words and entities or just enti-ties. The models then make predictions about whether an entity pair is actual or fabricated.
 Figure 1: Selected topics from a 400-topic LDA run of the 3-year NY Times data. In each topic we list the most likely words in the topic with their proba-bility, and below that the most likely entities.

For all of these tasks, single samples were taken  X  after 400 iterations  X  from 10 randomly-seeded runs. We found 400 it-erations to be sufficient by monitoring in-sample perplexity every 10 iterations, and observing some degree of conver-gence (i.e. flattening of the log likelihood). Samples were averaged before predicting entities or classifying pairs. We ran T=200 topics for all experiments, and included T=100 topic runs for entity pair classification, and a T=400 topic run for the 330,000-document NY Times 1 data. We deter-mined that Dirichlet priors  X  =0 . 1and  X  =0 . 01 maximized test set likelihood on the NY Times 2 data, and fixed these values for all experiments.
Topics from LDA, CI-LDA, SwitchLDA and CorrLDA1 contain distributions over words, and over entities, while topics from CorrLDA2 contain distributions over words, and over entity topics . Three topics from the 3-year NY Times 1 data are shown in Figure 1. The Sept. 11 topic is clearly about the breaking news describing what and where, but not who (i.e. no mention of Bin Laden). The FBI Investigation topic lists 9/11 hijackers Mohamed Atta and Hani Hanjour, while the Harry Potter/Lord of the Rings topic combines these same-genre runaway successes.

Recall that an advantage of CorrLDA2 is that it can group related entities, and assign these entity groups to word top-ics. We illustrate this ability by showing CorrLDA2 topics from the NY Times 2 data in Figures 2 and 3. Five loosely-related word topics about Sept. 11 and Washington contain mixtures of five entity topics that span different groups of entities, from ones specifically related to Sept. 11 (World-Trade-Center), to ones related to US security (NSC, CIA). The Computers topic contains just a single entity topic of computer manufacturers. The Arts topic neatly includes two separate groups of entities; one relating to theater, and one relating to music. This illustrates a useful feature of the CorrLDA2 model, namely the ability to naturally separate two sets of entities (theatre and music in this case) that are associated with a single word topic. Figure 2: Sept. 11 and Washington-related word topics and entity topics from a 200-topic CorrLDA2 run of the 6-month NY Times 2 data. The word top-ics include a mix of entity topics (not entities). The lower level shows the entities in each entity topic. Figure 3: Computer and Arts-related word topics and entity topics from a 200-topic CorrLDA2 run of the 6-month NY Times 2 data.
 Table 1: Predicting entities in a 7/2/02 Cox News-papers article (NY Times News Service). The top box shows an excerpt from the article, with redacted entities indicated by XXXX. Below that we alpha-betically list (for evaluation purposes) all the en-tities that are mentioned in the article. The bot-tom shows the most likely entities predicted by the model, with matches underlined. actual entities: afghanistan allah britain darwin eunice-moscoso europe fbi federal france germany god hamburg jihad minnesota osama-bin-laden pennsylvania pentagon u-s u-s-district-court united-flight united-states virginia world-trade-center zacarias-moussaoui predicted entities: fbi united-states afghanistan u-s taliban pakistan washington osama-bin-laden america federal new-york pentagon
Since we are primarily interested in modeling and making predictions about entities, we evaluated all five models on a specific entity prediction task. In this entity prediction task, the models were first trained on words and entities. The models then make predictions about entities in each test set document using some or all of the words in each test set document. The likelihood of an entity in an unseen test document is p ( e | d )= t p ( e | t ) p ( t | d ), where p ( e learned during training, and the topic mix in the test doc-ument p ( t | d ) is estimated by resampling some or all of the test document words using the saved p ( w | t ) word distribu-tion. We illustrate this process in Table 1 using an example from the NY Times 2 data. The table shows an excerpt from a 7/2/02 news article about the Sept. 11 attacks. The top box shows an excerpt from the article, with redacted entities indicated by XXXX. Using the model parameters learned in training, the models compute, using all or some of the words in this top box, the likelihood of every possible entity (10,000 entities for NY Times, 5,700 entities for FBIS). The bottom box lists these predicted entities in order of likelihood, and matches with actual entities are underlined. We then de-termine from the list of actual entities, the highest (best) ranked, lowest ranked and median rank. For this example, the top predicted entity ( X  X bi X ) is an actual entity, so the best rank is 1. These best and median ranks are averaged over all the documents in the training set (11,000 docs for NY Times, 5,000 docs for FBIS).

Our proposed CorrLDA2 model gives a 7% improvement in average best rank, and a 4% improvement in average me-dian rank over the standard LDA model for the NY Times 2 data. This average is computed over 11,000 test documents. The remaining three models fall in between LDA and Cor-rLDA2 (Table 2). Note that random guessing would pro-duce an average median rank of 5000 (since there are a total of 10,000 unique entities). Articles contain on average 18 different entities, so a median rank around 400 seems rea-
Table 2: Entity prediction results for NY Times. Figure 4: Average best rank versus number of ob-served words for NY Times 2 data. sonable (relative to an average best rank of 20). This slight improvement of CorrLDA2 over LDA is seen even when we only partially observe the words in the test document. Fig-ure 4 shows that when just 8, 32 and 128 randomly selected words are chosen from each document, CorrLDA2 consis-tently produces better entity rankings than LDA (note that documents contain on average 300 words).
It is useful to be able to compute the likelihood of a pair of entities co-occurring in future documents, particularly if they have previously never been seen together [10]. Our entity-topic models can infer relationships between entities, even when those entities never appear together in any doc-ument. We measure this relationship using the entity-entity t p ( e i rameters.
 For this experiment, we generate two sets of entity pairs. The first set ( X  X rue pairs X ) contains pairs that were never seen in any training document, but were seen in test doc-uments. Some examples include breaking news stories in 2002 about Martha Stewart &amp; ImClone; and Jack Grub-man &amp; WorldCom; both these pairs relate to events that occurred after the 2000-2001 period containing the training documents. The second set ( X  X alse pairs X ) contains pairs that were never seen in any training or test document. To control for entity frequency, the false pairs set is generated from the true pairs set, by a random permutation of the second entity in each pair. So while the pair Tony Blair &amp; Johnny Adair (loyalist leader), is seen in the FBIS test doc-uments, the pair Tony Blair &amp; Francoise de Panafieu (city councilor in Paris) never appear together.

Table 3: Classification accuracy of entity pairs.
The N true pairs and N false pairs are combined into one list, and we compute the ( e 1 ,e 2 ) affinity for each of the 2 N pairs, and save the median value. Given the equal numbers of true and false, we classify as true, pairs whose affinity is above the median, and classify as false, pairs whose affinity is below the median. For the NY Times 2 data, N = 2000 and for the FBIS data, N = 200 (the lower FBIS number is due to the much greater sparsity of entities in FBIS which create fewer pairing possibilities).

An important question is whether the text (i.e. the non-entity words) available during training improves classifica-tion accuracy. For both the NY Times and FBIS data, we trained the LDA model using (i) both words and entities, and (ii) just the entities. The prediction accuracy for T=100 and T=200 topics (Table 3) show several results: all models and data sets do better than random guessing; using words &amp; entities improves accuracy by 3% to 30% over using just entities; and 100 topics gives uniformly better accuracy than 200 topics. The dependency on number of topics is a simple case of overfitting. The accuracy improvement (by up to 30%) by training on words and entities is a powerful result  X  it tells us that words that tend to co-occur with entities (and therefore topics) help us better understand and model these entities. In the FBIS dataset, where entities made up only 2.5% of the written words this improvement was the most dramatic. This can be explained by the fact that there is relatively limited info rmation in the entities alone; the 40-fold boost in data from including words  X  even ones that are not relevant  X  clearly drives the ability to topically characterize an entity and therefore make better predictions about the connection of that entity to other entities. As a footnote, we mention that we chose LDA for this classifica-tion task as a matter of convenience and because we did not expect much difference by using the other models.
Given a collection of news articles, we can create a social network by aggregating, for each article, the co-mentions of entities. For example George Bush and Saddam Hussein co-appeared in over 400 news articles in the NY Times 2 training data of 29,000 articles. But can we infer latent entity-entity relationships purely based on topical informa-tion associated with each entity? And what if our named entity recognition and entity resolution system misses enti-ties or produces slight variations in the entity string? As describedintheSectiononclassificationofentitypairs,we can measure entity-entity affinity from our trained entity-topic models.

A latent entity-entity network based on the 400-topic LDA run of 3-year NY Times 1 data shows associations between pairs of entities that never appeared in any single news arti-cle (Figure 5). We see Edmond Pope (American convicted of espionage in Russia) connected to Boris Berezovsky (a Rus-sian businessman). While these two people never appeared together in any one of our 330,000 news articles, a Google search for this pair indicates a close connection. Ayman al-Zawahiri (Al-Qaeda, Bin Laden X  X  physician) and Wadih el-Hage (Al-Qaeda, 1998 US embassy bombings) are also never co-referenced, but have an obvious association.
Not only can our entity-topic models identify connections in these latent social networks, they can also topically de-scribe the nature of the connection between any two entities, and ultimately provide the evidence (as a list of most rele-vant documents) that supports the connection. The statis-tical nature of these models alleviates the problem of imper-fect entity resolution, and, by leveraging word information, can discover relationships even when entity mentions are relatively scarce.
We have developed two new graphical models  X  CorrLDA2 and SwitchLDA  X  specifically designed for text data that contains words and entities (e.g. persons, organizations, lo-cations). We compare these two plus three other models on various entity prediction tasks using two large collec-tions of news articles. We show how one can leverage the latent structure in text to make up to a 30% better pre-diction about entities by learning the relationships between entities mentioned in the text and topics learned from word co-occurrences. For one data set that is rich with entities, our CorrLDA2 model shows an improved ability to predict unseen entities in test documents.

Finally we gave some examples of how this type of entity-topic modeling can be applied to construct social networks of entities based on latent information, showing links between people who never co-appear in any document. Gaining extra knowledge  X  through text  X  about entity-entity relationships is especially useful when entity mentions are sparse.
The models discussed in this paper are all generalizable to handle multiple classes of word tokens in data. For example one could model the interrelationships between words, peo-ple, organizations and locations mentioned in, say, a series of news articles. Other application areas of this entity-topic modeling include medical literature (e.g. PubMed) where one could create entity-topic models where the entities are genes and proteins mentioned in the text.
This work was supported by the National Science Foun-dation under awards I IS-0083489 (as part o f the Knowledge Discovery and Dissemination Program) and ITR-0331707.
Additional author: Mark Steyvers, Department of Cogni-tive Sciences, University of California, Irvine ( msteyver@uci.edu ). [1] D. Blei and M. I. Jordan. Modeling annotated data. In [2] D. Blei and J. Lafferty. Correlated topic models. In co-referenced, but have an obvious association. [3] D. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet [4] E. Brill. Some advances in transformation-based part [5] W.Buntine,J.Lofstrm,J.Perki,S.Perttu, [6] D. Cohn and T. Hofmann. The missing link X  X  [7] E. Erosheva, S. Fienberg, and J. Lafferty.
 [8] T. L. Griffiths and M. Steyvers. Finding scientific [9] T.L.Griffiths,M.Steyvers,D.Blei,andJ.B.
 [10] J. O. Madadhain, J. Hutchins, and P. Smyth.
 [11] A. McCallum, , A. Corrada Emmanuel, and X. Wang. [12] A. McCallum and B. Wellner. Conditional models of [13] M.Steyvers,P.Smyth,M.Rosen-Zvi,andT.Griffiths. [14] J. Zhu, A. Goncalves, and V. Uren. Adaptive named
