 Review assignment is a common task that many people such as conference organizers, journal editors, and grant admin-istrators would have to do routinely. As a computational problem, it involves matching a set of candidate reviewers with a paper or proposal to be reviewed. A common defi-ciency of all existing work on solving this problem is that they do not consider the multiple aspects of topics or ex-pertise and all match the entire document to be reviewed with the overall expertise of a reviewer. As a result, if a document contains multiple subtopics, which often happens, existing methods would not attempt to assign reviewers to cover all the subtopics; instead, it is quite possible that all the assigned reviewers would cover the major subtopic quite well, but not covering any other subtopic. In this paper, we study how to model multiple aspects of expertise and assign reviewers so that they together can cover all subtopics in the document well. We propose three general strategies for solving this problem and propose new evaluation measures for this task. We also create a multi-aspect review assign-ment test set using ACM SIGIR publications. Experiment results on this data set show that the proposed methods are effective for assigning reviewers to cover all topical aspects of a document.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms, Experimentation Topic models, Expert retrieval, Review assignment, Evalu-ation Metrics
Review assignment is a common task that many people such as conference organizers, journal editors, and grant ad-ministrators would have to do routinely. The task usually has to do with assigning a certain number of experts to re-view a research paper or a grant proposal and judge its qual-ity. Since the assignment must be based on both the content of the paper or proposal and knowledge about the expertise of all candidate reviewers, it is not an easy task, and it is quite challenging to optimize all the assignments. Currently such assignments are done manually by editors or conference organizers. Some conference systems support bidding on pa-pers by reviewers, shifting some of the assignment task to the reviewers. But it is still mostly a time-consuming man-ual process. Another deficiency of manual assignment is that the assignment may be biased. For example, in bidding, a popular paper may attract more bids than a non-popular pa-per. Also, the matching of ex pertise may be more accurate for reviewers whose expertise is largely known to an editor or a conference program chair, but less accurate for review-ers whom the assigning person is not very familiar with. For these reasons, it is highly desirable to develop techniques to automatically do this task. Indeed, recently, a prototype system for reviewing proposals has been developed at Na-tional Science Foundation (NSF), which has been shown to improve productivity [9].

As a research problem, automatic review assignment ap-pears to have been first studied by Dumais and Nielsen [6], and later studied by a few other researchers [2, 9, 13]. (For a detailed review of the previous work, please refer to Sec-tion 6.) In most existing works, the problem is cast as a retrieval problem in which the query is a paper or proposal to be reviewed and an expert is represented as a text doc-ument (e.g., based on his/her publications). Then, top N documents are taken as assignments.

One common deficiency of all the existing works is that a paper is matched as a whole unit without considering multi-ple aspects 1 of a paper. As a result, if a document contains multiple subtopics, existing methods would not attempt to assign reviewers to cover all the subtopics; instead, it is quite possible that all the assigned reviewers would cover the ma-jor subtopic quite well, but not covering any other subtopic. Since a paper or proposal often contains multiple subtopics, such simplistic view of matching often leads to non-optimal assignments. For example, if a paper is about applying ma-chine learning techniques to develop new retrieval models for
Aspect or subtopic will be used exchangeably throughout the paper. Web search, ideally it should be reviewed by reviewers with expertise to cover three subtopics, i.e.,  X  X achine learning, X   X  X etrieval models, X  and  X  X eb search. X  However the existing methods may assign three reviewers that are all experts on machine learning but do not know well about Web search.
In this paper, we study how to match reviewers with a paper based on matching of multiple aspects of expertise so that the assigned reviewers would not only have the required expertise to review a paper but also can cover all the aspects of a paper in a complementary manner. This is a challenging task because we have no knowledge in advance about the aspects covered in a paper and it is also unclear how we should select reviewers to maximize their coverage of all the aspects.

We propose three general strategies for multi-aspect re-view assignment: the first one is Redundancy Removal in which we model both relevance and redundancy of review-ers X  expertise and attempt to achieve broad coverage through avoiding assigning reviewers with similar expertise to other assigned reviewers. In other words, we hope to increase the aspect coverage indirectly through eliminating the redun-dancy among reviewers. The second method is Reviewer As-pect Modeling , in which we improve the aspect coverage more directly through modeling the possible aspects in reviewers X  profiles with mixture language models such as PLSA [10]. With the learned expertise/topic aspects, we can represent both a reviewer and a paper or proposal to be reviewed in the space of these aspects and match them at the level of aspect representation. The third method is Paper Aspect Modeling in which we find aspects in a submitted paper. Our general idea is to partition the words in the paper into the same number of subsets as the number of reviewers we would like to assign, and we can then use each subset as a query to retrieve a reviewer. We further propose two dif-ferent ways to implement this general idea, leading to two specific methods: The first method is to partition the words through word clustering based on the mutual information similarity between words. Each cluster of words can then be used to retrieve a reviewer. The second method is to seg-ment the paper based on similarity between sentences and group adjacent sentences that are similar to each other to-gether as a segment. Each segment can then be used to retrieve one reviewer.

Evaluating the review assignment task as we defined (i.e., matching multiple aspects) is challenging in two ways: First, it is unclear how to define appropriate metrics to measure assignment performance, and second, no existing test set is available. To solve these two problems, we propose four evaluation measures for this task and create a multi-aspect review assignment test set using ACM SIGIR publications from years 1971-2007. Experiment results show that all the proposed methods are effective for increasing the coverage of subtopics in review assignment and outperform a state of the art baseline retrieval method. The reviewer aspect modeling method using PLSA is shown to outperform both redundancy removal and paper aspect modeling method.
In summary, our contributions are as follows:
All the proposed methods are general, thus they can be applied to all review assignment tasks, including related ex-pertise matching tasks such as forming a thesis committee for a Ph.D. student (matching reviewers with a thesis) and forming a panel on a topic (matching candidate panelists with a topic description).

The rest of the paper is organized as follows. We first de-fine the problem of multi-aspect review assignment in Sec-tion 2. We then propose several different methods for multi-aspect review assignment task in Section 3. In Section 4, we describe our experimental design and propose evaluation measures. In Section 5, we present our evaluation results. We discuss related work in Section 6 and conclude in Sec-tion 7.
In many ways, the review assignment problem can be viewed as an expert retrieval problem in which the paper is a query and the reviewer is a document , and we want to retrieve k experts to review a paper. There are two basic tasks: 1. Obtain text representation for a reviewer 2. Match reviewers with the paper.

We assume that a reviewer can be represented by his/her sample publications and focus on studying how to perform the second subtask. In particular, we want to study how to match experts with papers based on multiple aspects of expertise. For us, thus, the goal is to retrieve k experts that are not only relevant to the paper but also are able to work together to cover all the aspects of a paper. There are several challenges in doing review assignment based on multiple aspects: 1. Ranking of experts must consider dependency between 2. The notion of multiple aspects is implicit, making it 3. Evaluation of the task is also challenging as the tra-
In this section, we propose three general strategies for solving the multi-aspect review assignment problem: (1) Re-dundancy Removal, (2) Reviewer Aspect Modeling, and (3) Paper Aspect Modeling. Throughout this section, we will use R = { r 1 , ..., r N } to denote the set of text documents representing the expertise of N reviewers, respectively, and Q to denote the text of the paper to be reviewed.
The idea of the redundancy removal approach is to re-duce the redundancy among the assigned reviewers so as to indirectly maximize the coverage of topic aspects by the as-signed reviewers. Specifically, when picking reviewers, we would consider both the relevance value and novelty value (or equivalently redundancy) of a reviewer relative to the other reviewers assigned to the same paper.

One way to implement this strategy is to use a greedy algorithm to gradually select a reviewer for a paper. In each step, given a set of previously picked reviewers, the next best reviewer is the one that is both relevant to the query topic and different from previously picked reviewers. This strategy is often called maximal marginal relevance (MMR) ranking [4]. With this strategy, after we X  X e already selected reviewers r 1 , ..., r i  X  1 ,thescoreofreviewer r i is computed as follows: s ( r i ; r 1 , ..., r i  X  1 )=(1  X   X  ) S R ( r i ,Q )+  X S where Q is the paper to be reviewed, S R ( r i ,Q )isarelevance-based measure and is computed as the negative KL-divergence of the query model estimated based on Q and the document model estimated based on r i [17], S novelty is the novelty value of r i w.r.t. r 1 , ..., r i  X  1 ,andand  X   X  [0 , 1] is a novelty parameter to be empirically set. A larger  X  would cause more aggressive re dundancy rem oval.

To compute the novelty score S novelty ,weuseamethod proposed in [16]. The best performing novelty measure they reported is MixAvg. This novelty measure is based on a two-component generative mixture model in which one compo-nent is the old reference topic model  X  o estimated based on r , ..., r i  X  1 and the other is the background language model (e.g., general English model). Specifically, let  X  B be a back-ground language model with a mixing weight of  X  , the log-likelihood of a new reviewer r = w 1 , ..., w n is: and the estimated novelty score is obtained by: the EM algorithm can be used to find the unique  X   X  that maximizes the score. We denote this method by MMR.
Instead of attempting to indirectly cover multiple topic aspects of a paper to be reviewed through eliminating the redundancy among reviewers (e.g. MMR), an intuitively better strategy may be to directly cover topic aspects of the paper through explicitly modeling subtopics in the publica-tions of reviewers with mixture language models. Indeed, even if we avoid redundancy, there is no guarantee that a new reviewer would cover the uncovered aspects, as the re-viewer can be different from the already selected reviewers because of covering some non-relevant topics. Thus if we can somehow directly model the potential topic aspects and try to match a paper with reviewers based on their topic-aspect representation, we may potentially achieve better results.
To implement this idea, we assume that there is a space of K topic aspects, each characterized by a unigram lan-guage model. Let  X  =(  X  1 ,...,  X  K ) be a vector of topics.  X  is a unigram language model and p ( w |  X  i ) is the probability of word w according to topic  X  i . Consider that we have a set of m reviewer expertise documents (each representing a reviewer). We can then learn these K topic aspects from this set of reviewer expertise documents using a topic model such as PLSA [10], which is described below.
Let R = { r 1 , ..., r N } be the set of reviewer expertise doc-uments and r i be the document for reviewer i . The log-likelihood of the whole collection according to PLSA is: where V is the set of all the words in our vocabulary, c ( w, r is the count of word w in r i ,and p ( a |  X  i ) is the probability of selecting topic aspect  X  a for document r i .Intuitively, p ( a |  X  i ) encodes the coverage of different topic aspects in the expertise document of reviewer i . p ( w |  X  a ) is the probability of word w accordingtotopic  X  a .

We may use the EM algorithm to compute the maximum likelihood estimate of all the parameters including  X  and  X   X  X .

Once we obtain  X  i which is a distribution over all the possible topic aspects p ( a |  X  i )( a =1 , ..., K ), we can think of it as a new way to represent our document r i in terms of topic aspects. With a similar model to the one presented above, we can also estimate the subtopic coverage in our query paper Q with the main difference being that we have  X  already known so we only need to estimate  X  Q which would give us a distribution over subtopics for our query paper, i.e., p ( a |  X  Q ), where a =1 , ..., K .

We now describe how we match a paper with reviewers based on this new subtopic-based representation.
In order to match a paper with a reviewer based on their topic representations, we compute the following Kullback-Leibler (KL) divergence value:
The KL-divergence between two distributions, p and q, is defined as [5]: D ( p || q )= x p ( x )log p ( x ) q ( x ) lows: where  X   X  (0 , 1] indicates how much redundancy we would like to model.

The intuition behind this is that  X  Q gives us a measure of which topic aspect is relevant; for example, a high p ( a indicates that topic aspect a is relatively more relevant (than other aspects). As a result, if  X  Q assigns a high probability to some aspects, then we would like to cover these relevant aspects more than other less relevant ones. Thus, the best r is the one that works together with r 1 ,..., r k  X  1 to achieve a coverage distribution most similar to the topic coverage given by the query. So, the parameter  X  controls how much we want to rely on the previously picked reviewers r 1 ,..., r to cover all topic aspects of the query. When  X  =0, the best r is the one that covers all relevant aspects by itself and if  X &gt; 0, part of the coverage would be explained by previously seen reviewers, and the best r k would be the best that covers those aspects which are not covered yet. Another parameter of this model is the number of aspects we want to have, which will be empirically set.
Instead of learning the subtopics/aspects from reviewer expertise documents, we can also use an alternative strat-egy of modeling aspects based on the paper to be reviewed. The general idea is the following: If we want to assign n re-viewers, we can simply partition the words in the paper into n groups and use each to retrieve an expert. Let q 1 , ..., q queries based on the n partition groups, and r j denote the text representation of the expertise of the j -th reviewer, then for each q i we use the following language modeling approach (i.e., query likelihood) with Dirichlet prior smoothing [18] to retrieve the first top reviewer. where N r j is the number of words in the reviewer representa-tion r j , p ( w | R ) is a background language model estimated using the entire set of reviewer documents R ,and  X  is a smoothing parameter to be empirically set. We then merge the reviewers from all the clusters together. In case two clus-ters nominated the same reviewer, we resolve the conflict by allowing the reviewer to represent whichever cluster gives it ahigherscore.

We present two different ways to partition the words in a paper based on Mutual Information-based Clustering and Simple Discourse Analysis, respectively.
In this method, we use K-Means [12] algorithm to cluster words in the paper into k clusters ( k = 3 in our experi-ments). We can then construct a query language model for each cluster and use it to retrieve a top-reviewer as described above.

Our idea is to cluster the words in a query paper into dissimilar categories, i.e., the resulting intracluster similar-ity is high but intercluster similarity is low. The similar-ity measure between words in the query is computed based on the Mutual Information [5] between each pair of words. Mutual Information measures the tendency of two words co-occurring in a collection. This similarity measure is the ba-sis of the clustering algorithm; the similarity between word w and a cluster is computed as the mean of the similar-ity between word w and all the words in the cluster. We then, retrieve one reviewer for each cluster. In this case, we can ensure that reviewers with diverse expertise are re-trieved (due to dissimilarity between clusters), so that these retrieved reviewers can work together to cover all topics of the query.
Another way for finding aspects in the paper is to seg-ment the paper into k parts. While in clustering each cluster may correspond to words scattering in the whole paper, in this simple discourse analysis approach, each segment corre-sponds to words occurring physically together in the paper. In general, any text segmentation method can be applied.
In our experiments, we used the following simple approach, which can be regarded as a version of the TextTiling ap-proach proposed in [8]. Specifically, for partitioning a query into n parts, we first calculate the similarity between every two adjacent sentences based on Cosine Similarity of the corresponding TF-IDF weight vectors. We then iteratively segment the query abstract (or paper) at the boundaries where the similarity between two adjacent sentences is the lowest. Once we obtain n segments, similarly to the case of mutual information clustering, we can retrieve one reviewer for each segment according to the above language modeling approach.
In this section, we present our test data and measures used for evaluating our methods.
Since there is not any data set available for review as-signment task, we need to create our own data set. The data set in  X  X nterprise Track X  in TREC cannot be used for our task since we need a collection for matching reviewers with papers based on multiple aspects not only for retriev-ing experts/reviewers. So, to construct such a dataset, we have crawled abstract papers of ACM SIGIR proceedings from years 1971-2006 from the ACM digital library 2 .We have only considered abstract papers in conferences such as World Wide Web, SIGIR and CIKM in order to consider only IR researches for evaluating purposes. Then, we con-sider authors of these papers as prospective reviewers. For modeling reviewers X  expertise, we create a profile for each author in our pool by concatenation of all papers written by that specific author. If a paper has more than one author, we replicate that paper, one for each author. From the afore-mentioned dataset we further considered the authors having published more than three papers in these years so the num-ber of prospective reviewers in our collection is reduced to 189.

We used the SIGIR 2007 papers to simulate papers that are to be reviewed. There are 73 papers with at least two aspects. We experimented with both abstracts and full pa-pers.

Evaluating of such a system is very challenging. Since the actual assignments of reviewers to papers are confidential, we cannot use such data to evaluate the effectiveness of our methods. Even if we had such information, it would not necessarily indicate the best matching. As a result, we have to create a gold standard for evaluating our approaches. To achieve this purpose, an information retrieval expert identi-fied 25 major subtopics based on the topic areas in the Call for Papers of ACM SIGIR in most recent five years and ses-sion titles in the recent ACM SIGIR conferences. The expert then reads the abstracts of all the 73 test papers and all the 189 expertise profiles, and assi gn relevant expertise/topic aspects to each paper and each reviewer. This serves as a gold standard to evaluate our methods. This data set is pub-licly available at http://timan.cs.uiuc.edu/data/review.html to facilitate further study of the multiple-aspect review as-signment problem. http://www.acm.org/dl
While we can conveniently cast the multi-aspect review assignment problem as a retrieval problem, the traditional relevance-based precision and recall measures cannot be di-rectly applied to measure performance because they are un-able to reflect the coverage of multiple aspects in assigned reviewers.

Intuitively it is desirable to include reviewers covering many topic aspects so that they can collectively cover all aspects in the query. So, one way to account for this goal is to measure the number of different distinct topic aspects that are covered by n assigned reviewers as a function of aspects in the query. This measure tells us whether we can cover all aspects of the query. Consider a query with n A topic aspects A 1 ,..., A n A and let n r denote the number of distinct topic aspects that these n assigned reviewers can cover. Coverage can be defined as the percentage of topic aspects covered by these n reviewers:
In addition to maximizing the coverage of topic aspects, we also would like to maximize the number of reviewers that can cover each aspect. Thus given the same level of cover-age, we would prefer an assignment where each aspect is covered by as many reviewers as possible. Intuitively, this is related to the overall confidence of the assigned reviewers in reviewing each topic aspect of the  X  X uery paper. X  We thus define a Confidence measure to capture the redundancy of reviewers in covering each aspect.

Formally, let A 1 ,..., A n r be the n r distinct aspects covered by n retrieved reviewers and n A i be the number of reviewers that can cover aspect A i .The Confidence measure is defined as:
Note that we normalize the confidence values with the number of covered topic aspects n r , thus a high confidence value may be obtained by intentionally covering fewer as-pects. This is why confidence alone would not be so mean-ingful and it should be combined with the coverage. In this sense, the relation between coverage and confidence is sim-ilar to that between precision and recall. A perfect assign-ment should have both high coverage and high confidence.
One way to combine coverage and confidence is to normal-ize confidence over all aspects rather than just the covered aspects to compute an average confidence . Since a missed aspect would decrease the average confidence, it serves as a combination of coverage and confidence. Using the nota-tions introduced earlier, the Average Confidence measure is defined as follows:
We normalize all three measure s by considering their cor-responding optimal values that can be gained from our gold standard data set. Since measures are different; we describe algorithms for finding optimal values specifically for each measure. In general, finding the exact optimal values for these measures is NP-hard [16], so we opt to use greedy algorithms to compute an approximate value.

For finding the optimal Coverage , we use an MMR-based approach when selecting the next wide-coverage reviewer; i.e., we first pick the reviewer that covers the most number of aspects of the paper, then we take away the covered as-pects and pick the next reviewer to cover as many of the remaining aspects as possible. For finding the optimal Con-fidence , we first pick the aspect, e.g. A 1 , of the paper that most of the reviewers in the collection can cover it, we then select other reviewers that can cover aspect A 1 but do not cover any of the remaining aspects of the paper (to mini-mize n r ). For finding optimal Average Confidence , we select each reviewer to work independently to cover most aspects of the paper. In all cases, we follow some previous work [11, 16] and normalize each value with its corresponding optimal value to make the values more comparable across different query papers.
 We have also considered the weighted harmonic mean of Coverage and Confidence measures called balanced F score :
Ideally, we would like that the assigned reviewers to max-imize both Coverage and Confidence but they are usually contradictory. Since our focus of study is to examine the effectiveness of multi-aspect matching, we use Coverage as the primary measure to evaluate different methods, though we report all other measures as well. All the proposed algorithms are implemented using the Lemur toolkit 3 . Since our work is the first study of review assignment based on multiple-aspect matching, there is no natural baseline to be compared with. One main hypothesis we would like to test is whether assigning reviewers based on aspect matching can achieve better aspect coverage than not considering aspect matching. In order to test this hy-pothesis, we compare all the aspect-based methods with a baseline method that doesn X  X  do aspect matching. We use a state of the art retrieval method (i.e., language model-ing with Dirichlet smoothing [18]) as our Baseline approach. Specifically, for each author a in our collection, we construct one big document r j as a profile for author by concatena-tion of all documents in our collection written by the j -th reviewer. Reviewers are ranked based on the probability of the query paper q given the reviewer document r j ,whichis computed as: where the variables have the same meaning as in the query likelihood formula used for paper aspect modeling (see Sec-tion 3.3.)
In order to evaluate the effectiveness of our methods, we compare all well-tuned methods. The parameter for Base-line, SDA and MIC is Dirichlet parameter whereas for MMR, the novelty parameter matters and for PLSA, the number of aspects and novelty parameters are important. We show sensitivity to these parameters in section 5.2. The number of reviewers assigned to each paper in our experiments is three. http://www.lemurproject.org/ Aspects
In this section, we show our evaluation results based on the four evaluation measures. Results are presented in Table 1 for all the methods considering both abstract and whole papers with at least two aspects (73 papers in total). For MMR method, we retrieved top 20 prospective reviewers from well-tuned Baseline method and do re-ranking based on this set. From the table, we can make the following observations: Multi-aspect matching vs. Baseline: All the multi-aspect matching approaches outperform the Baseline method in terms of coverage. In most cases, the improvement is sta-tistically significant as shown in Table 4, where we show the Wilcoxon Signed-Rank test for both abstract and full pa-pers. This indicates that the proposed multi-aspect match-ing methods are effective in capturing and maximizing the coverage of multiple topic aspects by the assigned reviewers. Table 4: Wilcoxon Signed-Rank Test Results (P-values) for Coverage Measure and all Queries (Ab-stracts and Full Papers) Coverage-Confidence tradeoff: Although all the multi-aspect matching methods outperform the Baseline in cover-age, some of them have lower confidence than the Baseline; as a result, they do not all outperform the Baseline in terms of average confidence and F score, which combine coverage with confidence. This is not surprising and simply indicates that when we attempt to increase coverage, we tend to risk on lowering the confidence; such a tradeoff is similar to the familiar recall-precision tradeoff.
 PLSA vs. others: Despite the effect of coverage-confidence tradeoff, the PLSA method stands out as a method that out-performs the Baseline by all measures; indeed, it also out-performs all other proposed methods by all measures. Some of the improvements in coverage are statistically significant as shown in Table 4.
 Abstract vs. Full paper: While a full paper is far more informative than an abstract, our results show that abstracts may be sufficiently informative for multi-aspect review as-signment. Indeed, for both MMR and PLSA, the perfor-mance on abstracts is consistently better than on full pa-pers. MMR and Baseline are particularly hurt by using full papers as queries. Since both of them rely on ranking re-viewers based on a single query language model constructed based on a full paper, this suggests that the query language model built based on a full paper may be noisier than one based on an abstract, which presumably is cleaner. An-other possible reason is that our expertise representation for reviewers is built based on abstracts, 4 so more detailed rep-resentation obtained using a full paper may not be so useful. In the future, we should further look into this issue by ex-perimenting with using full papers to represent expertise. In contrast, however, both paper aspect modeling approaches (i.e., MIC and SDA) apparently have benefited from full pa-pers; they all perform better on full papers than on abstracts by all measures except in one case (coverage of SDA). This indicates that the more detailed information offered by the full paper does help us better capturing the multiple topic aspects in a paper than in the case of an abstract as we should expect.
 Reviewer vs. Paper aspect modeling: The fact that PLSA outperforms MIC and SDA indicates that reviewer as-pect modeling is a better strategy than paper aspect model-ing. However, as discussed earlier, both paper aspect mod-eling approaches can gain more through using full papers
At the time of this study, we were not able to obtain full papers for these candidate reviewers. abstracts (left) and full papers (right) instead of abstracts for assigning reviewers, suggesting that it may be interesting to further study how we can combine these two strategies. For example, we may consider using a translation model to match paper aspects with expertise aspects.
 MIC vs. SDA: Overall these two ways of capturing paper aspects perform similarly, though SDA seems to be slightly better. An interesting observation is that the confidence value for MIC approach is low for both abstracts and full papers. This may be because the MIC approach lacks the overall match of the paper since a word cluster may not necessarily represent a major aspect of the paper well even though the words in the cluster are coherent globally. In other words, the partitioning of words in a paper in this way may not necessarily correspond to the separation of the major topic aspects of the paper.
To further understand how these algorithms behave for papers with different number of topic aspects, We divide our queries into bins where each bin has a different number of aspects in the query. We have queries with at most 4 aspects, as a result, we have 4 bins. In bin 1(one aspect), we have 12 queries and in bin 2, we have 32 queries, in bin 3, 35 queries and in bin 4, we have 6 queries. With such decomposition, we can obtain a detailed view of the performance of each method for both abstract queries and full papers. The results are shown in Table 2 and Table 3, for abstracts and full papers, respectively. These results provide adecomposedviewofthoseinTable1.

In general, we see that in most cases, the performance on papers with more distinct aspects is worse than that on those with fewer distinct papers. This is expected as when a paper has more distinct aspects to cover, it is harder to cover all aspects and harder to have all reviewers to cover every aspect. However, most of the observations we made earlier are confirmed in these decomposed results; in particular, we again see that PLSA outperforms all other methods in all but a few cases.
The analysis made so far is based on optimal results ob-tained from parameter tuning. We now take a closer look at some of the parameters involved in all the methods and examine how they affect the performance and understand how they should be set.

The first parameter we study is the Dirichlet prior smooth-ing parameter  X  ; it is relevant to the Baseline method and the two paper aspect modeling methods, MIC and SDA since they involve ranking experts using the KL-divergence re-trieval model with unigram language models. (It also affects MMR, but the influence is through the Baseline method.) In general, we find that the performance is not very sensitive to this parameter when it is set to a relatively large value (e.g., larger than 4,000); a similar pattern of sensitivity was also found in using Dirichlet prior smoothing for regular ad hoc retrieval [18]. In Figure 1, we plot the sensitivity curves of coverage to this parameter for the Baseline, MIC, and SDA on both abstracts and full papers. We see that for all the in MMR for abstracts (left) and full papers(right) abstracts (left) and full papers (right) parameter values, MIC and SDA outperform the Baseline in coverage, again confirming that MIC and SDA are effective in modeling and matching multiple aspects.

The sensitivity curves of other measures are similar with a general trend of becoming more stable as the value of the smoothing parameter increases. The optimal setting ap-pears to be between 4,000 and 5,000.

The MMR method has an additional parameter (i.e.,  X  ) controlling the degr ee of redundancy removal. In Figure 2, we plot the sensitivity curves of all the four measures to this parameter in both cases of abstracts and full papers. We see that as long as  X  is set to a relatively large value (  X   X  for abstracts and  X   X  0 . 5 for full papers), the performance is stable and close to optimal.

PLSA has two parameters, the novelty parameter  X  and the number of aspects K . In Figure 3, we show the sensi-tivity curves of coverage to the novelty parameter for sev-eral different numbers of aspects (10, 20, 50, and 70). We see that using too few aspects (10) clearly hurts the per-formance, but the difference between 20, 50 and 70 aspects is relatively small, and they are all clearly better than 10 aspects. While the performance is fluctuating at different values of the novelty parameter  X  , the magnitude of differ-ence is not very big when a relatively large number of aspects is used ( K  X  20). In such cases, all the coverage values are higher than the Baseline values (0.75 for abstracts and 0.72 for full papers). The sensitivity curves for the other three measures (Confidence, Average Confidence, and F score) are all similar.

To see more clearly how the number of aspects affects performance in PLSA, we furthe r plot sensitivity curves of all the four measures to this parameter in Figure 4, where we see that as long as a relatively large number of aspects ( K  X  20) is used, the performance is stable and close to optimal. Review assignment task has a long history. Dumais and Nielsen [6] use Latent Semantic Indexing (LSI) trained on the abstracts provided by prospective reviewers. The recent work in [9], is a recommendation system for recommending panels of reviewers for NSF grant applications. They use in PLSA K for abstracts (left) and full papers (right) TF-IDF weighting measure for finding the match between reviewers and papers. Similarly, the work in [2] uses web to find abstracts of papers written by reviewers and then uses TF-IDF weighting to rank reviewers for a given paper. In a recent work on matching papers with reviewers [13], authors propose a novel topic model, Author-Persona-Topic (APT). They use statistical topic model to discover expertise. APT model is based on the idea that authors frequently write about different topic area combinations; so they learn the topical components as well as divide each author X  X  papers into several  X  X ersonas X . Each persona clusters papers with similar topical combinations. These works are similar to our work in a sense that they all try to match a paper/proposal to reviewers, but the main difference between our work and theirs is that none of the existing works consider matching reviewers with a paper based on matching of multiple as-pects of expertises.
 Expertise Modeling has received a lot of interests recently. There is also an Enterprise Track in TREC which is devoted to  X  X xpert Finding X  task. Expert finding is about finding people rather than documents and the goal is to retrieve a ranked list of experts with expertise on a given topic. There has been an extensive work on Enterprise track of TREC. For example, probabilistic models and language models are presented in [1, 14, 7] to solve the problem of expert find-ing. Our work is related to this body of work on expert finding, but the problem set up is different; as an expert finding problem, our problem is more challenging because we attempt to match expertise based on multiple aspects.
Authors in [4] argue for the value of diversity or relevant novelty in the ranked results of a query, and they propose the maximum marginal relevance (MMR) ranking function that incorporates such divers ity in a ranked result set. In a recent work on a subtopic retrieval [16], authors assume that a query may have different subtopic interpretations, and re-order the results so that the top includes some results from each subtopic. Our work is quite related to this work in the sense that our problem and the subtopic retrieval problem share a lot of similarities and a similar strategy can be po-tentially applicable to both problems. Indeed, one of our proposed strategies (i.e., redundancy removal) and some of the evaluation methods are essentially similar to those pro-posed in [16]. However, despite the similarity, there are some important differences between our problem and the subtopic retrieval problem: (1) Our queries are long abstracts, which are quite different from keyword queries, and they make it appropriate to consider modeling multiple aspects on the query side, which we exploit to propose some new methods. (2) While in subtopic, the goal is high recall and ranking all the documents, our goal is to retrieve a fixed number of re-viewers. As a result, we need different evaluation measures; in particular, the aspect precision measure proposed in [16] is completely inappropriate for our task.

Our work is also related to the study of topic modeling or aspect modeling. In [3, 10], the authors introduce gen-erative probabilistic models for collections of text corpora. They model the collection of data as a finite mixture over an underlying set of topics. Each topic is a finite mixture over topic probabilities and each document is a finite mixture over a set of topics. In our problem, the reviewers X  exper-tise is modeled as a finite mixture over the set of topics. Author-Topic model is also proposed in [15]. They propose a new unsupervised learning algorithm for extracting topics of documents and the interests of authors. Their generative model extends the previous work by using probabilistic top-ics to author modeling by allowing the mixture weights for different topics which are to be determined by the authors of papers. When we directly model subtopics, we use PLSA [10] to generate subtopics in our corpus. Our work differs from this line of work in that we go beyond topic modeling to address the issue of matching expertise and maximizing the coverage of expertise based on topic modeling.
Review assignment is an important yet labor-intensive task. Automatic assignment of reviews is appealing for mul-tiple reasons, including reducing the labor of the assigners and potentially improving the quality of assignments. A common limitation of existing approaches to automatic re-view assignment is that they all match a paper as a whole without modeling aspects of topics or expertise, resulting in unbalanced assignments. In this paper, we studied how to match reviewers with a paper based on matching of multi-ple aspects of expertise so that the assigned reviewers would not only have the required expertise to review a paper but can also cover all the aspects of a paper in a complemen-tary manner. We proposed three general strategies for solv-ing this problem, including removing redundancy, modeling aspects based on reviewer expertise, and modeling aspects based on the paper to be reviewed, and proposed four spe-cific methods to implement these strategies, including MMR, PLSA, MIC, and SDA. We also defined four measures (i.e., Coverage, Confidence, Average Confidence and F score) to evaluate methods for multi-aspect review assignment. We further created the first test collection for evaluating such a task based on ACM SIGIR publications.

We systematically tested all the methods with this data set. Experiment results show that all the proposed meth-ods are effective for increasing the coverage of topic as-pects in review assignment and outperform a state of the art baseline retrieval method. In particular, the PLSA multi-aspect matching method, which is a reviewer aspect model-ing method, performs the best. Our results also show that while reviewer-aspect modeling outperforms paper-aspect modeling, the latter gains more as we move from using ab-stracts to using full papers for assignment, suggesting poten-tial benefit of combining these two complementary strate-gies, which should be a very interesting future research di-rection.

Due to the lack of resources for evaluation, our conclu-sions are inevitably preliminary, thus another important fu-ture research direction is to further evaluate these methods with more data sets ideally created with multiple human assessors.

All the proposed methods are general, thus they can be applied to all review assignment tasks, including related ex-pertise matching tasks such as forming a thesis committee for a Ph.D. student (matching reviewers with a thesis) and forming a panel on a topic (matching candidate panelists with a topic description). In all these other applications, it is also desirable to match experts based on multiple aspects. We plan to explore all these applications in the future. For our experiments, we retrieve the fixed number of review-ers, e.g., three; another interesting future research direction would be to vary this number and see how it affects the performance.

Finally, our problem setup has assumed that the assign-ment of reviewers to each paper is independently done, but in a real scenario of assigning papers for a conference, we generally have constraints such as review quota of a reviewer. Thus another interesting future research direction is to con-sider the larger problem of assigning a set of reviewers to a set of papers with review quota constraints. We can further extend our approaches to multi-aspect expertise matching to solve such a problem through constrained optimization.
We thank the anonymous reviewers for their useful com-ments. This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933, IIS-0713571, and IIS-0713581, and work sup-ported by NIH/NLM grant 1 R01 LM009153-01. [1] K. Balog, L. Azzopardi, and M. de Rijke. Formal [2] C. Basu, H. Hirsh, W. Cohen, and C. Nevill-Manning. [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [4] J. Carbonell and J. Goldstein. The use of mmr, [5] T.CoverandJ.Thomas. Elements of Information [6] S. Dumais and J. Nielsen. Automating the [7] H. Fang and C. Zhai. Probabilistic models for expert [8] M. A. Hearst. Texttiling: Segmenting text into [9] S. Hettich and M. J. Pazzani. Mining for proposal [10] T. Hofmann. Probabilistic latent sematic indexing. In [11] K. Jarvelin and J. Kekalainen. Ir evaluation nethods [12] B. Larson and A. Chinatsu. Fast and effective text [13] D. Mimno and A. McCallum. Expertise modeling for [14] D. Petkova and W. B. Croft. Hierarchical language [15] M. Rosen-Zvi, T. Griffiths, P. Smyth, and M. Steyvers. [16] C. Zhai, W. Cohen, and J. Lafferty. Beyond [17] C. Zhai and J. Lafferty. Model-based feedback in the [18] C. Zhai and J. Lafferty. A study of smoothing methods
