 1. Introduction
Many classic studies of information retrieval examined the contributions made to retrieval performance through use of various kinds of document features and system characteristics ( Cleverdon, 1967; Salton &amp;
Lesk, 1968 ). This form of study of the inclusion of different types of system characteristics, such as controlled vs. uncontrolled vocabularies, the relative utility of including terms of class nouns vs. verbs , stemmed vs. unstemmed terms, and so forth, continues to the present. The measure we propose here captures the percent of the upper bounds performance provided by using a particular retrieval system options or characteristics, and is referred to here as the percent of perfect performance ( PPP ). The PPP measure is inspired by the sta-tistical R 2 value, which measures the extent to which information about one variable predicts the variation in another variable and is scaled from 0% to 100%. Similarly, the PPP measure computes the percent of the optimal ranking performance that is provided by the system being studied, providing information about whether the performance is nearly optimal, or perhaps it is only slightly above random. If a system using a certain method has a PPP value of 10%, it achieves 10% of the possible performance, which also implies that there remains 90% of the possible performance to be obtained using other methods. The PPP method may be computed retrospectively based upon existing documents and relevance judgments, or it may be predicted ana-lytically, based upon parameter values.

The performance of ordering systems that are used to retrieve documents can be measured using a number 1968 ), and the closely related E and F measures ( Swets, 1969; Van Rijsbergen, 1974; Shaw, 1986 ). Many of these measures can be shown to have strong relationships ( Demartini &amp; Mizzaro, 2006; Egghe, 2004; Losee, 2000 ). More recently, measures have been explicitly designed to work without full knowledge of relevance for all documents ( Buckley &amp; Voorhees, 2004 ).

Using the average search length (ASL) measure of retrieval system performance, the average position of relevant documents in the ordered list of documents, has some benefits over the other measures in that it ument weights, and its interpretation is simple ( Losee, 2000, 2006 ). The ASL serves as the basis for the PPP performance measure developed below. Positions in the ordered list of documents are numbered so that posi-list. As the average position of relevant documents in the ordered list of documents, a low value, approaching 1, represents the average position of relevant documents being near the front of the ordered list of documents, while a high value, approaching N , the number of documents in the ordered list, represents the average posi-tion of relevant documents being near the end of the ordered list. Given the ordered list of documents, r , n , r , n , n ,with r here representing a relevant document and n representing a non-relevant document, and the documents are strongly ordered from left to right, then the two relevant documents, at positions 1 and 3, would produce an ASL of (1 + 3)/2 = 2, with position 2 being determined to be the average position of rel-evant documents. When weak ordering occurs and several successive documents have the same document weight, the position used in computing the ASL for each of the documents with equal weights is the average position for those documents with that given weight. As the expected position of a relevant document, this can be computed from data, as in the example above, or the ASL can be predicted analytically.

The average search length can be applied to an entire dataset or to a portion of it. While the empirical results below examine the retrieval performance of the entire ordered list of documents, the ASL may also be computed for a portion of the ordered list, usually starting at the beginning of the ordered set and moving in a fixed distance. For example, one may compute the ASL for only the first 10 document or the first 100 documents. This provides an effective measure of performance when high precision searches are being studied or when relevance is available for only an initial set of the retrieved documents.

The ASL may be normalized different ways to produce a normalized average search length (NASL), depending on the desired characteristics of the NASL ( Losee, 2006 ). The measure NASL is the percent of all documents that are ranked ahead of the average position of the relevant documents and thus the proba-bility that a randomly selected document will be ranked ahead of the average position of relevant documents.
Just as their are numerous methods for estimating a probability or probability distribution, such as Bayesian methods, maximum likelihood, or method of moments, the NASL may similarly be estimated using several different approaches, all of which are reasonable. The ASL can be normalized by dividing by N for N values in the hundreds or greater, yielding the NASL. Here N is the number of documents being studied, which might be all documents or just the first 10 or 100 documents. Note that dividing ASL by N yields an NASL that can have the value 1 but never reaches down to 0.

The NASL can be computed empirically from the ASL by scaling the ASL, whose range of values for N documents is 1 X  N down to the range of 0 X 1 by computing NASL as (ASL 1)/( N 1). While this formula-tion produces an NASL that ranges from 0 to 1, we may choose to have an NASL that has a slightly different range. If N = 3 and we have three different ASL values, 1, 2, and 3, the lowest might be placed in the middle of the bottom 1/3, that is, at 1/6, while the second value would be in the middle of the middle third (e.g., at point 3/6), and the highest value in the middle of the top third (e.g., at point 5/6, halfway between 4/6 and 6/6). We can achieve this with
Thus, for 3 ASL positions of 1, 2, and 3, the three NASL values will be 1/6, 3/6, and 5/6. Eq. (1) is used in the experimental results below.

One can compute a value related to NASL, W , as the percent of documents in the first half of the ranked list that are ahead of the average position of relevant documents (with the average computed from the entire list) ( Losee, 2006 ). This may be described as the probability that a document from the top half of the ranked list of documents is ranked ahead of the expected position of a relevant document. If we assume that the NASL is one half or less, that is, the performance is equal to or better than random, then we may compute W  X  2 NASL.

Documents from the first half of the list are used instead of the entire list because it is desirable that the measure focus on the positive aspects of retrieval, occurring when the average position of a relevant document is located in the first half of the ordered list of documents. Using this positive section of the ordered list of documents is most easily enabled by assuming that the ranking method is better than random, and then multiplying the probability that a document is ahead of the expected position of relevant documents by 2.
The worst case value for W is then 1, which occurs when random ordering occurs, and the best case value is then W  X  0, when the expected position of relevant documents is at the front of the ordered list of docu-ments. Achieving this range of 0 X 1 for W is the primary motivation for this computation of W .
The development of PPP was originally based on the outgrowth of a quantitative measure (and resulting qualitative analysis) of how a single feature would contribute toward improving retrieval performance. The relative feature utility (RFU) is computed from the number of features (e.g., terms) or feature sets of one type whose use produces equivalent performance to using a single feature or feature set of another type ( Losee, 2006 ). A feature here is a characteristic of a document that the system developer decides to incorporate into ranking or ordering algorithms. A document might contain the term the or it might be absent; this is a binary feature that is present or absent. A system may be implemented so as to include a feature or characteristic when the system developer or manager decides that such a feature should be included when calculating query and document similarity; many system managers in English speaking countries are likely to decide that a word with little meaning such as the might be excluded from the set of features to be used in document ranking cal-culations by the retrieval system. To simplify discussion here, we assume that all features are binary and that relevance is binary, although non-binary feature frequencies and continuous relevance are easily incorporated into calculating NASL and thus W ( Losee, 1998, 2006 ). Given two binary features, i and j , where i might be used to represent the presence or absence of the term information and j might be used to represent the presence or absence of the term document , W i and W j are the performance probabilities associated with features i and j .
One may compute the number of systems with W j performance probabilities that produce the same numeric value as a system with performance probability W i by solving W
We assume that all the W i values are independent and identically distributed, as are the W ysis below. Denoted as M , the relative feature utility of system type i compared to system type j is and indicates that there are M occurrences of statistically independent type j features that together give us the performance associated with using a single type i .

As an example, consider the number n of coin tosses, each toss with probability of 1/2, that result in the same probability as achieving a specific roll on a 16 sided die. We could express this as algebraically solving the equation (1/2) n = 1/16 for n . In this case n = 4. Those unfamiliar with logarithms might note that log asks the question,  X  X 2 to what power is 8 X  X  and the answer is clearly 3, since 2 as 16  X  4 M , we find that, using logarithms computed to (arbitrarily chosen) base 2, we may solve this as M  X  log 2 16 = log 2 4. Because 2 to the fourth power is 16, and 2 to the second power is 4, this becomes M  X  4 = 2  X  2, which is consistent with our knowledge that, going back to the earlier problem, 16  X  4 and 16 = 4 2 .

In many decision making situations, it is desirable to be able to compare the relative utility of various options. For example, a single noun might be expected to have the same ordering capability as 1.5 or 2 adjectives ( Losee, 2006 ). M is easily interpreted and can be used in other computations, such as our measure of the percent of possible performance that is provided by using a particular option set. Consider a situa-tion where NASL i = 0.43 and NASL j = 0.48. The value M is then computed as M  X  log  X  2 0 : 43  X  = log  X  2 0 : 48  X  X  3 : 69. This implies that retrieval option i will result in 3.69 times the performance that will of type i . 2. Percent perfect performance P
The performance of a retrieval system may be compared to the level of random performance and of upper bounds performance by first using the ASL measure to measure retrieval performance, whether of an entire system or when studying specific features, and then performing further calculations to produce the perfor-mance value. When comparing the relative feature utility performance achieved using document ordering system x with the relative feature utility performance obtained at the upper bounds, we can compute the per-cent perfect performance (PPP). Such a value can provide a measure of the relative percent of achievable per-formance above the random performance provided by a situation. For example, we might say that using only nouns or using folksonomies provides performance that is 10% of the way from random performance to the upper bounds, leaving another 90% to be achieved using additional methods.

Using the RFU and Eq. (2) above, one may compute the improvement of a system i over a baseline system c and the improvement of the upper bounds system u over the same baseline system. The percent performance
P of system i in the context of the upper bounds system u may be computed by dividing the appropriate M values, the number of c values equivalent to system i , normalized by dividing by the number of c values in the upper bounds system, based on an arbitrary constant base system c ,as
The feature or system c may be interpreted as a baseline performance level, with the numerator (or denomi-nator, respectively) in Eq. (3) showing the number of baseline systems that are performatively equivalent (have the same numeric performance value) to system i (or the upper bounds, respectively). When the number of baseline systems performatively equivalent to system i is divided by the number possible (the upper bounds), the percent of the upper bounds performance provided by system i  X  X  performance is produced. The P values may be multiplied by 100 to provide the percent of perfect performance (PPP) provided by a system compared to the upper bounds.
 It was noted above that when the M values are computed from the random level of performance using Eq. (3) ,
NASL = 0.5, then M is infinite as (log x )/(log1) = 1 because we are dividing by log1 = 0. However, using the ratio in Eq. (3) , the relative merit of the different M values may be computed so that the base points, whether they are NASL = 0.5 or another value NASL = c which might be very close to random (e.g., NASL = 0.4999), will cancel out so that the portion of the M value for the upper-bounds due to x may be computed. Thus, the performance using a ranking algorithm and features denoted as x given upper bounds performance u , is sug-gested by the right side of Eq. (3) as
Note that if the upper bound is arbitrarily set to NASL for system x as 0.1, for example, then the best level bounds remains at 0.1, P 0 : 1 0 : 5  X  log  X  2 0 : 5  X  = log  X  2 0 : 1  X  X  0 or 0% and for x = 0.3, P log  X  2 0 : 1  X  X  0 : 318. The latter result may be interpreted as implying that performance at level x is almost 32% of the possible level of performance obtained with a perfect ordering procedure.

While one may compute P using the feature model proposed above, it may also be computed more directly from the ASL values, with a focus being more on the performance measure (e.g., ASL) rather than the document model (e.g., features). This will be examined more below.
 Fig. 1 shows the performance that is obtained over a range of NASL values and a range of upper bounds
NASL values. This non-linear relationship shows that a high performance value is obtained when the NASL being studied approaches the upper bounds. 3. Ordering performance measures
A range of performance measures have been developed to evaluate the ordering of documents, given a range of different considerations ( Demartini &amp; Mizzaro, 2006; Harter &amp; Hert, 1997 ). Most traditional ordering performance measures assume the existence of a relevance judgment for each document. The most popular per-formance measures are probably precision, the percent of documents retrieved that are relevant, and recall, the percent of relevant documents in the database that have been retrieved. Combinations of precision and recall may be used as single number measures of performance at various points in the search process. The F measure is related to the harmonic mean of precision and recall and may be studied at a specific point in the document retrieval process.

Some other single number measures of ordering performance address the length of time or the number of documents that are examined when moving to a specific place in the ordered set of documents. The expected search length (ESL) measures the number of non-relevant documents occurring before a specific point in the ordered list of documents ( Cooper, 1968 ), while the average search length (ASL) measures the number of documents encountered as one moves to the average position of relevant documents.

As search engines have become popular and the size of retrieval databases grows from thousands to millions and to billions (thousands of millions), an increasing number of the searches conducted are high-precision searches where a few useful documents are desired. Most of the measures discussed above require that relevance judgments be available for all relevant documents, a practical difficulty when there is a very large number of documents. A popular measure, the mean average precision (MAP), measures the precision after each relevant document is retrieved and then averages these precision values ( Buckley &amp; Voorhees, 2004;
Jarvelin &amp; Kekalainen, 2002; Sanderson &amp; Zobel, 2005; Voorhees, 2001 ). MAP requires relevance judgments for only those documents up to the point at which the MAP is computed, often the documents that are retrieved in a high precision search, or up to a specific point in a search. It becomes more and more difficult search progresses. The more the relevant documents are located near the beginning of the ordered list of doc-uments, the higher will be the MAP. Given different levels of relevance beyond the simple binary relevant vs. non-relevant values, measures expanding beyond the MAP approach may be used, such as the discounted cumulative gain, which considers more than two relevance levels and discounts the value of a document the further the document is from the beginning of the list ( Jarvelin &amp; Kekalainen, 2002; Voorhees, 2001 ).
Because ASL and NASL may be computed from an initial set of retrieved documents, or from all the documents, one may compute the P measure from either all the documents or from the set of documents of a certain size at the beginning of the ordered list. In this way, the P measure can be used to study high precision searches, as do the MAP and cumulative gain measures.
The PPP ( P ) performance measure that was developed in the previous section acts as a single number measure of ordering performance. While developed here to measure the percent performance toward optimal performance, with the ASL as the basic performance measure, this same technique is also applicable to other performance measures that are probabilities or average probabilities. 4. Normalized measures vs. P
Measures are often normalized to place them in a range of 0 X 1, 0 X 100, or some similar range that allows for simple interpretations and comparison of different values. Normalizing a measure frequently occurs by taking the value v and the range of the possible values, from the lowest value v dividing the degree to which the value being examined exceeds the lowest value; this is normalized by (divided by) the range of possible values:
The normalized v value is thus in the range of 0 X 1, with Normalized( v
Normalized measures of performance are particularly useful when studying performance with different length measures. The author has found a normalized version of the ASL measure useful for comparing differ-ent performance values, as well as for studying the normalized upper bounds performance. Cooper has advo-cated the use of a normalized form of the expected search length and this may be the most popular normalized search length measure ( Cooper, 1968 ).

The P measure functions as a percent of optimal performance and is developed explicitly to be a linear percent, normalized so as to range from 0% to 100%, which is the same as a probability of 0 X 1. Returning to our earlier explanation of the basis of the relative feature utility, one can compute the number of occur-rences, M , of system type j events with performance probability p of a single system event of type i with performance probability p (or, similarly, p 1 = M i  X  p j ). Solving algebraically for M produces an equation like Eq. (2) .
One can similarly solve for the M associated with the upper bounds of performance. Somewhat differently than with the RFU, the M used in our P measure is the fraction of an occurrence of a type j event, instead of the number of occurrences as in the RFU. Thus, the PPP measure P is computed as the percent of the occur-rence of the upper bounds performance that produces the performance associated with a single event of type i , the performance being studied. Thus, M will be a fractional value representing the fraction of the upper bounds. Continuing with the notation above, and denoting the upper bound performance as type u with prob-probability that a document in the top half of the documents is ahead of the average position of a relevant document. Note that we could not solve for M if the W values were not probabilities. 5. Upper bounds There are several different upper bounds, or levels of maximum performance, that may be used in computing
P values. Starting at the highest possible upper bounds, an oracle might be able to look at media and determine regardless of the degree to which the query and the documents X  terms match. For example, two documents might have the same feature profile visible to the searcher, while one is considered relevant by a user and the other labeled non-relevant . Ranking by this omniscient level of upper bounds would place the relevant docu-ment before the non-relevant document. We denote the performance given options x and given this type of upper bounds as P 1 x .

The best possible upper bounds would occur when the R relevant documents in the list of documents being studied occur at the beginning of the list of documents before any non-relevant documents occur. The NASL value of this may be computed by noting that the expected position of a relevant document is at R /2 + 1/2, which is the best case ASL. Using this ASL, we may compute the upper bounds NASL (NASL Eq. (1) :
Those without access to omniscience might have a lower level of upper bounds in which the relevance of a document is fully determined by the features present or absent in the documents and the feature space that is used and thus knowing the document X  X  features, such as whether it has particular terms, along with the full set of possible features, allows one to definitively determine the document X  X  relevance. Upper bounds perfor-mance at level x for this level of upper bounds is denoted as P expected to be worse than or equal to the upper bounds performance provided by using the oracle described
Given a large number of terms or features being used in the ordering, the best possible ordering will be very similar, if not identical to, the best possible ordering described above, P there are about 2 b possible sets of document characteristics, and given even a small vocabulary of, for example, 50 terms, one would find so many sets of characteristics that, if the terms were approximately evenly distrib-uted, the NASL 1 would usually be very closely approximated by NASL
Documents may be ordered based solely on the features present in the query, with the document weights being then determined by the presence or absence of the query terms in the documents. Empirical performance are in the query, then the upper bounds performance using only query terms is worse than the level of perfor-mance obtained when using all terms in the features space. In the special case where the query has all the features in the feature space, then the performance P AllTerms
In many cases, we can determine what percent of the contribution to performance is provided by incorpo-rating an intermediate system feature i when considering the performance of final system feature f . Beginning with direct performance P 1 f and the performance P 1 f ; i system component i as part of computing performance at level f is computed as D ple, if the performance with feature f by itself is 12% of the way to perfection and it improves to 15% when using feature i , we can conclude that feature i improved performance by 3%. 6. How to apply PPP X  X n example
As an example of how to apply PPP, consider an ordered list of documents with r or n , denoting relevance or non-relevance, respectively, as well as the profile of features that are suggested for use based on the query, with the features shown in binary as a subscript:
When computing average search length, documents with equal profiles are treated as though they are all located at the center position of the equally profiled documents. The ASL for this ordered set of documents is thus two relevant documents at position 2 (the center of the first three documents), and a relevant document at position 6.5, the middle of positions 6 and 7 for r 01 3 = 10.5/3 = 3.5 From this, we may compute the NASL using Eq. (1) as NASL = (3.5 0.5)/7 = 3/7 = 0.43.
To compute the upper bounds of performance, we will reorder documents so they are in weakly decreasing order by the average precision of the documents with a given profile. Thus, the upper bounds ordering becomes
The ASL for this upper bounds ordering is computed from two documents at position 2 and one document at position 4.5, thus ASL = (2 + 2 + 4.5)/3 = 8.5/3 = 2.833. The NASL is computed as NASL = (2.833 0.5)/ 7 = 2.33/7 = 0.33. Note that we could compute the NASL 1 as ( R /2)/ N or (3/2)/7 = 3/14 = 0.21.
We may compute the percent perfect performance by using Eq. (3) . We find that P  X  log  X  2 0 : 43  X  = log  X  2 0 : 33  X  X  log 0 : 86 = log 0 : 66  X  0 : 36. We can thus state that our original ordering method and these documents achieved 36% of the performance that is possible, leaving another 64% to be obtained, with performance measured by ASL.
The utility of such a number is more obvious when we have two different ranking procedures or retrieval options. Given the data from earlier in this section, if assigning part-of-speech tags to the document profiles allowed us to separate the two relevant documents with profile 10 from the non-relevant document with profile 10, the ranking technique is likely to produce better rankings. Note that if we consider part-of-speech tags as an option for our system, the upper bounds must be computed so as to be consistent with this improved per-formance. Using part-of-speech tags would improve some of the upper bound NASL measure of performance.
The following section shows the comparison of different retrieval options for some standard test databases and several well understood techniques, such as part-of-speech tagging and term stemming. 7. Empirical results
Several tests were conducted that provide results measured using the PPP P measure. Rankings were con-sistent with the CLMF (coordination level matching  X  frequency) weight, a modification of coordination level matching in which the number of terms in the document that are also in the query are counted ( Losee, 2006 ).
This is like the TF IDF (term frequency times inverse document frequency) weighting except that all terms are given the same term weight. One should note that unlike IDF (inverse document frequency) weighting which would give common words such as stopwords (common, non-subject bearing terms) a very low weight, CLMF gives stopwords an equal weight as other terms and thus ridding documents of stopwords becomes more effec-tive in our results than would be found with TF IDF weighting.

The CLMF provides a simple, easy to understand weight (simpler than TF IDF). Our concern in this (and most other studies) is simplicity and clarity, rather than achieving the best results possible. Increasing the reader X  X  understanding of what occurs and the nature of the relationships between system variables is the goal of many scientists; clearly, achieving the best results possible is also a valid goal for researchers.
Two standard databases are used in producing these measurements on the Nyltiac ( http://Nyltiac.com ) retrieval system. The first 50 queries of the CF database, composed of documents with the subject heading cystic fibrosis (CF) in the National Library of Medicine database, are used ( Wood, Wood, &amp; Shaw, 1989; Moon, 1993 ) and the parts-of-speech (POS) tags are supplied by the Brill tagger ( Brill, 1994 ) for the CF POS Tagged database. The MED1033 database, composed of 1033 documents extracted from the National
Library of Medicine X  X  database, was used for other analyses. Because of the design of this database, it is easier for systems using MED1033 to retrieve documents labeled as relevant ( Kwok, 1990; Shaw, Burgin, &amp; Howell, 1997 ). The MED1033 Tagged database is part-of-speech tagged as with the CF POS Tagged database.
The stopword list used here contains 425 stopwords, and removing stopwords may have a larger impact on performance in this study than when using smaller lists of stopwords containing only a few dozen terms.
Results given in Table 1 show the upper bounds (at the bottom of the table) having P levels of 100%. With case ignored and stems and stopwords removed, the percent of perfect performance was about 15% for the CF database and about 43% for the MED1033 database (both untagged). Using part-of-speech tags produces slightly lower results. This data suggests that the greatest contribution is provided by removing stopwords, while most of the other options provide little improvement. Knowing that removing the stopwords and case sensitivity produces about 15% of the possible performance for CF and about 43% of the possible perfor-mance for the MED1033 database gives us an idea as to whether we should include this kind of processing; a 15% improvement is probably enough to justify most system designers to incorporate an option. The 43% performance improvement leaves us 57% of performance improvement remaining to be addressed by other methods. 8. Discussion and conclusions
Using the P measure of percent of perfect performance (PPP), the percent of the possible (upper bounds) performance accounted for by using the current system and document features, we have been able to illustrate how one might measure the contribution to performance of several information retrieval options. Using such a measure allows us to understand the relative utility of different features in terms of percent improvement toward optimality, a value that most searchers may understand with little training, which is not the case for most other retrieval measures. By comparing the performance for two systems, with one being the upper bound, we are able to compute the percent of the upper bound performance of one system that is provided by the performance of a non-optimal system. We believe that this simplicity and comparison to upper bounds is advantageous to both researchers, searchers, and to those making decisions about the use of systems in orga-nizational contexts.

Empirical results were provided showing the application of the PPP measure using traditional retrieval test databases. As information retrieval matures as a science and becomes more analytic, the ability to predict retrieval performance becomes increasingly important. As the ASL and NASL values may be predicted ana-lytically, one may predict PPP performance based on analytic considerations. For example, one might be interested in predicting what the performance curve looks like as the percent of relevant documents in a data-base increase, or as the difficulty in locating these documents increases; these performance results, presented as percents of upper bounds performance, may be produced with relatively little effort using graphic packages.
We should note that when comparing our results to the theoretical upper bounds, we do not wish to imply that all or even many humans can provide ranking at the P optimal performance that is routinely achievable by human analysis and the sorting of documents, and what the parameters are for this level of performance. Information retrieval performance might then be determined based on what percent of the performance achievable by most humans (H), P methods x . Information retrieval systems might have as their goal ordering documents at the level of humans, or scholars may wish to surpass human performance if it is significantly below P Acknowledgement
The author wishes to thank Lewis Church, Lee Roush, and two anonymous referees for comments on an earlier version of this article.
 References
