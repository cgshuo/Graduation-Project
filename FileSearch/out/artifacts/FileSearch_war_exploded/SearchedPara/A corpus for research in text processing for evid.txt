 Diego Molla  X  1  X  Mar X   X  a Elena Santiago-Mart X   X  nez 1  X  Abeed Sarker 1  X  Ce  X  cile Paris 2 Abstract Evidence based medicine (EBM) urges the medical doctor to incorpo-rate the latest available clinical evidence at point of care. A major stumbling block in the practice of EBM is the difficulty to keep up to date with the clinical advances. In this paper we describe a corpus designed for the development and testing of text processing tools for EBM, in particular for tasks related to the extraction and summarisation of answers and corresponding evidence related to a clinical query. The corpus is based on material from the Clinical Inquiries section of The Journal of Family Practice. It was gathered and annotated by a combination of automated information extraction, crowdsourcing tasks, and manual annotation. It has been used for the original summarisation task for which it was designed, as well as for other related tasks such as the appraisal of clinical evidence and the clustering of the results. The corpus is available at SourceForge ( http://sourceforge.net/projects/ ebmsumcorpus/ ).
 Keywords Corpus Evidence based medicine Annotation Crowdsourcing Text summarization Clinical guidelines urge medical practitioners to perform evidence based medicine (EBM): a practice that requires practitioners to incorporate the best available evidence from published research when making clinical decisions. Good practice of EBM requires the medical practitioner to keep up to date with the recent developments in his or her area of expertise, or at least to be able to search and retrieve the latest clinical evidence. Whereas there has been significant research and development on search, classification and presentation of documents that contain the relevant information, as evidenced by the search tools embedded in websites such as PubMed, 1 The Cochrane Library, 2 and Trip Database, 3 research focused on the extraction of the clinical evidence, summarisation and presentation of this evidence to the medical practitioner is much more limited. A major problem that hinders research on those areas is the limited availability of annotated corpora for the development and evaluation of such systems.

In this paper we present a corpus that has been designed for the development and evaluation of text processing systems for EBM. The corpus is available at SourceForge. 4 The specific application of this corpus is in the area of query-based multi-document summarisation, where the summaries focus on the answers and the clinical evidence related to specific medical questions asked by a general practitioner or family doctor. The data has been sourced from the publicly available Clinical Inquiries section of The Journal of Family Practice , 5 with kind permission from the publishers. The data contains the clinical question, summaries of the answer at several levels of detail, and additional information such as the quality of the evidence and references to relevant published papers. The annotation of the data has been accomplished via a range of annotation methods including automatic extraction of the data from the source, manual annotation and rephrasing of the text, and crowdsourcing experiments.

Section 2 introduces EBM and argues for the need of a corpus such as the one presented in this paper. Section 3 justifies the choice of data sources and format. Section 4 describes the range of techniques that were used to create the corpus. Section 5 presents some statistics of the corpus and lists some of the uses of the corpus. Finally, Section 6 concludes the paper. The term evidence based medicine (EBM) was introduced during the 1990s as a means to characterise the practice of systematically incorporating published research as the basis of clinical decision making. In particular, Sackett et al. ( 1996 ) define EBM as  X  X  X he conscientious, explicit, and judicious use of current best evidence in making decisions about the care of individual patients. X  X  A major stumbling block in the practice of EBM is the need for medical practitioners to access the most up to date clinical advances that are relevant to the patient X  X  needs. Given the large amount of medical literature available, this is difficult and time-consuming. For example, PubMed indexes more than 24 million citations (bibliographic data, plus abstracts in most citations) from medical literature and increasing. To alleviate the need to search the primary literature, organisations such as The Cochrane Collaboration 6 maintain collections of systematic reviews of the clinical evidence related to specific topics, but these reviews become quickly outdated and they do not cover all possible topics that could be important at point of care.

The effective practice of EBM requires a principled manner to phrase the clinical question, systematically retrieve the best available evidence, and critically appraise the evidence before applying it to the patient. Research advances in Natural Language Processing help in each of these steps, as has been shown in Athenikos and Han ( 2010 ) X  X  survey of biomedical question answering and Afantenos et al. ( 2005 ) X  X  survey of summarisation from medical documents. For example, the clinical question has been classified automatically (Yu and Cao 2008 ). Search engines such as PubMed retrieve the information. Knowledge-extraction techniques have been used to extract information related to key aspects of the question (Demner-Fushman and Lin 2007 ; Niu et al. 2003 ). And query-based summarisation systems attempt to extract the relevant evidence and present it to the user (Elhadad et al. 2005 ; Fiszman et al. 2004 ).

To investigate the tasks associated with evidence-based answer generation, and to explore possible approaches for performing these tasks, we require data from real-life evidence-based medicine practice where question-oriented medical sum-maries are generated by domain experts. In particular, we require the original queries, human-authored answers to clinical queries, and source documents from which the answers have been derived. While there is an abundance of data for automatic summarisation tasks in other domains, to the best of our knowledge there are limited corpora suitable for summarisation for EBM. As reported by Molla  X  ( 2010 ), the results of past research related to query-based summarisation of medical text are not readily comparable due to the lack of appropriate corpora. Thus, past research either used corpora that are not currently available, such as CENTRI-FUSER/PERSIVAL (Elhadad et al. 2005 ), or their evaluation was based on human judgements, such as SemRep (Fiszman et al. 2004 ), and therefore the evaluation results are not replicable automatically.

Other than the data presented in this article, currently-available resources are usually either small, or do not contain all the desired information. For example, the Parkhurst Exchange collection 7 is a collection of clinical questions with their answers, but the answers do not contain explicit pointers to primary literature. BioScope 8 is a corpus annotated for negations, speculations, and their linguistic scopes, but it is not capable of supporting research on automatic medical Question Answering (QA) due to the lack of annotations with answers or summaries.
To our knowledge, other than our corpus, the only data set of comparable size containing similar information is the one currently being produced by the BioASQ challenge. 9 In particular, the 2014 data for Task 2 Phase B included 310 clinical questions with their exact (factoid-style) answer and the ideal (one-sentence) answer, plus text extracts and references to relevant documents. Figure 1 shows a fragment of the 2014 data set. Similarly, the 2015 data for task 3 Phase B included 810 questions with similar annotations. This dataset will be very useful for the development and testing of clinical question-answering systems.

The Text Analysis Conference (TAC) launched a shared task for biomedical summarisation in 2014. 10 The goal of this task was to explore the use of citing papers for building community-based summaries. As such, the goal is different from EBM-related tasks in that there is no specific clinical answer that the summaries need to address. Still, the data that accompanied the shared task could be useful for summarisation tasks in general, and biomedical summarisation in particular. The training set contains 20 documents only, and the test set has 30 documents only. The usefulness of this corpus for methods based on machine learning is therefore limited, and at the time of writing this paper there were no plans to expand the We have built a corpus using data from the Clinical Inquiries section of the Journal of Family Practice (JFP) and PubMed. The Journal of Family Practice is an American medical journal focusing on the domain of family practice. Each monthly issue of the journal contains a section called Clinical Inquiries , and all the data for our corpus is collected from this section. We now explain the structure and content of this section of the journal, and its applicability to research on automatic evidence-based answer generation.

The Clinical Inquiries section of the JFP is a reliable source of information regarding real life evidence-based medicine practice. The articles in this section, being evidence-based answers to clinical queries, are similar in terms of structure. Each article has the same goal: to provide a systematic analysis of the best available medical evidence in response to a clinical query. Thus, preparation of each article in this section requires domain experts to analyse the best available medical evidence, summarise the relevant evidence, and derive bottom-line recommendations.
As a collection, these articles are attractive to work with as they have a number of features amenable to the study of evidence-based summarisation. Importantly, each article contains both single-and multi-document summaries, allowing for the creation of a corpus that can flexibly support research in both forms of text summarisation. Furthermore, the Clinical Inquiries articles contain additional information such as grades indicating the qualities of the evidences. This makes it possible for the corpus built from this data source to support research in related areas of evidence-based medicine, such as the automatic appraisal of evidence. Data from this section of the JFP also enables the analysis and replication of the various tasks associated with evidence-based medicine practice.

Figures 2 and 3 show the important parts of an article from the Clinical Inquiries section of JFP (Mounsey and Henry 2009 ). As the figures show, an article in this section contains: 1. A clinical question. This appears as the title of each Clinical Inquiry article. In 2. The bottom-line, evidence-based answer(s) or recommendation(s). A single 3. For each part of the evidence-based answer, a grade indicating the quality of the 4. Detailed justifications for the evidence-based answers. The detailed justifica-5. References to published medical research papers from which the information in
The articles in Clinical Inquiries section of the JFP are very carefully constructed reviews. The relevant documents are identified via exhaustive literature searches by medical experts. Molla  X  and Santiago-Mart X   X  nez ( 2011 ) point out several significant advantages of using data from this resource, rather than direct systematic reviews such as the Cochrane Reviews, 12 or other less structured resources: 1. The format is relatively uniform across all inquiries. Therefore, it enables a 2. The text in each inquiry is much more compact than in a Cochrane review, 3. The procedure to find answers in the Clinical Inquiries section of the JFP is 4. The presence of short answers followed by longer explanations makes this
Further on the above points, note that the overall purpose and target audience of systematic reviews is different from that of the Clinical Inquiries section of the JFP. Systematic reviews attempt to summarise all available and relevant primary literature, and the resulting reviews contain all relevant information. Such reviews are useful for researchers, or as input for establishing protocols. However, the Clinical Inquiries section of the JFP aims to provide concise information that is directly relevant to the clinician, who is interested on determining the best course of action that would help a particular patient. The fact that the Clinical Inquiries refers to systematic reviews is indicative of this difference. The Cochrane Clinical Answers 13 are probably more comparable for the purposes of our study.

In the next section, we explain the data collection process, the annotation of the data, and the creation of the corpus in detail. The collection of data from JFP to produce the corpus involved several steps, namely: the automatic extraction and conversion of text, and the manual annotations. In this section, we first provide an overview of the corpus itself and then discuss some of the data collection details. We commence by providing an overview of the corpus using formal notations that we reuse later on in the article. 4.1 Corpus overview The corpus is encoded in XML format and consists of a set of records R  X f r 1 r m g . Each record r i represents a Clinical Inquiry article from the JFP, and contains one clinical query q i so that we have a set of questions Q  X f q 1 q m g . Each r i has associated with it a set of one or more bottom-line answers to the query A i  X f a i 1 a in i g . Each bottom-line answer a ij of r i has a grade g associated with it, so that each record contains a set of grades, G i  X f g i 1 g in i g . For each bottom-line answer a ij , there exists a set of human-authored detailed justifications (single-document summaries) L ij  X f l ij 1 l ijo ij g . Each detailed justi-fication l ijk is associated with at least one source document abstract D ijk  X f d ijk 1 d ijks ijk g . Note that the total number of source abstracts is not necessarily equal to the total number of detailed justifications, since multiple detailed justifications might be associated with the same source document abstract, and one detailed justification might be associated with several document abstracts. Figure 4 diagrammatically illustrates the structure of a sample record from the corpus. This structure of the corpus was chosen because it is ideal for both single-and multi-document, query-focused, text-to-text summarisation.

We now briefly look back at how the data for the corpus was collected. 4.2 Data extraction Data were collected from all the publicly available Clinical Inquiries articles of the JFP (dating from the year 2001 X 2010), after obtaining permission from the publishers. The questions, bottom-line answers, evidence grades, detailed justifi-cation texts and references were all automatically downloaded and stored in a database. 4.3 Annotation of detailed justifications The source text from the JFP articles did not provide explicit connections from each justification to the specific bottom-line answer, as can be seen from the extract shown in Fig. 2 . Therefore, the corpus had to be prepared by manually identifying the detailed justifications associated with the bottom-line summaries. We utilised a web-based annotation tool 14 that, for each article, displays the question and each of the answer parts. Each bottom-line summary has associated empty text areas where the annotator can copy and paste all the detailed justifications associated with that bottom-line summary.

The total number of pages to annotate was distributed among three annotators (the first three authors of this paper). A small percentage of the pages were annotated by all annotators (the annotators did not know beforehand which of the pages were annotated by all), to check for inconsistencies. The annotation process was done in several stages, with periodic checks on the common pages to detect and solve systematic inconsistencies in the annotation criteria. During those checks the annotators agreed on a set of criteria, an extract of which is: 1. Remove temporal expressions not mentioned in the original text and phrases 2. Remove general text not directly associated with a referenced document or a 3. If there are multiple references associated with a justification, split it into 4. If a paragraph directly associated to a bottom-line answer does not have any
These criteria mostly addressed the need for each answer justification to be self-contained, and to match an answer justification to one reference only whenever possible. After inspection of a random sample of the common pages, the annotators agreed that the variations in the annotations were acceptable. Figures 5 , 6 , and 7 show three screenshots of the annotation tool. The sample record is different to the one shown in Fig. 2 . Figure 5 shows the question, the bottom-line answer along with two manually edited detailed justifications, each of which refer to a specific source document from which the information was obtained. Figure 6 is the next screenshot showing a text area for insertion of more detailed justifications and some text from the JFP article. Figure 7 shows the bottom of the annotation page with the references, where annotators have the option of correcting erroneous PubMed IDs or SORs before saving their work.
 4.4 Extracting reference information via crowdsourcing In addition to all the information available from the JFP, our target task of summarisation requires the source texts associated with the detailed justifications. Each Clinical Inquiries article contains multiple bibliographical references to medical publications. Although a small number of those point to online sources, the majority are full bibliographical references pointing to traditionally published resources such as medical journals or magazines. We wanted to obtain as many of the associated source articles as possible. Since most medical articles are regularly logged into the PubMed database, we chose to obtain electronic copies of the article abstracts from there. It was not possible to obtain full articles since we did not have access to all the venues from which they were available. The easiest way to obtain an electronic copy of an article abstract is to download it from the PubMed website using the article X  X  PubMed ID. Therefore, completing the corpus required identifying the PubMed IDs of the abstracts.

We attempted to determine the PubMed IDs of the references automatically, by performing a series of searches on the PubMed database. The searches were based on the reference text, after removing pagination information. However, visual inspection of a small random sample revealed that this method often did not find the correct IDs. We therefore designed a crowdsourcing task for finding the PubMed IDs (Molla  X  and Santiago-Mart X   X  nez 2011 , 2012 ).

The crowdsourcing task was created using Amazon Mechanical Turk. 15 The final accuracy of the annotation task was manually checked on a random sample of 100 references. No errors were detected. Finally, once all IDs were found, the abstracts were automatically downloaded from PubMed and added to the corpus. We chose to download the articles in XML format, which contains useful meta-data, the abstract text and additional annotations such as classification tags and MeSH terms. Note that it was not possible to obtain all the abstracts. The next section discusses various important statistics associated with the corpus in more detail.

Figure 8 shows the actual corpus in XML format. Figure 9 presents a screenshot showing the structure of a PubMed abstract. The abstract (PubMed ID: 12230591) is also chosen from the same record. The final statistics of the corpus are as follows:  X  Number of questions : 456;  X  Number of bottom-line answers associated with the records: 1396;  X  Number of bottom-line answers with quality grades specified: 1225 (171  X  Number of detailed justifications associated with the bottom-line answers: 3036;  X  Number of unique referenced articles (PubMed abstracts) associated with the
On average, each question has 3.06 bottom-line answers; each bottom-line answer has 2.17 associated detailed justifications; each detailed justification has 1.22 references. There are 6.57 references, on average, for each question. 16 Despite our best efforts, we failed to locate a number of the source documents on PubMed. All such cases (312, 10.7 %) were noted and recorded in the corpus. Furthermore, in a number of cases, although the referenced documents were found in PubMed, the XML versions of the documents did not contain any text from the abstract. There were a total of 311 (10.7 %) such cases.
 The distribution of the quality grades is: 345 for A, 535 for B, 330 for C, 15 for D. 17 A total of 171 bottom-line summaries do not have any quality grade assigned, and therefore, these entries are unusable for research in automatic grading of evidence. However, their applicability to automatic summarisation research does not change.

Figures 10 and 11 illustrate the distributions of bottom-line summaries, detailed justifications, references, and quality grades in the corpus.

We envisage multiple uses of this corpus. Below we describe some of the uses that we have made of it. 5.1 Evidence-based summarisation The intended primary use of the corpus is to perform automatic, evidence-based text summarisation. The design of the corpus enables the development and testing of single text summarisation systems. For this task, the abstracts can be used as source texts and the human-authored detailed justifications can be used as the gold standard. Since the questions are also available in the corpus, they can be used for performing query-focused, single-document summarisation.

Table 1 shows an example of information in our corpus that can be used for single-document summarisation. The figure shows the question, the human-authored detailed justification (to the bottom-line answer which is not shown in the example), and text from the referenced abstract. The goal of automatic summarisation in this case is to identify and extract information from the source texts that most closely resembles the human-authored summaries.

We used the corpus to develop a single-document summariser (Sarker et al. 2013 ). Our system focused on three-sentence summaries. We analysed the characteristics of each of the three sentences of ideal summaries and used that information to build three independent classifiers, one per target sentence. The classifiers scored the candidate sentences according to a range of features including sentence position, sentence length, similarity to the question, sentence type based on the PIBOSO taxonomy (Kim et al. 2011 ), n-grams, parts of speech, domain information based on the Unified Medical Language System (UMLS), and question type. We compared the resulting ROUGE evaluation results with those of all possible three-sentence extracts and observed that our system achieved results within the 96.8 % percentile.

The corpus is also suitable for performing evidence-based multi-document summarisation. For this task, the bottom-line answers are the target summaries, and all the abstracts associated with the bottom-line summary are the source texts. Table 2 provides an example of multi-document summarisation based on the corpus. The bottom-line summary is associated with two detailed justifications (not shown in the example), which in turn cite two research publications. The goal of the summarisation task is therefore to synthesise information from the source texts to generate the bottom-line summaries.

We have analysed the corpus for the goal of multi-document summarisation and proposed a two-stage summarisation process that first performs single-document summarisation and then uses the summaries as input to a multi-document summariser (Sarker and Molla 2012 ). 5.2 Evidence appraisal The literature associated with evidence-based medicine suggests that the appraisal of evidence is an integral component of evidence-based decision making. It is also a time-consuming component of the medical practice. Data in our corpus suggest that the appraisal of evidence can be considered to be an important subtask of the overall summarisation process. However, there has been minimal research on the systematic quality assessment of medical evidence, and this research is often based on bibliometrics or direct mapping to specific types of publications or studies (Molla  X  and Sarker 2011 ). Our corpus can contribute to the development and testing of clinical evidence appraisal systems, since it contains annotations for the qualities of the evidences associated with the bottom-line answers. The grades assigned use the Strength of Recommendation (SOR) taxonomy (Ebell et al. 2004 ) and the grade assignment process involves the systematic assessment of the source documents to identify key information indicating the evidence grade. As explained earlier, the quality grade for a bottom-line recommendation depends on the evidence associated with it. The corpus contains the evidence associated with each recommendation in the form of the PubMed articles ( d ijk ). We can, therefore, use information from these articles to predict evidence quality grades for the bottom-line recommendations.
Table 3 gives an example of the type of information in our corpus that can be used for automatic appraisal. The figure shows the question, the grade assigned to a bottom-line summary associated with the question (the bottom-line summary is not shown in this case), and the source abstracts responsible for the grade assigned. The goal of this task is therefore to analyse the information in the source texts to deduce the quality of the evidence presented in them. Note that the input information for this task may be identical to the summarisation task, so that the two tasks of evidence appraisal and text summarisation can be executed in parallel. We can therefore model the problem of automatic grading of evidence as a text classification task and attempt to solve it using machine learning algorithms. This corpus has been used in a shared task organised by the Australasian Language Technology Association (ALTA) in 2011 (Molla  X  and Sarker 2011 ). The shared task made available a partition of the data for development, and a separate partition for testing. The data included the clusters of documents, together with the target SOR code in the development data. 5.3 Clustering As has been said earlier, each question has multiple candidate answers, and each candidate answer has a set of documents that back it. We wanted to test whether clustering techniques would suffice to partition the overall set of documents relevant to a question into these subsets that were relevant to each candidate answer. For this clustering task, the input data is a question together with its related set of documents, and the output data is the groupings of the documents. An example of the use of the corpus for a clustering task is shown in Table 4 .

We have conducted preliminary experiments that use K-means clustering on several document distance metrics (Shash and Molla  X  2013 ). We observed that simple K-means based on unigram-based features is a baseline that is very hard to beat. The baseline was finally beaten by using supervised clustering and multi-objective optimisation techniques (Ekbal et al. 2013 ). However, note that traditional clustering, including K-means, produces non-overlapping clusters, and it would be interesting to test whether overlapping clustering would produce better results. We have developed a corpus for the development and testing of Natural-language processing techniques for evidence based medicine (EBM). The corpus was sourced from the Clinical Inquiries section of the Journal of Family Practice, with kind permission from the publishers. We combined automatic methods to extract parts of the data from the sources, and employed human annotators to assign the specific text that is related to each component of the answer. We also used anonymous annotators in crowdsourcing techniques to identify the PubMed ID of the references. The corpus is available at the SourceForge project page. 18
The corpus contains the abstracts of the references. Whereas it might be desirable to include the full documents, we encountered substantial obstacles, including issues with copyright, multiple formats, including formats that are not easy to process such as PDF, and even the fact that the full text was not available in some cases, especially for the older references.

The corpus was designed for tasks related to query-based multi-document summarisation of medical publications, but it can also be used for other tasks. We have briefly documented its use for single-document summarisation, evidence-based grading, and clustering. We welcome the use of this corpus for further research in these and other areas of natural language processing.
 References
