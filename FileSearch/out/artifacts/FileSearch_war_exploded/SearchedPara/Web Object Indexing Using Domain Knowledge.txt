 A web object is defined to represent any meaningful object embedded in web pages (e.g. images, music) or pointed to by hyperlinks (e.g. downloadable files). In many cases, users would like to search for information of a certain  X  X bject X , rather than a web page containing the query terms. To facilitate web object searching and organizing, in this paper, we propose a novel approach to web object indexing, by discovering its inherent structure information with existed domain knowledge. In our approach, first, Layered LSI spaces are built for a better representation of the hierarchically structured domain knowledge, in order to emphasize the specific semantics and term space in each layer of the domain knowledge. Meanwhile, the web object representation is constructed by hyperlink analysis, and further pruned to remove the noises. Then an optimal matching between the web object and the domain knowledge is performed, in order to pick out the structure attributes of the web object from the knowledge. Finally, the obtained structure attributes are used to re-organize and index the web objects. Our approach also indicates a new promising way to use trust-worthy Deep Web knowledge to help organize dispersive information of Surface Web. H.2.8 [ Database Management ]: Database Applications  X  Data Mining ; I.7.m [ Document and Text Processing ]: Miscellaneous; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Clustering, Selection Process. Algorithms, Performance, Experimentation. Web object, indexing, domain knowledge, latent semantic indexing, link analysis, confidence propagation, information retrieval, music indexing Contemporary web search engines are mainly  X  X age oriented X , that is to say, their indexing granularity is web page. As a result, they are only able to provide search results in the form of ranked web pages with respect to user's query. However, in many cases, the users want to search for information of a certain  X  X bject X  rather than the web page. For example, users may use query  X  X rtist Beatles X  to search for the biography about Beatles band, their albums and songs, instead of the web pages that contain the query terms only. To meet this requirement, an  X  X bject X  oriented search engine is required. Consequently, it is expected that some integral technique could be developed to index the web objects. In this paper,  X  X eb objects X  are defined to represent those meaningful objects embedded in web pages (e.g. images), or pointed to by hyperlinks (e.g. song streaming). Usually, the surrounding texts (including anchor text) can preliminarily describe a web objects. Complementary information of the web object may also be possible presented in the neighboring pages that have hyperlinks among them. Figure 1 illustrates two examples of web object, with or without descriptions in the surrounding texts. Figure 1(a) stands for a  X  X ook X  object, with a little valuable information in the surrounding text. The information about its author, publisher, and the biography of the author are found in the pages hyperlinked with it. Figure 1(b) illustrates a  X  X ong X  object with the descriptions of its containing album and performer in the neighboring pages. In a broad sense, almost everything on the web can be regard as some kind of web object, including those virtual objects or concepts described in web pages (e.g. a music review or book review). Web objects usually lie in some implicit structure organizations. For example, song objects belong to one layer in the hierarchical structure of artist-album-song, as Figure 2 shows. Structured organization of web objects provides a good directory to facilitate user to browse web resources distributed dispersively. Usually, we have some a priori concepts about this ontology. For example, in some Deep Web sites, web objects are described according to the ontology shown in figure 2, where each node is described by an individual web page, and the hierarchical relationships between them are presented by hyperlinks, such as All Music Guide (for music) [8] and Amazon (for books). These Deep Webs provide meaningful and domain-specific structures for web objects, and thus can be greatly helpful in web object indexing. In fact, even if the domain knowledge does not have a structure exactly the same the ontology. An example is given in the experiment section. However, for most web objects in the surface web, this structure does not exist, or exists implicitly . Thus, a natural idea is to utilize the structured domain knowledge in the Deep Web, or other knowledge databases, to help the information organization of Surface Web. Assuming that in our domain knowledge (such as the above-mentioned Deep Web), each document is a description of a certain web object, and the relationships among these knowledge documents are as shown in figure 2. Then the description of a web object is mentioned as a  X  X nowledge document X  in the following sections. It is noted that, web objects can be indexed, using not only the information contained in the knowledge document that best matches the web object, but also the information contained in the best-match X  X  relative documents, according to the hierarchical structure. Figure 2 Hierarchical organization of music web objects, which includes three layers: ar tist, album and song. To facilitate web object searching and organizing, in this paper, we propose a general framework to index web objects, and further, organize them based on their implic it structure, with the help of hierarchically structured domain knowledge. To achieve such an objective, we should solve two major difficulties, 1. Lack of information. Usually, there is little descriptive textual information around web objects. The information may be noisy and insufficient to represent web objects. It is critical to enrich the description of the target web object and prune the noisy information. 2. Difficult to identify structured attributes. Even if the description of a web object is sufficient and extracted, it still leaves a difficult problem to extract the structure information of the web object. Automatic discovery of implicit structure is very challenging. Although some hierarchical clustering methods are proposed to detect semantic structure, the obtained hierarchy is usually not meaningful to the target web object that is domain specific [15]. It will also meet some traditional obstacles like polysemy and synonymy due to various word usages in different web pages. Moreover, the traditional wrapper-based information extraction approaches are also not suitable in these cases, since a wrapper is usually suitable to only one kind of web pages but could not adapt to all kinds of web pages, which are usually diverse in their design format. To deal with these problems, we propose a novel non-clustering and non-wrapper approach to index web objects in various web pages, by integrating link analysis and domain knowledge. Hyperlink analysis is used to enri ch the textual description of the web object; and domain knowledge is used to help noise removing and structure attributes construction of the domain-specific web objects. The rest of this paper is organized as follows: Section 2 presents the proposed framework. Section 3 describes our layered indexing scheme for hierarchical domain knowledge. Section 4 proposes our approach to web object representation through text and link analysis. Section 5 presents the matching process between web objects and the documents in knowledge database. Finally, the proposed approach is evaluated in Section 6, and an example application in music domain is presented in Section 7. Related work and conclusions are given in Section 8 and 9. Figure 3 illustrates our proposed framework for web object indexing. It is mainly composed of three steps: knowledge space building, web object representation, and web object indexing. Step One: Knowledge Space Building. In the first step, the domain knowledge is reorganized by Latent Semantic Indexing (LSI), for further use in web object representation and identification. A knowledge source is usually hierarchical or tree-like (as shown in Figure 2), where different layers have different semantics. Therefore, it is a better choice to represent knowledge layer by layer. Thus, in this step, know ledge is indexed in each layer respectively, and Layered LSI spaces are built for the domain knowledge. Step Two: Web Object Representation . Meanwhile, we need the preliminary description of the target web object. In this step, textual information of the web object is extracted and pruned. Firstly, a preliminary neighborhood graph is constructed around target web object with hyperlink analysis, in order to enrich the description of the web object. Then, the content of each web page is projected to the Layered LSI spaces of domain knowledge, to remove the irrelevant words to the web object. In order to obtain a better web object representation, the neighborhood graph is further pruned to remove noisy pages. After pruning, all web pages in the pruned neighborhood graph are combined together to get a new description vector of the target web object. Step Three: Web Object Indexing . Based on the above two steps, the major task in this step is to identify the structure attributes of the target web object by the matching of its description vector with domain knowledge. To accomplish this task, the similarity between the description vector of web object and each knowledge document is firstly measured in the Layered LSI spaces; and then a process of structure-based confidence propagation is performed to select the knowledge document which best matches the web object. The corresponding structure attributes in the knowledge document are then used to index the web object. Although the proposed framework emphasizes on utilizing the domain knowledge and the corresponding hierarchical structure, our approach is still feasible for non-hierarchical structure, which can be considered as a special case of hierarchical structures, with the layer number equal to one. It is noted that, in this paper, we mainly focus on web objects indexing, and assume web objects have been detected. In practice, web object detection is usually domain specific and thus can be solved with some simple heuristic rules. Both the web object description and knowledge database are textual, but usually are authored by different authors. This fact leads to their different term spaces. Consequently, directly using domain knowledge to identify web object is not feasible. In order to solve this problem, in our approach, Layered Latent Semantic Indexing is used to index documents in the knowledge database. In some degree, LSI is capable to deal with polysemy and synonymy problems as indicated in many textual retrieval applications [6]. Usually, each layer of the hierarchically structured knowledge represents different concept, and has different term space. For instance, the three layers in music domain knowledge represent the concepts of artist, album and song, respectively; and the term  X  X eatles X  is obviously more probable to appear at the  X  X rtist X  layer, but not at the layer of  X  X lbum X  or  X  X ong X . Moreover, the scale of the term space of each layer is also different. Thus, if all layers are indexed together into one LSI space, the layer with smaller term space will be overwhelmed by the larger one. For instance, terms from smaller space will have a much lower frequency, after combined with a much larger space. That means, the term importance in the corresponding layer will be unfairly reduced. Therefore, in our approach, we index each layer independently and thus compose Layered LSI spaces. We will show the advantages of this layered method in the experimental section. In our approach, the textual content contained in each document or record of the knowledge source is considered as a knowledge document . These documents are tokenized to construct a term-document co-occurrence matrix. To improve the effectiveness of LSI, TFIDF is used as the weigh ting function in the co-occurrence matrix instead of term frequency only. In our approach, the simplest TFIDF formula is employed, as: where w denotes a term, f(w) represents the term frequency, N is the dimension of term space, and D(w) stands for the set of documents that contain term w . Supposing there are c layers in knowledge database, and the weighted term-document matrix in each layer is denoted as A =1,...,c ) , each matrix can be decomposed by SVD as Then, largest k singular values are selected to construct the latent semantic structure of i A , denoted as k i A : Thus, each document (or query) q can be represented as 
U q in the new semantic space. Domain knowledge will be further used in the web object representation and web object indexing. In these modules, all the web pages (describing the target web object) are mapped to the Layered LSI spaces, in order to remove irrelevant words that are out of domain. As mentioned above, each layer of domain knowledge has its unique semantic and term space. Thus, when we project each page into the LSI space of one layer, in some degree, only the words that are accord with the corres ponding semantics are amplified; while other irrelevant words are suppressed or removed. Therefore, in the further processing, the similarity measures, either between two web pages or between web object and knowledge document, are computed in the Layered LSI spaces. It is more reasonable than direct comparison in the original space. Moreover, such a similarity comparison is actually measured in the LSI space of each layer, respectively. That is, the comparison is made from different semantic aspects. The basic idea behind this process is that we can use the information from all kinds of aspects to cross-verify the final decision. Our experiments also indicate the effectiveness of this method. The preliminary information of the web object can be found in the web page containing it or the corresponding web block [15]. However, the information is usua lly insufficient to describe the web object and may also contain many noisy words. In order to improve the web object representation, a neighborhood graph around the web object is constructed with text and hyperlink analysis. Neighborhood pages are considered in this case since they usually can help verify or complement information about web object. In this section, we first present the algorithm on neighborhood graph building and then address how to prune the noisy pages, which are irrelevant to the target web object. To build a neighborhood graph, the web page or the web block containing the target web object is taken as the graph centre, and all the web pages are  X  X onnected X  according to their hyperlinks. The utilized approach to neighborhood graph building is based on the technique proposed by Harmandas [4]. That is, it constructs an undirected graph, where each web page is taken as a node and each hyperlink as an edge, as illustrated in Figure 4. In the following sections, the web page (or web block) containing the target web object is referred as container for simplicity, while the pages linked with the container are called neighborhood pages , Figure 4 A 2-layer neighborhood graph with 1-and 2-step pages In general, the less steps the page is from the container, the more closely the corresponding node is related to the target web object. Therefore, the number of the Graph Layer is an important parameter in web object representation. With larger neighborhood layer number, more information can be used to describe the web object, but more irrelevant (noisy) information is also introduced. In the experiments, we will discuss its selection. The neighboring graph is assumed containing more information to describe the target web object, compared with the container itself only. However, in the graph there are possible many noisy pages, which do not contribute to our task and will even lead to 'topic drift' [1], such as the ' contact us ' page and ' how to buy ' page. They are necessary to be removed for the further processing. To remove the noisy pages, the similarity between each neighborhood page and the container should be calculated, assuming the pages similar to the container are more relevant to the web object. In our approach, the cosine measure is used to measure the similarity between each neig hborhood page and container in each layer X  X  LSI space. The similarity in the j th-layer actually represents the confidence that whether the corresponding page has mutual semantics with the container from the view of the j th-layer. Traditionally, the pages with small similarity can be considered as noisy pages [1]. In our experiments, a portion of pages with the smallest similarity in each layer is taken as noisy pages. The threshold is called cutoff percent in our paper and is determined experimentally. By measuring the similarities between the container and the other pages in different layers of the domain knowledge, we can get one candidate set of noisy pages from each layer, respectively. Since different pages have different semantics or functions, they may get different confidence in different layers. Therefore, only the intersection of these candidate sets is confirmed as noisy pages, which will be pruned from the neighborhood graph. For some special cases where the knowledge database is not hierarchical, e.g., for that consists of some plain text documents only, our pruning scheme is still available with the layer number equal to 1. Once removing noisy pages in neighborhood graph, we can integrate the textual information of all pages together to better describe the web object. In our approach, the weighted centroid (of the neighborhood graph) is simply used as a description vector of the web object, which is defined as, where G is the pruned neighborhood graph, D i is a web page in G , according to the path steps from the corres ponding web page to the container. A general rule is, the fewer steps the page has, the higher weighting it will be assigned. In our approach, the weighting is experimentally defined as equation (5) and then normalized to one, where d i is the corresponding path steps, and the coefficient  X 2 X  is heuristically added in order to avoid zero in the denominator. To this end, we get a description vector of the target web object in Layered LSI spaces. Then, the vector is used to extract more exact structure attributes of the target web object, by discovering the most appropriate knowledge document which best matches the web object. Thus, the problem is converted to textual information matching, that is, matching between the web object representation and each document in domain knowledge. At last, the obtained structure attributes in knowledg e document will be used to index web objects. To achieve object-knowledge matching, the similarity between the description vector and each knowledge document in each layer is calculated using cosine measure. Usually, a web object belongs to only one certain layer of knowledge database. Therefore, our goal can be simply achieved by finding the candidate document that is most similar to the target web object, from the corresponding target layer in the knowledge database. However, this method might be not accurate e nough, since the similarity score only provides some confidence, and some noises would harm the matching accuracy. To deal with this issue, the hierarchical structure of domai n knowledge is utilized, and each candidate document in the target layer is re-scored or re-ranked by confirming with its relatives (including ancestors and offspring). For example, as Figure 5 shows, when we re-rank the confidence of the node T (a candidate document), the confidence of its ancestors (node 1-2) and offspring (node 3-7) are all incorporated. This process is called confidence propagation . The confidences from the candidate document X  X  relatives actually represent the similarities between the candidate document and the target web object in the corresponding semantic attributes (layers). The final score is thus cross-verified from various semantic aspects. The confidence propagation from the relatives of a candidate document can be divided into two directions, propagating from its ancestors (root) nodes (top-down), and propagating from its offspring nodes (bottom-up), as Figure 5 illustrates. In the top-down propagation, we directly sum the confidence score of up-layers in the path from the root to the candidate document. It is feasible since the path from root to the candidate document is unique. However, from its offspring, the bottom-up propagation path is not unique. We could also sum scores of all paths from the candidate document to its offspring for confidence propagation. However, it is usually unfair since the candidate documents with lots of offspring would be over-weighted. To deal with this problem, the candidate document with a  X  X ig X  sub-tree is punished by dividing the number of its children, in our approach. The detailed formula of confidence propagation is shown in equation (6), which sequentially combines the information from upper and lower layers, as well as normalizes information coming from lower layers. where s(.) represents the original confidence of a document, S(.) means the updated confidence score of a document, U(.) and D(.) means the propagated confidence from bottom-up and top-down, respectively. r is a knowledge document and its subscript represents its layer number. L is the layer number of domain the set of parents and children of a document, respectively, and NC(.) is the number of its children. After confidence propagation, the candidate document with the highest similarity score S(r TL ) is chosen as the optimal document, which best describes the structure attributes of the target web object. The structured attributes of this web object are further used in indexing and organization. It is noted that, in the case that the knowledge database is not hierarchical, we need not do any confidence propagation. Once web object is matched with a knowledge document, the trust-worthy domain knowledge can be used to further index and organize web objects. It can be divided into two cases: 1. If the domain knowledge is fully structured, such as a database, the keys of each record can be used to represent the structure information of the web object and then index them. For example, if a song object is matched with a record in the database, in which the corresponding metadata, such as artist, album, genre and release year, are available as keys of the record, then these metadata can be further used to index the song object as  X  X ey words X  or  X  X ndex terms X . Furthermore, based on the structure of the domain knowledge, the key words of the related records can be used to describe and index the target web object, too, and many applications can be developed based on these structure indices. 2. If the domain knowledge is not a structure database, for example, the knowledge documents only contain plain texts, we can use the text to describe web object better. Then, traditional indexing scheme in text search/retrieval can be used to index the web objects. In this section, we evaluate the performance of the proposed approach in the music domain, including, 1. We evaluate the overall performance of our approach, respect to various settings of neighborhood graph, such as the graph layer and cutoff percent mentioned in section 4. We also look into the performance on different web objects. 2. We compare our approach with the algorithm proposed in [4] (note that the algorithm in [4] is just a specific version of our approach in step 2 (web object representation), with the cutoff percent equals 0). 3. We evaluate the effectiveness of each m odule of our proposed framework, including domain knowledge indexing, neighborhood graph in web object representation, and confidence propagation in web object matching. In order to measure the precision of our approach, the process of web object indexing is viewed as a ranking problem, as follows. That is, on one hand, the knowledge documents are ranked according to their similarities with the description vector of the web object, and then the top N are returned; and on the other hand, only one correct document is labeled in the ground truth, assuming there is only one  X  X est description X  for each web object in our domain knowledge. Hence, the ranking of the labeled document in the N returned results can be used to evaluate the performance, as where Q is the set of web objects and q i is a target web object, R is the top N results corresponding to object q i , and | X | is to get the count. In our experiments, P@1 , P@5 , and P@10 are used for performance evaluation. All our algorithms are implemented in C++, and experiments are run on two workstations with two 3.0G Hz Intel CPUs each and 3.0G memory. For SVD, we use SVDPACK which can deal with large-scale sparse matrix [9][13]. Music is one kind of most valuable objects on the web, and many mature music databases exist as Deep Web sites, which can be used in our experiments as domain knowledge conveniently. Therefore, our approach is performed and evaluated in the music domain. In this section, we present the data preparation for domain knowledge and testing music objects, which are further divided into song objects, album objects and artist objects. In our approach, the domain knowledge about music is collected from All Music Guide [8], which is a Deep Web site. In order to fit the structure of domain knowledge into our proposed ontology, the artist pages, album pages and song pages are downloaded, and then organized into a three-layered, tree-like structure, in which each layer represents artist, album, and song, respectively, as shown in Figure 2. A specific HTML parser is developed to re-build up the knowledge hierarchy from the crawled pages. The parser is used first to explore the implicit semantic relationships between pages, based on some heuristic rules, such as the nomination of the URLs; and second, to extract specific keywords, such as artist name, album title, and genre, for further web objects indexing and other applications. The details of our collected domain knowledge are shown in Table 1. Overall, there are 32513 pages, including 26279 song pages, 4635 album pages and 1599 artist pages.
 After the knowledge is ready, LSI is used to index them. In text retrieval domain, many researchers reported that keeping 100 -300 dimensions is a good trade-off between performance and computation complexity of LSI. In our experiments, the dimension is set as 200 accordingly. Based on our algorithms, three layers of the knowledge tree are indexed respectively. For comparison, we also used the non-layered LSI, by combining all documents of three layers into a whole corpus. As mentioned above, we mainly focus on web objects indexing, and assume web objects have been detected. Since there is few work that addresses the detection task, in our experiments, web objects are detected with some simple heuristic rules. For example, a song object usually associates with some file-type indicators, such as file extension 'rm' or 'wma', and keywords  X  X isten X . In order to collect the testing web objects, we first collect the top queries from search log of All Music Guide in one week, which includes hundred of songs, albums and artists, respectively. Then, we throw the valid queries into G oogle to obtain some related web pages, and crawl pages by using them as seeds. In the crawled pages, only those, which contain hyperlinks pointing to music files (with special extensions, such as rm, mp3, wma), are labeled as music object pages. It is noted that, the web objects are not selected out from the pages that G oogle returns to us, hence, it is very rare that the page containing the web object can be returned by Google, using the original query terms. Besides, if the selected music object has no proper description in the knowledge tree, it will be removed, in order that each potential music object has corresponding structure information in the domain knowledge. However, we have also tried some of these web objects in our approach. Although it is impossible to return the correct result (because there is no correct records in the knowledge), we find that the returned results usually have some semantic similarity with the target object. For example, if an album X  X  description is not in the knowledge source, usually another album sharing same artist with the target will be found. A 2-layer neighborhood graph is extracted for each music object. We do not consider 3-step pages (in the third layer), since the web pages increase explosively but a majority of the pages are irrelevant to the target music object. We also limit the total page number under 500 in neighborhood graph building, in order to limit the computations. Finally, the ground truth is manually annotated for evaluations. Table 2 shows the details of our testing music objects. In the first experiment, we evaluate the overall performance of the proposed approach on testing song objects. In summary, our approach is configured as following modules, domain knowledge representation using Layered LSI spaces, neig hborhood graph building and pruning, and hierarchical web object matching. However, the performance is also related to two parameters, graph layer and cutoff percent in the construction of the neighborhood graph. Figure 6 illustrates the system performance (P@5) on song objects indexing, corresponding to various graph layer and cutoff percents. In our experiments, the graph layer is chosen as 1 or 2, due to the explosive number of irrelevant 3-step pages; and the cutoff percent ranges from 25% to 100%, with an interval of 5%. Figure 6 Overall performance respect to various graph layers and cutoff percents From Figure 6, it can be seen that more than 80% web objects can be correctly found in the returned top 5 knowledge documents, with 60% cutoff in 1-layer graph and 85% in 2-layer graph. It can performance is firstly increases and then decreases. It is reasonable, since when the cutoff percent is small, noisy information mainly affects the performance; while the cutoff percent is large, relevant information may be also filtered so that the precision decreases. Moreover, the best performance using 2-layer graph is 4% better than that of 1-layer graph. However, a majority (85%) of pages in the 2-layer neighborhood graph are considered as noise and removed. Although it means some waste in page crawling and parsing, in the following experime nts, we still uses 2-layer graph (including container, 1-step and 2-step pages) with optimal cutoff 85% as our default setting, in order to get higher performance. In our experiments, we also find the performance of our approach is heavily depended on descriptive information of the song object. In order to look into the performance on different testing song objects, we manually classify them into three sets, including  X  self-descriptive  X ,  X  multi-page descriptive  X  and  X  non-descriptive  X  object. The  X  X elf-descriptive X  object contains sufficient information in its container page for human to tell its structure property; and the  X  X ulti-page descriptive X  object have to be identified from the container combining with the neighborhood pages; while the  X  X on-descriptive X  object does not contain sufficient discriminative information in its neighborhood graph. Figure 7 illustrates the performances corresponding to different sets of song objects. From the figure, we find that the performance on self-descriptive set first increases and then almost keep the same when cutoff percent grows, while the performance on multi-page descriptive set has a increasing phrase and decreasing phrase, with optimal cutoff equal to 85%. It actually reflects the tradeoff between the noisy information and complementary information contained in the neighborhood pages. However, the performance of non-descriptive set is always poor. It is intuitive since no sufficient information is provided in the neighboring pages. The above section evaluates the overall performance of our proposed approach. In this section, we evaluate the effectiveness of each m odule which is corresponding to each step in section 2, including Layered LSI spaces for domain knowledge, neig hbor-hood graph, and confidence propagation. In our approach, domain knowledge is used, and Layered LSI spaces are built to represent domain knowledge, since different layer of domain knowledge usually has different semantics and different term spaces. In order to evaluate the effectiveness of domain knowledge and Layered LSI, an experiment is designed to compare the methods with different usages of knowledge, in the module of neighborhood graph pruning (keep other modules the same). The compared methods include 1) Layered LSI on domain knowledge, 2) one-layer LSI on domain knowledge, and 3) without domain knowledge (that is, comparing pages in neighborhood graph directly [1]). Figure 8 Precision comparison on different indexing approaches The performance comparison of these three approaches is shown in figure 8. It is obvious that the performance of Layered LSI indexing is best. It is about 5% better than that without LSI, and about 3% better than one-layer indexing. It not only indicates knowledge is helpful in web object representation, but also indicates that layered indexing can obtain better results. In section 4, the neighborhood graph is built to enrich the description of the target web object, since the container page usually does not provide sufficient discriminative information. It is especially suitable for those multi-page descriptive objects. Figure 9 illustrates the performance of multi-page descriptive object indexing, comparing among 1) 1-layer neighborhood graph (with optimal cutoff 60%), 2) 2-layer neighborhood graph (with optimal cutoff 85%), and 3) without neighborhood graph (only the container page). Figure 9 Performance comparisons with/without neighborhood graph for multi-page descriptive song objects It can be seen that, with considering neighborhood pages, the performance is dramatically improved. For example, the P@1 and P@5 are almost doubled after using 1-layer graph or 2-layer graph. In Section 5, confidence propagation is performed to re-score the similarity between a candidate document and the target web object. After propagation, the confidence of candidate document is verified by confirming with its relatives, and consequently, better results are obtained. Figure 10 illustrates performance comparison between with confidence propagation and without confidence propagation, in the module of web object matching. Figure 10 Performance comparisons with/without confidence propagation It can be seen that, with confidence propagation, the precision (P@1, P@5 and P@10) is improved about 5%-7%. It indicates confidence propagation is also very helpful in web object indexing. To explore the performance on music objects in the same domain but located in different layers of the knowledge database, we also evaluate the performance on album objects and artist objects. The detailed comparisons are illustrated in Figure 11. It is noted that, the performance on artist and album object is much better than song object, especially on P@1. There are two reasons. Firstly, in music domain, the terms used to describe an artist or album is usually much more than a song. In other words, there is much more information th at can be used to distinguish artists and albums. Secondly, information in song page is usually not discriminative enough. By combining with neighborhood pages, the description of a song object has a large overlap with those songs from the same album. As a result, it is usually difficult to discriminate the songs from a same album, while it is relatively easy to identify an album or artist object. Therefore, the P@1 of song object is much lower than that of albums and artists; however, for P@10, the values are very similar since an album usually has about 10 songs only. The proposed technology on web object indexing can be used in various domains. To illustrate furt her use of indexed web objects, an example prototype on music search is developed in this section. The prototype is designed to organize numerous music resources dispersed on the Surface Web. With the obtained structured indices using our approach, the search results are organized according to their implicit structure. Moreover, all the related information, which is discovered from the knowledge documents, could also be associated with the search results and presented to users. Figure 12 illustrates a snapshot of the prototype system. With the query  X  X esterday X , the returned results are organized according to their artist, album and year. As shown in the left panel of Figure 12, the different versions of  X  X esterday X  and similar songs were clustered to different artists, such as the Beatles, Boys II man and Cilla Black; and then they are further classified into different albums, such as  X  X essions X  and  X  X esterday...and Today X . Moreover, the related information of current category, which are obtained from the matched knowledge documents, is shown in the right panel of Figure 12, including  X  X ong details X ,  X  X lbum information X  and  X  X rack list X . The indexed web objects, which are obtained from the Surface Web and associated with the current category, are also listed in the  X  X bject Links X  block. The prototype provides more related and organized information than contemporary web music search engines. The obtained structure is more meaningful in the domain than those obtained by traditional semantic clustering approaches [14]. Previous works on object-oriented indexing can be traced back to the framework of image retrieval proposed by Harmandas [4], in which a model for Web images retrieval is presented. The model is based on combining the text content and hypertext in neighborhood graph. However, their definition on objects is limited to web images, and they did not consider finding the underlying structures of web objects, either. There are also some efforts to obtain music structure information from Deep Web sites. For example, Windows Media Player 10 has the function to extract album information of a song from MSN Music, based on other metadata embedded in the ID3 tag. Using a connective cluster of web pages to improve web search is also addressed in some work. Kleinberg's HITS [2] is an early important effort using link analys is to rank documents, while it only depends on in/out-degrees of links. Bharat et al. [1] identified a problem of HITS: topic drift , and proposed to resolve it by content analysis in local graph. This modification achieved a remarkable better precision in topic distillation application. By manipulating the weights of pages, Chang et al. [3] created customized authority by adding more weights to the documents that users are interested in. Recently, researchers pr oposed to use connective analysis to improve term weighting scheme in hyperlink environment [5]. However, none of them considered using domain knowledge to improve the combination of link analysis and content analysis. Similar to our work (matching web object representation in knowledge database), "DB+IR" [10][11][12] was proposed in the database domain to use unstructured query to search structured database. Different from traditional SQL (Structure Query Language) technologies, some works in "DB+IR" regard database records as plain text, and use IR technologies to index and search database. Some researchers also proposed to improve search precision by using the structure of database, such as XSearch[12], which utilized the structure of XML DOM tree, and proposed to use ILF(Inverse Leaf Frequency) to punish common terms in leaf nodes, which was repl acement of traditional IDF. In this paper, we propose a novel approach to indexing web objects by using corresponding domain knowledge. Although in this paper our method is utilized and evaluated in the music domain only, we believe that our approach is feasible for general web objects indexing. Here we conclude our approach: 1. Domain knowledge from traditional databases or Deep Web sites could be used to index web objects to help build a domain-specific structure. This method overcomes the disadvantages of traditional approaches on hierarchical clustering and automatic ontology building. 2. Indexing hierarchical knowledge database layer by layer is a good strategy, which emphasizes each layer's concepts and suppresses noises effectively. Moreover, this method enables us to compare objects from various aspects (various attributes of object). It is an improvement of th e traditional approaches using knowledge. 3. Confidence propagation can effectively utilize the relation-ships among nodes of knowledge tree. Each node's confidence can be amplified or suppressed with the confidences of its relatives. It actually also provides an enhancement for traditional IR methods. In the future works, we will apply our approach into more domains to build efficient systems. We will also further develop or integrate a better approach for web object detection, and use technologies on web information extraction to obtain better description of the web objects. [1] K. Bharat, and M. R. Henzinger. Improved Algorithms for [2] J. Kleinberg. Authoritative sources in a hyperlinked [3] H. Chang, D. Cohn, and A. McCallum. Creating customized [4] V. Harmandas, M. Sanderson, and M. D. Dunlop. Image [5] K. Sugiyama, K. Hatano, M. Yoshikawa, and S. Uemura. [6] S. Deerwester, S. Dumais, et. al. Indexing by Latent [7] Wordnet 2.0: A lexical database for the English language. [8] All Music Guide. http://www.allmusic.com [9] B. Michael, D. Theresa, et. al. SVDPACKC (Version 1.0) [10] B. Y. Ricardo, and M. P. Consens. The continued saga of [11] V. Hristidis, and Y. Papakonstantinou. DISCOVER: [12] S. Cohen, J. Mamou, Y. Kanza, and Y. Sagiv. XSEarch: A [13] SVDPACK. http://www.netlib.org/svdpack [14] B. Liu, C. W. Chin, and H. T. Ng. Mining Topic-Specific [15] D. Cai, S. Yu, J. Wen, and W.-Y. Ma. VIPS: a Vision-based 
