 sumption in such a case is that movies come in interacting latent causes explain each observed datum.
 features; similarly each attribute (column) has a hidden ve ctor of matrix matrices, and tration parameter and Beta weights for the columns of (B) BMF shown pictorally. binary features and the unknown weights. Binary matrix factorization is a model of an and columns. The entries of similarly each column has an unobserved binary vector by a matrix approximation of the data: the noise (observation) distribution logistic, with mean model found in [5].
 matrices described in [5]. For finite sized matrices generate the entries in column where they remain exchangeable. The resulting prior depends only on the number in each column. An identical prior is used on concentration prior . The variable was set to The appropriate prior distribution over weights depends on the observation distribution the linear-Gaussian variant, a convenient prior on covariance hyperpriors: the weights and use a linear-Gaussian output process.
 Remarkably, the Beta-Bernoulli prior distribution over to the case where only tractable, but is also nearly as efficient as the finite ve rsion. dure which, if run for sufficiently long, will produce correc t posterior samples. 3.1 Finite binary latent feature matrices The posterior distribution of a single entry in single entries of where on hoods corresponding to row This ratio is a simple function of the model X  X  predictions our variables. Let taken column-wise from data distribution is written as: can be drawn for for row between the new proposed configuration and the current config uration. present here only a sketch of the procedure. Two nonzero entr ies in split-merge algorithm also performs restricted Gibbs scan s on columns of probability. 3.4 Predictions over all binary matrices variables.
 By averaging predictive distributions, our algorithm impl icitly integrates over experiments, we show samples from the posteriors of and precision values (higher weight precision results in le ss variation in weights). 4.1 Modified  X  X ars X  problem tures. Data consists of The generation process is as follows: since images, sampled from the IBP, and global precisions and are set to from zero mean Gaussians. Model estimates of the expected reconstruction using MCMC samples of bars on the bottom half. neighbours in pixel space.
 Figure 3: Bars features. The top row shows values of second row shows a sample of set of basis images which can be added together with binary co efficients ( In Figure 3 we show the generating, or true , values of features from the Markov chain. Because the model is generat ed by adding multiple captured features. The learned composed of overlapping bar structure (learned 4.2 Digits We train logistic BMF with 100 examples each of digits show the mean and mode ( halves of the original digits.
 the average image of the data which have each feature in have distinct digit forms and others are overlapping. In row G, the basis images By adjusting the features that are non-zero in each row of images together. Finally, in row H we show in rows F and G, and from the top halves of the images. (F) The average of all digits for each reconstructions of the digits is possible. (H) represents a bias feature. 4.3 Gene expression data Bayesian special case of our model in which the matrix of the data and its expected reconstruction are ordered such that contiguous regions in ing BMF to model gene expression data would be to fix certain co lumns of the data in more detail. Figure 5: Gene expression results. (A) The top-left is the final right is regions that have both contiguous regions for each feature pair.
