 We describe a method for applying parsimonious language models to re-estimate the term probabilities assigned by relevance models. We apply our method to six topic sets from test collections in five different genres. Our parsimonious relevance models (i) improve retrieval effectiveness in terms of MAP on all collections, (ii) sig-nificantly outperform their non-parsimonious counterparts on most measures, and (iii) have a precision enhancing effect, unlike other blind relevance feedback methods.
 H.3 [ Information Storage and Retrieval ]: H.3.3 Information Search and Retrieval Algorithms, Theory, Experimentation, Measurement Parsimonious Models, Language Models, Relevance Feedback
Relevance feedback is often applied to better capture a user X  X  in-formation need [ 1 , 5 , 12 ]. Automatically reformulating queries (or blind relevance feedback) entails looking at the terms in some set of (pseudo-)relevant documents and selecting the most informative ones with respect to the set or the collection. These terms may then be reweighed based on information pertinent to the query or the documents and X  X n a language modeling setting X  X e used to esti-mate a query model, P ( t |  X  Q ) , i.e., a distribution over terms t for a given query Q [ 7 , 13 ].

Not all of the terms obtained using blind relevance feedback are equally informative given the query, even after reweighing. Some may be common terms, whilst others may describe the general domain of interest. We hypothesize that refining the results of blind relevance feedback, using a technique called parsimonious language modeling [ 3 ], will improve retrieval effectiveness. Hiem-stra et al. [ 3 ] already provide a mechanism for incorporating (parsi-monious) blind relevance feedback, by viewing it as a three compo-nent mixture model of document, set of feedback documents, and collection. Our approach is more straightforward, since it consid-ers each feedback document separately and, hence, does not require the additional mixture model parameter. To create parsimonious language models we use an EM algorithm to update the maximum-likelihood (ML) estimates. Zhai and Lafferty [ 13 ] already proposed an approach which uses a similar EM algorithm; it differs, however, in the way the set of feedback documents is handled. Whereas we parsimonize each individual document, they apply their EM algo-rithm to the entire set of feedback documents.

To verify our hypothesis, we use a specific instance of blind rel-evance feedback, namely relevance modeling (RM) [ 5 ]. We choose this particular method because it has been shown to achieve state-of-the-art retrieval performance. Relevance modeling assumes that the query and the set of documents are samples from an underly-ing term distribution X  X he relevance model. Lavrenko and Croft [ 5 ] formulate two ways of approaching the estimation of the pa-rameters of this model. We build upon their work and compare the results of our proposed parsimonious relevance models with RMs as well as with a query-likelihood baseline. To measure the effects in different contexts, we employ five test collections taken from the TREC-7, TREC Robust, Genomics, Blog, and Enterprise tracks and show that our proposed model improves performance in terms of mean average precision on all the topic sets over both a query-likelihood baseline as well as a run based on relevance mod-els. Moreover, although blind relevance feedback is mainly a recall enhancing technique [ 9 ], we observe that parsimonious relevance models (unlike their non-parsimonized counterparts) can also im-prove early precision and reciprocal rank of the first relevant result. Relevance models use a set of (pseudo-)relevant documents D to estimate a query model P ( t |  X  Q ) . We use method 2, as proposed by Lavrenko and Croft [ 5 ]: where q 1 ,...,q k are the query terms, P ( D i | t ) = P ( t | D and where c ( t ; D i ) is the count of term t in document D i the probability of observing t in the collection. Relevance models perform better when they are subsequently interpolated with the original query using a mixing weight  X  [ 4 ]: where | Q | denotes the length of the query.

Parsimonious language models may be used to reduce the amount and probability mass of non-specific terms in either queries, docu-ments, or feedback documents by iteratively adjusting the individ-ual term probabilities based on a comparison with a large reference corpus, such as the collection [ 3 ]. While relevance models already contain a way of incorporating a reference corpus, viz. Eq. 2 , we propose to make the estimate P ( t |  X   X  q ) more sparse. Doing so would enable more query-specific terms to receive more probability mass, thus making the resulting query model more to the point. We ap-proach this by parsimonizing the individual estimates P ( t | D ) in Eq. 1 through applying the following EM algorithm until the esti-mates do not change significantly anymore: E-step: e t = c ( t ; D )  X   X P ( t | D ) (1  X   X  ) P ( t | C ) +  X P ( t | D ) M-step: P ( t | D ) = e t P
To measure the effectiveness of our proposed feedback approach, both compared to a baseline and to relevance models, we use test collections from four genres (news, domain-specific, intranet, user generated content), using only the title field: 1. TREC disks 4 and 5, minus the Congressional Record , 2. TREC disks 4 and 5, minus the Congressional Record , 3. TREC Blog with two times 50 topics from 2006 and 2007 [ 6 ] 4. TREC Genomics with 36 topics from 2007 [ 2 ], and 5. TREC Enterprise with 50 topics from 2007 (document search Test collection Run MAP P @ 10 MRR TREC-7 TREC Robust 2004 TREC Blog 2006 TREC Blog 2007 TREC Genomics 2007 TREC Enterprise 2007 Table 1: Results per test collection for the baseline query-likelihood run (QL), relevance models (RM), and parsimonious relevance models (PRM) (best results are marked in boldface).  X  /  X  indicates a statistically significant difference as compared to the baseline or to the RM run respectively, using a two-tailed paired t-test at p &lt; 0 . 01 .
 For each topic set we construct three runs: (i) a baseline query-likelihood run without any relevance feedback or parsimonization (QL) [ 7 ], (ii) a run based on blind relevance feedback with Lavren-ko X  X  relevance model (RM) [ 5 ], and (iii) a run using blind relevance feedback with parsimonized relevance models (PRM). We fix  X  = 0 . 15 [ 3 ] and sweep over possible values for  X  and |D Q on mean average precision (MAP), precision at 10 (P @ 10), and mean reciprocal rank (MRR) using the optimal parameter settings (which were obtained empirically).

The results of our experiments are listed in Table 1 . The scores of the baseline approach are at the same level as, or better than, the median scores at the corresponding TREC task. From Table 1 we arrive at 3 observations: (i) parsimonizing relevance models has a positive effect on retrieval effectiveness in terms of MAP on all collections; most interesting are the statistically significant im-provements on TREC Robust 2004, since this specific collection is known for its difficulty at handling relevance feedback; (ii) par-simonizing relevance models improves the performance of these models on all measures, and in most cases significantly so; (iii) in most test settings the parsimonious relevance models improve re-trieval performance with regard to early precision and reciprocal rank, even though blind relevance feedback is considered to only have a recall enhancing effect [ 9 ].
We have used parsimonious language models to re-estimate term probabilities assigned by relevance models. We have evaluated the method on five test collections involving four document gen-res. Results show that parsimonious relevance models (i) improve retrieval effectiveness in terms of MAP on all collections, (ii) sig-nificantly outperform their non-parsimonized counterparts on most measures, and (iii) have a precision enhancing effect, unlike other blind relevance feedback methods.
This work was carried out in the context of the Virtual Labo-ratory for e-Science project. The work was also supported by by the Netherlands Organisation for Scientific Research (NWO) under project numbers 220-80-001, 017.001.190, 640.001.501, 640.002.-501, STE-07-012 and by the E.U. IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104. .
