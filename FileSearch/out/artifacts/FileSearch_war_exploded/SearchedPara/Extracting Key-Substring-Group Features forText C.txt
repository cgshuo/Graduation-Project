 In many text classification applications, it is appealing to take every document as a string of characters rather than a bag of words . Previous research studies in this area mostly focusedondifferentvariantsof generative Markov chain models. Although discriminative machine learning meth-ods like Support Vector Machine (SVM) have been quite successful in text classification with word features, it is nei-ther effective nor efficient to apply them straightforwardly taking all substrings in the corpus as features. In this pa-per, we propose to partition all substrings into statistical equivalence groups , and then pick those groups which are important (in the statistical sense) as features (named key-substring-group features) for text classification. In particu-lar, we propose a suffix tree based algorithm that can ex-tract such features in linear time (with respect to the total number of characters in the corpus). Our experiments on English, Chinese and Greek datasets show that SVM with key-substring-group features can achieve outstanding per-formance for various text classification tasks.
 H.3 [ Information Storage and Retrieval ]: Content Analy-sis and Indexing; H.2.8 [ Database Management ]: Data-base Applications X  Data mining ;I.2.6[ Artificial Intelli-gence ]: Learning; I.5.2 [ Pattern Recognition ]: Design Methodology X  Classifier design and evaluation; Feature eval-uation and selection Algorithms, Experimentation.
 Text Mining, Text Classification, Machine Learning, Feature Extraction, Suffix Tree.
 Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00.
Previous research studies on string-based text classifica-tion mostly focused on different variants of generative Markov chain models [42]. However, generative learning methods are often inferior to discriminative learning methods, in terms of classification performance.
 Although discriminative machine learning methods like Support Vector Machine (SVM) [15, 24, 31, 53, 55] and AdaBoost [52, 51] have been quite successful in text classifi-cation with word features, it is neither effective nor efficient to apply them straightforwardly taking all substrings in the corpus as features.

In this paper, we propose to partition all substrings into statistical equivalence groups , and then pick those groups which are important (in the statistical sense) as features (named key-substring-group features) for text classification. In particular, we propose a suffix tree [21] based algorithm that can extract such features in linear time (with respect to the total number of characters in the corpus). Thus dis-criminative machine learning methods are enabled to work on string representations of documents. In other words, this paper focuses on quadrant IV in Figure 1. Our experiments on English, Chinese and Greek datasets show that SVM with key-substring-group features can achieve outstanding performance for various text classification tasks.
In the rest of this paper, we first survey related works (in section 2), then propose our idea and algorithm for extract-ing key-substring-group features (in section 3), later present experimental results to show the effectiveness and efficiency (in section 4), finally make concluding remarks (in section 5).
The generative approach to string-based text classifica-tion assumes that each document is generated by a Markov chain model [42] according to its class. For each class C ,a Markov chain model M C is constructed using the collection of training documents, then a test document d is classified to arg max C Pr[ C | d ], where Pr[ C | d ] can be computed using the Bayes X  X  rule and Pr[ d | C ]givenby M C .

Markov chain models could be in fixed order/memory or variable order/memory.

Markov chain models in fixed order n are usually called n-gram language models [20, 50] in the field of natural lan-ing technique that can perform string-based text classifica-tion in a discriminative way.
 In spite of the success of SVM with words-based kernels, SVM with string kernel has not become popular for text classification tasks. On the one hand, the computational efficiency of string kernel is still not comparable to its words-based counterparts, though fast string kernel algorithms via dynamic programming [41], trie [55] or suffix tree [62] can accelerate the computation speed to some degree. On the other hand, string kernel has not shown text classification performance that is as good as those of words-based kernels [41]. The effectiveness of string kernel might be impaired by the following factors.

Firstly, it is known that string kernel leads to the ridge problem [24] when applied to natural language documents, i.e., kernel values between almost identical documents are much larger than those between different documents. A doc-strings with itself. However, even if two documents of length | d | share all words (with average length | w | ) but in different | d | ( | w | +1) / 2 matches only, where | w || d | .Thusthe Gram matrix has a dominant diagonal, which is generally considered problematic.

Secondly, string kernel utilizes all distinctive substrings in the corpus as features, therefore the redundancy of features is quite high, which is likely to hurt the performance of SVM in text classification [19].

Furthermore, string kernel weights different substring fea-tures only according to their lengths. Since the number of substring features used implicitly in string kernel is ex-tremely large, it would be infeasible to apply effective feature selection techniques(using  X  2 or information gain , etc.) [68, 19], feature weighting techniques (such as TF  X  IDF [60, 2]), or advanced kernel functions [28, 35, 72].
A corpus D = { d 1 ,d 2 ,...,d m } of size n (i.e., m k =1 | d k | = n )containsabout n ( n +1) / 2 substrings. Our most impor-tant insight comes from the fact that these substrings could be divided into statistical equivalence groups . All the sub-stringsinsuchanequivalencegrouphaveexactlyidentical distribution in the corpus, so it is not necessary to distin-guish among them for statistical machine learning. That is to say, a group of substrings could be taken in whole as a single feature. To further reduce the dimension of the fea-ture space, we filter the substring-groups by some statistical criteria. The suffix tree [21] data structure, which is exten-sively used in string data indexing and querying, makes it possible to extract this sort of features very efficiently.
In order to explain this, we first review the suffix tree data structure (in section 3.1), then propose the idea of key-substring-group features (in section 3.2 and 3.3), finally present the feature extraction algorithm and its time com-plexity analysis (in section 3.4). Definition 1. [21] Consider a string S of n characters: S = c 1 c 2 ...c n .The suffix tree T for S is a compacted trie [34] that stores all suffixes of S . Specifically, T is a rooted directed tree with exactly n leaves numbered 1 to n .Let r denote the root of T . Each internal node other than r has at
Theorem 4. [21] For any node v (other than r )in T with path-label x X  where x denotes a single character and  X  denotes a (possibly empty) substring, there must exist a unique node s ( v ) with path-label  X  in T .If v is an internal node, we can jump from v to s ( v ) in constant time via the suffix link between them. If v is a leaf, we can jump from v to s ( v ) in constant time through the so-called  X  X kip/count X  trick.

Theorem 5. [21] Ukkonen X  X  algorithm [59] can construct the suffix tree T for a string S of length n , along with all its suffix links, in O ( n ) time.

Definition 4. [21] A generalized suffix tree can be con-structed for a set of strings, i.e., a corpus, where each suffix of each string corresponds to a leaf.
The huge number of substrings in the training corpus D is an obstacle to making use of discriminative learning meth-ods for text classification. However, by examining the suffix tree T for D , we see that all substrings of D could be clus-tered into a relatively small number of equivalence groups , according to their match points in T .

Definition 5. Each substring P in D must have a match point in T , hence correspond to a node P . We define the substring-group SG v corresponding to a node v in T to be the set of substrings { P j | P j = v } .

Our idea is to take the substring-groups rather than indi-vidual substrings as features which would have been compu-tationally expensive. The reduction of the computation over substrings to a computation over groups is made possible by the following theorems.

Theorem 6. The substrings in a group SG v have exactly identical distribution over D . Thatistosay,ifasubstring P  X  SG v occurs in a document d  X  D for f times, any other substring P  X  SG v must also occur in that document d for f times.

Proof. This could be proved straightforwardly using De-finition 1 and Theorem 3.

Almost all machine learning [44] methods for text classi-fication only require the statistics of features, such as term frequency (TF) and document frequency (DF), to train clas-sifiers. According to the above Theorem, it is not necessary to distinguish among substrings in one group for (statisti-cal) machine learning. That is to say, such a substring-group could be taken in whole as a single feature. Note that this strategy would also help to solve the ridge problem and the high-redundancy problem of string kernel (Section 2.2.1).
Since all substrings in a group SG v have the same oc-currence frequency in D , we also call that frequency the occurrence frequency of SG v ,denotedby freq ( SG v ). From Theorem 3, we know that freq ( SG v ) equals to number of leaves in the subtree of T rooted at v .

Theorem 7. The substring-groups corresponding to the nodes in T (other than r ) partition the set of all substrings in D .
 in that group. The more unique characters following a sub-string, the more contextual independent it is, hence the more suitable to be taken as a feature.

The last two criteria ( -p and -q ) aim at removing highly redundant features. In fact, the probability Pr [ SG v | SG u ] is proportional to the mutual information [12, 46] between SG v and SG u , while the probability Pr [ SG v | SG s ( v ) ] is pro-portional to the mutual information [12, 46] between SG v and SG s ( v ) . If two substring groups have a high mutual in-formation, it should be sufficient to include just one of them as a feature.

The frequency information required by the above criteria can be computed efficiently using T ,(Theorem3).

Filtering substring groups by these five criteria not only reduces the dimension of feature space so as to improve ef-ficiency, but also helps to solve the ridge problem and the high-redundancy problem of string kernel (Section 2.2.1). Moreover, since the number of key-substring-group features is relatively small, some effective feature weighting tech-niques (such as TF  X  IDF [60, 2]) and advanced kernel func-tions [28, 35, 72] can be applied easily, as in words-based text classification.

We call the selected substring-groups key-substring-groups because they are statistically critical. We propose to use these key-substring-groups as features to facilitate discrimi-native learning for string-based text classification.
The above feature selection criteria are unsupervised: they do not exploit the class labels of documents. It is possible to use more effective supervised feature selection criteria like  X  2 or information gain [68, 19] afterward.

Another good unsupervised feature selection criterion is the document frequency (DF) [68]. We are able to count the DF for every substring in linear time taking advantage of constant-time Least Common Ancestor (LCA) algorithm [4, 21], as in [65], though we omit the details here due to the space limit.

Our idea about key-substring-groups has been partially inspired by the works on suffix tree (or suffix array or PAT tree) based key-phrase extraction [8, 11, 37] and document clustering [69, 73]. The essential difference is that we do not care about whether the extracted key-substring-groups are semantically meaningful or not. In contrast, our concern is the utility of key-substring-groups as features for text clas-sification.
We propose a feature extraction algorithm for key-substring-group features based on suffix tree [21], as shown in Figure 3. It converts each document in the training 2 corpus to a bag of key-substring-groups . The source code will be made open 3 .

Theorem 9. The key-substring-group extraction algorithm has linear time complexity O ( n ) ,where n is the number of characters in the corpus.

Proof. The construction of the suffix tree T using Ukko-nen X  X  algorithm takes O ( n ) time, according to Theorem 5. Then, each of the three preparation sub-routines perform a
With some little trick as in the matching statistics algo-rithm [9, 62], the proposed feature extraction algorithm could also handle the unseen test documents, though we omit the details here due to the space limit. http://www.dcs.bbk.ac.uk/  X  dell/ recursive traverse (depth-first-search) of T .Since T has n leaves and at most n  X  1 internal nodes according to Defini-tion 1 and Theorem 1, a traverse takes O ( n )time. Thecon-version from a document d to a bag of key-substring-groups takes O ( | d | ) time, because it processes | d | suffixes each in O (1) time according to Theorem 4. The trick here is to take full advantage of the suffix links to move from one suffix to the next suffix. Therefore the total time for conversion is O ( m k =1 | d k | )= O ( n ). In summary, the time complexity of the proposed key-substring-group extraction algorithm is O ( n ).

The above theorem indicates that the feature extraction algorithm is efficient and scalable, which has been confirmed by our experiments.
To evaluate our proposed approach, we conducted four ex-periments using SVM with key-substring-group features: (1) English text topic classification; (2) Chinese text topic clas-sification; (3) Greek text authorship classification; and (4) Greek text genre classification. In each experiment, we first extracted the key-substring-group features by the proposed linear-time algorithm 4 . The algorithm X  X  parameters l , h , b , p and q were tuned in the following way. Initially we took the default values for those parameters which would not fil-ter any nontrivial substring-group out. Then we reduced the number of features gradually by adjusting the parameters, while keeping an eye on the cross-validation performance on the training data. Finally we selected the parameter con-figuration that could achieve a good trade-off between the number of features and the cross-validation performance. After the key-substring-group features were extracted, we weighted every feature value using the classic TF  X  IDF [60, 2] scheme, and normalized all feature vectors to unit length. LibSVM 5 [7] was employed as the implementation of SVM 6 . We chose the linear kernel and set all its other parameters to their default values. In all these experiments, our proposed approach worked very well.
We did the English text topic classification experiment on the ten largest categories of the Reuters-21578 dataset 7 (with the  X  X pteMod X  training/testing split). Note that this dataset is widely regarded as the home-ground for words-based text classification but not string-based text classifica-tion.

All documents were pre-processed simply by lowercasing every alphabetical characters (letters) and converting every block of consecutive non-alphabetical characters to one  X  X pace X  character . Such pre-procession could be done easily on-the-fly .
In English or Greek text, words are separated by whitespace characters or punctuations. We uncondition-ally discarded every key-substring-group that does not start from the beginning of a word, in experiments (1), (3) and (4). This trick slightly increased the program speed without affecting the performance. http://www.csie.ntu.edu.tw/  X  cjlin/libsvm/
LibSVM uses the one-vs-one ensemble method [26] for mutual-exclusive multi-class classification tasks, i.e., in ex-periments (2), (3) and (4). http://www.daviddlewis.com/resources/testcollections/ reuters21578/ words-based SVM (polynomial kernel)  X  X   X  X   X  X  86.0% [29] string-based SVM (character sequence kernel) 77.3% [41]  X  X   X  X   X  X  substring-group features  X  and some representative existing approaches. gorithm did show very high efficiency in practice. The pro-gram implemented in C++ took less than 30 seconds on an ordinary Pentium-4 PC to process the Reuters-21578 top10 dataset. LibSVM also ran very fast, because the number of features was controlled to be relatively small.
We did the Chinese text topic classification experiment on the TREC-5 People X  X  Daily News dataset, as in [22, 23]. This dataset is a subset of the Mandarin News Corpus an-nounced by the Linguistic Data Consortium (LDC) in 1995. There are six topic categories: (1) Politics, Law and Society; (2) Literature and Arts; (3) Education, Science and Culture; (4) Sports; (5) Theory and Academy; (6) Economics. Each category contains 500 training documents and 100 test doc-uments. All documents were cleaned by removing SGML tags and merging continuous whitespace characters.
Setting the feature extraction parameters as -l 20 -h 8000 -b 8 -p 0.8 -q 0.8 , we got a classification accuracy of 87.3% (524/600) and a micro-averaged [67] F 1 measure [2] of 87.3% . For comparison, He et al. reported a micro-averaged F 1 measure of about 82% also using SVM but after word segmentation [22, 23]; Peng et al. reported a micro-averaged F 1 measure of 86.7% using character-level n-gram language model [47].
We did the Greek text authorship classification experi-ment on a dataset used by Stamatatos et al. [57]. This dataset consists of 200 Greek articles written by 10 different modern Greek authors: (1) S. Alaxiotis; (2) G. Babiniotis; (3) G. Dertilis; (4) C. Kiosse; (5) A. Liakos; (6) D. Maroni-tis; (7) M. Ploritis; (8) T. Tasios; (9) K. Tsoukalas; and (10) G. Vokos. There are 20 articles from each author  X  10 ar-ticles for training and another 10 articles for testing (as in [57]).

Since this dataset is not large, we just used the default feature extraction parameter configuration which would not filter any nontrivial substring-group out. The classification accuracy we obtained is 92% (92/100). For comparison, Stamatatos et al. reported a classification accuracy of 72% using deep natural language processing [57]; Peng et al. re-ported a classification accuracy of 90% using character-level n-gram language model [47].
We did the Greek text genre classification experiment on a dataset used by Stamatatos et al. [57]. This dataset con-sists of 200 Greek articles in 10 different genres: (1) press editorial; (2) press reportage; (3) academic prose; (4) official documents; (5) literature; (6) recipes; (7) curriculum vitae; (8) interviews; (9) planned speeches; (10) broadcast news. There are 20 articles in each genre  X  10 articles for training and another 10 articles for testing (as in [57]).
Since this dataset is not large, we just used the default feature extraction parameter configuration which would not filter any nontrivial substring-group out. The classification accuracy we obtained is 94% (94/100). For comparison, Stamatatos et al. reported a classification accuracy of 82% using deep natural language processing [57]; Peng et al. re-ported a classification accuracy of 86% using character-level n-gram language model [47].
The motivation of this work is to make discriminative learning methods work for string-based text classification. Our proposed key-substring-group feature extraction tech-nique solves the effectiveness and efficiency problems of string kernel, and opens numerous promising directions. SVM with key-substring-group features has exhibited very promising text classification performance, independent of language and task. Notably it can outperform traditional words-based text classification methods even on the Reuters-21578 top10 dataset, which is widely regarded as the home-ground of the latter. Therefore it is reasonable to believe that key-substring-group features are able to excel in various non-traditional text classification tasks that are naturally suit-able to string-based approaches, e.g., spam filtering [5, 45]. Obviously key-substring-group features could also be used for text clustering [16, 69]. It is possible to go even further to consider applications in other areas like gene/protein se-quence classification/clustering [39, 66].
We thank the anonymous reviewers for their helpful com-ments. [1] A. N. Aizawa. Linguistic techniques to improve the [2] R. Baeza-Yates and B. Ribeiro-Neto. Modern [3] T. C. Bell, J. G. Cleary, and I. H. Witten. Text [4] M. A. Bender and M. Farach-Colton. The LCA [5] A. Bratko and B. Filipi  X  c. Spam filtering using [20] J. Goodman. A bit of progress in language modeling, [21] D. Gusfield. Algorithms on Strings, Trees, and [22] J. He, A.-H. Tan, and C.-L. Tan. A comparative study [23] J. He, A.-H. Tan, and C. L. Tan. On machine learning [24] R. Herbrich. Learning Kernel Classifiers: Theory and [25] D. Holmes and R. Forsyth. The federalist revisited: [26] C.-W. Hsu and C.-J. Lin. A comparison of methods [27] P. Jackson and I. Moulinier. Natural Language [28] T. Jebara, R. Kondor, and A. Howard. Probability [29] T. Joachims. Text categorization with support vector [30] T. Joachims. A statistical learning model of text [31] T. Joachims. Learning to Classify Text using Support [32] D. Jurafsky and J. H. Martin. Speech and Language [33] B. Kessler, G. Nunberg, and H. Schtze. Automatic [34] D. Knuth. The Art of Computer Programming .
 [35] J. D. Lafferty and G. Lebanon. Diffusion kernels on [36] J. D. Lafferty and C. Zhai. Document language [52] R. E. Schapire and Y. Singer. BoosTexter: A [53] B. Scholkopf and A. J. Smola. Learning with Kernels . [54] F. Sebastiani. Machine learning in automated text [55] J. Shawe-Taylor and N. Cristianini. Kernel Methods [56] N. Slonim, G. Bejerano, S. Fine, and N. Tishby. [57] E. Stamatatos, N. Fakotakis, and G. Kokkinakis. [58] W. J. Teahan and D. J. Harper. Using compression [59] E. Ukkonen. On-line construction of suffix-trees. [60] C. van Rijsbergen. Information Retrieval .
 [61] V. N. Vapnik. The Nature of Statistical Learning [62] S. Vishwanathan and A. Smola. Fast kernels for string [63] I. H. Witten. Applications of lossless compression in [64] I. H. Witten, Z. Bray, M. Mahoui, and W. J. Teahan.
