 1. Introduction
Diagnosis of process faults in chemical processes has been an active area of research ( Srinivasan, 2007 ). Successful identifica-tion of process faults at an early stage can increase the success rate of fault recovery during operations and prevent unnecessary shutdowns. Also, automatic detection and diagnosis of faults are necessary to prevent costly accidents by providing time critical diagnostic information to plant operators. In literature, several fault diagnosis methodologies have been proposed for fault detection and identification (FDI) in chemical processes (Venkatasubramanian et al., 2003a,b,c ; Srinivasan et al., 2005a,b;
Ng and Srinivasan, 2009a ). Each FDI method has its strengths and shortcomings, which are process and fault dependant. A method that works well under one circumstance might not work well under another when different features of the process come to the fore. Combining FDI methods of different types is hence an attractive solution for monitoring processes operating under a wide range of operating conditions.

In addition to being adaptive towards different operating conditions, combining different FDI methods can also achieve higher diagnostic resolution by combining the strengths of existing FDI methods. It has already been shown in the pattern recognition literature that a judicious combination of classifiers generally outperforms a single one ( Rahman and Fairhurst, 1999 ;
Lin et al., 2003 ; McArthur et al., 2004 ). The main reason for combination of classifiers is that different types of classifiers can often complement one another and improve performance as a result of collaboration. When diagnosing faults in complex processes, designing a perfect classifier for all possible scenarios can be difficult, and combining different fault diagnostic methods is shown to be a good alternative wherein different features of heterogeneous diagnostic classifiers can be synergistically con-solidated. To facilitate the integration of heterogeneous diagnostic classifiers, a multi-agent system is proposed in this paper to integrate various FDI methods. The organization of this paper is as follows: Section 2 provides the review of some previous work in the fields of FDI, decision fusion and agent-based methods.
Section 3 describes the proposed multi-agent approach for collaborative FDI and the underlying decision fusion strategies.
The proposed multi-agent with its decision fusion strategies are tested with two case studies, namely startup of a lab-scale distillation column and the Tennessee Eastman Challenge pro-blem in Section 4 and Section 5, respectively. 1.1. Review of FDI methods
In general, existing FDI methods can be broadly classified into two categories namely qualitative model-based and quantitative model-based methods. Qualitative model-based methods include techniques such as trend analysis and expert systems. Trend analysis is based on the abstraction of process data into a set of trends ( Cheung and Stephanopoulos, 1990 ). Monitoring is then performed on the identified trends, which are made up of primitives that describe the qualitative behavior of the process variables. Classical trend analysis approaches are based on monitoring an ordered set of primitives that describe the evolution of a process variable. When a fault occurs, process variables vary from their nominal ranges and exhibit trends that are characteristic of the fault. Hence, different faults can be mapped to their characteristic trend signatures. Extension of trend analysis through fuzzy reasoning is reported in Dash et al. (2003) .

During transitions, each variable might display a different trend during different phases of the transition. There are also occasions where process exhibits different trends during transi-tions due to normal operating variations, thus complicating trend comparison. Classical trend analysis is therefore not sufficient to monitor transitions adequately. Sundarraman and Srinivasan (2003a, b) overcome the above problems through enhanced trends. Three types of matching degrees  X  shape matching degree, magnitude matching degree, and duration matching degree  X  were introduced to facilitate trend comparison during transition.
The main shortcoming of trend analysis is that it is designed for monitoring individual variables. It does not take into account the correlation between the variables in the process.

Expert systems, or rule-based systems, use rules to perform monitoring. They are best suited to situations where plant operators have a good knowledge regarding the nuances of the transitions and the underlying process. Honda and Kobayashi (2000) used a fuzzy rule-based inference system for the direct control of batch operations. The process phase is first recognized by fuzzy inference, and then a fuzzy neural-network based control system is used to control the batch process. They illustrated their methods on mevalotin precursor production, vitamin B2 produc-tion, and sake mashing processes. In Muthuswamy and Srinivasan (2003) , a rule-based expert system is developed for automation and supervisory control of semi-batch fermentation processes.
They characterized transitions using features in process variables and represented them as multivariate rules. These rules track the process across phases and automatically detect the current active phase using online data. Different monitoring rules are formu-lated for each phase of a transition. The rule-based transition characterization method was shown to be robust to measurement noise and easily comprehendible to the operators. Nevertheless, rule-based systems are process specific; at times, it is hard to extract rules to adequately model complex processes.

First-principle models, statistical models, signal processing models, and neural-networks are clustered under quantitative model-based systems. Extensive coverage of quantitative model-based approaches for monitoring and diagnosing faults during steady-state can be found in Chen and Patton (1999) and
Venkatasubramanian et al. (2003c) . Quantitative models are built either from first-principles knowledge or from using input X  output data. In Bhagwat et al. (2003a) , a non-linear model-based approach was proposed to monitor process transitions. Estimation of process states and residuals was achieved through open-loop observers and Kalman filters. To address the issues arising from the discontinuous nature of transition, the scheme uses knowl-edge of the standard operating procedure and divides each transition into phases. For monitoring, each phase is associated with a model component and different filters and observers are selected for fault detection in that phase. However, accurate models of highly complex processes operating in multiple regimes are seldom available and difficult to develop, thus limiting their practical applicability. Multiple model-based approaches have therefore been used to model, control, and monitor transitions. In Bhagwat et al. (2003b), a multi-linear model-based fault detection scheme was proposed based on decomposition of operation of a non-linear process into multiple locally linear regimes. Kalman filters and open-loop observers were used for state estimation and residuals generation in each regime. Analysis of residuals using thresholds, faults maps, and logic-charts enabled on-line detec-tion and isolation of faults.

Signal processing methods can be applied to analyze the normal/abnormal status of a process by comparing the online profile of process variables with those of previously known runs. The underlying methods perform time synchronization between process signals from different runs before comparing them based on predefined similarity metrics. Methods for signal processing include dynamic time warping (DTW) and dynamic programming (DP). Applications of DTW for process monitoring can be found in Gollmer and Posten (1996) and Kassidas et al. (1998a, b) . One known shortcoming of DTW is its high computational cost, which grows exponentially with the length of process data. This can be minimized by using landmarks such as peaks or local minima in the signals to reduce the complexity of signal comparison (Srinivasan and Qian, 2005, 2007 ). These landmarks, called singular points, can be used to decompose a long continuous signal into multiple, short, semi-continuous ones. However, one known shortcoming of DTW algorithm is the essential require-ment that the starting and ending points of the signals to be compared should coincide. Such shortcomings obviate their direct practice for online applications since the points in the historical database that should be matched with the starting and ending points of the online signal are unknown. To overcome these shortcomings, Srinivasan and Qian (2006) proposed dynamic locus analysis which is an extension of Smith and Waterman (1981) discrete sequence comparison algorithm for online signals comparison.

With the increasing availability of inexpensive sensors, the number of measured variables for most industrial processes easily ranges in thousands. This has lead to the popularity of multi-variate statistical methods, which bring forth powerful means to monitor transitions. Principal components analysis (PCA) is one such multivariate dimensionality reduction technique that is widely used for developing data-driven models ( Jackson, 1991). Applications of PCA and its variants for process monitoring can be found in MacGregor and Kourti (1995) and Chen and Liu (2002) . Most of the reported work in multivariate statistical analysis is directed to processes where the correlation between the process variables remains the same. These approaches are not directly applicable to transitions due to statistical non-stationarity and time-varying dynamics. In order to overcome this, an extension called dynamic PCA (DPCA) has been proposed ( Ku et al., 1995 ). In Srinivasan et al. (2004) , DPCA has been used to classify process states based on historical operating data. Process data is first segmented into modes and transitions. Steady-state modes are identified by using a moving window approach which is capable of rejecting outliers. A DPCA-based similarity factor is used to compare transitions with historical data, which can be used for online FDI. Since the run-length variations common across different instances of transient operations restrict the application of time-wise unfolding methods with PCA, multiple models can be used to overcome such shortcoming. Doan and Srinivasan (2008) and Ng and Srinivasan (2009b) , and multiple PCA models are used for monitoring transient operations.

Neural-network based approaches are another popular area for fault diagnosis in continuous processes ( Kavuri and
Venkatasubramanian, 1993 ). They have been popular for classi-fication and function approximation. In Fabro et al. (2005) , recurrent neural-networks were used to identify process states and predict process behavior. Control actions for different phases of transition are provided through sets of fuzzy controllers. They illustrated their approach through a distillation-column startup case study. Theoretically, artificial neural-networks can approx-imate any well-defined non-linear function with arbitrary accuracy. Unfortunately, there is no universal criterion for selecting a specific structure of neural-network for a practical application. Usually the structure of the network is decided based on the input dimensionality and the complexity of the underlying classes. The construction of an accurate neural classifier for such multivariate, multi-class temporal classification problem suffers from the  X  X  X urse of dimensionality X  X . To overcome the above drawbacks, Srinivasan et al. (2005c) proposed the use of two new neural network architectures, namely one-variable-one-network (OVON) and one-class-one-network (OCON). In both structures, the original classification problem is decomposed into a number of simpler classification problems. The new neural-networks architectures are hence simpler in structure, faster to train, and yield substantial improvement in classification accuracy com-pared to classical neural-network structure. However, a priori knowledge of the sub-states of each variable is needed to derive the sub-state identification layer of OVON, which can be cumbersome for processes with large number of variables. High misclassification rate is also reported during state change when there is no clear separation between the states.

Though there exist a variety of FDI methods for process monitoring and fault diagnosis, it is worth noting that each FDI approach has its corresponding strengths and shortcomings, which are process dependant. A method that works well under one circumstance might not work well under another when different features of the process come to the fore. A comparison of the strengths and shortcomings of different FDI methods is shown in Table 1 . Since no single FDI method is able to address the numerous facets of process monitoring and fault diagnosis, collaboration among heterogeneous methods is needed to bring forth the benefits of each method to improve monitoring resolution and robustness of the FDI system. The rationale of such an approach is based on the precept that the strengths of various methods can be brought together to bear on the problem and the drawbacks of an individual method can be overcome through collaboration .

When multiple FDI methods are used for monitoring and fault diagnosis, the integration between the heterogeneous methods becomes a challenge. Distributed computing methods such as multi-agent systems, which facilitate collaboration among dis-tributed entities, are an attractive approach to combine various
FDI methods. An agent wrapper can be used to encapsulate a FDI method and its results made available to other agents via messages. Next, we briefly review previous application of multi-agent approach in the domain of monitoring and fault diagnosis. 2. Multi-agent systems
The term  X  X gent X  is defined as a computer system that is situated in some environment, and is capable of performing autonomous actions in that environment in order to meet its design objectives ( Wooldridge and Jennings, 1995 ). An agent can thus be viewed as an computational entity that automates some aspect of task management or decision making to benefit its end user. Agent-based approaches offer opportunities to solve com-plex problems collaboratively using heterogeneous methods. An agent is generally characterized by its underlying plans/methods, and is allowed to interact with other agents via messages using a common agent communication language.

Agent-based computing has been used in various fields since its introduction in the early 90s. Some popular areas in which agent-based computing have been used include supply-chain management, process simulation, and monitoring and fault diagnosis. Mangina et al. (2001) applied multi-agent system for monitoring of the startup sequences of an industrial gas turbine.
Their agents used knowledge-based approach to recognize the different phases of turbine operations and have the phase-specific knowledge of each agent updated. Abnormal operations of turbine are detected by placing appropriate linear threshold across the state variables. Cho et al. (2003) used an agent-based model to monitor communication network. They developed their agents with different knowledge-based reasoning systems to diagnose known network faults. In Maturana et al. (2004) , a multi-agent architecture was used to integrate automation systems for the control of chilled water system used in US-Navy Vessels. Their diagnostic component includes a suite of data acquisition, signal processing, diagnostic and prognostic algorithms. Under abnor-mal events, an agent can interrogate the diagnostic component of other agents to validate a fault hypothesis. 2.1. Decision fusion
Consider a FDI agent k being used for classification of input sample x . Suppose there are J classes of known process states and assign each of the input sample, x , to one of the J +1 classes, i.e., x , implying that the sample is not assigned into any known classes. In general, any classifier is able to provide output in one of the following forms ( Xu et al., 1992): 1. Abstract form : only a unique class j or some subsets of the available classes are produced in its predictions e ( x )= C 2. Rank form : all available classes C j , are ranked and sorted, i.e., j o j 0 y o j * . 3. Measurement form : a measurement, S j ( t ), is given to quantify the degree of similarity between the sample x and C j .
Among the three forms of predictions, predictions generated in measurement form contain the highest amount of information while predictions generated in the abstract form contain the least.
If there are R classifiers denoted by k r , r =[1, R ], a total of R predictions will be generated from the classifiers, denoted here as e ( x ), r =[1, R ], where e r ( x ) can be in any of the prediction forms above.

The problem of decision fusion is then one of locating the best possible class information C j , for an input sample x by evaluating the combined predictions, E ( x )={ e 1 [ e 2 [ y [ e R 1 classifiers ( Fig. 1 ).

Decision fusion has been used in applications ranging from earth resource monitoring, weather forecasting, vehicle traffic control to military target classification and tracking, etc. Clemen (1989) provides a good compilation of early work in the area of decision fusion. Existing approaches for decision fusion can be classified as utility-based and evidence-based methods. Utility-based methods provide the simplest way to fuse decisions. These methods do not utilize any prior knowledge or evidence from previous predictions, but are based on some aggregating techni-que which evaluates the combined utility functions generated from each classifier. Methods based on utility techniques include simple average, voting techniques, and their variants. In contrast, evidence-based approaches use a priori information regarding the previous performance of each classifier to combine decisions. A popular approach that forms the basis of many evidence-based approaches in the pattern recognition literature is the Bayesian method. All fusion methodologies can be applied with any type of outputs, i.e., abstract, rank, or measurement form. Next, we review these decision fusion methods. 2.1.1. Voting-based fusion
Voting has been a popular form of utility-based decision fusion. There exist various variants of voting schemes, e.g.: plurality voting, approval voting, cumulative voting, borda count, etc. Each of these voting schemes differ in the way that the utility function is aggregated. When R predictions are produced from R classifiers, e 1 , y , e R , the decisions from all classifiers can be consolidated through application of a majority or plurality decision rule. Let the predictions e r ( x ) produced by the r th classifier on a class C j represented as a binary vector T T  X 
In majority rule, the final decision (class assignment) needs to be supported by at least half of the classifiers (majority voting). This can be expressed as ( Suen et al., 1990 ; Xu et al., 1992 ) E  X  x  X  X  where T E j  X  e r  X  x  X  A C j  X  X  P R r  X  1 T r  X  e r  X  x  X  majority threshold of R/2 in the case of plurality or simple majority rule. In this case E  X  x  X  X  C j ; if C j  X  argmax
The consolidated prediction E ( x ) is the class C j which receives the maximum number of votes even if it is lesser than R /2. Applications of different voting techniques to combine classifiers for handwritten numerals recognition can be found in Lam and Suen (1997) and Lin et al. (2003) .

The voting-based fusion is based solely on the prediction produced by the classifiers. Each classifier is usually treated equally without considering its performance. However, it has been well recognized that predictions from some classifiers generally outperform other classifiers in certain circumstances. Such information can be incorporated through Bayesian-based fusion. The Bayesian technique is a popular method in evidence gathering and uncertainty reasoning to calculate the posteriori probability of an event. 2.1.2. Bayesian-based fusion
The Bayesian fusion approach stores the historical priori class-specific performance of each classifier k r , r =[1, R ] using condi-tional probability. The final predictions are then estimated through the Bayes rule of calculating posteriori probability. For a classification problem of J classes on measurement x , if all classes are mutually exclusive (two classes cannot occur con-currently), the Bayesian inference process for evaluating the conditional probability of a class j from k r becomes P  X  x A C j 9 k r  X  x  X  X  j  X  X  P  X  k r  X  x  X  X  j
The conditional probability that implies x A C j , j A [1, J ], is often estimated from previous performance of k r using a confusion matrix ( CM ). A confusion matrix CM stores class-specific perfor-mance of k r , and is normally constructed by testing k r training dataset.

Consider a training dataset containing N samples tested with classifier k r . All predictions from the classifier k r can be recorded through a confusion matrix as follows ( Xu et al., 1992 ): CM r  X  where n ij r , i =[1, J ] and j =[1, J +1] indicates the number of samples belonging to class C i , but assigned to class C j by k r elements of CM r is then the true predictions from k r for the i th class. If the constructed CM r from a training dataset is composed results generated from classifier k r is 100% accurate. The total samples N can be represented as N  X  P J i  X  1 P and the total number of samples in each class C i can be evaluated as n  X  and the number of samples that is assigned to class C j by k n  X 
Utilizing information stored in CM r , the conditional probability that implies x A C j , j A [1, J ] given the evidence that k computed as ( Xu et al., 1992) P  X  x A
C j 9 k r  X  x  X  X  j  X  X 
Eq. (8) can be considered as the confidence of a classifier regarding the assignment of sample x to class C j .When R classifiers are in use, there will be R confusion matrixes, CM r , r =[1, R ], and R evidences, e = e ing the proposition that x A C j in the form of a conditional probability.
Xu et al. (1992) suggested the following estimate to be used for combining individual conditional probabilities: P  X  x A C j 9 e r  X  x  X  X  j ; E  X  X 
Based on Eq. (9), a sample x can be classified into class j based on the combined conditional probability of each class. The class C with the highest P E j can be selected as the optimal combined prediction
E  X  x  X  X  j ; if C j  X  argmax
In Zheng et al. (2005) , Bayesian-based fusion was used to integrate various image processing models for diagnosing dis-eases. Rahman and Fairhurst (1999) proposed a decision combi-nation strategy using a priori information sources for machine printed character recognition. In Foggia et al. (1999), a threshold-based rejection criteria was proposed for Bayesian combination by using information regarding reliability of a classifier. They showed that introduction of a rejection option, i.e. rejecting an input sample when the reliability is under a threshold, can reduce the rate of misclassification when applied to the handwritten characters recognition problem. In McArthur et al. (2004) , three
FDI methods have been combined based on Bayesian approach to diagnose faults in a transformer. They analyzed faults using k -means clustering, backpropagation neural-network, and user written rules, and showed that collaboration of FDI methods improved system performance. In the next section, a multi-agent based framework is proposed for collaborative FDI. 3. Collaborative agents for managing efficient operations
An agent-based framework called collaborative agents for managing efficient operations (CAMEO) is described here to render effective management of process operations possible. The proposed framework is abstracted hierarchically into environ-ment, host, and agents. An environment in CAMEO is a neighbor-hood that supports any plant operation. All entities within the plant are part of the environment, i.e., software, hardware, controllers, human operators, etc. An agent environment might contain one or more hosts where computational agents reside.
The host contributes processing capability to the agents by sharing available processors. They also enable inter-agent com-munication. High performance computing units such as computing clusters or supercomputers are excellent candidates for hosts. 3.1. Agents
The agents within CAMEO form the primary entities of the proposed framework. Each agent encapsulates certain knowledge of a process and contains routines/algorithms capable of com-pleting a certain task. Each class of agent is host independent; they can be initiated and executed on different hosts. Each agent does not necessarily form a complete application but is rather a reusable, self-contained piece of software that can rationally provide decision support during process operations. Six classes of agents have been incorporated into CAMEO: 1. Data management agents 2. Operation and control agents 3. Supervision agents 4. Consolidator agents 5. Fault tolerance and recovery agents 6. Visualization and user interface agents Data management agents provide the conduit to the process and its states for all other agents. Raw sensors data are read, de-noised, and reconciled by data management agents and the smoothed data is made available to other interested agents through messages.

The operation and control agents aid the operator in executing and controlling the different steps of a process transition. Some examples of these follow: Regulatory control agents interact with the DCS and perform actions such as reconfiguring the controller settings based on the current state of the process. The sequential control agents coordinate among the discrete steps required for executing sequential operations. The alarm management agents help reconfigures the alarm management system to the current state and thus prevent alarm floods and nuisance alarms (Srinivasan et al., 2004 ).

The goal of process supervision agents is to ensure safe operation. For effective process supervision, knowledge about the current process state is essential since it provides the basis to define  X  X  X bnormal X  X  occurrences as well as to identify them. Also, one way to prevent abnormal situations and make plant automation applications to function appropriately is to make them self-aware of the domain of their applicability. These applications can then enable or disable themselves in specific modes and reconfigure themselves with the correct settings when different states have to be handled differently. In order to automate such switching, context-sensitive information regarding the process state should be provided to the plant automation applications, which would then reconfigure themselves to the operating state. This is possible only if the different process states are previously known and their occurrence can be identified online ( Srinivasan et al., 2005a ). In CAMEO, this role is performed by the state identification agents.

Other supervisory agents are monitoring agents , who detect abnormalities, and diagnostic agents , responsible for identifying the root cause of abnormalities. Monitoring and diagnostic agents can incorporate the various methods such as qualitative trend analysis, neural-networks, rule-based system, etc. Successful fault isolation is achieved by collaboration between supervisory agents and consolidator agents. Results from monitoring agents are sent to consolidator agents to resolve possible conflicts generated from the multiple monitoring techniques that maybe applicable in any given state.

When multiple methods are used concurrently, conflict resolution is needed to arbitrate among the different solutions proposed by the different methods and provide one consolidated solution to the operator. In CAMEO, this role is performed by
Consolidator agent . The consolidator agents contain the logic to enforce consistency among different agents and are the bedrock for the collaboration mechanisms. Other agents including super-vision agents, operation and control agents, and visualization agents also interact with consolidator agents to enforce consis-tency. Consolidator agents in turn use process state, historical performance, domain knowledge and other basis to fuse inde-pendent pieces of information contributed by other agents. The consolidated conclusion regarding the normality of the process operation and the root causes of any deviation provide the basis for planning corrective actions.

Having identified the root cause, the fault tolerance and recovery agents use process knowledge, available preplanned
SOPs for specific situations, as well as historical records of corrective control actions to guide the process into a safe state to recover from an abnormal situation. The sequence of corrective measures generated by these agents can be implemented directly by the operation and control agents or communicated to the operations personnel through user interface agents .

A state specific context-sensitive graphical user interface is provided by the visualization agents in CAMEO. Visualization agents use powerful visualization tools such as self-organizing maps ( Ng and Srinivasan, 2008a ) in facilitating plant personnel to visualize the progression of process more easily. The visualization agents incorporate methods for projecting high dimensional data onto a much lower dimensional grid and thus provide a means for plant personnel to visualize even long duration process transi-tions effectively. User interface agents can also automate alerts to inform the relevant plant personnel regarding events occurring in the plant, e.g., detection of fault, successful diagnosis of faults, state change operation, etc. The interactions among the different agents are illustrated in Fig. 2 . The consolidator agent plays a central role as explained next. 3.2. Consolidator agent
In this work, we consider diagnosis of faults based on various features extracted from the original time-domain signals, their reduced subspace through principal components, and their similarity in clusters through self-organizing map. We use the neural-network ( Srinivasan et al., 2005b), principal components analysis ( Raich and C -inar, 1996 ), and self-organizing maps ( Ng and Srinivasan, 2008a, b ) for FDI. Each FDI method is represented as a monitoring agent, A m , and/or diagnostic agent, A d for all monitoring and diagnostic agents are combined through consolidator agent, A c . 3.2.1. Fusion of monitoring results
Let there be R monitoring agents A r m , r =[1, R ]. The monitoring results produced from each A r m , g r ( t ), based on the analysis drawn from sample x t , can be represented as a binary variable g  X  t  X  X 
The g r ( t ) generated by all A r m , i.e., [ g 1 ( t ), g combined to evaluate if the process is in a abnormal state, i.e., G ( t )={ g 1 ( t ) [ g 2 ( t ) [ y [ g R ( t )}=1.

Based on plurality voting, the underlying process is considered abnormal when a plurality of the monitoring agents A r m reports an abnormality G  X  t  X  X  1 ; if
The strategy for detecting faults using Bayesian combination strategy relies on evaluating the conditional probability, P ( x C 9 e r ( x )= j ). Prior to online implementation, the offline fault predictions of each agent for a training sample is analyzed and results are collected into a confusion matrix, CM r , as described in Section 2. During online application, when a new measurements x t is obtained, the conditional probability for each fault is calculated based on Eq (8). The status of the process is then decided based on the conditional probabilities
G  X  t  X  X  3.2.2. Fusion of diagnostic results
In CAMEO, the diagnostic agents are responsible to classify a sample x t into one of the known fault classes under the presence of fault. The predictions from each diagnostic agent are in the measurement form with a degree of similarity, S j r ( t ), between x and fault j . Suppose there are R diagnostic classifiers A
Let the fault database consist of J fault classes DB ={ C
When the process is abnormal, each A r d will locate a set of candidates { C j }, j A [1, J +1], were, class C J +1 corresponds to the novel fault. Decision fusion (using either voting or Bayesian-based) seeks to identify the most probable fault class, C combining the predictions from all A r d , i.e., e 1 ( x A j
A [1, J +1]
Similar to fusion of monitoring results, diagnosis results can be fused based on plurality voting. Let T j r be the number of votes cast by Agent A r d for fault j .

T j  X  1  X  14  X 
The combined prediction (fault candidates), C V opt , is determined based on the votes of all fault diagnosis agents A r d as follows:
C  X  argmax
In general, the voting strategy is more accurate in determining the actual fault when there is broad agreement among the diagnostic agents. In case of significant conflict, when there is no overlap in the diagnostic agents X  results, i.e., e r ( x A 8 r a r 0 , voting cannot uniquely resolve among the fault candidates.
In such a scenario, a subset of fault classes that receive equal number of votes will be identified by (15).

In the Bayesian fusion method, we use the diagnostic results produced from the diagnostic agents, A r d . Upon detection of an abnormality, i.e., when G ( t )=1, the results produced by all A collected to form a fault candidate pool CP . The posteriori probability of fault j based on the evidence of agent A r
P  X  x t A C j 9 e r  X  x t  X  X  j  X  X 
A diagnosis agent may not always return a single fault candidate. In these cases, following (14), each A r d is assigned unit credit, which is distributed equally among all fault candidates FC proposed by A r d . Similar to (15), the highest combined belief derived from the conditional probabilities of the candidates in CP forms the basis for identifying the optimal candidate C B
C  X  argmax 3.3. Implementation
The architecture and algorithms described above have been implemented in Matlab ( Mathworks, 2002 ). FDI algorithms can be computationally expensive, therefore a parallel implementation of the framework is essential to enable decision support in real-time. The agent-based framework offers a natural scheme to realize this in practice since the agents do not have to be co-located in the same processor, but can be distributed across a cluster to exploit multiple processors and communicate with each other through message passing. The message passing interfac e (MPI) is a popular interface to send messages across a network of computers ( Kepner and Ahalt, 2004 ). It allows the coordination of programs running on either distributed memory computers or on shared memory systems. Each agent within CAMEO is host independent, and can be executed from any location. Agents communicate with each other by exchanging messages (see Fig. 3 ) based on a purpose-designed ontology. Each agent has a vocabulary containing the queries it is capable of responding to, for example, the monitoring agent is capable of each representing different tasks. An agent thus communicates with another by sending it appropriat e requests within the latter X  X  vocabulary.

The inter-agent communication occurs through exchange of files as shown in Fig. 4 . Suppose agent A a intends to request agent A Diagnosis Agents perform monitoring, further it needs to convey some task specific parameters to A b . A common directory is used by A a and A communication. A a first writes the message C A , buffer , which includes the essential data and parameters for the task to the common message for A b . The agent A b continually checks the common directory for existence of C A , lock .Once C A , lock is detected, A the message C A , buffer and executes the requested task. Both C C
The message passing architecture enables agents in CAMEO to function freely across different hosts in an agent environment. The distributed multi-agent framework enables speedup of computationally demanding tasks by exploiting multiple proces-sors. Two performance measurement indicators are used in this work to measure the impact of speedup, overall speed enhance-ment index , S p , and the overall system efficiency index , E S  X  E  X 
The amount of computational resources required in a plant is variable and depends on the state of the process. For instance, during normal operations  X  diagnosis and fault tolerance and recovery agents do not need execute. However, high computa-tional loads would occur in periods when the process is in an abnormal state. To manage this variable demand on computa-tional resources, a special class of agents called mobile agents are used to manage the workload of the local hosts. Mobile agents provide a means for inter-host transport of other agents. Agents that require high computing resources are transported to another host through mobile agents. Thus, CAMEO has been designed to provide optimal performance by spreading the processing load on available processors (see Fig. 5 ). It been tested on a Linux cluster with 16 nodes containing 2 Pentium Xeon 3.06 GHz processors each and 2 GB memory. The nodes are connected together via high-speed, low-latency Myrinet interconnects. During execution,
CAMEO is initialized in one node and uses Secure Shell command (SSH Communication Security, 2006) to launch additional Matlab processes in other nodes as needed (Ng, 2008).

Thus, the agents developed in CAMEO are able to mimic aspects of a human decision maker by (1) reacting to numerous circumstances due to changes in plant operating conditions, (2) exhibiting social-ability such as cooperation, negotiation, and migration, and (3) showing proactiveness towards fulfillment of designed objectives. Next, we illustrate the proposed multi-agent system and its decision fusion strategy using two case studies. 4. Case study I: fault diagnosis of Tennessee Eastman challenge Problem
In this section, the proposed FDI method is tested for online disturbance identification on the Tennessee Eastman (TE) industrial challenge problem ( Downs and Vogel, 1993 ). The TE process produces two products ( G and H )andabyproduct( F ) from reactants A , C , D ,and
E (Fig. 6 ). The control structure of Lyman and Georgakis (1995) ,as implemented by Chiang and Braatz (2003) is used here. The process has five units, namely: a two-phase reactor, a product condenser, a flash separator, a recycle compresso r, and product stripper. There are altogether 22 continuous process measurements, 12 manipulated variables, and 19 composition measurements sampled less frequently. We use the 22 continuous process measurements to identify unknown process disturbances online. Following Downs and
Vogel (1993) , fourteen process faults (ref erred to as IDVs) are tested here. Since the existing controller is able to provide good recovery for IDVs 3, 4, 9, 14, 15, 16, &amp; 19, these are excluded from the analysis.
Each training dataset contains 2700 min of process operations with a sampling interval of 3 min. All faults were introduced at 1 h of operating time for the training data. In comparison, the testing dataset used for each run is created by simulating the process for 48 operating hours (2880 min) with the faults introduced at 480 min. The signals for each test run are thus different from the training data in terms of run-length and time of fault introduction. 4.1. FDI agents
Three FDI agents are used in this case study for monitoring and fault diagnosis -neural-networks (NN), self-organizing maps (SOM), and principal components analysis (PCA). The predictions from all three FDI agents used are in measurement forms and various decision fusion schemes are used to fuse the results ( Ng, 2008 ).
NN agent : For this case study, a three layered feed-forward neural-network with size [30 30 15] using tan-sigmoid transfer function for the two hidden layers, and a linear transfer function for the output layer is used for fault identification. The neural-network monitoring and diagnostic agents, A m NN and A d NN , are trained with the time-series data of the fault F j , j A L =[1, 15], with F corresponding to the case of normal operation. For training of neural-network, the portion of F j corresponding to normal opera-tions is removed so that only the distinctive patterns from the fault is used for neural-network training. For this, we use sum of
Euclidean distance to measure similarity. Subsequently, the training data are ranged normalized and a backpropagation neural-network is trained. The neural-network monitoring agent
NN performs monitoring by observing the predictions of x t (normal operation). A sample is deemed abnormal when it violates a user specified lower bound d lNN and upper bound d uNN ,i.e. g  X  t  X  X 
For fault identification, the faults C j satisfying the following criteria are extracted e
NN  X  x t  X  X  j ; if S j 4 d
A d lNN and d uNN of 0.8 and 1.2 are used in this case study for both the A m NN and A d NN agents. The performance of the NN agent for the various IDVs is shown in Table 2 . Among the three FDI agents in this case study, it has the highest recognition rate (96.04%) and shortest diagnostic delay (91.5 min).

SOM agent : A SOM monitoring agent, A m SOM , uses a two-dimensional map consisting of neurons to perform cluster analysis on the online measurement x t . The SOM is constructed based on the method proposed in Ng and Srinivasan (2008a) . The same training data as used for training the neural-network monitoring agent, A m NN were used for the SOM agent as well. The fully trained SOM consists of 26 17 map units and the neurons are clustered into 70 clusters through k -means. The A m NN monitoring by tracking any deviations from the nominal operat-ing cluster on the SOM space, i.e., the process is deemed to be abnormal by A m SOM if the BMUs enter an abnormal region on the SOM clusters or if quantization error is large ( Vesanto, 1999 ) E  X  : x i m b i : 4 d mSOM  X  22  X 
Fault identification by A d SOM is through fault signature analysis (Ng and Srinivasan, 2008b ) by comparing the fault signature generated from online measurement to the training data of each class of fault. A similarity S j SOM , which is defined as the ratio of matching elements among the online fault signature and those in the fault database is used for fault isolation. A S j SOM implies that the real-time signal clearly matches a specific reference pattern while S j SOM E 0 implies that the real-time state-signature does not match the reference pattern. A fault is identified when there is only one remaining S j SOM larger than d SOM , a user-specified threshold. In this case study, we used d SOM =0.9 for fault isolation. Among the various methods, the SOM agent identifies the highest number of faults (13 IDVs) as shown in Table 2 . It fails to isolate fault IDV20. The average detection delay of A d SOM is 115.9 min and the average diagnosis delay 180.8 min (see Table 3 ).
PCA agent : The PCA monitoring agent, A m PCA ,istrainedwithnormal operating data X R only. The X R is first auto-scaled and a PCA model is developed by retaining the first 16 PCs with the cumulative variance of 95.18%. PCA based fault detection is based on the limit violation of T statistic and SPE value of the normal model. For fault identification, the fault reconstruction approach as proposed in ( Raich and C 1996)isusedbyPCAdiagnosticagent, A d PCA , through constructing a PCA model PCA j for each fault class C j , j A [1, 15]. The PCA model that shows an in-control status (by quanti fying the similarity of the online samples with each PCA j over a defined window) during abnormal operation is considered to flag the right class of fault. The PCA agent has the shortest detection delay among the three agents with an average of 69.0 min. However, it can only distinguish between 10 IDVs. The reconstructed fault mode ls in the PC subspace are shown in
Fig. 7 . It can be observed that several IDV models overlap in the PC subspace which leads to a low recognition rate (74.8%). Next, we describe the fusion of FDI results for two scenarios. 4.2. Online FDI results 4.2.1. Scenario 1: condenser cooling water inlet temperature high
Run-05 corresponds to a step change in the condenser cooling water inlet temperature. The fault was introduced at t =460 min as a step change to variable XMV(11)  X  the condenser cooling water flow. When the fault occurs, the liquid flow rate of the outlet stream from the condenser to the vapor/liquid seperator increases. This causes the pressure of the separator and stripper to decrease. For this fault, the control loops are able to compensate the change and all measured variables are returned to set-point. The time it takes to reach steady-state is approximately 10 h. and the timeline of major events are shown in Fig. 8 . In this scenario, the neural-network monitoring agent, A m NN , detects the fault at t =483 min when the predictions with normal operating class fall from S 482 =0.984 to S 483 =0.003. The fault is subsequently isolated at t =543 min by its diagnostic agent, A d NN , when the similarity of Run-05 with IDV05 increases from 0.576 at t =540 min to 0.841 at t =543 min. In contrast, the SOM and PCA monitoring agents incur additional delay and detect the fault at t =495 min and t =486 min, respectively. The PCA agent is unable to isolate this fault as the scores of IDV05 overlap with those during IDV07 and IDV12 in the PC space.

Bayesian consolidation detects the fault at t =483 min. Since the A d NN agent is not able to contribute any candidate to the candidate pool, CP , at the time of detection for decision fusion (as predictions with all other classes are lesser than the required threshold, i.e., S F j 4 0 : 8), the fault is isolated only at t =543 min when A d NN identifies IDV05 as the actual fault class. The combined probability evaluated for IDV05 through Bayesian fusion is 0.7212. Based on voting-based fusion, the fault is detected at t =486 min and isolated at t =594 min. The time of detection and fault isolation is slower as agreement is needed among at least two agents. 4.2.2. Scenario 2: reactor cooling water inlet temperature oscillates randomly
Run-11 corresponds to a fault in the reactor cooling water inlet temperature. The fault was introduced at t =460 min as a series of large variations in XMV(10)  X  reactor cooling water flow rate. The oscillations in XMV(10) cause a fluctuation in the reactor pressure (XMEAS7) and temperature (XMEAS9). These also affect other downstream units such as product separator and stripper.
The timeline of major events is shown in Fig. 9 . In this scenario, the PCA monitoring agent, A m PCA , first detects the fault at t =513 min when the Hotelling X  X  T 2 statistic violates the allowable 99% limit ( T 2 identifies IDV{11, 12, 14, 17} as potential fault candidates. These four candidates hence form the candidate pool for fusion. Both A m and A m SOM have a larger detection delay with time of detection at t =519min ( A m SOM )and t =534 min ( A m NN ), respectively. The SOM diagnostic agent, A d SOM , is able to isolate the fault at t=573 min. In to provide any useful FDI information until t =957 min. Based on the confusion matrix generated from historical training data, the combined probability for the four candidates identified by the A d i.e., IDV{11, 12, 14, 17} at time of fault detection ( t =513 min) are {0.250, 0.170, 0.003, 0.239}. Since a separation ratio of 1.05 is required for Bayesian-based fusion, an additional delay of 63 min is necessary for the isolation of the fault. In this scenario, the fault is isolated when the SOM agent is able to isolate IDV12 from IDV11, and the combined belief observed for the four candidates becomes {1.206, 0.838, 0.318, 0.004}. IDV11 is hence flagged as the fault.
Similar analysis was performed for the other scenarios. The summary FDI results based on voting and Bayesian decision fusion strategies are shown in Table 3 . All faults are identified successfully by both the decision fusion methods. Bayesian fusion yields better results with (i) higher prediction accuracy (96.62% vs 96.18% for voting), and (ii) shorter detection (65.6 min) and diagnosis delay (73.1 min). Bayesian fusion is able to diagnose all of the faults by effectively combining the strengths from each FDI classifiers. Both decision fusion methods show success in reducing
Type-II errors (99.77% for Bayesian and 98.58% for voting) compared to any single FDI approach. Significant improvement in time of fault isolation is observed for fusion based on Bayesian method where a minimum improvement of 20.1% (18.4 min) has been observed. Since voting-based decision fusion requires plurality of votes from all agents for fault detection and identification, the method suffers from higher detection delay, as any early detection by one agent is not adequate. This case study clearly illustrates the potential benefits of combining heterogeneous FDI agents. 5. Case study II: fault diagnosis during distillation unit startup
In this section, the proposed method is tested on a lab-scale distillation unit. The schematic of the distillation unit is shown in
Fig. 10 . The distillation column is of 2 m height and 20 cm width and has 10 trays; the feed enters at tray 4. The system is well integrated with a control console and data acquisition system.
Nineteen variables  X  all tray temperatures, reboiler and condenser temperature, reflux ratio, top and bottom column temperature, feed pump power, reboiler heat duty, cooling water inlet and outlet temperature  X  are measured at 10 s interval. Cold startup of the distillation unit with 30% v/v ethanol X  X ater mixture as bottoms is performed following the standard operating procedure (SOP). The startup normally takes 2 h. From an operation point of view, four operating states can be identified during the startup, namely, reboiler heating phase, boiling phase, reflux heating phase, and final steady-state phase. Different faults, both equipment failure and operator errors are introduced at different states of the operation and monitored using the proposed approach. The details regarding the startup SOP and fault details are available in Srinivasan and Qian (2006) .
Various decision fusion methods are used here with the diagnosis results produced from all monitoring and diagnostic agents fused. Similar to previous case study, three FDI methods capable of monitoring and diagnosing faults during transient operations are implemented. We describe each of the FDI agent next. 5.1. FDI agents
In this case study, three FDI agents were implemented to detect and diagnose faults, namely (1) self-organizing map agent,
SOM (Ng and Srinivasan, 2008a, b ), (2) DPCA agent with non-parametric bound, A m KDE (Wand and Jones, 1994 ), and (3) neural-network based agent, A m NN (Hagan et al., 1996 ). All associated agents have diagnostic capabilities and are modeled separately as diagnostic agents, A d r , where r A [ SOM , KDE , NN ]. feed-forward neural-network with [15 15 11] nodes using tan-sigmoid transfer function for the initial two hidden layer, and linear transfer function for the final layer. The training data corresponding to the ten DSTs and one normal operation are used to form the neural-network training matrixes. The training matrix is then of size 11 columns of binary elements. All state variables are ranged normalized, and the weights and biases of the neural-network are obtained with scaled conjugate gradient training method. The monitoring and fault isolation methods used by the NN , and A d NN are similar to the previous case study.
Self-organizing map agent , A m SOM : The A m SOM is also constructed using all available training data as used for training of neural-network. The constructed SOM for this case study consists of 33 22 map units and is further clustered to 40 clusters. Similar to previous case study, abnormal operations are detected through
SOM based on cluster-sequence analysis while fault identification is performed through A d SOM based on state-signature comparison.
Non-parametric PCA agent , A m KDE : The A m KDE for monitoring is constructed with normal operation data only. The bounds for monitoring are created by constructing non-parametric bounds around the estimated density of the score of the normal operating trajectory in the PC space. Fault detection with A m KDE is based on evaluating the deviation of the online trajectory from the nominal
KDE bounds. For isolating known process faults, a separate DPCA model is created for each class of fault, F j , j A [1, 11]. The DPCA models that show an in-control status (by quantifying the similarity of the online samples with each DPCA j over a defined window) during an abnormal operations are considered to flag the right class of fault.

The A m NN and A d NN can detect and diagnose all ten disturbances (see Table 4 ). The A d NN shows good recognition rate in this case study with 87.15% accuracy and an average detection and diagnosis delay of 23.4 samples and 57.2 samples, respectively. The A m are able to detect and diagnose all 10 disturbances introduced with a detection delay of 36.0 samples and diagnosis delay of 87.8 samples. A m KDE is unable to detect DST05. All remaining disturbances are successfully detected and diagnosed with a detection delay of 35.4 samples and a diagnosis delay of 57.2 samples. Among the FDI agents, A d SOM shows the lowest recognition rate with 56.2% recognition rate. Next, we describe the fusion of FDI results for two scenarios  X  DST03 and DST08. 5.2. Online FDI results 5.2.1. Scenario 1: high feed flow
Run-03 corresponds to a disturbance of high feed pump. The fault was introduced at t =3580 s when the feed pump was set to 1.5 times its nominal value. This causes the reboiler to overflow if the fault is left undiagnosed. For this fault, the neural-network agent A d NN detects and diagnoses the fault earliest at t =3600 s. The
SOM agent A m SOM detects the fault at t =3650 s (with diagnosed time at t =3710 s) while the non-parametric agent A m KDE fault at t =3700 s (with diagnosed time at t =3980 s). When the FDI results from all agents are fused, Bayesian approach gives the minimum detection and diagnostic delay by identifying the fault promptly at t =3600 s given the high combined probability of A d towards this class of fault  X  DST03. The voting approach incurs additional delay and is able to isolate the fault at t =3650 s when and A d SOM detect the fault and identify DST03 as their candidate. 5.2.2. Scenario 2: high reflect ratio
Run-08 corresponds to a disturbance of high reflux ratio. The disturbance was caused by human error at t =3460 s when the reflux ratio was set to twice its nominal value. This causes a reduction in product throughput and increases energy consump-tion of the system. The timeline of events during monitoring is shown in Fig. 11 . In this case, all monitoring agents are able to detect the fault at t =3480 s and the diagnostic agents initiated to locate the root cause of the fault. The fault is first isolated by A d at t =3500 s when the predictions from online samples match to those of DST08. The SOM (fault isolated at t =4700 s) and DPCA agents (fault isolated at t =4740 s) are able to diagnose the fault only during second stage of fault propagation. When the results are fused, both decision fusion methods give similar performance by detecting the fault at t =3480 s and isolating it at t =3500 s as DST08 is the fault with highest votes as well as posteriori probability.

Similar analysis was also conducted for other scenarios as shown in Table 4 . By combining the three FDI agents, both voting and Bayesian fusion strategies are able to diagnose all ten disturbances with significant improvements in speed of fault detection and diagnosis. The voting strategy shows higher recognition rate with 98.36% (see Table 5 ) as redundant information is eliminated effectively by seeking agreement from majority of the FDI agents. Though it is able to classify samples more accurately, voting suffers from higher delay for fault detection and diagnosis as in the previous case study. Significant improvement in time for fault isolation is observed with Bayesian fusion where a minimum improvement of 57.9% (33.1 samples) is observed. This can be attributed to the more explicit, class specific a priori performance information in the form of the confusion matrix used in the Bayesian fusion strategy. The S p and E p observed using multiple processors are shown in Fig. 12 . During abnormal events when high level of CPU flops are required, average processing time reduces from 12 s per sample with a single processor to 2.7 s per sample with 6 processors, which demonstrates the success of the parallelism achieved in the proposed multi-agent fault diagnosis architecture. 6. Conclusions
A novel multi-agent based framework has been developed for detecting and diagnosing faults in the process industries. It offers a mean to integrate seamlessly various fault detection and identification techniques. The framework, called collaborative agents for managing efficient operations (CAMEO), consists of different monitoring methods, each modeled as a software agent, which observe the process in real-time and flag abnormalities independently. A decision fusion strategy forms the bedrock for collaboration and conflict resolution among these agents. Fault diagnosis agents are then triggered as needed to isolate faults; their results are also fused through voting or Bayesian combination by the consolidator agent. Collaboration among the agents is engendered through a standardized communication formalism. Extensive testing of the proposed method to multiple case studies demonstrates the framework X  X  ability to reduce both Type-I and Type-II errors, and speed up time of fault detection and diagnosis considerably compared to any single FDI technique.

CAMEO thus offers a flexible and efficient framework to loosely couple various techniques in a concerted way so that short-comings of each can be overcome through collaboration. New algorithms can be incorporated easily using an agent wrapper that parses incoming communication and sends the results to the consolidator agent through standardized messages. The multi-agent approach to combining dissimilar FDI techniques is markedly different from other hybrid FDI approaches (such as Mylaraswamy and Venkatasubramanian, 1997 ; O  X  zyurt and Kandel, 1996; Vedam and Venkatasubramanian, 1999 ) where specific forms of interactions are enforced by the designer. The multi-agent architecture also offers the opportunity to distribute the FDI algorithms, which are often computationally intensive, across multiple processors. Another advantage of the multi-agent formalism is that agents can observe one another X  X  results in a given situation in real-time in order to learn new fault classes or improve their performance through parameter (threshold) tuning. Our future work will address such adaptive FDI agents. References
