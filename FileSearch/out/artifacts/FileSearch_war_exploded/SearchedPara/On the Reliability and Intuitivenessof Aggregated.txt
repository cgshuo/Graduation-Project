 Aggregating search results from a variety of diverse verti-cals such as news, images, videos and Wikipedia into a sin-gle interface is a popular web search presentation paradigm. Although several aggregated search (AS) metrics have been proposed to evaluate AS result pages, their properties re-main poorly understood. In this paper, we compare the properties of existing AS metrics under the assumptions that (1) queries may have multiple preferred verticals; (2) the likelihood of each vertical preference is available; and (3) the topical relevance assessments of results returned from each vertical is available. We compare a wide range of AS metrics on two test collections. Our main criteria of comparison are (1) discriminative power, which represents the reliability of a metric in comparing the performance of systems, and (2) intuitiveness, which represents how well a metric captures the various key aspects to be measured (i.e. various aspects of a user X  X  perception of AS result pages). Our study shows that the AS metrics that capture key AS components (e.g., vertical selection) have several advantages over other met-rics. This work sheds new lights on the further developments and applications of AS metrics.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval] Keywords: aggregated search; evaluation; metric; diver-sity; reliability; discriminative power; intuitiveness
Search engines operating verticals dedicated to specific media types or genres (e.g. news, image, blogs) commonly present results from several verticals dispersed throughout the standard  X  X eneral web X  results, for example by adding image results to the ten blue links for the query  X  X ar X . This search paradigm, known as aggregated search [4], has three main challenges: vertical selection ( VS ), item selection ( IS ) and result presentation ( RP ). Vertical selection deals with deciding which verticals are implicitly intended by a query. Item selection deals with selecting a subset of items from each vertical to present on the aggregated page. Result pre-sentation deals with organising and embedding the various types of results on the result page. The most common pre-sentation strategy is to merge the results into one ranked list of blocks , and is now the  X  X e facto X  standard.
Evaluating aggregated search (AS) is challenging as there are a variety of compounding factors. Consider the query  X  X oga poses X  which suggests that a visual element in the re-sult page would be interesting to many users. Some users may prefer an AS page because they prefer a specific ver-tical (e.g. image). Other users may desire results that are vertically diverse ( VD ) (e.g. items from image, blog and discussion). These two scenarios relate to VS performance in AS. Users may prefer result sets that are composed mostly of topically relevant items; this relates to IS performance. Finally, users may prefer to see relevant items from selected verticals towards the top of the result page; this is dealt with by RP . Any combination of those factors (and the cor-responding components of AS) can influence a user X  X  per-ceived quality of the AS result page.

Although various approaches and metrics have been pro-posed to evaluate AS systems, either each component in isolation [4, 3] or as a whole [15, 27], no work exists that aims to comprehensively understand them. In particular, in [27], a series of AS metrics were proposed, studied, and ap-plied to a number of AS approaches. The aim therein was to model all, or a subset, of the four AS compounding factors discussed above, VS , IS , RP and VD . The main differences between these metrics are the way they model each factor and combine them. However, how well the metrics capture and combine those factors remain poorly understood. In other words, how these metrics behave, i.e. their reliability and intuitiveness , when evaluating AS performance has not been studied. This is the aim of this paper.

We investigate the reliability and the intuitiveness of AS metrics. We focus on how the metrics reflect the four AS compounding factors, VS , IS , RP and VD . By reliability , we mean the ability of a metric to detect  X  X ctual X  perfor-mance differences as opposed to those observed by chance, and by i ntuitiveness , we mean the ability to capture any property deemed important in a metric. In this paper, reli-ability is measured using discriminative power [17]. We use the randomised Tukey X  X  Honestly Significant Differences test [7] because, as shown by [21], this test is less likely to find significant differences that are not  X  X ctual X . For intuitive-ness, we quantitatively measure the preference agreements using the concordance [21] of a given AS metric with a  X  X a-sic X  single-component metric for each of the four AS factors. Finally, to gain further understanding, we examine AS met-rics X  ability to capture the combination of key components.
The contribution of our work is three-fold: (a) Our work is the first endeavour to study the reliability and intuitiveness of AS metrics; (b) We present an examination of an exten-sive set of metrics, including a comprehensive set of adapted diversity metrics; and (c) We use two AS test collections to carry out our investigation.

Section 2 discusses previous work. We describe the met-rics investigated in this study in Section 3, which also con-tains the description of our  X  X eta-evaluation X  methodology. Details of the test collections and experimental setup are provided in Section 4. Section 5 reports our experimental results. We conclude and discuss future work in Section 6.
Section 2.1 reviews both traditional and diversity metrics used in IR. Section 2.2 provides an overview of existing work on evaluating AS, either by measuring the performance of key components in isolation or as a whole. Finally, Section 2.3 summarises methodologies that have been used to com-pare metrics.
Traditional IR evaluation is based on topical relevance, qrel ( q, d ), between a query q and a document d . Traditional IR metrics ignore the document type (e.g. vertical) and mea-sure the quality of a ranked list l by modelling the gain G @ l of a user reading all documents in that list l . For instance, P @ k assumes that after reading the top k results in l , a user X  X  gain G @ k solely depends on the number of relevant documents within the top k results.

Although this metric is simple and widely used, it does not take into account the ranking position, and furthermore, as-sumes that relevance is a binary judgement. To incorporate graded relevance and to take a more fine-grained user model into account, nDCG @ l was proposed [11]. By diminishing the impact of lower ranked relevant documents, nDCG @ l measures the performance of l by cumulating the dimin-ished gain for each position r . A function g ( r ) is defined to measure the gain of reading a document. The more relevant the document is, the higher the gain to the user. Finally, the metric score is normalized by an ideal ranked list l  X  obtained by ranking all relevant documents in descending order of their relevance. Other metrics have been proposed (e.g. RBP [13], ERR [8]); the major difference between nDCG and them is the assumed user model and how g ( r ) is defined.
To consider rewarding topical diversity in ranked lists, a set of diversity metrics have been proposed recently; these include  X  -nDCG [9], IA -nDCG [1] and D #-nDCG [16].  X  -nDCG extends nDCG to account for diversity by discount-ing the gains that accrue according to the intent (subtopic) previously encountered in the ranked list. The novelty-biased gain NG ( r ) is defined as: where J i ( r ) = 1 if a document at rank r is relevant to the i intent and 0 otherwise; C i ( r ) = P r k =1 J i ( k ) is the number of documents observed within the top r results that contained the i th intent. The strength of the novelty-biased discount is controlled by  X  .

Agrawal et al. [1] apply a traditional measure to each subtopic independently and then combined each value to give the expected value of the measure across all intents. This assumes that for a query q with several intents i , the probability of each intent P ( i | q ) is available. For example, nDCG for a given intent i ( nDCG i ) is computed first, and then the intent-aware IA -nDCG is computed as:
D -nDCG [16], by analogy to g ( r ) within nDCG , calcu-lates a global gain GG ( r ) at rank r given various intents: g ( r ) is the gain value for a document at rank r for intent i . Intent recall I -rec [24], i.e. number of intents covered by a ranked list, can be boosted with the following measure:  X  controls the trade-off between relevance and diversity.
These metrics were proposed to evaluate the diversity of ranked lists over subtopics, and have been recently adapted to measure AS performance [27]. We discuss these adapta-tions next.
Current AS metrics measure either each AS component ( VS , IS , RP ) in isolation or as a whole. An AS page P is composed of a set of blocks { B 1 , B 2 , ...B n } , where each block B i consists of a set of items { I i 1 , I i 2 , ...I can be a  X  X eneral web X  page or a vertical result.
Vertical selection ( VS ) has been studied in [4, 12, 26, 28], where the aim is to measure the quality of the set of se-lected verticals, compared with an annotated set obtained by collecting manual labels from assessors [4, 26, 28] or de-rived from user interaction data [12]. The quality is mostly evaluated with standard measures of precision, recall and f-measure [4, 26] using a binary annotated set. Recently, risk has also been incorporated into risk-aware VS metrics [28].
Recent attempts to evaluate the utility of the whole AS page [3, 15, 27] consider the four key factors VS , VD , IS , RP together. For example, [15] evaluate the utility of a page based on a user engagement metric (CTR) when user interaction data is available. Others [5] evaluate the utility of the page by asking annotators to make assessments based on a number of criteria (relevance, diversity) for each page. A lthough those works comprehensively evaluate AS pages, it remains costly to gather assessments for all AS pages.
Arguello et al. [3] collected pairwise preferences on verti-cal block-pairs from users, and then measured the AS page quality by calculating the distance between the page in ques-tion and the ideal (reference) page; the shorter the distance, the better the page. The ideal reference page is obtained by using a voting method for aggregating all pairwise block preference data into a single ranking.

Zhou et al. [27] followed the Cranfield paradigm and pro-posed an evaluation framework for measuring AS page qual-ity using two types of assessments, item topical-relevance and vertical-orientation, gathered independently. Topical-relevance assessment qrel ( q, d ) specifies the topical relevance between a document and a query, whereas vertical-orientation orient ( v i , q ) is the fraction of users that prefer a page to contain items from the vertical v i rather than  X  X eneral web X  results for a query q . An example is AS DCG ( P ), a metric defined as the expected gain G ( B i ) of reading each block B i on page P divided by the expected effort E ( B i ) spent, normalized by the score AS DCG ( P  X  ) of an ideal page P The gain G ( B i ) combines vertical-orientation orient ( v and topical-relevance qrel ( q, I ik ), relating the quality of the block in an independent manner: where the function g () is used so that the relative gain of the vertical can be altered using a tuning parameter  X  . The effort of examining a block E ( B i ) is defined as the accu-mulative effort of reading all the items within it, that is E ( B i ) = P | B i | k =1 E ( I ik ) where the effort E ( I depend on the media type of the item.
 Several existing diversity metrics were adapted to evaluate AS in [27] by treating subtopics as verticals and subtopic importance as vertical-orientation as follows: (i) replacing subtopic importance with orient ( v i , q ); (ii) substituting the user model for ranks to a model that applies to blocks; and finally (iii) normalising according to the ideal AS page. All AS metrics model and combine factors of AS ( VS , VD , IS , RP ) differently. In this paper, we use a subset of them for in-depth analysis of their properties.
To date, and to our knowledge, no existing studies com-paring the reliability and usefulness of metrics in the con-text of AS have been reported. However, this current study is similar to the work by Sakai et al. [16, 21] and Clarke et al. [10] that compare diversity metrics. We therefore follow a similar methodology. For example, we also use discrim-inative power [7] to evaluate AS metrics. The novelty of our contribution lies in the insight that our study brings to the AS area, rather than the more usual linear ranked-list approach. Furthermore, the comprehensive examination on how AS metrics capture and measure the different AS com-ponents is both novel and timely.

Discriminative power is not the only way to evaluate an evaluation metric. Indeed, highly discriminative metrics, while desirable, may not necessarily measure everything that we may want measured . Recently, Sakai [21] proposed the intuitiveness test 1 for this exact purpose. The intuitiveness test compares a metric of interest with a simple golden stan-dard metric that captures the most important properties that the metric should satisfy. In our study, we apply the intuitiveness test within the context of AS and define four golden standard metrics, respectively, for the four AS fac-tors VS , VD , IS , RP . This allows us to investigate how AS metrics capture the key desirable properties of AS.
We should add that other approaches, especially those re-lying on human subjects (for instance to assess a metric X  X  predictive power), are important. For example, by employ-ing Mechanical Turk users, Zhou et al. [27] and Sanderson et al. [19], respectively, examined the predictive power of AS metrics and IR metrics. For example, if a metric prefers one AS page or ranked list over another, does the user also prefer the same page/list? One finding in both works was that AS metrics and IR metrics agree reasonably well with human preferences. Although informative from a user perspective, compared to our study, these studies do not give us much insight into how reliable metrics are at ranking systems, or how well the metrics capture key AS components. We first summarize the AS metrics tested in this study in Section 3.1. Sections 3.2 and 3.3 describe the two methods comparing the  X  X oodness X  of AS metrics, using discrimina-tive power and an intuitiveness test, respectively.
As discussed in Section 2, various AS metrics have been proposed to evaluate key components of AS systems, either in isolation or as a whole. We select a subset of existing AS metrics, listed in Table 1. Some metrics incorporate all four factors ( VS , IS , RP , VD ) (e.g. AS DCG ) whereas others re-late to a subset (e.g.  X  -nDCG ). For metrics concerned with the same subset of factors, the way these factors are incor-porated can vary. For example, AS DCG and AS RBP mainly vary on their assumed user browsing model so that they give different diminishing returns for documents at later ranks. We also include simple metrics that capture one AS fac-tor (detailed in Section 3.3). The selected metrics allow us to investigate all four factors, both individually and when combined, as well as the various categories (traditional IR metrics, adapted diversity metrics, AS metrics and simple single-component metrics) to which they belong. Some met-rics possess parameters that can be tuned to (de)emphasize a factor (e.g.  X  in  X  -nDCG rewards VD differently). In this work, we leave the tuning of these parameters as future work and use standard parameter settings for each metric (we follow settings from previous work [27]).

We briefly explain the differences between the selected metrics (Section 2 has full details). In short, both nDCG and P @10 ignore the vertical type and only consider IS (and RP for nDCG ). Without considering the intent like-lihood P ( i | q ),  X  -nDCG rewards VD by diminishing redun-dant relevant documents. Although incorporating P ( i | q ), IA -nDCG considers each intent independently, and was shown
T his was later renamed as the concordance test [18]. to be biased in rewarding relevant documents with high in-t ent (with a lower emphasis on VD ) [10]. Comparatively speaking, for each rank, D -nDCG and D #-nDCG accumu-late the global gain for all intents, and have been proven to reward VD more. The differences between them is that D #-nDCG explicitly boosts VD by linearly combining D -nDCG with I -rec . AS DCG , AS RBP , AS ERR reward all four components of AS, but differ in the assumed user browsing model (which affects RP ).
Given a test collection and a set of runs, the discriminative power of a metric is measured by conducting a statistical sig-nificance test for every pair of runs, and then counting the number of significant differences. In this paper, we use the randomised version of Tukey X  X  Honestly Significant Differ-ences (HSD) test [7]. This test takes the entire set of runs into account when judging the significance of each run pair. This test is more conservative (compared to e.g. bootstrap test [17]), and hence less likely to lead to significant differ-ences that are not  X  X eal X . We choose this test because of its reliance on modern computational power instead of statisti-cal assumptions.

The main idea behind Tukey X  X  HSD is that if the largest mean difference observed is not significant, then none of the other differences should be significant either. Given a set of runs, the null hypothesis H 0 is  X  X here is no difference be-tween any of the systems X . We perform randomised Tukey X  X  HSD as shown in Algorithm 1 (taken from [7]). From a given matrix X whose element at (row i , column j ) represents the performance of the j th run for the i th topic, we create B new matrices X b by permuting each row at random; then, for ev-ery run pair, we compare the performance  X  of this run pair with the largest performance  X  observed within X b . Finally, for each run pair, we obtain the Achieved Significance Level (ASL or p-value), which represents how likely this would be under H 0 (null hypothesis). As in any other significance test, H 0 is rejected if ASL &lt;  X  .

Using the results of the randomised Tukey X  X  HSD test, we also estimate the performance  X  required to achieve a statistical significance at  X  for a given topic set size as shown in Algorithm 2: we take the smallest observed  X  from all the run pairs that were found to be significantly different.
We now discuss the concordance test that examine the in-tuitiveness of metrics. AS metrics aim to balance four key AS factors ( VS , IS , RP and VD ) when assessing perfor-foreach pair of runs ( r 1 , r 2 ) do count( r 1 , r 2 ) = 0; for b = 1 to B do foreach pair of runs ( r 1 , r 2 ) do Algorithm 1: Obtaining the Achieved Significance
Level with the two-sided, randomised Tukey X  X  HSD given a performance value matrix X whose rows represent topics and columns represent runs. foreach pair of runs ( r 1 , r 2 ) with a significant difference at  X  do
Algorithm 2: Estimating the performance  X  re-quired for obtaining a significant difference at  X  with the randomised Tukey X  X  HSD test. mance. Inevitably, they tend to be complex, making it par-ticularly difficult to determine if a metric is X  X easuring what we want to measure X . To address this, Sakai [21] proposed a method for quantifying X  X hich metric is more intuitive X , and this has been applied to measuring intuitiveness for diversity IR metrics. We now apply his approach to AS.

The concordance test algorithm [21] is shown in Algo-rithm 3. The algorithm computes relative concordance scores for a pair of metrics M 1 and M 2 and a gold standard metric M
GS . The latter represents a basic property that a candi-date metric should satisfy. For our study, we consider four simple metrics as our gold standards, one for each AS fac-tor. Note that these gold standards are simple and some of them (e.g. VS , VD , IS ) are set retrieval metrics based on binary relevance. Since different AS metrics employ differ-ent position-based discounting and different ways to define graded topical relevance, the gold standards should be as agnostic to these differences as possible. Their purpose is to separate out and test the important properties of the more complex AS metrics. The four gold standard metrics are: For a vertical v i and query q , we consider the vertical to be relevant if orient ( v i , q ) is greater than 0 . 5. 2 Note that the
W e set the threshold to 0 . 5, as an assessor majority prefer-ence of 50% is a suitable percentage since the assessments are neither too noisy (25%) or stringent (100%). In addition, the relevant vertical set obtained from this simple thresholding approach is similar to that obtained from Arguello et al. X  X  voting approach [3] (where more relevance assessments are needed).
Disagreements = 0; Correct 1 = 0; Correct 2 = 0; foreach pair of runs ( r 1 , r 2 ) do Intuitive ( M 1 | M 2 , M GS ) = Correct 1 /Disagreements ; Intuitive ( M 2 | M 1 , M GS ) = Correct 2 /Disagreements ;
Algorithm 3: Computing the concordance of met-rics M 1 and M 2 based on preference agreement with golden standard metric M GS . vertical recall rec v can also be referred to as I -rec (intent recall). Moreover, the simple RP metric corr is similar to Arguello et al. voting approach [3]. However, rather than assigning a higher weight to higher positions in the page, corr calculates the correlation by weighting each position equally.
 The steps conducting the concordance test are as follows: We first obtain all pairs of AS systems/pages for which M and M 2 disagree with each other. Then, out of these dis-agreements, we count how often each metric agrees with the gold standard metric. In this way, we can discuss which of the two metrics is the most  X  X ntuitive X . Moreover, we can argue that an ideal metric should be consistent with all four gold standards; we therefore add one additional step by counting how often the metric agrees with a subset of or all four gold standards.
To provide findings not tailored to one data set, and hence generalisable, our experiments are conducted on the two test collections described in Section 4.1. The methodology em-ployed to simulate AS systems is presented in Section 4.2.
An AS test collection consists of a number of verticals, each populated by items of that vertical type, a set of topics expressing information needs relating to one or more verti-cals, and assessments indicating both the topical-relevance of the items and the perceived user-oriented usefulness of their associated verticals to each of the topics.
The first test collection is an AS test collection [25] cre-ated by reusing an existing web collection, ClueWeb09. The verticals were created either by classifying items in the web collections into different genres (e.g. blog, news) or by adding items from existing multimedia collections (e.g. image, video). The topics and topical-relevance assessments of the items across the verticals were obtained by reusing the assessments developed in two TREC evaluation tasks (TREC Web Track and Million-Query Track). The verticals used are listed in Table 2, and correspond to real-world usage of verticals by commercial search engines.
 The second AS test collection [14] is a new dataset used in the TREC FedWeb track 2013. 3 T he collection contains search result pages from 108 web search engines (e.g. Google, Yahoo!, YouTube and Wikipedia). For each engine, several query-based samplings were provided for vertical selection. Relevance judgements were collected by judging both the snippet created by the engine, and the actual document con-tent for the results returned by the engines for a set of queries (reused TREC Web Track 2010 queries). To use the same verticals listed in Table 2, we manually mapped the 108 search engines into them. This was straightforward since the engine categories used were similar to those in Table 2.
The vertical-orientation information of each topic from the first test collection was obtained by providing the ver-tical names (with a description of their characteristics) and asking a set of assessors to make pairwise preference as-sessments, comparing each vertical in turn to the reference  X  X eneral web X  vertical ( X  X s adding results from this verti-cal likely to improve the quality of the ten blue links? X ) [26]. Note that since the two test collections contain the same set of topics (reused from TREC Web Track 2010), the vertical-orientation information from the first collection could be used for the second collection. Some details and statistics of the two test collections are shown in Table 3.
For each topic, we simulate a set of aggregated search pages/systems. We assume that a page consists of ten  X  X en-eral web X  blocks (one  X  X eneral web X  page is a block) and up to three vertical blocks dispersed throughout those ten blocks (where each vertical block consists of a fixed number of three items). Recall that there are three key components of an aggregated search system that can be varied: (i) Ver-tical Selection ( VS ) (ii) Item Selection ( IS ) and (iii) Result Presentation ( RP ). We generate pages by simulating an AS system in which the three components vary in quality.
We simulate four different state-of-the-art VS strategies, namely ReDDE [23], CRCS(e) [22], click-through [2] and vertical-intent [2]. Deriving from sampled vertical represen-tation, ReDDE and CRCS(e) model each verticals average document score in a full-dataset retrieval (all sources to-gether). By contrast, click-through and vertical-intent use, respectively, users X  click-through data and issued queries from a search engine log (AOL-log). Similar to [2], we model VS as a classification task and, for each single VS approach (e.g. ReDDE), the output is n independent prediction prob-ability scores (one per vertical, n is the number of verticals).
Assuming four vertical positions (ToP, MoP, BoP, None) h ttps://sites.google.com/site/trecfedweb/ on the page, each candidate vertical prediction is compared w ith three threshold parameters  X  1  X  3 (one for each posi-tion) to assign the corresponding embedding position. A given vertical is assigned to the highest position for which the vertical prediction probability is greater than or equal to all thresholds below it, and the verticals within the same position are ordered by descending order of prediction proba-bility. Using similar techniques in [3], we obtained a separate development set to tune the three threshold parameters.
For IS we simulate three potentially different levels of relevance. These are Perfect , BM25 , and TF . Perfect selects all items in the vertical that are topically relevant. BM25 and TF select the top three ranked items from the rankings provided by the BM25 and a simple TF (term-frequency) weighting, respectively, with the PageRank score as a prior for both BM25 and TF .

For RP , we simulate three different result presentation approaches: Perfect , Random and Bad . Perfect places the vertical blocks on the page so that gain could potentially be maximised, i.e. all the relevant items are placed before non-relevant items. However, if these items are part of a vertical, we position the highest orientated vertical first. Random randomly disperses the vertical blocks on the page while maintaining the position of the  X  X eneral web X  blocks. Bad reverses the perfectly presented page.

By varying the quality of each of the three key compo-nents, we can vary the quality of the result pages created by an aggregated search system in a more controlled way. For each topic, we can create 36 (4  X  3  X  3) system runs. Therefore, for the discriminative power test, we have C 2 (630) system pairs. Using this approach we can create a near ideal aggregated page for a query by using Perfect VS , Perfect IS , and Perfect RP . This is a greedy approach to the problem and is used as our method of normalisation.
We experiment with both the discriminative power (Sec-tion 5.1) and the intuitiveness (Section 5.2) of AS metrics. Using the two AS test collections (VertWeb11 and Fed-Web13), we evaluated two sets of metrics in terms of their discriminative power. The first set consists of metrics that evaluate only a subset of the AS components, namely, nDCG , P @10 (Traditional Metrics), prec v , rec v , mean -prec and corr (Single-Component Metrics as described in Section 3.3). The second set includes the (recently proposed) AS metrics [27] AS DCG , AS RBP , AS ERR and adapted diversity met-rics  X  -nDCG , IA -nDCG and D #-nDCG . Note that we
C ertain combinations of VS, IS, and RP do not create unique simulated pages. used the standard parameter settings (e.g. setting  X  = 0 . 5 for  X  -nDCG , etc.). We leave the appropriate tuning of the metrics X  parameters for future work.
 Figures 1 and 2 show the ASL (Achieved Significance Level) curves of some selected AS metrics, using the ran-domised Tukey X  X  HSD on FedWeb13 and VertWeb11 col-lections, respectively. Part (a) in each figure (higher part) shows the results with the first set of metrics (traditional IR and AS component-based metrics). Part (b) (lower part) shows the results for the second set of metrics (AS and adapted diversity metrics). The most discriminative met-rics are those closer to the origin in the figures. Table 4 cuts those two figures in half vertically at  X  = 0 . 05 to quantify the discriminative power and the performance  X  required to achieve statistical significance for a given number of topics (56 for VertWeb11 and 50 for FedWeb13). For example, the left side of Table 4 (a) shows that the discriminative power of the component-based metrics mean -prec according to the Tukey X  X  HSD test at  X  = 0 . 05 is 125/630 = 19 . 8% (125 sig-nificantly different run pairs were found) and the  X  required for achieving statistical significance is around 0 . 12. Let  X  M 1  X  M 2  X  denotes the relationship  X  M 2 outperforms M 1 in terms of discriminative power. X  First, by comparing the different component-based metrics in terms of discrimi-native power as shown in Part (a) (higher) of Figures 1 and 2, and the left side of Table 4 (a) and (b), the following trends can be observed: prec v  X  rec v  X  ( mean -prec , P @10)  X  ( nDCG , corr ). We summarise our findings below: Figure 1: FedWeb13 Discriminative Power Evalua-t ion: ASL curves based on the randomised Tukey X  X  HSD. y-axis: ASL (i.e., p-value); x-axis: run pairs sorted by ASL.

Next, as shown in Part (b) (lower) in Figures 1 and 2, and the right side of Table 4 (a) and (b), we compare the different AS metrics in terms of their discriminative power. We see that IA -nDCG  X  D #-nDCG  X  ( AS RBP ,  X  -nDCG )  X  AS DCG  X  AS ERR . Our findings are summarised below: Figure 2: VertWeb11 Discriminative Power Evalua-t ion: ASL curves based on the randomised Tukey X  X  HSD. y-axis: ASL (i.e., p-value); x-axis: run pairs sorted by ASL.
Highly discriminative metrics, while desirable, may not necessarily measure what we expect them to measure. The aim of this section is to answer the question: how do the dif-ferent AS metrics differ from one another, and which ones are more intuitive than others for the purpose of evaluat-ing search result aggregation? We answer this question by conducting a concordance test. Table 5 shows the  X  X ntuitiveness X  scores for a variety of A S metrics, computed using the preference agreement algo-rithm shown in Algorithm 3. As specified in Table 1, our tested AS metrics include a variety of adapted diversity met-rics, existing AS metrics and a set of single-component AS metrics. We select a subset of them (  X  -nDCG, IA -nDCG , AS DCG , D #-nDCG ) representing different frameworks in modelling AS evaluation. In addition, to provide insights on the effectiveness of different user models (e.g. position-based discount model, cascade model) that are used for AS eval-uation, we also include AS DCG , AS RBP and AS ERR and investigate their ability to capture different key AS compo-nents. Due to space limitation, we only show results in Table 5 for the FedWeb13 collection as the results are similar for both test collections. Note that as we have 36  X  35 / 2 = 630 run pairs, we have 50  X  630 = 31500 pairs of aggregated search pages for the tests.

As we are testing intuitiveness with respect to four AS factors ( VS , IS , RP and VD ), Part (a) of Table 5 uses the precision of returned vertical set prec v as the gold-standard, representing how the AS metrics favour aggregated pages that select majority-preferred (relevant) verticals. Part (b) uses the recall of verticals rec v as the gold-standard, repre-senting how AS metrics favour search results with a more diverse sets of verticals. Part (c) computes the intuitiveness scores by showing how AS metrics favour a returned set con-taining a large number of relevant documents, as measured by the mean of precision for each vertical results. Finally, Part (d) measures the  X  X oodness X  of AS systems embedding vertical results into  X  X eneral web X  results, where we use the Spearman Rank Correlation between the AS page of interest and the reference AS page ( X  X erfect X  page) as the measure. For example, Table 5 (a) shows that if we compare  X  -nDCG and IA -nDCG in terms of the component VS (the ability to select relevant verticals), there are 10222 disagreements, and that whereas the intuitive score for  X  -nDCG is only 0 . 742, that for IA -nDCG is 0 . 792. This means that, given a pair of AS pages for which  X  -nDCG and IA -nDCG disagree with each other, IA -nDCG is more likely to agree with prec v the vertical preference than  X  -nDCG .

Let  X  M 1 &gt; M 2  X  denotes the relationship  X  M 1 statistically significantly outperforms M 2 in terms of concordance with a given gold-standard metric. X  As we assume that the sim-ple single-component metrics discussed here can properly reflect the performance of each component, when compar-ing different frameworks (  X  -nDCG , IA -nDCG , D #-nDCG and AS DCG ) for capturing individual key AS component, several trends can be observed from Table 5: 5
I n general, note that pairwise statistical significance is not transitive. However, it turns out that our results do not violate transitivity.
The above translate to the following observations. The intent-aware (IA) metric [1] and recently proposed AS-metric evaluation framework [27] work best for rewarding selecting relevant verticals based on the intuitiveness score. The D # and IA frameworks favour rewarding vertical diversity (pro-moting diverse set of results from different verticals). It is not surprising that the D # framework behaves similarly to rec v since D # boosts diversity by incorporating I -rec into it. The D # and AS metrics tend to reward result page with more topically relevant items whereas AS RBP works best, compared to other user models. The  X  -nDCG and AS ERR metrics consistently perform worst with respect to vertical orientation (VS), vertical diversity (VD) and topical rele-vance (IS). Finally, the  X  -nDCG and AS ERR metrics are better correlated with result presentation (RP) evaluation. A closer examination shows that this can be explained by the fact that the cascade model can better discriminate the small relevance  X  X xchanges X  (differences) at the top or bot-tom of the search result page.

To investigate how the above metrics accurately combine the various AS components, we conduct a further concor-dance test to answer the following question: how often does a given metric agree with a set of components at the same time? For space limitation, we only report results (Part (e) to (g) in Table 5) relating to the component combinations capturing the most crucial aspects of AS, namely VS+IS (orientation and relevance), VS+IS+VD (orientation, rele-vance and diversity) and VS+IS+RP+VD (ultimate utility). Our findings are as follows: We therefore find that D #-nDCG and AS RBP performs best when combining components whereas the D # metric captures better vertical diversity (VD) and AS RBP models better vertical orientation and relevance (VS, IS). Moreover, we quantitatively show the advantages of metrics that cap-ture key components of AS (e.g. VS) over those that do not (e.g.  X  -nDCG ). Further investigations are needed to bet-ter understand why certain combination approaches work better than others.
In this paper, we measured the performance of AS metrics based on both their discriminative power and intuitiveness. To our knowledge, this is the first study to extensively ex-amine properties of metrics in the context of AS. We used an extensive set of existing AS metrics and adapted diversity metrics and test them across two AS test collections. Our main findings are: In terms of both discriminative power and intuitiveness, we demonstrated that the AS-metrics (especially AS RBP ) are the most powerful metrics to evaluate aggregated search. In addition, our work presents a framework to conduct a meta-evaluation for aggregated search using test collections. This is relatively inexpensive to conduct, compared with previous work involving human subjects for annotating preference of large amount of AS page pairs [27].

Our results have several implications: (1) compared with homogeneous federated search, aggregation over heteroge-neous sources might require more refined evaluation mea-sures that not only capture item topical relevance , but also vertical orientation , in order to better model and discrimi-nate system performances; (2) a more principled framework to incorporate and combine key factors ( VS , VD , IS , RP ) of AS could provide better insights in understanding evalu-ation results.

Future work will include a thorough testing of AS met-rics by tuning parameters within various metrics to (de)-emphasise some of the key factors, and a comparison with meta-evaluation results from human subjects to test the re-liability of our approach and results.
 Acknowledgments This work was partially supported by the EU LiMoSINe project (288024).
