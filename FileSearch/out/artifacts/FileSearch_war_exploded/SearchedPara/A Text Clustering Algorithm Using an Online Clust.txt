 In this paper, we propose a text clustering algorithm using an online clustering scheme for initialization called FGSD-MM+. FGSDMM+ assumes that there are at most K max clusters in the corpus, and regards these K max potential clusters as one large potential cluster at the beginning. Dur-ing initialization, FGSDMM+ processes the documents one by one in an online clustering scheme. The first document will choose the potential cluster, and FGSDMM+ will create a new cluster to store this document. Later documents will choose one of the non-empty clusters or the potential clus-ter with probabilities derived from the Dirichlet multinomial mixture model. Each time a document chooses the poten-tial cluster, FGSDMM+ will create a new cluster to store that document and decrease the probability of later docu-ments choosing the potential cluster. After initialization, FGSDMM+ will run a collapsed Gibbs sampling algorithm several times to obtain the final clustering result. Our exten-sive experimental study shows that FGSDMM+ can achieve better performance than three other clustering methods on both short and long text datasets.
 Text Clustering; Gibbs Sampling; Dirichlet Multinomial Mix-ture
Text clustering [1] is a widely studied problem with many applications such as document organization, summarization, and browsing. In [27], we introduced a collapsed Gibbs Sam-pling algorithm for the Dirichlet Multinomial Mixture mod-el (GSDMM) for text clustering. GSDMM represents each cluster as a large document combined by the documents in the cluster and records the size of each cluster. The proba-bility of a document belonging to each cluster is proportional to the size of the cluster and the frequency of each word of the document in the cluster. In this way, GSDMM consid-ers both the size of the cluster and the similarity between the document and the cluster when assigning a documen-t to a cluster. GSDMM assumes that there are at most K max clusters in the corpus. This assumption is weaker than assuming the true number of clusters in the corpus. Besides, this assumption can certainly be right when we set the assumed maximum number of clusters K max to be the number of documents in the corpus. During initialization, GSDMM randomly assigns the documents to K max cluster-s. Then GSDMM traverses the documents for several iter-ations. For each document, GSDMM re-assigns it to one of the K max clusters according to the probability of the doc-ument belonging to each cluster derived from the Dirichlet multinomial mixture (DMM) model [16]. In [27], we found that GSDMM can achieve good performance as long as the assumed maximum number of clusters K max is larger than the true number of clusters. However, it is difficult to set a proper maximum number of clusters for GSDMM as we do not know the true number. As a result, we may have to choose a really large K max to ensure safety which will re-sult in the complexity of GSDMM to be large. In addition, GSDMM randomly assigns the documents to K max clusters during initialization, which will introduce noise into cluster-ing. Because the documents in an initial cluster have high probability of coming from different classes.

An observation is that when the assumed maximum num-ber of clusters K max is much larger than the true number of clusters (such as K max is set to be the number of doc-uments in the corpus), the number of non-empty clusters K non drops quickly when we run GSDMM. Actually, the probability of a document choosing each empty cluster is the same, and there is no difference which empty cluster the document chooses. Therefore, we can regard all empty clusters as a potential cluster, and the probability of a docu-ment choosing this potential cluster is ( K max K non ) times the probability of that document choosing a specific empty cluster. In each iteration, each document can choose one of the K non non-empty clusters or the potential cluster accord-ing to its probability of belonging to the K non + 1 clusters. We call this new text clustering algorithm as Fast GSDMM (abbr. to FGSDMM). The time complexity of FGSDMM is linear to the number of non-empty clusters.
 Although FGSDMM can reduce the time complexity of GSDMM, its space complexity is still linear to the assumed maximum number of clusters. Besides, the initialization step of FGSDMM is the same as GSDMM, which will in-troduc e noise into clustering. An observation is that the initialization approach of FGSDMM wastes much informa-tion, because it does not consider the differences between the clusters. Actually, we can estimate the probability of a doc-ument choosing each cluster as we already know the current representation of the clusters. Under the above inspiration, we propose a new initialization approach for FGSDMM us-ing an online clustering scheme called FGSDMM+. The space complexity of FGSDMM+ is linear to the number of non-empty clusters, because it only needs to store the in-formation of the non-empty clusters. Besides, FGSDMM+ introduces less noise into clustering, because it assigns each document to a cluster according to the probability of the document belonging to each cluster.
 In the experimental study, we compared FGSDMM+ with GSDMM [27], K-means [7], and LDA [5]. We found that FGSDMM+ can achieve better performance than these meth-ods on both short and long text datasets. We compared the speed of FGSDMM+, FGSDMM, and GSDMM with differ-ent assumed maximum number of clusters K max . We found that the speed of GSDMM is approximately linear to the assumed maximum number of clusters K max , and the speed of FGSDMM+ gets almost stable when the assumed maxi-mum number of clusters is larger than the true number of clusters. We also investigated the influence of the parame-ters to the number of clusters found by FGSDMM+ and the performance of FGSDMM+.
 The contributions of this paper are summarized as follows. The remainder of this paper is organized as follows. In Section 2, we first introduce the Dirichlet Multinomial Mix-ture (DMM) model, then we introduce the GSDMM algo-rithm. In Section 3, we propose the FGSDMM algorithm whose time complexity is linear to the number of non-empty clusters. In Section 4, we propose the FGSDMM+ algorith-m which uses an online clustering scheme for initialization and can detect the number of clusters automatically. Sec-tion 5 describes the design of experiments to evaluate the performance of our algorithm and the influence of parame-ters to the algorithm. In Section 6, we review the related work of text clustering. We finally present conclusions and future work in Section 7. In this section, we will introduce the Dirichlet Multinomial Mixture (DMM) model [16] and the GSDMM algorithm we proposed in [27]. In the next two sections, we will point out the drawbacks of GSDMM and propose two new clustering algorithms to cope with these drawbacks.
 K max assumed maximum number of clusters K non number of non-empty clusters
The Dirichlet Multinomial Mixture (DMM) model [16] is a probabilistic generative model for documents, and embod-ies two assumptions about the generative process: (1) the documents are generated by a mixture model [15], and (2) there is a one-to-one correspondence between mixture com-ponents and clusters. The graphical representation of the Dirichlet Multinomial Mixture (DMM) model is shown in Figure 1 (Table 1 shows the notations used in this paper), which is equivalent to the following generative process: Here,  X  X S  X  means  X  X is distributed according to S  X , so the right side is a specification of distribution. When generating document d , the Dirichlet Multinomial Mixture (DMM) model first selects a mixture component (cluster) z for document d according to the mixture weights (weights of clusters),  X , in Equation 2. Then document d is gener-ated by the selected mixture component (cluster) from dis-tribution p ( d j  X  z d ) in Equation 4. The weight vector of the clusters,  X , is generated by a Dirichlet distribution with a hyper-parameter  X  , as in Equation 1. The cluster parame-ters  X  z are also generated by a Dirichlet distribution with a hyper-parameter  X  , as in Equation 3.

In this paper, the probability of document d generated by clust er z d is defined as follows: Here, we make the Naive Bayes assumption: the words in a document are generated independently when the document X  X  cluster assignment z d is known. We also assume that the probability of a word is independent of its position within the document.
In this part, we give the derivation of the collapsed Gibbs sampling algorithm for the Dirichlet Multinomial Mixture (DMM) model, which is different from the one presented in [27]. The documents  X  d = f d i g D i =1 are observed and the cluster assignments  X  X  = f z i g D i =1 are latent. The conditional probability of document d choosing cluster z given the in-formation of other documents and their cluster assignments can be factorized as follows: p ( z d = z j  X  X   X  d ,  X  d,  X ,  X  ) Here, we use the Bayes Rule in Equation 6, and apply the properties of D-Separation [4] in Equation 7.

The first term in Equation 7 indicates the probability of document d choosing cluster z when we know the cluster as-signments of other documents. The second term in Equation 7 can be considered as a predictive probability of document d given  X  d z;  X  d , i.e., the other documents currently assigned to cluster z .

The first term in Equation 7 can be derived as follows: Here, Equation 8 exploits the Sum Rule of Probability [4]. We use the Product Rule of Probability [4] in Equation 9 and apply the properties of D-Separation in Equation 10. The posterior distribution of  X  is a Dirichlet distribution, because we assumed Dirichlet prior Dir ( X  j  X  ) for the Multi-nomial distribution M ult (  X  X  j  X ). In Equation 13, we adopt the  X  function used in [11], which is defined as  X (  X  X  ) = x  X ( x ), we can get Equation 16 from Equation 15. In E-quation 16, m z;  X  d is the number of documents in cluster z without considering document d , and D is the total num-ber of documents in the dataset. Equation 16 indicates that document d will tend to choose larger clusters when we only consider the cluster assignments of the other documents.
Then, we derive the second term in Equation 7 as follows: p ( d j z d = z,  X  d z;  X  d ,  X  ) Here, Equation 17 exploits the Sum Rule of Probability [4]. We use the Product Rule of Probability in Equation 18 and apply the properties of D-Separation [4] to obtain Equation 19. The posterior distribution of  X  z is a Dirichlet distribu-tion, because we assumed Dirichlet prior Dir ( X  z j  X  ) for the Multinomial distribution M ult (  X  d z j  X  z ). Because the  X  func-tion has the following property: ( x + m ) ( x ) = we can get Equation 24 from Equation 23. In Equation 24, N d and N d are the number of occurrences of word w in document d and the total number of words in document d , respectively, and N d = are the number of occurrences of word w in cluster z and the total number of words in cluster z without considering document d , respectively, and n z;  X  d = notice that Equation 24 actually evaluates some kind of sim-ilarity between document d and cluster z , and document d will tend to choose a cluster whose documents share more words with it.

Finally, we have the probability of document d choosing cluster z d given the information of other documents and their cluster assignments as follows: The first part of Equation 25 relates to Rule 1 of GSDMM (i.e., choose a cluster with more documents), as it will have larger value when m z;  X  d (number of documents in cluster z withou t considering document d ) is larger. This is also known as the  X  X icher gets richer X  property, which will lead larger clusters to get larger [24]. The second part of Equa-tion 25 relates to Rule 2 of GSDMM (i.e., choose a cluster which is more similar with the current document), which ac-tually defines the similarity between the document and the cluster. It is a product of N d parts that correspond to the N d words in document d . For each word w in document d , the corresponding part measures the the frequency of word w in cluster z . When a cluster has more documents that share same words with document d , the second part of E-quation 25 will be larger, and document d will be more likely to choose that cluster.

Most text clustering algorithms [1] represent documents with the vector space model [21], in which each document is represented with a vector in length of the size of the vocab-ulary. Differently, GSDMM represents each document with its words and the frequency of each word in the document. GSDMM represents each cluster as a large document com-bined by the documents in the cluster and records the size of each cluster. In detail, GSDMM uses three count variables to record the information of each cluster: n w z (frequency of word w in cluster z ), n z (number of words in cluster z ), m (number of documents in cluster z ).

GSDMM assumes that there are at most K max cluster-s in the corpus. This assumption is weaker than assuming the true number of clusters in the corpus. Besides, this as-sumption can certainly be right when we set the assumed maximum number of clusters K max to be the number of documents in the corpus. During initialization, GSDMM randomly assigns the documents to K max clusters. Then GSDMM traverses the documents for several iterations. In each iteration, GSDMM re-assigns each document to one of the K max clusters according to the probability of the docu-ment belonging to each cluster computed from Equation 25. Because the documents tend to choose clusters with more documents, large clusters will tend to get larger and small clusters will tend to get smaller. After several iterations, some clusters will become empty, as a result, GSDMM can detect the number of clusters automatically.
In [27], we found that GSDMM can achieve good perfor-mance as long as the assumed maximum number of clusters K max is larger than the true number of clusters. However, it is difficult to set a proper maximum number of clusters for GSDMM as we do not know the true number of cluster-s in the dataset beforehand. As a result, we may have to choose a really large assumed maximum number of clusters K max to ensure safety which will result in the complexity of GSDMM to be large.

An observation is that sampling a cluster from the K max clusters for each document wastes much time when the as-sumed maximum number of clusters K max is much larg-er than the true number of clusters, because most of the clusters will become empty quickly. For example, when we set K max to be the number of documents in the corpus, the number of non-empty clusters K non will decrease really quickly when we run GSDMM.

Actually, the probability of a document choosing each empty cluster is the same, and there is no difference which empty cluster the document chooses. Therefore, we can regard all empty clusters as a potential cluster, and the probability of a document choosing this potential cluster is ( K max K non ) times the probability of that documen-t choosing a specific empty cluster. In each iteration, each document will choose one of the K non non-empty clusters or the potential cluster according to its probability of belonging to the K non + 1 clusters.
 Algo rithm 1: FGSDMM Data : Documents  X  d , Maximum number of clusters
Result : Cluster assignments of the documents  X  X  . begin
The pro bability of a document choosing one of the K non non-empty clusters is shown in Equation 25. We denote the potential cluster that represent the K max K non empty clusters as K non + 1, and the probability of a document choosing the potential cluster is as follows: p ( z d = K non + 1 j  X  X   X  d ,  X  d ) /
We call this new text clustering algorithm as FGSDMM (abbr. for Fast GSDMM) which is shown in Algorithm 1. The main difference between FGSDMM and GSDMM is in the way of assigning a document to a cluster. GSDMM sam-ples a cluster from the K max clusters for each document in each iteration. This means the time complexity of GSDMM is linear to the assumed maximum number of clusters K max Different from GSDMM, the time complexity of FGSDMM is linear to the number of non-empty clusters. Although FGSDMM can reduce the time complexity of GSDMM, its space complexity is still linear to the assumed maximum number of clusters K max . Because FGSDMM needs to store the frequency of each word in each cluster and the size of each cluster. For text datasets, the size of the vocabulary is in the order of 10 5 , which means FGSDMM needs much memory when the assumed maximum number of clusters K max is really large.

In addition, FGSDMM randomly assigns the documents to K max clusters during initialization, which will introduce noise into clustering. This is because the documents in an initial cluster have high probability of coming from different classes.

During initialization, FGSDMM traverses the documents and randomly assigns each document to one of the K max clusters with equal probability. In detail, the first documen-t will choose one of the K max empty clusters with equal probability. Later documents will choose one of the K non non-empty clusters or one of the K max K non empty clus-ters with equal probability.
 Algori thm 2: FGSDMM+ Data : Docum ents  X  d , Maximum number of clusters
Result : Cluster assignments of the documents  X  X  . begin An obse rvation is that the initialization approach of FGS-DMM wastes much information, because it does not con-sider the differences between the clusters. Actually, we can estimate the probability of a document choosing each clus-ter as we already know the current representation of the clusters. Under the above inspiration, we propose a new initialization approach for FGSDMM using an online clus-tering scheme. We call this new text clustering algorithm FGSDMM+ which is shown in Algorithm 2.

Just like FGSDMM, FGSDMM+ assumes that there are at most K max clusters in the corpus. Different from FGS-DMM, FGSDMM+ does not allocate space for these K max clusters at the beginning. Instead, it regards the K max po-tential clusters as one large potential cluster.
During initialization, FGSDMM+ processes the documents one by one in an online clustering scheme. The first docu-ment will choose the potential cluster, and FGSDMM+ will create a new cluster to store this document. Later docu-ments will choose one of the K non non-empty clusters or the potential cluster with probabilities computed from E-quation 25 and Equation 26, respectively. Each time a doc-ument chooses the potential cluster, FGSDMM+ will create a new cluster to store that document and decrease the prob-ability of later documents choosing the potential cluster. In this way, the number of non-empty clusters will grow from zero to a certain value, and FGSDMM+ can obtain the ini-tial clustering results. The above initialization approach is identical to the Chinese restaurant process [18] when the maximum number of tables is assumed to be K max .
After initialization, FGSDMM+ will run a Gibbs sam-pling algorithm several times to obtain the final clustering result. The Gibbs sampling step of FGSDMM+ is similar to FGSDMM, except that FGSDMM+ needs to create a new cluster to store a document when it chooses the poten-tial cluster. The space complexity of FGSDMM+ is linear to the number of non-empty clusters K non , because it only needs to store the information of the non-empty clusters. Besides, FGSDMM+ will introduce less noise into cluster-ing, because it assigns each document to a cluster according to the probability of the document belonging to each cluster. We use three real text datasets in the experimental study: http ://qwone.com/  X jason/20Newsgroups/ http://www.daviddlewis.com/resources/testcollections/ reuters21578/ https://github.com/jackyin12/GSDMM/ http://trec.nist.gov/data/microblog.html Tab le 2: Statistics of the text datasets ( D :Number of docu-ments, K :Number of clusters, V : Vocabulary size, Avg Len: Average length of the documents)
The preprocessing step includes converting all letters into lowercase, removing stop words, and stemming. After pre-processing, the statistics of these text datasets are shown in Table 2. We can see the average length of the documents in NG20 and R52 is much larger than that of Tweet89. We plan to evaluate the performance of the clustering methods on both short and long texts with these datasets.
In this part, we introduce the evaluation metrics used in this paper: Homogeneity (H), completeness (C), and Nor-malized Mutual Information (NMI). We used the implemen-tation of these metrics in sklearn 5 in the experimental study.
Homogeneity and completeness [20] are two intuitive e-valuation metric using conditional entropy analysis. Homo-geneity represents the objective that each cluster contains only members of a ground true group. Completeness repre-sents the objective that all members of a ground true group are assigned to the same cluster. Homogeneity (H) and com-pleteness (C) scores are formally defined as follows [20]: wh ere n c is the number of documents in class c , n k is the number of documents in cluster k , n c;k is the number of documents in class c as well as in cluster k , and N is the number of documents in the dataset.

The Normalized Mutual Information (NMI) is widely used to evaluate the quality of the clustering results. NMI mea-sures the amount of statistical information shared by the random variables representing the cluster assignments and the ground true groups of the documents. Normalized Mu-tual Information (NMI) is formally defined as follows [23]: wh ere n c is the number of documents in class c , n k is the number of documents in cluster k , n c;k is the number of documents in class c as well as in cluster k , and N is the number of documents in the dataset. When the clustering results perfectly match the ground true classes, the NMI value will be one. While when the clustering results are randomly generated, the NMI value will be close to zero.
We compare FGSDMM+ with the following three cluster-ing methods: htt p://www.scikit-learn.org In this part, we compare the performance of FGSDM-M+ with GSDMM [27], K-means [7], and LDA [5]. We will not report the result of FGSDMM here, because the perfor-mance of FGSDMM is the same as GSDMM. All algorithms were implemented in java and conducted on a Linux serv-er with Intel Xeon E5310 1.60GHz CPU and 19GB memo-ry. We vary the number of clusters for each dataset. For FGSDMM+ and GSDMM, they correspond to the assumed maximum number of clusters. For all algorithms, we set the maximum number of iterations at 100 (to make a fair comparison). For each algorithm, we run 20 independent trials on each dataset, and report the mean and standard deviation of the NMI of the results in Table 3.

From Table 3, we can see that FGSDMM+ always achieves the highest performance compared with the other three clus-tering methods on all datasets. Meanwhile, the standard de-viations of the 20 independent trials of FGSDMM+ are quite small which means that FGSDMM+ has high consistency. An interesting observation is that all methods perform bet-ter on the short text dataset (Tweet89) than other two long text datasets (NG20 and R52). One possible explanation is that Tweet89 is easier for clustering because it is about events and has smaller vocabulary as shown in Table 2.
In this part, we try to investigate the influence of the assumed maximum number of clusters K max to the speed of FGSDMM+, FGSDMM, and GSDMM. We set  X  = 1,  X  = 0 . 05, and the number of iterations at 30 for all datasets. Figure 2 shows the speed of FGSDMM+, FGSDMM, and GSDMM with different assumed maximum number of clus-ters K max . We set the number of iterations at 30, and vary the assumed maximum number of clusters K max as differ-ent times of the true number of clusters in each dataset. We can see that the speed of GSDMM is approximately linear to the assumed maximum number of clusters K max . Because GSDMM needs to re-assign each document to one of the K max clusters according to Equation 25 in each iteration. FGSDMM can improve the speed of GSDMM, however, the improvement is not obvious on NG20 and R52. The speed of FGSDMM+ gets almost stable when the assumed maxi-mum number of clusters is larger than the true number of clusters.
In this part, we try to investigate the influence of the num-ber of iterations to the number of clusters found by FGSD-Figu re 3: Performance of FGSDMM+ with different number of iterations.
 MM+ and its performance. We set  X  = 1,  X  = 0 . 05, and K max = 2 K true ( K true is the true number of clusters) for all datasets.

Figure 3 shows the performance of FGSDMM+ with dif-ferent number of iterations. From Figure 3, we can see that FGSDMM+ can get stable performance within about ten iterations. This shows that FGSDMM+ is fast to converge. Another observation is that FGSDMM+ can achieve quite good performance with only the initialization step on R52 and Tweet89. The initialization approach of FGSDMM+ is actually an online clustering algorithm which has two prop-erties: 1) it can process the documents one by one in the order of arriving; 2) it can detect new clusters which means it can deal with the problem of concept drift. While the per-formance of the initialization step of FGSDMM+ on NG20 is relatively worse, we will try to investigate this phenomenon in our future study on stream text clustering.
 Figure 4 shows the number of clusters found by FGSDM-M+ with different number of iterations. From Figure 4, we can see that the number of clusters found by the initializa-tion step FGSDMM+ is near the true number of clusters in the datasets. We can also see that the number of clusters found by FGSDMM+ grows on NG20 and R52, while drops on Tweet89. This means later iterations of FGSDMM+ can amend the result of the initialization step. Figu re 4: Number of clusters found by FGSDMM+ with different number of iterations.
In this part, we try to investigate the influence of the assumed maximum number of clusters K max to the number of clusters found by FGSDMM+ and its performance. We set  X  = 1,  X  = 0 . 05, and the number of iterations at 30 for all datasets.

Figure 5 shows the performance of FGSDMM+ with dif-ferent assumed maximum number of clusters. We vary the assumed maximum number of clusters as different times of the true number of clusters. From Figure 5, we can see that FGSDMM+ can achieve good performance as long as the assumed maximum number of clusters K max is larger than the true number of clusters in the datasets.
 Figure 6 shows the number of clusters found by FGSD-MM+ with different assumed maximum number of clusters. We vary the assumed maximum number of clusters as dif-ferent times of the true number of clusters. Different from NG20 and R52, the number of clusters found on Tweet89 grows slightly when K max gets larger than the true num-ber of clusters. One possible explanation is that the average length of the documents in Tweet89 is only 8.56, which is quite short compared with that of NG20 (137.85) and R52 (59.61). The influence of the similarity between the doc-ument and the cluster is relatively small (product of less Figu re 6: Number of clusters found by FGSDMM+ with different maximum number of clusters. Figu re 7: Number of clusters found by FGSDMM+ with different values of  X  . than nine fractions averagely) on Tweet89, and the  X  X ich-er gets richer X  property becomes relatively more importan-t. When K max gets larger, the probability of a document choosing the potential cluster gets larger, because there are ( K max K non )  X  pseudo documents in the potential cluster.
We also find that FGSDMM+ can achieve good perfor-mance when the assumed maximum number of clusters K max is set to be the number of documents on all the datasets. The number of clusters found on NG20, R52, and Tweet89 are around 26, 55, and 192, respectively.
In this part, we try to investigate the influence of  X  to the number of clusters found by FGSDMM+ and its perfor-mance. We set  X  = 0 . 05, K max = 2 K true ( K true is the true number of clusters), and the number of iterations at 30 for all datasets.

Figure 7 and Figure 8 show the number of clusters found by FGSDMM+ and the performance of FGSDMM+ with d-ifferent values of  X  , respectively. An observation is that the influence of  X  is similar to the assumed maximum number of clusters K max . The reason is that  X  is the pseudo num-ber of documents in each cluster, and the probability of the document choosing the potential cluster grows when  X  gets larger. As the average length of the documents in Tweet89 is only 8.56, the similarity between the document and the cluster plays a relatively small rule, and the growth of  X  can increase the probability of the documents choosing the po-tential cluster. On the other hand, the number of clusters found on NG20 and R52 does not grow when we vary  X  from 0.1 to 2, because the similarity between the document and the cluster plays a so important rule and the increase of  X  cannot effect the choice of the documents.
In this part, we try to investigate the influence of  X  to the number of clusters found by FGSDMM+ and its perfor-mance. We set  X  = 1, K max = 2 K true ( K true is the true number of clusters), and the number of iterations at 30 for all datasets.
 Figure 9 and Figure 10 show the performance of FGSDM-M+ and the number of clusters found by FGSDMM+ with different values of  X  , respectively. From Figure 10, we can see that the number of clusters found by FGSDMM+ drops when  X  gets larger. As we know,  X  is the pseudo frequen-cy of each word in each cluster. When  X  gets larger, the probability of a document choosing a cluster is less sensitive to the similarity between the document and the cluster. As a result, the influence of the  X  X icher gets richer X  property becomes more important, and FGSDMM+ will find fewer clusters. Although the number of clusters found by FGS-DMM+ vary a lot when we vary  X  from 0.01 to 0.1, the NMI of the results remains relatively high. An explanation is that FGSDMM+ can achieve larger completeness with larger  X  , and can achieve larger homogeneity with smaller  X  . In other words, FGSDMM+ can balance completeness and homogeneity with  X  .
General surveys on text clustering can be found in [1], [2], and [3]. [22] and [28] give experimental comparisons of different text clustering algorithms.

Partitional algorithms like K-means [10] and K-medoids [17] find the clusters by partitioning the entire dataset in-to a pre-determined number of clusters. The advantage of partitional algorithms is that they are efficient and easy to implement. The Spherical K-means algorithm [7] is used extensively for document clustering due to its low computa-tional and memory requirements. The disadvantages of par-Figu re 10: Number of clusters found by FGSDMM+ with different values of  X  . titional algorithms is that they need to specify the number of clusters in advance, and they are sensitive to initialization.
Hierarchical algorithms [26] recursively find nested clus-ters either in an agglomerative mode or a divisive mode. The hierarchical algorithms are particularly useful to sup-port a variety of searching methods because they naturally create a tree-like hierarchy which can be leveraged for the search process [25]. The drawback of hierarchical algorithms is that they need to assume the true number of clusters or a similarity threshold, and their time complexity is quadratic in the number of documents.

Density-based algorithms [8] define the clusters as areas of higher density than the remainder of the dataset. The advantage of density-based algorithms is that they do not need to specify the number of clusters in advance, and can detect the outliers of the dataset. However, they have limi-tations in handling high-dimensional data like text. Because the feature space of high-dimensional data is usually sparse, density-based algorithms have difficulty to distinguish high-density regions from low-density regions [13].

A comparative study on model-based text clustering algo-rithms can be found in [29]. The most widely used model-based clustering method is the Gaussian Mixture Model (G-MM) [19], which assumes that data points are generated by a mixture of Gaussian distributions. However, the com-plexity of GMM is too large for high-dimensional data like text. Nigam et al. [16] proposed an EM-based algorithm for the Dirichlet Multinomial Mixture model (DMM) for classi-fication with both labeled and unlabeled documents. When only unlabeled documents are provided, it turns out to be a clustering algorithm.

In [27], we introduced a collapsed Gibbs Sampling algo-rithm for the DMM model (GSDMM) for text clustering, which can infer the number of clusters automatically as long as the assumed maximum number of clusters is larger than the true number of clusters. GSDMM assumes that there are at most K max clusters in the corpus. In [27], we found that GSDMM can achieve good performance as long as the assumed maximum number of clusters K max is larger than the true number of clusters. However, it is difficult to set a proper maximum number of clusters for GSDMM as we do not know the true number. As a result, we may have to choose a really large K max to ensure safety which will re-sult in the complexity of GSDMM to be large. In addition, GSDMM randomly assigns the documents to K max clusters during initialization, which will introduce noise into cluster-ing. Because the documents in an initial cluster have high probability of coming from different classes.

Topic models like LDA [5] and PLSA [12] are probabilistic generative models that can model texts and identify latent semantics underlying the text collection. In [14], the au-thors investigated the performance of LDA and PLSA on text clustering. They treat the topics found by topic models as clusters and assign each document to the cluster with the highest value in its topic proportion vector. Different from LDA, we assume that each document is generated by only one topic (cluster) and the words in the document are gen-erated independently when the document X  X  cluster assign-ment is known. We find that this model is more effective for the text clustering task, and our extensive experimen-tal study shows that our algorithm can achieve significantly better performance than LDA on both long and short text datasets.
In this paper, we propose a text clustering algorithm using an online clustering scheme for initialization called FGSD-MM+, which has low time and space complexity and can detect the number of clusters automatically. Our exten-sive experimental study shows that FGSDMM+ can achieve better performance than three other clustering methods on both short and long text datasets. We compared the speed of FGSDMM+, FGSDMM, and GSDMM with different as-sumed maximum number of clusters K max . We found that the speed of GSDMM is approximately linear to the assumed maximum number of clusters K max , and the speed of FGSD-MM+ gets almost stable when the assumed maximum num-ber of clusters is larger than the true number of clusters. This work was supported in part by National Basic Re-search Program of China (973 Program) under Grant No. 2014CB340505, National Natural Science Foundation of Chi-na under Grant No. 61532010 and 61272088, and Tsinghua University Initiative Scientific Research Program under Grant No.20131089256. [1] C. C. Aggarwal and C. Zhai. A survey of text [2] D. C. Anastasiu, A. Tagarelli, and G. Karypis. [3] N. O. Andrews and E. A. Fox. Recent developments in [4] C. M. Bishop and N. M. Nasrabadi. Pattern [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [6] A. Cardoso-Cachopo. Improving Methods for [7] I. S. Dhillon and D. S. Modha. Concept [8] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A [9] T. L. Griffiths and M. Steyvers. Finding scientific [10] J. A. Hartigan and M. A. Wong. Algorithm as 136: A [11] G. Heinrich. Parameter estimation for text analysis. [12] T. Hofmann. Probabilistic latent semantic indexing. In [13] A. K. Jain. Data clustering: 50 years beyond k-means. [14] Y. Lu, Q. Mei, and C. Zhai. Investigating task [15] G. McLachlan and K. Basford. Mixture Models: [16] K. Nigam, A. McCallum, S. Thrun, and T. M.
 [17] H.-S. Park and C.-H. Jun. A simple and fast algorithm [18] J. Pitman. Exchangeable and partially exchangeable [19] D. Reynolds. Gaussian mixture models. Encyclopedia [20] A. Rosenberg and J. Hirschberg. V-measure: A [21] G. Salton, A. Wong, and C. S. Yang. A vector space [22] M. Steinbach, G. Karypis, V. Kumar, et al. A [23] A. Strehl and J. Ghosh. Cluster ensembles X  X  [24] Y. W. Teh. Dirichlet process. In Encyclopedia of [25] E. M. Voorhees. Implementing agglomerative [26] P. Willett. Recent trends in hierarchic document [27] J. Yin and J. Wang. A dirichlet multinomial mixture [28] Y. Zhao and G. Karypis. Empirical and theoretical [29] S. Zhong and J. Ghosh. Generative model-based
