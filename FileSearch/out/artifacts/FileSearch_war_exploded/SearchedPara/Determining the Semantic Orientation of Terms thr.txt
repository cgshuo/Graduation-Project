 Sentiment classification is a recent subdiscipline of text clas-sification which is concerned not with the topic a document is about, but with the opinion it expresses. It has a rich set of applications, ranging from tracking users X  opinions about products or about political candidates as expressed in online forums, to customer relationship management. Functional to the extraction of opinions from text is the determination of the orientation of  X  X ubjective X  terms contained in text, i.e. the determination of whether a term that carries opin-ionated content has a positive or a negative connotation. In this paper we present a new method for determining the ori-entation of subjective terms. The method is based on the quantitative analysis of the glosses of such terms, i.e. the definitions that these terms are given in on-line dictionar-ies, and on the use of the resulting term representations for semi-supervised term classification. The method we present outperforms all known methods when tested on the recog-nized standard benchmarks for this task.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering; Search process ; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Linguistic processing ; I.2.7 [ Artif-icial Intelligence ]: Natural Language Processing X  Text analysis ; I.5.2 [ Pattern Recognition ]: Design Method-ology X  Classifier design and evaluation Algorithms, Experimentation Opinion Mining, Text Classification, Semantic Orientation, Sentiment Classification, Polarity Detection
Text classification (TC) is the task of automatically at-tributing a document d i to zero, one or several among a predefined set of categories C = { c 1 ,...,c n } basedonthe analysis of the contents of d i . Throughout the history of TC, topic-relatedness (aka thematic affinity ,or aboutness )has been the main dimension in terms of which TC has been studied, with categories representing topics and classifica-tion coinciding with the assignment to c j of those documents that were deemed to be about topic c j .

With the improvement of TC technology, and with the ensuing increase in the effectiveness and efficiency of text classifiers, new (and less obvious) dimensions  X  X rthogonal X  to topic-relatedness have started to be investigated. Among these, of particular relevance are genre classification ,asin deciding whether a given product description is a Review or an Advertisement ; author classification (aka authorship at-tribution ), as in deciding who, among a predefined set of candidate authors, wrote a given text of unknown or dis-puted paternity; and sentiment classification , as in deciding whether a given text expresses a positive or a negative opin-ion about its subject matter. It is this latter task that this paper focuses on.

In the literature, sentiment classification [4, 14] also goes under different names, among which opinion mining [2, 5, 11], sentiment analysis [12, 13], sentiment extraction [1], or affective rating [3]. It has been an emerging area of re-search in the last years, largely driven by applicative inter-est in domains such as mining online corpora for opinions, or customer relationship management. Sentiment classification can be divided into several specific subtasks: 1. determining subjectivity , as in deciding whether a given 2. determining orientation (or polarity ), as in deciding 3. determining the strength of orientation , as in deciding Functional to all these tasks 1 is the determination of the orientation of individual terms present in the text, such as determining that (using Turney and Littman X  X  [18] exam-ples) honest and intrepid have a positive connotation while disturbing and superfluous have a negative connotation, since it is by considering the combined contribution of these terms that one may hope to solve Tasks 1, 2 and 3. The con-ceptually simplest approach to this latter problem is prob-ably Turney X  X  [17], who has obtained interesting results on Task 2 by considering the algebraic sum of the orientations of terms as representative of the orientation of the document they belong to; but more sophisticated approaches are also possible [7, 15, 19].

We propose a novel method for determining the orien-tation of terms. The method relies on the application of semi-supervised learning to the task of classifying terms as belonging to either Positive or Negative . The novelty of the method lies in the fact that it exploits a source of infor-mation which previous techniques for solving this task had never attempted to use, namely, the glosses (i.e. textual def-initions) that the terms have in an online  X  X lossary X , or dic-tionary. Our basic assumption is that terms with similar orientation tend to have  X  X imilar X  glosses: for instance, that the glosses of honest and intrepid will both contain ap-preciative expressions, while the glosses of disturbing and superfluous will both contain derogative expressions. The method is semi-supervised, in the sense that 1. a small training set of  X  X eed X  Positive and Negative 2. before learning begins, the training set is enriched by We test the effectiveness of our algorithm on the three bench-marks previously used in this literature, and first proposed in [6, 9, 18], respectively. Our method is found to outper-form the previously known best-performing method [18] in terms of accuracy, although by a small margin. This result is significant, notwithstanding this small margin, since our method is computationally much lighter than the previous top-performing method, which required a space-and time-consuming phase of Web mining.
In Section 2 we review in some detail the related literature on determining the orientation of terms. The methods and results presented in this section are analysed and taken as reference in Section 3, which describes our own approach to determining the orientation of terms, and in Sections 4 and 5, which report on the experiments we have run and on the results we have obtained. Section 6 concludes.
Task1maybeseenasbeingsubsumedbyTask2incase this latter also includes a Neutral category. Similarly, Task 2 may be seen as being subsumed by Task 3 in case this lat-ter contains an ordered sequence of categories ranging from Strongly Negative to Neutral to Strongly Positive .
The work of Hatzivassiloglou and McKeown [6] has been the first to deal with the problem of determining the orien-tation of terms. The method attempts to predict the orien-tation of (subjective) adjectives by analysing pairs of adjec-tives (conjoined by and , or , but , either-or ,or neither-nor ) extracted from a large unlabelled document set. The un-derlying intuition is that the act of conjoining adjectives is subject to linguistic constraints on the orientation of the ad-jectives involved (e.g. and usually conjoins two adjectives of the same orientation, while but conjoins two adjectives of opposite orientation). This is shown in the following three sentences (where the first two are perceived as correct and the third is perceived as incorrect) taken from [6]: 1. The tax proposal was simple and well received 2. The tax proposal was simplistic but well 3. (*) The tax proposal was simplistic and well Their method to infer the orientation of adjectives from the analysis of their conjunctions uses a three-step supervised learning algorithm: 1. All conjunctions of adjectives are extracted from a set 2. The set of the extracted conjunctions is split into a 3. A clustering algorithm uses the graph produced in Step For their experiments, the authors used a term set consist-ing of 657/679 adjectives labelled as being Positive / Negative (hereafter, the HM term set ). The document collection from which they extracted the conjunctions of adjectives is the unlabelled 1987 Wall Street Journal document set 2 .Inthe experiments reported in [6], the above algorithm determines the orientation of adjectives with an accuracy of 78.08% on the full HM term set. Available from the ACL Data Collection Initiative as CD-ROM 1 ( http://www.ldc.upenn.edu/Catalog/ ).
Turney and Littman [18] have approached the problem of determining the orientation of terms by bootstrapping from a pair of two minimal sets of  X  X eed X  terms (hereafter, we will call such a pair a seed set ): which they have taken as descriptive of the categories Posi-tive and Negative . Their method is based on computing the pointwise mutual information (PMI) of the target term t with each seed term t i as a measure of their semantic association. Given a term t , its orienta-tion value O ( t ) (where positive value means positive orienta-tion, and higher absolute value means stronger orientation) is given by The authors have tested their method on the HM term set from [6] and also on the categories Positive and Negative defined in the General Inquirer lexicon [16]. The General Inquirer is a text analysis system that uses, in order to carry out its tasks, a large number of categories 3 ,eachone denoting the presence of a specific trait in a given term. The two main categories are Positive / Negative , which con-tain 1,915/2,291 terms having a positive/negative polarity. Examples of positive terms are advantage , fidelity and worthy , while examples of negative terms are badly , cancer , stagnant . In their experiments the list of terms is reduced to 1,614/1,982 entries (hereafter, the TL term set )afterre-moving terms appearing in both categories (17 terms  X  e.g. deal ) and reducing all the multiple entries of a term in a category, caused by multiple senses, to a single entry.
Pointwise mutual information is computed using two meth-ods, one based on IR techniques (PMI-IR) and one based on latent semantic analysis (PMI-LSA). In the PMI-IR method, term frequencies and co-occurre nce frequencies are measured by querying a document set by means of a search engine with a  X  t  X  X uery,a X  t i  X  query, and a  X  t NEAR t i  X  X uery, and using the number of matching documents returned by the search engine as estimates of the probabilities needed for the computation of PMI in Equation 1. In the Al-taVista search engine 4 ,whichwasusedintheexperiments, the NEAR operator produces a match for a document when its operands appear in the document at a maximum distance of ten terms, in either order. This is a stronger constraint than the one enforced by the AND operator, that simply requires its operands to appear anywhere in the document.
In the experiments, three document sets were used for this purpose: (i) AV-Eng , consisting of all the documents in the English language indexed by AltaVista at the time of the ex-periment; this amounted to 350 million pages, for a total of
The definitions of all such categories are available at http://www.webuse.umd.edu:9090/ http://www.altavista.com/ about 100 billion term occurrences; (ii) AV-CA , consisting of the AV-Eng documents from .ca domains; this amounted to 7 million pages, for a total of about 2 billion term occur-rences; and (iii) TASA , consisting of documents collected by Touchstone Applied Science Associates 5 for developing  X  X he Educator X  X  Word Frequency Guide X ; this amounted to 61,000 documents, for a total of about 10 million word occurrences.

The results of [18] show that performance tends to in-crease with the size of the document set used; this is quite intuitive, since the reliability of the co-occurrence data in-creases with the number of documents on which co-occurrence is computed. On the HM term set, the PMI-IR method us-ing AV-Eng outperformed by an 11% margin (87.13% vs. 78.08%) the method of [6]. It should be noted that, in or-der to avoid overloading the AltaVista server, only a query every five seconds was issued, thus requiring about 70 hours for downloading the AV-Eng document set. On the much smaller TASA document set PMI-IR was computed locally by simulating the behaviour of AltaVista X  X  NEAR operator; this document set brought about a 20% decrease in accuracy (61.83% vs. 78.08%) with respect to the method of [6]. Us-ing AND instead of NEAR on AV-Eng brought about a 19% decrease in accuracy with respect to the use of NEAR on the TL term set (67.0% vs. 82.84%). The PMI-LSA measure was applied only on the smallest among the three document sets (TASA), due to its heavy computational requirements. The technique showed some improvement over PMI-IR on the same document set (a 6% improvement on the TL term set, a 9% improvement on the HM term set).
Kamps et al. [9] focused on the use of lexical relations de-fined in WordNet (WN) 6 . They defined a graph on the ad-jectives contained in the intersection between the TL term set and WN, adding a link between two adjectives whenever WN indicates the presence of a synonymy relation between them. On this graph, the authors defined a distance mea-sure d ( t 1 ,t 2 ) between terms t 1 and t 2 , which amounts to the length of the shortest path that connects t 1 and t 2 (with d ( t 1 ,t 2 )=+  X  if t 1 and t 2 are not connected). The orien-tation of a term is then determined by its relative distance from the two seed terms good and bad , i.e.
 The adjective t is deemed to belong to Positive iff SO ( t ) &gt; 0, and the absolute value of SO ( t ) determines, as usual, the strength of this orientation (the constant denominator d ( good , bad ) is a normalization factor that constrains all val-ues of SO to belong to the [  X  1 , 1] range).

With this method, only adjectives connected to any of the two chosen seed terms by some path in the synonymy relation graph can be evaluated. This is the reason why the authors limit their experiment to the 663 adjectives of the TL term set (18.43% of the total 3,596 terms) reachable from either good or bad through the WN synonymy relation (hereafter, the KA set) . They obtain a 67.32% accuracy value, which is not terribly significant given the small test set and the limitations inherent in the method. http://www.tasa.com/ http://wordnet.princeton.edu/
We present a method for determining the orientation of a term based on the classification of its glosses. Our process is composed of the following steps: 1. A seed set ( S p , S n ), representative of the two cate-2. Lexical relations (e.g. synonymy) from a thesaurus, or 3. For each term t i in S p  X  S n or in the test set (i.e. the set 4. A binary text classifier is trained on the terms in S p Step 2 is based on the hypothesis that the lexical relations used in this expansion phase, in addition to defining a re-lation of meaning, also define a relation of orientation: for instance, it seems plausible that two synonyms may have the same orientation, and that two antonyms may have opposite orientation 8 . This step is thus reminiscent of the use of the synonymy relation as made by Kamps et al. [9]. Any rela-tion between terms that expresses, implicitly or explicitly, similar (e.g. synonymy) or opposite (e.g. antonymy) orien-tation, can be used in this process. It is possible to com-bine more relations together so as to increase the expansion rate (i.e. computing the union of all the expansions obtain-able from the individual relations), or to implement a finer selection (i.e. computing the intersection of the individual expansions).

In Step 3, the basic assumption is that terms with a sim-ilar orientation tend to have  X  X imilar X  glosses: for instance, that the glosses of honest and intrepid will contain both appreciative expressions, while the glosses of disturbing and superfluous will contain both derogative expressions. Note that, quite inevitably, the resulting textual represen-tations will also contain  X  X oise X , in the form of the glosses related to word senses different from the ones intended 9
Altogether, the learning method we use is semi-supervised (rather than supervised), since some of the  X  X raining X  data used have been labelled by our algorithm, rather than by human experts.
In general a term t i may have more than one gloss, since it may have more than one sense; dictionaries normally as-sociate one gloss to each sense. This intuition is basically the same as that of Kim and Hovy [10], whose paper was pointed out to us at the time of going to press.
Experiments in which some unintended senses and their glosses are filtered out by means of part-of-speech analysis are described in Section 5.

Performing gloss classification as a device for classifying the terms described by the glosses, thus combining the use of lexical resources and text classification techniques, has two main goals: (i) taking advantage of the richness and preci-sion of human-defined linguistic characterizations as avail-able in lexical resources such as WordNet; and (ii) enabling the classification of any term, provided there is a gloss for it in the lexical resource. This latter point is relevant, since it means that our method can classify basically any term. This is in sharp contrast with e.g. the method of [6], which can only be applied to adjectives, and with that of [9], which can only be applied to terms directly or indirectly connected to the terms good or bad through the WordNet synonymy relation.
We have run our experiments on the HM, TL, and KA term sets, described in Sections 2.1, 2.2, and 2.3, respec-tively. As discussed in Section 3, the method requires boot-strapping from a seed set ( S p , S n )representativeofthecat-egories Positive and Negative .Intheexperimentswehave alternatively used the same seven positive and seven nega-tive terms used in [18] (the Tur training set), as listed in Section 2, or the singleton sets { good } and { bad } (the Kam training set), as used in [9]. Note that Kam is a proper subset of Tur .
We have used WordNet version 2.0 (WN)as the source of lexical relations, mainly because of its ease of use for automatic processing. However, any thesaurus could be used in this process.

From the many lexical relations defined in WN, we have chosen to explore synonymy ( Syn ;e.g. use / utilize ), di-rect antonymy ( Ant D ;e.g. light / dark ), indirect antonymy ( Ant I ;e.g. wet / parched ) 10 , hypernymy ( Hyper ;e.g. car / vehicle )and hyponymy ( Hypon ,theinverseofhypernymy; e.g. vehicle / car ), since they looked to us the most obvious candidate transmitters of orientation. We have made the as-sumption that Syn , Hyper ,and Hypon relate terms with the same orientation, while Ant D and Ant I relate terms with opposite orientation.

The function ExpandSimple , which we have used for ex-panding ( S p , S n ), is described in Figure 1. The input pa-rameters are the initial seed set ( S p , S n ) to be expanded, the graph defined on all the terms by the lexical relation used for expansion, and a flag indicating if the relation expresses similar or opposite orientation between two terms related through it. The training set is built by initializing it to the seed set (Step 1), and then by recursively adding to it all terms directly connected to training terms in the graph of the considered relation (Step 2) 11 . The role of Steps 3 and 4 is to avoid that the same term be added to both S p and S this is accomplished by applying the two rules of Priority
Indirect antonymy is defined in WN as antonymy extended to those pairs whose opposition of meaning is mediated by a third term; e.g. wet / parched , are indirect antonyms, since their antonymy is mediated by the similarity of parched and dry . It should be remarked that Ant D  X  Ant I .
For non-symmetric relations, like hypernymy, the edge di-rection must be outgoing from the seed term.
Figure 1: Basic expansion function for seed sets. ( X  X f a term belongs to S p (resp. S n ), it cannot be added to S n (resp. S p ) X ) and Tie-break ( X  X f a term is added at the same time to both S p and S n , it is not useful, and can thus be eliminated from both X ). The relations we have tested in seed set expansion are: Restricting a relation R to a given part of speech (POS) (e.g. adjectives) means that, among the terms related through R with the target term t , only those that have the same POS as t are included in the expansion. This is possible since WN relations are defined on word sen ses, rather than words, and since WN word senses are POS-tagged 12 .

After evaluating the effectiveness of individual relations (see Section 5), we have chosen to further investigate the combination of the best-performing ones, i.e.: Syn ( J )  X  Ant D ( J ), Syn ( J )  X  Ant D ( J ), Syn ( J )
In the experiments reported in this paper the only restric-tion we test is to adjectives, since all the terms contained either in the Tur or in the Kam seed sets are adjectives. Syn ( J )  X  Ant I ( J ), and the corresponding versions not re-stricted to adjectives.

In the experiments, we have used these relations itera-tively, starting from the seed set ( S p , S n ) and producing various chains of expansion, iterating until no other terms canbeaddedto S p  X  S n 13 .
The creation of textual representations of terms is based on the use of glosses extracted from a dictionary.
We have first experimented with the (freely accessible) online version of the Merriam-Webster dictionary 14 ( MW ). We have gathered the MW glosses by using a Perl script that, for each term, queries the MW site for the dictio-nary definition of the term, retrieves the html output from the server, isolates the glosses from the other parts of the document (e.g. side menus, header banner), and removes html tags. After this processing, some text unrelated to the glosses is still present in the resulting text, but more precise text cleaning would require manual processing, because of the extremely variable structure of the entries in MW .For this reason we have switched to WordNet, leaving the use of MW only to a final experiment on an optimized setting.
Glosses in WN have instead a regular format, that allows the production of cleaner textual representations ( Figure 2 for an example). In WN, the senses of a word t are grouped by POS; each sense s i ( t )of t is associated to (a) a list of descriptive terms that characterize s i ( t ) 15 , (b) the gloss that describes s i ( t ), and (c) a list of example phrases in which t occurs in the s i ( t ) sense. While descriptive terms and glosses usually contain terms that have a strong relation with the target term t , example phrases often do not contain any term related to t , but only t in a context of use.
We have tested four different methods for creating textual representations of terms. The first one puts together the de-scriptive terms and the glosses (we dub it the DG method ), while the second also includes the sample phrases ( the DGS method ); if the lexical relation used for expansion is limited to a given POS (e.g. adjectives), we use only the glosses for the senses having that POS. We have derived the third and fourth method by applying to the DG and DGS tex-tual representations negation propagation [1], that consists in replacing all the terms that occur after a negation in a sentence with negated versions of the term (e.g. in the sen-tence This is not good ,theterm good is converted to the term  X  good ), thus yielding the DG  X  and DGS  X  methods.
We have classified terms by learning a classifier from the vectorial representations of the terms in ( S p , S n ), and by then applying the resulting binary classifier ( Positive vs. Negative ) to the test terms. We have obtained vectorial representations for the terms from their textual represen-tations by performing stop word removal and weighting by cosine-normalized tfidf ; we have performed no stemming.
We have reached a maximum of 16 iterations for the Ant D relation when used on the Kam seed set. http://www.m-w.com/
We have also ran some experiments in which we have used the descriptive terms directly in the expansion phase, by considering them synonyms of the target term. These ex-periments have not produced positive results, and are thus not reported here. Figure 2: WordNet output for the term unfortunate .
The learning algorithms we have tested are the naive Baye-sian learner using the multinomial model ( NB ), support vector machines using linear kernels, and the PrTFIDF prob-abilistic version of the Rocchio learner [8] 16 .
The various combinations of choices of seed set, expan-sion method (also considering the variable number of ex-pansion steps steps), method for the creation of textual rep-resentations, and classification algorithm, resulted in several thousands different experiments. Therefore, in the following we only report the results we have obtained with the best-performing combinations.

Table 5 shows the accuracy obtained using the base seed sets ( Tur and Kam ) with no expansion and the NB classi-fier. The accuracy is still relatively low because of the small size of the training set, but for the KA term set the result obtained using DGS  X  representations is already better than the best accuracy reported in [9] on the same term set.
Table 5 shows an average 4 . 4% increase (with standard de-viation  X  =1 . 14) in accuracy in using DGS representations versus DG ones, and an average 5 . 7% increase (  X  =1 . 73) by using representations obtained with negation propaga-tion versus ones in which this has not been used. We have noted this trend also across all other experiments: the best performance, keeping all other parameters fixed, is always obtained using DGS  X  representations. For this reason in the rest of the paper we only report results obtained used the DGS  X  method.

Applying expansion methods to seed sets improves results just after a few iterations. Figure 3 illustrates the accuracy values obtained in the classification of the TL term set by applying expansion functions to the Kam seed set, using the various lexical relations or combinations thereof listed in Section 4.2. The Hyper relation is not shown because it has always performed worse than with no expansion at all; a possible reason for this is that hypernymy, expressing the relation  X  X s a kind of X , very often connects (positively or neg-
The naive Bayesian and PrTFIDF learners we have used are from McCallum X  X  Bow package ( http://www-2.cs.cmu.edu/~mccallum/bow/ ), while the SVM learner we have used is version 6.01 of Joachims X  SV M light ( http://svmlight.joachims.org/ ).
 Table 1: Accuracy (%) in classification using the base seed sets (with no expansion), the NB learner and various textual representations.
 atively) oriented terms to non-oriented terms (e.g. quality is a hypernym of both good and bad ).

Figure 3 also shows that the restriction to adjectives of the lexical relations (e.g. Syn ( J ), Ant D ( J ), Ant I duces better results than using the same relation without restriction on POS (e.g. Syn (  X  ), Ant D (  X  ), Ant I (  X  average increase in accuracy obtained by bounding the lex-ical relations to adjectives versus not bounding them, mea-sured across all comparable experiments, amounts to 2.88% (  X  =1 . 76). A likely explanation of this fact is that many word senses associated with POSs other than adjective are not oriented, even if other  X  X djective X  senses of the same term are oriented (e.g. the noun good , in the sense of  X  X rod-uct X , has no orientation). This means that, when used in the expansion and in the generation of textual representa-tions, these senses add  X  X oise X  to the data, which decreases accuracy. For instance, if no restriction on POS is enforced, expanding the adjective good through the synonymy rela-tion will add the synonyms of the noun good (e.g. product ) to S p ; and using the glosses for the  X  X oun X  senses of good will likely generate noisy representations.

Looking at the number of terms contained in the expanded sets after applying all possible iterations, we have, using the Kam seed set, 22,785 terms for Syn (  X  ), 14,237 for Syn ( J ), 6,727 for Ant D (  X  ), 6,021 for Ant D ( J ), 14,100 for Ant 13,400 for Ant I ( J ), 26,137 for Syn (  X  )  X  Ant I (  X  ), and 16,686 for Syn ( J )  X  Ant I ( J ). Expansions based on the Tur seed set are similar to those obtained using the Kam seed set, prob-ably because of the close lexical relations occurring between the seven positive/negative terms. Across all the experi-ments, the average difference in accuracy between using the Tur seed set or the Kam seed set is about 2.55% in favour of the first (  X  =3 . 03), but if we restrict our attention to the 100 best-performing combinations we find no relevant difference (0.08% in favour of Kam ,  X  =0 . 43).

Figure 3 shows that the best-performing relations are the simple Syn ( J )and Ant I ( J ) relations, and the combined re-lations Syn ( J )  X  Ant I ( J ), Syn ( J )  X  Ant D ( J ); these results are confirmed by all the experiments, across all learners, seed sets, and test sets.

Tables 2, 3 and 4 show the best results obtained on each seed set ( Tur and Kam ) on the HM, TL and KA test sets, re-spectively, indicating the learner used, the expansion method and the number of iterations applied, and comparing our results with the results obtained by previous works on the same test sets [6, 9, 18].

On the HM test set (Table 2) the best results are obtained with SVMs (87.38% accuracy), using the Kam seed set and accuracy (%) Figure 3: Accuracy in the classification ( NB classi-fier) of the TL term set, using various lexical rela-tions to expand the Kam seed set. the Syn ( J )  X  Ant I ( J ) relation. Our best performance is 0.3% better than the best published result [18] and 12% better than the result of [6] on this dataset.

On the TL test set (Table 3) the best results are obtained with the PrTFIDF learner (83.09%) using the Kam seed set and the Syn ( J )  X  Ant I ( J ) relation, thus confirming the results on the HM term set. Ou r best performance is 0.3% better than the only published result on this dataset [18].
On the KA test set (Table 4) the best results are obtained with SVMs (88.05%), again using the Kam seed set and the Syn ( J )  X  Ant I ( J ) relation, again confirming the results on the TL and HM term sets. Our best performance is 31% better than the only published result on this dataset [9].
In a final experiment we have applied again the best-performing combinations, this time using textual represen-tations extracted from the Merriam-Webster on-line dictio-nary (see Section 4.3) instead of WN. We have obtained accuracies of 83.71%, 79.78%, and 85.44% on the HM, TL, and KA test sets, thus showing that it is possible to obtain acceptable results also by using resources other than WN.
In our comparisons with previously published methods we note that, while improvements with respect to the methods of [6, 9] have been dramatic, the improvements with respect to the method of [18] have been marginal. However, com-pared to the method of [18], ours is much less data-intensive: Table 3: Best results in the classification of TL. in our best-performing experiment on the TL term set we used an amount of data (consisting of the glosses of our terms) roughly 200,000 times smaller than the amount of data (consisting of the documents from which to extract co-occurrence data) required by the best-performing experi-ment of [18] (about half a million vs. about 100 billion word occurrences) on the same term set. The time required by our method for a complete run, from the iterative expansion of seed sets to the creation of textual representations, their indexing and classification, is about 30 minutes, while the best-performing run of [18] required about 70 hours. In an experiment using a volume of data only 20 times the size of ours (10 million word occurrences), [18] obtained accuracy values 22% inferior to ours (65.27% vs. 83.09%), and at the price of using the time-consuming PMI-LSA method. We should also mention that we bootstrap from a smaller seed set than [18], actually a subset of it containing only 1+1 seed terms instead of 7+7 17 .
We have presented a novel method for determining the orientation of subjective terms. The method is based on semi-supervised learning applied to term representations ob-tained by using term glosses from a freely available machine-readable dictionary. When tested on all the publicly avail-able corpora for this task, this method has outperformed all the published methods, although the best-performing known method is beaten only by a small margin [18]. This result is valuable notwithstanding this small margin, since it was obtained with only 1 training term per category, and with a method O (10 5 ) times less data-intensive and O (10 2 )times less computation-intensive than the method of [18]
Additionally, we should mention that our results are also fully reproducible. This is not true of the results of [18], due (i) to the fluctuations of Web content, and (ii) to the fact that the query language of the search engine used for those experiments (AltaVista) does not allow the use of the NEAR operator any longer. Table 4: Best results in the classification of KA. This work was partially supported by Project ONTO-TEXT  X  X rom Text to Knowledge for the Semantic Web X , funded by the Provincia Autonoma di Trento under the 2004 X 2006  X  X o ndo Unico per la Ricerca X  funding scheme. [1] S. R. Das and M. Y. Chen. Yahoo! for Amazon: [2] K. Dave, S. Lawrence, and D. M. Pennock. Mining the [3] S. D. Durbin, J. N. Richter, and D. Warner. A system [4] Z. Fei, J. Liu, and G. Wu. Sentiment classification [5] G. Grefenstette, Y. Qu, J. G. Shanahan, and D. A. [6] V. Hatzivassiloglou and K. R. McKeown. Predicting [7] V. Hatzivassiloglou and J. M. Wiebe. Effects of [8] T. Joachims. A probabilistic analysis of the Rocchio [9] J.Kamps,M.Marx,R.J.Mokken,andM.D.Rijke.
 [10] S.-M. Kim and E. Hovy. Determining the sentiment of [11] S. Morinaga, K. Yamanishi, K. Tateishi, and [12] T. Nasukawa and J. Yi. Sentiment analysis: [13] B. Pang and L. Lee. A sentimental education: [14] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? [15] E. Riloff, J. Wiebe, and T. Wilson. Learning [16] P. J. Stone, D. C. Dunphy, M. S. Smith, and D. M. [17] P. Turney. Thumbs up or thumbs down? Semantic [18] P. D. Turney and M. L. Littman. Measuring praise [19] T. Wilson, J. Wiebe, and R. Hwa. Just how mad are [20] H. Yu and V. Hatzivassiloglou. Towards answering
