 With the booming of social networks, such as Facebook and Flickr, we have witnessed a dramatical growth of multimedia data, i.e. image, text and video. Consequently, there are increasing dem ands to effectively organize and access these resources. Normally, feature vect ors, which are used to represent afore-mentioned resources, are usually very large. However, it has been pointed out in [1] that only a subset of features carry the most discriminating information. Hence, selecting the most representati ve features plays an essential role in a multi-media annotation framework. Previous works [2,3,4,5] have indicated that feature selection is able to remove redundant and irrelevant information in the feature representation, thus improves subsequent analysis tasks.

Existing feature selection algorithms are designed in various ways. For exam-features one by one. While dealing with multi-label problems, the conventional problems for each concept respectively. Hence, feature correlations and label correlations are ignored [7], which will deteriorate the subsequent annotation performance.

Another limitation is that they only use labeled training data for feature se-lection. Considering that there are a large number of unlabeled training data notation. Over recent years, semi-supe rvised learning has been widely studied training data [8,9,10]. Inspired by this motivation, feature learning algorithms based on semi-supervised framework, ha ve been also proposed to overcome the insufficiency of labeled training samples. For example, Zhao et al. propose an algorithm based on spectral analysis in [5]. However, similarly to Fisher Score [6], their method selects the most discri minating features one by one. Besides, correlations between labels are ignored.

Our semi-supervised feature selection algorithm integrates multi-label feature correlations are simultaneously mined.

The main contributions of this work can be summarized as follows: 1. We combine joint feature selection wit h sparsity and semi-supervised learn-3. Since the objective function is non-smooth and difficult to solve, we propose
The rest of this paper is organized as follows. In Section 2, we introduce 3. Finally, we conclude this paper in Section 4. is built upon a reasonable assumption that different class labels have some in-followed by an iterative algorithm with guaranteed convergence to optimize the objective function. 2.1 Formulation of Proposed Framework R (1  X  i  X  n )isthe i -th data point and n is the total number of training data. we assume that there is a low-dimensional subspace shared by different labels. be generalized as follows: projects features in the original space into a shared low-dimensional subspace. as { y following objective function: Note that to make the problem tractable we impose the constraint Q T Q = I . Following the methodology in [2], we incorporate (1) into (2) and obtain the objective function as follows:
By defining W = V + QP ,where V =[ v 1 ,v 2 ,  X  X  X  ,v c ]  X  R d  X  c and P = [ p subspace, we can rewrite the objective function as follows:
Note that we can implement the shared feature subspace uncovering in differ-ent ways by adopting different loss functions and regularizations. Least square By applying the least square loss function, the objective function arrives at:
As indicated in [12], however, there are two issues worthy of further consid-handle the two issues. We can rewrite the objective function as follows:
Meanwhile, we define a graph Laplacian as follows: First, we define an affinity as: k nearest neighbours in the original feature space. Then, the graph Laplacian, L , is computed according to L = S  X  A ,where S is a diagonal matrix with S
Note that multimedia data have been normally shown to process a manifold structure, we adopt manifold regularization to explore it. By applying mani-fold regularization to the aforementioned loss function, our objective function arrives at:
We define a selecting diagonal matrix U whose diagonal element U ii =  X  ,if x i is a labeled data, and U ii = 0 otherwise. To exploit both labeled and unlabeled we assume that F holds smoothness on both the ground truth of training data and on the manifold structure. Therefore, F can be obtained as follows: By incorporating (9) into (6), the objective function finally becomes:
As indicated in [13], W is guaranteed to be sparse to perform feature selection across all data points by W 2 , 1 in our regularization term. 2.2 Optimization closed form. We propose to solve this problem in the following steps. By setting the derivative of (10) w.r.t. P equal to zero, we have
By denoting W =[ w 1 ,...,w d ] T , the objective function becomes where D is a matrix with its diagonal elements D ii = 1 2 w i
Note that for any given matrix A ,wehave A 2 F = tr ( A T A ). Substituting P in (10) by (11), we can rewrite the objective function as follows:
According to the equation ( I  X  QQ T )( I  X  QQ T )=( I  X  QQ T ), we have: By the setting the derivative w.r.t. W to zero, we have: where M = XX T +  X D +  X I .

Then the objective function becomes: By setting the derivative w.r.t. F to zero, we have: Thus, we have where
Then, the objective function can be written as
According to the Sherman-Woodbury-Morrison matrix identity,
Thus, the objective function arrives at where Theorem 1. The global optimization Q  X  can be obtained by solving the following ratio trace maximization problem: where Proof. See Appendix.
 in complexity. However, as the solution of Q requires the input of D which is shown in Algorithm 1, we propose an iterative approach to solve this problem.
The proposed iterative approach in Algorithm 1 can be verified to converge to the optimal W by the following theorem. Following the work in [12], we can prove the convergence of Algorithm 1. In this section, experiments are conducted on three datasets, i.e. MIML [16], MIRFLICKR [17] and NUS-WIDE [18] to validate performance of the proposed algorithm. 3.1 Compared Methods To evaluate performances of the propo sed method, we compare it with the fol-lowing algorithms: 4. Spectral Feature Select ion [SPEC] [15]: It employs spectral regression to Algorithm 1. The algorithm for solving the objective function 5. Sub-Feature Uncovering with Sparsity [SFUS] [12]: It incorporates the lat-6. Semi-supervised Feature Selection v ia Spectral Analysis [sSelect] [5]: It is 3.2 Dataset Description Three datasets, i.e. , MIML [16] Mflickr [17] and NUS-WIDE [18] are used in the experiments. A brief description of the three datasets is given as follows.
MIML: This image dataset consists of 2 , 000 natural scene images. Each dataset belong to more than one class. On average, each image has 1 . 24 class labels.

MIRFLICKR: The MIRFLICKR image dataset consists of 25 000 images collected from Flickr.com. Each image is associated with 8 . 94 tags. We choose 33 annotated tags in the dataset as the ground truth.

NUS-WIDE: The NUS-WIDE image dataset has 269 , 000 real-world images which are collected from Flickr by Lab for Media Search in the National Univer-sity of Singapore. All the images have been downloaded from the website, among which 59 , 563 images are unlabeled. By removing unlabeled images, we use the remaining 209 , 347 images, along with ground-truth labels in the experiments. 3.3 Experimental Setup In the experiment, we randomly generat e a training set for each dataset con-sisting of n samples, among which m samples are labeled. The detailed settings are shown in Table 1. The remaining data are used as testing data. Similar to the pipeline in [2], we randomly split the training and testing data 5 times and report average results. The libSVM [19] with RBF kernel is applied in the ex-periment. The optimal parameters of the SVM are determined by grid search Except for the SVM parameters, the regularization parameters,  X  ,  X  and  X  ,in The number of selected features can be found in Table 1. 3.4 Performance Evaluation Tables 2, 3 and 4 present experimental results measured by MAP, MicroAUC and MacroAUC when using different numbers of labeled training data (5  X  c , 10  X  c and 15  X  c ) respectively.

Taking MAP as an example, it is observed that: 1) The proposed method is better than All-Fea which does not apply feature selection. Specifically, the proposed algorithm outperforms All-Fea by about 5.5% using 10  X  c labeled training data in the MIML dataset, which indicates that feature selection can contribute to annotation performance. 2) Our method has consistently better performances than the other supervised fe ature selection algorithms. When using 5  X  c labeled training data in the MIML dataset, the proposed algorithm is better than the second best supervised feature selection algorithm by 3.8%. 3) The proposed algorithm gets better performances than the compared semi-supervised feature selection algorithm, which demonstrates that mining label correlations is beneficial to multimedia annotation.
 3.5 Convergence Study c labeled training data in MIML dataset are tested in this experiment.  X  ,  X  and  X  are fixed at 1 which is the median value of the tuned range of the parameters.
Figure 1 shows the convergence curve of the proposed algorithm w.r.t. the objective function value in (10) on the MIML dataset. It is observed that the objective function values converge within 4 iterations.
 3.6 Influence of Selected Features In this section, an experiment is conducted to study how the number of selected features affect the performance of the proposed algorithm. Following the above experiment, we still use the same setting.

Figure 2 shows MAP varies w.r.t. the number of selected features. We can observe that: 1) When the number of select ed features is relatively small, MAP of annotation is quite small. 2) When th e number of selected features rises to 280, MAP increases from 0.269 to 0.284. 3) When we select 280 features, MAP arrives at the peak level. 4) MAP keeps stable when we increase the number benefits to the annotation performance. 3.7 Parameter Sensitivity Study Another experiment is conducted to test the sensitivity of parameters in (10). Among different parameter combinations, the proposed algorithm gains the best performance when  X  =10 1 ,  X  =10 4 and  X  =10 2 . We show the MAP variations algorithm changes corresponding to different parameters. In summary, better In this paper, we have proposed a novel framework for semi-supervised feature analysis by mining label correlation. First, our method simultaneously discov-ers correlations between labels in a shared low-dimensional subspace to improve the annotation performance. Second, to make the classifier robust for outliers, l into a semi-supervised scenario which exp loits both labeled and unlabeled data. We evaluate the performance of a multimedia annotation task on three differ-ent datasets. Experimental results have demonstrated that the proposed algo-rithm consistently outperforms the other compared algorithms on all the three datasets. In this appendix, we prove Theorem 1.
 To prove Theorem 1, we first give the following lemma and prove it. Lemma 1. With the same notations in the paper, we have the following equation: where Proof. Proof of Theorem 1 Proof. From Eq. (29), we can tell that N is independent from Q .Byemploying Lemma 1, the objective function arrives at: where K = I  X   X N  X  1 . At the same time, we have:
N Thus, K = I  X   X N  X  1 = C . According to the property of trace operation that tr ( UV )= tr ( VU ) for any arbitrary matrices U and V , the objective function can be rewritten as: The objective function is equivalent to: Acknowledgement. This work was partially supported by the Australian Re-search Council the Discovery Project DP No. 130104614 and DP No. 140100104. Any opinions, findings and conclusions or recommendations expressed in this ma-Australian Research Council.

