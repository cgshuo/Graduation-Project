 1. Introduction
Nowadays, industrial processes are more and more complex for that reason they include a lot of sensors. Consequently, an important amount of data can be obtained from a process. A process dealing with many variables can be named multivariate process. However, the monitoring of a multivariate process cannot be reduced to the monitoring of each process variable because the correlations between the variables have to be taken into account. Process monitoring is an essential task. The final goal of the process monitoring is to reduce variability, and so, to improve the quality of the product ( Montgomery, 1997 ). The process monitoring includes four procedures: fault detection (decide if the process is under normal condition or out-of-control); fault identification (identify the variables implicated in an observed out-of-control status); fault diagnosis (find the root cause of the disturbance); process recovery (return the process to a normal status).

There are three major kinds of approaches for process monitoring ( Chiang et al., 2001 ): data-driven, analytical and knowledge-based. Theoretically, the best method is the analytical one because this method constructs mathematic models of the process. However, obtaining detailed models for large systems (lots of inputs, outputs and states) is quite impossible. In the knowledge-based category, they are methods which are based on qualitative models like fault tree, failure mode, effects, and criticality analysis (FMECA), expert systems ( Venkatasubrama-nian et al., 2003 ). Finally, data-driven methods are techniques based on rigorous statistical development of process data. We are interested in monitoring large systems, and so, we are concerned with data-driven methods.

In the literature, we can find many different data-driven techniques for process control. A lot of methods have been submitted for the fault detection of industrial processes: univariate statistical process control (Shewhart charts) ( Shewhart, 1931; Montgomery, 1997 ), multivariate statistical process control ( T and Q charts) ( Hotelling, 1947; Westerhuis et al., 2000 ), and some principal component analysis (PCA) based techniques ( Jackson, 1985 ) like Multiway PCA or Moving PCA ( Bakshi, 1998 ). Kano et al. (2002) compared different techniques. Concerning the fault identification procedure, one of the better statistical techniques is the MYT (Mason Young Tracy) decomposition of the T 2 statistic ( Mason et al., 1997 ). Finally, for the fault diagnosis techniques we can name the book of Chiang et al. (2001) which presents a lot of them (PCA based techniques, Fisher discriminant analysis, partial least squares (PLS) based techniques, etc.).

The aim of this article is to present a new method for the diagnosis of faults in large industrial systems. This method is based on Bayesian networks and particularly Bayesian network classifiers. The major interest of this method is the combination of a discriminant analysis and distance rejections in a Bayesian network in order to detect new types of fault in the system.
The article is structured in the following manner. In Section 2, the classical method to diagnose faults with Bayesian network classifiers is introduced. Section 3 explains a method to apply a distance rejection on a fault of a system with a Bayesian network, and presents the combination of this distance rejection with the classical diagnosis with Bayesian network. Section 4 presents an application of the proposed method for the diagnosis of three types of fault on the benchmark Tennessee Eastman Problem.
Finally, we conclude on interests and limitations of this method, and some perspectives of the fault diagnosis with Bayesian network are presented. 2. Bayesian network for fault diagnosis 2.1. Fault diagnosis as classification task
Once a problem (fault) has been detected in the evolution of the process by the mean of a detection method, we need to identify (diagnosis) the belonging class of this fault. Thereby, the diagnosis problem can be viewed as the task to correctly classify this fault in one of the predefined fault classes. The classification task needs the construction of a classifier (a function allocating a class to the observations described by the variables of the system). Two types of classification exist: unsupervised classification whose objective is to identify the number and the composition of each class present in the data structure; supervised classification where the number of classes and the belonging class of each observation is known in a learning sample and whose objective is to class new observations to one of the existing classes. For example, given a learning sample of a bivariate system with three different known faults as illustrated in Fig. 1 , we can easily use supervised classification to classify a new faulty observation. A feature selection can be used in order to select only the most informative variables of the problem ( Verron et al., 2008a ). In this study, we will use the Bayesian network as a supervised classification tool. 2.2. Bayesian network
A Bayesian network (BN) ( Pearl, 1988 ) is a probabilistic graphical model where each variable is a node. Edges of the graph represent dependences between linked nodes. A formal definition of Bayesian network ( Jensen, 1996 ) is a couple { G , P }where: f G g is a directed acyclic graph, whose nodes are random variables
X  X  { X 1 , X 2 , y , X n } and whose missing edges represent condi-tional independences between the variables, f P g is a set of conditional probability distributions (one for each variable): P  X f p  X  X 1 j pa  X  X 1  X  X  , ... , p  X  X n j pa  X  X is a table defined by p  X  X i  X  x j i j pa  X  X i  X  X  with x i , x 2 i , ... , x
X i and n i is the number of these modalities. The joint probability should read like the following equation: p  X  x  X  X  with x  X  X  x j 1 1 , x j 2 2 , ... , x j n n  X  .
 Theoretically, variables X 1 , X 2 , y , X n can be discrete or contin-uous. However, in practice, for exact computation, only the discrete and the Gaussian case can be treated. Such a network is often called conditional Gaussian network (CGN). In this context, to ensure availability of exact computation methods, discrete variables are not allowed to have continuous parents (see Lauritzen and Jensen, 2001; Madsen, 2008 ).

In concrete terms, the conditional probability distribution is described for each node by his conditional probability table (CPT).
In a CGN, three cases of CPT can be found. The first one is for a discrete variable with discrete parents. For example, we take the case of two discrete variables A and B of respective dimensions a and b (with a 1 , a 2 , y , a a the different modalities of A , and b is represented in Table 1 .

As we can see, the utility of the CPT is to condense the information about the relations of B with his parents. We can denote that the dimension of this CPT (number of conditional probabilities) is a b . In general the dimension of the CPT of a discrete node (dimension a ) with p parents (discrete) Y 1 (dimension y 1 , y 2 , y , y p )is a Q p i  X  1 y i .

The second case of CPT is for a continuous variable with discrete parents. Assuming that B is a Gaussian variable, and that
A is a discrete parent of B with a modalities, the CPT of B can be represented as in the Table 2 where P  X  B j a 1  X  N  X  m a that B conditioned to A  X  a i follows a multivariate normal density function with parameters m a
The third case occurs when a continuous node B has a continuous parent A . In this case, we obtain a linear regression and we can write, for a fixed value a of A , that B follows a Gaussian sion coefficient. The three different cases of CPT enumerated can evidently be combined for different cases where a continuous variable has several discrete parents and several continuous (Gaussian) parents.
 The classical use of a Bayesian network (or conditional Gaussian network) is to enter evidence in the network (an evidence is the observation of the values of a set of variables). Therefore, the information given by the evidence is propagated in the network in order to update the knowledge and obtain a posteriori probabilities on the non-observed variables. This propagation mechanism is called inference. As its name suggests, in a Bayesian network, the inference is based on the Bayes rule. A lot of inference algorithms (exact or approximate) have been developed, but one of the more exploited is the junction tree algorithm ( Jensen et al., 1990 ).

In this article, we propose a method exploiting a CGN in order to diagnose a system, and particularly in the difficult case of simultaneous faults. 2.3. Conditional Gaussian network for fault diagnosis
In the context of the diagnosis of industrial systems, Bayesian networks and conditional Gaussian networks have been already used and they give convenient results compared to other classification tools like support vector machines, neural networks or k-nearest neighborhoods ( Pernkopf, 2005; Perzyk et al., 2005; the CGN have been previously demonstrated ( Verron et al., 2007a,c ), we choose this classifier in this article which is equivalent to a discriminant analysis (DA). Therefore, we name the class node DA (coding the different known faults of the system), and the observation node X (a normal multivariate node). Fig. 2 presents the CGN equivalent to a discriminant analysis, with the probability tables associated to each node. To simplify, the a priori probability of each class F i is fixed to p  X  F i  X  X  1 = k , where k is the number of known faults. The node X follows the different normal probability densities  X  N  X  conditionally to the class of DA , where l i is the mean vector of the fault F i , R i is the covariance matrix of the fault F i estimated on the fault database by maximum likelihood estimation (MLE) ( Duda et al., 2001 ). In the mere example of
As we previously mentioned, the objective of a fault diagnosis method is to classify new observations to one of the existing classes. However, in certain cases, the observation may be a new type of fault (unknown or unseen before). This is the case when the observation is distant of any known class of fault (example of the point A in Fig. 1 ). In order to detect these new types of fault, we have to use a criterion called distance rejection (see Denoeux et al., 1997 ). Then, we will see how to take into account this criterion in a CGN, and how to combine it with the diagnosis of the previous CGN. 3. Integration of distance rejection 3.1. Distance rejection in a CGN
Assuming one class of fault (Gaussian), the rejection of an observation is equivalent to a T 2 control chart ( Hotelling, 1947 )on if an observation is out-of-a-normal region. In that case, a statistical distance ( T 2 ) is computed from a given multivariate observation to the center of a normal multivariate area defined by his mean and his variance X  X ovariance matrix. If the T 2 value of the observation is beyond a fixed limit, the observation is declared out-of-control. In other words, we can say that statistically, we decide that this observation cannot be issued from the original area. In previous works ( Verron et al., 2007b, 2008b ), we have demonstrated that a T 2 control chart could be modelized with a Bayesian network, and more particularly with a CGN. Assuming l the mean vector and R i the covariance matrix of the fault F we can monitor the fault F i with the following rule: if p  X  F i  X  True j x  X  o p  X  F i  X  True  X  then the observation x cannot be attributed to the fault F i (distance rejection for the fault F CGN of Fig. 4 realize this rule.

In Fig. 4 , we can see that a coefficient c is implicated in the distance rejection. This coefficient is the root (non-equal to 1) of the following equation: 1 c  X  pc CL ln  X  c  X  X  0  X  2  X  where p is the dimension of the system, and CL is the control limit of the T 2 control chart. For a detailed demonstration of the Eq. (2), see Appendix A. In numerous cases, CL is equal to w 2 a , p to the a value of the w 2 distribution with p degree of freedom
X ( Montgomery, 1997 ). Consequently, a allows tuning the distance rejection: the higher is a , the stronger is the distance rejection. 3.2. Fault diagnosis with distance rejection in a Bayesian network
A discriminant analysis can be combined with the distance rejection notion in a CGN. Indeed, on one hand the probabilities associated with the different known faults (discriminant analysis) are computed, and on the other hand, we can know if a suspected observation can be attributed to none, one or several types of faults (distance rejection on each type of fault). Fig. 5 presents the
CGN proposed for the fault diagnosis with integration of distance rejection.

In the previous section, the different conditional probability tables of nodes X have been already detailed. We will study the other conditional probability tables implicated in Fig. 5 .
Firstly, we can notice that a node has been added: the node  X  X  D  X  X , as Diagnosis. This node represents the final decision concerning the suspected observation. This node has k +1 modalities one for each existing fault ( F i ) and one for a New type of Fault NF . The a priori probabilities table of this node is fixed in order to not advantage any modality. As we can remark in Table 3 , each modality has the same a priori probability: 1 =  X  k  X  1  X  . Each node F i has the conditional probabilities table of Table 4 .
We have set the different probabilities in order to respect the following rules: if D  X  F i , then it is certain that the observation is from fault F if D  X  NF , then it is certain that the observation is not from fault
F , if D  X  F j , then it is certain that the observation is not from fault
F .
 Table 5 presents the CPT of the node DA . We can see that the knowledge of a fault F i in the node D allows setting the knowledge of the node DA , expressed by P  X  DA  X  F i j D  X  F i  X  X  1. However, the knowledge on D of a new type of fault NF does not give information about the discrimination between the different each fault F i .

The interest of the Diagnosis node D is to add the different results of distance rejection to the result of the discriminant analysis. The application of the network in the Fig. 5 to the bivariate example gives Fig. 6 . This mere example clearly shows that the classification space has a new area corresponding to the space location of new type of fault ( NF ).

Now, we are going to study an application of this approach on a benchmark problem: the Tennessee Eastman Process ( Fig. 7 ). DA F 1
F 2 . . .
 Fk
X 4. Application to the TEP 4.1. Presentation of the TEP We have tested our approach on the Tennessee Eastman Process. The Tennessee Eastman Process (TEP) is a chemical process. It is not a real process but a simulation of a process that was created by the Eastman Chemical Company to provide a realistic industrial process in order to evaluate process control and monitoring methods. The article of Downs and Vogel (1993) entirely describes this process. The authors also give the Fortran code of the simulation of the process. Ricker (1996) has implemented the simulation on Matlab. The TEP is composed of five major operation units: a reactor, a condenser, a compressor, a stripper and a separator. Four gaseous reactants A, C, D, E and an inert one B are fed to the reactor where the liquid products F, G and H are formed. This process has 12 input variables and 41 output variables. The TEP has 20 types of identified faults. This process is ideal to test monitoring methods. However, it is also a benchmark problem for control techniques because it is open-loop unstable. A lot of articles present the TEP and test their approaches on it. For example, in fault detection, we can cite Kano et al. (2002) and Kruger et al. (2004) . Some fault diagnosis techniques have also been tested on the TEP ( Chiang et al., 2001, 2004; Kulkarni et al., 2005; Maurya et al., 2007 ) with the plant-wide control structure recommended in Lyman and Georgakis (1995) .In Chiang et al. (2004) and Kulkarni et al. (2005) , authors focus on only three types of fault and give the datasets they used. For this reason, we will take the same data from these articles and compare our approach to those of the others.

As we said, we have taken into account three types of faults: fault 4, 9 and 11 (see Table 6 ). These three types of fault are good representations of overlapping data and consequently are not easy to classify. As indicated in Table 6 , each type of fault is composed of two datasets: a training sample and a testing sample, containing respectively 480 and 800 observations. We precise that in the next part of this paper all computations have been made on Matlab with the BNT (BayesNet Toolbox) developed by Murphy (2001) . 4.2. Proposed approach without distance rejection
In the first part of this application, we have applied the proposed approach without the integration of the distance rejection. In the aim of objectively evaluating our procedure and comparing it with the results of other published methods (like Support Vector Machines), we have classified 2400 new observa-tions (800 of each type of fault) of the TEP. The results are given in Table 7 . In the Bayesian network (BN) approach, we compute the misclassification rate (percentage of observations which are not well classified). The results of other methods on the same data are also given. The results for the Fisher discriminant analysis (FDA), support vector machines (SVM), proximal support vector machines (PSVM) and independent support vector machines (ISVM) methods are extracted from Chiang et al. (2004) and Kulkarni et al. (2005) .

In Table 7 , we can observe that the BN approach outperforms all the other methods. The confusion matrix for the Bayesian network is given in Table 8 and gives us the possibility to see how the discrimination of the different faults is done. Each column of the matrix represents the instances in a predicted class, while each row represents the instances in an actual class. For example, for 800 tested observations of fault 4, the diagnosis procedure gives 141 observations as the fault 11, and 659 observations as the fault 4, so 17.62% (141/800) of misclassified observations for the fault 4. Considering the three faults, the misclassification rate is 18.87% (28+66+141+218/2400). 4.3. Integration of distance rejection
In this second part, the distance rejection is taken into account and we fix a to 0.001 for each known fault. The CGN has to classify 800 new observations of some characteristic faults of the TEP (namely fault 7, 8, 10, 12, 13, and 14) for a total of 800 6  X  4800 new observations (new types of fault). Table 9 represents the confusion matrix for this case where the label NF means New type of Fault.

Table 9 shows the results of the integration of the distance rejection in the Bayesian network. We can observe that the discrimination of the three known faults is not really affected by this integration. Indeed, the misclassification rate for this three faults is 19.62% (654+580+695/2400), instead of 18.87% pre-viously (without the distance rejection). Moreover, this table shows correctly the advantage of this approach since 4352 observations on 4800 have been correctly classified as a new type of fault NF (classification error rate of 9.33%). In this case, an unsupervised classification tool as the k-mean algorithm would be able to identify the different classes of these observations.
Finally, we can say that the proposed CGN allows detecting new types of fault (detection in more than 90% of the cases), without losing performances of discrimination between known faults (error rate of 18.87% has increased to 19.62%). Thus, this CGN can take into account distance rejection with minor loss of perfor-mances. 5. Conclusions and outlooks
The main interest of this article is the presentation of a new method for the fault diagnosis of industrial processes which uses a faults database to construct a Conditional Gaussian Network.
This CGN is able to discriminate between the different known faults of the system, but is also able to recognize some new types of fault on the system. The performances of this approach have been tested on a concrete example: the Tennessee Eastman
Process. The results of the method are good and outperform some previous results of other published methods.

The evident outlook of the proposed approach is the extension to the Gaussian models mixture (GMM) ( McLachlan and Basford, 1988 ) in order to take into account some non-normal classes.
Indeed, GMM can be easily modelized in a CGN for classification task. However, analytically, the distance rejection of a GMM is a problem and can be an interesting research field.
 Appendix A. Coefficient c demonstration This appendix presents the demonstration of Eq. (2).

As in the case of the T 2 control chart Montgomery (1997) ,we will fix a threshold (control limit CL for the control chart) on the a posteriori probabilities allowing to take decisions on the process: if, for a given observation x , the a posteriori probability to be allocated to F i ( P ( F i )), then this observation is allocated to F rule can be rewritten as: x A F i if P  X  F i j x  X  4 P  X  F x A
F is to define c in order to obtain the equivalence between the CGN and the multivariate T 2 control chart.

We want to keep the following decision rule x A
F i if T 2 o CL  X  3  X  with this decision rule: x A
F i if P  X  F i j x  X  4 P  X  F i  X  X  4  X  We develop the second decision rule
P  X  F j x  X  4 P  X  F i  X 
P  X  F j x  X  4  X  P  X  F i  X  X  X  P  X  F i j x  X  X  P  X  F i j x  X  X 
P  X  F j x  X  4 P  X  F i  X  P  X  F i j x  X  X  P  X  F i  X  P  X  F i j x  X 
P  X  F j x  X  P  X  F i  X  P  X  F i j x  X  4 P  X  F i  X  P  X  F i j x  X 
P  X  F j x  X  X  1 P  X  F i  X  X  4 P  X  F i  X  P  X  F i j x  X 
P  X  F j x  X  P  X  F i  X  4 P  X  F i  X  P  X  F i j x  X 
P  X  F j x  X  4 However, the Bayes law gives
P  X  F j x  X  X  and
P  X  F j x  X  X  As a consequence, we obtain
P  X  F  X  P  X  x j F i  X  P  X  x  X  4
P  X  F  X 
P  X  F  X 
P  X  x j F  X  4 P  X  x j F i  X  X  7  X 
In the case of a discriminant analysis with k classes C i conditional probabilities are computed with Eq. (8), where f represents the probability density function of the multivariate Gaussian distribution of the class: P  X  x j C i  X  X  f  X  x j C i  X  P k Eq. (7) can be written as f  X  x j F i  X  4 f  X  x j F i  X  X  9  X  We recall that the probability density function of a multivariate Gaussian distribution of dimension p , of parameters l and R ,ofan observation x is given by If the law parameters are l and c R , then the density function becomes In identifying the expression  X  x l  X  T R 1  X  x l  X  as the T observation x , we can write f  X  x j F i  X  4 f  X  x j F i  X   X  2 e 2 4 T T 2 o p ln  X  c  X  However, we search the value(s) of c allowing the equivalence the following equation for c: p ln  X  c  X  1 1 c or, equivalently 1 c  X  pc LC ln  X  c  X  X  0  X  14  X  References
