 In recent years, learning BNs from data has become an attractive and active research issue in fields of machine learning and dat a mining. At present, there has been many successful algorithms used for learning BN st ructures from complete data, which in-clude search &amp; scoring based algorithms such as Bayesian method[1], Minimal De-scription Length (MDL) based method [2], dependency analysis based algorithms such as SGS[3], PC[3], TPDA[4], SLA[4] as well as the combination of the two kinds of algorithms mentioned above[5].

But when the training data is incomplete (i.e. containing missing values), BN learn-ing becomes much more difficult. Now there has been some advances in learning BNs from incomplete data. One earlier research on BNs learning from incomplete data was made by Chickering[1], which deals with missing data by Gibbs sampling method. The most significant advance was the SEM algorithm[6]. SEM algorithm turns the learn-ing problem with incomplete data into the easy solving learning problem with complete data using EM algorithm. Another research was made by W. Myers et al. They complete the incomplete data using generic operations, and evolve the network structures and missing data at the same time[7]. Using the idea of Friedman and Myers for reference, we put forward EM-EA algorithm[8], which completes data using EM algorithm and evolves BNs using an evolutionary algorithm (EA). And we also presented a method, named EMI, for estimating (conditional) Mutual Information from incomplete data and described an Extended TPDA (E-TPDA) algorithm which could learn BN structures from incomplete data[9].

Although E-TPDA is more efficient than SEM and EM-EA algorithms, it needs to calculate conditional mutual information, wh ich is not reliable when the dataset is not big enough. Furthermore, calculating conditiona l mutual informatio n and finding small d-separation set in E-TPDA algorithm also lead to high computation cost. In this paper, we present a scoring function based on mutual information. To decrease computational complexity, we introduce Max-Relevance and Min-Redundancy (MRMR) criterion into the scoring function. When the dataset is incomplete, we use EMI method to estimate the Mutual Information (MI) from the incomplete dataset. As for whether a node order-ing is manually given or not, we develop two versions of algorithms, named as MRMR-E1 and MRMR-E2 respectively. Finally, through experiments on Alarm network, we compare MRMR-E1 with K2 and E-TPDA algorithms given a node ordering and com-pare MRMR-E2 with CB and E-TPDA algorithms without given a node ordering. Suppose that P is the underlying true distribution over variables X = { X 1 ,...,X n } and Q is a distribution over these variables defined by some Bayesian network model, then the K-L cross entropy between P and Q , C KL ( P, Q ) , is a distance measure of how close Q is to P and is defined by the following equation: The goal of BN learning is to search for the BN model with minimal C KL ( P, Q ) [2]. And also in [2], Lam and Bacchus pr oved the following theorem: Theorem 1. The K-L cross entropy C KL ( P, Q ) is a monotonically decreasing function of n i =1 I ( X i ,  X  i ) . Hence, it will be minimized if and only if the sum is maximized. measures the mutual information of a local structure, X i and its parents  X  i ,andis defined by the following equation: Where x i and  X  i respectively represent the value of X i and the instantiation of  X  i . According to the above theorem, we can use n i =1 I ( X i ,  X  i ) as a scoring function of BN models and search for the BN model with maximal value of n i =1 I ( X i ,  X  i ) . Given the node ordering that means all the parents of any variable can only occur to the left of variable in the ordering, maximizing each I ( X i ,  X  i ) is equivalent to maximizing of a BN, and BN learning can be implemente d by determining a set of parents for each variable on condition that the mutual information between the variable and its parents gets to the maximum. Unfortunately, despite the theoretical computability of the local score, the computational cost is very high because of the difficulty in getting an accurate estimation for multivariate probability distribution P ( x i , X  i ) .

Peng et al called it Max Dependency criterion maximizing the mutual information between a variable and a variable set in their study of feature selection and proposed an alternative criterion for selecting features, named Max-Relevance and Min-Redundancy (MRMR) criterion[10], which can be formalized as follows: Where F represents the feature set, and C is the goal variable. Suppose already having F m  X  1 , the feature set with m X  X  F
This type of feature selection that adds one feature at one time is called the  X  X irst-order X  incremental search, which can be used to find the near-optim al features defined by  X  ( . ) . Peng et al proved the following theorem[10]: Theorem 2. For the first-order incremental sea rch, MRMR is equivalent to Max De-pendency.
 According to the above theorem, we can use first-order incremental style of MRMR criterion to find all the local structures of a BN, which is equivalent to the local scor-ing function defined by Eq. (2). When there exists missing values in dataset, we use EMI method to estimate each I ( X j ,X i ) , the details of EMI method can be found in literature [9]. 3.1 Learning Procedure of MRMR-E1 Given a node ordering, we use the idea of K2 algorithm for reference and develop MRMR-E1 algorithm. MRMR-E1 adds incrementally those nodes among the prede-cessors in the ordering to the parent set of a node according to MRMR criterion and stops the adding process when no additional single parent can satisfy MRMR criterion. The details of MRMR-E1 algorithm is described as follows:
In the above procedure,  X  c i is a set of the parent candidates of X i ; MI [ n, n ] is a matrix storing mutual information; u i denotes the max number of parents of X i .They can be calculated by the following procedure:
Where u max is the max number of parents of all the nodes;  X  is the threshold of the mutual information, which can be set as a small positive number in reality. I ( x i ,x j ) is estimated mutual information, which can be calculated by EMI method[9]. 3.2 Learning Procedure of MRMR-E2 To avoid the overdependence of the node ordering in K2 algorithm, Singh and Valtorta presented an algorithm, called CB[11], which can use CI tests to generate a node or-dering. CB algorithm basically consists of two phases: Phase I uses CI tests to generate an undirected graph, and then orients the edges to get an ordering on the nodes; Phase II takes as input a total ordering consistent with the DAG generated by phase I, and applies the K2 algorithm to construct the network structure.

Using the Phase I of CB algorithm, we extend MRMR-E1 and present another ver-sion of algorithm, namely MRMR-E2. MRMR-E2 use the total ordering generated in Phase I of CB algorithm as the input of MRMR-E1 algorithm. The procedure of MRMR-E2 algorithm is as follows.
 In the above procedure, ord denotes the order of CI relations being tested, and each REPEAT circulation tests each order of CI relations, first for 0 th order CI relations, then for 1 st order CI relations, and so on until the score of the learned network does not increase any more. g ( X i ,  X  i ) is a scoring function for local structures of a BN, which can be given by the following equation when a uniform priori is adopted[12]:
In the above equation, q i is the number of particular instantiations of  X  i and r i is the number of values that variable X i can take; N ijk represents the times of the occurrence For evaluating our algorithms, we compare MRMR-E1 with K2 and E-TPDA algo-rithms given a node ordering and compare MRMR-E2 with CB and E-TPDA algorithms without given a node ordering. Because K2 and CB algorithms can X  X  learn BNs from incomplete datasets, they simply delete the cases with missing values in datasets and learn BNs from relative small and complete datasets in reality. We do this is to show the effects of the cases with missing values to the accuracy of learned BNs. We implement K2, CB, E-TPDA and our algorithms in a Java based system. All the experiments were conducted on a Pentium 3.2 GHz PC with 1G MB of RAM running under Windows 2003. The experimental outcome is the average of the results run 5 times for each level of missing data. 4.1 Experimental Results Given a Node Ordering The given total ordering on the 37 nodes is consistent with the partial order of the nodes as specified by Alarm network. The threshold of Mutual Information in E-TPDA and MRMR-E1 algorithms is set as 0.01.

Table 1 and Table 2 show respectively t he learning accuracy and the running time (Seconds) of K2, E-TPDA and MRMR-E1 given a node ordering. The  X  X +B X  in Table 1 represents there are A extra arcs and B missing arcs in the learned networks compared with the true network. We treat a reversed a rc as if there are a missing arc and an extra arc. From this table, we can see that the accuracy of K2 algorithm degrades very sharply with the increase of missing data, which states that simply deleting the data records with missing entries will lose large amount of useful information. And also the learned networks by MRMR-E1 are more accurate than that by E-TPDA algorithm for 10,000 and 20,000 cases and equal accurate to that by E-TPDA algorithm for 40,000 cases. That MRMR-E1 outperforms E-TPDA for 10,000 and 20,000 cases is because that the calculation of conditional mutual information in E-TPDA algorithm is not reliable for relative small datasets.

From Table 2, the running time of K2 algorithm decreases very quickly with the increase of missing data because K2 runs on ve ry small datasets after deleting the data records with missing entries. While fixing the percentage of missing data, the running time of E-TPDA and MRMR-E1algorithms is roughly linear to the size of the data samples. Nevertheless, MRMR-E1 is more efficient than E-TPDA, whose running time is about half of that of E-TPDA algorithm. Th is also proves that calculating conditional mutual information and finding small d-s eparation set in E-TPDA algorithm lead to high computation cost. 4.2 Experimental Results Without Given a Node Ordering In our experiments, CB algorithm also uses the partial order and a bound of 15 on the maximal degree of the undirected gr aph just as in literature[11].
 Table 3 and Table 4 show respectively t he learning accuracy and running time of CB, E-TPDA and MRMR-E2 without given a node ordering. The experimental results in Table 3 and Table 4 are similar respectively to that in Table 1 and Table 2, which also show that MRMR-E2 is advantageous over E-TPDA algorithm in terms of accuracy and efficiency without given a node ordering. Compared with that in Table 1, the accuracy showed in Table 3 decreases to some extent and the running time in Table 4 increases to some extent compared with that in Table 2. This is because, without given a node ordering, the three algorithms have to compute the partial order between variables or infer the orientation of an arc which d ecreases their accuracy and efficiency. In this paper, we present a scoring function evaluating BN structures based on mutual information and introduce MRMR criterion into the scoring function to decrease com-putational complexity. When the dataset is incomplete, we use EMI method to estimate the Mutual Information (MI) from the incomplete dataset. As for whether a node order-ing is manually given or not, we develop two versions of algorithms, named as MRMR-E1 and MRMR-E2 respectively. The experime ntal results on Alarm network show that MRMR-E1 and MRMR-E2 algorithms are advantageous over E-TPDA in terms of ac-curacy and efficiency whether given a node ordering or not. The experimental results also state that making fully use of data records with missing entries will significantly improve the accuracy of learned BNs.
 This work is supported by NSF of China under grant NO. 60503017 and 60673089 as well as Science Foundation of Beijing Jiaot ong University under Grant No. 2005SM012.
