 Lawrence McAfee lcmcafee@stanford.edu Kunle Olukotun kunle@stanford.edu Stanford University, 450 Serra Mall, Stanford, CA 94305 Neural networks (NN) have gained much renewed in-terest in recent years, as they have been shown to out-perform many application-specific machine learning al-gorithms across several domains (Bengio, 2009). Given their potential promise for helping to move the field of machine learning towards true artificial intelligence, recent research trends have shown researchers X  eager-ness to test NNs on larger datasets (Raina, 2009; Cai et al., 2011). However, due to the core linear algebra routines that compose most applications, NNs are be-coming increasingly limited by the amount of available computational power. In cases where large datasets are desired, researchers typically resort to structurally sparse networks, which commonly refers to networks with either dense local receptive fields (Bengio &amp; Le-cun, 2007) or non-dense receptive fields (Coates &amp; Ng, 2011). However, in order to make larger scale networks run efficiently, researchers often find themselves need-ing to have expert systems knowledge to build their applications.
 To the benefit of algorithm efficiency, available compu-tational power increases each year. This benefits many applications in general, but the relative efficiency in-crease many applications see is nowhere near as fast as the pace of hardware advancement. This is due to the fact that for most applications, programmers utilize general purpose compilers to perform much of the opti-mization work such that the programmer can continue to focus on the higher-level issues that their domain requires. However, general purpose compilers (e.g., GCC) are not capable of fully optimizing specialized application domains. Some general purpose platforms are more specialized to certain domains, such as MAT-LAB for linear algebra-based development, and are better suited for many routines that are used to com-pose NNs. However, as NN applications get more com-plex, even a platform such as MATLAB is no longer optimal due to a lack of domain specific knowledge about the underlying data structures.
 Within the domain of NNs, one piece of domain spe-cific knowledge that can be used to increase efficiency is knowing that most network architectures  X  where the architecture is defined by the choices for layer sizes, mini-batch size, and interlayer connectivity  X  do not change during training. This means that the data structures used to store the network architecture are capable of being statically optimized, and then gener-ated code can be made to run the specific architecture as efficiently as possible.
 In this paper, we present SONNC (pronounced  X  X onic X ), a Statically Optimizing Neural Network Compiler 1 . The main contributions of this paper are:  X  SONNC, a neural network compiler that focuses on statically analyzing and optimizing NNs, and gener-ates efficient parallel C++ code.  X  We demonstrate analyses and optimizations which use NN domain-specific knowledge.  X  We demonstrate the conciseness of code utilizing SONNC by using SONNC X  X  front-end interface to MATLAB.  X  We show that SONNC without any explicit perfor-mance tuning, outperforms hand-optimized C++ code by 3.3X X 7.8X, and MATLAB code by 9.2X X 24X. Several machine learning (ML) development platforms have been introduced recently to help with scaling to larger applications. A few popular platforms are Op-tiML (Sujeeth et al., 2011), Theano (Bergstra et al., 2010), and GraphLab (Low et al., 2010). OptiML is a domain-specific language for ML built on top of the heterogeneous computing platform Delite (Chafi et al., 2011). It provides abstractions to allow a pro-grammer to develop ML applications, while Delite im-plicitly takes care of parallelizing and running the ap-plication across multiple CPUs and GPUs. Theano is a compiler for symbolic mathematical expressions. Although meant to be a general symbolic compiler, Theano is designed to handle ML applications. Pro-grammers develop their applications using Python and Numpy data types, and Theano implicity generates C++ and CUDA. GraphLab provides a parallel ab-straction similar to MapReduce for running large scale machine learning applications on a cluster.
 Similar to SONNC, each of these platforms provides useful abstractions for programming large scale ma-chine learning algorithms. These platforms are de-signed to optimize ML applications in general by us-ing optimized routines and data structures. Unlike SONNC, however, none of these other platforms focus on statically optimizing NN data structures. Neural networks are a quickly growing field within ML, and many NN applications are composed of very computa-tionally expensive operations. SONNC aims to provide these additional abstractions and static optimizations to allow NNs to run more efficiently. SONNC makes it easier for an end user to continue scaling applications without considering the complex-ities of tuning high performance code. As an example, if a user wants to design a network that uses struc-turally sparse connectivity  X  either locally dense or unstructured sparsity  X  a great deal of development effort would need to go into developing a sparse data structure that is efficient for indexing and updating the nonzero values in the weight matrix. When using SONNC, however, the user only needs to define the network X  X  connectivity at the beginning of her code, and the rest of the code remains unaffected by the un-derlying data structure. This way, the programmer only needs to focus on algorithmic intent rather than worry about the details of implementation .
 In addition to optimizing for data structures of the NN application, SONNC also optimizes the algorithm X  X  op-erations by transforming and condensing sequences of routines into more efficient routines. NNs typically have a very straighforward data flow, with minimal high-level control structures. This makes it possible to perform alterations on the execution graph to make the algorithm more efficient by improving caching and reducing overhead. 3.1. Compiler Stages The following sections briefly overview each stage of the compiler, which include building an execution graph, analyzing and optimizing the data structures and operations, and then generating efficient parallel code. 3.1.1. Building an Execution Graph SONNC is a standalone compiler, rather than a new programming language. As such, supported data types and operations must be embedded into an existing lan-guage to allow a user to use the system in a natural way. (See Section 5.1 for a description of the currently available data types in MATLAB.) Once an algorithm is written, an additional compilation function must be called to let SONNC perform its optimizations. SONNC supports two high-level matrix data types: a dense matrix type and a sparse matrix type. A dense matrix is used to denote a variable where any element can contain a nonzero value. A sparse matrix, how-ever, denotes a variable whose sparsity structure does not change after initialization. In practice, the sparse type is typically only used for weight data in a NN. In addition to these two matrix types, vector and scalar types are supported. All of the standard linear algebra operations between matrices and vectors are supported that are commonly used in NN algorithms, including multiplication, elemental (e.g., dot) operations, norms, and non-linearity operations.
 When SONNC X  X  data types are connected together via the supported operations, an execution graph of the NN application is implicitly built. This graph contains the flow of operations necessary to compute the nodes at the output (i.e., weight and bias updates) from the nodes at the input (i.e., the training set). 3.1.2. Analysis and Optimization Static graph optimizations . SONNC performs sev-eral common static compiler optimizations, including dead code elimination, operation re-writing, subex-pression elimination, and method fusion. An example of subexpression elimination is the pre-computing of all-constant-input operations. For example in the it-erative shrinkage thresholding algorithm (ISTA), the algorithm repeatedly runs the update expression: The only variable being updated in this expression is Z , the approximation to the sparse codes. Hence, to speed up the update expression, we can expand out the expression and precompute W T W and XW such that time is not wasted repeatedly computing these constant values.
 Method fusion is an important optimization for at-taining high performance. The compiler scans the ex-ecution graph for recognized operation sequences, and replaces them with more concise and efficient opera-tions that typically have better caching and less over-head. This is a place where having domain specific knowledge becomes very useful; there are many oper-ation sequences that are shared between various NN algorithms. For example, restricted Boltzmann ma-chines, autoencoders, and backpropogation networks all share an operation sequence of matrix multiplica-tion followed by bias addition followed by a nonlinear-ity. SONNC would recognize this sequence and con-vert it into its own internal operation, which in the case of a sigmoid nonlinearity would be called Mult-BiasSigm . Since the bias and nonlinearity operations must be applied to each element of the preceding data matrix, significant savings can be made if these oper-ations can be performed while the data is still in the CPU cache immediately following the matrix multipli-cation. SONNC contains several operation sequences that it can recognize and replace with more efficient routines.
 Data structure optimization . Since the NN archi-tecture does not change during training, we can pa-rameterize the underlying data structures such that the generated code is optimized to run as efficiently as possible for the specific network architecture. The number of threads is also chosen during this stage of the compiler. The entire data structure optimization process will be described in greater detail in Section 4. 3.1.3. Multithreading and Code Generation Once the number of threads is chosen in the previ-ous stage, the graph is expanded into a multithreaded graph where each node represents an operation per-formed by a single thread. Thread synchronization points are determined during this phase, and this is the last internal representation of the application be-fore code generation. C++ code is then generated to perform the NN application. Data structure optimization has the single biggest im-pact on performance in comparison to the other opti-mizations that SONNC performs. A network X  X  archi-tecture, again, is defined by choices for the layer sizes, mini-batch size, and interlayer connectivity. During this stage, the underlying data structures are parame-terized to run efficiently for the specific application. The number of threads is also chosen during this stage of optimization. Although not directly a parameter that affects the underlying data structures, the num-ber of threads must be chosen jointly with the matrix blocking size (described below) in order to yield good parallel performance. Choosing the right number of threads can have a large impact on performance. The optimal number of threads varies significantly based on matrix dimensions and connectivity structures. For example, even with the same matrix dimensions, the optimal number of threads between a network that uses dense local receptive fields and a network that uses non-dense local receptive fields can vary by a fac-tor of two or four. 4.1. Underlying Data Structure Although SONNC contains the two high-level matrix data types described in Section 3.1.1 (i.e., a dense type and a sparse type), the system contains several under-lying data structures including a dense structure, lo-cally dense sparse structure, a few general sparse struc-tures, and a hybrid sparse structure. Each of these underlying structures are appropriate for different cir-cumstances, and the compiler chooses which to use for each matrix within a target application. The choice of an underlying data structure is not always intuitive. For example, when a user defines a network with dense local receptive fields, a logical choice for the underlying data structure might be to use a locally dense sparse data structure, which stores information on the loca-tions of rectangular dense blocks within a sparse ma-trix. In many cases, this is the best data structure to use for the application. But this is only true when the receptive field dimension is large enough. When the receptive field is small (e.g., less than about 5 x 5), the overhead of performing small dense matrix multi-plications actually increases above the simpler general sparse structure. With small enough receptive fields, the general sparse structure can outperform the locally dense structure by 1.5X X 2X. 4.2. Data Structure Parameterization In addition to choosing the correct underlying data structure, each of these data structures is parameteri-zable, effectively making a wide range of different un-derlying structures to choose from. The two most im-portant of these parameters are the matrix blocking size and the data layout in memory. These parame-ters apply to both dense and sparse data structures. The blocking size determines how the matrix X  X  data is partitioned in memory by splitting up the matrix into separate square blocks. Smaller block sizes increase concurrency, but also increase overhead in reading and writing the matrix data. The data layout parameter sets whether matrix elements are stored in memory us-ing row-major order, column-major order, or another format. Both the blocking size and data format sig-nificantly impact cache reuse. While the block size is typically set globally for all matrices, the data format is set individually for each variable and depends heav-ily on the operations and neighboring variables (in the execution graph) that directly interact with a variable. One important point to note is that the compiler must have knowledge of the L1 and L2 cache sizes in order to properly set the blocking size. In SONNC X  X  cur-rent implementation, it implicitly discovers these val-ues during installation, which is described in the next section. 4.3. Joint Parameter Selection SONNC X  X  ability to properly choose the underlying data structure, matrix parameterization, and num-ber of threads represent the most important aspect of SONNC as a statically optimizing NN compiler. Properly tuning these parameters can give up to two orders of magnitude difference in performance. The joint impact of these parameters is non-linear, and so the heuristics used to optimize this stage are critical to getting good performance. To perform this tun-ing process, SONNC initially must run several timing tests during its installation in order to calibrate to the CPU. SONNC times matrix multiplications for several matrix dimensions, block sizes, and number of threads in order to create a large lookup table. To keep this lookup table from being too large, parameter values are swept over exponentially, and matrix dimensions are only tested up to 10,000, block sizes up to 1,000, and number of threads up to 32. For applications with matrix dimensions larger than this, timing becomes more easily predictable from the lookup table. The timing values in this table are generally nonlinear due to caching. They are also nonconvex as a function of the number of threads. Currently, SONNC uses linear interpolation between data points in order to choose parameter values for a specific application.
 This tuning process is also an ongoing area of active research for the compiler. Future plans for the tuning process include training a deep learning algorithm on the parameter space in order to better learn the non-linearities. For the current implementation, however, linear interpolation has shown to work very well when using power-of-2 spacing when creating the lookup ta-ble.
 One other important point to note is that this parame-ter selection operation is very fast. Using the parame-ter values mentioned previously during the installation phase, SONNC builds a lookup table that is stored as a 160MB file which is loaded into memory during each use. When a new application is being optimized, SONNC simply interpolates the neighboring matrix settings from the lookup table to set the block size and thread count. Since this operation only includes linear array scanning and vector averaging, it required around 2-2.5 seconds to perform parameter selection. While more sophisticated and computationally expen-sive methods were tested, linear interpolation worked well in practice. One of SONNC X  X  goals is to make it easier to write concise and expressive code, while attaining the per-formance of optimized C++ code. This way, program-mers can focus primarily on the algorithmic intent of their applications. However, this is often not possible with nontrivial data structures, such as when network architectures contain sparse interlayer connectivities. For example, if a LRF network is being defined, a programmer has a choice to use either a custom data structure or MATLAB X  X  sparse structure. Unfortu-nately, either option would require additional hand-coded routines for efficient random indexing. This in turn increases code complexity significantly. When using SONNC, however, the only difference be-tween whether a user would like to use dense, locally dense, or unstructured sparse connectivity is a mat-ter of how the matrix is initialized. The remainder of the network X  X  algorithmic description would be data structure independent. The following section gives an example of how the SONNC compiler could be used in practice. 5.1. MATLAB-Embedded Data Types and Although SONNC X  X  main contribution is its power-ful static optimization routines, a front-end interface for MATLAB is provided to allow end users to easily integrate the SONNC back-end into existing applica-tions. This should in many cases automatically lead to more concise code and much higher performance for NN applications. SONNC embeds four data types into MATLAB: a Vector type, a Scalar type, a DenseMa-trix type, and a SparseMatrix type.
 SONNC also overloads many common symbolic oper-ators and other methods in MATLAB such that code can be written using standard MATLAB syntax. Once a user has declared her variables using SONNC data types, much of the remainder of her code should be identical to as it would be otherwise in MATLAB. The main difference is that the body of the NN convergence loop is separated from declaration of the convergence loop construct. The body of the loop is written first, followed by a declaration of the convergence loop with its stopping criterion. 5.2. Example Code Algorithm 1 shows an example use of SONNC data types inside a MATLAB script that implements a sin-gle layer LRF backpropogation network. This exam-ple highlights the use of the four embedded data types (e.g., DenseMatrix SparseMatrix , Vector , and Scalar ), one control structure ( untilConverged ), and one other method, runNN , used to compile and run the appli-cation. This example demonstrates the SparseMatrix constructor being initialized with a dense Matlab ma-trix structure (the LRFs are stored inside a mostly zero  X  X ense X  matrix). SparseMatrix can additionally be ini-tialized with either MATLAB X  X  sparse data structure, or a cell array that contains information of the loca-tions and data of submatrices within a larger sparse matrix, which is useful for locally dense sparse vari-ables. untilConverged specifies the convergence stopping cri-terion. The convergence loop iterates until the normal-ized difference between successive values of the Scalar type cost falls below the specified tolerance. The out-put of untilConverged is a data type that simply com-bines the information of the looping structure and the execution graph, and is used as the input to the com-piler. The code is then compiled and executed using the runNN method.
 As can be observed in this example code, the NN X  X  routines are not dependent on the weight matrix data structure. SONNC makes it simple to initialize data structures as desired, without needing to worry about tuning code for performance. This section presents performance results for a set of NN applications written in MATLAB using SONNC data types. We compare these results to hand-optimized reference implementations written using both MATLAB and C++ code. In addition, we ana-lyze the performance improvements achievable due to SONNC X  X  static optimizations that were overviewed in Section 3.1. 6.1. Methodology We compare the performance results for three differ-ent NN applications: the restricted Boltzmann ma-chine (RBM), the autoencoder (AE), and the iterative shrinkage thresholding algorithm (ISTA). For each of these algorithms, we use two different sparsity pat-terns: local receptive fields (LRF) and unstructured sparsity. These experiments were run on a machine containing two quad-core Intel Xeon X5550 2.67GHz processors and 24GB of RAM. The version of MAT-LAB used is R2011b 7.13. The SONNC applications are algorithmically identical to the hand-optimized MATLAB and C++ implementations. For the hand-optimized versions, we made a reasonable effort to write efficient sparse routines. To implement LRFs by hand in both MATLAB and C++, we use an array-based structure where each entry contains a dense sub-matrix and the index of its upper left corner. For un-structured sparsity, we use MATLAB X  X  builtin sparse data structure for comparison. In C++ we use the compressed sparse block format (Buluc et al., 2009), which has several parallelization benefits. To paral-lelize the C++ code, we divide the work up evenly Algorithm 1 LRF Backprop Net (SONNC-based) over the number of threads in the processor.
 In the following discussion, the SONNC-based code, hand-optimized MATLAB, and hand-optimized C++ code will be simply referred to as SONNC, MATLAB, and C++ code, respectively.
 One important point to note for using MATLAB X  X  sparse data structure is that writing to only the nonzero elements as the result of a matrix multipli-cation is an inefficient process. This significantly im-pacts performance for AEs and RBMs for the unstruc-tured sparsity experiments. While these results are in-cluded for completeness, a more fair comparison is to the C++ implementation in this case.
 Timing was only performed between the lines of code immediately before and after the convergence loop, so as not to be affected by initialization procedures. Each application was run 10 times using 100 iterations of the convergence loop in order to smooth out any fluctua-tions due to caching and other variables. We present here the averaged time of the last five executions. 6.2. Performance Comparison Figures 1 X 4 show the performance comparison between the SONNC, MATLAB, and C++ implementations. The reported speedup is relative to the hand-optimized version in each case. In each experiment, the SONNC code runs significantly faster than either of the hand-coded implementations.
 SONNC shows the most benefit for the AE and RBM with unstructured sparsity, attaining over 200X speedup over MATLAB in some cases (Figure 1). This is because, as mentioned above, updating the nonzeros of the MATLAB sparse structure is an inefficient pro-cess. In the other tests, SONNC yields around 9X X 24X speedup over MATLAB. In contrast to the AE and RBM, ISTA obtains relatively modest speedup (about 10X) over MATLAB when using unstructured spar-sity because in ISTA the weight matrix never needs to be updated. SONNC also performs better than the C++ code, generally yielding around 4.2X X 7.8X speedup (Figure 2). The important thing to note here is that optimizations performed by SONNC  X  i.e., us-ing knowledge of the sparsity structure  X  allow it to outperform C++ code that is optimized primarily for load balance.
 When using LRFs (Figures 3 X 4), however, the MAT-LAB implementation is able to run much more effi-ciently for the weight updates than when using un-structured sparsity. When using locally dense spar-sity, MATLAB is still able to utilize its underlying BLAS imiplementation to perform the matrix multi-plications. Even when using LRF sparsity, however, SONNC is still able to yield 15X-24X speedup over MATLAB, and 3.3X X 6.1X speedup over C++.
 Additionally, as detailed in Section 5, SONNC is able to yield these levels of performance with much more succint code. If the user ever wants to switch their net-work between a dense, LRF, or unstructured sparse in-terlayer connectivity, it is just a matter of changing the matrix X  X  initialization, and all the performance bene-fits will automatically be available due to the implicit compiler optimizations.
 6.3. Impact of Optimizations Figures 5 and 6 present the impact of two of the more important optimizations described in Section 3.1. Fig-ure 5 shows the impact of proper matrix parameter-ization, which includes choosing the best underlying data structure format and number of threads. Tuning these parameters has the most impact of any optimiza-tion stage. This figure demonstrates the nonlinearity of jointly tuning the matrix blocking size and number of threads. For any given block size, there is typically a single optimal setting for the number of threads. How-ever, each block size has a different optimal setting for the number of threads, since smaller block sizes can utilize more threads. But smaller block sizes also have increasingly more overhead. This figure shows that, in this case, selecting the correct combination of these parameters has a 1.5X impact on performance for the two different block sizes.
 Figure 6 shows the impact of method fusion. SONNC has a large number of common and NN-specific oper-ation sequences that it can recognize and replace with more efficient routines. The new routines typically op-timize cache reuse by combining adjacent operations into the same loop. An example of this is the MultBi-asSigm method described in Section 3.1.2. This figure shows that method fusion is able to attain a 1.9X to 2.6X speedup for the tested algorithms. Less speedup is attained for ISTA, which is algorithmically simpler than the AE or RBM, and so has fewer operation se-quences that can be fused. Many promising neural network learning algorithms are facing computational challenges as they scale to larger datasets. Although the available computational power increases each year, the pace of neural net-work algorithmic efficiency does not advance as quickly due to the use of general purpose compilers that NN programmers rely on to optimize their applications. As NNs are applied to larger datasets, and algorithm complexity increases, application efficiency is becom-ing critical in order to continue advancing the field of research. In this paper, we presented SONNC, a compiler that performs static optimization of a NN application in order to generate high performance par-allel code. In addition to standard compiler optimiza-tions, SONNC relies on the domain specific knowledge that NN architecture does not change during training, which allows the compiler to optimize the underlying data structures used to store the network X  X  architec-ture. We showed that SONNC was able to outperform MATLAB implementations by 9X X 24X, and C++ im-plementations by 3.3X X 7.8X. Additionally, we demon-strated how programmer productivity can be increased when using SONNC. SONNC abstracts the underlying data structure, which reduces code complexity, but still allows algorithms to attain performance better than optimized C++ code.
 Bengio, Y. Learning deep architectures for ai. In Foun-dations and Trends in Machine Learning , 2009. Bengio, Y. and Lecun, Y. Scaling learing algorithms towards ai. In Large-Scale Kernel Machines , 2007. Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-
Farley, D., and Bengio, Y. Theano: A cpu and gpu math expression compiler. In Proceedings of the Python for Scientific Computing Conference , 2010. Buluc, A., Fineman, J., Frigo, M., Gilbert, J., and
Leiserson, C. Parallel sparse matrix-vector and matrix-transpose-vector multiplication using com-pressed sparse blocks. In Parallelism in Algorithms and Architectures , 2009.
 Cai, Z., Vagena, Z., Jermaine, C., and Haas, P. Very large scale bayesian inference using mcdb. In Big Learn Workshop, Advances in Neural Information Processing Systems , 2011.
 Chafi, H., Sujeeth, A., Brown, K., Lee, H., Atreya, A., and Olukotun, K. A domain-specific approach to heterogeneous parallelism. In Principles and Prac-tice of Parallel Programming , 2011.
 Coates, A. and Ng, A. Selecting receptive fields in deep networks. In Advances in Neural Information Processing Systems , 2011.
 Low, Y., Gonzalez, J., Kyrola, A., Bickson, D.,
Guestrin, C., and Hellerstein, J. Graphlab: A new framework for parallel machine learning. In 26th
Conference on Uncertainty in Artificial Intelligence , 2010.
 Raina, R. Large-scale deep unsupervised learning using graphics processors. In Proceedings of the 26th Annual International Conference on Machine Learning , 2009.
 Sujeeth, A., Lee, H., Brown, K., Chafi, H., Wu, M., Atreya, A., Olukotun, K., Rompf, T., and Odersky,
M. Optiml: An implicitly parallel domain-specific language for machine learning. In Proceedings of the 28th International Conference on Machine Learn-
