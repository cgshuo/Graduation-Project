 1.1 Motivation have been published on Twitter during a 7-month period, more than 300 million 150K comments per day [3] . 
There are a few social media datasets such as Spinn3r [4] . About 30 million articles (50GB of data), including 20,000 news sources and millions blogs were added to spinne3r per day [2] . As people access SNS frequently, advertisement could be broad-quakes and [18] studies influenza spread. For crawling, one of the most important factors is the freshness. Denev, Mazeika, Spaniol and Weikum [5] designed a web crawling framework named SHARC. It pays cerns more about the current time point. Even though SNS crawling is related to web lowing points and brings new technical challenges. 1. New messages are published more frequently. 2. The messages on SNS are shorter than web pages. 3. The SNS is closely related to users X  daily life. 4. SNS network is more complex. mand. With the consideration that the goal of crawling SNS information is to gather new information, this paper aims to crawl as more new messages as possible with the limited resources. 1.2 Contributions For crawling, the limitations on resources include bandwidth, computation power and politeness etiquette. For instance, for twitter.com , we may be permitted to get at most 200 tweets with a Twitter API call. The restriction on the method call is 350 calls per bottleneck of most SNS crawlers. 
To leverage the limited resources and freshness requirements of the crawler, we posts respectively. With these models, the time of post updating for different users is predicted and the crawler could access the posts of users only when corresponding is updated. As a result, the latest information is collected with limited resources. Combing the steps discussed above, we propose C rawl based on U ser Vi siting M odel (CUVIM in brief) based on the observations and classifications of users X  beha-information could also be crawled with the techniques in this paper. 
According to different behaviors on updating the posts, we classify SNS users into count, and authority account. This is the first contribution of this paper. 
We design different updating time predication model and accordingly develop effi-cient crawling strategies. This is the second contribution of this paper. 
Concretely, for the inactive accounts and frequently changing accounts which change not very frequently, the changes can be described by the Poisson process and son model and take web crawling strategy to crawl SNS data. 
From reasonable constant accounts and authority accounts who post messages fre-lives. According to this observation, we can crawl many fresh and useful messages in the day while almost no new messages at night. And we build the Hash Model to visit those active users to crawl the information efficiently. 
As the third contribution, extensive experiments are performed to verify the effec-collect 1,853,085 messages that they post during 2012-11-01 and 2013-01-01. From experimental results, the Poisson Process Model collects 12.14% more messages than a round-and-robin (RR) style method. The Hash Model collects about 50% more mes-sages than a RR style method. This paper is organized as follows. Section 2 reviews related work at crawler field; Section 3 introduces our work and discovery from the data; Section 4 introduces our clusions and proposes further research problems. Only a few methods are proposed to crawl SNS data. crawling method and develop algorithms to gather more information of the specific SNS users. 
TwitterEcho is an open source Twitter crawler developed by Bo X njak et al. [19]. It crawler [20], Noordihuis et al. collect Twitter data and rank Twitter users through the PageRank algorithm. Another attempt is to crawl parallel. Duen et al. implemented a parallel eBay crawler in Java and visited 11,716,588 users in 23 days [22]. The three crawling sequence with the given resource. Whitelist accounts were once available on Twitter. Kwak et al. crawled the entire Twitter site successfully, including 41.7 million user profiles and 106 million tweets prove the crawl efficiency. 
Another related work to SNS message collector is web crawlers. Generally the page changing follows the Poisson process model. The Poisson process model as-changes are modeled as following formula. considering the active SNS users who post messages frequently, the change rate is not user may post many messages in the day and much less at night, so the changing rate is not same for the day and night. 
Many web crawlers are proposed. Reprehensive measures for web crawling are sharp-and schedule crawling to achieve those targets. Differently, we choose the total number of new SNS messages as our target, and schedule according to the SNS users X  behavior. importance metrics including ordering schemes and performance evaluation measures to for estimating the change frequency of pages to make the web crawler work better [12] pagerank coverage guarantee high personalized to improve the crawler [14] . At first, to design the proper crawler for SNS, we discuss the features of SNS data and the classification of SNS users by their behavior in this section. Considering the four dience and channel as following in SNS relationships to study the SNS, where both A and B are users. audience, that means, A can check B  X  X  SNS messages. channel, that means, A  X  X  messages will be checked by B . messages. For effective crawling, we choose several top SNS users from the influence ranking list in Sina weibo (http://weibo.com), which is a famous SNS with millions of users. They are added to the crawling list as the seeds and some of their channels are randomly selected. The channels behaves more active than the audiences, thus we can iterations when we get enough users in the crawling list. ly, we have crawled the data for all users in a round-and-robin style. 
We crawled 10,000 users X  data for two months, and got 1,853,085 messages in to-tal. According to the experimental results, we have following observations. 1. Users are quite different in posting frequency. The users in the crawling list have 2. The frequency of new messages may change with time. An extreme user post 18 3. The frequency of posting new messages is related to the users X  daily life. Ex-4. Some accounts are maintained by professional clerks or robots, such as a With above observations, we classify all users into 4 types by their behavior. To illu-four users belonging to each type in Figure 1 to 4. It shows the total number of new post with 15 minutes as the unit. In Figure 1, 3 and 4, the line in the figure means the number of all messages in the 2 months. In Figure 2, each line means a day. 1. Inactive account . The users in this type post nothing in a long period or have lit-tle channels and audiences. 
From Figure 1, it can be observed that the figure of inactive account has at most a few messages in a few weeks. When observing for a long period, those users behave as web pages. It means that an inactive user may post three messages a month while a page may change three times this month. The number of possible changes between each equal time unity  X  when  X  is large enough. Thus we can descript the behavior of this type by Poisson process. silence later. For example, the one who post 18 messages and did not post anything in later 3 days. It is hard to design an effective crawling strategy for those people. 
Figure 2 shows the instable changing account. The user post 2 messages on Monday, 13 messages on Tuesday and nothing in the later three days. It is the most irregular one among all figures. It may be illustrated that the user has a sudden trip and cannot con-nect to the SNS, or the work those days are busy so he pays little attention to SNS. And those users may become reasonable constant users when he returns or finishes the work. behavior, thus we cannot schedule the crawler well. We treat those users as the inactive once. We found that they post many messages every day in the recent week. 3. Reasonable constant user . Most valid accounts belong to this type. For example, frequency of the new messages is obviously influenced by the users' daily life, and the new messages often occur frequently in the afternoon and at night when the user takes a rest. Hence we can predict their behavior by historic data. 
Figure 3 shows the behavior of reasonable constant user. Such users love the SNS peaks, the afternoon and the night. The curve in the left of the peak grows up and the one after the peak goes down. 4. Authority account : Such accounts are maintained by several clerks or just robots. people and is reviewed by more users. For example, the New York Time updates news quickly on Twitter and has a large group of audience. 
Figure 4 shows the be h than the usual users X  and h result, there are one peak f o reason is that clerks or rob o the Poisson Model and Hash Model. pling [9] can be applied to those users. 
The Hash model records the disciple of very active users X  behavior. For those us-ers, we consider the frequency change in a day, and the change rate  X   X  is not same all the day long. Hence the Poisson process cannot descript it. One effective way is to record the historic data change rate to predict the parameters for crawling. 4.1 Poisson Process Model According to the observation of inactive user s, we build a Poisson Process Model for their posting frequency, since those users behave like the updating of web pages, in-cluding changing constantly in a comparative long period (e.g. a month). We assume depths, and URL names. 
However, the web crawling metrics require to be modified to fit SNS. The SHARC potentiality. We define the potentiality for SNS as follows. ty of the user is the expected number of new messages between  X   X  and query time t , averaged through observation interval [0, n  X  ]: where is the crawling penalty.  X  X  users is defined as, the period of the crawling interval  X  X  , but not on the user. And we obtain Theorem 1. Theorem 1 (Properties of the Schedule Penalty). Double crawling delay will lead to fourfold crawling penalty and double potentiality: For the interests of space, we omit the proof the Theorem.  X  thus we get the potentiality minimum. 
Algorithm 1 depicts the Process Model algorithm for inactive users in SNS. All the us-ers are known advance. We can sort and scan all the users only once to schedule the craw-ler. The time and space complexity are both O( n ). Thus we scan the user list only once. 4.2 Hash Model According to the observation of reasonable constant users and authority accounts, we build the Hash model. The changing rate  X   X  for those users is stable from the obser-vations for a long time (e.g. a week or longer). However, the new SNS messages are users according to the averaged  X   X  of the day, we will waste much resource. For example, we visit the user in the Figure 3 according to the Poisson Process Model. In the Poisson process, the number of possible changes in each time unit start crawling at 0:00 and crawl twice a day, it may be 0:00 and 12:00. However, crawling at 3:00 and 19:00 seems the best strategy. Thus the Poisson Process Model does not fit those active users perfectly. 
The number of active users X  new messages changes frequently and is comparative randomly. Hence it is hard to find a precise and suitable model. On the other hand, the number of such kind of users is not large enough and a statistics model such as Gaus-predict the users X  behavior is to record the historic data. 
With such considerations, we define Hash Model to obtain the messages of the This model is built for the users who need to be crawled frequently, at least twice or more a day. This model uses a hash table to record the number of new messages in a short recent period and the earlier data has less weight. According to this historic data, span, so we can schedule the crawler better. of new messages posted by the same user in each hour.  X   X  values 0 at the beginning and the new value where the parameter n is the number of messages posted by the user at the i th hour of the day. The weight for k days before is 0.5  X  . As a result, the parameter for today is 0.5 and 0.25 for yesterday. decreases very fast. To avoid such phenomenon, the longest crawling span threshold s is required, that means, we will crawl the user at least once s hours or days. suitable for some special cases. For some special dates, such as the weekend, people show that 1496 messages were post on Tuesday while only 874 messages on Sunday. Thus we could predict users' behavior by the last weekend data according to this fea-day. It is required to consider about the near and similar holiday data, or even the last year's vocation data with the hash model. 
Algorithm 2 depicts the Hash Model for one user.  X  X 
The length of the hash table is k , we crawl c messages each time and the user post  X  and  X  X  X   X  is the number of minus remainingMessage , thus number of the remaining messages that are not crawled yesterday will be count., and the longest crawling span 
First, we update the lastCrawlTime and  X  X  X   X  , and calculate the values of the hash table and the value of  X  X  X   X   X  X  X ...  X  . Second, we scan the sums. If there are enough and update the lastcrawltime. 
The time and space complexity is O( n ). in this section. The experiments are performed on a PC with 2.10GHz intel CPU and weibo has 368 million registered users in 2012.08 and more than 100 million messag-es are post everyday [15] . 
We crawled 10,000 randomly selected users in 2 months days from 2012.11.01 to about 3.04 messages per day. 5.1 Experimental Results for Poisson Process Model To test the effectiveness of Poisson Process Model, we crawled the 10,000 users, and accessed every user once. From observati ons, the Poisson Process Model crawled 421,722 messages, while the round-and-robin style crawled 376,053 messages. Thus the Poisson Process Model is 12.14% better. 5.2 Hash Model We crawl the 10,000 users by hash model. We assume the crawling limit is 100 mes-sages at one crawling, the longest crawling span threshold s is 30 days, and the the 0.1. The result is shown in Table 1. For co mparisons, we also conduct the experiment described in Table 2. 
The data in Table 1 shows that with the increase of the weight, the total crawl time increase and more messages will be crawled, while the crawling efficiency decreases. shows that the crawling efficiency for the RR style method is reasonalbe stable when the crawling frequency is low. rithms are proposed according to the models. Experimental results show that the Hash Model is about 50% better than the Round-Rabin method, and the Poisson Process Model is 12.14% better than the RR method with randomly selected users. quired so that we can predict the hot pot in SNS. Acknowledgements. This paper was partially supported by NGFR 973 grant 2012CB316200, NSFC grant 61003046 and NGFR 863 grant 2012AA011004. Doc-toral Fund of Ministry of Education of China (No.20102302120054), the Fundamen-tal Research Funds for the Central Universities(No. HIT. NSRIF. 2013064). 
