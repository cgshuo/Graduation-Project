 Detecting and classifying the arguments of predi-cates has been an active area of research in recent years, driven by the availability of large-scale se-mantically annotated corpora such as the FrameNet (Baker et al., 1998) and the Propbank (Palmer et al., 2005). It is generally formulated as a seman-tic role labeling (SRL) task, where each argument of the predicate is assigned a label that represents the semantic role it plays with regard to its pred-icate (Gildea and Jurafsky, 2002; Hacioglu et al., 2003; Pradhan et al., 2004b; Xue and Palmer, 2004; Toutanova et al., 2005; Koomen et al., 2005). It has been the shared task for the CoNLL competition for two consecutive years (Carreras and M`arquez, 2004b; Carreras and M`arquez, 2005). This line of research has also expanded from English to other languages (Sun and Jurafsky, 2004; Xue and Palmer, 2005). So far, however, most of the research efforts have focused on analyzing the predicate-argument structure of verbs, largely due to absence of an-notated data for other predicate types. In this pa-per, we report SRL experiments performed on nom-inalized predicates in Chinese, taking advantage of a newly completed corpus, the Chinese Nombank (Xue, 2006), which we describe in greater detail in Section 2. The rest of the paper is organized as fol-lows. Section 3 describes the architecture of our sys-tem as well as the features we used in our experi-ments. In Section 4 we describe the experimental setups and report our experimental results. We first present experiments that use hand-crafted parses as input, providing a measurement of how well the Nombank annotation can be bootstrapped from the syntactic structure in the treebank. We then describe a more realistic experimental setup in which an au-tomatic parser is first used to parse unsegmented raw text and its output is then fed into our SRL system. We also discuss whether verb data can be used to im-prove the SRL accuracy of nominalized predicates. Finally we describe a preliminary experiment that uses reranking techniques to improve the SRL ac-curacy on hand-crafted parses. Section 5 attempts to put our results in perspective in the context of related work. Section 6 concludes our paper. The Chinese Nombank extends the general anno-tation framework of the English Proposition Bank (Palmer et al., 2005) and the English Nombank (Meyers et al., 2004) to the annotation of nomi-nalized predicates in Chinese. Like the English Nombank project, the Chinese Nombank adds a layer of semantic annotation to the Chinese Tree-Bank (CTB), a syntactically annotated corpus of 500 thousand words. The Chinese Nombank annotates two types of elements that are associated with the nominalized predicate: argument-like elements that are expected of this predicate, and adjunct-like el-ements that modify this predicate. Arguments are assigned numbered labels (prefixed by ARG , e.g., ARG0...ARGn) while adjuncts receive a functional tag (e.g., TMP for temporal, LOC for locative, MNR for manner) prefixed by ARGM . A predicate gen-erally has no more than six numbered arguments and the complete list of functional tags for adjuncts and their descriptions can be found in the annotation guidelines of this project.

The Chinese Nombank also adds a coarse-grained sense tag to the predicate. The senses of a predicate, formally called framesets , are motivated by the ar-gument structure of this predicate and are thus an integral part of the predicate-argument structure an-notation. Sense disambiguation is performed only when different senses of a predicate require different sets of arguments. These senses are the same senses defined for the corresponding verbs in the Chinese Proposition Bank, but typically only a subset of the verb senses are realized in their nominalized forms. The example in 1 illustrates the Chinese Nombank annotations, which are the labels in bold in the parse tree. Take u  X  ( X  X evelopment X ) as an example, f1 is the frameset identifier. Of the four expected argu-ments for this frameset, ARG0 the cause or agent, ARG1 the theme, ARG2 the initial state and ARG3 the end state or goal, only ARG1 is realized and it is  X  W ' X ( X  X ross-Strait relations X ). The predi-cate also has a modifier labeled ARGM-TMP , 8 ( X  X ereafter X ).

Typically the arguments and adjuncts of a nomi-nalized predicate are realized inside the noun phrase headed by the nominalized predicate, as is the case for u  X  ( X  X evelopment X ) in Example 1. A main exception is when the noun phrase headed by the nominalized predicate is an object of a support verb, in which case the arguments of this predicate can occur outside the noun phrase. This is illustrated by 5 y ( X  X lanning X ) in Example 1, where the noun phrase of which it is the head is the object of a sup-port verb ? 1 ( X  X onduct X ), which has little mean-ing of its own. Both arguments of this predicate,  X  b  X  W ( X  X he two sides of the Taiwan Strait X ) and 8  X  W ' X u  X  ( X  X he development of the cross-Strait relations X ), are realized outside the noun phrase. There are also a few other general ten-dencies about the arguments of nominalized predi-cates that are worth pointing out. The distribution of their arguments is much less predictable than verbs whose arguments typically occupy prominent syn-tactic positions like the subject and object. There also tend to be fewer arguments that are actually realized for nominalized predicates. Nominalized predicates also tend to take fewer types of adjuncts ( ARGM s) than their verbal counterpart and they also tend to be less polysemous, having only a subset of the senses of their verb counterpart.

The goal of the semantic role labeling task de-scribed in this paper is to identify the arguments and adjuncts of nominalized predicates and assign appropriate semantic role labels to them. For the purposes of our experiments, the sense information of the predicates are ignored and left for future re-search. The predominant approach to the semantic role la-beling task is to formulate it as a classification prob-lem that can be solved with machine-learning tech-niques. Argument detection is generally formulated as a binary classification task that separates con-stituents that are arguments or adjuncts to a pred-ARG0/REL2 VV VP  X  b  X  W the two sides of the Straits icate from those that are not related to the pred-icate in question. Argument classification, which classifies the constituents into a category that cor-responds to one of the argument or adjunct la-bels is a natural multi-category classification prob-lem. Many classification techniques, SVM (Pradhan et al., 2004b), perceptrons (Carreras and M`arquez, 2004a), Maximum Entropy (Xue and Palmer, 2004), etc. have been successfully used to solve SRL prob-lems. For our purposes here, we use a Maximum En-tropy classifier with a tunable Gaussian prior in the does multi-category classification and thus can be straightforwardly applied to the problem here. The classifier can be tuned to minimize overfitting by ad-justing the Gaussian prior. 3.1 A three-stage architecture Like verbal predicates, the arguments and adjuncts of a nominalized predicate are related to the pred-icate itself in linguistically well-understood struc-tural configurations. As we pointed out in Section 2, most of the arguments for nominalized predicates are inside the NP headed by the predicate unless the NP is the object of a support verb, in which case its arguments can occur outside the NP. Typically the subject of the support verb is also an argument of the nominalized predicate, as illustrated in Example 1. The majority of the constituents are not related to the predicate in question, especially since the sentences in the treebank tend to be very long. This is clearly a lingustic observation that can be exploited for the purpose of argument detection. There are two com-mon approaches to argument detection in the SRL literature. One is to apply a binary classifier directly to all the constituents in the parse tree to separate the arguments from non-arguments, and let the ma-chine learning algorithm do the work. This can be done with high accuracy when the machine-learning algorithm is powerful and is provided with appro-priate features (Hacioglu et al., 2003; Pradhan et al., 2004b). The alternative approach is to combine heuristic and machine-learning approaches (Xue and Palmer, 2004). Some negative samples are first fil-tered out with heuristics that exploit the syntactic structures represented in a parse tree before a binary classifier is applied to further separate the positive samples from the negative samples. It turns out the heuristics that are first proposed in Xue and Palmer (2004) to prune out non-arguments for verbal pred-icates can be easily adapted to detect arguments for the nominalized predicates as well, so in our exper-iments we adopt the latter approach. The algorithm starts from the predicate that anchors the annotation, and first collects all the sisters of this predicate. It then iteratively moves one level up to the parent of the current node to collect its sisters till it reaches the appropriate top-level node. At each level, the sys-tem has a procedure to determine whether that level is a coordination structure or a modification struc-ture. The system only considers a constituent to be a potential candidate if it is an adjunct to the current node. Punctuation marks at all levels are skipped. After this initial procedure, a binary classifier is ap-plied to distinguish the positive samples from the negative samples. A lower threshold is used for pos-itive samples than negative samples to maximize the recall so that we can pass along as many positive samples as possible to the next stage, which is the multi-category classification. 3.2 Features SRL differs from low-level NLP tasks such as POS tagging in that it has a fairly large feature space and as a result linguistic knowledge is crucial in design-ing effective features for this task. A wide range of features have been shown to be useful in previous work on semantic role labeling for verbal predicates (Gildea and Jurafsky, 2002; Pradhan et al., 2004b; Xue and Palmer, 2004) and our experiments show most of them are also effective for SRL of nominal-ized predicates. The features for our multicategory classifier are listed below:  X   X   X   X   X   X   X   X   X   X 
Not all the features used for multicategory clas-sification are equally effective for binary classifica-tion, which only determines whether or not a con-stituent is an argument or adjunct to the nominal-ized predicate. Therefore, the features for the binary classifier are a subset of the features used for multi-category classification. These are path, path plus dominating verb, head word and its part-of-speech and sisterhood. 4.1 Data Our system is trained and tested on a pre-release version of the Chinese Nombank. This version of the Chinese Nombank consists of standoff annota-tion on the first 760 articles ( chtb_001.fid to chtb_931.fid ) of the Penn Chinese Treebank 2 . This chunk of data has 250K words and 10,364 sen-tences. It has 1,227 nominalized predicate types and 10,497 nominalized predicate instances. In com-parison, there are 4,854 verb predicate types and 37,183 verb predicate instances in the same chunk of data. By instance, the size of the Nombank is be-tween a quarter and one third of the Chinese Propo-sition Bank. Following the convention of the se-mantic role labeling experiments in previous work, we divide the training and test data by the num-ber of articles, not by the predicate instances. This pretty much guarantees that there will be unseen predicates in the test data. For all our experiments, 688 files are used as training data and the other 72 files ( chtb_001.fid to chtb_040.fid and chtb_900.fid to chtb_931.fid ) are held out as test data. The test data is selected from the double-annotated files in the Chinese Treebank and the complete list of double-annotated files can be found in the documentation for the Chinese Tree-bank 5.1. Our parser is trained and tested with the same data partition as our semantic role labeling sys-tem. 4.2 Semantic role tagging with hand-crafted In this section we present experimental results us-ing Gold Standard parses in the Chinese Treebank as input. To be used in real-world natural language applications, a semantic role tagger has to use au-tomatically produced constituent boundaries either from a parser or by some other means, but experi-ments with Gold Standard input will help us evaluate how much of a challenge it is to map a syntactic rep-resentation to a semantic representation, which may very well vary from language to language. There are two experimental setups. In the first experiment, we assume that the constituents that are arguments or adjuncts are known. We only need to assign the correct argument or adjunct labels. In the second experiment, we assume that all the constituents in a parse tree are possible arguments. The system first filters out consituents that are highly unlikely to be an argument for the predicate, using the heuristics described in Section 3. A binary classifier is then applied to the remaining constituents to do further separation. Finally the multicategory classifier is applied to the candidates that the binary classifier passes along. The results of these two experiments are presented in Table 2. experiments Compared with the 93.9% reported by Xue and Palmer (2005) for verbal predicates on the same data, the 86.9% the system achieved when the con-situents are given is considerably lower, suggest-ing that SRL for nominalized predicates is a much more challenging task. The difference between the SRL accuracy for verbal and nominalized predicates is even greater when the constituents are not given and the system has to identify the arguments to be classified. Xue and Palmer reported an f-score of 91.4% for verbal predicates under similar experi-mental conditions, in contrast with the 71.6% our system achieved for nominalized predicates. Care-ful error analysis shows that one important cause for this degradation in performance is the fact that there is insufficient training data for the system to reliably separate support verbs from other verbs and deter-mine whether the constituents outside the NP headed by the nominalized predicate are related to the pred-icate or not. 4.3 Using automatic parses We also conducted an experiment that assumes a more realistic scenario in which the input is raw un-segmented text. We use a fully automatic parser that integrates segmentation, POS tagging and pars-ing. Our parser is similar to (Luo, 2003) and is trained and tested on the same data partition as the semantic role labeling system. Tested on the held-out test data, the labeled precision and recall are 83.06% and 80.15% respectively for all sentences. The results are comparable with those reported in Luo (Luo, 2003), but they cannot be directly com-pared with most of the results reported in the litera-ture, where correct segmentation is assumed. In ad-dition, in order to account for the differences in seg-mentation, each character has to be treated as a leaf of the parse tree. This is in contrast with word-based parsers where words are terminals. Since semantic role tagging is performed on the output of the parser, only constituents in the parse tree are candidates. If there is no constituent in the parse tree that shares the same text span with an argument in the manual annotation, the system cannot possibly get a correct annotation. In other words, the best the system can do is to correctly label all arguments that have a con-stituent with the same text span in the parse tree.
The results show a similar performance degrada-tion compared with the results reported for verbs on the same data in previous work, which is not unex-pected. Xue and Palmer (2005) reported an f-score of 61.3% when a parser is used to preprocess the data. 4.4 Using verb data to improve noun SRL Since verbs and their nominalized counterparts are generally considered to share the same argument structure and in fact the Chinese Nombank is an-notated based on the same set of lexical guide-lines (called frame files ) as the Chinese PropBank, it seems reasonable to expect that adding the verb data to the training set will improve the SRL accu-racy of the nominal predicates, especially when the training set is relatively small. Given that verbs and their nominalized counterpart share the same mor-phological form in Chinese, adding the verb data to the training set is particularly straightforward. In our experiments, we extracted verb instances from the CPB that have nominalized forms in the portion of the Chinese Treebank on which our SRL exper-iments are performed and added them to the train-ing set. Our experiments show, however, that sim-ply adding the verb data to the training set and in-discriminately extracting the same features from the verb and noun instances will hurt the overall perfor-mance instead of improving it. This result is hardly surprising upon closer examination: the values of certain features are vastly different for verbal and nominal predicates. Most notably, the path from the predicate to the constituent being classified, an im-portant feature for semantic role labeling systems, differ greatly from nominal and verbal predicates. When they are thrown in the same training data mix, they effectively create noise and neutralize the dis-criminative effect of this feature. Other features, such as the head words and their POS tags, are the same and adding these features does indeed improve the SRL accuracy of nominal predicates, although the improvement is not statistically significant. 4.5 Reranking In a recent paper on the SRL on verbal predicates for English, (Toutanova et al., 2005) pointed out that one potential flaw in a SRL system where each ar-gument is considered on its own is that it does not take advantage of the fact that the arguments (not the adjuncts) of a predicate are subject to the hard con-show that by performing joint learning of all the ar-guments in the same proposition (for the same predi-cate), the SRL accuracy is improved. To test the effi-cacy of joint-learning for nominalized predicates in Chinese, we conducted a similar experiment, using a perceptron reranker described in Shen and Joshi (2004). Arguments and adjuncts of the same predi-cate instance (proposition) are chained together with their joint probability being the product of the indi-vidual arguments and the top K propositions are se-lected as the reranking candidates. When the argu-ments are given and the input is hand-crafted gold-standard parses in the treebank, selecting the top 10 propositions yields an oracle score of 97%. This ini-tial promise does not pan out, however. Performing reranking on the top 10 propositions did not lead to significant improvement, using the five feature classes described in (Haghighi et al., 2005). These are features that are hard to implement for individual arguments: core argument label sequence , flattened core argument label sequence , core argument labels and phrase type sequence , repeated core argument labels with phrase types , repeated core argument la-bels with phrase types and adjacency information . We speculate that the lack of improvement is due to the fact that the constraint that core (numbered) arguments should not have the same semantic role label for Chinese nominalized predicates is not as rigid as it is for English verbs. However further error analysis is needed to substantiate this speculation. Compared with large body of work on the SRL of verbal predicates, there has been relatively lit-tle work done in analyzing the predicate-argument structure of nominalized predicates. There are even less work done for the nominalized predicates for Chinese. (Hull and Comez, 1996) implemented a rule-based system for identifying the arguments for nominal predicates and (Lapata, 2002) has a system that interprets the relation between the head of noun compound and its head, but no meaningful compar-ison can be made between our work and theirs. Per-haps the closest work to that of ours is that of (Prad-han et al., 2004a), where they reported preliminary work for analyzing the predicate-argument structure of Chinese nominalizations, using a small data set of 630 proposition for 22 nominalizations taken from the Chinese Treebank. Since different data sets are used, the results cannot be meaningfully compared.
The results reported here for nominalized pred-icates are consistent with what Xue and Palmer (2005) reported for the SRL of Chinese verbs with regard to the role of the parser in their semantic role labeling system: there is a substantial perfor-mance drop when the automatic parser is used. At present, improvement in Chinese parsing is hindered by insufficient training material. Although the Chi-nese Treebank has a decent size of 500K words, it is evenly divided into two portions of very differ-ent sources, Xinhua newswire from mainland China and Sinorama magazines from Taiwan. Due to their very different styles, training on one portion of the data does not help or even hurt the parsing accuracy of the other portion. The lack of sufficient train-ing material is compounded by inherent properties of the Chinese language that makes Chinese pars-ing particularly difficult. Chinese segmentation is a much more difficult problem than tokenization of English text and Chinese words do not have mor-phological clues that can help parsing decisions. We believe further improvement in SRL accuracy will be to a large extent contingent on the parsing accu-racy, which requires more training material. We reported first results on the semantic role label-ing of nominalized predicates in Chinese, using a sizable annotated corpus, the Chinese Nombank, as training and test data. Compared with that of ver-bal predicates, SRL of nominalized predicates gen-erally presents a more challenging problem, for all experimental conditions. While the smaller train-ing set compared with that of verbal predicates may provide partial explanation for the degradation in performance, we believe another important reason is that the arguments for nominalized predicates do not occupy prominent syntactic positions such as the subject and object, as arguments of verbal predicates often do. As a result, the syntactic structure repre-sented in the parse tree does not provide as much of a clue for their detection and classification. However, this makes SRL of nominalized predicates a more pressing issue to solve, as they represent a substan-tial proportion of the predicates in the corpus. Our results also show that the k-best propositions pro-duced by the local classifier have a very high ora-cle score, which perhaps indicates a promising path that deserves further exploration, based on careful analysis of the errors. We intend to continue to ex-periment with new features and parameters for the reranking algorithm. I would like to thank Martha Palmer for her unwa-vering support for this line of research. This work is funded by the NSF ITR via grant 130-1303-4-541984-XXXX-2000-1070.

