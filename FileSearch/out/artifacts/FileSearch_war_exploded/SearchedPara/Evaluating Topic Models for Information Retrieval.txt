 We explore the utility of different types of topic models, both probabilistic and not, for retrieval purposes. We show that: (1) topic models are effective for document smoothing; (2) more elaborate topic models that capture topic dependencies provide no additional gains; (3) smoothing documents by using their similar documents is as effective as smoothing them by using topic models; (4) topics discovered on the whole corpus are too coarse-grained to be useful for query expansion. Experiments to measure topic models X  ability to predict held-out likelihood confirm past results on small corpora, but suggest that simple approaches to topic model are better for large corpora.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval General Terms: Algorithms, Experimentation Keywords: Topic Model, Retrieval, Evaluation
Topic models are a very popular approach for represent-ing the content of documents. A document is assumed to draw its vocabulary from one or more topics. Topics are represented as probability distributions over the vocabulary, where differing topics give different words high probabili-ties. We infer a set of topics which can be used to describe the contents of a collection. The high probability topics and words within them can be viewed as a loose description of the collection, with better topic models providing bet-ter descriptions. A natural question is whether these topics are useful to help retrieve documents on the same topic as a query  X  intuitively relevant documents have topic distri-butions that are likely to have generated the set of words associated with the query[1, 2]. In fact, early research on topic models suggested that they might be used for infor-mation retrieval (IR)[2], but it was not until recently that they were successfully applied to large-scale and realistic col-lections [6]. Our goal in this study is to explore the utility of different topic models for retrieval purposes.
Different topic models specify different document gener-ative procedures, which can lead to very different topics. We use a selection of representative machine-learning topic models: the Mixture of Unigrams (MU) [1], Latent Dirich-let Allocation (LDA)[1, 6] and Pachinko Allocation Model Figure 1: Likelihood comparison on the NIPS dataset. (PAM) [4]. We also construct a special kind of topic model based on Relevance Modeling (RM) [3], which treats each document as the representative of its own topic.
We first follow a standard machine learning evaluation approach to compare the document modeling performance of different topic models[1]. We calculate the log Empirical Likelihoods (log-ELs) of generating a held-out test data from different models[4], then further divide the log-ELs by that from the unigram model to obtain relative log-ELs  X  rl TM for better comparison. Thus, rl unigram is always 1 . 0 even using different sized test data and low scores indicate better representation of the corpus.

We first use a small NIPS abstract dataset [4], with 1647 abstracts from NIPS proceedings. Following [4], we split the dataset into two subsets with 75% and 25% of the data respectively. We train the models with the larger set and calculate likelihood for the smaller set. We use 50 super-topics for PAM, with the number of sub-topics varying from 10 to 180. Figure 1 shows how the relative log-ELs change with different number of topics and different models. Our re-sults are consistent with previous results[4]: PAM and LDA always perform better than simple models (MU and RM); PAM performs the best. By viewing each document as a topic, RM performs better than MU, and close to LDA. We then use large scale corpora for comparison which has not been done before to the best of our knowledge. We utilize five large TREC corpora used in previous research [5, 6]. They are AP, FT, SJMN, LA, WSJ, with between 90 and 242K documents and roughly 100 queries per corpus. For each of the five corpora, we split it into halves: training different models with one half and calculating likelihood for Figure 2: Likelihood comparison with different number Table 1: MAPs of different methods on the testing cor-the other half. PAM has not been used in this comparison because its training is too expensive. Figure 2 shows the re-sults on AP test data. It was surprising that the results on large corpora do not mimic those on small corpora: LDA has higher rl than smoothed MU. We speculate that smoothed MU performs better than LDA when using relatively small number of topics for modeling distributions of large corpora.
There are two obvious approaches to including topic mod-els in IR. The first is the document modeling approach, where we calculate P ( w | D ) by smoothing the document with topics from different topic models: when smoothing the doc-ument with the highest ranked topic it is in and using the topic from MU, LDA, PAM and RM, we have retrieval meth-ods  X  CBDM[5], BT-LBDM, BT-PBDM and RMDE-1, re-spectively; when smoothing the document with a weighted combination of all topics that it contains and using the topics from MU, LDA, PAM and RM, we have MBDM, LBDM[6], PBDM and RMDE, respectively. The second is the query expansion approach, where we topics similarly add words to the query and run the revised query: when only using the most similar topic from MU, LDA, PAM and RM, we have retrieval methods  X  BT-CBQE, BT-LBQE, BT-PBQE and RM-1, respectively; when using all the topics from MU, LDA, PAM and RM, we have CBQE, LBQE, PBQE and RM, respectively.

Five TREC corpora and queries are used again for evaluat-ing different topic model based retrieval methods. The same data had been used for evaluating CBDM and LBDM[5, 6]. We use the full AP corpus for training and the other four corpora (FT, SJMN, LA, WSJ) for testing. The number of topics for MU and LDA is tuned to be 2000 and 800 respec-tively. For PAM, we use 800 sub-topics and 100 super-topics. We also include a simple language modeling retrieval base-line  X  QL, which does not use topic models and only use Dirichlet smoothing.
 Table 1 show the best retrieval results. Our results of CBDM and LBDM are only slightly different from the orig-inal results [5, 6] due to small differences in the imple-mentations. We have the following observations: (1) Using topic models for document smoothing can improve IR per-formance of the typical smoothing technique; complicated topic models like LDA and PAM have some benefits: LBDM and PBDM achieve higher MAPs than CBDM. (2) The doc-ument expansion approach RMDE, which borrows idea from RM to do document smoothing and does not actually iden-tify topics in the collection, performs better than CBDM, and sometimes similar to LBDM. (3) PBDM performs simi-lar to LBDM although PAM is more complicated topic mod-eling approach than LDA. (4) When topic models are trained with the whole corpus, topic models for documents smooth-ing always outperforms for query expansion.

However, the RM approach, the one that does not actu-ally identify topics in the collection, outperforms all topic modeling approaches consistently. We must conclude that these topic modeling approaches used in these ways are not appropriate for document retrieval. We speculate that the coarse-grained information in full-blown topics is more con-fusing than useful for this task. We do note that topic model based retrieval methods following the document smoothing approach can be combined with RM [5, 6] to further improve the IR performance although the improvement is very small.
In this paper, we have explored the utility of different types of topic models for IR. Experimental results show that topics trained on the full corpus are more useful for docu-ment smoothing than for query expansion. Applying com-plicated models like LDA for document smoothing can im-prove IR performance, but more powerful model like PAM does not necessary provide further benefits. RM outper-forms all these topic modeling approaches consistently in most cases: one possible reason is that topics discovered by topic models in large scale corpus are not as fine-grained as the query-specific topic calculated by RM.
