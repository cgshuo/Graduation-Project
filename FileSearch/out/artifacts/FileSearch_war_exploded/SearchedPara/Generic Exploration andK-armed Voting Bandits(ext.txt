 Tanguy Urvoy tanguy.urvoy@orange.com Fabrice Clerot fabrice.clerot@orange.com Raphael F  X eraud raphael.feraud@orange.com Sami Naamane sami.naamane@orange.com Orange-labs, 2 avenue Pierre Marzin, 22307 Lannion, FRANCE The stochastic multi-armed bandits became popular as a stripped-down model of exploration versus ex-ploitation balance in sequential decision problems. In its simplest formulation, we are facing a slot machine with several arms. The rewards of these arms are mod-eled by unknown but bounded and independent ran-dom variables. To maximize our long term reward, we would like to play an arm with maximal expected value but we need to explore efficiently all the arms in order to find it.
 The cost of ignorance is traditionally expressed in term of expected regret : the expected difference of re-ward between a playing policy established with perfect knowledge of the environment parameters and a given  X  X naware X  policy. Following Lai &amp; Robbins (1985), several regret analysis have been proposed (see for in-stance Auer et al., 2002; Audibert et al., 2008; Auer &amp; Ortner, 2011).
 1.2. Dueling bandits The dueling bandit problem, introduced by Yue &amp; Joachims (2009) to formalize online learning from pref-erence feedback, shares the indirect or parametric feed-back property with rigged bandits. The initial motiva-tion to depart from the absolute-reward model came from information retrieval evaluation where the im-plicit feedback by means of click logs is strongly bi-ased by the ranking itself. A solution was proposed by Joachims (2003) to circumvent this problem: by interleaving two ranking models, and checking where the user clicked, one obtains an unbiased  X  but pair-wise  X  preference feedback. Further experiments were performed in (Chapelle et al., 2012).
 The original definition of the dueling bandits problem (Yue et al., 2012; Yue &amp; Joachims, 2011) was built upon strong assumptions about the preference matrix: existence of a strict linear ordering , stochastic tran-sitivity and stochastic triangular inequality (see Yue &amp; Joachims, 2011). An extension of this setting with restricted pairing was proposed by Di Castro et al. (2011), but this extension also assumes the preference matrix to be the byproduct of an inherent value for each arm.
 In a situation where the preferences reflects the ex-pression of a mixed crowd, there can be several incon-sistencies or voting paradoxes which contradict these assumptions. The definition we propose here is more relaxed: we do not assume the existence of a perfect linear order, neither do we assume the existence of an inherent value of arms. We simply try to sample efficiently the preference matrix in order to propose a  X  X est element X  similar to the one we would choose with perfect knowledge of the crowd preferences. Electing a  X  X est element X  or a  X  X est linear ordering X  from such a preference matrix is a tough but old and well-studied problem (see Charon &amp; Hudry, 2010, for a survey), but the works about online and noisy declina-tions of this problem are scarce (see however Raviku-mar et al., 1987; Feige et al., 1994, for related prob-lems). If we change the election criterion according to the vast social choice theory (see Chevaleyre et al., 2007, for a survey),we can decline the dueling ban-dits in several unexplored flavors: for instance Borda bandits, Copeland bandits, Slater bandits, or Kemeny bandits. 1.3. Toward generic exploration algorithms The traditional approach to deal with exotic sequential decision problems is to design tailor-made algorithms which handle simultaneously the exploration of the en-there exists a neighborhood of  X  where f is constant. In order to analyze the performance of our algorithm in the next section, we will need a finer description of this neighborhood. For instance, the binary decision function defined in Figure 1 is constant on the neigh-borhood of  X  but it is also independent of x 2 for any configuration where x 1  X  1 { 2.
 Let us introduce some more notations: hereafter } x } 8 :  X  max i | x i | will denote the l 8 norm, B p  X  ,r q :  X  t x | } x  X   X  } 8  X  r u will denote the l 8 ball (or box) of radius r around  X  , and e i will denote the standard basis vector with a 1 in the i th coordinate and 0 X  X  else-where.
 Definition 2. Let H be a subset of F . The decision function f is independent of its parameter i on H if for any  X  P r  X  1 , ` 1 s we have: For instance on Figure 1 the decision is independent of x 2 on the set B p  X  ,  X  2 q . This local independence , parametrized by the  X  i radii, captures the sensitiv-ity of the decision to its input parameters around the environment state  X  . We propose a generic zooming algorithm to solve the N dimensional parametric decision problem with high The proof is given in Appendix A.1.
 By definition, if f is independent of each parameter i on B p  X  ,  X  i q , then f is constant on the minimal box B p  X  ,  X  q , where  X   X  min i  X  i . An exploration policy without elimination would reach this neighborhood af-This means that SAVAGE will outperform a uni-form exploration policy as soon as the  X  i are not equal. It is also worth noting for practical purpose, that this improvement will hold even if we replace IndepTest p f, H ,i q with a sufficient condition of in-dependence. Such a relaxation requires however to replace (2) by (3) to ensure termination. 3.1. Independence predicates The independence predicate IndepTest p f, H ,i q is a property of the decision function and its feasible set. It can thus be specialized via symbolic calculus or hand-crafted for specific problems where the properties of f and F are well known.
 For example in traditional multi-armed bandits set-tings, when f p x 1 ,...,x N q P arg max t x 1 ,...,x N u and H p t q is encoded by a product of confidence intervals r a i ,b i s , we can use the SAVAGE algorithm with the following specialized predicate IndepTest f p H p t q ,i q : With this predicate, we fall-back almost to the  X  X rm elimination X  of (Even-Dar et al., 2002). We how-ever slightly depart from this algorithm by forcing in-clusion of the successive confidence sets: r a i ,b i s :  X  r max t a i ,  X   X  i  X  c p t i qu , min t b i ,  X   X  i ` c p t i qus . If we rather want to retrieve the p m ` 1 q th best arm like in rigged bandits, the independence predicate be-comes: A simple formalization of the independence allows us to apply SAVAGE and Theorem 1 to several other vari-ants of multi-armed bandits.
 When the knowledge about f or F is scarce, and the dimension of the problem is not too high, another solution that we only explored empirically is to es-timate the independence predicate by  X  X ntrospective X  simulations. We used the multi-start random-walk ap-proximation detailed in Algorithm 2. It provides an  X  X lmost-everywhere statement X  of the property with an asymmetric risk of failure which can be made ar-bitrary low by increasing the number of samples (pa-rameters m and M ). This kind of method is widely where N  X , X   X  |t i |  X  i  X   X  {  X  u| .
 When T  X  8 , its exploration time is bounded by: See Appendix A.2 for the proof. The access to the decision metric L may also be explicit in which case we can use directly @ d,d 1 P f p H q , L p d,d 1 q  X   X  , in place for (6) but it is difficult to dispense with the Lipschitz hypothesis to obtain a generic bound. From now on, we call K  X  K preference matrix a K  X  K matrix p x i,j q such that x i,j ` x j,i  X  1 for each i,j P t 1 ,...,K u (we use lower-case letters to match the no-tations of Section 2).
 The K -dueling problem, as presented in (Yue et al., 2012; Yue &amp; Joachims, 2011), assumes the existence of an environment preference matrix  X  from which we only have a noisy perception modeled, as in (Feige et al., 1994), by a K  X  K-matrix of random variables X i,j P r 0 , 1 s verifying E r X i,j s  X   X  i,j . Our aim is to de-sign a sequence of pairwise experiments p i t ,j t q called duels for t  X  1 ,...,T in order to find the best arm. They also assume the following properties for the pref-erence matrix(WLOG for a proper indexation of the matrix): strict linear order: if i  X  j then  X  i,j  X  1 2 ;  X  -relaxed stochastic transitivity: if 1  X  j  X  k stochastic triangular inequality: if 1  X  j  X  k These last three assumptions are realistic when the preference matrix is the result of a perturbed linear or-der. This is indeed the case for some generative models where the number of parameters of the environment is assumed to be K : the inherent values of arms. In a situation where the preferences may contain cycles (or voting paradoxes ) there is no clear notion of what the best arm is, and the notion of regret is unclear. To avoid these problems, we propose to consider a  X  X oting X  variant of K -dueling bandits where a pair-wise election criterion is used to determine the best candidate from the preference matrix. Several election systems can be used, but we will focus here on a sim-ple and well-established one: the Copeland pairwise aggregation method (see Charon &amp; Hudry, 2010). the feasible set F to the K  X  K-preferences matrices admitting a Condorcet winner : We can obtain a formal independence test in F cond by replacing (8) with: To stop exploration with an  X  -approximation 1 of the winner, we replace Accept p f, H , W q by: Theorem 3. If the environment state  X  is known to admit a Condorcet winner i  X   X  f p  X  q then SAVAGE with F Cond as feasible set and (12) as acceptance con-dition is an p  X , X  q -PAC algorithm with horizon T for the Copeland bandits problem. When T  X  8 , its sam-ples complexity is bounded by: Where for each j  X  i  X  we have  X  j  X   X  i  X  ,j  X  1 2 (indexed WLOG by increasing values of  X  j ).
 When T  X  8 its exploration time is bounded by: A proof is given in Appendix A.4 This is a signif-icant improvement from (9) but it does not remove the quadratic term K 2 . This leading K 2 factor is the price we pay for accepting less constrained preference matrices. 4.2. Borda bandits Another simple way to elect the winner of the matrix is to use Borda count. Each competitor is ranked ac-cording to its mean performance against others: The main advantage of this criterion is that it both of-fers stability (the utility is linear) and clearly reduces the dimension of the problem to only K parameters: x i,  X  for i  X  1 ,...,K . This means that we can sim-ply wrap a classical bandit algorithm to search for the Borda winner of the matrix. It is quite easy however to 5.1. Bandits simulations For bandits problems the decision space and the explo-ration space coincide, but we are here in a pure explo-ration setting where the arm we predict to be the best is not necessary the one we explore. We considered the following algorithms for our bandits simulations: Uniform: baseline uniform exploration policy (each Naive UCB: UCB1 (as in Auer et al., 2002); Naive Elimination: applies Action elimination al-Wrapped UCB: applies UCB1 to the wrapped re-Wrapped Elimination: applies Action elimination SAVAGE: applies Algorithm 1 with  X  p t q :  X  2 NT SAVAGE Sampling: Algorithm 1 with a sampled clearly outperforms state-of-the art algorithms in such situations.
 The construction of a generic exploration algorithm reaching optimality for any provided decision function remains as a challenging open problem.
 We would like to thank the reviewers for their careful reading and helpful comments.
 Audibert, J.Y., Munos, R., and Szepesv  X ari, Cs.
Exploration-exploitation trade-off using variance es-timates in multi-armed bandits. Theoretical Com-puter Science , 2008.
 Audibert, J.Y., Bubeck, S., and Munos, R. Best arm identification in multi-armed bandits. In COLT , Ha  X  X fa (Isra  X el), 2010. Omnipress.
 Auer, P. and Ortner, R. UCB revisited: Improved regret bounds for the stochastic multi-armed ban-dit problem. Period.Math.Hungar. , 61(1 X 2):55 X 65, 2011.
 Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis of the multiarmed bandit problem. Mach. Learn. , 47(2-3):235 X 256, 2002.
 Bubeck, S., Munos, R., and Stoltz, G. Pure explo-ration in finitely-armed and continuous-armed ban-dits. Theor. Comput. Sci. , 412(19):1832 X 1852, 2011. Chapelle, O., Joachims, T., Radlinski, F., and Yue,
Y. Large-scale validation and analysis of interleaved search evaluation. ACM Trans. Inf. Syst. , 30(1):6, 2012.
 Charon, I. and Hudry, O. An updated survey on the linear ordering problem for weighted or unweighted tournaments. Annals OR , 175(1):107 X 158, 2010.
 Chevaleyre, Y., Endriss, U., Lang, J., and Maudet,
N. A short introduction to computational social choice. In SOFSEM , volume 4362 of LNCS , pp. 51 X  69. Springer-Verlag, 2007.
 Di Castro, D., Gentile, C., and Mannor, S. Bandits with an edge. CoRR , abs/1109.2296, 2011.
 Even-Dar, E., Mannor, S., and Mansour, Y. PAC bounds for multi-armed bandit and Markov deci-sion processes. In COLT , pp. 255 X 270, London, UK, 2002. Springer-Verlag.
 A.1. Proof of Theorem 1 Theorem 1. 1 -Correctness by H p t q its corresponding hypothesis set: and by the union bound: Therefore,  X  p t q being properly chosen according to (1), we have: In particular, when T  X  8 , we have  X  t  X  1 1 t 2  X   X  2 6 . From now on we will assume that  X  P H at any step of the algorithm. 2 -Sample complexity / exploration time assume that parameter i was not eliminated after 2  X  2 H , hence f p x 1 q  X  f p x q .
 t  X  t j . The function c p t q being strictly decreasing, we also have: ensures that c p t i q  X   X  i { 2.
 Indeed, starting from: If we apply the decreasing function to both sides of (18) inequality, we obtain: where: For any x  X  6 we have hence for any N  X  2, 0  X   X   X  1, and 0  X   X  i  X  1: and finally A.2. Proof for Theorem 2 arguments as in Appendix A.1.
 to Theorem 1.
 When T  X  8 , we obtain the same result after samples of parameter i .
 B p  X  ,  X   X  q where samples when T is finite.
 And after samples when T  X  8 .
 A.3. Proof of Property 1 Property 1. Two possibilities: w.r.t. x i,j on B p  X  ,  X  i,j q .
 independence w.r.t. any parameter on B p  X  ,  X  q . A.4. Proof of Theorem 3 Theorem 3. 1 -Domino arms elimination have: x 1 ,l  X  1 2 . Consider the following two facts: fact 1: @ x P H p t q ,f p x q  X  l (because x l, 1  X  1 2 );  X  X ymmetry X  of preference matrices, we have: 2 -Summation Appendix A.1).
 time. The worst-case total cost is hence upper-bounded by: If we reorder the sum we have: hence the result.
 B.1. A note about Explore then Exploit Algorithm 4 Explore then exploit  X  d  X  t :  X  EXPLORE p  X   X  0 , X   X  1 { T,T q for s :  X  p  X  t ` 1 q to T do end for We split this sum into exploration and exploitation phases: B.2. A few implementation details and Random .
 proposed set F is empty or too flat.
 B.3. An example to illustrate the formal Copeland independence predicate undetermined preference matrix: This matrix is defined from H  X   X  i  X  j r a i,j ,b i,j s by: U Predicate (11) is encoded by: B.5. Other parametric decision problems B.5.1. Argmax bandits classical bandit setting.
 B.5.2. Counting bandits B.6. Additional dueling bandits experiments strategy on the long run with these kind of  X  X lat X  preference matrices. B.7. Decision functions visualization attempt we navigate in fog, but we have the map.

