 The skyline operator was first proposed in 2001 for retrieving inter-esting tuples from a dataset. Since then, 100+ skyline-related pa-pers have been published; however, we discovered that one of the most intuitive and practical type of skyline queries, namely, group-by skyline queries remains unaddressed. Group-by skyline queries find the skyline for each group of tuples. In this paper, we present a comprehensive study on processing group-by skyline queries in the context of relational engines. Specifically, we examine the compo-sition of a query plan for a group-by skyline query and develop the missing cost model for the BBS algorithm. Experimental results show that our techniques are able to devise the best query plans for a variety of group-by skyline queries. Our focus is on algorithms that can be directly implemented in today X  X  commercial database systems without the addition of new access methods (which would require addressing the associated challenges of maintenance with updates, concurrency control, etc.).
 H.2.4 [ Database Management ]: Systems X  query processing, re-lational databases Design, Performance
The skyline operator, which returns a set of tuples not dominated by any other tuple, is important for many multi-criteria decision making applications. Since its introduction in [1], numerous vari-ants of the operator have been proposed. Surprisingly, if we review the original skyline specification [1] (see below), there is one very important and basic part of the specification, hitherto unaddressed: Specifically, the processing of skyline queries with the GROUP BY emphasize that we are not going to duplicate those efforts. Instead, we try to utilize the existing techniques whenever they are applica-ble. However, if any issues remain open, we will provide technical solutions in their respective sections.

In the relational engine setting, the query optimizer needs to per-form cost estimation for different query plans and then select the best one. A recent paper [2] follows this traditional cost-based principle and provides cost estimation equations (i.e., skyline car-dinality, I/O cost, and CPU cost) for two skyline algorithms, Block-Nested-Loop (BNL) [1] and Sort-Filter-Skyline (SFS) [3]. We fol-low this line and devise the cost model for the Branch and Bound Skyline (BBS) algorithm [9], which turns out to be a useful com-ponent for processing our group-by skyline query as well. In fact, we are also aware of recent work on efficient skyline algorithms (e.g., [6, 14]); however, in this paper, we first focus on algorithms that can be directly implemented in today X  X  commercial database systems without the addition of new access methods (which would require addressing the associated challenges of maintenance with updates, concurrency control, etc.).
The R-tree Group-by Skyline Algorithm (RGS) [9] is the only group-by skyline algorithm to date. It operates on an R-tree which is built on all the attributes (grouping and skyline attributes) of a data set D . Unfortunately, the RGS algorithm has several deficien-cies in processing group-by skyline queries such as: (i) huge main memory consumption, (ii) inapplicability for ad-hoc queries, and (iii) performance degradation due to additional dimensions in the R-tree.

The first advantage of forming a query plan with operators over using RGS is that after estimating the cost of each feasible query plan, the query optimizer has the flexibility to choose the plan with the lowest cost to execute [7]. Furthermore, memory thrashing sel-dom occurs because only one group of tuples is retrieved from the preceding grouping operator and it remains within main memory for skyline processing.

To enable the query optimizer to decide which is the best group-by skyline query plan, we provide a comprehensive cost analysis of each implementation.

The traditional query evaluation method combines a grouping operator followed by a skyline operator to form a so-called group-by skyline evaluation plan .

For the grouping operator, we consider using recursive hashing and sorting [4] when no index is available. Additionally, since no index is available, we consider using the BNL algorithm [1] and the SFS algorithm [3] for the skyline operator. In the case of indexed data, we only consider the BBS algorithm [9] as it is I/O-optimal.
Skyline computation is a CPU intensive operation [1, 2], so we consider not only its I/O cost but also its CPU cost (as number of comparisons).

To process group-by skyline queries, we need to invoke a sky-line algorithm over tuples within the same group, for each of the  X  groups returned by the preceding grouping operator. In the follow-ing, we present the I/O cost and CPU cost of BBS [9]. The CPU costs of BNL and SFS are studied by [2] already and are reformu-lated for group-by skyline query processing in [7]. Additionally, the I/O cost for BNL and SFS are also detailed in [7].
The BBS algorithm [8] is an R-tree-based skyline algorithm. It performs in a similar way as the RGS algorithm but is I/O optimal. BBS maintains a heap when traversing the R-tree such that it al-ways evaluates and expands the entry that is closest to the origin among all unvisited entries. Initially, it inserts the root of the R-tree into the heap. Entries in the heap are organized according to the mindist function such that entries that are closer to the origin will be deheaped first. In each iteration, the top element e is de-heaped and examined against the skyline computed so far. If e is not dominated by any current skyline objects, either e is output as a new skyline object (if e is an object) or the child entries of e are in-serted into the heap (if e is an intermediate node). BBS terminates when the heap is empty.

Now we develop the cost model of BBS. For group-by skyline processing, it is necessary to build an R-tree on the attributes of S for each group of tuples produced by the preceding grouping oper-ator. This can be implemented by an R-tree bulk-loading algorithm [5], which uses a space filling curve to sort the data points and then sequentially pack them into the tree. This includes the cost of one external sorting on the data set. As a result, for  X  groups of tuples, the R-tree bulk-loading algorithm incurs an I/O cost R tree load of: Next, we need to estimate the I/O cost and CPU cost of BBS. As each R-tree contains N/ X  tuples, by [13], we derive the height of the tree equal to: d log f N  X  e , where f denotes the average R-tree fanout. For the moment, we consider the nodes at the i th-level (e.g., the leaf nodes are at the 0-th level). By applying the model of [13], the number of nodes in the i -th level equals to N/ ( f i +1  X   X  ) . Let  X  i be the side length of a node in the i -th level. Since the dimensionality of the tree is |S| , the volume of each node is  X  |S| i . Assuming that the nodes at the same level do not overlap, the total volume of all nodes equals 1. Thus, we express  X  i as:
Figure 1 illustrates the R-tree nodes at the i -th level. The side length of each node is  X  i . According to the search order of BBS, the white node will be visited first. The white node contains at least one skyline point, which is guaranteed to dominate any point in any dark-gray node. In other words, all dark-gray nodes will be pruned. However, the light-gray nodes cannot be pruned as they may contain some skyline point. As a result, the BBS algorithm accesses only the white node and light-gray nodes at the level i . Since the dimensionality of the tree is |S| , the fraction of node accesses is equal to: 1  X  (1  X   X  i ) |S| . By summing the above cost
We have carried out experiments on both real data and synthetic data. The real data is the NBA players X  technical statistics from 1946 to 2007. It contains 20,788 tuples in total, where each tu-ple stores the statistics of a player in a season, containing: two attributes for group-by, and eight attributes for skyline. : games played ( gp ), points ( pt ), rebounds ( reb ), assists ( ast ), steals ( stl ), blocks ( blk ), free throws ( ftm ), three-point shots ( tpm ). We con-sidered queries on the following attributes (for synthetic datasets): Ind (independent attributes), Corr (correlated attributes), and Anti (anti-correlated attributes). Unless stated otherwise, each dataset contains N = 100000 tuples, with a total of 20 attributes ( a 1 , a , ... , a 20 ): attributes a 1 ...a 5 are for grouping (with domain size  X  = 5 ), attributes a 6 ...a 20 are for skyline (with domain size  X  = 10000 ). Specifically, a 6 ...a 10 are generated independently, a 11 ...a 15 are correlated to a 6 , and a 16 ...a 20 are anti-correlated to a 6 . By default, each query has |G| = 2 group-by attributes and |S| = 3 skyline attributes.
From the results in all our experiments, RGS was found to incur an extremely high number of I/Os because it demanded an R-tree to be built for all 20 attributes of the dataset and the prohibitive num-ber of entries/data points forced the min-heap to be placed on disk rather in the memory. In the case of anti-correlated datasets, RGS could not terminate within hours. Due to RGS X  X  impracticability, we omit RGS from the subsequent discussion.

In the following, a plan is said to be a winner (or loser) if it incurs the lowest (or highest) cost for the majority of tested cases. Sim-ilarly, we compute the estimated cost of each plan, and determine the winners/losers according to the estimation.
 Effect of the Data Size N . As the CPU cost is independent of the group-by formation, we use *BBS to represent all plans involving BBS (asterisk being a wildcard). Table 2a shows that the estimation is able to predict the winners/losers correctly in 11 out of 12 cases.
Although BBS is I/O-optimal for computing skyline, it does not excel above BNL in the context of group-by skyline query pro-cessing because it requires building R-trees at query-time. How-ever, BBS-related plans are effective in pruning because they are the CPU winners in most cases. For anti-correlated data, *BBS is not able to prune R-tree nodes effectively so its actual CPU cost is very close to *SFS.
 Effect of the Number of Skyline Attributes |S| . In Table 2b, we see that the estimated winners/losers are almost identical to the actual winners/losers. In the case of I/O cost in anti-correlated data, the actual costs of SRT-SFS and SRT-BBS are quite small at large |S| . Thus, the incorrect prediction is not a problem in this case. Effect of the Number of Group-by Attributes |G| . Our tech-nique is able to estimate winners/losers correctly in all tested cases. Results on Real Data. We then investigate the effect of group-by and skyline attributes on the NBA dataset. Our experiments
