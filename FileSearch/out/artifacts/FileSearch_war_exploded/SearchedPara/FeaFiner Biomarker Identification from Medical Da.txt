 Traditionally, feature construction and feature selection are two important but separate processes in data mining. How-ever, many real world applications require an integrated approach for creating, re ning and selecting features. To address this problem, we propose FeaFiner (short for Fea-ture Re ner), an ecient formulation that simultaneously generalizes low-level features into higher level concepts and then selects relevant concepts based on the target variable. Speci cally, we formulate a double sparsity optimization problem that identi es groups in the low-level features, gen-eralizes higher level features using the groups and performs feature selection. Since in many clinical researches non-overlapping groups are preferred for better interpretability, we further improve the formulation to generalize features using mutually exclusive feature groups. The proposed for-mulation is challenging to solve due to the orthogonality con-straints, non-convexity objective and non-smoothness penal-ties. We apply a recently developed augmented Lagrangian method to solve this formulation in which each subprob-lem is solved by a non-monotone spectral projected gradi-ent method. Our numerical experiments show that this ap-proach is computationally ecient and also capable of pro-ducing solutions of high quality. We also present a general-ization bound showing the consistency and the asymptotic behavior of the learning process of our proposed formulation. Finally, the proposed FeaFiner method is validated on Alzheimer's Disease Neuroimaging Initiative dataset, where low-level biomarkers are automatically generalized into ro-bust higher level concepts which are then selected for pre-dicting the disease status measured by Mini Mental State Examination and Alzheimer's Disease Assessment Scale cog-nitive subscore. Compared to existing predictive modeling methods, FeaFiner provides intuitive and robust feature con-cepts and competitive predictive accuracy.
 H.2.8 [ Database Management ]: Database Applications| Data Mining ; J.3 [ Life and Medical Sciences ]: Health, Medical information systems Algorithms Feature generalization, feature selection, sparse learning, aug-mented Lagrangian, spectral gradient descent, biomarkers
Alzheimer's Disease (AD) is a severe neurodegenerative disorder that progresses over time. Electronic Health Records data such as Alzheimer's Disease Neuroimaging Initiative (ADNI) database provide valuable resources for conducting a longitudinal study of AD research. ADNI data are col-lected through regular hospital visits of AD patients after their rst screening. In each visit, various measurements including cognitive scores (tests), lab tests, and brain im-ages are collected for each patient, which serve as a large pool of potential biomarkers. Identi cation of important biomarkers that track the progression of AD is a central task towards a better understanding of the disease and the development of e ective drugs. Many existing works build predictive models [30, 31, 36], perform longitudinal analy-sis [8, 39] and biomaker identi cation [42, 37, 11] directly over the raw features.

One of the fundamental challenges of biomarker identi-cation is the gap between lower level features and higher level clinical concepts. Physicians and healthcare providers think and operate in terms of higher level clinical concepts, while the EHR data are heterogeneous sequences of features in a much lower level of granularities. The low level features are noisy (not all measurements are trustworthy), redun-dant (many features are highly correlated) and sparse (clin-ical events are known to be sparsely populated over time). Because of the above characteristics, the direct use of those low level features are problematic. One important rami ca-tion is the instability of feature selection against such noisy, redundant and sparse feature matrix. With a small pertur-bation of samples or feature values, the results of feature selection may vary signi cantly.

When it comes to the predictive modeling in longitudinal studies and healthcare analysis, the data sources are typi-ca lly high dimensional. For example in the study of AD, popular biomarkers include brain images such as magnetic resonance imaging and positron emission tomography [27, 12], and genome information [14]. To deal with such high-dimensional data, sparse learning methods [32, 15] provide an e ective tool that performs embedded feature selection via sparsity-inducing norms such as  X  1 -norm [2]. Structural sparsity [9, 33, 16, 13] is recently introduced to control the structural patterns of the sparsity, exploring the inherent structures of the predictive modeling problems. The sparse learning has many successful applications in biomedical in-formatics and has produced new medical insights [7, 11, 33, 37, 39, 40, 42].

The  X  1 -norm regularized methods such as Lasso [33] en-joy nice properties in terms of feature selection. However, theories on these methods often heavily lie on assumptions on the design matrix, i.e., the irrepresentable condition [38]. When using Lasso for feature selection in high dimensional problems, strongly correlated features usually result in poor model selection performance [4]. Unfortunately in many clinical studies and healthcare analysis problems this is usu-ally the case; and thus the selected features are usually un-stable under slight perturbations of the data. To deal with this instability problem, speci c sparse learning methods are proposed to identify stable features via a large amount of boostrapping [22, 41], which is usually computationally ex-pensive and cannot completely resolve the problem if the correlation is high. Recently, B  X  uhlmann et al. proposed a two-stage approach that rstly learns feature groups by performing clustering on the design matrix and then per-forms Lasso on the new features constructed from the feature groups. Such two-stage approach has been shown to improve the condition of the design matrix used in the Lasso and is shown to have nice theoretical properties [3]. However, a separate feature group construction and feature selection may lead to suboptimal performance in terms of the stabil-ity of group selection and predictive performance. A more detailed discussion is given in Section 4.2.

Inspired by our experience on clinical predictive model-ing and the aforementioned issues in the approach in [3], we propose an integrated approach, called FeaFiner , for fea-ture construction and feature selection. The FeaFiner simul-taneously generalizes lower level features into higher level clinical concepts and selects the predictive clinical concepts. Speci cally, we propose a formulation for learning a sparse group structure matrix and a sparse prediction model via  X  -regularization, and adopt an ecient block coordinate de-scent algorithm for solving the formulation. In many clinical research applications, non-overlapping groups are preferred for better interpretability. To this end, we further propose an improved formulation that learns non-overlapping fea-ture groups via introducing additional orthogonality con-straints to the formulation. However, the proposed formu-lation is challenging to solve due to the orthogonality con-straints, non-convexity objective and non-smooth penalties. We solve our problem formulation using a novel augmented Lagrangian framework, recently developed in [19]. The key idea there is to solve this non-convex problem by a non-monotone spectral projected gradient method. The result-ing approach is computationally ecient and also capable of producing solutions of high quality. We also present a gener-alization bound showing the consistency and the asymptotic behavior of the learning process of our proposed formulation.
We perform extensive experiments on both synthetic data and real datasets for the clinical studies of Alzheimer's dis-ease. Results show that the proposed approach is capable of learning more stable feature groups than existing approaches while achieving superior predictive performance.
 Notations: The element-wise  X  1 -norm of a matrix X is denoted by  X  X  X  1 = the elementwise non negativity ( X i;j 0 ; 8 i; j ). 1 denotes the all-ones vector whose dimension is clear in context.
In the healthcare analysis and clinical studies, one impor-tant task is to identify important risk factors and biomarkers that relate to a certain disease or health status of inter-est, and build predictive models from patient data. In the studies of Alzheimer's disease, for example, many researches focus on building predictive models that perform early de-tection and identify stable biomarkers that are related to the progression of the disease. Sparse learning is among the most popular techniques that are capable of simultaneously building parsimonious predictive models from training data and perform biomarker identi cation via embedded feature selection.

Consider a prediction task from n subjects with p fea-tures, where each feature is the value of a certain risk factor or the measurement of a biomarker. We denote the patients by data matrix X 2 R n p and their corresponding response by y 2 R n , where the response can be a continuous met-ric that indicates a certain clinical status of the patients. Given the training data X and y we aim to learn a predic-tive model. In this paper we consider only a linear model with a p -dimensional model vector denoted by w 2 R p and the prediction given by ^ y = Xw 2 R n . The classical sparse learning method Lasso [32] learns a sparse model by solving the following  X  1 regularized optimization problem: where 1 is a speci ed parameter that controls the spar-sity of the model. By sparse model we mean there are many zeros in the model vector. If a feature has zero co-ecient in the model, it is considered to be irrelevant to the prediction task and thus can be removed from the model. The  X  1 -norm regularized formulations have been well stud-ied over the last decade and widely used in many medical researches and clinical studies. Though the  X  1 -norm based sparse learning methods yield high predictive power in prac-tice, the learnt models are usually shown to be unstable if the training data is slightly perturbed [22, 3]. To tackle this problem, the authors of [3] proposed to rstly nd corre-lated features via clustering and generate new higher-level features using the clustered groups. Sparse models are then built using the generated features. From the perspective of medical research and clinical studies, this approach is ap-pealing because higher level features generalized from noisy and correlated raw features may be more stable and inter-pretable. It has been shown both theoretically and empiri-cally that this approach gives more stable models. However, a separate feature generalization and selection may be sub-optimal in terms of both predictive performance and quality of the obtained feature groups.
In this paper we propose a formulation that simultane-ously performs feature generalization and selection to im-prove both the predictive performance and group quality. Before proceeding, we introduce some notations that will be used subsequently. For each group, we represent the group assignment information in a vector, and denote the i th group by g i 2 R p . If the j th feature belongs to this group, then the j th component of g i is non-zero and the rel-ative magnitude represents the `importance' of the feature in this group. The new feature generated from this group assignment is thus given by Xg i . Suppose we have k groups of features and we collectively denote the group structure by G = [ g 1 ; g 2 ; : : : ; g k ], and the generalized new features is then given by XG . To make each group meaningful, we require the elements of G to be non-negative. We denote by s 2 R k the new model vector associated with new feature groups. The resulting formulation of FeaFiner is given by: where is a speci ed parameter that controls the sparsity on the columns of G , i.e., the number of features included in a group. In the solutions obtained by solving Eq. (2), the  X  -norm lengths of most columns of G are exactly , pro-viding a good interpretation for the group membership. The approach in [3] is a special case of our formulation in Eq. (2). Note that the elastic net also encourages group e ects such that it tends to assign equal weights to the highly correlated features [43]. However, it cannot explicitly identify feature groups as does in (2).

The optimization problem (2) is generally non-convex since its objective function involves the product of two variables. A local optimal solution is thus often sought. One natural approach to solving problem (2) is by using the block coor-dinate descent algorithm, in which we alternatively solve G and s by xing one variable and optimizing with respect to another. The details are as follows: 1) Given G , we solve s : Solving s is a convex  X  1 -regularized problem, which can be eciently solved via the accelerated projected gradient method (APG) [24, 25]. 2) Given s , we solve G : where G ( ) = f G : G 0 ;  X  g i  X  1 ; i = 1 ; : : : ; k g ing G is a constrained convex optimization problem, which again can also be solved via the APG method. The Eu-clidean projection onto the convex set G ( ) can be eciently solved with a linear time complexity [18]. The overall algo-rithm for solving formulation (2) is presented in Algorithm 1.
The group structure obtained by solving formulation (2) may be largely overlapped because the proposed formulation does not impose any restriction on overlapping among the learnt groups. Nevertheless, in most clinical analysis appli-cations practitioners often prefer less overlapped groups or Al gorithm 1 The block coordinate descent method for solv-ing Eq. (2) ev en mutually exclusive groups, i.e., a particular biomarker should only belong to one feature group. To control the over-laps among groups, we impose the orthogonal constraints g g j = 0 for all i; j in addition to the non-negative con-straint G 0. An immediate consequence of these con-straints is that the resulting group assignments are mutu-ally exclusive. For the simplicity of discussion, we normalize group assignments and assume that the columns of G are of length 1 with respect to  X  2 norm, which together with the orthogonality of the columns of G implies that G T G = I . In addition, we use the  X  1 norm regularization to control the sparsity on G . Our improved formulation of non-overlapping FeaFiner is given by:
We observe that problem (5) is a constrained non-smooth optimization problem, which involves non-trivial constraint G T G = I . It is very natural to apply classical augmented Lagrangian method to solve (5). When applied to (5), aug-mented Lagrangian method needs to solve a sequence of sub-problems in the form of where L is the augmented Lagrangian function de ned by 2 R k k is the Lagrange multiplier and 2 R + is the penalty parameter, and  X  X  X  F is the Frobenius norm. The augmented Lagrangian algorithm framework of solving Eq. (5) is given in Algorithm 2 (e.g., see [26]).

At the k th iteration, the main computational e ort of Al-gorithm 2 lies in solving the augmented Lagrangian sub-problem (6) with = ( k ) and = ( k ) . This subprob-lem can be suitably solved by spectral projected gradient methods that were recently proposed in [35, 19] for solving a class of non-smooth optimization problems over a simple set. The discussion on one of these methods is postponed to Section 3.2.

As observed in our numerical experiment on Algorithm 2 for solving problem (5), the accumulation point of its gen-erated sequence almost always violates some constraints of the problem, especially the orthogonal constraint G T G = I . The similar phenomenon has also been observed in [19] for solving a class of sparse PCA problems with orthogonality constraints. To overcome this drawback, the authors of [19] proposed a novel augmented Lagrangian method. And they showed that every accumulation point of the novel method A lgorithm 2 The classical augmented Lagrangian algo-rithm for solving orthogonal FeaFiner A lgorithm 3 The novel augmented Lagrangian method for solving orthogonal FeaFiner m ust satisfy all constraints of the problem, and moreover under some suitable assumptions, each accumulation point is a KKT point of the problem (see Theorem 3.3 of [19]). It is not hard to verify that our problem (5) satis es all the conditions required in Theorem 3.3 of [19]. Therefore, problem (5) can be suitably solved by the novel augmented Lagrangian method proposed in [19]. We present in Algo-rithm 3 the framework of this method for solving the orthog-onal FeaFiner formulation (5). In contrast to Algorithm 2, Algorithm 3 has two novel features: one is that the La-grangian function is bounded above by along the gener-ated sequence; and another is that the penalty parameter ( k ) grows faster than the magnitude of Lagrange multiplier ( k ) . In our experiment we observe that this novel method can perfectly recover the orthogonal group assignments on G . Similar to Algorithm 2, the major computational part of Algorithm 3 lies in solving the augmented Lagrangian sub-problem (6) with = ( k ) and = ( k ) . We will discuss how to solve such subproblem eciently in Section 3.2. Some other implementation details such as strategies for choosing good starting points, post-processing techniques, and path-wise solutions will be discussed in Section 3.3.
As mentioned above, the major computational part of the novel augmented Lagrangian method for solving problem (5) Figure 1: Illustration of non-monotone objective values in the spectral gradient descent. In the line-search we start from the maximum objective value of the previous n L steps to nd the next step size, where n L is the window within which the objective values are not necessarily monotonically decreasing. lies in solving the subproblem in the form of (6). We now dis-cuss how to solve this subproblem eciently. The two vari-ables G ; s in (6) are coupled and thus bring non-convexity. Traditionally, block coordinate descent (BCD) method can be applied to solve this type of optimization problems [34]. Nevertheless, BCD may be easily trapped in a local mini-mizer in practice due to non-convexity and non-smoothness. Alternatively, we consider the G and s altogether and simul-taneously solve these two variables, in the hope of better exploiting the internal structure of the optimization prob-lem. Due to these considerations, we apply a non-monotone spectral projected gradient (SPG) method that was recently proposed in [19] for solving a class of non-smooth optimiza-tion problems over a simple set including problem (6) as a special case.

To apply the non-monotone SPG method to problem (6), we need the gradient of the smooth term in L which is de-noted by ~ L , that is, ~ L ( G ; s ) = 1 Th e gradient of ~ L with respect to G is given by: r G ~ L ( G ; s ) = r G  X  XGs y  X  2 2 =n  X  ; G T G  X  + 2  X  G and its gradient with respect to s is: r s ~ L ( G ; s ) = r s  X  XGs y  X  2 2 =n = During the line search procedure, we also need to solve the following proximal type  X  1 -regularized problems: min wh ich have closed form solutions given by respectively, where is the element-wise multiplication.
For the SPG method, one important issue is the choice of trial points and step size. Sicne problem (6) is non-convex, regular Armijo-type monotonic decreasing line search strat-egy may be too slow and also easily leads to a local solution. On the other hand, non-monotone line search strategy is more ecient and stable. Indeed, this strategy does not re-quire the monotonic decrease of the objective value between consecutive steps, but rather requires a decrease within a A lgorithm 4 Spectral projected gradient method for solv-ing augmented Lagrangian subproblem (6) A lgorithm 5 Non-Monotone Armijo Line Search for Spec-tral Projected Gradient Method ce rtain number of steps. The concept of non-monotone tech-nique is illustrated in Figure 1. It was shown in [19] that un-der some suitable assumption the non-monotone SPG method has a linear convergence rate. The algorithm for solving Eq. (6) by non-monotone SPG method is presented in Algo-rithm 4, and the associated non-monotone line search sub-routine is given in Algorithm 5.

The major di erence between the spectral projected de-scent and the traditional projected gradient method lies in that: 1) Algorithm 2 requires the starting point to be fea-sible in order to converge according to the theoretical anal-ysis in [19] (We will discuss the starting point strategies in Section 3.3.); 2) the initial step size is related to the ap-proximate second-order information and given by the inverse Rayleigh quotient via Eq. (11). Starting Point. In [4] the clustering assignment is used to combine features. We expect to obtain a fairly good staring point of G from the assignment matrix. We nor-malize the assignment matrix such that the  X  2 norm length is 1 and denote it as G km . Then the starting value of s can be obtained by solving the least squares problem s 0 = arg min s  X  XG km s y  X  2 2 =n , and from s 0 we can obtain G by solving Eq. (4) with G = 0.
 Group Number. In many existing methods, the number of clusters or groups is obtained by either cross validation or domain knowledge. For FeaFiner, a meaningful starting point of G is the k -means clustering assignment matrix, thus we can select k by using heuristics for choosing the cluster number for k -means such as the simple rule information criterion approaches such as AIC/BIC [5]. An alternative way of choosing the group number is by the ex-pected group size, i.e., the number of features in each group, ignoring the sparsity. Given k groups, intuitively the ex-pected group size is p=k . If ne-grained feature groups are needed, then a large k is needed and vice versa.
 Post-Processing. Because of the orthogonality constraint on G , the  X  1 -norm sparsity on G may not behave as in un-constrained optimization problems. Normally we can view the  X  1 -norm regularized problem as an equivalent constrained problem that requires the  X  1 length of columns of G to be less than or equal to a certain value. However, with the or-thogonal constraint in Eq. (5), the  X  2 -norm of columns of G is xed to be 1, which indicates that the  X  1 length is implic-itly lower-bounded. This means the solution G obtained by using Algorithm 2 may have some elements with very small values (e.g., less than 1 e 5) to ensure the unitary. Therefore we can add a post-processing step to set these small values to zeros and normalize the matrix after post-processing to be unitary, and then solve a Lasso problem in Eq. (3) to ob-tain the corresponding s . Also, the post-processed solutions can again be used as the starting point in Algorithm 2 in the hope that a better local solution can be found. Pathwise Solutions. The FeaFiner formulation in Eq. (5) has two sparse parameters G and S , which are typically estimated from data. In order to achieve high eciency, we can obtain pathwise solutions via a successive warm-start strategy : for a xed S , we order a list of g parameter candi-We use f G ; s g S and S , and to compute f G ; s g S the starting point. We nd that not only the pathwise solu-tion strategy delivers higher computational eciency, it also yields solutions that have higher quality than solving Eq. (5) independently for each parameter candidate. This strategy e ectively prevents the algorithm from converging to infe-rior local solution. For convex sparse learning formulations a typical pathwise solution strategy requires a reversed or-der of parameter candidates (from sparse to dense), so that the solution space (from the constrained perspective) is very small at the very beginning to ensure eciency. However, for the non-convex formulation of FeaFiner the pathwise strat-egy is reversed because in the dense case we know that the k-means solution G km serves as a good starting point. In the experiments we use this pathwise strategy for parameter selection.
In this section we provide some theoretical analysis of the proposed FeaFiner method. First we consider the follow-ing constrained reformulation of the FeaFiner with general Lipschiz continuous convex loss function  X  (with Lipschiz constant L ): min wh ere min = k arg min  X  g  X  the regularization parameter G .

Due to the constraint G T G = I and the element-wise non-negativity on G , it immediately follows that Lasso is special case of FeaFiner when k = n , G = I and s is solution of Lasso. We note that the original parameter space of the combined model w = Gs 2 R p is now expanded to a much larger parameter space R p;k R k; 1 , depending on K . The large parameter space may be a concern in practice because it is prone to over tting. With G T G = I , the  X  1 constraint on s provides e ective regularization on the resulting model w = Gs :
We next show that the generalization error of the op-timizer to the problem in Eq. (12) can be bounded and is related to the condition of the design matrix X . Let G and S = f s 2 R k :  X  s  X  1 g . Given any G and s , we denote the expected risk as: Let G and s be the optimal solution that minimizes the expected risk: ( G ; s ) = arg min Also, given data Z = ( X ; y ), the empirical risk is de ned as: and let G ( Z ) and s ( Z ) be the optimal solution that minimizes the empirical risk: The asymptotic convergence of the learning process is given in the following theorem:
Theorem 3.1. Let &gt; 0 and let be probability measure on R d R . With probability of at least 1 in the draw of Z n , we have: wh ere C 1 ( X ) =  X  ^ ( X )  X  := tr is the trace of the empirical covariance matrix, C  X  ^ ( X )  X  1 := max value, and ^ ( X ) is the empirical covariance matrix, i.e., ^
The proof structure is similar to that of [21] and to make the paper self-contained we include the detailed proof in the supplemental materials [1]. This theorem provides impor-tant insight into the proposed formulation in Eq. (5): 1) when n ! 1 , we have E ( G ( Z ) ; s ( Z ) ) E ( G ; s ) converges asymptotically to 0. 2) the convergence is related to the con-dition of design matrix X via C 1 ( X ) ; C 1 ( X ). If the design matrix has a low-rank structure, which gives a small C 1 ( X ), then it achieves fast convergence.
In the synthetic experiment we study the eciency of the proposed non-orthogonal FeaFiner (N-FeaFiner) in Al-gorithm 1 and orthogonal FeaFiner (O-FeaFiner) in Algo-rithm 2, and evaluate the quality of the group structures obtained by the two algorithms.
 Data Generation. We generate the data in the following way. Given the problem size n; p , we rstly generate a block diagonal covariance matrix 2 R k k , with the block size  X  p=k  X  . Within each block, we set the diagonal elements to be 1 and o -diagonal to be 0.9. The design matrix X is then sampled from N (0 ; ). The matrix G 2 R p k is generated according to the group structure de ned by the covariance matrix. Suppose the p features are partitioned into k groups f
I 1 ; I 2 ; : : : ; I k g . The i th column of matrix G is sampled as follows: G i;j U (0 ; 1) if j 2 I i and G i;j = 0 otherwise. s is sampled from N (0 ; 1) with half of the entries set to 0. Finally, we construct the response vector y = XGs +  X  , where  X  N (0 ; 10 3 ).
 Computational Eciency. We rst compare the com-putational cost of N-FeaFiner and O-FeaFiner. We set the precision of the outer-iteration of both algorithms to be 10 and the precision of the inner-iteration to be 10 6 . For N-FeaFiner, the inner-iteration is the  X  1 -regularized/constrained solvers of s and G , and for O-FeaFiner, the inner-iteration solves the SPG. We control the sparsity parameters so that the solutions of the two algorithms have approximately the same density (density is given by the number of non-zero elements divided by the number of total elements, and the density for G and s are 0 : 1 and 0 : 5 respectively). We per-form experiments in three settings: 1) x n = 100 ; p = 500 and vary k = 10 : 10 : 100; 2) x n = 100 ; k = 20 and d = 100 : 100 : 1000; 3) x d = 300 ; k = 10, and n = 100 : 100 : 1000. We repeat these experiments for 100 times and report the average time in Figure 2. We observe that 1) in general O-FeaFiner has advantage over N-FeaFiner in terms of computational cost; 2) the costs of both methods are linear w.r.t. the sample size n ; 3) when increasing the dimensionality of the original feature space p , the costs of both methods increase sublinearly; 4) when increasing the group size k , the time cost of N-FeaFiner increases linearly while for O-FeaFiner the cost increases sublinearly w.r.t. the group number.
 Group Overlap. The major di erence between N-FeaFiner and O-FeaFiner is the non-overlapping constraint on the group structure G in O-FeaFiner. We perform experiments to study the group structures obtained by the two methods. Fi gure 2: Comparison of computational complexity between the non-orthogonal FeaFiner (N-FeaFiner) and the improved orthogonal FeaFiner (O-FeaFiner) with varying sample size (left) n , dimensionality p (middle) and group number k (right). T able 1: The demographic information of the ADNI dataset used in this study.
 S imilar to the previous experiment, we construct a toy data of size n = 50 ; p = 12 ; k = 3. We train predictive mod-els using the two methods and present the learned group structures G in Figure 3. We observe that the O-FeaFiner algorithm can perfectly recover the location of the non-zero elements, while the group structure obtained by N-FeaFiner introduces irrelevant assignments and the groups overlap. Fi gure 3: Comparison of group structures obtained by N-FeaFiner (middle) and O-FeaFiner (right) on the toy data. The O-FeaFiner can perfectly recover the ground truth (left).
In this experiment we apply O-FeaFiner to analyze the feature groups and build e ective predictive models on the ADNI dataset 1 . In the ADNI project, images such as mag-netic resonance imaging (MRI) scans and important cognition-related clinical measurements such as Mini Mental State Ex-amination (MMSE) scores and Alzheimer's Disease Assess-ment Scale-cognitive subscores (ADAS-Cog) are obtained from selected patients repeatedly over a 6-month or 1-year interval. The MMSE scores and ADAS-Cog scores are shown to be correlated with the underlying AD pathology and pro-gressive deterioration of functional ability [28]. Experimental Settings. In this study we perform ex-periments to build predictive models from the 306 low-level features to predict the MMSE and ADAS-Cog scores at fu-ture time points (M06, M12, M24, M36). The prediction of one type of cognitive score at one time point is a regression task, and therefore there are in total 8 regression tasks. The low-level features are extracted from the baseline MRI brain scans of the patients, and a detailed list of the features is given in the supplemental materials [1]. The prediction at a particular time point makes use of all the samples that have the MMSE score at the particular time point as well as the baseline MRI scans. We split the samples into two parts: one third of the samples are served as an independent validation dataset used to estimate the tunable parameters and 90% of the remaining samples are used to build the model and 10% of the remaining samples are used to test the predictive models. The detailed demographic information of the data is given in Table 1. In the experiment we normalize the two Av ailable at http://adni.loni.ucla.edu/ Table 2: Comparison of predictive performance of the proposed approach (O-FeaFiner) and existing approaches (Lasso and CRL) on MMSE and ADAS-Cog prediction in terms of coecient of determi-nation ( R 2 ). Higher R 2 indicates better predictive performance.
 sco res for all the samples such that after the normalization their values are in the range of [0 ; 1]. We randomly split the training and testing data, and repeat the experiment for 10 times. We compare the proposed orthogonal FeaFiner with two baseline methods: To study the e ects of varying group number k , we manually choose three values (12 which is the simple rule of thumb number [20], 30 and 50) of k in CRL and FeaFiner methods and report results independently. We evaluate the three methods in terms of their predictive performance, model stability (for CRL and FeaFiner the models are built using grouped features) and the stability of the learned groups (for CRL and FeaFiner only).
 Predictive Performance. We evaluate the performance of the algorithms by the coecient of determination ( R 2 ) [29], which is widely used in the regression analysis of medical studies. Given the ground truth target vector y and its corresponding prediction ^ y , the R 2 metric is de ned by: R 2 = 1 ements are the mean of y . In Table 2 we report the average experimental results on MMSE and ADAS-Cog prediction in terms of predictive performance. We nd that Lasso and the proposed FeaFiner method achieve high predictive per-formance, while the CRL method does not perform well in most cases.
 Model Stability. To evaluate the stability of models, we de ne the following metric: for each feature f i we use the in-clusion function I ( f i ) to indicate if this feature is included in the model ( I ( f i ) = 1 if the feature is included and I otherwise) and var( I ( f i )) to denote the variance of the in-clusion w.r.t. models obtained from random splittings, and the feature variance is de ned by T able 3: Comparison of model stability of the proposed approach (O-FeaFiner) and existing ap-proaches (Lasso and CRL) on MMSE and ADAS-Cog prediction in terms of feature variance. A lower feature variance indicates that the models are more stable.
 T able 4: Comparison of group stability of the pro-posed approach (O-FeaFiner) and the existing ap-proach (CRL) on MMSE and ADAS-Cog prediction in terms of group variance). A lower group variance indicates that the groups used to generalize features are more stable.
 fea ture is included or excluded by all models of random split-tings, the feature variance is 0. For CRL and FeaFiner we report the variance of the features generated after grouping. The group assignments and models across di erent random splittings are aligned using the best correlation. In Table 3 we report the model stability of all competing methods on MMSE and ADAS-Cog predictions. We nd that the mod-els built by Lasso are not stable while CRL and FeaFiner produce much more stable models when k = 30 and k = 50 (especially at time points M24 and M36). However, using an improper k may yield unstable models for both methods. Group Stability. To evaluate the stability of the learned groups, we de ne the following metric: denote the group assignment vector for group i obtained from the q th ran-dom splitting experiment by g ( q ) i , and the group variance is de ned by: where I is the standard indicator function. The group vari-ance measures how likely the group assignment of one vari-able changes over di erent random splittings. We report the average group stability on MMSE and ADAS-Cog prediction in Table 4. We see that the groups learned by FeaFiner are in general more stable than CRL.
 Table 5: Examples of high-level feature groups ob-tained by the proposed FeaFiner algorithm ( k = 50 ) F eature Group Analysis. We list some examples of high-level feature groups learned by FeaFiner ( k = 50) in Ta-ble 5. A detailed list of feature groups from di erent tasks are available in the supplemental materials [1]. We observe several interesting patterns in the learned groups. First of all we nd that many feature groups exhibit bilaterally sym-metric patterns. For a certain brain area there are two low-level features, i.e., one for the left hemisphere and one for the right. If the feature from one hemisphere is included in a group, then the corresponding counterpart on the other hemisphere is also likely to be included in the same fea-ture group. This agrees with the observations from many medical researches, in which reductions on many bilaterally symmetric brain regions were found in the AD patients [10, 23]. Note that in some groups we also nd interesting asym-metric groups such as MMSE group 3 and ADAS group 2 in Table 5. The asymmetric feature such as Cingulate has been identi ed in some recent studies on asymmetry dis-ease biomarkers [6]. We have several low level features for a particular brain area (e.g., volumes, surface area and cor-tical thickness average/standard deviation). We nd from our experiments that features from the same brain area are likely to belong to the same group (e.g., ADAS Group 3). We also notice that in many feature groups the weights of the low-level features are not equally distributed; for ex-ample in MMSE Groups 1, 3 and ADAS Group 1. This indicates that in the same feature group some features may contribute more to the prediction than the others, and ex-isting clustering-based methods such as CRL are not able to obtain groups of such kind. In this paper we propose an integrated approach called FeaFiner for feature construction by simultaneously identi-fying a feature grouping structure which projects data from a high dimensional feature space to a low-dimensional and interpretable feature space, and learning a sparse model on the low-dimensional space. We propose two formulations: N-FeaFiner for learning overlapped groups and O-FeaFiner for learning mutually exclusive groups. We propose novel al-g orithms for solving the two problems. We have performed extensive experiments on both synthetic and real datasets to evaluate the proposed algorithms, and results show that the proposed method learns clinically meaningful feature groups, and demonstrates promising predictive performance on real medical data sets. One of our future works is to apply the proposed algorithm to other biomedical applications. This work was supported in part by NIH R01 LM010730, NSF IIS-0953662, MCB-1026710, and CCF-1025177.
