 E.1 [ Data Structures ]: Graphs and networks; G.2.2 [ Graph Theory ]: Graph algorithms; I.2.6 [ Learning ]: Classifica-tion Algorithms, Experimentation, Performance Classification, Annotation, Networked Data, Social Networks
The classification of linked semantic data occurs in many different contexts like Web page classification [17], document classification [5], Web spam detection [1], iamge or video an-notation [6, 16], blog labeling [4]. In addition to the content element description, some form of relational information is available, and could be used as a complementary information source for classifying objects. Most existing works in this do-main have considered that elements are connected with only one type of relation like authorship, friendship, etc... There are however many concrete problems for which several types of relations are available and could or must be exploited. Recently some work has started to develop in this direction. For example, [16] exploits the relations between different key frames represented by several modalities to annotate video shots, [15] performs protein classification using multiple pro-tein networks. The main problem with these aproaches is that the content information is handled only through the computation of similarities between elements, and not used directly for classification. Moreover, these methods rely on complex optimization methods that are not well suited for large networks. We consider in the paper the problem of learning to annotate documents in content information net-works, where the elements share multiple relations. This is one of the very first models for graph classification able to learn simultaneously from the content of the elements to be classified and from heterogeneous multiple relations between these elements. Experiments on different datasets allow us to evaluate the ability of the model to deal with a variety of contexts. The model is compared to mono-relational and multi-relational baselines methods and show both its abil-ity to aggregate the information from the different types of relations, and also to efficiently mix the content and the structure for improving the classification.

The paper is organized as follows. In Section 3 we intro-duce a classical transductive framework for learning node scores in mono-relational graph data, in Section 4 we in-troduce the extension to multi-relational data, in Section 5 we present the results of experiments performed on differ-ent datasets and compared to baseline methods. Section 6 presents the related work.
We consider a multi-graph G =( V,E ). It is defined by: Figure 1: Transductive Multi-Relational Graph Label-
We denote V = { v 1 , ...., v N } the set of labeled nodes where N is the number of labeled elements. For all these nodes, y i is a known score associated to node v i . The goal is to automatically compute a score  X  y i for the remaining unlabeled nodes V u = { v N +1 , ..., v N } ,where N u = N is the number of unlabeled elements, using both the content of the nodes and the complex structure of the multi-graph. For v i  X  V , we set by definition  X  y i = y i .

In our transductive context, we consider that the scores of the labeled nodes are known during the whole process.
We introduce below a general baseline transductive clas-sifier for mono-relational graphs. We will use this model for our extension to multi-relational graphs in section 4. One considers that there are a set of labeled nodes and unlabeled nodes available. The goal is to score all unlabeled nodes using the information available from both labeled and unla-beled nodes.

Informally, our baseline operates in two steps. The first step consists in training a classical learning machine over the content of labeled nodes. This content only machine can be any type of classifier such as a perceptron, an SVM or a generative model. Once trained on the labeled nodes, it will be used to compute an initial score,  X  y i , for all the unlabeled nodes of the graph. The second step consists in propagating these sc ores along the edges of the graph under the constraint that the propagated value should remain close to the initial scores computed in the first step. Propagation will smooth the cores so that graph neighbours tend to have similar scores. At the end of the process, the final score of a node is a balance between its content only score  X  y i and the scores of its neighbours. We detail below these two steps and then present in section 4 a model able to learn how to propagate the labels among different types of relations.
Classical propagation models have been developed for mono-relational graphs, i.e. R = 1. Most models [18, 3], rely on a smoothness assumption which considers that two connected nodes should have similar labels. This constraint is usu-ally handled through a regularization term which enforces the smoothness. The loss function for these models has the general form: L reg ( X  y 1 , ...,  X  y N )= 1 where  X  y i is the predicted score of node v i and  X  is a smooth-ness parameter. Here w (1) i,j is the weight of the edge between v i and v j in a mono-relational graph. Term 1 measures the error between the predicted score, and the content-only score on the unlabeled nodes. This term acts as a constraint so that  X  y i remains close to  X  y i . This is a change wrt the clas-sical formulation of transductive graph regularization which only consider propagation (i.e. term 2) and not content clas-sifiers on the unlabeled nodes. term 2 encourages smooth-ness. The final scores of the nodes are obtained through a minimization of this function:
We will describe now a new model able to learn to label in a multi-relational setting. This model learns an  X  X ptimal X  linear combination of the different relation weights. It ex-ploits a specific inference mechanism. We introduce the loss function in section 4.1.1, the inference technique in section 4.2 and the learning algorithm in 4.3.
Instead of using directly the weights of the relations in the loss function as it is done in equation 1 through the w factor, we will use a parametrized function  X   X  ( i, j )  X  defined on each pair of nodes v i , v j where  X  is the set of function in the following. Its parameters will be learned from the data as described in part 4.3. This function will provide normalized weights in [0,1] for the different relation types. The loss function used in this model is L multi ():
L multi ( X  y 1 , ...  X  y N ) has the same form as L reg ( X  y cept that w (1) i,j has been replaced by the  X   X  ( i, j ) function. Let us now describe this edge function.
Let us denote  X ( i, j ) a feature vector that describes the different relations between v i and v j : We define the edge function as a logistic function over the  X vector:
This setting considers that the edge function is defined with one parameter  X  r for each type of relation. This pa-rameter reflects the importance of relation r for propagat-ing the scores over unlabeled nodes. Note that in a single relation setting, it is equivalent to learning the smoothness hyperparameter. The logit function is used here for two rea-sons. First it forces the structural regularization term to be positive, second it prevents the  X  coefficients from growing too much and acts in practice as a smooth constraint on the  X  .
Inference consists in computing the predicted scores  X  y Note that this function only considers the unlabeled nodes, since for the training nodes we have  X  y i = y i .
We propose to solve the minimization of the function through an iterative algorithm based on a coordinate-wise descent method. This method consists in minimizing the L multi function coordinate by coordinate. As L multi () is a convex function w.r.t ( X  y N +1 , ...  X  y N ), the algorithm converges to the minimum of the function. At iteration t , one as to compute the minimum over a particular coordinate k knowing the predicted scores of all other nodes  X  y ( t ) 1 , ...,  X  y This minimum is found by solving the following equation:
This minimization is only computed for the unlabeled nodes and obtained when:
The solution is thus:
At each step, the new  X  y ( t +1) k value is a weighted average of the content value  X  y k and the  X  y i values of its neighbors. Hence, labels propagate via the relations while the content score is always present in the score combination. Note that the the algorithm uses a directed graph formulation (the two arcs ( k,i )and( i, k ) appear in the expression,) so that both undirected or directed edges could be used. The algorithm then consists in iterating equation 9 for any unlabeled node v k until convergence.
The goal here is to learn the parameters  X  of the edge function. We consider that the content only parameters have been previously learned. Let us define  X ( X  y i ,y i ), the cost of predicting score  X  y i instead of y i using the proposed inference procedure. The empirical cost is defined over the labeled nodes as: where  X  y  X  k is obtained by using the inference procedure i.e:
This function is difficult to minimize because it involves the minimization of an argmin function.

The structural parameters  X  are learned on the labeled data. From the labeled subgraph, we generate one learning example for each node: it is composed of this node and its neighbours nodes. We then fit the  X  parameters on these examples.

For labeled nodes, we would like to predict scores  X  y ( t as close as possible to the true score y k of v k . X  y ( t puted through the inference procedure, will depend on the  X  parameters (equation 9). We can optimize the  X  values to obtain predicted scores close to real scores. Learning  X  then consists in minimizing the prediction loss  X  defined for each labeled node v k :
This loss can be minimized by a classic gradient descent algorithm. We consider in this paper the use of a classical square-error loss for  X .
The complexity of our algorithm is : O ( N + N e + N u .N O ( N u .N e )since N e &gt;&gt; 1and N u &gt;&gt; 1. A mono-relational model (Step 1 and Step 3 only) has a complexity of O (( N + N u .N e ) .R )= O ( N u .N e .R ) (one model per relation). Our method is of the same order of practical complexity than mono-relational models and is faster in pratice. In the multi-class and multi-label settings, we learn one binary model per class for multi-relationnal and mono-relationnal algorithms. This multiplies both complexities by the number of classes, with the same analysis than above.
We have performed experiments on four datasets:
Table 1 gives some basic statistics over the different cor-pora. While the presented model computes scores to the nodes of a binary multi-graph, we learn here one scoring model for each possible category of the classification prob-lem. In the case of mono-label classification, the category with the higher score is assigned to the node. In multi-label, the scores of all the categories for each node are used to compute a precision @ n measure as explained below.
In order to compare our approach with existing multi-relational methods that have been developped for structure only networks 1 , we have performed two sets of experiments:
We divide our dataset in a training and a testing set. For the multiclass datasets, the results are evaluated using the classical micro-F 1 ( miF 1 ) -and macro-F 1 ( maF 1 ) measures. For the multi-label problems, we have also used the micro-F and macro-F 1 by assigning a node to a category if its score is greater than 0. In order to evaluate how good the label rank-ing is for each node of the graph, we have also computed the precision at 1 (P@1) and precision at 3 (P@3) over the or-dered list of labels. Three runs have been launched for each experiment and the results presented here are the average over these three runs. We have reported the best results with respect to the best  X  value.
Structure only experiments have been mainly done in or-der to compare our approach with state-of-the-art methods for multi-relational graph categorization. Results of the dif-ferent models are presented in Table 2. First, one can see that the performance of the mono-relational model mainly depends on the type of relation used. The Kato model clearly outperforms the mono-relational models obtaining about 60% micro-F 1 for the same corpus. Depending on the
These methods usually include any content information through the computation of similarity relations Table 3: Performance of the different model on the dataset and on the training size, our approach often achieves at least as well as the Kato model, particularly when the number of training nodes is not too small. This is partic-ularly true for the Flickr datasets where our model have a better precision at 1 and precision at 3, particularly when the size of the training set is small. These experiments show that the proposed algorithm learns well how labels do prop-agate. In this case, the multi-relational model is able to take advantage of the additional information brought by multiple relations and significantly outperforms other models.
The results of the Structure and Content experiments are reported in table 3 for the Cora and the Email datasets. The results on Flickr are not illustrated here because the content of these datasets is not relevant enough for content-based classification and complex image classification techniques only achieves a performance close to the random model. From this table, on can see first that the simultaneous use of content and structure information greatly improves the performance of the different approaches. In the Cora cor-pus, our method also outperforms the best Mono-Relational model showing the ability of the SC-Multi model to both mix the information contained in the different types of relations, but also aggregate this information with the content of the nodes of the network. The performance increase with the training set size is quite intuitive since the multi-relational model being a little more complex needs more data to gener-alize well. The performance varies according to the corpus. For the e-mail corpus, the multi-relational model is equiva-lent to the best mono-relational model for most experiments. This is certainly due to the fact that the Authorship relation (SC-Mono 1) is clearly dominant over the other relations.
Most work in this domain concerns learning in mono-relational graphs. As introduced before, there are from two main perspectives on this problem. The transducive ap-proach followed in this paper, has been initially motivated by the semi-supervised learning problem and initiated by Zhou [18] and Belkin [3]. All these models rely on the minimization of an objective function combining a classical classification loss function on labeled data and a smooth-ness term like L = of node i and  X  i,j is the weight of the edge between i and j . Note that [18] presents an interesting relation between regularized methods and a random walk formulation in the case of binary label propagation. The models are generally developped for undirected graphs, but some extensions for directed graphs have been proposed, e.g. [19]. The first general model that directly combines a content and a rela-tional classifier is he recent paper [1]. Besides classification, graph models have been used for ranking [2] or re-ranking [13] for ad-hoc search and topic distillation. For the in-ductive setting, collective classification models are generally used. They can handle both content and structure infor-mation. These models include Iterative Classification [14], Gibbs Sampling [10], Stacked Learning [9] and SICA [11].
Only a few works have considered multi-relational data and Multi-Graphs. [7] uses a weighted combination of ker-nels on relations. This approach produces a fully-connected graph, i.e. a n 2 complexity which is prohibitive for real data. [4] considers blog labeling in a multigraph setting where the weights are uniform and are not learned. [20] extend the ideas introduced in [19] to multigraphs. Here again, the model complexity is an issue. In an inductive context, dif-ferent from the one considered here, [12] proposes a collective classification method for the annotation of multi-relational networked data. Kato et al. in [8] propose a simple EM-like algorithm that has been used as a baseline model in our ex-periments. Basically, the model learns to weight the different types of relations alternating node scores and edge weights optimization. They also provide a bayesian probabilistic in-terpretation of their model. Wang et al. in [16] propose a similar multi-graph regularization model for video annota-tion. Here again, and alternate EM like approach is used to learn a linear combination of graph laplacians. One key difference is that our model directly employs during training and inference both a node content classifier and a propaga-tion along relations. Moreover, we do not need to set a prior on edge weights distribution, then the proposed model does not need an expensive hyperparameter search. Combined with the fact that we do not use alternate optimization pro-cedures, the proposed method is considerably faster than state-of-the-art ones. Another advantage is that it may be used for both directed and undirected graphs. Experiments show that most of the time, the proposed model is as good as or outperforms the EM-like baseline.
We have proposed a model for multi-class, multi-label classification in multi graphs. This model relies on an orig-inal inference and learning algorithm. It includes as special cases different previously proposed mono-relational models. Moreover, in comparison to existing multi-relational meth-ods, it considers both the structure and the content of multi-graphs and learning the weights of the different types of relations is made in one step instead of using alternate op-timization techniques. Experiments performed on different datasets show the model X  X  ability to extract relevant infor-mation from the rich relational information and to outper-form baseline methods. Perspectives now lie in different ex-tensions of this model and to an in depth analysis of its behavior. This work was partially supported by the French National Agency of Research -ExDeus/Cedres, ANR-09-CORD-010-04, Projects. [1] J. Abernethy, O. Chapelle, and C. Castillo. Witch: A [2] S. Agarwal. Ranking on graph data. In ICML , pages [3] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold [4] S. Bhagat, G. Cormode, and I. Rozenbaum. Applying [5] M. Bilgic, G. M. Namata, and L. Getoor. Combining [6] L. Cao, J. Luo, and T. Huang. Annotating photo [7] H. Chen, L. Li, and J. Peng. Error bounds of [8] T. Kato, H. Kashima, and M. Sugiyama. Integration [9] Z. Kou and W. Cohen. Stacked graphical models for [10] S. Macskassy and F. Provost. A simple relational [11] F. Maes, S. Peters, L. Denoyer, and P. Gallinari. [12] S. Peters, L. Denoyer, and P. Gallinari. Iterative [13] T. Qin, T.-Y. Liu, X.-D. Zhang, D.-S. Wang, W.-Y. [14] P. Sen, G. Namata, M. Bilgic, L. Getoor, and [15] K. Tsuda, H. Shin, and B. Sch  X  olkopf. Fast protein [16] M. Wang, X.-S. Hua, R. Hong, J. Tang, G.-J. Qi, and [17] T. Zhang, A. Popescul, and B. Dom. Linear prediction [18] D. Zhou, O. Bousquet, T. Lal, J. Weston, and [19] D. Zhou, J. Huang, and B. Sch  X  olkopf. Learning from [20] D. Zhou, J. Huang, and B. Scholkopf. Learning with
