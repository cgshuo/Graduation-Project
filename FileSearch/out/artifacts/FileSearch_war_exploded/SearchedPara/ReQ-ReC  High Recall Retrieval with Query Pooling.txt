 We consider a scenario where a searcher requires both high precision and high recall from an interactive retrieval pro-cess. Such scenarios are very common in real life, exemplified by medical search, legal search, market research, and litera-ture review. When access to the entire data set is available, an active learning loop could be used to ask for additional relevance feedback labels in order to refine a classifier. When data is accessed via search services, however, only limited subsets of the corpus can be considered X  X ubsets defined by queries. In that setting, relevance feedback [17] has been used in a query enhancement loop that updates a query. We describe and demonstrate the effectiveness of ReQ-ReC (ReQuery-ReClassify), a double-loop retrieval system that combines iterative expansion of a query set with itera-tive refinements of a classifier. This permits a separation of concerns: the query selector X  X  job is to enhance recall, while the classifier X  X  job is to maximize precision on the items that have been retrieved by any of the queries so far. The overall process alternates between the query enhancement loop (to increase recall) and the classifier refinement loop (to increase precision). The separation allows the query enhancement process to explore larger parts of the query space. Our ex-periments show that this distribution of work significantly outperforms previous relevance feedback methods that rely on a single ranking function to balance precision and recall. H.3.3 [ Information Search and Retrieval ]: Relevance Feedback Algorithms, Experimentation Relevance Feedback; Query Expansion; Active Learning Cheng Li and Yue Wang contributed equally to this work.
We are witnessing an explosive growth of text data in many fields, including millions of scientific papers, billions of electronic health records, hundreds of billions of microblog posts, and trillions of Web pages. Such a large scale has cre-ated an unprecedented challenge for practitioners to collect information relevant to their daily tasks. Instead of keeping local collections of data related to these tasks, many users rely on centralized search services to retrieve relevant infor-mation. These services, such as Web search engines (e.g., Google), literature retrieval systems (e.g., PubMed), or mi-croblog search services (e.g., Twitter search API, Topsy) typically return a limited number of documents that are the most relevant to a user-issued query. These existing re-trieval systems are designed to maximize the precision of top-ranked documents; they are good at finding  X  X omething relevant, X  but not necessarily everything that is relevant.
We focus on scenarios where a user requires a high recall of relevant results in addition to high precision. Such scenarios are not uncommon in real life, exemplified by social search, medical search, legal search, market research, and literature review. For example: a social analyst needs to identify all the different posts in which a rumor spreads in order to re-construct the diffusion process and measure the influence of the rumor; a physician needs to review all the patients that satisfy certain conditions to select cohorts for clinical trials; an attorney needs to find every piece of evidence related to her case from documents that are under legal hold; a scien-tist does not want to miss any piece of prior work that is related to his ongoing research. We denote all these tasks generically as  X  X igh-recall X  retrieval tasks.

Finding a needle in a haystack is hard; finding all the nee-dles in a haystack is much harder. Existing retrieval systems do not naturally meet this type of information need. To con-duct a comprehensive literature review using a search engine, we have to submit many alternative queries and examine all the results returned by each query. Such a process requires tremendous effort of the user to both construct variations of queries and examine the documents returned.

This high-precision and high-recall task becomes substan-tially harder as the collection grows large, making it impossi-ble for the user to examine and label all the documents in the collection, and impractical even to label all the documents retrieved by many alternative queries. In some contexts such as e-discovery, a computer-assisted review process has been used that utilizes machine learning techniques to help the user examine the documents. Such a process typically casts high-recall retrieval as a binary classification task. At the beginning, the user is required to label a small sample of doc-uments. A classifier trained using these labeled documents then takes over and predicts labels for other documents in the collection. An active learning loop can be used to ask for additional relevance labels in order to refine the classifier. These methods, however, require that the user has access to the full collection of documents and that it is feasible to execute her classifier on all the documents.

In other scenarios, the users either do not own the collec-tion or it is too large, so they can only access documents in the collection through an external search service. This makes it unrealistic to either examine or classify the entire collection of documents. Instead, only limited subsets of the document corpus can be considered, subsets defined by queries.

Existing retrieval systems are not tuned for high-recall re-trieval on the condition of limited access to the data via search services. In most cases, a system only aims to max-imize the precision in the documents that are retrieved by the current query. Relevance feedback has been used in a query enhancement loop that updates a query. Many search engines provide services to collect explicit and/or implicit feedback from the users or to suggest alternative queries to the users. These practices typically generate a new query that replaces the old one, which is expected to improve both precision and recall. Once a new query is issued, the results retrieved by the old queries are forgotten, unless they are manually harvested by the user.

We study a novel framework of retrieval techniques that is particularly useful for high-recall retrieval. The new frame-work features a ReQ-ReC (ReQuery-ReClassify) process, a double-loop retrieval system that combines iterative expan-sion of a query set with iterative refinements of a classifier. This permits a separation of concerns, where the query gen-erator X  X  job is to enhance recall while the classifier X  X  job is to maximize precision on the items that have been retrieved by any of the queries so far. The overall process alternates between the query expansion loop (to increase recall) and the classifier refinement loop (to increase precision). The separation of the two roles allows the query enhancement process to be more aggressive in exploring new parts of the document space: it can explore a non-overlapping portion of the corpus without worrying about losing the veins of good documents it had found with previous queries; it can also use queries that have lower precision because the clas-sifier will weed out the misses in a later stage. Our ex-periments show that this distribution of work significantly outperforms previous relevance feedback methods that rely on a single ranking function to balance precision and recall. The new framework also introduces many opportunties to investigate more effective classifiers, query generators, and human-computer interactive algorithms for labeling subsets, and especially to investigate what combinations work best together.

Unlike Web search engines that target users who have real-time, ad hoc information needs, the ReQ-ReC process targets users who care about the completeness of results and who are willing to spend effort to interact with the system iteratively and judge many (but not all) retrieved documents. The process has considerable potential in appli-cations like social media analysis, scientific literature review, e-discovery, patent search, medical record search, and mar-ket investigation, where such users can be commonly found.
The rest of the paper is organized as follows. We discuss related work in Section 2. Section 3 gives an overview of the ReQ-ReC double-loop framework and its key components. Section 4 describes several instantiations of the framework. Section 5 provides a systematic evaluation of the proposed methods. Finally, we conclude the paper in Section 6.
The ReQuery-ReClassify framework integrates and extends two well-established  X  X uman-in-the-loop X  mechanisms: rele-vance feedback in information retrieval, and active learning in text classification.

Relevance feedback was shown long ago to be effective for improving retrieval performance [17]. In a feedback pro-cedure, the retrieval system presents the top-ranked docu-ments to the user and collects back either explicit judgments of these documents or implicit feedback implied by certain actions of the user [9, 19]. The system then learns from the collected feedback and updates the query. The new query re-flects a refined understanding of the user X  X  information need [15, 28], which improves both precision and recall in the next round of retrieval. Even without real user judgments, retrieval performance may still benefit from simply treating the top-ranked documents as relevant, which is known as a process of pseudo relevance-feedback [1].

In a search session, relevance feedback can be executed for multiple rounds. Harman [8] studied multiple iterations of relevance feedback, and found that retrieval performance is greatly improved by the first two to three iterations, after which the improvements became marginal. Multiple itera-tions of relevance feedback have received more attention in content-based image retrieval [3, 16, 30].

In complicated search tasks, the user is often involved in a search session consisting of a series of queries, click-throughs, and navigation actions. Session -based retrieval aims at learning from these signals in order to better un-derstand the user X  X  information need, thus improving the relevance of results when the user issues the next query [19, 14]. Instead of improving the performance of the next query, ReQ-ReC aims to maximize the recall of the results collec-tively retrieved by all the queries in the search session. Like traditional iterative relevance feedback, the ReQ-ReC process also adopts multiple iterations of user interac-tion. Indeed, as shown in Section 3, iterative relevance feed-back is a special case instantiation of the ReQ-ReC frame-work. Instead of replacing the old query with a new query, however, ReQ-ReC can accumulate documents retrieved by any of the queries issued so far. By doing this, rather than optimizing both precision and recall through the choice of a single query, we place the burden of maximizing precision on a classifier, and new queries can be dedicated to improving only recall.

When it is feasible to process the entire collection of doc-uments, the problem of high-recall retrieval can be cast as a binary classification problem where the positive class cap-tures documents that are relevant to the information need and the negative class captures the rest. The practice of rel-evance feedback essentially becomes an active learning pro-cess, in which the system iteratively accumulates training ex-amples by selecting documents and asking the user for labels [18]. This strategy is commonly used in computer-assisted reviews for e-discovery, often referred to as the process of  X  X redictive coding X  [13]. Different active learning algorithms use specific strategies for selecting the documents to label, many of which attempt to maximize the learning rate of a  X  X ase X  classifier with limited supervision [18]. For text clas-sification, a popular choice of such a  X  X ase X  classifier is the support vector machine (SVM) [2]. Using SVM, a variety of document selection strategies have been explored. Tong and Koller [23] proposed to select documents closest to the decision hyperplane in order to rapidly shrink the version space and reduce model uncertainty. In contrast, Drucker et al. [4] selected documents with highest decision function values to avoid probing the user with too many non-relevant documents. Xu et al. [27] mixed these two strategies and achieved better retrieval performance.

Like active learning, the ReQ-ReC process also trains a bi-nary classifier. The major difference is that ReQ-ReC does not require knowledge about the entire document collection and thus does not classify all documents. Instead, it starts from a limited subset defined by the original query and ac-tively expands the space. This is a huge gain, as text classi-fication and active learning are usually computationally pro-hibitive for modern IR collections containing a large num-ber of documents. Indeed, previous studies that apply active learning to retrieval can only evaluate their approaches using moderate-scale collections (such as the 11,000-documents Reuters collections used in [4] and [27]), or only focus on the documents retrieved by one query (top 100 documents in [26] and top 200 in [22]). Given its big advantage in ef-ficiency, the ReQ-ReC process could potentially provide a new treatment for active learning, especially when the data collection is large and the positive class is very rare.
The idea of active learning has also been applied to rele-vance feedback for retrieval. Shen and Zhai [20] studied ac-tive feedback, where the system actively selects documents and probes the user for feedback instead of passively pre-senting the top ranked documents. It is shown that se-lecting diverse top-ranked documents for labeling is desir-able, since it avoids asking for labels on similar documents and thus accelerates learning. Xu et al. [26] improved this heuristic by jointly considering relevance, diversity, and den-sity in selected documents. Both techniques exploit density information among top-ranked documents, and select rep-resentative ones for feedback. Recently, Tian and Lease [22] combined uncertainty sampling ( Simple Margin ) and density-based sampling ( Local Structure ) in iterative rele-vance feedback to minimize user effort in seeking several to many relevant documents. The difference between our work and theirs is articulated by the difference between the ReQ-ReC process and relevance feedback described above: the addition of a classifier and use of results from all queries allows more aggressive exploration of alternative queries.
In this section, we introduce the general ReQuery-ReClassify (ReQ-ReC) framework, including its key components. Spe-cific instantiations of the framework will be discussed in the next section. The basic idea of the framework is to distribute the burden of maximizing both the precision and recall to a set of queries and a classifier, where the queries are responsi-ble for increasing the recall of relevant documents retrieved and the classifier is responsible for maximizing the precision of documents retrieved collectively by all of the queries in the set. The framework features a double-loop mechanism: the inner-loop classifies the retrieved documents, actively collects user feedback, and improves the classifier (ReClas-sify); the outer-loop generates new queries (ReQuery), issues API calls, and iteratively adds newly retrieved documents into the workset. In the rest of the paper, we refer to the framework as  X  X eQ-ReC X  or  X  X ouble-loop X  interchangeably.
The ReQ-ReC framework can be viewed as a double-loop review process, as illustrated in Figure 1. The process main-tains a set of queries, a pool of retrieved documents, and a binary classifier. With an initial query composed by the user, the system retrieves an initial set of documents using a search service. An inner-loop starts from there, in which the system iteratively presents a small number of documents (e.g., 10) selected from the current pool of retrieved docu-ments to the user and asks her to label them as either rele-vant or not. The classifier is consequently updated based on the accumulated judgments of the user, which is then used to reclassify the pool of documents. After a few iterations of the inner-loop, the the classifier X  X  predictions stabilize. At this point, the inner-loop will suspend. The system then proposes to add a new query to the query set, aiming to retrieve more relevant documents from the collection. Upon the approval X  X nd possible edits X  X f the user, the system will retrieve a new set of documents using the new query, and merge them into the pool of retrieved documents. The requery process makes up one iteration of the outer-loop of the framework. After new documents are retrieved and added into the pool, the system starts a new inner-loop and continues to update the classifier left from the last iteration. The whole review process will end when no more relevant documents can be retrieved by a new query or when the user is satisfied.

Another way to look at the framework is to imagine a search process in the information space (e.g. a vector space of documents and queries), as illustrated in Figure 2. The system interacts with the user as it navigates through the information space, aiming to delineate a manifold that con-tains as many relevant documents and as few non-relevant documents as possible. Each query can only reveal a small region of the information space that surrounds it. The  X  X irst guess X  on such a manifold is, of course, the region surround-Figure 2: A double-loop process of search in the in-formation space. (a) Each query only retrieves its surrounding region under inspection. (b) The inner-loop updates a classifier that refines the boundary between relevant and non-relevant documents. (c) The outer-loop expands the subspace which includes more relevant documents. ing the initial query. A classifier clarifies the boundary of the manifold (to maximize precision), which is iteratively refined with newly labeled data points selected from the revealed re-gions. To explore other regions in the space so as to expand the relevant manifold (to maximize recall), the system will estimate a promising direction and will make a new query to move in that direction into the uncharted space. This new region and all previously unveiled regions are combined as the current search space, in which the system continues to refine the boundary of the relevant manifold. The search process will end if the relevant manifold stops expanding, or if the user decides to terminate early.

From this perspective, each query contributes a new re-gion to the search space without giving up any already dis-covered regions. Such a pure  X  X xpansion X  of the search space will include many non-relevant documents, but the classifier is able to filter the non-relevant documents at the end and recover the true boundary of the relevant manifold. By con-trast, in a relevance feedback procedure, every new query will  X  X edefine X  the search space as the region surrounding the new query. Given a good query, this region indeed contains fewer non-relevant documents than our  X  X xpanded X  search space (i.e., achieves a higher precision), but it is also likely to contain fewer new relevant documents. In relevance feed-back, the challenge is to find a new query that both retrieves the relevant documents from the old query and also retrieves new ones. In ReQ-ReC, the challenge is simply to find a query that retrieves new relevant documents.
Given the high-level intuitions of the ReQ-ReC frame-work, we now discuss the key components in the double-loop. To facilitate the discussion, we introduce the notations in Table 1 and summarize the framework in Algorithm 1.
The ReQ-ReC framework assumes neither ownership nor full access to the document collection, but instead relies on a standard search service to retrieve documents from the index. The retrieval service X  X  ranking function can use any reasonable retrieval model that takes the input of a query q and outputs a certain number of ranked documents from the index (e.g., using a vector space model, a language modeling approach, or a boolean retrieval model). In most cases, the user has no knowledge about the algorithm that is employed by the external search service. In that case, the retrieval function is treated as a black box in the framework.
After each search process the retrieved documents will be merged into the pool of unlabeled documents D q , which ex-pands the workset for document selection and classification. Algorithm 1 The double-loop process Input: Initial query q 0 , index of document collection D Output: A set of labeled documents D l and a set of un-1: D q  X  X  X  2: D l  X  X  X  3: repeat // outer loop 4: D q  X  retrieve ( D ,q i )  X  X  q 5: repeat // inner loop 6: if D l ==  X  then 7: D s  X  selectK ( D q ) 8: else 9:  X  A  X  train A ( D q , D l ) 10: D s  X  selectK ( X  A , D q ) 11: end if 12: D l  X  X  l  X  label ( D s ) 13: D q  X  X  q  X  X  s 14:  X  R  X  train R ( D q , D l ) 15: predict ( X  R , D q ) 16: until meet stopping criteria for inner loop 17: q i +1  X  query ( { q i } , D q , D l ,  X  A ,  X  R ) 18: until meet stop criteria for outer loop
In every iteration of the inner-loop, during steps 6-10 of the algorithm the system selects K (e.g., 10) documents D from the pool of retrieved documents that are yet unlabeled, D , and asks the user for judgments. At the beginning of the double-loop process, where there are no judged documents, this process can simply return the top documents ranked by the retrieval function, select a more diverse set of docu-ments through an unsupervised approach, or even randomly sample from D q . Once labeled documents have been accu-mulated, the process is able to select documents based on an active learning strategy. Such a process aims to maximize the learning rate of the classifier and thus reduce the user X  X  effort on labeling documents.
Given an accumulated set of labeled documents, the clas-sification component learns or updates a binary classifier (i.e.,  X  R ) at step 14 and reclassifies documents from D step 15. Any reasonable classifier can be applied here.
In many high-recall retrieval tasks such as medical record search, it is important to find all patients that  X  X atch X  cer-tain conditions, but it is not necessary to rank the records identified as relevant [7]. In those cases, the labels of doc-uments in D q can be directly predicted by the classifier. In cases where ranking is desired, documents in D q and D l can be ranked/reranked using either the confidence values or the posterior probabilities output by the classifier, or by using an alternative machine learning method such as a regression or learning-to-rank model.
When the classifier appears to be achieving a stable pre-cision on the current workset of documents D q , the system proceeds to expand D q in order to increase the recall. This is done through constructing a new query (step 17) and re-trieving another set of documents through the search ser-vice. Any reasonable query expansion method can be ap-plied here, including the classical relevance feedback meth-ods such as Rocchio X  X  [15] or model-based feedback [28]. Other query reformulation methods can also be applied, such as synonym expansion [24] and semantic term matching [5].
Stop criteria of the inner-loop : new labels stop being requested when either of the following conditions is met:
Stop criteria of the outer-loop : new queries stop being submitted when either of the following conditions is met:
The key components of the general ReQ-ReC framework, document selection, classification, and query expansion can be instantiated in many ways. To illustrate the power of the framework, we describe five instantiations, beginning with iterative relevance feedback as a degenerate form and pro-gressively substituting elements that take greater advantage of the broader framework. Section 5 will provide perfor-mance comparisons of these instantiations.
Interestingly, an iterative relevance feedback process can be interpreted as a special case of the ReQ-ReC framework, if both the classification component and the document se-lection component simply adopt a ranking function that is based on the current query, q i . More specifically, define  X  to classify a document as relevant if it is in retrieve ( D ,q and define  X  A to always select the next highest ranked un-labeled item from retrieve ( D ,q i ). There is no difference in whether the results retrieved by the previous queries are kept in the document pool D q or not, if the results are eventually ranked based on the last query, q i .

Note that many query updating methods (in the context of relevance feedback) can be applied to generate the new query at each iteration. To establish a baseline for per-formance comparison, we choose Rocchio X  X  method [15], by which the next query is selected according to Equation 1: where ~q 0 is the original query vector, D r and D nr are the set of known relevant and nonrelevant documents, and  X  ,  X  , and  X  are parameters. The basic idea of Rocchio X  X  method is to learn a new query vector from documents labeled as positive or negative, and then interpolate it with the original query vector. When the parameters are well tuned, this achieves performance comparable to alternatives such as model-based feedback [28] and negative feedback [25].
The next two instantiations modify the relevance feed-back process by introducing a separate classifier,  X  R , rather than using the retrieval function as a degenerate classifier. This classifier is involved to maximize the precision of labels for D q . Here, keeping the documents retrieved by previous queries does make a difference, because  X  R will operate at the end to rank all of the results from all of the queries.
Any machine learning-based classifier, as well as any rea-sonable selection of features, can be used to identify relevant documents in D q . We adopt the support vector machine (SVM) [2] with unigram features and linear kernel. In cases where a ranked list of documents is desired, documents in D q are ranked by the score of the decision function w T x + b output by linear SVM.

We call this second instantiation of ReQ-ReC Passive . It is passive in the sense that the classifier is not used to control the interactive process with the user; we still choose the top-ranked documents for labeling and use Rocchio X  X  method of query expansion, as in our iterative RF instantiation. By comparing the performance of passive and the Iterative RF baseline, we can determine the effect of the classifier acting solely as a post-hoc reranking function.
Note that in Rocchio updating, the parameter that inter-polates the new query vector with the original query is quite sensitive. This is because when one relies on the query to maximize both precision and recall, the expansion has to be conservative so that the new query does not drift too far from the original query. When the burden of maximizing precision is transferred from the query to the classifier, we anticipate that this interpolation should become less critical. To test this, we introduce another simple instantiation by re-moving the original query vector (i.e., the ~q 0 component in Equation 1) from Rocchio, by setting  X  = 0. Note that this is a rather extreme case for test purposes. In reality, keep-ing closer to the original query may still be important even for the purpose of increasing recall. We call this instantia-tion Unanchored Passive , because the updated queries are no longer anchored to the initial query.
Next, we consider an instantiation of RecQ-ReC that makes use of the classifier to select documents for labeling in the inner loop. As before, we train the classifier using SVM. We select documents for labeling using uncertainty sam-pling [23], a simple active learning algorithm that selects examples closest to the decision hyperplane learned by the classifier. In each inner-loop iteration, we present to the user ten documents that are the most uncertain by the current classifier. Specifically, five are chosen from each side of the hyperplane. We call this instantiation Active because the classifier is active in choosing which documents to label.
Note that after the very first search process, the system has no labeled documents in the pool. A classifier cannot be trained and thus the uncertainty sampling cannot be ap-plied. At this cold start , we simply select the top 10 doc-uments returned by the search service as the first batch of documents to request user judgments.

As uncertainty-based active learning gradually refines the decision boundary of the classifier, every new query to the search service may affect its performance. This is because a new query expands the pool of documents D q with newly retrieved documents, which might dramatically change the distribution and the manifold of data in the search space. At this point, instead of gradually refining the old decision boundary, the classifier may need a bigger push to quickly adapt to the new distribution of data and approach the new decision boundary. In other words, it is important for the classifier to quickly explore the newly retrieved documents. Therefore, in the first inner-loop iteration after each new query brings back new documents, we select top ranked doc-uments for labeling instead of the most uncertain ones. Un-certain ones are picked in the following inner-loop iterations.
The final instantiation we consider modifies the query ex-pansion algorithm used in the Active instantiation. Pre-viously, we considered an unanchored version of Rocchio X  X  method of selecting the next query. Here, we consider a different modification of Rocchio X  X  method.

To maximize recall, we naturally want a new query to re-trieve as many relevant documents as possible. Even more importantly, these relevant documents should overlap as lit-tle as possible with the documents retrieved by previous queries. In other words, a new query should retrieve as many new relevant documents as possible.

Our idea is inspired by the theory of  X  X eak ties X  in soci-ology [6]. While strong ties trigger social communication, weak ties can bring in novel information. If we think of the top-ranked documents in a retrieved list as  X  X trong ties X  to the query, we can think of the lower-ranked documents as  X  X eak ties. X  We thus exploit documents that are judged as relevant, but ranked lower in the list returned by the search service. These documents are likely to act as bridges to expand the search space into other clusters of relevant documents.

Are there many such documents? In a relevance feedback process, there might be few, as the user always labels the top-ranked documents. In a ReQ-ReC process that actively selects documents, however, documents ranked lower by the retrieval function are more likely to be viewed and judged by the user.
 In Equation 1, instead of using all relevant documents D , we use its subset D rl , which includes the documents that are judged as relevant but ranked low by the original retrieval function. We employ a simple criterion to deter-mine which documents should be included in D rl . For each document d , we maintain its rank returned by the retrieval function, denoted as r d . If the document has been retrieved by multiple queries in the past, its highest rank in those re-trieved lists is kept. Let r l be the lowest rank r d of all the documents in D r . We include documents that are ranked lower than r l / 2 in D rl . This leads to inclusion in the next query of terms from relevant documents that were not highly weighted in previous queries. Since this method aims to di-versify new queries, while still using the classifier to actively choose documents for labeling, we refer to this method as Diverse Active .
In this section, we present empirical experiments to eval-uate the effectiveness of the ReQ-ReC framework and its instantiations. We start with a description of the data sets, metrics, and methods included in the comparisons.
There are several criteria for selecting the right data sets for evaluating ReQ-ReC. Ideally, the data sets should be large enough and standard search APIs should exist. A rep-resentative set of queries should also exist, and each query should have a reasonable number of relevant documents in the data set. To avoid the high variance of real-time user judgments and to facilitate comprehensive and fair compar-isons, we use existing judgments for each query to  X  X utomate X  the actual user feedback in the process. The same approach is used in most existing work on relevance feedback (e.g., [8, 20, 25]). We therefore require that many relevant judgments exist for each query.

We first select four large scale TREC data sets, the data sets used in TREC-2012 Microblog Track (MB12) [21], TREC-2013 Microblog Track (MB13) 1 , the TREC-2005 HARD Track (HARD), and the TREC-2009 Web Track (ClueWeb09 2 , category A) 3 . These data sets normally provide 50 X 60 queries and 500 X 1,000 relevant judgments for a query. https://github.com/lintool/twitter-tools/wiki/ http://lemurproject.org/clueweb09/ http://trec.nist.gov/data/web09.html
Note that there is a natural deficiency of using TREC judgments for the evaluation of a high-recall task, simply because not all documents in a TREC data set have been judged. Instead, judgments are provided for only a pool of documents that consist of the top-ranked documents sub-mitted by each participating team. In many cases, only a sample of the pool is judged. Therefore, it is likely that many relevant documents for a query are actually not labeled in the TREC provided judgments. This creates a problem for a  X  X imulated X  feedback process X  X hen the system requests the label of a document, the label may not exist in the TREC judgments. It is risky to label that document either as relevant or as irrelevant, especially because mislabeling a relevant documents as irrelevant may seriously confuse a classifier. In such situations, we ignore that document and fetch the next document available. The same treatment has been used in the literature [20]. When measuring the per-formance of a retrieved list, however, we follow the norm in the literature and treat a document not judged by TREC as negative.

To better understand the behavior of ReQ-ReC, it is desir-able to include a data set that is fully judged, even though a large data set like that is rare. Therefore, we include the 20-newsgroup data set (20NG) [11] for this purpose. As every document belongs to one of the 20 topics, we use the titles of 20 topics as the queries, following the practice in [4]. For words that are abbreviated in the topic titles, we manually expand them into the normal words. For example,  X  X ec X  is converted to  X  X ecreation, X  and  X  X utos X  to  X  X utomobiles. X  Al-though it is feasible to apply a classifier to the entire 20NG data set, we only access the data using rate-limited retrieval functions. The statistics of all five data sets in our experi-ments are presented in Table 2.

Both the 2013 Microblog Track 4 and the ClueWeb09 5 provide official search APIs, which are implemented using the Dirichlet prior retrieval function (Dirichlet) [29]. For other data sets, we maintain a similar search service using Lucene, 6 which also implements the Dirichlet prior function. Documents are tokenized with Lucene X  X  StandardAnalyzer and stemmed by the Krovetz stemmer [10]. No stopwords are removed.
Many popular metrics for retrieval performance, such as precision @ K and NDCG, are not suitable for high-recall tasks. We use two standard retrieval metrics that depend more on recall, namely the mean average precision (MAP) https://github.com/lintool/twitter-tools/wiki/ TREC-2013-API-Specifications http://boston.lti.cs.cmu.edu/Services http://lucene.apache.org/ [12] and the R-precision (R-Prec) [12]. R-precision mea-sures the precision at the R-th position for a query with R relevant judgments. The R-th position is where precision equals recall. To increase R-precision, a system has to si-multaneously increase precision and recall. For each query, we use the top 1,000 relevant documents (either labeled or predicted) to compute the measures.

When measuring performance, we include documents that the user labeled during the process. This is because a high-recall retrieval task is successful when more relevant docu-ments can be found, whether they are actually judged by the user or predicted by the system. If an interactive pro-cess does a good job of presenting more relevant documents to the user, it should not be punished by having those docu-ments excluded from the evaluation. In all methods included in comparative evaluation, we put the documents judged as relevant at the top of the ranked list, followed by those pre-dicted to be relevant using  X  R .
We summarize all baseline methods and ReQ-ReC instan-tiations included in our evaluation in Table 3. The most important baseline we are comparing with is the iterative relevance feedback as described in Section 4.1, in which a new query is expected to maximize both precision and re-call. We then include four instantiations of the ReQ-ReC framework, as described in Section 4.

In Passive and Unanchored Passive , we employed a neg-ative form of pseudo-relevance feedback: the lowest ranked 1,000 documents retrieved by the final query are treated as negative examples to train the classifier. The positive ex-amples for training came from the actual judgments.
For the MP13 and ClueWeb09 datasets, we used the of-ficial search APIs, which returned, respectively, 10,000 and 1,000 documents per query. For the three data sets without official search APIs, the parameter of the Dirichlet prior  X  for the base retrieval function was tuned to maximize the mean average precision and each query returned the top 2,000 matching documents.
 To obtain the strongest baseline, we set the parameters of Rocchio to those that maximize the mean average precision of a relevance feedback process using 10 judgments. We fix  X  to be 1 and conduct a grid search on the other two. For ClueWeb09, we set the parameters according to the recom-mendation in [12] as the rate limits of the API prevent us from tuning the parameters. We do not further tune the pa-rameters in the ReQ-ReC methods even though the optimal parameters for the baseline may be suboptimal for ReQ-ReC. The values of all the parameters used are shown in Table 4. In all our experiments, we also use the default pa-rameter of SVM ( c = 1). We stop the inner-loops when SVM confidence value produces stable ranking of D q , i.e., Spear-man X  X  rank correlation coefficient of previous and current rankings of D q is above 0.8 for two consecutive inner-loops.
Table 5 summarizes the performance of all included meth-ods, with one additional criterion to stop the process when the  X  X ser X  has judged 300 documents for a topic. Statistical significance of the results are provided by comparing to the baseline, iterative relevance feedback, and by comparing to another ReQ-ReC method. In general, methods developed Table 4: Parameter settings:  X  in Dirichlet prior;  X  and  X  in Rocchio (  X  fixed as 1); Results per query: number of documents returned by a search API call.
 under the ReQ-ReC framework significantly outperform it-erative relevance feedback. Diverse Active , which uses an active document selection strategy and a diverse query ex-pansion, achieves the best performance. For most data sets, the improvement over iterative relevance feedback is as large as 20%  X  30% of MAP and R-Precision. This is promising given the difficulty of improvements based on those two met-rics. On the largest data set, ClueWeb09, the best ReQ-ReC algorithm achieves more than 120% improvement over iter-ative relevance feedback.

We make the following remarks: The previous section summarizes the performance of ReQ-ReC methods when the stop criteria are met. To better un-derstand the behavior of ReQ-ReC, we provide the follow-ing analysis and plot the intermediate performance of three methods ( Iterative RF , Active , and Diverse Active ) through-out the user-interaction process. Note that each topic may accumulate judgments at a different pace and meet stop cri-teria earlier or later. We interpolate a per-topic curve by a piecewise linear function, and extrapolate it by extend-ing the end-point constantly to the right. These per-topic curves are then averaged to generate the aggregated curve.
Figure 3 plots the performance of each method against the number of documents the  X  X ser X  has judged so far through-out the ReQ-ReC process, measured using R-precision. All three curves start at the same point where there is no user judgment. At that point the ranking is essentially based on the original retrieval function (i.e., Dirichlet prior). When user judgments are beginning to be collected, there is a sig-nificant gain by iterative relevance feedback. Performance increases rapidly at the first 2 runs (20 judgments), and the growth becomes much slower after that. This is consistent with the findings in literature.

Methods developed under the ReQ-ReC framework ( Ac-tive and Diverse Active ) do not really take off until we obtain a reasonable number of judgments (50 on the HARD data set and 90 on the microblog data set). This is ascribed to significantly outperform iterative relevance feedback. the  X  X old start X  problem of supervised classification. When few labeled documents are available, the performance of a classifier does not outperform a simple ranking function.
As stated before, a ReQ-ReC process targets users who truly seek a high recall of relevant documents and are there-fore willing to spend more effort on interacting with the system and labeling more results. Indeed, after the first few iterations, the two methods developed under ReQ-ReC framework improve dramatically and become significantly better than iterative relevance feedback. For the users who are reluctant to label more than 50 documents, conventional relevance feedback may still be a better choice.

The cold start implies that there is considerable room for improving the performance of the ReQ-ReC. For example, a semi-supervised classifier may be used early on to achieve better precision with few training examples.

We also notice that the benefit of Diverse Active over Ac-tive kicks in later in the process, when there are around 150 judgments collected. At that point, getting new relevant documents becomes more challenging, as many documents retrieved by the new query may have already been retrieved by a previous query. At this stage, introducing some diver-sity to the query expansion brings in considerable benefit. Similar observations are made on the other three data sets.
Another interesting analysis is how well a method works with documents that have not been selected for labeling so far. We are particularly interested in this behavior because we have decided to include all judged documents when mea-suring the performance of the system (see Section 4).
We plot the residual MAP in Figure 4, which is the mean average precision computed purely based on documents that have not been presented to the user so far in the process. In general, the two ReQ-ReC methods ( Active and Diverse Active ) do a much better job in finding the relevant docu-ments and ranking them high, even if they are not judged by the user. On the microblog data set, we see that the resid-ual MAP decreases when more documents are presented to and labeled by the user. This may be simply because there are fewer relevant documents remaining in the collection. However, it is also likely due to the fact that the TREC judgments are not complete. There might be many rele-vant documents that were not judged by TREC at all. If a method successfully finds those documents, its performance may be significantly undervalued simply because we have to treat these documents as negative in computing the metrics.
We are therefore interested in how ReQ-ReC behaves if the data set is fully judged. Looking at the curves on the 20NG, we observe a contrary pattern, where the two ReQ-ReC methods actually enjoy a continuous growth of residual MAP, while the same metric for iterative feedback is still dropping. This is a promising finding that indicates the performance of ReQ-ReC may be underestimated on data sets with incomplete judgments (i.e., TREC data sets).
We present ReQ-ReC (ReQuery-ReClassify), a double-loop retrieval framework that is suitable for high-recall re-trieval tasks without sacrificing precision. The interactive process combines iterative expansion of a query set with it-erative refinements of a classifier. The work of maximizing precision and recall is distributed so that the queries increase recall and the classifier handles precision.

The ReQ-ReC framework is general, which includes classi-cal feedback methods as special cases, and also leads to many instantiations that use different combinations of document selection, classification, and query expansion methods. The framework is very effective. Some instantiations achieved a 20%  X  30% improvement of mean average precision and R-precision on most data sets, with the largest improvement up to 150% over classical iterative relevance feedback.
In order to clearly illustrate the power of the framework, we have intended to keep all the instantiations simple. It is a promising future direction to optimize the choices and com-binations of the key components of the ReQ-ReC framework. Findings from our experiments also indicate possibilities for investigating new classification and query expansion algo-rithms that are particularly suited to this framework. Acknowledgment. The authors thank Sam Carton, Kevyn Collins-Thompson, ChengXiang Zhai, and reviewers for their useful comments. This work is partially supported by the National Science Foundation under grant numbers IIS-0968489 and IIS-1054199, and partially supported by the DARPA under award number W911NF-12-1-0037.
