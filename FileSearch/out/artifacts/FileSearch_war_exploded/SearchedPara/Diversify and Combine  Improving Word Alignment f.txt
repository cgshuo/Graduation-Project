 Word alignment usually serves as the starting point and foundation for a statistical machine translation (SMT) system. It has received a signif-icant amount of research over the years, notably in (Brown et al., 1993; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2007; Hermjakob, 2009). They all focused on the improvement of word alignment models. In this work, we leverage existing align-ers and generate multiple sets of word alignments based on complementary information, then com-bine them to get the final alignment for phrase training. The resource required for this approach is little, compared to what is needed to build a rea-sonable discriminative alignment model, for ex-ample. This makes the approach especially ap-pealing for SMT on low-resource languages.

Most of the research on alignment combination in the past has focused on how to combine the alignments from two different directions, source-to-target and target-to-source. Usually people start from the intersection of two sets of alignments, and gradually add links in the union based on certain heuristics, as in (Koehn et al., 2003), to achieve a better balance compared to using either intersection (high precision) or union (high recall). In (Ayan and Dorr, 2006) a maximum entropy ap-proach was proposed to combine multiple align-ments based on a set of linguistic and alignment features. A different approach was presented in (Deng and Zhou, 2009), which again concentrated on the combination of two sets of alignments, but with a different criterion. It tries to maximize the number of phrases that can be extracted in the combined alignments. A greedy search method was utilized and it achieved higher translation per-formance than the baseline.

More recently, an alignment selection approach was proposed in (Huang, 2009), which com-putes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold. The alignments used in that work were generated from different align-ers (HMM, block model, and maximum entropy model). In this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function. There is no need for a pre-determined threshold as used in (Huang, 2009). Also, we utilize var-ious knowledge sources to enrich the alignments instead of using different aligners. Our strategy is to diversify and then combine in order to catch any complementary information captured in the word alignments for low-resource languages.

The rest of the paper is organized as follows. We present three different sets of alignments in Section 2 for an English-to-Pashto MT task. In Section 3, we propose the alignment combination algorithm. The experimental results are reported in Section 4. We conclude the paper in Section 5. We take an English-to-Pashto MT task as an exam-ple and create three sets of additional alignments on top of the baseline alignment. 2.1 Syntactic Reordering Pashto is a subject-object-verb (SOV) language, which puts verbs after objects. People have pro-posed different syntactic rules to pre-reorder SOV languages, either based on a constituent parse tree (Dr  X abek and Yarowsky, 2004; Wang et al., 2007) or dependency parse tree (Xu et al., 2009). In this work, we apply syntactic reordering for verb phrases (VP) based on the English constituent parse. The VP-based reordering rule we apply in the work is:  X  V P ( V B  X  ,  X  )  X  V P (  X  , V B  X  ) where V B  X  represents V B , V BD , V BG , V BN , V BP and V BZ .

In Figure 1, we show the reference alignment between an English sentence and the correspond-ing Pashto translation, where E is the original En-glish sentence, P is the Pashto sentence (in ro-manized text), and E  X  is the English sentence after reordering. As we can see, after the VP-based re-ordering, the alignment between the two sentences becomes monotone, which makes it easier for the aligner to get the alignment correct. During the reordering of English sentences, we store the in-dex changes for the English words. After getting the alignment trained on the reordered English and original Pashto sentence pairs, we map the English words back to the original order, along with the learned alignment links. In this way, the align-ment is ready to be combined with the baseline alignment and any other alternatives. 2.2 Stemming Pashto is one of the morphologically rich lan-guages. In addition to the linguistic knowledge ap-plied in the syntactic reordering described above, we also utilize morphological analysis by applying stemming on both the English and Pashto sides. For English, we use Porter stemming (Porter, Figure 1: Alignment before/after VP-based re-ordering. 1980), a widely applied algorithm to remove the common morphological and inflexional endings from words in English. For Pashto, we utilize a morphological decompostion algorithm that has been shown to be effective for Arabic speech recognition (Xiang et al., 2006). We start from a fixed set of affixes with 8 prefixes and 21 suffixes. The prefixes and suffixes are stripped off from the Pashto words under the two constraints:(1) Longest matched affixes first; (2) Remaining stem must be at least two characters long. 2.3 Partial Word For low-resource languages, we usually suffer from the data sparsity issue. Recently, a simple method was presented in (Chiang et al., 2009), which keeps partial English and Urdu words in the training data for alignment training. This is similar to the stemming method, but is more heuristics-based, and does not rely on a set of available af-fixes. With the same motivation, we keep the first 4 characters of each English and Pashto word to generate one more alternative for the word align-ment. Now we describe the algorithm to combine mul-tiple sets of word alignments based on weighted confidence scores. Suppose a link in the i -th set of alignments between the j -th source word and k -th target word in sentence pair (
S , T ). Similar to (Huang, 2009), we define the confidence of a where the source-to-target link posterior probabil-ity and the target-to-source link posterior probability q the lexical translation probability between source word s ments.

Our alignment combination algorithm is as fol-lows. 1. Each candidate link a 2. All candidates are sorted by soft votes in de-3. Repeat scanning all candidate links until no
In this way, those alignment links with higher confidence scores have higher priority to be in-cluded in the combined alignment. 4.1 Baseline Our training data contains around 70K English-Pashto sentence pairs released under the DARPA TRANSTAC project, with about 900K words on the English side. The baseline is a phrase-based MT system similar to (Koehn et al., 2003). We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then apply grow-diagonal-final ( gdf ). The decoding weights are optimized with minimum error rate training (MERT) (Och, 2003) to maximize BLEU scores (Papineni et al., 2002). There are 2028 sen-tences in the tuning set and 1019 sentences in the test set, both with one reference. We use another 150 sentence pairs as a heldout hand-aligned set to measure the word alignment quality. The three sets of alignments described in Section 2 are gen-erated on the same training data separately with GIZA++ and enhanced by gdf as for the baseline alignment. The English parse tree used for the syntactic reordering was produced by a maximum entropy based parser (Ratnaparkhi, 1997). 4.2 Improvement in Word Alignment In Table 1 we show the precision, recall and F-score of each set of word alignments for the 150-sentence set. Using partial word provides the high-est F-score among all individual alignments. The F-score is 5% higher than for the baseline align-ment. The VP-based reordering itself does not im-prove the F-score, which could be due to the parse errors on the conversational training data. We ex-periment with three options ( c bining the baseline and reordering-based align-ments. In c c ( a ijk | S, T ) in Eq. (3) are all set to 1. In we set confidence scores to 1, while tuning the weights with hill climbing to maximize the F-score on a hand-aligned tuning set. In c pute the confidence scores as in Eq. (1) and tune the weights as in c the effectiveness of having both weights and con-fidence scores during the combination.

Similarly, we combine the baseline with each of the other sets of alignments using c all result in significantly higher F-scores. We also generate alignments on VP-reordered partial words ( X in Table 1) and compared B + X and B + V + P . The better results with B + V + P show the benefit of keeping the alignments as di-versified as possible before the combination. Fi-nally, we compare the proposed alignment combi-nation c where the latter starts from the intersection of all 4 sets of alignments and then applies grow-diagonal-final (Koehn et al., 2003) based on the links in the union. The proposed combination approach on B + V + S + P results in close to 7% higher F-scores than the baseline and also 2% higher than gdf . We also notice that its higher F-score is mainly due to the higher precision, which should result from the consideration of confidence scores. Alignment Comb P R F Baseline 0.6923 0.6414 0.6659 V 0.6934 0.6388 0.6650 S 0.7376 0.6495 0.6907 P 0.7665 0.6643 0.7118 X 0.7615 0.6641 0.7095
B+V c
B+V c
B+V c
B+S c
B+P c
B+X c
B+V+P c B+V+S+P gdf 0.7238 0.7042 0.7138
B+V+S+P c Table 1: Alignment precision, recall and F-score (B: baseline; V: VP-based reordering; S: stem-ming; P: partial word; X: VP-reordered partial word). 4.3 Improvement in MT Performance In Table 2, we show the corresponding BLEU scores on the test set for the systems built on each set of word alignment in Table 1. Similar to the observation from Table 1, c c , and B + V + S + P with c B + V + S + P with gdf . We also ran one ex-periment in which we concatenated all 4 sets of alignments into one big set (shown as cat ). Over-all, the BLEU score with confidence-based com-bination was increased by 1 point compared to the baseline, 0.6 compared to gdf , and 0.7 compared to cat . All results are statistically significant with p &lt; 0 . 05 using the sign-test described in (Collins et al., 2005). In this work, we have presented a word alignment combination method that improves both the align-ment quality and the translation performance. We generated multiple sets of diversified alignments based on linguistics, morphology, and heuris-tics, and demonstrated the effectiveness of com-bination on the English-to-Pashto translation task. We showed that the combined alignment signif-icantly outperforms the baseline alignment with Alignment Comb Links Phrase BLEU Baseline 963K 565K 12.67 V 965K 624K 12.82 S 915K 692K 13.04 P 906K 716K 13.30 X 911K 689K 13.00
B+V c
B+V c
B+V c
B+S c
B+P c
B+X c
B+V+P c B+V+S+P cat 3749K 1258K 13.01 B+V+S+P gdf 1021K 653K 13.14
B+V+S+P c Table 2: Improvement in BLEU scores (B: base-line; V: VP-based reordering; S: stemming; P: par-tial word; X: VP-reordered partial word). both higher F-score and higher BLEU score. The combination approach itself is not limited to any specific alignment. It provides a general frame-work that can take advantage of as many align-ments as possible, which could differ in prepro-cessing, alignment modeling, or any other aspect. This work was supported by the DARPA TRANSTAC program. We would like to thank Upendra Chaudhari, Sameer Maskey and Xiao-qiang Luo for providing useful resources and the anonymous reviewers for their constructive com-ments.

