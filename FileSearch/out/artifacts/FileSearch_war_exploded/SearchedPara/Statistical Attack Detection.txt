 It has been shown in recent years that effective profile injec-tion or shilling attacks can be mounted on standard recom-mendation algorithms. These attacks consist of the insertion of bogus user profiles into the system database in order to manipulate the recommendation output, for example to pro-mote or demote the predicted ratings for a particular prod-uct. A number of attack models have been proposed and some detection strategies to identify these attacks have been empirically evaluated. In this paper we show that the stan-dard attack models can be readily detected using statistical detection techniques. We argue that insufficient considera-tion of the effectiveness of attacks under a constraint of sta-tistical invariance has been taken in past research. In fact, it is possible to create effective attacks that are undetectable using the detection strategies proposed to date, including the PCA-based clustering strategy which has shown excel-lent performance against standard attacks. Nevertheless, these more advanced attacks can also be detected with care-ful design of a statistical det ector. The question posed for future research is whether attack models that produce effec-tive attack profiles that are statistically identical to genuine profiles are really possible.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  collaborative filtering, robustness ;G.3 [ Probability and Statistics ]: Robust regression Performance, Security
The possibility of designing user rating profiles to deliber-ately and maliciously manipulate the recommendation out-put of a collaborative filtering system was first raised in [14]. One scenario proposed was that an author, motivated to increase recommendations of his book, might create a set of false profiles that rate the book highly, in an effort to artificially promote the ratings given by the system to genuine users. Since then, these attacks have been dubbed as shilling attacks [8] or profile injection attacks [1]. Sev-eral attack models have been proposed and the performance of these attacks in terms of influencing the system predic-tions has been evaluated for a number of memory-based and model-based collaborative filtering algorithms. It is gener-ally accepted that the best  X  X eneral purpose X  attack strategy is the so-called  X  X verage attack X  proposed initially in [8], al-though it requires some knowledge of the rating statistics of genuine users. To counteract attacks, researchers have in-vestigated the application of classification techniques to user profiles, in order to identify attack profiles and filter them from the dataset. Several attack detection strategies have been proposed. Of these, the principal components anal-ysis (PCA) based detection strategy, recently proposed in [10], yields the best detection performance, obtaining over 90% precision in the detection of Average and other attack models when evaluated on the Movielens dataset.

In this paper, we review attack models in the context of their detectability. We propose to use Neyman-Pearson sta-tistical detection to identify attack profiles and show how to statistically model the standard attacks that have been proposed to date. Our analysis shows that the success of the PCA-based detector is largely due to the unrealistic man-ner in which items are rated in attack profiles of standard attacks. With this realisation, we show how it is possi-ble to create an effective attack which is undetectable by the PCA detector with a simple modification of the Aver-age attack. To address the detection of such obfuscated at-tacks, we model an attacked dataset as a multivariate Gaus-sian mixture model and design supervised and unsupervised Neyman-Pearson detectors based on this model. These de-tectors significantly out-perform the PCA detector on ob-fuscated attacks.

The significance of this analysis is as follows. Firstly, it demonstrates that the attack models proposed to date are in-sufficient and have failed to properly address the question of designing undetectable attacks. Even simple modifications of the existing models are sufficient to fool the best detectors proposed to date. Secondly, we have shown that Neyman-Pearson statistical detection is a powerful tool that can be successfully applied to the detection of a wider range of ob-fuscated attacks. In the end, we argue that it is important to consider robustness of recommender systems, taking into ac-count the level of knowledge available to the attacker, both of genuine rating statistics and of the recommendation algo-rithm and how the attacker can exploit that knowledge in the development of effective attacks. While memory-based algorithms have shown weak robustness to standard attack models, we believe that carefully designed attacks can also exploit vulnerabilities of other recommendation algorithms. It is therefore important to design good detection algorithms for more sophisticated attacks than have been considered to date.

The paper is organised as follows. In Section 2, the at-tack detection problem is defined and the state-of-the-art in attack detection is reviewed in Section 3. In Section 4, a simple statistical model of standard attacks is developed and compared with PCA-based detection. Obfuscation is consid-ered in Section 5 and a Gaussian mixture statistical model is developed for detection. Final conclusions are drawn in Section 6.
 Column vectors are represented by bold lowercase letters such as x . The transpose of a column vector to a row vector is written as x T . Matrices are represented in roman text, capital letters such as A.

We consider a ratings database consisting of a set of user profiles , that is ratings that a user gave for items in the system. Write a user profile as the vector y T =( y 1 ,...,y where m is the total number of items in the system. The vector components y i  X  X   X  } X  X  where V { v min ,...,v max is a set of discrete rating values, assumed to be integers and  X  is used to represent that item i has not been rated. Rather than use the discrete scale, in the Gaussian model developed in Section 5, we consider y i  X  [  X  X  X  ,  X  ] i.e. that it takes any value on a continuous scale.
Figure 1 summarises the steps involved in attack detec-tion. This is a binary classification problem, with two pos-sible outcomes for each profile, namely, Genuine , meaning that the classifier has determined that the profile is that of a genuine system user or Attack , meaning that the classifier has determined that this is an instance of an attack profile. One approach to the detection problem, followed by work such as [5, 1], has been to view it as a problem of determin-ing independently for each profile in the dataset, whether or not it is an attack profile. This is the  X  X ingle profile X  input shown in Figure 1. The input is a single rating vector y u some user u from the dataset. Before processing by the clas-sifier, a feature extraction step may extract a set of features, f u =( f 1 ,...,f k ) from the raw rating vector y u . The classi-fier takes f u as input and outputs,  X  Attack  X  X r X  Genuine  X . I f the classifier is a supervised classifier, then a training phase makes use of an annotated dataset of profiles, i.e. a set of profiles labelled as Genuine or Attack, in order to learn the classifier parameters.

Because most attack scenarios consist of groups of pro-files working in concert to promote or demote a particular item, work such as [9, 13] has suggested that there is ben-efit to considering groups of profiles together when making the classification. This is represented by the  X  X roup of Pro-files X  input, in which the classifier considers an entire group of profiles, possibly after some feature extraction, and out-puts a label for each profile in the group. Note that not all steps may take place in any particular scenario. For in-stance, there may be no feature extraction, in which case, f = y and if unsupervised classifiers are used, then there is no need for a training phase.
In this paper, we will treat attack detection as an instance of a statistical detection problem, a problem of making a decision among a number of possible statistical models that describe some set of observations. In fact it is a binary hypothesis testing problem, in which the observations may have come from two possible choices or hypotheses, usually denoted by H 0 and H 1 . In our context, the observations are the profiles contained in the system database. Given an observation (i.e. a profile), the task is to make a decision between H 0 , that the profile is a genuine profile or H 1 the profile is an attack profile . Many decision rules are pos-sible, particularly if the hypotheses are not viewed as equal and different costs are assigned to different types of error. We focus on Neyman-Pearson detectors that do not require apriori probabilities of the hypotheses.

Let y  X  X  be an observation i.e. a user profile, over the space Y of all possible profiles. A test is formed based on f
Y | H 1 ( y )and f Y | H 0 ( y ), the probability distribution func-tions (pdf) of y under the assumption that H 1 or H 0 is true, respectively. We can partition Y into two regions R and R 1 such that H 0 is accepted if y  X  X  0 and H 0 is re-jected if y  X  X  1 , and therefore write the decision function as
The performance of a test can be determined by consider-ing the different types of error that can occur. A false alarm refers to H 0 being rejected when true and a miss refers to H 1 being rejected when true. Write p f as the probability of false alarm, p m as the probability of a miss and p d =1  X  as the probability of good detection . The Neyman-Pearson (N-P) criterion sets a bound  X  on p m and finds the deci-sion function that maximises p d under the constraint that p f  X   X  .The optimal test in this sense, is the N-P test which has the general form  X   X  0isathresholdand q is the probability with which H 1 is chosen when l ( y )=  X  and l ( y )isthe likelihood ratio defined as
Thus, if we have a good model of genuine profiles, rep-resented as the pdf f Y | H 0 ( y ) and a good model of attack profiles, represented as the pdf f Y | H 0 ( y ), then (3) provides the optimal test to distinguish between these alternatives. It is straight-forward to compute the pdf of the precisely de-fined attack models that have been proposed in the literature and we will do this in Section 4. A good model of genuine profiles is more difficult and will be discussed afterwards.
The threshold  X  of the N-P test can be computed analyt-ically for simple distributions and may be selected through an empirical evaluation in practice. The receiver operating characteristic (ROC) curve plots p d against p f for different threshold values and allows an appropriate trade-off between the detection performance and false alarm error to be cho-sen. In previous studies such as [10], precision and recall have been used to evaluate attack detection performance, with just a single figure reported for each attack configu-ration. However, this does not consider the detection per-formance over different detection sensitivities (which, in the case of the PCA clustering algorithm in [10] corresponds to different attack cluster sizes ). Hence, we prefer to present ROC results. To make the correspondence to previous stud-ies, note that p d is equivalent to precision.
A profile injection attack whose goal is to promote the ratings given to a target item is often referred to as a push attack . As a targeted pushed item in an attack profile will have unusually high ratings and other ratings may be chosen to support the influence of the profile towards high predic-tions for the target, distinctive characteristics are likely to exist in attack profiles and may be manifested in many ways [1]. An unsupervised individual profile detection algorithm isdescribedin[5]. Detectionisbasedoncertaincommonat-tributes of attack profiles, for example that there is a higher than usual rating deviation from mean in such profiles and that such profiles are likely to have a higher than usual sim-ilarity to their closest neighbours. In [3] profile attributes based on those proposed in [5] and others along similar lines were developed into features for inclusion in a feature vector input to a supervised classifer. The authors evaluated three supervised classifiers: kNN, C4.5, and SVM. SVM and C4.5 were found to have near perfect performance on identifying attack profiles correctly, but on the other hand, they also misclassify more genuine profiles than kNN. SVM had the best performance considering all error types.
In [13] an unsupervised group detection and filtering scheme is presented. Rather than filtering profiles from the dataset in a preprocessing step, in this method, filtering is applied to the profiles in the active user X  X  neighbourhood during predic-tion for a particular item. This approach has the advantage of identifying just those attack profiles that are targeting the active item. The strategy is based on an algorithm pro-posed in [6] in the context of reputation reporting systems that aims to provide a reputation estimate for buyers and sellers engaged in on-line marketplaces that is robust to ma-licious agents who attempt to fradulently enhance their own reputations. The approach involves the clustering of neigh-bourhoods into two clusters based on the cluster means and standard deviations.
In [10] the observation that attacks consist of multiple pro-files which are highly correlated with each other, as well as having high similarity with a large number of genuine pro-files motivates the development of a clustering approach to attack detection, using Probabilistic Latent Semantic Anal-ysis (PLSA) and Principal Component Analysis (PCA). The second of these strategies provides the better performance and is based on a PCA of the covariance matrix of the user profiles. Essentially this strategy attempts to identify a clus-ter where the sum of the pair-wise covariances between pro-files in the cluster is minimised (or maximised 1 ). PCA has been widely used as a dimension reduction strategy for high-dimensional data. Identifying profiles with dimensions, the method is explained intuitively in [10] as a method of identi-fying those highly-correlated dimensions (i.e. profiles) that would safely be removed by PCA. Alternatively, a cluster C can be defined by an indicator vector x such that x i =1if user u i  X  C and x i = 0 otherwise. With S defined as the covariance matrix, the sum of the pair-wise covariances of all profiles in C , may be written as the quadratic form Moreover, for the normalised eigenvectors e i of S, associated with eigenvector  X  i such that  X  1  X  X  X  X  X  X   X  m ,thequadratic form evaluates as With this observation, the method described in [10] may be understood as a method that seeks the binary vector y that minimises the quadratic form by choosing y so that it has strong correlation with those 3  X  5 eigenvectors correspond-ing to the smallest eigenvalues.

The UnRAP algorithm [2] also uses clustering to distin-guish attack profiles. It bases clustering on a measure called the H v score which has proved successful in identifying highly correlated biclusters in gene expression data. In the context of attack detection, the H v score measures for each user, a sum of the squared deviations of its ratings from the user mean, item mean and overall mean ratings. In general, the authors report that this method performs well particularly
We will discuss this point more carefully in a later section. for mid-size attacks, in which other methods show a dip in performance. As the UnRAP algorithm identifies highly cor-related profiles, we can expect that the comments we make later regarding the PCA-based algorithm will also apply to UnRAP.
We adopt the terminology of [11], in discussing standard attack models. In characterising attacks, the ratings in an attack profile are partitioned into three sets. The item i represents a target item that is the focus of the attack. The purpose of the attack profile is to manipulate the predicted ratings for the target item. We will focus on push attacks, where the goal is to promote the predicted ratings to the maximum value. A set S of selected items may also be rated and given specific ratings in order to support the at-tack. Finally, a set F of filler itemsischosenandgiven random ratings. In [11], filler items are not described as being specifically constructed to support the attack. How-ever, in practice, the selection of filler items and the ratings assigned to them has a major impact both on the success of the attack, from the attackers point of view, and on the detectability of the attack, from the system manager X  X  point of view. Hence, the distinction between S and F is not particularly useful.
Two general attack models were originally proposed in [8] and named the Random and Average attacks. The target i t is assigned the maximum rating (assuming a push attack). There are no selected items, S . The ratings of filler items are chosen independently from a Gaussian distribution. For the Random attack, it is assumed that the ratings are drawn from the Gaussian distribution N (  X ,  X  2 )where  X  and  X  rep-resent the overall mean and sta ndard deviation of all ratings across the entire database. For the Average attack, a rating r for item i , is drawn from N (  X  i , X  2 i )where  X  i and  X  resent the mean and standard deviation of all ratings given for item i .

To build a statistical model of these attacks, we assume all ratings in the profile have been generated using the filler item rule and ignore the fact that one target item has been specially rated. Note that, in this attack model, whether or not an item is chosen to be a filler is independent of the rating chosen for the item. Define  X  i as an indicator variable such that  X  i =1if y i =  X  and  X  i = 0 otherwise. The pdf, f
Y | H 1 ( y ), of y under hypothesis H 1 that it is an attack profile is given by
Furthermore, assuming that the continuous values gener-ated by the Gaussian distribution are rounded to the nearest integer, so that the probability of obtaining the rating value y is the probability that a Gaussian random number lies in the range [ y i  X  1 2 ,y i + 1 2 ], we can write
Pr[ Y i = y i | y i =  X  ]= Q where Q ( . ) is the Gaussian Q -function 2 .

It remains to determine Pr[ Y i =  X  ]. Random and Average attacks are usually evaluated as a function of | F | ,the filler size . Filler items are chosen with equal probability from the item set. As we will see later, this filler selection strategy is key to successful detection of the attacks, since it does not reflect how genuine users choose items to rate. If the filler size is l ,wetakePr[ Y i =  X  ]= l/m .
Examining the Random and Average attacks, it is clear that, while an effort has been made to ensure that the rat-ing values of the selected filler items are similar to genuine rating values, no effort has been made to ensure that those items selected as fillers are similar to the selections made by genuine users. In reality, every item does not have an equal chance of being rated; some items are much more popular than others. A model that is sufficient to distinguish genuine profiles from attack profiles can use (4) for the pdf f Y | of genuine profiles, but with Pr[ Y i =  X  ]settomodelthe rating behaviour of genuine users. Note that this model of genuine profiles suggests that the rating values given by a user for one item are independent of the values given for another item. If this were an accurate model of real rat-ing behaviour, then collaborative filtering algorithms could not be successful. However, all we require for detection is a model that distinguishes genuine from attack profiles. For each item i ,wesetPr[ Y i =  X  ]= p i ,where p i , the probability that an item is rated, can be estimated from a training set of genuine profiles, as the fraction of profiles that rated the item.
In [11] the unrealistic filler selection strategies of the Ran-dom and Average attacks is recognised. The Bandwagon attack is constructed on the basis that some items are more popular than others. The purpose is to the make more ef-fective attacks rather than to make attack profiles more like genuine profiles. In the Bandwagon attack, S is non-empy and contains some number s of popular items. Items in S are given the same rating value as the target item, in order to support the attack. The rest of the profile is filled with filler items in the same manner as an Average attack. In evaluations, s has been chosen as small relative to the over-all filler size. In fact, in [11], s = 1 is chosen. With such small s , the statistical differences been Bandwagon and Av-erage attacks are insignificant and the Average attack model can be used. It is possible to model the Bandwagon attack more precisely, in a straight-forward manner along the lines of the Average attack model above. We will not describe the model in detail here, but note that, Bandwagon attack profiles are in general more detectable than Average attacks when accurately modelled.
The ROC curve for statistical detection using the N-P test and the pdf X  X  developed in the previous sections is shown in i.e. Q ( x )=1 / (2 Figure 2: ROC curves for (a) Random and (b) Av-erage Attacks, Filler size = 5% Figure 2. It is compared with PCA-based detection. The 1,000,000 ratings Movielens dataset is used and the filler size is set to 5%, the overall sparsity of the Movielens dataset. For the PCA clustering strategy, the ROC is generated by varying the attack cluster size, from 1 to m and computing p f and p d for each size. The clustering is based on three principal components, as described in [10]. This detector is labelled as PCA Z1 in the figure and clearly performs ex-tremely well, as reported in [10]. Reasonable performance is also obtained from the N-P detector however, as it obtains over 90% precision at a cost of only 5% false alarm proba-bility on the Average attack. The noteworthy point here is that the only distinction between the genuine profile model and attack model on which the N-P test is based is in filler selection. Both are assumed to rate identically, once the filler items have been selected. It is interesting to ask then, how critical is the filler selection strategy to the success of the PCA detector. We explore this in the next section.
The PCA detection strategy is based on calculating the principal components of the covariance matrix of the ratings dataset, converted to z -scores. In [10], it is striking that, while an argument is made to suggest that Average attack profiles are highly similar to each other and to other genuine profiles, the method actually identifies an attack cluster as a set of profiles with low pairwise covariance .Ifthesimi-larity metric is based on covariance, then the method is ac-tually identifying a set of maximally dissimilar profiles. It transpires that this discrepancy and the explanation for the success of the method, may be understood by considering how missing values are handled. The z -score for a profile y is obtained by subtracting the profile mean  X  y and dividing by the standard deviation  X  y : Given that a typical profile contains many unrated items, the z -score depends strongly on how we deal with missing values. One strategy is to ignore them and calculate  X  y and  X  y basedontherateditemsonly.Calltheresultingmatrix of z -score ratings Z 0 . A second strategy is to treat missing values as zeros and compute  X  y and  X  y acrosstheentirepro-file. In the case of the 5 X  X oint rating scale of the Movielens dataset, with minimum rating 1, this is equivalent to con-sidering a missing rating to be an extremely low evaluation of the corresponding item. Call the resulting matrix Z 1 .In fact, as observed in Figure 2, using Z 0 is extremely unsuc-cessful. In fact, the success of the PCA Z1 strategy is largely explained by the fact that the covariance is dominated by the missing value distribution. It is also noteworthy that the standard Pearson correlati on formula that is generally applied in the user-based kNN algorithm, computes the cor-relation on the overlap ratings only. Profiles can therefore be highly correlated according to Pearson similarity, while having low Z 1 covariance. In fact, small overlap increases the probability of obtaining extreme values of the Pearson correlation. We exploited this fact in the original shilling attack, reported in [14].

Average attack profiles, whose filler items are selected with equal probability, are likely to have smaller pair-wise overlaps than genuine users, who are generally more likely to rate certain items over others. In fact, for the Movielens dataset, the mean overlap size for Average attack profiles is 9.8, when the sparsity is 5%. The mean overlap size for genuine profiles is 23.7, although the sparsity is the same. It follows that the success of the standard Average attack can be largely explained by its unrealistic filler selection strat-egy. However, as we have seen, this is also its weakness from the point-of-view of detection.
The possibility of obfuscating attacks has been considered previously in [16] where three different obfuscation strategies were proposed. These strategies represent ad hoc modifica-tions of the standard attack models that an attacker might employ in order to avoid detection. No guiding principle is employed in the design of these obfuscations. An effective obfuscation strategy must try to minimise the statistical dif-ferences between genuine and attack profiles. We have seen that an attack profile that is undetectable by the PCA de-tector must reduce the differences in filler selection that are key to the success of this detector.
In fact, our analysis has shown that an attacker should choose filler items according to their overall popularity among the genuine user base. A simple and effective strategy to ob-fuscate the Average attack is to choose filler items with equal probability from the top x % of most popular items, rather than from the entire catalogue of items, where x is chosen to ensure that the profiles are undetectable by the PCA detec-tor. However, choosing items from a subset of the catalogue increases the pair-wise overlap of attack profiles and, by our comments at the end of the previous section, renders the attacks less effective. Hence, the attacker must compromise between detectability and attack performance. We dub this attack strategy the AoP ( Average over Popular items) at-tack and will refer to an  X  X oP x % attack X  when the items are drawn from the top x % most popular items.

The AoP attack is evaluated on the Movielens dataset by applying a push attack in turn to 100 randomly chosen tar-get items. The attack size (number of attack profiles relative to the overall database size) is set to 3%. The standard user-based kNN algorithm, with k = 20 and using the Pearson similarity metric is used to make predictions for the target item. The prediction shift between predictions made pre-and post-attack for all users who rated the target item is evaluated. In Figure 3(a) the average prediction shift that results is plotted against filler size, for different values of x . In Figure 3(b), the performance is given as the percentage of hits obtained on the attack item, where a hit is defined as a rating of 4 or more. We see that as attacks are constructed of smaller subsets of popular items, the attacks become less effective. However, even when choosing from the top 20% of most popular items, a significant push is obtained, which can be best seen from the uplift in hits obtained over the pre-attack value.
 A ROC curve of detection of the AoP attack using the PCA detector is shown in Figure 4. A sharp disimprovement in detection quality is observed over the standard Average attack. For example, when filler items are confined to the top 60% of most popular items, to obtain over 90% precision comes at a cost of nearly 20% false alarm probability and detection is hopeless when 20% of the most popular items are used.
It is possible to design an N-P detector that successfully detects the AoP attack. However, a more accurate model of genuine ratings, f Y | H 0 ( y ), is required, that properly cap-tures the correlations in rating values, as well as the filler selection distribution. In fact, if all ratings are available, a good model for the distribution of y is a multivariate Gaus-sian distribution N (  X  0 ,  X  0 ), where  X  0 represents the vector of mean ratings for each item in the genuine profile and  X  0 is the covariance matrix of genuine profiles. This model has been previously successfully applied to model rating be-haviour in collaborative filtering. Similarly, we make the as-sumption that, f Y | H 1 ( y ), the distribution of attack profiles is also multivariate Gaussian, with mean  X  1 and correlation  X  . The entire dataset, of genuine and attack profiles, can therefore be modelled as a Gaussian mixture model .Asthe rating values in an AoP attack are chosen independently,  X  is diagonal, which should not be the case in general for  X 
To deal properly with missing values, we train a factor model using the expectation maximisation (EM) algorithm. Thus, following [4], we assume that the rating matrix can be represented by the linear model where X is a n  X  k matrix with x u,j representing the extent Figure 3: Effectiveness of Popular Average Attack (a) Prediction shift and (b) % hits, Attack size = 3% to which user u likes latent category j ,Aisa m  X  k matrix such that a i,j represents the extent to which item i belongs in latent category j , N represents independent noise in the ratings which is assumed to have variance  X  and k is the number of latent categories. We apply the EM algorithm presented in [4], to learn A, X and  X  from a training set of profiles (assumed to be genuine).

Given A and a user profile y , it follows that w =A T y is a k -dimensional multivariate Gaussian distribution. The N-P test for distinguishing between H 0 and H 1 based on w reduces to where  X  w 0 , the mean of w under H 0 is  X  w 0 ,isA T  X  0 the covariance is A T  X  0 A and similarly for H 1 .
An alternative, less computationally intensive strategy, since it does not require factor model training, is to choose the projection matrix A based on item clustering. Given the pair-wise item-item similarities, it is possible to cluster the items into clusters of similar items. The components of w are then chosen as the sum of the ratings for all items in each cluster. Many clustering algorithms are possible. We compute a partitioning of the similarity matrix using the metis graph partitioning software [7].
To apply the N-P detector (6), we need to estimate the mean and covariance of w under the two hypotheses. For a given attack model, it is possible to generate attack clusters and estimate the required parameters for a training set of genuine clusters and a training set of attack clusters. Of course one disadvantage of such supervised detection is that the actual attack may not be identical to the attack used for parameter estimation. We e valuate supervised detection on a dataset containing genuine profiles from Movielens and AoP 20% attack profiles, with 5% filler size. To compute the factor matrix, k = 25 is chosen. The resulting ROC curves are shown in Figure 5(a). Be careful to note that the different curves correspond to simulations in which different attack parameter estimations were derived by training on the corresponding attack model. In all cases, the actual attack was an AoP 20% attack.

When the detector is trained on AoP 20% attacks, it gives close-to perfect performance. Performance reduces if the training set does not reflect the real attack as may be ob-served from ROC curves that result from training on AoP 30%, 40% and 60% attacks.

The results of using the partitioning strategy with k =25 are shown in Figure 5(b). While they are not as good as the factor model when the training set consists of the profiles from the actual attack, this strategy performs a little better when the actual attack does not match the attack on which the model was trained.
Unsupervised Gaussian mixture model clustering can be applied to partition the dataset into two clusters and learn the required parameters of the mixture model:  X  w 0 ,A T  X   X  w 1 ,A T  X  1 , a 0 and a 1 ,where a i is the probability that a profile belongs in cluster i . The EM algorithm is used to train the model and the N-P test is applied using the re-sulting parameter estimates. The results for AoP 20%, 40% and 60% attacks are shown in Figure 6. As expected, the performance is significantly worse than supervised detection, but, comparing with Figure 4, significantly better than PCA detection. Nevertheless, a cost of misclassifying 10% of gen-uine profiles is required to obtain 80% or more precision on the AoP 20% attack, for instance, or only 50% precision on the AoP 60% attack. Figure 5: Detection of AoP Attack (a) using factor matrix and (b) using cluster projection matrix
The N-P detectors show that an attacker must consider rating values, as well as filler distribution in designing an obfuscation. The AoP strategy is just one simple means of obfuscating a profile and is presented to illustrate that the standard attack models that do not consider detectability, along with attack effectiveness, are not particularly useful. In practice, it is necessary to consider the knowledge avail-able to an attacker and how much an attacker can learn about the system. The Average attack assumes that an attacker can discover or learn the mean and standard devi-ation of genuine ratings for each item in the dataset. Given a sufficient number of genuine profiles, it is also possible to envisage that an attacker could estimate the covariance of genuine profiles and to use this estimate to generate mul-tivariate Gaussian attack profiles, to reduce the detection performance of the N-P detector presented above.

To illustrate that such obfuscated attacks can still be ef-fective, we consider an idealised  X  X erfectly obfuscated X  attack on the user-based kNN algorithm, in which attacks consist of genuine user profiles, whose rating for the target item has been set to maximum. A 3% attack is performed on 100 ran-domly selected items from the Movielens dataset, as before. The resulting mean prediction shift is 0.42 and the uplift in hits is 9.5%. This confirms that the user-based algorithm has an inherent vulnerability to profile-injection attack. Con-trary to work such as [12], it is not safe to assume that other algorithms do not exhibit such vulnerability. The attacker
Figure 6: Unsupervised Detection of AoP Attack can tailor attacks to the recommendation algorithm of the system under attack. We have shown in other work how it is possible to design attack profiles tailored to model-based recommendation algorithms. A stealthy attacker will con-sider attack effectiveness together with attack detectability, considering all available knowledge of the recommendation algorithm and the database profile statistics. The system designer, therefore must design detection strategies that are effective against such sophisticated attacks. In the end, at-tack detection must be based on the statistical differences between genuine and attack profiles and N-P detectors pro-vide an optimal way to detect such differences, once good statistical models are obtained.
In this paper, we have discussed profile injection attacks and developed statistical models for their detection. It is clear that attack and detection can result in an arms race scenario between the system manager and attacker. It is important to know the limits of this arms race and work such as [15] has provided an interesting theoretical basis. Nevertheless, our work shows that the optimal attacks are not random, but exploit explicit vulnerabilities in the rec-ommendation algorithms. Further work to understand the limits of such informed attack scenarios is required and will be the focus of our future research.
 This work is supported by Science Foundation Ireland, grant number 07/RFP/CMSF219. [1] C. A.Williams, B. Mobasher, and R. Burke. Defending [2] K. Bryan, M. O X  X ahony, and P. Cunningham.
 [3] R. Burke, B. Mobasher, and C. Williams.
 [4] J. Canny. Collaborative filtering with privacy via [5] P.A.Chirita,W.Nejdl,andC.Zamfir.Preventing [6] C. Dellarocas. Immunizing on X  X ine reputation [7] G. Karypis and V. Kumar. Modularity and [8] S. K. Lam and J. Riedl. Shilling recommender systems [9] B. Mehta, T. Hofmann, and P. Fankhauser. Lies and [10] B. Mehta and W. Nejdl. Unsupervised strategies for [11] B. Mobasher, R. Burke, R. Bhaumik, and C. Williams. [12] B. Mobasher, R. D. Burke, and J. J. Sandvig.
 [13] M.P.O X  X ahony,N.J.Hurley,andC.C.M.Silvestre.
 [14] M.P.O X  X ahony,N.J.Hurley,andG.C.M.Silvestre.
 [15] P. Resnick and R. Sami. The influence limiter: [16] C. Williams, B. Mobasher, R. Burke, R. Bhaumik,
