 This paper proposes a strategy to reduce the amount of hardware involved in the solution of search engine queries. It proposes using a secondary compact cache that keeps mini-mal information stored in the query receptionist machine to register the processors that must get involved in the soluti on of queries which are evicted from the standard result cache or are not admitted in it. This cache strategy produces exact answers by using very few processors.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Search process Algorithms, Performance Inverted Files, Parallel and Distributed Computing
Web search engine users tend to submit identical queries which are frequent enough to allow the efficient use of several levels of caching. The answers to popular queries are kept stored in a result cache (RCache) in the query reception-ist machine which we call the broker . When a query is not found in the RCache, the broker broadcasts it to P proces-sors for results calculation where each processor contains an inverted file which indexes a fraction 1 /P of the document collection. This index is used to speed-up the processing of queries and each processor can also have a cache memory with pre-computed atomic units of answers such as inter-sections and pieces of the index which are frequently hit by queries. We propose including an additional but highly compact cache in the broker to help it to select a reduced number of processors where to send the queries not found in the RCache. In one case, under steady state query traffic, these processors are the ones capable of providing the exact top-R results of the query. This improves query throughput since the processors that are not able to place documents into the top-R results are not contacted. In a second case, under sudden peaks in query traffic, the selected processors are the most likely to provide a good approximated answer to the query. In this case it is critical to avoid sending the query to all of the processors in order to prevent the sys-tem from saturation (this second case is treated in detail in a companion paper presented in [2]). We call this cache location cache or last-resort cache (LCache).

Notice that the average number of processors that con-tribute to the global top-R results for queries can be reduce d by doing proper distribution of document IDs onto proces-sors. Nevertheless, the computations performed by a single processor are still very expensive because of document in-tersection and ranking, and in fact the processor could also keep in an additional cache the set of relevant document IDs that are in the top-R results for the query [2]. The LCache approach requires the use of document clustering methods to evenly distribute the generated clusters of documents onto the processors. The clusters can be built from a subset of the whole document collection whilst the remaining docu-ments are distributed at random. The subset can be taken from a large query log and queries themselves can be used to correlate documents during the clustering process. This idea has been successfully used in the context of Web search engines by several authors with different motivations [5].
The data set used to evaluate the different alternative de-signs for the LCache corresponds to a sample of a query log file from the year 2009 and contains 2,109,198 distinct queries and 3,991,719 query instances. These queries were selected in chronological order from a bigger query log of 31,532,195 queries. They come from an actual log of the Yahoo! search engine. The vocabulary of the 3.9 million sample is compound by 239,274 distinct query terms. For each one of these queries we obtained the top-50 documents by using the Yahoo! BOSS API . The generated document collection corresponds to 71,429,721 documents. The API uses the Yahoo! services to get the same answers that are presented to actual users by the Yahoo! search engine.
We used an actual implementation of a document parti-tioned inverted file deployed in a cluster of 400 processors to illustrate the practical gain in performance under a pes-simistic scenario in which no cache of any kind is used. In Fi-gure 1 [Left] we show results obtained by executing upon the distributed inverted file a total of 20,000  X  P unique queries taken from the log of 31,532,195 queries, where P is the number of processors. The bars show the potential gain in performance defined as the ratio ( t P  X  t 3 ) /t P where t total running time for a case in which the query is serviced by 3 processors selected at random and t P is total running time for the case in which each query is broadcast to all P processors for answer calculation (below we show that the document clustering methods used in the experiments are able to reduce to about 3 the average number of processors per query). The results show that even in this case, sending queries to few processors has a positive impact in overheads reduction (about 18% for P = 400).

To evaluate the effect of reducing the average number of processors under situations of peaks in query traffic we re-sorted to a discrete-event simulator as the only way to re-produce exactly the same scenario for the different strategi es tested and be able to evaluate our metrics. The simulator implements the scheme in which the broker sends queries to P processors and waits for the answers in a concurrent man-ner. It is a multi-threaded simulator implemented by using co-routines to build concurrent objects of various types. A query-generator object simulates the arrival of queries us ing an actual work-load depicted in Figure 1 [Right]. The ob-jects simulating processors and devices were tuned up with the actual implementation mentioned above. In each pro-cessor we have a thread in charge of receiving the query and simulating its processing. We adjusted the processing time to achieve a steady state when we inject the average rate of queries over all the intervals shown in the Figure 1 [Right]. To deal with peaks in traffic we implemented a strategy similar to the one proposed in [6]. Each processor keeps a priority queue which maintains the queries waiting to receive service (we set a limit in the number of threads allowed to be active at any time in each processor). When the broker sends a query to a set of processor it assigns a priority order to the n processors to be contacted by the query. This is a linearly decreasing value between 1 and 1/ n . Each processor serves first the queries with the highest priorities. We measure efficiency at regular intervals which is defined as the average number of queries being serviced in the processors divided by the maximum over the proces-sors. Throughput is also measured at regular intervals and is defined as the number of completed queries divided by the length of the interval.
The LCache is meant to contain the most minimal data about the answer of each cached query, namely the list of processor IDs from which each document in the global top-R results come from, and it can be administered with any existing caching policy. As we describe below we tested our LCache strategy by using a hybrid static/dynamic caching policy proposed in [3]. Notice that if we distribute related document clusters into P processors and the remaining ones (sorted by centers similarity distance) in contiguous-ID p ro-cessors, then the lists of processors stored in the LCache ar e highly compressible [7]. This is useful for the static part o f the LCache. For the dynamic part, one can impose an up-per limit on the length of the list of processor IDs allowed to reside in the LCache. Moreover, because of clustering, the length of the list of processors IDs is expected to be smaller than R. Notice that the LCache can also work in a setting in which documents are evenly distributed at random onto the processors. Here small memory space is still feasible since for large scale systems the number of processors is expected to be larger than the number of global top-R results for queries that are presented to the users. This at least for the first page of results (efficient methods for the subsequent pages of results can be handled with better methods as discussed in [5]). In other words, it is perfectly feasible to configure a s ys-tem so that an entry in the LCache requires much less space than a corresponding entry in the result cache (RCache). The size of an entry in the RCache is expected to be of a few KBs which are used to store the result URLs and result snippets [3]. Our target is systems in which the RCache and LCache are kept in the broker machine, possibly in a scheme that combines main and secondary memory.

The LCache works in tandem with the result cache kept at the broker side and it has the following two uses. The first one is as a scheme able to further improve query through-put whilst it still allows the broker to provide exact answer s to queries. This can be made in at least two alternative ways. Firstly, as argued in [3], storing a query in the result cache should consider the cost of computing its answer upon the processors since the main objective of the result cache is improving query throughput. Queries requiring a com-paratively less amount of computing time can be prevented from being stored in the result cache. We suggest using the LCache for storing those queries which can lead to a poten-tial gain in throughput as they can now be sent directly to the processors capable of providing the global top-R result s. Secondly, independent of the kind of queries stored in the LCache, their computing time can be further reduced at the cost of more memory per LCache entry by storing R pairs (procID, docID) corresponding to the global top results of the respective query. The idea is to use each LCache en-try to go directly to the processor and retrieve the specific document in order to get the snippet and other data re-quired to construct the answer presented to the user. This only requires secondary memory operations and no docu-ments ranking is necessary. The pairs (procID, docID) are also highly compressible since we can re-numerate the docu-ment IDs at each processor following the order given by the document clusters stored in them. In any case, the source processor IDs are necessary to support the semantic part of the LCache discussed in [2].

As proposed in [3], it is possible to efficiently combine two caching strategies A and B as follows. Each query maintains a score for strategies A and B. When we evict an item from A, we try to store it in B if its score in B is higher than the lowest one in B. This causes the eviction of an item from B, which we try to store in A and so on. This eventually stops since in each cycle the scores of the evicted items grow up. The experimental results of [3] report a realization of a static-dynamic cache as the most efficient strategy in which the static part is served by a LFU (Least Recently Used) policy in which the score for each query is calculated as the product between the query frequency and an estimation of the cost of solving the query. The dynamic part is im-plemented by using the Landlord policy which grants each query in the cache a credit to  X  X tay-in X  X hich is proportiona l to the cost of solving the query. This cost is proportional to the length L i of the posting lists in the inverted file in-dex. In the experiments presented below for the combina-tion RCache/LCache we use a similar approach, namely in each cache we keep a static section and a dynamic one. The cost of each section is a function of the number of proces-sors required to solve a query stored in either cache. For the LFU-RCache we use cost = frequency  X  log( L )  X  n where L is the sum of the lengths of the posting lists associated with the query terms and n the number of processors used to get the global top-R results for the query. For the LFU-LCache we use cost = frequency  X  log( L )  X  ( P/n ) which gives higher priority to the frequent queries that require fewer proces-sors and yet are costly to be solved. We use log( L )  X  n and log( L )  X  ( P/n ) for the dynamic parts Landlord-RCache and Landlord-LCache respectively. In the following, we evalu-ate the effects of both parts separately since [3] has already shown that a cache which combines the static LFU (80%) and dynamic Lanlord (20%) policies achieves efficient per-formance on actual collections and Web queries.

If we define f R as the average fraction of queries that hit the RCache, then the reduction of processors used to solve a stream of queries is given by (1  X  f R )  X  P , and in the same way a combination RCache/LCache reduces this quantity to (1  X  f R  X  f L )  X  P + ( f L  X  n L ) where f L is the fraction of queries that hit the LCache and n L the average number of processors required by the queries stored in the LCache. The size of either cache has a direct impact in the hit ratio, for instance [3] reports of hits ratios above 50%. Thus it is more interesting to observe the relation between f R and f
L for a set of queries at a fixed f R . This having in mind the cost functions used to store queries in either RCache or LCache.

The results using the co-clustering algorithm used in [6] are presented in Figure 2. These results show that f L achieves competitive hits ratios. In all cases, the space used by the LCache is less than 10% of the space used by the RCache and the average value of n L that we observed in the experi-ment is 1.87 (i.e., queries in the LCache) whereas 2.9 is the average for the number of processors required by any query. This means that the LCache properly privilege small proces-sor queries. These results are for queries with top-10 resul ts and P = 16 and P = 128 (processors indicated as [16] and [128] in the figure).

The following results are for a situation in which we just place the most frequent queries in the LCache. We are interested in observing the average number of processors that different strategies for document clustering are able to achieve. This is presented in Figure 3 which shows re-sults for (1) random assignment of documents to processors Figure 2: Values for the ratio h L / ( h L + h R ) where h is LCache hits and h R is RCache hits. Figure 3: Average number of processors demanded by different document distribution methods. (RAND), (2) distribution based on the lexicographic sort of the document URLs (Sort-URL), (3) clustering of URLs by a method LC explained in the Appendix, (4) document clustering by using LC and a 31 millions query top-50 an-swers log (LC), and (5) co-clustering using the same answers log (CC). The results are for top-10 answers and top-30 an-swers. Notice that the LC method is more scalable than the co-clustering method of [1] and used by [5, 6]. For large number of processors P = 128, an interesting advantage of the LC-URL alternative is that it makes clustering of doc-ument URLs, which means that it does not depend of any particular query answers log. The LC method is also highly parallelizable [4] and the Appendix we explain how to ex-tend this clustering method to achieve the results presente d in the Figure 4. Those results show that URL-LC is able to achieve competitive performance.
 Finally, Figure 5 shows how the combination RCache and LCache with different strategies of clustering is able to ach ieve efficient performance under severe peaks in query traffic. The figure shows a given time interval which is fairly repre-sentative to all others. These results show that the random strategy and the one which only uses the RCache get satu-rated as they are not able to follow the variations in traffic. Appendix The List of Clusters (LC) data structure [4] is formed from a set of centers which are chosen from a co-llection of objects by using a heuristic that maximizes the sum of the distances of each new center to all current centers . Each database object x is attached to the cluster whose cen-
Figure 4: Simulated throughput and efficiency. ter c is the closest one to x . There exists a distance function d ( x, c ) which determines the distance between two objects x and c . To achieve good load balance we use clusters of fixed size K . Thus the extent of a cluster with center c is given by its radius r c which is the distance between the center c and its K -nearest neighbor.

We work with the concept of hyper-clusters and super-clusters . The first ones represent the intuition that LC-clusters that are close each other are very likely to get in-volved in the solution of a query by providing database ob-jects that are within the query ball, where by  X  X all X  we refer to the top-R results for the query. On the other hand, the second ones represent the intuition that user queries tend to be highly skewed and it is desirable to avoid the imbal-ance caused by many queries being directed to one or few processors whilst the other ones are less loaded. Thus the objective is to assemble highly correlated LC-clusters int o hyper-clusters whereas in super-clusters to assemble high ly un-correlated hyper-clusters. The total number of super-clusters is P , that is, the proposed algorithm ends up with one super-cluster per processor.
 To build the hyper-clusters we use the same LC algorithm. In this case, however, the objects to be hyper-clustered are the LC-clusters. Each LC-cluster i is defined by the tuple ( c , r i , B i ) with c i being the database object selected as the LC-cluster center, B i is the set of database objects stored in the LC-cluster i , and r i the covering radius, namely the distance between c i and the object in B i whose distance to c is the largest one.

The hyper-clusters of size K are formed as follows. The first hyper-center h 1 is a LC-cluster selected at random. Then the K  X  1 nearest neighbors to hyper-cluster h 1 are found and removed from the set of remaining LC-clusters. In this case, the distance between each LC-cluster i and the center h 1 is calculated as d ( h 1 , c i ) + r i . Once the K LC-clusters (center h 1 included) have been removed from the set of LC-clusters, the next one c i to be selected as the next hyper-center is the one with the maximum value d ( h 1 , c (no matter if d ( h, c i ) &lt; r i ). The following ones are the LC-clusters that maximize the sum d ( h k , c i )  X  r i for each current hyper-cluster k . Thus, the second hyper-center h 2 is the far-thest LC-cluster to h 1 . The third one h 3 is the one c j the greatest sum d ( h 1 , c j )  X  r j + d ( h 2 , c j )  X  r until obtaining N H = N G /K hyper-clusters where N G is the initial total number of LC-clusters and usually N H &gt; P . We set K to have N H = max { N G /P 2 , 200 } , value for which we obtained good results for the databases we tested (we exper-imented with P = 4 , ..., 128). Given the way we build the P super-clusters, for larger number of processors one should ensure N H  X  2 P .

Finally we set P super-clusters, one per processor, and apply the following strategy to distribute the hyper-clust ers onto the super-clusters. We take a hyper-cluster h 1 selected at random and place it in the first super-cluster. We then sort in ascending order the remaining hyper-clusters h k by using as sorting keys the values d ( h 1 , h k ). From the resulting sorted set we remove from it the first P  X  1 hyper-clusters and place them consecutively in the next P  X  1 processors, one per processor. We repeat the procedure by setting h 2 as the first hyper-center in the remaining sorted set (i.e., w e make h 2 to be the P -st hyper-center in the initial sorted set). We now sort by using as keys the values d ( h 2 , h k ) where h are the remaining hyper-clusters in the sorted set and repea t the procedure until mapping to processor the complete set of hyper-centers.

This ends up with all LC-clusters evenly distributed onto the processors. The balance in terms of number of LC-clusters per processor is almost perfect. [1] I. Dhillon, S. Mallela and D.S. Modha.
 [2] F. Ferrarotti, M. Marin and M. Mendoza. A [3] Q. Gan, T. Suel, Improved Techniques for Result [4] V. Gil-Costa, M. Marin and N. Reyes. Parallel query [5] D. Puppin, F. Silvestri, R. Perego, and [6] D. Puppin, F. Silvestri, R. Perego, and [7] H. Yan, S. Ding and T. Suel Inverted index
