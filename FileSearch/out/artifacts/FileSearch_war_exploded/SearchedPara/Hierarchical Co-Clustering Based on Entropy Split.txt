 Two dimensional contingency tables or co-occurrence matrices arise frequently in various important applications such as text analy-sis and web-log mining. As a fundamental research topic, co-clustering aims to generate a meaningful partition of the contingen-cy table to reveal hidden relationships between rows and columns. Traditional co-clustering algorithms usually produce a predefined number of flat partition of both rows and columns, which do not reveal relationship among clusters. To address this limitation, hi-erarchical co-clustering algorithms have attracted a lot of research interests recently. Although successful in various applications, the existing hierarchial co-clustering algorithms are usually based on certain heuristics and do not have solid theoretical background.
In this paper, we present a new co-clustering algorithm with sol-id information theoretic background. It simultaneously constructs a hierarchical structure of both row and column clusters which retain-s sufficient mutual information between rows and columns of the contingency table. An efficient and effective greedy algorithm is developed which grows a co-cluster hierarchy by successively per-forming row-wise or column-wise splits that lead to the maximal mutual information gain. Extensive experiments on real datasets demonstrate that our algorithm can reveal essential relationships of row (and column) clusters and has better clustering precision than existing algorithms.
 I.5.3 [ Pattern Recognition ]: Clustering-algorithms Algorithms, Experimentation, Theory Co-clustering, Entropy, Contingency Table, Text Analysis
Two dimensional contingency table arises frequently in various applications such as text analysis and web-log mining. Co-clustering algorithms have been developed to perform two-way clustering on both rows and columns of the contingency table. Traditional co-clustering algorithms usually generate a strict partition of the table [4, 6, 12]. This flat structure is insufficient to describe relationships between clusters. Such relationships are essential for data naviga-tion, browsing, and so forth in many applications such as document analysis.

To combine the advantages of both co-clustering and hierarchical clustering, various hierarchical co-clustering algorithms have been recently proposed [8, 3, 15, 1, 9]. However, existing hierarchi-cal co-clustering algorithms are usually based on certain heuristic criteria or measurements for agglomerating or dividing clusters of rows and columns. Such criteria may be domain-specific and thus suffer from lacking of generality. Another limitation of many exist-ing hierarchical co-clustering algorithms is that they often require the number of clusters (for both rows and columns) as an input. However, accurate estimation of the number of clusters may be im-possible in many applications, or require a pre-processing stage.
To overcome these limitations, we propose a hierarchical co-clustering algorithm with solid information theoretic background. Our approach aims to generate the simplest co-cluster hierarchy that retains sufficient mutual information between rows and column-s in the contingency table. More specifically, the mutual informa-tion between resulting row clusters and column clusters should not differ from the mutual information between the original rows and columns by more than a small fraction (specified by the user). Find-ing the optimal solution for this criterion however would take ex-ponential time. Thus, we devise an efficient greedy algorithm that grows a co-cluster hierarchy by successively performing row-wise or column-wise splits that lead to the maximal mutual information gain at each step. This procedure starts with a single row cluster and a single column cluster and terminates when the mutual infor-mation reaches a threshold. Other termination criteria (such as the desired number of row/column clusters) can be easily incorporated.
In summary, our hierarchical co-clustering algorithm has the three advantages: (1). Our algorithm builds cluster hierarchies on both rows and columns simultaneously. The relationships between clus-ters are explicitly revealed by the hierarchies. And the hierarchical structures inferred by our approach are useful for indexing and vi-sualizing data, exploring the parent-child relationships, and deriv-ing generation/specialization concepts. (2). Our algorithm uses an uniform framework to model the hierarchical co-clustering prob-lem, and the optimality of splitting the clusters is guaranteed by rigorous proofs. (3). Our algorithm does not necessarily require prior knowledge of the number of row and column clusters. In-stead, it uses a single input, the minimum percentage of mutual information retained, and automatically derives a co-cluster hierar-ch y. Moreover, it is also flexible to incorporate optional constraints such as the desired number of clusters.

Experiments on real datasets show that our hierarchical co-clustering algorithm performs better than many existing co-clustering algo-rithms.
Co-clustering is a branch of clustering methods that clusters both rows and columns simultaneously. The problem of co-clustering has been studied extensively in recent literatures. These clustering algorithms generate flat partitions of rows and columns. However, a taxonomy structure can be more beneficial than a flat partition for many applications such as document clustering. In this section, we present a survey of recent co-clustering algorithms.

A pioneering co-clustering algorithm based on information theo-ry was proposed by Dhillon et.al. in [6]. Taking the numbers of row and column clusters as input, the algorithm generate a flat partition of data matrix into row clusters and column clusters which max-imizes the mutual information between row and column clusters. The idea is generalized into a meta algorithm in [2]. It is proven in [2] that besides mutual information, any Bregman divergence can be used in the objective function and the two-step iteration al-gorithm can always find a co-cluster by converging the objective function to a local minimum. Even though the co-clustering model proposed in these papers is also rooted in information theory, such as mutual information and relative entropy, our approach generates hierarchical cluster structures. This key difference entails differen-t objective functions and, more importantly, different optimization techniques. In addition, linear algebra methods are also applied to derive co-clusters[4, 12].

By integrating hierarchical clustering and co-clustering, hierar-chical co-clustering aims at simultaneously constructing hierarchi-cal structures for two or more data types. Hierarchical co-clustering has recently received special attentions [8, 3]. A hierarchical divi-sive co-clustering algorithm is proposed in [15] to simultaneously find document clusters and the associated word clusters. It has also been incorporated into a novel artist similarity quantifying frame-work for the purpose of assisting artist similarity quantification by utilizing the style and mood clusters information [1]. Both hierar-chical agglomerative and divisive co-clustering methods have been applied to organize the music data [9].
We denote the two-dimensional contingency table as T . R = f r 1 ; r 2 ; :::; r n g represents the set of rows of T , where r row. C = f c 1 ; c 2 ; :::; c m g represents the set of columns, where c is the j th column. The element at the i th row and j th column is denoted by T ij . For instance, in a word-document table, each document is represented by a row and each word maps to a column. Each element stores the frequency of a word in a document.
We can compute a joint probability distribution by normalizing elements in the table. Let X and Y be two discrete random vari-ables that take values in R and C respectively. The normalized table can be considered as a joint probability distribution of X and Y . We denote p ( X = r i ; Y = c j ) by p ( r i ; c j ) for convenience in the remainder of this paper.

A co-cluster consists of a set of row clusters and a set of column clusters. We denote the set of row clusters as ^ R , where ^ r i represents the i th row cluster.

Similarly, we denote the set of column clusters as ^ C , where ^ c j represents the j th row cluster. We denote the number of clusters in ^ R as L ^ R = j ^ R j , and the number of clusters in as L ^ C = j ^ C j . Given the sets of row and column clusters, a co-cluster can be considered as a  X  X educe X  table ^ T from T . Each row (column) in ^ T represents a row (column) cluster. Each element in ^ T is the aggregation of the corresponding elements in T ,
Let ^ X and ^ Y be two discrete random variables that take values in ^ R and ^ C respectively. A normalized reduced table can be consid-ered as a joint probability distribution of ^ X and ^ Y . We will denote p ( ^ X = ^ r i ; ^ Y = ^ c j ) by p (^ r i ; ^ c j ) for convenience.
Note that the original contingency table can be viewed as a co-cluster by regarding each single row (column) as a row (column) cluster. Given any co-cluster ( ^ R; ^ C ) on a contingency table, we employ the mutual information between ^ X and ^ Y to measure the relationship between row clusters and column clusters.
 As you may observe, the mutual information of the original table I ( X; Y ) is larger than the mutual information of the aggregated co-clustering described in Theorem 3.1.

In order to prove Theorem 3.1, we first prove the following lem-mas based on the theorems proven by Dhillon et.al. [6].

L EMMA 3.1. Given two co-clusters, f ^ R (1) ; ^ C (1) g and where ^ R (2) is generated by splitting a row cluster in ^ Proof : Assume that ^ R (2) is generated by splitting ^ r ^ r 1 and ^ r
Because ^ r (2) 1 [ ^ r (2) 2 = ^ r (1) 1 , we have Therefore, where D ( p (^ r (2) 1 ; ^ c (1) ) jj p (^ r (1) 1 ; ^ c (1) divergence) between p (^ c (1) j ^ r (2) 1 ) and p (^ c (1) j non-negative (by definition). Therefore and then
Similarly , we have
L E MMA 3.2. Given two co-clusters, f ^ R (1) ; ^ C (1) g , and ^ C (2) is generated by splitting one column cluster in
The above two lemmas state that splitting either row or column-wise clusters increases the mutual information between the two set-s of clusters. Hence, we can obtain the original contingency table (i.e., each row/column itself is a row/column cluster) by performing a sequence of row-wise splits or column-wise splits on a co-cluster. By Lemmas 3.1 and 3.2, the mutual information monotonically in-creases after each split, which leads to the following theorem.
T HEOREM 3.1. The mutual information of a co-clustering, I ( always increases when any one of its row or column clusters is split, until the mutual information reaches its maximal value, I ( X; Y ) , where each row and column is considered as a single cluster.
The monotonicity property of mutual information leads to the following problem definition.
 Given a normalized two-dimensional contingency table, T , and a threshold (0 &lt; &lt; 1) , find a hierarchical co-clustering contain-ing a minimum number of the leaf row clusters ^ R and leaf column clusters ^ C , such that the mutual information corresponding to co-specify desired number of row or column clusters ( L ^ R = max or L ^ C = max c ) and ask for a co-cluster with maximal mutual information.
In this section, we present the details of our co-clustering algo-rithm. The monotonicity property of mutual information stated in Lemmas 3.1 and 3.2 inspires us to develop a greedy divisive algo-rithm that optimizes the objective function I ( ^ X; ^ Y ) at each step. The main routine is presented in Section 4.1.
The pseudocode of the algorithm is shown in Figure 1. In Step 1 of the main function Co-Clustering() , function InitialSplit() is called to generate the initial co-cluster f ^ R (0) ; ^ C clusters and two column clusters. In Step 2, the joint distribution p ( ^
X; ^ Y ) of this initial co-cluster is calculated. Then the algorithm goes through iterations. During each iteration, a split is performed to maximize the mutual information of the co-cluster. In Steps 5 and 6, each row or column cluster s i is examined by function S-plitCluster() to determine the highest gain in mutual information, I i , which can be brought by an optimal split on s i . ( s denote the resulting clusters after split.) Steps 7 to 9 select the row or column cluster whose split gives the highest gain I ( k ) form the split. In Step 10, the joint distribution p ( ^ X; according to the new co-cluster f ^ R ( k +1) ; ^ C ( k +1) g m continues until the mutual information ratio I ( ^ X; ^ threshold, , and/or the number of clusters (row or column) reaches the number of desired clusters, denoted by max c and max r that the termination condition can be easily modified to suit users X  needs.
Function InitialSplit() splits the contingency table into two row clusters and two column clusters. In Step 1, the joint distribution is set to the normalized table T . In Step 2, all rows are considered as in a single row cluster s 1 and all columns are considered as in a sin-gle column cluster s 2 . They are then split in Steps 3 and 4 by call-ing the function SplitCluster() . The initial co-cluster f ^ and the corresponding mutual information I (0) = I ( ^ X (0) are calculated accordingly in Steps 5 and 6.

Note that we split both row clusters and column clusters in this initial step. To ensure a good initial split, when the function Split-Cluster() is called, we tentatively treat each row as an individual cluster so that the initial column clusters are created by taking into account the row distribution. By the same token, we also tentative-ly treat each column as an individual cluster when we create the initial row clusters.
According to Lemmas 3.1 and 3.2, a split will never cause I ( decrease. In addition, only the split cluster may contribute to the increase of I ( ^ X; ^ Y ) . Suppose that a row cluster ^ r ^ r 1 and ^ r
Therefore, SplitCluster() can calculate the maximal value of I by examining each cluster to be split separately. However, it may still take exponential time (with respect to the cluster size) to find the optimal split. Therefore, SplitCluster() adopts a greedy algo-rithm that can effectively produce a good split achieving a local maximum in I . Elements in the cluster are initially randomly grouped into two sub-clusters. A sequence of iterations are tak-en to re-assign each element to its closer sub-cluster according to KL-divergence until I converges.
 The details of function SplitCluster () are shown in Figure 2. In Step 1, the joint probability distribution p ( ^ X; ^ Y ) is transposed if the input cluster s is a column cluster so that column clusters can be split in the same way as row clusters. In Step 2, cluster s is ran-domly split into two clusters. In Step 3, I is calculated according to Equation 1, and the weighted mean conditional distributions of ^ Y for both clusters s 1 and s 2 ( p ( ^ Y j s 1 ) and p ( according to Equation 2.
From Step 5 to Step 7, each element x i in cluster s is re-assigned to the cluster ( s 1 or s 2 ) which can minimize the KL-Divergence between p ( ^ Y j x i ) and p ( ^ Y j s j ) . p ( ^ Y j s dated at the end of each iteration. The procedure repeats until I converges. In Step 9, the two sub-clusters s 1 and s 2 , and I are re-turned. In order to prove that function SplitCluster () can find a split that achieves local maximum in I , we need to prove that the re-assignment of element x i in Steps 4-8 can monotonically increase I . Since the same principle is used to split row clusters and col-umn clusters, without loss of generality, we only prove the case of splitting row clusters.

A similar cluster split algorithm was used in [5] which re-assigns elements among k clusters. It is proven that such re-assignment can monotonically decrease the sum of within-cluster JS-divergence of all clusters which is
In our function SplitCluster() , we only need to split the cluster into two sub-clusters. Therefore, we show the proof for a special case where k = 2 . The following lemma was proven in [5].

L EMMA 4.1. Given cluster s containing n elements ( ( ^ Y the weighted mean distribution of the cluster ( ( ^ Y j s ) ) has the low-est weighted sum of KL-divergence of p ( ^ Y j s ) and p ( is, 8 q ( ^ Y ) , we have
T HEOREM 4.1. When splitting cluster s into two subclusters, s and s 2 , the re-assignment of elements in s as shown in Steps 5-7 of function SplitCluster() can monotonically decrease the sum of within-cluster JS-divergence of the two sub-clusters, s cluster JS-divergence of the two clusters before and after the l assignment of elements, respectively. And let p l ( ^ Y j be the corresponding weighted mean conditional distributions of sub-clusters before and after the l th re-assignment. We will prove that Q l +1 f s 1 ; s 2 g Q l f s 1 ; s 2 g . Assume that the two clusters after reassignment are s 1 and s 2 .

The first inequality is a result of Step 6 in SplitCluster() and the second inequality is due to Step 7 in SplitCluster() and Lemma 4.1. Therefore, we prove that the re-assignment of elements in s can monotonically decrease Q ( f s 1 ; s 2 g ) .

Note that the sum of I and Q ( f s 1 ; s 2 g ) is a constant [5], which is shown in Equation 3.
Therefore, since the re-assignment process monotonically de-creases Q ( f s 1 ; s 2 g ) , it will monotonically increase I as a re-sult. Thus function SplitCluster () can find a split that achieves local maximum in I .
In this section, we perform extensive experiments on real data to evaluate the effectiveness of our co-clustering algorithm. In Sec-tions 5.2, we use real datasets for experiments. We compare the quality of the clusters generated by our method with those gener-ated by previous co-clustering algorithms. We use micro-averaged precision [6, 12] as the quality measurement. In this section, we describe the experimental settings. We use 20 Newsgroup dataset from UCI 1 . We preprocess the 20 Newsgroup dataset to build the corresponding two dimensional contingency ta-ble. Each document is represented by a row in the table and 2000 distinct words are selected to form 2000 columns. Words are select-ed using the same method as in [14]. In order to compare the qual-ity of clusters generated by our method with those of previous al-gorithms, we generate several subsets of the 20 Newsgroup dataset using the method in [12, 14, 6]. Each subset consists of several major newsgroups and a subset of the documents in each selected newsgroups. The details are listed in Table 1. As in [12, 14, 6], each of these subsets has two versions, one includes the subject lines of all documents and the other does not. We use dataset subject dataset to denote these two versions respectively. http://kdd.ics.uci.edu/databases/20ne wsgroups/20newsgroups.html
In this section, we compare our co-clustering algorithm with sev-eral previous algorithms. For all the datasets, we empirically set = 0 : 7 in our algorithm.

The state of the art co-clustering algorithms used for comparison are: (1). NBVD [12]: Co-clustering by block value decomposition. This algorithm solves the co-clustering problem by matrix decom-position. (2). ICC [6]: Information-theoretic co-clustering. This algorithm is also based on information theoretic measurements and considers the contingency table as a joint probability distribution. (3). HCC [11]: a hierarchical co-clustering algorithm. HCC brings together two interrelated but distinct themes from clustering: hi-erarchical clustering and co-clustering. (4). Linkage [7]: a set of agglomerative hierarchical clustering algorithms based on linkage metrics. Four different linkage metrics were used in our experi-ments, i.e., Single-Link , Complete-link , UPGMA (average), WPG-MA (weighted average).

There are other existing co-clustering/clustering algorithms, such as [14, 10, 13], which conducted experiments on the same subset-s in Table 1. Since NVBD and ICC outperform these algorithms in terms of micro-averaged precision, we will not furnish a direct comparison with them. For convenience, we use HICC to represent our algorithm and use m-pre to represent micro-averaged precision. For the number of word clusters, ICC generates about 60 100 word clusters as reported in [6] while our algorithm HICC gener-ates about 50 80 word clusters. The number of word clusters generated by NVBD is not reported in [12]. While in the Linkage algorithms, since they only cluster the rows, each column can be considered as a column cluster. The comparison of micro-averaged precision on all datasets in Table 1 is shown in Table 2. In Table 2, the micro-averaged precision decrease slightly after we merge our original clusters into the same number of clusters as NVBD and IC-C. This is because cluster merge may over-penalize the incorrectly labelled documents. Nevertheless, our algorithm is still the winner in all cases. The Single-linkage metric has a very low precision comparing with all other algorithms. The reason may be that us-ing the shortest distance between two clusters as the inter-cluster distance suffers from the high dimensionality and the noise in the dataset.
In this paper, we present a hierarchical co-clustering algorithm based on entropy splitting to analyze two-dimensional contingency tables. Taking advantage of the monotonicity of the mutual infor-mation of co-cluster, our algorithm uses a greedy approach to look for simplest co-cluster hierarchy that retains sufficient mutual in-formation in the original contingency table. The cluster hierarchy captures rich information on relationships between clusters and re-lationships between elements in one cluster. Extensive experiments demonstrate that our algorithm can generate clusters with better precision quality than previous algorithms and can effectively re-veal hidden cluster structures.
