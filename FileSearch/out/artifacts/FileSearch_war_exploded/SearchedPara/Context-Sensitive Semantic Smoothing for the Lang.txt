 Semantic smoothing, which in corporates synonym and sense information into the language mode ls, is effective and potentially significant to improve retrieval performance. The implemented semantic smoothing models, such as the translation model which statistically maps document terms to query terms, and a number of works that have followed have shown good experimental results. However, these models are unable to incorporate contextual information. Thus, the resulting tran slation might be mixed and fairly general. To overcome this limita tion, we propose a novel context-sensitive semantic smoothing method that decomposes a document or a query into a set of weighted context-sensitive topic signatures and then translate those topic signatu res into query terms. In detail, we solve this problem through (1) choosing concept pairs as topic signatures and adopting an ontology-based approach to extract concept pairs; (2) estimating the translation model for each topic signature using the EM algorithm; and (3) expanding document and query models based on topic si gnature translations. The new smoothing method is evaluated on TREC 2004/05 Genomics Track collections and significant impr ovements are obtained. The MAP (mean average precision) achieves a 33.6% maximal gain over the simple language model, as well as a 7.8% gain over the language model with context-insensitive semantic smoothing. H.3.3 [ Information Search and Retrieval ]: Retrieval Models X  language models Algorithm, Experimentation, Performance Language Models, Information Re trieval, Genomic Information Retrieval, Semantic Smoothing, Topic Signature, Concept Pair initially proposed by Ponte and Croft [14], has been popular with the IR community in recent years due to its solid theoretical foundation and promising empirical retrieval performance. In essence, this approach centers on the document model estimation and the query generative likelihood calculation for ranking according to the estimated model. However, it is challenging to estimate an accurate document model. On one hand, because the query terms may not appear in the document, we need to assign a reasonable non-zero probability to the unseen terms. On the other hand, we need to adjust the probability of the seen terms to remove the effect of the background model or even irrelevant noise. Thus, the core of the language modeling approach to IR is to  X  X mooth X  the models. Zhai and Lafferty [16, 18] propose seve ral effective smoothing techniques that interpolate the document m odel with the background collection model. smoothing that incorporates synonym and sense information into the language model [10]. Berger and Lafferty [2] incorporate a kind of semantic smoothing into the language model by statistically mapping document terms onto query terms us ing a translation model trained from synthetic document-query pair s. The translation model is context-insensitive (i.e., it cannot incorporate sense and other contextual information into th e language model), however, and therefore the resulting translation ma y be mixed and fairly general. For example, the term  X  mouse  X  without context may be translated to both  X  computer  X  and  X  cat  X  with high probabilities. Jin [9] and Cao [3] present two other ways to trai n the translation models, but they still have the same context-insensitivity problem as [2]. flexible language model called KL-d ivergence retrieval model as a special case of their risk minimization retrieval framework. KL-divergence retrieval estimates the query model as well as the document model. Like the documen t model estimation, a typical method for query model estimation is to statistically translate the terms in the original query into other terms [1, 10]. In this paper, we also refer to the translation-based query model estimation as semantic smoothing . Similarly, if the translation model is context-insensitive, the resulting query mode l may be very general. Thus, it is urgent to develop a framework to semantically smooth query and document models in the langua ge modeling (LM) retrieval framework. smoothing method based on topic decomposition. A query or a document is decomposed into a set of weighted topic signatures and those topic signatures are translated into individual concepts for the purpose of query or document expansions. We define a topic signature as a pair of two topic concepts that are related to each other syntactically and semantically. Because two related concepts help to determine context for each other, the signature-based translation should have higher accuracy and result in better retrieval performance. For example,  X  mouse X  in conjunction with  X  X omputer X  could be a topic signature and the signature might be translated to probability due to additional contextual constraints. language models through (1) adopting an ontology-based approach to extract concepts and signature s from queries and documents; (2) developing an EM-based method to train the signature translation model; and (3) expanding documen t and query language models based on topic signature transla tions. The new smoothing method is tested on TREC04/05 Genomic collec tions. The experimental results show that significant improvement s are obtained over the simple language model as well as the model with context-insensitive semantic smoothing. The contributi on of this paper is three-fold. First, it proposes a new document representation using a set of weighted concepts and topic signatures. Second, it expands document and query language models through context-sensitive semantic smoothing. Third, it empiri cally proves the effectiveness of context-sensitive semantic smoothing for language modeling IR. 2, we review previous work on context-sensitive semantic smoothing, finalize the representa tion for topic signatures, and implement the topic signature extraction. In Section 3, we present the expanded language models with context-sensitive semantic smoothing. In Section 4, we test the new model on TREC04/05 Genomics Track collections. Sec tion 5 concludes our paper. Liu and Croft [12] propose a cluster language model and achieve great empirical improvement over the baseline model. Unlike our method, which decomposes a document or a query into a set of small topic signatures, their method aggr egates similar documents into clusters and then treats each cluster as a big document for ranking purposes. All documents in the relevant clusters are returned to the users. The two models are similar in the sense that both want to obtain a set of documents with sim ilar context rather than a single document in order to estimate a more accurate and smoothed model. The major difference is that a docum ent only belongs to a cluster in the cluster model whereas a document can have multiple topic signatures in our model. Furthermor e, many decisions need to be made empirically for clustering, based on the domain knowledge and the collection (e.g. the number of cl usters, clustering algorithm, static clustering or query-specific clustering), while the topic signature model does not have this problem. expansion technique in [15]. A HAL vector is used as the context of a concept. The degree of one concept inferring another can then be heuristically computed. They also invent a heuristic approach to combine multiple concepts, which enables information inference from a group of concepts (premises) to one individual concept (conclusion). Therefore, their query expansion technique is somehow context-sensitive. Bai et al. [1] sli ghtly adapt the above approach to the KL-divergence retrieval framework. Both of them achieve significant improvement over the simp le language model. IF can be computed simply from term co-occu rrence data without any external knowledge and is thus of value in practice. The major drawback of this approach is that it is unable to trace the information flow back to the documents or queries. Therefore, it is difficult to estimate an IF document model or query model (i.e., computing the generative probability of the premise of an IF from a query or a document). For a short query, the uniform distributi on assumption, as made in [1], may not be a problem. But for a document, it is obviously not reasonable. In other words, it can not be used to expand document models. Besides, the degree to wh ich one individual concept could be inferred from another combined concept is not theoretically motivated; its robustness needs to be further validated. The choice of topic signature representation plays a crucial role in our context-sensitive semantic sm oothing method. First, the topic signature must be context-sensitiv e and thus the signature should contain at least two terms, unle ss word sense is adopted. Second, terms within a signature should have syntactic relation. Otherwise, we cannot count their frequency in documents or queries and it becomes difficult to estimate signa ture document models and query models. Third, it should be easy to extract topic signatures from the text. Last, we hope all terms with in a signature have semantic relations inspired by the idea of [3], where WordNet semantic relationships are considered. represented by a set of weighted binary relations between topic concepts; a relation could be either syntax-based or entity-event paired without syntactic constraint . However, for the convenience of model estimations, only syntax-based relation is allowed in this paper. In addition, we impose sema ntic constraints on two concepts in order to reduce the noise. Thus, we end up with the definitions of topic signature below. to each other syntactically and semantically. For simplicity, t(w is also denoted as t ij . The implementation of the syntactic and semantic relationships between tw o concepts is determined by specific applications. represents a set of synonymous te rms in the domain. For example, C0020538 is a concept about the dis ease of hypertension in UMLS Metathesaurus (http://www.nlm.ni h.gov/research/umls); it also represents a set of synonymous terms including high blood pressure , hypertension , and hypertensive disease . Therefore, concept-based indexing and searching helps to relieve the synonym and polysemy problems in IR, especially genomic IR, where a term (e.g., a gene or a protein) might have many sy nonyms while also representing different concepts in different context [20]. fold. First, two-topic concepts help to determine the context for IR use while not producing too many concept combinations. Second, a number of existing approaches are available to extract binary relationships, which are similar to concept pairs. For example, in the area of NLP, especially in bioinf ormatics, pattern-based methods for binary relation extractions have been extensively studied in recent years [13]. Third, a concept pair itsel f is very similar to a short query. In TREC 2005 Genomics Track [8], structured concept pairs are directly used as ad hoc retrieval topics. In [19], concept pairs are treated as an index unit as well as a search unit. Last, documents are full of various concept pairs and it is possible to make a robust estimation of document m odels by linearly combining a set of topic signature models. In general, the extraction of topic signatures is done in two steps: the topic concept extraction and the concept pair extraction. The extraction of biological concepts a nd their binary relationships is a hot topic in bioinformatics and a survey of those methods can be found in [13]. However, on one hand, our extraction is for IR use and we are dealing with large corpora statistically; thus the extraction methods need not be perfect [5]. On the other hand, we are more interested in semantic corresponden ces between topic concepts than syntactic patterns. Thus, we use a generic ontology-based approach to extract topic signatures. Figure 1. Illustration of document indexing. V t , V signature set, document set and concept set, respectively. concept extraction by MaxMatcher is equivalent to maximizing the weighted overlap between the word sequences in text and the concepts in an ontology, such as the UMLS Metathesaurus. It outputs concept names as well as unique IDs representing a set of synonymous concepts. The unique con cept IDs are used as an index in our experiments. MaxMatcher distinguishes between major concepts and sub concepts in the manner defined below. concept is called a sub concept , otherwise it is called a major concept . However, the membership of a concept is context-dependent; a sub concept in one text could be a major concept in another. For example,  X  blood pressure  X  is a sub concept for the text  X  high blood pressure,  X  but is a major concept for text  X  the blood pressure is...  X  We index both sub concepts and major concepts in the experiment. order to show the robustness and effectiveness of the signature-based semantic smoothing. A pair of two topic concepts will be treated as a topic signature if they meet the following three requirements: (1) both of them are major concepts; (2) they appear in the same clause of an English sentence; and (3) their semantic types are compatible according to the domain ontology. For example, two proteins could be semantically compatible in UMLS (e.g., protein-protein interaction). concept names followed by the corresponding concept ID and semantic type. The concept pair of obesity and periodontal disease is a topic signature while the concept epidemiological study has no relationships with other concepts because it is in a separate clause. Suppose we have indexed all docum ents with concepts and topic signatures (see Figure 1). For each topic signature t k , we have a set of documents ( D k ) containing that topic signature. Intuitively, we can use the document set D k to approximate the translation model for t i.e., determining the probability of translating the signature to concepts in the vocabulary. If all concepts appearing in the document set center on the topic signature t k , we can simply use maximum likelihood estimates and the problem is as simple as frequency counting. However, some concepts address the issue of other topic signatures while some are background concepts of the collection. We use the generative model proposed in [17] to remove the noise. Assume the set of documents containing t k is generated by a mixture model (i.e., interpolating the tran slation model with the background collection model ) | ( C w p ), where  X  is a coefficient accounting for the background noise and  X  refers to the translation model of the topic signature t estimate the translation model using the EM algorithm [4]: where ) , ( from previous translation models [2, 3, 9, 10] in two aspects. First, previous translation models take an individual term as the topic signature, and are unable to incorpor ate contextual information into the model. Our model uses a gr oup of terms with syntactic and semantic relation to each other as the topic signatures. Consequently, the resulting translation will be more specific. and Lafferty [2] use document-query pairs to train translation probabilities. However, it is unlikely to obtain a large amount of real data. For this reason, they use synthetic data for model estimation. The title language model, proposed in [9], uses title-document pairs to train translation probabilities. The major drawback of the title model is that only a small portion of terms in the vocabulary would appear in the title. The Markov chain model [10] deals with translations in a different fash ion. However, the resulting query model is fairly general and the computation of the inverse matrix will be prohibitive to large collections. Cao [3] takes into account word semantics when computing term a ssociations, but he ignores the sense of the words; this model is roughly equivalent to our context-insensitive version of semantic smoothing introduced in Section 4.6. Lafferty and Zhai introduced the KL-Divergence retrieval model as a special case of their risk minimization retrieval framework [10]. This retrieval model estimates query m odels as well as document models; the relevance of a document to a query is equivalent to measuring the KL-divergence distance between th e query model and the document model: The introduction of query models makes the language modeling approach more flexible. Almost a ll previous language models for IR are the special cases of this new retrieval model. The context-sensitive semantic smoothing technique proposed in this paper works with the KL-divergence retrieval model. the maximum likelihood estimat e. To avoid assigning zero probability to unseen terms and to reduce the noise, it could be simply interpolated with a background collection model ) | ( C w p where  X  is a coefficient accounting for the background model. We use this simple mixture language model as the baseline in the comparative study and refer to it as DM0 . document model can be expanded by statistically mapping the topic signatures in the document to query terms. That is, The topic signature document model (i.e., the generative probability of topic signatures in a document) can be computed using a maximum likelihood estimate: Where ) , ( d t c refer to this translati on-based document model as DM1. The form of DM1 is same as the translation mode l described in [2]. However, the topic signature in DM1 is more gene ric. It could be individual terms, as used in [2], or context-sensitive c oncept pairs as used in this paper, or any other objects that can express a topic. signatures may not be very representative when the document is too short or the criterion of being a topic signature is too strict. Thus, the accuracy of the document model will be compromised. To overcome this limitation, we interpolate DM1 with DM0. This mixture model is referred to as DM2 . The translation coefficient (  X  ) controls the influence of the tr anslation component in the mixture model. The mixture model becomes DM0 when  X  is zero and becomes DM1 when  X  is one. In the experiment, we tune the translation coefficient to optimize the retrieval performance. Like the expansion mechanism fo r document models, a query model can be expanded through the signature-concept translation if the query could be decomposed into a set of representative topic signatures. That is, However, query descriptions are of ten very short; therefore, it is difficult to extract representative topic signatures, let alone compute the generative probability (i.e. the importance to the query). For this reason, we do not smooth query models during the initial search. Instead, we update the query model according to the top-ranked documents of the initial search, which is referred to as blind feedback or pseudo-relevance feedback. We expect that the feedback documents will give us a more preci se sense of what the query is about. re-estimating the query model according to the feedback documents within the KL-divergence retrieval framework [17]. By interpolating the feedback model with the initial query model, we obtain the final query model for the feedback search. The tunable feedback coefficient  X  controls the influence of the feedback model in the mixture query model. those documents are relevant to the query. Intuitively, topic signatures containing one or more query terms are probably more relevant to the initial query than those containing no query terms. Counting topic signatures containi ng at least one query term, we derive a feedback model below: where ) , ( F t c feedback document set F . The resulting feedback model, however, might be fairly general because the topic signature translation probability is simply trained from a set of documents containing that signature. To overcome this probl em, only self-translation (i.e., translating a topic signature to its own concepts) is allowed. Then we obtain a heuristic feedback model called FM0 : where: term associations, but it will still keep the effect of the background topic signatures. In other words, high-frequency topic signatures in F might be also frequent in the collec tion. For this reason, we use the approach introduced in [17] to remove the effect of the background collection model. This approach a ssumes that the topic signatures in feedback documents are generated by a mixture model (interpolating the signature feedback model with the signature collection model): Thus, we get our second feedback model called FM1 : The signature feedback model ) | ( F k t p  X  could be estimated using the EM algorithm [4] with th e following update formulas. directly to estimate the feedback model, whereas we use the same approach to estimate the signature feedback model first, and then translate context-sensitive topic signa tures to its own concepts. This difference may result in two advantag es. First, a term in a different context will be treated the same and counted together in [17]. However, the context of topic signa tures will be accounted for in our approach. Second, our approach will favor terms that frequently interact with other terms. We think it is better than simply accounting for the frequency of terms (even after removing the effect of the background model), as was done in [17], when estimating a query model. Roughly, a term with high occurrence frequency will also interact with other terms frequently , but in a fine sense, these two concepts are different. Our current implementation of topi c signature extraction relies on a domain ontology. For this reason, we validate our context-sensitive semantic smoothing method on genom ic collections because UMLS could be used as the domai n ontology for this area. 2005 [8]. The original collection is a ten-year subset of Medline abstracts and contains about 4.6 m illion abstracts. We only used the sub-collection (i.e., the human relevance-judged document pool, 48,753 documents for 2004 and 41,018 documents for 2005) for our experiment. The ad hoc retrieval tasks of the two tracks include 50 topics (queries), respectively. We use the simple language model introduced in [12] (i.e., DM0) as the baseline. To give readers the sense of how good the baseline language model is, we also report the performance of the Okapi retrieval model in Table 1. Roughly, the performance of the baseline language m odel is comparable to that of the Okapi model. Following the c onvention of TREC, we use the mean average precision (MAP) as the major performance measure and the overall recall at 1000 documen ts as a supplem ental measure. Table 1. Comparison of the baseline language model to the Okapi model. The Okapi formula is the same as the one in [10]. The number of relevant documents for TREC04 and TREC05 are 8266 and 4585, respectively. The asterisk indicates the initial query is weighted as described in Section 4.2. Collection SLM Okapi Change SLM Okapi Change TREC04 6411 6662 +3.9% 0.345 0.363 +5.2% TREC04* 6527 6704 +2.7% 0.364 0.364 +0.0% TREC05 4084 4124 +1.0% 0.255 0.250 -2.0% 
TREC05* 4135 4134 -0.0% 0.260 0.254 -2.3% We index all documents with UM LS-based concepts and topic signatures as shown in Figure 1. For each document, we record the frequency count of each concept and signature and the basic statistics. For each concept and topic signature, we record their frequency count in each document and the basic statistics. For concept indexing, we do not use an y stop list. For topic signatures appearing in more than one document, we estimate their translation models using the EM algorithms detailed in Section 3.1. query terms from topic descriptions is the same as the process of document indexing. In TREC04 Genomics Track, a topic was described in three sections: title, in formation need, and context. The  X  X ontext X  section provided the bac kground information of the topic. Assuming the background informati on could be learned from blind feedback, we intentionally ignore this section during query formulation. The final formulat ed query contains 4.3 terms on average. TREC05 Genomics Track provided more structured queries that look like a binary relation be tween two topic concepts. Because the queries are too short, we also include sub-concepts in the query. The final formulated query contai ns 5.1 terms on average (Query #135 was removed because it contains no relevant document). more important than those in the re maining sections. For this reason, we weight query terms according to the sections from which they are extracted. Following the method proposed in [12], we optimize the weight of different sections by maximizing the MAP of the baseline retrieval model. The weights for the  X  X itle X  section and the  X  X nformation need X  section are 1.0 and 0.6, respectively. In TREC 2005 Genomic Track, the topic description is presented in one section, but we found that the major concepts are more important than those sub-concepts. Similarl y, we weight the query terms according to whether they are sub-concepts or not. The method for weight optimization is the same as that for query section weighting. The weights for major concepts and the sub concepts are 1.0 and 0.2, weighted. We evaluate the document model with context-sensitive semantic smoothing (i.e., DM2). The coefficient (  X  ) controlling the influence of the background collection mode l in all document models, DM0-DM2 is optimized by maximizing MAP. The coefficient accounting for background noise is set to 0.3 when using an EM algorithm to train signature translation models. The result is shown in Table 2. The IR performance is significan tly improved for both TREC04 and TREC05 after adopting semantic smoothing on document models. Table 2. The comparison of the baseline language model (DM0) to document smoothing model (DM2) and query smoothing model (FM1). TREC04 Recall 6411 6749 +5.3% 6929 +8.0% TREC04* Recall 6527 6905 +5.8% 7039 +7.8% TREC05 Recall 4084 4167 +2.0% 4227 +3.5% 
TREC05* Recall 4135 4214 +1.9% 4235 +2.4% Figure 2. The variance of MAP with the translation coefficient (  X  ) , which controls the influence of the translation model in DM2. in Figure 2. For all four curves, the best performance is achieved at  X  =0.3; after that point, the perfo rmance is downward. A possible explanation is that the extracted topic signatures do not capture all points of the document, but the ba sic language model captures those missing points. For this reason, when the influence of the translation model is too high in the mixt ure model, the performance is downward and even worse than that of the baseline. Therefore, if we can find a better topic signature representation for documents and queries, or we can refine the extraction of topic signatures, the IR performance might be further improved. The blind feedback gives the chance to estimate an accurate query model and is thus expected to perform better than the baseline language model. We select the t op 50 documents for feedback using Model FM1; the coefficient accounting for background noise is set to 0.3 when using the EM algorithm to train signature feedback models. For the efficiency of retrieval, we only expand 10 top-ranked terms and then renormalize their probability. Expanding more terms will only slightly improve the results but will seriously affect the retrieval efficiency. The feedback query model is further interpolated with the initial query model (QM0); the feedback coefficient (  X  ) is optimized by maximizing MAP. Table 1. The feedback significantly raises the IR performance. The effect of feedback is also robust. As shown in Figure 3, the feedback model is always superior to the baseline when the feedback coefficient  X  is changed from 0 to 1 for TREC04, and is better than the baseline except at  X  =1.0 for TREC05. for both TREC04 and TREC05. However, the effect on TREC04 is clearly much more significant than on TREC05. A possible explanation is that TREC04 is  X  X asier X  than TREC05. Figure 3. The variance of MAP with the feedback coefficient (  X  ), which controls the influence of the feedback model in blind feedback (i.e. DM0+FM1). The document semantic smoothing ma ps related document terms to query terms while the query sema ntic smoothing (feedback) expands query terms to match document terms. Their effects will overlap to some degree when used together and therefore it is not expected to achieve the overall effect equal to the summation of both. Actually, the overall effect could be worse than each individual effect without careful parameter tuning. For example,  X  hypertension  X  and  X  obesity  X  can translate to each other with high probabilities; the initial query  X  hypertension.  X  In this case, if we still use document semantic smoothing in the feedback search, we may overestimate the importance of  X  obesity  X  to the original query and thus degrade the performance. Table 3. The interaction effect of document smoothing (DM2) and query smoothing (FM1).  X  X ax X  is the maximum effect achieved by DM2 or FM1.  X  X oth X  is the result of DM2+FM1.  X  X hange [1]  X  is the improvement of DM2+FM1 over DM0.  X  X hange [2]  X  is the improvement of DM2+FM1 over  X  X ax X . TREC04 Recall 6411 6929 7026 +9.6% +1.4% TREC04* Recall 6527 7039 7079 +8.5% +0.6% TREC05 Recall 4084 4227 4273 +4.7% +1.1% TREC05* Recall 4135 4235 4317 +4.4% +1.9% document semantic smoothing during the initial search and hope the top-ranked documents will be more relevant to the query and result in a more accurate feedback model. In the feedback search, we still use document smoothing, but set its influence to a small degree to avoid overestimation. The feedback coefficient  X  is set to 0.6 and the translation coefficient  X  for the initial search is set to 0.3 according to the performance curves show n in Figure 2 and 3. The  X  for feedback search is optimized by maximizing MAP. As expected, the optimal value ranges from 0.01 to 0.05. The final result is reported in Table 3. The interaction of documen t smoothing and query smoothing consistently achieves positive effect on the retrieval of TREC04 and TREC05. significant than on TREC04. It is most likely because the top-ranked documents returned by the basi c language model (i.e., without document semantic smoothing) on TREC04 are good enough to estimate an accurate feedback model. In general, the worse the performance of the basic language model, the more significant the interaction effect will be. The feedback model FM0 heuristica lly selects the topic signatures relevant to the query using term associations. FM1 uses a formal generative model to estimate the importance of each signature to the query and thus is expected to pe rform better than FM0 in terms of predicting the query. The comparis on of these two feedback models is shown in Table 4. Though both are effective, FM1 performs consistently better than FM0, as expected. Table 4. Comparison of blind feedback model FM1 to FM0 Collection FM0 FM1 Change FM0 FM1 Change TREC04 6808 6929 +1.7% 0.442 0.451 +2.0% TREC04* 6811 7039 +3.3% 0.449 0.460 +2.4% TREC05 4192 4227 +0.8% 0.270 0.279 +3.3% 
TREC05* 4215 4235 +0.5% 0.279 0.288 +3.2% Following the method proposed in [1] and [3], we can simply use the extracted topic signatures to estimate a context-insensitive translation model, i.e., mapping one concept to another: where )) , ( ( the whole collection. Then we ge t a context-insensitive version of DM2 denoted as DM2 X . are presented in Table 5. The translation coefficient (  X  ) is optimized by maximizing the MAP. The optimal  X  is 0.3 for DM2 and 0.01 for DM2 X . The optimal  X  for DM2 X  is extremely small, most likely due to two reasons. First, the context-insensitive smoothing does not capture the semantics of the query well, and thus the influence in the mixture model is downward. Sec ond, a topic signature in DM2 X  translates to a relatively small number of concepts and thus the average translation probability is much higher than in DM2. significantly better than context-insensitive semantic smoothing approaches. The gain of DM2 X  ove r the baseline language model is consistent with the conclusions of pr evious work, such as [1] and [3]. [1] achieved 3-4% gain using HAL relationships and [3] achieved 5-6% gain using WordNet relationshi p and cooccurrence relationship. Table 5. Comparison of the context-sensitive semantic smoothing (DM2) to the context-insensitive semantic smoothing (DM2 X ) on MAP. The rightmost column is the change of DM2 over DM2 X . Collection TREC04 0.346 0.367 +6.1% 0.395 +14.5% +7.6% TREC04* 0.364 0.384 +5.5% 0.414 +13.7% +7.8% TREC05 0.255 0.260 +2.0% 0.277 +8.6% +6.5% 
TREC05* 0.260 0.269 +3.5% 0.288 +10.8% +7.1% We compared our method with two state-of-the-art approaches: the query expansion using information flow [15] and the model-based feedback [17].Both of them work within the LM framework. The former is also a context-sensitive semantic smoothing approach. The latter proves empirically to be effective on other TREC collections [17]. Because the information flow can not take the advantage of weighted initial queries, we used unweighted queries for comparisons. The information fl ow approach did not support concept-based indexing; thus, the result from only word-based indexing was obtained for it. For each approach, we tried different parameter combinations and reporte d the best result in Table 6. It performed significantly better th an the local information flow approach, possibly because we imposed semantic constraint on topic signatures and used biological concep ts rather than single words as building blocks, which made the information inference more meaningful on the genomic coll ections. Interestingly, the incorporation of domain knowledge did not help much when using the simple language model for retrieval; the result for TREC 2005 was even slightly worse. This showed that the context-sensitive semantic smoothing using topic si gnatures provided an effective mechanism to incorporate domai n knowledge. The result of the model-based feedback was also improved by using the concept-based indexing, but less effective than our approach, especially for TREC 2004. Table 6. Comparison of the retrieval performance of six approaches on TREC genomic track 2004 and 2005.  X  X ord X  or  X  X oncept X  means the indexing unit used. The concept-based indexing is based on the UMLS Metathesaurus. All approaches are implemented by us. Simple Language Model (Word) 0.324 6328 0.258 4101 Simple Language Model (Concept) 0.345 6411 0.255 4084 Local Information Flow (Word) 0.378 6793 0.272 4220 Model-based Feedback (Word) 0.372 6742 0.279 4260 Model-based Feedback (Concept) 0.424 6896 0.290 4213 Topic Signature (Concept) 0.461 7026 0.295 4273 In this paper, we propose a novel context-sensitive semantic smoothing approach that decomposes a document and a query into a set of weighted context-sensitive t opic signatures and then translate those topic signatures into query te rms. We validated the approach on two genomics collections: TREC Genomic Track 2004 and 2005. The document smoothing, the query smoothing, and the interaction of both all proved to be effective and robust on the testing collections in comparison to the baseline langua ge model. We also implemented a context-insensitive version of semantic smoothing using extracted topic signatures. As expected, it is significantly less effective than the context-sensitive semantic smoothing, though it does achieve a slight improvement over the baselin e language model. Our approach was also compared to two other IR approaches, a context-sensitive smoothing approach using informati on flow and an effective model-based feedback approach. Our approach performed significantly experiments altogether concluded that the context-sensitive smoothing using topic si gnatures was effective to incorporate domain knowledge for genomic IR. presented a new document repres entation, i.e., representing a document as a set of weighted topic signatures and concepts. In particular, we chose concept pairs as topic signatures and adopted a generic ontology-based approach to extract concepts and concept pairs. The new representation could be applied to other retrieval, summarization, and text classification tec hniques. Second, we proposed an EM-based method to train the context-sensitive translation model for each signature and then formalized the query and document expansions based on si gnature translations. Third, we empirically proved the superiority of the context-sensitive semantic smoothing over context-insensitive semantic smoothing as well as non-semantic smoothing. on a domain ontology. For this reason, we only tested our method on two genomic collections because UMLS can be used as the domain ontology for this area. However, the proposed method could be applicable to any application domain. For future work, we will adopt other existing concept and relation extraction approaches (i.e., those without ontologies) and apply cont ext-sensitive semantic smoothing to more IR collections in general domains. This work is supported in part by NSF Career Grant IIS 0448023, NSF CCF 0514679, PA Dept of Health Tobacco Settlement Formula Grant (No. 240205 and No. 240196), and PA Dept of Health Grant (No. 239667). We also thank four anonymous reviewers for their comments on the paper. [1] Bai, J., Song, D., Bruza, P., Ni e, J.Y., and Cao, G.,  X  X uery [2] Berger, A. and Lafferty J.,  X  X nformation Retrieval as Statistical [3] Cao, G., Nie, J.Y., and Bai, J .,  X  X ntegrating Word Relationships [4] Dempster, A.P., Laird, N.M., and Rubin, D.B.,  X  X aximum [5] Grefenstette, G.,  X  X se of syntactic context to produce term [6] Harabagiu, S. and Lacatusu, F.,  X  X opic themes for multi-[7] Hersh, W. et al.  X  X REC 2004 Genomics Track Overview X , the [8] Hersh, W. et al.  X  X REC 2005 Genomics Track Overview X , the [9] Jin, R., Hauptmann, A., and Zhai, C.,  X  X itle Language Model [10] Lafferty, J. and Zhai, C.,  X  X  ocument Language Models, Query [11] Liu, X. and Croft, W.B.,  X  X  luster-based retrieval using [12] Miller, D., Leek, T., and Schwartz M.R.,  X  X  Hidden Markov [13] Mooney, R. J. and Bunescu, R.  X  X ining Knowledge from Text [14] Ponte, J. and Croft, W.B.,  X  X  Language Modeling Approach to [15] Song, D. and Bruza P.D.,  X  X owards Context-sensitive [16] Zhai, C. and Lafferty, J.,  X  X  Study of Smoothing Methods for [17] Zhai, C. and Lafferty, J.,  X  X odel-based Feedback in the [18] Zhai, C. and Lafferty, J.,  X  X wo-Stage Language Models for [19] Zhou, X., Hu, X., Lin, X., Han, H., and Zhang, X.,  X  X elation-[20] Zhou, X., Zhang, X., and Hu, X.,  X  X sing Concept-based 
