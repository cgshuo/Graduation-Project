 Users today are constantly switching back and forth from applications where they consume or create content (such as e-books and productivity suites like Microsoft Office and Google Docs) to search engines where they satisfy their information needs. Unfortunately, though, this leads to a suboptimal user experience as the search engine lacks any knowledge about the content that the user is authoring or consuming in the application. As a result, productivity suites are starting to incorporate features that let the user  X  X xplore while they work X .

Existing work in the literature that can be applied to this problem takes a standard bag-of-words information retrieval approach, which consists of automatically creating a query that includes not only the target phrase or entity chosen by the user but also relevant terms from the context. While these approaches have been successful, they are inherently limited to returning results (documents) that have a syntac-tic match with the keywords in the query.

We argue that the limitations of these approaches can be overcome by leveraging semantic signals from a knowledge graph built from knowledge bases such as Wikipedia. We present a system called Lewis for retrieving contextually rel-evant entity results leveraging a knowledge graph, and per-form a large scale crowdsourcing experiment in the context of an e-reader scenario, which shows that Lewis can outper-form the state-of-the-art contextual entity recommendation systems by more than 20% in terms of the MAP score. H.4 [ Information systems applications ]: Data mining  X 
This work was done during an internship at Microsoft.  X  This work was done while working at Microsoft.
 c  X  Entity recommendation, Context, Semantic, Knowledge base, Context-Selection Betweenness
Users today are constantly switching back and forth from applications where they consume or create content (such as e-books and productivity suites like Google Docs and Mi-crosoft Office) to search engines where they satisfy their in-formation needs (such as Bing or Google). Unfortunately, though, this leads to a suboptimal user experience as the search engine lacks any knowledge about the content that the user is authoring or consuming in the application [11, 19, 13].

How can we empower users to satisfy their information needs directly within the applications where they consume content? A significant step in this direction is enabling users to interact with anything on the document that they are working on, directly within the productivity application, and recommending results that are contextually relevant to the elements they are interacting with. Productivity suites are starting to incorporate features that realize this scenario, such as the  X  X nsights for Office X  feature in Microsoft Word Online.

As an example, consider a user reading on an e-reader the document shown in Figure 1, which describes the Cap-ture of Fort Ticonderoga, an important event in American history. At some point, she finds a mention to a historical figure called  X  X ilas Deane X  and decides that she would like to learn more about him. Just sending the query  X  X ilas deane X  to any of the major commercial search engines returns re-sults such as  X  X ilas Dean High School X  which are unrelated to the historical context of the document. A much more compelling user experience is the one shown in Figure 1, where the user has tapped on the phrase  X  X ilas Deane X  and is shown contextually relevant articles such as  X  X evolution-ary War X , where she can learn about Silas Deane X  X  over-all involvement in the American Revolutionary War, and  X  X enjamin Franklin X , where she can learn that Deane and Franklin were the first diplomats in American history, and they were sent together to France as commissioners from the Continental Congress. Figure 1: An example of contextual exploration.

Previous efforts in the literature have made significant progress towards realizing this scenario, including systems for contextual search [11, 19] and contextual insights [13]. These systems take a standard bag-of-word information re-trieval approach to the problem, which consists of auto-matically creating a query that includes not only the tar-get phrase or entity chosen by the user but also relevant terms from the context. More broadly, these approaches are related to relevance feedback in information retrieval [35], where a context (in the case of relevance feedback, the re-sults of a query; in our scenario, the document context) is used to refine an initial query (in our case, the phrase chosen by the user).

While these approaches have been successful, they are in-herently limited to returning results (documents) that have a syntactic match with the keywords in the query. For ex-ample, the Wikipedia articles for  X  X evolutionary War X  and  X  X enjamin Franklin X  have just a single passing mention to Silas Deane and are thus unlikely to be retrieved by a query that contains the terms  X  X ilas deane X . To tackle this prob-lem, in this paper we argue that such results can be obtained by more directly modeling the semantic connections between the target concept and the entities in the context where it appears.
 To illustrate our approach, consider the graph shown in Figure 2 (henceforth called knowledge graph ). The black node corresponds to the entity chosen by the user (Silas Deane), the gray nodes correspond to entities mentioned in the context (Green Mountain Boys, Fort Ticonderoga, Connecticut). The edges correspond to hyperlinks in the Wikipedia articles. As we can see, the node for  X  X evo-lutionary War X  acts as a bridge between Silas Deane and the context concepts Green Mountain Boys (the militia that captured Fort Ticonderoga) and Fort Ticonderoga. Our ap-proach leverages precisely this type of semantic connections to retrieve contextually relevant results.

The contributions of this paper include: Figure 2: A portion of the focused subgraph for our running example. (Black for the user selection node, gray for context nodes, and white for all other nodes.)
The rest of this paper is organized as follows. We formally define the contextual entity exploration problem in the next section, followed by a detailed description of our proposed method in Section 3. We evaluate our method in Section 4. Lastly, we review related problems and previous work in Section 5, and provide concluding remarks in Section 6.
The input to the contextual entity exploration problem consists of a user selection : the span of text that the user highlights with the mouse or taps with the finger, which im-plicitly determines the entity she would like to gain insights about (e.g.,  X  X ilas Deane X ); a context , consisting of the con-tent that the user is consuming or authoring; a knowledge base , that consists of entities that are candidates to be rec-ommended; and a knowledge graph , whose nodes are entities from the knowledge base; and an text-to-entity mapping that given some text from the user selection or context produces an entity from the knowledge base.

The contextual entity exploration problem is then formally defined as follows:
Definition 1. Given a quintuple ( s,C,B,G, X  ) , where s is a user selection, C is some text context, B is a knowledge base, G is a undirected graph whose nodes are entities in B , and  X  is a text-to-entity mapping; the objective of the contextual entity exploration problem is to produce a set of entities O such that O  X  B and every entity in O is relevant to s in the context of C .
 In this work, we will use Wikipedia as our knowledge base B and the hyperlink structure of Wikipedia as the edges of the knowledge graph. In particular, G will be an undirected graph G = ( B,E ) where there is an edge ( x,y ) in E if there is a link to entity y on the Wikipedia page for entity x or vice versa. Notice, however, that the techniques presented in this paper are independent of the actual method used to construct the knowledge graph.
In this section, we explain our approach to contextual en-tity exploration using a knowledge graph. Figure 3 provides an overview of the proposed system. We start by building a focused subgraph from the given knowledge graph for each problem instance, followed by scoring participating nodes in two ways, namely context-selection betweenness and person-alized random walk. Ranking the aggregated scores gener-ates a recommendation list, with a human readable justifi-cation.
The first step of the algorithm consists of mapping the user selection s and the context C to nodes in the knowl-edge graph using the text-to-entity mapping  X  . Notice that this mapping is given as input to the contextual entity ex-ploration problem; it can be any off-the-shelf entity linking system (e.g., [9]) 1 .

Continuing our running example of Figure 1, a mapping  X  would map the user selection  X  X ilas Deane X  to the entity for Silas Deane in Wikipedia 2 ; and extract from the document entities related to the surface forms that appear therein, e.g.,  X  X reen Mountain Boys X  3 and  X  X ort Ticonderoga X  4 .

The next step consists of creating a subgraph of the knowl-edge graph that contains candidate entities to be recom-mended as contextually relevant results, which we call fo-cused subgraph . Its nodes consist of the union of two sets V 0 and V 00 . V 0 is the set of entities obtained by applying the mapping  X  in the previous step (i.e., the entities associ-ated to the user selection and context); and V 00 is the set of entities reachable from nodes of V 0 in the knowledge graph G through a path of length one. The edges of the focused
For our experiments, we use an in-house entity linking sys-tem. We also conduct experiments where the mapping  X  is actually an oracle: i.e., a human manually provides a perfect mapping. Figure 4: Illustration of retrievable nodes with context-selection betweenness ( left ) and personal-ized random walk ( right ). (The black node in the center represents the user selection node, and three gray nodes are context pages.) subgraph can be obtained by adding the edges induced by V 0  X  V 00 in G , or by any other suitable heuristic. 5 Figure 2 demonstrates an example of the focused subgraph for our running example.

Once the focused subgraph is constructed, we proceed to scoring each candidate entity in the focused subgraph by capturing the semantic connection among the candidates, the user selection, and the context nodes. We explain the scoring methods in detail next.
We now present the two scoring methods that we use, and how we combine them to produce a final contextual relevance score.
This method captures to what extent a given candidate node serves as a bridge between the user selection node and the context nodes. For example, in Figure 2,  X  X evolutionary War X  gets a higher score than  X  X lacksmith X  because remov-
The heuristic that we employ in the Lewis system consists of adding all the edges that involve at least one context or user selection node; and the edges ( x,y ) such that either x has inlinks from the user selection node s and y in G ( s and y are common parent of x ), or x has outlinks to the user selection node s and y in G ( s and y are common children of x ). ing the former disconnects  X  X ilas Deane X  from two context nodes, whereas removing the latter does not disconnect the selection node from any context node. This makes intuitive sense because while the Revolutionary War is very relevant to Silas Deane and the context of the document, the entity  X  X lacksmith X  is irrelevant to the context (it is just mentioned on the Wikipedia page for Silas Deane because his father was a blacksmith).

We capture this intuition with a measure that we call context-selection betweenness (CSB). The measure is inspired by the notion of betweenness centrality [12], which measures how many shortest paths go through a given node v . It is defined as follows: where  X  i,j is the total number of shortest paths from node i to j , and  X  i,j ( v ) is the number of such paths that pass through node v .

For the contextual entity exploration problem, however, the original definition of betweenness centrality does not suffice. First, in the contextual entity exploration prob-lem, relevance is defined with respect to the context. Thus, the measure should consider only paths connecting the user selection node and the context nodes, not every path con-necting two nodes. This can be done by a straight-forward modification of the original definition, by simply by sum-ming over all ( s,c ) pairs for all c  X  C , instead of all possible pairs of nodes in the focused graph.

Another reason that we need to modify betweenness cen-trality is that not all context terms are equally relevant to the user selection. Thus, we need a weighted version of be-tweenness centrality. To compute this weight, we use Nor-malized Wikipedia Distance (NWD) [28], a measure of se-mantic distance of two nodes on graph that is widely used in the entity linking literature. The NWD between two nodes u and v is defined as where I x is the set of incoming edges to the node x , and V is the set of all nodes in Wikipedia. In our modified measure, each path from user selection node s to a context node c is weighted by max(1 / (  X   X  NWD ( s,c )) , 0) with some threshold  X  . (In our case,  X  = 0 . 5.) In this way, a context term more relevant to the target mention is more emphasized.

Putting it all together, we define context-selection between-ness of a node v as follows:
Definition 2. Context-Selection Betweenness of a node v is where w ( s,c ) = max(  X   X  NWD ( s,c ) , 0) , l ( p,c ) is the length of shortest path between user selection node s and context node c , sp ( s,c ) is a set of all shortest paths between s and c , paths between s and c .

Intuitively speaking, a higher CSB score means the node is playing a more important role connecting the user se-lection node s and other context nodes c , which are more relevant to p through shorter and more unique paths.
Figure 4 (left) illustrates how CSB works. We have three context (gray) nodes in this graph. For each user selection -context ( s  X  c ) path, marked with pink arrows, we assign scores by (3) to all participating nodes on it. As a result, we found three pink nodes including A with nonzero score. All other white nodes are 0, as they are not on any s  X  c path.
We also consider a measure of the relevance of a node to the user selection. This measure does not factor in the contributions of context nodes directly, but it does so indi-rectly since the focused graph is built from context nodes in the knowledge graph. In Section 4, we will show that while this measure does not suffice by itself to obtain an ap-propriate relevance score, it is quite effective when used in conjunction with the context-selection betweenness measure described above.

We compute this measure by computing a personalized random walk [18]. Intuitively, the random walk is simulat-ing the behavior of a user reading articles in Wikipedia as follows. We assume that a user starts reading the page most directly relevant to the user selection (the page for  X  X ilas Deane X  in our example) and then follow an interesting link from that page. She continues surfing articles in this way, until at some point she comes back to the article of the user selection.

Random walk [30] computes the stationary probability that a user would stay in the page. Personalized random walk [18] is a generalization of random walk in that it intro-duces a set of default pages which the user can jump from anywhere on the web with certain probability. It is proven to always have a unique solution with stationary probabil-ity for each page. [18] There are numerous previous works using personalized random walk for graph-based data min-ing. The most related to ours is WikiWalk [41], which used a general jump probability vector (possibly with different probability to different pages) for measuring similarity of two texts (without a notion of  X  X ser selection X ). Note that we call this random walk  X  X ersonalized X  only because it is the name known in literature, not because we personalize to the interests of an individual user.

To compute the random walk, we use a vector for all nodes, containing random jump probability to each node. We assign 0 &lt; x s &lt; 1 for the user selection node s and x / | C | where 0  X  x c  X  x s for each context node c  X  C . All the other nodes are assigned zero probability of random jump. Thus, users can come back to the user selection node during surfing with some probability x s , as well as to some context page with smaller probability x c / | C | , and restart navigation from there.

Figure 4 (right) illustrates an example of probability dis-tribution from personalized random walk with x s &gt; x 0. The user tends to stay on the user selection node with highest probability, followed by context nodes. Also, nodes close from the user selection and context nodes have slightly higher probability than nodes far from them. In other words, personalized random walk retrieves semantically relevant pages from the query and context terms by assigning higher prob-ability (score) to closely and densely connected nodes from the user selection and context nodes.
At this point, we have two scores for each node v : a context-selection betweenness score CSB ( v ), and a random walk score RW ( v ). We now explain how to combine them to obtain the final relevance score.

The random walk scores of a node are probability scores and thus sum up to 1. Thus, the expected value of RW ( v ) gets smaller when we have more nodes in the graph. To counter the effect of graph size, we consider | V | RW ( v ) in-stead of RW ( v ) itself, where V is the set of nodes in the focused graph. As the expected value of | V | RW ( v ) is al-ways 1 because it sums to | V | , we can interpret this score as how many times the node is preferred to visit compared to expectation. If | V | RW ( v ) = 3, for example, we interpret the page v is 3 times more recommendable than others.
Context-selection betwenness, on the other hand, is nor-malized by the sum over all context nodes. Thus, the CSB ( v ) score for each node tends to be inversely proportional to the number of context nodes. To counter this effect, we again consider | C | CSB ( v ) instead of CSB ( v ), where C is the set of context nodes. As the highest score of | C | CSB ( v ) is | C | , each s  X  c path distributes 1 to all participating nodes. Thus, we can interpret | C | CSB ( v ) score as the expected number of shortest paths from user selection s to any context node visiting v in the meanwhile.

We aggregate these scores reflecting their relative impor-tance. First, it is natural to trust context-selection between-ness score more when we have more context terms at hand. On the other hand, we trust context-selection betweenness less when we have a relatively large number of nodes in our focused graph compared to the number of context nodes | C | , as this may imply that either nodes in outside the context may not overlap so much (that is, the context is not topi-cally coherent) or user selection and context nodes have large number of connected nodes (so they are general terms, e.g, Water or Human ). In either case, therefore, the importance of | C | CSB ( v ) should be proportional to the ratio of | C | to | V | . With a scaling factor  X  , we finally propose the following aggregation equation to compute final score for node v : Definition 3. Relevance score of a node v is given by We sort this score for each entity in decreasing order, and recommend the top-k entities. We recommend nodes v sat-isfying | V | RW ( v ) &gt; 1 only. That is, we do not recommend pages with lower random walk score even than its expecta-tion. This is to remove some general (so not recommendable) context terms having very high | C | CSB ( v ) due to a cluster of context pages on the focused subgraph.
In this section, we evaluate our approach and compare it with several baselines using crowd-sourced data in the context of a real-world e-reader application. We employed snapshot of English Wikipedia from January 2nd, 2014 as our knowledge base, considering all pages from namespaces main and category . We further performed some preprocessing: removing stop words, consolidating redirec-tions 6 , and removing disambiguation pages 7 since they con-nect ambiguous entities which are not quite related with each other. (This is different from category or list pages, which contain semantically relevant pages.)
We performed experiments in the context of an e-reader application, as illustrated in Figure 1. To create suitable test data, we employed a corpus consisting of all English textbooks from the Wikibooks site 8 . The corpus consists of 2,600 textbooks that cover a broad spectrum of topics, such as engineering, humanities, health sciences, and social sciences. We sampled 900 paragraphs from this corpus, and for each paragraph we asked 100 crowd workers to select phrases for which they would like to learn more. Then, we performed weighted random sampling from the user-selected phrases to get 500 test cases (pairs of user selections and con-texts). For each test case, we pooled the top 8 results from our system as well as several baselines. For each result in the pool, we showed the original user selection and context to 10 crowd workers and ask them if they thought the rec-ommended page is good in the context. We applied some simple heuristics to remove spam labels, and used majority voting to get the final label.
 We considered 100 words before and after the user selec-tion as context for all compared methods. For the personal-ized random walk, we used x s = 0 . 05 (random jump prob-ability to the perfect node), x c = 0 (random jump proba-bility to any context node) 9 , and iterated up to 50 times. For context-selection betweenness, we used  X  = 0 . 5. We compared Lewis to baselines using Mean Average Precision (MAP) to take both precision and recall into account with a single metric.
 Figure 5 shows our user interface for crowd workers to eval-uate our recommendation. On the left side, workers can see the original context. The user selection is marked as light green box. On the right side, it shows an entity rec-ommended for the user selection. In order to let the user understand whether the entity is appropriate, we show the Wikipedia page of the entity. The workers are asked to an-swer how relevant the entity is to the user selection and the given context on the left-bottom of the page. We gave three options: 1) This article is what I X  X  expect to see if I high-lighted the text on the left, 2) This article is not what I X  X  expected, but I see a connection between the highlighted text and the article, and 3) This article is not what I X  X  ex-pected, and I don X  X  see a connection between the highlighted text and the article. We regarded 1) and 2) as relevant, and 3) as not relevant.
 During this evaluation, we faced the following challenge. As our corpora consist of various topics including literature, history, science, or engineering, it is rather difficult to find a worker sufficiently knowledgeable in all of these areas. Fur-
E.g,  X  X IT X  and  X  X assachusetts Institute of Technology X  refer to the same page. Users might link to this page using either of them.
E.g,  X  X pple (disambiguation) X  page contains links to both the fruit apple and the IT company Apple Inc. http://en.wikibooks.org
We tried some x c &gt; 0, but observed no significant differ-ence. thermore, crowd workers tend to answer without close in-spection but just relying on their background knowledge.
To resolve this issue, we added a justification sentence in the user interface (on the top-right of the page). There is recent work in the area of justification of recommenda-tions [16, 37, 43, 39, 8], and in our case we used a simple, yet effective, heuristic. From either the perfect page or the recommended page, we chose one sentence which satisfies one of the following conditions: 1) a sentence containing both (the perfect and the recommended) pages X  titles with hyperlink, 2) a sentence containing both (the perfect and the recommended) pages X  titles without hyperlink, 3) a sen-tence containing the other page X  X  title with hyperlink, 4) a sentence containing the other page X  X  title without hyperlink. In this way, we prefer a sentence which can explain relation-ship between the user selection and the recommended page. If we find more than one sentence in the same category, we chose the first occurence, because general explanation tends to come first in Wikipedia articles. If we can find no sen-tence satisfying any of those four conditions, we choose the first sentence of the recommended page. Table 1 illustrates several examples of our justifications.

We provided the same justifications for all baselines as well as our method to be fair. We considered four baselines. The first baseline consists of simply sending the user selection to a commercial search engine, without using any context. The second baseline con-sists of the widely used semantic relatedness measure Nor-malized Wikipedia Distance (NWD) [28]. In this baseline, we take the user selection node in the graph and compute NWD with respect to all other nodes in the focused sub-graph, and then return the entities with the top-k score. Finally, we consider two approaches that are representative of contextual entity exploration using bag-of-words IR tech-niques: the Leibniz system [13], and an algorithm based on positional relevance model (PRM) [25, 14], which is a state-of-the-art pseudo-relevance feedback algorithm. Notice that both NWD and Lewis need to perform entity linking to get the user selection entity. In our experiments, we placed the entity proposed by an in-house entity linking system at the top position for NWD and Lewis, which is for the benefit of fair comparison with the search engine and Leibniz, since their top results are generally entity linking results.
Table 2 compares performance of Lewis against the base-lines in terms of MAP@8. We can see that Lewis outper-forms all the baselines, with a MAP@8 score of 0.291. In particular, it outperforms the state-of-the-art contextual en-tity recommendation systems: the PRM pseudo-relevance feedback system, which achieves a MAP@8 score of 0.278; and Leibniz, which achieves a MAP@8 score of 0.262. The lowest MAP score of 0.244 corresponds to using NWD as a measure for entity recommendation.

The scores above correspond to the case in which we out-put up to eight entities. We also analyzed the case when we set a threshold in the maximum number of results. That is, we computed MAP@ k , for varying values of k . The results entity chosen by entity linking and by human (right). Table 2: Comparison of Lewis against baselines.
 are seen in Figure 6 (left). As we can see, Lewis consistently outperforms all baselines at every value of k .
 We also performed an ablation study where we consider each of the two scoring methods of Lewis in isolation. The re-sults for MAP@8 are shown in Table 3. We can see that the full Lewis system outperforms its ablations. The results for MAP@ k for varying values of k are shown in Figure 6 (mid-dle). We can also see that Lewis consistently outperforms the ablations at every value of k . In addition, we see that context-selection betweenness outperforms the random walk method. This means that user selection and context should be considered as first class citizens, as we advocate in this paper.
 Recall that in the construction of the focused graph, we use a mapping  X  to map the user selection to an entity node. All the results given so far are based on the use of an in-house entity linker. But this raises the following question: what is the dependency of Lewis on the actual entity linking method? To address this question, we consider an experi-ment where the mapping  X  is given by an oracle. That is, a human manually produced the correct entity linking result for the user selection. The Lewis system with an entity link-ing oracle gets a MAP@8 score of 0.336. In contrast, Lewis achieves a MAP@8 score 0.291 when we use our in-house entity linker. This means that if we used a different entity Table 4: Score decomposition with Silas Deane ex-ample. linker system, the MAP@8 score could be improved by at most 17%. Potential MAP improvement with different k is shown in Figure 6 (right).
 We also investigated how many entities Lewis and Leibniz return in common. Interestingly, only a small portion of the recommended entities overlap. Specifically, Lewis recom-mended 3,790 entities for 500 test examples (up to 8 entities for each) and Leibniz did so for 3,032 entities. Among these, only 580 entities (9.3%) were in both sets. Furthermore, if we exclude the user selection entity obtained via the en-tity linker, the number of overlapping entities drops to 320 (5.6%). This observation leads to a conclusion that IR-based and knowledge-graph based systems tend to retrieve quali-tatively different types of entities, and thus the combination of such system is a promising direction of future work. The focused subgraph we created for each problem instance contained 16,041 nodes and 118,380 edges in average. For each instance, in average, it took 0.3 seconds for constructing focused subgraph. It took 3.3 seconds in average for com-puting context-selection betweenness score, and 3.2 seconds in average for random walk score. (This excludes all pre-processing steps which were done offline, taking about 30 minutes.) The largest focused subgraph had 155,711 nodes and 1,617,403 edges, taking 0.6 seconds for constructing sub-graph, and 32 seconds for each scoring method.
 We present a couple of anecdotal examples. We start with running example of Figure 1, where the user selection is Silas Deane . The original text is as follows, with the user selection Silas Deane marked with a surrounding box.

Lewis retrieved contextually relevant results such as Amer-ican Revolutionary War and Capture of Fort Ticonderoga . We see that part of these results were retrieved by the ran-dom walk component, while for some others context-selection betweenness contributed more. Table 4 shows decomposed scores for CSB and RW contributions. For example, context-selection betweenness plays an important role to retrieve American Revolutionary War and Capture of Fort Ticon-deroga , because they are semantically connecting user selec-tion Silas Deane and context phrase Fort Ticonderoga . It also retrieves results for Benjamin Franklin , Arthur Lee , and Thomas Jefferson , who happened to be the other three peo-ple who, together with Silas Deane, can be considered to be the first diplomats of the United States. We see that the random walk component indeed plays more important role for retrieving these pages from Table 4. They are recom-mended due to their close relation with Silas Deane , despite they are not directly related to the context above.
Table 5 compares the output of the Silas Deane example from baselines. As we have seen above, CSB only retrieves contextually relevant pages. RW only and NWD baseline mostly retrieve pages for close figures of Silas Deane. Many of them are marked as unrelevant, because they are not re-lated to the context in spite of their relatedness to Silas Deane. Interestingly, Leibniz retrieved only 4 results for this example, where none of them except for Silas Deane is relevant to the given context. On the other hand, the entire Lewis system retrieves more relevant results to the context and the user selection by combining CSB and RW component.

The other example is an article about pronunciation and location of stress on the Greek word Ulysses . The user se-lection is Oxford English Dictionary :
We see in this example that Lewis is returning results relevant to both the user selection and the context again. Received Pronunciation , which is regarded as the standard accent of Standard English in the United Kingdom, is di-rectly relevant to the topic as well as the user selection. Greek language is also relevant to the context, as the pas-sage is talking about pronunciation of a greek word Ulysses . Another interesting result is Old English , as this article is talking about historical pronunciation (from 1895) of the word in English.
Contextual exploration [13, 23] is closely related to several information retrieval problems: entity linking and search, relevance feedback, and content recommendation.
The goal of entity linking is disambiguating the mention of an entity in unstructured text to the entity X  X  record in knowledge base, usually by using machine learning tech-niques and disambiguated training data. There is a rich literature, including [9, 17, 20, 27, 42]. Recently entity link-ing was applied to text streaming data [40] and broadcast-ing. [29] Contextual exploration in contrast is not limited to disambiguating an entity mention, but it also explores and recommends articles relevant to the mention as well as the context.

Entity search [31, 5] and related entity search [3] are also related to our work. Entity search aims to answer a query with direct entity answers extracted from documents or en-tity records in a knowledge base; related entity search/finding takes as input a query entity, the type of the target entity, and the nature of their relation, and outputs a ranked list of related entities. However, the task at hand is principally different, which is to recommend related entities that are related to the user selection, which can be any continuous text (including but not limited to entity mentions), in the context of the document being consumed.

Entity ranking and recommendation is another recent area of research related to our work. Lee et al. [22] utilized ran-dom walk based approach for entity ranking, and Agarwal et al. [2] approached the ranking problem for networked en-tities with Markov walks. Vercoustre et al. [38] proposed a method for ranking Wikipedia entities.
Relevance feedback has been shown to be effective to ex-pand a query to relax the syntactic mis-matching problem in information retrieval [35, 34, 6]. Specifically, when a user submits a query, an IR system would first return an initial set of result documents, then ask the user to judge whether some documents are relevant or not; after that, the system would expand the query by adding a set of related terms ex-tracted from the user X  X  judgments, and return a set of new results. When there are no real relevance judgments avail-able, alternatively, pseudo-relevance feedback [7, 21, 25, 14] may be performed, which simply assumes that a small num-ber of top-ranked documents in the initial results are rele-vant. Our work may also be regarded as a pseudo-relevance feedback approach which assumes not only the top-ranked documents but the context around the user selection as pseudo-relevant resources. In contrast to the works above, however, we do not use syntactic query expansion. Instead, we explore a novel way of leveraging Wikipedia semantics to rank the results directly.

In the Experiments section, we showed that our approach outperforms a method based on pseudo-relevance feedback. Furthermore, since the approaches are complementary, a promising direction of future work involves the combination of IR and semantic approaches.
Content recommendation has been studied extensively in the past. Traditional content-based recommendation [33, 1] is usually to recommend documents which reflect users X  long-term interests (e.g., a user might generally like sports). However, our work is recommending content related to users X  ad hoc interests implied by the user selection when reading a document.

Related content recommendation [24], cumulative citation recommendation [4, 44], and contextual advertising [32] are also in the direction of ad hoc content-based recommenda-tion. They recommend related content, such as news arti-cles, Wikipedia articles, Web documents, or ads, to a target document. However, these works are based on a problem formulation that is insufficient for contextual exploration, as it does not allow for a user selection as part of its input.
There are also several approaches that exploit the Wikipedia link-structure (or other knowledge base as a graph structure) to compute semantic similarity between text and knowl-edge base entities. One of the most popular areas exploit-ing knowledge graph structure is estimating semantic doc-ument or word similarity. WikiWalk [41] applied personal-ized page rank on a graph derived from Wikipedia to esti-mate semantic relatedness between documents. Gouws et al. [15] proposes a method for computing semantic relat-edness by spreading activation energy over the hyperlink structure of Wikipedia. Mihalcea et al. [26] also presents a method for measuring the semantic similarity of short texts, using corpus-based and knowledge-based measures of sim-ilarity. Wikipedia link-structure is also exploited for com-puting concept relatedness [36] and query expansion [10] as well.

We emphasize that contextual entity exploration problem is a fundamentally different task from the vast bodies of work above, in that contextual entity exploration takes as input both an entity and its context. The work on exploiting semantic similarity between documents takes a document as input (our  X  X ontext X ) and finds similar documents. However, it does not take a  X  X ivot X  entity as input (In our running example, the input entity used as pivot is  X  X ilas Deane X ). WikiWalk [41], for example, is very related to part of our approach in the sense that it performs a random walk on the Wikipedia graph. In contrast to our work, however, Wiki-Walk does not make a difference between the input entity and the context. Most importantly, we show in Section 4 that a baseline that consists exclusively of a random walk (like in WikiWalk) is clearly outperformed by our techniques which combine random walks with methods such as context-selection betweenness that treat the concepts of input entity and context entities as first class citizens. No problems listed above takes both an entity (user selection) and its context as input to recommend relevant entities from knowledge base.
In this paper, we presented Lewis, a system that pro-vides a solution for the contextual entity exploration prob-lem leveraging knowledge bases. A large scale evaluation of the approach shows significant performance improvement with respect to state-of-the art methods for contextual entity exploration. Furthermore, the results indicate that combin-ing IR-based and knowledge-graph based methods for this problem is a promising direction of future work.
We thank Ashok Chandra and Panayiotis Tsaparas for their insightful feedback. [1] G. Adomavicius and A. Tuzhilin. Toward the next [2] A. Agarwal, S. Chakrabarti, and S. Aggarwal.
 [3] K. Balog, A. P. de Vries, P. Serdyukov, P. Thomas, [4] K. Balog and H. Ramampiaro. Cumulative citation [5] I. Bordino, Y. Mejova, and M. Lalmas. Penguins in [6] C. Buckley and S. E. Robertson. Relevance feedback [7] C. Buckley, G. Salton, J. Allan, and A. Singhal. [8] W. Chen, W. Hsu, and M. L. Lee. Tagcloud-based [9] S. Cucerzan. Large-scale named entity disambiguation [10] J. Dalton, L. Dietz, and J. Allan. Entity query feature [11] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, [12] L. C. Freeman. A set of measures of centrality based [13] A. Fuxman, P. Pantel, Y. Lv, A. Chandra, [14] S. Gottipati and J. Jiang. Linking entities to a [15] S. Gouws, G. Van Rooyen, and H. A. Engelbrecht. [16] J. L. Herlocker, J. A. Konstan, and J. Riedl. [17] J. Hoffart, S. Seufert, D. B. Nguyen, M. Theobald, [18] G. Jeh and J. Widom. Scaling personalized web [19] R. Kraft, C. C. Chang, F. Maghoul, and R. Kumar. [20] S. Kulkarni, A. Singh, G. Ramakrishnan, and [21] V. Lavrenko and W. B. Croft. Relevance-based [22] S. Lee, S.-i. Song, M. Kahng, D. Lee, and S.-g. Lee. [23] Y. Lv and A. Fuxman. In situ insights. In Proc. of the [24] Y. Lv, T. Moon, P. Kolari, Z. Zheng, X. Wang, and [25] Y. Lv and C. Zhai. Positional relevance model for [26] R. Mihalcea, C. Corley, and C. Strapparava.
 [27] R. Mihalcea and A. Csomai. Wikify!: linking [28] D. Milne and I. Witten. An effective, low-cost measure [29] D. Odijk, E. Meij, and M. de Rijke. Feeding the [30] L. Page, S. Brin, R. Motwani, and T. Winograd. The [31] D. Petkova and W. B. Croft. Proximity-based [32] B. Ribeiro-Neto, M. Cristo, P. B. Golgher, and [33] S. Robertson and I. Soboroff. The trec 2002 filtering [34] S. E. Robertson and K. S. Jones. Relevance weighting [35] J. J. Rocchio. Relevance feedback in information [36] M. Strube and S. P. Ponzetto. Wikirelate! computing [37] P. Symeonidis, A. Nanopoulos, and Y. Manolopoulos. [38] A.-M. Vercoustre, J. A. Thom, and J. Pehcevski. [39] J. Vig, S. Sen, and J. Riedl. Tagsplanations: [40] N. Voskarides, D. Odijk, M. Tsagkias, W. Weerkamp, [41] E. Yeh, D. Ramage, C. D. Manning, E. Agirre, and [42] M. A. Yosef, J. Hoffart, I. Bordino, M. Spaniol, and [43] C. Yu, L. V. Lakshmanan, and S. Amer-Yahia.
 [44] M. Zhou and K. C.-C. Chang. Entity-centric
