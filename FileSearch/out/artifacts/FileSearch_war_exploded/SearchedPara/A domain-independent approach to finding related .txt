 1. Introduction
Most information retrieval (IR) systems, including commercial search engines, respond to the user X  X  query by retrieving documents. If a user is looking for entities that have a specific relationship to an entity already known to him, he has to man-ually find them in the documents retrieved by an IR system. Arguably, users with such information needs may prefer to use an IR system that retrieves entities, rather than documents, as this would eliminate the time-consuming process of reading through large amounts of text in order to find the relevant entities. For instance, if a user wishes to know which awards a particular film received, then it is likely that he would prefer to get a list of such awards from an IR system directly, rather than look for them in the retrieved documents.

Entity finding is a growing field in Information Retrieval and Computational Linguistics research communities as evi-denced by the emergence of several evaluation frameworks within the last few years. For instance, the Question Answering the so-called  X  X  X ist X  X  questions, where the correct response for a query was a list of entities, rather than a single answer. An example of a list question in the 2007 QA track is:  X  X  X hat singers made recordings with Ella Fitzgerald? X  X . The Enterprise track an area specified in the topic. An example of a query from the 2006 task is  X  X  X ind individuals with expertise regarding ontol-ogy engineering. X  X  In 2007 X 2009 INitiative for the Evaluation of XML retrieval (INEX) organised Entity Ranking track ( Demar-
Finding (REF) task of the Entity track of TREC 2009 ( Balog, de Vries, Serdyukov, Thomas, &amp; Westerveld, 2009 ) and TREC 2010  X  in the query. An example topic is given in Fig. 1 .

The components of the Entity track topic include the name of the entity, which is the focus of the topic (henceforth re-pages of the found entities. The collection used in the 2010 track is the English portion of ClueWeb09 Category A, containing approximately 504 million web pages. The dataset consists of 50 topics and their relevance judgements.

In this paper we propose a method for retrieving and ranking entities related to the entity given in the query by a specific relationship, and use the Related Entity Finding task of the Entity track 2010 as the main dataset for evaluation. We also eval-uate our method using the Entity track evaluation methodology on the  X  X  X ist X  X  questions of the QA track of TREC 2005 and 2006. The parameters for the methods were tuned on the 20 topics from the Entity track of TREC 2009.

The main goal of our research is to determine whether correct answer entities can be identified in an unsupervised man-ner by combining the following two characteristics: Relatedness of the candidate answer entity to the query topic.
 Likelihood that the candidate answer entity belongs to the sought (target) entity category as specified in the query. The high-level research questions that we aim to answer are: RQ1 : How to measure the relatedness of the candidate answer entity to the query topic?
RQ2 : How to estimate the likelihood that the candidate answer entity belongs to the target entity category specified in the query?
RQ3 : Is the likelihood that the candidate answer entity belongs to the target entity category useful in identifying correct answer entities?
Our approach to identifying candidate entities related to the query is first to retrieve documents in response to the initial query, extract an initial set of candidate entities from the text of the documents, and then rank them by relatedness to the query topic. As ranking methods we compare Pointwise Mutual Information, Pearson X  X  v
In order to estimate the likelihood that the candidate answer entity belongs to the target entity category specified in the query we propose a method whereby we automatically construct a set of seed entities, which represent hyponyms of the target entity category specified in the narrative, and then rank candidate entities based on their similarity to the seeds.
For example, the target entity category in the topic given in Fig. 1 is  X  X  X ecording companies X  X . The proposed method automat-cords X  X ), and computes the similarity of candidate entities to these seeds. Below is the summary of the main steps: (1) Retrieve an initial set of documents for the query from the Web; (2) Retain sentences containing query terms plus one preceding/following sentence; (3) Perform named-entity (NE) tagging of these sentences and extract candidate entities; (4) Extract category name from the narrative; (5) Generate a set of seed entities representing hyponyms of the target entity category; (6) Compute distributional similarity between each candidate entity from Step 3 and a seed entity from Step 5; (7) Rank candidate entities by distributional similarity to all seed entities.

The proposed approach is unsupervised and domain-independent, extracting entities from the texts of documents re-trieved for the user X  X  query. Our goal is to minimise the reliance on knowledge bases in the process. We only use Wikipedia in the query processing stage to determine boundaries of noun phrases in the narrative. This task could alternatively be han-dled without Wikipedia, by using a Noun Phrase chunker.

The main contributions of the paper are summarised below:  X  A novel method for calculating distributional similarity of words using BM25 document ranking function ( Robertson,
Walker, Jones, Hancock-Beaulieu, &amp; Gatford, 1995 ) is proposed. This method is compared to Lin X  X  distributional similarity method ( Lin, 1998 ), and shows improvements on the Entity track REF dataset of TREC 2010.  X  The proposed multi-step approach to estimating likelihood that the candidate answer entity belongs to the target entity category specified in the query. The evaluation results demonstrate that this likelihood is a useful factor in ranking can-didate answers.

The paper is organised as follows: Section 2 provides an overview of related work, Section 3 gives a detailed description of the proposed methods, Section 4 presents evaluation of the methods, in Section 5 we analyse performance on sample topics, and conclude the paper in Section 6 . 2. Related work
Previous approaches to finding relationships between entities can be divided into two groups based on the type of the problem they address: (1) pre-defined relationship finding, where the target relationship is given and the goal is to find pairs tionship finding, where the task is to discover interesting/pertinent relationships between entities in a corpus, or between a Grishman, 2004 ).

Approaches to the pre-defined relationship finding are usually supervised or semi-supervised. The former take as input a training set of manually labeled entity pairs or an annotated corpus and attempt to classify new pairs of entities into those that have or do not have a given relationship ( Culotta &amp; Sorensen, 2004; Kambhatla, 2004; Miller, Fox, Ramshaw, &amp; Weis-
Snowball system ( Agichtein &amp; Gravano 2000 ) takes a set of seed entity pairs, and iteratively finds new extraction patterns and entity pairs. A system by Ravichandran, and Hovy (2002) was developed for factoid question answering. It requires a small seed set of entity pairs (question entity and correct answer) for each question type (e.g., birth year, discoverer), and learns a set of patterns that are most likely to contain the correct answer. Rosenfeld and Feldman (2007) proposed a system for relationship finding given only a general relationship template and a small number of keywords representing the rela-the Web by using a small set of patterns generated from the relationship keywords, and then uses the found seed pairs to discover other entity pairs that have the given relationship.

The ad hoc related Entity Finding task, such as the one formulated in the Entity track of TREC, is particularly challenging, since the input to the system only consists of the query entity, the target entity type and a free-form one-sentence narrative.
Supervised approaches are not suitable for this task as they require a large number of manually labeled examples. Although semi-supervised methods are generally more attractive since they require only a small set of seed pairs, they are poorly sui-ted for this task, as the seed pairs are not given.

In our work we address the problem of the missing seeds by generating them automatically from the narrative. Specifically identify instances of operating systems as seeds. Our next stage is to compute the similarity between candidate entities and betweenacandidateandaseedentityiscalculatedbasedonthedistributionalsimilarityprinciple.Eachentityisrepresentedas a vector of weighted grammatical dependency triples, co-occurring with it in a corpus. We developed a new method for calcu-lating distributional similarity between candidate and seed entities using BM25 with query weights ( Robertson et al., 1995 ).
This is a new application for BM25 matching function, which has not been used for computing word similarity before. As a sec-ond method of computing candidate-seed similarity we adapted Lin X  X  ( Lin, 1998 ) distributional word similarity method.
There is a large number of methods developed for calculating distributional similarity between words. A comprehensive review of different approaches is given by Weeds and Weir (2006) . According to the distributional similarity principle, words that occur in similar contexts, are likely to mean similar things. Distributional similarity methods differ by what linguistic units they use as context. For example, Pantel, Crestan, Borkovsky, Popescu, and Vyas (2009) use noun phrase chunks to the left and right of a term as its context, while Lin (1998), Weeds and Weir (2006), Kotlerman, Dagan, Szpektor, and Zhitomir-grammatical relations as features leads to the identification of  X  X  X ighter X  X  relationships between words, whereas the use of document-and sentence-level word co-occurrences would lead to the identification of  X  X  X ooser X  X  relationships. So, while the former are better for identifying words belonging to the same semantic class, the latter are more appropriate for group-use grammatical dependency relations as features. 2.1. Approaches to related entity finding in TREC
In this section we review some of the approaches to related entity finding in the Entity track of TREC 2009. Most of the methods developed by participants of the Entity track of TREC 2009 start with the retrieval of some units of information (documents, passages, sentences) in response to the queries generated from the topic. The retrieved units are then used for extracting candidate entities. Below we discuss various approaches based on: (a) how the queries are constructed, (b) what units are retrieved (e.g., documents, passages, sentences), (c) how candidate entities are extracted and ranked. 2.1.1. Query construction
As an alternative to using  X  X  X ntity name X  X  and  X  X  X arrative X  X  sections of topics as queries directly, some query structuring and expansion methods are explored. Vinod Vydiswaran, Ganesan, Lv, He, and Zhai (2009) model the information need as a triple (topic entity; relationship type; target entity), of which the first two are known. The word(s) denoting the relationship are extracted from the narrative and expanded with WordNet synonyms. Fang, Si, Yu, Xian, and Xu (2009) expand the entities from narratives with their acronyms identified using a dictionary-based approach. 2.1.2. Retrieval units
Most approaches start with the retrieval of documents by using experimental IR systems and/or web search engines. For example, Vinod Vydiswaran et al. (2009) use documents retrieved by the Indri IR system, from which they select snippets containing the query terms. McCreadie, Macdonald, Ounis, Peng, and Santos (2009) use the Divergence from Randomness (DFR) model, a term proximity-based model, and the number of incoming links to the documents. They also experiment with community-based document ranking. Zhai, Cheng, Guo, Xu, and Liu (2009) use BM25, Fang et al. (2009) use Google results filtered by the ClueWeb09 Category B documents, and Wu and Kashioka (2009) compare the use of Indri and Google. 2.1.3. Candidate entity extraction and ranking
Vinod Vydiswaran et al. (2009) extract entities from the document snippets containing query terms, apply a NER tagger, apply a number of heuristics to prune the list of candidate entities, and rank entities by a combination of the frequency of candidate entities in the retrieved snippets, and their co-occurrence with the entity given in the topic. McCreadie et al. (2009) use DBPedia and US Census data to build detailed representations of entities found in the Clue-
Web09 Cat. B collection. The representation of each entity include alternative names (DBPedia aliases), DBPedia categories and documents in ClueWeb09 Cat. B containing the entity. They propose a voting model to rank entities.

Zhai et al. (2009) use titles and anchor texts in the retrieved documents as candidate entities. For each candidate entity they build a pseudo-document, consisting of top 100 sentences containing this entity. They experiment with ranking entities based on the similarity of their pseudo-documents and the pseudo-documents of the entity given in the topic.
Wu and Kashioka (2009) use Wikipedia X  X  hyperlink structure, to reduce the list of candidate entities. Entity scores are calculated based on the presence of hyperlinks between the candidate entity X  X  Wikipedia page and the Wikipedia page of the entity given in the topic. They then retrieve snippets containing each candidate entity, and calculate a similarity score between the set of snippets and the query entity, experimenting with a language modelling approach and Support Vector Machines.

Kaptein, Koolen, and Kamps (2009) calculate similarity between the entity given in the topic and each candidate entity based on the co-citation information from the hyperlink graph constructed for the ClueWeb09 Cat. B collection. They also propose a method that extracts candidate entities from Wikipedia.

Fang et al. (2009) combine a number of approaches for ranking candidate entities, such as extracting entities from tables and lists in the retrieved web documents and using proximity in retrieved documents between a candidate entity and the method consists of extracting the first term from the narrative, which usually represents the category of the sought entities, and checking for each candidate entity if it occurs in the body or categories of its Wikipedia page. 3. Methodology
In the following subsections we describe our approach to entity finding in detail. Fig. 2 provides an overview of the main components of our method.

In the first stage (rectangle 1 in Fig. 2 ), the system retrieves an initial set of documents for the query from the Web. Only the sentences containing query terms plus one preceding/following sentence are retained. Named Entity tagging is applied to these sentences, and candidate entities are extracted and ranked. In the second stage, the target category name is automat-ically identified from the topic narrative; and in stage 3 the system finds hyponyms of this category name, and selects seed entities from the hyponyms. In stage 4, the entities (candidates and seeds) are represented as vectors of weighted grammat-ical dependency triples, and pairwise (candidate-seed) similarity is calculated. In stage 5, candidate entities are ranked by similarity to all seeds. Stage 1 is described in Sections 3.1 and 3.2 , while stages 2 X 5 are presented in Section 3.3 . 3.1. Extracting candidate entities
There are multiple sources, from which entities can be extracted, such as knowledge bases, e.g., Wikipedia, or the texts of top documents retrieved in response to the query. In this paper we aim to investigate the effectiveness of entity extraction from the texts of top retrieved documents. In our preliminary experiments we compared the extraction of entities from top documents retrieved from the ClueWeb09 Category B 1 collection using BM25tp (a variant of BM25 incorporating term prox-imity information) ( B X ttcher, Clarke, &amp; Lushman, 2006 ) with the extraction from top documents retrieved from the Web by a paper. Fang et al. (2009) also found that the use of documents retrieved by a search engine from the Web leads to better per-formance in entity finding than the use of an IR system (Indri) on the ClueWeb09 Category B corpus.

As the first step, the queries to retrieve top documents from the Web are generated from the  X  X  X ntity name X  X  and  X  X  X arra-tract named entities and other noun phrases from the topic. For this purpose we use a Part-Of-Speech tagger ( Brill, 1995 ) and to retrieve the top 50 documents from a Web search engine. We did not evaluate alternative values for the number of top documents retrieved. Our motivation to use 50 is to keep the number of documents for subsequent in-depth analysis rea-sonably small, and at the same time have sufficient amount of text to extract entities from.

The retrieved 50 documents are parsed to remove HTML tags, script and style sections, and broken into sentences. We then extract sentences that contain at least one query term. If a query term is a noun, the system attempts to match its sin-gular and plural forms. For each such sentence, we also extract one sentence before and one after. Let this set of sentences be category types, we extract all entities tagged with the corresponding category labels. However, for topics of category  X  X  X rod-uct X  X  we extract entities labelled as  X  X  X rganization X  X  and  X  X  X iscellaneous X  X . 3.2. Ranking candidate entities by TF  X  IDF and co-occurrence association measures
Having extracted candidate entities from the top 50 documents retrieved for each topic, we evaluate the following entity ranking methods: TF  X  IDF.
 Pearson X  X  v 2 (chi-square).
 Pointwise Mutual Information (PMI).

Co-occurrence association measures ( v 2 and PMI) are calculated between each candidate entity and the topic entity, i.e. the entity given in the  X  X  X ntity name X  X  section of the topic ( Fig. 1 ). 3.2.1. TF  X  IDF The TF  X  IDF score for each candidate entity is calculated using the following equation: where TF  X  frequency of the entity in the sentence set {S} extracted from the top 50 retrieved documents; N  X  number of documents in the ClueWeb09 Category B collection; n  X  number of documents in the collection containing the entity. 3.2.2. Pearson X  X  v 2
Pearson X  X  Chi-square statistic compares the observed frequencies of two words with their expected frequencies given the independence assumption ( Manning &amp; Sch X tze, 1999 ). If the difference between the observed and expected frequencies is large, then the null hypothesis of independence can be rejected. The Chi-square statistic described in ( Manning &amp; Sch X tze, 1999 ) applies to adjacent co-occurrences. In our work, we consider that the candidate entity ( c ) co-occurs with the topic en-tity ( t )if c appears within the window of 100 words either side of t . The reason for choosing the span of 100 is to capture broader contextual associations between words, rather than specific lexico-syntactic relationships. Tables 1 and 2 show how expected and observed frequencies are calculated taking in account the window size greater than one around the topic entity t . Table 3 shows how v 2 is calculated based on Tables 1 and 2 . All frequencies are gathered from the ClueWeb09 Cat-egory B corpus. T is the number of tokens (term instances) in the corpus. The joint frequency of the topic and candidate enti-joint frequencies, we use the Wumpus search engine 2 to find the number of shortest substrings containing t and c within the span of 100. Each shortest substring may only contain one instance of t and c . The ideal window around t is 200, i.e. 100 words either side, however, in reality the window may hit the document boundary or a window around another instance of t in a doc-ument. Because of this the observed windows can be smaller, therefore the average window size ( be calculated. In our experiments, it is computationally too expensive to obtain observed window sizes around every instance of t in the ClueWeb09 Category B corpus, therefore we approximate
The v 2 is calculated using Eq. (2) : 3.2.3. Pointwise mutual information PMI compares the probability of the joint co-occurrence of two terms with the probability that they occur independently. modified version of PMI ( Vechtomova, Robertson, &amp; Jones, 2003 ). Frequencies were obtained in the same way as for v the same approximation for v is applied in calculating PMI.
 3.3. Ranking candidate entities by the similarity to the target entity category
Since the chosen NER tagger can only be used to identify entities of a few broad categories, such as organisations and peo-we apply the distributional similarity principle, which is based on the observation that semantically close words occur in sim-ilar contexts. If we have a small number of correct seed entities, we can rank the candidate entities by the distributional sim-ilarity to them. As discussed in Section 2 , many methods reported in the literature that utilise the distributional similarity in our task is that the seed words are not given. However, the topic narratives have descriptions of the categories of entities that are to be retrieved. Our approach is to find seed entities based on the described categories. For example, the narrative of
TREC 2010 REF topic #62 is  X  X  X hat cruise lines have cruises originating in Baltimore? X  X  We developed a method to extract the category name from the narrative, i.e.  X  X  X ruise lines X  X  in this topic, and adapted a method for the automatic acquisition of the hyponymy relation proposed by Hearst (1992) to find entities that belong to this category. Seed entities are then selected from the hyponyms. We also developed a new method for computing the distributional similarity between seeds and candidate entities using BM25 with query weights, and ranking the entities by similarity to all seed entities.

The methods represented in Fig. 2 as rectangles 2 X 5 are described in this section. Among the ranking methods proposed in Section 3.2 the best results on the training topics are obtained using TF  X  IDF. Therefore, it was decided to use entities ranked by TF  X  IDF as the input to subsequent stages. Since this list of entities can be large and may contain a lot of noise,
In our experiments we set m to 200. In order to determine the value for m we took the entity names ranked by the TFIDF for the training topics, and did a simple pattern matching with the correct answer set of entity names. This showed that of all the correct entities present in the TFIDF-ranked list, 81% were ranked in the top 200. Therefore, 200 seemed to be sufficient. Lar-ger values are likely to introduce more noise terms into the subsequent stages, and only a small number of relevant entities. 3.3.1. Extracting category names from topic narratives To extract category names, the narratives are first processed using Brill X  X  Part-of-Speech (POS) tagger ( Brill, 1995 ) and a from the narrative. Generally, the first noun phrase in the narrative is selected as the category name, unless it is a personal pronoun or the noun  X  X  X earcher X  X . Table 4 lists examples of NP-chunked narratives of the TREC 2010 topics and the extracted category names. #38 in Table 2 ). 3.3.2. Identifying seed entities After the category name is extracted from the topic narrative, the next step is to find entities that belong to this category.
We adapted the unsupervised hyponymy acquisition method proposed by Hearst (1992) . Hearst X  X  method uses six domain-and genre-independent lexico-syntactic templates that indicate a hyponymy relation. The templates and examples of sentences found for the topic  X  X  X  X  X  like to find which operating systems I can use on the EEE PC. X  X  are shown in Table 5 . The extracted category name for this topic is  X  X  X perating systems X  X .

For each topic, six queries are constructed using the above templates, and the category name is extracted from the topic a commercial search engine as a phrase (i.e. quote-delimited). If the total number of pages retrieved by all six queries is few-er than 10, the first word in the category name is dropped and the search is repeated. If again it returned fewer than 10 pages, the first two words are dropped, and so on until either 10 or more pages are retrieved, or the remaining category name con-sists of only a single word, in which case we use whatever number of pages were found. If a category name is a single word, the query includes the topic title in addition to the template, in order to minimise the extraction of unrelated entities.
The documents retrieved for each query are processed to remove HTML tags, and split into sentences. The sentences con-taining the hyponymy lexico-syntactic patterns are then processed using the LBJ-based NER tagger ( Ratinov &amp; Roth, 2009 ).
Depending on the expected position of hyponyms in the lexico-syntactic pattern, NEs either immediately preceding, or fol-are extracted. For each topic, we extract only NEs with tags corresponding to the entity type specified in the topic, i.e. NEs  X  X  X ocation X  X  and  X  X  X roduct X  X  respectively. Below is an example of the output of the NER tagger for the above topic:  X  X  ... on some modern multi-user operating systems such as [MISC Windows XP], [MISC Windows Vista], [ORG Mac OS X], [PER OpenSUSE], [ORG Ubuntu] and [ORG Fedora] . X  X 
The entity type for this topic is  X  X  X roduct X  X , the following entities are extracted as hyponyms of  X  X  X perating system X  X :  X  X  X in-as person, therefore it is not extracted.

One problem with using all found hyponyms as seed entities is that they can be unrelated to the topic. Some category names extracted from topic narratives are very broad, for example,  X  X  X ountries X  X  in topic #22 ( Table 4 ). Applying the above hyponym acquisition algorithm based on such high-level hypernym is bound to produce a very large number of topi-cally-unrelated hyponyms. Computing distributional similarity between candidate entities and these hyponyms is likely to be ineffective and possibly detrimental to performance. We therefore must ensure that we use only those hyponyms as seeds, for which there exists some evidence of relationship to the topic. For this purpose, we defined as seeds the inter-section of found hyponyms and entities extracted from the top 50 documents retrieved for the initial query as described in process, we do not perform entity re-ranking on this topic, and keep the original rank order, which is output by the TF  X  IDF method. We believe that one seed entity is an insufficient representation of the category of the sought entities, and could substantially degrade performance if it is incorrect. 3.3.3. Computing distributional similarity between candidate and seed entities Distributional similarity between entities is computed based on the commonality of their contexts of occurrence in text.
In their simplest form, contexts could be words extracted from windows around entity occurrences. Alternatively, they could be grammatical dependency relations, with which an entity occurs in text. The use of grammatical dependency relations is more constraining in calculating entity similarity, and allows us to identify tightly related entities, which could be inter-substituted in a sentence without making it illogical and ungrammatical. For example, grammatical dependency relations in Table 6 tend to occur with noun phrases referring to sports people.

In contrast if we only use co-occurring words ( X  X  X in X  X  and  X  X  X eammate X  X ) in calculating similarity, we would get more loosely related entities, e.g. names of teams, sponsors, etc. As discussed in Section 2 , several previous approaches to calcu-lating distributional similarity between words use grammatical dependency relations. Since we are interested in identifying entities that are of the same semantic category as the seed words, we decided to use grammatical dependency relations in calculating entity similarity.

For each seed and candidate entity we retrieve 200 documents from ClueWeb09 Category B using BM25 ( Robertson et al., 1995 ) implemented in the Wumpus search engine. Each document is split into sentences, and sentences containing the en-tity are parsed using the Minipar 3 syntactic parser ( Lin, 1993 ) to extract grammatical dependency triples. Each dependency triple consists of two words and a grammatical relation that connects the two entities (examples are given in Table 6 ). These dependency triples are transformed into features representing the context of each candidate and seed entity. To transform a consisting of these features and their frequencies of occurrence with this entity. To compute pairwise similarity between the vectors of seed and candidate entities, we use two approaches: similarity computed using BM25 with query weights (Section 3.3.3.1 ) and similarity computed using Lin X  X  method based on Mutual Information (Section 3.3.3.2 ). 3.3.3.1. BM25 based similarity method. In order to compute the similarity between the vectors of seed and candidate entities, we adapted the BM25 with query weights formula. For each seed and candidate entity we calculate a Query Adjusted Com-bined Weight (QACW) by ( Sp X rck Jones, Walker, &amp; Robertson, 2000 ). In the QACW formula, the vector of the seed entity is treated as the query and the vector of the candidate as the document: where F  X  the number of features that a candidate entity c and a seed entity s have in common; TF  X  frequency of feature f in the vector of candidate entity; QTF  X  frequency of feature f in the vector of the seed entity; K = k feature frequency normalisation factor; b  X  document length normalisation factor; DL  X  number of features in the vector of the candidate entity; AVDL  X  average number of features in all candidate entities.

We evaluated different combinations of values of b and k 1 best results in NDCG@R obtained with b = 0.8 and k 1 = 0.8.
 In order to calculate the IDF of a feature, we need to have access to a large syntactically parsed corpus, such as the Clue-
Web09 Category B collection. Since we do not have such a resource, and it is computationally demanding to produce one, we approximate IDF of a feature with the IDF of its component word. For example, for the feature  X  X  X in V:subj:N X X  X  we calculate the IDF of  X  X  X in X  X  by using its document frequency in the ClueWeb09 Category B collection.

Arguably, when calculating the similarity of candidate entities to seed entities, we should take into account how strongly each seed entity is associated with the original TREC topic. Candidate entities similar to those seed entities, which have weak association with the topic, should be downweighted compared to those candidate entities, which are similar to seed entities strongly associated with the topic. We propose to quantify this association by using entity weights calculated by one of the entity ranking methods presented in Section 3.2 . In our experiments we use entity weights calculated using the TF  X  IDF method (Section 3.2.1 ), which gave the best results on the training data. Thus, the candidate entity matching score with all seed entities is calculated according to: where w s  X  weight of the seed entity s calculated using the TF  X  IDF entity ranking method.

Only those candidate entities that have EntitySeedBM25 as greater than zero are retained. The final ranking of entities is achieved through a linear combination of TF  X  IDF and EntitySeedBM25 according to the following equation:
Values from 0.1 to 1 at 0.1 intervals were evaluated for a on the 20 topics from the Entity track of TREC 2009, with the best results in NDCG@R obtained with a = 0.5. 3.3.3.2. Lin X  X  similarity method. As our second approach for computing pairwise similarity between a seed and a candidate entity, we adapted Lin X  X  distributional similarity method ( Lin, 1998 ). This is a well-known distributional similarity method that uses grammatical dependency relations as features, weighted using Mutual Information. As in the method above, each example, (Rubens Barrichello N:subj:V win). Each feature is weighted by using Mutual Information as follows: frequencies of all dependency triples matching the pattern where the grammatical relation is r and the second word is w 0 .
For example, if the entity is  X  X  X ubens Barrichello X  X  and the feature is  X  X  X in V:subj:N Rubens Barrichello X  X , the Mutual Informa-tion between  X  X  X ubens Barrichello X  X  and the feature will be calculated as follows:
In Lin X  X  experiments ( Lin, 1998 ), frequencies are computed from a newswire corpus of 64 million words, however in our case it would have been infeasible to parse an entire ClueWeb09 Category B collection, therefore we only use top 200 documents retrieved for each candidate entity using BM25 as outlined in Section 3.3.3 .
 entity s and a candidate entity c is calculated as follows: After calculating the similarity score, the candidate entity X  X  matching score EntitySeedLin is calculated in the same way as
EntitySeedBM25 (Eq. (5) ). Only those candidates which have EntitySeedLin greater than zero are retained. The final ranking (TFIDFEntitySeedLin) is achieved through linear combination of EntitySeedLin with TF  X  IDF in the same manner as was done for TFIDFEntitySeedBM25 (Eq. (6) ). Again, values from 0.1 to 1 at 0.1 intervals were evaluated for a on the 20 topics from the
Entity track of TREC 2009, with the best results in NDCG@R obtained with a = 1. The fact that a = 1 gives the best perfor-mance in TFIDFEntitySeedLin means that Lin X  X  score itself is not useful for candidate entity ranking, but only for filtering out non-relevant entities, with the ranking being done by TFIDF. This contrasts with the TFIDFEntitySeedBM25 method, where the best a is 0.5, meaning that TFIDF and EntitySeedBM25 components contribute equally to the final entity score. 4. Evaluation
Our methods were evaluated on three datasets: (1) The dataset of the Related Entity Finding task of the Entity track of TREC 2010 ( Balog et al., 2010 ). (2) List questions from the Question Answering (QA) track of TREC 2005. (3) List questions from the QA track of TREC 2006.

All parameters were tuned on the 20 Related Entity Finding topics from the Entity track of TREC 2009. 4.1. Evaluation on the Related Entity Finding task of the Entity track of TREC 2010 the systems are required to retrieve one homepage, which must be represented as the ClueWeb09 Category A document ID. In fact, a working definition of an  X  X  X ntity X  X  in the Entity track is something that has a homepage.

Relevance judgements of entity homepages were done on a 3-point scale: 2  X  primary page (i.e. homepage of the correct entity), 1  X  descriptive page related to the correct entity, and 0  X  all other pages.

The two official evaluation measures are nDCG@R  X  normalised discounted cumulative gain at R , where R is the number of primary and relevant homepages for that topic, and P@10  X  fraction of primary homepages among the documents retrieved for the top 10 entities. The Mean Average Precision (MAP) and Precision at R were also calculated for TREC 2010 topics.
We developed a simple homepage finding algorithm, which consists of retrieving the top 10 webpages for each entity from a commercial Web search engine, filtering out a small number of common URLs, such as  X  X  X ictionary.com X  X ,  X  X  X ace-book.com X  X ,  X  X  X inkedin.com X  X ,  X  X  X ikipedia.org X  X  and using as homepage the top ranked page that also exists in the ClueWeb09
Category A collection. The evaluation procedure was the same for both training and test topics. The evaluation results on the tySeedBM25 in 3.3.3.1 and TFIDFEntitySeedLin in 3.3.3.2.

Since the  X  X  X FIDFEntitySeedBM25 X  X  and  X  X  X FIDFEntitySeedLin X  X  runs re-rank the top 200 entities in  X  X  X FIDF X  X , the latter is used best NDCG@R values on the 20 training topics ( Table 7 ). 4.2. Evaluation on the list questions from the QA track of TREC 2005 and 2006
QA list questions are formulated slightly differently from the Entity track Related Entity Finding (REF) topics. For each topic a target entity is specified, which is similar to the entity_name part of Entity track topics. Each topic has one or two list questions, formulated in a similar way as the narrative section of the Entity track topics. A major difference of QA list questions from the Entity track REF topics is that target entity types are not given. Also, some list questions are looking for answers of types other than  X  X  X erson X  X ,  X  X  X rganization X  X ,  X  X  X ocation X  X  and  X  X  X roduct X  X . Examples of such questions are: What are the names of the three Great Pyramids? Name unusual flavors created by Ben &amp; Jerry X  X .

For our evaluation we only selected questions seeking entities of the above four types, as other types do not necessarily fall under the definition of an entity accepted in the Entity track, i.e. something that has a homepage. We also manually questions in the QA 2005 and 2006 datasets is 93 and 89 respectively.

The evaluation methodology for the list questions in the QA track required the participating sites to submit an unordered set of answer X  X ocumentID pairs, where answer is the entity string and documentID is the ID of a document supporting the entity as an answer. The document collection in the official QA track evaluation was AQUAINT. The official evaluation mea-sure was an F-measure, computed as F =(2  X  IP  X  IR)/(IP + IR), where Instance Recall (IR) is the number of distinct instances (entities) judged correct and supported by a document out of the total number of known correct and supported instances, and Instance Precision (IP) is the number of distinct instances judged correct and supported by a document out of the total number of instances returned.

Unfortunately, it is not possible to use this evaluation methodology post-TREC since the number of judged supporting documents is very limited. Track organisers released sets of patterns representing the correct answer strings extracted from the answer pool, in order to allow researchers to perform post-TREC evaluations. The set contains only one pattern repre-senting each correct answer, and takes the form of a regular expression, such as  X  X (Holland|Netherlands) X  X . Two major limi-tations of this pattern set are: it only contains correct answers from the pool, and may therefore be incomplete for some topics, and, secondly, the patterns themselves may not exhaustively cover all spelling and lexical variations of answers.
The evaluation reported in this section was performed using these patterns. F-measure as well as standard evaluation measures used in the Entity track of TREC 2010 were calculated. Since supporting documents are not used, Instance Recall is re-defined as the number of distinct instances that match patterns out of the total number of patterns for the question, and Instance Precision as the number of distinct instances that match patterns out of the total number of instances returned.
Each pattern can only be matched once, in other words, any repeated matches on the same pattern are ignored. The evalu-ation results are shown in Tables 10 and 11 . 5. Discussion
The best performance on the TREC 2010 REF topics was obtained by the run  X  X  X FIDFEntitySeedBM25 X  X , which re-ranks and filters the top 200 entities in  X  X  X FIDF X  X . Figs. 4 and 5 show differences in performance by topic in nDCG@R and P@10 respec-tively on the 50 TREC 2010 topics.

As can be seen from the figures, re-ranking entities by the similarity to automatically extracted seed words overall has a positive effect on performance. Entity re-ranking is performed only if more than one seed entity is found (Section 3.3.2 ). Out of 50 TREC 2010 topics, 39 have more than one seed entity. For the remaining 11, the TFIDF entity ranking is kept. The aver-age number of seeds for all 50 topics is 16.6. We identified the following reasons for finding zero seeds for some topics: (a) A small number of hyponyms found. For example, in topic #47 (Who are the balloon manufacturers associated with (b) Errors in category name extraction (only two cases): Below we analyse in detail one of the topics that is improved, and one that is degraded as a result of re-ranking.
Positive example: TREC 2010 REF topic #23 ( X  X  X hat recording companies now sell the Kingston Trio X  X  songs? X  X ). TFIDFEn-titySeedBM25 has improvements over TFIDF in all measures, specifically nDCG@R increased from 0.225 to 0.301, and P@10 extracted from the narrative is  X  X  X ecording companies X  X , and the seed entities automatically identified using the method de-scribed in Section 3.3.2 are:  X  X  X arner bros X  X ,  X  X  X ecca X  X ,  X  X  X olumbia records X  X ,  X  X  X apitol records X  X ,  X  X  X ear family records X  X . therefore used in computing similarity. The top features ranked by the number of seed entities co-occurring with them are listed in Table 12 .
 Table 13 shows the top 10 entities ranked by TFIDF and TFIDFEntitySeedBM25.
 TFIDF method found only one name of a recording company ( X  X  X apitol records), which is also a correct answer, while TFID-FEntitySeedBM25 found four recording company names ( X  X  X olumbia records X  X ,  X  X  X lektra records X  X ,  X  X  X apitol records X  X  and  X  X  X ecca records X  X ), with the last two being the correct answers. This demonstrates how re-ranking of the entities by their similarity to seeds promotes entities of the correct category to the top.

Negative example: QA 2005 topic #97.6 ( X  X  X ounting Crows. List the Crows X  band members. X  X ). After re-ranking, perfor-mance dropped from 0.5896 to 0.1909 in nDCG@R and from 0.6 to 0.1 in P@10. The known correct entities for the topic gory name is  X  X  X and members X  X . The hyponymy finding method extracted 124 entities, with only the following six qualifying surprising that the resulting entity ranking is poor. Refinement of the automatic seed finding algorithm is one of our top pri-orities for the future work. The top 10 features ranked by the number of co-occurring seed entities are given in Table 14 , and the top 10 entities ranked by TFIDF and TFIDFEntitySeedBM25 are shown in Table 15 .

The TFIDF method found all known relevant entities among the top 10 entities retrieved, while TFIDFEntitySeedBM25 found only one. Interestingly, the latter promoted to the top unrelated entities, but which, nonetheless, represent musicians, such as  X  X  X oni mitchell X  X ,  X  X  X ruce springsteen X  X  and  X  X  X ob dylan X  X . One thing they have in common compared to the correct an-swers to this topic is that they all occur more frequently in ClueWeb09 Category B corpus. This means that they co-occur with a larger number of features and, consequently, are likely to have more features in common with the seeds, thereby to the top, more needs to be done to improve ranking by the strength of association of such entities with the topic.
Generally, it appears that the number of features with above-zero frequency per seed entity is positively correlated with performance. The Pearson coefficient between the average number of such features per seed per topic and nDCG@R is 0.3.
There is also a positive correlation (0.4) between nDCG@R and the number of seeds per topic. Both were calculated on 43 QA 2005 topics, which have more than one seed. It is also interesting to see which grammatical relationships are most repre-sentative in the features of both seed and candidate entities. Table 16 contains eight most frequent grammatical relation-ships in the feature vectors of candidates and seeds from TREC 2010 topics. 6. Conclusions
In this paper we propose an approach to finding related entities which relies primarily on statistical and linguistic meth-ods. The approach was evaluated using the Entity track dataset of TREC 2010, as well as the QA track list questions from TREC 2005 and 2006. Below we summarize our findings and possible directions for future work with respect to each research question formulated in Section 1 .
 RQ1 : How to measure the relatedness of the candidate answer entity to the query?
The candidate answer entities in our experiments were extracted from the sentences containing at least one query term plus one preceding and one following sentences in the top 50 documents retrieved for the query. Three measures were eval-uated for ranking these entities: Pointwise Mutual Information, Pearson X  X  v effective. It may be possible to achieve further improvements by investigating other methods of (a) finding the initial set of candidate answer entities and (b) calculating their association with the query. For instance other researchers (see Section 2.1 ) made use of the HTML document structure, extracting anchor text and table elements.

RQ2 : How to estimate the likelihood that the candidate answer entity belongs to the target entity category specified in the query?
Our approach was to identify target entity category in the narrative, find its hyponyms to be used as seeds, and compute distributional similarity of candidate answers to the seeds. The method for identifying entity category names works well.
Only 2 topics out of 50 in TREC 2010 REF dataset have incorrectly extracted category names. The seed finding method, how-ever, needs more work. Out of 50 topics in the same dataset, 11 have one or zero seeds. Our analysis indicates that the num-ber of seed entities is positively correlated with the performance of the entity finding system, therefore we need to investigate ways of increasing the number of seed entities, while still maintaining high confidence that they are represen-tative of the target entity category. In future, other hyponymy finding methods need to be evaluated for this purpose. Finally, we developed a novel method for calculating distributional similarity between words using an IR model (BM25). The method showed improvements over Lin X  X  distributional similarity method on two datasets. Furthermore, Lin X  X  method was only use-ranking candidate entities ( a = 0.5 gave best results).

RQ3 : Is the likelihood that the candidate answer entity belongs to the target entity category useful in identifying correct answer entities?
Evaluation results show that re-ranking of candidates by their similarity to seeds is effective, with some improvements being statistically significant over the baseline (TF  X  IDF). A detailed analysis of sample topics in Section 5 reveals that the method is generally good at promoting entities of the correct category to the top ranks, which is its primary purpose. How-ever, more work needs to be done at better integrating this measure with the topic association measure. As demonstrated in one of the examples in Section 5 , entities that have strong similarity to seeds, and belong to the correct category, but have weak association with the topic, may be ranked highly. This is especially the case when the correct answers are low-fre-quency words/phrases, and hence there is little context available for accurately determining their distributional similarity to the seeds.

One aspect of related entity finding that this work does not address is identification of the type of semantic relationship between the topic entity and candidate entities. In some cases, there exists only one or a small number of possible relation-ships between the topic entity and entities of the correct category type, for instance,  X  X  X hat recording companies now sell the Kingston X  X  Trio X  X  songs? X  X  (TREC 2010 REF topic #23). This could be considered as one of the most typical relationships between a recording company and a musician or band, therefore it is likely that if a musician/band and a recording company name have a strong association in the corpus, they have this relationship. On the other hand, consider a query such as  X  X  X ame people who have won the Iditarod. X  X  (QA 2006, topic #185.5). There could be a number of possible relationships between  X  X  X eople X  X  and  X  X  X ditarod X  X , for instance, participants, sponsors, winners. For topics such as this it may be helpful to make use of the additional information provided in the query to identify entities with the correct relationship. References
