 JIANG-CHUN CHEN and JYH-SHING ROGER JANG National Tsing Hua University, Taiwan 10: 2  X  J.-C. Chen and J.-S. R. Jang 1. INTRODUCTION Mandarin Chinese is a tonal language and each character carr ies a tone out of five possible candidates. These five tones are mostly chara cterized by their pitch contours, as Figure 1 shows. In continuous speech, the pitch contour of a syllable is heavily affected by the pitch contours of the pre ceding and succeed-ing syllables. Moreover, the tone of a given Chinese charact er also depends on the rule of tone sandhi Chao [1968], which refers to the chang e of tones during a fluent utterance. (For example, the tone sequence of  X  X ong t ong, X  meaning  X  X resident X  in Mandarin Chinese, is t3-t3. However, due to p honological con-straints, it should be pronounced as t2-t3 in a fluent continu ous utterance.) Hence continuous tone recognition within a given utterance remains a chal-lenging task in the literature.
 (SVM) and Gaussian mixture models (GMM), respectively, hav e reported sig-nificant improvement [Peng and Wang 2005; Qian et al. 2007] ov er previ-ously proposed hidden Markov model (HMM)-based methods [Le e et al. 2002]. These two approaches both use cross-syllable acoustic feat ures for tone model-ing after the syllables are identified by forced alignment of HMM based on a speaker-independentspeech recognition engine. However, it is well known that the HMM-based acoustic modeling cannot precisely detect ch anges between acoustic models [Toledano et al. 1998; Hosom 2002]. The impr oper alignment might lead to deteriorating performance in tone recognitio n. In particular, if some of the unvoiced part of a syllable is erroneously includ ed in the voiced part for pitch tracking, the pitch contour will be untrustwo rthy for tone recog-nition. In order to deal with this problem, we propose a robus t approach called Tone Recognition Using Extended Segments (TRUES) for HMM-b ased contin-uous tone recognition. The proposed approach extracts an un broken pitch con-tour from a given utterance based on dynamic programming ove r time-domain acoustic features of average magnitude difference functio n (AMDF). The pitch contour of each syllable is extended for tri-tone HMM modeli ng, such that the influence from inaccurate syllable boundaries is lessened. Finally, a tone lat-tice is constructed and the most likely tone path is determin ed using dynamic programming that satisfies the context constraints of tri-t one models. related work of tone recognition. Section 3 introduces the p roposed TRUES for tone recognition. Section 4 demonstrates the experimental results. Section 5 gives concluding remarks and future work. 2. RELATED WORK In the past decade, a number of approaches to tone recognitio n have been pro-posed in the literature. These approaches can be categorize d into two types, including one-step [Seide and Wang 2000; Zhang et al. 2006] a nd two-step [Wang et al. 1994; Chen et al. 2001; Lin and Lee 2003; Peng and W ang 2005; Wang et al. 2006a; 2006b; Qian et al. 2007] approaches. One-s tep approaches, also known as embedded tone modeling, employ both spectral a nd the pitch features for better HMM modeling of tonal syllables. Howeve r, most of the one-step approaches for tone recognition aim at the task of l arge vocabulary continuous speech recognition (LVCSR) instead of computer -assisted pronun-ciation training (CAPT) for Mandarin Chinese. (The basic di fference between CAPT and LVCSR is that in CAPT, we need to identify the error ma de by the speaker, while in LVCSR, we need to find the most likely transc ription of the utterance.) It is indeed possible to use one-step approache s for CAPT. How-ever, by using a two-step approach for CAPT where the phone an d the tone are separately modeled, it would be easier for us to identify the cause of mistakes made by the system. Moreover, as far as Mandarin Chinese is co ncerned, there are about 1 , 345 tonal syllables, and the corpus collection and transcri ption for one-step approaches involve a tremendous amount of human la bor. Therefore, in this study we shall focus on two-step approaches, also kno wn as explicit tone modeling, in which syllables within an utterance are id entified via forced alignment first, and tone recognition is then performed on ea ch segmented syllable to find its tone, in particular for the applications of speech assessment for Mandarin Chinese.
 et al. 2001] or voiced-only [Lee et al. 2002]. In these studie s, the performance was generally acceptable but not totally satisfactory sinc e cross-syllable in-formation was not fully exploited. Chen et al. [1997] used an exponentially decayed function as the smooth pitch contour for the unvoice d part for HMM modeling based on both pitch and mel frequency cepstral coef ficient (MFCC). A similar approach has also been proposed by Chang et al. [200 0]. However, only the voiced part (toneme) is considered to carry tone inf ormation, but not the unvoiced part (preme). In Zhang et al. [2006], an improve d HMM based on multi-space distribution (MSD) [Tokuda et al. 2002] is adop ted, which takes the pitch contours of unvoiced and voiced parts together wit h their MFCC in a unified manner for tonal syllable modeling for LVCSR. Again , the pitch con-tour of the unvoiced part is assumed to be a fixed number that do es not provide full information toward tone recognition. More recently, P eng and Wang [2005] and Qian et al. [2007] proposed the use of cross-syllabic fea tures and demon-strated the improvement over the commonly used intra-sylla bic features [Lee et al. 2002]. In particular, Qian et al. [2007] have proposed supratone model-ing with improved performance over conventional voiced-on ly approaches [Lee et al. 2002]. By using bi-tone or tri-tone as the basic unit fo r GMM training, supratone modeling takes contextual information of voiced regions into consid-10: 4  X  J.-C. Chen and J.-S. R. Jang eration for tone recognition. Similar concepts have been pr oposed by Peng and Wang [2005].
 by the unvoiced part of a syllable for tone recognition. Thou gh there is no pitch for unvoiced parts, some sort of related information could p rovide some transi-tion information from the preceding or to the succeeding syl lables. Moreover, syllable/phone boundaries provided by forced alignment is generally imprecise and the use of the whole syllable (including both the unvoice d and voiced parts) can in general enhance the robustness of tone recognition wh en dealing with inaccurate boundaries. As a result, this article proposes t he use of an unbro-ken pitch contour of an utterance derived via dynamic progra mming (DP) over time-domain acoustic features of AMDF. The pitch contour ov er voiced parts does provide pitch information for tone recognition. On the other hand, the  X  X akeup X  pitch contour over unvoiced parts is not really pit ch at all, but it does provide transition information from the preceding and to the succeeding voiced parts, subjected to the constraints of DP. We can thus extend the pitch contour of each syllable to deal with imprecise boundaries, and construct an HMM-based tri-tone model using both pitch and energy. By usi ng TRUES, we have achieved a relative error rate reduction of 49.13% co mpared with the state-of-the-art approach of supratone modeling [Qian et a l. 2007]. A detailed description of the proposed method follows in the next secti on. 3. THE PROPOSED APPROACH Figure 2 is the flowchart of the proposed TRUES approach. Firs t of all, an utterance is segmented into syllables via forced alignment of Viterbi decod-ing using a speaker-independent speech recognition engine , which was trained on a balanced corpus of Mandarin Chinese recorded by 300 subj ects in Tai-wan. Each speech feature vector has 39 dimensions, includin g 12 MFCC and 1 log energy, along with their delta and double-delta values . From the speech corpus, 174 right-context-dependent (RCD) 1 biphone models were derived via Hidden Markov Model Toolkit (HTK) [Young et al. 2004]. Regar ding the per-formance of forced alignment, please refer to another artic le by our lab [Lin et al. 2005], which achieves a moderate segmentation accura cy of 90% within 50 milliseconds. Moreover, some techniques are used here to improve the ef-ficiency and effectiveness of the segmentation, including o ptimization of beam search [Jang and Lin 2002] and dynamic short-pause insertio n [Chen et al. 2004]. At the same time, we apply the proposed DP-based pitch tracking over AMDF matrices to obtain a smooth unbroken pitch contour for a given utter-ance, as explained in Section 3.1. Then we extend the pitch co ntour of each syllable to obtain an extended segment for tri-tone modelin g via HMM, as described in Section 3.2. For recognition, we construct the tri-tone lattice for a given test utterance and determine the best hypothesis by DP with contextual constraints imposed by tri-tone models, which will be cover ed in Section 3.3. 3.1 Unbroken Pitch Determination Using Dynamic Programmin g In order to preserve the pitch transition between two voiced parts, we have de-veloped a DP-based pitch tracking method that gives a smooth unbroken pitch contour for a given utterance. We can control the smoothness of the pitch con-tour via a penalty term that penalizes on pitch difference be tween two neigh-boring frames. The larger the penalty term is, the smoother t he pitch contour will be. In practice, we can tune the penalty term such that th e identified pitch contours can be as close as possible to those transcribed man ually. In fact, the penalty term is rather robust such that a wide range of its value can give approximately the same satisfactory performance, as expla ined in Section 4. For simplicity, we shall refer to the proposed pitch determi nation algorithm as unbroken pitch determination using dynamic programming (U PDUDP) in the subsequent discussion.
 16 bits. To derive the pitch contour, we first compute the AMDF [Huang et al. 2001] for each frame with a frame size of 40 milliseconds (640 samples) and a frame step of 10 milliseconds (160 samples). The  X  X oving par t X  of our AMDF is actually only the first half of the frame, as shown in the fol lowing equation: 10: 6  X  J.-C. Chen and J.-S. R. Jang where frame[ u ] is the u th sample value of a given frame, and amdf( j ) is the j th value of the AMDF vector. Since we are only using half of the frame as the moving part, we need to flip the frame horizontally for bet ter results if the energy of the first half is smaller than that of the second half . In other words, the moving part is always the half with larger energy. Figure 3 shows typi-cal voiced and unvoiced frames (where the unvoiced frame req uires horizontal flipping) together with their AMDF vectors.
 minimum AMDF of a frame within the index range of [16, 320] (co rresponding to a frequency range of 50  X  1000Hz, or 31.35  X  83.21 semitones) to determine the frame X  X  pitch. However, it is well known that the undesir able effect of half or double frequencies is likely to happen, leading to an octave below or above the real pitch. As a result, we need to have a more robust mechanism to identify a smooth pitch contour. Note that we can put all the A MDF vectors of a given utterance into a 320  X  n matrix, where n is the number of frames, as shown in the second plot of Figure 4. Our mission is to find a pat h through the AMDF matrix such that we achieve a balance between the value o f AMDF (as small as possible) and the smoothness of the pitch contour. M ore specifically, 320, we can define a cost function as follows: where amdf i is the AMDF vector of frame i ,  X  is the transition penalty term, and m is the exponent for the difference in a path of two neighborin g frames. Obviously, the first term in the cost function is the AMDF valu es along the path, while the second term controls the smoothness of the pa th (thus the computed pitch contour). If  X  is larger, then the path and the corresponding 10: 8  X  J.-C. Chen and J.-S. R. Jang pitch contour are smoother. In particular, if  X  is zero, then minimizing the above objective function is equivalent to minimum-picking of the AMDF vector of each frame. The value of m is empirically set to 2 in this study. where the optimum-valued function D ( i , j ) is defined as the minimum cost starting from frame 1 to i , with p i = j . Then we can come up with the recurrent equation for, D ( i , j ) as follows:
D ( i , j ) = amdf i ( j ) + min AMDF to define a cost function for DP-based pitch tracking, as proposed by Boersma and Weenink [2007]. However, the use of AMDF provide s the follow-ing advantages:  X  X omputation of AMDF is simpler since there is no multiplica tion involved, as required by ACF.  X  X CF causes overflow on 32-bit integers when the frame size is 640 samples with 16-bit resolution. The dynamic range of AMDF is much sma ller and thus we can compute AMDF using 32-bit integers in a much more e fficient manner.
 roughly the same performance in terms of pitch tracking. As a result, we have adopted DP over AMDF, which is less computation intensi ve. Wang and Seneff [2000] have also proposed DP-based pitch tracking in the frequency domain. Comparatively, our approach requires less computa tion since only time-domain processing is required.
 over an AMDF matrix. The upper plot is the waveform of the utte rance of  X   X   X   X   X   X   X  ( X  X i lu chan sheng chang X  in Hanyu Pinyin, meaning  X  X t the western road, a cicada sings X ), which is a sentence randomly selected from the most famous 300 poems in the Tang Dynasty. The central plot is the AMDF matrix shown as an image, with darker background indicating lower values of AMDF. From the AMDF image, we can almost locate the optimum path visually, especially during voiced regions. The optimum pa th at  X  = 13 , 000 is indicated by white dots over the AMDF image. The lower plot shows the obtained unbroken pitch contour, where the segmented pitch contour (shown as circles) corresponds to the voiced regions identified by f orced alignment. characteristics: (1) The proposed pitch tracking algorithm is rather robust i n the sense that (2) We do not distinguish voiced from unvoiced frames, leadi ng to a smooth (3) The smoothness of the pitch contour is controlled by the p enalty  X  . In fact, 10: 10  X  J.-C. Chen and J.-S. R. Jang pitch tracking (RAPT 2 ) [Talkin 1995; SFS 2004] and UPDUDP. Three observa-tions are in order:  X  X he pitch contour by UPDUDP is smoother than the one by RAPT, which has undesirable rapid fluctuations.  X  X he pitch contour by UPDUDP is unbroken while the one by RAPT is only available within voiced regions. Admittedly, there should be no pitch within unvoiced/silent regions. However, our UPDUDP algorithm te nds to fill up the unvoiced parts with a DP-optimized path that connects bo th voiced re-gions. This  X  X akeup X  pitch contour over an unvoiced region p rovides impor-tant contextual information for pitch modeling, as explain ed in Section 4.  X  X he first two tones are t1 and t4. Theoretically, t4 should be high-low, but it was affected by the preceding t1, such that a short segment of low-high is created at the very beginning of the second syllable.
 tone, X  which is a commonly used technique for context-depen dent tone modeling. 3.2 Tone Recognition Using Extended Segments (TRUES) In conventional tone modeling, only the voiced part of a syll able is used [Wang et al. 1994; Chen et al. 1997; Wang et al. 2006a; 2006b] while t he information of the unvoiced part is usually discarded, as shown in the firs t subplot of Fig-ure 6(b). One of the recently proposed tone modeling approac hes is supratone modeling by Qian et al. [2007], where each syllable is repres ented by a pitch vector of length 3 within the voiced region. Then the bi-tone (with a pitch vec-tor of length 6) or tri-tone (with a pitch vector of length 9) i s employed as the basic unit for GMM training. The second subplot of Figure 6(b ) illustrates bi-tone-based supratone modeling, where the voiced part in a sy llable is equally divided into three parts to find their average pitch values (i ndicated by three asterisks).
 takes information from both voiced and unvoiced regions in a syllable into con-sideration. Moreover, to incorporate contextual informat ion, we extend the re-gion of interest from the syllable under consideration to in clude portions of its preceding and succeeding syllables. Acoustic features of t he extended segment are then extracted for tri-tone-based HMM training. Our app roach, TRUES, has the following advantages over previously proposed tone recognition methods: (1) The extension of a syllable X  X  pitch contour can cope with inevitable inaccu-(2) Pitch contours obtained from UPDUDP are more robust than intra-frame-(3) UPDUDP can generate continuous unbroken pitch contours for a given ut-10: 12  X  J.-C. Chen and J.-S. R. Jang tains pitch contours in the silent region (between 1.02 and 1 .09 seconds) and the unvoiced region (between 1.09 and 1.15 seconds). This  X  X  akeup X  pitch contour actually contains the transition information from the current t4 to the next t2, which has rarely been considered in previously prop osed approaches. (For simplicity, we shall use t1, t2, t3, and t4 to represent t ones 1, 2, 3 and 4, respectively.) In other words, if the succeeding syllable h as a voiced consonant (such as the second syllable  X  X u X  in Figure 6), then the propo sed TRUES is sim-ilar to bi-tone-based supratone modeling. However, if the s ucceeding syllable has an unvoiced consonant (such as the fourth syllable  X  X hen g X  in Figure 6), then it takes on a new meaning other than supratone modeling. The perfor-mance of the proposed TRUES will be detailed in Section 4. 3.3 Dynamic-Programming Optimization over Tri-tone Latti ce In the recognition phase, each syllable can generate T log probabilities, assum-ing there are T HMMs for the syllable. For syllables not at the beginning or end of an utterance, T is equal to 125 since we have five tones for each of the preceding, current, and succeeding syllables. Otherwise, T is equal to 25 since either the preceding or the succeeding syllable is silence.
 ing data. To deal with this problem, these models are tied wit h other similar models according to the knowledge-based regression class, which is similar to the tying of acoustic models that Woodland et al. [1994] prop osed. We can put these candidates into a tri-tone lattice and use DP to find the optimum path with the largest accumulated log probability while tak ing the contextual transition constraint into account. The transition constr aint states that if the current syllable is t  X   X  tb + tc (where the asterisk indicates a wild card), then the succeeding syllable must be in the format of tb  X  tc + t  X  . This is similar to the work by Qian et al. [2007] and Wang et al. [2006a]. Figure 7 demonstrates two legal paths that could be obtained from DP.
 language models to reduce the search space. For instance, we can eliminate some of the impossible combination between a syllable and it s tone [Qian et al. 2007]. However, in this study, the tone recognition is mainly used for tone assessment in a CAPT system for Mandarin. As a result, th e second-language (L2) learners usually have little idea about phono logical constraints and language models. Therefore, we cannot take full advanta ge of these con-straints/models to improve tone recognition in this study. 4. EXPERIMENTAL RESULTS 4.1 Speech Corpus We used the Tang Poetry corpus 3 [Tang Poetry Corpus 2002 X 2006] for the ex-periment of continuous tone recognition in this study. The m icrophone-based corpus is composed of about 3,211 utterances in total for eac h year starting from the year 2002. These utterances are based on the 3,211 se ntences col-lected from the most famous 300 poems from the Tang Dynasty (a lso known as  X   X   X   X  y  X   X  in Chinese). The corpus was recorded by students who took th e Audio Signal Processing and Recognition course in the CS Dep t. of Tsing Hua University, Taiwan. Since the students could record the cor pus either at home or at the lab without supervision, the recording quality is s omewhat inconsis-tent. In this study, we used the corpus recorded from 2002 to 2 006, consisting of 14,104 utterances recorded by 90 males and 20 females, wit h a sampling rate of 16 kHz and 16-bit resolution. Most of the sentences co nsist of five or seven syllables. The corpus of 2003-2006 (10,894 utterance s, 66,439 syllables, 100 subjects) was used as the training data while the corpus o f 2002 (3,210 ut-terances, 19,462 syllables, 10 subjects) were used as the te st set. Pitch track-ing was performed at a 40-millisecond frame size and 10-mill isecond frame step. Each feature vector has six dimensions, including pit ch, energy, their delta, and double-delta values. To deal with key transposit ion, the mean vec-tor of each extended segment was subtracted from the origina l six-dimensional feature vector.
 all the five tones in Mandarin Chinese, including tone 1 (high flat), tone 2 (low rising), tone 3 (high low rising), tone 4 (high falling), and tone 5 (neutral tone). Tone 5 consists of only 0.14% of the corpus, indicating the sc arcity of available data, which leads to poor performance for this tone, as detai led later. Viterbi decoding based on a speaker-independent speech rec ognition engine. 10: 14  X  J.-C. Chen and J.-S. R. Jang We found 267 syllables to be less than 40 milliseconds or more than 600 mil-liseconds, which are considered to be erroneous cases in for ced alignment. Ut-terances containing these erroneous syllables were remove d from the data set. Furthermore, the well-known tone sandhi problem of consecu tive tones 3 was resolved by manual transcription. (For consecutive tones 3 , there are 283 dou-blets, 26 triplets, and 4 quaternaries in the corpus.) We use d HTK to train the left-to-right tri-tone HMM with diagonal covariance. N o decision tree was applied in building tri-tone models since the Tang Poetry co rpus is quite large and there is little problem of data sparseness. However, it i s believed that by tying data for a rarely seen model using the decision tree, it is likely to improve the performance, particularly for the models without suffic ient training data, such as t5. 4.2 Performance Comparison and Discussion Figure 8 shows the performance of supratone modeling, (exce pt for RCD Monotone 4 which was cited by Qian) [Qian et al. 2007] with RAPT (adopted by SFS) and the proposed UPDUDP pitch determination algorit hm. (All our subsequent discussion on performance is in terms of recogni tion correctness rate, unless otherwise specified.) Obviously, the proposed UPDUDP outper-forms RAPT when supratone modeling and RCD Monotone are adop ted for tone recognition, as demonstrated by three solid lines (by U PDUDP) and three dot lines (by RAPT) in the figure. More specifically, the best p erformance of supratone can be improved from 49.56% (supratone/bi-ton e with RAPT) to 70.65% (supratone/tri-tone with UPDUDP), indicating a sig nificant reduction of 41.81% in error rate by the use of UPDUDP. The improvement i s mostly due to the robustness of UPDUDP, which is particularly effec tive for the Tang Poetry corpus since it was recorded by students in various no nstudio environ-ments under no supervision. In other words, the recording qu ality is not tightly controlled, which reflects the real situation when the users are using a CAPT system. As a result, the recording quality is not as good as th at of the Can-tonese speech database used by Qian. Our experiment indicat es that RAPT is not able to extract the pitch contour reliably from such a noi se corpus, while the proposed UPDUDP can employ DP to do a better job for tone re cognition. Moreover, it should be noted that the phonological rules, wh ich were used in Qian X  X  original supratone modeling to enhance the performa nce by removing illegal combinations of syllables and tones in lattice refin ement (LR), are not used here. This is simply because Qian X  X  approach targets to ne recognition for LVCSR, while ours targets tone recognition for CAPT wher e the user (L2 learner) knows little about the correct combination of syll ables and tones.  X  to 30,000. The determination of this value will be detailed l ater. with some of its less-featured variants, with respect to the number of mixtures per state for the tri-tone-based HMM. The sweet spot of 85.07 % is found at 64 mixtures per state for the proposed full-featured TRUES (sh own as a solid line with cross markers) using a segment extension ratio of 1.2 fo r both ends and a penalty  X  = 30,000. (More details of these parameters will be covered l ater.) We shall list some of our observations and insights into Figure 9, as shown next. Note that if the whole syllable is taken into consideration f or recognition, we denote it as  X  X oiced + unvoiced. X  On the other hand, if only th e voiced part (including voiced consonants/vowels) is used for recognit ion, we denote it as  X  X oiced only. X  (1) LR can consistently enhance the performance by a large ma rgin, as shown (2) The use of  X  X akeup X  pitch for the unvoiced part can consis tently enhance 10: 16  X  J.-C. Chen and J.-S. R. Jang (3) The use of segment extension does not always enhance the p erformance. (4) The improvement from  X  X RUES W/O LR X  to  X  X RUES X  (10.67%= 8 5.07% X  modeling by 14.42% (= 85 . 07% X 70.65%) in recognition rate, corresponding to an relative error rate reduction of 49.13%. The success of TR UES can be at-tributed to the following factors: (1) UPDUDP provides robust pitch contours for voiced parts, and useful (2) The inclusion of unvoiced parts and segment extension pr ovides essential (3) Segment extension does provide extra robustness when de aling with inac-is already higher than 70.65% of the supratone modeling with UPDUDP in Figure 8. One possible reason is that TRUES uses the whole pit ch contour for recognition while supratone modeling uses only a reduce d representation of the pitch contour which does not contain the information o f subtle or minor fluctuations. As a result, in supratone modeling, the advant age of using con-text information cannot cover the loss of using the reduced r epresentation for tone recognition.
 TRUES. From the table, it is obvious that t3 is highly confusa ble with t2 and t4, which is consistent with other approaches [Chen and Wang 1995; Lin and Lee 2003] to tone recognition for Mandarin Chinese, since th e pitch pattern of t3 is more similar to either t2 or t4. Moreover, the rising end of t3 is usually missing, especially when the speech rate is high, making it h ard to distinguish from t4. On the other hand, the extension sometimes creates a lengthened ris-ing end of t3, making it hard to distinguish from t2. As a resul t, more work should be spent on analyzing the behavior of t3 since it is the major factor for downgrading the overall performance. (Though the recognit ion rate of t5 is lower, but its data count is much smaller than the other tones .) first syllable toward the leading silence or the last syllabl e toward the trail-ing silence. This is due to the fact that the  X  X akeup X  pitch co ntours of lead-ing/trailing silence are only the same-value extension of t he corresponding voiced part, which do not carry too much transition informat ion. In fact, if 10: 18  X  J.-C. Chen and J.-S. R. Jang we treat the first or last syllables with the same extension ru le for middle syl-lables, the overall recognition rate is only 83.83%, with 35 .84% of the misclas-sifications coming from either the first or the last syllables . Furthermore, since the make-up pitch contour of the unvoiced part of a beginning syllable carries little context information in continuous tone recognition , it might be helpful to skip this part. Due to the time constraint, we shall leave thi s alternative as one of the possible directions for future work.
 were used in TRUES for the aforementioned experiments. 4.3 Parameters Tuning Figure 10 demonstrates the relationship of tone recognitio n rates with respect to penalty values used in the proposed UPDUDP pitch tracking method, where the HMM configuration is set to 8-8-8-8. From the figure, we can observe that when the penalty  X  increases, the curve of t1 also increases all the way. This is reasonable since a large value of  X  will level off the pitch contour, making it more like a t1 pitch pattern. A similar trend occurs with the c urves of t2 and t4, with smaller increasing rates. For t3, the recognition rate goes up initially, and then goes down as the penalty is equal to or greater than 15,00 0. This indicates that when the penalty is too large, the level-off effect is to o strong such that pitch contours of t3 start to deviate from the correct pitch c ontours. The overall performance achieves its peak at  X  = 30,000, which is used in TRUES for all our experiments in this study.
 frame-based minimum-picking of AMDF. The corresponding pi tch contour is usually rugged and uneven, especially at unvoiced/silent r egions. As a result, it should be obvious that the tone recognition performance usi ng UPDUDP with a zero smoothing penalty should be poor when compared with th at of RAPT. Therefore, no experiment is conducted on UPDUDP with a zero p enalty. Figure 11 where the total number of mixtures is kept at 128. We can observe that the overall optimum performance occurs at 32-32-32-32 . However, the performance of each tone model varies. Tones 1, 2, 3, and 4 hav e their peak performance at configurations 128, 64-64, 26-26-26-25-25, and 32-32-32-32, re-spectively. In other words, different tones may require dif ferent configurations to achieve their optimum performance. More sophisticated a lgorithms for such context-dependent configuration generation is one of our di rections for imme-diate future work.
 bers of mixtures for 4-state tri-tone HMMs. The extension ra tio is defined as the length of the extended syllable segment divided by its or iginal length. The extension ratio is fixed for all syllables in this study. More over, the segment is always extended no matter if it will cover preceding/succee ding voiced vowel parts or not.
 10: 20  X  J.-C. Chen and J.-S. R. Jang 1.2 can achieve overall better performance when the number o f mixtures is varied. As a result, TRUES adopts segment extension of ratio 1.2 throughout this study.
 have conducted another experiment to compare the performan ce of extended and voiced-only segments with respect to inaccurate bounda ries obtained by shifting the boundaries via forced alignment. For simplici ty, these two meth-ods employ extended segments (with extension ratio of 1.2) a nd voiced-only segments in the framework of TRUES without LR. Figure 13 show s the perfor-mance of these two methods, where the boundary shift is from  X  40ms to 40ms. The result demonstrates that, when the shift amount is large , the degradation using extended segments is obviously less than that of voice d-only segments. This verifies our conjecture that extended segments are more robust in dealing with inaccurate syllable boundaries. 5. CONCLUSIONS AND FUTURE WORK In this article, we have proposed TRUES for continuous tone r ecognition for Mandarin Chinese. We have also presented UPDUDP, which is a p itch deter-mination algorithm that can identify unbroken pitch contou rs to be used with TRUES. With the use of tri-tone HMM modeling on extended segm ents and DP-based LR, the proposed TRUES outperforms several recent approaches to tone recognition, as was described in Section 4. We have also shown the char-acteristics through various experiments in this study.
 can be used for tone recognition of other tonal languages, su ch as Taiwanese, Cantonese, Tibetan, Punjabi, and so on. Other immediate dir ections for future work are listed next. (1) Besides tone recognition, the concept of TRUES can be app lied to the re-(2) For each tri-tone HMM model, we would like to investigate an efficient way (3) On analyzing the error cases, we found that males have a mu ch higher er-10: 22  X  J.-C. Chen and J.-S. R. Jang
