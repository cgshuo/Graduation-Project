 Given two homogeneous rating matrices with some overlapped users/items whose mappings are unknown, this paper aims at an-swering two questions. First, can we identify the unknown map-ping between the users and/or items? Second, can we further utilize the identified mappings to improve the quality of recommendation in either domain? Our solution integrates a latent space match-ing procedure and a refining process based on the optimization of prediction to identify the matching. Then, we further design a transfer-based method to improve the recommendation perfor-mance. Using both synthetic and real data, we have done extensive experiments given different real life scenarios to verify the effec-tiveness of our models. The code and other materials are available at http://www.csie.ntu.edu.tw/~r00922051/matching/ I.2.6 [ ARTIFICIAL INTELLIGENCE ]: Learning X  Parameter learning Collaborative Filtering; Transfer Learning; Matrix Factorization
Nowadays, recommendation systems are widely deployed for not only e-business services but also brick and mortar stores. In general there are two possible strategies in designing a recommen-dation system: content-based [10] and collaborative filtering (CF) based [5] approaches. The former relies on the content and profile information of users and items to perform recommendation, while the latter relies mainly on the ratings of items given by users to identify similar users and items for recommendation. This paper mainly focuses on the analysis of CF-based models and assumes only rating data are available.

Collecting sufficient amount of rating data has been recognized as a critical factor in designing a successful CF-based recommen-dation system. As CF-based recommendation systems become in-creasingly popular, it is not hard to imagine more and more rating data would become available to models for different businesses. One might wonder what kind of useful knowledge one can extract from several different rating sets of completely independent ser-vices.

For example, a newly established online song requesting service, which possesses some internal ratings about its users X  preferences to songs, can take advantage of the publicly available rating data from other similar services (e.g. Yahoo! music service). The rea-son is that we know there exists some overlapping between users and/or items in the two services, only the exact mappings are un-known. We call the newly established service  X  X he target domain X  and the other service  X  X he source domain X . One would then ap-preciate a model that, based only on the two sets of rating data, is able to identify not only the user mappings but the item map-pings between the two domains. Such connection can then be used to enrich the understanding of the users and items in the target do-main. One would be even happier if the model can exploit the likely  X  X arginally correct X  correspondence it found previously to boost the performance of a CF system built in the target domain. In our experiments, we provide 8 effective scenarios (e.g. whether user sets are completely overlapped, partially overlapped, or completely independent; or whether the ratings are disjoint across matrices or not) to verify the usefulness of such model.

Formally, we assume there are two homogeneous rating matri-ces: a target matrix R 1 and an auxiliary data matrix R 2 and columns represent users and items, and each entry in the ma-trix represents the rating of an item given by a user. In these ma-trices, the two user sets and two item sets are overlapped to some extent, but we do not know how to associate the users and the items between the matrices. We want to answer the following two ques-tions: 1. Given only R 1 and R 2 , is it possible to find out the mapping 2. Given the noisy mapping we obtained, is it possible to trans-
The underlying assumption we made is that the rating behavior of both domains are similar; we call such rating data the  X  X omoge-neous X . The problems sound hard because the user and item corre-spondences are both unknown, which prevents us from exploiting any supervised learning algorithm. If one side is already matched (e.g. if the items are matched), then one can use approaches like nearest neighbor to match the other side (e.g. users), as discussed in [13]. Unfortunately, such method cannot be applied when the columns and rows of the rating matrices both have gone through an unknown permutation process. Fortunately, the matching problem is not impossible to solve. Recall the underlying assumption of collaborative filtering: simi-lar users shall rate items similarly and similar items shall receive similar ratings from users. Viewing collaborative filtering as a ma-trix completion process, the above assumption actually suggests the rows/columns of the rating matrix are indeed dependent. That is, the matrix is of low-rank. The low-rank assumption has been vali-dated by experiments. Owing to this assumption, researchers have proposed a family of strategies based on matrix factorization (MF), which has proven to be one of the most successful solutions for collaborative filtering [6, 1].

Extending from the concept of CF, our key hypothesis is that if two users rated some items similarly in one domain, they shall rate similar items similarly in the other domain. The low-rank assump-tion on both matrices plays a key role to the solution we propose in this paper. Our idea is to find a way to transform the incom-plete matrices R 1 and R 2 into low-rank approximation, and match users/items in the latent space. One plausible solution is to perform matrix factorization on R 1 and R 2 then try to match users/items in the latent space. Figure 1 shows an illustrative example.
At the first glance, it seems non-trivial to find the relationship between R 1 and R 2 . Assuming that both matrices are of low-rank, they can be factorized into a user-latent matrix and item-latent matrix. Then it might be possible to match users based on the two user-latent matrices, as shown in the first decomposition in Figure 1. Unfortunately, matrix factorization is not unique, so R R 2 can be factorized using different latent space, prohibiting any further matching of users or items (see the second decomposition in Figure 1).

Another plausible idea is to exploit the idea of singular value de-composition (SVD) on the rating matrices R 1 and R 2 , and then match users and items based on the singular vectors. This paper proposes a trick that allows us to obtain singular value decomposi-tion on an incomplete matrix. However, the solution of SVD is still not unique, i.e. sign difference may exist, and the decomposition is unstable when noise occurs. We then propose a greedy-based algo-rithm that solves the sign problem and searches the nearest neigh-bors in the latent space. Then, we propose to adjust the match-ing results through a process that minimizes the prediction error on the existing ratings of R 1 using R 2 , making the matching results more resistant to noise. Experiments show that our method can ac-curately find the correspondence given clean low-rank data, while achieving much higher matching accuracy than other competitors if given real, noisy data. Finally, based on the discovered matching outcome, we propose a transfer learning approach that transfers the rating information from R 2 to R 1 to boost the performance of the recommendation system in R 1 . We conduct extensive experiments including a variety of scenarios to verify the effectiveness of our model.
User identification has been studied under different settings by different research communities. In natural language processing, author identification [2] is formulated as a text categorization prob-lem, in which a supervised learning model is built based on features extracted from the authors X  manuscripts. In social network analy-sis, Narayanan and Shmatikov [14] propose to link two social net-works and de-anonymize users by first identifying some seed nodes and then iteratively propagating the mapping.

Some approaches have recently been proposed to link users from different social media sites. Zafarani and Liu propose MOBIUS for cross-media user identification [20]. Based on features extracted from user names, they learn a classification model to determine whether an account belongs to a certain person. Liu et al. [9] as-sociates users with an unsupervised approach by calculating the rareness of the names. When a rare name (such as pennystar881) occurs on different websites, it is very likely that the two accounts are owned by the same person. Yuan et al. [19] propose another unsupervised approach to link users. They find that some users may explicitly display their other accounts on the profile page or disclose the account links when they share a post across websites. They design an algorithm to automatically capture all such infor-mation to link users.

Most of the studies differ from us because our model relies on rating data, and we do not assume there are any labeled pairs avail-able for training. The closest work to us is [13], in which Narayanan and Shmatikov study how to de-anonymize users from rating data. In their setup the item mapping is known, which makes the task easier than the one we are trying to solve. Moreover, our model embeds itself a second objective: to improve the rating prediction in the target matrix.
Many models have been proposed for transfer learning in collab-orative filtering [17, 11, 18, 21, 15, 16, 4, 3]. The common goal is to transfer information across several data matrices. Most of the models assume that there is a one-to-one correspondence between users or between items across domains.

For example, in collective matrix factorization [17], there is a rat-ing matrix (users by movies) and a label matrix (genres by movies) for transferring. The shared movie side then becomes the bridge for transferring, as they assume the latent factors of the movies are similar. Similar ideas can be applied on matrices of different time frames [18], on binary versus numerical matrices [15, 16], or on a rating matrix and a social relation matrix [11, 4].

To sum up, the above models transfer information across two or more heterogeneous data matrices based on restrictions that the latent factors of the shared user side or item side are similar. Our setting is quite the opposite: the data matrices are homogeneous while both the user and item correspondence are unknown.
We only find two models that transfer information between rat-ing matrices without exploiting user and item correspondences and will discuss them in details here.

Codebook transfer (CBT) [7] and rating-matrix generative model (RMGM) [8] transfer information between two rating matrices with-out assuming user correspondence or item correspondence. The basic assumption of the two models are of the form: where B is a shared K by K matrix. The major difference between these models and our model is that these models do not assume user correspondence or item correspondence between R 1 and R Instead, they assume that R 1 and R 2 have homogeneous rating pattern; they enforce constraints on U  X  X  and V  X  X  while assuming the homogeneous pattern matrix, B , is shared across R 1 and R However, the constraints make the optimization process compli-cated and limit the expressive power of the model.

In CBT, U and V are constrained to be 0 -1 matrices, and there can only be one entry with value 1 in each row. After adding the constraints, R  X  UBV T can be viewed as a co-clustering process that simultaneously divide users and items into groups. Assume a user is in i -th user group and an item is in j -th item group, then the rating of such user and item ( r ij in R ) is predicted to be the value of corresponding group rating value in B ( b ij ). Thus, the formulation of CBT is actually associating groups of users (items) in R 1 with groups of users (items) in R 2 .

However, the hard clustering constraint greatly reduces the ex-pressive power of CBT; many lower-rank matrices cannot be fac-tored under such constraint. Furthermore, to solve CBT, the opti-mization process requires the auxiliary matrix R 2 to be fully ob-served, or it has to fill in missing entries with data mean before factorization. Since there are much more missing entries than ob-served ones in a sparse rating matrix, filling in the missing values manually can seriously bias the model.

RMGM relaxes the constraints in CBT from hard clustering to soft clustering by using a probabilistic graphical model, and it does not require R 2 to be fully observed. To be more specific, the joint probability defined in RMGM is and the rating prediction is where m ( i ) and n ( i ) are user and item in i -th domain, and C C n are the cluster for the user and item, respectively. If we rewrite the rating prediction as the term P r rP ( r | C m , C n ) is similar to B in CBT. P ( C and P ( C n | n ( i ) ) are similar to U i and V i , respectively. When the rating prediction is perfect, R = U i BV T i .
 Even though RMGM relaxes the hard clustering constraints in CBT, constraints still exist: the elements in each row of U and V must be nonnegative and sum up to 1 . This again limits the ex-pressive power of the model and complicate the optimization task. Besides, the objective function of RMGM is the self-defined like-lihood. It deviates from common evaluation measures such as root mean square error. Consider the following example: When we evaluate this model by root mean square error, the error is 0 because R is exactly the product of the three matrices. How-ever, the data likelihood defined by RMGM is also 0 because the term P ( r = 2 | C m , C n ) is always 0 . Thus, RMGM will try to find other parameters to fit the data, but they are not necessarily mini-mizing the prediction error. Nevertheless, RMGM is considered as the state-of-the-art and will be our main competitor in the evalua-tion.

In conclusion, past models do not try to solve correspondence and impose extra constraints in the factorization equation. Our model directly solves the correspondence problem and leverages such information to transfer across matrices.
Given two partially observed rating matrices R 1 and R 2 , we made the following assumptions: 1. There are some common users and common items across R 1 2. The two matrices represent the same homogeneous domain.
To find out the correspondence between R 1 and R 2 , intuitively we want to solve where G user and G item are 0-1 matrices that represent user and item correspondences. The symbol  X  implies the corresponding entries in R 1 and G user R 2 G T item shall be as similar as possible. The appar-ent challenge is that the rating matrices are partially observed, and very few entries are observed in both R 1 and R 2 . Thus, we want to modify the above equation to deal with this problem.
We want to exploit the low-rank property of the rating matrices to solve the problem. To do so, we replace the original rating matrix by its fully filled low-rank approximation (i.e. no missing values). We propose the following two objective functions: We want to find G  X  X  that satisfies the above criteria. The symbol  X  R stands for the low-rank approximation of R . In Equation 1, we hope to map the low-rank approximation R 2 to the low-rank approximation of R 1 . In Equation 2, we hope the observed entries
The extra regularization terms associated with  X  restrict the la-tent factors of matched users/items to be the same. Therefore, if the matching is correct, since the corresponding users or items are forced to align in their latent space, the extra information from the other domain can then be exploited.

Acknowledging the fact that our matching is not perfect, special care needs to be taken to prevent incorrect matching from hamper-ing the prediction performance. Thus, we add an arctan function to alleviate the influence of outliers. When the matching results become unreliable, our model degenerates into regular matrix fac-torization model as  X  would become 0 after parameter selection. The objective is still differentiable and can be solved via standard approaches such as gradient descent. Y ahoo! Music (20000, 10000) integer, 0-100 5.4%
Our goal is to link the users and items in two homogeneous rat-ing matrices, R 1 and R 2 , and then use the corresponding infor-mation in R 2 to improve rating prediction in R 1 . However, we cannot find two independent datasets that provide the ground truth for user/item mappings to evaluate our model. Fortunately, we can use a real dataset and split it into two rating sets as R Another advantage of using such strategy for evaluation is that we can then test different scenarios (i.e. different splitting condition) to evaluate the usefulness of our model under a variety of different assumptions. The user and item ids of R 2 are randomly permuted, while the permutation (unknown to our model) becomes the ground truth of the correspondence.
 We conduct experiments on a synthetic dataset and a real dataset, Yahoo! music dataset. The synthetic dataset is a noise-free low-rank matrix for verifying the soundness of different models. It is a rank-50 matrix, generated by the following MATLAB command We sample 5% of it as R and linearly scale the minimum and max-imum values to  X  1 and 1 . Yahoo! music dataset has been used as the benchmark data in KDD Cup 2011 [1]. For both datasets, we take out a subset R and split it into R 1 and R 2 . The statistics of R are listed in Table 1.

With the capability to control the mapping condition, now we are ready to test the effectiveness of our model under different assump-tions. Below lists the conditions (see Figure 7 for details): 1. Disjoint Split. We assume a user will not rate the same item 2. Overlap Split. We assume ratings given in R 1 may or may 3. Contained Split. We assume R 2 is more frequently visited For the three splits, we vary the overlap ratio for ratings, but we assume that every user and item in R 1 can be found in R 2 versa. This assumption may not be true in some cases so we created two additional scenarios. 4. Subset Split. We assume the users and items in R 1 are sub-5. Partial Split. We assume the user/item set in R 1 are par-For subset split and partial split, we simply assume that their over-lap ratio for ratings is similar to that of the overlap split.
Because R 1 is the target domain in which we want to evaluate whether rating prediction accuracy can be boosted by knowledge transfer, we divide its data into training, validation and testing sets. R 2 , on the other hand, belongs to the source domain, thus we only need to divide it into training and validation set. Validation sets are used for parameter tuning in factorization models. Each of the two validation sets takes up 2.5% of the original data R , while the testing set takes up 5%. Each of the validation and testing sets does not overlap with any other set so as to ensure the sanity of our experiment. The training sets of R 1 and R 2 are sampled from R according to the scenarios described above (Figure 7).
The first experiment is to evaluate the matching quality. To our knowledge, there has not yet been solutions proposed to utilize only incomplete rating information to perform user/item alignment, so we compare our method with other baseline models.

We define the following three baseline models. Here we only de-scribe the user-matching procedure as the item-matching procedure is identical. 1. Matching based on Regular Matrix Factorization. We 2. Matching based on User Mean/Item Mean. Find nearest 3. Matching based on Rating Lists. We first sort the ratings As mentioned in the introduction section, latent factor matching from simple matrix factorization does not work. The solution of matrix factorization is not unique since Even when R 1 = R 2 , we can still obtain very different P P . In fact, experiment results show that the first two baselines perform no better than random guess, so we omit them from the result table to save space. We found the third baseline that works much better than the first two, and therefore its results are included in our table.

Second, we conduct an experiment to evaluate the quality boost of rating prediction in R 1 after transferring information from R There has not yet been much work on information transfer without knowing the underlying correspondence. Hence, for rating predic-tion we use rating-matrix generative model (RMGM) as the state-of-the-art for comparison.
For our factorization models, we use gradient descent and back-tracking line search to solve the objective function. The dimension K is fixed to 50 , and the parameters  X  and  X  are automatically selected by observing the error rate of the validation set. After se-lection,  X  is determined to be 0 for noise-free low-rank dataset and 5 for Yahoo! music dataset, and  X  ranges from 0 to 400 . Besides, we employ the data scaling and early stopping procedure. We scale the ratings in the training set to zero mean and unit variance, and scale the values back in the prediction phase; the training process stops when validation error starts to increase.

We implement two versions of rating-matrix generative model (RMGM) [8] for comparison. They can be solved by standard expectation-maximization algorithm. 2 The original RMGM uses categorical distribution for P ( r | C m , C n ) . However, categorical distribution does not reflect the ordinal relation among the rating values. Therefore, we implement another version of RMGM that uses Gaussian distribution. For both models, the original and Gaus-sian RMGMs, we set the latent dimension K to 50 (we find that a larger K leads to similar performance), and we conduct the same early stopping procedure. For Gaussian RMGM, we find that the variance of Gaussian is better set to a constant parameter that can be tuned by observing the error rate of the validation set. We have also conducted the data scaling process for Gaussian RMGM.
In the matching process, the matching can output either the most likely candidate or a ranked list of candidates for each user/item in R 1 . Therefore, we can use accuracy as well as mean average precision (MAP) as the evaluation criteria. However, in the partial split, some users and items in R 1 do not appear in R 2 . Thus, when
An example code is provided by the author of RMGM: https: //sites.google.com/site/libin82cn/ Contained Split evaluating the matching result of such scenario, we remove these users and items from consideration.

The matching results are shown in Table 2 and Table 3. The results on the noise-free low-rank dataset demonstrate the sound-ness of our approach. When there is no noise in the rating ma-trix, our latent space matching can precisely match all users and items by comparing the singular vectors regardless of the splits. On the other hand, the performance of baseline model (rating list) improves when more ratings are shared between domains , but the results are still far from perfect.

On Yahoo! music dataset, we see a similar trend: when more rat-ings are shared, the results are generally better. We observe that the contained split is easier to match than overlap split, while both are easier to match than disjoint split. The matching accuracy for the subset split and partial split are slightly worse (though still com-parable) than that of the overlap split. It is because although the Contained Split ratio of shared ratings is similar among these splits, some users and items can never be aligned in the subset split and partial split.
For all cases, the latent space matching we proposed already en-joys a significant boost comparing to the best baseline model, while the matching refinement through optimization further leads to great improvement over latent space matching. It is reasonable because there are some drawbacks of latent space matching, which have been discussed in Section 3.1.4. Disjoint Split 27.47 26.59 24.24 23.34  X  Overlap Split 27.54 26.48 24.23 23.49  X  Contained Split 27.58 26.53 24.29 23.92  X 
Next we want to evaluate whether our matching can indeed be used to improve the quality of a recommendation system in the tar-get domain. We resort to the experiment on rating prediction and use root mean square error (RMSE) as the evaluation criterion. The hypothesis to be verified with this experiment is that despite the ex-istence of user/item mis-matching, our model can still improve the prediction accuracy of the target domain after information transfer. Here we focus on the Yahoo! music dataset because in the noise-free low-rank dataset the RMSE before transferring is already close to 0 .

The results are shown in Table 4. Gaussian RMGM outperforms the original RMGM, likely due to the fact that the actual rating itself does not follow categorical distribution. However, RMGM does not perform well in this experiment. One of the reason is that the likelihood function optimized by RMGM does not directly reflect the objective for evaluation (discussed in Section 2.3), while in our model minimizing the prediction error (i.e. RMSE in this experiment) is one of the direct objectives.

For all cases, our proposed approach leads to significant im-provement over the original matrix factorization (MF) model, which is widely considered as one of the most effective single domain model. Interestingly, although the results in Table 3 show that the matching accuracy can be ranked as disjoint &lt; overlap &lt; contained split, results in Table 4 show that in terms of rating prediction, the improvement for the disjoint split is the best, while the improve-ment for the contained split is the worst. We believe this is because when there is less overlap of ratings, the amount of information R provides to R 1 increases, and our solution is capable of leveraging the extra information to provide better prediction outcome.
Finally, we want to test what happens when the original assump-tion of our model is violated: that is, what happens if the user sets or the item sets do not overlap at all. It implies the matching is al-ways wrong and our matching algorithm can at best identify some  X  X imilar X  users/items. Thus, we create three other splits and they are illustrated in Figure 8. 1. User Disjoint Split. The user sets of R 1 and R 2 are disjoint 2. Item Disjoint Split. The user sets are identical (without 3. User-Item Disjoint Split. The user sets and item sets are
The results are shown in Table 5 and Table 6. For the disjoint dimension, our algorithm can only identify the  X  X imilar X  entities. (thus we do not report their matching accuracy.) Despite this, it is very promising to see that the rating prediction can still be im-pro ved, most likely due to the fact that on the other overlapped dimension the correspondence can be identified. If both sides are disjoint, our model cannot yield any boost, as  X  becomes 0 after parameter tuning and our model degenerates into regular matrix factorization model.

In short, when original assumption is violated, our model may still lead to some improvement and it is at least no worse than reg-ular matrix factorization model. Such discovery delivers an en-couraging and important message for practical usage because the aforementioned scenarios are all very common in the real world. User Disjoint Split Item Disjoint Split
We present a novel yet intuitive two-step algorithm for a very challenging and seldom tackled task of identifying user/item cor-respondences between two homogeneous rating matrices. After the correspondences are identified, we introduce a transfer learning approach to boost the rating prediction accuracy in the target do-main. Our two-stage matching algorithm not only aims at matching users/items in the latent space, but also refines the matching based on the objective to predict the observed values in the target domain. The refinement not only boosts the quality of matching but also fa-cilitates further transferring process to enhance rating prediction. We test our model on 8 different scenarios, each corresponds to a real-world scenario of transferring. The results are very promising as our model can identify the matching between user/item sets from different domains to some extent. More importantly, except one ex-tremely difficult scenario where the users/items are both completely disjoint, our model significantly boosts the rating prediction perfor-mance.
This work was sponsored by AOARD grant number No. FA2386-13-1-4045, Ministry of Science and Technology, National Taiwan University and Intel Corporation under grants NSC102-2911-I-002-001 and NTU103R7501, and grant 102-2923-E-002-007-MY2, 102-2221-E-002-170, 101-2628-E-002-028-MY2.
 [1] G. Dror, N. Koenigstein, Y. Koren, and M. Weimer. The [2] M. Gamon. Linguistic correlates of style: authorship classi-[3] L. Hu, J. Cao, G. Xu, L. Cao, Z. Gu, and C. Zhu. Personal-[4] M. Jamali and L. Lakshmanan. Heteromf: recommendation in [5] Y. Koren and R. Bell. Advances in collaborative filtering. In [6] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization tech-[7] B. Li, Q. Yang, and X. Xue. Can movies and books collabo-[8] B. Li, Q. Yang, and X. Xue. Transfer learning for collabo-[9] J. Liu, F. Zhang, X. Song, Y.-I. Song, C.-Y. Lin, and H.-W. [10] P. Lops, M. de Gemmis, and G. Semeraro. Content-based [11] H. Ma, H. Yang, M. R. Lyu, and I. King. Sorec: social recom-[12] A. Mnih and R. Salakhutdinov. Probabilistic matrix factoriza-[13] A. Narayanan and V. Shmatikov. Robust de-anonymization of [14] A. Narayanan and V. Shmatikov. De-anonymizing social net-[15] W. Pan, E. W. Xiang, N. N. Liu, and Q. Yang. Transfer learn-[16] W. Pan and Q. Yang. Transfer learning in heterogeneous col-[17] A. P. Singh and G. J. Gordon. Relational learning via collec-[18] L. Xiong, X. Chen, T.-K. Huang, J. G. Schneider, and J. G. [19] N. J. Yuan, F. Zhang, D. Lian, K. Zheng, S. Yu, and X. Xie. [20] R. Zafarani and H. Liu. Connecting users across social media [21] Y. Zhang, B. Cao, and D.-Y. Yeung. Multi-domain collabora-
