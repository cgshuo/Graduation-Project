 Analy zing gene expression data from microarray devices has many important applications in medicine and biology , but pres ents significant challenges to data mining. Microarray data typically has m any attributes (genes ) and few exam ples (sam ples ), making the process of correctly analy zing such data difficult to formulate and prone to common mi stakes. For this reason it is unusually important to capture and record good practices for this form of data mining. This pape r pres ents a proces s for analy zing microarray data, including pre-pr ocessing, gene selection, randomization testing, classification and clustering; this process is captured with "Clem entine Appli cation Templates". The paper describes the process in detail and includes three case studies, showing how the process is app lied to 2-clas s clas sification, multi-class classification and cl ustering analy ses for publicly available m icroarray datas ets. microarray s, gene expressi on, data mining process, application tem plate, Clem entine. Cells in the same organism norma lly have the same genes, but these genes can be expressed differently , i.e. manufacture different messenger R NA or mRNA, which in turn manufacture different proteins, allowing creati on of a huge variety of different with changes in the m RNA levels of many genes. Detection and cure of many diseases can be assisted by understanding gene expression in human and an im al tis sues and cells . genes simultaneously . There ar e several different ty pes of microarray s, including Different ty pes of microarray use different technologies for measuring RNA expression levels ; detailed description of these technologies is bey ond the scope of this paper. Here we will focus currently the most popular commercial array s. However, the methodology for analy sis of data from other array s would be similar, but would use differe nt technology -specific data preparation and cleaning s teps . This ty pe of m icroarray is a silicon chip that can m easure the expression levels of thousa nds of genes simultaneously . This is done by hy bridizing a complex mixture of mRNAs (derived from tissue or cells) to m icroarray s th at display probes for different detected using a fluores cent dy e and a s canner that can detect fluorescence intensities. The scanne rs and associated software perform various forms of image analy sis to m eas ure and report raw gene expression values. This allows for a quantitative readout of gene expression on a gene-b y-gene basis. As of 2002, microarray s such as the Affy metr ix U133 2-chip set, can measure expression of over 30,000 genes, likely a majority of the expressed human genome. Microarray s have many potential applications, including: 
Figu re 2: An examp le raw microarray image for a single sample (image courtesy of Affy metrix). 
Brigh tness of d ots rep res ents th e in ten sity of gen es exp res sed. Th e image on th e left is tran slated by microarray s oftw are in to n umbers lik e th e on es on th e right Microarray data analy sis is fast becom ing an essential tool in biom edical res earch. The main ty pes of data analy sis needed to support the above applications include:  X  Gene Selection  X  in data mini ng terms this is a process of  X  Classification  X  here we cla ssify diseases based on gene  X  Clustering  X  using clustering we can find new biological To get the best results from data mining in any field it is advisable to use a well-defined process and to adopt best practices for the task at hand. In this section we outline the CRISP-DM standard data mining process model, and show how best practices for a given application can be captured in a CRISP-DM framework using a visual programming environment. Now widely adopted as a standard process model for data mining, CRISP-DM [3,16] offers many benefits to data mining practitioners; one of these is to provide a standard high-level structure and terminology for the da ta mining process. If we wish to capture bes t practice for a par ticular data mining application, we must describe the process in detail, and CRISP-DM gives us a framework for doing so. Figure 3 shows the  X  X hases X  of the CRISP-DM process and the main inform ation flows between them; these phases provide a high-level classification of data mining activities, such as  X  data understanding X ,  X  X ata preparation X  and  X  X odeling X , to help us describe data mining. For a given data mining application, best practice will specify a number of steps, for example certain data preparation and modeling s teps , which are com bined to solve the problem at hand. Figure 4 shows a diagram depic ting how a collection of data preparation and modeling steps fit together for the task of genome classification in a m ulti-class problem . (In figure 4, ellipses represent operations applied to th e data and rectangles repres ent raw data or interm ediate data wh ich is created by one step and used in another. Codes starting with P repres ent data preparation steps and codes starting with M represent modeling steps.) A diagram such as that in figure 4 describes a  X  X odule X ; a collection of modules forms an  X  X pplication template X , a complete description of the data mining pr ocess for one application. The steps in the process are realized using a data mining toolkit. The diagram in figure 4 uses only an informal notation, to help an analy st understand how a collection of steps and data files fit together. To capture a data m ining process in sufficient detail that it can be re-us ed, each s tep m ust be realized in an executable form . This can be achieved using a data mining toolkit based on visual programming, such as Clementine [6,12] , depicted in figure 5. In figure 5, Clem entine X  X  m ain drawing area contains a stre am diagram ; an executable visual program which performs one of the modeling steps in figure 4. Each icon in the stream diagram depicts a low level operation in the data mining process; data flows from data sources (circular icons) through data manipulation operations (hexagona l icons) to a C5.0 modeling to various reports and visualizations. to various reports and visualizations. stream diagram of this kind. A collection of s tream diagram s and Clem entine Application Tem plate, or  X  CAT X  [7] . stream diagram of this kind. A collection of s tream diagram s and Clem entine Application Tem plate, or  X  CAT X  [7] . stream s of a CAT according to the role they play in this standard view of the data mining process. Clementine also supports the use of CRISP-DM via its CRISP-DM Project facility ; figure 5 shows the streams from one module of a Microarray analy sis CAT organized using this tool. stream s of a CAT according to the role they play in this standard view of the data mining process. Clementine also supports the use of CRISP-DM via its CRISP-DM Project facility ; figure 5 shows the streams from one module of a Microarray analy sis CAT organized using this tool. capture bes t practice for a s pecific data m ining application. Clem entine s tream s form the detailed realization of a proces s. modules, each of which is depicted inform ally as a data m ining process diagram, and the CRISP-DM framework is used to clas sify the s teps of the proce ss. The remainder of this paper specifies in detail the capture of bes t practice for m icroarray gene expression data analy sis. capture bes t practice for a s pecific data m ining application. Clem entine s tream s form the detailed realization of a proces s. modules, each of which is depicted inform ally as a data m ining process diagram, and the CRISP-DM framework is used to clas sify the s teps of the proce ss. The remainder of this paper specifies in detail the capture of bes t practice for m icroarray gene expression data analy sis. Com paring microarray data analy sis with the m ore wides pread applications of data mining, such as analy tical CRM using challenges, for two reasons. Firs t, the typical number of records especially for hum an data, becaus e of the difficulty of collecting and processing samples. is normally large  X  ty pically many thousands. When building predictive m odels, having so m any colum ns relative to the num ber of rows is likely to lead to  X  false positives X   X  gene com binations what correlate with a target variable purely by decision trees and neural networks, which find complex non-linear com binations of features , and therefore have a large m odel space in which to find s purious m odels . At a very high level, the goal of data preparation in microarray data analy sis is the sam e as fo r all data m ining, that is to producing the best possible models. However the unusual properties of microarray data give a s pecial character to this phas e of the data mining process; in this context data preparation takes place in two s tages . likelihood of getting chance correlations by using random ization. This technique randomly permutes the class column many times, and compares the strength of correlation obtained with a randomized class column with that from the actual class column. most promising genes, for ex ample 100-200, and build models using only these genes. Genes can be ranked by comparing the mean expres sion value for each clas s with that of the res t, and com puting m eas ures like T-values or signal-to-noise (S2N) ratios. Models produced in this way are m ore accurate, and generalize better, than m odels produced using the com plete set of available genes. preparation. This stage contains thos e as pects of data preparation which are independent of any cla ss data; these are thresholding, norm alization and filtering. The thresholding and filtering operations achieve a substantia l amount of feature reduction, typically by about 50%. cleaning; techniques of this kind are broadly applicable, but the details will vary with the m icroarray device used to produce the data. The thresholding and filteri ng details given in this paper therefore have the s tatus of exam ples only , and are specific to the Affy metrix device. otherwise very wide, microarray data, are collectively referred to as  X  X eature reduction X . PROCESS The second stage contains those aspects which make specific use of class data, and are broadly termed  X  X eature selection X . Here we are performing data re duction by narrowing the set of features to thos e relevant to the s pecific s et of clas ses to be analy zed. Thes e operations are therefore s pecialized to the for  X  X ure X  discovery , for example uses of clustering to discover new classes which may be i ndependent of known classes. The proces s of analy zing m icroa rray gene expression data is summarized in figure 7, which show s the major flows of data and the iterative relationship between model building and feature and parameter selection. The process in figure 7 is that used for modeling the classification of genomes, where both gene data and class data are used  X  the modeling is  X  X upervised learning X  . For data-sets which are too sm all to separate training and test data (a com mon situation with Microarray data), cross-validation can be used to evaluate the likely quality of a m odel. W here the m odeling is unsupervised, no separation of training and test data is required, and feature selection based on class is explicitly excluded because the purpose of the exercise is to discover new classes. level of mRNA, which in Affy metrix devices is m easured indirectly by comparing PM (pe rfect match), and MM (mismatch) probes, using 20 probes in the HuFL6800 chip, and 11 probes in the lates t chip. The M AS-4 s oftware com bines the PM and MM values by subtracting MM from PM values, so it is possible for gene expression to be negativ e, which means that M M probes have stronger signals than PM. Since we do not know what is matching the MM probes, these signals are not useful, and for this reason, data processed with MAS-4 software is heuristically set to a mi nimum of 20. Affy me trix MA S-5 software does not generate negative expression values a nd does its own thresholding. sample several times [17] , have shown that the m eas urem ents were reproducible above valu es of 100, and much less reproducible below 100. For data believed to be more noisy , a lower threshold of 100 would be appropriate. intensities above this level tend to have a non-linear correlation with the actual expression levels. While clas sification algorithm s can us e the actual expres sion levels, data norm alization is required for clustering. The best results are obtained by normalizing the data to mean zero, standard deviation 1 across genes. Since many genes are not expr essed at all or do not vary sufficiently to be useful, a filtering operation is usually applied before adding the class inform ation. Ty pical filtering excludes genes with low variation acr oss sa mple s, for e xample where M axValue(G) and M inValue(G) are the m axim um and minimum values of gene G a cross a ll sa mple s. After data preparation and cleaning, we apply feature selection by adding class information and looking for genes that can distinguish between classes. Most learning algorithms look for non-linear combinations of features and can easily find many spurious combinations given small number of records and large num ber of genes. To deal with this problem, we next reduce numbe r of genes by a linear method, such as T-values for m ean difference between clas ses: T-test for Mean Difference = or a simplified version, called Signal to Noise ratio (S2N) = where N K is the num ber of exam ples in clas s K , Avg sample mean gene expression for class K , and s standard deviation of ge ne expression for class K . com pute thes e form ulas for each clas s vs . all the other clas ses. [11] or Pomeroy [10] which analy ze different tumor ty pes), we frequently see that s ome of the clas ses are much more clearly characterized than others . Thus , if we s elect genes only with the highest values of the statis tic, we risk having only genes representative of only one class and not get enough genes from all classe s. select an equal num ber of genes from each clas s. This is im plem ented by com puting for each gene a s eparate m eas ure rank for each clas s, and then ordering genes by the rank. Becaus e of the large num ber of co lum ns com pared to a relatively sm all num ber of sam ples, we are likely to get false positives, i.e. genes that appear to be correlated with the target clas ses but are not. value for the T-value. If we can as sume equal variances , num ber of degrees of freedom is computed as ( N 1 + N 2 -2). However, the top 200 genes with the highest T-value for the ALL/AML experiment, we find that about 80% of the genes have significantly different (at p=0.01) variance. We should then use number of degrees of freedom is es tim ated as [9] Us ing this form ula, we can com pute the s ignificance value for the top genes. For example, am ong the top 200 genes for ALL/AML case, the p-value for the most significant gene is around 10 while the p-value for the least significant gene is around 0.001. p=0.001 level. that is randomly permuting the class column a number of times. We then com pute the T-value value for the m ean difference for each random ization and for each gene, and com pute for each gene the ma ximum a nd mi nimum T-value obtained. 
Figure 8: Randomiz ation T values for ALL/AML data, for gene J04027 after 500 randomiz ations. Note that the low est each class, only about 11 genes have m axim um (or m inim um) T-microarray data. From biological considerations, we know that a multiple genes working in parallel, rather than strong inputs from one gene. For this reason, seque ntial clas sifiers like Decis ion Trees, though they have been applied to microarray data, do not work well (see e.g. Dubitzky [4] ), becaus e they try to find the smallest gene sets. This also ma kes them less robust against false positives. multiple tests is to use Bonferroni adjustm ent. i.e. to divide the desired significance by the number of tests. To get genes with expression difference significant at 0.01 value, if we test 1000 genes, we would establish a threshold of 0.01/1000 = 0.00001 for each gene. clas sifier, s ince we ris k elim inating valuable genes . clas sification include: in the following sections, to select the bes t subs et.  X  K-nearest neighbors (Pomeroy [10] )  X  robust for small simplicity . However, unlike the T-test, there is no obvious randomization approach can be us ed to estimate the significance level.  X  Support Vector Machines [1] -these seem to produce the Neural Nets are m ore nois e tole rant and are designed to handle many parallel inputs, thus well su ited to genetic classification. Although they usually cannot handle thousands of genes in the full dataset, they seem to work well for reduced number of genes. We have experimented with a number of approaches available within Clementine (e.g. C5.0 and C&amp;RT) , but obtained the best results with neural nets. significance level and fals e detecti on rate is provided by Tusher [14] . Reducing the number of genes to those with sufficiently low significance level is only the beginning. Usually , we still get several hundred genes that satisfy those conditions which is still too many for most classificati on algorithms. We can further determine a good subset of genes by using a wrapper approach. algorithm. Here we propose to use it to determine a  X  X ood X  subset of genes. the num ber of samples is small, and we als o can s elect m ore genes if the s ignificance tes ting indicates that there are m any genes with significant differences. the top genes for each class, then we have the second best genes for each class, etc. If there are duplicate genes, we leave them in. V-fold cross-validation. Five fold is sufficient for an initial run, while leave-one-out cross-validation is considered to give most accurate results. If we are using a random ized algorithm , such as Clementine neural nets, the results may differ from run to run, so we suggest repeating each cross-validation run at least 10 tim es. As an exam ple of a 2-clas s clas sification, we us ed Leukem ia data with 2 clas ses: ALL/AM L (Golub, [5] ), available from the M IT Whitehead Institute. This dataset has a training set with 38 Affy metrix expressions for about 7,000 genes. 
Figure 10: ALL and AML Leukemia  X  Visually similar, but First we applied a preproces sing stream to threshold gene expression values to at least 20 and at most 16,000. (Maximum value  X  minimum value) &lt; 500 across samples and (Maxim um value / m inim um value) &lt; 5 across sam ples. This of genes. We next computed the mean expression values of genes for each clas s, and com puted T-value for the difference, as suming unequal variances. W e then select the top 100 genes from each class with the highest T-value. We used randomization stream to verify that the s elected genes had T-values significantly higher than those obtained by randomization. models using potential subsets with 1, 2, 3, 4, 5, 10, etc genes from each clas s, and for each gene s ubs et evaluated the error rate by using 10-fold cross-validation. beginning (when number of genes is too sm all), with a gradual decreas e as the num ber of genes approaches the optim um plateau, and then a gradual increase as the number of genes becomes too select is the one in the m iddle of th e plateau with the lowes t error. desired behavior, with 10 genes being at the center of the optim um plateau. W e selected th is gene subset to build a new neural net model on the full training data (using 70 percent as the training data) and applied it to th e test data (34 sam ples), which development. of 34 (97% accuracy ), which com pares well with many previously reported results on the same data. 
Figu re 11: Cros s-valid ation errors for d ifferen t gen e subsets , for ALL/AML d ata, each cros s-valid ation rep eated 10 times . 
Cen tral p oint is th e average error for each cros s-valid ation , Note: the single m isclassificati on was on sample 66 which has been consistently misclassified by other methods and is believed to be incorrectly labeled by th e pathologist [13] , which suggests that the classifier was actually 100% correct. These results are as good as any reported on this datase t, which was the subject of CAMDA-2000 conference and com petition [2] . The goals of cluster analy sis for microarray data include finding natural classes in the data, id entify ing new classes and gene correlations, refine existing ta xonomies and supporting biological analy sis and discovery . As an exam ple of m ulti-class data, we used Brain dataset A, from [10] , available from the MIT W hitehead institute [15] . This dataset has 42 examples, about 7,000 genes, and 5 classes: Medulloblastoma, Malignant g lioblastoma, AT/RT (rhabdoid), Norm al cerebellum , and P NET. clustering. We applied two cl ustering algorithms included in Clementine (Kohonen SOM and TwoSte p) to the Leukemia data. We subdivided ALL into 2 classes: ALL-T and ALL-B. This gave us 3 natural classes: A LL-T, ALL-B, and AML, and we were interes ted in s eeing whethe r the clustering could rediscover the natural classes. Leukemia dataset. by analy zing each clas s agains t all others and com puting a signal to noise statistic (a sim plified ve rsion of the T-test), which is simply the difference between average for this clas s and the res t, divided by sum of standard devia tions for this class and the rest. for clas sification; we als o norm alized each sample separately to Mean = 0, S td. Deviation = 1 by subtracting from each gene expression the mean value for the sample and dividing it by the standard deviation. (It is also possible to normalize across genes, but we found that these results are harder to interpret). more strongly than the others, so if we choose genes solely by decreasing T-values we might choose the most representative genes for one class only . To avoid this problem we use the the norm alized data and com pared the dis covered clas ses to natural classes. When run without specify ing the number of clusters, TwoStep quickly found 2 clusters, which matched well with AML/ALL division (see Figure 13). 20 genes per class, and for each subset built neural net m odels using 10-fold cros s-validation. W e aggregated average errors per clas s. dataset, we did not have a separate test set. 
Figur e 13: P ropor tion of natur al classe s in 2 c luste rs found When asked to find 3 clusters, TwoStep produced clusters that matched closely the separation between AML, ALL-T, and ALL-B (see Figure 14). 
Figur e 14: P ropor tion of natur al classe s in 3 c luste rs found Similar results were obtaine d with Kohonen clustering. Other types of analysis possible with Clementine include combining clinical and genetic data and performing outcome / treatment success prediction. many ideas presented here are a pplicable to other domains, such as cheminformatics or drug design, where the number of attributes is very large and much larger than the number of samples. For example, a version of these techniques was used to analyze chemical structure data with 100,000 Boolean attributes and 20,000 examples. sensitive classification. In many domains, classification errors are not equal -for example, missing a treatable cancer is a much more serious error than missing an untreatable cancer. The C5.0 algorithm allows cost-sensitive l earning that can be used to develop models that minimize cost of errors, according to a custom error cost matrix; this could be used to develop classification models which would be more useful in practice. which can be used to access the huge medical textual resources. methodology can lead to good results. In this paper we have shown how to use Clementine A pplication Templates to obtain very good results in analyzing mi croarray datasets, based on best practice and organized using CRISP-DM. Our thanks to Pablo Tamayo fo r helpful suggestions and ideas. [1] Brown et al., Knowledge-based analysis of microarray gene [2] CAMDA 2000, Proceedings of Critical Assessment of [3] Chapman, P., Clinton, J., Kerber, R., Khabaza, T., Reinartz, [4] Dubitzky et al., Symbolic and Subsymbolic Machine [5] Golub et al., Molecular Classification of Cancer: Class [6] Khabaza, T. and Shearer, C., Data Mining with Clementine, [7] Khabaza, T. &amp; Sigerson, D., WebCAT: the Clementine [8] Kohavi, R, John, G., Wra ppers for Feature Subset [9] NIST Engineering and Statistics Handbook , http:// [10] Pomeroy et al., Prediction of central nervous system [11] Ramaswamy, S. et al, Multiclass cancer diagnosis using [12] Shearer, C. and Khabaza, T., Data Mining by Data Owners, [13] Tamayo, P., personal communication, 2002. [14] Tusher, Tibshirani, and Chu, Significance analysis of [15] Whitehead (MIT) Institute Ca ncer Genomics Publications [16] Wirth, R. &amp; Hipp, J., CRI SP-DM: Towards a Standard [17] Saccone, R. A., Rauniyar, R. K. and Patti M.-E., Sources of 
