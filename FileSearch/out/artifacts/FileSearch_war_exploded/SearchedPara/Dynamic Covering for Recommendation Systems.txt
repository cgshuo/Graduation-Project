 In this paper, we identify a fundamental algorithmic prob-lem that we term succinct dynamic covering (SDC), arising in many modern-day web applications, including ad-serving and online recommendation systems such as in eBay, Net-flix, and Amazon. Roughly speaking, SDC applies two re-strictions to the well-studied Max-Coverage problem [14]: Given an integer k , X = { 1 , 2 ,...,n } and I = { S 1 ,...,S S  X  X , find J  X  I , such that |J|  X  k and ( S S  X  X  S ) is as large as possible. The two restrictions applied by SDC are: (1) Dynamic: At query-time, we are given a query Q  X  X , and our goal is to find J such that Q T ( S S  X  X  S ) is as large as possible; (2) Space-constrained: We don X  X  have enough space to store (and process) the entire input; specifically, we have o ( mn ), and maybe as little as O (( m + n ) polylog ( mn )) space. A solution to SDC maintains a small data structure, and uses this datastructure to answer most dynamic queries with high accuracy. We call such a scheme a Coverage Ora-cle .

We present algorithms and complexity results for cov-erage oracles. We present deterministic and probabilistic near-tight upper and lower bounds on the approximation ratio of SDC as a function of the amount of space available to the oracle. Our lower bound results show that to ob-tain constant-factor approximations we need  X ( mn ) space. Fortunately, our upper bounds present an explicit tradeoff between space and approximation ratio, allowing us to de-termine the amount of space needed to guarantee certain accuracy.
 H.0 [ Information Systems ]: General Algorithms dynamic covering, recommendation systems, max-coverage problem
The explosion of data and applications on the web over the last decade have given rise to many new data management challenges. This paper identifies a fundamental subproblem inherent in several Web applications, including online rec-ommendation systems, and serving advertisements on web-pages. Let us begin with a motivating example.

Example 1.1. Consider the online movie rental and stream-ing website, Netflix [3], and one of their users Alice. Based on Alice X  X  movie viewing (and rating) history, Netflix would like to recommend new movies to Alice for watching. (In-deed, Netflix threw open a million-dollar challenge on signif-icantly improving their movie recommendations [4].) Con-ceivably, there are many ways of devising algorithms for rec-ommendation ranging from data mining to machine learn-ing techniques, and indeed there has been a great deal of such work on providing personalized recommendations (see [6] for a survey). Regardless of the specific technique, an important subproblem that arises is finding users  X  X imilar X  to Alice, i.e., finding users who have independently or in conjunction viewed (and liked) movies seen (and liked) by Alice. Abstractly speaking, we are given a universal set of all Netflix movies, and Netflix users identified by the subset of movies they have viewed (and liked or disliked). Given a specific user Alice, we are interested in finding (say k ) other users, who together cover a large set of Alice X  X  likes and dis-likes. Note that for each user, the set of movies that need to be covered is different, and therefore the covering cannot be performed statically , independent of the user. In fact, Net-flix dynamically provides movie recommendations as users rate movies in a particular genre (say comedy), or request movies in specific languages, or time periods. Providing rec-ommendations at interactive speed, based on user queries (such as a particular genre), rules out computationally-expensive processing over the entire Netflix data, which is very large. Therefore, we are interested in approximately solving the aforementioned covering problem based on a subset of the data.
Netflix currently has over 10 millions users, over 100,000 movies, and obviously some of the popular movies have been viewed by many users, and movie buffs have rated a large number of movies; Netflix owns over 55 million discs.
The main challenge that arises is to statically identify a subset of the data that would provide good approximations to the covering problem for any dynamic user query.
 Note that very similar challenges arise in other recommen-dation systems, e.g., Alice visits an online shopping website like eBay [2] or Amazon [1], and the website is interested in recommending products to her based on her current query for a particular brand or product, and her prior purchasing (and viewing) history.

The example above can be formulated as an instance of a simple algorithmic covering problem, generalizing the NP-hard optimization problem max k-cover [14]. The input to this problem is an integer k , a set X = { 1 ,...,n } , a family I  X  2 X of subsets of X , and query Q  X  X . Here ( X , I ) is called a set system , X is called the ground set of the set-system, and members of X are called elements or items . We make no assumptions on how the set system is represented in the input, though the reader can think of the obvious rep-resentation by a n  X  m bipartite graph for intuition. This n  X  m bipartite graph can be stored in O ( nm ) bits, which is in fact information-theoretically optimal for storing an ar-bitrary set system on n items and m sets. The objective of the problem is to return J  X  I with |J|  X  k that collec-tively cover as much of Q as possible. Since this problem is a generalization of max k-cover, it is NP-hard. Never-theless, absent any additional constraints this problem can be approximated in polynomial time by a straightforward adaptation of the greedy algorithm for max k-cover 2 , which attains a constant factor e e  X  1 approximation in O ( mn ) time [16]. However, we further constrain the problem as follows, rendering new techniques necessary.

From the above example, we identify two properties that we require of any system that solves this covering problem: 1. Space Constrained: We need to (statically) prepro-2. Dynamic : The query Q is not known a-priori, but
We call this covering problem (formalized in the next sec-tion) the Succinct Dynamic Covering (SDC) problem. More-over, we call a solution to SDC a Coverage Oracle . A cover-age oracle consists of a static stage that constructs a datas-tructure, and a dynamic stage that uses the datastructure to answer queries.
The greedy algorithm for max k-cover, adapted to our prob-lem, is simple: Find the set in I covering as many uncovered items in Q as possible, and repeat this k times. This can clearly be implemented in O ( mn ) time, and has been shown to yield a e/ ( e  X  1) approximation.

Next we briefly present another, entirely different, Web application that also needs to confront SDC . In addition, we note that there are several other applications facing sim-ilar covering problems, including gene identification [13], searching domain-specific aggregator sites like Yelp [5], top-ical query decomposition [9], and search-result diversifica-tion [10, 12].

Example 1.2. Online advertisers bid on (1) webpages match-ing relevancy criteria and (2) typically target a certain user demographic. Advertisements are served based on a com-bination of the two criterion above. When a user visits a particular webpage, there is usually no precise information about the users X  demographic, i.e., age, location, interests, gender, etc. Instead, there is a range of possible values for each of these attributes, determined based on the search query the user issued or session information. Ad-servers therefore attempt to pick a set of advertisements that would be of interest (i.e.,  X  X over X ) a large number of users; the user demographic that needs to be covered is determined by the page on which the advertisement is being placed, the user query, and session information. Therefore, ad-serving is faced with the SDC problem. The space constraint arises because the set system consisting of all webpages, and each user identified by the set of webpages visited by the user is prohibitively large to store in memory and process in real-time for every single page view. The dynamic aspect arises because each user view of each page is associated with a dif-ferent user demographic that needs to be covered.
Next we outline the main contributions of this paper. Related work is presented next, and future directions are presented in Section 6. To maintain the flow of the paper, we defer some of the longer proofs of our results to Appendix A, with brief sketches appearing in the main body of the paper.
Our study of the tradeoff between space and approxima-tion ratio is in the spirit of the work of Thorup and Zwick [19] on distance oracles . They considered the problem of compressing a graph G into a small datastructure, in such a way that the datastructure can be used to approximately answer queries for the distance between pairs of nodes in G . Similar to our results, they showed matching upper and lower bounds on the space needed for compressing the graph subject to preserving a certain approximation ratio. More-over, similarly to our upperbounds for SDC, their distance oracles benefit from a speedup at query time as approxima-tion ratio is sacrificed for space.

Previous work has studied the set cover problem under streaming models. One model studied in [8, 15] assumes that the sets are known in advance, only elements arrive online, and, the algorithms do not know in advance which subset of elements will arrive. An alternative model assumes that elements are known in advance and sets arrive in a streaming fashion [18]. Our work differs from these works in that SDC operates under a storage budget, so all sets cannot be stored; moreover, SDC needs to provide a good cover for all possible dynamic query inputs.

Another related area is that of nearest neighbor search. It is easy to see that the SDC problem with k = 1 corresponds to nearest neighbor search using the dot product similarity measure, i.e., sim dot ( x,y ) = dot ( x,y ) n . However, following from a result from Charikar [11], there exists no locality sensitive hash function family for the dot product similarity function. Thus, there is no hope that signature schemes (like minhashing for the Jaccard distance) can be used for SDC .
We start by defining the succinct dynamic covering (SDC) problem in Section 3.1. Then, in Section 3.2 we summarize the main technical results achieved by this paper. We now formally define the SDC problem.

Definition 3.1 (SDC). Given an offline input consist-ing of a set system ( X , I ) with n elements (a.k.a items ) X and m sets I , and an integer k  X  1 , devise a coverage oracle such that given a dynamic query Q  X  X , the oracle finds a J  X  I such that |J|  X  k and ( S S  X  X  S ) T Q is as large as possible.

Definition 3.2 (Coverage Oracle). A Coverage Or-acle for SDC consists of two stages: 1. Static Stage: Given integers m , n , k , and set system 2. Dynamic Stage: Given a dynamic query Q  X  X , Note that our two constraints on a solution for SDC are il-lustrated by the two stages above. (1) We are interested in building an offline data structure D , and only use D to answer queries. Typically, we want to maintain a small data structure, certainly o ( mn ), and maybe as little as O (( m + n ) polylog ( mn )) or even O ( m + n ). Therefore, we cannot store the entire set system. (2) Unlike the traditional max-coverage problem where the entire set of elements X need to be covered, in SDC we are given queries dynamically. There-fore, we want a coverage oracle that returns good solutions for all queries.

Given the space limitation of SDC, we cannot hope to exactly solve SDC (for all dynamic input queries). The goal of this paper is to explore approximate solutions for SDC, given a specific space constraint on the offline data structure D . We define the approximation ratio of an oracle as the worst-case, taken over all inputs, of the ratio between the coverage of Q by the optimal solution and the coverage of Q by the output of the oracle. We allow the approximation ratio to be a function of n , m , and k , and denote it by  X  ( n,m,k ).

More precisely, given a coverage oracle A , if on inputs k, X , I ,Q (where implicitly n = |X| and m = |I| ) the or-acle A returns J  X  I , we denote the size of the coverage as A ( k, X , I ,Q ) := | ( S S  X  X  S ) T Q | . Similarly, we denote the coverage of the optimal solution by OPT ( k, X , I ,Q ) := max {| ( S S  X  X   X  S ) T Q | : J  X   X  I , |J  X  |  X  k } . We then ex-press the approximation ratio  X  ( n,m,k ) as follows. Where the maximum above is taken over set systems ( X , I ) with |X| = n and |I| = m , and queries Q  X  X  .

We will also be concerned with randomized coverage ora-cles. Note that, when we devise randomized coverage oracle, we use randomization only in the static stage; i.e. in the construction of the datastructure. We then let the expected approximation ratio be the worst case expected performance of the oracle as compared to the optimal solution. The expectation in the above expression is over the random coins flipped by the static stage of the oracle, and the max-imization is over X , I ,Q as before. We elaborate on this benchmark in Section 4.

We study the space-approximation tradeoff; i.e., how the (expected) approximation ratio improves as the amount of space allowed for D is increased. In our lowerbounds, we are not specifically concerned with the time taken to com-pute the datastructure or answer queries. Therefore, our lowerbounds are purely information-theoretic : we calculate the amount of information we are required to store if we are to guarantee a specific approximation ratio, independent of computational concerns. Our lowerbounds are particularly novel and striking in that they assume nothing about the datastructure, which may be an arbitrary sequence of bits. We establish our lowerbounds via a novel application of the probabilistic method that may be of independent interest.
Even though we focus on space vs approximation, and not on runtime, fortunately the coverage oracles in our up-perbounds can be implemented efficiently (both static and dynamic stage). Moreover, using our upperbounds to trade approximation for space yields, as a side-effect, an improve-ment in runtime when answering a query. In particular, observe that if no sparsification of the data is done up-front, then answering each query using the standard greedy ap-proximation algorithm for max k-cover [16] takes O ( mn ) time. Our oracles, presented in Section 4, spends O ( mn ) time up-front building a data structure of size O ( b ), where b is a parameter of the oracle between n and nm . In the dy-namic stage, however, answering a query now takes O ( b ), since we use the greedy algorithm for max k-cover on a  X  X parse X  set system. Therefore, the dynamic stage becomes Table 1: Summary of SDC results giving the faster as we decrease size of the data structure. In fact, this increase in speed is not restricted to an algorithmic speedup as described above. It is likely that there will also be speedup due to architectural reasons, since a smaller amount of data needs to be kept in memory. Therefore, trading off approx-imation for space yields an incidental speedup in runtime which bodes well for the dynamic nature of the queries.
Table 1 summarizes the main results obtained in this pa-per for SDC input with n elements, m sets, and integer k  X  1. The lower bound in the table is for any nonnegative constants  X  1 , X  2 not both 0, and the randomized upperbound is parameterized by with 0  X   X  1 / 2. The upper and lower bounds are developed in Sections 4 and 5 respectively.
In this section, we show a coverage oracle that trades off space and approximation ratio. We designate a trade-off parameter , where 0  X   X  1 / 2. For any such , stores e O ( nm 1  X  2 ) bits. Therefore, setting a small value of achieves a better approximation ratio, at the expense of storage space. As is common practice, we use e O () to denote suppressing polylogarithmic factors in n and m ; this is rea-sonable when the guarantees are super-polylogarithmic, as is the case here.

The oracle we show is randomized, in the sense that the static stage flips some random coins. The datastructure con-structed is a random variable in the internal coin flips of the static stage of the oracle. We measure the expected approx-imation ratio (a.k.a approximation ratio, when clear from context) of the oracle, as defined in Equation (1). For every fixed query Q independent of the random coins used in con-structing the datastructure, this ratio is attained in expec-tation. In other words, our adversarial model is that of an oblivious adversary : someone trying to fool our oracle may choose any query they like, but their choice cannot depend on knowledge of the random choices made in constructing the datastructure.

In Section 5 we will see that our oracle attains a space-approximation tradeoff that is essentially optimal when com-pared with oracles that are deterministic. In other words, no deterministic oracle can do substantially better. We leave open the questions of whether a better randomized oracle is possible, and whether an equally good deterministic oracle exists.
The following theorem states the main result of this sec-tion.

Theorem 4.1. For every with 0  X   X  1 / 2 , there is a randomized coverage oracle for SDC that achieves an O min( m , approximation and stores e O ( nm 1  X  2 ) bits.
 The remainder of this section, leading up to the above re-sult, is organized as follows. Before proving Theorem 4.1, to build intuition we show in Section 4.2 (Remark 4.2) a much simpler deterministic oracle, with a much weaker approxima-tion guarantee. Then, we prove Theorem 4.1 in two parts. First, in Section 4.3, we show a randomized coverage oracle that stores e O ( nm 1  X  2 ) bits and achieves an O ( m / proximation in expectation. Then, in Section 4.4, we show a deterministic oracle that achieves a O ( tion and stores e O ( n ) bits. Combining the two oracles into one in the obvious way yields Theorem 4.1.
Remark 4.2. There is a simple deterministic oracle that attains a m/k approximation with e O ( n ) space. The static stage proceeds as follows: Given set system ( X , I ) , for each i  X  X we  X  X emember X  one set S  X  I with i  X  S (breaking ties arbitrarily). In other words, for each S  X  I we define b S  X  S such that n b S : S  X  X  o is a partition of X . We then store the  X  X parsified X  set system X , b I = n b S : S  X  X  o clear that this can be done in linear time by a trivial greedy algorithm. Moreover, ( X , b I ) can be stored in e O ( n ) space as a n  X  m bipartite graph with n edges.

The dynamic stage is straightforward: given a query Q , we simply return the indices of the k sets in b I that collectively cover as much of Q as possible. It is clear that this gives a m/k approximation. Moreover, since b I is a partition of X , it can be accomplished by a trivial greedy algorithm in polynomial time.

Next we use randomization to show a much better, and much more involved, upperbound that trades off approxi-mation and space.
Consider the set system ( X , I ), where X is the set of items and I is the family of sets. We assume without loss that each item is in some set. We define a randomized oracle for building a datastructure, which is a  X  X parsified X  version of ( X , I ). Namely, for every S  X  I we define b S  X  S , and store the set system X , b I = n b S o ( X , b I ) can be stored in e O ( nm 1  X  2 ) space. We construct the datastructure in two stages, as follows.
When presented with a query Q  X  X , we use the stored datastructure ( X , b I ) in the obvious way: namely, we find c S ,..., c S k  X  b I maximizing | ( S k i =1 b S i ) T Q | , and return the name of the corresponding original sets S 1 ,...,S k ever, this problem cannot be solved exactly in polynomial time in general. Nevertheless, we can instead use the greedy algorithm for max-k-cover to get a constant-factor approx-imation [16]; this will not affect our asymptotic guarantee on the approximation ratio. The following two lemmas com-plete the proof that the above oracle achieves an O ( m / approximation with e O ( nm 1  X  2 ) space.

Lemma 4.3. The datastructure ( X , b I ) can be stored using e O ( nm 1  X  2 ) bits.
 The proof of the above lemma, appearing in Appendix A.1, shows that ( X , b I ) can be stored as a bipartite graph with a small number of edges, by accounting for the edges created in each stage of our algorithm.
 Lemma 4.4. For every query Q , the oracle returns sets S ,...,S k such that for any S  X  1 ,...,S  X  k  X  X  .
 Note that S 1 ,...,S k are random variables in the internal coin-flips of the static stage that constructs the datastruc-ture. The expectation in the statement of the lemma is over these random coins. The proof of the lemma, appear-ing in Appendix A.1, distinguishes two cases depending on whether the majority of the items covered by the optimal solution appears in significant or insignificant sets.
This coverage oracle is similar to the one in the previous section, though is much simpler. Moreover, it is determinis-tic. Indeed, we construct the datastructure by the following greedy algorithm that resembles the greedy algorithm for max-k-cover
Observe that b I is a partition of X . When presented with a query Q  X  X , we use the datastructure ( X , n b
S : S  X  X  o ) in the obvious way. Namely, we find the sets c S ,..., c S k  X  b I maximizing | ( S k i =1 b S i ) T Q | , and output the corresponding non-sparse sets S 1 ,...,S k . This can easily be done in polynomial time by using the obvious greedy algo-rithm, since b I is a partition of X .

Note that the oracle described above is very similar to the oracle from Section 4.2: The dynamic stage is identical. The static stage, however, needs to build the partition us-ing a specific greedy ordering  X  as opposed to the arbitrary ordering used in Section 4.2. The following two Lemmas complete the proof that the oracle achieves an O ( approximation with e O ( n ) space.

Lemma 4.5. The datastructure ( X , b I ) can be stored using e O ( n ) bits
Proof. Observe that each item is contained in exactly one b S  X  b I . Therefore, the bipartite graph representing the set system ( X , b I ) has at most n edges. This establishes the Lemma.
 Lemma 4.6. For every query Q , the oracle returns sets S ,...,S k with for any S  X  1 ,...,S  X  k  X  X  .
 The full proof of the lemma, appearing in Appendix A.1, considers two cases based on whether the majority of the elements covered by an optimal choice are in big or small sets. This section develops lower bounds for the SDC problem. We consider deterministic oracles that store a datastructure of size b ( n,m,k ) for set systems with n items, m sets, max-imum number of allowed sets k . Moreover, we assume that n  X  b ( n,m,k )  X  nm , since no nontrivial positive result is possible when b ( n,m,k ) = o ( n ), and a perfect approxima-tion ratio of 1 is possible when b ( n,m,k ) =  X ( nm ).
The main result of this section is stated in the following theorem, which says that our randomized oracle in the pre-vious section achieves a space-approximation tradeoff that essentially matches the best possible for any deterministic oracle.

Theorem 5.1. Consider any deterministic oracle that stores a datastructure of size at most b ( n,m,k ) bits, where n  X  b ( n,m,k )  X  nm . Let ( n,m,k ) be such that b ( n,m,k ) = stant  X  &gt; 0 . Moreover, when does not attain an approximation ratio of O ( n 1 / 2  X   X   X  &gt; 0 .
 The proof of the theorem above is somewhat involved. There-fore, to simplify the presentation we prove in Section 5.2 a slight simplification of Theorem 5.1 that captures all the main ideas: Our simplification sets k = 1, and proves the Then, in Section 5.3 we prove the approximation ratio for the case of in Section 5.4, we demonstrate how to modify our proofs for any k , yielding Theorem 5.1.

We fix  X  &gt; 0. For the remainder of the section, we use b and as shorthand for b ( n,m,k ) and ( n,m,k ), respectively. We let  X  ( n,m,k ) be the approximation ratio of the oracle, and use  X  as shorthand. Observe that 0  X   X  1 / 2.
We simplify Theorem 5.1 by assuming k = 1 and m  X   X  n . The result is the following proposition, stated using the shorthand notation described above.

Proposition 5.2. Fix k = 1 and parameter with 0  X   X  1 / 2 . Assume m  X  oracle that stores a datastructure of size at most b = nm bits. The oracle does not attain an approximation ratio of O ( m  X   X  ) for any constant  X  &gt; 0 .

We assume the approximation ratio  X  attained by the or-acle is O ( m  X   X  ) and derive a contradiction. The proof uses the probabilistic method (see [7]). We begin by defining a distribution on set systems, and then go on to show that this distribution  X  X ools X  a small coverage oracle with positive probability. We will show that there is a set system ( X , I ) and a query Q that forces the algorithm to output a set S  X  I that is not within  X  from optimal. We use the probabilistic method. Namely, we exhibit a distribution D over set systems ( X , I ) such that, for every deterministic oracle storing a datastruc-ture of size b , there exists with non-zero probability a query Q for which the oracle outputs a set of approximation worse than  X  . To show this, we draw two set systems i.i.d from D , and show that with non-zero probability both the follow-ing hold: the two set systems are not distinguished by the coverage oracle, and moreover there exists a query Q that requires that the algorithm return different answers for the two set systems for a O ( m  X   X  ) approximation.

We define D as follows. Given the ground set X = { 1 ,...,n } , we let I = { A i } m i =1 and draw A 1 ,...,A m i.i.d as follows: We let A i be a subset of X of size nm  X  drawn uniformly at ran-dom.
Next, we draw two set systems ( X , I = { A i } m i =1 we lowerbound the probability that ( X , I ) and ( X , I 0 not distinguished by the coverage oracle. We call such an occurence a  X  X ollision X . The following result, proved in Ap-pendix A.2, lower bounds the probability of a collision.
Lemma 5.3. The probability that the same datastructure is stored for ( X , I ) and ( X , I 0 ) is at least 2  X  b
Next, we lowerbound the probability that a query Q exists requiring two different answers for ( X , I ) and ( X , I der to get the desired  X  = O ( m  X   X  ) approximation. We call such a query Q a fooling query . We define a set of queries that are  X  X andidates X  for being a fooling query: A set Q  X  X  is called a candidate query if Q = A i S A 0 i 0 for some i 6 = i In other words, a query is a candidate if it is the union of a set from ( X , I ) and a set from ( X , I 0 ) with different indices.
Ideally, candidate Q = A i S A 0 i 0 would be a fooling query by forcing the oracle to output i for ( X , I ) and i 0 for ( X , I in order to guarantee the desired approximation. However, this need not be the case: consider for instance the case when, for some j 6 = i,i 0 , both A j and A 0 j have large inter-section with Q , making it ok to output j for both. We will show that the probability that none of the candidate queries is a fooling query is strictly less than 2  X  b when n and m are sufficiently large. Doing so would complete the proof: col-lision occurs with probability  X  2  X  b , and a fooling query exists with probability &gt; 1  X  2  X  b , and therefore both occur simultaneously with positive probability. This would yield the desired contradiction.
We now upperbound the probability that none of the candidates is a fooling query. Observe that if candidate Q = A i S A 0 i 0 is not a fooling query, then there exists A  X  I S I 0 \{ A i ,A 0 i 0 } with | A T Q | X  nm  X  / X  . Therefore one of the following must be true: 1. There exists A  X  I S I 0 \{ A i ,A 0 i 0 } with | A T A 2. There exists A  X  I S I 0 \{ A i ,A 0 i 0 } with | A T A
Therefore, if none of the candidates were fooling queries, then there are many  X  X airs X  of sets in I S I 0 that have an intersection substantially larger than the expected size of nm  X  2 . This seems very unlikely. Indeed, the remainder of this proof will demonstrate just that.

If none of the candidates are fooling queries, then by ex-amining (1) and (2) above we deduce the following. There exists 3 a set of pairs P  X  ( I S I 0 )  X  ( I S I 0 ) such that: 1. | P | X  m  X  2 =  X ( m ) 2. The undirected graph with nodes I S I 0 and edges P Consider constructing P as follows: For candidate query Q = A 1 S A 0 2 , find the set in I S I 0 \{ A 1 ,A 0 2 } with a large intersection with one of A 1 or A 0 2 as in (1) or (2). Say for instance we find that A 7 has a large intersection with A include ( A 1 ,A 7 ) in P , mark both A 1 and A 7 as  X  X ouched X , and designate A 1 a  X  X eft X  node and A 7 a  X  X ight X  node. Then, we repeat the process with some candidate Q 0 = A i S A 0 some  X  X ntouched X  A i and A 0 i 0 . We keep repeating until there are no such candidates. Throughout this greedy process, we mark at most two members of I S I 0 as  X  X ouched X  for every pair we include in P . Note that some A i may be  X  X ouched X  more than once. As long as there are at least 2 untouched sets in each of I and I 0 , the algorithm may continue. 3. If ( B,C )  X  P then | B T C | X   X ( nm  X  2 +  X  )
We now proceed to bound the probability of existence of such a P , and in the process also bound the probabil-ity that none of the candidate queries are fooling. Recall that members of I S I 0 are drawn i.i.d from the uniform distribution on subsets of X of size nm  X  . For every pair ( B,C )  X  I S I 0 , we let R ( B,C ) = | B T C | denote the size of their intersection. It is easy to see the random variables {R ( B,C ) } B,C  X  X  S I 0 are pairwise independent. Therefore, any acyclic set of pairs is mutually independent, by basic probability theory. Thus, if we fix a particular P satisfying (1) and (2), the probability that P satisfies condition (3) is at most
We now want to estimate the probability that the intersec-tion of B and C is a factor  X ( m  X  ) more than its expectation of nm  X  2 . Therefore, we consider an indicator random vari-able Y i for each i  X  X , designating wheter i  X  B  X  C . If Y were independent, we could use Chernoff bounds to bound the probability that R ( B,C ) is large. Fortunately, it is easy to see that the Y i  X  X  are negatively-correlated: i.e., for any L  X  X  1 ,...,n } , we have Pr [ V i  X  L Y i = 1]  X  Q i  X  L . Therefore, by the result of [17], if we  X  X retend X  that they are independent by approximating their joint-distribution by i.i.d bernoulli random variables, we can still use Chernoff Bounds to bound the upper-tail probability. Therefore, us-ing Chernoff bounds 4 we deduce that the probability that the intersection of B and C is a factor  X ( m  X  ) more than 2 satisfies condition (3) is at most
Y
Now, we can sum over all possible choices for P satisfying (1) and (2) to get a bound on the existence of a P satisfying (1), (2) and (3). It is easy to see that there are at most m m choices for P that satify (1) and (2). Using the union bound, we get the following bound on the existence of such a P . m
Where the last inequality follows by simple algebraic ma-nipulation from our assumption that m  X  when n and m are sufficiently large. Recall that, by our previous discussion, this expression also upperbounds the probability that none of the candidate queries are fooling queries. But, when n and m are sufficiently large, this is strictly smaller than 2  X  b = 2  X  nm 1  X  2 . Thus, by our previ-ous discussion, this completes the proof of Proposition 5.2. We maintain the assumption that k = 1, and extend Proposition 5.2 for the case when We use the following version of the Chernoff Bound: Let X ,...,X n be independent bernoulli random variables, and let X = P i X i . If E [ X ] =  X  and  X  &gt; 2 e  X  1, then Pr [ X &gt; (1 +  X )  X  ]  X  2  X   X   X  . the following result is similar to that of Proposition 5.2, and appears in Appendix A.2.

Proposition 5.4. Fix k = 1 and parameter with 0  X   X  1 / 2 . Assume oracle that stores a datastructure of size at most b = nm bits. The oracle does not attain an approximation ratio of O ( n 1 / 2  X   X  ) for any constant  X  &gt; 0 .
In this section, we generalize Proposition 5.2 to arbitrary k . The generalization of Proposition 5.4 to arbitrary k is essentially identical, and therefore we leave it as an exercise for the reader. We now state the generalization of Propo-sition 5.2 to arbitrary k , the proof of which again follows Proposition 5.2 and appears in Appendix A.2.

Proposition 5.5. Let parameter be such that 0  X   X  1 / 2 . Assume m  X  cle that stores a datastructure of size at most b = nm bits. The oracle does not attain an approximation ratio of
This paper introduced and studied a fundamental prob-lem, called SDC, arising in many large-scale Web applica-tions. A summary of results obtained by the paper appear in Table 1 (Section 3.2). The main specific open question that arises is whether there is a deterministic oracle that is as good as the randomized oracle proposed in Section 4. More generally, a detailed analysis of practical subclasses of SDC seems to hold promise. [1] Amazon. www.amazon.com . [2] eBay. www.ebay.com . [3] Netflix. http://www.netflix.com . [4] Netflix Prize. http://www.netflixprize.com . [5] Yelp. www.yelp.com . [6] Gediminas Adomavicius and Er Tuzhilin. Toward the [7] N. Alon and J. Spencer. The Probabilistic Method . [9] Francesco Bonchi, Carlos Castillo, Debora Donato, [10] Jaime G. Carbonell and Jade Goldstein. The use of [11] M. Charikar. Similarity estimation techniques from [12] Harr Chen and David R. Karger. Less is more: [13] Nello Cristianini and Matthew W. Hahn. Introduction [14] M. R. Garey and D. S. Johnson. Computers and [15] YJ. Naor and N. Buchbinder. Online primal-dual [16] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An [17] Alessandro Panconesi and Aravind Srinivasan.
 [18] B. Saha and L. Getoor. On maximum coverage in the [19] Mikkel Thorup and Uri Zwick. Approximate distance Proof of Lemma 4.3: We store the set system as a bi-partite graph representing the containment relation between items and sets. To show that the bipartite graph can be stored in the required space, it suffices to show that ( X , is  X  X parse X ; namely, that the total number of edges ( x, b X  X  b I such that x  X  b S is O ( nm 1  X  2 ). We account for the edges created in stages 1 and 2 separately. 1. Every significant item is connected to a single set. This 2. For every insignificant set, we store  X  nm  X  2 items, Proof of Lemma 4.4: We fix an optimal choice for S  X  1 ,...,S I , and denote OPT = | ( S k i =1 S  X  i ) T Q | . Since, by construc-tion, b S  X  S for all S  X  X  , it suffices to show that the output of the oracle satisfies | ( S k i =1 b S i ) T Q |  X  OPT tation. Moreover, since the dynamic stage algorithm finds a constant factor approximation to max {| ( S k i =1 c S ,..., c S k  X  b I} , it is sufficient to show that there exists S ,...,S k  X  X  with E [ | ( S k i =1 b S i ) T Q | ]  X  OPT
We distinguish two cases, based on whether most of the items ( S k i =1 S  X  i ) T Q covered by the optimal solution are in significant or insignificant sets. We use the  X  X ignificant X  and  X  X nsignificant X  designation as used in the static stage algo-rithm. Moreover, we refer to b S  X  b I as significant (insignif-icant, resp.) when the corresponding S  X  I is significant (insignificant, resp.). 1. At least half of ( S k i =1 S  X  i ) T Q are significant items : 2. At least half of ( S k i =1 S  X  i ) T Q are insignificant Proof of Lemma 4.6: Fix an optimal choice of S  X  1 ,...,S and denote OPT = | ( S k i =1 S  X  i ) T Q | . Recall that the oracle finds c S 1 ,..., c S k  X  b I maximizing | ( S k i =1 b S outputs the corresponding original sets S 1 ,...,S k .
It suffices to show that there are some b S 1 ,..., with | ( S k i =1 b S i ) T Q | X  OPT/O ( p n/k ). We distinguish two cases, based on whether most of ( S k i =1 S  X  i ) T Q are in big or small sets in b I .

Recall that b I forms a partition of X . We say b S  X   X  X ignificant X  if | b S |  X  p n/k , otherwise b S is  X  X nsignificant X . Similarly, we say an item i  X  X is  X  X ignificant X  if it falls in a significant set in b I , otherwise it is  X  X nsignificant X . Notice that there are at most n  X 
First, we consider the case where at least half the items in ( S k i =1 ) S  X  i T Q are significant. Since there at most significant sets in b I , by the pigeonhole principle there are k of them that collectively cover a k/ all significant items in ( S k i =1 S  X  i ) T Q . This would guarantee the O ( p n/k ) approximation, as needed.
 Next, we consider the case where at least half of ( S k i =1 are insignificant. By examining the greedy algorithm of the static stage, it is easy to see that each S  X  I contains at most p n/k insignificant items. Therefore, there are at most k  X  p n/k = we deduce that OPT = | ( S i S  X  i ) T Q |  X  2 optimal covers O ( approximation to show that there are b S 1 ,..., b S k  X  b collectively cover k items of Q . It is easy to see that this is indeed the case, since b I is a partition of X . This completes the proof. 2
There are 2 b possible datastructures. Let p i denote the probability that, when presented with random ( X , I )  X  D , the oracle stores the i  X  X h datastructure. We can write this probability of  X  X ollision X  of the two i.i.d samples ( X , I ) and ( X , I 0 ) as P 2 b i =1 p 2 i . However, since P i p i sion is minimized when p i = 2  X  b for all i . Plugging into the above expression gives a lowerbound of 2  X  b , as required.
Instead of replicating almost the entire proof of Proposi-tion 5.2, we instead point out the key changes necessary to yield a proof of 5.4 and leave the rest as an easy excercise for the reader.

The proof proceeds almost identically to the proof of Propo-sition 5.2, with the following main changes:  X  Modifications to Section 5.2.1 : When defining D ,  X  We perform similar calculations throughout, accomo- X  Modifications to Section 5.2.4 : We eventually ar-
The proof of Proposition 5.5 follows the outline of the proof of Proposition 5.2. The necessary modifications to the proof of Proposition 5.2 are as follows:  X  Modifications to Section 5.2.1 : We define distri- X  Modifications to Section 5.2.2 : Instead of sampling  X  Modifications to Section 5.2.3 : We now define a  X  Modifications to Section 5.2.4 : Similarly, if a can-We thank Philip Bohannon, Hector Garcia-Molina, Ashwin Machavanajhhala, Tim Roughgarden, and Elad Verbin for insightful discussions.
This is not true when k is almost equal to m . However, the theorem becomes trivially true when k &gt; m 1 / 6 , so we can without loss assume that k is not too large.
