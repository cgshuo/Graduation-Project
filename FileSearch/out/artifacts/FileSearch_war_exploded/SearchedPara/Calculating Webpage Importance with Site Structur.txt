 Typical search engines today use hyperlink information to measure the importance of a Most link analysis algorithms such as [3,5,6,7] mainly focus on hyperlink information. The underlying assumption is that the Web is a flat graph, where all pages are identical Symmetrically, if having a micro look, we will find that each website is also organized improve the link analysis algorithms by introducing site hierarchy. 
From the view of website administrator and the view of web user, we can get the following two rules: 
Rule 2: Generally speaking, a parent page would be more important than its child 
The above constraints reflect the goal of the website administrator when he/she Thus, we have the confidence that page importance computation with site structure constraints will improve the effectiveness and precision of Web information retrieval system. The rest of this paper is organized as follows. In Section 2, we propose how to combine hyperlink graph and site structure graph together to refine PageRank algorithm. In Section 3, we describe the optimization based PageRank algorithm with site structure constraints. Experimental results are reported in Section 4. And last, we give the concluding remarks and future work in Section 5. As well known, PageRank algorithm simulates a random walk on the hyperlink graph, and it assumes that hyperlinks represent human endorsement. According to the Rule 1 in Section 1, parent and child pages also endorse each other. Similar to the hyperlink graph, we can construct a site structure graph. In such a way, we will get two graphs. Let A and A * represent the adjacency matrix of the hyperlink graph and site structure graph separately. To integrate the site structure information for Web page ranking, we need to fuse these two graphs for the random walk model. 2.1 Additive Graph Fusion Algorithm merge the adjacency matrix A and A * to get a new adjacency matrix B where 
For the new graph with adjacency matrix B , we can follow the standard PageRank algorithm to compute the PageRank for each page in the Web. We call it by additive graph fusion algorithm (AGF for short). 2.2 Multiplicative Graph Fusion Algorithm In the additive graph fusion algorithm, we get a new graph by adding hyperlink graph (MGF for short). 
Different from the additive graph fusion algorithm, we do not multiply the two graph to a probability transition matrix, an d then multiply the two transition matrices together to get a new graph for random walk model. The details of this algorithm are shown as follow: 
Same as the standard PageRank algorithm, we first normalize each row of the get a row-stochastic matrix * A from the adjacency matrix A * . Then we get a matrix C for the new graph by multiplication: 
It is easy to get that C is also a row-stochastic matrix. And the stationary distribution of C is used to measure the importance of each web page. As mentioned in Rule 2 in Section 1, from the view of website administrator, most of the parent pages should be more important than their child pages because the children constraints, we need to refine the importance of pages within the same site by optimization. We call this algorithm OB for short. importance scores calculated by standard PageRank algorithm are 12 ,,, k  X  X   X  . Let L we refine the importance scores by adding a constant value to those pages in the same 
During this refinement process, on one hand, we try to make those pages in the same site consistent with the level priority; while on the other hand, we do not want to make too much change for the original PageRank. Note that the change of the original PageRank depends on the weight vector w . The smaller the module of w , the less the change of the original PageRank is. So as a result, we can formulate the optimization problem as below. 
Considering that the level priority shown in Rule 2 is true for general sense, but it may be unsatisfied for some special cases, we introduce relaxation variables ij  X  to the optimization model of (4) as follows, where C controls the trade-off between the modification to original PageRank and the priority and the PageRank remains unchanged. the limitation of paper length. To compare our new PageRank algorithms with the standard PageRank (PR) [7], we baseline with the precision at 10 (P@10) of 0.104. top 2000 pages from the relevance list, and combine the relevance score with importance score as follow: 
The P@10 of all algorithms under investigation is shown in Figure 2. All the four three algorithms outperform the standard PageRank algorithm, which shows the shows the validation of the site structure constraints mentioned in the introduction. In both AGF and MGF, we integrate site structure information by modifying the random walk graph. However, we do not know clearly how much site structure contributes to we can say, OB makes the best use of site structure information among all the algorithms in Figure 2. while ranking web pages, which was neglected by traditional link analysis algorithms. Based on this motivation, we modified the standard PageRank algorithm from two importance analysis: additive graph fusion algorithm, multiplicative graph fusion algorithm and optimization-based algorithm. Experiments on the topic distillation task of TREC2003 showed that all the new algorithms outperform the standard PageRank algorithm. Particularly the optimization-based algorithm significantly boosted the retrieval accuracy. 
HITS [6] is another popular link analysis algorithm. For the future work we would like to apply site structure constraint to modify HITS algorithm. 
