 Kernel method has been developed as one of the standard approaches for nonlinear learning, which however, does not scale to large data set due to its quadratic complexity in the number of samples. A number of kernel approximation methods have thus been proposed in the recent years, among which the random features method gains much popularity due to its simplicity and direct reduction of non-linear problem to a linear one. Different random feature functions have since been proposed to approximate a variety of kernel func-tions. Among them the Random Binning (RB) feature, proposed in the first random-feature paper [21], has drawn much less attention than the Random Fourier (RF) feature proposed also in [21]. In this work, we observe that the RB features, with right choice of op-timization solver, could be orders-of-magnitude more efficient than other random features and kernel approximation methods under the same requirement of accuracy. We thus propose the first analysis of RB from the perspective of optimization, which by interpret-ing RB as a Randomized Block Coordinate Descent in the infinite-dimensional space, gives a faster convergence rate compared to that of other random features. In particular, we show that by draw-ing R random grids with at least  X  number of non-empty bins per grid in expectation, RB method achieves a convergence rate of O (1 / (  X R )) , which not only sharpens its O (1 / Monte Carlo analysis, but also shows a  X  times speedup over other random features under the same analysis framework. In addition, we demonstrate another advantage of RB in the L1-regularized set-ting, where unlike other random features, a RB-based Coordinate Descent solver can be parallelized with guaranteed speedup pro-portional to  X  . Our extensive experiments demonstrate the superior performance of the RB features over other random features and ker-nel approximation methods.  X  Both authors contributed equally to this manuscript Kernel approximation, Random Binning Features, large-scale ma-chine learning, faster convergence, strong parallelizability
Kernel methods have great promise for learning non-linear model from simple data input representations and have been demonstrated successful for solving various learning problems such as regres-sion, classification, feature extraction, clustering and dimensional-ity reduction [26, 30]. However, they are typically not first choice for large-scale nonlinear learning problems, since large number of samples ( N ) presents significant challenges in terms of computa-tion and memory consumptions to Kernel methods for computing the dense kernel matrix K  X  R N  X  N which requires at least a O ( N 2 ) complexity. To scale up the kernel methods, there have been great efforts addressing this challenge from various perspec-tives such as numerical linear algebra, sampling approximation, op-timization and functional analysis [4, 5, 7, 21, 27, 33].
A line of research [7, 27, 28, 31] approximates the kernel ma-trix K using low-rank factorizations, K  X  Z T Z , where Z  X  R
N  X  R matrix with R N . Among them, Nystr X m method [5, 8, 11, 27, 31] is probably one of the most popular approaches, which reduces the total computational costs to O ( NRd + NR R ) or O ( NRd + NRm ) depending whether the algorithm per-forms on K explicitly or implicitly through Z , where d and m are the input data dimension and the number of iterations of an iterative solver respectively. However, the convergence of the low-rank ap-proximation is proportional to O (1 / implies that the rank R may need to be near-linear to the number of data points in order to achieve comparable generalization error compared to the vanilla kernel method. For large-scale problems, the low-rank approximation could become almost as expensive as the exact kernel method to maintain competitive performance [29].
Another popular approach for scaling up kernel method is ran-dom features approximation [21, 22]. Unlike previous approach that approximates kernel matrix, Random Features approximate the kernel function directly via sampling from an explicit feature map. Random Fourier (RF) is one of the feature maps that attracted con-siderable interests due to its easy implementation and fast execution time [4, 12, 20, 22, 32], which has total computational cost and stor-age requirement as O ( NRd + NRm ) and O ( NR ) respectively, for computing feature matrix Z and operating the subsequent algo-rithms on Z . A Fastfood approach and its extension [12, 32] was proposed to reduce the time of computing Fourier features from O ( Rd ) to O ( R log d ) by leveraging Hadamard basis functions, which improves the efficiency for prediction but not necessarily for training if d m . Although RF has been successfully applied to speech recognition and vision classifications on very large datasets [3, 10, 17], a drawback is that a significant large number of random features are needed to achieve a comparable performance to exact kernel method. This is not surprising since the convergence of ap-proximation error is in the order O (1 / is the same as that of low-rank kernel approximations.

Mercer X  X  theorem [19] guarantees that any positive-definite ker-nel permits a feature-map decomposition. However, the decompo-sition is not unique. One may find different feature maps to con-struct the same kernel function [21, 33]. Therefore, we ask follow-ing question: do some of the feature maps lead to faster conver-gence than the others in terms of approximation? In this paper, we address this question by reconsidering the Random Binning (RB) feature map, which was proposed in the first Random-Feature pa-per [21] but has drawn much less attentions since then compared to the RF feature. Our main contributions are fourfold.

First, we propose the first analysis of RB from the perspective of optimization. By interpreting RB as a Randomized Block Coor-dinate Descent (RBCD) in the infinite-dimensional space induced from the kernel, we prove that RB enjoys faster convergence than other random features. Specifically, by drawing R grids with ex-pected number of non-empty bins per grid lower bounded by  X  , RB can achieve a solution comparable to exact kernel method with O (1 / (  X R )) precision in terms of the objective function, which is not only better than the existing O (1 / analysis [21], but also shows a  X  times speedup over the rate of other random features under the same analysis framework [33].
Second, we exploit the sparse structure of the feature matrix Z , which is the key to rapidly transform the data features into a very high-dimension feature space that is linearly separately by any re-gressors and classifiers. In addition, we discuss how to efficiently perform the computation for a large, sparse matrix by using state-of-the-art iterative solvers and advanced matrix storage techniques. As a result, the computational complexity and storage requirements in training are still O ( NRd + NRm ) and O ( NR ) , respectively.
Third, we show that Random Binning features is particularly suitable for Parallel Coordinate Descent solver. Unlike other ran-dom features, RB guarantees a speedup proportional to  X  due to a sparse feature matrix. This is particularly useful in the Sparse Random Feature setting [33], where L1 regularization is used to induce a compact nonlinear predictor and Coordinate Descent is presumably the state-of-the-art solver in such setting.

Finally, we provide extensive experiments to demonstrate the faster convergence and better parallelizability of RB in practice. Compared to other popular low-rank approximations, RB shows superior performance on both regression and classification tasks under the same computational budgets, and achieves same perfor-mance with one to three orders of magnitude reduction in time and memory consumptions. When combined with Coordinate Descent to solve an L1-regularized objective, RB shows an almost linear speedup, in contrast to RF that has almost no speedup.
In this work, we consider the problem of fitting a nonlinear pre-diction function f : X  X  Y in Reproducing Kernel Hilbert Space H from training data pairs { ( x n ,y n ) } N n =1 via regularized Empiri-cal Risk Minimization (ERM) where L ( z,y ) is a convex loss function with Lipschitz-continuous derivative satisfying | L 0 ( z 1 ,y )  X  L 0 ( z 2 ,y ) |  X   X  | z includes several standard loss functions such as the square-loss L ( z,y ) = 1 2 ( z  X  y ) 2 , square-hinge loss L ( z,y ) = max(1  X  zy, 0) 2 and logistic loss L ( z,y ) = log(1 + exp(  X  yz )) .
The RKHS H can be defined via a positive-definite (PD) kernel function k ( x 1 , x 2 ) that measures similarity between samples as One can also define the RKHS via a possibly infinite-dimensional feature map {  X   X  h ( x ) } h  X  H with each h  X  H defining a feature function  X   X  h ( x ) : X  X  R . The space can be expressed as H = f (  X  ) = Z where w ( h ) specifies weights over the set of features {  X  The Mercer X  X  theorem [19] connects the above two formulations of RKHS by stating that every PD kernel k ( .,. ) can be expressed as an integration over some basis functions {  X  h ( . ) } h  X  H k ( x 1 , x 2 ) = However, the decomposition (4) is not unique, so one can find dif-k ( .,. ) . In particular, as an example used extensively in this work, the Laplacian Kernel allows decomposition based on (i) Fourier basis map [21], (ii) RB map [21], and also (iii) map based on infinite number of decision trees [15] to name a few. On the other hand, different kernels can be constructed using the same set of basis function {  X  h different distribution p ( h ) . For example, the RB feature map can be used to construct any shift-invariant kernel of the form [21] by sampling the "width" of bins  X  j for each feature j from a distri-of k j (  X  ) , assuming the kernel has a non-negative second derivative.
In this section, we describe the Random Binning (RB) feature map, which has decomposition of the form where B  X  is a grid parameterized by  X  = (  X  1 ,u 1 ,..., X  specifies the width and bias of the grid w.r.t. the d dimensions, and  X 
B  X  ( x ) is a vector which has Algorithm 1 Random Binning Features
Given a kernel function k ( x 1 , x 2 ) = Q d j =1 k j ( | x p j (  X  )  X   X k 00 j (  X  ) be a distribution over  X  . for r = 1 ...R do end for .
 data with RB Features. and  X  b ( x ) = 0 otherwise for any b  X  B  X  . Note for each grid B , the number of bins | B  X  | is countably infinite, so  X  B infinite dimension but only 1 non-zero entry (at the bin x lies in). Figure 1 illustrates an example when the raw dimension d = 2 . The kernel K ( x 1 , x 2 ) is thus interpreted as the collision probabil-ity that two data points x 1 , x 2 fall in the same bin, when the grid is generated from distribution p (  X  ) . In [21], it is pointed out for any kernel of form (6) with nonnegative second derivative k 00 can derive distribution p (  X  ) = Q d j =1 p j (  X  j ) U ( u p range [ a,b ] .

To obtain a kernel approximation scheme from the feature map (7), a simple Monte Carlo method can be used to approximate (7) by averaging over R grids { B  X  r } R r =1 with each grid X  X  parameter  X  drawn from p (  X  ) . The procedure for generating R RB features from raw data { x n } N n =1 is given in Algorithm 1.

Using a Monte-Carlo analysis, one can show the approximation to (7) yields approximation error of order O (1 / Representer theorem, one can further bound error of the learned predictor w
RF z ( x )  X  f as shown in [21] (appendix C). Unfortunately, the rate of conver-gence suggests that to achieve small approximation error , one needs significant amount of random features proportional to  X (1 / and furthermore, the Monte-Carlo analysis does not explain why empirically RB feature achieves faster convergence than other ran-dom feature map like Fourier basis by orders of magnitude.
In this section, we first illustrate the sparse structure of the fea-ture matrix Z of RB and discuss how to make efficient computa-Figure 2: Example of the sparse feature matrix Z N  X  D erated by RB. In this special case, Z has the number of rows N = 200 and columns D = 395 , respectively. The number of grids R = 10 and the nnz ( Z ) = 2000 . Note that for i th row of Z , nnz ( Z ( i, :)) = R and R  X  D  X  NR . tion and storage format of Z . Then by interpreting RB features as Randomized Block Coordinate Descent in the infinite-dimensional space, we prove that RB has a faster convergence rate than other random features. We illustrate them accordingly in the following sections.
A special characteristic of RB compared to other low-rank ap-proximations is the fact that the feature matrix generated by RB is typically a large, sparse binary matrix Z  X  R N  X  D , where the value of D is determined by both number of grids R and the kernel width parameter (ex.  X  in the case of Laplacian Kernel). Different from other random features, D , rather than R , is the actual number of columns of Z . A direct connection between D and R is that the matrix has each row i satisfying nnz ( Z ( i, :)) = R and therefore R  X  D  X  NR . Intuitively speaking, RB has more expressive power than RF since it generates a large yet sparse feature matrix to rapidly transform the data space to a very high dimension space, where data could become almost linearly separable by the classi-fiers. Fig. 2 gives an example to illustrate the sparse structure of Z .

In the case of Kernel Ridge Regression (L2-regularization with square loss), if using RB feature to approximate the RKHS, one can solve (1) directly in its primal form. The weighting vector is simply the solution of the linear system: Note since Z is a large sparse matrix, there is no need to explicitly compute the covariance matrix Z T Z , which is much denser than Z itself. One can apply state-of-the-art sparse iterative solvers such as Conjugate Gradient (CG) and GMRES to directly operate on Z [25]. The main computation in CG or GMRES is the sparse matrix-vector products. Let m be the number of iterations, then the total computational complexity of iterative solver is O ( mnnz ( Z )) = O ( mNR ) . In addition, since most elements in Z are zeros, the Compressed Sparse Row type matrix storage format should be em-ployed for economically storing Z [9], which gives computational cost and memory requirement as O ( mNR ) and O ( NR ) respec-tively, a similar cost to that of other low-rank approximations de-spite its much higher dimension. In testing phase, each point pro-duces a sparse feature vector z ( x )  X  X  D based on the grids stored during training, yielding a sparse vector z ( x ) with nnz ( z ( x ))) = R and computing the decision function z ( x ) T w RB onlyl requires O ( dR + R ) .

When the ERM is smooth but not quadratic, a Newton-CG method that solves smooth problem via a series of local quadratic approx-imation gives the same complexity per CG iteration [14], and note that most of state-of-the-art linear classification algorithms have complexity linear to nnz ( Z ) , the number of nonzeros of feature matrix [6]. In section 4, we further discuss cases of L1-regularized problem, where a Coordinate Descent algorithm of cost O ( nnz ( Z )) per iteration is discussed.
In [33], a new approach of analysis was proposed, which inter-preted Random Features as Randomized Coordinate Descent in the infinite dimensional space, and gives a better O (1 /R ) rate in the convergence of objective function. In this section, we extend the approach of [33] to show that, RB Feature can be interpreted as RBCD in the infinite-dimensional space, which by drawing a block of features at a time, produces a number of features D significantly more than the number of blocks R , resulting a provably faster con-vergence rate than other RF. While at the same time, by exploit-ing state-of-the-art iterative solvers introduced in section 3.1, the computational complexity of RB does not increase with number of features D but only with the number of blocks R . Consequently, to achieve the same accuracy, RB requires significantly less training and prediction time compared to other RF.

A key quantity to our analysis is an upper bound on the collision probability  X   X  which specifies how unlikely data points will fall into the same bin, and its inverse  X   X  := 1 / X   X  which lower bounds the number of bins containing at least one data point. We define them as follows.
 Definition 1. Define collision probability of data D on bin b  X  B as Let  X   X  := max b  X  B  X   X  b be an upper bound on (9) , and  X  be a lower bound on the number of nonempty bins of grid  X  . is denoted as the lower bound on the expected number of (used) bins w.r.t the distribution p (  X  ) .

In the RB matrix, the empirical collision probability is simply the average number of non-zeros per column, divided by N , a number much smaller than 1 as in the example of Fig. 2. Our analysis assumes a smooth loss function satisfying the following criteria. Assumption 1. The loss function L ( z,y ) is smooth w.r.t. response z so difference between function difference and its linear approxi-mation can be bounded as for some constant 0  X   X   X  X  X  .

This assumption is satisfied for a wide range of loss such as square loss (  X  = 1) , logistic loss (  X  = 1 / 4) and L2-hinge loss (  X  = 1 ).

We interpret RB as a Fully Corrective Randomized Block Coor-dinate Descent (FC-RBCD) on the objective function where Loss (  X  w ;  X  ) = 1 N P N n =1 L (  X   X  w ,  X  ( x with "  X  " denoting the component-wise product. The goal is to show that, by performing R steps of FC-RBCD on (11), one can obtain a  X  w
R with comparable regularized loss to that from optimal solution of (1). Note one advantage of analysis from this optimization per-spective is: it does not rely on Representer theorem, and thus R ( w ) can be L2 regularizer  X  2 k w k 2 or L1 regularizer  X  k w k latter has advantage of giving sparse predictor of faster prediction [33]. The FC-RBCD algorithm maintains an active set of blocks A ( r ) which is expanded for R iterations. At each iteration r , the FC-RBCD does the following: 1. Draw  X  from p (  X  ) ( derived from the kernel k ( .,. ) ). 2. Expand active set A ( r +1) := A ( r )  X  B  X  . 3. Minimize (11) subject to a limited support supp (  X  w )  X  X 
Note this algorithm is only used for analysis. In practice, one can draw R blocks of features at a time, and solve (11) by any optimization algorithm such as those mentioned in section 3.1 or the CD method we introduce in section 4.

Due to space limit, here we prove the case when R ( . ) is the non-smooth L1 regularizer  X  k  X  w k 1 . The smooth case for R ( w ) = k  X  w k 2 can be shown in a similar way. Note the objective function (11) can be written as by a scaling of variable  X  w =
The below theorem states that, running FC-RBCD for R itera-tions, it generates a solution  X  w R close to any reference solution w in terms of objective (12) with their difference bounded by O ( Theorem 1. Let R be the number of blocks (grids) generated by FC-RBCD, and w  X  be any reference solution, we have Proof. Firstly, we obtain an expression for the progress made by each iteration of FC-RBCD. Let B := B  X  ( r ) be the block drawn at step 1 of FC-RBCD, and  X  w ( r +1) be the minimizer of (11) subject to support supp (  X  w )  X  A ( r +1) given by the step 3. Since B  X  A for any  X  B : supp (  X  )  X  B . Then denote b i as the bin x sumption 1), we have Loss (  X  w ( r ) +  X  B )  X  Loss (  X  w ( r ) )  X  1 where the second inequality uses the fact  X  b i = 1 and Now consider the regularization term, note since block B is drawn from an inifinite-dimensional space, the probability that B is in active set is 0 . Therefore, we have B  X  X  ( r ) =  X  ,  X  w R B (  X  w ( r ) B ) = 0 . As a result, Let  X  B be the minimizer of RHS of (16). It satisfies  X  B  X v  X  ( r )  X  B = 0 for some  X  B  X   X  R (  X  B ) , and thus, Now taking expectation w.r.t. p (  X  ) on both sides of (17), we have E [ F (  X  w ( r ) +  X  B )]  X  F (  X  w ( r ) )  X  X  X  1 where  X   X  := uses the fact that the number of used bins  X   X  ( r ) = 1 / X  non-negative correlation with the discriminative power of block B measured by the magnitude of gradient with soft-thresholding k  X   X 
B +  X  g B k (i.e. fewer collisions on grid B implies B to be a better block of features ).

The result of (18) expresses descent amount in terms of the prox-imal gradient of the reparameterized objective (12). Note for B : B  X  X  ( r ) =  X  , we have w ( r ) B = 0 , and R B (  X   X  )  X  X  on the other hand, for B  X  X  ( r ) , we have 0  X  arg min since they are solved to optimality in the previous iteration. Then E [ F (  X  w ( r ) +  X  )]  X  F (  X  w ( r ) )  X  X  X   X  = R ( step is to show the descent amount given by RHS of (19) decreases achieved by considering  X   X  of the form  X  ( w  X   X  w ( r )  X   X  [0 , 1] as follows: E [  X 
F ( w ( r ) +  X   X  )]  X   X  F ( w ( r ) )  X  min k  X  min  X  min  X  min where the second and fourth inequalities are from convexity of  X  F ( . ) . The  X  minimizing (20) is  X   X  := min(  X  (  X  F ( w which leads to
E [  X  F ( w ( r ) +  X   X  )]  X   X  F ( w ( r ) )  X  X  X   X  ( if  X 
F ( w ( r ) )  X   X  F ( w  X  )  X   X   X  k w  X  k 2 ; otherwise, we have E [ scent method. Therefore, for r 0 := r  X  c &gt; 0 , solving the recursion (21) leads to the conclusion.

Note we have k regularized case, and thus the FC-RBCD guarantees convergence of the L1-norm objective to the (non-square) L2-norm objective. The convergence result of Theorem 1 is of the same form to the rate proved in [33] for other random features, however, with an additional multiplicative factor  X   X  1 that speeds up the rate by  X  times. Recall that  X  is the lower bound on the expected number of bins being used by data samples for each block of features B which in practice is a factor much larger than 1 , as shown in the Figure 2 and also in our experiments. In particular, in case each grid B  X  has similar number of bins being used, we have D  X   X R , and thus obtain a rate of the form Note for a fixed R , the total number of features D is increasing with kernel parameter 1 / X  in the case of Laplacian Kernel, which means the less smooth the kernel, the faster convergence of RB. A simple extreme case is when  X   X  0 , where one achieves 0 training loss, and the RB, by putting each sample in a separate bin, converges to 0 loss with R = 1 , D = N . On the other hand, other random features, such as Fourier, still require large R for convergence to 0 loss. In practice, there are many data that require a small kernel bandwidth  X  to avoid underfitting, for which RB has dramatically faster convergence than other RF.
In this section, we study another strength of RB Features in the context of Sparse Random Feature [33], where one aims to train a sparse nonlinear predictor that has faster prediction and more com-pact representation through an L1-regularized objective. In this case, the CD method is known as state-of-the-art solver [23, 34], and we aim to show that the structure of RB allows CD to be paral-lelized with much more speedup than that of other random features.
Given the N  X  D data matrix produced by the RB Algorithm 1, a RCD Method solves by minimizing (23) w.r.t. a single coordinate j Algorithm 2 Sparse Random Binning Features via Parallel RCD 0. Generate RB feature matrix Z by Algorithm 1 1. z 1 = 0 , w 1 = 0 . for t=1......T (with  X  threads in parallel) do end for at a time, where is the gradient of loss term in (23) w.r.t. the j -th coordinate, and M j :=  X  1 N P N i =1 z 2 ij is an upper bound on  X  jj L ( . ) . Note, by focusing on single coordinate, (24) has a tighter quadratic upper bound than other algorithms such as Proximal Gradient Method, and allows simple closed-form solution where To have efficient evaluation of the gradient (25), a practical imple-mentation maintain the responses wise minimization takes O ( nnz ( z j )) time for both gradient evalu-ation and maintenance of (27), where z j := ( z ij ) i  X  [ N ] rithm is summarized in Alg. 2, which just like the iterative solver introduced in section 3.1, has cost O ( nnz ( Z )) for one pass of all variables j  X  [ D ] .
The RCD, however, is hard to parallelize [23]. It is known that simultaneous updates of two coordinates j 1 , j 2 could lead to di-vergence, and although one can enforce convergence by shortening the step size 1 M with parallelization without additional assumption [1, 24].
On the other hand, in [24], it is shown that a function with par-tially separable smooth term plus a separable non-smooth term can be parallelized with guaranteed speedup in terms of overall complexity, where  X ( w ) is a non-smooth separable function and each function f i ( w ) is a smooth depends only on at most  X  num-ber of variables. The form (28), fortunately, fits our objective (23) with features z i generated by RB. In particular, the generating pro-cess of RB guarantees that, for each block of feature B  X  sample can fall in exactly one bin b = ( b x n 1  X  u 1  X  therefore each sample inolves at most R features out of D . Specif-ically, let  X ( w ) :=  X  k w k 1 and we have  X  = R . Then by Theorem 19 of [24], a parallel RCD of  X  threads that selects coordinate j uniformly at random achieves a speed-up (i.e. time-of-sequential/time-of-parallel) of When D,R 1 , and  X  = a  X   X  + 1 where  X   X  := D/R , (29) becomes which equals (  X   X  + 1) / 2 when a = 1 and approaches  X   X  when a  X   X  . Therefore, it is guaranteed in theory that parallelization can speedup RCD significantly as long as  X   X  = D/R 1 . We give our sparse RB Features algorithm based on parallel RCD in Alg. 2. Note for other Random Features, there is no speedup guaranteed and our experiment shows that Parallel RCD performed on Random Fourier features could even have no speedup.

Note that the speedup achieved in this section is orthogonal to ing  X  , the advantage of RB over other Random Features is super-linearly increasing if a parallel RCD is used. Note also that the results (29), (30) also apply to algorithms that utilize Coordinate Descent as subproblem solvers such as Proximal (Quasi) Newton Method [13, 35]. Those methods are typically employed for com-putationally expensive loss functions.
In this section, we present extensive sets of experiments to demon-strate the efficiency and effectiveness of RB. The datasets are cho-sen to overlap with those in other papers in the literature, where the details are shown in the table 1. All sets except census are avail-able at LIBSVM data set [2]. All computations are carried out on a DELL dual socket with Intel Xeon processors at 2.93GHz for a total of 16 cores and 250 GB of memory running the SUSE Linux op-erating system. We implemented all methods in C++ and all dense matrix operations are performed by using the optimized BLAS and LAPACK routines provided in the OpenBLAS library. Due to the limited space, we only choose subsets of our results to present in each subsection. However, these results are objective and unbiased.
We perform experiments to investigate the characteristics of RB by varying the kernel parameter  X  and the rank R , respectively. We use a regularization  X  = 0 . 01 to make sure the reasonable performance of RB and other low-rank kernels, although we found that RB is not sensitive to this parameter. We increase the  X  in the large interval from 1 e  X  2 to 1 e 2 so that the optimal  X  locates within the interval. We apply CG iterative solver to operate on Z directly. In order to make fair runtime comparison in each run, we set the tol = 1 e  X  15 to force similar CG iterations with different  X  .
We evaluate the training and testing performance of regression and classification, when varying  X  with fixed R . In [21], it does not consider the effect of  X  in their analysis, which however has a large impact on the performance since D depends on the number of bins which is controlled by  X  . Fig. 3 shows that the training and testing performance coincidentally decrease (increase) before they diverge when D grows by increasing  X  . This confirms with our analysis in Theorem 1 that the larger  X  , the faster convergence of RB Feature (recall that the convergence rate is O (1 / (  X R )) ).
Second, one should not be surprised that the empirical training time increases with D . The operations involving the weighting vec-tor w RB could become as expensive as a sparse matrix-vector oper-ation in an iterative solver. However, the total computational costs are still bounded by O ( NR ) but the constant factor may vary with different datasets. Fortunately, in most of cases, the training time corresponding to the peak performance is just slightly higher than the smallest one. In practice, there are several ways to improve the computation costs by exploiting more advanced sparse matrix tech-niques such as preconditioning and efficient storage scheme, which is out scope of this paper and left for future study.

Finally, we evaluate the training and testing performance when varying R with fixed  X  . Fig.4 shows that the training and testing performance converge almost linearly with D , which again con-firms our analysis in Theorem 1. In addition, we observe that RB has strong overfit ability which turns out to be a strong attribute, especially when the hypothesis space has not yet saturated.
We present a large sets of experiments to compare RB with other most popular low-rank kernel approximations, including RF [21], Nystr X m [31], and recently proposed independent block approxi-mation [29]. We also compare all methods with the exact kernel as a benchmark [26]. We do not report the results of the vanilla ker-nel on covtype and SUSY since the programs run out of memory. To make a fair comparison, we also apply CG on RB and Nys-tr X m directly on Z to admit similar computational costs. Since the independent block kernel approximation approximates the kernel matrix directly, we employ direct solver of dense matrix for this method. In practice, the CG iterative solver has no need to solve in high precision [3], which has also been observed in our experi-ments. Thus, we set the tolerance to 1 e  X  3 .

Fig.5 clearly demonstrates the superiority of RB compared to other low-rank kernels. For example, in the first column, RB sig-nificantly outperforms other methods in testing performance on all of these datasets, especially when R is relatively small. This is because RB enjoys much faster convergence rate to the optimal function than other methods. The advantage generally diminishes when R increases to reasonably large. However, for some large datasets such as covtype and SUSY, increasing number of random features or R boosts the performance extremely slow. This is con-sistent with our analysis that RB enjoys its fast convergence rate of O (1 / (  X R )) while other methods has slow convergence rates O (1 / sights about how many number of random features or how large rank R that is needed for achieving similar performance of RB. In particular, RB is often between one and three orders of magnitude faster and less memory consumptions than other methods.

In the second column, we also observe that the training time of all low-rank kernels are linear with R , which is expected since all these methods has computational complexity of O ( kNR ) . The difference in training time between these low-rank kernels is only within some constant factors. However, we point out that the com-putations of RF, Nystr X m and independent block approximation are mainly carried out by the high-optimized BLAS library since they are dense matrices. In contrast, the computations of RB are most involved in sparse matrix operations, which are self-implemented and not yet optimized. In addition, more advanced sparse matrix techniques such as preconditioning can be explored to significantly accelerate the computation, which we leave it as future work. We perform experiments to compare RB with RF when using RCD to solve L1-regularized Lasso and kernel SVM for both re-gression and binary classification problems. Since the goal is to demonstrate the strong parallel performance of RB, we implement the basic parallel implementation of RCD based on simple shared memory parallel programming model with OpenMP. We leave the high-performance distributed RCD implementation as one of the future works. We define the speedup of RCD on multicore imple-mentation as follows: As shown in Fig.6, when the sparsity level of the feature matrix Z is high, the near-linear speedup can be achieved [16, 18]. This is because the minimization problem can almost be separated along the coordinate axes, then higher degrees of parallelism are possi-ble. In contrast, if Z is lack of sparsity, then the penalty for data correlations slows the speedup to none. This is confirmed by no gain of parallel speedup of RF since Z is always fully dense. Ob-viously, in order to empower strong parallel performance of RB, a very large D is expected, which interestingly coincides with power of its faster convergence. Therefore, one can enjoy the double ben-efits of fast convergence and strong parallelizability of RB, which is especially useful for very large-scale problems.
In this paper, we revisit RB features, an overlooked yet very powerful random features, which we observe often to be orders of magnitude faster than other random features and kernel approx-imation methods to achieve the same accuracy. Motivated by these impressive empirical results, we propose the first analysis of RB from the perspective of optimization, to make a solid attempt to quantify its faster convergence, which is not captured by tradi-tional Monte-Carlo analysis. By interpreting RB as a RBCD in the infinite-dimensional space, we show that by drawing R grids with at least  X  expected number of non-empty bins per grid, RB achieves a convergence rate of O (1 / (  X R )) . In addition, in the L1-regularized setting, we demonstrate the sparse structure of RB fea-tures allows RCD solver to be parallelized with guaranteed speedup proportional to  X  . Our extensive experiments demonstrate the su-perior performance of the RB features over other random feature and kernel approximation methods. This work was done while L. Wu was a research intern at IBM Research. J. Chen is supported in part by the XDATA program of the Advanced Research Projects Agency (DARPA), adminis-tered through Air Force Research Laboratory contract FA8750-12-C-0323.
