 Most commercial database management systems sort tu-ples of a relation by their primary keys for the purpose of supporting efficient insertions, deletions, and updates. However, primary keys are usually auto-generated integers, which bear little useful information about user data. Sec-ondary indexes have to be created sometimes to help retrieve tuples by columns other than the primary key. Evidently, a better solution is to sort the data by columns that ap-pear frequently in retrieval conditions. Unfortunately, this method does not work, at least not immediately, when the relation is vertically partitioned, which is a popular tech-nique to reduce I/O overhead, since it is difficult to keep tuples of two partitions in exactly the same order unless the sorting columns are replicated, which again wastes storage space and disk bandwidth unnecessarily. In this paper, we introduce a positional access method that allows a partition to be sorted by another one but incurs little storage overhead and provide details about how to improve its performance. H.2.2 [ Database Management ]: Physical Design X  Access methods ; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Indexing methods Algorithms, Design, Performance Vertical partitioning, positional access method
In relational model, primary keys are usually employed to uniquely identify different tuples of a relation. In order to support efficient insertions, deletions, and updates of data, most commercial database management systems maintain the logical order of tuples of a relation by their primary keys.

However, primary keys are usually auto-generated inte-gers, from which we can hardly extract any useful informa-tion about user data except the order of tuple creations. Sometimes, it is preferred to sort the relation by some non-primary-key columns.

Data partitioning is a another popular technique that may help reduce I/O overhead. We will focus on vertical par-titioning in this paper. Partitioning a relation vertically brings many benefits, such as reduced I/O overhead, rein-forced reference locality, opportunity for caching and prefetch-ing, and improved intra-and inter-query concurrency [15, 9, 7, 18, 4, 3, 17]. In order to identify rows of the same logical tuple in different partitions, primary keys or system-generated surrogates are usually replicated in all partitions [15, 17] and join operations ( ) are performed to reconstruct the original relation.

Unfortunately, it is not easy to maintain a user-defined order for a partitioned relation. An obvious solution is to replicate all sorting columns as well as the primary keys, but a better choice is to manipulate tuples by their positions.
In the rest of this paper, we will refer partitions that are sorted by their own columns as master partitions, and those following the order of another partition as slave partitions. A master can have many slaves at the same time.

We highlight the characteristics of the index structure that is needed as follows.  X 
First, tuples in slaves should be kept in exactly the same order as those in the master . The access method for a master should be able to sort the tuples by a user-defined order like a B-tree and tell their sequential numbers. And for slaves , we need an index structure that can insert, delete, and update tuples by their sequential numbers.
Besides, the storage overhead should be minimized.  X  The sequential numbers in the index should be mutable.
That is, when a new tuple is inserted in or removed from the middle, positions of all the following tuples should be adjusted. Obviously, automatic mechanisms are preferred and traditional B-tree does not meet our requirement.
The main contributions of this paper are summarized as follows. First of all, we highlight a storage method that can improve query performance immensely and clarify the re-quirement to the index structure that is needed. Then we introduce an index structure that meets our requirement and provide an efficient implementation in the database environ-ment. Last but not least, our empirical evaluation demon-strates that our implementation outperforms the original structure in most cases and presents some advantages over the mechanism that C-Store employs.
Berkeley DB [1], a general-purpose embedded database engine, provides an access method named Recno , which pro-vides a basic implementation of the technique introduced in this paper and motivates this work. However, the origi-nal implementation employs no optimization and the per-formance is at the mercy of dataset.

MonetDB, a full fledged column-oriented database system, stores each column of a relation independently and provides position-based operators [7, 13]. However, the position re-flects the order of tuple insertions rather than the natural order of any column. In order to answer complicated queries, MonetDB sorts the relation on chosen columns on the fly.
The advantage of sorting a partition after another one has been noticed by C-Store (Vertica) [18, 3, 12], another column-oriented database system that employs a different mechanism. Taking insertions as an example, all inserted tu-ples are kept in main memory at first, and when the buffer gets full, C-Store sorts the buffered tuples and flushes all partitions to the disk as a group. Tuples in the disk-resident partitions are physically orders and a sparse index is built for each partition. When too many groups of partitions are gen-erated in the disk, C-Store merges some of them into larger ones in the background for the sake of query performance.
The efficiency of C-Store is beyond question and this mech-anism meets our requirement perfectly. However, there are still some drawbacks of the algorithm C-Store employs.  X 
First, merge operations require large free disk space, which is not always satisfied by a pratical system, let alone the energy consumption of extra disk arrays.  X 
Second, merge operations can be easily interrupted by other operations since they usually have a lower prior-ity, and the performance will be seriously affected before the finish.  X 
Last, for some devices, such as flash-based SSDs, merge operations will introduce many write and erase operations, which will reduce the lifespan of the device.

On the contrary, the access method proposed in this pa-per is basically a tree structure like B-tree. Any existing reorganization algorithm [21] can be easily employed. Some of them can be suspended at any time, and users may feel the performance enhancement gradually.

It must be noted that, though most related works are de-scribed in literatures on column-oriented databases, the in-troduced techniques apply to all architectures where a rela-tion is vertically partitioned to improve query performance.
The proposed access method is based on counted tree [14, 19, 20] (also known as ranked tree), which is a useful tool to build a text editor. Basically, the structure of a counted tree is similar to that of a B-tree [6, 8] except that each item in internal nodes is a &lt; counter , pointer &gt; pair. The pointer points to a descendant and the corresponding counter repre-sents the total number of user-defined records in the subtree rooted at the descendant (Figure 1).

Obviously, a counted tree is balanced as a B-tree, and the height of the tree is O ( log k N ), where N is the total number of records in the tree and k is the maximum fan-out of internal nodes. The only difference is that we search the tree by counting records rather than by comparing the search key with the seperating keys. Search operations can be done in O ( log k N )time.

For operations that alter the tree structure, such as inser-tions and deletions, we should make sure that the counters remain accurate. These counter update operations still take O ( log k N )time.

If a leaf node in which a new record is to be inserted is already full, it should be splitted into two nodes. and a new cause the split of its parent. This procedure may propagate all the way up to the root. If the root node is splitted, a new root is created and the height of the tree is increased.
The counted tree structure enables us to operate records according to their positions. We still need a data structure for master partitions, which keeps tuples by their natural order and can tell their positions at the same time. The data structure that meets our requirement is called counted B-tree [2], which is first introduced in an exercise of [14]. A counted B-tree is the combination of a B-tree and a counted tree, which is basically a B-tree but has a counter field for each pointer in internal nodes. This field enables a counted B-tree to tell the positions of newly inserted tuples and re-trieve tuples by their positions.

Though counted B-trees and counted trees exhibit similar characteristics, we will focus on counted trees in this paper.
One drawback of the original counted tree is that each time a record is inserted or removed, all nodes from the root to the leaf will need to be modified, and thus many dirty pages will be generated in the cache. Since database systems usually employ the steal and no-force policy as sug-gested in [11], too many dirty pages will definitely hurt the performance of the cache, especially when the buffer man-ager tries to control the ratio of dirty pages.

Meanwhile, well-known techniques to improve the write performance of B-tree, such as the LSM-tree [16], the buffer tree [5], and the partitioned B-tree [10], do not work directly for a counted tree due to the mutable-key requirement.
In this section, we propose a method that can help im-prove the update performance of counted trees, following the basic idea of LSM-tree [16].
The LSM-tree idea is very simple. The data structure is composed of a hierarchy of two or more similar components. These structures are distributed among devices of the stor-age hierarchy. The top component (or C 0 )iskeptinmain memory entirely, providing fast insertion speed, while lower layers are stored in slower but larger devices, such as mag-netic disks. User data is moved downwards periodically in the background or on demand to limit memory overhead.
In this paper, we deal with the simplest case. That is to say, there are two components in the tree, namely C 0 and C C 0 is kept in memory and C 1 is stored in disk. Frequently accessed nodes of C 1 are also cached in main memory. Algorithm 1 Search a memory-based counted tree 1: Pos  X  1 2: ptr  X  Root 3: while ptr is an internal node do 4: total  X  0 5: for each item item of ptr do 7: break 10: Pos  X  Pos  X  total 11: ptr  X  item . descendant 12: Scan ptr , find Item that covers Pos , and update Pos Algorithm 2 Search a hierarchical counted tree 2: if Item is an insertion record then 3: return Item 4: Pos  X  Pos 7: if Item is an insertion record then 8: return Item
Unlike LSM-tree, C 0 can only be a counted tree in our situation. However, it is not necessary for the nodes to be as large as disk blocks.

Whenever C 0 fails to answer a query, the requested posi-tion needs to be modified before searching C 1 . Therefore, some necessary information should be maintained in C 0 .In our design, we have three types of items in C 0 .  X 
Placeholder . A placeholder covers a segment of continuous positions in C 1 . If a lookup meets a placeholder, C 1 should be searched to answer the query.  X 
Insertion Record . An insertion record contains a newly inserted record, which will be flushed to C 1 eventually.  X 
Deletion Record . A deletion record matches a series of positions in C 1 that should be removed.

In addition, internal nodes of C 0 should count the posi-tions covered by the three types of items separately.
When the size of C 0 reaches a predefined threshold, we should merge C 0 and C 1 together and create a new C 0 com-ponent like LSM-tree. This method minimizes the average response time of operations but may result in an unaccept-able worst-case response time for a real-time system. To handle this problem, we amortize the delay of merge opera-tions by flushing only a few records each time. And before data in C 0 is completely flushed, another memory compo-nent C 0 is used to deal with incoming operations. After that, C 0 is deleted and C 0 becomes the new C 0 .
Detailed descriptions of search, insertion, and deletion al-gorithms for C 0 and C 0 are provided in this section. Update operations can be implemented as deletions of old values followed by insertions of new ones.
 Algorithm 3 Insert an object at a given position 2: if Item is a placeholder then 3: split Item if necessary and reset Item properly 4: insert Obj before Item 5: update related counters in the search path 6: if theleafthatcontains Item overflows then 7: split it and update C 0 9: flush some items in C 0 12: create an empty C 0 and restart partial flush
Algorithm 1 presents the way to search a memory-based counted tree ( C 0 and C 0 ). The procedure starts at the root (Line 2). All internal nodes encountered on the path are scanned (Line 5-9) to decide which descendant to follow. And at last, the search stops at some leaf node.

It must be noted that only the C hold and the C ins counters are computed (Line 6, 8), since only these counters deter-mine the actual position of a record in C 0 or C 0 . However, in case the memory tree fails to answer a lookup query, the sum of the C hold and the C del counters should be used in the subsequent operations (Line 9).

A complete process of the point search operation is given in Algorithm 2. First, C 0 is searched and the backup posi-tion Pos is calculated (Line 1-3). In case C 0 fails to answer the query, C 0 is searched in the same way if the record at Pos has not been flushed to C 1 (Line 6-8), otherwise we skip C and search C 1 directly (Line 10). The decision (Line 5) can be made by comparing Pos and the current flushed position.
If C 0 also fails to answer the query, we adjust the backup position Pos at first (Line 9) and search C 1 .

Range search can be easily implemented by maintaining a pointer in each component. We will not present the detailed algorithm due to space limitations.
The insertion algorithm is given in Algorithm 3. The first step is to find the proper leaf node in C 0 and insert the new record (Line 1-4). If the position is embodied by a placeholder, we have to split it in the first place. Then C is adjusted in order to maintain its property (Line 5-7).
When the total size of C 0 and C 0 exceeds a predefined threshold, some items in C 0 are flushed to C 1 (Line 9). If C 0 is successfully cleared, we delete the old C 0 component, let C 0 be the new C 0 , get ready for another flush cycle, and create a new C 0 to serve coming operations (Line 10-12).
To serve a deletion request, a proper leaf is found and modified, relevant counters on the search path are updated, the leaf is splitted if necessary, and C 0 is flushed if the threshold is exceeded (Algorithm 4).

There are two kinds of situations that may involve a dele-tion operation. If an insertion record is found at the position, remove it simply. In other cases, the position where the dele-tion takes place should be embodied by a placeholder, then the position is seperated from the placeholder and converted to a deletion record. Algorithm 4 Delete an object at a given position 2: if Item is an insertion record then 3: remove Item 4: else 5: split Item if necessary and reset Item properly 6: convert Item to a deletion record 7: combine Item with adjacent deletion records 8: update related counters in the search path
Deletion of an record may seperate a placeholder and then split the leaf, resulting in the growth of the tree. This part (Line 9) is identical to that in Algorithm 3.
In this section, we compare the proposed access method with the original counted tree as well as the merge method used by C-Store 1 . We also examine the validity of partial flush operations. Our experimental platform is a Dell workstation running Ubuntu 10.04 and 32 MB is allocated as the memory pool. The LRU replacement algorithm is employed. Unless other-wise specified, each page contains 4096 bytes, the maximum size of C 0 and C 0 is 16 MB (the rest for the LRU cache), and the maximum fanout of the memory components is 8. The partial flush algorithm flush 32 items each time the memory threshold is exceeded.

We use two datasets, the lineitem table of TPC-H and a synthetic dataset. The lineitem table is sorted by shipdate and we use the 8 least frequential used fields (with the pri-mary key replicated). All records in the synthetic dataset have a fixed length of 100 bytes and the position of each insertion is uniformly selected within [1 ,N +1], where N represents the total number of records before the insertion.
During the evaluation, we always load sufficient data in advance to warm up the cache before loading data. But query tests are always cold-started, which aims to evaluate the impact of different memory management policies. Be-sides, we only have insertions in the experiments, since costs of deletions and updates are similar to that of insertions.
Finally, we compare the proposed method against the merge method. This time, 128 MB is reserved for the cache, and another 128 MB for the memory components. Only the results of the TPC-H dataset are reported. In our implemen-tation, RS X  X  (Read-optimized Store) are not merged until all the data are loaded in order to evaluate the impact of too many RS X  X  to the performance, and each merge operation chooses two smallest RS X  X  as the input. Besides, we assume that the sparse indexes are small enough to reside in main memory entirely, and data compression is not in use. We employ the reorganization algorithm introduced in [21].
Performance Comparison. As shown in Figure 2, the loading performance of the proposed method is much better than the baseline most of the time, no matter whether the partial flush optimization is enabled. If the page size is not
In this section,  X  X tree X  represents the basic counted tree, and  X  X ctree X  without any declaration stands for the proposed hierarchical counted tree without the partial flush feature. Figure 3: Performance of Queries about 1,000 Ran-domly Selected Rows Figure 4: Performance of Queries about 1,000 Re-cently Inserted Rows very large so that enough pages can be cached, loading speed of the basic counted tree can be very good for biased dataset. As for the impact of the proposed method to query perfor-mance, it depends on the character of the queries. Since the hierarchy method always caches the latest inserted data in the memory pool, queries referring to these data could obtain maximal benefit (Figure 4). However, when queries exhibit low temporal locality, the advantage of the hierarchy method may not be so evident or even reversed (Figure 3).
Impact of C 0 Parameters. For biased dataset such as TPC-H, a large memory component is not necessary for the performance of insertion operations (Figure 5(a)). A relatively small C 0 may suffice, depending on the extent to which the data are clustered. And for other datasets where the data arrives randomly, the size of C 0 and C 0 is very important for loading performance. The larger, the better, since adequate records can be buffered and clustered be-fore writing out (Figure 5(a)). Next, we vary the maximum fanout between 2 and 1024 in this experiment. The result is presented in Figure 5(b). As can be seen, both extremes result in a longer response time compared with a modest value. Any value around [16 , 128] seems to be fine.
Effect of Partial Flush. Partial flush operations are designed to amortize the delay of flush operations and im-prove the worst-case response time. As can be seen in Fig-ure 6, if partial flush is not employed, the response time exhibits periodical peaks due to the flush operations. The worst-case response time can be worse than the original counted tree, whose performance tends to be predictable. After adopting partial flush operations, the worst-case per-formance is improved dramatically and the average perfor-mance is almost always better than the baseline. Figure 5: Impact of C 0 (and C 0 ) Parameters on In-sertion Performance Figure 6: Effect of Partial Flush on Insertion Per-formance
Comparison with the Merge Method. In the load-ing stage, the merge method (without merge operations) presents 5x faster speed than our method,and the gap ex-pands as more data are loaded in the database and when the randomness of data increases. However, the query perfor-mance of the merge method is not very competitive unless the RS X  X  are merged to only a few (Figure 7(a)(b)). On the contrary, performance of exactly match queries of the proposed method is very stable, even without any reorgani-zation, and the sequential scan performance can be as fast as the merge method after reorganization. Figure 7(c) presents the latency trend of the reorganization algorithms. As the RS X  X  get larger, merge operations tend to have longer la-tency. In general, the proposed method is suitable for a wider range of workloads than the merge method.
In this paper, we introduce a positional access method that allows partitions of a relation to be sorted after another one but incurs little storage overhead. We also provide some optimizations for the proposed data structure.

It should be noted that, as far as we know, there is no perfect method to incorporate the LSM-tree technique into a counted B-tree, the access method for the master parti-tion, since we need to get the position of a newly inserted record immediately. One possible option is to give pages of the master a high priority in the cache to reduce miss ra-tio; another is to maintain a mirror of the master partition (with sorting keys only) permanently in the main memory and store a hierarchical master in secondary storage. All in all, considering that a master partition may have many slave partitions, the hierarchical counted tree proposed in this paper is still a promising technology. This work was partly supported by the National Grand Fundamental Research 973 Program of China under Grant No. 2011CB302206, the National S&amp;T Major Project of China under Grant No. 2011ZX01042-001-002, a project of Tsinghua University under Grant No. 20111081073, and an IBM SUR project.
