 Hash-based similarity search reduces a continuous similarity rela-tion to the binary concept  X  X imilar or not similar X : two feature vec-tors are considered as similar if they are mapped on the same hash key. From its runtime performance this principle is unequaled X  while being unaffected by dimensionality concerns at the same time. Similarity hashing is applied with great success for near sim-ilarity search in large document collections, and it is considered as a key technology for near-duplicate detection and plagiarism anal-ysis.

This papers reveals the design principles behind hash-based search methods and presents them in a unified way. We introduce new stress statistics that are suited to analyze the performance of hash-based search methods, and we explain the rationale of their effectiveness. Based on these insights, we show how optimum hash functions for similarity search can be derived. We also present new results of a comparative study between different hash-based search methods.
 H.3.1 [ INFORMATION STORAGE AND RETRIEVAL ]: Con-tent Analysis and Indexing; H.3.3 [ INFORMATION STOR-AGE AND RETRIEVAL ]: Information Search and Retrieval; F [ Theory of Computation ]: MISCELLANEOUS Theory, Performance hash-based similarity search, locality-sensitive hashing, dimension reduction
This paper contributes to an aspect of similarity search that re-ceives increasing attention in information retrieval: The use of hashing to significantly speed up similarity search. The hash-based search paradigm has been applied with great success for the follow-ing tasks:
From the retrieval viewpoint hash-based text retrieval is an in-complete technology. Identical hash keys do not imply high sim-ilarity but indicate a high probability of high similarity. This fact suggests the solution strategy for the aforementioned tasks: In a first step a candidate set D q  X  D , | D q || D | , is constructed by a hash-based retrieval method; in a second step D q is further investigated by a complete method.

The entire retrieval setting can be formalized as follows. Given are ( i )aset D = { d 1 ,...,d n } of documents each of which is de-scribed by an m -dimensional feature vector, x  X  R m ,and( ii )a similarity measure,  X  : R m  X  R m  X  [0; 1] , with 0 and 1 indi-cating no and maximum similarity respectively.  X  mayrelyonthe l norm or on the angle between two feature vectors. For a query document d q , represented by feature vector x d q , and a similarity threshold  X   X  [0; 1] , we are interested in the documents of the  X  -neighborhood D q  X  D of d q , which is defined by the following condition: where x d denotes the feature vector of d . Within informa-tion retrieval applications the documents are represented as high-dimensional term vectors with m&gt; 10 4 , typically under the vector space model. We distinguish between the real documents, d and their representations as feature vectors, since one and the same document may be analyzed under different models, different repre-sentations, and different similarity measures, as will be the case in this paper.

In low-dimensional applications, say, m&lt; 10 , the retrieval problem can be efficiently solved with space-partitioning meth-ods like grid-files, KD-trees, or quad-trees, as well as with data-partitioning index trees such as R-trees, Rf-trees, or X-trees. For significantly larger m the construction of D q cannot be done better than by a linear scan in O ( | D | ) [28]. However, if one accepts a decrease in recall, the search can be dramatically accelerated with similarity hashing. As will be discussed later on, the effectiveness of similarity hashing results from the fact that the recall is con-trolled in terms of the similarity threshold  X  for a given similarity measure  X  .

To motivate the underlying ideas consider an m -dimensional document representation under the vector space model with a weighting scheme. An X  X dmittedly X  X ery simple similarity hash function h  X  , h  X  : { x 1 ,..., x n } X  N , could map each term vec-tor x on a single number h  X  ( x ) that totals the number of those terms in x starting with the letter  X  X  X . If h  X  ( x d 1 )= h assumed that d 1 and d 2 are similar.

Though this example is s imple, it illustrates the principle and the problems of hash-based similarity search:
If h  X  is purposefully designed and captures the gist of the feature vectors, search queries can be answered in virtually constant time, independent of the dimension of x .
First, we want to point out that hash-based similarity search is a space partitioning method. Second, it is interesting to note that, at least in theory, for a document set D and a similarity threshold  X  a perfect space partitioning for hash-based search can be stated. To make this plausible we have formulated hash-based similarity search as a set covering problem. This generic view differs from the computation-centric de scriptions found in the r elevant literature.
Consider for this purpose the R m being partitioned into over-lapping regions such that the similarity of any two points of the same region is above  X  , where each region is characterized by a unique key  X   X  N . Moreover, consider a multivalued hash func-tion, h  X   X  : R m  X  X  ( N ) , which is  X  X erfectly similarity sensitive X  with regard to threshold  X  . 1  X  d 1 ,d 2  X  D : Rationale and Utilization. h  X   X  assigns each feature vector membership set N d  X  X  ( N ) of region keys, whereas two sets, N d 1 ,N d 2 ,shareakeyiff x d 1 and x d 2 have a region in common. Figure 1, which is used later on in a different connection, serves as an illustration.

Based on h  X   X  we can organize the mapping between all region keys K , K := as a hash table h , h : K  X  X  ( D ) . Based on h the  X  -neighborhood D q of d q can be constructed in O ( | D q | ) runtime: 2
Observe that h  X   X  operationalizes both perfect precision and per-fect recall. For a set D that is completely known and time-invariant such a function may be found. However, in most cases the equiva-lence relation of Equation (1),  X   X   X  , cannot be guaranteed:  X  If  X  is not a conclusion of  X  , D q contains documents that do  X  If  X  is not a conclusion of  X  , D q does not contain all docu-
For the time being only the existence of such a partitioning along with a hash function is assumed, not its construction.
In most practical applications O ( | D q | ) is bound by a small con-stant since | D q || D | . The cost of a hash table access h (  X  ) is assessed with O (1) ; experience shows that for a given application hash functions with this property can always be stated [7].
Current research is concerned with the development of similarity hash functions that are robust in their behavior, efficient to be com-puted, and, most importantly, that provide an adjustable trade-off between precision and recall.
Our contributions relate to retrieval technology in general; they have been developed and analyzed with focus on text retrieval tasks under arbitrary classes of vector space models. In detail:
Despite the use of sophisticated data structures nearest neigh-bor search in D degrades to a linear search if the dimension of the feature vectors is around 10 or higher. If one sacrifices exactness, that is to say, if one accepts values below 1 for precision and re-call, the runtime bottleneck can be avoided by using hash-based search methods. These are specifically designed techniques to ap-proximate near(est) neighbor search within sublinear runtime in the collection size | D | .
Only few hash-based search methods have been developed so far, in particular random projection, locality-sensiti ve hashing, and fuzzy-fingerprinting [20, 18, 11, 26]; they are discussed in greater detail in Subsection 2.3 and 2.4.

As will be argued in Subsection 2.2, hash-based search meth-ods operationalize X  X pparently or hidden X  X  means for embedding high-dimensional vectors into a low-dimensional space. The in-tended purpose is dimension reduction while retaining as much as possible of the similarity information. In information retrieval em-bedding technology has been developed for the discovery of hid-den semantic structures: a high-dimensional term representation of a document is embedded into a low-dimensional concept space. Known transformation techniques include latent semantic index-ing and its variants, probabilistic latent semantic analysis, iterative residual rescaling, or principal component analysis. The concept representation shall provide a better recall in terms of the semantic expansion of queries projected into the concept space.

Embedding is a vital step within the construction of a similar-ity hash function h  X  . Unfortunately, the mentioned semantic em-bedding technology cannot be applied in this connection, which is rooted in the nature of the use case. Hash-based search focuses on what is called here  X  X pen X  retrieval situations, while semantic embedding implies a  X  X losed X  or  X  X emi-closed X  retrieval situation. Table 1: Classification of embedding paradigms used for index-ing in information retrieval.
 This distinction pertains to the knowledge that is compiled into the retrieval function  X  : Q  X  D  X  R ,where Q and D designate the computer representations of the sets Q and D of queries and documents respectively.
We propose the scheme in Table 1 to classify embedding meth-ods used in information retrieval. The scheme distinguishes also whether or not domain knowledge is exploited within the embed-ding procedure ( unbiased versus biased). E. g., locality sensitive hashing works on arbitrary data, while fuzzy-fingerprinting as well as shingling exploit the fact that the embedded data is text. A simi-lar argumentation applies to MDS and probabilistic LSI.

Aside from their restriction to (semi-)closed retrieval most of the embedding methods in the right column of Table 1 cannot be scaled up for large collections: they employ some form of spectral decom-position, which is computationally expensive.
We developed a unified view on hash-based search methods by interpreting them as instances of a generic construction principle, which comprises following steps: 1. Embedding. The m -dimensional feature vectors of the docu-2. Quantization. The real-valued components of the embedded 3. Encoding. From the k quantized components a single num-
Against the analysis presented in Section 3, the concept of opti-mality implied here must be seen more differentiated.

Some or all of these steps may be repeated for one and the same original feature vector x in order to obtain a set of hash codes for The next subsection exemplifies this construction principle for two hash-based search methods: loca lity-sensitive ha shing and fuzzy-fingerprinting. Subsection 2.4 explains the properties of hash-based search methods in terms of the precision and recall semantics.
Locality-sensitive hashing (LSH) is a generic framework for the construction of hash-based search methods. To realize the embed-ding, a locality-sensitive hash function h  X  employs a family of simple hash functions h , h : R m  X  N .From H  X  asetof k functions is chosen by an independent and uniformly distributed random choice, where each function is used to compute one com-ponent of the embedding y of an original vector x . Several hash families H  X  that are applicable for text-based information retrieval have been proposed [6, 8, 3]. Our focus is on the approach of Datar et. al. [8], which maps a feature vector x to a real number by com-puting the dot product a T  X  x . a is a random vector whose compo-nents are chosen from an  X  -stable probability distribution.
Quantization is achieved by dividing the real number line into equidistant intervals of width r each of which having assigned a unique natural number. The result of the dot product is identified with the number of its enclosing interval.

Encoding can happen in different ways and is typically done by summation; the computation of h (  X  )  X  for a set  X  of random vectors a ,..., a k readsasfollows: where c  X  [0 ,r ] is a randomly chosen offset of the real number line. A multivalued hash function repeats the outlined steps for different sets  X  1 ,..., X  l of random vectors.

Fuzzy-fingerprinting (FF) is a hash-based search method specif-ically designed for text-based information retrieval. Its under-lying embedding procedure can be understood as an abstraction of the vector space model and happens by  X  X ondensing X  an m -dimensional term vector x toward k prefix classes. A prefix class comprises all terms with the same prefix; the components of the embedded feature vector y quantify the normalized expected devi-ations of the k chosen prefix classes. 5
Quantization is achieved by applying a fuzzification scheme,  X  , which projects the exact deviations y 1 ,...,y k on r deviation inter-vals:  X  : R  X  X  0 ,...,r  X  1 }
Encoding is done by computing the smallest number in radix r notation from the fuzzified deviations; the computation of h a particular fuzzification scheme  X  readsasfollows: where y i is the normalized expected deviation of the i -th prefix class in the original term vector x . Similar to LSH, a multivalued hash function repeats the quantization and encoding steps for dif-ferent fuzzification schemes,  X  1 ,..., X  l .  X  -stability guarantees locality sensitivity [17, 23]. An example for an  X  -stable distribution is the Gaussian distribution.
For the normalization the British National Corpus is used as ref-erence. The BNC is a 100 million word collection of written and spoken language from a wide range of sources, designed to repre-sent a wide cross-section of current British English [2]. considered having a similarity of at least  X  . In the example h { h  X  ( x ) ,h
The most salient property of hash-based search is the simplifi-cation of a continuous similarity function  X  to the binary concept  X  X imilar or not similar X : two feature vectors are considered as simi-lar if their hash keys are equal; otherwise they are considered as not similar. This implication is generalized in Equation (1) at the out-set; the generalization pertains to two aspects: ( i ) the equivalence relation refers to a similarity threshold  X  ,and( ii ) the hash function h  X  is multivalued.

With the background of the presented hash-based search meth-ods we now continue the discussion of precision and recall from Subsection 1.1. Observe that the probability of a hash collision for two vectors x d 1 , x d 2 decreases if the number k of simple hash functions (LSH) or prefix classes (FF) is increased. Each hash func-tion or each prefix class captures additional knowledge of hence raises the similarity threshold  X  . This can be broken down to the following formula, termed Property 1:
Being multivalued is a necessary condition for h  X  to achieve a recall of 1 . A scalar-valued hash function computes one key for one feature vector x at a time, and hence it defines a rigorous par-titioning of the feature vector space. Figure 1 illustrates this con-nection: The scalar-valued hash function h (1)  X  responsible for the shaded partitioning assigns different keys to the vectors x d 2 , despite their high similarity (low distance). With the multi-valued hash function, h  X  = { h (1)  X  ,h (2)  X  } , which also considers the outlined partitioning, the intersection h  X  ( x d 1 )  X  is a monotonic relationship between the number of hash codes and the achieved recall, which can be broken down to the following formula, termed Property 2:
However, there is no free lunch, the improved recall is bought with a decrease in precision.
The embedding of the vector space model into a low-dimensional space is inevitably bound up with information loss. The smaller the embedding error is, the better are precision and recall of the con-structed hash function, because the affine transformation in Step 2 and 3 (cf. Subsection 2.2), which maps an embedded vector onto a hash code, is distance-preserving.

The section starts with a derivation of the globally optimum em-bedding under the cosine similarity measure, and then uncovers the inferiority of this embedding compared to the prefix class embed-ding of fuzzy-fingerprinting (Subsection 3.2). This observation is explained by the idea of threshold-centered embeddings, for which we introduce the formal underpinning in the form of new error statistics, called precision stress and recall stress at a given similar-ity threshold  X  . By extending the idea toward thresholded similarity matrices we show how optimum embeddings for similarity hashing in closed retrieval situations can be developed (Subsection 3.3).
Multidimensional scaling (MDS) designates a class of tech-niques for embedding a set of objects into a low-dimensional real-valued space, called embedding space here. The embedding error, also called  X  X tress X , is computed from the deviations between the original inter-object similarities and the new inter-object similari-ties in the embedding space.

Given n objects, the related similarity matrix, S , is a symmetric n  X  n matrix of positive real numbers, whose ( i, j ) -th entry quan-tifies the similarity between object i and object j . Let each object be described by an m -dimensional feature vector x  X  R m ,andlet X be the m  X  n matrix comprised of these vectors. 6
Without loss of generality we assume each feature vector x normalized according to the l 2 -norm, i. e., || x || 2 =1 . Then, under the cosine similarity measure, S is defined by the identity X T X ,where X T designates the matrix transpose of X .

An important property of the cosine similarity measure is that under the Frobenius norm an optimum embedding of X directly constructed from its si ngular value decomposition (SVD). With SVD an arbitrary matrix X can be uniquely represented as the product of three matrices: 7
U is a column orthonormal m  X  r matrix,  X  is an r  X  r diagonal matrix with the singular values of X ,and V is an n  X  r matrix. I. e.,
In IR applications X is the term-document-matrix. For applying an MDS only S must be given. Unique up to rearrangement of columns and subspace rotations. U T U = I and VV T = I where I designates the identity matrix. Using these properties the matrix S can be rewritten under both the viewpoint of its singular value deco mposition and the viewpoint of similarity computation:
 X V T represents a set of points with the same inter-object simi-larities as the original vectors X . The nature of the cosine similar-ity measure implies the direct construction of S and, in particular, the identities rank ( S )= rank ( X )= rank (  X V T ) .Conversely, if we restrict the dimensionality of the embedding space to k ,the resulting similarity matrix  X  S is also of rank k . According to the Eckart-Young Theorem the optimum rank-k approximation  X  S  X  S under the Frobenius norm can be obtained from the SVD of S restricting the matrix product to the k largest singular values [10]:
In the information retrieval community the embedding Y SVD  X  k V T k of document vectors X is known as representation in the so-called latent semantic space, spanned by k concepts. The em-bedding process became popular under the name of latent semantic indexing (LSI) [9].
 Remark 1. A common misconception is that LSI projects the docu-ment vectors into a subspace in order to represent semantic similar-ity. Rather, LSI constructs new features to approximate the original document representations. And, if the dimension of the embed-ding space is properly chosen then, due to the reduction of noise and the elimination of weak dependencies, this embedding is able to address retrieval problems deriving from the use of synonymous words. As a consequence the retrieval performance may be im-proved in semi-closed retrieval applications. Hofmann argues sim-ilarly [16]: the superposition princ iple underlying LSI is unable to handle polysemy.
Though the embedding Y SVD minimizes the embedding error of
X , it is not the best starting point for constructing similarity-sensitive hash codes. The main reason is that an MDS strives for a global stress minimization, while hash-based search methods con-centrate on the high similarities in S in first place. 8 Thenatureof this property is captured by the following definition, which relates the threshold-specific stress of an embedding to the statistical con-cepts of precision and recall. Figure 2 illustrates the definition. Definition 1 (precision stress, recall stress) Let D be a set of ob-jects and let X and Y be their representations in the n -dimensional and the k -dimensional space respectively, k&lt;n . Moreover, let  X  : X  X  X  X  [0; 1] and  X   X  : Y  X  Y  X  [0; 1] be two similarity measures, and let  X   X  [0; 1] be a similarity threshold.  X  defines two result sets, R  X  and  X  R  X  , which are comprised of those pairs { x i ,x j } , x i ,x j  X  D , whose respective representations in
X and Y are above the similarity threshold  X  :
Then the set of returned pairs from the embedding space,  X  R defines the precision stress at similarity threshold  X  , e Likewise, the set of similar pairs in the original space, fines the recall stress at similarity threshold  X  , e r  X  Remark 2. The precision stress and the recall stress of an em-bedding Y are statistics that tell us something about the maxi-mum precision and recall that can be achieved with similarity hash codes constructed from Y . The larger the precision stress is the higher is the probability that two embedded vectors, claimed being similar though their similarity in the original space,  X  X he similarity threshold controls the effective embedding error. X  This property complements the two properties of hash-based search methods stated in Subsection 2.4.
 some threshold the embedding of fuzzy-fingerprinting, Y FF s ij =  X  ( x i , x j ) , is low. Likewise, the larger the recall stress is the higher is the probability that two vectors in the original space, x , x j , are mapped onto different codes though their similarity, s is high.

For the three embeddings, Y SVD , Y FF ,and Y LSH , obtained from optimum MDS, fuzzy-fingerprinting, and LSH respectively, we have analyzed the precision stress and the recall stress at various similarity thresholds and with different corpora. The results reflect the predicted behavior: 1. Because of its generality (domain independence) the LSH 2. At some break-even point the retrieval performance of prefix
Figure 3 illustrates this behav ior for a sample of 2000 docu-ments drawn from the Reuters Corpus Volume 1 (RCV1) [24]. With other corpora and other parameter settings for the hash-based search methods this characteristic is observed as well. We analyzed in this connection also specifically compiled corpora whose simi-larity distribution is significantly skewed towards high similarities: Figure 4 contrasts the similarity distribution in the original Reuters Corpus (hatched light) and in the special corpora (solid dark). Remark 3. For most retrieval tasks an X  X ven high X  X recision stress can be accepted, since the necessary subsequent exact similar-ity analysis needs to be performed only for a very small fraction |
D q | / | D | of all documents. Remember that the construction meth-ods for the hash-based search methods provide sufficient means to fine-tune the trade-off between the precision stress, e p call stress, e r . Figure 4: Similarity distribution in the original Reuters Corpus and in the special compilations with increased high similarities.
Threshold-centered embeddings are tailored document models for special retrieval tasks such as near duplicate detection or high similarity search. They tolerate a large embedding error in the low similarity interval [0 , X  ] and strive for a high fidelity of similarities from the interval [  X , 1] . This principle forms the rationale of hash-based search.

With Y SVD , obtained by optimally solving an MDS, an embed-ding that minimizes the accumulated error over all similarities is at hand. We now introduce a threshold-optimum embedding, which minimizes the accumulated error with respect to the inter-val [  X , 1] . The presented ideas address the closed retrieval situ-ation in first place X  X or open retrieval situations the construction of an optimum embedding requires a-priori knowledge about the term distribution in the collection D . 9 Though the typical use case for hash-based search are open retrieval situations, the derivation is useful because ( i ) it provides additional theoretical insights and ( ii ) it forms a basis to reason about performance bounds.
The  X  -specific retrieval analysis of the preceding subsection sug-gests the construction principle of Y  X  . Instead of approximating the original similarity matrix S a  X  X hresholded X  similarity matrix S  X  is taken as basis, introducing this way the binary nature of simi-larity hashing into the approximation process. For a given threshold  X  the matrix S  X  is defined as follows: where f  X  ( s ) is a combination of two sigmoidal functions that de-fine an upper threshold  X  and a lower threshold  X  respectively. Sim-ilarity values from [  X  ;1] are amplified toward 1, similarity values from [0;  X  ) are moved toward  X  . The following rationale reveals the underlying trade-off: with increasing difference  X   X   X  the am-plification above  X  improves the robustness in the encoding step (cf. Subsection 2.2), with increasing  X  the contraction toward  X  re-duces the error in the embedding step and hence allows for shorter codes. f  X  can be realized in different ways; within our analyses two consecutive tanh -approximations with the thresholds  X  =0 . 1 and  X  =0 . 8 were employed.
Remember that Y FF is a domain-specific embedding which ex-ploits knowledge about document models and term distributions. Table 2: Results of a near-duplicate retrieval analysis, based on RCV1 and the experimental setup like before. The precision achieved with Y  X  outperforms even the Y FF embedding.

Since S  X  is a symmetric matrix it is normal, and hence its Schur decomposition yields a spectral decomposition:
Z is an orthogonal matrix comprising the eigenvectors of S  X  is a diagonal matrix with the eigenvalues of S  X  .If S  X  is positive definite its unique Cholesky decomposition exists:
X := Z T can directly be interpreted as matrix of thresholded document representations. As was shown in Subsection 3.1, the dimension of the embedding space, k , prescribes the rank of the approximation  X  S  X  of S  X  . Its optimum rank-k -approximation, is obtained by an SVD of S  X  , which can be expressed in the factors of the rank-k -approximated SVD of X .Let O X Q T be the SVD of
X and hence O T Remark 4. Y  X  :=  X  k Q T k is an embedding of X optimized for similarity hashing. Due to construction, Y  X  is primarily suited to answer binary similarity questions at the a-priori chosen thresh-old  X  .Since S  X  is derived from S by sigmoidal thresholding, the document representations in Y are insusceptible with respect to a rank-k -approximation. This renders Y  X  robust for similarity com-parisons under the following interpretation of similarity: where , denotes the scalar product. Ta ble 2 illustrates the supe-riority of Y  X  : For the interesting similarity interval [  X , 1] it outper-forms the classical embedding as well as the embedding strategies of sophisticated hash-based search methods.
 Remark 5. To obtain for a new n -dimensional vector x its optimum k -dimensional representation y  X  at similarity threshold  X  ,a k projection matrix P can be stated: y  X  = Px , where P is computed from X T P T = Y  X  T Remark 6. The transformations imply the thresholded similarity matrix S  X  being positive definite. An efficient and robust test for positive definiteness was recently proposed by Rumb [25]. If not positive definite it can be approximated by a positive definite matrix S  X  + , which should be the nearest symmetric positive defi-nite matrix under the Frobenius norm. As shown by Higham, is given by following identity [14]: where H is the symmetric polar factor of G .
Finally, this section demonstrates the efficiency of locality-sensitive hashing, fuzzy-fingerprinting, and hash-based search in general. We report results from a large-scale experiment on near-duplicate detection and plagiarism analysis, using a collection of 100 , 000 documents compiled with Yahoo, Google, and AltaVista by performing a focused search on specific topics. To compile the collection a small number of seed documents about a topic was cho-sen from which 100 keywords were extracted with a co-occurrence analysis [22]. Afterward, search engine queries were generated by choosing up to five keywords, and the highest ranked search results were downloaded and their text content extracted.

To render retrieval results comparable the two hash functions were parameterized in such a way that, on average, small and equally-sized document sets were returned for a query. As de-hash functions, which is done with the number of fuzzification schemes and random vector sets respectively: two or three differ-ent fuzzification schemes were employed for fuzzy-fingerprinting; between 10 and 20 different random vector sets were employed for locality-sensitive hashing. The precision of fuzzy-fingerprinting is controlled by the number k of prefix classes and the number r of deviation intervals per fuzzification scheme. To improve the preci-sion performance either of them or both can be raised. Note that k is application-dependent; typical values for r range from 2 to 4. The precision of locality-sensitive hashing is controlled by the number k of combined hash functions. For instance, when using the hash family proposed by Datar et al., k corresponds to the num-ber of random vectors per hash function [8]; typical values for k range from 20 to 100.

The plots in Figure 5 contrasts performance results. With respect to recall either approach is excellent at high similarity thresholds ( &gt; 0 . 8 ) compared to a linear search using a cosine measure. How-ever, high recall values at low similarity thresholds are achieved by chance only. With respect to precision fuzzy-fingerprinting is sig-nificantly better than locality-sensitive hashing X  X  fact which di-rectly affects the runtime performance. With respect to runtime performance both hashing approaches perform orders of magni-tude faster than a linear search . For reasonably high thresholds  X  the similarity distribution (Figure 4) along with the precision stress (Figure 3, left) determine a sublinear increase of the result set size |
D q | for a document query d q (Equation 2).
 Remark 7. The computation of the baseline relies on a non-reduced vector space, defined by the dictionary underlying D . Note that a pruned document representation or a cluster-based preprocessing of D , for example, may have exhibited a slower X  X ut yet linear growth. Moreover, the use of such specialized retrieval models makes the analysis results difficult to interpreted.
The paper analyzed the retrieval performance and explained the retrieval rationale of hash-based search methods. The starting point was the development of a unified view on these methods, along with the formulation of three properties that capture their design principles. We pointed out the sel ective nature of hash-based search and introduced new stress statistics to quantify this characteristic.
The concept of tolerating a large embedding error for small sim-ilarities while striving for a high fidelity at high similarities can be used to reformulate the original similarity matrix and thus to derive tailored embeddings in closed retrieval situations.

The presented ideas open new possibilities to derive theoret-ical bounds for the performance of hash-based search methods. Whether they can be used to develop better search methods is sub-ject of our research: by construction, Y  X  outperforms other em-beddings. It is unclear to which extent this property can be utilized in similarity search methods designed for open retrieval situations. The theoretical analysis of the trade-off between  X  and  X  as well as the Remarks 5 and 6 provide interesting links to follow. [1] R. Ando and L. Lee. Iterative Residual Rescaling: An [2] G. Aston and L. Burnard. The BNC Handbook. [3] M. Bawa, T. Condie, and P. Ganesan. LSH Forest: [4] A. Broder, S. Glassman, M. Manasse, and G. Zweig.
 [5] D. Cai and X. Hee. Orthogonal Locality Preserving [6] M. S. Charikar. Similarity Estimation Techniques from [7] T. Cormen, C. Leiserson, and R. Rivest. Introduction to [8] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. [9] S. Deerwester, S. Dumais, T. Landauer, G. Furnas, and [10] C. Eckart and G. Young. The Approximation of one Matrix [11] A. Gionis, P. Indyk, and R. Motwani. Similarity Search in [12] X. He, D. Cai, H. Liu, and W.-Y. Ma. Locality Preserving [13] M. Henzinger. Finding Near-Duplicate Web Pages: a [14] N. Higham. Computing a Nearest Symmetric Positive [15] G. Hinton and R. Salakhutdinov. Reducing the [16] T. Hofmann. Unsupervised Learning by Probabilistic Latent [17] P. Indyk. Stable Distributions, Pseudorandom Generators, [18] P. Indyk and R. Motwani. Approximate Nearest Neighbor  X  [19] I. Jolliffe. Principal Component Analysis . Springer, 1996. [20] J. Kleinberg. Two Algorithms for Nearest-Neighbor Search [21] J. Kruskal. Multidimensional Scaling by Optimizing [22] Y. Matsuo and M. Ishizuka. Keyword Extraction from a [23] J. Nolan. Stable Distributions X  X odels for Heavy Tailed [24] T. Rose, M. Stevenson, and M. Whitehead. The Reuters [25] S. Rump. Verification of Positive Definiteness. BIT [26] B. Stein. Fuzzy-Fingerprints for Text-Based IR. In Proc. of [27] B. Stein and S. Meyer zu Ei X en. Near Similarity Search and [28] R. Weber, H. Schek, and S. Blott. A Quantitative Analysis [29] H. Yang and J. Callan. Near-Duplicate Detection by
