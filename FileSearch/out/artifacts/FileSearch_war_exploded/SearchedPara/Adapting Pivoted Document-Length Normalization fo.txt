 AND 1. INTRODUCTION In both academia and industry, the vector sp ace model (VSM) is one of the most popular and widely used information retrieval (IR) models, providing a basis for measuring and visualizing the similarities among queries and their corresponding documents. Compared to Boolean models, VSM can more readily handle natural language queries. It does so by indexing the query as a set of query terms w ith associated query term weights related to how frequently a query term appears in a query. Previous research investigated two major issues associated with the VSM: how term weights can be transformed into a suitable vector space and how the similarity score should be normalized with respect to different document sizes. The transformation issue has been addressed by using term weights that are weighted by the inverse document frequency d j for the j -th term and by the j -th term impact with different document sizes, it naturally raises our second issue, namely document-length normalization. was addressed with what we classify as four different experimental approaches. In the first approach, Lee et al. [1997] compares the inner product (i.e., no normalization), cosine, and jaccard coefficients , and found that the cosine similarity scores produced better results. However, if term weights (such as TF-IDF weights) are determined by using inverse document frequency (IDF), cosine normalization is determined by the IDF value of each term. Such IDF values are not available until all documents are indexed. Determining cosine normalization makes incremental updates troublesome. Cosine normalization favors short documents [Singhal et al. 1996], but has a negative impact on retrieval effectiveness. The se cond approach is to normalize the impact of term frequency on the weight of the term as a whole. The Smart system X  X  [Salton and Buckley 1988] term frequency factor and the frequency weight s of the Inquery system [Allan et al. 1997] are examples of normalizing term frequency w ith respect to the maximum term frequency in the same document, thus indirectly addressing the document-length normalization problem. The third approach, as in the Ok api system [Robertson et al. 1994], uses document byte length for normalization to address the problem of ranking long documents higher than desired, since long documents have higher term frequencies and more terms that match a query. Finally, Singhal et al. [1996] suggests a novel document-length normalization that linearly transforms the cosine normalization value of the document. Linear transformation is based on comparing the required document-length normalization using relevant documents to the actual cosine normalization values of the retrieved documents; this approach turns out to be similar to the BM-25 term frequency factor [Robertson et al. 1994]. Since linear transformation (i.e., a straight line) is normalization . Typically, the average document lengt h is chosen as the pivot, and the can improve retrieval performance by as much as 15% to 20%. comparisons [Singhal et al. 1996; Nie et al. 2000] being made among various indexing strategies (e.g., character, word, and bigram [Nie and Ren, 1997]). Luk and Kwok [2002] recently compared the retrieva l effectiveness of different retrieval models and Chinese indexing strategies. They found that bigram indexing and the 2-Poisson model [Robertson and Walker 1994] were a robust and effective combination of indexing strategies and retrieval models; Luk and Kwok [2002] also found that VSM was slightly faster than the 2-Poisson model, PIRCS retr ieval [Kwok 1995], or the logistic regression the least effective retrieval model was the VSM, probably due to the use of cosine normalization. Chinese IR evaluation in TREC [Buckley et al. 1996; Nie et al. 1997] and NTCIR [Yang and Ma 2003; Zhang, et al. 2003; Juang and Tseng 2003] the retrieval implementation differences. The researchers used byte-length normalization [Juang and Tseng 2003], cosine normalization [Yang and Ma 2003], and other modified forms [Zhang et al. 2003], but rarely used pivoted unique document-length normalization before NTCIR-4. Japanese information retrieval extensively; Savoy and his co-worker have shown that pivoted document-length normalization is eff ective for Chinese document retrieval in the NTCIR-4 and NTCIR-5 collections [Savoy 2005b; Abdou and Savoy 2005; Savoy 2005a]. However, the title queries for these two collections are specified using delimited terms, whereas title queries for NTCIR-3 are unse gmented. In addition, no statistical tests were carried out to confirm that the perfor mance of pivoted unique normalization was the statistical significance tests, and we ca n confirm that pivoted (unique) document-length normalization is effective for the NTCI R-3 collection. Other current problems in using VSM for Chinese information retrieval follow: to review and evaluate various pivoted document-length normalizations. The evaluation uses three common indexing strategies: char acter, word, and bigram. In Section 3 we examine how the pivot and the slope of the pivoted document-length normalization can be determined automatically for each query. We found that the best slope for pivoted document-length normalization depends on the number of unique query terms (i.e., on query size). Hence we propose some new adaptive pivoted document-length normalizations and evaluate them via different test collections (i.e., TREC-5, TREC-6, and NTCIR-3) to examine their generality. In Section 4, in order to demonstrate the competitiveness of the VSM, we investigat e whether pseudo-relevance feedback (PRF) can enhance the retrieval effectiveness of the VSM using the adaptive pivoted document-length normalization. The last section concludes our discussion. 2. DOCUMENT-LENGTH NORMALIZATOIN Singhal et al. [1996] focused on the problems of normalizing cosine similarity because it is widely used in information retrieval. They observed that although it is more likely that a larger document will be more relevant than a smaller one, since it is more likely to find more terms to match it, their cosine normalization using the Euclidean document-length ||D i || 2 usually penalized large documents. By plotting a scatter diagram illustrating the likely relevance of documents of various lengths, Singhal et al. [1996] found that the cosine normalization curves do not match the curve for the likelihood that a document will be relevant. Further, they showed that when the transformed cosine normalization effectiveness was better than cosine normalization. A plausible explanation for this interesting outcome is that the retrieval list can be regarded as a random sample of a set of documents. Hence, if the distribution over the document length of this random sample does not match the underlying distribution over the relevant document lengths, we can conclude that the random sample is biased at certain document lengths. If this is correct, we can expect that within Chinese information retrieval the distribution of the relevant documents over different document lengths should match the distribution of the retrieved documents over different lengths, too. To validate this, we plotted the scatter diagram (Figure 1) of the distribution of relevant and retrieved documents across different document lengths using the NTCIR-3 test collection with 42 concept queries similar to documents across different lengths and the distribution of retrieved documents based on retrieved documents based on pivoted normalization using average document length as the pivot and a slope value of 0.2. Figure 1 shows that the polynomial fitted curve of the pivoted unique normalization is more visually similar to the curve based on the (rigid) relevance judgment than to cosine normalization. 2.1 Pivoted Normalization Singhal et al. [1996] proposed two types of normalization: pivoted cosine and pivoted unique. Both are based on the idea that the original document length is linearly transformed into another document length. The linear transformation is specified using two parameters, namely, the slope and the pivot . Pivoted normalization is generalized into a generic function, norm ( . ) that is specified differently for the two different pivoted normalization methods (i.e., cosine and unique pivoted). We will start with the basic VSM. query vector q , using the following formula: of the given vector. Specifically, the Euclidean length of D i is ranking of the documents in the retrieval list. Hence, we define a modified similarity score sim c ( . , . ) as norm ( . ), and by formulating the normalized term weight, n i,j , as follows: applications, q j may just indicate the presence or absence of the j -th query term, or it may defined as collection, and d j is the number of documents that contain the j -th term. Note that when the query term does not occur in any document, it is ignored. In previous research (e.g., were several document term weights ( w i,j ), and several normalization functions ( norm ( . )). We examine these in the rest of this section. for the document vector D i is equal to  X  i,j , which is defined as change when the index is updated, the document length cannot be precomputed for dynamic collections. Alternatively, we equate w i,j to an alternative term weight  X  i,j which is only a function of the term frequencies, that is, The cosine normalization using  X  i,j of this alternative is called cosine normalization without IDF. The query term weight, q j , of cosine normalization without IDF is the same as in Eq. (2), since the IDF for query terms can be updated quickly. pivot and the slope. The pivot specifies the i nvariant point of the linear transformation of the normalization. The slope controls the gradient of the linear transformation of the normalization. If the slope is large, the impact of the linear transformation will be greater. The normalization function, norm c ( . ), of pivoted cosine normalization with or without IDF is where the pivot  X  c is typically chosen as the m ean document length, that is, || D i || 2 in norm c ( . ), and the pivot is the Euclidean document length that depends on whether w Singhal et al. [1996]. frequency in the document and that of the aver age term frequency in the document, that is, modified slightly, as follows: pivot  X  u is chosen correspondingly as the mean number of unique terms in a document, that is, and the slope is al so set to 0.2. slopes are assumed to be 0.2, as recommended in Singhal et al. [1996]. Note that document length || D i || 2 is determined using the corresponding term weights in the document-length normalization. For example, the term weight  X  i,j is used as w i,j in Eq. (1) IDF. The table does not show the j -th query term weight q j , since it is the same (i.e., Eq. (2)) for different document-length normalizations. 2.2. The Experimental Set-Up We used three common Chinese indexing strate gies to evaluate pivoted document-length normalization. The strategies were used in open IR evaluation workshops (e.g., TREC and NTCIR). Stop words were filtered before inde xing; the stop word list used here is the same as the one in Luk and Kwok [2002]. Th e first strategy, Chinese character indexing, indexes every Chinese character, apart from stop words. The second strategy, bigram indexing, indexes every overlapping consecu tive sequence of two Chinese characters, except when the sequence matches a stop word . In the third strategy, we used the ROCLING word list [Chen and Huang, 1993] to match the text from beginning to end. indexed, and the next position is the current position plus the length of the matched word. If the match word is a stop word, it is not indexed; if no words are matched, it is assumed that the matched word is the current single character. This kind of greedy matching algorithm is known as the maximal forward-matching algorithm [Kit and Liang 1989]. Table II summarizes the storage costs and th e document length statistics of the three strategies. The index size is the storage for our extensible inverted index, which does not store any position information. The document le ngth is measured in terms of the number of index terms in a document. 2001] to evaluate Chinese information retrieval. The data together with the directory structure occupies 747M bytes of storag e (measured using the du UNIX utility). The documents were published by five Taiwanese publishers and encoded in Big5. There were 42 topics for evaluation, and each topic had four types of queries: title (T) queries, description (D) queries, concept (C), queries and narrative (N) queries. According to Voorhees and Buckley [2002], a 6% difference in the mean average precision (MAP) is a potentially meaningful difference result, evaluated using 42 random topics. We used the Wilcoxon matched-paired signed-ranks test to obtain statistical significance results because it is a nonparametric test that do es not require any assumptions about the underlying distribution of the MAP values. The long queries used for the evaluation were formulated by concatenating all the differen t query types (i.e., T, D, C, and N). The NTCIR-3 provides two grades of relevance, rigid and relaxed; we used the rigid relevance grade in this article. 2.3 Results Table III shows the mean average precision (MAP) of the VSM using different document-length normalization methods for title (short) and long queries. The common 
Dictionary size (MB) 84 8 2 index size (MB) 1,329 627 595 total (MB) 1,413 635 597 
Relative storage (with respect to the amount of storage for documents) 189% 85% 80% Minimum Document Length 8 9 9 Maximum Document Length 10,752 8,695 12,102 Mean Document Length 482 378 538 Document Length Standard Deviation 298 239 334 VSM uses cosine normalization with IDF document term weighting. The MAPs of the VSMs with cosine normalization are similar to those in the NTCIR open evaluation workshop. For example, MAP results using bigram indexing are 0.163 for title queries and 0.239 for long queries (i.e., TDCN). These results are similar to those using bigram indexing and the VSM with cosine normalization (0.17 for title queries and 0.23 for long queries [Luk 2003]). Similar MAP results we re obtained even though the common VSM [Luk 2003] in the NTCIR-3 workshop used the query term frequency for q j , whereas the VSM here defined q j using Eq. (2). higher than the corresponding MAPs that do not use it, with different indexing strategies cosine normalization for long queries. The MAP differences between the pivoted document-length normalization and the corresponding cosine normalization without pivoted normalization and without using IDF are statistically significant at p  X  0.01, Interestingly, the pivoted cosine normalization with IDF document term weighting results in MAPs lower than the corresponding perfor mance of the pivoted cosine normalization without adding IDF document term weighting. other normalizations with the same indexing strategy for the same type of query, except for the cases using pivoted cosine normalization without IDF, combined with word or bigram indexing for title queries. Pivoted unique normalization enables statistically significantly better performance than MAPs w ithout it, except for character indexing for long queries. Pivoted unique normalization X  X  re trieval effectiveness is similar to the best performance of the 2-Poisson retrieval model with bigram indexing (21% vs. 21% for title queries and 30% vs. 29% for long queries in Luk [2003]). Like English ad hoc retrieval, the VSM X  X  performance with pivoted unique normalization is similar to the 2-Poisson model. It is also important to examine whether the VSM X  X  retrieval speed with different document-length normalizations is as fast as, if not faster than, the 2-Poisson model. Figure 2 shows the scatter diagram for the retr ieval time of each query, measured against the number of unique query terms in that query (as in Vines and Zobel [1999]). Retrieval retrieval models that use various document-length normalizations. Hence it is sufficient to examine the average retrieva l time of a query in order to compare the retrieval speeds of the various retrieval models that use different document-length normalizations. normalization. Here the VSM with different no rmalizations appears to process as quickly lower the retrieval speed of the VSM. 3. ADAPTIVE PIVOTED DOCUME NT-LENGTH NORMALIZATION Adaptive pivoted (document-length) normalization is based on the idea that its parameters can be determined or adapted accordi ng to the kind of queries that are entered values. So we proposed that the new similarity score, sim a ( . , . ), becomes This adaptive pivoted normalization is different from those based on the query size because they (e.g., Zhang et al. [2003]) simp ly add a separable mathematical term in the similarity calculation. Here the query size can change the pivot and the slope of the pivoted normalization. Once the slope and pi vot are estimated using query size, document similarities are calculated in the same manne r as pivoted document-length normalization. Therefore, the retrieval speed of the VSM using adaptive document-length normalization is almost the same as the VSM using the fixed pivoted document-length normalization. We carried out some experiments to estimat e the best pivots and evaluated the adaptive pivoted normalization with the estimated pivot. However, we found that MAP performance appeared to be sensitive to errors in the slope estimation when the pivot varied. So our adaptive scheme uses the fixed pivot and only estimates the best slope according to query size. 3.1 Slope Estimation According to Figure 3, the best slope values appear to depend on query size. The long queries have larger best slopes than short queries do. The fitted curves for the best slopes and query sizes are all logarithmic curves with a squared correlation value above 0.5. It is square correlation values of the fitted curves are high (i.e., &gt; 0.74), except for the pivoted unique normalization with word indexing. We also put the estimated slope X  X  lower and upper bounds in order to ensure that the MAP performance is more robust. In this article Pivoted Normalization Unique Cosine without IDF Cosine with IDF the lower bound of the slope is 0.05; the upper bounds of the slope vary with the indexing strategies and the type of pivoted document-length normalization. The upper bounds are tabulated in Table V. 3.2 Results Table VI shows the adaptive pivoted normalization X  X  MAP performance using various indexing strategies, as well as the im provement in MAP pe rformance over the corresponding original pivoted normalization using fixed parameter values (i.e., the relative pivot value is 1.0 and the slope is 0.2). We show in Table VI that adaptive pivoted normalization does not always im prove MAP performance compared to the corresponding fixed pivoted normalization. However, when the adaptive pivoted normalization performed worse than the corresponding pivoted normalization, the reductions in MAP were less than 0.6 percentage points, and are not statistically significant. sometimes significantly improve the MAP of (fixed parameter) pivoted normalization. In Table VI. MAP (%): Comparing VSM with Fixed and Adaptive Pivoted Document-Normalization Unique Cosine without IDF Cosine with IDF Mean Query Size one rare case (i.e., adaptive pivoted cosine normalization without IDF using character corresponding MAP achieved using cosine normalization for both title and long queries and for all the indexing strategies . It appears that adaptive pi voted unique normalization can reduce MAP sensitivity to the improvements in pivoted unique normalization. 3.3 Simplified Chinese Document Retrieval effectiveness because the parametric equations are derived from the same set of queries used to evaluate adaptive pivoted normalization. It is not clear whether the same kind of performance enhancement could be obtained with a different set of queries or with a different set of data collections. Therefore, we used two different sets of test collections (i.e., TREC-5 and TREC-6) for ad hoc Chinese information retrieval to examine the wider applicability of adaptive pivoted normalization. The TREC-5 and TREC-6 test collections are chosen because the document s are encoded in GB, which consists of simplified Chinese characters instead of Big5, which represents traditional complex Chinese characters. In addition, the docum ents in NTCIR-3 ar e produced by five Taiwanese (news) publications , whereas the documents in TREC are taken from two (news) publications on the Chinese mainland. So we expected that the content in the TREC test collections would be substantially different from that in NTCIR-3 test collections. The other test collections (TERC-9 and NTICR-2) have Big-5-encoded documents, which might overlap with the NTCIR-3 test collection, and so are not used here. The TREC-5 and TREC-6 test collections contain news articles similar to the NTCIR-3 test collection. The number of queries for TREC-5 and TREC-6 are 26 and 28 respectively, which are smaller than the number of queries for NTCIR-3 (i.e., 42). fixed parameter values for different queries and the adaptive pivoted (document-length) normalization are shown in Table VII. In addition, the best 11-point average precision performances using the BM11 weighting schemes derived from the 2-Poisson model of our retrieval system [Luk and Wong, 2003] is also included, together with the best 11-point average precision performances reported in the TREC Chinese ad hoc information retrieval [Luk and Kwok 2002]. performance over the corresponding pivoted unique normalization that uses fixed parameter values. There are only two (under lined) entries in Tabl e VII where the MAPs of the adaptive pivoted normalization are lower than the corresponding MAPs of pivoted normalization. For these two entries, the MAPs of adaptive pivoted normalization are reduced by no more than 0.2 percentage poin ts, compared to the corresponding (fixed parameter) pivoted normalization. The decreases in MAP are not statistically significant at p  X  0.01. For the other entries in Tabl e VII, the MAPs of adaptive pivoted normalization are the same or higher than those of the corresponding (fixed parameter) Table VII. MAPs (%): Comparing Adaptive Pivoted Normalization (A) to Original Fixed Parameter Value Pivoted Normalization (F) for the TREC-5 and TREC-6 Test Collections pivoted normalization. Twelve entries in Tabl e VII (marked with ^) registered that the MAPs of adaptive pivoted norma lizations are statistically si gnificantly higher than the MAPs of corresponding (fixed parameter) pivoted normalization, at p  X  0.01. Four out of twelve entries in Table VII show that MAP improvement using adaptive pivoted normalization is larger than ten percentage points, compared to using pivoted queries where the original MAPs of the pi voted normalization are significantly lower than the other corresponding pivoted normalizations. one set of 54 queries using the same TREC-5 and TREC-6 simplified Chinese document collection. This provides more queries as samples for performing statistical significance tests and simplifies the comparison between different types of pivoted normalization. the adaptive pivoted normalization are lower than that of the corresponding fixed pivoted normalization. The decrease in MAPs is only 0.1 of a percentage point, and is not statistically significant at p  X  0.01. Otherwise, the MAPs of the adaptive pivoted normalization are the same or higher than the MAPs of the corresponding pivoted normalization. Eight entries in Table VIII show that the MAPs of the adaptive pivoted normalization are statistically significantly higher than those of the corresponding pivoted normalization at p  X  0.01. 4. PSEUDO-RELEVANCE FEEDBACK In open evaluation workshops, pseudo-relevance feedback (PRF) [Ballesteros and Croft 1997] has been used extensively to enhance retrieval effectiveness. PRF expands the initial query with terms that are assumed relevant and are selected from the top K retrieved documents. Here the VSM uses both PRF and pivoted document-length normalization to examine whether retrieval effectiveness can be enhanced further. are used in this evaluation, and more query types are examined: title (T), description (D), concept (C), title and concept (TC), and long queries (TCDN). We used bigram indexing storage-efficient. The PRF setting is similar to the 2-Poisson model in Luk and Wong 140 unique terms are extracted from the top six documents. The terms are ranked by the product of total term frequency, document frequency in the top six documents, and inverse document frequency. length normalization with PRF and bigram indexing. The 2-Poisson model with PRF and bigram indexing [Luk and Wong 2003] is included in the table for comparison because it is one of the competitive retrieval models and one of the weighting schemes used in the open NTCIR-3 Chinese IR evaluation. Both the fixed and the adaptive pivoted normalizations with PRF obtained similar MAPs to the corresponding 2-Poisson model performed favorably compared with the corresponding MAP of the 2-Poisson model. Table IX show any statistical significant differences at p  X  0.01. However, if the statistical significance is lowered to p  X  0.025, then the adaptive pivoted unique normalization at p  X  0.025 for title queries. It seems that adaptive pivoted unique normalization can improve MAP performance for the retrieval models for title queries. Also, the adaptive pivoted unique document-length normalization performed the best of the other two adaptive pivoted document-length normalizations for all query types. 5. ENGLISH DOCUMENT RETRIEVAL We are interested in whether adaptive pivoted unique document-length normalization can also enhance the retrieval effectiveness of ot her languages. Because English is a popular language and is substantially different from Chinese, we examined English ad hoc retrieval. We used the TREC-6, TREC-7, and TREC-8 English ad hoc retrieval test collections for our evaluation [Voorhees and Harman 1997]. There are 50 queries for each test collection. compared to NTCIR and TREC Chinese news articles. In particular, the Congressional Record (CR) and the Federal Register documents are long compared to news articles in test collections (e.g., the Financial Times ; see Voorhees and Harman [1997] for details). Table X shows document length statistics (the number of index terms) in the test collections used here. The document length X  X  standard deviation is about two or more variations and/or there is a significant group of long documents in the collection, suggesting that document-length distribution may not be unimodal. The documents in the TREC-7 and TREC-8 test collections are the sa me as in TREC-6, except that they do not contain any CR documents. Subtracting the CR documents causes a significant drop in the standard deviation in document length, from 1,275 to 667. However, the average document length only dropped slightly, from 316 to 294, indicating that the pivot may have not changed that substantially. The range in document length is quite large, from 6 to about 300,000 index terms. Table XI shows MAPs for adaptive pivoted unique normalization and the fixed parameter pivoted document-length normalization for the title and long queries in TREC-6, TREC-7, and TREC-8. Since English documents do not have character and bigram indexing, we were interested to know which estimated best slopes using different Chinese indexing strategies would be appropriate for English document retrieval. document-length normalization at p  X  0.01 is statistically significantly different from the MAP of the corresponding cosine normalization (labeled  X  X one X  in Table XI). For title queries, the MAPs of all the adaptive pivoted unique normalizations in Table XI are statistically significantly higher than those of the fixed pivoted unique normalization at p  X  0.01. It appears that the estimated best slopes using character indexing gave a slightly better performance for title queries. For long que ries, there appears to be no statistically significant difference at p  X  0.01 between the MAP of fixed pivoted normalization and that of the corresponding adaptive pivoted no rmalization. It appears that the adaptive pivoted normalization can enhance the MAP for short queries, without hurting the MAP for long queries for the refe rence TREC test collections. 6. CONCLUSION AND DISCUSSION Based on our evaluation, we confirm that pivoted document-length normalization is effective in enhancing the performance of the vector space model (VSM) for Chinese information retrieval, evaluated using traditi onal Chinese character text in NTCIR-3 and simplified Chinese character text in TREC-5 and TREC-6. The pivoted document-length normalization can improve the effectiveness of all Chinese indexing strategies in this article, compared to cosine normalization without any pivoted normalizations. In particular, we found that pivoted unique document-length normalization was found to be effective more often than the other two normalization methods because (1) it obtains model (i.e., the 2-Poisson model); (3) it can be used with pseudo-relevance feedback (PRF) to obtain the best retrieval effectiven ess, compared to other normalizations with PRF; and (4) it can be updated incrementally to support incremental indexing that is more costly for pivoted cosine normalization with inverse document frequency weighted distances. best slope using the query size. This adaptive scheme is able to enhance the retrieval Chinese document retrieval. In general, the adaptive scheme sheds some light on the fact that query size might be one of the factors to consider in designing retrieval models. We suspect that adaptive document-length normalization may be useful for other languages as well, particularly those in the same lan guage families as Chinese and English. If VSM is used, then the adaptive pivoted unique document-length normalization might be a good choice for effective retrieval. ACKNOWLEDGEMENT We thank the anonymous reviewers and Noriko Kando for improving the paper, and ROCLING for providing the Chinese word list. Robert thanks the Center for Intelligent Information Retrieval, University of Mass achusetts, for facilitating him to develop the basic IR system there. REFERENCES 
