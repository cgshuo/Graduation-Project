
For simplicity, through the paper, we will use l-dimensional data and equations for illustration, how-ever our approach can be easily generalized to any di-mensions. For convenience, we define some standard notations, except where otherwise stated. We assume whose underlying density function f(t) is to be esti-mated. The symbol p(x) is used to denote the estima-tion of f(z). Generally s means s- X ,, and E(z) means the expected value, or mean, of variable x. parametric and nonparametric. The parametric ap-proach assumes that f(z) belongs to some paramet-ric family of distributions, such as the gamma fam-ily. The main drawback is that estimation accuracy depends heavily on the assumption. The nonparametric approach does not assume any pre-specified functional the CF tree), and we use the sum of all the CF-kernel functions to estimate the overall data distribution. We call it the CF-kernel method and demonstrate that 1) it scans the dataset once, and afterwards it improves the space/time complexity from O(n) to O(m), where m is the number of subclusters; 2) The estimation obtained from the CF-kernel method approximate that obtained from the kernel method, and the approximation can be made as accurate as desired by increasing available memory. 
The rest of the paper is organized as below: Sec. 2 gives a review of the CF-tree. Sec. 3 describes the details of the CF-kernel method. The performance results are presented in Sec. 4, and the final comments and conclusions are given in Sec. 5 and Sec. 6. The details of CF and CF tree, and relevant insertion and rebuilding algorithms were introduced in [ZRL96]. Here we provide a brief review. CF is a triplet summarizing the information that we maintain about a cluster. That is, CF = (N, L&gt;, SS), where N is the number of data points in the cluster, LS is the linear sum of the N data points, i.e., Cy=, X ??i, and SS is the square sum of the N data points, i.e., xi X ==, zi2. With CF X  X  of clusters known, we can calculate the mean and standard deviation of each cluster, or various distances between clusters very easily. 
A CF tree is a height-balanced in-memory tree with two parameters: branching factor B and threshold T. Each nonleaf node contains at most B, entries of the pointer to its i-th child node, and CFs is the CF of the subcluster represented by this child. So a nonleaf node represents a cluster made up of all the subclusters represented by its entries. A leaf node contains at most In addition, each leaf node has two pointers,  X  X rev X  and  X  X ext X , which are used to chain all leaf nodes together for efficient scans. A leaf node also represents a cluster made up of all the subclusters represented by its entries. But all entries in a leaf node must satisfy a threshold requirement with respect to a threshold value T. In this paper, we use the threshold restrict that the standard 
A CF tree has been shown to be a compact summary of the dataset because each leaf entry is not a single data point but a subcluster (which absorbs many data points with its standard deviation under a specific threshold T). Such a CF tree can be built dynamically as new data objects are inserted. That is, a new insertion is guided into the correct subcluster in a CF tree leaf node for clustering just as a new insertion is guided into the correct position in a B+ tree for sorting. When as well as the empirically-assumed density function of subcluster i, gi(x). Under the distribution assumption in subcluster and with the definition of the CF-kernel function, the following properties of (X,(x), as well the theorem about fin compared with f;(x) can be proved easily: 1) If K(z) and gi(z) are density functions, then C&amp;(x) is a density function too. 2) If K( ) 2 and gi(x) are bounded, then CKi(z) is bounded too. 3) If K(z) is symmetric, and gi(z) is symmetric, then (Xi(x) is symmetric too. 
Equivalence Theorem: The kernel estimation &amp;(x), as shown in equ.(l), and the CF-kernel estimation f&amp;(z), as shown in equ.(2), are statistically equivalent. That is, E(f;((e) -f&amp;(x)) = 0. 
The following conclusions about memory (i.e., num-ber of subclusters) versus accuracy also follow: 1) If m = n, that is the memory is large enough for each subcluster to contain only one data point, then the distribution assumption in subcluster is not needed at all because no information is lost, and the CF-kernel method is exactly the same as the kernel method i.e., purely nonparametric method. 2) If m = 1, that is the memory is so limited that we have to collapse all n data points into a single subcluster, then the distribution assumption in subcluster becomes extremely critical because it is exactly the density estimation we are looking for. 
All information except the CF triplet is lost due to the limited memory. As a result, the CF-kernel method becomes too parametric to be useful. 3) If m is some reasonable value between 1 and n so that the distribution assumption in subcluster be-comes more acceptable, the CF-kernel method and the kernel method should have almost the same accuracy as we will show experimentally. How to compute CK,(x) efficiently is the key to the 
CF-kernel method. From the definition, it relies on the forms of K(x) and gi(t). As shown in [WJ95] that the choice of K(z) is not critical to the accuracy, in this section, we choose the standard normal density function for K(x), i.e., K(x) = &amp;e-G, and hence K,(x-t) = memory is reasonably large, the choice of gi(t) becomes not critical either. So here as an example and as default, we assume a normal distribution to approximate gi(t), i.e., gi(t) x *e standard deviation &amp;i can be computed from the CF of the subcluster, which is maintained in the CF tree. 
However, one can also assume other distributions to approximate gi(z), such as uniform distribution. As we will show experimentally later that when the memory DS CF tree is similar to those used in Phase 1 of BIRCH [ZRL96] except that the outlier handling option is turned off for fair comparisons with the kernel method. Among them, the memory size is set to about 5% of the dataset size, this size is used as default unless otherwise stated in the experiments. 
Metrics: To compare two density estimations on the same dataset, say &amp;(x) and f&amp;(x), we 1) take 1000 breakpoints uniformly from the whole data range; 2) define the relative density difference D(f) as z(frc(l)-fCh.(l)l; 3) over the 1000 breakpoints, compute (fK(~)+fCK(z)) the average of the absolute values of O(f^) X  X , and denote it as Is(p). This average difference is used as a simple indicator of overall difference between the two estimations. All the times are measured in seconds and memory are measured in kbytes. 
Kernel versus CF-kernel: In this section we compare the performance of the kernel method and the CF-kernel method in terms of their running times, memory requirements, and estimation difference o(f). From Table 2, we can see that for all four datasets: 1) the kernel method runs much slower (more than 150 times) than the CF-kernel method does; 2) the kernel method uses much more memory (20 times) than the CF-kernel method does. difference B(f) b t CF-kernel estimation is very small and can be almost ignored. Fig. 2 plots the density estimation obtained with the CF-kernel method on the 1000 breakpoints over the data range for all four datasets. Visually, they look very similar to those obtained with the kernel method shown in fig. 1. So the CF-kernel method improves the time/space efficiency dramatically while keeping the estimation accuracy comparable with the kernel method. gi(z) Effects: In th is section, for all four datasets, we will compare two CF-kernel estimations with all other settings the same except that one assumes gi(z) as a normal distribution whereas the other assumes gi(z) as a normal distribution. Table 3 shows that: 1) The average difference fi(f^) between them is very close to zero. This confirms that, with respect to accuracy, assuming gi(z) as uniform or assuming gi(Z) 
Figure 4: &amp;f) between &amp;(z) and f&amp;(s) versus m as memory increases. 5 Compared with Histogram 
Another widely-used nonparametric method for den-sity estimation is the histogram method[Sil86, Dev87, 
WJ95]. Assume the data range is known, and cut into disjoint and contiguous intervals (i.e., bins of equal or different widths), the histogram estimation of f(2) is defined as: 
The main drawbacks of the histogram method are: 1) curse of dimension[Sil86]; 2) the data range must be known in advance in order to allocate the bins effectively, so it is not an incremental method; 3) 
The discontinuity of f;(z) makes the derivatives or other smoothing metrics unavailable; 4) the estimation accuracy depends not only on the bin widths but also on the bin locations. problems encountered by histogram techniques. As mentioned in section 1, it has a lot of advantages over histogram techniques. In this paper, our major 
