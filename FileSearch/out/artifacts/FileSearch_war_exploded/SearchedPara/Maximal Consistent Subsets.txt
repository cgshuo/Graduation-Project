 San Diego State University
Default unification operations combine strict information with information from one or more can be added to it without creating an inconsistency. Although this problem is NP-complete, the search space. In this article, we propose a novel optimization, leaf pruning, which in some cases yields an improvement in running time of several orders of magnitude over previously range of problems and applications. 1. Introduction
In unification-based grammatical frameworks, it often desirable to combine information from possibly inconsistent sources. Over the years, a number of default unification operations have been proposed 1 , which combine a strict feature structure with one or more defeasible feature structures. These operations preserve all information in the linguistic knowledge representation problems, including lexical inheritance hierarchies (Copestake 1993; Ginzburg and Sag 2001), lexical semantics (Lascarides and Copestake 1998), grammar induction (Briscoe 1999; Villavicencio 2002; Petersen 2004), anaphora processing (Gurevych et al. 2003; Alexandersson, Becker, and Pfleger 2004), among many others.
 involve something like Carpenter (1992) X  X  credulous default unification as one step.
The result of credulously adding the default information in G to the strict information in F is:
In other words, cred -unify ( F , G ) is the result of unifying F with the maximal consistent subset(s) of the atomic constraints in G . A subset of constraints is maximally consistent with respect to the subsumption ordering if no constraint can be added to it without cre-ating an inconsistency. In general, there may be more than one subset of the constraints in G that is consistent with F and maximal, so the result of credulous default unification is a set of feature structures.
 course anaphora comes from Grover et al. (1994). Consider the mini-discourse: Jessy likes her brother. So does Hannah. To resolve the anaphoric predicate in the second sentence, we can set up meaning representations for the source and the target: and credulously unify the source with the strict information in the target.
 atomic ground constraints:
Then, we find the maximal subsets of the remaining constraints which are mutually consistent with the target. This yields two solutions: corresponding to the sloppy identity and the strict identity readings of the anaphoric expression. 2. Algorithm
A key step for applying most default unification operators is finding the maximal con-sistent subsets of a set of constraints C . Unfortunately, finding these maximal consistent subsets is an expensive operation, and, in fact, is NP-complete. Let T = the set of conflicts in C , where a conflict is a minimal set of constraints that are mutually inconsistent with the target. For this example, T consists of the two conflicts:
Removing any one of the constraints from a conflict T i would break that conflict, so if we could remove from C at least one member of each T i , the remaining constraints would be 154 mutually consistent with the target information. Finding the maximal consistent subsets of C then is equivalent to finding the minimal subset C  X  at least one member of C . This is the hitting subset problem, a classic NP-complete problem (Karp 1972).

C for consistency. One way to proceed is to construct a spanning tree of the boolean was removed from the parent set to create that subset. For example, subset was formed by removing c 2 from { c 1 , c 2 , c 3 } ,so k = 2. The descendants of a node are constructed by successively dropping each of the constraints c ensures that we will visit every subset of C exactly once.
 of the subset lattice, with one important optimization. Because the cardinality of the
Furthermore, because each subset is produced by removing constraints from its parent set, every node in a subtree is a subset of its root. This means that once a consistent set is found, no descendants of that set can be maximal, and that subtree can be pruned from the search space. However, consistent subsets that are maximal in their branch of the tree may turn out not to be globally maximal. For example, in Figure 1, if is consistent and { c 1 , c 3 } is not, a breadth-first search would identify both { c pseudo-maximal results like { c 1 } .
 ing ), the organization of the search space into a binomial tree allows another valuable optimization. The deepest leaf node in any subtree is the set formed from the root by leaf. Because no superset of an inconsistent set of constraints can be consistent, if the foot of a subtree is inconsistent then clearly no node in the tree can be consistent, and the entire tree can be skipped (call this leaf pruning ). Taking both root pruning and leaf pruning together, the only subtrees that need to be explored are those whose root is inconsistent but whose deepest leaf is consistent. No other subtrees can possibly contain a solution.
 tions. Like Grover et al. (1994), this algorithm requires a post-check to remove pseudo-maximal subsets from results . A queue is used to keep track of subsets S that are yet to be checked for consistency, along with the index k of the constraint that was last dropped, and a flag leftmost that indicates whether that subset is the leftmost child. Because the deepest leaf node of the leftmost child is the same as the deepest leaf node of the parent, we are guaranteed that the deepest leaf of a leftmost child is consistent. Keeping track of leftmost children allows us to avoid a substantial number of redundant consistency checks. 3. Evaluation
The graphs in Figure 3 and Figure 4 show an empirical comparison between a breadth-pruning (BFS-RL) on randomly generated problems. The graphs show the number of subsets that were checked for consistency, as it relates to and p , the probability that two members of C are consistent. Larger values for p generally lead to fewer but larger maximal consistent subsets. All counts are averaged across 100 randomly generated sets of ground constraints. In generating random problems, we make the simplifying assumptions that all constraints are consistent with any inde-feasible information, and that a subset of constraints that are pairwise consistent is a consistent subset.
 close to the worst case maximum 2 | C | . A possible reason for this is that root pruning 156 will have the greatest effect when consistent subsets are found in the interior nodes of the binomial search tree. However, the configuration of the search space is such that most nodes are either leaves or very close to leaves, and only a few nodes have a large number of descendants. Therefore, root pruning mostly removes very small subtrees and has only a small effect on the overall cost of the algorithm.
 advantages of leaf pruning do not outweigh the cost of the extra consistency checks required to implement it. As | C | increases, though, leaf pruning can offer substantial improvements. For | C | = 19 and p = 0.1, leaf pruning eliminates more than 99.5% of the search space, leading to a 185-fold improvement in running time. As p increases, the benefits of leaf pruning do become more modest. Larger values of p mean fewer inconsistent leaf nodes, so fewer subtrees are able to be eliminated. Even so, the savings from leaf pruning can still be dramatic: at | C | = 19 and p = 0.9, leaf pruning yields a nearly five-fold improvement in speed.
 plication to application. An anonymous reviewer reports that in one application, the resolution of non-monotonic lexical inheritance for constraint-based grammars, p is based grammar development platforms do not support defaults (Copestake X  X  [2002]
LKB is a notable exception), and so grammar engineers tend to avoid the use of default overriding. Ginzburg and Sag (2001) propose a more comprehensive use of defaults, and grammars written following these principles would likely have a much lower value of p . To my knowledge, however, these ideas have not yet made their way into any large-scale grammar implementations.
 for robust parsing and automatic grammar augmentation via a kind of explanation-based learning. For this application, all features of a rule in the base XHPSG grammar (Tateisi et al. 1998) are considered defaults that can be overridden if necessary to get a successful parse of a sentence. In this case, | C | is likely very large and grows quickly with the length of the sentences being parsed. The value of p will depend on the coverage of the base grammar, but can be expected to be fairly close to 1.0 for most domains. In situations such as this, where p is expected to fall close to the worst case for leaf pruning, one could consider inverting the search direction of the algorithm in
Figure 2. Rather than beginning with C and removing constraints until a consistent subset is found, we could instead begin with the empty subset and add constraints until an inconsistency is found. In either case, the frontier in the search space between consistent and inconsistent subsets is where maximally consistent subsets will be found, 158 and leaf pruning can be used to eliminate regions of the search space that contain only consistent or inconsistent subsets. 4. Conclusions
Finding the maximal consistent subsets of a set of ground constraints is an impor-tant sub-problem for many natural language processing and knowledge representa-tion tasks. Unfortunately, the problem is NP-complete, and in the worst case requires checking all 2 | C | subsets of C for consistency. Previously proposed algorithms have produced approximate solutions (Boppana and Halld  X  orsson 1992), or have weakened the requirements to make finding a solution easier (Ninomiya, Miyao, and Tsujii 2002). improves on the method of Grover et al. (1994) and is able to achieve substantial gains over the worst case running time for a large class of problems. An efficient method for finding maximal consistent subsets can make default unification practical for problems such as large-scale lexical representation, on-line discourse processing, or ontology construction.
 References
