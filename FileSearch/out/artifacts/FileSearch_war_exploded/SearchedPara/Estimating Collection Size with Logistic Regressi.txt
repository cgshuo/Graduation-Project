 Collection size is an important feature to represent the content summaries of a collection, and plays a vital role in collection selection for distributed search. In uncooperative environments, collection size estimation algorithms are adopted to estimate the sizes of collections with their search interfaces. This paper proposes heterogeneous capture (HC) algorithm, in which the capture probabilities of documents are modeled with logistic regression. With heterogene ous capture probabilities, HC algorithm estimates collection size through conditional maximum likelihood. Experimental results on real web data show that our HC algorithm outperforms both multiple capture-recapture and capture history algorithms. H.3.3 [ Information Storage and Retrieval ]: Information search and Retrieval Keywords : Distributed Search, Collection Size Estimation Collection size, the number of docum ents in a collection, is an important element to represent the content of the collection, and has great effect on resource selection in distributed search. In uncooperative environments, where collections do not export their sizes initially, the broker of distri buted search needs to estimate them by itself. Several algorithms based on capture-recapture mechanism have been proposed for collection size estimation, such as capture-recapture (CR)[4], multiple capture-recapture (MCR)[5], and capture history (CH)[5]. Generally, these algorithms estimate collection sizes with the number of duplicate documents in different samp les. Homogeneous capture probabilities across documents, that every document has the same probability to be sampled, are assumed in these algorithms. However, in fact, the capture probabilities across documents are heterogeneous, being biased to wards long documents or those with high static ranks, e.g., PageRank scores[2][5]. In order to allow heterogeneous capture pr obabilities across documents and capture times, heterogeneous capture (HC) algorithm is proposed in this paper. It is inspired by a capture-recapture method with heterogeneous capture probabilities used in ecology[1][3]. Using covariates, HC algorithm models the capture probabilities with logistic regression, and es timates collection size through conditional maximum likelihood. E xperimental results on real web data show that our HC algorithm outperforms existing capture-recapture based algorithms. , where P i is the probability for document i being captured at least once. In addition, as the covariates in HC, the length and the term frequency of query can be acquired by parsing the content of the document, while the static rank of the document, e.g., PageRank score, is hard to obtain directly without the link graph of all the documents in the collection. So we use the average position of the document in the returned lists across all the queries to approximate its static rank. It is based on the assumption that documents at the top of returned lists across many queries tend to have higher static scores than t hose at the bottom of the lists. Three kinds of collection size estimation algorithms, HC, MCR, and CH, were evaluated with real web data. We crawled 50 Chinese web sites and built a site search engine for each of them. These site search engines are viewed as distributed collections, whose sizes vary from 793 to 342159, and the average is 29440. In addition, the ranking function used in these site search engines is Okpai BM25 formula combined with PageRank algorithm. Mean Absolute error ratio (MAER )[6] is adopted to measure the accuracy of collection size estimation. Let N i be the actual size of collection i and  X  i N be the estimated value, and then MAER of n collections is defined as follows. Searching queries in these site search engines, we captured and recorded the returned documents . We collected 100 documents on each capture and selected queries randomly from the captured documents. In total, 100 captures we re used in our experiments. The performance of three algorithms with different numbers of captures is shown in Figure 1. While the MAER values of MCR with different numbers of captures fluctuate around 41%, the performance of HC and CH improves with the increase of captures. From 10 to 100 captures, the MAER value of HC varies from 37.4% to 20.0%, while that of CH varies from 38.2% to 23.9%. To sum up, HC is the best among these algorithms, and the success is due to consideration of heterogeneous capture probabilities across documents and capture times. To explore the importance of covariates in HC algorithm, Table 1 shows the mean weights of cova riates in Equation 1 across all collections with 100 captures. The values of all covariates are normalized between 0 and 1. As we can see, the weights of length, static rank and term frequenc y are 0.61, 4.46 and 14.24, respectively. It indicates that term frequency is the most important 
