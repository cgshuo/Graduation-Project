
Sombut Foitong X , Pornthep Rojanavasu X , Boonwat Attachoo X , and Ouen Pinngern X  features [1  X  3]. The main advantages of MI are the robustness to noise and feature selection problems and the MIFS can be used in any classifying systems for its simplicity whatever the learning algorithm may be. Kwak [2] verified that MIFS does not work in nonlinear problems. He proposed MIFS  X  U to improve MIFS for solving nonlinear problems. He applied Taguchi method [7] to MIFS  X  U in situation two or more combined features which are dominant features. However, this method spent a appropriate feature subsets. The classificati on accuracy is applied to evaluate feature subsets, various machine learning algorithms may be used as selection criteria. However, these machine learning algorithms take a lot of computational time for each training process. Moreover, the MIFS  X  U can be mistaken when dealing with highly nonlinear problem. 
Rough set theory [8] has been used successfu lly as a selection tool to discover data dataset by purely structural methods. Given a dataset with discretized attribute values, attributes that are the most informative; all other attributes can be removed from the dataset with minimal information loss. rough sets theory including concepts of entropy, mutual information, and dependency of attribute. Section 3 shows detail of the MIFS and the MIFS  X  U and describe our proposed method. The proposed method is applied to several problems to show their effectiveness in Section 4. Conclusions and future works are discussed in Section 5. 2.1 Entropy and Mutual Information probability distribution p ( x , y ), the mutual information is: 
For continuous random variables, the en tropy and mutual information are defined as to perform integration. Therefore, we divide the continuous input feature into several discrete partitions and calculate the entropy and mutual information using the partitions that divide the continuous space [1]. 2.2 Rough Sets Theory Then a B  X  indiscernibility relation IND ( B ) is defined as: 
It is clear that IND ( B ) partitions the universe U into equivalence classes: by constructing the B -lower approximations of X : Let B  X  C , then the B -positive regions of D can be defined as: D , if and if k = 0, then D does not depend on B . In this section, we describe detail of the MIFS and the MIFS  X  U methods. We describe the limitations of those methods using several examples ( Example 1 , 2) . Last section, we propose an algorithm using Rough Sets which can breakdown this limitation and estimate the appropriate feature subsets. 3.1 MIFS and the MIFS  X  U referred to [1] for a full algorithm description of MIFS algorithm. Kwak [2] proposed the MIFS  X  U method for improve the MIFS method, which did not work well in when we apply to nonlinear problems [2] the MIFS  X  U showed better performance than the MIFS. However, both the MIFS and the MIFS  X  U do not work when two or illustrate the limitation of those methods as in Example 1 , 2 in section 4.1. Therefore, detail of algorithm will be described in section 3.2. 3.2 Proposed Algorithm (MIFS  X  U Rough) We used the Variable Precision Rough Set Model (VPRS) [10], the aim to increasing discriminatory capacities of Rough Set theory by using parameter  X  controlled grades of conditional probabilities and to the robustness to noise. 
The  X  -positive region ) ( X POS B  X  is given by: where X  X  U/D ,  X  is threshold values( 1 0  X   X   X  ) , and |x| is cardinality of x. 
For dependency of attribute defined in (7), we revised to apply which  X  -positive region and new define as: 
The feature selection algorithm using th e MIFS  X  U and Rough set are summarized as follows: 
The MIFS  X  U and Rough set in Feature Selection (MIFS  X  U Rough) 
In our experimental, we set  X  = 0.85,  X  = 0.2 and  X  = 0.99. These parameters are that approximate reduct closely to exact reduct, if  X  is small then performance is down. In this section, we apply the MIFS  X  U Rough algorithm to some of the classification problems and illustrate the effectiveness of the proposed method when comparative with other methods. 4.1 Simple Problems For example 1, we illustrate limitation of the MIFS and the MIFS  X  U methods when result of MIFS, MIFS-U and MIFS-U Rough. Example 1 : Let X 1 and X 2 be random variables which uniformly distributed on [-1, 1] and assume that there input feature normalized the input feature to have the values in [0, 1]. The order of selection by the MIFS and the MIFS  X  U are illustrate in Table 1. 
As in Table 1, both the MIFS and the MIFS  X  U select the MIFS  X  U methods when deal with highly nonlinear problem. Example 2 : Let } , , , { patterns belonging to two classes {0, 1}. For input feature generated from N (0.2, 0.3); for 
We normalized the input feature to have the values in [0, 1] and divided each input feature into ten partitions to calculate mutual information. { X 3 , X 4 }. While the MIFS  X  U Rough can solve this problem correctly. 4.2 Sonar Target a nd Ionosphere Datasets Sonar dataset consists of 208 patterns including 104 training and testing patterns each. continuous valued attributes with a binary class. This dataset consists of 351 patterns with 50% training and testing pattern each. Those datasets is from UC-Irvine machine network for 300 epochs in all cases as in [2]. 
For Sonar dataset, we selected 3  X  12 features among the 60 features, and trained used with the momentum of 0.0 and learning rate of 0.2 as in [2]. In Table 3, we compared the performance of the MIFS  X  U Rough with MIFS and number in the parentheses is showed standard deviations that corresponding. For each column in table, we show an appropriate number of features which are filtered by the MIFS  X  U. We can see that MIFS  X  U Rough produce better performances than others and also with full features selection (60), while MIFS-U used only 4 features. 
For Ionosphere dataset, we used MLP with momentum of 0.1 and learning rate of 1.0 as in [2]. We selected 3 to 20 features from 33 features. From Table 4, we can see that our proposed method produce best performance when select only 5 features from 33 features, while MIFS-U select 15 features. 4.3 Other UCI Datasets To investigate the performance of our proposed method, we conducted another 569 instances, 30 features with 2 classes. The Wine datasets consists of 178 instances, 14 features with 3 classes. The Dermatology dataset (Derm.) consists of 366 instances, 17 features with 26 classes. 
The comparison on feature subset selection and accuracy rate are shown in Table 5 and 6. All results are average over 10 times (train 50%, test 50%). We used MLP as momentum and learning rate of MLP are 0.3 and 0.1, respectively, for Wdbc dataset.  X  X ermatology X  and  X  X etter X  datasets, we use the decision tree classifier C4.5 as most experiments, we can see the proposed method perform better than the MIFS and MIFS  X  U. transform ,but the MIFS and the MIFS  X  U can not handling when two or more quadratic mutual information, but this method have highly computational effort. Then, we proposed MIFS  X  U Rough for overcoming these problems. 
We tested our proposed method on two simple problems and several real-world subsets and obtain better or equivalent performance compare with MIFS and MIFS-U. 
