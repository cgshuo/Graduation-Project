 A vast majority of queries in search engine are short ones. For example, the average query length of MSN search is 2.4 words [6]. However, there are also a non-negligible proportion of long queries  X  about 10% of queries are 5 words or longer [6]. Current search engines present convincing performance over short keywords queries but usually fail to handle verbose or colloquial queries compe-tently [8].
 On the other hand, verbose queries can be found in Community Question Answering services (CQA), which has b een proven as an amenable form to so-cial media for allowing anyone in the community to ask and answer questions in natural language. It is difficult to encourage users to answer unattractive questions in CQA, especially for those information-driven ones, since answering informational questions requires certain in-depth knowledge that only a small proportion of the population have the capacity of resolving it. Enabling search engines to answer verbose queries efficiently and effectively can thus remove the needs of submitting navigational questions to CQA services.

In this paper, we endeavor to address the following three questions:  X  What X  X  the performance for current search engines in handling navigational  X  Can we identify navigational question from CQA services automatically?  X  Given a navigational question from CQA, how can we identify the most
We define questions resolved (or largely explained) by the linked web pages (i.e., in the corresponding answers) as navigational question , which are simu-lated as verbose queries for evaluating search engine performance. The rationale is that queries from CQA services are l ess artificial when compared with TREC QA queries and less constraint when co mpared to search queries, where users are prone to generate queries in a simple keyword style. However, due to the inhomogeneous nature of the CQA services, questions cannot be treated as nav-igational questions straight away. For example, as revealed by Chen [5], that 43% of questions in CQA are subjective intent and 10.2% are social intent. To solve this problem, Huston et al. [8] use a method in which they consider queries from certain categories as verbose quer ies, which is then submitted to search engines. This method is effective in filterin g short web-style queries, however, it may fail to remove the question with subjective (sentiment-based) opinions or social interactions intent. In this paper, we use the dichotomy of navigational vs. non-navigational, in which navigational questions can be resolved by (or at least largely explained by) web information while non-navigational questions usually require participants in the community to answer manually.

Automatically identifying navigational intent of a new question is not an easy task since it is hard to recognize navigational intent by textual features. For example, the question  X  X an anybody recommend decent free music cre-ation software? X  with a survey style s eems to be a transactional intent , but it is actually a navigational question with the best answer like  X  X yrogen is ok, http://www.hydrogen-music.org. X  This implies that navigational intent is not always easy to be inferred solely based on textual features. Rather, metadata features, such as asker X  X  asking experi ence or the category from which the ques-tion corresponds to, is crucial for the intent deduction. Thus we build a predictive model through machine learning based on both text and metadata features.
The rest of this paper is organised as follows. In Section 2, we review the related work. In Section 3, we evaluate the performance of current search engines for handling verbose queries. In Sectio n 4, we investigate the usefulness of text and metadata features for identifying the user intent of questions. In Section 6, we make conclusions and discuss the future work. This paper is related to a series of techniques, spanning from search evaluation, question classification to retrieval models.

The current search engines have been evaluated in various ways. Liu [1] as-sesses the effectiveness of Google, Bing, and Blekko by surveying 35 undergradu-ate students in Computer Science, from which he concludes that Google and Bing share a comparable performance in 2011. Liu et al. [12] provide a comprehensive study on predicting user satisfaction in CQAs and discuss how to evaluate it through machine learning. The work most similar to ours is [8] in which Huston et al. use Yahoo! Answers questions to evaluate search engine performance with Yahoo! API and Bing API respectively, and they find Bing is slightly better than Yahoo in 2011. Our approach is somewhat similar to these works but instead of evaluating relevance documents with human judgment, we propose to automate the evaluation process by matching bet ween the associated URLs in the answers and the search engine results. It may be not as accurate as human judges  X  since not all the associated URLs are good answers, and since large amount of relevant web pages may be omitted. But our approach wins by sheer weight of numbers (especially when we can obtain v irtually unlimited number of questions from CQA sites), which cancels out the sid e-effects of the incomplete judgment.
With regard to the task of navigational intent identification, Broder X  X  seminal work [2] puts the intent of web search queries into three categories: informa-tional, navigational, and transactional. Uichin et al. [11], later on, proposed a framework to automate the process of navigational intent identification in web search, in which user-click behavior and anchor-link distribution features are found to be useful for detecting navigational intent. Sadikov et al. [16] model user X  X  navigational intent by clusterin g document click and session co-occurrence information. However, all these taxonomies cannot be directly applied to CQA due to the different expectations within people X  X  mind-sets: in CQA users nor-mally ask natural language questions which are addressed to human beings, whereas in Web search users submit keyw ord queries which are addressed to automated search engines. To the best of our knowledge, this is the first work which attempts to understand user X  X  navigational intent in CQA. Google is arguably the most powerful search engine in the world, and Bing has been rising up enormously recently, both of which are proven to be viable search-ing paradigms. Which one is a better choice is still one of the most controversial topics in the IR community. In light of this, we conduct a experiment which test search engine X  X  ability to answer navigational questions of Yahoo! Answers. 3.1 Setup Vast amount of navigational questions (see Section 1) are available on CQA services. Indeed, in 2005, 11.5% of questions in Yahoo! Answers have at least one URL in one of the answers and 5.5% of questions include at least one URLs in the corresponding best answer. Use rs cannot access the linked page themselves either because they don X  X  have the nece ssary search optimization skill or they prefer communicating with people rather than the texts produced by search engines.

The following examples illustrate navigational questions that askers currently post in Yahoo! Answers:  X  Navigational: What is the best free online photography portfolio website?  X  Non-navigational: How much should you tip a pizza delivery man?
The search engine evaluation experimen t is derived from a dataset crawled by ourselves, which is collected from Yahoo! Answers 1 dating from 2013/03/15 to 2013/04/01, contains a total of 54483 questions (note that after data cleansing, this is only a subset and cannot cover all the questions during that period of time). We adopt this dataset for the search engine evaluation task because they are collected fairly new and should have been well indexed by both Google and Bing. There are 5747 navigational questions in this dataset, from which 3752 ones are from top 10 categories which are then simulated as testing data to evaluate search engines.

Google API 2 and Bing API 3 are employed for evaluation because we noticed that  X  X lack box X  approach has been extensively used in many recent works [7,8] and is becoming more and more important for commercial purposes. 3.2 Stopword Removal There are many stopword lists available in the IR community, but we chose to create a new stopword list since the lan guage that used in the test questions is more noisy than the regular English text. We adopt an IDF-weighting scheme to the Yahoo! Answers repository. Specifically, we construct a stopword list by taking the top 100 words from the inverse document frequency ranking. This process identified words such as  X  X elp X ,  X  X nyone, X  and X  X hat X  which may not appear in the standard stopword list, but are usually useless search words. 3.3 Noun Phrase Detection Learning from the previous work that noun phrases from the query can help identify the key concepts within the query, we used the Standford parser toolkit [10,15] to automatically extract those potential noun phrases. Considering that we are using a search engine as a black box, the usage of the noun phrase technique is restrictive from the query: it is impossible to assign weights to terms in terms of condence or priority. There are many ways to enable the search engine to communicate with the extracted noun phrases, we report two such methods:  X  The first method put each of the extracted noun phrases in the query in  X  The second method only keep the ext racted noun phrases and quotation For example, 2 noun phrases:  X  X he websi te X  and  X  X merican eagle X  are detected in the query:  X  X hat is the website for American eagle? X 
Using the first method, we would generate the query: what is  X  X he website X  for  X  X merican eagle X ?
Using the second method, we would generate the query:  X  X he website X   X  X merican eagle X  3.4 Search Results The retrieval perfor mances, measured by Precision at 10 (P@10) [13] and Mean Average Precision (MAP) [13], are reported in Table 1 and Figure 1. For rel-evance judgement, only URLs appeared in the answers are regarded as rele-vant web pages. Note that we employ MAP instead of MRR (Mean reciprocal rank) because there are often several URLs appear in the answers such that the number of relevant web pages is usually uncertain. Even though the relevance judgments for the verbose queries are incomplete and the absolute retrieval per-formance is relatively low (which is expected because of the sparseness of the relevance judgment), our approach is probably more reasonable to traditional ones  X  it has been demonstrated by Carter ette [3,4] that evaluation over more queries with fewer or noisier judgments is preferable to evaluation over fewer queries with more judgments. The large nu mber of the testing data compensates for the incompleteness of the judgment. Another concern is that the searching results may be time-sensitive since most search engines are regularly updated on a hourly basis. In order to reduce this risk, we submitted all queries of the above approaches to search engines within a short time session, spanning from 26/03/2013 to 30/03/2013. One should also note that search engines often return the Yahoo! Answers original web pages, which are removed from the results to allow an impartial judgment.

Table 1 reports the retrieval results for all of the query processing techniques when applied to Yahoo! Answer testing data using the Google and Bing search engines. The results from the two search engines are very similar in terms of precision@10; when it comes to MAP(Mean Average Precision) , however, Google overwhelms Bing with almost 50% improvement. This suggest that Google and Bing have a comparable ability to capture the desired documents, but Google is superior to Bing when ranking user X  X  desired documents. Also the use of quotations of the noun phrases( method one) for the query reformulation is clearly not effective. But both noun phrase( method two) and stopword removal produce significant improvements. The most effective technique, however, is the combination of the above two.

Some users may be curious about which search engine advances in which top-ics (especially for those working in advertisement industry such that people can strategize their investment more smartly ). For that reason, we also present a sep-arate performance comparison under each top 10 Yahoo! Answers navigational categories. Although most of the categories share a comparable performance in Figure 1, it is clear that Google excels in Car Transportation, Travel ,and Home Gardens categories, whereas Bing can hardly beat Google for any categories (some categories show inconsistent results over precision@10 and MAP ,suchas Bossiness Finance ). To address the task of navigational question prediction in CQA, a variety of personal information and social relationship features are collected and exploited to model users X  social behaviors behind their search intent. 4.1 Setup The classification experiment is based on Yahoo! Answers dataset which is de-rived from Yahoo! Answers Comprehensive Questions and Answers (v1.0), a dataset kindly provided by Yahoo Research Group 4 . It consists of 4483032 ques-tions and respect answers from 2005/01/01 to 2006/01/01. We use this dataset for question classification task since there is a rich metadata features set available for understanding user X  X  asking behaviors. 4.2 Classification Performance Measure Since the class sizes are imbalanced in this problem, we use the F 1 score [13] instead of accuracy to measure the performance of question classification. The F 1 score is the harmonic mean of precision P and recall R : F 1 = 2 PR P + R ,where two versions of F 1 score namely, micro-averaged F 1 (mi F 1 ) and macro-averaged F 1 (ma F 1 ) [5]. The former carries out aver aging over all test questions while the latter over all question categories, therefore the former is dominated by performance on major question categories while the latter treats all question categories equally. The results reported in the next section are all predicated on (ma F 1 ). 4.3 Classification Results We use the SVM implemented by Platt et al. [14] with a probabilistic output and adopt a linear kernel in this task  X  we find that linear kernel generally outperform non-linear ones. We use 5-fold cross-validation to get a good value of C and  X  : Choosing C with the range of 0 . 01 &lt;C&lt; 0 . 1 is good for all gives a good performance. The parameters used in our experiment is C =0 . 5 and  X  =1  X  10  X  12 . The parameter for the class weights is set as navigational : non  X  navigational =0 . 9:0 . 1 since the classification task is an imbalance problem in nature.

Table 2 depicts the performance (ma F 1 ) of [binary] question classification through supervised learnin g (linear SVM) with different sets of features, by using 10-cross validation. It X  X  quite surprising to us that metadata features are even more important than textual features by giving insight of user X  X  asking behaviors. However, the mixture classifier with both text features and metadata features works better than textual features classi fier or metadata features classifier alone that only looks at one perspective of the user intent. Given a question is navigational intent, we experimented with various language modelling techniques for question retrieval. The following language models are incorporated into our framework: 5.1 Classic Language Model Using the classic (query-likelihood) language model [18] for information retrieval, we can measure the relevance of an archive question d with respect to the query question q as: assuming that each term w in the query q is generated independently by the unigram model of document d . The probabilities P cla ( w | d ) are estimated from the bag of words in document d with Dirichlet prior smoothing. 5.2 Translation-Based Language Model It has been demonstrated that the lexic al gaps between a query question and archive questions could be addressed by the translation-based language model [9,17]: where P ( w | t ) represents the probability of a document term t being translated into a query term w . As in [17], we estimate such word-to-word translation prob-abilities P ( w | t ) on a parallel corpus that consists of 200,000 archived question-answer pairs from Yahoo! Answers. 5.3 Intent-Based Language Model We propose to take user intent into account for question retrieval in the language modelling framework: where C k represents a category, P ( w | C k ) is its corresponding unigram language model 5.3 and P ( C k | d ) is the probability that the document d belongs to that category (see Section 4).
 Estimating Unigram Models for User Intent. Given the probabilistic clas-sification results on all archive questions, we can obtain the unigram language model for each user intent category C k through maximum-likelihood estimation: where tf ( w,d ) is the term frequency of word w in document d . It is possible to employ more advanced estimation meth ods, which is left for future work. 5.4 Mixture Model To exploit evidences from different pers pectives for question retrieval, we can mix the above language models via linear combination: where  X  ,  X  ,and  X  are three non-negative weight parameters satisfying  X  +  X  +  X  = 1. When  X  = 0, the complete mixture model backs off to the current state-of-the-art approach, i.e., the combination of the classic language model and the translation-based language model only [17]. 5.5 Experimental Setup We conducted question retrieval experiments on two real-world CQA datasets. The first dataset, YA, comes from Yahoo! Answers. It is part of Yahoo! Labs X  Webscope 5 L6 dataset that consists of 4,483,032 questions with their answers from 2005-01-01 to 2006-01-01. The second dataset, WA, comes from WikiAn-swers. It contains 824,320 questions with their answers collected from WikiAn-swers 6 from 2012-01-01 to 2012-05-01.
We experimented with question retrieval using a similar set-up as in [17]: 50 questions were randomly sampled from the YA and WA datasets respectively for testing (which were excluded from the CQA retrieval repositories to ensure the impartiality of the evaluation), and the top archive questions (i.e., search results) returned for each test query question were manually labelled as either relevant or not. 5.6 Experimental Results In order to see whether intent relevance can improve the search performance, we compared the following three approaches:  X  the baseline approach which only employs the classic language model (C);  X  the state-of-the-art approach which combines the classic language model and  X  the proposed hybrid approach which blends the classic language model,
The model parameters were tuned on the training data to achieve optimal results, as shown in Table 3. In the mixture models (C+T) and (C+T+I), the ratio between parameter values  X  and  X  was same as that in [17].

In accordance with the observation in [17], adding the translation-based lan-guage model (C+T) brings substantial performance improvement to the classic language model (C). More importantly, it is clear that our proposed hybrid approach incorporating the intent-based language model (C+T+I) outperforms the state-of-the-art approach (C+T) significantly, according to both P@10 and MAP on YA and WA. The contribution of this paper is three fold. First, this is the first work which attempts to understand user X  X  navigational intent in CQA  X  to the best of our knowledge. Second, we propose a novel evaluating method which automates the verbose query evaluation process b y matching between the associated URLs in the answers (of the navigational question) and the search engine results. The current best search engines, namely Google and Bing, are evaluated with naviga-tional questions (act as verbose queries), from which we show that Google is still the best search engine. In addition, we find that the best way of query refinement for the current search engines is to combine both noun phrase( method two) and stopword removal techniques. Third, we present a intent-based language model which supersedes that of the existing category-based language model, since one question can belong to multiple (user inte nt) categories to different degrees, and since a probabilistic question classifier is built automatically by taking into con-sideration of both textual features and metadata features.

For future work will answer what X  X  the best way for query refinement in search engine(query expansion or query reducti on), for which this work is the foundation for future research of utilizing phrases/concepts detection techniques for query expansion.

