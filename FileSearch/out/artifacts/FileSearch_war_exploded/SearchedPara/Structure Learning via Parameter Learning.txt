 A key challenge in information and knowledge manage-ment is to automatically discover the underlying structures and patterns from large collections of extracted informa-tion. This paper presents a novel structure-learning method for a new, scalable probabilistic logic called ProPPR. Our approach builds on the recent success of meta-interpretive learning methods in Inductive Logic Programming (ILP), and we further extends it to a framework that enables ro-bust and efficient structure learning of logic programs on graphs: using an abductive second-order probabilistic logic, we show how first-order theories can be automatically gen-erated via parameter learning. To learn better theories, we then propose an iterated structural gradient approach that incrementally refines the hypothesized space of learned first-order structures. In experiments, we show that the proposed method further improves the results, outperforming compet-itive baselines such as Markov Logic Networks (MLNs) and FOIL on multiple datasets with various settings; and that the proposed approach can learn structures in a large knowl-edge base in a tractable fashion.
 [ Information Systems Applications ]: Miscellaneous Probabilistic Prolog; structure learning; personalized PageRank
Many information-management tasks (including classifi-cation [20], retrieval [10], information extraction [28], and information integration [29, 6]) can be formalized as learn-ing and inference in an appropriate probabilistic first-order logic. To simplify the task, in many cases, first-order logic clauses in probabilistic logic systems are hand-written by developers, and the tasks consist of only parameter learning and inference. However, building such a hand-written first-order logic system can be very challenging: in the initial stage, domain experts or the developers themselves have to manually define the logic clauses that situates the domain-specific application scenarios. However, this approach might not generalize well to real-world problems, and the prede-fined clauses can be limited. Another issue is about the efficiency: constructing first-order logic programs manually can be very time-consuming, and have high financial costs. Finally, the maintenance cost is also non-trivial: when new data comes in, developers have to manually analyze the new dataset for updating and expanding the existing first-order clauses. Therefore, automated learning of logic program structures is of crucial significance for building robust first-order logic systems.

Unfortunately, many of the existing structure learning methods for learning probabilistic first-order logics are not efficient enough to be used for practical-sized datasets: when the total number of predicates and entities in a database become large, the costs of searching through all possible candidates to construct first-order clauses are also growing rapidly. For example, some existing techniques for learn-ing Markov Logic Networks (MLNs) [31] may take days to run [14, 15], even though the input datasets include only a few dozen predicates and a few thousand grounded atoms.
This paper presents a structure-learning method for a new, scalable probabilistic logic called ProPPR [37]. ProPPR is efficient enough to support inference over large, noisy knowledge bases, and supports parameter learning us-ing a parallelized version of stochastic gradient descent. In some cases, ProPPR is dramatically faster than prior ap-proaches: for instance, ProPPR takes well under a minute on an ordinary desktop to learn numeric parameters for a theory with hundreds of clauses, over a knowledge base con-taining a million constants, while a state-of-the-art MLN implementation requires several hours for the same task, on a knowledge base only 1/1000 of the size [38]. However, there are no existing methods for learning ProPPR theories: previous experiments have used theories learned assuming radically different semantics for the clauses, or hand-written theories.

We present here a structure-learning method based on a recently-learned approach to learning the structure of con-ventional logic programs, in which logic programs are gener-ated by using a second-order abductive program to  X  X rove X  that every observed positive example is covered. In ab-ductive reasoning, assumptions are made, as necessary, to complete a proof: in this setting, the assumptions made by Table 1: A simple program in ProPPR [37]. See text for explanation. the second-order program concern the existence of elements of the first-order program being generated. This approach, which is embodied in a system called Metagol [25], turns out to be both elegant and powerful, providing a conceptu-ally clear framework for such important tasks as predicate invention, and learning recursive programs.

In this paper, we adapt a Metagol-like approach to learn-ing ProPPR rules. In particular, we present an  X  X bductive X  stochastic second-order program for ProPPR, in which ev-ery assumption corresponds directly to a useful clause in a ProPPR program, and where further, every learnable param-eter corresponds directly to a first-order clause . Structure learning is performed by computing the gradient of these features on training data, and constructing a small first-order stochastic program, consisting of those clauses that are potentially useful according to the gradient information. Parameters for this first-order program can then be learned in the usual way. We show that on small problems, this approach provides more accurate theories on the task of knowledge base completion , where the goal is to learn an interrelated set of rules to infer missing facts in an incom-plete knowledge base. The method is also scalable enough to perform knowledge base completion for realistic knowledge bases containing tens of thousands of facts. We then propose an iterative variant of the gradient-guided structure learning approach that incrementally refines the hypothesized space of plausible clauses. Furthermore, to demonstrate the ro-bustness of our approach, we also show promising results in two additional structure learning tasks in the biomedical and anthropological domains.

In the next section, we review the foundations and basic characteristics of ProPPR. In Section 3, we introduce the idea of using an abductive second-order theory for structure learning in ProPPR, focusing the problem of knowledge base (KB) completion, and learning inter-related relations. In Section 4, we demonstrate the robustness of our approach by showing the results for learning structures in the NELL KB, a biomedical ontology, as well as a complex kinship system. We then discuss related work in Section 5. Finally, we conclude in Section 6.
Table 1 and Figure 1 illustrate a ProPPR theory and a corresponding proof graph. We refer the reader to prior papers [37, 38] for a detailed explanation of ProPPR X  X  se-mantics: briefly, however, a ProPPR theory T is a Horn clause (Prolog) theory, where each clause c is associated with a function  X  c which computes a set of features. Concep-tually, a ProPPR program is executed by backward chain-ing, as in Prolog; however, rather that simply searching for a proof of a goal Q to determine if it is  X  X rue X  according to the theory, ProPPR will use the the number of  X  X roba-ble X  proofs to assign a degree of  X  X ruth X  to a goal. In or-der to do this, ProPPR builds a graph rooted at the query goal, where each node corresponds to a conjunction of goals A ,...,A k and a substitution  X  that imply the query goal (i.e. T  X  ( A 1 ,...,A k  X  )  X  Q X  ), and each edge corresponds to the application of a clause c .

In the experiments of this paper, the feature-functions  X  are simple: each feature corresponds to a Prolog goal, which may include bound variables from the head of the clause, the bindings of which are supplied when  X  c is invoked. A node corresponding to an the empty conjunction (denoted 2 ) is a leaf of the graph, and corresponds to a completed proof of the query goal.

Additionally, every edge created by the clause c is la-beled with the features produced by  X  c for that application of c . As in many deductive-database systems, we distin-guish between the two types of clauses: the large number of unit clauses (aka facts) that comprise the  X  X atabase X , or extensionally-defined predicates (in the figure, the clauses defining handLabeled , hasWord and linkedBy ); and the small number of clauses that comprise the  X  X ntensional X  predicates (e.g., about and sim ). Applications of any database clause are labeled with the special feature db .

Note that each different 2 node corresponds to a differ-ent proof for Q , and different proofs may be associated with different substitutions  X  and hence different solutions to the query Q . (In the figure, for instance, the leaf in the lower left corresponds to the solution  X  = { Z = fashion } of the query Q = about(a,Z) , while the leaf in the lower right cor-responds to { Z = sport } .) Following the related formalism of stochastic logic programs (SLPs) , we will assign a prob-abilistic score to every node in the proof graph, and then, to assign a probabilistic score to a particular solution Q we simply marginalize over all leaf nodes ( 2 i , X  i ) such that Q X 
Finally, we associated a score with each node in the graph by performing a personalized PageRank (PPR) (aka  X  X an-dom walk with restart X ) process, where transition probabili-ties are based on edge weights, which are in turn determined by a weighted function of the features. In more detail [37, 38] the random process which defines the node weights are as follows. (a) Compute the total weight z of each out-going edge as g ( P i  X  i f i ), where the sum is taken of the features f 1 ,...,f K which label that edge. Here, g (  X  ) is an edge strength function, and in this study, we choose the well-known hyperbolic tangent function tanh as g . The outgoing edges include an implicit restart edge , which goes back to to query node, and a self-loop edge which connects every empty goal list (i.e., solution node) to itself. (b) Normalize the edge weights to form a probability distribution, and pick an edge from that distribution. (c) Follow the edge to its destination and repeat.

These semantics are similar to those used SLPs [7], with two changes. One is the addition of the restart edges, which allow for a fast approximate proof procedure, in which only a small subset of the full proof graph is generated. In par-ticular, if  X  upper-bounds the reset probability, and d up-perbounds the degree of nodes in the graph, then one can efficiently find a subgraph with O ( 1  X  ) nodes which approx-imates the weight for every node within an error of d [37], using a variant of the PageRank-Nibble algorithm of Reid et al [1] . The second change is the addition of the feature functions,  X  c , which are used heavily in this work.
ProPPR X  X  parameter learning framework is implemented using a parallel stochastic gradient descent variant to op-timize the L 2 -regularized logarithmic loss using the super-vised personalized PageRank algorithm [2]: given the train-ing queries and known solutions, we perform a random walk with restart process, and upweight the weights (edges) that are more likely to end up with a known positive solution.
The PPR-based scores used by ProPPR are quite differ-ent from the probability scores adopted by MLNs [31] and similar formalisms: intuitively, they measure the proportion of short proofs which support a belief, rather than counting the proportion of models in which the belief is true. Past experiments have used these scores in a retrieval context, to order potential answers to a query. In this context it has been shown that with parameter-learning, ProPPR per-forms well on tasks such as entity resolution, and inference over noisy knowledge bases [37]. Below we will consider the more difficult problem of learning the clauses of a ProPPR program.
To illustrate and motivate the problem of structure learn-ing for ProPPR, we will use a classic problem introduced by Hinton in 1986 [11]. In this problem we have two families, each with twelve individuals, and twelve binary relations be-tween these individuals: husband, wife, father, mother, son, daughter, brother, sister, uncle, aunt, nephew, and niece. From this data, we can also define 104  X  X ueries X , such as un-cle(charlotte,Y) , each of which has some positive (correct) answers (e.g. uncle(charlotte,james) ), and some incorrect an-swers. In our experiments the universe of potential answers (which we use ProPPR to rank) consists of all person pairs that are related by one of the twelve known predicates. We measure mean average precision (MAP) 1 over all the T e test queries.

In past experiments, high accuracies have been obtained by holding back a small number of test queries and train-ing on the rest. We confirmed these results with two sys-tems: Quinlan X  X  FOIL [30] and Alchemy with structure-learning [14]. We designated one family as test and one as train, and we then performed 12 experiments where we
Note that AP not only measures precision at rank k , but also measures recall: it uses the denominator to penalize the cases where positive solutions are missing. MAP has shown to be very robust and stable in many tasks [21], and it is widely used in relation learning tasks (e.g. [32] [16]). Figure 3: Completing an incomplete DB of fam-ily relations. X-axis: the percentage of background facts missing. Y-axis: the MAP result. held out the queries from a single relation: in other words, for relation R , database consisted of facts defining the other 11 relations for both the train and test family; the train-ing data consisted of the queries for R from the train fam-ily; and the test data was the queries for R from the test family. FOIL obtained precision of 100% for all 12 rela-tions, and Alchemy obtained precision of 100% on 11 of the 12. This is not surprising, since all of the predicates have succinct definitions in terms of the others: for instance, wife(X,Y)  X  husband(Y,X) (in this data).

However, the prior systems do not perform well if they need to learn interrelated concepts. We held out six pairs of relations (wife/husband, sister/brother, aunt/uncle, niece/nephew, and daughter/son) and repeated the same experiments. Alchemy X  X  mean average precision (MAP) on the six problems drops to 27%, and FOIL X  X  drops to zero. The problem for both systems stems from the use of pseudo-likelihood 2 to estimate the semantics of the partially-learned program from examples , rather than actual inference us-ing the learned program. As a typical result, for the re-lation pair aunt/uncle, FOIL learns the rules uncle(X,Y) :-husband(X,Z), aunt(Z,Y) and aunt(X,Y) :-wife(X,Z), un-cle(Z,Y) , which are circular.

Another illustration of the weaknesses of existing algo-rithms is provided by the following set of experiments. We trained with a DB containing all but k% of the facts for the training family, and all of the training-family queries as training data: thus, we are asking the system to learn rules which can complete an incomplete database. As test data, we used a database with all but k% of the facts for the test family, and again, all test-family queries as the test set. The results are shown in Figure 3: neither FOIL nor Alchemy X  X  MLN method 3 outperform the simple baseline of predicting exactly the facts in the incomplete database. 4
Or in FOIL X  X  case, an approach broadly similar to pseudo-likelihood.
Alchemy X  X  performance is quite sensitive to the precise set of missing facts, so we average over ten runs in the figure.
Note that we have also experimented with a more re-cent  X  X earning with Structural Motifs (LSM) X  variant [15] for learning MLN, but the results were much worse than Alchemy: we only observe a MAP of 10.7 on the missing 5% setting. This is because LSM is designed to learn long
Motivated by this, we introduce a new technique which we call the iterated structural gradient method for structure-learning in ProPPR. In particular, as noted in the intro-duction, we implement a Metagol-like method for intro-ducing structure. We start with an  X  X bductive X  stochas-tic second-order program, in which every assumption corre-sponds directly to a useful clause in a first-order program, and where further, every learnable parameter corresponds directly to a first-order clause. Table 2 shows the theory that we use: this theory assumes that the first-order DB contains only binary facts, which are encoded for the second-order theory as triples of the form rel(r,x,y) : e.g., the fact father(james,colin) is represented as rel(father,james,colin) . Thus, rule (g) is a second-order version of the baseline algo-rithm of Figure 3: to interpret the predicate P ( X,Y ), rule (g) simply checks for the fact rel(P,X,Y) .

Rules (a-c) can be viewed as a more powerful interpreter for the binary predicate P , which also makes assumptions about the presence of rules in the first-order theory. For in-stance, in an application of rule (b), the goal of  X  X nterpreting X  (with the predicate interp ) the a predicate uncle(arthur,Y) is reduced to the goal of interpreting (with the lower-level predicate interp0 ) some predicate nephew(Y,arthur) , as-suming that the first-order theory contains a clause un-cle(X,Y) :-nephew(Y,X) .

The way the assumption mechanism is implemented is quite simple. Associated with every interpretive action X  i.e., clauses (a-c) X  X s an extra goal, such as abduce ifInv for clause (b). These abductive goals are defined to always suc-ceed, but whenever the proof step which lets them succeed is applied, the corresponding edge in the proof graph is labeled with an appropriate feature (e.g., f ifInv(uncle,nephew) ) which records that this assumption was made. Table 3 gives an example proof in the theory, showing how the abductive feature f ifInv(uncle,nephew) might be used. ProPPR X  X  nat-ural bias towards short proofs (in the second-order theory) guides it toward near-minimal sets of assumptions regard-ing the first-order theory. Furthermore, the gradient of the abductive features indicates the utility of the corresponding first-order clauses.

Structure learning is performed by computing the gradi-ent of these features on training data, and then producing a small first-order stochastic program, consisting of those clauses that are potentially useful according to the gradient information. Parameters for this first-order program can then be learned in the usual way.

We thus adopt the learning algorithm of Table 4, which we call the iterated structural gradient (ISG) method. As an extended example, we consider the operation of ISG on the problem of learning aunt/uncle together. In the first iteration, the following rules are proposed (not in order, and abbreviating interp0 as in0 ): in0(aunt,X,Y) :-in0(sister,X,Z), in0(father,Z,Y). in0(uncle,X,Y) :-in0(brother,X,Z), in0(mother,Z,Y). in0(aunt,X,Y) :-in0(nephew,Y,X). clauses (with more than 5 predicates) using recurring short patterns, whereas in our task, our goal is to learn short clauses with a maximum of 3 predicates in a clause. An-other issue is that LSM has more than 20 hyperparameters to tune, which makes the structure learning process sensitive to the choice of the datasets and hyperparameters. second-order rules to which they correspond.
 Table 3: A slightly-abbreviated sample proof using the second-order theory. The second column is the rule used at that point in the derivation, along with the features generated by that clause application, if any. (For clarity, we list the process of bind-ing rel(R,Y,arthur) to the head of the unit clause rel(nephew,colin,arthur) :-, and then removing it, as two steps, DB 1 and DB 2 .) in0(R,Y,arthur),ab ifInv(uncle,R) rel(R,Y,arthur),ab ifInv(uncle,R) rel(nephew,colin,arthur), in0(aunt,X,Y) :-in0(niece,Y,X). in0(uncle,X,Y) :-in0(nephew,Y,X). in0(uncle,X,Y) :-in0(niece,Y,X).

The first two of these are correct rules, and the remaining ones are over-general, as they confuse aunts and uncles. In the second iteration, ISG proposes the rules: in0(aunt,X,Y) :-in0(wife,X,Z), in0(uncle,Z,Y). in0(uncle,X,Y) :-in0(husband,X,Z), in0(aunt,Z,Y). in0(aunt,X,Y) :-in0(wife,X,Z), in0(aunt,Z,Y). in0(uncle,X,Y) :-in0(husband,X,Z), in0(uncle,Z,Y). in0(aunt,X,Y) :-in0(uncle,X,Y). in0(uncle,X,Y) :-in0(aunt,X,Y). in0(aunt,X,Y) :-in0(aunt,X,Y). in0(uncle,X,Y) :-in0(uncle,X,Y)
The first two of these are correct, while the remaining rules are, to various degrees, overgeneral and/or redundant. However, after parameter-learning, the learned theory per-forms perfectly on the test set.

Figure 3 shows the performance of ISG on the database-completion task, and compares it to FOIL, MLNs with struc-ture learning, and the KB-only baseline. Table 5 shows re-sults for the leave-two-relation out experiments discussed above. We also introduce two additional baselines for com-parison: one is to perform the main loop only once, which we call the structural gradient (SG) method, and a final Table 4: The Iterated Structural Gradient (ISG) Algorithm 1. For t = 1 , 2 ,... : 2. Discard all rules but the added ones, and retrain the Table 5: Average precision performance for learning two mutually-related relations at once.
 father+mother 0.0 23.32 42.53 70.05 100.0 husband+wife 0.0 4.73 3.20 39.63 79.4 daughter+son 0.0 11.49 22.74 70.05 100.0 sister+brother 0.0 3.29 10.37 62.18 78.85 uncle+aunt 0.0 10.41 53.35 79.41 100.0 niece+nephew 0.0 6.49 28.54 72.25 80.09 average 0.0 9.96 26.79 65.60 89.70 baseline is parameter-learning for the second-order theory, which we label PL in the table. Note that ISG performs quite well on the task of learning two interrelated predi-cates, even though it is quite difficult for the other systems (e.g. FOIL and MLNs). In addition to this, the ISG method usually converges quickly: empirically it typically converges within only 5 iterations.

Our method is superior because instead of using pseudo-likelihood, we take a holistic point of view: we use a second-order abductive logic to construct the hypothesis space, and relax the structure learning problem to first-order param-eter learning using supervised personalized PageRank with log likelihood. It is not surprising that the proposed ISG method has a good performance: the ISG method incremen-tally adds newly-learned gradient-guided plausible inference rules to the structure learning rule set, which helps to refine the overall multi-epoch structure learning process.
As tests of generality, we applied the proposed ISG ap-proach on two larger-scale, more widely-used inference tasks. One is from the domain of biomedicine: in particular, we consider the Unified Medical Language System (UMLS) dataset [23, 22], which has been used in many structure learning tasks [13, 19]. The dataset 5 contains 46 predi-cates and 6,529 beliefs 6 : most predicates are verbs such as  X  X easures X ,  X  X ccurs in X , and  X  X reats X  , which indicate re-lations among entities. The entities are concepts such as  X  X nzyme X ,  X  X ammal X , and  X  X irus X  . We used the following experimental procedure:
Note that we choose this particular difficult setting be-cause when removing the major predicate, it shows whether http://alchemy.cs.washington.edu/data/umls/
Note that LSM [15] was too slow to run on this dataset, so we use Alchemy X  X  structure learning algorithm to learn MLNs.
Similarly, this means that we use  X  X ffects X  as the non-evidence predicate for pseudo-likelihood computation in learning MLNs.
 Table 6: Average precision performances for learn-ing structures in UMLS, a biomedical ontology. or not our approach is able to model the long tail of the Zipfian distribution of facts and relations, which previous systems may not be capable of.

Table 6 shows the experimental results for ISG, com-paring them to Alchemy, PL, and SG. Although we learn a single predicate, rather than several inter-related pred-icates, this problem is still difficult, perhaps because the most common and well-connected predicate was not present in the database of facts. We see that even ProPPR X  X  parameter-learning baseline PL obtains an averaged result of 6.2, which almost triple the result obtained by MLN X  X  pseudo-likelihood structure learning approach. We also see that the gradient-based structure learning approaches out-perform both the baseline and MLN, with ISG achieving a mean average precision of 11.7. The left figure in Fig. 4 shows the precision of predictions for affects predictions as a function of rank for each system, for a representative run. The rules proposed by ISG are qualitatively plausible: for example, in the second run, ISG proposes the following rules (in their first-order format): affects(X,Y) :-causes(X,Z), isa(Z,Y). Table 7: Average precision performances for Alyawarra kinship systems. affects(X,Y) :-causes(X,Z), associated with(Z,Y). affects(X,Y) :-complicates(X,Z), complicates(Z,Y). affects(X,Y) :-complicates(X,Z), result of(Z,Y). affects(X,Y) :-interacts with(X,Z), causes(Z,Y). affects(X,Y) :-interacts with(X,Z), diagnoses(Z,Y). affects(X,Y) :-produces(X,Z), associated with(Z,Y). affects(X,Y) :-produces(X,Z), disrupts(Z,Y).

We see that all the above clauses obtained by the ISG method are plausible inference rules in the biomedical domains: for example, the first clause  X  X ffects(X,Y) :-causes(X,Z), isa(Z,Y). X  is a formula of direct causes, where as the second clause  X  X ffects(X,Y) :-causes(X,Z), associ-ated with(Z,Y). X  concerns the indirect causes. Interestingly, the third clause the ISG method has learned is a case of transitive complication. Again as further test of robustness, we analyzed the Alyawarra kinship dataset [9], which is a more complex dataset from Hinton X  X  domain of family kinship [11]. The Alyawarra are an aboriginal tribe from central Australia: the tribe has four kinship sections, and the author of the corpus asked 104 tribe members to provide kinship terms for each other. The original author of the dataset, an an-thropologist named Denham, has since recorded the demo-graphic features for each of his subjects, and created the ground truth partition by assigning each tribe members to one of the clusters. The version of the Alyawarra dataset we use has 25 predicates, and 10,686 beliefs 9 . The task is to infer the latent paths that associate these predicates, and use them to make binary link predictions.

Similar to the setup in the UMLS dataset, we use the most frequent predicate  X  X erm 16 X  as the hold-out target query relation in both training and testing, and use other predicates as background facts for ProPPR and evidence in MLN. We randomly select 90% of the data for training, and the rest for testing. Experiments are repeated 10 times. Again, this is a challenging setting, as a key predicate is not present in the background database of facts. http://alchemy.cs.washington.edu/data/kinships/
Again, we are unable to run LSM [15] on this dataset due to the number of grounded assertions and relations, and Alchemy was used to learn the MLNs.
 Table 8: Summary of the KBs used in experiments on completing subsets of NELL X  X  KB.

The detailed experimental results are shown in the Ta-ble 7. The right figure in Fig. 4 shows rank-based results from various systems of a sample run in this dataset. We see that the overall performances are consistent with the results from previous subsections: the proposed iterative structural gradient approach over ProPPR outperforms both the base-line methods and the MLN structure-learning approach.
Finally, as a larger-scale and more realistic task, we ex-plore learning inference rules for the NELL knowledge base. The NELL (Never Ending Language Learning) research project is an effort to develop a never-ending learning system that operates 24 hours per day, for years, to continuously improve its ability to read (extract structured facts from) the web [5]. NELL is given as input an ontology that de-fines hundreds of categories (e.g., person, beverage, athlete, sport) and two-place typed relations among these categories (e.g., athletePlaysSport(Athlete, Sport) ), which it must learn to extract from the web. NELL is also provided a set of 10 to 20 positive seed examples of each such category and rela-tion, along with a downloaded collection of 500 million web pages from the ClueWeb2009 corpus (Callan and Hoy, 2009) as unlabeled data, and access to 100,000 queries each day to Google X  X  search engine. NELL uses a multi-strategy semi-supervised multi-view learning method to iteratively grow the set of extracted  X  X eliefs X .

Inference on NELL X  X  learned KB is challenging for two reasons. First, the learned KB is not only incomplete, but also noisy, since it is extracted imperfectly from the web. For example, a football team might be wrongly recognized as two separate entities, one with connections to its team members, and the other with a connection to its home sta-dium. Second, the inference problems are large.

Following prior work [37, 38], we used a number of varying-sized versions of the NELL knowledge base (KB): specifically we took KBs containing 1,000, 10,000 and 100,000 entities, centered around two NELL concepts,  X  X oogle X  and  X  X aseball X . We took M NELL queries from these KBs to use for training queries, and a disjoint M to use for testing queries. All facts not associated with these queries were used as the database. We used M = 5000 for the 100k-entity KBs, M = 1000 for the 10k-entity KBs, and M = 100 for the 1k-entity KBs 10 . Note that the baseline method of predicting using the database would, by construc-tion, achieve an average precision of 0% on these test sets.
The performance of ISG on these tasks is shown in Fig-ure 5 and Table 8. (Runtime is using 20 threads for learning on a conventional desktop machine.) Even though the data is noisy, ISG learns large and useful theories X  X heories such that the high-confidence predictions do indeed correspond, in most cases, with facts actually in the NELL knowledge base.
Our overall approach builds on Metagol [25], but differs in many respects: most notably, unlike Metagol, our sys-tem learns probabilistic programs in ProPPR, rather than  X  X ard X  Prolog programs. ProPPR X  X  use of the (approxi-mate) PageRank-Nibble-based proof method leads to many other differences: for instance, to learn recursive programs, Metagol requires a well-founded ordering of the Herbrand base to prevent infinite loops, while ProPPR does not; also, Metagol uses iterative deepening search to find a minimal set of abductions for each positive example, an NP-hard problem in the worst case, while ProPPR X  X  proof methods instead find a large set of approximately minimal abduc-tions. Metagol has been recently extended to learn a certain class of probabilistic programs [26], but has not been used to learn recursive theories of the size considered in this paper.
Experimentally, our experimental comparisons focus mainly on the widely-adopted MLN formalism X  X n partic-ular the approach described by Kok and Domingos [14] which uses a beam search, coupled with pseudo-likelihood based parameter estimation, to learn MLNs. As noted above, use of pseudo-likelihood, while efficient, causes prob-lems in learning multiple related predicates, the specific task addressed here, and other more recently-proposed MLN On these problems, we were not able to successfully run MLN X  X  structure-learning, even with only 1,000 entities. structure-learning schemes (e.g., [24, 12, 15]) do not ap-pear to address this issue. In our preliminary experiments, we have also investigated the performance of LSM [15] on our datasets, but it fails to outperform the existing pseudo-likelihood based MLN structure learning algorithm in Alchemy. We hypothesize that the reason may be that LSM is a variant for learning long clauses for MLNs by ex-amining short recurring patterns (aka Structural Motifs), thus when motifs are too short, the benefits of LSM may not retain. Another issue we encounter with LSM is the complexity of the setup: it includes more than 25 hyper-parameters, making the process of adapting LSM to new problems very difficult.
 The Alchemy implementation of structure-learning for MLNs that we used in our experiments is well-documented and stable, but is based on an arguably suboptimal inference substrate. Faster inference schemes (e.g., [33, 27, 35, 34] could possibly support structure-learning approaches that do not rely on the pseudo-likelihood approximation. Al-though these faster MLN inference methods have not yet been incorporated into structure-learning systems, integra-tion of these lines of work is a plausible alternative to the approach we have described and experimentally tested here. We note, however, there is no theoretical analysis of the complexity of these methods, and experiments with both FROG [33] and LazySAT [35] suggest that unlike ProPPR they still lead to a groundings that grow with DB size, al-beit more slowly; we also caution that heuristic inference speedups based on hand-coded MLNs need not necessarily transfer well to MLNs generated automatically.
 Our work also builds heavily on Lao et al X  X  Path Ranking Algorithm (PRA) [17, 16], which supports structure learn-ing; however, PRA can learn only a very limited type of program (roughly, disjunctions of non-recursive chains of bi-nary predicates). The underlying ProPPR logic used here can be viewed as combining ideas from PRA with stochastic logic programs (SLPs) [7]. Relative to SLPs, ProPPR adds a restart to the random-walk process, and the addition of a more flexible scheme for featurizing the logic. The fea-turization scheme was heavily used in the structure-learning proposed in this paper: in fact, the ability to attach arbi-trary feature sets, computed on-the-fly at proof time, ap-pears to be unique to ProPPR, among first-order logics. To the best of our knowledge, SLPs have not been coupled with structure-learning methods.

ProbLog [8] is an alternative probabilistic logic program-ming formalism, which grounds probabilistic program by converting the space of possible proofs to a binary decision diagram (BDD), which can be very large, in the worst case, for recursive programs. There has been some prior work on learning BDD-based probabilistic programs, however; in particular, they have been used as the substrate for struc-ture learning in Bellodi and Riguzzi X  X  systems SLIPCASE [3] and SLIPCOVER [4]. In these systems, beam search is used to explore a space of probabilistic logic programs, and candidate programs are scored by running a small num-ber of iterations of EM. In past experiments, SLIPCASE has been run with either a small depth bound (e.g., three), or else limited to non-recursive theories, so it is not clear that it will perform well on the larger mutually-recursive programs considered here. Some limited comparison can be seen in prior work [37], which evaluates ProPPR on the We-bKB dataset: ProPPR obtained an AUC of 0.80 here with a simple fixed theory, compared to 0.61 for ProbLog with the same fixed theory, or 0.76 for SLIPCOVER X  X  learned the-ory. However, experimental comparisons with SLIPCASE and similar systems remains a topic for future work. The BDD-based approach seems especially promising in conjunc-tion with approximate reasoning methods [36], but to our knowledge, these have not been integrated with structure-learning approaches.

Our motivating task of knowledge-base completion has also been addressed with radically different methods, no-tably information extraction from text coupled with gener-alization and/or inference methods (e.g., [18, 39]). We focus here exclusively on the inference task, ignoring lexical clues, similar to the early work in this area with PRA [17].
We propose an abductive, meta-interpretive, second-order probabilistic logic based structure learning approach for ProPPR, a recently-developed scalable probabilistic lan-guage [37]. ProPPR is efficient enough to support inference over large, noisy, knowledge bases, and supports parameter learning using a parallelized version of stochastic gradient descent.

We implement structure-learning for ProPPR using a scheme suggested by the Metagol [25] system. We define a second-order abductive logic program where each assump-tion, and each learnable parameter, corresponds to a hy-pothesized ProPPR clause. Structure learning exploits this correspondence, finding clauses using the efficient, paralleliz-able, stochastic gradient descent based parameter-learning framework that exists in ProPPR. In our implementation of iterative structural gradients (ISG), steps in parameter-learning space are interleaved with structural changes to the second-order theory. We mainly experiment with theories of short rules, and our method can be naturally extended to learn longer chains and high-arity predicates.

In empirical evaluations, we show that this structure learning approach obtains promising results on data from several domains and tasks, including reasoning about kin-ship, biomedical reasoning, and a large-scale KB completion task. Additional experiments show that the approach scales well, and can effectively learn theories with hundreds of rules, from thousands of noisy examples, against a database with tens of thousands of facts noisy facts, in a few min-utes of time on a conventional desktop. Hence, compared to popular ILP methods such as FOIL, or pseudo-likelihood based structure learning methods for MLNs, the approach has advantages in both the average precision, and runtime efficiency.
 We thank Stephen Muggleton and Dianhuan Lin for interest-ing discussions of an early version of this paper. We are also grateful to anonymous reviewers for useful comments. This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program, and a Google Research Award. The authors are solely responsible for the contents of the paper, and the opinions expressed in this publication do not reflect those of the funding agencies. [1] Reid Andersen, Fan R. K. Chung, and Kevin J. Lang. [2] Lars Backstrom and Jure Leskovec. Supervised [3] Elena Bellodi and Fabrizio Riguzzi. Learning the [4] Elena Bellodi and Fabrizio Riguzzi. Structure learning [5] Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr [6] William W. Cohen. Data integration using similarity [7] James Cussens. Parameter estimation in stochastic [8] Luc De Raedt, Angelika Kimmig, and Hannu [9] Woodrow Denham. The detection of patterns in [10] Norbert Fuhr. Probabilistic datalog X  X  logic for [11] Geoffrey E Hinton. Learning distributed [12] Tuyen N Huynh and Raymond J Mooney.
 [13] Charles Kemp, Joshua B Tenenbaum, Thomas L [14] Stanley Kok and Pedro Domingos. Learning the [15] Stanley Kok and Pedro Domingos. Learning markov [16] Ni Lao and William W. Cohen. Relational retrieval [17] Ni Lao, Tom M. Mitchell, and William W. Cohen. [18] Ni Lao, Amarnag Subramanya, Fernando C. N.
 [19] Ni Lao, Jun Zhu, Xinwang Liu, Yandong Liu, and [20] Daniel Lowd and Pedro Domingos. Efficient weight [21] Christopher D Manning, Prabhakar Raghavan, and [22] Alexa T McCray. An upper-level ontology for the [23] Alexa T McCray, Anita Burgun, Olivier Bodenreider, [24] Lilyana Mihalkova and Raymond J Mooney.
 [25] Stephen Muggleton and Dianhuan Lin.
 [26] Stephen H Muggleton, Dianhuan Lin, Jianzhong Chen, [27] Feng Niu, Christopher R  X e, AnHai Doan, and Jude [28] Hoifung Poon and Pedro Domingos. Joint inference in [29] Hoifung Poon and Pedro Domingos. Joint [30] J. Ross Quinlan. Learning logical definitions from [31] Matthew Richardson and Pedro Domingos. Markov [32] Sebastian Riedel, Limin Yao, Andrew McCallum, and [33] Jude Shavlik and Sriraam Natarajan. Speeding up [34] Parag Singla and Pedro Domingos. Memory-efficient [35] Parag Singla and Pedro Domingos. Lifted first-order [36] Guy Van den Broeck, Ingo Thon, Martijn van Otterlo, [37] William Yang Wang, Kathryn Mazaitis, and [38] William Yang Wang, Kathryn Mazaitis, Ni Lao, Tom [39] Limin Yao, Sebastian Riedel, and Andrew McCallum.
