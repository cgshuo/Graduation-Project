 To date, data generates and arrives in the form of stream to propagate discussions of public events in microblog services. Discovering event-oriented topics from the stream will lead to a better understanding of the change of public concern. However, as the massive scale of the data stream, traditional static topic models, such as LDA, are no longer fit for topic detection and tracking tasks. In this paper, we propose a central topic model (CenTM), where a Multi-view Clustering algorithm with Two-phase Random Walk (MC-TRW) is de-vised to aggregate the LDA X  X  latent topics into central topic-s. Furthermore, we leverage the aggregation of central topics alternately with MC-TRW and sequential topic inference to improve the scalability in the stream fashion, so as to derive the dynamic central topic model (DCenTM). Specifically, our model is able to uncover the intrinsic characteristics of the central topics and predict the trend of their intensity along a life cycle. Experimental results demonstrate that the proposed central topic model is event-oriented and of high generalization, it therefore can dispose the topic trend prediction effectively and precisely in massive data stream. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Central Topic, Multi-view Clustering, Trend Prediction
Microblog systems, such as Twitter and Sina Weibo, have become one of the most popular and influential social me-dia platforms to share the latest news and express user opinions. For example, more than 80% of the Sina Weibo users participate in the topic discussion of the 2014 FIFA c  X  World Cup Brazil and over 76% of the users post feeds (the messages released by users in social networks, also known as user generated contents) on Sina Weibo when they were watching the games 1 . These short messages are created and published in real time from different people to broadcast the latest scores as well as the user reviews. Discovering valuable topics from microblog stream and predicting their evolution is therefore important for a wide variety of applications, such as emergencies detection [22], user interest modeling [23], controversial topics discovering [20], spatio-temporal visualization [16] and so on.

While a massive amount of microblog feeds are generated in every minute, the low value density of information always spoils our research. For example, according to the analysis of 2,000 feeds during the U.S.-Slovenia game 2 , only 7.6% of the feeds are meaningful, while 29% are useless observations (most boing 90 minutes), 24% are self-promotions for pun-dits and companies, and 6% are spams. The characteristics of microblog feeds, such as volume, velocity and low value density are challenging topic mining and trend prediction in microblog stream: (1) Low value density of microblog feeds makes it difficult for generating a group of topics with meaningful description (coherence) by traditional X  X ag-of-words X  X lustering method-s or latent topic models, such as K-means and LDA. Table 1 demonstrates two lists of topic terms of the final match Germany VS Argentina in World Cup Brazil . They are gen-erated by K-Means and Online-LDA [1] respectively, where the data comes from Sina Weibo, including 45,000 feeds. In Table 1, all of the five topics generated by these two methods are about only one certain event World Cup Brazil . Besides, some of them also mingle with advertisement words such as  X  X ash X . These incoherent or incompact topics may reduce the accuracy of topic trend prediction. (2) Dynamic change of microblog feeds implies to generate new topics continuously. However, classical topic models, such as Online-LDA, don X  X  perform well in extracting mean-ingful topics from a large number of rapidly changing feeds (also called as feed stream).

To tackle the first challenge, we consider the generation of a group of more event-oriented topics, which are along with coherence by latent topic clustering based on co-training model [6]. We call these event-oriented topics as central h ttp://www.199it.com/archives/255612.html. http://www.esquire.com/the-side/feature/world-cup-tweets-062110. top ics . Then, we design a novel multi-view clustering al-gorithm based on a two-phrase random walk (MC-TRW) to obtain a group of central topics. To tackle the second challenge, feeds collection is treated as a stream in temporal order and divided into feed batches at regular time intervals. The central topics of each feed batch can be mined by the MC-TRW algorithm. However, topics in the stream are continuously evolving, which means the central topics of current batch are usually related to those in the prior batch. Therefore, in this paper, the central topics of the feed stream are generated by combining the MC-TRW algorithm with the knowledge of central topics distribution in the prior batch.

To sum up, in this study, we propose a novel topic model, which can extract meaningful topics from feed stream effi-ciently and also can be updated quickly to capture the evolu-tion history of topics dynamically. The elements of the pro-posed topic model include: 1) a central topic model (CenT-M): It is a topic model developed from co-training idea to aggregate the latent topics that are trained by Online-LDA. In this model, we propose a two-phrase random walk to construct an intrinsic relevant matrix among different views and design a multi-view clustering algorithm (MC-TRW) based on this matrix. 2) DCenTM: It is a dynamic variant of CenTM that generates a series of central topics for the microblog feed stream by alternately acting the MC-TRW algorithm and topic inference method on the batch sequence. Furthermore, we develop a topic trend prediction task on the DCenTM topic model for exploratory analysis. The contributions of our work are the following:
The rest of the paper is structured as follows: Section 2 reviews the related work. Section 3 is the pre-processing. Section 4 presents our central topic model, including CenTM and DCenTM. Section 5 describes the topic trend prediction in detail. Section 6 demonstrates the experimental evalua-tion. At last, Section 7 concludes our finding.
Clustering based methods : One line of research is based on the texts or phrases clustering to mine topics. Gong et al. [15] propose a distributed clustering algorithm to dy-namically group user generated subscribed topics into topic clusters, which could unify their supporting structures and achieve better utilization of existing platforms. Adam et al. [24] follow the idea of the query model and retrieve user X  X  events by selectively materializing low-rate user X  X  feeds in underlying event streams. Zimmermann et al. [31] also propose a text stream clustering method with two-level hi-erarchy to detect, track and update global bursty topics and identify local subtopics. Recently, a framework called Twevent [13] has achieved sound performance on event de-tection. This segment-based strategy can primely retain semantic interpretation by considering both internal and external knowledge.

Topic models : Another line of research is on the basis of latent Dirichlet allocation (LDA) [5], a classical proba-bilistic topic model. Some of its variants, such as DTM [4] and Online-LDA [1], are widely used in constructing topic models for text collections. These variants of LDA model with temporal consideration are able to extract topics in real time from a massive amount of texts, and usually perform better than the simple LDA in terms of perplexity metrics. Moreover, in order to extract topics that are oriented to events, some models tend to impose semantic coherence on their topics, including the sparsity based SACM [28] and the knowledge based LTM [7].

Topic prediction : The aim of topic prediction is to learn the rules of how topic changes over time and to predict further trend on the basis of these changes. By granting the topics with corresponding life circle states, [29] can monitor the evolution of topics in a more intuitive way. TM-LDA [27] is a model to mine topics and predict how the topics transit over time. It learns the transition parameters among topics by minimizing the prediction error on topic distributions. In view of the real world occasions, spatial information is also incorporated as to figure out the structural aspects of topic transition [3]. [26] stands on the shoulder of the social media reaction to propose a framework to detect the impact and sentiment shifts of events. Assuredly, all of these methods are effective in the prediction tasks, but they need more side information which is not often accessible.

Multi-view clustering : The multi-view clustering algo-rithm can play to its strengths when the topics generated by K-Means or LDA models are incoherent or incompact. Abhishek et al. [12] propose a co-training approach to mu-tually update the similarity matrices to generate the dis-criminative eigenvectors for spectral clustering. [9] performs the subspace projection in multiple mixture models, and exploits the principle of iterated conditional models (ICM) and Bayesian model to optimize for better performance. [18] and [14] try to model a variant of the non-negative matrix factorization to refine the consensus matrix by all the coefficient matrices. Similar to [18] and [14], we try to exp lore the consensus clustering view, which is a trade-off for all the separate views, while the difference is that our consensus matrix is constructed by a proposed two-phrase random walk .
In this section, we pre-process the microblog stream by selecting high-quality feeds from the whole stream. Ac-cording to statistics, only 36% of the rated microblog feeds are worth reading [2], while less than 10% in microblog stream are topic keywords relevant feeds [19]. Along with such ideas, the feed X  X  quality can be measured by combining different facets, such as 1) social features, including review number, retweet number, praise number, and reading count, 2) content features, including text length and URL impor-tance, and 3) influence features, which depend on whether the author and his followers play a dominant role in the social networks. In this study, we are mainly interested in the social and textual facets, and select the review number , retweet number , text length and URL importance as the attributions for feed quality evaluation.

Microblog feeds are usually ongoing as a stream which is infinite and rapidly changing. Obviously, the global analysis on feed stream is rather difficult. Here, the feed stream S is divided into feed batches S = { D 1 ; D 2 ; :::; D t order to reduce the impact of spam and redundant tweets on the performance of the central topic model, feed X  X  quality in each batch D t is evaluated with an EM based algorithm [19] proposed in our previous work and then high-quality feeds HD t are selected to train the central topic model. Set four attribution scores of the i th feed in the t th batch D follows: where is a smoothing factor.

Here note that the f ( N j i )( j = 1 ; 2 ; 3 ; 4) of different attri-butions are different. According to our statistical analysis [19], the attributions such as the review number and the retweet number satisfy the power-law distribution, so set f ( N j i ) = log ( N j i )( j = 1 ; 2), and set the unsatisfied attri-bution text length as f ( N 3 i ) = N 3 i . As for the URL impor-tance, we first count all the distinct URLs in feed batch D number of the distinct URLs. Then, each URL X  X  importance is calculated as f ( N 4 i ) = log( feed in t th batch.

These four attributions of all feeds form into a score ma-trix F t = { f j ti }  X  R n t 4 , where n t denotes the number of feeds in the t th batch. Then, our quality evaluation algorith-m is employed to estimate the weights of each attribution quality of each feed is:
With the quality evaluation above, the high-quality feeds can be identified from each batch and compose a high-quality feed batch HD t . In addition, an assumption can be hold that the weight vectors  X  t between the different batches are quite similar. Therefore, in the experimental process, the vector  X  t of the t th batch is taken as the initial value of the ( t + 1) th batch to accelerate the convergence of the EM algorithm.
In this section, based on multi-view clustering, we propose a central topic model to aggregate latent topics generated by Online-LDA. Preliminary work of this model is introduced in Section 4.1, including multi-view similarity matrices con-struction and cluster number estimation. Then, Section 4.2 focuses on discussing the two-phase random walk, which is designed to obtain an intrinsic relevant matrix for multi-view clustering. Finally, the central topic model is morphed into a dynamic variant to mine central topics more promptly in feed stream, as discussed in Section 4.3.
Given a high-quality feed batch HD t , the Online-LDA is employed to extract K latent topics and gain two matrices: feed-topic distribution matrix X t  X  R Q t K and topic-term distribution matrix Y t  X  R K V , where Q t is the number of high quality feeds in HD t and V is the number of terms in HD t . These latent topics are usually incoherent, resulting in poor performance of topic trend prediction. To tackle this problem, central topics with more summarized descrip-tion terms are further extracted by aggregating these latent topics.

In fact, the matrix of feed-topic X t and the matrix of topic-term Y t are able to describe the latent topics from two different views: term distribution and feed relevance. In this paper, these two matrices are used for designing a multi-view clustering to aggregate the latent topics.
One important preparation point of multi-view clustering is to construct two kinds of similarity matrices for the views. For the term distribution view, a symmetrical fashion of Kullback-Leibler divergence (KLD) metric is introduced to measure the similarity of every two latent topics Y t i and Y Thus, a similarity matrix W t 1  X  R K K is constructed via Eq.2 to describe the latent topics from the view of term distribution.

For the feed relevance view, we consider to use topic rel-evant feeds collection to construct the similarity matrix. Each column X t i ( i = 1 ; 2 ; :::; K ) in the feed-topic matrix X reveals the related feeds set of the latent topics Y t i to some extent. For a certain latent topic Y t i , if its membership is larger than a given minimum similarity threshold, then the feed HD t i is included into its related text set RD t i . In this way, each feed can be allocated to different latent topics to form the topic relevant feeds set. At last, the Jaccard metric is implemented to measure the similarity between latent topics Y t i and Y t j :
Al gorithm 1: Multi-view Clustering based on Two-phrase Random Walk(MC-TRW)
Input : Feed-topic distribution matrix X t and Output : Central topics with term distribution matrix 1 Generate the similarity matrix W t 1 with Eq.2 for the term distribution view; 2 Generate the similarity matrix W t 2 with Eq.3 for the feed relevance view; 3 Estimate the number of the central topics r t according to section 4.1.2; 4 Set G t j = 0 , V t i = W t i ; i = 1 ; 2; 5 for i  X  1 ; K do 6 V t i ( k; k ) = 0 ; i = 1 ; 2; 7 G t i ( k; k ) = sum ( V t i ( k )) ; i = 1 ; 2; 8 end 9 Set probability transition matrices: 10 Calculate the intrinsic relevant matrix:
IR t = ( I  X  (1  X  )(1  X  ) P t 1 P t 2 ) 1 ( (1  X  ) P t 2 I + I ); 11 Normalize the intrinsic relevant matrix:
IR t = ( IR t + ( IR t ) T ) = 2; 12 Perform spectral clustering on IR t to generate central topics; Th en, another similarity matrix W t 2  X  R K K is constructed via Eq.3 to describe the latent topics from the view of feed relevance.
Another key point of the multi-view clustering is how to decide the number of the clusters. The decision of the cluster number is widely discussed in self-adaptive clustering. Here, we just take a simple but effective approach to estimate the cluster number.

In terms of the two views, the term distribution of the latent topic (i.e., W t 1 ) is more essential and is treated as the dominant view. Generally, the top-N eigenvalues of the graph Laplacian of W t 1 have strong discrimination ability for clustering [12]. Based on this, we design a clustering number estimation approach with following steps: 1) rank all eigenvalues of the graph Laplacian in descending order; 2) set a container with capacity cont =  X  evsum , where is a ratio and evsum is the sum of all the eigenvalues in the graph Laplacian ; 3) put the eigenvalues into the container in turn until the sum of the eigenvalues in this container is larger than cont ; 4) the number of the eigenvalues in the container is the estimated value of the clusters number.
We propose a two-phase random walk based on the restart version of random walk [25] to construct the intrinsic rele-vant matrix in a heuristically way. In the random work with restart model, the node i jumps to other nodes with transi-tion probabilities { p ik } ( k = 1 ; 2 ; :::K; i  X  = k ), and jumps to itself with probability p ii = . Usually, the transition yields to a steady state after several times of jumping. Here, in the multi-view clustering, the intrinsic relevant matrix between the latent topics also changes as the walker goes by and the walker walks crosswise between two views, we call it as two-phase random walk .

In order to construct the intrinsic relevant matrix based on the two-phase random walk, the similarity matrix W t 1 and W 2 are regarded as archetypes of the probability transition matrices in random walk model. What calls for our special attention is that the diagonal elements in the probability ma-trix must be set as zeroes. Thus, the two similarity matrices W i ( i = 1 ; 2) are modified to form the non-self similarity matrix V i ( i = 1 ; 2) to fit the random walk paradigm as follows: And the probability transition matrices are: where G t i is the diagonal matrix of V t i and G t i ( k; k ) is the sum of the k th row of the V t i .
 We define the intrinsic relevant matrices as IR t i ( i = 1 ; 2). Then the deduction of them can be described as a two-phase fashion: where and are the self-transition probabilities. During the first phase in s th iteration, the intrinsic relevant matrix IR 1 is used to update IR t 2 , and vice versa in the second phase. Obviously, Eq.5 can be simplified as a recursive form as: Set: Then Eq.6 can be rewritten as:
Following the convention [25], it can be clear that the intrinsic relevant matrices can converge to a certain matrix, which is irrelevant to the initial value. Then the converged IR t can be directly computed as follows:
The converged intrinsic relevant matrix IR t of the two views is the key to the latent topic clustering. Usually, the symmetry of the IR t cannot be guaranteed, so set IR t = ( IR t + ( IR t ) T ) = 2 as the symmetrical matrix.
Finally, the IR t is used as the similarity matrix in spectral clustering [17] to generate central topics. The clustering result lays out the partition of the latent topics, each center of the cluster represents a central topic, which is highly event-oriented and summarized. The whole process of this aggregation algorithm is called as the Multi-view Clustering based on Two-phrase Random Walk (MC-TRW), which is described in Algorithm 1.
In Section 4.2, the multi-view clustering is used to con-struct the central topic model. However, as the huge volume of microblog feed stream and with the real-time requirement, Fi gure 1: The whole procedure of before-topic inference applying the multi-view clustering method to mine central topics for every feed batch may not be suitable. Fortunately, rapidly and progressively changed feed stream usually makes two consecutive batches share a certain number of common topics. It means that, if the previous knowledge of the central topics of the ( t  X  1) th batch is acquired, then the central topic of the next stage (i.e., t th batch) can be inferred from it. We name this idea as before-topic inference .
Along this line, we propose a dynamic central topic mod-el. In this model, we not only suppose that the two-phase random walk of the multi-view clustering is vital in gener-ating central topics, but also take the before-topic inference idea into consideration to maintain a balance between time consumption and accuracy.

Now, let X  X  discuss how to infer the central topics of the t batch from that of the ( t  X  1) th batch in detail. Assuming that: 1) central topics of the ( t  X  1) th batch have been where r t 1 is the number of the central topics; 2) the latent topics of the t th batch has been obtained by Online-LDA, denoted as the term-topic distribution matrix Y t (  X  R K V and 3) the central topics number of the t th batch has been estimated by its term distribution matrix Y t , denotes as r This estimation process has been discussed in Section 4.1.2. Then, the central topics CY t of the t th batch can be inferred from the CY t 1 and Y t with the following steps:
First, initial state of the central topics matrix CY t 0 is constructed by considering two situations of the relationship between r t 1 and r t :
If r t 1  X  r t , select the whole r t 1 central topics CY as a part of initial state of the t th batch X  X  central topics, namely CY t 0 = [ CY t 1 ; 0 ; :::; 0 ].

If r t 1 &gt; r t , select the top-r t central topics from CY according to topic cluster size as the initial state of the t batch X  X  central topics, namely where topic cluster size means the included number of the CY t 1 with the r th largest latent topic cluster.
Second, update the central topics by allocating each latent topic to each existed central topic cluster. By computing the similarity matrix LC t (  X  R K r t ) between each latent topic Y t j ( j = 1 ; 2 ; :::K ) in Y t and each initial central topic to the most similar initial central topic to form the clusters. Thus, each initial central topic is updated by the center of each cluster. The updated central topic matrix is denoted as CY t 1 .

Third, delete the disappeared central topics from the CY t and add new ones in it. Obviously, not all the central topics inherited from CY t 1 still appear in this batch. That is to say, in matrix LC t , there may exist columns that all elements are zeroes. Similarly, there may exist some new latent topics that cannot be revealed by the CY t 1 . This sit-uation is manifested as there are some rows in the LC t with all elements that are less than a given minimum similarity threshold . These elements are the new latent topics, which are not belong to any existed central topics and need to be aggregated as new central topics. As the total central topics number has been estimated, the new central topics num-bers nr t is then inferred from disappeared topics numbers (denoted as dr t ) with the following formula:
With the nr t and new latent topics, the spectral clus-tering is then applied to aggregate the new central topics NCY t . Finally, the central topics CY t of the t th batch is the constitution of the new topics NCY t and the inherited old topics, namely, CY t = [ NCY t ; CY t 2 ], where CY t subset of CY t 1 in which rows denoting disappeared topics are removed. The whole process is shown in Figure 1.
In this section, the focused central topics (called topics for short) are used to predict the trend of the topic intensity in microblog stream. Rather than to track the topic life circle as in [29], we turn to predict the topic trend and intensity, which also reveal the popularity of this topic.
To put it simply, topic intensity can be measured as the proportion of the topic relevant microblog feeds in a feed batch. A series of intensity values of a certain topic pro-duced by a feed batch sequence compose a time series, which enables the intensity of the topic predicted by regression analysis. However, this way of prediction is far from enough, b ecause some intrinsic characteristics, such as the trend and the fluctuation behind the intensity are more charming. In this study, to model the trend prediction, we assume that the topic intensity consists of two parts: topic trend and topic fluctuation. The former stands for the intrinsic com-ponent of the topic intensity, denoted as TT = { T T i } ( i = 1 ; 2 ; :::; T ), while the latter uncovers the entropy of the topic, denoted as TF = { T F i } ( i = 1 ; 2 ; :::; T ), T is the time span (i.e., lasting batches number) of topic. Specially, the topic fluctuation in each batch can be evaluated by the inversed entropy of the topic: where all the p ( w i ) is revealed in the topic term distribution matrix CY t . In order to integrate TT and TF to explain the topic intensity, the two components are normalized to [0, 1] and integrated with Eq.8: where is the weight of the trend. Then for a central topic z , the intensity time series can be represented as TI = {
T I i } ( i = 1 ; 2 ; :::; T ). Here, the topic trend is a latent vari-able, which could not be observed by the present knowledge.
Generally, the Hodrick-Prescott (HP) filter [10] is a rea-sonable way to extract the trend component in many e-conomic applications. However, as the time series of the topic intensity is not strictly stationary, the HP filter is unaccommodated in this situation. Therefore, we resort this problem to the Kalman filter [11]. Kalman filter is a model for estimating the change of the states recursively in the dynamical system with the help of the relevant observations.
In this paper, the topic life circle can be treated as a stochastic dynamical system, which is mainly composed of topic trend and topic fluctuation. Then the states change along the life circle as: where CF t = [ T T t ; T F t ] T is the combined state, U a transition matrix from state CF t to CF t +1 , and ! t is a Gaussian noise with mean zero and covariance  X  !;t .
At the same time, the topic intensity formula can be rewritten in matrix style: where M = [ ; (1  X  )] is a measurement matrix, t is a Gaussian noise with mean zero and covariance  X  ;t . Eq.9 and Eq.10 altogether define the state space model of the Kalman filter. The former is called as the state equation and the latter is the measurement equation.

In the Kalman filter based topic prediction model, given the topic intensity time series { T I 1 ; T I 2 ;  X  X  X  ; T I measurement matrix M , our goal is to predict the topic trend and topic fluctuation factor d CF t +1 j t as well as the topic intensity c T I t +1 j t in time t +1, which can be described as follows: F eeds (after filtering) 6.6 million 2.3 million Ba tches 10 2 46 2
Tim e span 20 14/01/01 20 12/10/01
Rep resentative W orld Cup NB A ev ents T ransformers IV Ip hone 5s is the expectation operator, and d CF t j t 1 is the single step estimation of the state when all the state information is given before t .

By employing the Kalman filter, we can not only predict the topic intensity, but also have an insight into the trend and fluctuation in future step, which uncovers more intrinsic characteristics along the topic life. For multi-step predic-tion, the topic trend and topic intensity can be calculated iteratively with the computation of Eq.11 and Eq.12.
We evaluate the performance of our approach on two dataset-s. The first is on the Sina Weibo 3 , a popular Chinese mi-croblog service that enables users to post short text messages of up to 140 characters. In this paper, 54 collections of feeds are used as experimental dataset. Each of them is a collection of relevant feeds of a hot event and is crawled by our SNS Hunter 4 by inputting the hashtag and start&amp;end timestamp of the issue. In order to evaluate the performance of the proposed topic models, all of these collections are mixed together in temporal order to form a microblog feed stream. The second is the Twitter user generated contents given by [30].

The Sina Weibo feeds are divided into 102 equal-interval batches (two days per batch), while the Twitter feeds are divided into 462 batches (one day per batch). The detailed information of these two datasets is listed in Table 2.
The review number , retweet number , text length and URL importance are employed as the attributions to measure the tweets quality. Each feed in the stream gains normalized scores from the four attributions by Eq.1. Ten sequential feed batches are selected for evaluation, and the weights in each batch are presented in Figure 2.

From Figure 2, we can see the weights of these four attri-butions are approximately the same in every batch, which means our EM based algorithm is an appropriate method to evaluate the weights of different attribution. And this also assures that our initialization in the EM weight scoring is quite reliable. In addition, the review number is a very h ttp://weibo.com/. http://sc.whu.edu.cn/. important factor to the contribution of the high quality, which confirms to our cognition of high quality feeds.
Generalization ability is a basic indicator for a certain model or a system. Therefore, we use perplexity as a metric to evaluate the topic model X  X  capability of predicting unseen data. Formula of the perplexity is shown as follows: wh ere M is the model learned from training dataset, p ( w is the joint probability of all the words in the text d under this model and V d is the number of words in the text d . A lower perplexity score indicates better generalization ability of the model.

Three baselines are chosen for comparison. The first is called as CenTM-MC, in which the central topics of all batches are generated by MC-TRW. The second is called as CenTM-SP, in which the central topics are generated by spectral clustering of the latent topics. The last is the Online-LDA (O-LDA). All the batches in both two datasets are used to evaluate the perplexity of different models. The latent topic numbers in Sina Weibo dataset are set as 50, 100 and 150 respectively. As for the Twitter dataset, the topic numbers are set as 20, 40 and 60 respectively.
Figure 3(a) and 3(b) show the average perplexity of dif-ferent topic models with the different latent topic numbers K . In most cases, the perplexity of the DCenTM is the lowest, which demonstrates the considerable generalization ability. At the macroscopic level, with the increasing of K , the perplexity also grows respectively to indicate the lower generalization ability of the numerous topics. Besides, the perplexity among the CenTM-MC, DCenTM and CenTM-SP are quite similar, as they are all based on the theory of LDA and with the clustering of the latent topics. It is also worth noting that the growth of the perplexity of the CenTM-SP behaves sharply in both two datasets, while the CenTM-MC doesn X  X . This means that the MC-TRW outperforms the spectral clustering in the aggregation of the latent topics. In terms of these two cases, the DCenTM acts the most steadily, while the O-LDA gives an awful result. This enhances our confidence in the robustness of the central topic model in dealing with data streams.
The main difference between our central topics and tra-ditional latent topics is the distinctiveness among topics. To evaluate this, the average Kullback-Leibler divergence (KLD) is employed.

As shown in Figure 3(c) and 3(d), the average KL diver-gence of the central topics (in CenTM-MC, DCenTM and CenTM-SP) are higher than the latent topics (in O-LDA), which indicates that the central topics are more unique than the latent topics. This also has a better accordance with our assumption that the central topics are more event-oriented and share fewer common points.

Apart from the KL divergence, the point-wise mutual information (PMI) is often used to quantitatively measure the coherence of the topics [21]. But in our experiments, we resort to other metrics as the computation of PMI depends on an external corpus. In order to intuitively understand the coherence and distinctiveness of the central topics, we make them correspond to the real world events which are pre-labelled in the dataset. Table 3 depicts the map of the events and the central topics in a certain batch 5 . Here, the central topics in the DCenTM is aggregated by before-topic infer-ence. The symbol  X # X  represents an event, and each event can be described in different topic terms, e.g., the event  X #1 World Cup X  comprises of two topic terms,  X  X orld Cup X  and  X  X ermany X . In this feed batch, there exists four true events (#1, #2, #3 and #4), while the estimated central topic number is seven (the sum of the number in the bracket of each topic model). If the number of the central topics is equal to that of the events, then each central topic can be exactly reflected to each event, which lives up to an excellent central topic model, but this can X  X  be always available in real world dataset. The number in the bracket is the occurrence of this central topic. If this number is greater than 1, there must be an overlap between the central topics. In this table, we can see all of the central topics in CenTM-MC and DCenTM are highly consistent with the events, fewer overlaps (Only the World Cup) or fewer vacancies. What X  X  more, they are capable of discovering new topic terms of the events, e.g., the  X  X raduation X  of the event  X  X EE X  (short for College Entrance Examination). Compared with CenTM-MC, DCenTM can uncover the  X  X ermany X  from the even-t  X  X orld Cup X , which performs the best. However, the central topics of the CenTM-SP are rather poor, as they suffer from overlaps heavily, especially for the central topic  X  X orld Cup X . Next, we compute the events-topics consis-tency (ETC) of the CenTM-MC, DCenTM, CenTM-SP and O-LDA with
Here the overlap is the number of central topics that occur more than once, e.g., in Table 3, overlap(CenTM)=3-1=2 , overlap(CenTM-SP)=6-1=5 . We additionally incorporate the knowledge based topic model LTM[7] as a baseline. As for the latent topics in O-LDA, we compute the ETC in regardless of the overlap to simplify the problem of its loose-ness. The ETCs of these five methods in ten batches are shown in Figure 3(e) and 3(f). The latent topics number K is set as 50 in Sina Weibo dataset and 20 in Twitter dataset. We can see that the ETC values of the CenTM-MC, DCenTM and CenTM-SP are higher than O-LDA in most cases, which is a powerful evidence for the event-oriented characteristic of the central topics. And because of the simple spectral clustering, the central topics generated by
Th e map of events and topics is organized manually. CenTM-SP are not event-oriented enough, so the ETCs of the CenTM-SP are clearly lower than those of the CenTM-MC and DCenTM. The performance of LTM is quite similar ro DCenTM, because its topics are released according to knowledge. However, as its topic number is fixed, it can not deal with the topic overlap, resulting in slightly lower ETCs compared with DCenTM.
Based on the model evaluation methods proposed in Sec-tion 6.2.3, we compare the running time of CenTM-MC with DCenTM. They are implemented on ten sequential batches in Sina Weibo dataset to obtain central topics re-spectively and the running time of them is presented in Figure 4(a). Obviously, the running time of CenTM-MC and DCenTM are strictly equal in some batches. In these cases, the central topics are all aggregated by the MC-TRW. As for the other cases, the central topics in DCenTM are generated by before-topic inference, which costs less time in comparison to the MC-TRW. From the results in Figure 4(b) and 4(c), we may also see that, to process a certain number of feeds, the running time of DCenTM grows more slowly than the CenTM-MC and CenTM-SP, indicating a better scalability in data streams. Unlike these three models, the LTM needs to execute LDA and frequent itemset mining as to obtain knowledge in advance, so it seems a little more time-consuming.
In this section, we mainly evaluate the accuracy of the topic trend prediction as well as the topic intensity predic-tion. Obviously, the topics are all referred to the central topics unless with special address.

With Eq.11 and Eq.12, we can compute and update the topic trend and intensity alternately to approach the desti-nation. However, in this case, the state transition matrix U is not a fixed one, which is different from the standard linear Kalman filter. In our scenario, the states are the trend and the fluctuation (related to topic entropy) with no dependency, thus U is a two-dimension diagonal matrix. Since the observed topic intensity has a positive correlation with the latent trend factor, so the first diagonal value of U t;t 1 can be estimated by the slope between T I t 1 and T I while the second diagonal value of U t;t 1 can be estimated by the slope between T F t 1 and T F t . For the U t +1 ;t stepwise Support Vector Regression(SVR) [8] can be taken into the observation of the topic intensity and fluctuation before time t +1.

For Sina Weibo dataset, experiments are conducted on the topics between June 2, 2014 and July 21, 2014 with totally 25 time points (two days per time point). Specifically, we choose the World Cup as the example to perform the multi-step prediction. We therefore release the topic intensity series from June 2 to July 11 to predict the topic intensity and trend in the next five points. For the Twitter dataset, we choose the event NBA . Its selected time span is from November 28, 2013 to December 22, 2013, also with totally 25 time points, and the last 5 points are left out to verify the prediction. We name our prediction method as KFP, meanwhile the SVR is employed as a baseline.
 As the trend is a latent intrinsic factor uncovered by the Kalman filter model, there is no such thing as the standard trend value. However, according to Eq.8, once the intensity and the fluctuation are fixed, the trend can also be calculated respectively and this value of trend can be regarded as the standard trend for evaluation. Here, we set the weight of the trend to 0.8, and then we use the residual as the measurement to present the results of the prediction.
Figure 5 shows the topic trend, fluctuation and intensity along all the tested time points in Sina Weibo dataset. We can find that the fluctuation changes steadily along the time, and the trend is tightly coherent with the intensity. What X  X  Fi gure 5: A depiction of topic intensity, trend and fluctuation more, we get the trend and intensity at a local maximum around the 22nd time point (in July14&amp;15). It is well known that the final of the World Cup was held in July 14, thus the discussions explode heavily in this time point. After the world cup, the number of the relevant feeds decreases sharply, which leads to the decline of the topic intensity and trend. Because of this sudden change of the topic, the fluctuation also behaves unnaturally compared with the previous trace.

Table 4 describes the relative residuals of the multi-step prediction in both two datasets. In Table 4, the  X  X  X  repre-sents the intensity,  X  X  X  represents the trend and the  X  X  X  represents the fluctuation. The left list in each column indicates our method KFP, while the right one denotes the baseline SVR. The results in Table 4 demonstrate that our prediction method outperforms the SVR in both the sin-gle step and multi-step cases in considering of the relative residual. Furthermore, with the step increasing, the errors of the prediction become much severe in both of the KFP and SVR. We also should note that the residuals for the second step in Twitter dataset are abnormally higher than the rest steps, this is because with the pausity of relevant tweets in that day, the prediction accuracy becomes more impressionable. This will motivate us to establish a more robust prediction framework in the future.
In this study, we address the challenges of mining mean-ingful topics in microblog feed stream. Based on the multi-view characteristic of the latent topics, we propose a nov-el central topic model with the combination of multi-view clustering in two-phase random walk and develop it with before-topic inference to better fit the stream. The pro-posed model is able to generate event-oriented topics for static feed collections as well as the feed stream, and can be reasonably applied in real-time applications. We choose cen-tral topics rather than latent topics to fulfill the predefined mining tasks. Experimental results show that our approach can establish topic models with high generalization ability. Meanwhile, these topic models can extract a group of cen-tral topics that are event-oriented and more discriminative between each other. In addition, the central topics perform well in the application of the topic trend prediction.
This research is supported by the Natural Science Founda-tion of China(No.61472291, No.61272110, No.71420107026), and the China Postdoctoral Science Foundation under con-tract No.2014M562070. [1] L. AlSumait, D. Barbar  X a, and C. Domeniconi. On-line [2 ] P. Andr  X e, M. S. Bernstein, and K. Luther. Who gives [3] S. Ardon, A. Bagchi, A. Mahanti, A. Ruhela, A. Seth, [4] D. M. Blei and J. D. Lafferty. Dynamic topic models. [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [6] A. Blum and T. Mitchell. Combining labeled and [7] Z. Chen and B. Liu. Topic modeling using topics from [8] H. Drucker, C. J. Burges, L. Kaufman, A. Smola, and [9] S. G  X  unnemann, I. F  X  arber, and T. Seidl. Multi-view [10] R. J. Hodrick and E. C. Prescott. Postwar us business [11] R. E. Kalman. A new approach to linear filtering and [12] A. Kumar and H. D. III. A co-training approach for [13] C. Li, A. Sun, and A. Datta. Twevent: segment-based [14] J. Liu, C. Wang, J. Gao, and J. Han. Multi-view [15] T. Milo, T. Zur, and E. Verbin. Boosting topic-based [16] M. Musleh. Spatio-temporal visual analysis for [17] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral [18] F. Pan, X. Zhang, and W. Wang. Crd: Fast [19] M. Peng, J. Huang, and H. Fu. High quality microblog [20] A. Popescu and M. Pennacchiotti. Detecting [21] M. R  X  oder, A. Both, and A. Hinneburg. Exploring the [22] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake [23] K. Sasaki, T. Yoshikawa, and T. Furuhashi. Online [24] A. Silberstein, J. Terrace, B. F. Cooper, and [25] H. Tong, C. Faloutsos, and J.-Y. Pan. Fast random [26] M. Tsytsarau, T. Palpanas, and M. Castellanos. [27] Y. Wang, E. Agichtein, and M. Benzi. Tm-lda: [28] Y. Xu, T. Lin, W. Lam, Z. Zhou, H. Cheng, and [29] X. Yang, A. Ghoting, Y. Ruan, and S. Parthasarathy. [30] X. Zhu, Z.-Y. Ming, and Y. Hao. Customized [31] M. Zimmermann, I. Ntoutsi, Z. F. Siddiqui,
