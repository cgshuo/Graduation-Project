 We investigate the impact of query result prefetching on the efficiency and effectiveness of web search engines. We pro-pose offline and online strategies for selecting and ordering queries whose results are to be prefetched. The offline strate-gies rely on query log analysis and the queries are selected from the queries issued on the previous day. The online strategies select the queries from the result cache, relying on a machine learning model that estimates the arrival times of queries. We carefully evaluate the proposed prefetching techniques via simulation on a query log obtained from Ya-hoo! web search. We demonstrate that our strategies are able to improve various performance metrics, including the hit rate, query response time, result freshness, and query degradation rate, relative to a state-of-the-art baseline. H.3.3 [ Information Storage Systems ]: Information Re-trieval Systems Design, Performance, Experimentation Web search engine, result caching, prefetching
Commercial web search engines are expected to process user queries under tight response time constraints and be able to operate under heavy query traffic loads. Operating under these conditions requires building a very large infras-tructure involving thousands of computers and making con-tinuous investment to maintain this infrastructure [7]. Opti-mizing the efficiency of the web search systems is important to reduce the infrastructure costs. Even small improvements may immediately translate into significant financial savings for the search engine.

Recent research has shown that result caching [5, 15] is a viable technique to enhance the overall efficiency of search engines. The main idea in result caching is to store the results of frequently or recently processed queries in a large cache and readily serve subsequent occurrences of queries by the cache. This way, significantly more expensive computa-tions at the backend query processing system are avoided, leading to important efficiency gains in the form of reduction in query response latencies and backend query workloads.
Unlike earlier works that have a focus on limited-capacity caches [5, 15], more recent works assume result caches with infinite capacities [11]. The main reason behind this infinite cache assumption is the cheap availability of storage devices that are large enough to store the results of all previous user queries issued to the search engine. Under the infinite cache assumption all future queries can be served by the cache except for compulsory misses which have to be served by the search backend. However, an infinite result cache suffers from the staleness problem [11]. The dynamic nature of web document collections requires frequent index updates, which may render some cache entries stale [1, 8]. In some cases, the search results served by the cache may not be fresh enough in terms of their content and this may degrade the user satisfaction.

In practice, an effective solution to the freshness prob-lem is to associate every result entry in the cache with a time-to-live (TTL) value [2, 11]. In this technique, a cache entry is considered stale once its TTL expires. The hits on the expired entries are treated as misses and are pro-cessed by the backend, leading to fresh results. This way, the TTL approach sets an upper bound on the staleness of any search result served by the cache. Unfortunately, it sacri-fices some of the efficiency gains achieved by means of result caching. Since the cache hits on the expired cache entries are treated as cache misses, the query traffic volume hitting the backend search system significantly increases with re-spect to a scenario where no TTL value is associated with the cache entries. In general, the increased backend query volume leads to an increase in the query response latencies and more backend resources are needed to handle the query traffic. Moreover, there is a higher risk that the spikes in the query volume will lead to an overloaded backend, in which case certain queries may have to be processed in the degra-dation mode, i.e., search results are only partially computed for these queries and the user experience is hampered.
The above-mentioned negative consequences of the TTL approach can be alleviated by combining it with a prefetch-ing 1 strategy that has the goal of updating results of cache entries that are expired or about to be expired before a user requests them [11]. An ideal prefetching strategy would have all queries be readily served by the cache. In practice, there is no perfect knowledge of queries that will be issued in the future. Hence, prefetching strategies can only be heuristics.
The observations made before form the main motivation of this paper. In particular, we aim to devise strategies to identify cache entries that are likely to be requested when they are expired. We proactively fetch the associated search results using the idle compute cycles of the backend. The main challenge associated with prefetching is to predict when an expired cache entry will be requested. The pre-dicted times can be used to select queries whose results are worth prefetching and to prioritize them to obtain the high-est performance benefit. Although related ideas on cache freshness [11] and batch query processing [12] appear in the literature (see Section 7), our work is novel in terms of the following contributions.
The rest of the paper is organized as follows. In Sec-tion 2, we discuss our system model and motivate the result prefetching problem through observations made over a real-life web search query log. In Sections 3 and 4, we present the proposed offline and online prefetching strategies, respec-tively. We provide the details of our data and experimental setup in Section 5. The experimental results are presented in Section 6. We provide the related work in Section 7. The paper is concluded in Section 8.
System architecture. In this work, we assume the search engine architecture shown in Fig. 1. User queries are issued to the search engine X  X  main frontend, which con-tains an infinite result cache where the entries are associated with a TTL value. Queries whose results are found in the cache and not yet expired are readily served by the result cache. Otherwise, they are issued to the frontend of a se-lected backend search cluster, which is composed of many nodes that host a large index build on the document col-lection (only one cluster is displayed in the figure). After a
The term prefetching is used in [13, 16] differently to imply requesting the successive results pages for a query. Figure 1: The sketch of a search engine architecture with a query prefetching module. query is processed at the backend cluster, the computed re-sults are cached together with the time of the computation so that the expiration time can be determined. The query prefetching module interacts with both the search engine frontend and the search cluster frontend. This module is responsible for selecting a set of queries that are expired or about to be expired (from the query logs or the result cache) and issuing them to the search cluster X  X  frontend, which is-sues them to the backend. The results computed by the backend are then cached like regular user queries.

Query prefetching problem. Our focus in this work is on the query prefetching module. The idea behind this mod-ule is to avoid, as much as possible, potential cache misses by proactively computing, i.e., prefetching, the results of queries that are about to expire before they are requested by the users. At first glance, the problem of prefetching looks trivial as it seems easy to identify queries that are expired or about to be expired. The problem, however, is quite challenging because not all prefetching operations are useful and prefetching the results of a query consumes back-end resources. In particular, if the results of a query are prefetched, but the prefetched results are not requested be-fore they are expired, prefetching leads to waste of resources.
The most important benefit of prefetching is the increase in the cache hit rate [11]. This immediately corresponds to reduced average query response times as more queries can be served by the cache. In addition, the freshness can also be improved if unexpired cache entries are prefetched. 2 Fi-nally, the fraction of queries whose results are computed in the degraded mode can be reduced if prefetching can de-crease the amount of processing at peak query traffic times. In summary, the benefits expected from prefetching are re-duced query response time, improved result freshness, and reduced query result degradation.

The query results need to be prefetched, as much as possi-ble, when the user query traffic volume is low so that the user queries are not negatively affected from the query processing overhead incurred due to prefetching. Hence, the feasibility of prefetching depends on the availability of the low traf-fic hours. This raises the question whether the low traffic periods are long enough to prefetch sufficiently many query results. Moreover, the fraction of queries that can benefit from prefetching needs to be quantified. Finally, the poten-tial risk for query result degradation needs to be identified.
It is interesting to note that prefetching only expired cache entries results in increased staleness. In fact, cache staleness not necessarily impacts on results freshness as some expired results might not be retrieved in the future. Figure 2: The user query traffic hitting the backend under differ-ent TTL values. We will look into these issues in what follows by analyzing a sample taken from the query traffic received by Yahoo! web search during a particular day.

Motivating observations. The upmost curve in Fig. 2 shows the user query traffic received by the search engine, i.e., the traffic that would hit the backend if there was no result cache. The bottom curve indicates the backend traf-fic in case of an infinite result cache with an infinite TTL, i.e., only the compulsory cache misses hit the backend. The curve in between shows the backend traffic volume when the TTL is set to one hour. We observe that, during the peak hours, a significant amount of cache misses (about one-third) are due to expired cache entries.

According to Fig. 3, there is a good amount of opportunity for prefetching (especially during the night) when compared to the amount of requests on expired cache entries. We ob-serve that the traffic volume reaches its peak from 11AM to 4PM, resulting in a risk for query result degradation in scenarios where the backend traffic rate is comparable to the peak sustainable throughput (PST) of the backend (e.g., when the TTL is one hour). In general, the backend query load decreases with increasing TTL values, but the poten-tial for reduction in the query load due to prefetching also decreases since there are fewer expired queries.

In Fig. 4, we try to quantify the existing opportunity for prefetching, the risk of overflow, and the potential for load reduction, assuming two different TTL values and three dif-ferent PST values. The reported values are in terms of queries per second, averaged over the entire day. As an ex-ample, with a TTL of one hour and a PST of 20 query/sec, the overflow rate is about five queries per second.
Prefetching strategies. We evaluate two alternative types of prefetching strategies. The first set of techniques are offline and rely on query log mining. In these strategies, the prefetched queries are selected from the previous day X  X  query logs. The second set of prefetching strategies are online. The idea here is to predict the next occurrence times of cached queries and try to prefetch expired or about-to-expire cache entries before they are requested while being in an expired state. Both types of strategies have two phases: the selection phase, where the queries whose results are to be prefetched are determined, and the ordering phase, where the selected queries are sorted in decreasing order of their importance and are prefetched in that order.
There is a high correlation between the repetition of some queries and the time of the day, i.e., some queries are more likely to be issued around the same time of the day. Accord-ing to Fig. 2 in [10], a large fraction of queries are submitted about 24 hours after their latest submission. Hence, a rea-sonable strategy is to prefetch certain queries in the previous day X  X  query logs. This strategy is based on query log mining and is completely offline. Hence, the query selection process does not incur any overhead on the runtime system.
Let T denote the TTL of the result cache. Let  X  denote the current time point and  X  0 denote the time point exactly 24 hours before  X  . Assume that the time interval [  X  0 , X  ) is split into N time intervals, each interval having a length of s = (  X   X   X  0 ) /N time units. Let  X   X  1 , X  2 ,..., X  N  X  be a sequence of N time intervals, where  X  i = [  X  0 + ( i  X  1)  X  s, X  0 Each time interval  X  i is associated with a query bucket b that stores the queries issued to the search engine within the respective time interval. The queries to be prefetched during a particular future time interval [  X  + ( i  X  1)  X  s, X  + i  X  s ) are limited to those in the set of buckets  X  b i ,b i +1 ,...,b i.e., the queries issued within the past time interval [  X  1)  X  s, X  0 + ( i  X  1)  X  s + T ). Hence, the prefetched queries are selected as At the end of a time interval  X  i , we switch to the next query set Q i +1 and start prefetching results of queries in that set.
Since it may not be possible to prefetch all queries as-sociated with a time interval, the queries need to be or-dered based on their importance, i.e., the benefit that can be achieved by prefetching a query. While ordering the queries, the past frequency of a query plays an important role. Herein, we discuss three simple strategies to determine the importance of a query. 3 The strategies differ in the way the past query frequency is weighted. In what follows, w ( q )
It is possible to come up with variants of these strategies. represents the weight of a query q and S is the sample of queries considered in the selection phase mentioned above.
Unit-weighted frequency. This strategy assumes that a query X  X  importance is equal to its frequency in S , i.e., where q j denotes an occurrence of q in the query sample S . This strategy simply prefers prefetching frequent queries as an attempt towards increasing the likelihood of prefetching a query that will repeat again.

Workload-weighted frequency. This strategy assigns a heavier weight to those queries appearing during high-traffic periods, i.e., those that are issued when the backend query traffic volume is higher. The motivation here is to prefetch more queries from busy traffic hours so that the number of queries hitting the search cluster backend is re-duced in those hours. We compute the weight of a query as where v ( q j ) represents the backend query traffic volume in queries per second when q j is observed. This strategy aims at reducing the backend query volume at busy hours and, indirectly, it also aims at reducing the number of degraded query results.

Time-weighted frequency. This strategy give a higher priority to queries that occurred closer in time to  X  0 . where t ( q ) denotes the time at which q j is issued. The as-sumption here is that the likelihood of prefetching a repeat-ing query will increase.
Online strategies are designed to identify queries that are already expired or about to expire and are going to be issued to the backend due to a TTL miss. Those queries are pro-cessed beforehand, possibly when the load at the backend is low. The key feature of this strategy is the prediction of the next time point ( e q ) at which query q will be issued while its cached results are in an expired state.
 In the rest of the section, we use the following notation. We denote by l q the latest request time and by u q the latest computation time of the results of query q . As in the pre-vious section,  X  denotes the current time and T denotes the TTL. We denote by s the selection period (i.e., we update the prefetching queue at every s time units) and by f q the number of occurrences of q up to  X  .

To estimate e q , each time q is submitted, we predict the time to its next appearance, n q . Under the assumption that q appears every n q seconds, e q can be calculated as l q where k is the smallest natural number greater than 0 that satisfies u q + T &lt;l q + k  X  n q , i.e., k = b u q + T  X  l
The value for n q is computed through a machine learning model using the features given in Table 1. Temporal fea-tures are based on when a query is submitted. Query string features are based on the syntactic content of the query. Re-sult page features capture the characteristics of the results returned by the search engine. Frequency features are based Table 1: The features used by the learning model Feature type Feature Description Temporal hourOfDay Hour of submission timeGap Time since last occurence Frequency on counters associated with terms of the queries. We use gradient boosted decision trees to train our model [14].
The key point in the online strategy is to prefetch queries satisfying  X  &lt; e q  X   X  + T when there is processing capacity. As a na  X   X ve method of query selection, one could scan the list of queries in the cache and pick those with e q values that satisfy the constraint. In practice, this is not feasible as the cost of scanning the cache for every submitted query is prohibitive. Hence, we resort to use a bucket data structure where each bucket stores queries with e q values within a time-period [ t,t + s ). Insertions and deletions in the bucket list can be realized in O (1)-time.

As illustrated in Fig. 5, the prefetching module maintains a list of buckets. Each time a query is requested, we remove it from the current bucket, modify e q and place it back in a new bucket. In particular, we have two situations. If the request is a hit, we modify e q and potentially move the query into another bucket. Otherwise, if it is a miss, we delete it from its current bucket and issue it to the backend. Once it has been processed, we update e q and eventually insert q in the bucket list. Finally, once in every s seconds we select queries having e q  X  (  X , X  + T ] from the corresponding buckets and place these in the prefetching queue. To prioritize the queries in the prefetching queue, we evalu-ate the following three methods:
Based on age-frequency . A query is assigned a weight In other words, older and more frequent queries get a higher priority. This strategy tends to favor frequent queries that were refreshed long time ago. The goal of this method is to optimize the freshness of the results.

Based on age-frequency and degradation . We ex-tend the weight function to include degradation. This strategy favors older and degraded queries. The goal of this method is to optimize the freshness of the results and reduce the degradation.

Based on processing cost and expected load . Each query is prioritized according to its expected processing cost times the expected difference in the backend load. where  X  ( q ) is the estimated processing cost and L ( e q backend load that is observed 24 hours prior to e strategy aims at reducing the query response time and in-creasing the throughput.
Prefetching strategies. We evaluate a number of com-peting strategies. The first type of strategies are baseline strategies that do not employ any prefetching. These are no-ttl , which assumes that there is an infinite result cache with no TTL for entries so that any repetition of a query is a hit, and no-prefetch , which assumes that there is an infinite result cache with a TTL, but the prefetching logic is not activated. The second type of strategies are the offline strategies discussed in Section 3: off-freq , off-vol , and off-time . The third type of strategies are the online strate-gies discussed in Section 4: on-age , on-agedg , and on-load . The fourth type are the oracle versions of online strategies in which the exact next occurrence times of queries are as-sumed to be known. Following the convention in naming the online strategies, we refer to the oracle strategies as oracle-age , oracle-agedg , and oracle-load . The last implemen-tation is an interpretation of the age-temperature refreshing strategy [11], which we refer to as age-temp .

The original age-temp strategy maintains a two-dimensional bucket structure, where one of the dimensions corresponds to the result age and the other dimension cor-responds to the query frequency (temperature). In our im-plementation of this strategy, each age bucket is defined by the selection interval s and the temperature is computed as log 2 ( f q ), where f q is the number of times a query is seen since the beginning of the first day. Each time a query is requested, we increase its frequency and potentially move it to a bucket corresponding to a higher temperature. Each time a query is processed, we move it to the bucket with the same temperature but the age dimension corresponding to  X  . Furthermore, in our experiments, the temperature of a query cannot decrease. As an alternative, we have evaluated the use of a sliding time-window to dynamically maintain the number of occurrences. However, we have observed that with a window of reasonable size, e.g., 24 or 48 hours, the hit rate tends to be lower. Therefore, in our experiments, we count occurrences starting from the beginning of the first day. Further, we prioritize buckets by the corresponding age  X  temperature value and restrict selection to the first 30 , 000 queries satisfying a minimum age requirement, which prevents too fresh queries from being repeatedly refreshed. We denote the minimum result age as R and use it as the limiting factor for queries selected also by the other meth-ods. Finally, among the selected queries, we give a higher priority to expired queries instead of unexpired ones.
In the offline strategies, the only cache state information that is made available to the prefetching module is whether an entry is older than R or not. Other information relies only on the query log from the past day and includes nothing about the current cache state. In the online strategies, the decisions are made based on the current cache state and the prediction model mentioned before. As predicting, based on a limited query log, the arrival times of queries that request expired entries is a challenging problem, we allow an entry to be prefetched up to two times without being hit in between.
We use the oracle strategies to show the effect of accu-rate prediction of the next request for an expired entry. The oracle strategies have no information about future hits, nei-ther the number of occurrences nor the time at which they will occur. Nevertheless, they demonstrate that accurate prediction of the next requests for expired entries is enough to achieve good performance. Note that, in our results, we report only the results for the oracle-agedg strategy since it always outperforms the other two alternatives.

Simulation setup. We sample queries from five consec-utive days of Yahoo! web search query logs. The queries in the first three days are used to warmup the result cache. No prefetching is done in these days. At the end of the third day, we start allowing queries into our data structures. The age-temperature baseline starts with queries collected from the first three days. The offline strategies start with queries collected during the third day. The online strategies use the first three days to create a model. As no predictions are Figure 6: Temporal splitting of the query log with respect to the tasks. made before the beginning of the fourth day, we use aver-age inter-arrival times to initiate the bucket structure at the end of day three. We assume that the prefetching logic is activated at the beginning of the fourth day. Finally, for all of the techniques, we use the fourth day to stabilize the per-formance and the fifth day for the performance evaluation.
For the prediction model used by the online strategies, the first day of the training period is used to compute the static features (e.g., query and term frequencies). 4 The instances for which the next arrival times are tried to be predicted are extracted from the second day. The target values (i.e., the next arrival times that are tried to be predicted) are obtained from the second and third days. This setup is il-lustrated in Fig. 6. The importance values for the features used by the model are displayed in Fig. 7. As expected, the query frequency and the time gap between the two consec-utive occurrences of a query turn out to be the most im-portant features. For the fourth and fifth days, we train a learning model using the preceding three days.
 System parameters and query processing logic.
 The experiments are conducted using a discrete event sim-ulator that takes as input the user queries along with their original submission timestamps. We simulate a system with K compute nodes and P query processors, which allows up to P queries to be concurrently processed with a speedup proportional to K . We assume that the entire inverted in-dex is maintained in the memory. In all experiments, we set the TTL ( T ) to 16 hours, which is chosen by an analysis of the opportunity, risk, and potential benefits of prefetch-ing shown in Fig. 4. We fix P to 8 and vary K to create three different scenarios corresponding to poor ( K = 850), medium ( K = 1020), and high performance ( K = 1224) sys-tems. With 850 nodes, both no-prefetch and no-ttl expe-rience a degradation of efficiency at peak performance. With 1020 nodes (20% above the first scenario), no-ttl reaches the peak sustainable throughput only marginally. With 1224 nodes (20% above the second scenario), both no-prefetch and no-ttl perform well.

If more than P queries are concurrently issued to the backend, the most recent queries are put into a processing queue. To prevent the queue from growing indefinitely, we monitor the processor usage P l over the last minute. When P &gt; 0 . 95  X  P , we degrade the currently processed query by P/Q  X  , where Q  X  is the number of queries being processed at the backend at that time.
We use a document collection containing several billion documents to compute the features that rely on the doc-ument frequencies of terms.
Since prefetching happens while user queries are sched-uled, we monitor the average number of user queries being processed over the last minute Q l and maintain a budget of B = b f  X  P  X  Q l c processors to be used for prefetching. By default, we use f = 0 . 875 to keep the processor utilization around 87.5 percent. To prevent the system from prefetch-ing recently processed queries, we restrict the minimum age of queries being selected with a parameter R . In our ex-periments we use three different values, TTL/2, TTL/4 and TTL/8. Finally, the prefetching queue is updated/reset once in every s = 10 minutes.

Evaluation metrics. We evaluate the performance us-ing five different metrics. The metrics are cache hit rate, query response time, average hit age, degradation rate, and backend query traffic rate. The hit rate reflects how good the prefetching algorithm is at having query results avail-able at request. The change in response time reflects the real improvement from prefetching versus its overhead. The upper bound for both metrics is limited by the number of compulsory misses and corresponds to the hit rate and la-tency of no-ttl . The average hit age reflects how good the prefetching method is at keeping cached results fresh. The degradation rate reflects the amount of degraded results be-ing served to the user and is defined as the total degrada-tion divided by the number of queries being answered by the frontend. The total degradation is the sum of the rel-ative degradations of the results returned by the frontend. Finally, the average backend query traffic measures the av-erage number of queries issued to the backend. The lower bound on this number is again defined by no-ttl . In this section, we present the results of our experiments. Our first observation is that in all the cases that we analyze (except for one), the best possible results are obtained, as expected, by the oracle-agedg strategy (the last line in the tables). Obviously, not being feasible in practice, the oracle strategy is presented only as an upper bound to the results that can be attained by the other techniques. Indeed, as the prediction accuracy gets closer to that of the oracle, we should get better and better results.

Note that, in the sole case of the average hit age results reported in Table 6, the oracle-agedg strategy leads to sub-optimal results. This behavior can be explained by observ-ing that the age decreases as we increase the number of times an entry is subject to prefetching. The oracle, instead, min-imizes the amount of prefetching and thus some entries (es-pecially those requested further in the future) tend to have higher ages.
Table 2 shows the cache hit rate of different strategies for various parameters. The no-prefetch and no-ttl strategies are the two extreme cases. The no-prefetch strategy does not involve prefetching and relies only on the TTL to cope with staleness problem. The no-ttl strategy correspond to an infinite cache scenario, where only the compulsory misses incur a cost. In between these two extremes, by varying the K and R parameters, we observe that, in all cases, the offline strategies perform worse than the age-temp baseline. Our online strategies, however, outperform the baseline. In par-ticular, we compare the hit rate performance of a strategy A versus that of B by computing the relative hit rate improve-ment with respect to oracle-agedg as I = HR A  X  HR B HR HR A , HR B , and HR O are the hit rates of the A, B, and oracle-agedg strategies, respectively. The improvement to-wards the oracle of our on-age versus the baseline age-temp ranges from 12% of on-age with K = 1224 and R = T/ 8 to 22 . 78% of on-age with K = 850 and R = T/ 4 with an average of 19 . 23%. Fig. 8 shows the hit rate trend of four different strategies: no-prefetch , no-ttl , on-agedg , and oracle-agedg . According to the figure, our method consis-tently outperforms the no-prefetch strategy.

In Table 3, we report the response time of our system by varying the simulation parameters. Query response time and hit rate are related and results confirm this behavior. As in the case of hit rate, we report the improvement with respect to the oracle that are similar to those of hit rate figures. The improvement towards the oracle of our on-age versus the baseline age-temp ranges from 12 . 4% of on-age with K = 1224 and R = T/ 8 to 27 . 5% of on-age with K = 850 and R = T/ 8 with an average of 21 . 1%. The hourly trend for the average response time is shown in Fig. 9, where the effect of peak load reduction is evident. From 11AM to 6PM, the peak load response time is greatly reduced. Therefore, in this case, the user experience should be greatly improved.
Another important aspect to consider when evaluating prefetching strategies is the load we put on the backend (Ta-ble 4). If we do not adopt any prefetching strategy, the load on the backend would certainly be affected only by the hit rate. Indeed, the higher the hit rate the less the number of queries hitting the backend. This is confirmed by the numbers in Table 4 as the no-prefetch strategy is the one attaining, in all cases, the minimum amount of workload at the backend. We remark that our goal is to optimize the exploitation of the infrastructure, i.e., to increase the overall load by keeping the amount of degraded queries as low as possible. Numbers in Table 4, thus, are more ex-planatory if read in conjunction with the results presented in Table 5. Disregarding the case K = 1224, in which the computational capacity is too high to be overloaded even by a great number of prefetching operations (with the ex-ception of on-load which tries to prefetch expensive queries first), in all cases, on-age and on-agedg attain the greatest reduction in terms of degraded queries, despite the average backend query traffic is greater than the baseline. Figs. 10 and 11 confirm the results reported in the tables and show even more remarkably the effect of prefetching on the back-end query traffic, which results to be roughly flattened and far apart from the no-cache case. The degradation (Fig. 11) is also greatly reduced during the peak hours. In this case, we report degradation of query results for the case K = 850 instead of K = 1020 as in the other cases. This is because, for K = 1020, the degradation is negligible.

We also report the average hit age measured in number of minutes passed since the last update. It is worth being pointed out that measuring the average hit ratio does not say much about the quality or staleness of results in the cache. In fact, if the majority of entries are updated but requested after a period of time very close to the TTL, the ideal strategy would not update them. Indeed, this would make the average hit age increase when those queries are actually requested only slightly before their expiration. We report these results in Table 6 and Fig. 12.
 Finally, we measure the accuracy of prefetching methods. We define the accuracy as the fraction of correct prefetch-ing actions. That is, the average between the fraction of prefetching operations never followed by a hit with respect to the total number of prefetching operations and the frac-tion of compulsory misses relative to the number of misses. As illustrated in Table 7, while performing less prefetching, the offline techniques have in general higher accuracy. In ad-dition, it is worth to note that there is a high potential for improvement. If we compare prefetching accuracy with that of oracle-agedg , we can observe that we are still far from being close to the maximum. On the other hand, the worst accuracy is achieved by on-load , which illustrates that pri-oritizing frequent queries maximizes the probability of useful prefetching. We note that the age-temp baseline performs poorly with respect to our strategies.

In what follows, we enumerate what we retain to be the take away messages from this work. First, the baseline re-lies only on the knowledge of the past frequency and the current age. This approach cares about keeping popular en-tries fresh and does not care if they will be used again in the future. Second, the offline strategies rely only on the in-formation about the queries submitted in the previous day. Even though they are worse than the baseline, they perform surprisingly well, given that they use only one day of his-tory. Third, the online strategies rely on predicted future query expirations. This prediction task is very difficult to Figure 8: Result cache hit rate ( K = 1020 ). carry out, as experiments show, yet a mildly good predic-tion policy gives very good results in terms of search backend exploitation, low query degradation, and hit-ratio. Fourth, even if oracle has a perfect knowledge of all the future misses, it does not have all the information needed to order the queries in the best possible way to reduce the number of misses, for instance. However, we use the oracle strategies as an upper bound to the effectiveness of prefetching with respect to the cache content. Finally, in Figs. 8 X 12, we ob-serve that, during the peak hours, the benefits of prefetching are greatly amplified thus confirming our initial hypothesis, i.e., prefetching has a great potential for improving search engine utilization and peak-load performance.
Herein, we survey the previous work on result caching, freshness, and batch query processing. Interested readers may refer to [10] for a broader survey on search efficiency.
Result caching. So far, a large body of research focused on increasing the result cache hit rates [4, 20] or to reduce the query processing cost of backend search systems [15, 21]. Depending on how the cache entries are selected, static [6], dynamic [22], or hybrid [13] caching strategies are followed. The earlier works assumed limited-capacity caches, where the main research issues are admission [5], prefetching [16], and eviction [20], whereas the recent works mostly adopted an infinite cache assumption [11]. A number of proposals are made to combine other layers of caching with result caching, resulting in two-level [3, 22], three-level [17, 18], or even five-level [19] caching architectures.

Cache freshness. The most recent works deal with re-sult caching in the context of maintaining the freshness of the results served by the cache [1, 8, 9, 11]. In this line of research, the cache entries are associated with TTL values that are fixed for all queries. In [2], each query is associated with a different TTL value depending on its freshness re-quirements. In several works, the TTL approach is coupled either with more sophisticated techniques such as invalida-tion of potentially stale cache entries [1, 8, 9], where the goal is to predict the stale cache entries by exploiting certain in-formation obtained during index updates. The search results whose freshness is potentially affected by the updates are then marked as invalid so that they are served by the back-end system in a successive request, rather than by the cache. The approach followed in [11], on the other hand, relies on proactive refreshing of cached search results. In this ap-proach, the indexing system does not provide any feedback on staleness. Instead, some presumably stale search results are selected and refreshed based on their likelihood of being requested in future and also depending on the availability of free processing cycles at the backend search system. Our work proposes strategies that are alternative to that in [11] and also focuses on different performance metrics.

Batch query processing. In [12], some efficiency op-timizations are proposed for batch query processing in web search engines. Those optimizations (e.g., query reorder-ing) are orthogonal to ours and, in case of a partially disk-resident index, they may be coupled with our prefetching techniques. Since we assume an in-memory index, however, we did not consider those optimizations in our work.
We investigated the impact of prefetching on search en-gines. We showed that an oracle strategy that has a perfect information of future query occurrences can achieve the best attainable results. We made an attempt to close the gap with the oracle by testing two alternative set of prefetching techniques: offline and online prefetching. We showed that the key aspect in prefetching is the prediction methodology. Furthermore, we showed that our accuracy in predicting fu-ture query occurrences is only about a half of the best that can be done. We plan to extend this work in several direc-tions. First, we are going to design more effective techniques for predicting the expiration of a query. Second, we are go-ing to evaluate the economic impact of prefetching on the search operations. Finally, we would like to design spec-ulative prediction techniques that will be able to prefetch queries that were not even submitted. This would reduce the number of compulsory misses that form an upper bound on the cache performance.
Simon Jonassen was an intern at Yahoo! Research and supported by the iAd Centre, Research Council of Norway and NTNU. [1] S. Alici, I. S. Altingovde, R. Ozcan, B. B. Cambazoglu, [2] S. Alici, I. S. Altingovde, R. Ozcan, B. B.
 [3] I. S. Altingovde, R. Ozcan, B. B. Cambazoglu, and [4] R. Baeza-Yates, A. Gionis, F. Junqueira, V. Murdock, [5] R. Baeza-Yates, F. Junqueira, V. Plachouras, and [6] R. Baeza-Yates and F. Saint-Jean. A three level search [7] L. A. Barroso, J. Dean, and U. H  X  olzle. Web search for [8] R. Blanco, E. Bortnikov, F. Junqueira, R. Lempel, [9] E. Bortnikov, R. Lempel, and K. Vornovitsky. Caching [10] B. B. Cambazoglu and R. Baeza-Yates. Scalability [11] B. B. Cambazoglu, F. P. Junqueira, V. Plachouras, [12] S. Ding, J. Attenberg, R. Baeza-Yates, and T. Suel. [13] T. Fagni, R. Perego, F. Silvestri, and S. Orlando. [14] J. H. Friedman. Stochastic gradient boosting. Comput. [15] Q. Gan and T. Suel. Improved techniques for result [16] R. Lempel and S. Moran. Predictive caching and [17] H. Li, W.-C. Lee, A. Sivasubramaniam, and C. L. [18] X. Long and T. Suel. Three-level caching for efficient [19] M. Marin, V. Gil-Costa, and C. Gomez-Pantoja. New [20] E. P. Markatos. On caching search engine query [21] R. Ozcan, I. S. Alting  X  ovde, and O. Ulusoy. Cost-aware [22] P. C. Saraiva, E. Silva de Moura, N. Ziviani,
