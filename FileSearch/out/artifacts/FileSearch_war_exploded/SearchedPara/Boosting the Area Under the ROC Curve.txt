 members of the target class ( X  X rue positives X ), and how many are not ( X  X alse positives X ). implies, examining the area under the resulting curve.
 if close to the best possible value of 1 .
 classification of Long and Servedio [9].
 to classifier without needing any rebalancing.
 of noise in a similarly abstract model. We address sampling i ssues in Section 5. c negative examples, so D is a mixture of the distributions D + and D  X  . that if h ( x false positive rate and  X   X   X  = D  X  [ h ( x )  X   X  ] (1 , 1) corresponding to  X  =  X  and  X  X  X  respectively.
 Given h : X  X  R and D , the AUC can be defined as AUC( h ; D ) = Pr above is equal to the area under the ROC curve for h .
 of h (  X  ( )) is 1 2 for any function h. This motivates the following definition: D , returns a function h : X  X  R that has AUC( h ; D )  X  1 of different settings. the booster successively passes distributions D functions h analysis of AdaBoost, this is easy to do. In this model we prov e the following: The AUCBoost algorithm makes T = O ( log(1 / X  ) support of size m , AUCBoost takes O ( mT log m ) time.
 ranker can be used to generate a two-sided weak learner.
 outputs a hypothesis h that satisfies both Pr +  X . We say that such an h has two-sided advantage  X  with respect to D . negative examples.
 ROC curve corresponding to h. Since the AUC is at least 1 thresholding h at  X  , the class conditional error rates p satisfy p Suppose that p  X   X   X  = p  X  +  X  (1  X  p  X  ) .
 yields For any fixed value of p the maximum of (1) is achieved at p is maximized at p Lemma 5 Let h : X  X  { X  1 , 1 } and suppose that Pr Pr x  X  X   X  [ h ( x ) =  X  1] = 1  X   X   X  . Then we have AUC( h ; D ) = 1  X   X  + +  X   X  2 . Proof: We have of AdaBoost passes its reweighted distribution D Lemma 4 to convert the resulting weak ranking function to a cl assifier h and D  X  The analysis of AdaBoost (see [5]) shows that T = O log(1 / X  ) error rate at most  X  under 1 function with AUC at least 1  X   X  .
 defines h of errors of each type. denote the resulting distribution over (noise-corrupted) labeled examples ( x, y ) . Boosting weak rankers in the presence of independent miscla ssification noise. We now show | {z } | {z } output -1 output 1 function h : X  X  R such that AUC( h ; D )  X  1 series of bins B the algorithm first constructs distributions D D on membership in B using each of D this, it creates t + 1 bins for the next round by assigning each element ( x, y ) of B h i,t ( x ) =  X  1 rounds, which is an input parameter of the algorithm.
 v would have been routed during the training phase: it starts i n node v goes to v node of the branching program in layer T + 1 , it is at some node v example followed. See Figure 3.
 Lemma 7 ([9]) Suppose that Basic MartiBoost is provided with a hypothesis h vantage  X  w.r.t. D a branching program H such that D + [ H ( x ) =  X  1]  X   X  and D  X  [ H ( x ) = 1]  X   X  . Given access to a noise-tolerant weak ranker A with advantage  X  , at each node v Rank algorithm runs A and proceeds as described in Lemma 4 to obtain a weak classifie r h H booster in the presence of independent misclassification no ise: Proof: Fix any node v is unaffected by the value of y. Consequently if D conditioned on reaching v simply D tolerant weak ranker A is invoked at a node v advantage  X / 4 with respect to the underlying distribution D MartiBoost algorithm itself, and can achieve any AUC value 1  X   X  even for  X  &lt;  X . such that AUC( h ; D )  X  1 We let m such that each class, positive and negative, has at least m standard analysis.) for the MartiBoost algorithm. The main challenge in [9] is th e following: for each node v the induced distribution D tiny fraction of examples drawn from D are (say) positive and reach v is what we need to achieve good AUC . We turn now to the details. Given a node v and each value b  X  X  X  1 , 1 } , if then the node v sis h will have D + [ H ( x ) =  X  1]  X   X  and D  X  [ H ( x ) = 1]  X   X  . Proof: We analyze D + [ h ( x ) =  X  1] ; the other case is symmetric. Given an unlabeled instance x  X  X , we say that x freezes at node v branching program causes it to terminate at a node v frozen by SMartiRank). We have D [ x freezes and c ( x ) = 1] = P D + [( h ( x ) =  X  1) &amp; ( x does not freeze )]  X   X  union bound  X  details are omitted.
 p We first observe that for any distribution D and any bit b , we have Pr be obtained from O ( 1 additive  X  p/ 10 , and thus to estimate the RHS of (2) to within an additive  X   X p to determine whether node v accurate estimate of p b that it suffices to run the algorithm using these estimated va lues). We have estimate of the second multiplicand can be obtained using O ( 1 least a  X  (1  X  2  X  ) fraction of draws will reach v distribution D  X  from D  X  is class that must be made in order to simulate a draw from D  X  Thus, O ( T 2 m and  X  &lt; 1  X  , the SMartiRank algorithm makes m 1  X   X  outputs a branching program H such that AUC( h ; D )  X  1  X   X  . We are very grateful to Naoki Abe for suggesting the problem o f boosting the AUC.
