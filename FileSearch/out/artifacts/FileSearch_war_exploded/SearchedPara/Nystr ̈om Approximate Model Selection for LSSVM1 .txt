 Support vector machine (SVM) [18] is a learning system for training linear learning machines in the kernel-induced feature spaces, while controlling the capacity to prevent overfitting by generalization theory. It can be formulated as a quadratic programming problem with linear inequality constraint s. The least squares support vector machine (LSSVM) [16] is a least squares version of SVM, which considers equality constraints instead of inequalities for classical SVM. As a result, the solution of LSSVM follows directly from solving a system of linear equations, instead of quadratic programming.
Model selection is an important issue in LSSVM research. It involves the selection of kernel function and associated kernel para meters and the selection of regularization parameter. Typically, the form of kernel function will be determined as several types, such as polynomial kernel and radial basis function (RBF) kernel. In this situation, the selection of kernel function amounts to tuning the kernel parameters. Model selection can be reduced to the selection of kernel param eters and regularization parameter which minimize the expectation of test error [4]. W e usually refer to these parameters collec-tively as hyperparameters . Common model selection approaches mainly adopt a nested two-layer inference [11], where the inner layer trains the classifier for fixed hyperpa-rameters and the outer layer tunes the hyperpar ameters to minimize the generalization error. The generalization error can be estimated either via testing on some unused data (hold-out testing or cross validation) or via a theoretical bound [17,5].

The k -fold cross validation gives an excellent estimate of the generalization error [9] and the extreme form of cross validation, leave-one-out (LOO), provides an almost unbiased estimate of the generalization error [14]. However, the naive model selection strategy based on cross validation, which adopts a grid search in the hyperparameters space, unavoidably brings high computationa l complexity, since it would train LSSVM for every possible value of the hyperparameters vector. Minimizing the estimate bounds of the generalization error is an alternative to model selection, which is usually realized by the gradient descent techniques. The commonly used estimate bounds include span bound [17] and radius margin bound [5]. Generally, these methods using the estimate bounds reduce the whole hyperparameters space to a search trajectory in the direction of gradient descent, to accelerate the outer layer of model selection, but multiple times of LSSVM training have to be implemented in the inner layer to iteratively attain the minimal value of the estimates. Training LSSVM is equivalent to computing the inverse of a full n  X  n matrix, so its complexity is O ( n 3 ), where n is the number of training ex-amples. Therefore, it is prohibitive for the large scale problems to directly train LSSVM for every hyperparameters vector on the search trajectory. Consequently, efficient model selection approaches via the acceleration o f the inner computation are imperative.
As pointed out in [5,3], the model selection criterion is not required to be an unbiased estimate of the generalization error, instead the primary requirement is merely for the minimum of the model selection criterion to provide a reliable indication of the mini-mum of the generalization error in hyperparam eters space. We argue that it is sufficient to calculate an approximate criterion that can discriminate the optimal hyperparame-ters from the candidates. Such considerati ons drive the proposal of approximate model selection approach for LSSVM.

Since the high computational cost for calculating the inverse of a kernel matrix is a major problem of LSSVM, we consider to approximate a kernel matrix by a  X  X ice X  ma-trix with a lower computational cost when calculating its inverse. The Nystr  X  om method is an effective technique for generating a low rank approximation for the given kernel matrix [19,13,8]. Using the low rank approximation, we design an efficient algorithm for solving LSSVM, whose complexity is lower than O ( n 3 ). We further derive a model approximation error bound to measure the effect of Nystr  X  om approximation on the deci-sion function of LSSVM. Finally, we present an efficient approximate model selection scheme. It conforms to the two-layer iterative procedure, but the inner computation has been realized more efficiently. By rigorous e xperiments on several benchmark datasets, we show that approximate model selection can significantly improve the efficiency of model selection, and meanwhile guara ntee low generalization error.

The rest of the paper is organized as follows. In Section 2, we give a brief introduc-tion of LSSVM and a reformulation of it. In Section 3, we present an efficient algorithm for solving LSSVM. In Section 4, we analyze the effect of Nystr  X  om approximation on the decision function of LSSVM. In Section 5, we present an approximate model selec-tion scheme for LSSVM. In Section 6, we report experimental results. The last section gives the conclusion. We u s e X to denote the input space and Y the output domain. Usually we will have X X  R d , Y = { X  1 , 1 } for binary classification. The training set is denoted by by a feature mapping of the input space,  X  : X X  X  . The parameters ( w , b )ofthelinear classifier are given by the minimizer of a regularized least-squares training function where  X &gt; 0 is called regularization parameter. The basic training algorithm for LSSVM [16] views the regularized loss function (1) as a constrained minimization problem Further, we can obtain the dual form of Equation (2) as follows we can write Equation (3) in a matrix form  X  = (  X  vector.
 If we let K  X , n = K +  X  I n , we can write the first row of Equation (4) as (4), we can obtain The system of linear equations (4) can then be rewritten as Since K  X , n = K +  X  I n is positive definite, the inverse of K  X , n exists. Equation (7) can be solved as follows: we first solve The solution (  X  , b ) of Equation (4) are then given by The decision function of LSSVM can be written as f ( x ) = n i = 1  X  i K ( x i , x ) + b .
If Equation (8) is solved, we can easily obtain the solution of LSSVM. However, the complexity of calculating the inverse of the matrix K  X , n is O ( n 3 ). In the following, we will demonstrate that Nystr  X  om method can be used to speed up this process. We first introduce a fundamental result of matrix computations [10]: for any matrix A  X  R m  X  n and positive integer k , there exists a matrix A for  X  = F , 2.  X  F and  X  2 denote the Frobenius norm and the spectral norm. Such A k is called the optimal rank k approximation of the matrix A . It can be computed through the singular value decomposition (SVD) of A .If A  X  R n  X  n is symmetric positive semi-definite (SPSD), A = U  X  U T ,where U is a unitary matrix and  X  = diag(  X  1 ,..., X  n )is where U i is the i th column of U .
 We now briefly review the Nystr  X  om method [8,19]. Let K  X  R n  X  n be an SPSD matrix. The Nystr  X  om method generates a low rank approximation of K using a subset of the columns of the matrix. Suppose we randomly sample c columns of K uniformly without replacement. Let C denote the n  X  c matrix formed by theses columns. Let W be the c  X  c matrix consisting of the intersection of these c columns with the corresponding c rows of K . Without loss of generality, we can rearrange the columns and rows of K based on this sampling such that: Since K is SPSD, W is also SPSD. The Nystr  X  om method uses W and C from Equation (10) to construct a rank k approximation K of K for k  X  c defined by: where W k is the optimal rank k approximation to W and W + k is the Moore-Penrose If we write the SVD of W as W = U W  X  W U T W ,then where  X  W , k and U W , k correspond the top k singular values and singular vectors of W . The diagonal elements of  X  W , k are all positive, since W is SPSD and k  X  rank( W ). If we plug Equation (12) into Equation (11), we can obtain where we let V : = CU W , k  X  + W , k  X  R n  X  k .

For LSSVM, we need to solve the inverse of K +  X  I n . To reduce the computational cost, we intend to use the inverse of K +  X  I n as an approximation of the inverse of K +  X  I
To efficiently calculate the inverse of K +  X  I n , we further introduce the Woodbury formula [12] where A  X  R n  X  n , X  X  R n  X  k , Y  X  R k  X  k and Z  X  R k  X  n .

Now, we can obtain The last equality of Equation (15) is directly derived from the Woodbury formula with A =  X  I
The essential step of solving LSSVM is to solve Equation (8). If we let u = [  X  ,  X  ] and z = [ 1 , y ], Equation (8) is equivalent to Using Equation (15) to replace  X  I n + K with  X  I n + K , we can obtain
We further introduce a temporary variable t to efficiently solve Equation (16): We now present an algorithm of solving LSSVM (Algorithm 1).
 We estimate the computational complexity of Algorithm 1 in Theorem 1.
 Algorithm 1. Approximating LSSVM using Nystr  X  om method Theorem 1. The computational complexity of Algorithm 1 is O ( c 3 + nck ) . Proof. The computational complexity of step 1 is O ( c 3 ), since the main computational part of this step is the SVD on W . In step 2, matrix multiplications are required, so its complexity is O ( kcn ). In step 3, the inverse of  X  I k + V T V is solved by computing The last matrix multiplication to obtain t requires O ( k 2 ). Therefore the total complexity ofstep3is O ( k 3 + nk ). The complexity of step 4 is O ( nk ). The complexity of step 5 is O ( n ), since the multiplication and subtraction between two vectors need to be done. For O ( c 3 + nck ). For large scale problems, we usually set c n .
 Compared to Related Work. Theorem 1 shows that if Nystr  X  om approximation is given, we can solve LSSVM in O ( k 3 ). Williams et al. [19] used Nystr  X  om method to speed up Gaussian Process (GP) regression. After Nystr  X  om approximation was given, they solved GP regression with O ( nk 2 ) complexity. Cortes et al. [6] scaled kernel ridge regression (KRR) using Nystr  X  om method. The complexity of their method is O ( n 2 c ) with Nystr  X  om approximation (Section 3.3 of [6]). In this section, we analyze the effect of Nystr  X  om approximation on the decision function of LSSVM.

We assume that approximation is only used in training. At testing time the true kernel function is used. This scenario has been considered by [6]. The decision function f derived with the exact kernel matrix K is defined by K ( x , x )  X   X  .
We first consider the effect of Nystr  X  om approximation on  X  of Equation (8). Let  X  denote the solution of ( K +  X  I n )  X  = 1 . We can write invertible matrices A , B . Thus,  X   X   X  2 can be bounded as follows: Since K and K are positive semi-definite matrices, the eigenvalues of K +  X  I n and K +  X  I n are less than or equal to 1 / X  .

We further consider  X  of Equation (8). Replacing 1 with y , we can obtain the similar bound
As the assumptions, we use the true kernel function at testing time, so no approxi-mation affects k x . For simplicity, we assume the offset b to be a constant  X  . Therefore, the approximate decision function f is given by f ( x ) = [  X  ;  X  ] T [ k x ;1].
We can obtain By Schwarz inequality, From Equation (9), we know that  X  =  X   X   X  b =  X   X   X   X  ,so We l e t  X  0 =  X / n . Substituting t he upper bound of  X   X   X  2 into Equation (22), we can obtain We further introduce a kernel matrix approximation error bound of Nystr  X  om method [13] to upper bound K  X  K 2 .
 Theorem 2. Let K  X  R n  X  n be an SPSD matrix. Assume that c columns of K are sampled uniformly at random without replacement, let K be the rank-k Nystr  X  om approximation where i  X  D ( c ) K ii is the sum of largest c diagonal entries of K .
 Since K  X  K 2  X  K  X  K F , if we combine Equation (24) with Theorem 2, we can directly obtain the following theorem.
 Theorem 3. Let K  X  R n  X  n be an SPSD matrix. Assume that c columns of K are sampled uniformly at random without replacement, let K be the rank-k Nystr  X  om approximation | f ( x )  X  f ( x ) | X  where i  X  D ( c ) K ii is the sum of largest c diagonal entries of K .

Theorem 3 measures the effect of kernel matrix approximation on the decision func-tion of LSSVM. It enables us to bound the relative performance of LSSVM when the Nystr  X  om method is used to approximate the kernel matrix. We refer to the bound given in Theorem 3 as a model approximation error bound . In order to find the hyperparameters that minimize the generalization error of LSSVM, many model selection approaches have been proposed, such as the cross validation, span bound [17], radius margin bound [5], PRESS criterion [1] and so on. However, when optimizing model selection criteria, all these approaches need to solve LSSVM completely in the inner layer for each iteration.

Here we discuss the problem of approximate model selection. We argue that for model selection purpose, it is sufficient to calculate an approximate criterion that can discriminate the optimal hyperparameters from candidates. Theorem 3 shows that when Nystr  X  om approximation is used, the change of learning results of LSSVM is bounded, which is a theoretical support for approximate model selection. In the following, we present an approximate model selectio n scheme, as shown in Algorithm 2.

We use the RBF kernel K x i , x j = exp  X   X  x i  X  x j 2 to describe the scheme, but this scheme is also suitable for other kernel types. Algorithm 2. Approximate Model Selection Scheme for LSSVM
Let S denote the iteration steps of optimizing model selection criteria. The complex-ity of solving LSSVM by calculating the inverse of the exact kernel matrix is O ( n 3 ). For radius margin bound or span bound [5], a standard LSSVM needs to be solved in the inner layer for each iteration, so the tot al complexity of these two methods is O ( Sn 3 ). For PRESS criterion [1], the inverse of kern el matrix also needs to be calculated for each iteration, so its complexity is O ( Sn 3 ). From Theorem 1, we know that using Algo-rithm 1, we could solve LSSVM in O ( c 3 + nck ). Therefore, if we use the above model selection criteria in the outer layer, the complexity of approximate model selection is  X  However, the complexity of approximate model selection using t -fold cross validation as outer layer criterion will be O ( tS  X  S  X  ( c 3 + nck )). In this section, we conduct experiments on several benchmark datasets to demonstrate the effectiveness of approximate model selection. 6.1 Experimental Scheme The benchmark datasets in our experiments are introduced in [15], as shown in Table 1. For each dataset, there are 100 random tr aining and test pre-defined partitions 1 (except 20 for the Image and Splice dataset). The use of multiple benchmarks means that the evaluation is more robust as the selection of data sets that provide a good match to the inductive bias of a particular classifier become s less likely. Likewise, the use of multiple partitions provides robustness against sensitivity to the sampling of data to form training and test sets.

In R  X  atsch X  X  experiment [15], model selection is performed on the first five training sets of each dataset. The median values of the hyperparameters over these five sets are then determined and subsequently used to evaluate the error rates throughout all 100 partitions. However, for this experimental scheme, some of the test data is no longer statistically  X  X ure X  since it has been used during model selection. Furthermore, the use of median of the hyperparameters would introduce an optimistic bias [3]. In our ex-periments, we perform model selection on the training set of each partition, then train the classifier with the obtained optimal hype rparameters still on the training set, and finally evaluate the classifier on the corresponding test set. Therefore, we can obtain 100 test error rates for each dataset (except 2 0 for the Image and Splice dataset). The statistical analysis of these test error rates is conducted to assess the performance of the model selection approach. This experimental scheme is rigorous and can avoid the major flaws of the previous one [3]. All experiments are performed on a Core2 Quad PC, with 2.33GHz CPU and 4GB memory. 6.2 Effectiveness Following the experimental setup in Sectio n 6.1, we perform model selection respec-tively using 5-fold cross validation (5-fold CV) and approximate 5-fold CV, that is, approximate model selection by minimizing 5-fold CV error (as shown in Algorithm 2). The CV is performed on a 13  X  11 grid of (  X , X  ) respectively varying in [2  X  15 , 2 9 ] and [2  X  15 , 2 5 ] both with step 2 2 .Weset c = 0 . 1 n and k = 0 . 5 c in Algorithm 1.
We compare effectiveness of two model selec tion approaches. Effectiveness includes efficiency and generalization. Efficiency is measured by average computation time for model selection. Generalization is measur ed by the mean test error rate (TER) of the classifiers trained with the optimal hyperparameters produced by different model selec-tion approaches.

Results are shown in Table 2. We use the z statistic of TER [2] to estimate the sta-TER of two approaches, and e x and e y the corresponding standard errors, then the z nificance level. From Table 2, approximate 5-fold CV is significantly outperformed by 5-fold CV only on the Splice dataset, but the difference is just 2 . 5%. Besides, according to the Wilcoxon signed rank test [7], neither of 5-fold CV and approximate 5-fold CV is statistically superior at the 95% level of significance.
 However, Table 2 also shows that approximate 5-fold CV is more efficient than 5-fold gain is more obvious, which is in accord with the results of comp lexity analysis. In this paper, Nystr  X  om method was first introduced into the model selection problem. A brand new approximate model selection approach of LSSVM was proposed, which fully exploits the theoretical and computational virtue of Nystr  X  om approximation. We designed an efficient algorithm for solving LSSVM and bounded the effect of kernel matrix approximation on the decision function of LSSVM. We derived a model approx-imation error bound, which is a theoretical support for approximate model selection. We presented an approximate model selectio n scheme and analy zed its complexity as compared with other classic model selection approaches. This complexity shows the promise of the application of approximate model selection for large scale problems. We finally verified the effectiveness of our approach by rigorous experiments on several benchmark datasets.

The application of our theoretical results and approach to practical large problems will be one of major concerns. Besides, a new efficient model selection criterion directly dependent on kernel matrix approximation will be proposed in near future.
 Acknowledgments. The work is supported in part by the Natural Science Foundation of China under grant No. 61170019, and the Natural Science Foundation of Tianjin under grant No. 11JCYBJC00700.

