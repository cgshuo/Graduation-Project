  X  for a wide range of models.
 Assume we are given a dataset of N  X  1 inputs x a new query input x density function p ( x proach. Therefore, a general density estimation task is, gi ven a dataset X = x sampled which leads to the following simplification: The joint likelihood factorizes into a product of independe nt singleton marginals p flexible and might over-fit when the marginal p Consider the parametric ML and MAP setting where parameters  X  = {  X  define the marginals. We will use p ( x |  X  To mimic ML, simply set p (  X   X  the level of agreement between two marginals p affinity metric (Bhattacharyya, 1943) between two distribu tions: This is a symmetric non-negative quantity in both distribut ions p one if and only if p ( isd ) data: posterior as Equation 1 and when p (  X  disguises the fact that the samples are not quite id or iid . However, the conditional distributions p ( x |  X  eters  X  This produces a unique estimate for the parameters as was the case for id and iid setups. log-concave prior distributions.
 wise log-Bhattacharyya affinities: concave terms and concave log-Bhattacharyya affinities, it must be concave. first explore the  X  = 1 / 2 setting and subsequently discuss the  X  = 1 setting. nential family form as follows: log-concave in the parameters  X  putable in closed form as follows: an iterative algorithm to maximize the isd posterior. We find settings of  X  the isd posterior or log p eters is available  X   X  parameters (denoted  X   X  in log p  X  ( X ,  X ) that are variable with  X  n : log p  X  ( X ,  X  n ,  X   X  /n ) = const +  X  T n T ( x n )  X  this term is log-concave in  X  set to zero to maximize: iterative update rule for  X  compute a linear variational lower bound on each A (  X   X  : This gives an iterative update rule of the form of Equation 2 w here the  X  kept fixed at its previous setting (i.e. replace the right han d side  X  iterated multiple times until the value of  X  each iterative update of  X  (yet not log-concave) version of the isd score which has the form: log  X  p  X  ( X ,  X ) = const + X A (  X  n ) = We next examine marginal consistency, another important pr operty of the isd posterior. 3.1 Marginal Consistency in the Gaussian Mean Case over an observation and its associated marginal X  X  paramete r (which can be taken to be x and parameters  X  observations and N  X  1 models.
 Proof 2 Start by integrating over x Assume the singleton prior p (  X  Z Z In the (white) Gaussian case A (  X  ) =  X  T  X  which simplifies the above into: Z ( N  X  1) models raising it to the appropriate power  X / ( N  X  1) :
Z Z Gaussians, hidden Markov models, latent graphical models a nd so on). efficiently pull together parameters  X  makes it possible to maximize a lower bound on the isd posterior in these cases. Assume a current set of parameters is available  X   X  =  X   X  that increases the posterior while all other parameters (de noted  X   X  settings. It suffices to consider only terms in log p The application of Jensen X  X  inequality above produces an au xiliary function Q (  X  P h p ( x n , h |  X  n ) teriors given the previous parameters  X   X  X posterior. While it is possible to directly solve for the max imum of Q (  X  drawn from p ( x |  X   X  of large numbers) which is merely the update rule for EM for  X  x X etc.) which maximizes a lower bound on p tions, the arguments for log-concavity no longer hold. obtain potentially superior models  X   X  SPIRAL -5.61e3 -1.36e3 -1.36e3 -1.19e3 -7.98e2 -6.48e2 -4.86e2 -2.26e2 -1.19e2 MIT-CBCL -9.82e2 -1.39e3 -1.19e3 -1.00e3 -1.01e3 -1.10e3 -3.14e3 -9.79e2 -9.79e2 DIABETES -6.25e3 -2.12e5 -2.85e5 -4.48e5 -2.03e5 -3.40e5 -8.22e2 -8.28e2 -8.09e2 CANCER -5.80e3 -7.22e6 -2.94e6 -3.92e6 -4.08e6 -3.96e6 -1.22e2 -5.54e2 -5.54e2 data via P demanding method). In another synthetic experiment with hi dden Markov models, 40 sequences certain values of  X  produced higher test log-likelihoods than id and iid . An alternative is the Kullback-Leibler divergence D ( p and its symmetrized variant D ( p and p Thus, B ( p bound on the Bhattacharyya is q ( x ) = 1 and is therefore equal to the Bhattacharyya affinity. Thus we have the following property: 2 JS ( p m , p n ) = D ( p m k p m / 2 + p n / 2) + D ( p n k p m / 2 + p n / 2) = min Simple manipulations then show 2 JS ( p close ties between Bhattacharyya, Jensen-Shannon and symm etrized KL divergences.
