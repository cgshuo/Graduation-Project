 The problem of aligning ontologies and database schemas across different knowledge bases and databases is fundamen-tal to knowledge management problems, including the prob-lem of integrating the disparate knowledge sources that form the semantic web X  X  Linked Data [5].

We present a novel approach to this ontology alignment problem that employs a very large natural language text corpus as an interlingua to relate different knowledge bases (KBs). The result is a scalable and robust method (PID-GIN 1 ) that aligns relations and categories across different KBs by analyzing both (1) shared relation instances across these KBs, and (2) the verb phrases in the text instantia-tions of these relation instances. Experiments with PIDGIN demonstrate its superior performance when aligning ontolo-gies across large existing KBs including NELL, Yago and Freebase. Furthermore, we show that in addition to align-ing ontologies, PIDGIN can automatically learn from text, the verb phrases to identify relations, and can also type the arguments of relations of different KBs.
 I.2.4 [ Artificial Intelligence ]: Knowledge Representation Formalisms and Methods; I.2.6 [ Artificial Intelligence ]: Learning; I.2.7 [ Artificial Intelligence ]: Natural Language Processing Algorithms, Experimentation Ontology Alignment, Knowledge Bases, Graph-based Self-Supervised Learning, Label Propagation, Natural Language Processing. Over the last few years, several large, publicly available Knowledge Bases (KBs) have been constructed, such as DB-Pedia [3], Freebase [6], NELL [8], and Yago [18]. These KBs consist of both an ontology that defines a set of cat-egories (e.g., Athlete, Sports ) and relations (e.g., player-PlaysSport(Athlete, Sport) ), and the data entries which in-stantiate these categories (e.g., Tiger Woods is an Athlete ) and relations (e.g., playerPlaysSport(Tiger Woods, Golf ) ). The growth of the Semantic Web [4] has contributed sig-nificantly to the construction and availability of such KBs. However, these KBs are often independently developed, us-ing different terminologies, coverage, and ontological struc-ture of categories and relations. Therefore, the need for automatic alignment of categories and relations across these and many other heterogeneous KBs is now greater than ever, and this remains one of the core unresolved challenges of the Linked Data movement [5].

Research within the Ontology Matching community has addressed different aspects of this Ontology Alignment prob-lem, with recently proposed PARIS [17] being the current state-of-the-art in this large body of work (see [16] for a recent survey). PARIS is a probabilistic ontology matcher which uses the overlap of instances between two relations (or categories) from a pair of KBs as one of the primary cues to determine whether an equivalence or subsumption rela-tionship exists between those two relations (or categories). PARIS, and the instance overlap principle which it shares with most previous ontology alignment systems, has been found to be very effective in discovering alignments when applied to KBs such as DBPedia [3], Yago [18], and IMDB 2
Despite this recent progress, the current state-of-the-art remains insufficient to align ontologies across many practi-cal KB X  X  and databases, especially when they share few or no data entries in common. To overcome this shortcoming, we introduce a new approach that is capable of matching the categories and relations across multiple KB X  X  even in the extreme case where they share no data entries in com-mon. The key idea is to introduce side information in the form of a very large text corpus (in our case, 500 million dependency-parsed web pages). Our approach, called PID-GIN, effectively grounds each KB relation instance (e.g., playerPlaysSport(Rodriguez, baseball) ) by its mentions in this text, then represents the relation in terms of the verbs that connect its arguments (e.g., the relation playerPlaysSport(x,y) might frequently be expressed in text by verbs such as  X  X  plays y X  or  X  X  mastered y X  ). The distribution of verbs as-sociated with instances of any given relation forms a KB-independent representation of that relation X  X  semantics, which can then be aligned with relations from other KBs. In essence, the verb distributions associated with relations pro-vide an interlingua that forms the basis for aligning ontolo-gies across arbitrary KBs, even when their actual data en-tries fail to overlap. PIDGIN integrates this text information with information about overlapping relation instances across the two KB X  X  using a graph-based self-supervised learning strategy, to determine their final ontology alignment.
In particular, we make the following contributions:
We first illustrate the core ideas behind PIDGIN through a motivating example. Let us consider the alignment prob-lem involving two KBs as shown in Figure 1. In this case, KB 1 contains the relation ( bornIn ) with two instances, while KB 2 contains the relation personBornInCity with two other instances. In this case, we would like to discover the align-ment KB 1 :bornIn  X  KB 2 :personBornInCity despite the fact that the relation names are different, and their instances do not overlap. Note that previous ontology alignment algo-rithms (such as PARIS [18]) will not be able to discover the desired alignment.

Need for Interlingua : In order to overcome such over-lap sparsity, PIDGIN analyzes a large natural language text corpus to determine the expression pattern of instances from different relations. For example, in the text corpus, PID-GIN finds that instances of the KB 1 :bornIn relation is of-ten expressed using the verb phrase  X  X as born in X  , e.g., (Bill Clinton, bornIn, Hope) is expressed using this verb phrase in the sentence  X  X ormer President Bill Clinton was born in Hope, AK X  . PIDGIN also finds that KB 2 relation instance (Barack Obama, personBornInCity, Honolulu) is expressed using the same verb phrase in sentences such as  X  X resident Barack Obama was born in Honolulu, HI X  . So, even though there is no direct overlap of instances between the relations KB 1 :bornIn and KB 2 :personBornInCity , PID-GIN might be able to discover the equivalence KB 1 :bornIn  X  KB 2 :personBornInCity by exploiting overlapping expres-sion patterns (i.e., verbs) of these two relations in natural language text, the interlingua .
In this section, we outline the terminology used and present the problem definition. We define a Knowledge Base (KB) K to be a 6-tuple ( C,O C ,I C ,R,O R ,I R ), where C is the set of categories (e.g., athlete , sports ), O C is the category on-tology, which specifies the subset/superset hierarchical re-lations among categories (e.g., that athlete is a subset of person ), I C is the set of entity-category pairs (e.g., ( Tiger Woods , athlete )) for categories in C , R is the set of rela-tions (e.g., athletePlaysSport(athlete, sports) ), O R is the re-lation ontology, which specifies the hierarchy of relations (e.g., ceoOf(person,company) is a special case of the relation worksFor(person, company) ), and I R is the set of entity-relation-entity triples for relations in R (e.g., (Tiger Woods, athletePlaysSport, Golf ) ). We allow O C and O R to be empty, i.e., the KB may have a flat category and relation structure. Each instance of a relation r  X  R is a 3-tuple ( e 1 ,r,e where ( e 1 ,c 1 )  X  I C and ( e 2 ,c 2 )  X  I C for some c Note that each entity can be referred to by one or more Noun Phrases (NP). For example, the entity Tiger Woods , can be instantiated in text using either the NP Tiger Woods or the NP Eldrick Tont Woods . Let N ( e ) be the set of NPs corresponding to entity e .

Let D be the Subject-Verb-Object (SVO) based interlin-gua consisting of tuples of the form ( np 1 ,v,np 2 ,w ), where np 1 and np 2 are noun phrases (NP) corresponding to sub-ject and object, respectively, v is a verb, and w  X  R + 3 normalized count of this tuple in a large text corpus.
Given two sets of relations R 1 and R 2 , we define the align-ment between them to be the set A ( R 1 ,R 2 ) = { ( r 1 ,a,r r  X  R 1 ,a  X  { X  ,  X  X  ,r 2  X  R 2 ,w  X  R } , where  X  signifies re-lation equivalence,  X  less general, and R is the set of real numbers. In other words, ( r 1 ,  X  ,r 2 ,w ) signifies that relation r  X  R 1 is less general than relation r 2  X  R 2 , with w  X  R representing the confidence in this alignment. Similarly for the equivalence alignment (  X  ). We similarly define the cat-egory alignments A ( C 1 ,C 2 ) = { ( c 1 ,a,c 2 ,w ) | c { X  ,  X  X  ,c 2  X  C 2 ,w  X  R } .
 Problem Definition : Given two knowledge bases (KBs) K I 2 ), and a syntactically-parsed text corpus D , we would like to discover the category and relation alignments, A ( C 1 and A ( R 1 ,R 2 ) respectively.
PIDGIN is able to exploit large web text as interlingua by posing the ontology alignment problem as a classifica-tion problem over an appropriately constructed graph. The system consists of two stages: 1. Graph Construction : Given two KBs and a Subject-2. Alignment as Classification over Graph : Once For ease of explanation and readability, we present all ex-amples and descriptions involving two ontologies. However, please note that PIDGIN is capable of handling multiple ontologies simultaneously.

We now turn to describing PIDGIN X  X  two stages in detail. R ,O R 2 ,I R 2 ), and a SVO-based interlingua D 4 , PIDGIN first constructs a graph G = ( V,E,W ), where V is the set of vertices, E is the set of edges, and W i,j representing the weight of the edge ( i,j )  X  E . We describe the construction of this graph below.

We first initialize V =  X  , E =  X  , and W is an all zero matrix, with edge weight 0 indicating absence of the edge. All edges in G are undirected and untyped.
At the end of this stage, we end up with a graph G = ( V,E,W ) as shown in Figure 1(b) when given Figure 1(a) as input to PIDGIN.
At this point, we have a graph G = ( V,E,W ) with n = | V | and m = | E | . PIDGIN poses the ontology alignment prob-lem as one of classification of nodes in G . It starts out by attempting to align relations from the two KBs, and pro-duces category alignment as a by-product. So, we shall first look at how PIDGIN solves the relation alignment problem.
For each relation r 1  X  R 1 , PIDGIN generates two labels and injects them as seed labels into nodes of the graph G as follows: Let L 1 be the union of these labels, with | L 1 |  X  2  X | R Starting with this seed information, PIDGIN now applies Modified Adsorption (MAD) [19], a graph-based self-supervised learning (SSL) algorithm, to classify the rest of the nodes in the graph taking transitivity in account. Since nodes corre-sponding to R 1 were injected, let  X  Y 1  X  R n  X | L 1 | be the esti-mated label score matrix generated by MAD, where  X  Y 1 ( r,l ) is the score of label l  X  L 1 on node r  X  V . In the current setting, MAD will solve the following optimization problem to estimate  X  Y 1 : where  X  1 , X  2 , and  X  3 are hyperparameters; L 0 1 = L 1 with  X  as the none-of-the-above label; S is a seed node se-lection vector with S v = 1 if v  X  R 1 and 0 otherwise; Y is the score of seed label l on node v (if any); M is a mod-ified version of edge weight matrix W ; and F is a regular-ization matrix. In the MAD objective above, the first and third terms encourage the algorithm to match seed scores (if any) and regularization targets in a soft way, respectively, while the second term encourage smooth label score vari-ation over graph. This is essentially soft enforcement of transitivity, making MAD a suitable inference scheme for PIDGIN. MAD X  X  objective is convex which it solves exactly by iteratively updating scores of labels on the nodes. This takes the form of propagating labels over the graph. These updates can be easily implemented in MapReduce, thereby making MAD, and hence PIDGIN, suitable for large ontol-ogy alignment problems 5 . We refer the reader to [19] for further details on MAD.

After estimating  X  Y 1 as described above, PIDGIN repeats the same process, but in the reverse direction, i.e., it now injects the nodes corresponding to r  X  R 2 with label set L and propagates those labels to rest of the nodes using MAD. As before, we end up with an estimated label score matrix  X  Y
The final set of relation alignments, A ( R 1 ,R 2 ), discovered by PIDGIN can be divided into the following two subsets: The equivalence alignments between relation sets R 1 and R 2 are estimated as follows: where  X  Y 1 and  X  Y 2 are the label score matrices estimated by MAD as described in previous section. In other words, to establish the equivalence r 1  X  r 2 , we want to make sure that relation node r 2 is assigned the label l r 1 specific to relation r with a high score by MAD, and vice versa. We define fi-nal equivalence alignment score as  X  Y  X  ( r 1 ,r 2 ) =  X   X  Y
Subsumption alignments between relation sets R 1 and R 2 are estimated as follows: In other words, PIDGIN infers alignment r 2 ,  X  ,r 1 if the sub-sumption label l ~r 1 specific to node r 1  X  R 1 is assigned by MAD with higher score than the label l r 1 to node corre-sponding to relation r 2  X  R 2 . We shall call this score for r ,  X  ,r 1 by  X  Y  X  ( r 1 ,r 2 ) =  X  Y 1 ( r 2 ,l ~r 1 ).
From Section 4.2.1, we have  X  Y  X  ( r 1 ,r 2 ) as the relation equivalence score estimated by PIDGIN for relations r 1  X  R and r 2  X  R 2 . PIDGIN uses this estimate to establish cate-gory equivalence alignments as follows. Given a relation r , let Dom( r ) and Ran( r ) be its domain and range categories, i.e., categories of the two entities connected by this relation. We define, We define the final category equivalence alignments as, A ( C 1 ,C 2 ) = { ( c 1 ,  X  ,c 2 ,H Dom  X  ( c 1 ,c 2 ) + H Table 1: Statistics of KBs used in experiments. We use NELL as
We conduct our experiments on several large scale open-domain publicly available real-world KBs, namely NELL [8] (a large scale KB extracted automatically from web text), Yago2 [11] (a large scale KB extracted automatically from semi-structured text of Wikipedia infoboxes), Freebase [6] (a large scale KB created collaboratively and manually by hu-mans), and KB Population (KBP) dataset (a smaller scale, manually constructed dataset used in the 2012 Text Analysis Conference for entity-linking, slot-filling and KB population tasks 6 ). Table 1 shows the statistics of the KBs used in our experiments.

In each experiment, we are given two KBs to align: KB 1 and KB 2 with sets of relations R 1 and R 2 . For equivalence alignments, PIDGIN returns for each relation r 1  X  R 1 For each r 1 , we compare our ranked list of r 2 with that of PARIS . We infer PARIS equivalence alignments from its output subsumption alignments P 12 and P 21 , i.e., P { ( r 1 ,  X  ,r 2 ,score p 12 ) } and P 21 = { ( r 2 ,  X  ,r score p 12 is PARIS confidence measure that r 1  X  r 2 score p 21 is PARIS confidence measure that r 2  X  r 1 . We com-pute PARIS equivalence alignments P = { ( r 1 ,  X  ,r 2 ,score score p 21 ) } , i.e., we define equivalence (  X  ) relation between r 1 and r 2 if both r 1  X  r 2 and r 2  X  r 1 . We also compare against a baseline that computes the equivalence of r 1 and r using Jaccard similarity measures based on the number of overlap instances that r 1 and r 2 have, i.e., Jaccard ( r call this overlap-based alignment JACCARD (inst) .

For subsumption alignments, PIDGIN returns for each relation r 1  X  R 1 , a list of r 2  X  R 2 ranked by  X  Y  X  ( r Section 4.2.1).

The list of the systems evaluated include: In the experiments, we want to answer the following:
When KB 1 and KB 2 are aligned, for each relation r 1  X  R a list of relations r 2  X  R 2 that align to r 1 is returned, ranked by some scores. We treat this ranked list as a returned  X  X ocument X  for r 1 . The document is considered relevant at top-k , if any of the relations r 2 in its top-k matches the gold standard r gold 2 for r 1 . The gold standard r gold 2 for r R , is a set of relations that are deemed equivalent to (or subsume) r 1 by human annotators for the task of equivalence (or subsumption) alignment between R 1 and R 2 . In the case that | r gold 2 | &gt; 1, we consider a document relevant if it returns at least one of the relations in this set. We measure precision of a system as the number of relevant documents returned by the system, divided by the total number of documents returned by the system. We measure recall of a system as the number of relevant documents returned by the system over the number of relations r 1 for which there is a gold standard alignment i.e., | r gold 2 | &gt; 0. The F1 -score measures the harmonic mean of the precision and recall. We report these precision, recall, and F1-scores at various values of k . Precision of a 100% at k = 1 means that for every relation r  X  R 1 , its gold standard mapping can be found at the top Table 2: Precision, Recall and F1 scores @ k =1 of Relation Table 3: Precision, Recall and F1 scores @ k =1 of relation sub-of the list of alignments for r 1 . Precision of 100% at k = 5 means that for every relation r 1 , its gold standard mapping can be found somewhere within the top 5 relations in the output list of alignments for r 1 .
We find alignments between Freebase and NELL, Yago2 and NELL, and KBP and NELL. We evaluate precision, recall and F1 scores of resulting alignments against the gold standard alignments produced by human annotators. In this set of experiments, we compare the performance of PIDGIN, PARIS, and JACCARD (inst) for relation equiva-lence alignment. We report precision, recall and F1-scores at k = 1 of the equivalence alignments returned. We observe in Table 2 that PIDGIN always has much higher recall (with-out sacrificing precision) than PARIS or JACCARD (inst). In two of the three experiments, precision of PIDGIN is also highest. We conjecture that recall improves due to the im-proved coverage of the alignments by the use of interlingua and transitivity of inference in PIDGIN. For example, the re-lation /medicine/medical treatment/side effects in Freebase has no instance overlap with any relation in NELL; thus PARIS and JACCARD (inst) are not able to find align-ments for this relation. However, its instances co-occur with some similar verbs in the SVO such as  X  may promote  X ,  X  can cause  X ,  X  exacerbate  X  as instances of the relation drughasside-effect in NELL. Thus PIDGIN is able to map this Freebase relation to the appropriate NELL relation. Another example is the reified relation /sports/league/arena stadium in Free-base that has no instance overlap with relations in NELL. However, its instances are represented by some similar NP pairs as instances of the relation leaguestadiums in NELL. Thus PIDGIN is able to find a mapping of this relation. Table 4: F1 scores @ k = 1, 3 and 5 of Category Equivalence
The effect of over-reliance on instance overlap to predict alignments is worse when the KB is small. In the KBP dataset, PARIS only returns equivalence alignments for 2 out of the 17 relations in KBP. No correct alignments are returned by either PARIS or JACCARD at k = 1. We also observe that precision, recall and F1-scores are highest when aligning Freebase and NELL. This maybe because Freebase has the largest number of facts and is probably the cleanest KB (since it is created manually). The scores are the lowest when aligning KBP and NELL, probably because the KBP is very small and have sparse edges to both NELL and the SVO. In future, for such small and domain-specific KB, we can expand our interlingua to include more targeted natural language text, for example by using web search to find more documents that mention entities in the KB. In these experiments, we compare the performance of PID-GIN and PARIS for relation subsumption alignment. We report precision, recall and F1-scores at k = 1 of the sub-sumption alignments returned. 7 We observe in Table 3 that, similar to the equivalence alignment results, PIDGIN always has much higher recall (without sacrificing precision) than PARIS. In two of the three experiments, precision of PIDGIN is highest. We conjecture that the improved recall (and subsequently F1) is due to the use of interlingua and transitivity of inference in PIDGIN.
We compare the performance of PIDGIN and PARIS for inferring category equivalence alignment between Yago2 and NELL. PARIS infers category alignment from its instance alignment [17]. We report the F1-scores @ k =1, 3, and 5 of the returned alignments against the gold standard align-ments. We observe that PIDGIN category alignments have highest F1-scores at different values of k (Table 4). Some ex-amples of PIDGIN alignments include Yago2 category word-net actor 109765278 aligned to NELL category actor , Yago2 category yagoURL aligned to NELL category website , and Yago2 category yagoLegalActor aligned to NELL category agent .
In these experiments, we evaluate whether adding more resources extracted from text to the graph improves align-ment performance. We construct three graphs: the first contains only relation and instance nodes (PIDGIN (inst)). The second has added NP pair nodes representing instances (PIDGIN (inst + NPs)), while the third contains relation, instance, NP pair, and verb nodes (PIDGIN (inst + NPs + verbs)). We report F1-scores obtained by PIDGIN on these Figure 2: F1 scores @ k = 1 and 5 of relation equivalence align-three graphs. We observe in Figure 3 that adding more re-sources to the graph improves performance. Using all the resources available seems to result in the best performance than using relations and instances alone.

Previous works have suggested that adding more ontolo-gies as background knowledge improves the resulting align-ments [1], the results of our experiments here seem to con-firm this. We construct a graph containing the SVO data and the relations and instances from more than two KBs. We propagate relation labels as before to align Freebase and NELL (with added Yago2 in the graph), to align Yago2 and NELL (with added Freebase in the graph), and to align KBP and NELL (with added Freebase and Yago2 in the graph). The transitivity of inference in our approach allows two re-lations with no instance overlap to be aligned through an interlingua , which can take the form free text or structured ones. In this case, the additional KBs are another interlin-gua . We observe in Figure 2 that adding data from more KBs (PIDGIN (multiple)) results in a comparable or im-proved performance of alignment compared to using data from only two KBs (PIDGIN (binary)). When the KB is small (e.g., KBP), adding more data to the graph approxi-mately doubles the F1-scores. We believe this is due to the increased connectivity of the graph (more paths for prop-agating NELL labels to corresponding KBP labels). This increased connectivity combined with transitivity of infer-ence improves performance. Adding more KBs to the al-ready large graph (Freebase and NELL or Yago2 and NELL) does not seem to improve performance. This begs the ques-tion that we can explore in future of how much background knowledge is necessary to improve alignment performance.
In these experiments, we evaluate whether the transitivity of inference and joint inference in label propagation (PID-GIN) improve alignment performance when compared to the overlap-based approach (JACCARD). We observe in Figure 4 that using PIDGIN improves performance (F1-scores) at various values of k compared to the overlap-based approach Figure 3: F1 scores @ k = 1 and 5 of relation equivalence align-even when the same set of resources are being used in both. We also experiment with using cosine based similarity on the same set of resources to compute alignment. We observe but do not report here that cosine-based approach gives compa-rable or worse performance than using Jaccard similarity.
We also notice in separate experiments that in some cases, adding NP pairs and verbs to the overlap-based approach de-creases performance when compared to using instance over-lap alone. This maybe due to the amount of noise that is inherent in the natural language corpus from which the SVO dataset is obtained. However, we have observed pre-viously in Section 5.4 that adding these resources does not hurt PIDGIN performance. The joint inference framework of PIDGIN that jointly optimizes similarities coming from instances, NP pairs and verbs, may be what makes it more tolerant to noise in the interlingua than a simple overlap-based similarity. In these experiments, we compare the performance of PID-GIN and PARIS when different amounts of noise are added to the KB. We randomly pick some instances in a KB and randomly switch them to other relations in the same KB -adding noise to the KB. Note that the original KB may already contain noise due to noisy extractions or wrongly added facts, for example. We observe in Figure 5 that the performance (F1-scores at k = 5) of PIDGIN decreases only gradually with the amount of noise added, while the perfor-mance of PARIS decreases rapidly and in some cases drops to zero, which means no correct alignments are returned by PARIS. Surprisingly, adding 20% of noise to KBP improves performance. Due to the randomness with which we intro-duce noise , some relations in KBP that are noisy actually become cleaner when the random process coincidentally re-moves these instances and assigns them to other relations.
In this section we consider capabilities of PIDGIN to per-form tasks beyond aligning relations across ontologies. Figure 4: F1 scores @ k = 1 and 5 of relation equivalence align-
For each Freebase, Yago2 and KBP relation, we convert the ranked list of NELL relations aligned to it into a ranked list of the corresponding NELL  X  domain,range  X  pairs, sum-ming up the scores of similar  X  domain,range  X  pairs in the list. For each Freebase, Yago2 and KBP relation, we then have a ranked list of NELL  X  domain,range  X  as candidate argument types for the relation. For example, KBP re-lation per:city of birth has NELL  X  person , city  X  type. We evaluate F1-scores of these ranked candidate types against the gold standard argument types. We observe in Figure 6 that PIDGIN types have higher F1-scores than PARIS for typing relations in different KBs. Some examples of typ-ing produced by PIDGIN include Yago2 isPoliticianOf as-signed NELL  X  person , geoPoliticalLocation  X  type, and Free-base /business/industry/name (i.e., the type of industry a company operates in) being assigned NELL category pair  X  company , economicsector  X  .
As a by product of label propagation on the graph, each verb and NP-pair node in the graph will be assigned scores for each relation label. Exploiting these scores, we can esti-mate the probability that a verb v represents a relation r as score of label r assigned to verb node v . Since a verb may represent different relations depending on the NP-pair with which it co-occurs e.g., the verb enter has different meaning when it appears with an NP-pair  X  Paul , room  X  from when it appears with an NP-pair  X  John , American Idol  X  ; when esti-mating P ( v | r ) we also take into account the scores of r on the NP-pair nodes  X  NP 1 ,NP 2  X  with which verb v co-occurs. Similar as before, P ( v | r )  X   X  Y ( v,r ) P sure  X  Y ( v,r ) = P T v , np 2  X  , and where  X  Y ( T v ,r ) =  X  Y (  X  np 1 ,np 2 multiply this estimate with the tf-idf score of the verb, which is proportional to the number of times a verb appears for a Figure 5: F1 scores @ k = 5 of relation equivalence alignments relation, but is offset by the total number of times the verb appears with all the relations. This helps to reduce the ef-fect of common verbs such as is , become that represent many relations.

Using this scoring, for each relation we can return a ranked list of verbs that represent the relation. Some of the verbs returned are shown in Table 5. As we can see in Table 5, the system is able to distinguish verbs representing the relation /medicine/medical treatment/side effects :  X  exacerbate  X ,  X  can cause  X  from the verbs representing the antonym relation drugPossiblyTreatsPhysiologicalCondition :  X  relieve  X ,  X  can help alleviate  X  even when the two relations have the same domain ( drug ) and range ( physiological condition ). The system is also able to recognize the directionality of the relation. For example, for the relation acquired , which represents the in-verse of the relation acquired (as in company X acquiring company Y); the system is able to return the correct verbs: bought and purchase , which are the inverse forms of the verbs bought and purchase (as in is bought by and is pur-chased by ). Of practical importance is the fact that PIDGIN can learn verbs representing relations in Freebase and Yago2 whose facts are created manually or extracted via carefully constructed regular-expression matching. We can now use these verbs to automate an extraction process for the on-tologies used by Freebase and Yago2.
Here we examine the accuracy of automatically extract-ing new relation instances from text based on assignment of relation labels on NP pair nodes. We can estimate the probability that an NP-pair  X  np 1 ,np 2  X  belongs to a relation r, by estimating P (  X  np 1 ,np 2  X  X  r )  X   X  Y (  X  np 1 ,np For each relation we can then return a ranked list of NP-pairs that can belong to the relation, some of which may not already be in the KB and can be proposed as new in-stances for the relation. We return the top 100 NP-pairs for each relation, pick those that are not already in the KB and evaluate the average precision. Consistent with our re-Figure 6: F1 scores @ k = 1, 3 and 5 when typing relations sults so far, precision of proposed new instances improves with the size of the KB. New instances proposed for Free-base have the highest average precision of 73.9% while new instances proposed for NELL, a smaller KB, have an average precision of 65.5%. Correct new instances discovered include  X  ratatouille , brad bird  X  for Freebase /film/directed by , and  X  wwl , cbs  X  for NELL televisionStationAffiliatedWith . In the future, we want to use verb patterns that PIDGIN learns for relations in Section 5.7.2 to extract new NP pairs belonging to the relations from any free text.
Ontology Alignment has received considerable attention in prior research, please see [16] for a recent survey. PARIS [17] is a recently proposed state-of-the-art ontology align-ment system that is most related to PIDGIN. PARIS primar-ily relies on instance overlap-based cues to align instances, categories, and relations from two KBs. This can be prob-lematic in many cases of practical interest where there is sparsity in instance overlap, and noise in the data. PIDGIN overcomes these limitations using of natural language text as interlingua and graph-based self-supervised learning. In extensive comparisons, we find that PIDGIN significantly outperforms PARIS on the ontology alignment task.

Other prior work on ontology alignment have primarily relied on lexical and structural matching of the elements (i.e., categories or relations) in the ontology only [16], with the exception of a few that also consider instance similarities [17, 12]. And in most cases, category alignment has been the sole focus. PIDGIN exploits instance similarities, and aligns both categories and relations.

Low alignment recall (  X  30%) in real-world datasets has been a common problem in most previous ontology align-ment systems [10]. One way to improve recall is through iterative matching to increase completeness of alignments [2, 10, 12], which is similar in spirit to PIDGIN X  X  use of la-bel propagation for alignment inference. Background knowl-edge has been found to be useful in improving recall [16]. Most systems that use background knowledge share a com-mon theme: to expand elements X  features before lexical sim-Table 5: Examples of relation-verb pairs automatically learned ilarity computation. WordNet or WordNet-like data is a popular choice for background knowledge, with exceptions such as [9] that use documents related to instances to cre-ate lexical resource for the elements and use cosine similar-ity measures on the lexicons. Other works use background knowledge to construct additional paths to align elements. These works however, require background knowledge to be provided in the form of formal ontology [2, 15], which is not always available. An exception is [13] which uses unstruc-tured documents and co-occurrence of category instances in documents to align categories. However, they only align categories. Co-occurrences of relation instances (i.e., a pairs of concept-pairs) in documents is significantly sparser com-pared to co-occurrence of concept pairs (as used in previous work). This sparsity can lead to low recall of their relation alignment. PIDGIN overcomes these limitations by exploit-ing a large coverage, schema-free natural language text cor-pus as interlingua. However, PIDGIN is flexible enough to incorporate other (and multiple) types of background knowl-edge (e.g., formal ontology) whenever available.

Although increasing the amount of background knowledge has been shown to improve alignment further [1], combin-ing these pieces of information in prior work requires careful consideration due to the different structures of the back-ground knowledge [2]. PIDGIN X  X  graph construction pro-vides a flexible way of integrating several background knowl-edge easily without restrictive constraints. Furthermore, in-tegration of alignments obtained from multiple background knowledge sources has been rather ad-hoc in prior work [16]. In contrast, PIDGIN integrates heterogeneous evidence in the graph automatically and in a principled manner via its joint inference and optimization process.

Most previous systems employing iterative matching have relied on hand-written rules, with the exception of S-match [10]. However, S-match uses ontology-level information only, without considering instances. In PIDGIN, iterative match-ing is automatically obtained via the transitivity of inference in label propagation (MAD). Another system that uses label propagation for ontology alignment is [20]. However, they do not incorporate any interlingua in their approach.
One approach that may strike resemblance to PIDGIN is Similarity Flooding [14], but there are critical differences. Firstly, it is not clear how natural language-based interlingua which do not have any well-defined schema can be incorpo-rated into Similarity Flooding. PIDGIN is flexible enough to incorporate such side information, and improve resulting ontology alignment. Secondly, Similarity Flooding requires both ontologies to have similar edge types, while PIDGIN doesn X  X  impose any such strong requirement. Thirdly, Simi-larity Flooding is also sensitive to noise in the ontology since the cross join of elements amplifies a noisy element in one ontology to elements in other ontologies. PIDGIN is more robust to noise as it doesn X  X  multiply the effect of a noisy alignment, and instead integrates it with other competing alignments to produce a final alignment.
In this paper we introduce PIDGIN, a novel, flexible, and scalable approach to automatic alignment of real-world KB ontologies, demonstrating its superior performance at align-ing large real-world KB ontologies including those of NELL, Yago and Freebase. The key idea in PIDGIN is to align KB ontologies by integrating two types of information: relation instances that are shared by the two KBs, and mentions of the KB relation instances across a large text corpus.
PIDGIN uses a natural language web text corpus of 500 million dependency-parsed documents as interlingua and a graph-based self-supervised learning to infer alignments. To the best of our knowledge, this is the first successful demon-stration of using such a large text resource for ontology align-ment. PIDGIN is self-supervised , and does not require hu-man labeled data. Moreover, PIDGIN can be implemented in MapReduce, making it suitable for aligning ontologies from large KBs.

We have provided extensive experimental results on mul-tiple real world datasets, demonstrating that PIDGIN sig-nificantly outperforms PARIS, the current state-of-the-art approach to ontology alignment. We observe in particular that PIDGIN is typically able to improve recall over that of PARIS, without degradation in precision. This is presum-ably due to PIDGIN X  X  ability to use text-based interlingua to establish alignments when there are few or no relation instances shared by the two KBs. Additionally, PIDGIN automatically learns which verbs are associated with which ontology relations. These verbs can be used in the future to extract new instances to populate the KB or identify re-lations between entities in documents. PIDGIN can also assign relations in one KB with argument types of another KB. This can help type relations that do not yet have argu-ment types, like that of KBP. Argument typing can improve the accuracy of extraction of new relation instances by con-straining the instances to have the correct types.

In the future, we plan to extend PIDGIN X  X  capabilities to provide explanations for its inferred alignments. We also plan to experiment with aligning ontologies from more than two KBs simultaneously.
 [1] Z. Aleksovski, M. Klein, W. Ten Kate, and [2] Z. Aleksovski, W. ten Kate, and F. van Harmelen. [3] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, [4] T. Berners-Lee, J. Hendler, O. Lassila, et al. The [5] C. Bizer, T. Heath, and T. Berners-Lee. Linked [6] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and [7] J. Callan, M. Hoy, C. Yoo, and L. Zhao. Clueweb09 [8] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. [9] D. Fossati, G. Ghidoni, B. Di Eugenio, I. Cruz, [10] F. Giunchiglia, P. Shvaiko, and M. Yatskevich. [11] J. Hoffart, F. Suchanek, K. Berberich, [12] Y. Jean-Mary and M. Kabuka. Asmov: Ontology [13] C. Kingkaew. Using unstructured documents as [14] S. Melnik, H. Garcia-Molina, and E. Rahm. Similarity [15] M. Sabou, M. d X  X quin, and E. Motta. Using the [16] P. Shvaiko and J. Euzenat. Ontology matching: state [17] F. M. Suchanek, S. Abiteboul, and P. Senellart. Paris: [18] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: a [19] P. P. Talukdar and K. Crammer. New regularized [20] P. P. Talukdar, Z. G. Ives, and F. Pereira.

