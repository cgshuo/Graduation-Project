 corinna@google.com convergence of the training algorithm is guaranteed.
 performance improvements.
 dimensions.
 demonstrating the effectiveness of our technique.
 problem. Let K coefficients of the form: Any kernel function K Note that K of coefficients of size m . Instead, we can assume that for some subset I of all p -tuples ( k be written as a product of non-negative coefficients general form of the polynomial combinations we consider bec omes , . . . , p . More specifically, we consider kernels K  X  defined by where  X  =( where the quadratic kernel can be given the following simple r expression: results for degrees d up to 4 . 3.1 Optimization Problem m , S = (( x space. The family of hypotheses H kernel Hilbert space (RKHS) associated to a PDS kernel funct ion K parameter vector  X  defining the kernel K S .
 [ y 1 , . . . , y m ]  X   X  R m kernel K optimization algorithm for a fixed kernel matrix K  X   X  R m by [16]: The related problem of learning the kernel K min-max optimization problem [9]: where M is a positive, bounded, and convex set. The positivity of  X  ensures that K natural choices for the set M are the norm-1 and norm-2 bounded sets, These definitions include an offset parameter  X  are:  X  norm-1-type regularization may not lead to a sparse solutio n. 3.2 Algorithm Formulation theorem to permute the min and max operators, which can lead to optimization problems com-technique is unfortunately not applicable.
  X  the optimum is given by minimization in terms of  X  only: point or gradient methods are not guaranteed to be successfu l at finding a global optimum. to guarantee the convergence of a gradient-descent type alg orithm to a global minimum. projects  X  back to the feasible set M there are in fact concave regions of the function near 0 .
 Algorithm 1 Projection-based Gradient Descent Algorithm
Input:  X  repeat until k  X   X   X   X  k &lt;  X  unnecessary since  X  F is never positive.
 ters. 3.3 Algorithm Properties In what follows,  X  denotes the Hadamard (pointwise) product between matrices . Proposition 1. For any k  X  [1 , p ] , the partial derivative of F :  X   X  y  X  ( K to where U Proof . In view of the identity  X  Matrix U Proposition 2. Any stationary point  X   X  of the function F :  X   X  y  X  ( K maximizes F :  X  Thus, for any such stationary point  X   X  , F (  X   X  ) = y  X  ( K maximum.
 within the feasible set, unless the function is constant.
 Proposition 3. If any point  X   X  &gt; 0 is a stationary point of F :  X   X  y  X  ( K function is necessarily constant.
 alently, y is an eigenvector of K Thus, assumption indicate that the optimum is found at the boundary.
 convex region C . If the boundary region defined by k  X   X   X  separating the terms that depend on K product of two matrices.
 Proposition 4. The function F :  X   X  y  X  ( K following condition holds for all  X   X  C and all u :
Data m p lin. base lin.  X  Parkinsons 194 21 . 70  X  . 03 . 70  X  . 04 . 70  X  . 03 . 65  X  . 03 . 66  X  . 03 . 64  X  . 03 Iono 351 34 . 82  X  . 03 . 81  X  . 04 . 81  X  . 03 . 62  X  . 05 . 62  X  . 05 . 60  X  . 05 Sonar 208 60 . 90  X  . 02 . 92  X  . 03 . 90  X  . 04 . 84  X  . 03 . 80  X  . 04 . 80  X  . 04 Breast 683 9 . 70  X  . 02 . 71  X  . 02 . 70  X  . 02 . 70  X  . 02 . 70  X  . 01 . 70  X  . 01 where M = 1  X  vec(  X  X   X  )  X   X  ( K matrix with zero-one entries constructed to select the term s [ M ] non-zero only in the ( i, j ) th coordinate of the ( i, j ) th m  X  m block. that of its gradient and shown to be Expanding each term, we obtain: and  X   X  ( K and [ N ] can be represented with the Frobenius inner product, For any  X   X  R p , let K the case where K condition of Proposition 4 can be rewritten as follows: Using the fact that V is diagonal, this inequality we can be further simplified A sufficient condition for this inequality to hold is that eac h term (4[ K or equivalently that 4 K 2 min i with 2GB of RAM. deviation) versus the number of bigrams (and kernels) used. 4.1 UCI Datasets norm-1-regularized and norm-2-regularized weighting usi ng in less than 25 steps, each requiring a fraction of a second (  X  0 . 05 seconds). where over-fitting is an issue. 4.2 Text Based Dataset when learning the kernel with norm-1 or norm-2 regularizati on using results were statistically significant. fewer number of iterations, typically less than 5. 4.3 Higher-order Polynomials coefficients of  X  are in the form of products of needed to be estimated.
 regularization with be advantageous is scenarios where overfitting is an issue. reported will further motivate such analyses. [9] C. Cortes, M. Mohri, and A. Rostamizadeh. L [10] C. Cortes and V. Vapnik. Support-Vector Networks. Machine Learning , 20(3), 1995. [16] C. Saunders, A. Gammerman, and V. Vovk. Ridge Regressio n Learning Algorithm in Dual [17] B. Sch  X olkopf and A. Smola. Learning with Kernels . MIT Press: Cambridge, MA, 2002. [22] V. N. Vapnik. Statistical Learning Theory . Wiley-Interscience, New York, 1998.
