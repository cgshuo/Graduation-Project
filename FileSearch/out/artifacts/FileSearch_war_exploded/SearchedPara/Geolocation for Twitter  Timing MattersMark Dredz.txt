 Geolocation  X  the task of identifying a social media message X  X  location  X  can support a variety of down-stream applications, such as advertising, personal-ization, event discovery, trend analysis and disease tracking (Watanabe et al., 2011; Hong et al., 2012; Kulshrestha et al., 2012; Broniatowski et al., 2013). Geolocation work has mostly focused on Twitter, since tweets are readily accessible and true loca-tion available from user geocoded tweets ( inter alia (Eisenstein et al., 2010; Han et al., 2014; Rout et al., 2013; Compton et al., 2014; Cha et al., 2015; Jur-gens et al., 2015; Osborne et al., 2014; Dredze et al., 2013)).

Most previous work consider the task of author geolocation, the identification of a author X  X  primary (home) location (Eisenstein et al., 2010; Han et al., 2014). Author geolocation systems rely on multiple tweets from each author to identify the location. In this work, we consider the task of tweet geolocation, where a system identifies the location where a single tweet was written (Osborne et al., 2014; Dredze et al., 2013). This approach is necessary when geolo-cation decisions must be made quickly, with limited resources, or when the location of a specific tweet is required.

When focusing on a single tweet, time becomes relevant. Intuitively, tweets written in the morning might be in different locations (at home) than say tweets written during the day (at work). This in-formation is often ignored but can provide impor-tant clues as to a tweet X  X  location. Likewise, mod-els built using historical data never adapt as time evolves. These factors may have a significant impact on geolocation accuracy, and downstream system X  X  should be sensitive to these variations.

For the first time, we consider the impact of time on Twitter geolocation and predict where a post was made (rather than the more usual, and easier task of author location). We take a supervised learning approach, training a multi-class classifier to identify the city of a tweet. We train a system on 250 million tweets sampled from a 45 month period, perhaps the largest evaluation to date. We find that:  X  Geolocation accuracy is cyclical, varying signif- X  While access to massive training data improves  X  Periodically updating geolocation models, even Our study is similar to that of Pavalanathan and Eisenstein (2015), who called into question the ac-curacy of geolocation models due to mismatches be-tween the behavior of users in available training data as compared to users encountered in live data. While our work provides a cautionary tale, it provides a guide for how these models can be used in practice. We start with every geocoded tweet (based on the  X  X ocation X  field) from January 1, 2012 to September associated with a specific location by Twitter (the  X  X ocation X  field is populated.)
We took several steps to remove tweets that were not relevant to the task. We removed tweets posted by location sharing services (FourSquare and jSwarm) since these are not written by users. We removed retweets for the same reason. We also remove tweets that do not have a specific lati-tude/longitude (geo) while nevertheless containing a location. Twitter allows user X  X  to tag a tweet with a location (populating the location field) even when the user X  X  device does not provide a latitude and lon-gitude (geo field). To ensure we know the precise location of the user we only consider tweets with the geo field.

We matched each tweet to a city using the proce-dure of Han et al. (2014), with 3,709 cities derived contained a tweet; locations without tweets were mostly in Africa and China, which has low Twit-ter usage. Following Han et al. (2014) we focus on English tweets only, removing non-English tweets based on the metadata language code. We also iden-tified the tweet X  X  country for a country prediction task (161 labels). We divided this dataset into two time periods. We use tweets from January 1, 2012 to March 30, 2015 for a standard train/dev/test eval-uation, selecting 2 and test sets. Data from March 31, 2015 to Septem-ber 30, 2015 forms an  X  X ut of time X  sample.

The most common cities were Los Angeles, Lon-don, Jakarta, Chicago, Kuala Lumpur and Dallas. The city clustering procedure of Han et al. (2014) greatly influences this list. For example, Los Ange-les ends up as one large city, whereas the New York City area is divided into several smaller cities. We treat geolocation as a multi-class task, with each city (or country) a label (Jurgens et al., 2015). Features All of our features are extracted from a single tweet (text or metadata) without requiring ad-Text : We extracted unigrams and bigrams from the text of each tweet after tokenizing with Twokenizer (O X  X onnor et al., 2010). We removed all punctua-tion, and replaced unique usernames and urls with placeholder tokens. Numbers were replaced with a
NUM token. Profile location : Unigrams and bi-grams extracted from the user supplied profile loca-tion field, as well as a feature for the entire loca-tion string. These fields often provide clues as to the user X  X  location, e.g.  X  X ew York Living X . Time-zone : Each tweet has a timezone that reflects a spe-cific location, e.g.  X  X acific Time (US &amp; Canada) X ,  X  X tlantic Time (Canada) X ,  X  X asablanca X . We also include the UTC offset of the timezone. Time : We use a feature indicating the hour of the day (in UTC time) at which the tweet was posted.
 Learning We used vowpal wabbit (version 8.1.1) (Agarwal et al., 2011), a linear classifier trained us-ing stochastic gradient descent with adaptive, indi-vidual learning rates (Duchi et al., 2011) that mini-mizes the hinge loss. We used feature hashing with a 31-bit feature space. We selected the best model and parameters based on initial tests using development data . All other parameters used default settings. We report the four evaluation metrics of Han et al. (2014): city accuracy (AccCi), country accu-racy (AccCo), accuracy within 161 km (100 miles) (Acc@161), and the median error in km (Median). Baselines We include two baselines: (1) the ma-jority predictor: always predicts the most popular label. (2) alias matching: we create a list of aliases for each of the 2983 cities from the genomes dataset, which includes the smaller cities clustered together by Han et al. (2014). We search each tweet and the user X  X  profile location for these aliases, assign-ing a tweet with a matched alias to the corresponding city; unmatched tweets are assigned the majority la-bel. When multiple cities match a tweet, we selected the correct one (if present) using oracle knowledge. About 90% of matches were in the profile. This strategy is similar to that of Dredze et al. (2013). Duplicates A tweet may be duplicated in our dataset, appearing in both training and held out data, or appearing multiple times in held out data. We de-fine duplicates as tweets with identical feature repre-sentations. We removed duplicates from dev and test splits, to ensure evaluation examples are unseen in training, yielding 22,966 dev and 23,240 test tweets. We begin by establishing the models X  performance with a large training set, as measured on held out evaluation data drawn from the same time period. Here we use a standard setting, where there is no online adaptation. We include results for city and country models trained with the tweet text features alone (content). These evaluations train with a sam-ple of 25,822,353 tweets, similar to previous large scale training for geolocation (Han et al., 2014).
Table 2 shows our model beating both baselines, with the additional features generally improving over content features alone. Interestingly, improve-ments from adding features appears to be additive: the final model X  X  accuracy is nearly the sum of the individual improvements from each feature set. On the non-deduped test dataset (25,941 tweets), the ac-curacy was higher (city: 0.2920, country: 0.8777) but the trends of adding features remain unchanged. Our time feature, which captures a temporal prior over locations, does not seem to help, providing only a small boost.
 We consider the impact of training data size in Figure 1, including a model trained on 258,222,490 tweets, an order of magnitude larger than Han et al. (2014), which improves accuracy by roughly 3%. This figure provides guidance on how much data is necessary to do well on this task.

To summarize: our approach yields tweet level geolocation accuracy similar to, or better than, state small datasets (tens of millions of training examples, which can be obtained from the Twitter streaming API), one can obtain a reasonable model. We now consider factors that influence geoloca-tion temporal accuracy using our largest city model (258M training tweets), which has an accuracy of 0.3302 on test data (0.3062 excluding duplicates). 6.1 Question 1: How do daily and weekly Twitter traffic varies over the course of a day and a week. User behavior may change at different times, and different locations are active at different times.
Figure 3 shows the number of tweets and test ge-olocation accuracy by the hour of the day (b) and day of the week (c). The day of the week has a minor impact on geolocation accuracy; the standard deviation of the 7 days is 2.7% of the total mean. Tweet volume has a negative correlation with accu-racy (  X  0 . 435 ), i.e. more tweets may be indicative of more people from different locations tweeting, which makes the task harder. Notably, Monday is significantly harder, with an accuracy of 1.5 standard deviations below the mean. However, the hour of the day has much more significant impact on accuracy; some times of the day are significantly easier and harder than the average. The standard deviation is 6.8% of the mean, and tweet volume is strongly neg-atively correlated with accuracy (  X  0 . 647 ). Geoloca-tion is easier during times when there are fewer loca-tions actively tweeting. This is most apparent during Figure 1: Varying training data size.
 the nighttime in the US, where there are much fewer tweets overall and many fewer active locations. In short, the accuracy of a geolocation system depends on when it is running. 6.2 Question 2: How do changes over time We now turn to our data sample taken after the train-ing data: a 10% sample of 49,307,720 tweets from strate the accuracy of a trained model deployed on new data over time.

Evaluating on these tweets (duplicates included), our model yields an accuracy of 0.2661, down from 0.3302, a 19% relative drop. Surprisingly, this isn X  X  a gradual change over time; the drop is quite rapid. The week immediately following the training period has an accuracy of 0.2884. Figure 4 shows the de-
What factors contribute to this rapid drop? We consider two: new users and reposted tweets. New Users One factor affecting geolocation perfor-mance might be new users joining, posting a few tweets and then no longer posting. In a sense, users have a temporal lifespan, after which information originating from them is of less predictive value. One measure of this is the number of users encoun-tered in the evaluation data, which have never been previously encountered, either in training or earlier in the evaluation data. Over the six month evaluation period, the number of new tweets from geocoded users per day increases , even as a percentage of all tweets (Figure 3(a)).

We remove all tweets in the evaluation period from users that we have previously encountered, ei-ther in training or earlier in evaluation data. Accu-racy drops to 0.1859, a 30% relative decrease from 0.2661, suggesting that the training data learns fea-tures specific to the users it observes. By compar-ison, the alias match baseline has an accuracy of 0.2113 on this data.

While trained models remain effective on users present in training, it has difficulty generalizing to new users. Far from a small percentage of the total, new users make up a significant number of tweets, at a rate that does not appear to be slowing. Reposted Tweets Users often repost content, which can include repeating simple message (e.g.  X  X eeling good! X ) or tweeting the same content to multiple users. Users are more likely to repost content shortly after it was first created, making the number of re-posts go down over time. For example, while 8% of test tweets from the same time period as training data are duplicates (they appear in the training data), only 3.8% of tweets in the six month evaluation pe-riod are duplicates.

How much of an impact do these reposts have on accuracy? For the test data from the same time pe-riod, we saw model performance drop from 0.3302 to 0.3062, a fairly large difference. By comparison, removing reposts in the the six month evaluation pe-riod drops accuracy from 0.2661 to 0.2541, a more modest change. Reposts help to inflate geolocation accuracy, and their decrease as time progresses from training removes this accuracy inflation. Our results so far are sobering: shortly after a static model is deployed performance degrades to a model using two orders of magnitude less training data (compare the drop in  X  6.2 with Figure 1). Increasing the amount of training data might be an option, but given our previous results on new users, etc., this is unlikely to be sufficient.

A simple method for addressing model degrada-tion over time is to continuously update the model over time using online learning on new data as it be-comes available. For example, we can continuously download a stream of (at least) 1% of geocoded tweets from the Twitter API to use as training for updating a deployed system. What is the impact on a system X  X  accuracy when it is updated on these geocoded tweets with SGD updates (  X  3)?
Figure 4 shows the performance of our system in an online setting (dashed black line). This model up-dates on every 100th example (1% of all geocoded tweets) encountered in the six-month evaluation pe-riod. When we update this previously trained static Figure 4: Accuracy over the six months following train-model, we see a quick recovery to accuracy levels that meet or exceed those on the test set from the same time period as training (horizontal line.)
Finally, we consider the case where a practitioner starts from scratch with no training data, but up-dates using just 1% of geocoded tweets. Can some-one with access to no prior training data build an effective model? Encouragingly, within 20 days the new model (solid blue line) catches the previ-ously trained static model (solid black line,  X  X x-isting model: no updates X ). This is an extremely promising result as it suggests that most practition-ers who do not have access to all geolocated data can produce geolocation prediction models that ap-proximate models trained using hundred of millions of examples. We have presented a tweet geolocation system that considers an order of magnitude more data than any prior work. Despite hundreds of millions of train-ing examples, the resulting system is sensitive to the time the tweet was authored. Additionally, accuracy suffers when deployed on data beyond the training period. We show that online updates can mitigate problems caused by concept drift. In short, sheer volume of data is not enough: geolocation models should adapt to new data. Encouragingly, starting from no training data and updating on just 1% of geocoded tweets, within 20 days we can recover a model that catches a static model previously trained on hundreds of millions of tweets.
 Acknowledgments We thank Bo Han and Tim Bald-
