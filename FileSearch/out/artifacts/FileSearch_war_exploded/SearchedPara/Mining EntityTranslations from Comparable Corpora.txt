 This paper addresses the problem of mining named entity trans-lations from comparable corpora, specifically, mining English and Chinese named entity translation. We first observe that existing ap-proaches use one or more of the following named entity similarity metrics: entity, entity context, and relationship. Inspired by this observation, in this paper, we propose a new holistic approach, by (1) combining all similarity types used and (2) additionally consid-ering relationship context similarity between pairs of named enti-ties, a missing quadrant in the taxonomy of similarity metrics. We abstract the named entity translation problem as the matching of two named entity graphs extracted from the comparable corpora. Specifically, named entity graphs are first constructed from compa-rable corpora to extract relationship between named entities. En-tity similarity and entity context similarity are then calculated from every pair of bilingual named entities. A reinforcing method is uti-lized to reflect relationship similarity and relationship context sim-ilarity between named entities. According to our experimental re-sults, our holistic graph-based approach significantly outperforms previous approaches.
 I.2.7 [ Natural Language Processing ]: Machine Translation, Text Analysis; H.2.8 [ Database Application ]: Data Mining Languages, Experimentation Named Entity Translation, Graph Mapping
Named entities (NE) normally refer to a range of concepts like people names, place names, organization names, product names, and so on. Large quantities of new named entities appear every day in newspapers, web sites, TV programs, and so on. However, their translations usually cannot be found in general purpose bilingual dictionaries. Automatic NE translation extraction is thus important to machine translation systems and cross-language information re-trieval applications.

For this NE translation extraction task, early researches focused on finding NE correspondence in parallel sentences (or parallel cor-pora) [12, 7, 3]. Though such NE translation extraction from par-allel corpora can achieve very high accuracy, obtaining large-scale parallel corpora is not a trivial task, especially for uncommon lan-guage pairs.

To avoid the high costs of obtaining parallel corpora, many re-searchers have switched to comparable corpora for building large scale bilingual lexicons. Comparable corpora refer to those texts that are not translations of each other but talk about the same or related topics. For instance, news articles published in different languages within the same period can be regarded as comparable corpora. The amount of comparable corpora is much larger than that of parallel corpora, and they are easier to obtain, which makes them a desirable source for large-scale bilingual lexicon building.
Existing w ork can be categorized, based on the types of similar-ity metrics used (Table 1):
More specifically , early work applied E , using either rule-based phonetic similarity [19] or statistics-based phonetic similarity [11, 6]. However, pure E -based methods did not achieve satisfactory results, as some translations do not have similar pronunciation and there are also NEs with ambiguous pronunciation. In contrast, [15, 4] proposed to extract word translations from comparable corpora, to use EC . More EC -based approaches were proposed [2, 22, 8] adopting different context representation.

More recently, holistic approaches [16, 21] combining multiple similarity metrics have been proposed and outperformed preceding work. First, [16] combines E and EC using a language model and a probabilistic transliteration model respectively to find Chinese-English translation pair. In contrast, [21] combines E and R . As most NE translations do not appear in a general purpose bilingual word dictionary, [21] adopts an iterative approach to gradually re-fine R .

In this paper, we complete the picture, by (1) combining E , EC , and R , and (2) additionally considering RC , which is a missing quadrant in the taxonomy. Like EC , RC , R elationship C ontext, considers the textual context similarity between an English NE pair and a Chinese NE pair, i.e. , surrounding text explaining the rela-tionship of the NE pair, as evidence for whether the two relation-ships are comparable. For example, Barack and Michelle described as couple in both corpora is a strong evidence.

Specifically, we first construct NE relationship graphs from com-parable corpora and initialize pairwise translation similarity be-tween bilingual NE pairs, using E and EC . We then iteratively re-inforce the overall translation similarity until convergence using R and RC . Our experimental results suggest that our new holistic ap-proach significantly outperforms previous state-of-art approaches.
To summarize, this paper has the following contributions: 1. We present a new named entity translation similarity met-2. We propose a new holistic English-Chinese NE translation 3. We perform various kinds of evaluations to present the effec-
The rest of this paper is organized as follows. Section 2 reviews existing work. Section 3 formally defines the problem and Sec-tion 4 then develops our framework. Section 5 reports experimental results and Section 6 concludes the paper.
As described earlier, existing work has focused on using entity context similarity ( EC ), entity similarity ( E ), and relationship sim-ilarity ( R ) for mining cross-language term translations from com-parable corpora.

Early work applies entity similarity ( E ), using either rule-based phonetic similarity [19] or statistics-based phonetic similarity [11, 6, 17]. However, pure E -based methods did not achieve satisfac-tory results, as some translations do not have similar pronunciation and there are also NEs with ambiguous pronunciation. Meanwhile, [15, 4, 2, 22, 8] extracted translation pairs with high entity context similarity ( EC ) using word co-occurrences. More recently, holis-tic approaches combining two similarity notions have been studied, using E and EC [16, 1], and E and R [21] for computing overall similarity respectively. In this paper, we implemented [16] and [21] as baseline approaches, which we denote as Shao and You respec-tively from this point on.

More specifically, Shao [16] combines E and EC . For com-puting E , Shao uses a probabilistic pronunciation model for ma-chine transliteration using expectation maximization (EM) algo-rithm. For computing EC , Shao uses a language model used in a typical information retrieval (IR) problem which considers each Chinese word as a query and each English word as a document using their context words. Each of these two models ranks En-glish translation candidate words for each Chinese word. After that, Shao examines the top M English translation candidate words in both ranked lists, finds English words that appear in top M positions in both ranked lists, and selects the English word with the highest average rank position as the translation of the Chinese word. On the other hand, You [21] combines E and R . For computing E , You computes edit distance (ED) between Chinese Pinyins and English strings as they are pronounced similarly. For R , You uses monolingual entity co-occurrences, crawled from entity search web sites. You develops an entity matching algorithm which initializes the translation similarity between named entities by using E and iteratively reinforces the translation similarity by using R . After the convergence of the reinforcement step, You extracts one-to-one matches maximizing the overall similarity by using greedy match-ing algorithm.

Our work generalizes these efforts by combining three notions ( E , EC , and R ) and by also considering relationship context simi-larity ( RC ), with the following distinctions from each. Compared to Shao , our approach has at least two advantages. First, NEs co-occur frequently, and co-occurrence information is an important factor to determine NE translation. However, NE trans-lation pairs rarely appear in a dictionary and thus are ignored in Shao . By iterative reinforcement in our approach, the correctly translated NEs can help us find the translations of their neighbors in the next iteration, which enables us to make use of most NEs and provide more accurate NE translations. Second, in Shao , the can-didate ranking of a given NE is independent of the rankings of other NEs. It is very likely that one target language NE is considered the translation of many source language NEs ( i.e. , one-to-many map-ping). However, in our scenario, we want to find the best one-to-one mapping. That is, if Chinese NE is mapped to an English NE, it will not be mapped to other English NEs.
 Compared to You , our approach has the following advantages: First, You gets named entities and their relationship similarity ( R ) directly from entity search web sites and thus cannot be easily ex-tended to other languages; Second, since the entity search web sites only provide the co-occurrence relationship between NEs, it does not contain common words but only the related NEs, and thus may not be sufficient for finding the right translation; Third, when map-ping NEs in the source and target graphs, You only considers rela-tionship similarity ( R ) and ignore the relationship context similar-ity ( RC ), which may result in incorrect mapping.

Meanwhile, some existing work focuses on Web corpus [10, 14, 9], of a larger scale with higher asymmetry than comparable corpora, as an alternative resource to extract NE translations with promising results. However, these approaches require either crawl-ing the corpus on a scale that cannot be easily hosted, or frequently issuing external queries to web search engines, both of which are costly.
This section formally defines the problem and introduces the no-tations.

We abstract the NE translation mining problem as the match-ing of two NE relationship graphs extracted from the comparable corpora. More formally, we construct two graphs G e = ( V and G c = ( V c ; E c ) from each English and Chinese corpus where V is a set of NEs (nodes) and E is a set of relationships (edges). Our goal is to populate a hard decision (or similarity) matrix R |
V | -by-| V c | matrix, where each R  X  ij denotes whether the two NEs e  X  V e and c j  X  V c are translations of each other, i.e. , 1 if they are and 0 otherwise. As an intermediate goal, we populate a soft decision matrix R where each R ij indicates the likelihood to be a translation.

Towards this goal of populating R , we holistically combine all the similarity notions used in the past X  E , EC ( Shao [16]), and R ( You [21]) X  together with the new notion of RC .

R 0 using E and EC : A soft decision matrix R can be initialized by using pairwise entity similarity ( E ), as done in You . However, it cannot represent translation pairs with little phonetic similarity, e.g. , Jackie Chan and  X  (pronounced Cheng Long). To compen-sate for this weakness, we also consider entity context similarity ( EC ) around the NEs, as done in Shao . For example, if words like  X  X ctor X  co-occur frequently with Jackie Chan in the English corpus, the Chinese word for  X  X ctor X  is highly likely to co-occur with in the Chinese corpus.

In order to consider E and EC , we first construct two | V |
V | translation similarity matrices, S E and S EC , for E and EC respectively. We then combine S E and S EC to build the initial soft decision matrix R 0 .

R  X  reflecting R and RC : You uses relationship similarity ( R ) between monolingual NE pairs in each corpus, which considers co-occurrences between NEs, to iteratively reinforce the soft decision matrix R . For instance, the Chinese translations of some NEs co-occurring frequently with  X  X ackie Chan X  in the English corpus, e.g. ,  X  X hris Tucker X , are also likely to co-occur frequently with the Chinese corpus. As a result, if many English named entities co-occur with  X  X ackie Chan X  and their Chinese translations with , we can give a high similarity score for the NE translation pair ( X  X ackie Chan X ,  X  ).

We take one step further to take advantage of the relationship context similarity ( RC ) between two monolingual NE pairs in the two corpora. In other words, the relationship between  X  X ackie Chan X  and  X  X hris Tucker X  can be represented by context words, co-occurring with these named entity pairs, such as  X  X ush Hour X . Similarly, in G , context words for the translations of  X  X ackie Chan X  and  X  X hris Tucker X  can be collected, e.g. , the Chinese translation for  X  X ush Hour X . The similarity of context words between two monolingual NE pairs in the two NE graphs, or RC , would give additional evi-dence that the right translation pairs have been found.
However, in order to consider R and RC , we must already know the translations of NEs, which makes this problem a chicken-and-egg dilemma. The nature of this problem therefore suggests an it-erative process. That is, as we identify highly likely NE translation pairs from R 0 , we can update these translated pairs to the named entity dictionary, such that, the remaining NEs can be updated to R 1 from this dictionary. From this update, we may identify more translation pairs, which we continue until pairwise similarity con-verges to R  X  .

This holistic approach complements the current state-of-the-art approaches, as summarized in Table 2.
 Table 2: The similarity metrics used by current state-of-art ap-proaches and our new holistic approach.

As described in the previous section, our goal is to populate a hard decision matrix R  X  , using both entity-based ( E and EC ) and relationship-based ( R and RC ) similarities. Our framework con-sists of the following steps: 1. Building NE graphs and initializing R 0 : building two NE 2. Reinforcing into R  X  : reinforcing the soft decision matrix 3. Populating R  X  from R : generating a binary hard decision
We discuss each of these steps in the following sections respec-tively. This section discusses how we build two NE relationship graphs, G e and G c , from each English and Chinese corpus, and how we generate the initial soft decision matrix R 0 by combining two entity-based similarity metrics, E and EC . To extract the NE relationship information from the English and Chinese corpora, we first build two NE relationship graphs, G ( V e ; E e ) and G c = ( V c ; E c ) , from each English and Chinese cor-pus where V is a set of NEs (nodes) and E is a set of relationships (edges) between NEs.

For mining V e from the English corpus, we use a CRF-based En-glish NE tagger. We then build an English relationship graph G regarding an NE that appears more than times in the English cor-pus as a node and connecting two nodes (NEs) if the correspond-ing NE pair co-occurs more than times in the English documents in the corpus. We also build a Chinese graph G c using the same approach after mining V c from the Chinese corpus. We segment the Chinese corpus and identify NEs using a Chinese word breaker based on [5]. In this paper, we set = 20 based on empirical observations.
We build the initial soft decision matrix R 0 by using two entity-based similarity metrics, E and EC . We first compute the entity similarity matrix S E and the entity context similarity matrix S We then combine S E and S EC to generate R 0 .
 For computing the entity similarity matrix S E between English NE e in V e and Chinese NE c in V c , we adopt the Edit-Distance (ED) between e and c used in You [21].
More specifically , we first convert c into its Pinyin form P Y Pinyin is the romanization representation of pronunciation of Chi-nese character. We then compute the string similarity between En-glish string e and Chinese Pinyin P Y c , as English strings and Chi-nese Pinyins are pronounced similarly. To compute the string sim-ilarity, we use the Edit-Distance defined in You . Using the Edit-Distance score, entity similarity between English NE e i and Chi-nese NE c j is computed as: where e i  X  V e , c j  X  V c , and Len ( e i ) and Len ( P Y length of e i and P Y c j respectively.

For computing the entity context similarity matrix S EC , we first define the entity context as the surrounding words w of the NE. Formally, let d = { w 1 ; w 2 ; :::; w n } be a given document which contains an English NE e in a corpus and l be the length of the entity context (or window size). When e is the i th word w document d , entity context word list is: In this paper, we set the length of the window size l = 24 , based on empirical observations.

For the given English NE e , we aggregate all the context win-dows of e by summing up the occurrence of each context word w in the English corpus, and build N e -by-N w co-occurrence matrix where N e is the number of English NEs and N w is the number of context words. We then define a context association vector CA of NE e , using the Log-Odds (LO) association measure, which was the winner among all the context association metrics reported in [13], including chi-square and log-likelihood. The LO association measure between NE e and a single context word w is defined as: where O ij are the cells of the 2  X  2 contingency matrix of e and w . That is, O 12 and O 21 stand for the sum of co-occurrence matrix elements containing either e or w exclusively ( i.e. , e but not w , or w but not e respectively), O 11 for both and O 22 for neither. CA can be similarly defined for Chinese NEs.

Now, we define the vector similarity between CA e and CA c which shows the entity context similarity ( EC ) of English NE e and Chinese NE c , as CA e and CA c contain the relation score between NE and its context words. For such a computation, we need to  X  X lign X  two vectors, such that scores in the same vector position correspond to the words of the same meaning, such as  X  X resident X  and  X  :  X . For this purpose, we use an English-Chinese word dictionary to align words with the same meaning.

Specifically, among entity context words of e and c , we stem the words and keep w e  X  CA e and w c  X  CA c , only if the context word pair ( w e ; w c ) appears as a translation in the dictionary. In other words, we prune out entity context words (1) if the English word does not appear in the dictionary or (2) its dictionary trans-lation does not appear in the Chinese context. We also prune out stopwords and numbers as they do not add much contextual infor-mation.

However, as the context word translation pair may not map one-to-one to each other, there can be a word with multiple translations. In this case, we divide the association score among all possible translations: if w e has two translations w (1) c and w (2) occurrences in the Chinese corpus as n and m respectively, we di-vide w e into w (1) e and w (2) e indicating the two different senses of the word, corresponding to w (1) c and w (2) c respectively. We then add new context association vector entries CA e ( w (1) e ) and CA CA e ( w e ) . For example, if CA e = { ( w a e ; 0 : 5) ; ( w w e has two Chinese translations ( w a 1 c and w a 2 c ) with their oc-currences in the Chinese corpus as 4 and 6 respectively, we re-move ( w a e ; 0 : 5) and add two new entries ( w a 1 e ; 0 : 5 ( w e ; 0 : 5 NE e is CA e = { ( w a 1 e ; 0 : 2) ; ( w a 2 e ; 0 : 3) ; ( w refine context association vectors for Chinese words.

Once all context association vectors have been refined, we com-pute the cosine similarity between context association vector of En-glish NE e i and context association vector of Chinese NE c where e i  X  V e and c j  X  V c .

Finally, we discuss how we combine S E and S EC . We first use min-max normalization for the values in S E and S EC into the same range of [0:1]. After that, we merge two similarity matrices using the following equation: where R ij is the initial similarity score ( R 0 ij ) between English NE e and Chinese NE c j . From the initial similarity matrix R 0 , we iteratively reinforce R ij of the nodes i  X  V e and j  X  V c from the matching similarities of their neighbor nodes u and v using relationship-based similarity metrics, R and RC .

The basic intuition is built on the assumption that two NEs with a strong relationship co-occur frequently in both corpora ( R ) with the similar context words ( RC ). In order to express this intuition, we formally define an iterative reinforcement model as follows. Let R ij denote the similarity of nodes i and j at t -th iteration: where B t ( i; j; ) is an ordered set which contains the best neighbor matching pairs between node i  X  X  neighbor u and node j  X  X  neighbor v , and S RC iu;jv is the RC similarity matrix element between edge ( i; u )  X  E e and edge ( j; v )  X  E c for the pair ( u; v ) listed in B ( i; j; ) . We will discuss how we populate B t ( i; j; ) later. is the parameter to control the weight of the RC similarity.
Our reinforce model is expressed as a linear combination of (a) relationship similarity and relationship context similarity ( S
RC iu;jv ) ) = 2 k and (b) initial similarity R 0 ij . is the coefficient for interpolating the two terms.

Similar to entity context similarity S EC , relationship context similarity S RC is computed as the cosine similarity between two context association vectors, CA ( e;e  X  ) and CA ( c;c  X  ) English NE pairs and all Chinese NE pairs respectively. The dif-ferences are that we generate context association vector CA based on the co-occurring context words from the documents where two NEs e and e  X  co-occur, and set the size of window l as the size of the given document. Similarly, we compute CA ( c;c  X  ) tify the cosine similarity. Algorithm 1 Find Common Neighbors ( R t , S RC , i , j , , ) 1: B t ( i; j; )  X  X  X  2: for all ( i; u )  X  E e and ( j; v )  X  E c do 4: end for 5: S  X  Sort ( SIM ) { S is the ordered set of elements in SIM } 6: for SIM iu;jv  X  S do 7: if SIM iu;jv &lt; then 8: return B t ( i; j; ) 9: end if 10: if both u and v are not matched yet then 11: B t ( i; j; )  X  B t ( i; j; )  X  X  ( u; v ) } 12: end if 13: end for 14: return B t ( i; j; )
In our reinforcement model, as evidence that English NE i is a translation of Chinese NE j , we consider the relationship similarity R between i and j . In R , we check the extent to which i and j share common neighbors using B t ( i; j; ) , which is an ordered set of the best matching translation pairs between neighbors of i and j , as similarly used in You [21] (Algorithm 1). More specifically, we consider the neighbor pair ( u; v ) with R t uv  X  ( S RC a matching pair, where u is a neighbor of i and v is a neighbor of j (Line 6-13). But, instead of keeping all such pairs in B we make sure to eliminate many-to-many matches, by not adding ( u ; v  X  ) , if u  X  or v  X  appears in the pair with a higher matching score (Line 10-12). When ( u; v ) k is the pair in B t ( i; j; ) with the k -th highest similarity score, we apply decaying coefficient 1 = 2 guarantee the convergence, as
Once R converges to R  X  , we use R  X  as a soft decision matrix, representing the likelihood of each bilingual NE pair being a trans-lation, to generate a binary hard decision matrix R  X  , showing the one-to-one correspondence that is most likely. Such a problem can be stated as the maximum weighted bipartite matching problem and has been studied in [20]. However, computing the optimal match-ing solution incurs high computational cost. We thus approximate this problem using a greedy matching algorithm (Algorithm 2). Algorithm 2 Greedy Matching Algorithm ( R , ) 1: S  X  Sor t ( R ) { S is the ordered set of element R ij 2: for R ij  X  S do 3: if both i and j are not matched yet then 4: if R ij  X  then 5: R  X  ij = 1 { i and j are matched} 6: else 7: R  X  ij = 0 { i and j are not matched} 8: end if 9: else 10: R  X  ij = 0 { i or j are already matched} 11: end if 12: end for 13: return R  X 
Algorithm 2 works as follows: (1) Elements in the soft decision matrix R are sorted by the decreasing order of the similarity score R ij (Line 1); (2) We check each element in S and set R  X  ij if neither i  X  V e nor j  X  V c was selected and the similarity score R ij is larger than the given threshold (Line 2-12), otherwise we set R  X  ij = 0 .
This section reports our experimental results to evaluate our pro-posed approaches. First, we report our experimental settings in Section 5.1. Second, we validate the effectiveness of our approaches over a real-life dataset in Section 5.2.
This section reports our experimental settings to evaluate our proposed approaches.
 We used the same dataset (but with different time period) as Shao [16] did. For the English corpus, we used the English Giga-word Corpus (LDC2009T13) published by the Xinhua News Agency which contains news articles from January 2008 to December 2008. For the Chinese corpus, we used the Chinese Gigaword Corpus (LDC2009T27) also published by Xinhua News Agency during the same period. The English corpus contains 100,746 news docu-ments and the Chinese corpus contains 88,029 news documents. As two corpora handle the news documents during the same pe-riod, they are regarded as comparable corpora in our experiments. From the English and Chinese corpora, we extracted person names by using NE taggers and built English and Chinese person name graphs independently as discussed in Section 4.

For the purpose of comparison, we implemented the following four existing approaches with ours:
We re-implemented You to use the same NE relationship graphs we extracted from the comparable corpora, and used the entity sim-ilarity ( E ) module of You for re-implementing Shao as well. In implementing EC , Shao , and our methods, we used an in-house English-Chinese general word dictionary containing more than 80,000 English-Chinese translation word pairs from Web resources such as online dictionaries to compute contextual similarities ( EC and RC ). As a result, this dictionary does not contain NE translation pairs and mainly consists of the pairs of general words.
For the main evaluation metrics, we used standard precision, re-call, and F1 scores: Figure 1: The translation examples of person NEs. The shaded cells indicate the correctly translated pairs.
We designed our experiments to follow the experiment setting used in most previous work ( Shao and You ) on name translation for fair comparison with them. Their settings aim to estimate a pre-cision of name translations rather than recall, for which we need to ensure that the gold standard set has (a) sufficient amount of named entities in Chinese and English corpora and (b) such enti-ties are not biased. To build ground truth on above conditions, we firstly selected 500 English person names randomly from all the person names extracted from the English corpus. Then, we hired an annotator to translate them into their Chinese translation names. These 500 English and Chinese named entity translation pairs were used for evaluation as ground truth. Among the 500 Chinese person name translations in the pairs, only 203 were matched to the person names extracted from our Chinese corpus. We used all 500 pairs to find the best parameters and computed the evaluation measures only using 203 matched pairs, as it is more suitable for real-life translation scenarios. This also follows Shao  X  X  [16] practice to make the results easy to compare.

In the evaluation, we optimized parameters as following: We used a fixed value of = 0 : 15 which is one of the robust parameter settings in [21]. We performed a 5-fold cross-validation by divid-ing 500 English-Chinese NE translation pairs into 5 groups (each group contains 100 NE translation pairs). We used four groups to optimize necessary threshold parameters of each method (for in-stance, in EC , E , You , and our two approaches, in our ap-proach, and M value in Shao ), and used the remaining group as a test dataset using optimized parameters. The optimal parameters were determined by the ones maximizing F1-scores.
This section reports our experimental results using the evalua-tion datasets. In particular, we evaluated (1) the effectiveness of the overall framework using precision, recall, and F1-score by com-paring our approaches with four existing approaches including two state-of-the art approaches [16, 21], (2) the effectiveness of our framework in terms of frequency of NEs, (3) the contribution of our reinforcement model, greedy matching algorithm, and param-eters in improving NE translation quality, (4) other types of NEs using precision, recall, and F1-score, and (5) the effect of fuzzy matching. (In all experimental results, Bold numbers indicate the best performance for each metric).
We first evaluated the effectiveness of the overall framework us-ing precision, recall, and F1-score by comparing our approaches with four existing approaches.
 Table 3: The average precision, recall, and F1-score of each ap-proach computed using 5-fold cross-validation. E + EC + R shows the best recall, and E + EC + R + RC shows the best precision and F1-score.

Table 3 presents our experimental results. Our proposed ap-proaches, E + EC + R and E + EC + R + RC , clearly outperform other approaches including both You and Shao . Specifically, E + EC + R shows the best recall, and E + EC + R + RC shows the best precision and F1-score. These performance improvements of ours over both You and Shao were statistically significant according to the Stu-dent X  X  t-test at P &lt; 0 : 01 level.
 More specifically, E + EC + R performs significantly better than You and Shao . This result indicates that considering entity con-text similarity ( EC ) complements [21] by supplying more evidence for name translation, while considering relationship similarity ( R ) complements [16], and the reinforcement framework can effec-tively integrate them for name translation. Lastly, considering re-lationship context similarity ( RC ) was effective in terms of preci-sion, but shows relatively lower recall compared to ours without RC ( i.e. , E + EC + R ). This can be explained by the sensitivity of RC to data sparsity X  When data are sparse, e.g. , for rare name rela-tionship pairs, the number of documents where such pairs co-occur is small and using RC may not be sufficiently precise to eliminate rare but relevant pairs ( i.e. , low recall). However, when data are sufficiently abundant, RC can be a very effective device for precise name translation identification.
 In addition, we present representative translation examples in Figure 1, where the shaded cells indicate the correctly translated pairs. We can observe that each notion of E , EC , R , and RC con-tributes to translation accuracy.
In this section, we evaluated the performances of all approaches for high frequency and low frequency NEs to understand how the frequency of an NE affects the performances. To build high fre-quency and low frequency NE sets, we divided our 500 person NE evaluation dataset into two equally sized subsets according to the frequencies of their English corpus parts (top 250 NEs and bottom 250 NEs). The average numbers of frequency of high frequent NE set and low frequent NE set in our experiment are 213.62 and 27 respectively.

Table 4 presents our experimental results. HF shows the results of high frequent NEs and LF shows the results of low frequent ones. Ratio shows the  X  X ecreasing ratio" on the two subsets, HF and LF . To compute this, we divide the decrease from the score from the high frequency NEs to low frequency ones by the score from the high frequency NEs. That is, the lower decreasing ratio is, the more rob ust the scheme is over varying NE frequencies. As the E approach does not use contextual similarity, its performance will not be affected by NE frequency. Considering this point, we also show the second lowest scores in bold .

As Table 4 shows, our approaches, E + EC + R and E + EC + R + RC , clearly outperform other approaches in both high frequent and low frequent NEs. The Ratio results also show that our approaches are relatively more robust over most frequencies. We evaluated the contribution of our reinforcement model using MRR [18], the average of the reciprocal ranks of the query results as the following formula: where q is a ground truth translation pair, Q is the set of ground truth translation pair, and rank q is the rank of the right transla-tion answer in the list of translation candidates returned by our ap-proach.

Specifically, we compared the MRR score of our framework us-ing initial (or baseline) similarity matrix ( R 0 ) with the score using reinforced similarity matrix ( R  X  ).
 Table 5: MRR score of baseline and reinforced matrices using E + EC + R + RC . Our reinforcement model effectively improves MRR scores in all cases.

Table 5 summarizes our experimental results. We observed an in-crease in MRR scores in all cases after applying our reinforcement model, which indirectly shows the effectiveness of our reinforce-ment model. Note, as the initial matrix R 0 is not affected by , the MRR scores of all baselines are identical.
We also evaluated the effectiveness of matching a soft decision matrix R into a hard decision matrix R  X  , using precision, recall, and F1-score. As matching translations using R can occur one-to-many assignment problem, jointly finding translations for all source NEs can avoid many errors that may occur in Shao .

Table 6 shows the evaluation results of E + EC + R + RC . Our holis-tic approach using R  X  shows consistently higher precision than R . The average number of returned translation pairs is 54 : 00 with R and 45 : 4 with R  X  . Among these pairs, the number of correctly re-turned pairs is 24 : 00 and 23 : 60 with R and R  X  respectively. These results indicate that R  X  can effectively filter some wrong transla-tion pairs X  on average, a 15 : 56% reduction of incorrect translation pairs.
 In overall, E + EC + R + RC using R  X  shows consistently higher F1-score which indicates our matching algorithm improves the trans-lation quality.
In this section, we study our performance over varying to see the effect of relationship context similarity ( RC ). We also varied for all values and found = 0 : 25 to be optimal for most values, suggesting the robustness of our framework.
 Table 7: Influence of the parameter of our approach E + EC + R + RC .

Table 7 presents our results. A higher tends to lead to higher precision but lower recall, while a lower leads to high recall and low precision. precision and F1-score, and R shows higher recall.

In o verall, when = 0 : 7 , E + EC + R + RC shows the best F1-score.
To check the extension possibility of our approaches, we com-pared the performances of all approaches using the location type of named entities in the same corpora. We first extracted location type of NEs by using the same NE taggers. We then randomly se-lected 100 English location NEs from the English corpus and found their Chinese translations manually. As a result, 86 of their Chinese translations were found from the Chinese corpus. We used same parameters used in the person NE translation.
 Table 8: The measure scores of all approaches computed using 100 location type of NEs. E + EC + R + RC outperforms all other approaches.

Table 8 presents our experimental results. Compared to the Ta-ble 3, EC , E , and Shao show high precision (more than 50% preci-sion improvement), while You shows the worst precision improve-ment. These results indicates that each translation similarity mea-sure has a different contribution for different types of NEs X  EC and E have big contribution for location type of NEs, and R has big contribution for person type of NEs.

As also shown in the Table 8, our proposed approaches, E + EC + R and E + EC + R + RC , clearly outperform other approaches. Specif-ically, E + EC + R + RC shows the best performance over all evalu-ation metrics. This result indicates that our approach, combining and optimizing many similarity metrics, is robust and effective in cross-type NE translation.
 Figure 2 shows the difference of F1-score of each approaches. Our two approaches clearly outperform other approaches in cross-type NE translation. Figure 2: The difference of F1-Score between person type of NE and location type of NE.
In this section, we study our performances using fuzzy matching not exact matching. Fuzzy matching is the matching considering partial name match as the right translation. For example, if our approach returns a translation pair (Barack Obama,  X  X  j ), the exact matching algorithm considers it as a wrong translation, as  X  j is Obama not Barack Obama, while fuzzy matching algorithm considers it as a right translation.

Table 9 shows the evaluation results. Compared to the Table 7, fuzzy matching results show higher precision, but lower recall. It is because both of the number of pairs that have right translation and the number of correctly returned pairs are increased, but the former is larger than the latter.
This paper studies the problem of mining named entity transla-tions from comparable corpora. Current state-of-the-art approaches are holistic, as they combine entity, entity context, and relationship similarity. In this paper, we propose a new holistic approach, by combining all similarity types used and additionally considering re-lationship context similarity between named entities. Specifically, named entity graphs are first constructed from comparable corpora. Entity and entity context similarities are then calculated from every Table 9: The performance of E + EC + R + RC using fuzzy match-ing. Fuzzy matching results show higher precision but lower recall.
 pair of bilingual named entities. A reinforcing method is utilized to reflect relationship and relationship context similarities between named entities. According to our experimental results, our new holistic graph-based approach significantly outperforms previous approaches and is extensible for various types of NEs.

We leave the followings as future work: This research was supported by Microsoft Research and the MKE (The Ministry of Knowledge Economy), Korea, under IT/SW Cre-ative research program supervised by the NIPA (National IT Indus-try Promotion Agency) (NIPA-2010-C1810-1002-0003). [1] A. Baron and M. Freedman. Who is Who and What is What: [2] M. Diab and S. Finch. A statistical word level translation [3] D. Feng, Y. L X , and M. Zhou. A New Approach for [4] P. Fung and L. Y. Yee. An IR approach for translating new [5] J. Gao, M. Li, and C.-N. Huang. Improved source-channel [6] L. Haizhou, Z. Min, and S. Jian. A joint source-channel [7] F. Huang, S. Vogel, and A. Waibel. Automatic extraction of [8] H. Ji. Mining name translations from comparable corpora by [9] L. Jiang, S. Yang, M. Zhou, X. Liu, and Q. Zhu. Mining [10] L. Jiang, M. Zhou, L.-F. Chien, and C. Niu. Named entity [11] K. Knight and J. Graehl. Machine transliteration. Comput. [12] J. Kupiec. An algorithm for finding noun phrase [13] A. Laroche and P. Langlais. Revisiting context-based [14] D. Lin, S. Zhao, B. V. Durme, and M. Pasca. Mining [15] R. Rapp. Automatic identification of word translations from [16] L. Shao and H. T. Ng. Mining new word translations from [17] P. Virga and S. Khudanpur. Transliteration of proper names [18] E. M. Voorhees. The TREC question answering track. Nat. [19] S. Wan and C. M. Verspoor. Automatic English-Chinese [20] D. B. West. Introduction to Graph Theory . Prentice Hall, [21] G.-w. You, S.-w. Hwang, Y.-I. Song, L. Jiang, and Z. Nie. [22] K. Yu and J. Tsujii. Bilingual dictionary extraction from
