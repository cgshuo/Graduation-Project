 J. Bergstra bergstra@rowland.harvard.edu Rowland Institute at Harvard 100 Edwin H. Land Boulevard Cambridge, MA 02142, USA D. Yamins yamins@mit.edu Department of Brain and Cognitive Sciences Massachusetts Institute of Technology Cambridge, MA 02139, USA D. D. Cox davidcox@fas.harvard.edu Rowland Institute at Harvard 100 Edwin H. Land Boulevard Cambridge, MA 02142, USA Many computer vision algorithms depend on hyper-parameter choices such as the size of filter bank, the strength of classifier regularization, and positions of quantization levels. These choices can have enormous impact on system performance: e.g. in (Pinto &amp; Cox, 2011), the authors extensively explored a single richly-parameterized model family, yielding classifica-tion performance that ranged from chance to state-of-the-art performance, depending solely on hyperparam-eter choices. This and other recent work show that the question of  X  X ow good is this model on that dataset? X  is ill-posed. Rather, it makes sense to speak of the quality of the best configuration that can typically be discovered by a particular search procedure in a given amount of time, for a task at hand. From this perspective, the tuning of hyperparameters is an im-portant part of understanding algorithm performance, and should be a formal and quantified part of model evaluation.
 On the other hand, ad hoc manual tuning by the al-gorithm inventor, while generally hard to reproduce or compare with fairly, can be efficient. A system X  X  designer has expectations for how his or her system should work, and he or she can quickly diagnose un-expected deviations.
 In this work we explore the possibility that manual op-timization is no longer efficient enough to justify the lack of formalization that it entails. Recent develop-ments in algorithm configuration raise the efficiency of automatic search, even in mathematically awkward search spaces, to a level where the result of hand-tuning can be matched and exceeded in a matter of hours on a small cluster of computers. Using these ideas, we implemented a broad class of feed-forward image feature extraction and classification models in order to formalize the steps of selecting the param-eters of a model, and evaluating that model on a task. We compared random search in that model class with a more sophisticated algorithm for hyperparam-eter optimization, and found that the optimization-based search strategy recovered or improved on the best known configurations for all three image classifi-cation tasks in our study. This success motivates us to suggest that questions regarding the utility of mod-eling ideas should generally be tested in this style. Automatic search is reproducible, and thus supports analysis that is impossible for human researchers to perform fairly (e.g.  X  X ow would you have tuned ap-proach Y if you had not already learned to optimize approach X ? X ) To support research in hyperparam-eter optimization, we provide our optimization algo-rithms and specification language for download as free open source software. This software replicates the re-sults presented in this work, and provides a foundation for general algorithm configuration in future work. Our work extends two veins of research with little his-torical overlap: feed-forward model architectures for computer vision, and techniques for algorithm config-uration.
 Feed-forward models in computer vision. There is a long tradition of basing computer vision systems on models of biological vision (Fukushima, 1980; Le-Cun et al., 1989; Riesenhuber &amp; Poggio, 1999; Lowe, 1999; Hinton et al., 2006; DiCarlo et al., 2012). Such efforts have arrived at a rough consensus model in which nonlinear image features are computed by a feed-forward neural network. Each layer of the net-work comprises a relatively standard set of transfor-mations, including: (i) dimensionality expansion (e.g. by convolution with a filter bank), (ii) dynamic-range reduction (e.g. by thresholding), (iii) spatial smooth-ing (e.g. pooling or soft-max), (iv) local competition (e.g. divisive normalization), and (v) dimensionality reduction (e.g. sub-sampling or PCA). Feature extrac-tion is usually followed by a simple classifier read-out trained on labeled data.
 Beyond this high-level consensus, however, many de-tails remain unresolved: which specific operations should be involved, in what order should they be ap-plied, how many layers should be used, what kinds of classifier(s) should be used, and how (if at all) should the filter values be learned from statistics of input data. Many competing modeling approaches can roughly be thought of as having made different design choices within a larger unformalized space of feed-forward algorithm configurations.
 Algorithm configuration. Algorithm configuration is a branch of optimization dealing with mathemati-cally difficult search spaces, comprising both discrete and continuous variables as well as conditional vari-ables that are only meaningful for some combinations of other variables. Bayesian optimization approaches have proved useful in these difficult domains (Mockus et al., 1978). A Bayesian optimization approach cen-ters on a probability model for P (score | configuration) that is obtained by updating a prior from a history H of (configuration, score) pairs. This model can be queried more quickly than the original system in order to find promising candidates. Search efficiency comes from only evaluating these most promising can-didates on the original system. Gaussian processes (Rasmussen &amp; Williams, 2006) have often been used as the probability model, but other regression models such as decision trees have also proved successful (Hut-ter, 2009; Brochu, 2010; Bardenet &amp; K  X egl, 2010; Hutter et al., 2011; Bergstra et al., 2012). In these approaches, the criterion of Expected Improvement (EI) beyond a threshold  X  is a popular heuristic for making propos-als (Jones, 2001). In that approach, the optimization algorithm repeatedly suggests a configuration c that optimizes EI ( c ) = R y&lt; X  yP ( y | c,H ) while the experi-mental history of (score, configuration) pairs, H , ac-cumulates and changes the model. Recently Bergstra et al. (2011) suggested an approach to Bayesian opti-mization based on a model of P ( c | y ) instead. Under some assumptions this approach can also be seen to optimize EI.
 Hyperparameter optimization in computer vision is typically carried out by hand, by grid search, or by ran-dom search. We conjecture that Bayesian optimization is not typically used because it is relatively new tech-nology, and because it requires a layer of abstraction between the researcher toying with settings at a com-mand prompt and the system being optimized. We show that although algorithm configuration is a young discipline, it already provides useful techniques for for-malizing the difficult task of simultaneous optimiza-tion of many hyperparameters. One of the contribu-tions of our work is to show how useful Bayesian op-timization can be, even when optimizing hundreds of hyper-parameters. Our approach to hyperparameter optimization has four conceptual components: 1. Null distribution specification language. We propose an expression language for specifying the hy-perparameters of a search space. This language de-scribes the distributions that would be used for ran-dom, unoptimized search of the configuration space, and encodes the bounds and legal values for any other search procedure. A null prior distribution for a search problem is an expression G written in this specifica-tion language, from which sample configurations can be drawn.
 For example: specifies a joint distribution in which a is distributed normally with mean 0 and variance 1, and b takes ei-ther value 0, or a , or a value drawn uniformly between 2 and 10. There are three hyperparameters at play here, shown in bold: the value of a , the value of the choice, and the value of the uniform.
 More generally, the expressions that make up the null distribution specification can be arbitrarily nested, composed into sequences, passed as arguments to de-terministic functions, and referenced internally, to form an directed acyclic expression graph (DAG). 2. Loss Function. The loss function is the criterion we desire to minimize. It maps legal configurations sampled from G to a real value. For example, the loss functions could extract features from a particular image dataset using configuration parameters speci-fied by the random sample from G , and then report mis-classification accuracy for those features. Typi-cally the loss function will be intractable analytically and slow enough to compute that doing so imposes a meaningful cost on the experimenter X  X  time. 3. Hyperparameter Optimization algorithm (HOA). The HOA is an algorithm which takes as in-puts the null prior expression G and an experimen-tal history H of values of the loss function, and re-turns suggestions for which configuration to try next. Random sampling from the prior distribution specifi-cation G is a perfectly valid HOA. More sophisticated HOAs will generally commandeer the random nodes within the null prior expression graph, replacing them with expressions that use the experimental history in a nontrivial way (e.g. by replacing a uniform node with a Gaussian mixture whose number of compo-nents, means, and variances are refined over the course of the experiment). 4. Database. Our approach relies on a database to store the experimental history H of configurations that have been tried, and the value of the loss function at each one. As a search progresses, the database grows, and the HOA explores different areas of the search space.
 The stochastic choice node, which randomly chooses an argument from a list of possibilities, is an impor-tant aspect of our approach. Choice nodes make it possible to encode conditional parameters in a search space (Hutter, 2009). To continue the example above, if the choice node is evaluated such that b takes the value of a , then our parameterization of G allows the optimizer to infer that whatever score we obtain has nothing to do with the hyperparameter associated with uniform (2 , 10). Visual system models have many configurable components, and entire components can be omitted from a particular pipeline configuration, so it is natural to describe the parameters of an op-tional component using conditional parameters. The use of conditional parameters makes credit assignment among a set of hyperparameters more efficient. Our implementation of these four components is avail-able for download as both a general purpose tool for program optimization and a specific visual system model for new image classification data sets (Bergstra, 2013; Bergstra et al., 2013).
 We evaluate the viability of automatic parameter search by encoding a broad class of feed-forward classi-fication models in terms of the null distribution spec-ification language described in the previous section. This space is a combination of the work of Coates &amp; Ng (2011) and Pinto et al. (2009), and is known to con-tain parameter settings that achieve the state of the art performance on three data sets (i.e, loss functions): LFW, Pubfig83, and CIFAR-10.
 The full model family that we explore is illustrated in Figure 1. Like Coates &amp; Ng (2011), we include ZCA-based filter-generation algorithms (Hyv  X arinen &amp; Oja, 2000) and coarse histogram features (described in their work as the R-T and RP-T algorithms). Like Pinto et al. (2009), we allow for 2-layer and 3-layer sequences of filtering and non-linear spatial pooling. Our search space is configured by a total of 238 hyper-parameters  X  far too large for brute force search, and an order of magnitude larger in dimensionality than the 32-dimensional space searched by Bergstra et al. (2011). The remainder of this section describes the components of our model family. An implementation of the model is available for download (Bergstra et al., 2013).
 The inter-layers (Figure 1a) perform a filter bank nor-malized cross-correlation, spatial pooling, and possi-bly sub-sampling. These layers are very much in the spirit of the elements of the Pinto &amp; Cox (2011) model, except that we have combined the normalization and filter bank cross-correlation into a single mathematical operation (fbncc, Equations 1-2). y = fbncc( x,f ) (1) y The fbncc operation is a filter bank convolution of each filter f k with a multi-channel image or feature map x , in which each patch  X  x ij of x is first shifted by its mean  X  m (motivating  X  u ij . =  X  x ij  X   X  m ) then scaled to have ap-proximately unit norm. Whereas Pinto &amp; Cox (2011) employed only random uniform filters f k , we include also some of the filter-generation strategies employed in Coates &amp; Ng (2011): namely random projections of ZCA components, and randomly chosen ZCA-filtered image patches. Filter-generation is parametrized by a filter count K  X  [16 , 256]), a filter size S f  X  [2 , 10], a random seed, and a band-pass parameter in the case of ZCA. The pair-indexed hat-notation  X  x ij refers to a patch volume from x at row i and column j that includes S f rows and columns as well as all channels of x ; Our fbncc implementation is controlled by log-normally distributed hyperparameter  X  which defines a low-variance cutoff, a binary-valued hyperparame-ter  X  that determines whether that cutoff is soft or hard, and a binary-valued parameter that determines whether the empirically-defined patch mean  X  m should be subtracted off or not.
 Local spatial pooling (lpool, Equation 3) was imple-mented as in Pinto &amp; Cox (2011). The operation is parameterized by a patch size S p  X  [2 , 8], a sub-sampling stride i 0 /i = j 0 /j  X  { 1 , 2 } , and a log-normally distributed norm parameter p . The triple-indexed  X  x ijk refers to a single-channel patch sur-face from x at row i , column j , and channel k that extends spatially to include S p rows and columns. The outer-layers (Figure 1b) combine the fbncc op-eration of inter-layers with different pooling options. Rather than sampling or optimizing the filter count, the filter count is determined analytically so that the number of image features approaches but does not ex-ceed sixteen thousand (16,000). Pooling is done either (1) with lpool and lnorm (Equation 4) as in Pinto &amp; Cox (2011), or (2) with spatial summation of posi-tive and negative half-rectified filter bank responses (dihist, Equation 5). Within pooling strategy (2) we used two strategies to define the spatial patches used in the summation: either (2a) grid cell summation as in Coates &amp; Ng (2011), or (2b) box filtering. The dif-ference between (2a) and (2b) is a trade-off between spatial resolution and depth of filter bank in making up the output feature set. y = lnorm( x )  X  y ij = y = dihist( x )  X  y ijk = Hyperparameter  X  of the lnorm operation was log-normally distributed, as was the  X  hyperparameter of dihist. In approach (2a) we allowed 2x2 or 3x3 grids. In approach (2b) we allowed for sub-sampling by 1, 2, or 3 and square summation regions of side-length 2 to 8.
 The last step in our image-processing pipeline is a clas-sifier , for which we used an ` 2 -regularized, linear, L2-SVM. For the smaller training sets we used liblinear via sklearn as the solver(Fan et al., 2008; Pedregosa et al., 2011), for larger ones we used a generic L-BFGS algorithm in the primal domain (Bergstra et al., 2010). Training data were column-normalized. The classi-fier components had just two hyperparameters: the (b) strength of regularization and a cutoff for low-variance feature columns. Code for these experiments was writ-ten in Python, with the feature extraction carried out by Theano (Bergstra et al., 2010) and hyperparam-eter optimization carried out by the hyperopt pack-age (Bergstra, 2013). We evaluate the technique of automatic hyperparam-eter configuration by comparing two hyperparameter optimization algorithms: random search versus a Tree of Parzen Estimators (TPE) (Bergstra et al., 2011). The TPE algorithm is an HOA that acts by replac-ing stochastic nodes in the null description language with ratios of Gaussian Mixture Models (GMM). On each iteration, for each hyperparameter, TPE fits one GMM ` ( x ) to the set of hyperparameter values associ-ated with the smallest (best) loss function values, and another GMM g ( x ) to the remaining hyperparameter values. It chooses the hyperparameter value x that maximizes the ratio ` ( x ) /g ( x ). Relative to Bergstra et al. (2011) we made two minor modifications to the search algorithm. The first modification was to down-weight trials as they age so that old results do not count for as much as more recent ones. We gave full weight to the most recent 25 trials and applied a lin-ear ramp from 0 to 1.0 to older trials. This is a heuris-tic concession to TPE X  X  assumption of hyperparameter independence: as our search moves through space, we use temporal distance within the experiment as a sur-rogate for distance in the search space. The second modification was to vary the fraction of trials used to estimate ` ( x ) and g ( x ) with time. Out of T observa-tions of any given variable, we used the top-performing  X  T/ 4 trials to estimate the density of ` . We initialized TPE with 50 trials drawn from the null configuration description. These hyper-hyperparameters were cho-sen manually by observing the shape of optimization trajectories on LFW view 1. We did not reconfigure TPE for the other data sets. The TPE algorithm took up to one or two seconds to suggest new points, so it contributed a negligible computational cost to these experiments. 5.1. TPE vs. Random Search: LFW and Random search in a large space of biologically-inspired models has been shown to be an effective approach to face verification (Pinto &amp; Cox, 2011) and identification (Pinto et al., 2011). Our search space is similar to the one used in those works, so LFW (Huang et al., 2007) and PubFig83 (Pinto et al., 2011) provide fair playing fields for comparing TPE with random search.
 For experiments on LFW, we follow Pinto &amp; Cox (2011) in using the aligned image set, and resizing the gray scale images to 200  X  200. We followed the offi-cial evaluation protocol  X  performing model selection on the basis of one thousand images from  X  X iew 1 X  and testing by re-training the classifier on 10  X  X iew 2 X  splits of six thousand pairs. We transformed im-age features into features of image pairs by apply-ing an element-by-element comparison function to the left-image and right-image feature vectors. Following Pinto &amp; Cox (2011) we used one comparison function for model selection (square root of absolute difference) and we concatenated four comparison functions for the final  X  X iew 2 X  model evaluation (product, absolute dif-ference, squared difference, square root of absolute dif-ference).
 The PubFig83 data set contains 8300 color images of size 100  X  100, with 100 pictures of each of 83 celebri-ties (Pinto et al., 2011). For our PubFig83 exper-iments we converted the un-aligned images to gray scale and screened models on the 83-way identification task using 3 splits of 20 train/20 validation examples per class, running two simultaneous TPE optimization processes for a total of 1200 model evaluations. Top-scoring configurations on the screening task were then tested in a second phase, consisting of five training splits of 90 train/10 test images per class. Each of the five second phase training sets of 90 images per class consisted of the 40 images from the first phase and 50 of the 60 remaining images.
 The results of our model search on LFW are shown in Figure 2. The TPE algorithm exceeded the best ran-dom search view 1 performance within 200 trials, for both our random search and that carried out in Pinto &amp; Cox (2011). TPE converged within 1000 trials to an error rate of 16.2%, significantly lower than the best configuration found by random search (21.9%). On LFW X  X  test data (view 2) the optimal TPE configu-ration also beats those found by our random search (84.5% vs. 79.2% accurate). The best configuration found by random search in Pinto &amp; Cox (2011) does well on View 2 relative to View 1 (84.1% vs. approx-imately 79.5%) and is approximately as accurate as TPE X  X  best configuration on the test set. On Pub-Fig83, the optimal TPE configuration outperforms the best random configuration found by our random search (86.5% vs 81.0% accurate) and the previous state of the art result (85.2%) Pinto et al. (2011). 5.2. Matching Hand-Tuning: CIFAR-10 Coates &amp; Ng (2011) showed that single-layer ap-proaches are competitive with the best multi-layer al-ternatives for 10-way object classification using the CIFAR-10 data set (Krizhevsky, 2009). The success of their single-layer approaches depends critically on correct settings for several hyperparameters governing pre-processing and feature extraction. CIFAR-10 im-ages are low-resolution color images (32  X  32) but there are fifty thousand labeled images for training and ten thousand for testing. We performed model selection on the basis of a single random, stratified subset of ten thousand training examples.
 The results of TPE and random search are reported in Figure 3. TPE, starting from broad priors over a wide variety of processing pipelines, was able to match the performance of a skilled and motivated domain expert. With regards to the wall time of the auto-matic approach, our implementation of the pipeline was designed for GPU execution and the loss function required from 0 to 30 minutes. TPE found a config-uration very similar to the one found by in Coates &amp; Ng (2011) within roughly 24 hours of processing on 6 GPUs. Random search was not able to approach the same level of performance. In this work, we have described a conceptual frame-work to support automated hyperparameter optimiza-tion, and demonstrated that it can be used to quickly recover state-of-the-art results on several unrelated im-age classification tasks from a large family of computer vision models, with no manual intervention. On each of three datasets used in our study we compared ran-dom search to a more sophisticated alternative: TPE. A priori, random search confers some advantages: it is trivially parallel, it is simpler to implement, and the independence of trials supports more interesting analysis (Bergstra &amp; Bengio, 2012). However, our ex-periments found that TPE clearly outstrips random search in terms of optimization efficiency. TPE found best known configurations for each data set, and did so in only a small fraction of the time we allocated to random search. TPE, but not random search, was found to match the performance of manual tuning on the CIFAR-10 data set. With regards to the compu-tational overhead of search, TPE took no more than a second or two to suggest new hyperparameter assign-ments, so it added a negligible computational cost to the experiments overall.
 This work opens many avenues for future work. One direction is to enlarge the model class to include a greater variety of components, and configuration strategies for those components. Many filter-learning and feature extraction techniques have been proposed in the literature beyond the core implemented in our experiment code base. Another direction is to improve the search algorithms. The TPE algorithm is conspic-uously deficient in optimizing each hyperparameter in-dependently of the others. It is almost certainly the case that the optimal values of some hyperparame-ters depend on settings of others. Algorithms such as SMAC (Hutter et al., 2011) that can represent such interactions might be significantly more effective op-timizers than TPE. It might be possible to extend TPE to profitably employ non-factorial joint densi-ties P (config | score). Relatedly, such optimization al-gorithms might permit the model description language to include constraints in the form of distributional pa-rameters that are themselves optimizable quantities (e.g. uniform (0 , lognormal (0 , 1))). Another impor-tant direction for research in algorithm configuration is a recognition that not all loss function evaluations are equally expensive in terms of various limited re-sources, most notably in terms of computation time. All else being equal, configurations that are cheaper to evaluate should be favored (Snoek et al., 2012). Our experiments dealt with the optimization of clas-sification accuracy, but our approach extends quite naturally to the optimization (and constrained opti-mization via barrier techniques) of any real-valued cri-terion. We could search instead for the smallest or fastest model that meets a certain level of classifica-tion performance, or the best-performing model that meets the resource constraints imposed by a particular mobile platform. Having to perform such searches by hand may be daunting, but when the search space is encoded as a searchable model class, automatic opti-mization methods can be brought to bear. This work was funded by the Rowland Institute of Harvard, and the National Science Foundation (IIS 0963668).
 Bardenet, R. and K  X egl, B. Surrogating the surro-gate: accelerating Gaussian Process optimization with mixtures. In ICML , 2010.
 Bergstra, J. Hyperopt: Distributed asyn-chronous hyperparameter optimization in Python. http://jaberg.github.com/hyperopt, 2013.
 Bergstra, J. and Bengio, Y. Random search for hyper-parameter optimization. Journal of Machine Learn-ing Research , 13:281 X 305, 2012.
 Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P.,
Pascanu, R., Desjardins, G., Turian, J., and Ben-gio, Y. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy) , June 2010.
 Bergstra, J., Bardenet, R., Bengio, Y., and K  X egl, B. Algorithms for hyper-parameter optimization. In NIPS*24 , pp. 2546 X 2554, 2011.
 Bergstra, J., Pinto, N., and Cox, D. D. Machine learn-ing for predictive auto-tuning with boosted regres-sion trees. In INPAR , 2012.
 Bergstra, J., Yamins, D., and Pinto, N. Hyperpa-rameter optimization for convolutional vision ar-chitectures. https://github.com/jaberg/hyperopt-convnet, 2013.
 Brochu, E. Interactive Bayesian Optimization: Learn-ing Parameters for Graphics and Animation . PhD thesis, University of British Columbia, December 2010.
 Coates, A. and Ng, A. Y. The importance of encod-ing versus training with sparse coding and vector quantization. In Proc. ICML-28 , 2011.
 DiCarlo, J. J., Zoccolan, D., and Rust, N. C. How does the brain solve visual object recognition? Neuron , 73:415 X 34, 2012 Feb 9 2012. ISSN 1097-4199.
 Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-
R., , and Lin, C.-J. Liblinear: A library for large linear classification. Journal of Machine Learning Research , 9:1871 X 1874, 2008.
 Fukushima, K. Neocognitron: A self-organizing neural network model for a mechanism of pattern recogni-tion unaffected by shift in position. Biological Cy-bernetics , 36(4):193 X 202, 1980.
 Hinton, G. E., Osindero, S., and Teh, Y. A fast learn-ing algorithm for deep belief nets. Neural Computa-tion , 18:1527 X 1554, 2006.
 Huang, G. B., Ramesh, M., Berg, T., and Learned-
Miller, E. Labeled faces in the wild: A database for studying face recognition in unconstrained en-vironments. Technical Report 07-49, University of Massachusetts, Amherst, October 2007.
 Hutter, F. Automated Configuration of Algorithms for Solving Hard Computational Problems . PhD thesis, University of British Columbia, 2009.
 Hutter, F., Hoos, H., and Leyton-Brown, K. Sequen-tial model-based optimization for general algorithm configuration. In LION-5 , 2011. Extended version as UBC Tech report TR-2010-10.
 Hyv  X arinen, A. and Oja, E. Independent component analysis: Algorithms and applications. Neural Net-works , 13(4 X 5):411 X 430, 2000.
 Jones, D.R. A taxonomy of global optimization meth-ods based on response surfaces. Journal of Global Optimization , 21:345 X 383, 2001.
 Krizhevsky, A. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
 LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D.
Backpropagation applied to handwritten zip code recognition. Neural Computation , 1(4):541 X 551, 1989.
 Lowe, D. G. Object recognition from lo-cal scale-invariant features. In Proceedings of the International Conference on Computer Vi-sion 2 (ICCV) , pp. 1150 X 1157, 1999. doi: 10.1109/ICCV.1999.790410.
 Mockus, J., Tiesis, V., and Zilinskas, A. The applica-tion of Bayesian methods for seeking the extremum. In Dixon, L.C.W. and Szego, G.P. (eds.), Towards Global Optimization , volume 2, pp. 117 X 129. North Holland, New York, 1978.
 Pedregosa, F., Varoquaux, G., Gramfort, A., Michel,
V., Thirion, B., Grisel, O., Blondel, M., Pretten-hofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot,
M., and Duchesnay, E. Scikit-learn: Machine Learn-ing in Python. Journal of Machine Learning Re-search , 12:2825 X 2830, 2011.
 Pinto, N. and Cox, D. D. Beyond simple features: A large-scale feature search approach to unconstrained face recognition. In Proc. Face and Gesture Recog-nition , 2011.
 Pinto, N., Doukhan, D., DiCarlo, J. J., and Cox, D. D.
A high-throughput screening approach to discover-ing good forms of biologically inspired visual repre-sentation. PLoS Comput Biol , 5(11):e1000579, 11 2009.
 Pinto, N., Stone, Z., Zickler, T., and Cox, D. D. Scaling-up Biologically-Inspired Computer Vision:
A Case-Study on Facebook. In IEEE Computer Vi-sion and Pattern Recognition, Workshop on Biolog-ically Consistent Vision , 2011.
 Rasmussen, C. E. and Williams, C. K. I. Gaussian Processes for Machine Learning . MIT Press, 2006. Riesenhuber, M. and Poggio, T. Hierarchical models of object recognition in cortex. Nature Neuroscience , 2:1019 X 1025, 1999.
 Snoek, J., Larochelle, H., and Adams, R. P. Practi-cal bayesian optimization of machine learning algo-rithms. In Neural Information Processing Systems ,
