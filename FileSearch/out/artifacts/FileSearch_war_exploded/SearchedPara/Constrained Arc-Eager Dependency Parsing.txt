 Uppsala University Bar-Ilan University Google and have linear time complexity with greedy decoding or beam search. We show how such preserve the linear time complexity of the parser. 1. Introduction
Data-driven dependency parsers in general achieve high par sing accuracy without re-lying on hard constraints to rule out (or prescribe) certain syntactic structures (Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald , Crammer, and Pereira 2005; Zhang and Clark 2008; Koo and Collins 2010). Neverthel ess, there are situations where additional information sources, not available at the time of training the parser, may be used to derive hard constraints at parsing time. For ex ample, Figure 1 shows the parse of a greedy arc-eager dependency parser trained on the Wall Street Journal section of the Penn Treebank before (left) and after (right) being constrained to build a single subtree over the span corresponding to the named enti ty  X  X at on a Hot Tin Roof, X  which does not occur in the training set but can easily be foun d in on-line databases. In this case, adding the span constraint fixes both preposition al phrase attachment errors.
Similar constraints can also be derived from dates, times, o r other measurements that can often be identified with high precision using regular exp ressions (Karttunen et al. 1996), but are under-represented in treebanks. parsers based on the arc-eager transition system (Nivre 200 3, 2008), which perform a single left-to-right pass over the input, eagerly adding de pendency arcs at the earliest possible opportunity, resulting in linear time parsing. We consider two types of con-subtree over one or more (non-overlapping) spans of the inpu t; arc constraints instead require specific arcs to be present in the output dependency g raph. The main contri-bution of the article is to show that both span and arc constra ints can be implemented as efficiently computed preconditions on parser transition s, thus maintaining the linear runtime complexity of the parser. 1 cause the phenomena we wish to integrate as hard constraints are by definition not available in the parser X  X  training and test data. Moreover, adding hard constraints may be desirable even if it does not improve parsing accuracy. Fo r example, many organi-zations have domain-specific gazetteers and want the parser output to be consistent with these even if the output disagrees with gold treebank an notations, sometimes because of expectations of downstream modules in a pipeline . In this article, we con-centrate on the theoretical side of constrained parsing, bu t we nevertheless provide some experimental evidence illustrating how hard constrai nts can improve parsing accuracy. 2. Preliminaries and Notation
Dependency Graphs. Given a set L of dependency labels, we define a dependency graph for a sentence x = w 1 , . . . , w n as a labeled directed graph G = ( V word w i in the sentence, and a set of labeled arcs A  X  V represents a dependency with head w i , dependent w j , and label l . We assume that the final word w n is always a dummy word R OOT and that the corresponding node n is a designated root node.
 ( V (i) G [ i , j ] contains all nodes corresponding to words between w is a directed tree, and (iii) it holds for every arc ( i , l , j )  X  G 250 two constraints on a dependency graph G for a sentence x : It is clear from the definitions that every PDT is also a Every PDG can be created by starting with a PDT and removing some arcs.

Arc-Eager Transition-Based Parsing. In the arc-eager transition system of Nivre (2003), a the nodes V x of some sentence x , and A is a set of dependency arcs over V label set L ). Following Ballesteros and Nivre (2013), we take the initi al configuration root node, and we take a terminal configuration to be any config uration of the form as the buffer , and we will use the variables  X  and  X  for arbitrary sublists of  X  and B , respectively. For reasons of perspicuity, we will write  X  with its head (top) to the right top of the stack  X  and the node j as the first node in the buffer B .
 defined formally in Figure 2 (disregarding for now the Added P reconditions column):
A transition sequence for a sentence x is a sequence C 0, m rations, such that c 0 is the initial configuration c s ( x ), c there is a legal transition t such that c i = t ( c i graph derived by C 0, m is G c
Complexity and Correctness. For a sentence of length n , the number of transitions in the arc-eager system is bounded by 2 n (Nivre 2008). This means that a parser using greedy inference (or constant width beam search) will run in O ( n ) time provided that transitions plus required precondition checks can be perfo rmed in O (1) time. This holds for the arc-eager system and, as we will demonstrate, its con strained variants as well. of
PDT s (Nivre 2008). For a specific sentence x = w 1 , . . . , w tion sequence for x produces a PDT (soundness), and that any some transition sequence (completeness). 2 In constrained parsing, we want to restrict the system so that, when applied to a sentence x , it is sound and complete for the subset of
PDT s that satisfy all constraints. 3. Parsing with Arc Constraints is required to be included in the parser output. Because the a rc-eager system can only derive PDT s, the arc constraint set has to be such that the constraint graph G can be extended to a PDT , which is equivalent to requiring that G task of arc-constrained parsing can be defined as the task of d eriving a 252 that G C  X  G . An arc-constrained transition system is sound if it only de rives proper extensions of the constraint graph and complete if it derive s all such extensions. Added Preconditions. We know that the unconstrained arc-eager system can derive a ny
PDT for the input sentence x , which means that any arc in V from the initial configuration, including any arc in the arc c onstraint set A order to make the parser respect the arc constraints, we only need to add preconditions that block transitions that would make an arc in A C unreachable. through the following preconditions, defined formally in Fi gure 2 under the heading A
RC C ONSTRAINTS for each transition:
Complexity and Correctness. Because the transitions remain the same, the arc-constrain ed parser will terminate after at most 2 n transitions, just like the unconstrained system.
However, in order to guarantee termination, we must also sho w that at least one transition is applicable in every non-terminal configurati on. This is trivial in the un-constrained system, where the S HIFT transition can apply to any configuration that has a non-empty buffer. In the arc-constrained system, S HIFT is an arc a  X  A C involving the node i to be shifted and some node on the stack, and we need to show that one of the three remaining transitions is then permissible. If a involves i and the node on top of the stack, then either L is permissible (in fact, required). Otherwise, either L EFT permissible, because their preconditions are implied by th e fact that A conditions in constant time. This can be achieved by preproc essing the sentence x and arc constraint set A C and recording for each node i  X  V x its leftmost constrained dependent (if any), and its rightm ost constrained dependent (if any), so that we can evaluate the preconditions in each config uration without having to scan the stack and buffer linearly. Because there are at mo st O ( n ) arcs in the arc constraint set, the preprocessing will not take more than O ( n ) time but guarantees that all permissibility checks can be performed in O (1) time.
 that it derives all and only PDT s compatible with a given arc constraint set A tence x . Soundness follows from the fact that, for every arc ( i , l , j )  X  A Completeness follows from the observation that every PDT G compatible with A a
PDG and can therefore be viewed as a larger constraint set for whi ch every transition sequence (given soundness) derives G exactly.

Empirical Case Study: Imperatives. Consider the problem of parsing commands to personal assistants such as Siri or Google Now. In this setti ng, the distribution of utterances is highly skewed towards imperatives making the m easy to identify.
Unfortunately, parsers trained on treebanks like the Penn T reebank (PTB) typically do a poor job of parsing such utterances (Hara et al. 2011). Ho wever, we know that if the first word of a command is a verb, it is likely the root of t he sentence. If we take an arc-eager beam search parser (Zhang and Nivre 2011) t rained on the PTB, it gets 82.14 labeled attachment score on a set of commands. the same parser so that the first word of the sentence must be th e root, accuracy jumps dramatically to 85.56. This is independent of simply k nowing that the first word of the sentence is a verb, as both parsers in this experim ent had access to gold part-of-speech tags. 4. Parsing with Span Constraints parsing can then be defined as the task of deriving a PDT transition system is sound if it only derives dependency gra phs compatible with the span constraint set and complete if it derives all such graph s. In addition, we may add the requirement that no word inside a span may have dependent s outside the span ( NONE ), or that only the root of the span may have such dependents (
Added Preconditions. Unlike the case of arc constraints, parsing with span constr aints cannot be reduced to simply enforcing (and blocking) specifi c dependency arcs. In this sense, span constraints are more global than arc constr aints as they require en-tire subgraphs of the dependency graph to have a certain prop erty. Nevertheless, we can use the same basic technique as before and enforce span constraints by adding new preconditions to transitions, but these precond itions need to refer to vari-ables that are updated dynamically during parsing. We need t o keep track of two things: 254
Given this information, we need to add preconditions that gu arantee the following: In addition, we must block outside dependents of all words in a span under the N condition, and of all words in a span other than the designate d root under the R condition. All the necessary preconditions are given in Fig ure 2 under the heading S
Complexity and Correctness. To show that the span-constrained parser always terminates after at most 2 n transitions, it is again sufficient to show that there is at le ast one permissible transition for every non-terminal configurati on. Here, S the word i to be shifted is the last word of a span and # other three transitions must be permissible. If # CC = 1, then R if # CC &gt; 1 and the word on top of the stack does not have a head, then L permissible; and if # CC &gt; 1 and the word on top of the stack already has a head, then R
EDUCE is permissible (as # CC &gt; 1 rules out the possibility that the word on top of the stack has its head outside the span). In order to obtain linea r parsing complexity, all preconditions should be verifiable in constant time. This ca n be achieved during initial sentence construction by recording the span s ( i ) for every word i (with a dummy span for words that are not inside a span) and by updating r ( s ) (for every span s ) and # described herein.
 sense that it derives all and only PDT s compatible with a given span constraint set S a sentence x . Soundness follows from the observation that failure to hav e a connected the added preconditions. Completeness can be established b y showing that a transition sequence that derives a PDT G compatible with S C in the unconstrained system cannot violate any of the added preconditions, which is straightfo rward but tedious.
Empirical Case Study: Korean Parsing. In Korean, white-space-separated tokens corre-spond to phrasal units (similar to Japanese bunsetsus ) and not to basic syntactic cat-egories like nouns, adjectives, or verbs. For this reason, a further segmentation step is needed in order to transform the space-delimited tokens to u nits that are a suitable input for a parser and that will appear as the leaves of a syntactic t ree. Here, the white-space boundaries are good candidates for posing hard constraints on the allowed sentence structure, as only a single dependency link is allowed betwe en different phrasal units, and all the other links are phrase-internal. An illustratio n of the process is given in
Figure 3. Experiments on the Korean Treebank from McDonald e t al. (2013) show that adding span constraints based on white space indeed improve s parsing accuracy for an arc-eager beam search parser (Zhang and Nivre 2011). Unla beled attachment score increases from an already high 94.10 without constraints to 94.92, and labeled attach-ment score increases from 89.91 to 90.75.

Combining Constraints. What happens if we want to add arc constraints on top of the span constraints? In principle, we can simply take the co njunction of the added preconditions from the arc constraint case and the span cons traint case, but some care is required to enforce correctness. First of all, we hav e to check that the arc constraints are consistent with the span constraints and do not require, for example, that there are two words with outside heads inside the the sam e span. In addition, we need to update the variables r ( s ) already in the preprocessing phase in case the arc constraints by themselves fix the designated root because th ey require a word inside the span to have an outside head or (under the R OOT condition) to have an outside dependent. 5. Conclusion
We have shown how the arc-eager transition system for depend ency parsing can be modified to take into account both arc constraints and span constraints, without affecting the linear runtime and while preserving natural n otions of soundness and completeness. Besides the practical applications discuss ed in the introduction and case studies, constraints can also be used as partial oracles for parser training. References 256
