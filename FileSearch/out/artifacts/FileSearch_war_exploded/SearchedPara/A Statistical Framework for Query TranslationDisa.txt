 YI LIU, RONG JIN, and JOYCE Y. CHAI Michigan State University 1. INTRODUCTION
To bridge the gap between different languages, machine translation has been used extensively in many research areas of multilingual information process-ing, such as cross-language information retrieval (CLIR). In CLIR, we can ei-ther translate queries into the language of documents or translate documents into the language of queries. Usually, it is simpler and more efficient to trans-late queries because of their shorter length. Most query translation algorithms require external linguistic resources, among which parallel corpora and bilin-gual dictionaries are the most commonly used. Methods based on parallel cor-pora usually learn the association between words of the source language and words of the target language, and apply the learned association to estimate the translation of queries. Examples in this category include statistical translation models [Xu and Weischedel 2001; Federico and Bertoldi 2002; Nie and Simard 2002; Kraaij et al. 2003] and relevance language models [Kraaij et al. 2000;
Lavrenko and Croft 2001; Lavrenko et al. 2002]. The main drawback of these methods is that they critically depend on the availability of parallel bilingual corpora, which are often difficult to acquire, especially for minor languages.
Thus, dictionary-based approaches for CLIR are usually preferable because of the increasing availability of machine-readable dictionaries.

Compared to the approaches based on parallel corpora, the key disadvan-tage of the bilingual dictionary-based approaches is that they lack the ability in disambiguating the translation of query terms among multiple candidates.
Very often, a number of translations (which we call translation candidates in this paper) are found in a bilingual dictionary for a single query word, but most of them are irrelevant to the semantic meaning of the query. Hence, it is crucial for a dictionary-based approach to reduce the ambiguity in translating query words as much as possible. However, on the other hand, given the short length of a query, it is usually impossible to completely resolve the translation ambi-guity resulting from the multiple interpretation of the query. Thus, it is also important for any dictionary-based CLIR approach to maintain the uncertainty in translating queries when the ambiguity is hard to resolve.
 In the past, several approaches [Jang et al. 1999; Gao et al. 2001, 2002;
Adriani 2000a; Kraaij and Pohlmann 2001] have been proposed to resolve the query-translation ambiguity in dictionary-based CLIR. The simplest one is to use all the translation candidates of each query word provided by the dictionary, with equal weights [Davis 1996; Kraaij and Pohlmann 2001]. This amounts to no-sense disambiguation when translating query words. Other approaches try to resolve the translation ambiguity by measuring the coherence of a translation candidate to the entire query. Typically, the coherence score, of a translation candidate is computed using word co-occurrence statistics. Given a query, a translation candidate of a query word is assigned with a high coherence score when it co-occurs frequently with the translations of other query words. The translation candidates with the highest coherence scores are selected to form the final translation for the original query. In Davis [1996], Hull and Grefen-stette [1996], Adriani [2000b], Kraaij et al. [2000], Kraaij and Pohlmann [2001] and Gao et al. [2001], only one translation candidate is selected for each query word; in Jang et al. [1999] and Maeda et al. [2000], a translation candidate is selected when its coherence score exceeds a predefined threshold, which allows multiple translations to be selected for each query word. We will refer to both approaches mentioned above as selection-based approaches , because they all have to make a binary decision for each translation candidate regarding if it will be included in the translated query or not. Given the usually short length of queries and the large variance existed in mapping information across dif-ferent languages, such binary decisions are usually difficult, if not impossible, to make. We call this problem the  X  translation-uncertainty problem . X  Another problem with the selection-based approaches is that the translation of one query word is usually determined independently from the translations of others. This assumption is reflected in the calculation of coherence scores. Usually, the coher-ence score of a translation candidate to a given query is computed as the sum of its similarities to every translation candidate for other query words. As a result, coherence scores are estimated independently from the choice of translations for query words, which leads the selection of translation candidates for different query words to be independent. We call this problem  X  translation-independence assumption . X  Although this problem has been addressed in previous work [e.g.,
Gao et al. 2001], usually greedy approaches are applied and, therefore, only suboptimal solutions can be obtained.
 In this paper, we propose a novel statistical framework for dictionary-based
CLIR. This framework will allow us to estimate the translation probabilities of query words by maximizing the overall coherence of the translated query, which we call  X  maximum coherence principle . X  Particularly, the proposed framework explicitly addresses the two problems mentioned above: to resolve the transla-tion uncertainty problem, the proposed framework maintains the uncertainty in translating queries through the estimation of translation probabilities of query words; to remove the translation independence assumption, the proposed framework allows the translation probabilities of all query words to be esti-mated simultaneously. Furthermore, the proposed framework is formulated as a quadratic programming problem [Gill et al. 1981], whose global optimal so-lution can be found efficiently using standard optimization packages, such as
Matlab. This is in contrast to several existing approaches, such as the propa-gation approach in Monz and Dorr [2005], where the solution is determined by an iterative procedure, which is not only time-consuming but also sensitive to the initialization of parameters or the stop criterion employed in the iterative procedure.

In addition to the general framework, we also present in this paper two real-izations of the proposed framework that employ different coherence measure-ments: the  X  X aximum Coherence Model X  that adopts the raw word-to-word similarity for coherence measurement, and the  X  X pectral Query-Translation
Model X  that measures the coherence score of each translation candidate based on the normalized word similarity. As will be explained later, the Spectral
Query-Translation Model-based on the normalized similarity measurement, can be further explained as a graph partitioning approach for query-translation disambiguation, which is employed in spectral clustering. Our empirical stud-ies with TREC datasets have shown that both models outperform the selection-based approaches with relative improvements ranging from 10 to 50%.
The rest of the paper is structured as follows: Section 2 briefly reviews the related work in selection-based approaches for query-translation disambigua-tion and spectral clustering for graph partitioning. In Section 3 we describe our proposed statistical framework for dictionary-based CLIR and the procedure to solve the related optimization problem. Sections 4 and 5 provide the details for two realizations of the proposed framework, respectively. Section 6 presents the experimental results with discussions. Section 7 concludes this work. 2. RELATED WORK
We first review the previous work in the selection-based approaches for query-translation disambiguation, followed by the discussion of spectral clustering that is strongly related to the proposed Spectral Query Translation Model. 2.1 Selection-based Approaches for Query Translation Disambiguation
One of the major factors that can potentially degrade the effectiveness of dictionary-based cross-language information retrieval is the ambiguity in translating query words [Ballesteros and Croft 1997; Gao et al. 2001]. In the efforts to resolve this translation ambiguity, a number of recent studies [Davis 1996; Hull and Grefenstette 1996; Jang et al. 1999; Adriani 2000b; Kraaij et al. 2000; Maeda et al. 2000; Gao et al. 2001; Kraaij and Pohlmann 2001] have sug-gested the strategy of translation selection by exploiting word co-occurrence patterns. Usually a similarity measurement between two translation candi-dates is defined in the form of word co-occurrence statistics. With the word sim-ilarities, we can then measure the coherence of a translation candidate with regard to the theme of the entire query. Only those translation candidates with high coherence scores will be selected for the query-translation.

Ideally, for each query word, we should select the translation candidate(s) that is consistent with the selected translation candidates for other query words. Apparently, this becomes a  X  X hicken X  X gg X  problem, since the selection of translation candidates for one query word is determined by the translation candidates selected for other query words. Thus, due to the computational concern, most selection-based approaches [Adriani 2000a; Gao et al. 2001, 2002] adopted approximate approaches that usually only produce suboptimal solutions. In particular, for each query word, those approaches choose the translation candidates that are consistent with all the translation candidates provided by the dictionary for all the query words, including both the selected and the unselected translation candidates. Formally, a translation selection strategy can be formulated as follows: A 2. For each set S i c. Select the word q t i in S i with the highest coherence score
The definition of similarity between two words in the above algorithm can take various forms of co-occurrence statistics, such as Dice similarity (as in
Adriani [2000a]), mutual information (as in Jang et al. [1999] and Maeda et al. [2000]) or its variants (as in Gao et al. [2001, 2002]). In addition to selecting the most likely translation for each query word, other selection-based approaches have been tried, such as selecting the best N translations [Davis 1996] or select-ing translations whose coherence scores exceed a predefined threshold [Jang et al. 1999; Maeda et al. 2000].

Apparently, the above approximate algorithm is not ideal. In particular, the coherence score for a translation is computed with regard to both selected and unselected translations. As a result of such an approximation, transla-tion of different query words are determined independently, which leads to the translation-independence problem, as discussed in the introduction section. In the proposed statistical framework, by formulating the problem of translation selection in a quadratic programming form, we are able to efficiently estimate the translations of all query words simultaneously . Furthermore, in contrast to the selection-based approaches that make binary decision for each transla-tion candidate, the new framework employs soft probabilities for representing both selected and unselected translation candidates. This is particularly use-ful when binary decisions are hard to make, for instance, all the translation candidates of a query word have very similar coherence scores. 2.2 Spectral Clustering
Spectral clustering approaches view the problem of data clustering as a prob-lem of graph partitioning. Each data point corresponds to a vertex in a graph.
Any two data points are connected by an edge whose weight is the similarity between the two data points. To form data clusters, the graph is partitioned into multiple disjoint sets such that only the edges with small weights are re-moved. Based on different criteria imposed on the partitioning, there are three major variants for spectral clustering: ratio cut [Chung 1997], normalized cut [Shi and Malik 2000], and min-max cut [Ding et al. 2001]. In the following, we briefly recapitulate the 2-way normalized cut algorithm, since it is the most widely used spectral clustering algorithm and has the closest relation to our proposed work.

Let G ( V , E ; W ) denote an undirected graph, where V is the vertex set, E is the edge set, and W = ( w i , j ) n  X  n is a matrix with w weight between the i th and the j th vertex. Define D = diag ( d a two-way normalized cut algorithm minimizes the following objective function: where we define S ( A , B ) = i  X  A j  X  B w i , j as the cut value, d d The above objective function can be rewritten as
If we introduce a cluster indicator vector q , with each element q the objective function becomes
Note that the minimizer to the above normalized cut value J is a binary vector q , with each element q i indicating the cluster membership of a vertex.
Given its combinatorial nature, it is difficult to solve the optimization problem efficiently (NP-hard). However, if we relax the cluster memberships to real values under the constraints the normalized cut algorithm can be formulated as follows:
Furthermore, if we define  X  q = D 1 2 q , we reach the following equivalent opti-mization problem
Note the above problem is in the form of Rayleigh quotient, and its solution can be found by solving the following eigenvalue system [Golub and Loan 1989] 3. THE STATISTICAL FRAMEWORK FOR DICTIONARY-BASED CLIR
The essential idea of the framework is to learn a set of translation probabili-ties for query words from the word co-occurrence statistics that maximizes the overall coherence of the translated query. In the following subsections, we will describe the components of the general statistical framework for dictionary-based cross-language information retrieval, including uncertainty modeling, the retrieval model, translation probabilities learning, and the solution to the related optimization problem, followed by a summary that sketches the steps of applying the proposed framework to CLIR. 3.1 Notation
The term  X  X ource language X  and a superscript s are used when referring to the language of queries. Similarly, the term  X  X arget language X  and a superscript t are for the language of documents. Let a query of the source language be denoted by q s ={ w s 1 , w s 2 , ... , w s m s } , where m q . Let r k denote the set of translation candidates provided by the dictionary for a word w s k in the source language. The union of translation candidate sets for all the words in q s is then denoted by R = m s k = 1 r k translation candidates for query q s , i.e., the size of R , is denoted by m
T = [ t q . Each element t k , j in T is 1 if the j th word in the target language appears in the dictionary as a translation for the k th word in the source language, and 0 otherwise. 3.2 Modeling the Uncertainty in Query Translation
To address the problem of translation uncertainty, we build the statistical framework by introducing translation probabilities. For a given query word, instead of making binary decision for its translation candidates, we estimate the probability of translating the query word into each translation candidate.
More importantly, the translation probabilities are estimated under the con-text of the entire query, namely a translation candidate will be assigned large probability mass if it is coherent with the semantic meaning of the entire query and vice versa.

Let p k , j denote the probability of translating a word w into a word w t j of the target language, given the context of query q as
In order to be consistent with the dictionary T , we assume that translation probability p k , j = 0, if the word w t j does not appear in the dictionary as the translation of word w s k . In other words, p k , j could be nonzero only if the word w is one of the translation candidates for the word w s k . This assumption can be formally expressed by the following constraints:
In addition, we have a constraint to ensure that each query word has only one ideal translation given the context of the query q s .

To simplify our notation, we further introduce the matrix P denote all the translation probabilities for query q s . Then, the above two sets of constraints can be rewritten as where e = [1, 1, ... ,1] T . 3.3 The Retrieval Model
The introduction of translation probabilities p k , j can be well accommodated by a statistical retrieval model for CLIR. In particular, we estimate Pr( d i.e., the probability for a document d t in the target language to be relevant to a query q s in the source language. By the Bayes X  law, this probability can be approximated as The last step assumes that document prior Pr( d t ) follows a uniform distribution.
Hence, in the following, we will compute log Pr( q s | d t
To model the translation uncertainty, we rewrite the expression for log Pr( q s | d t ) as follows:
In the second step of the above derivation we employ the Jensen X  X  inequality [Rudin 1987], which can be viewed as the first step toward the variational ap-proximation [Jaakkola 1997; Jordan et al. 1999]. In the third step, we ignore the term log Pr( q s ) that is independent from the document d the roles between q t and q s using the Bayes X  law (similar to Eq. (16)) by as-suming that the prior of the translated query q t follows a uniform distribution, i.e., Pr( q t ) is a constant. In the fourth step,  X  represents the mathematical expectation of a random variable. To estimate Pr( w t | q observing the word w t in the translation of the query q s into a summation over words in the source language:
Finally, from Eqs. (16) X (18) we have
Here Pr( w t | d t ) is a monolingual language model for a document d language; Pr( w t | w s ; q s ) is the probability for translating a query word w query q s in the source language, which can also be seen as the weight assigned to the query word w s . For the sake of simplicity, an uniform distribution is assumed for probability Pr( w s | q s ). As indicated in Eq. (19), the key component to the above retrieval model is how to estimate the translation probabilities
Pr( w t | w s ; q s ). 3.4 Learning the Translation Probabilities
In this subsection, we will describe the essential part of the statistical frame-work, i.e., automatically learning translation probabilities from the word co-occurrence statistics. We will begin with the definition of an overall coherence measurement for translating a query, followed by formulating the learning pro-cess in an optimization form.

Using the translation probabilities introduced in the previous subsection, we can now define a measurement for the overall coherence when translating a query q s , i.e., where o j , j is a pair-wise similarity that measures the correlation between two words w t j and w t j in the target language.

The above measurement is motivated by the intuition that appropriate trans-lations of query words tend to be coherent with each other. In other words, if w and w t j are appropriate translations for words w s k and w expect that (1) both translation probabilities p k , j and p values, and in the same time, (2) w t j and w t j are related by a large coherence measurement o t k , k . Hence, what is implied by this intuition is that the assign-ment of probability p k , j and p k , j should be synchronized with the coherence measurement o t j , j . This synchronization is expressed in Eq. (20) through the multiplication of the three terms p k , j , p k , j , and o mizing the overall coherence in Eq. (20), we enforce the consistency between the probability assignment and the coherence measurement by assigning large values to p k , j and p k , j only when the corresponding coherence measurement o
The similarity information (i.e., o t j , j ) can be derived from monolingual word co-occurrence statistics using the metric, such as information gain. The ex-pression for the overall coherence can be simplified using the matrix notation introduced in Eqs. (14) and (15): surement of any two words in the target language.

It is important to note that the coherence measurement defined above is significantly different from the coherence measurement defined in Gao et al. [2002]. The key difference between them lies in the fact that the measurement defined in Gao et al. [2002] is based on the concept of translation selection, namely, only the best translation candidate is chosen for every query word.
As a consequence, the resulting formalization in Gao et al. [2002] is, indeed, a combinatorial optimization problem, and, therefore, is difficult to solve effi-ciently. By relaxing the binary choice of translation for query words to transla-tion probabilities, on one hand, we resolve the difficulty in optimization, and, on the other hand, we are able to explore the translation uncertainty, which could be important when the translation ambiguities are difficult to resolve.
Now our goal is to determine the translation probabilities such that the overall coherence is maximized. Putting together both the objective function in
Eq. (21) and the constraints in Eqs. (14) and (15), the learning process of the query-dependent translation probabilities can be formalized as the following optimization form
Notice that in the above objective function, in addition to the first term that corresponds to the overall coherence measurement for query translation an-other term  X  C p e T PP T e is introduced. This additional term is called a regu-larizer in machine learning and plays the similar role as the prior in Bayesian learning [Mitchell 1997]. Note that e T PP T e = k , k j p the sum of all elements in PP T , and its maximizer is to assign uniform dis-tributions to all translation probabilities P . Thus, by including the regularizer in the objective function, we essentially introduce an uninformative prior for
P , i.e., without the context of a given query, we assume that all translation candidates provided by a bilingual dictionary are equally likely to be selected.
The regularizer approach has been widely used in many well-known machine learning models, including the logistic regression model [Nigam et al. 1999] and support vector machines [Burges 1998]. Parameter C p is introduced to balance the trade-off between the overall coherence measurement and the regularizer.
Another important issue with the objective function in Eq. (22) is the choice of similarity measurement O . A different similarity matrix O can result in rather different performance in information retrieval. We will show two of them in the later sections. Finally, we would like to emphasize that by solving the resulting optimization problem in Eq. (22), we are able to acquire the translation prob-abilities of all query words simultaneously through the computation of matrix P . This is in contrast to a number of existing approaches for dictionary-based
CLIR, where the selection of translations for individual query words are deter-mined either independently or by certain greedy means that usually leads to suboptimal solutions. 3.5 Solving the Optimization Problem
The optimization problem in Eq. (22) is, in fact, a quadratic programming (QP) problem [Gill et al. 1981], since we find the objective function consists exclu-sively of quadratic terms with respect to the set of variables constraints are linear to those variables. A standard QP problem has the fol-lowing form where the vector x is the unknown variable, and matrices H , A , and E and vectors b , c , and d are known. In this section we will discuss how to reformulate our optimization problem (22) into the standard QP form, so that it can be easily recognized by most of existing QP solver softwares.

Since the standard QP form takes a vector form of the optimization variables, we first need to rearrange the unknown variables in the matrix P (i.e., the set of translation probabilities) as a vector; then we need to reformulate the objective function as well as all the constraints in terms of the vector representation of the set of translation probabilities.

To begin with, we concatenate all the row vectors in the matrix P together to form a vector, i.e., where  X  p is the rearranged vector, consisting exactly the same set of variables in the matrix P .

Then, we need to reformulate the objective function in Eq. (22) with respect to the vector  X  p , i.e., finding a matrix H such that e T POP is satisfied. It is easy to verify that such a matrix H can be written in a succinct form by using the kronecker product operator  X  1 as follows
Here 1 stands for a matrix of all elements 1.
Similar procedures can be applied to reformulate the constraints in Eq. (22), using the following matrices
Finally, the optimization problem in Eq. (22) can be rewritten in a standard form of the QP problem as follows where e is a vector with all elements equal to 1, and the matrices H , E and the vector  X  p are given in Eqs. (24) X (26). In our experiments, the QP package from
Matlab [The Mathworks ] is used to solve the above problem. 3.5.1 Remark. For the QP problem formulated in Eq. (27), the problem size appears to be large because the number of variables in vector q is m i.e., the product between the number of unique query words and the number of distinct translation words provided by the dictionary. However, notice that in the constraint (29), p , i.e., the upper bound of translation probabilities, is a con-catenation of translation vectors t i obtained from T , the matrix notation of the bilingual dictionary. Given that most query words only have a few translations, most of the elements in the matrix T will be zeros. As a result, most elements in the upper bound vector q are zeros, which leads to the zero values for the corresponding translation probabilities in q . Hence, the number of nonzero translation probabilities in q is no more than the total number of translations provided by the bilingual dictionary for the query words, which is usually much smaller than the product m s m t . Thus, the computation cost of the maximum coherence model is modest for real CLIR practice, if not overestimated. 3.6 Summary and Discussion
By putting together the uncertainty modeling, translation probability learn-ing and the retrieval model, we summarize the steps of applying the proposed framework to CLIR in Figure 1. In those steps, pair-wise similarity computa-tion and language model preparation can all be done offline. In query time, most computation comes from solving the QP problem, in addition to the conventional retrieval process.

It is important to note that although in this study we limit ourselves to the bag of words approach without exploring other linguistic structures, such as phrases, our framework can essentially be generalized to the linguistic struc-tures other than the bag of words. This can be achieved by treating all the w and w s k as the units in the targeted linguistic structures and Pr( w likelihood of translation between the units in the new linguistic structures. We focus our discussion on word translation mainly because of the following two concerns: 1. As mentioned in the introduction part, one of the main motivation behind our work is to resolve the problem of CLIR when only bilingual dictionaries are available. Since, in practice, most bilingual dictionaries are word-based, we focus our study on the word-based approaches. 2. Since many user queries, particularly the ones from the World Wide Web, are less likely to be grammatically structured, we believe it is important for a robust CLIR framework to minimize its dependence on deep linguistic analysis of user queries.

Despite of the above claim, we believe that given more linguistic resources available and well grammatically structured queries, our framework could be improve by appropriately incorporating the linguistic constraints derived for the translated queries. For example, we can incorporate the linguistic knowl-edge into the similarity matrix O , or we can introduce it into the regularizer in the optimization problem (22) to make it more informative. These framework extensions will be considered in future work. 4. MAXIMUM COHERENCE MODEL
In the previous studies of selection-based approaches, several metrics have been used for measuring the similarity between two words in the target language, i.e., the o t j , j , including variants of mutual information [Gao et al. 2001; Jang et al. 1999], and the dice similarity [Adriani 2000a, 2000b]. In this model, a typical variant of mutual information (which has been used in previous studies [Gao et al. 2002]) is used as the pair-wise similarity metric
Pr( w t j ) is the unigram probability for word w t j , and Pr( w bility for word w t j and w t j to co-occur in the same documents. Both probabilities can be acquired by simply counting the term frequency of single words and the frequencies of co-occurrence between two words. Note that Eq. (30) is different from the standard definition for mutual information in that only co-occurrence information is used. Because of the computation concern, in Eq. (30) the correla-tion between two words when at least one of them does not occur in documents is ignored. According to the definition in Eq. (30), we see that two words will be regarded as similar if they co-occur often in the document collection. Using matrix notation S = [ s t j , j ] m t  X  m t and substitute the coherence matrix
O with the similarity matrix S , we have the following model
We call the above model  X  X aximum coherence model, X  since it is a straightfor-ward implementation of the proposed framework. Parameter C mined empirically by cross-validation. Heuristically, we hope C in the same scale as those elements in the matrix S , since C between the similarity measurement S and regularizer I . Hence, we heuristi-
Here  X  is a scaling factor. In our experiments we tested different values of the range of [0 . 1, 10] and chose the one with good retrieval performance. 5. SPECTRAL QUERY TRANSLATION MODEL
One problem with the maximum coherence model proposed in the previous section is the difficulty in determining the value of parameter C lem arises because the two terms in objective function in Eq. (31), namely, the overall coherence measurement of translated query, and the regularizer, are in different scales. As a result, we have to emprically search for appropriate C
In order to put the two terms on the same scale, we introduce the concept of normalized similarity matrix: for a given similarity matrix S define a diagonal matrix D = diag ( d 1 , d 2 , ... , d n the normalized similarity matrix is  X  S = D  X  1 2 SD  X  1 malization procedure, we are able to bring down the scale of matrix S to O (1).
As a result, both the coherence term and the regularization term are on the same scale. Thus, instead of finding appropriate C p empirically, we can simply set it to 1, which leads to the following realization of the proposed framework:
We call the above model  X  X pectral query-translation model X  because it can be interpreted as a graph-partitioning approach for query-translation disambigua-tion, which is strongly related to spectral clustering. We will further elaborate on this interpretation in the following subsection. 5.1 Query Translation Disambiguation as Graph Partitioning
In order to see the relationship between graph partitioning and dictionary-based CLIR model in Eq. (32), for a given query q s and its translation candidate set R , we present the related similarity information S through an undirected weighted graph. Each translation candidate w t k  X  R is represented by a vertex.
Any two translation candidates related to two different query words are con-nected by an edge if they ever co-occur in at least one document. A nonnegative weight is assigned to each edge to indicate the similarity between the two con-nected words. Here we use the measurement defined in Eq. (30) as the edge weight.

With the constructed graph for a given query and its translation, we hypoth-esize that the best (or the most coherent) translation of a query corresponds to the most strongly connected component within the graph. To separate the strongly connected component from the rest of the graph, a graph-partitioning algorithm can be employed to divide the graph into two disjoint clusters: a clus-ter for strongly connected component and a cluster for the rest of the graph.
To this end, we inherit the idea from the normalized cut algorithm. For the graph constructed for the translation candidate set R , its adjacency matrix is exactly the similarity matrix S = [ s t j , j ] m t  X  m t is L = D  X  S . Following the formalization of the normalized cut algorithm [Shi and Malik 2000], the optimal two-way partitioning is found by solving the following minimization problem v is a binary variable with 1 indicating the corresponding word being included in the query translation and 0 for not being included.

It is not difficult to see that the optimization problems in Eqs. (32) and (33) are, in fact, equivalent, if we set or in another more explicit form where Pr( w s k | q s ) takes a uniform distribution, i.e., Pr( w
What is suggested by Eq. (34) or (35) is to relax the cluster indicator v a soft membership instead of a binary value. This soft membership, from the graph-partitioning point of view, indicates how likely the strongly connected component will include the particular translation candidate w links the soft membership with the probability Pr( w t j | of including translation candidate w t j in the translation of query q model proposed as the optimization problem (32) can be perfectly explained from a graph-partitioning perspective. Note that the relaxation of a binary indicator to a soft membership in the perspective of graph partitioning is in accordance with our introduction of translation probabilities in the statistical framework. This is because both of them try to address the uncertainty problem in the process of query translation.

Figure 2 gives an illustrative example of this graph-partitioning perspec-tive. In the example, the query is composed of four Chinese words, and around each Chinese word are its translation candidates in English provided by a
Chinese X  X nglish dictionary. The thickness of lines connecting two English words roughly represents their similarity. The number below each English word is its translation probability estimated from the spectral query-translation model. Based on the graph representation in Figure 2, we can easily see a strongly connected component consisting of words  X  X ndependent, X   X  X ign, X  and important to note that Figure 2 only serves as an illustration of the proposed spectral query translation model. In particular, all translation candidates will be weighted by their translation probabilities determined by the optimization algorithm. 5.1.1 Remark. In the above, we discuss the relationship between the spec-tral query-translation model and the spectral graph partitioning. However, it is worth pointing out that the solution to the spectral query-translation model can not be acquired by the eigen analysis that is used for solving spectral graph partitioning. This is because the spectral query-translation model seeks for the optimal translation probabilities (which leads to soft cluster memberships) that maximize the overall coherence measurement. In contrast, most spectral graph-partitioning algorithms, such as normalized cut, search for the binary cluster memberships that minimize the graph cut. It is such difference that leads to the quadratic programming formalization, instead of an eigenvector problem, for the spectral query-translation model. 5.2 Maximum Coherence versus Spectral Graph Translation
Aside from its graph-partitioning explanation, the spectral query-translation model is advantageous to the maximum coherence model in that it is able to reduce the translation probabilities for the  X  X ommon X  words, which may other-wise dominate in the final query translation. To see this, consider an element  X  s j , j in the normalized similarity matrix
Suppose translation candidate w t j is a common word that appears frequently in the document collection of the target language. This implies that the sum tive for the purpose of document retrieval, we use the normalization factor 1 the probability of including common words in the final query translation.
Based on the above analysis, we see that the spectral query-translation model is more appealing than the maximum coherence model. This is further con-firmed by our empirical studies on cross-language information retrieval pre-sented in the next section. 6. EXPERIMENTS AND DISCUSSIONS statistical framework for cross-language information retrieval. In particular, four research questions will be addressed in this empirical study: 1. Is the proposed statistical framework effective for cross-language informa-tion retrieval ? To obtain a comprehensive view, we compare the maximum coherence model and the spectral query-translation model to the existing selection-based approaches using a variety of queries and documents. 2. How important is it for a query disambiguation algorithm to include trans-lation uncertainty in its analysis ? To address this question, we will examine the importance of including translation uncertainty in cross-language infor-mation retrieval through case studies. 3. How important is it to remove the translation independence assumption for cross-language information retrieval ? To address this question, we will 4. How does the maximum coherence model empirically compare to the spectral 6.1 Experiment Setup All our experiments are retrieval of English documents using Chinese queries.
The document collections used in this experiment are from TREC ad-hoc test collections, including
In addition to the homogeneous collections listed above, we also test the proposed model against heterogeneous collections that are formed by combining multiple homogeneous collections. In particular, two heterogeneous collections are created: collection AP88-89 + WSJ87-88 and collection AP89 88 + DOE1-2. In a heterogeneous collection, words are more likely to carry multiple senses than words from a homogeneous collection, which will increase the difficulty for an automatic algorithm to disambiguate the senses of query words using the pair-wise word similarities. The SMART system [Salton 1971] is used to process document collections. Each document is first parsed into tokens with stop words removed, and then tokens are stemmed using the Porter algorithm. Finally, each document is represented as a bag of stemmed words.
We also implemented pivoted document length normalization weighting scheme [Singhal et al. 1996] in SMART system for the retrieval process. Since our goal is to illustrate the advantage of the proposed statistical framework, we do not apply more sophisticated procedures for text analysis in our experiment, such as phrase identification.

Our queries come from a manual Chinese translation of TREC-3 ad-hoc topics (topic 151 X 200). To fully examine the effectiveness of the proposed mod-els, we test it against both the long Chinese queries and the short Chinese queries. A short Chinese query is created by translating the  X  X itle X  field of an English query into Chinese; a long Chinese query is formed by combining the Chinese translations of both the  X  X itle X  field and the  X  X escription X  field in an
English query. The average length of short Chinese queries is 9.64 Chinese characters, and 30.72 Chinese characters for long queries. For Chinese trans-lations, we also manually segment the sentences into words with stop words removed, then feed them into our query translation framework. Figure 3 gives an example of the title field and description field (in the bottom panel), which is used to form a Chinese query in our experiments.

Since most of the words in a short query are highly relevant to the topic of the query, we would expect that query-disambiguation approaches, based on word similarities, will work well. The analysis of CLIR with long queries could be tricky because of the following two conflicting aspects: (1) On one hand, a long query provides significantly richer context than the short one in disambiguating the word sense of translation. Therefore, we would expect that the long queries may achieve a better performance than the short queries in CLIR. (2) On the other hand, a long query tends to include words either irrelevant or only slightly relevant to its topic. As a result, even a translation word that is coherent with the translations of many query words may not necessarily be a good candidate for selection. In our experiment, we will examine which factor among the two plays the major role in CLIR. Hence, a long query may pose a more challenging problem than a short query for a translation disambiguation algorithm based on word similarity information.

Finally, the relevance judgments for the original English queries are used as the relevance judgments for their Chinese translations. The Chinese X 
English dictionary used in our experiments comes from Linguistic Data Consor-tium (LDC, http://www.ldc.upenn.edu), which consists of translations for 53061
Chinese words. Since our experiments do not involve the processing of English phrases, for any English phrase that is the translation of a Chinese word, we simply treat it as a bag of words.

To evaluate the effectiveness of the proposed framework and models, we im-plement two baseline models that take translation selection approaches. The first baseline model selects the most likely translation for each query word, which we call  X  X STO X . Specifically, we follow the  X  X pproximate translation se-lection algorithm X  described in Section 2.1. The second model, which we call  X  X LTR, X  makes no attempt at translation disambiguation by simply including all the translations provided by the dictionary for query words into the final query translation. Finally, for easy reference, we use the abbreviation  X  X AC X  for our maximum coherence model and  X  X QT X  for our spectral query-translation model. The constant C p for the regularizer term in the maximum coherence model is set to be 4 j , j s t j , j / ( m t ) 2 , based on our empirical experience. 6.2 Comparison to Selection-based Approaches
Table I lists the average precision across 11 recall points for both the homoge-neous collections and the heterogeneous collections. As indicated in Table I, the proposed models (i.e.,  X  X AC X  and  X  X QT X ) outperform the two baseline models for both short queries and long queries across all four different collections. For the purpose of reference, we also list the results of monolingual information retrieval in the first column of Table I. Furthermore, we plot the precision-and 5, respectively. As illustrated in Figures 4 and 5, for all four collections, the precision-recall curves of the proposed models always stay above the curves of the other two baseline models. Based on these results, we conclude that the proposed statistical framework performs substantially better than the other two selection-based approaches for cross-language information retrieval.
A further examination of results in Table I gives rise to the following observations: 1. In general, the retrieval accuracy for heterogeneous collections appears to be worse than that for homogeneous collections. In particular, a substantial decrease in the average precision is observed for all four models when the collection of DOE1-2 is included in the heterogeneous collection. This result is in accordance with our previous analysis, i.e., words from heterogeneous collections are more likely to have multiple senses, thus resulting in higher translation ambiguity. 2. A better retrieval is achieved for short than for long. The degradation in performance between long and short queries is more significant for hetero-geneous than for homogeneous collections. Usually long queries bring rich context, as well as noise. Our result indicates that among the two factors, the second one, i.e., long queries consist of many irrelevant words, seems to be more influential than the first one, i.e., long queries provide rich context for word sense disambiguation, in query translation. This result seems to contradict the general belief that long queries tend to yield better retrieval accuracy than the short ones, given its rich context. However, it is worth 3. The  X  X STO X  model does not consistently outperform the  X  X LTR X  model. In particular, since our experiments focus on CLIR with a bag of words, we did not employ any linguistic tools other than removing stop words and stem-ming keywords. In contrast, in Gao et al. [2001] linguistic tools are used to identify appropriate English phrases and their Chinese translation, which has been shown as an important factor in CLIR [Ballesteros and Croft 1997;
Gao et al. 2001]. Although phrase analysis is important to CLIR, we believe that a generic probabilistic model is beneficial to CLIR of any language, par-ticularly when linguistic resources are scarce. Other differences between our baseline  X  X STO X  model and the model in Gao et al. [2001] include the differ-ent ways of mutual information computation and slightly different transla-tion selection strategies. 6.3 The Necessity of Including Translation Uncertainty
To demonstrate the uncertainty in query translation, in Table II, we list the translation probabilities for three Chinese words 3 that are estimated by the maximum coherence model. As we can see, a significant variance exists in the distribution of translation probabilities across different Chinese words. The first example in the figure shows an almost uniform distribution over all trans-lations, while the third one illustrates a very skewed distribution. Meanwhile, the second example provides a distribution that is neither uniform nor totally skewed. These three examples illustrate the  X  translation uncertainty problem,  X  which we have addressed in previous sections. Furthermore, the diversity in the distribution of translation probabilities makes it difficult for a selection-based approach to perform well over all different cases. For example, the  X  X STO X  model is able to work well for the third example, but will fail in the first one.
On the other hand, the  X  X LTR X  model would be perfect for the first example, but not for the third one. Based on the above analysis, we conclude that it is im-portant to capture the translation uncertainty and the diversity of translation uncertainty in a probabilistic model. 6.4 The Impact of Translation Independence Assumption
To illustrate the impact of the translation independence assumption on query translation disambiguation, consider the example in Table III. This query consists of four Chinese words; English translations for each Chinese word provided by the dictionary are listed in the second column. The original En-glish query is also included at the bottom of the figure. The English transla-tions selected by the  X  X STO X  model are listed in the third column, indicated by small crosses. The translation probabilities from Chinese words to their En-glish translations estimated by MAC and SQT are listed in the last two column, respectively.

Compared to the original English query, we see that the  X  X STO X  model makes incorrect translation selection for both the first and the second Chinese words.
For the first one, the correct English translation should be  X  X ndependent, X  in-stead of  X  X tand (alone) X . 4 The better translation for the second Chinese word should be  X  X ublish X  instead of  X  X ress. X  One reason for such mistakes is that in the  X  X STO X  model, the coherence score of a translation is computed based on all the English translations provided by the dictionary for the Chinese words in the query. Thus, the coherence score of one translation word is completely inde-pendent from the selection of other translations. Since both  X  X tand X  and  X  X ress X  are common in English, their overall coherence scores turn out to be larger than the coherence scores of other words, which lead them to be selected by the  X  X STO X  model. In contrast, in both the maximum coherence model and the spectral query-translation model, the estimation of translation probabilities for one word is dependent on the estimation of translation probabilities for other words. As a result, they are able to adjust the mistakes by assigning significant amounts of probability mass to the correct translations. For example, for the first Chinese word, both models are able to assign a probability to the correct
English translation  X  X ndependent X  comparable to the probability assigned to the translation  X  X tand (alone). X 
Note that neither the maximum coherence model nor the spectral query-translation model is able to always assign the largest probabilities to the best translation candidates (such as for the first Chinese word in Table II). However, in general by maximizing the coherence of the entire translated query both models tend to shift more probability mass to the best translation candidates, and thus alleviate the mistakes brought by those selection-based approaches originated from their false assumption on translation independence. We believe this is one of the major advantages of these two models over all the selection-based approaches. 6.5 Performance: MAC versus SQT
Table I reveals a slightly higher average precision across 11 recall points of  X  X QT X  model compared to  X  X AC X  model. In Figures 4 and 5 the precision-recall curves of  X  X QT X  model generally stays above those of  X  X AC X  model, though the margin is not always clear. All the experimental results suggest that the spec-tral query-translation model is slightly better than the maximum coherence model.

This observation is in accordance with our theoretical analysis on the two models at the end of Section 5. Apart from the advantage of saving the trouble of choosing the optimal regularizer constant C p , the benefit of normalizing the coherence matrix can be observed from the example presented in Table IV, where we list and compare the probabilities of including different translation candidates in the final query translation for one example query word from both models. As can been seen, the common word  X  X at X  is assigned with a significant amount of probability mass in the maximum coherence model, although it is almost irrelevant to the context of the entire query. At the same time, the word almost zero probability (or too small to represent). As indicated by the results listed in Table IV, the spectral query-translation model is able to overcome this problem by redistributing some of the probability mass on the common word spectral query-translation model has a better way to estimate the translation probabilities of common words than the maximum coherence model.
 6.6 Computational Efficiency
To examine the computational efficiency, we calculate the averaged number of seconds that are spent by the proposed algorithms to solve the optimiza-tion problem for each query. All the experiments are conducted on a PC with a
Pentium 4 2.8-GHz CPU and 1-G RAM that runs Matlab 7.1 on a Windows sys-tem. We directly use the quadratic programming function provided by Matlab to solve the QP problem in the proposed algorithms for query-translation dis-ambiguation. The results are 0 . 027 s per query for short queries, and 3 per query for long queries. Clearly, our algorithm is sufficiently fast for short queries, but a little slow for long queries. To improve the computational effi-ciency of long queries, we can first divide a long query into a number short ones and then translate each short query using the proposed framework. Since the computational complexity of quadratic programming is O ( n where n is the number of translation probabilities, by significantly reducing the number of query words, we reduce the number of translation probabilities and, therefore, improve the computational efficiency. Note that the above approach is based on the assumption that most of the context needed for query-translation disambiguation can be found in the neighborhood of each query word. In the future, we plan to experiment with the proposed approach to improve the effi-ciency of long queries. 7. CONCLUSIONS
In this paper, we propose a novel statistical framework for cross-language information retrieval. It utilizes word co-occurrence statistics for estimating translation probabilities that are effective for query disambiguation. Compared to the selection-based approaches, the merits of the framework are twofold: (1) It preserves the translation uncertainty through the estimation of trans-multaneously. Two realizations of the proposed framework, namely, the maxi-mum coherence model and the spectral query-translation model, are presented based on different choices of coherence measurements. Our analysis indicates that the spectral query translation model can also be interpreted as a graph-partitioning approach for query-translation disambiguation. Empirical results under various scenarios have shown that the proposed framework is able to per-form substantially better than the existing selection-based approaches. Further analysis indicates that the spectral query-translation model is more effective and reliable than the maximum coherence model when dealing with common words.

