 Youjin Chang  X  Minkoo Kim  X  Vijay V. Raghavan many approaches have been tried to solve this problem by expanding initial queries and reweighting the terms in the expanded queries using users X  relevance judgments. Although relevance feedback is most effective when relevance information about retrieved documents is provided by users, it is not always available. Another solution is to use correlated terms for query expansion. The main problem with this approach is how to construct the term-term correlations that can be used effectively to improve retrieval performance. In this study, we try to construct query concepts that denote users X  information needs from a document space, rather than to reformulate initial queries using the term correlations and/or users X  relevance feedback. To form query concepts , we extract features from each document, and then cluster the features into primitive concepts that are then used to form query concepts . Experiments are performed on the Associated Press (AP) dataset taken from the TREC collection. The experimental evaluation shows that our proposed framework called QCM (Query Concept Method) outperforms baseline probabilistic retrieval model on TREC retrieval.
Keywords concept-based information retrieval . query reformulation 1. Introduction
An information retrieval (IR) system returns a set of documents satisfying the information need expressed by a user X  X  question. The purpose of information retrieval is to retrieve all the relevant documents, while filtering out non-relevant document (van Rijsbergen, 1979).
Nowadays, Web is staged in the center of the information technology. In search engines, trated through the commonly observed phenomenon, during information search on the Web, where the queries are usually of a few words long and a large number of hit documents are returned to the user. Most people are not good at making effective queries right away. There-fore, they spend large amount of time in reformulating their queries to accomplish effective retrieval.

Many researchers have tried to find appropriate solutions for representing users X  interest correctly (Han et al., 1994; Koenemann, 1996; Qiu and Frei, 1993; Salton and Buckley, 1990;
Xu and Croft, 1996). They have studied the query reformulation method for improving the initial query through query expansion and term reweighting. The query reformulation involves two basic steps: expanding the initial query with new terms and reweighting the terms in the expanded query. These query expansion approaches are grouped in three categories (Han et al., 1994): manual query expansion, automatic query expansion and semi-automatic query expansion. Manual or semi-automatic query expansion needs a third-party to accomplish an expansion. However, automatic query expansion expands initial query without other X  X  intervention.

In this paper, we propose an automatic query expansion method that constructs query con-cepts that denote users X  information needs from a document space. Our approach to generate query concepts is novel in the sense that we do not employ either Relevance Feedback (RF) (Salton and Buckely, 1990; Baeza-Yates and Ribeiro-Neto, 1999) or term-term correlations derived from a predefined knowledge base. Relevance feedback is a well-known method in query reformulation. Relevance feedback chooses important terms from previously retrieved documents that have been identified as relevant by the users or system, and enhances the importance of selected terms in a modified query.

In our method, we assume that there exist primitive concepts (basis concepts) in a document space, and they can be used to form high-level concepts that can be employed in the field of information retrieval. Hence, we first extract features of given documents and cluster them into primitive concepts, and then, based on these concepts, we form query concepts .
In Section 2, we describe related works. In Sections 3 and 4, we outline how primitive concepts and query concepts would be constructed. In Section 5, we present the experimental results that are obtained on a part of TREC collection. In the experimental tests, we evaluate our Query Concept Method (QCM) with previous approaches such as Pseudo Relevance
Feedback (PRF) (Rocchio, 1971; Baeza-Yates and Ribeiro-Neto, 1999). PRF is a sort of blind relevance feedback generally used as a fully automatic query expansion. The results show that our proposed method outperforms previous approaches. 2. Related work
There are many works related to automatic query reformulation that improves initial queries through query expansion and term reweighting (Bodner and Song, 1996; Chang and Hsu, 1998; Han et al., 1994; Klink, 2001; Koenemann, 1996; Qiu and Frei, 1993; Rocchio, 1971; reformulation do not rely on users to make relevance judgments. They are often based on language analysis (Bodner and Song, 1996; Bookman et al., 1999; Spark Jones and Tait, 1984), term co-occurrences, PRF (Rocchio, 1971; Baeza-Yates and Ribeiro-Neto, 1999), or concept-based retrieval (Kim et al., 2000; Klink, 2001; Nakata et al., 1998; Qiu and Frei, 1993).

According to Bodner and Song X  X  research (1996), language analysis approaches require a deep understanding of queries and documents at higher computational costs. The require-ments to achieve a deep understanding are still an open problem in the field of artificial intelligence and this query reformulation technique has also been shown to have only small improvements in retrieval performance.

Without users X  relevance feedback, there are other strategies to reformulate queries. The idea involves identifying terms that are related to the query terms. Those terms might be synonyms, stemming variations, or terms that are close to the query terms in the text. Two basic types of strategies are global analysis and local analysis. In automatic global analysis, the similarity thesaurus obtained is based on term-term relationships. Unfortunately, this approach does not work well in general because the relationships captured in a thesaurus frequently are not valid in the local context of a given user query (Baeza-Yates and Ribeiro-
Neto, 1999). Automatic local analysis adopts clustering techniques for query expansion. The local clustering techniques are based on the set of documents retrieved for the original query and use the top ranked documents for clustering neighbor terms. Such a clustering is based on term co-occurrence inside documents. The idea of applying a global analysis technique to a local set of documents retrieved is called local context analysis. An earlier work done by
Xu and Croft (1996) illustrated the advantage of combining techniques form both local and global analysis.

The Pseudo Relevance Feedback (PRF) is a representative method of automatic query expansion. Since precise user X  X  feedback is difficult to obtain, in PRF, multiple documents at the top of the ranked list are assumed to be relevant. This procedure has been found to be highly effective in some cases, most likely those in which the original query statement are long and precise (Baeza-Yates and Ribeiro-Neto, 1999). This approach may impose some problems on selecting terms that are unrelated to relevance and happen to appear in documents that meet the selection criteria. Unreliable terms will be added to the query with subsequent adverse effects on retrieval behavior.

Another related area of our research is concept-based retrieval (Kim et al., 2000; Klink, 2001; Nakata et al., 1998; Qiu and Frei, 1993). It starts from the considerable interest in bridging the gap between the terminology used in defining queries and the terminology used in representing documents (Kim et al., 2000). It treats those query words not as literal strings of letters, but as representing concepts, therefore using concept-based retrieval can retrieve relevant documents even if we do not contain the specific words used in the query. Concept-based retrieval experiments often tested the effects of thesaurus-based query expansion on
Boolean retrieval performance. Some of them use a thesaurus such as a WordNet (Fellbaum, terms. But, this approach requires a lot of time with the processing of individual queries, and does not work well in general because the relationship captured in a thesaurus are not impossible to make an optimal thesaurus in every field of study and the problem caused by  X  X olysemy X  which has more than one distinct meaning (e.g. chip, model) can be another problem.

While previous query reformulation methods focus on reformulating initial queries by expanding and reweighting the terms in the queries by depending on users X  relevance judg-ment and/or predefined knowledge base such as a thesaurus, in our approach, we try to create a set of concepts from a document space appropriately, and then reformulate initial queries with query concepts . To construct query concepts , we extract features from each document, and cluster them into primitive concepts that can be used to form query concepts . There are similar studies on extracting query concepts from a document space (Kim et al., 2000; Nakata et al., 1998; Qiu and Frei, 1993; Wong and Fu, 2000).

Nakata et al. (1998) introduced a notion of the Concept Index, which aims to index im-portant concepts described in a collection of documents belonging to a group, and provides user-friendly cross-references among them to aid concept-oriented document space naviga-tion. The Concept Index relied on users to identify important concepts by marking keywords and phrases that interest them. Nakata X  X  work addressed a group of individuals who shared the same interest or a task and would profit from making use of the knowledge possessed by the group. This approach supports the hypothesis that documents have a concept that users aim for and want to retrieve. However, it is different from ours since they use collaboration among the members of a group for extracting concepts. In our approach, we try to automatically construct query concepts from a document space.

Kim and his colleagues (2000) proposed a method to automatically construct query con-cepts from typical thesauri. Their approach uses production rules to capture query concepts (or topics). Although their experiment successfully constructs concepts from thesauri based on the semantics and showed that the automatically constructed rules are more effective than hand-made rules in terms of precision, their experiments were performed on small collec-tions with a domain-specific thesaurus. In order to generate rules, they used a method to pre-specify weight values for the relationships NT (Narrow Term), BT (Broad Term) and RT (Related Term). Since thesauri usually do not provide degrees of relatedness between terms, whenever they use a different thesaurus, they need to adjust the weighting values for the relationships. It means the lack of expansibility of system. Nevertheless, their philosophy to capture user X  X  query concept has a connection with basic concept of our research.
We also consider that the Latent Semantic Indexing (LSI) and Probabilistic Latent Seman-tic Indexing (PLSI) could have similar research interests to our research. LSI (Deerwester et
The techniques for identifying the concept vectors have represented documents, terms and queries directly in the concept space. Our research has investigated that we extract primitive (basic) concepts from a document space directly and then construct query concepts by com-bining those concepts. Indeed, there is the association of ideas between their approaches and ours in that both start from an interest of concept-based representation.

For the methodology, the following is an approach similar to ours for generating primitive concepts through extracting information from document and clustering document features.
Wong and Fu (2000) tried to construct concepts through the incremental document clustering technique for extracting features, which is more suitable to Web document classification. The main difference between their approach and ours is that their construction of concepts is for
Web classification; not for constructing queries. Even if the procedure to make concepts from documents is similar, they didn X  X  offer a method to form query concepts that can be directly applied to a user X  X  query.

The goal of our research is to make query concepts that are close to users X  information needs from a document space. In the next section, we describe how to construct primitive concepts that can be used to form initial concepts from a document space. 3. Extracting concepts from a document space
We assume that there are primitive concepts (basis concepts) in a document space, and they can be used to form any concepts used in the field of information retrieval. In order to form primitive concepts, we assume that documents contain features that characterize primitive concepts. We have two steps to form primitive concepts: (1) to extract features from each process has two sub-steps: (1) to select significant sentences, (2) to partition these sentences into feature vectors. Figure 1 shows the main steps of constructing primitive concepts. In the next section, we describe each step in detail. 3.1. Extracting features from a document space
In this step, we aim to extract a set of features that are especially unique elements of each document. Let us suppose that there is an apple and someone ate it. We can explain about the apple like this:  X  X his apple tastes sweet. It X  X  red. It X  X  smaller than my fist. I think it X  X  very delicious! X  The terms,  X  X weet X ,  X  X ed X ,  X  X mall X  and  X  X elicious X  are attributes of the apple.
While  X  X weet X  and  X  X elicious X  represent a similar sense,  X  X weet X ,  X  X ed X ,  X  X mall X  are totally different properties and can not be mixed-up to form a single concept. Nevertheless, those adjectives represent an apple. In the same manner, we assume that a document consists of a set of orthogonal components. They can be composed of unit, such as a paragraph and/or sentence. The main point is that document can be represented by these orthogonal components (features). Now, we have a problem how to extract such features. To exact the features, we will perform the following steps: (1) extracting features that describe a document, (2) merging similar features among them into one feature.

In order to extract features (in the form of a vector) from a document, we adopt well-known and simple summarizing techniques (Edmundson, 1969; Lam-Adesina and Jones, 2001;
Tombros and Sanderson, 1998). The earlier research of Lam-Adesina and Jones (2001) stated that summary generation methods seek to identify document contents that convey the most  X  X mportant X  information within a document. They applied a very robust summarizer that can handle different text types likely to be encountered within a retrieval system. Their sentence extraction method for summary generation was formed by scoring the sentences in a document sentences as the summary. In our research, we also select significant sentences using Luhn X  X  keyword cluster method (1958), the title term frequency method and the location method suggested by Edmundson (1969) then generate summary for extracting document features.
After finding significant components of a document, we integrate them into orthogonal components within a document and name them as  X  X eatures X  or  X  X eature vectors X  that charac-terize the document. In order to construct feature vectors, we simply partition the significant sentences. Generated feature vectors do not contain the terms in other feature vectors of the same document. So, this processing makes feature vectors to be orthogonal to each other. This process might give rise to generate a single feature per document if the document contains only one topic. However, we observed that many documents also had several features, which means there were documents which had multiple topics within a document. 3.1.1. Selecting significant sentences (SSS)
To select significant sentences from a document, we use Luhn X  X  keyword cluster method (1958), title term frequency method and the location method (Edmundson, 1969; Tombros and Sanderson, 1998; Lam-Adesina and Jones, 2001). Luhn X  X  keyword cluster technique, though simple, is one of famous methods to produce summaries used alone or in combination with other methods. The technique of frequency analysis of words is used to determine the significance in a document. In that method, they used the most significant cluster in a sentence to measure the significance of the sentence. Luhn (1958) suggested that sentences in which the greatest number of frequently occurring distinct words are found in greatest physical proximity to each other, are likely to be important in describing the content of the document in which they occur. The significance score factor for a sentence is given by SW is the number of significant words and TW is the total number of words. To decide significant words in a document, we follow the work of Tombros and Sanderson (1997) which conclude significant term occurrence(STO), was 7; where a medium sized documents (between 25 X 40 sentences). For documents beyond the scope of medium size, reasonable term occurrence values was defined as me STO = 7 + [0 . 1  X  (25  X  NS)] for documents with NS
NS was the number of sentences in the document and STO documents with NS &gt; 40 (Tombros and Sanderson, 1997). Therefore, if one term occurs over
STO times, the term is considered as a significant term. Secondly, we score each sentence by a title term frequency. The title of an article often reveals the major subject of that document (Lam-Adesina and Jones, 2001). This hypothesis was examined in TREC documents where this attribute in scoring sentences, each constituent term in the title section is looked up in the body of the text. Thirdly, we give a location score to the first two sentences of a document for applying the location method. Edmundson (1969) stated that the location of a sentence within a document is often useful in determining its importance to the document. We assign a location score as 1/ NS where NS is the number of sentences in the document. The final score for each sentence is calculated by summing the individual score factors obtained for the above three methods.

In order to generate an appropriate length of summary, Lam-Adesina and Jones (2001) stated that it was essential to place a limit on the number of sentences to be used as summary contents. Following their suggestion, we set the lower bound of summary length to 15% of original document length and the maximum value up to six sentences, which is the reasonable number of significant sentences selected for a given document (Lam-Adesina and Jones, 2001; Tombros and Sanderson, 1998). Finally we choose the highly ranked k sentences as significant sentences. 3.1.2. Partitioning selected significant sentences into feature vectors
As mentioned before, our goal is to construct query concepts that denote users X  information needs. For this purpose, we try to find out primitive concepts (basis concepts) that can be used to form the query concepts . We treat distinct features of documents as good candidates of primitive concept. In this research, we assume that a document can have several distinct inating contents from a document, because it is possible that documents can contain several distinct topics as the form of cluster. To find out the features from a document, we group the significant sentences selected from a document (described in Section 3.1.1) into partitions such that they have distinct meanings.

Suppose that we represent a sentence as a vector of terms in the sentence with their tf weight values, and there are no stopwords in the sentence. We can consider a set of vectors
S
To make feature vectors for the document, we partition S as follows. Let us consider each vector s i as a subgraph such that the vertices of the subgraph are terms of the vector and they are connected. We then consider a feature vector as a connected component (Cormen et al., 2001) of the graph consisting of subgraphs s 1 , s connected components of the graph, feature vector for the document is orthogonal to other feature vectors. For example, there are four significant sentences for document d assume that a sentence is represented as a set of term and its weight value pairs. See Fig. 2. s = { (computer, 2.0), (information, 5.0), (software, 3.0) } s = { (hardware, 1.0), (computer, 2.0) } s = { (car, 2.0), (handle, 1.0), (signal, 3.0) } s = { (information, 5.0), (retrieval, 4.0) }
In the above example, for document d 1 , we can construct two feature sets f maximally connected component theory: f = { (computer, 2.0), (information, 5.0), (software, 3.0), (hardware, 1.0), (retrieval, 4.0) f = { (car, 2.0), (handle, 1.0), (signal, 3.0) }
We can construct feature vectors vf 1 and vf 2 corresponding to f follows. We can see that vf 1 and vf 2 are orthogonal.
 vf 1 (0.0 2.0 0.0 1.0 5.0 4.0 0.0 3.0) vf 2 (2.0 0.0 1.0 0.0 0.0 0.0 3.0 0.0) 3.2. Clustering feature vectors into primitive concepts
In the previous section, we construct the feature vectors for each document. The feature vectors in a document are orthogonal to each other, but they might not be orthogonal to the feature vectors from other documents. See Fig. 3. The Relation ( f relation between f 1 and f 2 is a definitely orthogonal. However, we can not guarantee that the Relation ( f 1 , f 3 ) is always orthogonal. For example, we assume that both of document d and d 2 are about  X  X he future of Computer Industry X  and the feature f  X  X etrieval X  as described in Section 3.1. If the feature f terms such as  X  X omputer X ,  X  X ardware X ,  X  X ommunication X  and  X  X nformation X , the feature f f are quite similar to each other. Therefore, we could not consider all the feature vectors as good candidates of primitive concepts.

To alleviate this problem, we cluster the feature vectors such that the centroid vectors of clusters are approximately orthogonal to each other. We call the constructed centroid vectors  X  X rimitive concepts X . The clustering method is generally used for descriptive modeling in data mining. Mannila (2002) stated that data mining is the analysis of (often large) observational data sets to find unsuspected relationships and to summarize the data in novel ways that are both understandable and useful to the data analyst. If we assume that document collection is a large observational data set which has latent meanings, we can find those concepts through global modeling and local pattern discovery. In previous section, we have already identified features of each document using summarization locally. In this step, we are going to try to cluster features of the whole collection of documents globally.

Many effective clustering algorithms are available (Pelleg and Moore, 2000; Quaresma and Rodrigues, 2000; Willett, 1988; Wong and Fu, 2000; Zhang et al., 1996). In order to cluster feature vectors into primitive concepts, we first selected two popular methods from already-developed methods; for instance, K -means and X -means clustering methods (Pelleg et al., 2000). However, they did not work well in our case. We conjectured that the reason why the previous clustering methods did not work well is that our feature vectors are extremely sparse. Therefore, we develop a simple clustering method to establish the primitive concepts as described in Table 1. In our clustering method, if more than u % (e.g., 80%) terms in a feature vector f i are contained in the centroid vector of a cluster C recompute the centroid vector of C j such that C j = ( C j and v % (e.g., 20%) of terms in f i are contained in C j , then we ignore f terms in f i are contained in C j , we keep trying to put f to any clusters, we create a new cluster that contains f i pass and reallocation method which were used in early work in cluster analysis in IR (Frakes and Baeza-Yates, 1992).

After the first step, we get m clusters. The centroids of m clusters are to be the vectors for the newly generated primitive concepts. Since the proposed algorithm ignores some feature vectors which did not belong to any clusters, we need to consider these missing features to be assigned to the generated clusters. Moreover, we should evaluate the quality of the generated clusters by analyzing whether features could belong to other clusters. For this purpose, we apply the above algorithm to the feature vectors once again. In this step, we compare the feature vectors ( f 1 ,..., f n ) with the generated centroid vectors ( C
This reallocation process is operated by selecting some initial partition of the feature vectors and then moving the features from cluster to cluster to obtain an improved partition. Before we reassign features to the generated centroid vectors, we sort the generated centroid vectors by the number of terms in each cluster. See Fig. 4. Since the generated concept vectors are not exactly but approximately orthogonal, the feature vectors could be assigned to a large cluster in the reallocation step. Hence, we sort them out by the number of terms in each cluster, and then apply the above algorithm to clustering the feature vectors again (reallocation). 4. Information retrieval using primitive concepts
For a query reformulation based on query concepts , we select its most associated primitive concepts with the initial query and generate all possible interpretations of the query. The query during the reformulation process. These query concepts are constructed by combining primitive concept vectors. For this purpose, we consider the rule to formulate query concept vector (in the form of a vector). The construction method is as follows (see Fig. 5). 1. Select first top N primitive concept vectors those are similar to the initial query q cosine similarity.
 2. Generate all possible combination of primitive concepts under a DNF (Disjunctive Normal
Form) with at most three primitive concept vectors among the ten selected primitive concept vectors. These are called candidates query concepts . 3. Choose the DNF that is most similar to the initial query q
The selected DNF is called QC best . 4. Construct the enhanced query q =  X  q 0 +  X  QC best , where 0 the weighting constants.

We use MAX operation for OR (  X  ) operation in the DNFs that are generated in step (2) above. For example, let the selected primitive concepts be C and the initial query be q 0 . We can generate all possible DNFs as follows:
From all possible DNFs, we select one that is most similar to q
Suppose that C 1  X  C 3 is the QC best that is the most similar to the initial query q construct the enhanced query q =  X  q 0 +  X  ( C 1  X  C 3 ). 5. Experiments
For the evaluation of proposed methods, we conducted experiments on the Associated Press (AP) subset of TREC collection; disk 1, 2 and 3 (Harman 1995, Hawking et al. 1999). The AP dataset in directories  X 88 X ,  X 89 X  and  X 90 X  of TREC collection totally contains about 240,000 documents. 50 topics (topic 101 X 150) were chosen to evaluate the performance. For our investigation, title and description fields of the topics were chosen since we assumed that documents and queries have multiple concepts. When we used only a few words in the title field of topics, it was not enough to represent concepts in query topics.

We removed stopwords and used Porter X  X  algorithm (Porter, 1980) for stemming. Now, we describe our experimental results in a series of comparisons as follows. Basically, we conduct baseline retrieval, PRF and our Query Concept Method (QCM) respectively. 5.1. Baseline and PRF methods
We conducted a baseline based on BM25 Probabilistic Model for term weighting (Robertson et al., 1994). We calculated the weight of terms in document d where tf is term frequency, w (1) is the weight based on the basic probabilistic model, N is the total number of documents, n is the total number of documents containing term k , R is the total number of relevant documents for the query, r is the number of relevant documents containing term k and K is the normalized document length. We also set the default parameters k , b , k 3 used in BM functions as follows. k 1 = 1 . 2 , b 1994).

Secondly, we chose the Rochhio X  X  Relevance Feedback methods (1971) to examine PRF approach. We used a Standard Rocchio formulation to calculate the modified query as follows,
Notice that D r and D n stand for the sets of relevant and non-relevant documents (among the retrieved ones) according to the user judgment, respectively. constants. We set  X  =  X  = 1 , X  = 0 which yields a positive feedback strategy (Baeza-Yates and Ribeiro-Neto, 1999). Since PRF is a kind of PRF, we assumed that highly ranked p retrieved documents were relevant and took q terms among term pool set for term ranking in the query expansion process. We assumed that p = 15 and q ranking criteria was the Robertson selection value ( rsv ) (Robertson, 1990). The rsv is defined as, where r ( i ) is again the number of relevance documents containing term i , and rw ( i ) is the standard Robertson/Spark Jones relevance weight. rw ( i ) is defined as, documents for the query, and N is the total number of documents.
 Thirdly, we experimented with variation of baseline and PRF by summary information.
This summary file was generated during the extraction of feature vectors (see Sections 3.1 and 3.2). When we selected the significant sentences from each document and created new summary files. We labeled these approaches as  X  X ummary Baseline X  and  X  X ummary PRF X  respectively. We conducted these alternative experiments to examine the effectiveness of summary information. Unfortunately, when we retrieved documents using summary infor-mation only, the results of  X  X ummary Baseline X  were not successful.  X  X ummary RRF X  was equally hard to achieve good results since  X  X ummary Baseline X  has already missed much information of documents during summarization process. However, Lam-Adesina and Jones (2001) showed that query expansion using document summaries could be considerably more effective than using full-document expansion. Their research has reported an investigation into the use of document summarization for term-selection in pseudo relevance feedback.
Table 2 shows the retrieval performances of baseline, PRF,  X  X ummary Baseline X ,  X  X um-mary PRF X . We evaluate the results based on precisions at 5, 10, 15, 20, 30 and 100 docu-ments and Mean Average Precision (MAP). As we mentioned in Section 2, PRF approach might have some problems on selecting terms that are unrelated to relevance and happen to appear in documents that meet the selection criteria (Baeza-Yates and Ribeiro-Neto, 1999).
We surmise that an initial query could not provide appropriate terms to expand. As a result, the direction of blind relevance feedback performed unsuccessfully. 5.2. Query concept methods (QCM)
Finally, we conducted Query Concept Method (QCM). Generally, our methods showed better results than those of previous methods. We used the controlled parameter u for clustering and five different sets of  X  and  X  . Since the modified query is generated by q =  X  q
Table 3 shows the comparison between baseline and our methods on the retrieval perfor-mances and statistical improvement (percentage change). Rel is the total number of relevant documents over all queries and Rret is the total number of relevant documents retrieved over all queries. The evaluation includes recall/precision averages at 0.0 to 1.0, MAP, pre-cision at 5 to 1000 documents cutoff and R-precision (R-Pr) which denote a precision after
Rel documents. We observe that the QCM runs consistently outperform baseline run for all queries. The result of QCM3 (  X  = 0 . 5 , X  = 0 . 5) shows 13% improvement on MAP (0.1387 vs. 0.1568). The result of QCM2 (  X  = 0 . 6 , X  = 0 . 4) shows about 13.3% improvement on R-precision (0.1833 vs. 0.2077).
 Figure 6 shows the retrieval precision curves of Baseline, PRF, Summary variations and
QCM methods (from  X  = 0 . 8 , X  = 0 . 2to  X  = 0 . 2 , X  = 0 methods perform consistently well and all of the average precision of QCM methods are better than those of other methods. 5.3. Query Concept Method (QCM) on TREC 8
We experimented on TREC 8 collection for evaluating our proposed method in a large about 520,000 documents distributed on two CD-ROM disks (TREC disks 4 and 5) taken from the following sources: Federal Register (FR), Financial Times (FT), Foreign Broadcast
Information Service (FBIS) and LA Times (LAT). 50 topics (topic 410 X 450) were chosen to evaluate the performance. The title and descrip-tion fields of the topics were selected for constructing initial queries. Around 500 stopwords were removed and Porter X  X  algorithm (Porter, 1980) was used for stemming. We conducted baseline, PRF and QCM runs based on a vector space model. Luhn X  X  keyword cluster method (1958), the title term frequency method (Tombros, 1998) and the location method suggested by Edmundson (1969) were used in the process of extracting features from each document.
The controlled parameter u = 0 . 8 and v = 0 . 2 were used in the clustering procedure. Table 4 shows recall/precision at 0.0 to 1.0, MAP, precision at 5 to 1000 documents cutoff and
R-precision of the baseline, PRF and QCM method on TREC 8 collection. In this case, we only show the best results of QCM at  X  = 0 . 8 and  X  = 2. We observe that the result of RRF is slightly improved on MAP and R-precision. Unfortunately, the results of QCM can not outperform baseline but are just as much as baseline. Although QCM could not achieve better results on TREC 8 collection, our proposed method was valuable for a query reformulation in case of poorly performing queries, which means that initially retrieved documents could not satisfy users (Chang et al., 2004). 6. Conclusion
This paper has proposed a new paradigm for automatically enhancing initial queries. In the proposed approach, we constructed query concepts that denote users X  information needs.
We suggested a framework to construct the query concepts, which extracted features from a document space and clustered them into primitive concepts that are basis elements of the query concepts . With the constructed concepts, we have shown promising experimental results. For the improvement of performance, we think that we could adopt more robust summarization methods and/or clustering methods proper to construct primitive concepts. However, since another research topic. Our future work is also to consider many other directions not only for extracting features for a document, but also for clustering features and constructing primitive concepts, since our ultimate object is to make query concepts suitable for user X  X  information need.
 References
