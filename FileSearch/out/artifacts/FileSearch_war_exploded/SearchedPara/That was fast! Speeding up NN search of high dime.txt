 Emanuele Coviello ecoviell@ucsd.edu Adeel Mumtaz adeelmumtaz@gmail.com City University of Hong Kong, Tat Chee Avenue, Kowloon, Hong Kong Antoni B. Chan abchan@cityu.edu.hk City University of Hong Kong, Tat Chee Avenue, Kowloon, Hong Kong Gert R.G. Lanckriet gert@ece.ucsd.edu Nearest neighbor (NN) search is a core routine in many applications of machine learning and information re-trieval. Formally, given a database X , a query q and a dissimilarity d , the problem consists of finding the x  X  X for which d ( x,q ) is minimum. In this work we are particularly interested in the case where the data points represent probability distributions, and the dissimilarity measure is the Kullback-Leibler (KL) divergence (relative entropy). KL has been exten-sively used to compare generative models of docu-ments, e.g., in text analysis (Pereira et al., 1993; Blei &amp; Lafferty, 2007) and content-based image retrieval (Puzicha et al., 1999; Rasiwasia et al., 2007), where documents are often modeled using histograms, as well as in computer vision for video classification (Chan &amp; Vasconcelos, 2005), where videos are modeled as linear dynamical systems.
 Brute-force search is often prohibitive on large datasets. As a consequence, a large body of work, starting from KD-trees (Friedman et al., 1977) and metric-ball trees (Omohundro, 1989; Uhlmann, 1991; Moore, 2000), has investigated the use of spatial data structures to accelerate search. These consist of a hierarchical space decomposition based on geometric properties and database X  X  statistics, that enables fast search by pruning out portions of the search space via a branch and bound exploration. Cayton (2008) ex-tends these ideas to the class of Bregman divergences, of which KL is a member. In particular, Bregman Ball trees (bbtrees) are defined in terms of Bregman balls, and search uses bounds on the Bregman divergence from a query to a Bregman ball. Succesively, Nielsen et al. (2009a; 2009b) developed an extension of the bbtree to symmetrized Bregman divergences, Zhang et al. (2009) adapted the VA-file and R-tree to decom-posable Bregman divergence, and Cayton (2009) used the bbtree for efficient range search. Abdullah et al. (2012) formally analyze approximated Bregman NN search. These accelerated search methods are not im-mediately practical in high dimensions (Moore, 2000; Cayton, 2008), where the number of close neighbors is often very large, and consequently the procedure needs to run on many nodes, which results in several additional divergence computations and a total com-putation time that may become comparable or worse than brute force.
 The pruning operation of (Cayton, 2008) requires pro-jecting the query onto the shell of a Bregman ball, which is found using a bisection search. However, this has 2 limitations: 1) the procedure is not amenable to distributions outside the exponential family (e.g., la-tent variable models); 2) two divergence calculations are required for each iteration of the bisection search, which leads to slow performance when the divergence calculation is computationally intensive (e.g., in high dimensions, or latent variable models).
 To do more efficient NN search, Bbtrees naturally han-dle approximate NN operations, by stopping search early after a fixed budget of leaves has been visited (Cayton, 2008). This increases efficiency without an excessive degradation of quality.
 In this paper we propose a novel branch and bound method based on variational approximations. The cor-responding bisection procedure has a small computa-tional overhead, since it requires only one divergence computation at each backtracked node , independent of the number of iterations, and then needs to evaluate only scalar functions at each iteration. The algorithm provides a speedup over (Cayton, 2008) on medium and high-dimensionality, and over brute force search at each dimensionality, and returns almost perfect NNs. In addition, our algorithm readily serves search of la-tent variable distributions.
 Other data structures for approximated NN search rely on mapping techniques, for example locality sensitive hashing (reviewed in (Slaney &amp; Casey, 2008)) adapted to non-metric dissimilarities by (Mu &amp; Yan, 2010). In Section 2 we overview Bregman and KL divergences. In Section 3 we discuss Cayton X  X  (2008) NN search with bbtrees, which lays the basis for our novel contri-bution, in Section 4. Experiments on histogram data and linear dynamical systems are reported in Sections 5 and 6, and conclusions are drawn in Section 7. This section provides background on Bregman and KL divergence and describes some related properties. 2.1. Bregman divergences Given a strictly convex differentiable function f (  X  ), the Bregman divergence based on f is d f ( x,y ) = f ( x )  X  f ( y )  X  X  X  X  f ( y ) ,x  X  y  X  . A Bregman divergence d f ( x,y ) is convex in x , and the Bregman ball of radius R around  X  is a convex set. The interested reader may refer to (Cayton, 2008; Banerjee et al., 2005) for more details. Let f  X  be the dual function of f . 1 Since f is strictly convex, f  X  is strictly convex as well, and f and f  X  are Legendre dual of each other (Rockafellar, 1996). One important property is that the gradient  X  f defines a bijective mapping, and the inverse mapping is given by the gradient of the dual function f  X  , i.e.: Consistently with the exposition in (Cayton, 2008), we use prime (i.e., 0 ) to denote the results of applying  X  f . 2.2. Regular exponential families, regular Given a pair of random variables with p.d.f.s X and Y , respectively, their KL divergence is the functional D ( X||Y ) = The KL divergence is never negative, and is zero iff X = Y (a.e.).
 There is an interesting correspondence between Breg-man divergences and KL divergence, which occurs when we deal with exponential family distributions (Banerjee et al., 2005). In particular, for any regular Bregman divergence d f based on f (  X  ), there is associ-ated a regular exponential family of distributions where  X  (  X  ) are the sufficient statistics,  X  0 are the canon-ical parameters, and the dual function f  X  is the parti-tion function. The gradient operation  X  f  X  maps from canonical to mean parameters  X  (Wainwright &amp; Jor-dan, 2008). f (  X  ) is the negative entropy and its gradi-ent  X  f maps  X  to the canonical parameters  X  0 (Wain-wright &amp; Jordan, 2008). d f is the KL divergences be-tween members of F f  X  .
 Hence, when X and Y are distributions from the same exponential family with mean parameters x and y re-spectively, we have that where the l.h.s is a functional of two p.d.f.s and the r.h.s. is a function of the mean parameters. In this section we briefly review the branch and bound method for Bregman ball trees by Cayton (2008). 3.1. BB trees Bregman ball trees and the associated search rou-tine follow the same principles of KD trees (Friedman et al., 1977) and metric ball trees (Omohundro, 1989; Uhlmann, 1991; Moore, 2000), with the difference that they are based on Bregman balls instead of rectangular cells or metric balls. A bbtree consists of a binary tree that partitions a database X = { x 1 ,...,x n } . Every node i is associated with a subset of points X i  X  X , and defines a Bregman ball with center  X  i and radius R i such that  X  x  X  X i : x  X  B (  X  i ,R i ). Each non-leaf node i is associated with left and right child nodes, l and r , and X i is consequently split between X l and X r The entirety of leaf nodes covers X . A Bregman Ball tree can be grown from the database X in a top-down fashion, recursively using (Banerjee et al., 2005). 3.2. Searching with BB trees Given a query q , we are interested in its left-NN 2 Search with a BB tree proceeds as follow. The al-gorithm initially descends the BB tree, starting from the root. At every non-leaf node the algorithm de-scends through the most promising child node and temporarily ignores the sibling node. Once the algo-rithm reaches a leaf node X i , it selects the candidate nearest neighbor x c = arg min x  X  X At this point, the algorithm backtracks, and explores an originally ignored sibling j if where the right side of (7) is the Bregman projection of q onto the Bregman ball B (  X  j ,R j ) (see Figure 1(a)). Cayton (2008) proves that the optimal x p of (7) corre-sponds to an x p 0 =  X  f ( x p ) along the line  X  X  0 +(1  X   X  ) q 0  X   X   X  1, 3 lays on the shell of B (  X  j ,R j ) and can be found by bisection search over  X  .
 At step i of the bisection search, given  X  i , a point x and the corresponding x  X  This in fact consists of computing a left-sided centroid (Nielsen &amp; Nock, 2009) 3.3. Bregman projection vs. stopping early Since exact evaluation of the Bregman projection (7) is not needed, Cayton (2008) derives stopping conditions based on upper and lower bounds Upper and lower bounds are computed at each itera-tion of the bisection, until either d ( x c ,q ) &lt; a (prune the node) or d ( x c ,q ) &gt; A (explore the node). The lower bound a is given by weak duality: where L (  X  ) is the Lagrangian of the right side of (7) and 0  X   X   X  1. The upper bound A comes directly from the primal problem: if x  X   X  B (  X  j ,R j )  X  d f ( x  X  ,q )  X  min Hence the algorithm operates bisection search on  X  as follows (see Figure 1(b)). At the i -th step, given  X  , the algorithm computes the left-sided centroid x d ( x  X  Algorithm 1 CanPrune by Cayton (2008) neither bound holds, the algorithm continues the bisec-tion. The procedure from (Cayton, 2008) is reported in Algorithm 1 for the reader X  X  convenience. Note that in each iteration of CanPrune , two divergences are cal-culated in Step 4. In this section we present a novel and approximated bisection search based on variational inequalities. The proposed method requires only one divergence oper-ation at each backtracked node (independent of the number of iterations), is efficient in high dimensions, and can be applied to latent variable models. The ef-fect of the approximation is discussed in Section 4.4. 4.1. Overview and notation In this section we assume that d f is a regular Bregman divergence (i.e., KL divergence for an exponential fam-ily). Let M , Q , X c be the distributions corresponding, respectively, to the node, the query, and the candidate nearest neighbor. Their mean parameters are, respec-tively,  X  , q and x c . Define a mixture distribution of components M and Q with weights  X  and 1  X   X  : Our algorithm works as illustrated in Figure 1(c). In-stead of explicitly computing the left-sided centroid x  X  (Step 3 of CanPrune , Figure 1(b)) at each recursive it-eration, it uses a mixture model  X  (equation (13)). This allows to approximate d f ( x  X  , X  ) and d f ( x with lower bounds to D ( X  ||M ) and D ( X  ||Q ). The bounds depend only on d f (  X ,q ) and d f ( q, X  ) (which are fixed), and on the mixing parameter  X  . As a con-sequence, bisection search only requires evaluating of simple scalar functions (as opposed to divergences). 4.2. Variational lower bounds Our algorithm builds on the variational approximation to the KL divergence between mixtures by Hershey and Olsen (2007), 4 and on the observation that the approximation holds as an inequality whenever one of the two terms is unimodal. 4.2.1. Variational approximation to the KL Let A = {  X  i , A i } be a mixture with weights  X  i and components A i . Similarly, let B = {  X  j , B j } be a dif-ferent mixture. Assume the mixture components A i and B j belong to some family for which we can com-pute the KL divergence. Consider the KL divergence between A and B : D ( A||B ) = E A [log Hershey and Olsen (2007) derive a lower bound to the expected log-likelihood terms on the right-hand side of (14), and in turn an approximation (as the difference of two lower bounds) to the KL: 4.2.2. Variational lower bound to KL Assume that A =  X  =  X  M + (1  X   X  ) Q is a mixture of two components, and that B consists of a single component, and consider their KL divergence The first term cannot be computed exactly, but can be lower bounded (Hershey &amp; Olsen, 2007). The second term can be computed exactly since it only involves ex-pected log-likelihoods of individual components. Con-sequently, the variational approximation to D ( X  ||B ) holds as a lower bound . Dealing with lower bounds (as opposed to approximations) allows to characterize the errors made by Algorithm 2 (see Section 4.4).
 Lower bounds to D ( X  ||M ) and D ( X  ||Q ) are easily de-rived (Coviello et al., 2013): ` m =  X  log [  X  + (1  X   X  ) exp { X  D ( M||Q ) } ] ` q =  X  log [  X  exp { D ( M||Q ) } + (1  X   X  )] 4.2.3. Monotonicity When D ( Q||M ) is very close to zero, ` m is not always monotonic (see (Coviello et al., 2013) for an illustra-tion). This would make the bound useless for bisec-tion search. Luckily, we can easily derive a monotonic bound ` + q (  X  ) = max(0 ,` q (  X  )), i.e., by setting ` when it is not informative (i.e., when it is negative). Lemma 4.1 ` + m (  X  ) = max(0 ,` m (  X  )) is monotonic in [0 1] .
 Proof ` m (  X  ) is convex in [0 1]. 5 As a consequence of convexity, ` m (  X  ) can cross zero at most twice, and in particular at most twice in [0 1].
 We have that ` m (1) = 0 and ` m (0) = D ( Q||M )  X  0. If ` m (  X  ) crosses zero only once (i.e., at  X  = 1), then ` (  X  ) is monotonic for  X   X  [0 1]. If ` m (  X  ) crosses zero also at  X   X   X  (0 1), we have that ` m (  X  ) is monotonic for  X   X  [0  X   X  ] (and until it reaches its minimum). 6 The result follows.
 Similarly, we can define the lower bound ` + q (  X  ) = max(0 ,` q (  X  )) (which is monotonic as well). 4.3. Approximated pruning algorithm Our algorithm is based on the quantities: ` (  X  ) , ` + m (  X  ) , ` L (  X  )  X  ` + q (  X  ) + The algorithm performs bisection search over  X  , and attempts to locate the  X  for which l + m (  X  ) = R , using Algorithm 2 CanPruneApprox early stopping. As a lower bound we use ` L (  X  ) instead of L (  X  ). We set the upper bound to ` + q (  X  ) whenever ` (  X  ) &lt; R . 7 At each iteration i , the algorithm computes the quan-Else, if ` + m (  X  i ) &lt; R , it updates the upper bound, and bound holds, the algorithm continues the bisection. The procedure is summarized in Algorithm 2. 4.4. Discussion The proposed algorithm executes faster on a node than Cayton X  X  original algorithm. In particular, at every recursive iteration the latter requires updating the centroid x  X  plus two divergence operations (i.e., computing d f ( x  X  ,q ) and d f ( x  X  , X  )). On high dimen-sional data, this results in a significant overhead, since individual divergence operations are expensive, and, most importantly, the procedure needs to be executed on a large fraction of the nodes (due to the large number of close neighbors in high dimensions (Moore, 2000)). Not surprisingly, Algorithm 1 did not work well for exact NN search on high dimensional data (Cayton, 2008), where it registered slow-downs (in-stead of speedups) relative to brute force search. On the other hand, our method drastically reduces the amount of computation per backtracked node to a sin-gle divergence operation, independently of the number of iterations in the bisection search. At each recursive call of Algorithm 2, ` + m (  X  ) and ` + q (  X  ) are functions of the scalar  X  and of the fixed quantities d f ( q, X  ) and d (  X ,q ). Since d f ( q, X  ) is already computed by the search routine when descending the tree (for choosing between a node X  X  left and right child), only one addi-tional divergence needs to be computed. We expect this to give our algorithm an edge for efficiency on high-dimensional data.
 The solution proposed by Cayton (2008) to speed up retrieval time uses the bbtree for approximate search, by fixing a maximum budget of leaves that can be explored for each query, after which backtracking is stopped. This is suboptimal, since the approximation is independent of the query and may blindly ignore (once the budget is depleted) promising portions of the search space only because they appear later in the backtracking order. Our approximation, on the other hand, only depends on the parameters of the query and of the nodes of the tree (as opposed to a fixed leaf budget), and the resulting backtracking will adapt better to individual queries and the local structure of the tree, as illustrated below.
 In fact, we can argue that Algorithm 2 tends to be over-explorative on nodes close to the query, and under-explorative on nodes further away. Since we are using in sequence an approximation and a lower bound, in general (19) (20) are not bounds to d f ( x  X  ,q ) and d ( x  X  , X  ). However, as shown by the next claim, when the query and the node have very close distributions, (19) and (20) hold as lower bounds.
 Claim 4.2 If q 0 =  X  0 +  X  0 , for  X  0 small, we have d ( x  X  , X  )  X  l + m (  X  ) (similarly, d f ( x  X  ,q )  X  ` + Proof We have x 0  X  =  X  0 + (1  X   X  )  X  0 and  X  0 = x 0  X   X  (1  X   X  )  X  0 . Consider the Riemannian manifold around x (with curvature  X  f  X  ( x 0  X  )), and that the Riemannian metrics associated to f and f  X  have identical infinites-imal length (Amari, 2009; Nielsen &amp; Nock, 2009). Con-sequently we have: 8 where we use the notation  X  = 1 2  X  t  X  2 f ( x  X  )  X  to reduce clutter. Similarly, we have that d f ( q, X  ) = 1 2  X  and d (  X ,q ) = 1 2  X . Using the approximations exp { a } = 1 + a and log(1 + a ) = a (for | a | small), we have that: where (24) follow from Jensen inequality and (26) form the fact that  X  (1  X   X  )  X  0 for  X   X  [0 1]. Since d ( x  X  , X  )  X  0 we also have d f ( x  X  , X  )  X  ` + m (  X  ). In this case, the decision whether to prune a node is based on a looser lower bound (i.e., ` L (  X  )  X  L (  X  ) in-stead of L (  X  ) in Algorithm 1), and Algorithm 2 will consequently prune less frequently . Similarly, check-ing the condition ` + m (  X  ) &lt; R results in inflating the ball around the node, and may determine more explo-rations .
 On the opposite, if q is far from  X  , (19) and (20) may actually hold as upper bounds, making the backtrack-ing under-explorative . 9 This has the effect that our algorithm better accounts for the sparsity of data rel-ative to the dimensionality, reducing the backtrack-ing for high-dimensional data. Skipping a large region of space centered far away form the query determines computational savings, but does not affect NN perfor-mance too much. Even if the region could potentially contain points very close to query, it most likely does not, since the points it contains are very few (relative to the dimensionality).
 Our results in Section 5 demonstrate that, on moderate-and high-dimensional data, our algorithm is very efficient and returns almost perfect nearest neigh-bors. In addition, the approximation does not require empirically tuning any meta-parameter (such as the leaf budget in (Cayton, 2008)).
 Interestingly, since our algorithm requires only the computation of divergence terms (and bypasses the computation of the centroid), it is by no means lim-ited to the exponential family. In fact, it can be readily applied to NN-search of any family of distributions, 10 as long as the divergence between individual members can be computed (efficiently). In Section 6 we illus-trate this for time series models with latent variables. Algorithm 1, on the other hand, cannot be adapted to latent variable models in an efficient way. In par-ticular, the left-sided centroid x  X  cannot be computed analytically, since the hidden-state bases correspond-ing to the query and the node (i.e., Q and M ) may be We first consider experiments on histogram data, where we can directly compare our proposed method against Cayton X  X  (2008) exact and approximate NN search. 13 We consider the 9 histogram datasets from (Cayton, 2008) listed in Table 1, most of which are fairly high dimensional. We refer to our algorithm as Variational , Cayton X  X  exact and approximated search as Cayton and CaytonApprox , respectively, and brute force search as Brute .
 Performance is measured in terms of speedup relative to brute force search, and by the average number of elements in the database that are closer to the query than the returned one (NC) (Cayton, 2008). For ex-act algorithms (e.g., Cayton and Brute ), NC = 0 always. All results are averages over queries not in the database.
 In Table 1 we compare performance of Variational to Cayton . In general, on moderate to high dimen-sional data ( d  X  32), our algorithm provides larger speedups than Cayton . The gain in computational efficiency has a very modest effect on the quality of Variational , as demonstrated by the low NC values. In addition, whereas Cayton is slower than brute force search on the two datasets of highest dimensionality ( d = 371 and d = 1111), Variational always reg-isters a speedup. In particular, on  X  X ift signatures X , Variational provides a substantial 6  X  speedup over brute force search, with an NC of 0 . 0784. Further examining the NC values, Variational finds the NN 95.7% of the time, the 2nd NN 2.6%, and 2+ NN 1.7% (with a low average approximation ratio of = 0 . 3). 14 In Figure 2 we compare Variational to CaytonApprox , on three datasets of medium to high dimensionality. For both methods, we set a fixed budget  X  d of divergence computations allowed per query, after which backtracking stops. 15 In general, Variational is superior to CaytonApprox , achieving more effective speedups and lower NC values X  X ote that in Figure 2(c) the left-most point for CaytonApprox (dashed red line) corresponds to no speedup, and is hence of no practical in-terest. In particular, the threshold-free version of Variational (from Table 1 and circled in Figure 2) does not require setting any meta-parameter and is efficient also on high dimensional data. These results show that, by adapting backtracking to the query and the local structure of the tree, our approx-imation effectively explores promising regions of the search space and achieves computational savings by under-exploring less promising ones. In this section we test our algorithm on NN search of dynamic textures (DTs) (Doretto et al., 2003), which have been used to model video (Coviello et al., 2012; Mumtaz et al., 2012) and music (Barrington et al., 2010; Coviello et al., 2011).
 A DT model represents a n -dimensional time series of length T , y 1 ,...,y T , as generated by a linear dynam-ical system (LDS): where S t  X  R m is a lower-dimensional hidden state process ( m &lt; n ) that governs the dynamics, A is a m  X  m transition matrix, C a n  X  m bases matrix, and V t and W t are Gaussian noise processes. The KL di-vergence between two DTs can be computed efficiently with a recursion (Chan &amp; Vasconcelos, 2005), so we can readily use our branch and bound search.
 We consider the 4 datasets used by Coviello et al. (2012), three of video and one of music. A dataset is first divided into training/test splits, and a database of DTs X is compiled from the training set, with each DT modeling a portion of a video (or song). The database (of DTs) is then hierarchically organized in a tree structure using the Bag-of-Systems (BoS) Tree from (Coviello et al., 2012), which produces balls around each DT-node M , compact in terms of D (  X ||M ). Fi-nally, we model portions of test videos (or songs) as DTs, and for each we find its NN in X using the BoS Tree in tandem with our backtracking algorithm. Videos are preprocessed into a dense sampling of spatio-temporal cubes of pixels (as in (Coviello et al., 2012)). Each training video is then modeled as a dy-namic texture mixture (DTM) with 4 components, and the databases X are formed by selecting all the video-level DT components. For each test video, we com-pute a query-DT from each cube, using (Doretto et al., 2003). Songs are first represented as a dense sampling of sequences of low-level features (as in (Coviello et al., 2012)). Each training song is modeled as a DTM with 4 components, and all the song-level DTs form the database X . For each experiment we compute average speedup over Brute and NC, using the same cross val-idation splits as (Coviello et al., 2012). Results on the video and music datasets are in Table 2. Note that in average Variational returns good answers. For example, on the CAL500 music data, where we regis-tered the highest NC value, Variational still returns in average answers in the top 0 . 35%. We have presented a branch and bound algorithm based on variational approximations. We have shown nearly perfect and efficient NN-search on histogram data of challenging dimensionality, as well as applica-bility to more complex generative time series models. Acknowledgements The authors thank Lawrence Cayton for providing code and data from (Cayton, 2008), and Malcolm Slaney for helpful discussion. E.C., A.B.C. and G.R.G.L. acknowledge support from Google, Inc. E.C. and G.R.G.L. acknowledge support from Yahoo!, Inc., the Sloan Foundation, KETI under the PHTM pro-gram, and NSF Grants CCF-0830535 and IIS-1054960. A.M. and A.B.C. were supported by the Research Grants Council of the Hong Kong Special Adminis-trative Region, China (CityU 110610).
 Abdullah, A., Moeller, J., and Venkatasubramanian,
S. Approximate Bregman near neighbors in sub-linear time: Beyond the triangle inequality. SCG , 2012.
 Amari, S. Information geometry and its applications: convex function and dually flat manifold. Emerging Trends in Visual Computing , 2009.
 Banerjee, A., Merugu, S., Dhillon, I.S., and Ghosh, J. Clustering with Bregman divergences. JMLR , 2005. Barrington, L., Chan, A.B., and Lanckriet, G. Mod-eling music as a dynamic texture. IEEE TASLP , 2010.
 Blei, D.M. and Lafferty, J.D. A correlated topic model of science. The Annals of Applied Statistics , 2007. Cayton, L. Fast nearest neighbor retrieval for bregman divergences. In ICML , 2008.
 Cayton, L. Efficient Bregman range search. NIPS , 2009.
 Chan, Antoni B. and Vasconcelos, Nuno. Probabilistic kernels for the classification of auto-regressive visual processes. In IEEE CVRP , 2005.
 Coviello, E., Chan, A.B., and Lanckriet, G.R.G. Time series models for semantic music annotation. IEEE TASLP , 2011.
 Coviello, E., Mumtaz, A., Chan, A.B., and Lanckriet,
G.R.G. Growing a Bag of Systems Tree for fast and accurate classification. In IEEE CVPR , 2012. Coviello, E., Mumtaz, A., Chan, A.B., and Lanckriet, G.R.G. Supplement to  X  X hat was fast! Speeding up NN search of high dimensional distributions X . 2013. Doretto, G., Chiuso, A., Wu, Y. N., and Soatto, S. Dynamic textures. Intl. J. Computer Vision , 2003. Friedman, J.H., Bentley, J.L., and Finkel, R.A. An algorithm for finding best matches in logarithmic expected time. ACM TOMS , 1977.
 Hershey, J.R. and Olsen, P.A. Approximating the
Kullback Leibler divergence between Gaussian mix-ture models. In IEEE ICASSP , 2007.
 Moore, A.W. The anchors hierarchy: Using the trian-gle inequality to survive high dimensional data. In UAI , 2000.
 Mu, Y. and Yan, S. Non-metric locality-sensitive hash-ing. In AAAI CAI , 2010.
 Mumtaz, A., Coviello, E., Lanckriet, G., and Chan, A. Clustering Dynamic Textures with the Hierarchical
EM Algorithm for Modeling Video. IEEE TPAMI , 2012.
 Nielsen, F. and Nock, R. Sided and symmetrized Breg-man centroids. IEEE Transactions on IT , 2009. Nielsen, F., Piro, P., and Barlaud, M. Bregman vantage point trees for efficient nearest neighbor queries. In IEEE ICME , 2009a.
 Nielsen, F., Piro, P., Barlaud, M., et al. Tailored Breg-man ball trees for effective nearest neighbors. In EuroCG , 2009b.
 Omohundro, S.M. Five balltree construction algo-rithms . International Computer Science Institute, 1989.
 Pereira, F., Tishby, N., and Lee, L. Distributional clus-tering of English words. In Association for Compu-tational Linguistics , 1993.
 Puzicha, J., Buhmann, J.M., Rubner, Y., and Tomasi,
C. Empirical evaluation of dissimilarity measures for color and texture. In IEEE ICCV , 1999.
 Rasiwasia, N., Moreno, P.J., and Vasconcelos, N. Bridging the gap: Query by semantic example. IEEE Transactions on Multimedia , 2007.
 Rockafellar, R.T. Convex analysis . Princeton univer-sity press, 1996.
 Slaney, M. and Casey, M. Locality-sensitive hashing for finding nearest neighbors. IEEE, SPM , 2008. Uhlmann, J.K. Satisfying general proximity/similarity queries with metric trees. IP letters , 1991.
 Wainwright, M.J. and Jordan, M.I. Graphical mod-els, exponential families, and variational inference. Foundations and Trends in Machine Learning , 2008. Zhang, Z., Ooi, B.C., Parthasarathy, S., and Tung,
A.K.H. Similarity search on Bregman divergence:
