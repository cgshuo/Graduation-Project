 From information retrieval to recommender systems, from bi oinformatics to financial market anal-ysis, the amount of data available to researchers has explod ed in recent years. While large, these datasets are often still sparse: For example, a biologist ma y have expression levels from thousands of genes from only a few people. A ratings database may contai n millions of users and thousands of movies, but each user may have only rated a few movies. In su ch settings, Bayesian methods provide a robust approach to drawing inferences and making p redictions from sparse information. At the heart of Bayesian methods is the idea that all unknown q uantities should be averaged over when making predictions. Computing these high-dimensiona l average is thus a key challenge in scaling Bayesian inference to large datasets, especially f or nonparametric models.
 Advances in multicore and distributed computing provide on e answer to this challenge: if each pro-cessor can consider only a small part of the data, then infere nce in these large datasets might become more tractable. However, such data parallelisation of inference is nontrivial X  X hile simple models might only require pooling a small number of sufficient stati stics [1], inference in more complex models might require the frequent communication of complex , high-dimensional probability distri-butions between processors. Building on work on approximat e asynchronous multicore inference for topic models [2], we develop a message passing framework for data-parallel Bayesian inference applicable to a variety of models, including matrix factori zation and the Indian Buffet Process (IBP). Nonparametric models are attractive for large datasets bec ause they automatically adapt to the com-plexity of the data, relieving the researcher from the need t o specify aspects of the model such as the number of latent factors. Much recent work in nonparametric Bayesian modelling has focused on the Chinese restaurant process (CRP), which is a discrete di stribution that can be used to assign data points to an unbounded number of clusters. However, many rea l-world datasets have observations that may belong to multiple clusters X  X or example, a gene may h ave multiple functions; an image may contain multiple objects. The IBP [3] is a distribution o ver infinite sparse binary matrices that allows data points to be represented by an unbounded number o f sparse latent features or factors. While the parallelisation method we present in this paper is a pplicable to a broad set of models, we focus on inference for the IBP because of its unique challeng es and potential.
 Many serial procedures have been developed for inference in the IBP, including variants of Gibbs sampling [3, 4], which may be augmented with Metropolis spli t-merge proposals [5], slice sam-pling [6], particle filtering [7], and variational inferenc e [8]. With the exception of the accelerated Gibbs sampler of [4], these methods have been applied to data sets with less than 1,000 observations. To achieve efficient paralellisation, we exploit an idea rec ently introduced in [4], which maintains a distribution over parameters while sampling. Coupled wit h a message passing scheme over pro-cessors, this idea enables computations for inference to be distributed over many processors with few losses in accuracy. We demonstrate our approach on a prob lem with 100,000 observations. The largest application of IBP inference to date, our work opens the use of the IBP and similar models to a variety of data-intensive applications. The IBP can be used to define models in which each observation i s associated with a set of latent factors or features. A binary feature-assignment matrix Z represents which observations possess which hidden features, where Z For example, the observations might be images and the hidden features could be possible objects in those images. Importantly, the IBP allows the set of such pos sible hidden features to be unbounded. To generate a sample from the IBP, we first imagine that the row s of Z (the observations) are cus-tomers and the columns of Z (the features) are dishes in an infinite buffet. The first cust omer takes the first Poisson(  X  ) dishes. The following customers try previously sampled di shes with probability m takes Poisson(  X /n ) new dishes. The value Z process allows an unbounded set of features but guarantees t hat a finite dataset will contain a finite number of features with probability one. The process is also exchangeable in that the order in which customers visit the buffet has no impact on the distribution of Z . Finally, if the effect of possessing a feature is independent of the feature index, the model is al so exchangeable in the columns of Z . We associate with the feature assignment matrix Z , a feature matrix A with rows that parameterise the effect that possessing each feature has on the data. Give n these matrices, we write the probability of the data as P ( X | Z, A ) . Our work requires that P ( A | X, Z ) can be computed or approximated efficiently by an exponential family distribution. Specific ally, we apply our techniques to both a fully-conjugate linear-Gaussian model and non-conjugate Bernoulli model.
 Linear Gaussian Model. We model an N  X  D real-valued data matrix X as a product: where Z is the binary feature-assignment matrix and A is a K by D real-valued matrix with an independent Gaussian prior N (0 ,  X  2 of the N by D noise matrix  X  is independent with a N (0 ,  X  2 posterior on the features A is Gaussian, given by mean and covariance Bernoulli Model. We use a leaky, noisy-or likelihood for each element of an N  X  D matrix X : Figure 1: Diagrammatic representation of the model structu re and the message passing process. Each element of the A matrix is binary with independent Bernoulli ( p and  X  determine how  X  X eaky X  and how  X  X oisy X  the or-function is, re spectively. Typical hyperpa-rameter values are  X  = 0 . 95 and  X  = 0 . 2 . The posterior P ( A | X, Z ) cannot be computed in closed form; however, a mean-field variational posterior in which w e approximate P ( A | X, Z ) as product of independent Bernoulli variables Q K,D We describe both synchronous and asynchronous procedures f or approximate, parallel inference in the IBP that combines MCMC with message passing. We first part ition the data among the proces-sors, using X p to denote the subset of observations X assigned to processor p . We use Z p to denote the latent features associated with the data on processor p . In [4], the distribution P ( A | X was used to derive an accelerated sampler for sampling Z  X  n is the set of all observations except n . In our parallel inference approach, each processor p main-tains a distribution P p ( A | X P p are updated via message passing between the processors.
 The inference alternates between three steps: The sampler is approximate because during Gibbs sampling, a ll processors resample elements of Z at the same time; their posteriors P p ( A | X, Z ) are no longer the true P ( A | X, Z ) . Message Passing We use Bayes rule to factorise the posterior over features P ( A | Z, X ) : If the prior P ( A ) and the likelihoods P ( X p | Z p , A ) are conjugate exponential family models, then side of equation (4). For example, the sufficient statistics in the linear-Gaussian model are means and covariances; in the Bernoulli model, they are counts of h ow often each element A The linear-Gaussian messages have size O ( K 2 + KD ) , and the Bernoulli messages O ( KD ) , where K is the number of features. For nonparametric models such as t he IBP, the number of features K grows as O (log N ) . This slow growth means that messages remain small, even for large datasets. The most straightforward way to compute the full posterior i s to arrange processors in a tree archi-tecture, as belief propagation is then exact. The message s from processor p to processor q is: where N ( p ) \ q are the processors attached to p besides q and l p are the sufficient statistics from processor p . A dummy neighbour containing the statistics of the prior is connected to (an arbitrarily designated) root processor. Also passed are the feature cou nts m p feature k within processor p . (See figure 1(b) for a cartoon.) Gibbs Sampling In general, Z The probability P ( Z m values of m updated, approximate m current stage (good for popular features).
 The collapsed likelihood P ( X | Z ) integrating out the feature values A is given by: where the partial posterior P ( A | Z can be efficiently computed by subtracting observation n  X  X  contribution to the sufficient statistics. 1 For non-conjugate models, we can use an exponential family d istribution Q ( A ) to approximate P ( A | X, Z ) during message passing. A draw A  X  Q  X  p ( A ) is then used to initialise an uncollapsed Gibbs sampler. The outputted samples of A are used to compute sufficient statistics for the likelihood P ( X | Z ) . In both cases, new features are added as described in [3].
 Hyperparameter Resampling The IBP concentration parameter  X  and hyperparameters of the likelihood can also be sampled during inference. Resamplin g  X  depends only on the total number of active features; thus it can easily be resampled at the root a nd propagated to the other processors. In the linear-Gaussian model, the posteriors on the noise and f eature variances (starting from gamma priors) depend on various squared-errors, which can also be computed in a distributed fashion. For more general, non-conjugate models, resampling the hyp erparameters requires two steps. In The processors each compute the likelihood of the current an d proposed hyperparameter values and propagate this value back to root. The root evaluates a Metro polis step for the hyperparameters and propagates the decision back to the leaves. The two-step approach introduces a latency in the resampling but does not require any additional message pass ing rounds.
 Asynchronous Operation So far we have discussed message passing, Gibbs sampling, an d hy-perparameter resampling as if they occur in separate phases . In practice, these phases may occur asynchronously: between its Gibbs sweeps, each processor u pdates its feature posterior based on the most current messages it has received and sends likeliho od messages to its parent. Likewise, the root continuously resamples hyperparameters and propa gates the values down through the tree. While another layer of approximation, this asynchronous for m of message passing allows faster pro-cessors to share information and perform more inference on t heir data instead of waiting for slower processors.
 Implementation Note When performing parallel inference in the IBP, a few factors n eed to be simply matched features by their index, that is, assumed tha t the i th feature on processor p was also the i th feature on processor q . In the IBP, we find that this indiscriminate feature merging is often disastrous when adding or deleting features: if none of the o bservations in a particular processor are using a feature, we cannot simply delete that column of Z and s hift the other features over X  X oing so destroys the alignment of features across processors. Because all Z p  X  X  are sampled at once, the posteriors P p ( A | X, Z ) used by each processor in section 3 are no longer exact. Below we show how Metropolis X  X astings ( MH) steps can make the parallel sampler exact, but introduce significant computational ove rheads both in computing the transition use of computational resources (especially as any finite cha in will not be exact); empirically, the approximate sampler behaves similarly to the MH sampler whi le finding higher likelihood regions in the data.
 Exact Parallel Metropolis Sampler. Ideally, we would simply add an MH accept/reject step after each stage of the approximate inference to make the sampler e xact. Unfortunately, the approximate sampler makes several non-independent random choices in ea ch stage of the inference, making the reverse proposal inconvenient to compute. We circumvent th is issue by fixing the random seed, mak-ing the initial stage of the approximate sampler a determini stic function, and then add independent random noise to create a proposal distribution. This approa ch makes both the forward and reverse transition probabilities simple to compute.
 Formally, let  X  Z p be the matrix output after a set of Gibbs sweeps on Z p . We use all the  X  Z p  X  X  to where the likelihood terms P ( X | Z ) and P ( Z ) are readily computed in a distributed fashion. For the transition distribution Q , we note that if we set the random seed r , then the matrix  X  Z p from the Gibbs sweeps in the processor is some deterministic functio n of the input matrix Z p . The proposal  X  is a (stochastic) noisy representation of  X  Z p in which for example where K should be at least the number of features in  X  Z p . We set Z p  X  in figure 2.) To compute the backward probability, we take Z p  X  and apply the same number of Gibbs sampling sweeps with the same random seed r . The resulting  X  Z p  X  is a deterministic function of Z p  X  . The backward probability Q ( Z p  X   X  Z p ) which is the probability of going from Z p  X  to Z p using 6. While the transition probabilities can be computed in a distr ibuted, asynchronous fashion, all of the processors must synchronise when deciding whether to accep t the proposal.
 Experimental Comparison To compare the exact Metropolis and approximate inference t ech-niques, we ran each inference type on 1000 block images of [3] on 5 simulated processors. Each test was repeated 25 times. For each of the 25 tests, we create a held out dataset by setting elements of the last 100 images as missing values. For the first 50 test i mages, we set all even numbered dimensions as the missing elements, and every odd numbered d imension as the missing values for the last 50 images. Each sampler was run for 10,000 iteration s with 5 Gibbs sweeps per iteration; statistics were collected from the second half of the chain. To keep the probability of an acceptance reasonable, we allowed each processor to change only small p arts of its Z p : the feature assignments Z In table 1, we see that the approximate sampler runs about five times faster than the exact samplers out data. Both the acceptance rates and the predictive likel ihoods fall as the exact sampler tries to take larger steps, suggesting that the difference betwee n the approximate and exact sampler X  X  performance on predictive likelihood is due to poor mixing b y the exact sampler. Figure 4 shows empirical CDFs for the number of features k , IBP concentration parameter  X  , the noise variance  X  2 and the feature variance  X  2 exact Metropolis samplers (gray) for the variances; the con centration parameter is smaller, but the feature counts are similar to the single-processor case. Figure 2: Cartoon of MH proposal Figure 3: Empirical CDFs: The solid black line is the approxi mate sampler; the three solid gray lines are the MH samplers with n equal to 1, 5, and 10 (lighter shades indicate larger n . The approximate sampler and the MH samplers for smaller n have similar CDFs; the n = 10 MH sampler X  X  differing CDF indicates it did not mix in 7500 iterations (reasonable s ince its acceptance rate was 0.0062). We ran a series of experiments on 10,000 36-dimensional bloc k images of [3] to study the effects of various sampler configurations on running time, performa nce, and mixing time properties of log-likelihoods using 1, 7, 31 and 127 parallel processors s imulated in software, using 1000 outer iterations with 5 Gibbs inner iterations each. The parallel samplers have similar test likelihoods as the serial algorithm with significant savings in running tim e. The characteristic shape of the test likelihood, similar across all testing regimes, indicates how the features are learned. Initially, a large number of features are added, which provides improvements i n the test likelihood. A refinement phase, in which excess features are pruned, provides furthe r improvements.
 Figure 4 shows hairiness-index plots for each of the test cas es after thinning and burn-in. The hairi-ness index, based on the method of CUSUM for monitoring MCMC c onvergence [9, 10], monitors how often the derivatives of sampler statistics X  X n our case, the number of features, the test likeli-hood, and  X   X  X hange in sign; infrequent changes in sign indicate that the sampler may not be mixed. The outer bounds on the plots are the 95% confidence bounds. Th e index stays within the bounds suggesting that the chains are mixing.
 Finally, we considered the trade-off between mixing and run ning time as the number of outer it-erations and inner Gibbs iterations are varied. Each combin ation of inner and outer iterations was set so that the total number of Gibbs sweeps through the data w as 5000. Mixing efficiency was Figure 4: Change in likelihood for various numbers of proces sors over the simulation time. The corresponding hairiness index plots are shown on the left. Figure 5: Effects of changing the number of inner iterations on: (a) The effective sample size (b) Total running time (Gibbs and Message passing).
 Table 2: Test log-likelihoods on real-world datasets for th e serial, synchronous and asynchronous inference types.
 measured via the effective number of samples per sample [10] , which evaluates what fraction of the samples are independent (ideally, we would want all samp les to be independent, but MCMC produces dependent chains). Running time for Gibbs samplin g was taken to be the time required by the slowest processor (since all processors must synchroni ze before message passing); the total time reflected the Gibbs time and the message-passing time. As see n in figure 5, completing fewer inner Gibbs iterations per outer iteration results in faster mixi ng, which is sensible as the processors are communicating about their data more often. However, having fewer inner iterations requires more frequent message passing; as the number of processors becom es large, the cost of message passing We tested our parallel scheme on three real world datasets on a 16 node cluster using the Matlab Distributed Computing Engine, using 3 inner Gibbs iteratio ns per outer iteration. The first dataset the high-dimensionality of the dataset makes it challengin g for other inference approaches. The piano dataset [12] consisted of 57,931 samples from a 161-di mensional short-time discrete Fourier transform of a piano piece. Finally, the binary-valued Flic kr dataset [13] indicated whether each of 1000 popular keywords occurred in the tags of 100,000 imag es from Flickr. Performance was measured using test likelihoods and running time. Test like lihoods look only at held-out data and thus they allow us to  X  X onestly X  evaluate the model X  X  fit. Tab le 2 summarises the data and shows that all approaches had similar test-likelihood performance.
 In the faces and music datasets, the Gibbs time per iteration improved almost linearly as the number of processors increased (figure 6). For example, we observed a 14x-speedup for p = 16 in the music dataset. Meanwhile, the message passing time remained smal l even with 16 processors X 7% of the Gibbs time for the faces data and 0.1% of the Gibbs time for the music data. However, waiting for synchronisation became a significant factor in the synchron ous sampler. Figure 6(c) compares the times for running inference serially, synchronously and as ynchronously with 16 processors. The Figure 6: Bar charts comparing sampling time and waiting tim es for synchronous parallel inference. asynchronous inference is 1.64 times faster than the synchr onous case, reducing the computational time from 11.8s per iteration to 7.2s. As datasets grow, parallelisation is an increasingly attra ctive and important feature for doing infer-ence. Not only does it allow multiple processors/multicore technologies to be leveraged for large-scale analyses, but it also reduces the amount of data and ass ociated structures that each processor needs to keep in memory. Existing work has focused both on gen eral techniques to efficiently split variables across processors in undirected graphical model s [14] and factor graphs [15] and specific models such as LDA [16, 17]. Our work falls in between: we leve rage properties of a specific kind of parallelisation X  X ata parallelisation X  X or a fairly broad class of models.
 Specifically, we describe a parallel inference procedure th at allows nonparametric Bayesian models based on the Indian Buffet Process to be applied to large data sets. The IBP poses specific challenges to data parallelisation in that the dimensionality of the re presentation changes during inference and may be unbounded. Our contribution is an algorithm for data-parallelisation that leverages a com-pact representation of the feature posterior that approxim ately decorrelates the data stored on each processor, thus limiting the communication bandwidth betw een processors. While we focused on the IBP, the ideas presented here are applicable to a more gen eral problems in unsupervised learning including bilinear models such as PCA, NMF, and ICA.
 Our sampler is approximate, and we show that in conjugate mod els, it behaves similarly to an ex-act sampler X  X ut with much less computational overhead. Howe ver, as seen in the Bernoulli case, variational message passing for non-conjugate data doesn X  t always produce good results if the ap-proximating distribution is a poor match for the true featur e posterior. Determining when variational message passing is successful is an interesting question fo r future work. Other interesting directions include approaches for dynamically optimising the network topology (for example, slower proces-sors could be moved lower in the tree). Finally, we note that a middle ground between synchronous and asynchronous operations as we presented them might be a s ystem that gives each processor a certain amount of time, instead of a certain number of iterat ions, to do Gibbs sweeps. Further study along these avenues should lead to even more efficient data-p arallel Bayesian inference techniques. [1] C. Chu, S. Kim, Y. Lin, Y. Yu, G. Bradski, A. Ng, and K. Oluko tun,  X  X ap-reduce for machine [2] A. Asuncion, P. Smyth, and M. Welling,  X  X synchronous dis tributed learning of topic models, X  [3] T. Griffiths and Z. Ghahramani,  X  X nfinite latent feature m odels and the Indian buffet process, X  [4] F. Doshi-Velez and Z. Ghahramani,  X  X ccelerated inferen ce for the Indian buffet process, X  in [5] E. Meeds, Z. Ghahramani, R. Neal, and S. Roweis,  X  X odelin g dyadic data with binary latent [6] Y. W. Teh, D. G  X  or  X  ur, and Z. Ghahramani,  X  X tick-breaking construction for th e Indian buffet [7] F. Wood and T. L. Griffiths,  X  X article filtering for nonpar ametric Bayesian matrix factoriza-[8] F. Doshi-Velez, K. T. Miller, J. Van Gael, and Y. W. Teh,  X  X  ariational inference for the In-[9] S. P. Brooks and G. O. Roberts,  X  X onvergence assessment t echniques for Markov Chain Monte [10] C. R. Robert and G. Casella, Monte Carlo Statistical Methods . Springer, second ed., 2004. [11] A. M. Mart X  X nez and A. C. Kak,  X  X CA versus LDA, X  IEEE Trans. Pattern Anal. Mach. Intelli-[12] G. E. Poliner and D. P. W. Ellis,  X  X  discriminative model for polyphonic piano transcription, X  [13] T. Kollar and N. Roy,  X  X tilizing object-object and obje ct-scene context when planning to find [14] C. G. Joseph Gonzalez, Yucheng Low,  X  X esidual splash fo r optimally parallelizing belief prop-[15] D. Stern, R. Herbrich, and T. Graepel,  X  X atchbox: Large scale online Bayesian recommenda-[16] R. Nallapati, W. Cohen, and J. Lafferty,  X  X arallelized variational EM for Latent Dirichlet Al-[17] D. Newman, A. Asuncion, P. Smyth, and M. Welling,  X  X istr ibuted inference for Latent Dirich-
