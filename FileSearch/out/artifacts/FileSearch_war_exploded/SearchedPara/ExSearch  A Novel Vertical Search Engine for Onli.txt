 E-Commerce has shown its exponent ially-growing business value in the past decade. However, in contrast to the successful examples in online sales, such as Amazon 1 and eBay barter business is still underexplored due to the lack of corresponding information aggregation service. In this paper, we design and implement a novel vertical search engine, called ExSearch, to aggregate online barter information for developing the barter market. Different from classical general purpose Web search engines, ExSearch adopts a focused crawler to gather related information from vari ous websites. We propose to automatically extract the barter information from free-text Web pages such that the unstructured information is represented in structured databases. In additio n, we utilize the data mining techniques such as regression to fulfill the missing information, which cannot be extracted from the Web pages. Finally, we validate and rank the search results according to user queries. Experimental results show that each component module in our proposed ExSearch system is efficient and effective. The volunteer users are satisfied by and interested in this novel vertical search engine. H.3.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Information Search and Retrieval; H.4.m [ INFORMATION SYSTEMS APPLICATIONS ]: Miscellaneous Design, Economics, Management. Barter, focused crawler, information extraction, machine learning, ranking, search engine, vertical Recently a true story, in which a man named Kyle MacDonald used a paper clip to barter for a house 3 http://www.amazon.com 2 http://www.ebay.com http://www.usatoday.com/t ech/news/2006-04-16-paper-clip-barter_x.htm. underexplored online market. He used the community power of the Internet to barter a red paper clip for something better. After a cross-continental trading trek, he successfully got one year's free rent on a house in Phoenix. Though the online exchangers, who barter with others on the Internet , have appeared for a long time, the online barter did not show sufficient market value until a large number of users appear to seek this kind of information recently. In terms of e-Commerce, the online sales business models, such as the successful examples propos ed by Amazon and eBay, have shown their great market value. The under-exploration of online barter business motivates us in developing tools for exchangers, i.e. users who are interested in barter business. Though there appear many commercial systems for online barter business in recent years, their markets are always limited by the information provided by their own frequent users or registered users. The barter information is generally distributed in the online forum, newsgroups and commercia l online barter websites all over the Web. The lack of inform ation aggregation service makes it very hard for online exchangers to easily find sufficient information they want. In this paper, we design and implement a novel vertical search engine, called ExSearch , to aggregate online barter information for further developing this market. Different from general purpose search e ngines, there are a number of challenges in the ExSearch service. First, we need to answer the question on how to automatically gather this kind of barter information from the Internet? S econd, for advanced search, we need to extract barter informa tion from free text Web pages to formulate and store it in structured databases. Third, since there is often missing information, such as the price of an object for barter from the information source, we need to automatically fill these missing information for better re trieval coverage. Finally, the information availability in the search results will seriously affect the satisfaction of users. It is a challenging task to validate the availability of the information in the search results. To deal with these challenges, the ExSearch system consists of five major modules: focused crawler, information extraction module, machine learni ng module for data enhancement, indexing module and retrieval module. First, we adopt the focused crawler framework proposed by Chakrabarti et al. [26] to gather online barter related Web pages. Second, in order to extract barter information in crawled Web pages, we apply a rule-based Information Extraction (IE) algorithm proposed by Oren Etzioni et al. [22] to construct th e structured database. Third, to fill in the missing object information, we classify the objects to predefined product categories through a modified K-Nearest-Neighbor classifier and estimate the value of the object through regression. Then, to better mana ge the data, we employ both B-tree [5] index and inverted inde x [7] to organize the structured data. Finally, we validate the information availability by some heuristic rules and rank the search results by interpolation of multiple search criteria. The remainder of this paper is or ganized as follows. In Section 2, we review some related commercial systems of the online barter business. In Section 3, we presen t the detailed system design and implementation. After that in Sec tion 4, we show the experimental results of each module and the overall system evaluation by a user study. Finally, we conclude this paper in Section 5. Electronic Commerce or e-Commerce in short is an emerging concept, which is used for describing any type of online business via computer networks includi ng the Internet and intranet. E-Commerce can be broken into three major categories, such as Business-to-Business (B2B), Business-to-Customer (B2C), Customer-to-Customer (C2C), etc. According to the market research from DCCI 4 , in China of 2007, the increase of the sales transaction is 25.5% for B2B, 33.5% for B2C and 90% for C2C. Those persuasive results show that e-Commerce has shown great market value. Taking the general online sales as example, dozens of e-Commerce websites such as Amazon have been accepted by users. In the first half of 2008, Amazon North American Web sales grew by 33% according to the report of comScore Inc However, the online barter, which is a C2C business model, is underexplored in both technical research and e-Commerce marketing reports. exchanged for other goods and/or se rvices, without use of money. Online barter service is exchange network which provides barter commercial online barter services were proposed to develop the online barter market. For example, the Barter.net [30] which is a part of the Internet Marketing Community (IMC), is a directory of barter companies and others willing to trade products and services. However, those online commercial barters are B2B trade and the charge for information access limits the general Internet users to benefit from them. On the othe r hand, the commercial products such as bartercard[31], TradeAway [36], Barterswitch [32] are C2C style trade, which typically uses an online forum to allow users to join the online barter business. However, most of these services only maintain very limited information about online barter, which is provided by their frequent or registered users. In addition, although some of the above systems provide the search functionality for barter, the search service is as simple as general purpose search, which does not cons ider specific properties of online barter such as object novelty, price and so on. As for some other commercial websites such as SF bay area barter classifieds at craigslist.org [33], they are not specialized for barter information which only contains several channels for this. Different from these commercia l systems, we propose the ExSearch, which is a novel verti cal search engine that can aggregate this kind of information to better serve the users. In this section, we introduce the overall ExSearch system design and implementation. In Section 3.1, we introduce the system http://www.dcci.com.cn/ http://www.comscore.com/ http://en.wikipedia.org/wiki/Barter architecture. In Section 3.2, we introduce the core modules and implementation of these modules. In this section, we introduce the architecture of our proposed ExSearch system and its major f unctionalities. Figure 1 shows the system flowchart. There are five major modules, which are (1) focused crawler; (2) data parsing &amp; information extraction; (3) machine learning for data enhancement; (4) data indexing &amp; follows. At the beginning, we collect information from the Internet in two different ways. On one hand, we manually collect some specific websites for online barter and crawl the related information automatically. On the other hand, we use the focused crawler to identify and crawl related barter information from general newsgroups and Weblogs etc. Then data parsing and information extraction module will preprocess the data and extract the predefined properties of the objects. Through this way, we translate the barter information in free texts into a structured database. Since some of the objects X  properties are not explicitly declared, we use the machine learning and data enhancement module to predict the missing or implicitly expressed object properties. After the first two preprocessing steps, the data is clean and complete. Then we index the data in the database for efficient retrieval. Finally, the end users can search through the interactive user interface, and the retrieval model will validate and rank the returned results according to the user inputs. Figure 2 shows the user interface of the ExSearch system, which provides two ways for users to explore this web service. One is to browse the objects taxonomy to se lectively pick their favorite categories if the users have no clear idea on what to barter for. The classification algorithm for building the taxonomy for users to browse will be introduced in Section 3.2.3.1. The other way is to search the structured database. ExSearch system allows users to search objects by one or more ob ject properties including: object name, novelty, price as well as location. Once the search operation is performed, pages satisfy ing all search constraints are retrieved and ranked. Furthermore, users can choose their favorite ranking method such as ranking by post time, price, location etc. Without loss of generality, we let } , 2 , 1 , { L of barter information we collected from the Web. Each item d free text document or a structured table we crawled. In Section 3.2.1, we introduce how we co llect the related information D by both human knowledge and automatic focused crawler. After the data crawling, we extract information from each d i to translate the free text into structured database. The features of the structured d include, (1) the name of the object for exchange which is represented by o i ; (2) the category of the object belongs to, which is represented by c i ; (3) the location of the object for exchange, i.e. l object X  X  novelty, which is represented by n i ; (6) the information availability deadline t i ; (7) contact informati on of the object owner u ; and finally (8) some additional information a i the original information source. In Section 3.2.2, we introduce the details of information extraction module we used, which can represent each document as need to tell whether a user is lo oking for an object or has an object to exchange. Since the free texts we crawled are hard to cover all the features we want to fill in the structured data, in Section 3.2.3, we introduce the machine learni ng techniques that we use to estimate and fill in the missing values. After that, in Section 3.2.4, we discuss how to store and index the data for efficient retrieval. Finally in Section 3.2.5, we introduce the ranking strategy used in this system. Moreover, since the information availability is very important to the users X  need fulfillment, we also propose an algorithm for result validation in Section 3.2.5. With the fast e-Commerce growth, vast repositories of online barter information are available on the Internet. However, the information is frequently posted in online forums, newsgroups as well as user blogs, which leads to data dispersion problem. In order to collect as much data as possible, we propose to adopt the focused crawler to gather data from various website sources. A focused crawler [26] is a Web r obot that select ively downloads Web pages that are relevant to a pre-defined set of topics. The topics are generally represente d by exemplary Web pages. As some examples, Pinkerton [3] proposed to predict the relevance by utilizing the anchor text of links. However, it does not work well for crawling large amount of pa ges. Chakrabarti et al. [26] proposed a generic architecture of a focused crawler and designed two hypertext mining algorithms, classifier and distiller, to guide the crawler. Diligenti et al. [17] proposed to apply context graph so that the crawler can extract in formation about the context. A set of classifiers are trained on examples to estimate the distance of the current page from the closest on-topic page. But the training procedure is quite complex. Menczer et al. [9] proposed to use more sophisticated techniques su ch as reinforcement learning and evolutionary adaptation for longer crawls. Hsu [4] also proposed relevance context graph to solve this problem. Their crawler can measure the page relevance by calc ulating the word distribution of both general and topic-specific feat ure words so as to prioritize the crawling order. In this pa per, we adopted Chakrabarti X  X  generic framework [26] and app lied specialized rules for our ExSearch system. First, we manually collect a se t of websites which are newsgroups or related commercial sites for online barter as seed URLs. Second, we crawl Web pages that ar e relevant to online barter and filter out irrelevant ones so as to save both disk and network resources. We find that those s eed URLs always contain some other online barter related websites. For example, in the homepage of TradeAway [36], it also contains the link to the ReExchange website [35]. We add those websites into our candidate URL pools. Finally, as an additional function to collect more information distributed on the Web, we use the focused crawler to gather related information stored outside the professional websites. For example, a user may post that he wants to barter an iphone for a digital camera in his pers onal blog. It is hard for the existent commercial systems to consider these potential customers. We rely on the search function to get such information by defining search criteria . Besides the basic focused crawling function, our crawler also incrementally crawl online barter information daily. One important application of incremental crawling is to check the availability of each crawled post, we add a verifier function to validate whether the post expires or not. There are three possible cases to judge the expiration. Case 1 : the posting information has expired and is removed by website administrators. Case 2 : the posting information has expired and the link redirects to an error page. Case 3 : the information poster explicitly limits the end time of exchange in posting and the crawl time exceeds the end time or the exchange has succeeded. For cases 1 and 2, after a round of incremental crawling, the invalid URLs are filtered out from the system. For case 3, our informati on extraction component extracts the poster-specified end time and all the pending or successive exchange records to judge the va lidation. If the crawled Web page is valid, the up-to-date data is appended or updated to the database for further process. More details for search result verification are provided in Section 3.2.5. There is a potential issue caused by the gap of crawling period. In our ExSearch system, the focused after the crawler has crawled it, then the post X  X  information will be incorrect until the next crawl to update. We will further study this problem in future work. ExSearch system will extract the 8 features from either structured tables or free text document. Michael et al. [19] has proposed WebTables which can address the problem of extracting from structured HTML tables well. In this paper, we mainly discuss how to extract information from free text document. Information Extraction (IE) is a technique used to extract specified information in text documents and present the information in a structured format [18, 28]. Related algorithms are generally classified into two categories: statistical learning and rule-based learning [28]. Over the past decade , a number of sta tistical models were proposed to learn from the hand-annotated training documents automatically. For instances, Hidden Markov Model and Conditional Random Field have been utilized for designing IE systems [6, 27]. However, the model parameters, which are learned from a training corpus, are always hard to estimate in these algorithms. In contrast, the rule-based approaches are commonly used in domain specific information extraction systems due to its effectiveness. Rule-based methods train IE ru les for identifying relevant information in text documents from a set of hand-tagged training examples. The supervised rule-based methods are applied in many IE systems, such as WHISK [28], RAPIER [18] etc. However, inthese systems, the hand-tagged training sets are relatively small. Thus, extraction rules learned from these small hand-tagged sets have limited coverage. Oren Et zioni et al. [23] proposed an unsupervised framework of IE, where the extraction rules are instantiated by a set of generi c, domain-independent templates. Based on this idea, they develop an IE system named as KNOWITALL. However, many of the efficient extraction rules for a domain cannot match any template [22]. To overcome the drawback, they also propose a supervised method for rule learning [22]. The method first issues search engine queries for each seed instances which generated by KNOWITALL, then learns some rules from the context string of these seed instances in the pages returned by the search engine. It can generate more domain-specific rules than KNOWITALL. Motivated by these effective systems, we adopt Oren Etzioni X  X  [22] ideas for information extraction from the Web pages containing barter information. For ease of understanding, we give a real post in a newsgroup as a running example:  X  X sed single seat gocart with Honda engine for sale or trade. would like a good pr essure washer . Runs great! If interested make an offer or call xxx-xxx-xxxx. X  All the information to be extracted, which has been introduced at the beginning of this section, is genera lly considered in two categories. One is the information which has explicit patterns such as contact information like phone number and em ail address. It is relatively easier to deal with them because they all follow a general regular expression pattern. The other is the information which has no explicit patterns. We propose a ru le-based IE framework on the basis of the work proposed by Oren Etzioni to address this issue. In most of the IE tasks on the Web, the Web pages are preprocessed in order to achieve high quality results. In this paper, we delete the nodes in HTML documents corresponding to tables, images, and embedded scripts (such as JavaScript). The words in the text are all stemmed and tagged by the Part-of-Speech (POS) tool [34]. Moreover, to extract high quality extraction rules, we retain only the heads of the noun phrases which are annotated as specified information in the training corpus. The process for learning IE rules has four phases: (1) First, we learn the seed extr action patterns from a relatively small annotated corpus (hundreds of Web pages). For the tagged information in the corpus by POS, we extract their contexts using a context window of suitable width. Then the tagged term in the context is replaced by the tag  X  X ClassName} X . In the above instance, the noun phrases with head  X  X ocart X  are tagged as object for exchange, and the phrases are replaced by the string  X  X OwnStuff} X . Thus, the first sentence in the post document becomes  X  X OwnStuff} for sale or trade. X  Then the context is passed through a suffix tree constructor so that all substrings are detected with their count [8]. Fi nally, we pass each phrase in the suffix tree through a filter to reta in only those phrases containing the string of  X  X ClassName} X . Thus, we obtain a group of seed extraction patterns such as  X  X rade {OwnStuff} for X ,  X  X OwnStuff} trade for X  or  X &lt;title&gt;{OwnStuff} for X . When we utilize these patterns to extract specified info rmation (e.g. OwnStuff), the text corresponding to  X  X OwnStuff} X  is extracted first, and then a noun group regular expression [16] is used to determine whether the text is the target information. (2) Second, we rank the patterns that we extracted based on their defined as, tagged term in the target class and other f is the number of the patterns that wrongly match the term in the target class, where the tagged term is the phrase replaced by  X  X ClassName} X . Table 1 lists the affinity rank of the extraction rules for class  X  X wnStuff X . Table 1. Affinity rank of extraction rules for class  X  X wnStuff X  (3) Next, we search the entire corpus to find out more Web pages that include the top n patterns extracted in step (2). We tag the terms in these pages utilizing the seed patterns of step (1). Thus, a new tagged training set can be used to learn more extraction rules. This procedure can be run iterativ ely until the top ranked rules do not change much. (4) Finally, we rank the new patterns in terms of their affinity, and step (3) compose the final IE rules set. In real computation, when extracting specified information, these patterns will be applied sequentially until correct info rmation can be extracted. Based on the algorithm proposed above, we can extract relevant information from the free text information. Table 2 lists the extraction results for the running example. 
OwnStuffNoveltyDegree n i N/A As introduced in previous subsecti ons, we have 8 features to be extracted to construct the struct ured database. However, not all exchangers would like to provide the complete information in their posts. Among various feat ures, the object category and object value/price are always missing features but are important for advanced search and ranking, as well as for other barter users. Inspired by the development of machine learning techniques, we aim to fill the information automatically in this section. In Section 3.2.3.1, we introduce how to classify the objects to predefined product categories through KNN classifi er. In Section 3.2.3.2, we introduce how to estimate the missing value of objects through regression. In most of the pages we crawled from the Internet, the exchangers did not provide the category information for their owned objects. However, if a user is seeking for information in some predefined category to barter for, it is hard to accurately return relevant results without the objects X  class information. In addition, the information browsing functionality of ExSearch highly depends on this classification results. To break this bottleneck, we integrate a data enhancing algorit hm with the classical KNN style algorithm is quite simple since the category of information only depends on the object name, which has nothing to do with other object properties such as the pric e or location. However, a key challenge is that each product may have more than one name from the users X  perspective and we cannot guarantee to collect all possible product names for traini ng. To solve this problem, we adopt an algorithm call ReCom [15] to enhance the classification based on the query log of a commercial search engine. Similar to the assumption of ReCom [15], a basic assumption of our strategy is that  X  X f two object names, which are implied by two different search queries, alwa ys click the same group of Web pages, then they have high pr obability to belong to the same product class and vice versa. X  Suppos e the name of an object is implied in a set of search queries, we collect all the clicked pages of these queries and weight thes e pages according to their click-through frequency. After that, we collect all the other queries that also clicked these pages and weight these queries by the click frequency of themselves to these pages multiplied by the weight of these pages. Finally we extract all the noun phrases from these queries by the POS tool [34]. We merge all the same noun phrases by summing their weights. Then for each object name, we can build a profile of it by all the related noun phrases with user provided weights learned from the log. Different from the classical ReCom algorith m, we only conduct one step of iteration instead of waiting for the algorithm to converge. On the other hand, we have a predefined product tree. Each node of this tree has a collection of product names or service names for training. The tree structure and the training data of our system were crawled from several commercial online sales systems and were manually edited. We build an inverted index [7] for all the training data. Then for each input noun phrase, we can return all the category information of it if it exists in the training dataset. Finally, for each object name, we use all the noun phrases in its profile to retrieve category information. Each related category can be weighted by the sum of corresponding noun phrases X  weights. Then for each object name, the top category with the largest weight is considered as its fi nal category. This classification algorithm is actually a weight ed KNN procedure. Though many other classifiers such as support vector machines (SVM), Na X ve Bayesian (NB), Boosting were proposed for classification problems, we select the KNN style approach due to its effectiveness and there has no training procedure required. The exchangers always post their information as  X  X eeking for objects which have comparable va lue with mine X . However, only a small ratio of exchangers would like to provide the explicit price information for their objects in their posts. Thus in the ExSearch system, we propose to use re gression technique [12] for estimating the value range of each object to fulfill this information gap. We firstly extract the estimated price of an object with their novelty information through inform ation extraction [22] as in Section 3.2.2. Then we can formulate a group of training data in each node of our product taxonomy. We build different estimation functions for different classes of objects since we assume that the objects belonging to the same cl ass will have similar value reduction rate. We simply consider the novelty of the objects by how many days they have been us ed. The training data are all the pairs in the format &lt;# days used, $ price&gt;. The regression function we chose is, Where a, b and c are the three parameters to be estimated. The variable x stands for the number of da ys used and the variable y stands for the price of the object . We choose this function as the regression model since it satisfies the intuition that the value of a second hand product will decrease rapidly at the beginning. However, the value reduction will slow down after a period of time. Many users would like to provide the time information in months or years. We translate all of them into days in our system. Note that the regression model is not applied to several categories such as wine and arts since they will not obey this kind of regression function. In this way, we only apply this estimation feature to the categories which both obey the regression function and have enough training data. As introduced above, all the onlin e barter information can be formulated as structured data. In this section we focus on the indexing of structured data for quic k retrieval. In ExSearch system, there are two types of index, which are database index and text index . Firstly, we adopted the classi cal B-tree [5] to build database index for features that are not valued by free texts. These features include category c i , location l i , value/price p i and degree of object novelty n i . The data is incrementall y updated with the increasing size of online barter information, B-tree is selected for data indexing since it is easy to deal with the large scale data and more importantly, it can adaptively deal with the streaming data. Secondly, for the features valued by free texts, we build the text indexing for them. These features include the object name o well as the additional information a i etc. In the area of text retrieval, Bag of Words (BOW) m odel is widely used, which is a core technology of current text re trieval systems. In this system, we employ the inverted index, the basic data structure for BOW model, which is the same as some previous works [7]. In this section, we introduce the major function of ExSearch system, which allows users to issue queries. There are two imminent problems to overcome for the search engine design. First, for online barter, the information availability will change rapidly. For example, it is hard for us to know whether an  X  X pod X  to be exchanged is still available after this information is indexed availability of the in formation. Second, sa me as general purpose Web search, the search results ra nking is a challenging task which will greatly affect the user satisfaction. Since each post has a life cycle, it will result in very bad user experience if many objects to barter for in the search results are unavailable or many of the objects have already been exchanged with others. This motivates us to validate the information availability before re turning objects to users. We propose several heuristic rules to validate the availability of search results. URLs that we have crawled. The Focused crawler will check whether the URL is still availabl e when re-crawling those posts. extract its post time and check whether the post is expired or not. Sometimes, the post contains time limit information. Then we directly check if the current time has exceeded the time limit. information crawled from the news groups, there will be followup posts for a given one. Then we an alyze the followed posts through text mining. If the same author explicitly expresses to close this information, we will remove it from our database. In our search user interface, a user query consists of five parts, which are object name, price, lo cation, category and degree of novelty. Except for object name , all other constraints are considered as filters on the search results. The  X  X rice X  constraint allows user to define a price range of the object they are looking for. So does the degree of objec t X  X  novelty. User can also define category and location of the object they want to barter for. When a query is submitted, the candidate information is retrieved by the object name firstly. After that is a filtering procedure on the candidates, the corresponding B-tr ee index is explored and only candidate objects which satisfy us er input are reserved. Here, information retrieval by object name is one of the most important functions. We represent the object name in the classical Bag of Words model and retrieve information by the Okapi BM25 [21] retrieval model. As a summary, the results ranking of the ExSearch consists of three key step s. Firstly, for a given query, we retrieve the related information from the text index according to the object name to be searched. All the returned results are considered as candidates. Note if the object name is null in the user query, all information indexed in the database can be considered as candidates. Secondl y, all other constraints in the user query are used as filters to only reserve the information which can satisfy the user X  X  query. Finally, we calculate a score for each search result. This score integrates two perspectives including the relevance score and availability score. Optionally, we allow users to customize the ranking strategy, including rank results by other features such as price, location, category and time. Figure 3 shows the workflow. In this section, we evaluate our proposed ExSearch system. From Sections 4.1 to 4.5, we evaluate each component module independently. The experimental environment is the server with a dual-core 2.81GHz AMD CPU, 15G B of RAM and a SCSI disk. The network bandwidth is 10MB/s. Section 4.6 shows the overall system evaluation by a user study. In order to perform the user study, 15 volunteers are invited to use the system, label the result and evaluate the system. To evaluate the effectiveness of our crawler, we run experiments on two aspects including relevance of the harvested objects of the focused crawler as well as the incremental crawling performance. The seed URLs we manually chose are listed in Table 3. From the seed URL list, we can see three seeds, atlanta.craigslist , newyork.craigslist and sfbay.craigslist , all belong to the very popular online forum craigslist . This motivates us to investigate whether there are more online barter related channels under this website. We run our focused crawler to crawl craigslist website and found 40 more barter channels, such as Miami.craigslist , kansascity.craigslist and so on. Figure 4 compares the relevant ra tio of harvested objects between focused crawler and the unfocused crawler when crawling craigslist website. Here we adop ted general breadth-first crawler as unfocused crawler in our experiment. The x-axis shows the number of crawled Web pages. The y-axis shows the relevance[26], which is the rate of barter related Web pages among all crawled ones. From the figure, we can see the unfocused crawler drops very quickly, while the focused crawler performs evenly. In all, it t ook about 5.25 hours to crawl 34,625 URLS including 31,289 valid objects by a single testing server in this experiment. 
Figure 4. Rate of relevant pa ge acquisition when crawling In order to show the capability of our focused crawler incrementally collecting data and updating expired information daily, we list the experimental results on 4 websites in Table 4. We daily ran the crawling and report the results from Oct 25 Oct 27 th 2008. In Table 4, #Base represents the number of Web pages in the database (DB), #New means the number of Web pages newly crawled and added to the DB, #Delete shows the number of Web pages expired a nd removed from the DB by validation rules, and W1 for atlanta.craigslist , W2 for swapitshop , W3 for tradeaway , W4 for u-exchange . From the table, we can see #New and #Delete are very similar for each website. In all, it takes about 10.6 hours to finish one round incremental crawling by a single testing server. In this section, we conduct so me experiments to evaluate the effectiveness of the IE used in ExSearch system, which can be defined as, We apply the IE module to a set of free text documents with 1,000 Web-pages which includes barter information. Volunteers are invited to manually label the results of the 1,000 Web-pages. Table 5 lists the recall and precis ion of our IE algorithm on each object feature. OwnStuffLocation and SubmitTime , the recall and precision can be as high as 98% because they always have the same pattern in the Web pages, just like the inform ation in structured documents. Meanwhile, the performances of Contact Information class including OwnerPhoneNumber and OwnerEmail are also good. In practice, we utilize the regular expression to extract the phone number or email of the users. Nevertheless, the performances on remaining features are not as goo d as the ones we just introduced, because information of these features all occurs in changing pattern among various Web page s. Generally speaking, the average experimental performance around 77% shows that the IE system we used is able to accurately extract information. The user study in Section 4.6 confirms that this performance can satisfy the basic requirements of users. Table 5. The IE performance for unstructured documents OwnStuffNoveltyDegree n i 0.675 0.890 In this section, we evaluate the performance of machine learning algorithms for data enhancement. Two sets of experiments are conducted for evaluating object cl assification and object price estimation respectively. Since our goal is to evaluate whether the system can satisfy the users, we only study whether our algorithms can give good enough performance. We did not involve many baseline algorithms for comparison since few existent algorithms were proposed in the same scenario with us. To evaluate the object classifi cation performance, we randomly sampled 1,000 records in our databa se after information extraction. The classes of the object names in these records are manually labeled by the volunteers. Since th e objects have clear class labels from the volunteers X  viewpoints, there is very high agreement among them. Thus no voting is requi red. We define the precision of classification as, The precision for the 1,000 records is as high as 93.8%. If we remove the data enhancing step , which is adopted from ReCom [15], the precision is dramatically reduced to 62.7%. This shows that the proposed approach which integrates ReCom and KNN is effective. To show the robustn ess of the classification, we randomly reduced the scale of training data in each category, Figure 5 shows the precision changes against the ratio of reduced training data scale. This figure shows that the click-through log based data enhancement makes the classification results will not rapidly decrease with the redu ced scale of training data. To evaluate whether we can accurately estimate the value of an object, which has no explicit pric e information in the information source, we collect a dataset with ground truth. In this dataset, we have 2,000 records which both have explicit price information and explicitly expressed how long th e object has been used. These 2,000 records in the database are collected from 4 classes, which are Toys, Books, Watches and DVDs respectively. Each class has 500 records and we randomly split the 500 records into 5 groups. We run the regression for 5 rounds. In each round, 4 groups of data are used for training and the remaining group is used for testing. The evaluation metric, i.e. error, is defined as, The results reported in Table 6 are the average of the 5 runs. From this table we can see that our proposed regression model can well approximate the user-provided pric e. As a baseline, we give the error of the linear regression in th e same table. It can be seen that the commonly used linear regression is not comparable with our proposed model, since we consider ed the regression model design by our prior knowledge. Though we can accurately estimate the object price through regression, we still cannot give the estimated the object. We will further explore this direction in our future work. In this section, we conduct the experiments from two different perspectives. On one hand, we evaluate the efficiency of both offline index and online retrieval. On the other hand, the effectiveness of online retrieval is evaluated. At first, we measure the offline index performance. We indexed 30,000 objects and the raw data size is 31M on disk. It costs 31 seconds to finish the index process and the index data takes 3M disk space. In order to evaluate the online retrieval performance, we sampled 665 queries, including 543 object Name queries, 51 location queries, 10 price queries, 61 ca tegory queries, and calculate the average query time. Table 7 shows the average query time of each feature varies along with various da ta scale. From the table, we can see that the location and category search almost linearly increase along with the data size. We predefined location and category with limited nodes and save the index using B-Tree data search the B-Tree, thus the query time increase linearly with data size. We also randomly combined the search queries of different features into 1,000 queries, and got 198 results per query on average. The average response time is 447 X s. At last, we picked up 50 popular queries from query log including  X  X oy X ,  X  X ook X ,  X  X atch X  etc, and asked our volunteer users, who are Chinese native speakers and also skillful in English reading, to label the top 5 results. The precision is calculated on English dataset only. Table 8 shows the e xperimental results. Precision@5 reaches 0.87, which is considered satisfactory for most query results. We also asked the volunteers to label the results without validation, that is, we didn X  X  guarantee the information availability. In Table 8, we can see that the pr ecision of search result without validation rules is about 10% lower than the precision of validated search results on average. In this section, we show the overall system evaluation result. We asked the 15 volunteers to help with the user study, most of whom like shopping online a lot. A questionnaire survey is distributed to all users. We define th e rating with 3 degrees, where 1 means bad, 2 means neutral, and 3 means go od. Table 9 shows the evaluation result. From the result we can see most users are satisfied by the system and search result. We investigated the reason for the low rating on Question 2. The users X  main con cern is with the truth of the information on the Web. One user ranked lower to Question 3 and 5 because the search result by object name returned some noise results. We also checked the reason for the low score on Question 7. The volunteers are interested in the recommendation results, but they thought the results are too similar and want to see more diverse results. About Question 8, although the volunteers thought it will be helpful, but exchange across continents will be problematic in terms of transportation. Overall, the volunteers showed their interests to the system and were satisfied by the functions. In this paper, we propose the details in implementing a novel vertical search engine, called ExSearch, to aggregate online barter information for online exchangers . It consists of several key modules. First, it adopts a focu sed crawler to gather related information from various websites. After that, it automatically extracts the barter information from free-text Web pages such that the unstructured information is repr esented in structured databases. And then, we utilize machine learning techniques to fulfill the missing information, which cannot be extracted from the Web pages. Finally, we provide an easy to use user interface for users to quickly retrieve information fr om ExSearch. As some advanced features, it provides both cross-language service and personalized service through machine translation and user interests learning. Some preliminary experimental results and user studies show that ExSearch system is efficient and effective. The volunteer users are satisfied by and interested in this novel vertical search engine. In the future work, we will continuously improve the retrieval performance and precision. We will use a distributed system to handle large scale barter informati on for evaluation by more users. We also plan to validate the tr uth of the information so as to prevent users being deceived. Fu rthermore, we want to build social network between users who have exchanged objects to help enrich their experience. [1] A. Pirkola, T. Hedlund, H. Keskustalo, and K. J X rvelin, Dictionary-Based Cross-Langua ge Information Retrieval: Problems, Methods, and Research Findings, Information Retrieval, 4(3-4), pp. 209-230, 2001. [2] B. Mobasher, R. Cooley, and J. Srivastava, Automatic personalization based on Web usag e mining, Communications of the ACM, 43(8) pp. 142-151, 2000 [3] B. Pinkerton, Finding what peop le want: Experiences with the WebCrawler, In Proceedings of the International World Wide Web Conference, Chicago IL USA, 1994. [4] C. Hsu, F. Wu: Topic-specific crawling on the Web with the measurements of the relevancy context graph. Information Systtem, 31(4-5), pp. 232-246, 2006 [5] D.Comer, The ubiquitous B-tr ee, Acm Computer Surveys, pp.121-138, 1979. [6] D. Freitag and A. McCallum, Information extraction with HMM structures learned by stochastic optimization, In Proceedings of National Conference on Artificial Intelligence, Austin, TX, USA, pp. 584-589, 2000. [7] D. Harman, and E. Fox, Inverted Files. In W. B. Frakes and R. A. Baesa-Yates, editors, Information Retrieval, Data Structures &amp; Algorithms, pp. 28-43, Prentice Hall, Englewood Cliffs, NJ, 1992. [8] D. Ravichandran and E. Hovy, Learning Surface Text Patterns for a Question Answering System, In Proceedings of the ACL, Philadelphia, PA, USA, pp. 41-47, 2002. [9] F. Menczer, G. Pant, and P. Srinivasan, Topical Web Crawlers: Evaluating Adaptive Algorithms, ACM Transactions on Internet Technology, pp. 378-419, 2004. [10] G. Salton and M.J. McGill, An Introduction to Modern Information Retrieval, McGraw-Hill, New York, 1993. [11] J. Breese, D. Heckerman, and C. Kadie, Empirical Analysis of Predictive Algorithms for Collaborative Filtering, In Proceedings of the Conference on Uncertainty in Artifical Intelligence, pp. 43-52, 1998. [12] J. Han, and M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann P ublishers, San Francisco, CA, 2001. [13] J. Hu, L. Fang, Y. Cao, etc., Enhancing text clustering by leveraging Wikipedia semantics, In Proceedings of SIGIR, Singapore, pp. 179-186, 2008 [14] J. Nie, M. Simard, and P. Isabelle etc, Cross-Language Information Retrieval Based on Parallel Texts and Automatic Mining of Parallel Texts from the Web, In Proceedings of SIGIR, Berkeley, California, USA, pp. 74-81, 1999. [15] J.Wang, H. Zeng, Z. Chen etc., ReCoM: reinforcement clustering of multi-type interrelated data objects, In Proceedings of SIGIR, Toronto, Canada, pp. 274-281, 2003. [16] K. Frantzi, S. Ananiadou and H. Mima, Automatic recognition of multi-word terms: the C-value/NC-value method, International Journal on Digital Libraries, 3(2), Springer Berlin, pp.115-130, 2000. [17] M. Diligenti, F. Coetzee, S. Lawrence, etc., Focused crawling using context graphs, In Proceedings of the Conference on VLDB, Cairo, Egypt, pp 527-534, 2000. [18] M. E. Califf and R. J. Moone y, Relational learning of pattern-match rules for Information Extraction, In Proceedings of the National Conference on Ar tificial Intelligence, Orlando, Florida, USA, pp. 328-334, 1999. [19] M. J. Cafarella, A. Halevy, Z. Wang, etc., WebTables: Exploring the power of tables on the web, In Proceedings of the Conference on VLDB, Auckland, New Zealand, pp. 538-549, 2008 [20] M. Kluck and F. Gey, The Domain-Specific Task of CLEF-Specific Evaluation Strategies in Cross-Language Information Retrieval, In Proceeding of the CLEF 2000, Springer Berlin, pp. 48-56, 2001 [21] N. Craswell, H. Zaragoza, a nd S. Robertson, Microsoft Cambridge at TREC-14: Enterprise Track, In Proceedings of the 14 th Text Retrieval Conference, 2005. [22] O. Etzioni, M. Cafarella, D. Downey, and A. M. Popescu, Methods for Domain-Independent Information Extraction from the Web: An Experimental Comparision, In Proceedings of National Conference on Artifici al Intelligence, San Jose, California, USA, pp. 391-398, 2004. [23] O. Etzioni, M. Cafarella, D. Downey and S. Kok, Web-Scale Information Extraction in KnowIt All, In Proceedings of the International World Wide Web Conference, Manhattan, NY, USA, pp. 100-110, 2004. [24] P. Indyk, A Small Approxima tely Min-Wise Independent Family of Hash Functions. In Proceedings of the Symposium on Discrete Algorithms, Baltimore, Maryland, USA, pp. 454-456, 1999. [25] P. Resnick, N. Iacovou, M. Suchak, and P. Bergstrom, GroupLens: An Open Architecture for Collaborative Filtering of Netnews, In Proceedings of CSCW, Chapel Hill, NC, USA, pp. 175-186, 1994. [26] S. Chakrabarti, M. van der Berg, etc., Focused crawling: a new approach to topic-specific web resource discovery, Computer Networks 31(11-16), pp. 1623-1640 1999. [27] S. Sarawagi and W. W. Cohen, Semi-markov conditional random fields for information extr action, In Advances in Neural Information Processing System s, Vancouver, Canada, 2005. [28] S. Soderland, Learning Info rmation Extraction Rules for semi-Structured and Free Text . Machine Learning, Springer Netherlands, pp. 233-272, 1999 [29] Y. Li, K. Bontcheva and H. Cunningham, SVM Based Learning System for Information Extraction, In Proceedings of international workshop of Determ inistic and Statistical Methods in Machine Learning 2004, Springer Berlin, pp. 319-339, 2005. [30] http://www.barter.net/ [31] http://www.bartercard.com/ [32] http://www.barterswitch.com/ [33] http://www.craigslist.org/about/sites [34] http://www.ims.uni-stuttgart.de/p rojekte/corple x/TreeTagger/ [35] http://www.reexchange.com/ [36] http://www.tradeaway.com/
