
Yuanzhe Zhang 1 , Xuepeng Wang 1 , Siwei Lai 1 , Shizhu He 1 , Kang Liu 1 , Jun Zhao 1 , The semantic web is receiving increasing attentions because of its bright future. In the semantic web, ontologies are essential components which are explicit specifications of conceptualization [10]. Thus, a large number of ontologies have been created in the last decade. Although many of them describe the same domain, they cannot share informa-tion with each other since they are designed by different conventions. Hence, ontology matching is required due to the heterogeneous and distributed nature of ontologies [4]. level techniques [7] are widely utilized. They take advantage of lexical information as essential elements. However, merely employing the string surface similarity is impracti-cable. For example,  X  X ournal X  and  X  X eriodical X  cannot be matched, while  X  X ournal X  and  X  X ourney X  actually might be aligned. To settle this problem, WordNet similarity [22] was employed to obtain the semantic similarities among elements. But the weakness is that the words in WordNet are insufficient. Many elements in ontologies cannot find their correspondences in WordNet. As a result, it is impossible to compute the semantic similarities between these elements with others.
 So we introduce word embeddings to this task. Word embeddings are able to repre-sent a majority of words as vectors in a semantic space. The words X  similarities then can be worked out by using simple strategies, such as the cosine similarity, Euclidean distance, etc. Word embeddings have been proved to be effective in several NLP tasks, such as named entity recognition, part-of-speech tagging and semantic role labeling [5]. However, there is no straightforward evidence that word embeddings are effective for ontology matching. Thus, in this paper, we learn word embeddings using Wikipedia as training data, and testify the capacity of them.
 ing behind the words. Compared with WordNet, word embeddings already contain more words. Moreover, we proposed a hybrid method to combine word embeddings and edit distance together. To the best of our knowledge, this is the first usage of word embed-dings in ontology matching field.
 ment Evaluation Initiative (OAEI) 2013 1 benchmark and conference track, real-world ontologies (Freebase [2], YAGO2 [11] and DBpedia [1]). Concretely, we compared the performance of edit distance, WordNet, Latent Semantic Analysis (LSA), word embed-dings and a hybrid method using both edit distance and word embeddings. The results demonstrate that the performance of the hybrid method outperforms the others. Section 3, we describe the setup of the paper. Section 4 presents our matching algorithm in detail. Section 5 shows the experiments and the analysis. We conclude this paper in the final section. Ontology matching, a solution to the semantic heterogeneity problem [23], is a cru-cial part to achieve the goal of the semantic web. So far, dozens of ontology matching systems have been created. Lexical information, including names and labels describ-ing entities are valuable to matching systems. Essential correspondences between two ontologies are found by element-level matchers. A simple and efficient method is to calculate the surface similarity between the strings. Most of the string-based metrics have been evaluated [25, 3]. Edit distance was widely adopted by many matching sys-tems, such as RiMOM [14], ASMOV [12] and AgreementMaker [6], to measure the similarity between two words.
 strings, like  X  X ournal X  and  X  X eriodical X . To deal with this problem, WordNet [20] was employed for ontology matching. Most of the state of the art matching systems took advantage of WordNet [17]. There are three principal ways to obtain WordNet simi-larity [17], namely edge-based method, information based method and hybrid method. WordNet definitely discovers the meaning of a word and resolves part of the synonym problem. The biggest shortcoming of WordNet is its low coverage. If either of the two words is out of range, the WordNet similarity between them cannot be figured out. through deep neural networks. Each dimension of the embeddings represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties [26]. positive results. These tasks include parsing [24], language modeling [19] and sentiment classification [9], etc. Collobert et al. [5] proposed a unified neural network architecture and learning algorithm, which improved the performance of named entity recognition, part-of-speech tagging and chunking. However, to the best of our knowledge, there is little work which employed word embedding in the task of ontology matching. 3.1 Problem Statement We define an ontology O by a simplified 3-tuple ( C, P, I ). C stands for the classes , denoting the concepts. P represents the properties , indicating the relations within the ontology. I means the individuals , which are the instances of classes. Classes, proper-descriptions, i.e., names , labels and comments . For instance, an entity X  X  name is  X  X our-nal X ;its label is  X  X ournal or magazine X  and its comment is  X  X  periodical publication collecting works from different authors X .
  X  O 1 , e 2  X  O 2 } . Every element in the set is called a correspondence. e 1 is an entity in O 1 , and e 2 is an entity in O 2 . con is the confidence of the correspondence. preconditions in advance. First, the hierarchy structure is not taken into account. It is because the structure-level matchers are error-prone and strongly depend on the results of element-level matchers [21]. Only contrasting the performance of the element-level matchers makes it easier to discover the capability of the newly added word embeddings method. Second, following the convention of OAEI 2013 benchmark and conference track, the correspondences only have equivalence (  X  ) relations. 3.2 WordNet Similarity There are many proposed ways to use WordNet to acquire word similarity. In this paper, we employ three representative methods.
 taking advantage of common super-concept and paths.
 nodes on the path from C 1 to C 3 . N 2 is the number of nodes on the path from C 2 to C 3 . N 3 is the number of nodes on the path from C 3 to root.
 putational method. It bases on the idea that the similarity between A and B is measured by the ratio between the amount of information needed to state the commonality of A and B and the information needed to fully describe what A and B are. The final formula is as follows.
 and C 2 .
 tion content to edge-based method to measure the semantic similarity between words. for word w . IC ( c 1 ) and IC ( c 2 ) are the information content of c 1 and c 2 respectively. LSuper ( c 1 , c 2 ) denotes the lowest super-ordinate of c 1 and c 2 . 3.3 Latent Semantic Analysis LSA assumes that there are some underlying structures in the pattern of words. Given training data, LSA first generates a matrix of co-occurrences of each word in each document, and the sequential order of the words is not concerned. Then singular-value decomposition (SVD) is applied. Instead of representing documents and terms directly as vectors of independent words, LSA represents them as continuous values on each of the k orthogonal indexing dimensions derived from the SVD analysis [8]. Thus we harvest a word represented by a vector in a latent semantic space. It is expected that synonyms will be mapped to the same direction in the latent semantic space. source software named S-Space 2 is available to train LSA models. We finally generate words represented by 50-dimensional vectors for experiments. 3.4 Word Embeddings Deep learning approaches gain focus owing to the great success of their applications in fields like computer visions. Tremendous researches have investigated the effectiveness of deep learning methods on NLP tasks. Word embeddings are trained by deep neural networks, and manage to demonstrate their powers in many traditional NLP tasks. In the field of ontology matching, however, they have never been utilized. Inspired by the potential of word embeddings, they are given great expectations to leverage the semantics of words. We are not intended to describe the training detail because it is complicated and not closely related to ontology matching task itself. But it is necessary to explain the word embeddings we use. is an efficient implementation of the continuous bag-of-words and skip-gram architec-tures for computing vector representations of words. The theory is detailed in [18]. In practice, we discard less frequent words that occur less than five times in the whole corpus. We generate unified 50-dimensional word embeddings. 4.1 Element-level Matching Algorithm Algorithm 1 Element-level matching algorithm ontologies are first parsed by JENA API 3 . Then the entities in both ontologies are pre-processed. We mainly extract the lexical information, e.g., names, labels and comments of an entity, in this step. Then element-level matching is implemented. Finally the eval-uation module provides the matching results.
 The correspondence ( e 1 , e 2 , con ) is then added to the alignment, the confidence con is the similarity between e 1 and e 2 . The full algorithm is described in Algorithm 1. Notice that in the hybrid method, Sim ( name 1 , name 2 ) , Sim ( label 1 , label 2 ) and Sim ( com  X  ment 1 , comment 2 ) are the maximal value of edit distance similarity and word embed-dings similarity. usually filtered by some constraint and further used by the structure-level matchers. Promoting the recall of element-level matcher is of great importance. 4.2 Sentence Similarity Algorithm In most scenarios, an entity in ontology is described by sentences consisting of more than one word. Sometimes entities have multi-word names, not to mention their labels and comments. How to get the similarity between these sentences becomes an obstacle for element-level ontology matching. To cope with this problem, we propose a heuristic method to compute sentence similarities. The basic idea of this method is that if more words in one sentence having similar words in the other, the two sentences are more similar. This method is similar to the way of getting the semantic similarity between sentences [15].
 Algorithm 2 Sentence Similarity between S 1 and S 2 similarity between S 1 and S 2 can be acquired by Algorithm 2. Where Sim ( w 1 , w 2 ) is the similarity between word w 1 and word w 2 . If w 1 and w 2 are represented by vec-tors, we use their dot product as similarity. In practice, we found that most words in a sentence have corresponding vectors. If a sentence contains many words that cannot be represented by vectors, it is probably that the sentence cannot be aligned by other sentences. So the proposed algorithm is rational.
 zero. This is common because of the low coverage of WordNet. Likewise, in word embeddings method, if either w 1 or w 2 is not in the training data, Sim ( w 1 , w 2 ) will be zero. Data Sets: The data sets we use contain two parts: temically generated from a seed bibliographic reference. This seed ontology is modified to generate new ontologies, and the task is matching the new ones to the original ontol-ogy. The goal of the conference track is to find alignments within a collection of seven ontologies describing the domain of organizing conferences. The ontologies are from different origins to make sure the heterogeneity. than 7000 properties, and we chose the most frequent 88 properties. YAGO2 has 125 properties, and we extract 1370 properties from DBpedia.
 s respectively, hence the performance could be measured by precision, recall and F-measure. There are dozens of test cases in each track, so the final results are given by the harmonic means according to [21]. Where | A i | denotes the correspondences found by matching system in each alignment, | C i | refers to correct correspondences and | R i | is the reference answers. the output alignments independently. Only the correspondences accepted by both the checkers are labeled as correct.
 employ Wu and Palmer [27] X  X  method to obtain edge-based similarity; employ Lin [16] X  X  method to obtain information-based similarity; employ Jiang and Conrath [13] X  X  method to obtain hybrid WordNet similarity. We use WordNet (Wup) , WordNet (Lin) and WordNet (Jcn) to represent them in the results. LSA on training data, thus the similarities can be calculated as cosine similarities. We train LSA on Wikipedia (version 20130805) to acquire semantic vectors. 5.1 Benchmark and Conference on benchmark test case 205, which replaces some names of entities with their syn-onyms. It is because we believe that word embeddings are adept in dealing with syn-onyms. The results of test case 205 can be seen in Table 2. The results of conference track are given by Table 3.
 ods do. benchmark and conference track. 5.2 Real-world Ontologies The matching tasks between these ontologies will yield three alignments. Our algorith-m only generates the alignments of properties, because the alignments of classes and individuals are too large to evaluate. The final results are shown in Table 4. appropriate matching. methods in all the three tasks. beddings method. It is because edit distance does not contribute to the hybrid method in these three tasks. 5.3 Comparisons among Word Embeddings two other word embeddings to fulfil the ontology matching tasks. These two word em-beddings are C&amp;W by Turian [26] and SENNA [5]. The comparisons are given by Figure 1 and Figure 2.
 word embeddings, and our word embeddings achieve better results than the other two. The reason is that these word embeddings are trained by different data. C&amp;W by Turian uses Reuters RCV1 as training data; SENNA uses partial Wikipedia and Reuters RCV1 as training data; our training data is the whole Wikipedia. This result indicates that the performance is influenced by the training data. In this paper, we have introduced word embeddings to ontology matching. The exper-iments show that using word embeddings as a supplement is preferable in ontology matching. The hybrid method combining edit distance and word embeddings achieves the best results.
 training word embeddings specifically. For example, when dealing with biomedical on-tologies, we can use relative training data instead of Wikipedia. This amounts to using external resources. It is hopeful since training corpus affect the performance dramatical-ly as our experiments demonstrated. Another possible improvement could be generated by a proper matching strategy. Assigning different weights to semantic similarity and traditions ones appropriately will boost the performance.
 This work was supported by the National Natural Science Foundation of China (No.612 02329, 61272332, 61333018), CCF-Tencent Open Research Fund and the Opening Project of Beijing Key Laboratory of Internet Culture and Digital Dissemination Re-search(ICDD201301).

