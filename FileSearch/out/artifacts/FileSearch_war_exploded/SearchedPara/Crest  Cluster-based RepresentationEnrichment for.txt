 The prevalence of Internet-enabled devices (e.g., laptops, tablets, and mobile phones) and the increasing popularity of social platforms are changing the way we consume and produce information on line. A large portion of the data acces-sible online is user-generated content in various forms, such as status updates, micro-blog posts, comments, and short product reviews. In other words, much user-generated textual c ontent is in the form of short text . The unique charac-teristics (e.g., shortness, noisiness, and sparsity) distinguish short text from the well written documents such as news articles and most Web pages. These unique characteristics call for a revisit of the techniques developed for text analysis and understanding, including text classification.

Text classification refers to the task of automatically assigning a textual doc-ument one or more predefined categories. It has been heavily studied for decades and many techniques have been proposed and have demonstrated good classifi-cation accuracies in various application domains [13,16]. Nevertheless, most text classification techniques take advantage of the information redundancy natu-rally contained in the well-written documents (or long documents in contrast to short text). When facing with short text, the shortness, noisiness, and sparsity, adversely affect the classifiers from achi eving good classification accuracies. To improve short text classification accuracy has since attracted significant atten-tion from both the industries and academia.

To deal with the shortness and sparsity, most solutions proposed for short text classification aim to enrich short text representation by bringing in additional semantics. The additional semantics could be from the short text data collection itself (e.g., named entities, phrases) [7] or be derived from a much larger external knowledge base like Wikipedia and WordNet [4,7,10]. The former requires shal-low Natural Language Processing (NLP) techniques while the later requires a much larger and  X  X ppropriat e X  dataset. Very recently, in stead of enriching short text representation, another approach known as search-and-vote is proposed to improve short text classification [15]. The main idea is to mimic human judg-ing processing by identifying a few topica l representative keywords from each short text and use the identified topical keywords as queries to search for similar short texts from the labeled co llection. Very much similar to k -nearest-neighbor classifier, the category label of the short text for classification is voted by using the search results. Note that, the aforementioned different approaches deal with the shortness and sparsity of short text from very different perspectives and are mostly orthogonal to each other. In other words, on the one hand, these different approaches could be combined to potentially achieve much better classification accuracies than any of the approaches alone; on the other hand, this calls for further research to improve each individual researches.

In this paper, we focus on improving short text classification accuracy by enriching the text representation, by not only using its raw words (e.g., bag-of-words) but also topical representations. Our approach naturally falls under the representation enrichment approach. However, our approach is different from the earlier works in representation enrichment because of two reasons. First, we do not use shallow NLP techniques to extract phrases or any specific patterns because most short texts are noisy preventing many existing NLP toolkits from achieving good accuracy. Second, we do no t use external knowledge base like Wikipedia because some of the short text data collection might be from very specific or niche areas where it is hard to find an  X  X ppropriate X  and large dataset. In other words, we consider that if we can discover internally useful knowledge solely from the training dataset when an  X  X ppropriate X  large external dataset is not available. More specifically, w e propose a generic method named Crest to first discover  X  X igh-quality X  topic clusters from the training data by grouping similar (but not necessary from the same category) training examples together to form clusters. Each short text instance is then represented using the topical similarities between the short text and the topic clusters in addition to its words feature vector. The main advantages of Crest include the following:  X  Low-cost in knowledge acquisition . As we mentioned above, Crest does not  X  Reduction in data sparsity . The topic clusters discovered from the training  X  Easy in implementation and combination .The Crest framework is easy to The rest of the paper is organized as follo ws. Section 2 surveys the related work in short text classification. Section 3 describes the Crest method. Section 4 reports the experimental results and Section 5 concludes this paper. Short text processing has attracted research interests for a long time, particularly in the meta-search applications to group similar search results into meaningful topic clusters. Nevertheless, the key research problem in search snippet cluster-ing is to automatically generate meaningful cluster labels [3]. Another direction of research in short text processing is to evaluate the similarity of a pair of short texts using external knowledge obtained from search engines [11,17]. In [1], se-mantic similarity between words is obtai ned by leveraging page counts and text snippets returned by search engine.

For short text classification, the work on query classification is more related as each query can be treated as a piece of short text. In [14], the authors use titles and snippets to expand the Web queries and achieve better classification accuracy on query classification task compared to using the queries alone. How-ever, the efficiency and the reliability issues of using search engine limit the employment of search-based method, e specially when the set of short text un-der consideration is large. To address these issues, researchers turn to utilize explicit taxonomy/concepts or implicit topics from external knowledge source. These corpora (e.g., Wikipedia, Open Directory) have rich predefined taxon-omy and human labelers assign thousands of Web pages to each node in the taxonomy. Such information can greatly enrich the short text. These research has shown positive improvement though they only used the man-made cate-gories and concepts in those repositories. Wikipedia is used in [6] to build a concept thesaurus to enhance traditional content similarity measurement. Sim-ilarly, in [8], the authors use Wikipedia concept and category information to enrich document representation to addr ess semantic information loss caused by bag-of-words representation. A weighted vector of Wikipedia-based concepts is also used for relatedness estimation of short text in [5]. However, lack of adapt-ability is one possible shortcoming of using predefined taxonomy in the above ways because the taxonomy may not be prop er for certain classification tasks. To overcome this shortcoming, the authors in [10] derived latent topics from a set of documents from Wikipedia and then used the topics as additional features to ex-pand the short text. The idea is further extended in [4], to explore the possibility of building classifier by learning topics at multi-granularity levels. Experiments show that the methods above using the discovered latent topics achieve the state-of-the-art performance. In summary, these methods try to enrich the rep-resentation of a short text using additional semantics from an external collection of documents. However, in some specific domain (e.g., military or healthcare) it might be difficult to get such high quality external corpora due to privacy or confidentiality reasons.

Most germane to this work is the approach proposed in [2] which applies probabilistic latent semantic analysis (pLSA) on text collection and enriches document representation using the late nt factors identified. However, pLSA be-comes less reliable in identifying latent topics when applying to very short texts, due to the difficulties of sparsity and shortness. In this paper, we use a different approach to find the topics embedded in the short text collection by clustering the documents in the collection. Most existing topic-based methods rely on large external sources (such as Wikipedia or search engines). However, there exist tough situations in some specific domains (e.g., military or healthcare) where lack of reliable high quality external knowledge repositories. This limits the employment of these methods. In this scenario, the only available res ource is the collection of labeled short texts. How to exploit the limited collection at utmost becomes crucial in short text classification.

The good performance of topic-based methods shows latent topics can be very useful to short text classification. Since the document collection is the only available resource in our scenario, we d erive latent topics from the document collection itself by exploiting clustering. Then, we use the topic clusters to enrich the representation for short texts. The general process of Crest ( C luster-based R epresentation E nrichment for S hort T ext Classification ) method is illustrated in Figure 1.

Suppose a document collection D = { ( x i ,y i ) } n i =1 has n shorttextdocuments, where x is pre-processed short text document and x  X  X = R d .Inthispaper,we adopt tf -idf [12] representation. And y is category label, y  X  Y = { 1 , 2 ,...,k } . L is a learning algorithm, training a classifier h : X  X  Y . 3.1 Topic Clusters Generation Clustering is good at finding knowledge structure inside data. Crest exploits clustering to find topics. Intuitively, for each high-level category, for example  X  X usiness X , it has its a few sub-topics, such as  X  X ccounting X , X  X inance X . The sub-topics could have different topical words, especially when the text is very short. In other word, each cluster contains terms and concepts mainly in one sub-topic which we could take advantage of to enrich short texts and reduce their sparsity.

However, due to the sparsity of short text, the similarity of a pair of short text instances may not be reliable enoug h when it is reflected by distance in a clustering method. Thus, the resulting clusters may not be qualified as topics. The challenge here is to select  X  X igh-quality X  clusters as topic clusters .Notethat, even though there exist many clustering methods, not all clusters generated by a clustering method is useful. For instance, a cluster containing very few documents (say, only one) or a large number of documents from many different categories are not useful clusters. The clusters with very few documents fail to cover enough concepts in a sub-topic while the clusters containing too many documents are not topically specific.

In summary, Crest selects  X  X igh-quality X  clusters as topic clusters with two criteria: (i) high support , i.e., the number of documents in a cluster is large; and (ii) high purity , i.e., the percentage of dominant category of the short texts in a cluster is high.

Suppose a cluster Q contains a set of short text instances, Q = { ( x i ,y i ) } q i =1 , then the support of Q is the number of instances in it, i.e., And the purity of Q is the percentage of dominant category of the short texts in it, which is defined as: where, I ( x ) is indicator function, I ( x )=1if x = 1 and 0 otherwise.
More specifically, Crest uses a clustering method, such as EfficientHAC [9], to group short texts into clusters. When a cluster X  X  purity is low, it does not rep-resent a sub-topic even if its support is h igh. Therefore, we select the clusters whose purity values are larger than a pre-defined threshold. We then get a set of candidate-clusters C = { C 1 ,C 2 ,...,C | C | } . To select clusters with high  X  X upport X  and  X  X urity X , we assign a weight to each cluster in C indicating the quality to be a topic cluster of each cluster. Let w i be C i  X  X  weight: Then the top N clusters with the highest weight s are selected as topic clusters T , which are rich of representative terms or concepts in particular sub-topics, and are later used to enrich short text X  X  representation.

In most cases, the weights of candidate-clusters in C are influenced more by their support values. It is reasonable, since the purity values of candidate-clusters in
C are all larger than a purity threshold, which is often a relatively high value to assure all clusters in C be of high purity. 3.2 Representation Enrichment Using Topic Clusters Crest enriches representation of short tex t by combining a short text instance X  X  original feature vector, i.e., tf -idf vector, and the additional information from the topic clusters. To extract knowledge from topic clusters, a good choice is to use the similarity between a short text instance x and each of the topic cluster T similarity between a short text instance x and a topic cluster T i reflects how likely the common terms or concepts of the sub-topic represented by T i would appear in the text if the  X  X hort X  text were longer.

For example, a short text (taken from the benchmark dataset used in our experiments) is  X  X anufacture manufact urer directory dir ectory china taiwan products manufacturers directory-taiwan china products manufacturer direc-tory exporter directory supplier directory suppliers business X . And there are two topic clusters: cluster 1 represents a sub-topic of  X  X usiness X  category, and cluster 2 represents a sub-topic of  X  X ealth X  category. Cluster 1 contains concepts like  X  X elation X ,  X  X roduce X ,  X  X achine X , and so on. Cluster 2 contains concepts like  X  X ymptoms X ,  X  X reatment X ,  X  X irus X ,  X  X iet X . Obviously, the short text is more similar to cluster 1. And if it were longe r, the word  X  X roduce X ,  X  X achine X  have a larger chance to appear in the text. Algorithm 1. The Crest Algorithm
Define the similarity between a short text x and a topic cluster T as: In sim ( x ,T ), the dot product is used to compute the initial similarity value between short text and topic cluster. S ince the lengths of topic clusters are varying, to reduce their influence, we normalize the lengths of both short text and topic cluster to get final similarity, i.e., cosine similarity.

Let s =( sim ( x ,T 1 ) ,...,sim ( x ,T N )) be the similarity vector, then the enriched representation of x is:
The pseudo code of Crest is shown in Algorithm 1, in which the clustering algorithm EfficientHAC can be replaced by another h ierarchical clustering algorithm. Since the problem setting of this paper is that there is no external knowledge sources, it is inappropriate to compare Crest with methods relying on some external knowledge source. We compare Crest with original representation of short text (i.e., tf -idf vectors, denoted by  X  X aw X ). In Crest , the clustering strategies EfficientHAC [9] is single-link, and the purity threshold is set to be 0.9. We test different values 10 , 30 , 50 , 70 , 100 , 120 for the number of topic clusters N . We use SVM as learning algorithm for both Crest and Raw representations using SVM light with default parameter settings 1 . We run experiments on the benchmark dataset of search snippets collected by [10] and the statistics of the dataset is shown in Table 1.

For each parameter settings, we run the experiment for 20 times, then compute the average value. We record the F 1 measurement. Table 2 shows the F 1 results, where the tabular in boldface means that Crest  X  X  result is significantly better than Raw by pairwise t -test with significance level at 0.95,  X  best  X  X sthebest F 1 value among Crest with different N  X  X ,  X  avg.  X  is the average F 1 value over all categories. The results are plotted in Fig. 2.

These results show that Crest improves the classificat ion performance con-siderably compared to Raw in every category with almost all parameter settings. Especially, in some specific categories such as  X  X usiness X  and  X  X olitics X , the im-provement is as large as 17.13% and 19.51%, respectively. The results show that Crest method utilizing topic clusters extracted from limited training examples to enrich short texts is a useful way to ov ercome the shortness and sparsity of short texts. From Fig. 2 we can see that Crest is very robust to the change of N , the number of topic clusters. Even when N is very small, Crest im-proves the performance largely in almost all categories. This shows the power of the enriched representation by exploring topic clusters. The only exception is that in category  X  X ngineering X , only when the number of topic clusters N is greater than 70 can Crest improves the performance. On e possible reason is that  X  X ngineering X  category has fewer instances than other categories but covers rel-atively a large topic. The instances in this category are harder to be gathered together by a clustering method. Crest manages to improve the performance of this category by increasing the number of topic clusters in N .

To further study how parameters will affect Crest , we record the F 1 results of Crest with different clustering strategies (single-link or complete-link) and different purity thresholds (0.85, 0.90, 0.95) while fixing N = 70. The results are shown in Fig. 3. Generally speaking, Crest is very robust to the change of these parameters when purity threshold is above 0.90. Since the topic clusters with higher purity would be more topic-specific, higher purity threshold leads to more helpful critical terms or concepts. On the other hand, clustering strategy doesn X  X  affect the performance significantly. Crest is slightly more sensitive to purity threshold when using the single-link strategy than using the complete-link strategy.

The above experimental results lead to the following conclusions: (1) Crest can greatly improve the short text classification performance in term of F 1 mea-sure by enriching the representation with topic information; and (2) Crest is robust to parameter settings. Short text classification problem attracts much attention from information re-trieval field recently. In order to handle i ts shortness and sparsity, various ap-proaches have been proposed to enrich shor t text to get more features like latent topics or other information. However, most of them rely on large external knowl-edge sources more or less. These methods solve the problem to some extent, but still leave large space for improvement, especially under the hard condition that no external knowledge source can be acquired. We proposed Crest method to handle the short text classification in such tough situation. Crest generates  X  X igh-quality X  clusters as topic clusters from training data by exploiting clus-tering method, and then uses the topic information to extend representation for short text. The experimental results showed that compared to the original representation, Crest can significantly improves the classification performance.
Though we see positive improvement brought by Crest , there are still room for further consideratio n to boost the performance. For example, we can try to combine Crest with other methods for short text classification, such as methods relying on external knowledge sources. And organizing  X  X igh-quality X  clusters in multi-granularity way to investigate whether it can further improve Crest is another interesting problem worth exploring.
 Acknowledgement. This work was supported by NSFC (No. 61105046),SRFDP (Specialized Research Fund for the Doctoral Program of Higher Education, by Ministry of Educatio n, No. 20110092120029), and Open F oundation of National Key Laboratory for Novel Software Technology of China (KFKT2011B01). The work of the second author was supported by MINDEF-NTU-DIRP/2010/03, Singapore.
