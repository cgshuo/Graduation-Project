 In recent years, outlier detection has b een widely used, specially in network intrusion detection [1], credit card abuse analysis [2], measurement result analysis of abnormal data [2] and so on. Lots of outlier detection algorithms in deterministic dataset have been proposed, such as model-based [3], index-based [4], distance-based [5], density-based algorithms [6] and so on. In these years, research has turned into uncertain datasets. Uncertainty is inherent in data collected in various applications, such a s sensor networks, marketing research, and social science [7]. Sensors in a wireless network can be at different positions at different times with different probabilities. Many datasets published are deformed to hide information for privacy protection. In this case, distance among data objects, region density and many other metric are uncertain. These properties prevent classic outlier detectio n methods in deterministic datasets to be used in uncertain datasets directly.

In order to detect outliers in uncertain datasets, C. C. Aggarwal et al. [8] propose a density-based  X  - X  algorithm to detect outliers in uncertain datasets. They estimate the density of regions. For any data object, with lower probability to be in a high density region, more likely it would be an outlier. They propose the definition of  X  -probability of a data object. That is defined as the probability that the uncertain data object lies in a region with (overall data) density at least  X  [8]. An uncertain data object X i would be a (  X  ,  X  )-outlier, if the  X  -probability of X i in some subspace is less than  X  .For  X  -probability estimation, authors use a sampling procedure to generate values according to some distribution. For this intention, a value is obtained from uniform distribution as the input of the inverse function of some cumulative density function. However, there are some limitations. First, the data must be in some determinate distribution. But its distribution would usually be unknown in real application. In some application, users can not get complete data distribution. Besides that, the method assumes that the inverse of the distribution can be calculated efficiently. It is difficult too. Although the data is assumed to be in normal distribution in their experiments, the sampling procedure is with heavy time cost. Similarly B. Jian et al. [7] use kernel density estimation to get den sities of uncertain objects and their instances. In that model, value of uncertain data is dominated by conditioning attributes. They use kernel density estimation with Gaussian kernels to estimate the probability density of a distribution. In this method, obvious conditioning attributes must be determined first. This limits its application. Wang et al. [9] introduce distance-based outlier detection into uncertain dataset for the first time. They utilize x-tuple model to describe data objects. The method enlightens our research. Nonetheless, they do n X  X  take into account data variety.
In order to overcome above problems, we propose the concept of relative outlier and a novel distance-based outlier detection algorithm, RPOS algorithm, focusing on top K outlier detection in uncertain datasets. In our research, all data objects exist with independent probabilities. A data object could show various values with different probabilities. We compare every pair of data objects to find the one more like to be an outlier relative to the other one. Global distribution of data value is unnecessary in our research. 2.1 Possible World and x-tuple Model We describe uncertain datasets using possible world semantics [10] and x-tuple model [11]. An x-tuple containing several tuples denotes a data object. A tuple containing several attributes denotes an instance. Every tuple has its own probability. Tuples in different x-tuples are independent. Tuples in the same x-tuple are mutually exclusive. Probability of an x-tuple is the product of probabilities of tuples in the x-tuple. An x-tuple may not exist if no its tuple exists. A subset of tuples from different x-tuples construct a possible world. Probability of a possible world is the product of probabilities of tuples in the possible world. An example of tuples, x-tuples, possible worlds and their probabilities can be shown in Table 1 and 2. 2.2 Relative Outlier Based on possible world semantics and x-tuple model, we introduce the concept of distance-based outlier into uncertain datasets. In a possible world, every tuple has an outlier score as defined in Definition 1. K tuples ranked at most k according to their outlier sco res in descending order are top K outliers in the possible world. If a tuple is a top K outlier in a possible world, the x-tuple containing the tuple would be a top K outlier in the possible world. The tuple X  X  outlier score is also the x-tuple X  X  outlier score. Since there would be many different possible worlds for data variety, top K outliers are uncertain.
Intuitively expected rank of an x-tupl e in different possible worlds could be used to detect outlier. But expected rank would be easily influenced by sparse noise. For example, an x-tuple A is ranked k +1 in every possible world, another x-tuple B is ranked 10 k in a possible world but k in others. x-tuple B is more like to be an outlier since it X  X  ranked in front of A in most possible world. However, the expected rank of B may be smaller than that of A .So A would be considered as outlier in error. Besides that, the concept of uncertain top-k query [12], that is returning a list of k records which has the highest probability to be the top-k list in all possible worlds, is similar with our problem. But it can also be influenced by sparse data. For example, an x-tuple A is ranked k +1 in every possible world, another x-tuple B is ranked k in a possible world but 10 k in others. Although A is not a top K outliers in any possible world, it X  X  more like to be an outlier than B , since it X  X  ranked in front of B in most cases.

In this paper, We propose the concept of relative outlier based on multiple comparisons. All x-tuples are compared with each other to evaluate their possibilities to be outliers. Above problems can be overcome in our method since outliers detected are more likely to have higher outlier scores than other x-tuples. Definition 2 defines the relative outlier between two x-tuples. Definition 3 defines top K relative outliers in an uncertain dataset.
 Definition 1. Outlier score of a tuple is the mean distance to its n nearest neighbors [13,1,14].
 Definition 2: For two x-tuples A and B in an uncertain dataset, if A is with higher probability to has higher outlier score than B , A is an outlier relative to B. Outlier score in a possible world is computed based on Definition 1.
For example, x-tuple A has higher outlier score than B with probability 0.5, x-tuple B has higher outlier score than A with probability 0.4 and A or B does not exit with probability 0.1. A would be an outlier relative to B .
 Definition 3: Top K relative outliers in an uncertain dataset are those x-tuples. They are ranked top K according to the amount of x-tuples relative to which they are outliers based on Definition 2.
 For example, x-tuple A is an outlier relative to another 5 x-tuples and x-tuple B is an outlier relative to another 6 x-tuple. B is more likely to be a top K outlier than A . If there are just K x-tuples who are outliers relative to at least another 6 x-tuples, B would be included in top K outliers, but A would be excluded. If there are K x-tuples who are outliers relative to at least another 7 x-tuples, both A and B would not be top K outliers. In this section, we propose the basic RPOS (Relative Probability Outlier Score) algorithm to detect the top K x-tuples most likely to be outliers. For x-tuples A and B , we compute the probability P ( A&gt;B ) meaning that A  X  X  outlier score is higher than B  X  X  and P ( B&gt;A ) meaning that B  X  X  outlier score is higher B , and vice versa. Relative Probability Outlier Score( RPOS ) of x-tuple A relative to B is 1 in this case. It X  X  noted as RPOS ( A  X   X  B )=1. At the same time, RPOS ( B  X   X  A )=-1. The sum of A  X  X  RPOS s relative to other x-tuples is all -RPOS ( A ) = X  X  S,X = A RPOS ( A  X   X  X ). S is the dataset. x-tuples with top K highest all -RPOS s would be outliers.

Algorithm 1 gives details of RPOS algorithm. It compares all x-tuples with others to calculate their all -RPOS s and sort all x-tuples in descending order(lines 2-7). OutlierScore algorithm computes an x-tuple X  X  relative probability outlier score with another x-tuple. sgn ( x ) = -1,if x &lt;0; 0,if x =0; 1,if x &gt;0.

Algorithm 2 gives details of OutlierScore algorithm. Because an x-tuple consists of several distinct tuples, comparison between two x-tuples is actually comparison among tuples from distinct x-tuples(lines 2-10). Unfortunately Definition 1 can not be used to compute a tuple X  X  outlier score directly. Because all tuples X  existence are uncertain, a tuple would exist in several possible worlds with distinct probabilities. So a tuple would have different neighbors in different possible worlds. In order to overcome this problem, we propose relative probability outlier score of a tuple, that is a tuple X  X  outlier score relative Algorithm 1. RPOS Algorithm 2. OutlierScore to another tuple. RelativeOutlierScore _ Tuple algorithm is used to calculate it. If RelativeOutlierScore _ Tuple returns 1, the first tuple would has higher probability to get larger outlier score than the second tuple. 3.1 Relative Probability Outlier Score of a Tuple We show details of RelativeOutlierScore _ Tuple algorithm in this subsection. Supposing t 1 i and t 2 j are respective tuples of x-tuples T 1 and T 2 , score [ t 1 i ] and score [ t 2 j ] are their deterministic outlier scores in a possible world and computed as follows: deterministic outlier score is greater than that of t 1 i . score [ t 2 j ,t 1 i ] =-1atthe same time; score [ t 2 j ,t 1 i ] = 1 at the same time.

In a large uncertain dataset, the probability that two tuples have same deterministic outlier score is near 0. score [ t 1 i ,t 2 j ] can be computed as follows: score [ t 1 i ,t 2 j ] =1,if P ( score [ t 1 i ] &gt;score [ t 2 j ] ) &gt; 0.5; score [ t 1 i ,t 2 j ] =-1,if P ( score [ t 1 i ] &gt;score [ t 2 j ] ) &lt; 0.5.
The intuitive method to compute score [ t 1 i ,t 2 j ] is to traverse all possible worlds, compute score [ t 1 i ] and score [ t 2 j ] and accumulate the probability Traversing all possible worlds would cost exponential time overhead. Suppose S is the dataset, | S | = N and every x-tuple includes N x tuples. For any tuple t , the amount of possible worlds containing t  X  X  n nearest neighbor tuples would be at least C n N N x n . In order to lower time cost, we use sampling technique to get an approximate P ( score [ t 1 i ] &gt;score [ t 2 j ] ).

Tuples in an x-tuple are sampled according to their probabilities. All tuples sampled from different x-tuples composed a possible world. X k is a random variable in the k th sampling. In the possible world produced by the k th sampling, X k =0. When all tuples X  probabilities are determined, P ( score [ t 1 i ] independent identical distribution random variables. m is sampling number. Variable X = X k ,1  X  k  X  m . So we can get E [ X ]= E ( X k )= E ( X k ) = mp . That means p = E [ X ] / m .Weuse X to estimate E [ X ] and estimate p using X/m . In order to ensure the accuracy of esti mation, sampling must satisfy some conditions.
 When p  X  0.5, the probability of a wrong estimation, namely X/m&gt; 0.5, is that: According to Chernoff bound : If m is determined, 1-exp {-(0.5-p ) 2 m /3 p } is the accuracy of the estimation. It X  X  only dominated by p .
 Similarly, when p &gt; 0.5, the probability of a wrong estimation is that: 1-exp { -( p -0 . 5) 2 m /2 p } is the accuracy of estimation. In summary, the confidence of the estimate is no less than 1-exp { -(0.5-p ) 2 m /3 p } . For example, when p =0.7 and m =100, accuracy of the estimation is no less than 0.94. Algorithm 3. RelativeOutlierScore_Tuple
DistanceOutlierScore ( t ) k the deterministic outlier score of t in the possible world produced by the k th sampling. In DistanceOutlierScore ( t ) k , outlier score of the target tuple in a possible world is computed as Definition 1. However, it X  X  in high time cost to detect a tuple X  X  n nearest neighbors like classic methods e.g. RBRP algorithm [15] in each sampling. In order to reduce time cost, we construct a neighbor list L t for every tuple t . The node of L t is a novel Neighbor structure: t neighbor : a neighbor tuple of t ; tag : it is used to state whether t neighbor is selected in the sampling. All Neighbor sin L t are sorted according to d neighbor in ascending order. Tuples in the same x-tuple with t will not be in L t .Let | L t | = L . In each sampling, we traverse L t in order and get n nearest selected tuples noted by tag s. The mean distance of these nd neighbor sis t  X  X  outlier score. When L is large enough, L t could contain almost all n nearest neighbors of t in each sampling. While n nearest neighbors may exist in the latter part of the list with a low probability. Too large L leads to redundant memory consumption. Proper value of L should be set. In basic RPOS algorithm above, every x-tuple has to be compared with all others. Its time cost is proportional to the square of a dataset X  X  cardinality. In order to improve running speed of basic RPOS algorithm, we introduce two pruning strategies. 4.1 Strategy 1 In order to detect outlier in high efficiency, we must find every tuple X  X  neighbors quickly. Neighbors of a tuple should be close to each other. These neighbors could construct in a cluster. Using for ref erence from existing methods [15,16], we cluster all tuples in a dataset. Usu ally distances among tuples in a cluster are much less than those among tuples in different clusters. n nearest neighbors of a tuple t would be in the same cluster with t or t  X  X  neighboring clusters. In order to construct L t , we check tuples in the cluster containing t first, and then tuples in neighboring clusters. Other clusters would be in the end. The distance from the L th tuple in L t to t will be a threshold in following process. We name the threshold as h . When we check a following neighbor tuple, if its distance to t is larger than h ,itwouldnotbeinsertedinto L t . On the contrary, it would be a candidate tuple and inserted into L t in ascending order. h is then updated. Because we check tuples near with t first, h will always be small. Tuples far away from t will be pruned soon. In order to accelerate clustering process, we first partition the dataset into several large clusters and then partition every cluster into some sub-clusters recursively.

When we check t  X  X  neighbor t ,if L t has be constructed with threshold h , the distance from one of t  X  X  n nearest neighbor tuples to t would be less than by min { h,D ( t,t ) + h } .

Further, before checking a neighbor cluster C of t , we compute the maximum and minimum distances from t to C first. The maximum distance from t to C is MaxD ( t , C )= D ( t , o )+ r ,where o is the center of C and r is the radius of C . The minimum distance from t to C is MinD ( t , C )= D ( t , o )-r . If MinD ( t , C ) &gt;h ,distancefrom t to any tuple in C can not be less than h . All tuples in C would not be inserted into L t . C will be jumped over. If MinD ( t , C )  X  h , we will check its sub-clusters. In this way, we only need to check a few tuples to construct L t . Smaller search space leads to lower time cost. 4.2 Strategy 2 When we compute all -RPOS of an x-tuple, the x-tuple has to be compared with all other x-tuples. When we compare two x-tuples, we have to compare all tuples from two x-tuples respectively. Time complexity in this process is O ( N 2 x N 2 ) . N is the cardinality of the dataset. N x is the amount of tuples in an x-tuple. But in real application outliers are in minority of the entire dataset. It X  X  wasteful to compare all pairs of x-tuples. Suppose there are K outliers in a dataset and X x-tuples have been checked in RPOS algorithm. If X&gt;K ,wesortthese X x-tuples based on their all -RPOS s and use the K th all -RPOS as the threshold namely H . If expected value of an x-tuple X  X  all -RPOS is less than H ,itcannot be an outlier. When new top K candidate outliers are detected, H is updated. In above process, expected all -RPOS value of an x-tuple can be computed using existed all -RPOS value (as that in line 5 of RPOS algorithm ) of the x-tuple plus the number of x-tuples will be compared with it.
 Further Acceleration 1: Efficiency of pruning process above would be influenced by H .Quickly H increases, more x-tuples could be pruned early. We sort all x-tuples a ccording to expected all-RPOS in descending order. Further Acceleration 2: In order to avoid redundant comparison in pairs of x-tuples, we record all x-tuples have b een compared with and the intermediate result of every x-tuple X  X  all -RPOS . In this section, we evaluate our RPOS algorithm in synthetic and real datasets. All algorithms are implemented in Java. Our experiments are ran on a machine with 2 Intel Core 2 Duo E8400 3GHZ CPUs and 8.1GB RAM running Linux 3.8 Ubuntu 13.04. In our research, all attributes of data are numerical and the value of each attribute is a real number. We compare RPOS algorithm with RBRP [15] and  X  - X  [8] algorithms. B.Jiang X  X  work [7] focuses on condition attributes and B. Wang X  X  work [9] neglects data diversity. It X  X  hard to compare them with our work. In order to use RBRP algorithm to detect outliers in uncertain datasets, we pretreat uncertain data for experiments. All tuples in an x-tuple are transformed into a tuple. The value of the new tuple in every dimension is the weighted mean value of all tuples of the x-tuple in the dimension. Weight of a tuple is its probability. 5.1 Dataset Synthetic Dataset. In order to test the effectiveness and efficiency of different outlier detection algorithms, we constr uct several synthetic datasets. The synthetic data includes N d attributes. Every data entity is an x-tuple including several tuples. A tuple X  X  value in every attribute is numeric. We produce some normal regions in the N d -dimension spa ce. Normal tuples are allocated in these regions. On the contrary, outliers will not be in normal regions. Besides synthetic data values, we also produce the probability for every tuple. We define N
R normal regions in N d dimensions respectively. R i is the normal region N d sub-regions construct a N d -dimension normal region R 1  X  , R 2  X  r = R i  X  / R i determines the size of sub-region R i  X  . We produce N R N d -dimension normal regions in this way. Data objects outside of these N R regions is abnormal. First we produce normal x-tuples in normal regions produced above. Then we produce outlier x-tuples with at least N x abnormal tuples in each x-tuple. In order to evaluate performance of RPOS algorithm, we insert some counterfeit outliers into the dataset as disturbance. A counterfeit outlier x-tuple contains several abnormal tuples. The tuples may be allocated far away from normal regions. But their quantity is small and their probabilities are smaller than those of abnormal tuples in real outliers x-tuples.
 Real Dataset. We choose the real MiniBooNE dataset 2 provided by UCI. Number of entities in this dataset is 130000. Number of attributes is 50. Attribute characteristics are real. We transform the original dataset into an uncertain dataset for our experiments. Every entity in the original dataset is an x-tuple containing one tuple in the uncertain dataset. We fluctuate the value of a tuple to produce other tuples in the x-tuple. The value of a tuple in every dimension fluctuates with probability p f . Fluctuation range is controlled by r f .Thereare at most N x tuples in an x-tuple. We add every tuple X  X  probability as in synthetic dataset. 5.2 Effectiveness Evaluation In this sub-section, we show the evaluation result of different algorithms X  effectiveness to detect outliers. Because n o instance in real datasets is labeled as an outlier, effectiveness eva luation is performed only in synthetic datasets. Some parameters are modified to pr oduce different datasets.

In order to test the influence of the data dimensionality, we perform experiments in 5 and 10 dimensions respectively. We set N R =100, N x =5. Amount of outliers N outlier =0.01 N . Amount of counterfeiters N counterfeiter =0.05 N . N is the cardinality of a dataset. For RPOS algorithm we set parameters L =200 and m =100 (see section 3). For  X  - X  algorithm we do experiments in two situations. First, parameter  X  is set to be 0.8 and  X  is set to be 0.3, and then  X  is set to be 0.9 and  X  is set to be 0.1. Space lack for more parameter setting. But they do not influence experimental results. In Figure 1(a), dimensions amount is N d =5. N d =10 in Figure 1(b). Precision and recall ratio are two test indexes. As shown in these figures, in various amount of dimensions, RPOS algorithm always performs best in three algorithms. With same recall rate, RPOS algorithm detects outliers in high precision.

We then change parameter N R to produce different uncertain synthetic datasets to evaluate the effectiveness of d ifferent algorithms. Data is processed in these experiments with N d =5. N R is set to be different values with r =0.5.  X  and  X  are set arbitrarily. Results can be found in Figure 1(c-f). As shown in above figures, RPOS algorithm performs best in almost all experiments. 5.3 Efficiency Evaluation In this sub-section, we show evaluation re sults of different algorithms X  time cost. Our experiments perform in both synth etic and real datas ets. We implement basic RPOS algorithm ( RPOS ), RPOS with pruning strategy 1 ( RPOS 1), RPOS with pruning strategy 2 ( RPOS 2), RPOS with both pruning strategies 1and2( RPOS 12),  X  - X  algorithm and RBRP algorithm in every experiment. Time complexity of O ( NlogN ) and O ( N 2 ) are shown for comparison. Synthetic datasets are produced with N R =100, r =0.5, N x =5. We change N d in different experiments. In RPOS algorithm, the rate K/N = r k =0.02. For instance, in a dataset containing 10000 x-tuples, K is 200. r k is set to be a constant to avoid its influence. Similarly, we set the rate between amount of clusters and dataset cardinality r C =0.25 to ensure stability of pruning effect. In  X  - X  algorithm, we set  X  =0.06 and  X  =0.5. Experiments results in 5 and 20 dimensions are shown in Figure 2(a,b). Data size increases in X-axis and corresponding time cost is shown in Y-axis.
As shown in Figure 2(a), pruning strategies 1 and 2, especially strategy 2 can accelerate RPOS algorithm. Time cost of basic RPOS algorithm and RPOS 1 are higher than O ( N 2 ) . The main time consumption is from comparison among tuples and x-tuples. Time cost of RPOS 2 and RPOS 12 are lower than O ( N 2 ) but higher than O ( NlogN ) . Pruning strategies can improve basic RPOS algorithm obviously. Speedup using pruning strategies in 20 dimensions is similar as in 5 dimensions.

Time cost are also evaluated in real datasets. First we set parameters p f =0.5, r =0.5. Other parameters keep consistent with those in synthetic datasets. Experiments are implemented in 5 and 20 d imensions respectively. Results of time cost can be shown in Figure 2(c,d). Basic RPOS algorithm runs with higher time complexity than O ( N 2 ) . pruning strategy 2 can improve RPOS algorithm greatly.

We then change p f and r f to produce different uncertainty with N d =20. Time cost is shown in Figure 2(e,f). We can find that results are similar in different experiments. With various values of p f and r f , RPOS 12 performs better than others in time cost.

