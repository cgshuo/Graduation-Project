 In this paper we develop an approach to learn stochastic 3D ge ometric models of object categories from single view images. Exploiting such models for object r ecognition systems enables going beyond simple labeling. In particular, fitting such models o pens up opportunities to reason about function or utility, how the particular object integrates i nto the scene (i.e., perhaps it is an obsta-cle), how the form of the particular instance is related to ot hers in its category (i.e., perhaps it is a particularly tall and narrow one), and how categories thems elves are related.
 Capturing the wide variation in both topology and geometry w ithin object categories, and finding good estimates for the underlying statistics, suggests a la rge scale learning approach. We propose exploiting the growing number of labeled single-view image s to learn such models. While our approach is trivially extendable to exploit multiple views of the same object, large quantities of such data is rare. Further, the key issue is to learn about the vari ation of the category. Put differently, if we are limited to 100 images, we would prefer to have 100 ima ges of different examples, rather than, say, 10 views of 10 examples.
 Representing, learning, and using object statistical geom etric properties is potentially simpler in the context of 3D models. In contrast, statistical models that e ncode image-based appearance character-istics and/or part configuration statistics must deal with c onfounds due to the imaging process. For example, right angles in 3D can have a wide variety of angles i n the image plane, leading to using the same representations for both structure variation and p ose variation. This means that the repre-sented geometry is less specific and less informative. By con trast, encoding the structure variation in 3D models is simpler and more informative because they are linked to the object alone. To deal with the effect of an unknown camera, we estimate the c amera parameters simultaneously while fitting the model hypothesis. A 3D model hypothesis is a relatively strong hint as to what the camera might be. Further, we make the observation that th e variations due to standard camera projection are quite unlike typical category variation. He nce, in the context of a given object model hypothesis, the fact that the camera is not known is not a sign ificant impediment, and much can be estimated about the camera under that hypothesis.
 We develop our approach with object models that are expressi ble as a spatially contiguous assem-blage of blocks. We include in the model a constraint on right angles between blocks. We further simplify matters by considering images where there are mini mal distracting features in the back-ground. We experiment with images from five categories of fur niture objects. Within this domain, we are able to automatically learn topologies. The models ca n then be used to identify the object category using statistical inference. Recognition of obje cts in clutter is likely effective with this ap-proach, but we have yet to integrate support for occlusion of object parts into our inference process. We learn the parameters of each category model using Bayesia n inference over multiple image examples for the category. Thus we have a number of parameter s specifying the category topology that apply to all images of objects from the category. Furthe r, as a side effect, the inference process finds instance parameters that apply specifically to each obj ect. For example, all tables have legs and a top, but the proportions of the parts differ among the insta nces. In addition, the camera parameters for each image are determined, as these are simultaneously fi t with the object models. The object and camera hypotheses are combined with an imaging model to p rovide the image likelihood that drives the inference process.
 For learning we need to find parameters that give a high likeli hood of the data from multiple ex-amples. Because we are searching for model topologies, we ne ed to search among models with varying dimension. For this we use the trans-dimensional sa mpling framework [7, 8]. We explore the posterior space within a given probability space of a par ticular dimension by combining standard Metropolis-Hastings [1, 14], with stochastic dynamics [18 ]. As developed further below, these two methods have complementary strengths for our problem. Impo rtantly, we arrange the sampling so that the hybrid of samplers are guaranteed to converge to the posterior distribution. This ensures that the space will be completely explored, given enough time.
 Related work. Most work on learning representations for object categorie s has focused on image-These approaches typically rely on effective descriptors t hat are somewhat resilient to pose change (e.g., [16]). A second force favoring learning 2D rep resentations is the explosion of read-ily available images compared with that for 3D structure, an d thus treating category learning as statistical pattern recognition is more convenient in the d ata domain (2D images). However, some researchers have started imposing more projective geometr y into the spatial models. For example, Savarese and Fei-Fei [19, 20] build a model where arranged pa rts are linked by a fundamental ma-trix. Their training process is helped by multiple examples of the same objects, but notably they are able to use training data with clutter. Their approach is different than ours in that models are built more bottom up, and this process is somewhat reliant on the presence of surface textures. A different strategy proposed by Hoeim et al. [9] is to fit a defo rmable 3D blob to cars, driven largely by appearance cues mapped onto the model. Our work also relat es to recent efforts in learning ab-stract topologies [11, 26] and structure models for 2D image s of objects constrained by grammar representations [29, 30]. Also relevant is a large body of ol der work on representing objects with 3D parts [2, 3, 28] and detecting objects in images given a pre cise 3D model [10, 15, 25], such as one for machined parts in an industrial setting. Finally, we have also been inspired by work on fitting deformable models of known topology to 2D images in the case of human pose estima-tion (e.g., [17, 22, 23]). We use a generative model for image features corresponding t o examples from object categories (Fig. 1). A category is associated with a sampling from categ ory level parameters which are the number of parts, n , their interconnections (topology), t , the structure statistics r s , and the camera regularity in how different objects are photographed durin g learning. We support clusters within categories to model multiple structural possibilities (e. g., chairs with and without arm rests). The cluster variable, z , selects a category topology and structure distributional parameters for attachment locations and part sizes. We denote the specific values for a p articular example by s . Similarly, we denote the camera capturing it by c . The projected model image then generates image features, x , for which we use edge points and surface pixels. In summary, the parameters for an image are  X  n ) = ( c , s , t , r c , r s , n ) .
 Given a set of D images containing examples of an object category, our goal i s to learn the model  X  ( n ) generating them from detected features sets X = x 1 , . . . , x D . In addition to category-level parameters shared across instances which is of most interes t,  X  ( n ) comprises camera models C = c , . . . , c D and structure part parameters S = s 1 , . . . , s D assuming a hard cluster assignment. In other words, the camera and the geometry of the training exam ples are fit collaterally. We separate the joint density into a likelihood and prior where we use the notation p ( n ) (  X  ) for a density function corresponding to n parts. Conditioned on the category parameters, we assume that the D sets of image features and instance parameters are independent, giving
The feature data and structure parameters are generated by a sub-category cluster with weights and distributions defined by r s = (  X  ,  X  s ,  X  s ) . As previously mentioned, the camera is shared across clusters, and drawn from a distribution defined by r c = (  X  c ,  X  c ) . We formalize the likelihood of an object, camera, and image features under M clusters as We arrive at equation (3) by introducing a binary assignment vector z for each image feature set, such that z m =1 if the m th cluster generated it and 0 otherwise. The cluster weights are then given by  X  m = p ( z m =1) .
 For the prior probability distribution, we assume category parameter independence, with the clus-tered topologies conditionally independent given the numb er of parts. The prior in (1) becomes For category parameters in the camera and structure models, r c and r s , we use Gaussian statistics with weak Gamma priors that are empirically chosen. We set th e number of parts in the object sub-categories, n to be geometrically distributed. We set the prior over edges in the topology given n to be uniform. 2.1 Object model We model object structure as a set of connected three-dimens ional block constructs representing object parts. We account for symmetric structure in an objec t category, e.g., legs of a table or chair, by introducing compound block constructs. We define two cons tructs for symmetrically aligned pairs (2) or quartets (4) of blocks. Unless otherwise specifi ed, we will use blocks to specify both simple blocks and compound blocks as they handled similarly .
 The connections between blocks are made at a point on adjacen t, parallel faces. We consider the organization of these connections as a graph defining the str uctural topology of an object category, where the nodes in the graph represent structural parts and t he edges give the connections. We use directed edges, inducing attachment dependence among part s.
 Each block has three internal parameters representing its w idth, height, and length. Blocks repre-senting symmetric pairs or quartets have one or two extra par ameters defining the relative positioning of the sub-blocks Blocks potentially have two external atta chment parameters u, v where one other is connected. We further constrain blocks to attach to at mos t one other block, giving a directed tree for the topology and enabling conditional independence amo ng attachments. Note that blocks can be visually  X  X ttached X  to additional blocks that they abut, but representing them as true attachments makes the model more complex and is not necessary. Intuitive ly, the model is much like physically building a piece of furniture block by block, but saving on gl ue by only connecting an added block to one other block. Despite its simplicity, this model can ap proximate a surprising range of man made objects.
 s = (  X , p o , b 1 , . . . , b n ) . We position the connected blocks in an object coordinate sy stem defined by a point p o  X  R 3 on one of the blocks and a y -axis rotation angle,  X  , about this position. Since we constrain the blocks to be connected at right angles on par allel faces, the position of other blocks within the object coordinate system is entirely defined by p o and the attachments points between blocks.
 The object structure instance parameters are assumed Gauss ian distributed according to  X  s ,  X  s in the likelihood (3). Since the instance parameters in the obj ect model are conditionally independent given the category, the covariance matrix is diagonal. Fina lly, for a block b i attaching to b j on faces defined by the k th size parameter, the topology edge set is defined as t = 2.2 Camera model A full specification of the camera and the object position, po se, and scale leads to a redundant set of parameters. We choose a minimal set for inference that ret ains full expressiveness as follows. Since we are unable to distinguish the actual size of an objec t from its distance to the camera, we constrain the camera to be at a fixed distance from the world or igin. We reduce potential ambiguity from objects of interest being variably positioned in R 3 by constraining the camera to always look at the world origin. Because we allow an object to rotate arou nd its vertical axis, we only need to specify the camera zenith angle,  X  . Thus we set the horizontal x -coordinate of the camera in the world to zero and allow  X  to be the only variable extrinsic parameter. In other words, the position of the camera is constrained to a circular arc on the y, z -plane (Figure 2). We model the amount of perspective in the image from the camera by parameterizing i ts focal length, f . Our camera instance parameters are thus c = (  X , f, s ) , where  X   X  [  X   X / 2 ,  X / 2] , and f, s &gt; 0 . The camera instance parameters in (3) are modeled as Gaussian with category para meters  X  s ,  X  s . 2.3 Image model We represent an image as a collection of detected feature set s that are statistically generated by an instance of our object and camera. Each image feature sets as arising from a corresponding feature generator that depends on projected object information. Fo r this work we generate edge points from projected object contours and image foreground from colore d surface points (Figure 3). We assume that feature responses are conditionally indepen dent given the model and that the G different types of features are also independent. Denoting the detected feature sets in the d th image by x d = x d 1 , . . . , x dG , we expand the image component of equation (3) to The function f ( n m )  X g (  X  ) measures the likelihood of a feature generator producing th e response of a detector at each pixel using our object and camera models. Ef fective construction and implementa-tion of the edge and surface point generators is intricate, a nd thus we only briefly summarize them. Please refer to our technical report [21] for more details.
 Edge point generator. We model edge point location and orientation as generated fr om projected 3D contours of our object model. Since the feature generator likelihood in (5) is computed over all detection responses in an image, we define the edge generator likelihood as where the probability density function e  X  (  X  ) gives the likelihood of detected edge point at the i th if the pixel is an edge point and 0 otherwise. This can be approximated by [21] are the number of background and missing detections. The den sity e e  X  approximates e  X  by estimating the most likely correspondence between observed edge point s and model edges.
 To compute the edge point density e  X  , we assume correspondence and use the i th edge point gen-erated from the j th model point as a Gaussian distributed displacement d ij in the direction perpen-dicular of the projected model contour. We further define the gradient direction of the generated edge point to have Gaussian error in its angle difference  X  ij with the perpendicular direction of the projected contour. If m j is a the model point assumed to generate x i , then where the perpendicular distance between x i and m j and angular difference between edge point gradient g i and model contour perpendicular v j are defined d ij = k x i  X  m j k and  X  ij = Surface point generator. Surface points are the projected points of viewable surface s in our ob-ject model. Image foreground pixels are found using k -means clustering on pixel intensities. Setting k = 2 works well as our training images were selected to have minim al clutter. Surface point detec-tions intersecting with model surface projection leads to f our easily identifiable cases: foreground, background, missing, and noise. Similar to the edge point ge nerator, the surface point generator likelihood expands to To learn a category model, we sample the posterior, p rameters shared by images of multiple object examples from t he category. Given enough iterations, a good sampler converges to the target distribution and an op timal value can be readily discovered in the process. However, our posterior distribution is high ly convoluted with many sharp, narrow ridges for close fits to the edge points and foreground. In our domain, as in many similar problems, standard sampling techniques tend to get trapped in these lo cal extrema for long periods of time. Our strategy for inference is to combine a mixture of samplin g techniques with different strengths in exploring the posterior distribution while still mainta ining convergence conditions. Our sampling space is over all category and instance paramet ers for a set of input images. We denote the space over an instance of the camera and object models wit h n parts as C  X  S ( n ) . Let T ( n ) be the space over all topologies and R ( n ) space with m subcategories and D instances is then defined as Our goal is to sample the posterior with  X  ( n )  X   X  such that we find the set of parameters that maximizes it. Since the number of parameters in the sampling space is a unknown, some proposals must change the model dimension. In particular, these jump moves (following the terminology of Tu and Zhu [27]) arise from changes in topology. Diffusion moves make changes to parameters within a given topology. We cycle between the two kinds of moves.
 Diffusion moves for sampling within topology. We found that a multivariate Gaussian with small covariance values on the diagonal to be a good proposal distr ibution for the instance parameters. Proposals for block size changes are done in one of two ways: s caling or shifting attached blocks. We found that both are useful good exploration of the object s tructure parameter space. Category parameters were sampled by making proposals from the Gamma p riors.
 Using standard Metropolis-Hastings (MH) [1, 14], the propo sed moves are accepted with probability The MH diffusion moves exhibit a random walk behavior and can take extended periods of time with many rejections to converge and properly mix well in reg ions of high probability in the target distribution. Hence we occasionally follow a hybrid Markov chain based on stochastic dynamics, where our joint density is used in a potential energy functio n. We use the common leapfrog dis-cretization [18] to follow the dynamics and sample from phas e space. The necessary derivative calculations are approximated using numerical differenti ation (details in [21]).
 Jump moves for topology changes. For jump moves, we use the trans-dimensional sampling ap-proach outlined by Green [7]. For example, in the case of a blo ck birth in the model, we modify the standard MH acceptance probability to The jump proposal distribution generates a new block and att achment edge in the topology that are directly used in the proposed object model. Hence, the chang e of variable factor in the Jacobian reduces to 1. The probability of selecting a birth move versu s a death move is given by the ratio of r /r b , which we have also defined to be 1. The complimentary block de ath move is similar with the inverse ratio of posterior and proposal distributions. We a dditionally define split and merge moves. These are essential moves in our case because the sampler oft en generates blocks with strong partial fits and proposing splitting it is often accepted. We evaluated our model and its inference with image sets of fu rniture categories, including tables, chairs, sofas, footstools, and desks. We have 30 images in ea ch category containing a single arbitrary Figure 4: Generated samples of tables (a) and chairs (b) from the learned structure topology and sta-tistical category parameters. The table shows the confusio n matrix for object category recognition. prominently in the foreground. This enables focusing on eva luating how well we learn 3D structure models of objects.
 Inference of the object and camera instances was done on dete cted edge and surface points in the images. We applied a Canny-based detector for the edges in ea ch image, using the same parameter-ization each time. Thus, the images contain some edge points considered noise or that are missing from obvious contours. To extract the foreground, we applie d a dynamic-threshold discovered in each image with a k -means algorithm. Since the furniture objects in the images primarily occupy the image foreground, the detection is quite effective.
 We learned the object structure for each category over a 15-i mage subset of our data for training purposes. We initialized each run of the sampler with a rando m draw of the category and instance parameters. This is accomplished by first sampling the prior for the object position, rotation and camera view; initially there are no structural elements in t he model. We then sample the likelihoods for the instance parameters. The reversible-jump moves in t he sampler iteratively propose adding and removing object constructs to the model. The mixture of m oves in the sampler was 1-to-1 for jump and diffusion and very infrequently performing a stoch astic dynamics chain. Figure 6 shows examples of learned furniture categories and their instanc es to images after 100K iterations. We visualize the inferred structure topology and statistics i n Figure 4 with generated samples from the learned table and chair categories. We observe that the topo logy of the object structure is quickly established after roughly 10K iterations, this can be seen i n Figure 5, which shows the simultaneous inference of two table instances through roughly 10K iterat ions.
 We tested the recognition ability of the learned models on a h eld out 15-image subset of our data for each category. For each image, we draw a random sample from th e category statistics and a topology and begin the diffusion sampling process to fit it. The best ov erall fit according to the joint density is declared the predicted category. The confusion matrix sh own in Figure 4 shows mixed results. Overall, recognition is substantively better than chance ( 20%), but we expect that much better results are possible with our approach. We conclude from the learned models and confusion matrix that the chair topology shares much of its structure with the other ca tegories and causes the most mistakes. We continue to experiment with larger training data sets, cl ustering category structure, and longer run times to get better structure fits in the difficult trainin g examples, each of which could help resolve this confusion.
 Figure 5: From left to right, successive random samples from 2 of 15 table instances, each after 2K iterations of model inference. The category topology and st atistics are learned simultaneously from the set of images; the form of the structure is shared across i nstances. Figure 6: Learning the topology of furniture objects. Sets o f contiguous blocks were fit across five image data sets. Model fitting is done jointly for the fifteen i mages of each set. The fits for the training examples is shown by the blocks drawn in red. Detect ed edge points are shown in green. Acknowledgments This work is supported in part by NSF CAREER Grant IIS-074751 1.
