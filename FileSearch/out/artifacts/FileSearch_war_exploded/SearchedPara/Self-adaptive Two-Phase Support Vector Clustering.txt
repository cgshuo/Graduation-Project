 Multi-Relational Data Mining (MRDM) [1] looks for patterns in Multi-Relational (MR) environment, namely multi connected table scenario. It requires mining from multi tables directly. Kernel function is often used in MRDM due to its fine quality of defining a nonlinear map from original space to feature space. For example, Support Vector Clustering (SVC) [2] uses non-bounded Support Vector (nbSV) to describe cluster contours. Support Vector Machine (SVM) [3] finds the optimal decision inter-face for highly structured data. 
This paper presents an algorithm STPSVC that is equipped with a multi-relational version Kernel to solve MRDM clustering task. STPSVC produces cluster contours firstly. Then a classification procedure is ex ecuted to find cluster centers information. The final cluster assignment is determined according to the affinity between data and cluster centers without suffering the expensive operations used in traditional SVC. STPSVC performs SVC procedure firstly to produce nbSVs and bSVs that are used to describe cluster contour bands. Then an iterative classification procedure to separate set of SVs from set of inner-clusters points is appended so as to extract cluster center information. The final cluster assignment is finished by computing similarity between point and each cluster center. Algorithm pseudocodes are as following: 1) SVC produces {} nbSV and {} bSV ; 2) {}{ }{} A data nbSV bSV = X   X  ; {}{} B nbSV bSV =+ ; 3) While ( _ iteration condition ; 4) SVM ( A, B ); 6) End 7) Aggregate centers according to ( _ , _ ) A ffinity Cen i Cen j ; 8) Clusters X  assignment according to ( , _ ) A ffinity x Cen i . 
Line 1 is Phase 1. It executes SVC process. Both nbSVs and bSVs are united to-gether to give a broad description for decision bands. Phase 2 is from Line 2 to Line 6. It iteratively performs SVM classification between the set of points located inside clusters, A , and the set of SVs, B . Update A as the new cluster contents, and B as the new decision bands. This makes dividing in terfaces generated in each run move closer and closer to central zones. We give the heur istic of the upper limit of runs of SVM as i  X  , where MinSize is the size estimate of the minimum cluster. MinSize is evaluated by following steps: a) Sort rows of Kernel matrix in a descending order. b) rough approximation of the smallest cluster size. Parameter coef indicates the width of ity is: according to: In MR environment a main table is mainly investigated. Its relational information is implicated by association keys (AK) . AK includes foreign key (FK) and referenced key (RK) . We think each key stands one expanding direction of data description, and affinity produced by each key can be considered as a local affinity. All local affinities are collected in a desirable way that: 
In (1), 
In (2), where 
In (3), where 12 , The global Kernel affinity is expressed by the product of affinity coming from Main table and the affinities from expanding information descriptions. More in de-stance. To measure local affinity between two objects in this expanding direction, their respective expanding instances can be introduced into the elementary Kernel directly. As to the map from RK to FK , there might be multi expanding tables because a RK might correspond to multi FKs of tables. If we denote by H v the set of tables that a RK v corresponds to, then the affinity produced by v is the product of local affini-kR R ++ is to compute the local affinity from table m . Moreover, under the map from a RK to a FK , one object could correspond to multi instances. So the local shown in (4). Gaussian Kernel serves as the elementary Kernel. According to the theorem of Kernel construction, formula (1 ) is of positive semi-definite property. 
Here, for each expanding table, we find q that satisfies the following constrain: 
In objection function (5), x p is found according to below st eps: a) Sort rows of dis-tance matrix D in an ascending order. b) Find the maximum gap between adjacent inter-cluster affinity. If this gap arrives at maximum, there would produce high intra-more apparently. And width parameter q under this setting is ex pected to be suitable. Firstly, STPSVC is applied on some real datasets: IRIS [4], WINE [4], and Breast Cancer (BC) [4]. The performance of STPSVC is compared in Table 1 with some other clustering algorithms: classical K-m eans; traditional SVC, Girolami method [5] and NJW [6]. For each algorithm, the minimum number of incorrectly clustered points is documented. Note that the errors of SVC in WINE and BC datasets are of-fered by us under the same experiment conditions. There, pure TPSVC just performs searching in some space. (With 2G P4 CPU PC, 256M memory, WinXP, MatLab7.0) 
For two TPSVC algorithms, TPSVC is a little better than STPSVC. And STPSVC shows finer quality in IRIS and BC dataset. But as far as WINE dataset is concerned. The fact that 178 points cover 13 dimens ions leads to the weak connection informa-tion in neighboring context. So of STPSVC has a higher error than others. 
Now STPSVC is applied on a relational problem: MUSK [7]. We use MUSK1 ver-sion. We fix the data and develop a two-level relation frame for it. To test quality of the designed Kernel, we employ it into classical SVM and compare it with other clas-sifiers: TILDE [8], SVM-MM and SVM-MI [9]. Their accuracy ratios with ten fold cross-validation are in Table 2. Our SVM procedure achieves better result, which shows the designed Kernel can grasp relational features effectively. Then, STPSVC approach is competitive with the searchin g one but with ease of parameterization. 
Finally, STPSVC is conducted on the document data of 387 students coming from some grade of Software College, Jilin University. This database contains six tables, Student, Rank, Classtype, Agegroup, Work and Activities, where Student Main table. The relationship among tables is shown in Fig 1. Table 4 gives clustering results pro-duced by STPSVC and TPSVC. To examine the effect of algorithm, in Fig 2, the statistics information of Main table, score data, is demonstrated after being processed by a weighted averaging method. Based on it, all students can be divided into four groups, which can be referred as 4 groups. And this intuitive analysis coincides with the results of STPSVC by and large, which forms 4 clusters. But when we investigate the content of corresponding clusters and group, we find that their content details differ and STPSVC provides result that is more agreed with the true comments. A novel STPSVC algorithm is presented in this paper. It obtains contour descriptions of clusters in Phase 1, and then performs SVM iterations between set of SVs and set of points inside clusters to find SVs that ar e located closer to cluster core zones. Fu-relational schema and to develop potent algorithms. This work is supported by the National Natural Science Foundation of China under Grant No. 60433020; 985 Project: Technological Creation Support of Computation and Software Science; and the Key Laboratory for Symbol Computation and Knowl-edge Engineering of the National Education Ministry of China. 
