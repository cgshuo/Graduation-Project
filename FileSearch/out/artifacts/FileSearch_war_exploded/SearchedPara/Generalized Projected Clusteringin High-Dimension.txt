 In a significant number of applications, data arrive in automatic streams. The processing and mining of data streams have attracted upon intensive research re-cently. In this paper, we investigate the pr oblem of generalized projected cluster-ing in high-dimensional stream, that is to identify densely populated subgroups of data points sharing strong correlations. The correlation indicates dependency between different attributes of the dataset, which is meaningful for a number of applications, e.g., in E-commerce, the positive linear correlations indicate sets of similar purchase patterns. An example is given in Fig. 1(a), where three clusters are shown in different colors.

Detecting such correlation connected cl usters, however faces special challenges in high-dimensional data streams: (1) The high-dimensional space is inherently sparse, which means the clusters and correlations are meaningful for specific subspace; (2) The correlation can be of arbitrarily complex direction, i.e., a set of attributes are dependent on another set; (3) The nature of streaming data imposes strict restriction on the complexity of clustering methods.

In this paper, we present ACID 1 , a framework that can detect clusters of data points exhibiting arbitrarily oriented correlations in streaming data. It mainly incorporates Principle Component Analysis (PCA), streaming cluster feature vector (SCF), and an extended CF-Tree [13]. The detailed algorithm is presented in Section 2 and 3. Problem Definition. We assume that data stream is of the model { X i } (1  X  i  X  t ), where t increases per time unit and the data point is of d dimension. We associate each point X with a weight (fading function [3]) to provide more importance for more recent dat a. Intuitively, correlati on connected clusters are spatially and temporally densely populated subgroups of data points, which exhibit strong correlations in some subspace. Formally, let  X   X  N (  X &lt;d )and  X ,  X   X  R + (0 &lt; X &lt; 1). We define a set of data points to be a correlation connected cluster if it satisfy : (1) Its covariance matrix  X  after PCA has the least ( d  X   X  ) eigenvalues e i with e i / e max &lt; X  ; (2) The time intervals between any two temporally continuous points are be less than  X  .
 Related Work. There has been extensive research done on the problem of subspace clustering in static data domain [2,5,12,8]. However, most methods make multiple passes of the dataset and iteratively update the clustering, which make them unsuitable for our purpose. Aggrawal et al. [3] introduced projected clustering into data stream domain, and proposed HPStream which can find axis parallel clusters in high-dimensional data streams. However, it will clearly miss arbitrarily oriented clusters, which are common in real-life data. Correlation Distance. Acluster C is associated with a set of values, including mean vector  X  , covariance matrix  X  and a set of eigenvectors V = { v k } (1  X  k  X  d  X   X  ) corresponding to the least ( d  X   X  ) eigenvalues. The definitions of symbols used throughout this paper are listed in Fig. 1(b). We introduce the correlation distance between a cluster C and a data point X as the Euclidian distance between the projection of  X  and X in the space spanned by V .This can be generalized to the case of two cl usters, with the mean vectors as their representatives. Note that if the cluster is too sparse, i.e., the number of data points is less than d , the covariance matrix is not meaningful. In this case, we compute the distance in the original axis system.
 SCF Vector. Streaming data imposes strict restriction on space and time com-plexity of clustering algorithms. We introduce Streaming Cluster Feature Vector (SCF) to capture the key statistical cha racteristics of a cluster. It is similar to Cluster Feature Vector (CF) [13] ex cept that each data point is multiplied by the weight (fading function), and the time stamp t last of the most recently added data point is introduced as a key feature. Like CF vector, SCF not only captures all the necessary statistical information of clusters in a compact way (space efficiency), but also makes it possible to incrementally update the key characteristics of clusters (time efficiency).
 SCF-Tree. The SCF-Tree is the core data structure in our algorithm. It is a variant of CF-Tree [13], thus shares most desired pr operties with CF-Tree. It has high scalability on the size of data because only the SCF structures are stored and it makes only single-pass scan of the data. It supports efficient dynamic insertion because all the clustering decisions made during inserting a new data point are local, without scanning all existing clusters. Following we define the structure and operation of a SCF-Tree.

The SCF-Tree has similar structure as the CF-Tree, but is adapted for stream-ing data in the following aspects: (1) the branching factor B is no more deter-mined by page size, but used to optimize the balance between the quality of clustering and the time efficiency; (2) The nodes contain the entries of SCF structures, instead of CF vectors; (3) Each SCF structure in the leaf nodes cor-responds to a micro cluster , which must satisfies the definition of correlation connected cluster, w.r.t.  X  ,  X  and  X  .

Being a variant of CF-Tree, SCF-Tree s upports efficient insertion of new data points, but uses correlation distance as the similarity metric. In addition, in processing streaming data, we expect to capture the continuous change of the underlying trends, without being overwhelmed by stale data. Hence we define the deletion operation for SCF-Tree. If a cluster has not been updated for a time window of  X  , it is regarded as outdated and will be purged. In both insertion and deletion operations, one has to update the SCF structure and other information of the current node, and the nodes along the path to root. Like CF-Tree, in face of skew input, a simple merging refinement step can be employed after insertion/deleteion to increase the clustering quality and space utilization. The micro clusters in the leaf nodes provide statistics about underlying correla-tion connected clusters, which can be sto red efficiently for the purpose of off-line analysis. In this section, we focus on the problem of efficient online construction of micro clusters.
 Online Clustering. The pseudo code of ACID algorithm is shown in Fig. 2. When a new data point X comes at time t , starting from the root, it descends the tree by choosing the closest child node based on the correlation distance, until reaches the leaf node. If on the path, a stale node is met ( t last &lt;t  X   X  ), the subtree rooted at this node will be deleted, and it re-chooses the next closest one. When the appropriate micro clusters are found, it is checked whether the micro cluster can  X  X bsorb X  X without violating the definition of correlation connected cluster. If no such micro cluster exists , a new micro cluster solely containing X will be created. Each insertion/deletion will cause the SCF of all the affected nodes on the path to root to be updated. In order to make better use of space and enhance the quality of clusters, a merging enhancement step can be employed after insertion/deletion operation.
 Delay of Update. The update of statistical information in SCF structure is trivial, due to its property of additivity and temporal multiplicity [13], however the PCA operation has complexity of O ( d 3 ) and the split/merge operations are also expensive. Thus in our approach we follow a  X  X elay of update X  policy to save these operations. We argue that there are mainly two reasons to do so.
First, it is important to take into consideration the existence of outliers, which can significantly affect the space and time complexity of clustering algorithms. Creating a new micro cluster not only incurs the update of SCF structure, co-variance matrix, and PCA operation of all the nodes on the path to root, but also may cause a chain of split operations. Thus in our approach, when a new data point X could not be absorbed into existing micro clusters, it does not create a new micro cluster or update the infor mation of affected nodes immediately, rather it waits for a time window of  X  . During this window, if no similar data point X  X  comes, which can be combined with X , X will be regarded as outlier, and discarded.

Second, even if the data point X canbeabsorbedbysomemicrocluster C , it may not be necessary to update all the nodes on the path from C to the root. Specifically, if after absorbing X , the change of mean vector of C does not exceed some threshold, we can update the SCF structures of the affected nodes only, without executing PCA operation to update eigenvectors. Analysis. The data streams are assumed to be infinite sequences of data points, thus we discuss the complexity of ACID algorithm by considering the cost of inserting one point in the worst case. Let the average rate of data stream be n points per second, then the maximum depth of the SCF-Tree is h = log B ( n X  ) (the outdated micro clusters will be purged). The cost of inserting a data point X include (1) compare X with the nodes on a path from the root to a leaf node ( O ( d 2 h )); (2) At the leaf node, check if any micro cluster can absorb X ( O ( Bd 3 )), and update (possibly split) this leaf node and all nodes on the path to the root ( O (( Bd 2 + d 3 ) h )); (3) In the worst case, the data point incurs a deletion operation on each node it visits ( O ( h ) if we adopt  X  X elay of update X ). So the total complexity of inserting a data point in the worst case is O (( Bd 2 + d 3 ) log B ( n X  ) ). In the experimental evaluation, we compare our method with HPStream [3] on both real-life and synthetic datasets. All the experiments are performed on a PC with Intel Pentium IV 2.67 GHz and 512MB memory and r unning Windows XP Professional. The setting of parameters of HPStream follows [3]. We adopt the cluster purity [1] as the metric of clustering quality: at different time points of the stream, we measure the purity of gen erated clusters to assess the accuracy of clustering.
 Clustering Quality. We apply ACID and HPStream algorithms to both the KDD-CUP X 99 Network Intrusion Detection stream dataset and the synthetic dataset. We generate the synthetic data, aiming to reflect continuous evolution of underlying natural clusters and correlations. The results are shown in Fig. 3(a) and Fig. 3(b) respectively. In both case s, ACID algorithm has better performance than HPStream, which is especially ev ident in the synthetic dataset. It can be explained by the fact that ACID is designed to detect arbitrarily oriented correlations, while HPStream focus on axis-parallel ones, thus ACID can more effectively detect clusters whose correlations are under continuous evolution. Scalability. We also measure the scalability of ACID against the stream speed and the dimension of data. Fig. 4(a) plots the impact the speed of data streams has on the performance of ACID algorithm. Fig. 4(b) shows how the wall exe-cution time for one data point varies as the dimension of data and stream speed change. It can be seen clearly that ACID h as stable processing speed and quality in these two cases. In this paper, we tackled the problem of identifying densely populated subgroups of points exhibiting arbitrarily oriented correlations in high-dimensional data stream. We present ACID, a method that can detect such clusters in streaming data by incorporating PCA , streaming cluster feature vector and SCF-Tree. We believe that the found correlation connect ed clusters can be of special interest for online analysis of underlying trends and distribution of data streams.
