 Integrating new knowledge sources into various learning tasks to improve their performance has recently become an interesting topic. In this paper we propose a novel semi-supervised learning (SSL) approach, called semi-supervised learning with Mixed Knowledge Information (SSL-MKI) which can simultaneously constraints together with unlabeled data. Specifically, we first construct a unified SSL framework to combine the manifold assumption and the pairwise constraints assumption for classification tasks. Then we present a Modified Fixed Point Continuation (MFPC) algorithm with an eigenvalue thresholding (EVT) operator to learn the enhanced kernel matrix. Finally, we develop a two-stage optimization strategy and provide an efficient SSL approach that takes advantage of Laplacian spectral regularization: semi-supervised learning with Enhanced Spectral Kernel (ESK). Experimental results on a variety of synthetic and real-world datasets demonstrate the effectiveness of the proposed ESK approach.
 I.2.6 [ Artificial Intelligence ]: Learning; H.2.8 [ Database Management ]: Database Applications X  X  Data mining Algorithms, Performance, Experimentation Semi-supervised learning (SSL), Kernel learning, Graph Laplacian, Nuclear norm regularization, Pairwise constraints Semi-Supervised Learning (SSL) has recently received a significant amount of attention in the machine learning and data mining communities [1, 2]. A large amount of SSL approaches have been proposed, including EM with generative mixture models, self-training, co-training, transductive support vector machines (TSVMs), and graph-based methods. Among them, a family of graph-based SSL methods is label propagation frameworks for learning from labeled and unlabeled data [5, 6]. Although graph-based SSL has been studied extensively, so far there are few comprehensive techniques to integrate weakly labeled data and pairwise constraints together for classification tasks. supervisory information [7]. But labeled instances are often difficult, expensive, or time consuming to obtain, as they require the efforts of the domain experts. Integrating new knowledge sources such as side information into classification tasks with insufficient training data has recently become an interesting topic classification accuracy. Pairwise constraints may be relatively easy to collect, and indicate that some pairs of instances are in the same class and some are not, known as the Must-Link (ML) and constraints can also be obtained from data labels where objects with the same label are must-link while objects with different labels are cannot-link. Pairwise constraints have been widely used in various tasks such as clustering [9-12] and distance metric learning [13-15] where it has been shown that the presence of performance improvement. However, there are relatively fewer works using additional pairwise constraints to support semi-supervised classification tasks [8]. Generally, SSL methods are derived based on two fundamental assumptions: the cluster assumption (also called label consistency) and the manifold assumption . In the cluster assumption, decision boundaries should not cross high density regions, but instead lie in low density regions. Typical methods include TSVMs [16, 17] and some convex relaxation methods. In the manifold assumption, data are assumed to be sampled from a low-dimensional manifold that is embedded inside of a higher dimensional input space. Many SSL approaches implement such an assumption by using the graph Laplacian of a neighborhood graph which can characterize the underlying manifold structure. Typical methods include Zhu et al. X  X  Gaussian random fields (GRF) method [3], Zhou et al. X  X  learning with local and global consistency (LGC) [4], manifold regularization [5, 6], etc. More recently, Li et al. [7] presented a pairwise constraint assumption that is very effective for classification tasks together with the cluster assumption. In the pairwise constraint assumption, those unlabeled data points involved in any ML constraint are classified into the same class and those involved in any CL constraint are classified into different classes. Despite many successes, most graph-based SSL methods mentioned above have a limitation in the difficulty of tuning optimal graph parameters. Specially, with limited labeled data, it may be ineffective to learn kernel parameters by cross-validation from labeled data only [18]. To address this limitation, various kernel learning approaches [7, 18-23] (also called non-parametric kernel learning) are proposed to learn a positive semidefinite (PSD) kernel matrix directly from the data incorporating label or side information. Several existing studies [7, 21] have shown that programming (SDP) solvers to learn the entire kernel matrix could be as high as , where n is the number of data points, which prohibits those approaches for practical applications [18], whereas there are several efficient approaches derived from the spectral decomposition of graph Laplacians, such as the Order-constrained Spectral Kernel (OSK) [19, 20], the Transductive Spectral Kernel (TSK) [22], and the graph Laplacian Regularized Kernel (LRK) [24]. In this paper, we consider a more general problem of semi-supervised classification which can handle sparse labeled data and additional pairwise constraints together with abundant unlabeled data, and propose a novel SSL approach, also called semi-supervised learning with Mixed Knowledge Information (SSL-MKI). We first construct a unified SSL-MKI framework to implement both the manifold assumption and the pairwise constraint assumption. Under the SSL-MKI framework, we also present a semi-supervised low-rank kernel learning model with nuclear norm regularization. Then we develop a two-stage optimization strategy and provide an efficient SSL-MKI regularization: semi-supervised learning with Enhanced Spectral Kernel (ESK). first small number l points 1 {} l L ii Xx belonging to classes are labeled, and the remaining points are unlabeled, and two sets of pairwise must-link and cannot-link constraints are denoted respectively by ML where be in the same class, and CL where {( , )} ij =xx be in different classes. Before we go into the details of our method, we will briefly review some of the related works in this section. As mentioned in the previous section, the common denominator of graph-based methods is to model the whole data set as a graph (, with the vertex set X , and the weight on the edge ij representing the similarity between i x and j x . For convenience, we adopt the local scaling parameter trick proposed in [25] to construct as a T -nearest neighbor ( T -NN) graph. Let us define a selecting function of the local scale, matrix nn W associated with is formed subsequently as Note that we set 0 ii W to avoid self-loops. We further denote the diagonal degree matrix nn D whose entries are given by 
D W , and the normalized graph Laplacian LIDWD The regularization framework proposed by Zhou et al. [4] can implement a global classification task as follows: where is a vectorial function, , to assign a label F : f F to each instance i x , is the regularization parameter, and is a class indicator matrix, where () is a regularizer to penalize the smoothness of the classifying function over the graph, and the is a loss function measuring the inconsistency between the predicted and initial label assignment. Then the classification function is where ( 01(1)1 =&lt; ) is the regularization parameter and symmetrically normalizes . The calculated matrix F method is an unreliable approach for model selection if only very few labeled instances are available [26]. In recent years, a large amount of low-rank matrix recovery methods have been proposed for matrix completion [27]-[30] problems. Among of them, there are several representative methods such as SVT [27], FPCA [28], and ALM [29]. And some works also provide theoretical guarantee that the task of the rank minimization problem can be accomplished via solving the nuclear norm (also known as the trace norm) minimization under some reasonable conditions. Specifically, Cand X s and Recht [30] proved that a given incomplete low-rank matrix (but unknown) 
Z satisfying certain incoherence conditions can be exactly recovered by the following model (5) with probability at least 1 B n from a subset of uniformly sampled entries {:(,) }
Zij whose cardinality is of the form || 2 log B rn n , where r is the rank of the desired matrix, and and are two positive constants. where || denotes the nuclear norm of a matrix, i.e., the sum of its singular values, the operator denotes element-wise multiplication, and the weight matrix 3.1 A General Framework As mentioned above, our goal is to use the given labeled data and pairwise constraints together with unlabeled instances for classification tasks. We propose a general SSL-MKI framework as follows: where K is a desired  X  X deal X  kernel matrix, (, ) is a regularizer to penalize the smoothness of the classifying function between the predicted and initial label assignment, and 
F 1 () F a loss function to measure the change between the predicted and  X  X deal X  kernels corresponding to the given pairwise constraints, such as the squared loss or hinge loss functions. 2 0 are the regularization parameters for and respectively. In the unified framework, we can implement both the manifold assumption and the pairwise constraint assumption . 3.2 Nuclear Norm Regularized Model Under the squared loss function, the model (7) is formulated as follows: 1 () =K FY Kt where is the set of pairwise constraints, and is a binary variable that takes 1 or 0 to denote belonging to the same class or not, and The time complexity of learning the entire kernel matrix in [7, 21] could be as high as . An effective heuristic of speedup is to perform low-rank kernel matrix approximation and matrix factorization [19, 22, 24, 31, 32]. Moreover, the number of the given pairwise constraints is far less than the one which is sufficient to complete the low-rank kernel matrix with high regularization into the above model (8). In other words, we set 
K QUQ , where consists of the m smoothest eigenvectors of the graph Laplacian size mm . The completion problem of the desired kernel K is then converted into the learning problem of a much smaller matrix ( ), subject to the constraint that . U mn 0 U Considering that the desired kernel matrix K is low-rank, and || || || || || || T
K = QUQ U , we present the following nuclear norm regularized problem, where is a positive regularization parameter, is the set of indices of known entries of Z , which is defined as It is generally difficult that the optimization problem (10) is minimized with respect to both variables simultaneously. Thus, we employ an alternating optimization idea to solve the above problem. In the next section, we present a two-stage optimization strategy and provide an efficient modified fixed point continuous algorithm to learn the enhanced matrix U . In this section, we present an effective two-stage optimization strategy for the SSL-MKI model (10). Furthermore, we should first choose to respect the pairwise constraints assumption and then the manifold assumption , considering that the given pairwise constraints are from reliable knowledge. Then we design an efficient modified fixed point continuous algorithm to learn the enhanced matrix U , and present a semi-supervised classification (SSC) algorithm, which aims to solve transduction classification tasks. The objective function (10) can be approximated by a two-stage optimization strategy as follows: In other words, the problem (10) can be efficiently solved using the following two stages: the first stage involving only one variable U is to compute the desired enhanced matrix U with the given pairwise constraints; and the second stage is to achieve the classification assignment based on the learned similarity matrix from the first stage. K 4.1 Modified Fixed Point Algorithm In the first stage, the optimization problem is formulated as follows: While the nuclear norm minimization problem (12) can be converted into a semidefinite programming (SDP) problem, the time complexity of each iteration of standard SDP solvers based on the interior-point method could be at least as [33]. To overcome this issue, many first-order algorithms have been developed to solve those problems, such as FPCA [28], which is a fixed point continuation algorithm. Furthermore, the FPCA method provably converges to the globally optimal solution and has been shown to outperform SDP solvers in terms of matrix recoverability. More recently, Ni et al. [34] proposed an Augmented Lagrange Multiplier (ALM) method for solving the low-rank representation problem with a PSD constraint. Our model (12) is also a nuclear norm minimization problem with a PSD constraint. In this part, we propose a Modified Fixed Point Continuation (MFPC) algorithm with an eigenvalue thresholding (EVT) operator to learn the enhanced matrix U , also called MFPC. The proposed MFPC algorithm can reduce the number of the auxiliary variables used in the ALM method [29] to accelerate its convergence. In the following subsections, we describe the proposed MFPC algorithm, and discuss the stopping criteria for iterations to acquire the optimal solution. Inspired by the fixed point continuation algorithm proposed by Ma et al. [28], which has been used to multi-label transductive learning [35], we develop a modified fixed point iterative algorithm with an EVT operator to solve the proposed nuclear norm minimization problem (12). Let 2 1 2 (): || ( )|| || + || T F gU U M QUQ Z , the derivative of the function () g with respect to U is given by where is the set of the subgradients of the nuclear norm, and Following [36], an explicit expression of the subdifferential of the nuclear norm at a symmetric matrix is given by the following lemma. Lemma 1 . Let be a real symmetric matrix, then mm U || || { [ ] [ ] : [ , ] 0, || || 1}, TT T UVV VV SVVS S where and are orthogonal eigenvectors associated with the positive and negative eigenvalues of U respectively, and || || In addition, the following optimality condition in [37] can be adopted for the proposed nuclear norm minimization problem (12). Theorem 1. Let () g be a convex function. Then U is an optimal solution to the problem (12), if and only if , and there exists a matrix such that Based on the above theorem, we can develop a modified fixed point iterative scheme for solving the problem (12) by adopting the operator splitting technique. The operator () T is defined as where 0 . And () T can be split into two parts: where 1* () || || () TI , 2 () () () TI h , and () I is an identity operator. Let 2 () YTU , then * ( ) || || TU U U Y , and . For tackling the proposed model (12), we need to solve the following nuclear norm minimization problem, The convex optimization problem (13) has a closed-form optimal solution [34], and the optimal solution is given by the eigenvalue thresholding (EVT) operator which will be defined later: Thus, our modified fixed point scheme for solving the problem (12) can be expressed by the following two-step iteration as follows: Definition 1 (Eigenvalue thresholding (EVT) operator) Assume UU , and its eigenvalue decomposition is given by UV V ! mr V r ! . Given , 0 v EVT ( ) v is defined as: where max{ , } should be understood element-wise. Theorem 2 . Suppose a symmetric matrix satisfies: 0 U 1. 2 ()|| || T F / MQUQZ m for a small positive constant . 2. . (16) EVT ( ( )) UUhU Then U is the unique optimal solution of the problem (12). Proof. Please refer to [28, 37]. 4.2 Implementation We develop a modified fixed point iterative scheme to learn the enhanced matrix U with a PSD constraint. As suggested in [28, 37], the continuation technique can accelerate the convergence of the fixed point iterative method, and the parameter " determines the rate of reduction of consecutive k , where is a moderately small constant. Thus, the continuation strategy is also adopted by our modified fixed point algorithm, which solves a sequence of the problem (12), easy to difficult, corresponding to a sequence of large to small values of k In the implementation of [28], the parameter is always set to 1, point continuation algorithm so that our algorithm X  X  convergence is guaranteed, where , denotes the Kronecker product of two matrices, and is a diagonal matrix which entries associated with are set to 1, and 0 otherwise. There are many ways to select the parameter compressing sensing tasks. We now specify a strategy, which is based on the Barzilai-Borwein (BB) method [38] for choosing the parameter k . The shrinkage iteration (14) first takes a gradient descent step with the step size k along the negative gradient direction of the smooth function , and then applies the EVT operator EV to accommodate the non-smooth term || . Therefore, it is natural to choose the parameter alone. Let () kT T H Q M QUQ Z Q , 1 kk UU U % , and hH H % To avoid the BB step size k being either too small or too large, we take where min max 0 ( are two constants. Because our ultimate goal is to learn the enhanced matrix U , the accurate solution of the problem (12) is not required. Therefore, we use the following criterion as a stopping rule, where is a small positive number. Experiments shows that Based on the previous analysis, we develop a Modified Fixed Point Continuation (MFPC) algorithm to learn the enhanced matrix U , as listed in Algorithm 1 .
 Theorem 3. The sequence { generated by our modified fixed point iterations with where is the set of optimal solutions of the problem (12). Proof. Please refer to [28, 37]. We now claim that our modified fixed-point continuation algorithm converges to an optimal solution of the problem (12). Algorithm 1 : MFPC algorithm 
Input : A data set of n instances 12 1 {} ll n X =x,x, ,x,x , ,x ,
Xx are labeled, and are unlabeled. number of nearest neighbors T and the constant . ML {( , )} ij =xx
CL {( , )} ij =xx Output : The enhanced matrix U .
 Initialize : Given M , , 0 U , " , and . tol
And select 12 0 L . 1. Construct the T -NN graph and compute the normalized 2. Compute the m eigenvectors 1 ,, * * m of L associated end for 4.3 Label Propagation The enhanced spectral kernel K has been constructed using the above proposed MFPC algorithm, and we would have to take advantage of it to predict the labels of the unlabeled instances. We also present a semi-supervised learning method with enhanced spectral kernel , also called ESK, as shown in Algorithm 2 . Here, our iteration equation can be written as follows: where 1/ 2 1/ 2 PD KD 1 . We will use the equation (20) to update the labels of each data point until convergence.
 We give a toy example to illustrate how our ESK algorithm works, as shown in Figure 1. At first glance, the toy data consists of three separate groups, and is composed of a mixture of Gaussian-like and curve-like groups, as shown in Figure 1(a). Moreover, we also present the comparison between the similarity matrix in the input space and the kernel matrices learned by OSK, TSK, and MFPC, where the data are ordered such that all the instances in two Gaussian-like groups appear first; all the instances in the curve-like group appear second. It can be clearly observed that the enhanced kernel matrix learned by our MFPC algorithm exhibits two clear block structures so that the two classes are well-separated groups. In addition, we can draw a similar conclusion as [23] that the kernel matrices learned by OSK and TSK have some uninformative eigenvectors even though which are optimally combined according to their own optimization criteria, and they fail to classify data points into the proper class. Algorithm 2 : ESK Algorithm Input : The enhanced matrix U and the constant .

Output : The assigned labels of all the data points. 1. Obtain the enhanced matrix U by solving the problem (12) 2. Construct the enhanced spectral kernel matrix T 3. Let be the limit of the sequence Figure 1: Classification results on the toy data set. (a) Toy data set with two labeled points and one ML constraint. (b) X (e) Classification results using LGC with 0.2 + , OSK, TSK, and the proposed ESK algorithm with only one iteration. (f) Similarity matrix for the toy data set in the input space. (g) X (i) Learned kernel matrices by OSK, TSK, and MFPC with the 5 m smoothest eigenvectors of graph Laplacians and a neighborhood size . The brighter a pixel, the greater similarity the pixel represents. 4.4 Valid Kernel Theorem 4 . If a normalized graph Laplacian nn L has the first eigenvectors m 1 ,, * * m corresponding to the smallest engenvalues, and the enhanced matrix U obtained by solving the problem (12) for ESK is symmetric positive semidefinite. Then the family of matrices Proof: Because 0 T UU , , and 1/ 2 1/ 2 () T UU U T K QUQ
QU QU ) K is certainly positive semidefinite and thus a valid kernel matrix. Remark : Similar to the existing spectral kernel learning approaches such as OSK and TSK, the kernel matrix K learned by the proposed MFPC algorithm is also nonparametric spectral the enhanced spectral kernel. Hence, the enhanced spectral kernel can be used in traditional kernel machines such as SVMs. 4.5 Complexity Analysis The main running time of the proposed ESK algorithm is consumed by constructing the k -NN graph, computing the enhanced kernel matrix, and iterating the procedure (20). The time complexity of computing the enhanced matrix U by solving the problem (12) is , where is the number iterations in the procedure (20). In the ESK algorithm, computing the smoothest eigenvectors of the sparse matrix ( On +tmn tm nm tnc 2 t efficiently performed using the Lanczos algorithm [39]. In this section, we present a set of experiments on many data sets, including a synthetic data set and many transductive settings. 5.1 Compared Algorithms We compared the performance of the proposed ESK approach with the existing state-of-the-art SSL algorithms or related SSL methods, and the results averaged over 50 independent runs are reported.
 The width of the RBF kernel for SVM is set using 5-fold cross validation.
 Gaussian function whose width is set by 5-fold cross validation. whose width is set by 5-fold cross validation, and all of the other hyperparameters are set by grid search as in [6]. to 2, and other parameters in TSK are set as in our ESK algorithm. For OSK, all parameters in OSK are set as in our algorithm. set the constant 001 =. . And the number of nearest neighbors is set by grid search as in [6]. T 5.2 Real-world Datasets We use three categories of real-world data sets in our experiments, which are selected to cover a wide range of properties. Specifically, these data sets include: The basic information of those data sets together with additional randomly chosen pairwise constraints is summarized in Table 1. Note that num_ M and num_ C denote the numbers of randomly chosen must-link constraints and cannot-link constraints, respectively. 5.3 Transduction Classif ication Results The performances of the existing state-of-the-art SSL methods and the proposed ESK algorithm on these real-world data sets and performance for each data set is shown in bold. Here, we fairly compare the performance of the proposed ESK algorithm only using the given labeled data (denoted as ESK_L) with four existing state-of-the-art SSL approaches and SVMs. And we also provide the classification results of the proposed ESK algorithm both using the sparse labeled data and additional constraints (denoted as ESK_LC). By applying SVMs as the final classifier, we contrast the enhanced spectral kernel for ESK with two competitive spectral kernels such as OSK and TSK. From these tables, we can observe the following: classification accuracies using the proposed ESK algorithm on the G50c, USPS0123 and 20-News data sets with the number of randomly labeled points varying from 2 to 20, from 4 to 40, and from 4 to 40, respectively, and against a number of randomly chosen pairwise constraints with only one labeled data point in each class, as shown in Figures 2 and 3. In the figures, the abscissa denotes the number of randomly labeled data or chosen pairwise constraints (we guarantee that there is at least one comparison, the classification results of four state-of-the-art SSL algorithms and SVMs are also plotted in the corresponding figure. It can be clearly observed that the proposed ESK algorithm is very stable, that is, even when we only label a very small fraction of the data, it can still get high classification accuracies and consistently outperforms the other five algorithms with the same amount of labeled data. Moreover, as the number of sparse constraints grows, the classification accuracy of the proposed ESK algorithm can be considerably improved and is better than that of graph Laplacian regularized kernel (LRK) [24]. This confirms that the proposed kernel learning model with nuclear norm regularization can avoid the over-fitting problems of LRK. In this paper we have proposed a novel semi-supervised learning approach with Mixed Knowledge Information (SSL-MKI), which can handle both labeled data and additional pairwise constraints together with unlabeled data. We first constructed a unified SSL-MKI framework that can implement both the manifold assumption and the pairwise constraint assumption. Under the above framework, we also presented a Modified Fixed Point Continuation (MFPC) algorithm with an eigenvalue thresholding (EVT) operator to learn the enhanced kernel matrix. Then we developed a two-stage optimization strategy and provided an efficient ESK approach. Unlike the general SSL method, the proposed ESK approach can effectively make use of the given pairwise constraints that can often be obtained with little human effort. Finally, we provided a variety of experiments to show the effectiveness of our ESK approach, from which we found that the proposed ESK algorithm outperforms the state-of-the-art SSL methods. This work is supported by the National Natural Science Foundation of China Nos. 61003198, 60970067; the Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project) No. B07048; the Program for Cheung Kong Scholars and Innovative Research Team in University No. IRT1170.
