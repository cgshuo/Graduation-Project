 One of the main purposes of unsupervised learning is to produ ce good representations for data, that can be used for detection, recognition, prediction, or visu alization. Good representations eliminate irrelevant variabilities of the input data, while preservi ng the information that is useful for the ul-timate task. One cause for the recent resurgence of interest in unsupervised learning is the ability to produce deep feature hierarchies by stacking unsupervised modules on top of each other, as pro -posed by Hinton et al. [1], Bengio et al. [2] and our group [3, 4 ]. The unsupervised module at one level in the hierarchy is fed with the representation vector s produced by the level below. Higher-level representations capture high-level dependencies be tween input variables, thereby improving the hierarchy can be fed to a conventional supervised classi fier.
 paradigm [5]. An encoder transforms the input into the representation (also known as the code or the feature vector), and a decoder reconstructs the input (perhaps stochastically) from the r epre-sentation. PCA, Auto-encoder neural nets, Restricted Bolt zmann Machines (RBMs), our previous sparse energy-based model [3], and the model proposed in [6] for noisy overcomplete channels are just examples of this kind of architecture. The encoder/dec oder architecture is attractive for two rea-sons: 1. after training, computing the code is a very fast pro cess that merely consists in running the input through the encoder; 2. reconstructing the input with the decoder provides a way to check that the code has captured the relevant information in the data. S ome learning algorithms [7] do not have a decoder and must resort to computationally expensive Mark ov Chain Monte Carlo (MCMC) sam-pling methods in order to provide reconstructions. Other le arning algorithms [8, 9] lack an encoder, which makes it necessary to run an expensive optimization al gorithm to find the code associated with each new input sample. In this paper we will focus only on encoder-decoder architectures. In general terms, we can view an unsupervised model as definin g a distribution over input vectors Y through an energy function E ( Y, Z, W ) : where Z is the code vector, W the trainable parameters of encoder and decoder, and  X  is an arbitrary positive constant. The energy function includes the reconstruction error , and perhaps other terms as well. For convenience, we will omit W from the notation in the following. Training the machine to model the input distribution is performed by finding the en coder and decoder parameters that minimize a loss function equal to the negative log likelihoo d of the training data under the model. For a single training sample Y , the loss function is The first term is the free energy F can be simpler to approximate this distribution over Z by its mode, which turns the marginalization over Z into a minimization: where Z  X  ( Y ) is the maximum likelihood value Z  X  ( Y ) = argmin reconstructed by the model: The second term in equation 2 and 3 is called the log partition function , and can be viewed as a penalty term for low energies. It ensures that the system pro duces low energy only for input vectors that have high probability in the (true) data distribution, and produces higher energies for all other input vectors [5]. The overall loss is the average of the abov e over the training set. Regardless of whether only Z  X  or the whole distribution over Z is considered, the main difficulty with this framework is that it can be very hard to compute the g radient of the log partition function in equation 2 or 3 with respect to the parameters W . Efficient methods shortcut the computation by drastically and cleverly reducing the integration domain. For instance, Restricted Boltzmann Ma-chines (RBM) [10] approximate the gradient of the log partit ion function in equation 2 by sampling values of Y whose energy will be pulled up using an MCMC technique. By run ning the MCMC for a short time, those samples are chosen in the vicinity of the t raining samples, thereby ensuring that the energy surface forms a ravine around the manifold of the t raining samples. This is the basis of the Contrastive Divergence method [10].
 The role of the log partition function is merely to ensure tha t the energy surface is lower around training samples than anywhere else. The method proposed he re eliminates the log partition function from the loss, and replaces it by a term that limits the volume of the input space over which the energy surface can take a low value . This is performed by adding a penalty term on the code rather than on the input . While this class of methods does not directly maximize the li kelihood of the data, it can be seen as a crude approximation of it. To understand the method , we first note that if for each vector Y , there exists a corresponding optimal code Z  X  ( Y ) that makes the reconstruction error (or energy) F  X  ( Y ) energy surface flat and indiscriminate. On the other hand, if Z can only take a small number of different values (low entropy code), then the energy F places (the Y  X  X  that are reconstructed from this small number of Z values), and the energy cannot be flat.
 More generally, a convenient method through which flat energ y surfaces can be avoided is to limit the maximum information content of the code . Hence, minimizing the energy F the information content of the code is a good substitute for m inimizing the log partition function . A popular way to minimize the information content in the code is to make the code sparse or low-dimensional [5]. This technique is used in a number of unsupe rvised learning methods, including PCA, auto-encoders neural network, and sparse coding metho ds [6, 3, 8, 9]. In sparse methods, the code is forced to have only a few non-zero units while most code units are zero most of the time. Sparse-overcomplete representations have a number o f theoretical and practical advantages, as demonstrated in a number of recent studies [6, 8, 3]. In par ticular, they have good robustness to noise, and provide a good tiling of the joint space of locatio n and frequency. In addition, they are advantageous for classifiers because classification is more likely to be easier in higher dimensional spaces. This may explain why biology seems to like sparse rep resentations [11]. In our context, the main advantage of sparsity constraints is to allow us to repl ace a marginalization by a minimization, and to free ourselves from the need to minimize the log partit ion function explicitly. In this paper we propose a new unsupervised learning algorit hm called Sparse Encoding Symmetric Machine (SESM), which is based on the encoder-decoder parad igm, and which is able to produce sparse overcomplete representations efficiently without a ny need for filter normalization [8, 12] or code saturation [3]. As described in more details in sec. 2 an d 3, we consider a loss function which is a weighted sum of the reconstruction error and a sparsity p enalty, as in many other unsupervised learning algorithms [13, 14, 8]. Encoder and decoder are con strained to be symmetric , and share learning algorithm which is closely related to those propos ed by Olshausen and Field [8] and by us previously [3]. The first step computes the optimal code by mi nimizing the energy for the given input. The second step updates the parameters of the machine so as to minimize the energy. In sec. 4, we compare SESM with RBM and PCA. Following [15], we evaluate these methods by measuring the reconstruction error for a given entropy of th e code. In another set of experiments, we train a classifier on the features extracted by the various methods, and measure the classification error on the MNIST dataset of handwritten numerals. Interes tingly, the machine achieving the best recognition performance is the one with the best trade-off b etween RMSE and entropy. In sec. 5, we compare the filters learned by SESM and RBM for handwritten nu merals and natural image patches. In sec.5.1.1, we describe a simple way to produce a deep belie f net by stacking multiple levels of demonstrated through the unsupervised discovery of the numeral class labels in the high-level code . In this section we describe a Sparse Encoding Symmetric Mach ine (SESM) having a set of linear fil-ters in both encoder and decoder. However, everything can be easily extended to any other choice of parameterized functions as long as these are differentiabl e and maintain symmetry between encoder and decoder. Let us denote with Y the input defined in R N , and with Z the code defined in R M , where M is in general greater than N (for overcomplete representations). Let the filters in enco der and decoder be the columns of matrix W  X  R N  X  M , and let the biases in the encoder and decoder be denoted by b where the function l is a point-wise logistic non-linearity of the form: with g fixed gain. The system is characterized by an energy measurin g the compatibility between pairs of input Y and latent code Z , E ( Y, Z ) [16]. The lower the energy, the more compatible (or likely) is the pair. We define the energy as: During training we minimize the following loss: The first term tries to make the output of the encoder as simila r as possible to the code Z . The second term is the mean-squared error between the input Y and the reconstruction provided by the decoder. The third term ensures the sparsity of the code by penalizing non zero values of code units; this t erm acts independently on each code unit and it is defined as h ( Z ) = P M ing to a factorized Student-t prior distribution on the non l inearly transformed code units [8] through the logistic of equation 6). The last term is an L1 regulariza tion on the filters to suppress noise and favor more localized filters. The loss formulated in equatio n 8 combines terms that characterize also other methods. For instance, the first two terms appear i n our previous model [3], but in that work, the weights of encoder and decoder were not tied and the parameters in the logistic were up-dated using running averages. The second and third terms are present in the  X  X ecoder-only X  model proposed in [8]. The third term was used in the  X  X ncoder-only  X  model of [7]. Besides the already-mentioned advantages of using an encoder-decoder architec ture, we point out another good feature of this algorithm due to its symmetry. A common idiosyncrasy for sparse-overcomplete methods using both a reconstruction and a sparsity penalty in the obj ective function (second and third term in equation 8), is the need to normalize the basis functions in the decoder during learning [8, 12] wi th somewhat ad-hoc technique, otherwise some of the basis func tions collapse to zero, and some blow up to infinity. Because of the sparsity penalty and the linear reconstruction, code units become tiny and are compensated by the filters in the decoder that grow wit hout bound. Even though the overall loss decreases, training is unsuccessful. Unfortunately, simply normalizing the filters makes less clear which objective function is minimized. Some authors h ave proposed quite expensive meth-ods to solve this issue: by making better approximations of t he posterior distribution [15], or by using sampling techniques [17]. In this work, we propose to e nforce symmetry between encoder and decoder (through weight sharing) so as to have automatic scaling of filters. Their norm cannot possibly be large because code units, produced by the encode r weights, would have large values as well, producing bad reconstructions and increasing the ene rgy (the second term in equation 7 and 8). Learning consists of determining the parameters in W , b equation 8. As indicated in the introduction, the energy aug mented with the sparsity constraint is minimized with respect to the code to find the optimal code. No marginalization over code distribu-tion is performed. This is akin to using the loss function in e quation 3. However, the log partition function term is dropped. Instead, we rely on the code sparsi ty constraints to ensure that the energy surface is not flat.
 Since the second term in equation 8 couples both Z and W and b minimize this energy with respect to both. On the other hand, once Z is given, the minimization with respect to W is a convex quadratic problem. Vice versa, if the parameters W are fixed, the optimal code Z  X  that minimizes L can be computed easily through gradient descent. This sugge sts the following iterative on-line coordinate descent learni ng algorithm: 1. for a given sample Y and parameter setting, minimize the loss in equation 8 with r espect to Z by gradient descent to obtain the optimal code Z  X  2. clamping both the input Y and the optimal code Z  X  found at the previous step, do one step of gradient descent to update the parameters.
 Unlike other methods [8, 12], no column normalization of W is required. Also, all the parameters are updated by gradient descent unlike in our previous work [ 3] where some parameters are updated using a moving average.
 After training, the system converges to a state where the dec oder produces good reconstructions from a sparse code, and the optimal code is predicted by a simp le feed-forward propagation through the encoder. In the following sections, we mainly compare SESM with RBM in order to better understand their differences in terms of maximum likelihood approximation, and in terms of coding efficiency and robustness.
 RBM As explained in the introduction, RBMs minimize an approxim ation of the negative log likelihood of the data under the model. An RBM is a binary stoc hastic symmetric machine defined by an energy function of the form: E ( Y, Z ) =  X  Z T W T Y  X  b T obvious at first glance, this energy can be seen as a special ca se of the encoder-decoder architecture that pertains to binary data vectors and code vectors [5]. Tr aining an RBM minimizes an approxima-tion of the negative log likelihood loss function 2, average d over the training set, through a gradient descent procedure. Instead of estimating the gradient of th e log partition function, RBM training uses contrastive divergence [10], which takes random sampl es drawn over a limited region  X  around the training samples. The loss becomes: Because of the RBM architecture, given a Y , the components of Z are independent, hence the sum over configurations of Z can be done independently for each component of Z . Sampling y in the neighborhood  X  is performed with one, or a few alternated MCMC steps over Y , and Z . This means that only the energy of points around training samples is pul led up. Hence, the likelihood function takes the right shape around the training samples, but not ne cessarily everywhere. However, the code vector in an RBM is binary and noisy, and one may wonder wh ether this does not have the effect of surreptitiously limiting the information conten t of the code, thereby further minimizing the log partition function as a bonus.
 SESM RBM and SESM have almost the same architecture because they b oth have a symmetric encoder and decoder, and a logistic non-linearity on the top of the encoder. However, RBM is trained using (approximate) maximum likelihood, while SESM is trai ned by simply minimizing the average energy F term to prevent flat energy surfaces, while RBM relies on an ex plicit contrastive term in the loss, an approximation of the log partition function. Also, the codi ng strategy is very different because code units are  X  X oisy X  and binary in RBM, while they are quasi-bin ary and sparse in SESM. Features extracted by SESM look like object parts (see next section), while features produced by RBM lack an intuitive interpretation because they aim at modeling th e input distribution and they are used in a distributed representation. 4.1 Experimental Comparison MNIST training dataset [18] in order to produce codes with 20 0 components. Similarly to [15] we have collected test image codes after the logistic non linea rity (except for PCA which is linear), and we have measured the root mean square error (RMSE) and the ent ropy. SESM was run for different values of the sparsity coefficient  X  quantized code produced by the encoder, P is the number of test samples, and  X  is the estimated variance of units in the input Y . Assuming to encode the (quantized) code units independent ly and with the same distribution, the lower bound on the number of b its required to encode each of them is given by: H is the number of quantization levels. The number of bits per pixel is then equal to: M in [15, 12], the reconstruction is done taking the quantized code in order to measure the robustness of the code to the quantization noise. As shown in fig. 1-C, RBM is very robust to noise in the code because it is trained by sampling. The opposite is true f or PCA which achieves the lowest RMSE when using high precision codes, but the highest RMSE wh en using a coarse quantization. SESM seems to give the best trade-off between RMSE and entrop y. Fig. 1-D/F compare the features learned by SESM and RBM. Despite the similarities in the arch itecture, filters look quite different in general, revealing two different coding strategies: dis tributed for RBM, and sparse for SESM. In the second experiment, we have compared these methods by m eans of a supervised task in order to assess which method produces the most discriminative repre sentation. Since we have available also the labels in the MNIST, we have used the codes (produced by th ese machines trained unsupervised) between outputs and targets, and has a mild ridge regularize r. Fig. 1-A/B show the result of these experiments in addition to what can be achieved by a linear cl assifier trained on the raw pixel data. Note that: 1) training on features instead of raw data improv es the recognition (except for PCA (E) (F) (G) (H) Figure 1: (A) -(B) Error rate on MNIST training (with 10, 100 and 1000 samples pe r class) and test set produced by a linear classifier trained on the codes p roduced by SESM, RBM, and PCA. The entropy and RMSE refers to a quantization into 256 bins. T he comparison has been extended also to the same classifier trained on raw pixel data (showing the advantage of extracting features). (same splits for all methods). The parameter  X  Comparison between SESM, RBM, and PCA when quantizing the co de into 5 and 256 bins. (D) Random selection from the 200 linear filters that were learne d by SESM (  X  of original and reconstructed digit from the code produced b y the encoder in SESM (feed-forward propagation through encoder and decoder). (F) Random selection of filters learned by RBM. (G) Back-projection in image space of the filters learned in the s econd stage of the hierarchical feature extractor. The second stage was trained on the non linearly t ransformed codes produced by the first stage machine. The back-projection has been performed by us ing a 1-of-10 code in the second stage machine, and propagating this through the second stage deco der and first stage decoder. The filters at the second stage discover the class-prototypes (manuall y ordered for visual convenience) even though no class label was ever used during training. (H) Feature extraction from 8x8 natural image patches: some filters that were learned. when the number of training samples is small), 2) RBM perform ance is competitive overall when few training samples are available, 3) the best performance is achieved by SESM for a sparsity level which trades off RMSE for entropy (overall for large trainin g sets), 4) the method with the best RMSE is not the one with lowest error rate, 5) compared to a SESM having th e same error rate RBM is more costly in terms of entropy. This section describes some experiments we have done with SE SM. The coefficient  X  a quasi-binary coding. The parameter  X  on the level of sparsity required by the specific application . 5.1 Handwritten Digits Fig. 1-B/E shows the result of training a SESM with  X  20000 digits scaled between 0 and 1, by setting  X  equal to 0.025 (decreased exponentially). Filters detect t he strokes that can be combined to form a digit. Even if the code unit activation has a very sparse dist ribution, reconstructions are very good (no minimization in code space was performed). 5.1.1 Hierarchical Features A hierarchical feature extractor can be trained layer-by-l ayer similarly to what has been proposed in [19, 1] for training deep belief nets (DBNs). We have train ed a second (higher) stage machine on the non linearly transformed codes produced by the first (l ower) stage machine described in the previous example. We used just 20000 codes to produce a highe r level representation with just 10 components. Since we aimed to find a 1-of-10 code we increased the sparsity level (in the second stage machine) by setting  X  feature detectors in the second stage machine look like digi t prototypes as can be seen in fig. 1-G. The hierarchical unsupervised feature extractor is able to capture higher order correlations among the input pixel intensities, and to discover the highly non-linear mapping from raw pixel data to the class labels. Changing the random initialization can somet imes lead to the discover of two different shapes of  X 9 X  without a unit encoding the  X 4 X , for instance. N evertheless, results are qualitatively very similar to this one. For comparison, when training a DBN , prototypes are not recovered because the learned code is distributed among units. 5.2 Natural Image Patches A SESM with about the same set up was trained on a dataset of 300 00 8x8 natural image patches randomly extracted from the Berkeley segmentation dataset [20]. The input images were simply times overcomplete code with 128 units. The parameters  X  0.4, 0.025, and 0.001 respectively. Some filters are localiz ed Gabor-like edge detectors in different positions and orientations, other are more global, and some encode the mean value (see fig. 1-H). There are two strategies to train unsupervised machines: 1) having a contrastive term in the loss function minimized during training, 2) constraining the in ternal representation in such a way that training samples can be better reconstructed than other poi nts in input space. We have shown that low RMSE and good recognition rate. We have also proposed a no vel symmetric sparse encoding method following the second strategy which: is particularl y efficient to train, has fast inference, works without requiring any withening or even mean removal f rom the input, can provide the best recognition performance and trade-off between entropy/RM SE, and can be easily extended to a compare different machines which is based on RMSE, entropy a nd, eventually, error rate when also labels are available. Interestingly, the machine achievin g the best performance in classification is the one with the best trade-off between reconstruction error an d entropy. A future avenue of work is to understand the reasons for this  X  X oincidence X , and deeper c onnections between these two strategies. Acknowledgments
