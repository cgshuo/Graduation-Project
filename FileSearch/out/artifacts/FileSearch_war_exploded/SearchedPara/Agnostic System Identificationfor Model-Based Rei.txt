 Robotics Institute, Carnegie Mellon University, PA USA J. Andrew Bagnell DBAGNELL @ RI . CMU . EDU Robotics Institute, Carnegie Mellon University, PA USA Model-based reinforcement learning (MBRL) and much of control rely on system identification : building a model of a system from observations that is useful for controller syn-thesis. While often treated as a typical statistical learn-ing problem, system identification presents different fun-damental challenges as the executed controller and data generating process are inextricably intertwined. Naively attempting to estimate a controlled system can lead to a model that makes small error on a training set, but exhibits poor controller performance. This problem arises as the policy resulting from controller synthesis is often very dif-ferent from the  X  X xploration X  policy used to collect data. While we might expect the model to make good predictions at states frequented by the exploration policy, the learned policy usually induces a different state distribution, where the model may poorly capture system behavior (Fig. 1). This problem is fully appreciated in the system identifica-tion literature and has been attacked by considering  X  X pen loop X  identification procedures and  X  X ersistent excitation X  (Ljung, 1999; Abbeel &amp; Ng, 2005) that attempt to suffi-ciently  X  X over X  the state-action space. Unfortunately, such methods rely on the strong assumption that the true system lies in the class of models considered: e.g. , for continuous systems, they may require the true system to be modeled in a class of linear models. With this assumption, they en-sure that eventually the correct model is learned X  e.g. , by learning about every discrete state-action pair or all modes of the linear system X  to provide guarantees.
 In this work, we provide algorithms for system identifica-tion and controller synthesis ( i.e. MBRL) that have strong performance guarantees with a weaker agnostic assumption that the system identification achieves statistically good prediction. We adopt a reduction-based analysis (Beygelz-imer et al., 2005) that relates the learned policy X  X  perfor-mance to prediction error during training. We begin by providing agnostic bounds for a simple generic  X  X atch X  al-gorithm that can represent many learning methods used in practice ( e.g. , building a model from open loop controls, watching an expert, or running a base policy we want to improve upon). Due to the mismatch in train/test distri-butions, uniform exploration is often the best option with this approach. Unfortunately, this makes the sample com-plexity and performance bounds scale with the size of the Markov Decision Process (MDP) ( i.e. state/action space). Next, we propose a simple iterative approach, closely re-lated to online learning, with stronger guarantees that do not scale with the size of the MDP when given a good ex-ploration distribution. The approach is very simple to im-plement and iterates between 1) collecting new data about the system by executing a good policy under the current model, as well as by sampling from a given exploration distribution, and 2) updating the model with that new data. This approach is inspired by a recent reduction of imitation learning to no-regret online learning (Ross et al., 2011) that addresses mismatch between train/test distributions. Our results can be interpreted as a reduction of MBRL to no-regret online learning and optimal control, and show that any no-regret algorithm can be used in such a way to learn a policy with strong agnostic guarantees. This enables MBRL methods to match the strongest existing agnostic guarantees of model-free RL methods (Kakade &amp; Lang-ford, 2002; Bagnell et al., 2003).
 We first introduce notation and related work. Then we present the batch method and our online learning approach with their agnostic guarantees (proofs are deferred to the supplementary material). Finally we demonstrate the ef-ficacy of our approach on a challenging domain from the literature: learning to perform aerobatic maneuvers with a simulated helicopter (Abbeel &amp; Ng, 2005). We assume the real system behaves according to some un-known MDP, represented by a set of states S and actions A (both potentially infinite and continuous), a transition func-tion T , where T sa denotes the next state distribution if we do action a in state s , and the initial state distribution  X  at time 1. We assume the cost function C : S  X  A  X  R is known and seek to minimize the expected sum of dis-counted costs over an infinite horizon with discount  X  . For any policy  X  , let  X  s be the action distribution performed by  X  in state s ; D t  X , X  the state-action distribution at time t if we started in state distribution  X  at time 1 and followed  X  ; D  X , X  = (1  X   X  ) P  X  t =1  X  t  X  1 D t  X , X  the state-action dis-tribution over the infinite horizon if we follow  X  , starting the value function of  X  (the expected sum of discounted costs of following  X  starting in state s ); Q  X  ( s,a ) = C ( s,a ) +  X  E s 0  X  T sa [ V  X  ( s 0 )] the action-value function of  X  (the expected sum of discounted costs of following  X  af-ter starting in s and performing action a ); and J  X  (  X  ) = E sum of discounted costs of following  X  starting in  X  . Our goal is to obtain a policy  X  with small regret, i.e. for any policy  X  0 , J  X  (  X  )  X  J  X  (  X  0 ) is small. This is achieved indirectly by learning a model  X  T of the system and solving for a (near-)optimal policy (under  X  T ); e.g. , using dynamic programming (Puterman, 1994) or approximate methods (Szepesv  X  ari, 2005; Williams, 1992). For continu-ous systems, an important special case is linear models with quadratic cost functions, and potentially additive Gaus-sian noise, known as Linear Quadratic Regulators (LQR) 1 which can be solved exactly and efficiently. Non-linear sys-tems with non-quadratic cost functions can also be solved approximately (local optima) using efficient iterative lin-earization techniques such as iLQR(Li &amp; Todorov, 2004). Related Work: In contrast with  X  X extbook X  system iden-tification methods, in practice control engineers often pro-ceed iteratively to build good models for controller synthe-sis. A first batch of data is collected to fit a model and obtain a controller, which is then tested in the real system. If performance is unsatisfactory, data collection is repeated with different sampling distributions to improve the model where needed, until control performance is satisfactory. By doing so, engineers can use feedback of the policies found during training to decide how to collect data and improve performance. Such methods are commonly used in prac-tice and have demonstrated good performance in the work of Atkeson &amp; Schaal (1997); Abbeel &amp; Ng (2005). In both works, the authors proceed by fitting a first model from state transitions observed during expert demonstrations of the task, and at following iterations, using the optimal pol-icy under the current model to collect more data and fit a new model with all data seen so far. Abbeel &amp; Ng (2005) show this approach has good guarantees in non-agnostic settings (for finite MDPs or LQRs), in that it must find a policy that performs as well as the expert providing the ini-tial demonstrations. Our method can be seen as making algorithmic this engineering practice, extending and gener-alizing the previous methods of Atkeson &amp; Schaal (1997); Abbeel &amp; Ng (2005), and suggesting slight modifications that provide good guarantees even in agnostic settings. Similarly, the Dataset Aggregation (DAgger) algorithm of Ross et al. (2011) uses a similar data aggregation proce-dure over iterations to obtain policies that mimic an ex-pert well in imitation learning. The authors show that such a procedure can be interpreted as an online learning al-gorithm (Hazan et al., 2006; Kakade &amp; Shalev-Shwartz, 2008), more specifically, Follow-the-(Regularized)-Leader (Hazan et al., 2006), and that using any no-regret online al-gorithm ensures good performance. Our approach can be seen as an extension of DAgger to MBRL settings.
 Our approach leverages the way agnostic model-free RL al-gorithms perform exploration. Methods such as Conserva-tive Policy Iteration (CPI) (Kakade &amp; Langford, 2002) and Policy-Search by Dynamic Programming (PSDP) (Bagnell et al., 2003) learn a policy directly by updating policy pa-rameters iteratively. For exploration, they assume access to a state exploration distribution  X  that they can restart the system from and can guarantee finding a policy per-forming nearly as well as any policies inducing a state dis-tribution (over a whole trajectory) close to  X  . Similarly, our approach uses a state-action exploration distribution to sample transitions and allows us to guarantee small re-gret against any policy with a state-action distribution close to this exploration distribution. If the exploration distri-bution is close to that of a near-optimal policy, then our approach guarantees near-optimal performance, provided a good model of data exists. This allows our model-based method to match the strongest agnostic guarantees of ex-isting model-free methods. Good exploration distributions can often be obtained in practice; e.g. , from human expert demonstrations, domain knowledge, or from a desired tra-jectory we would like the system to follow. Additionally, if we have a base policy we want to improve, it can be used to generate the exploration distribution  X  with potentially additional random exploration in the actions. We now describe a simple algorithm, refered to as Batch , that can be used to analyze many common approaches from the literature, e.g. , learning from a generative model 2 loop excitation or by watching an expert (Ljung, 1999). Let T denote the class of transition models considered, and  X  a state-action exploration distribution we can sample the system from. Batch first executes in the real system m state-action pairs sampled i.i.d. from  X  to obtain m sam-pled transitions. Then it finds the best model  X  T  X  X  of ob-served transitions, and solves (potentially approximately) the optimal control (OC) problem with  X  T and known cost function C to return a policy  X   X  for test execution. 3.1. Analysis Our reduction analysis seeks to answer the following ques-tion: if Batch learns a model  X  T with small error on train-ing data, and solves the OC problem well, what guarantees does it provide on control performance of  X   X  ? Our results illustrate the drawbacks of a purely batch method due to the mismatch in train-test distribution.
 We measure the quality of the OC problem X  X  solution as fol-lows. For any policy  X  0 , let  X  0 oc = E s  X   X  [  X  V  X   X  denote how much better  X  0 is compared to  X   X  on model  X  T (  X 
V  X   X  and  X  V  X  0 are the value functions of  X   X  and  X  0 learned model  X  T respectively). If  X   X  is an -optimal pol-icy on  X  T within some class of policies  X  , then  X  for all  X  0  X   X  . A natural measure of model error that arises from our analysis is in terms of L 1 distance between the predicted and true next state X  X  distributions. That is, error of  X  T , measured in L 1 distance, under the training distribution  X  . However, the L 1 distance cannot be eval-uated or optimized from sampled transitions during train-ing (we observe samples from T sa but not the distribution). Therefore we also provide our bounds in terms of other losses we can minimize from samples. This directly re-lates control performance to the model X  X  training loss. A convenient loss is the KL divergence between T sa and  X  T mizing KL corresponds to maximizing the log likelihood of the sampled transitions. This is convenient for com-mon model classes, such as linear models (as in LQR), where it amounts to linear regression. For particular cases where T is a set of deterministic models and the real system has finitely many states, the predictive error can be mea-sured via a classification loss at predicting the next state: whether  X  T predicts s 0 for ( s,a ) , or any upper bound on the 0-1 loss, e.g. , the multi-class hinge loss if T is a set of SVMs. In this case, model fitting is a supervised classifi-cation problem and the guarantee is directly related to the training classification loss. These are related as follows: holds with equality if ` is the 0-1 loss.
 In general, we can use any loss minimizable from samples that upper bounds L1 prd for models in the class. Our bounds are also related to the mismatch between the exploration distribution  X  and distribution induced by executing an-other policy  X  starting in  X  , denoted c  X   X  = sup s,a D  X , X  We assume the costs C ( s,a )  X  [ C min ,C max ]  X  ( s,a ) . Let C tor that relates model error to error in total cost predictions. Theorem 3.1. The policy  X   X  is s.t. for any policy  X  0 : This also holds as a function of KL prd or cls prd using Lem. 3.1. This bound indicates that if Batch solves the OC problem well and  X  T has small enough error under the training dis-tribution  X  , then it must find a good policy. Importantly, this bound is tight: i.e. we can construct examples where it holds with equality (see supplementary material). More interestingly is what happens as we collect more data. If the fitting procedure is consistent (i.e. picks a model with minimal loss in the class asymptotically), then we can re-late this guarantee to the capacity of the model class to achieve low error under the training distribution  X  . We de-note the modeling error, measured in L 1 distance, as L1 mdl all 0 in realizable settings, but generally non-zero in agnos-tic settings. After sampling m transitions, the generaliza-tion error L1 gen ( m, X  ) bounds with high probability 1  X   X  the denote the generalization error for the KL and classifica-tion loss respectively. cls gen ( m, X  ) can be related to the VC dimension (or multi-class equivalent) in finite MDPs. Corollary 3.1. After observing m transitions, with proba-bility at least 1  X   X  , for any policy  X  0 :
J  X  ( X   X  )  X  J  X  (  X  0 ) +  X  This also holds as a function of KL mdl + KL gen ( m, X  ) (or procedure is consistent in terms of L 1 distance (or KL, clas-sification loss), then L1 gen ( m, X  )  X  0 (or KL gen ( m, X  )  X  0 , gen ( m, X  )  X  0 ) as m  X  X  X  for any  X  &gt; 0 .
 The generalization error typically scales with the com-plexity of the class T and goes to 0 at a rate of O ( 1  X  (  X 
O ( 1 m ) in ideal conditions). Given enough samples, the dominating factor limiting performance becomes the mod-formance degrades for agnostic settings.
 Drawback of Batch: The two factors c  X   X   X  and c  X  0  X  are qual-itatively different. c  X  0  X  measures how well  X  explores state-actions visited by the policy  X  0 we compare to. This factor is inevitable: we cannot hope to compete against policies that spend most of their time where we rarely explore. c  X   X  measures the mismatch in train-test distribution. Its pres-ence is the major drawback of Batch . As  X   X  cannot be known in advance, we can only bound c  X   X   X  by considering all policies we could learn: sup  X   X   X  c  X   X  . This worst case is likely to be realized in practice: if  X  rarely explores some state-action regions, the model could be bad for these and significantly underestimate their cost. The learned policy is thus encouraged to visit these low-cost regions where few data were collected. To minimize sup  X   X   X  c  X   X  , the best  X  for Batch is often a uniform distribution, when possi-ble. This introduces a dependency on the number of states and actions (or state-action space volume) ( i.e. c  X   X   X  O ( | S || A | ) ) multiplying the modeling error. Sampling from a uniform distribution often requires access to a generative model. If we only have access to a reset model 3 and a base policy  X  0 inducing  X  when executed in the system, then c could be arbitrarily large ( e.g. , if  X   X  goes to 0 probability states under  X  0 ), and  X   X  arbitrarily worse than  X  0 . In the next section, we show that iterative learning meth-ods can leverage feedback of the learned policies to ob-tain bounds that do not depend on c  X   X   X  . This leads to better guarantees when we have a good exploration distribution  X  ( e.g. , that of a near-optimal policy), or when we can only collect data via a reset model. This also leads to better per-formance in practice as shown in the experiments. Our extension of DAgger to the MBRL setting proceeds as follows. Starting from an initial model  X  T 1  X  T , solve (approximately) the OC problem with  X  T 1 to obtain pol-icy  X  1 . At each iteration n , collect data about the sys-tem by sampling state-action pairs from distribution  X  n =  X  + 1 2 D  X , X  n : i.e. w.p. 1 2 , sample a transition occurring from an exploratory state-action pair drawn from  X  and add it to dataset D , otherwise, sample a state transition occur-ring from running the current policy  X  n starting in  X  , stop-ping the trajectory w.p. 1  X   X  at each step and adding the last transition to D . The dataset D contains all transitions observed so far over all iterations. Once data is collected, find the best model  X  T n +1  X  X  that minimizes an appropri-ate loss ( e.g. regularized negative log likelihood) on D , and solve (approximately) the OC problem with  X  T n +1 to ob-tain the next policy  X  n +1 . This is iterated for N iterations. At test time, we could either find and use the policy with lowest expected total cost in the sequence  X  1: N , or use the uniform  X  X ixture X  policy 4 over  X  1: N . We guarantee good performance for both. The last policy  X  N often performs equally well, it has been trained with most data. Our ex-perimental results confirm this intuition. In theory,  X  N good guarantees when the distributions D  X , X  i converge to a small region in the space of distributions as i  X   X  , but we do not guarantee this always occurs.
 Implementation with Off-the-Shelf Online Learner: DAgger as described can be interpreted as using a Follow-The-(Regularized)-Leader (FTRL) online algorithm to pick the sequence of models: at each iteration n we pick the best (regularized) model  X  T n in hindsight under all samples seen so far. In general, DAgger can also be implemented using any no-regret online algorithm (see Algorithm 1) to provide good guarantees. This is done as follows. When minimizing the negative log likelihood, the loss function of the online learning problem at iteration i is: L KL i E from sampled state transitions at iteration i , and evaluated for any model  X  T . The online algorithm is applied on the se-quence of loss L KL 1: N to obtain a sequence of models  X  over the iterations. As before, each model  X  T i is solved to obtain the next policy  X  i . By doing so, the online algo-rithm effectively runs over mini-batches of data collected at each iteration to update the model, and each mini-batch comes from a different distribution that changes as we up-date the policy. Similarly, in a finite MDP with a determin-istic model class T , we can minimize the 0-1 loss instead (or any upper bound such as hinge loss) where the loss at ` the particular classification loss. This corresponds to an online classification problem. For many model classes, the negative log likelihood and convex upper bounds on the 0-1 loss (such as hinge loss) lead to convex online learning problems, for which no-regret algorithms exist ( e.g. , gra-dient descent, FTRL). As shown below, if the sequence of models is no-regret, then performance can be related to the minimum KL divergence (or classification loss) achievable with model class T under the overall training distribution  X  = 1 N P N i =1  X  i ( i.e. a quantity akin to KL mdl or Batch ).
 Algorithm 1 DAgger algorithm for Agnostic MBRL.
 Input: exploration distribution  X  , number of iterations N , number of samples per iteration m , cost function
C , online learning procedure O NLINE L EARNER , opti-mal control procedure OCS OLVER .

Get initial guess of model:  X  T 1  X  O NLINE L EARNER () .  X  1  X  OCS OLVER (  X  T 1 ,C ) . for n = 2 to N do end for
Return the sequence of policies  X  1: N . 4.1. Analysis Similar to our analysis of Batch , we seek to answer the following: if there exists a low error model of training data, and we solve each OC problem well, what guarantees does DAgger provide on control performance? Our results show that by sampling data from the learned policies, DAgger provides guarantees that have no train-test mismatch factor, leading to improved performance.
  X  V i ( s )] , where tion of  X  i and  X  0 under model  X  T i . This measures how well we solved each OC problem on average over the iterations. For instance, if at each iteration i we found an i -optimal policy within some class of policies  X  on learned model  X  then  X  0 oc  X  1 N P N i =1 i for all  X  0  X   X  . As in Batch , the av-erage predictive error of the models  X  T 1: N can be measured in terms of the L 1 distance between the predicted and true T sa || 1 ] . However, as was discussed, the L 1 distance is not observed from samples which makes it hard to minimize. Instead we can define other measures which upper bounds this L 1 distance and can be minimized from samples, such as the KL divergence or classification loss: i.e. KL given the sequence of policies  X  1: N , let  X   X  =  X  the uniform mixture policy on the sequence.
 Lemma 4.1. The policies  X  1: N are s.t. for any policy  X  0 This also holds as a function of KL prd or cls prd using Lem. 3.1. the sequence of losses L KL 1: N implies 1 N P N i =1 L KL gret of the algorithm after N iterations, s.t. KL rgt  X  0 as N  X   X  . This relates KL prd to the modeling error of the by using a no-regret algorithm on L cls 1: N , cls prd  X  cls for cls rgt  X  0 . In some cases, even if the L 1 distance cannot be estimated from samples, statistical estimators can still be no-regret with high probability on the sequence of loss L i ( T nite MDPs if we use the empirical estimator of T based on data seen so far (see supplementary material). If we define Theorem 4.1. The policies  X  1: N are s.t. for any policy  X  This also holds as a function of KL mdl + KL rgt (or cls mdl using Lem. 3.1. If the fitting procedure is no-regret w.r.t the rgt  X  0 , cls rgt  X  0 ) as N  X  X  X  .
 Additionally, the performance of  X  N can be related to  X  if the distributions D  X , X  i converge to a small region: Lemma 4.2. If there exists a distribution D  X  and some cnv  X  0 s.t.  X  i , || D  X , X  i  X  D sequence { i cnv }  X  i =1 that is o (1) , then  X  N is s.t.:
J  X  (  X  N )  X  J  X  (  X  ) + Thm. 4.1 illustrates how we can reduce the original MBRL problem to a no-regret online learning problem on a par-ticular sequence of loss functions. In general, no-regret al-gorithms have average regret of O ( 1  X  cases) such that the regret term goes to 0 at a similar rate to the generalization error term for Batch in Cor. 3.1. Here, given enough iterations, the term c  X  0  X  H L1 mines how performance degrades in the agnostic setting (or c on the sequence of KL or classification loss respectively). Unlike for Batch , there is no dependence on c  X   X   X  , only on c  X  . Thus, if a low error model exists under training distri-bution  X  , no-regret methods are guaranteed to learn policies that performs well compared to any policy  X  0 for which c is small. Hence,  X  is ideally D  X , X  of a near-optimal policy  X  ( i.e. explore where good policies go).
 Finite Sample Analysis: A remaining issue is that the cur-rent guarantees apply if we can evaluate the expected loss ( L i , L at each iteration. If we run the no-regret algorithm on esti-mates of these loss functions, i.e. loss on m sampled tran-sitions, we can still obtain good guarantees using martin-gale inequalities as in online-to-batch (Cesa-Bianchi et al., 2004) techniques. The extra generalization error term is typically O ( q log(1 / X  ) Nm ) with high probability 1  X   X  . While our focus is not on providing such finite sample bounds, we illustrate how these can be derived for two scenarios in the supplementary material. For instance, in finite MDPs with | S | states and | A | actions, if  X  T i is the empirical estimator of T based on samples collected in the first i  X  1 iterations, antees that w.p. 1  X   X  , for any policy  X  0 : Here, mdl does not appear as it is 0 (realizable case). Given a good state-action distribution  X  , the sample complexity to proves upon other state-of-the-art MBRL algorithms, such &amp; Szepesv  X  ari, 2010) (when | S | &lt; 1 (1  X   X  ) 2 pendency on | S | 2 | A | is due to the complexity of the class ( | S | 2 | A | parameters). With simpler classes, it can have no dependency on the size of the MDP. In the supplementary material, we analyze a scenario where T is a set of kernel SVM (deterministic models) with RKHS norm bounded by antees that w.p. 1  X   X  , for any policy  X  0 :
J  X  ( X   X  )  X  J  X  (  X  )  X  J  X  (  X  0 ) +  X  for  X  cls mdl the multi-class hinge loss on the training set after N iterations of the best SVM in hindsight. Thus, if we have a good exploration distribution and there exists a good model in T for predicting observed data, we obtain a near-optimal policy with sample complexity that depends only on the complexity of T , not the size of the MDP. We emphasize that we provide reduction-style guarantees. DAgger may sometimes fail to find good policies, e.g. , when no model in the class achieves low error on the train-ing data. However, DAgger guarantees that one of the fol-lowing occur: either (1) we find good policies or (2) no models with low error on the aggregate dataset exist. If the latter occurs, we need a better model class. In contrast, Batch can find models with low training error, but still fail at obtaining a policy with good control performance, due to train/test mismatch. This occurs even in scenarios where DAgger finds good policies, as shown in the experiments. DAgger needs to solve many OC problems. This can be computationally expensive, e.g. , with non-linear or high-dimensional models. Many approximate methods can be used, e.g. , policy gradient (Williams, 1992), fitted value it-eration (Szepesv  X  ari, 2005) or iLQR (Li &amp; Todorov, 2004). As the models often change only slightly from one itera-tion to the next, we can often run only a few iterations of dynamic programming/policy gradient from the last value function/policy to obtain a good policy for the current model. As long as we get good solutions on average,  X  0 oc remains small and does not hinder performance.
 DAgger generalizes the approach of Atkeson &amp; Schaal (1997) and Abbeel &amp; Ng (2005) so that we can use any no-regret algorithm to update the model, as well as any explo-ration distribution. A key difference is that DAgger keeps an even balance between exploration data and data from running the learned policies. This is crucial to avoid set-tling on suboptimal performance in agnostic settings as the exploration data could be ignored if it occupies only a small fraction of the dataset, in favor of models with lower error on the data from the learned policies. With this modifica-tion, our main contribution is showing that such methods have good guarantees even in agnostic settings. We demonstrate the efficacy of DAgger on a challenging problem: learning to perform aerobatic maneuvers with a simulated helicopter, using the simulator of Abbeel &amp; Ng (2005), which has a continuous 21-dimensional state and 4-dimensional control space. We consider learning to 1) hover and 2) perform a  X  X ose-in funnel X  maneuver. We compare DAgger to Batch with several choices for  X  : 1)  X  adding small white Gaussian noise 5 to each state and action along the desired trajectory, 2)  X  e : run an expert controller, and 3)  X  en : run the expert controller with additional white Gaussian noise 6 in the controls of the expert. The expert controller is obtained by linearizing the true model about the desired trajectory and solving the LQR (iLQR for the nose-in funnel). We also compare against Abbeel X  X  algo-rithm, where the expert is only used at the first iteration. Hover: All approaches begin with an initial model  X  x t +1 = A  X  x t + B  X  u t , for  X  x t the difference between the current and hover state at time t ,  X  u t the delta con-trols at time t , A is identity and B adds the delta controls to the actual controls in  X  x t . We seek to learn offset ma-trices A 0 , B 0 that minimizes ||  X  x t +1  X  [( A + A 0 ) X  x ( B + B 0 ) X  u t ] || 2 on observed data 7 . We attempt to learn to hover in the presence of noise 8 and delay of 0 and 1. A delay of 1 introduces high-order dynamics that cannot be modeled with the current state. All methods sample 100 transitions per iteration and run for: 50 iterations when de-lay is 0; 100 iterations when delay is 1. Figure 2 shows the test performance of each method after each iteration. In both cases, for any choice of  X  , DAgger outperforms Batch significantly and converges to a good policy faster. DAgger is more robust to the choice of  X  , as it always ob-tains good performance given enough iterations, whereas Batch obtains good performance with only one choice of  X  in each case. Also, DAgger eventually learns a policy that outperforms the expert policy (L). As the expert pol-icy is inevitably visiting states far from the hover state due to the large noise and delay (unknown to the expert), the linearized model is not as good at those states, leading to slightly suboptimal performance. Thus DAgger is learning a better linear model for the states visited by the learned policy which leads to better performance. Abbeel X  X  algo-rithm improves the initial policy but reaches a plateau. This is due to lack of exploration (expert demonstrations) after the first iteration. While our objective is to show that DAg-ger outperforms other model-based approaches, we also compared against a model-free policy gradient method sim-ilar to CPI 9 . However, 100 samples per iteration were insuf-ficient to get good gradient estimates and lead to only small improvement. Even with 500 samples per iteration, it could only reach an avg. total cost  X  15000 after 100 iterations. Nose-In Funnel: This maneuver consists in rotating at a fixed speed and distance around an axis normal to the ground with the helicopter X  X  nose pointing towards the axis of rotation (desired trajectory in Fig. 1). We attempt to learn to perform 4 complete rotations of radius 5 in the presence of noise 10 but no delay. We start each approach with a linearized model about the hover state and learn a time-varying linear model 11 . All methods collect 500 sam-ples per iteration over 100 iterations. Figure 2 (bottom) shows the test performance after each iteration. With the initial model (0 data), the controller fails to produce the maneuver and performance is quite poor. Again, with any choice of  X  , DAgger outperforms Batch , and unlike Batch , it performs well with all choices of  X  . A video comparing qualitatively the learned maneuver with DAgger and Batch is available on YouTube (Ross, 2012). Abbeel X  X  method improves performance slightly but again suffers from lack of expert demonstrations after the first iteration. We presented a no-regret online learning approach to MBRL that has strong performance, both in theory and practice, even in agnostic settings. It is simple to imple-ment, formalizes and makes algorithmic the engineering practice of iterating between controller synthesis and sys-tem identification, and can be applied to any control prob-lem where approximately solving the OC problem is feasi-ble. Additionally, its sample complexity scales with model class complexity, not the size of the MDP. To our knowl-edge, this is the first practical MBRL algorithm with agnos-tic guarantees. The only other agnostic MBRL approach we are aware of is a recent agnostic extension of R (Szita &amp; Szepesv  X  ari, 2011) that is largely theoretical: it re-quires unknown quantities to run the algorithm ( e.g. , dis-tance between the real system and the model class) and its sample complexity is exponential in the class complexity. This work is supported by the ONR MURI grant N00014-09-1-1052, Reasoning in Reduced Information Spaces.
