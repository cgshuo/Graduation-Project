 TATSUNORI MORI, MASANORI NOZAWA, and YOSHIAKI ASADA Yokohama National University 1. INTRODUCTION
Recently, a large number of documents became available to us in an electronic format. However, it is, rather difficult to efficiently obtain necessary informa-tion from them. Although currently, we can obtain a set of relevant documents or answers by information retrieval (IR) and question answering (QA) technolo-gies, it is still necessary to examine the original documents.

One of the complementary technologies used for document retrieval by IR systems is the automatic multi-document summarization. In par-ticular, Answer-Focused Summarization has recently gained considerable attention [Hirao et al. 2001; Wu et al. 2002]. This technology is based on the empirical viewpoint that the information need of a user can be described by a set of questions. One of the four tasks in DUC 2003, the conference organized by NIST, was to generate summaries of multiple documents in response to a single question [Over and Yen 2003]. In multi-document summarization, a mod-erate amount of text is required for users to be able to understand the content, therefore, when a separate summary is prepared for each question that a user intends to know about, the amount of information that user finally reads would not be little.

Consequently, in order to realize multi-document summarization focused by multiple questions, we propose a method to calculate sentence importance using
QA scores of words, for responses to multiple questions, generated by a QA engine. We also describe the integration of this method with a generic multi-document summarization system. The main contribution of this paper can be summarized as follows: (1) Proposal of a method to calculate the importance of a sentence1 by integrating the QA scores of the words in the sentence for multiple questions into the importance of the sentence. (2) Proposal of a method to combine the sentence importance derived from the QA scores with that of a conventional scheme for important sentence extraction. 2. RELATED WORK
In this section, we present the relationship of this study to other studies. The proposed method is related to two types of summarization researches X  X he multi-document summarization and the question-focused summarization.
With regard to multi-document summarization, we utilize several existing methods. We employ the word weighting method based on Information Gain Ratio (IGR) [Mori 2002], as described in Section 5. Further, we adopt Maximal
Marginal Relevance (MMR) for redundancy control[Carbonell and Goldstein 1998]. We also utilize the Hanning window function for text summarization based on Hirao et al. [2001].

In terms of question-focused summarization, we introduce a new methodol-ogy to generate a summary of multiple documents to answer multiple questions.
It is achieved by the following: (1) calculation of importance of sentence by in-tegrating the QA scores of the words in a sentence for multiple questions and (2) combination of the sentence importance derived from the QA scores with that of a conventional scheme for important sentence extraction. In particular, to the best of our knowledge, only minimum research has been conducted on text summarization using QA systems. At DUC 2003, a research group from
Columbia University employed a QA system, however, they did not describe it in detail [Nenkova et al. 2003]. Gaizauskas et al. [2003] are currently working on a project called the  X  X ubreporter X  that aims at integrating the technologies, in-cluding question answering and multi-document summarization technologies. However, they have not published any papers in this regard.

The basic methodology for the question-biased summarization is to provide higher weight to words in a question [Tombros and Sanderson 1998; Berger and Mittal 2000; Nobata and Sekine 2003]. Okumura and Mochizuki [2000] focused on the lexical chains with respect to words in a question sentence, while
Hirao et al. [2001] focused on not only words in a question but also named en-tities (NEs) corresponding to the question type. Wu et al. [2002] examined the relationship between the types of questions and the unit length of text frag-ments during summary generation. Our experiments, however, show that our proposed method, which utilizes the information of the answers obtained from a QA system, outperforms the baselines that depend only on the information of questions. 3. OVERVIEW OF THE PROPOSED METHOD
In this study, we assumed that a set of documents to be summarized is provided as an IR result and a set of questions, corresponding to the information required by a user, is also provided. With regard to the questions, it is more natural for a user to reply to the questions one at a time while interacting with a system and, depending on the input, the system gradually outputs a part of the summary including an answer by taking into account its relationship to the previously displayed parts of the summary. In this study, however, as a primary approxi-mation, we assume that all questions are provided simultaneously. Under this assumption, the following are essential for multi-document summarization: (1) extraction of important parts according to information requirement, (2) reduc-tion of redundancy in documents, and (3) identification of differences among documents. We adopt the following techniques for achieving the above objec-tives. (a) Calculation of sentence importance based on scores of a QA engine for realizing (1). (See Section 4.) (b) Calculation of sentence importance based on
IGR with respect to probabilistic distribution of words for realizing (1) and (3). (See Section 5.1.) (c) Control of redundancy in summary text based on MMR for realizing (2) and (3). (See Section 5.3.) We also introduce (d) smoothing of sentence importance by the Hanning window function. (See Section 5.2.) Figure 1 illustrates an overview of the multi-document summarizer for
Japanese documents based on our proposed method. The system inputs include a set of document IDs, a set of questions, which represents user X  X  information requirements, and the summary length. The output generated by the system is in the form of an extract, i.e., a series of sentences extracted from the doc-uments. For example, in the case of topic 0500 (articles on the cloned sheep  X  X olly X ) in the test collection of NTCIR4 TSC3, 1 the input comprised nine doc-ument IDs, shown in Figure 2, and ten questions, shown in Figure 3, and a summary length of 491 characters. When an input is fed to the system, it first computes the importance score for each sentence by integrating the importance of the following two types: (a) sentence importance based on scores of a QA en-gine and (b) sentence importance based on IGR with respect to probabilistic distribution of words. Second, the sentence importance is smoothed by apply-ing the Hanning window function to maintain cohesion in the output summary.
Third, using MMR, the system reorders sentences so as to reduce the redun-dancy in the summary text while taking sentence importance into account. The system then selects the top n sentences of the ordered list. Finally, it orders the selected sentences according to the cluster structure of the original documents and the chronological order of the documents; these ordered sentences are out-put as a summary. Figure 3 is an example of the output summary, in which each phrase in italic face corresponds to one of the answers to the questions shown in Figure 4. 4. CALCULATION OF SENTENCE IMPORTANCE USING A QA ENGINE In order to deal with the given questions, we adopt a method that employs a
Japanese QA engine for factoid questions proposed by Mori et al. [2003]. This method does not require any preprocessing of documents and users can, there-fore, use various search engines by providing simple wrappers. Since perform-ing processing, such as Japanese morphological analysis, 2 of NEs, and so on, is computationally expensive, they introduced a search mech-anism for finding answers to a submitted question based on the A
Furthermore, they proposed a method to approximately estimate the score of words by less expensive means. This mechanism reduces the computational cost on irrelevant parts of documents and a real-time processing is achieved.
The QA engine assigns a score to each word in a document that represents the appropriateness of the word in terms of the answer to a question. Under the assumption that a word under consideration is the answer, i.e., if the word is associated with the interrogative in the question sentence, the score is cal-culated as the degree of matching between the remaining part of the question and that part of the sentence where the word appears. The measure of match-ing proposed by them is a linear combination of the following measures: the number of shared character bigrams, the number of shared words, the degree of case matching, the degree of matching between dependency structures, and the degree of matching between the type of NE in sentences and the type of question.

We thus propose a method that utilizes the QA score as the weight of a word and then calculates the sentence importance based on these scores. It should be noted that for multiple questions, we did not utilize the answer strings but the scores of each word. This method enables us to easily integrate multiple outputs generated by the QA engine for multiple questions into one value representing sentence importance.
 In this study, we assume that a set of questions is supplied to the system.
Consequently, not one but a tuple of QA scores is assigned to each word. Each QA score in the tuple is related to a question. The range of QA scores depends on the complexity and the type of question. Initially, drawing a comparison between the scores for different questions would have been meaningless. Therefore, we aim to assign a single unified importance value to each word with respect to a set of questions. To achieve this, we normalized the range of QA scores to make them comparable. In this paper, we adopt the T score given by Eq. (1) as a normalization method to examine the deviation from the average score of words because, when answering a question, it is not important to obtain absolute values of the scores of the words, but to obtain a relative order among them: where x is a score value to be normalized and D is a set of score values for a word in the sentence S i with respect to the question q , then the importance value Imp n QA ( S i ) for S i is calculated as follows: where Q is the set of given questions. Since the sentence importance depends on whether the sentence includes at least one of the answers, we adopt the maximum function to integrate the normalized scores. It should be noted that, typically, the QA score of a word differs from that of another word despite these words having the same surface expression. This is so because the scores depend on the contexts in which these words are embedded.

We expect that in summary generation, the proposed method will achieve a performance better than other existing methods, which usually use the fre-quency of words in questions, the question types, and/or the frequency of NEs in the documents. It may be considered that our QA-biased method is similar to other question-and/or NE-biased methods, since our QA engine assigns a score to each word according to the NE type as well as on the basis of the similarity between a question and a sentence in which the word appears. Our QA-biased method, however, mainly differs from the other methods with respect to the following aspects: 1. Question-and NE-biased methods use the information of words shared by a sentence and a question and the binary information on whether expressions of a certain NE type exist. On the other hand, our QA-biased method utilizes not only this information, but also more fine-grained information such as, the similarity of dependency structures among a sentence and a question. 2. Our QA engine calculates the QA score for a word by considering not only the sentence in which the word appears, but also the previous and following sentences in a predetermined sentence window.

One may aim at incorporating the above information into question-and NE-biased methods, this combination, however, would essentially be the same as the basic structure of the QA engine. 5. OTHER TECHNICAL ISSUES OF THE PROPOSED METHOD 5.1 Sentence Importance Based on Information Gain Ratio (IGR)
Mori et al. [2002] proposed a method to calculate the sentence importance using a word weighting technique based on IGR. This method extracts the information about similarity among given documents by hierarchical clustering and assigns an importance value to each word according to whether the probabilistic dis-tribution of the word is consistent with the cluster structure of the documents.
We adapt this method to obtain the sentence importance for the given docu-ments. Let C i be the i th subcluster of cluster C , then IGR of the probabilistic distribution of a word w in this cluster is defined as follows.

The following points should be noted here. 1. When a set of documents to be summarized is retrieved by an informa-2. We obtain one weight value for each word in each cluster. However, one doc-
The word weight described above is combined with the other existing word weights, i.e., TF and IDF . We define the importance Imp IGR
S i as the average weight of nominals in the sentence, given by Eq. (4). The importance value is normalized across a document by the T score and is denoted as Imp n IGR ( S i ). Finally, the integrated importance Imp is defined as the linear combination of Imp n IGR ( S i ) and Imp where  X  is the mixing factor.
 5.2 Smoothing of Sentence Importance by Hanning Window Function
In our method described so far, each sentence is processed separately. However, when a relatively large number of documents are supplied, there seems to be less cohesion between the sentences in the generated summary. This is because the method tends to equally choose a small number of important sentences from all the documents. To generate a long summary, it is necessary to improve the cohesion between the sentences while taking into account the importance of these sentences. We, therefore, introduce a method to smooth sentence im-portance using the Hanning window function. The sentence importance after being smoothed by the function of the window size W is defined as follows.
In typical cases where a sentence with moderate importance is placed between two very important sentences, the weight of the middle sentence is increased by the window function. As a result, this series of three sentences is likely to be selected as a part of the summary. In this case, the adoption of the middle sentence may improve the cohesion between the two very important sentences. 5.3 Control of Redundancy in Summary Based on MMR We introduce a redundancy control mechanism based on MMR proposed by
Carbonell and Goldstein [1998]. MMR is basically a method to reorder doc-uments or passages by taking into account both the redundancy and their relevance to a query. It is applied to selection of important sentences while considering redundancy when the unit of selection is changed to a sentence and the initial order of sentences is given by sentence importance as follows: where SS is the set of sentences to be summarized, A is the set of sentences already selected in the summary, Sim s is the similarity between two sentences, and  X  is the parameter to control the degree of redundancy. In this paper, we term it as MMI-MS (Maximal Marginal Importance  X  Multi-Sentence). When we iteratively apply Eq. (7) to SS after assigning an empty set to A ,wecan obtain an ordered list of sentences by taking both importance and redundancy into account. With regard to Sim s , we adopt a simple cosine similarity measure between the sentence vectors. Each element of a sentence vector represents the weight of a noun in a document. 5.4 Generation of Summary
Since we focused on the extraction of important sentences in this study, the summary generation phase is extraction-based and fairly simple. The summary generation process selects the top most important sentences until the total length/number of selected sentences reaches the given summary length.
The selected sentences are initially arranged in an order determined by the cluster structure and then in the chronological order of input documents as follows. First, the input documents are nonhierarchically clustered using the single-link clustering method. The obtained clusters and the documents in each cluster are then arranged in the chronological order Here, the date of a cluster is defined as the date of the oldest document in it. 6. EXPERIMENTAL EVALUATION
We evaluate the proposed method with NTCIR4 TSC3 formal run. NTCIR TSC is a series of evaluation workshops on text summarization organized by the
National Institute of Informatics, Japan [Fukusima and Okumura 2001]. The most recent workshop, i.e., NTCIR4 TSC3, was held in June 2004 [Hirao et al. 2004]. In this paper, we evaluate the proposed method from the following view-points: (1) the sentence precision and coverage of the extracts generated by the system in comparison to model extracts and (2) the answer coverage of the summaries generated by the system in comparison to the model abstracts. The model extracts and abstracts were prepared by the task organizers
For the formal run, the evaluation set includes 30 topics. Each topic consists of a list of document IDs of Japanese newspaper articles to be summarized, a title, two types of length of summaries ( X  X hort X  and  X  X ong X ), and two sets of questions for the Short and Long summaries that represent the items to be included in the summary. The compression ratio for the Short and Long summaries is approximately 5 and 10%, respectively. The questions for a topic are selected by the person who prepared the model abstract for the topic (see
Section 6.1). The question set on a topic for the Short summary is a proper subset of that for Long summary. The average number of questions per topic for the Short and Long summaries is 7.6 and 11.9, respectively. In an example shown in Figure 3, several questions are of factoid type, while the remaining are of other types such as definition, reason, and so on, which cannot be handled properly by our QA engine. The basic performance of the QA engine for the question sets are MRR = 0.594 for the Short summary and MRR the Long summary. 3
The parameters of the system are manually adjusted to fit the set of five example topics that were supplied by the task organizers prior to the formal run. While we do not apply the smoothing by the Hanning window function to generate Short summaries, the width of the Hanning window is four for Long summaries. The mixing factor  X  is set to 0.8 (for Short) or 0.7 (for Long). The parameter  X  for MMI-MS is set to 0 . 4 + 0 . 5  X  (1  X  Sim average similarity among sentences in a topic. 6.1 Performance of Important Sentence Extraction
In multi-document summarization, different sentences having the same con-tent may exist. Moreover, the content of one sentence may correspond with that of more than one sentence in another document. Thus, the i th sentence of the model extract ME is expressed as a set MS i , each element of which is a set A of sentence IDs of the original documents. On the other hand, an extract pro-duced by a system is a set SS of sentence IDs. The task organizers, therefore, de-fined the coverage c ( SS , MS i ) of the extract SS for the i th sentence of the model extract MS i , given by Eq. (8). The sentence coverage C ( SS , ME ) and the sen-tence precision P ( SS , ME ) of the extract SS with respect to the entire model extract ME were also defined, as given by Eqs. (9) and (10)[Hirao et al. 2004]: where the function memp ( e , S ) is the membership function.

In this experiment, we use the model extracts that were prepared by the task organizers in accordance with the corresponding model abstracts. Each model abstract has been prepared by one of the five specialists, who are ex-newspaper journalists.

Figure 6(a) and (b) show the relationship between the average sentence cov-erage and the average sentence precision of the extracts generated by the pro-posed system. In this figure, the label  X  IGR + MMR + QA  X  represents the proposed three baselines prepared by us. With the exception of suppressing the func-tion of the QA engine, IGR + MMR is identical to IGR + MMR + MMR + QB is a query-biased method in which the overlap of words between sentences and questions are taken into account and the sentence importance
Imp n QB ( S i ) is used instead of Eq. (2). Imp n QB ( S the sentence importance in Eq. (11) with the T score. IGR identical to IGR + MMR + QB with the exception that it adds an extra weight to sentences that include NEs ( ENAMEX , TIMEX , and NUMEX ), as given by Eq. (12). In the following equations, Q is a set of questions and NE ( S sentence S i .

The major difference between the proposed system and baselines is the use of the QA engine, particularly in cases where information about answers is required.

On the other hand, the label  X  X ead X  represents the lead method that selects the beginning of each document. It is a baseline method provided by the task organizers. Other plots correspond to other participants X  systems. It should be noted that the participants decided whether to use the questions in topic information for generating summaries. Thus, some systems might exist that do not use the questions. It should also be noted that no manually created extracts were supplied by the task organizers other than the model extracts, unlike the evaluation in the following section.

We also conducted the Wilcoxon matched pairs signed rank sum test to check the statistically significant differences between the proposed sys-tem IGR + MMR + QA with the three baselines in terms of coverage.
Short summaries, the proposed system is statistically significantly superior to the baseline IGR + MMR ( p = 0 . 000058 &lt; 0 . 01), but not superior to the the proposed system is superior to all the baselines for the Long summaries ( p = 0 . 00080 ( &lt; 0.01) for IGR + MMR , p = 0 . 046 ( and p = 0 . 011 ( &lt; 0.05) for IGR + MMR + QB + NE ).

The task organizers also provided content evaluation of systems X  summaries based on subjective assessment by five assessors, i.e., by the five creators of the model abstracts. They examined the summaries of the systems sentence by sen-tence and then evaluated them according to the degree of matching with their model abstracts. Each sentence in the model abstracts has an importance value assigned by the creator, and the value is taken into account during evaluation.
The averages are shown in Figure 7, where labels such as  X  X 0304 X  represent the system IDs of the other participants X  systems. We also conducted the Wilcoxon matched pairs signed rank sum test to check the statistically significant differ-ences of the proposed system with respect to other participants X  systems and the lead method. As for the Short summaries, the proposed system is statisti-cally significantly superior to some systems (F0303 at p &lt; F0310, and F0311 at p &lt; 0 . 01), but is not superior to the other systems (F0301,
F0304, F0306, and F0308). On the other hand, for the Long summaries, the proposed system is superior to all the other participants X  systems at p (F0304) or p &lt; 0 . 01 (others). 6.2 Performance in Terms of Coverage of Answers
Figure 8(a) and (b) show the average answer coverage, which represents the number of answers in model abstracts that are included in the summaries generated by a system. Task organizers provided two types of index for answer coverage (1) exact match : the average ratio of the exact answer strings present in the summaries and the model abstracts and (2) edit distance : the average score that is defined based on the edit distance EditD () between an answer string Ans i and a sentence string S , given as follows: where the function Len () returns the string length. The label  X  X uman X  in the figures corresponds to a set of summaries, each of which is created by one of the five specialists who did not prepare the corresponding model abstract. 6.3 Mixing Factor of Two Types of Sentence Importance
We conducted the same experiments as in the preceding sections except that the value of the parameter  X  varied from 0.0 to 1.0. Figures 9 and 10 show the performance of sentence extraction and the average answer coverage for questions, respectively. 7. DISCUSSION 7.1 Performance of Important Sentence Extraction
As shown in Figure 6a, when the summary length is Short, the performance of the proposed method ( IGR + MMR + QA ) is almost the same as the baselines X  method. Hence, for Short summaries, it might be concluded that words in questions convey enough information for appropriate summarization. On the other hand, in the case of Long summaries, the proposed method is su-perior to all the baselines and other participants X  systems, as shown in
Figure 6(b)). However, it should be noted that some other systems do not use the information of questions. In comparison with IGR + MMR the proposed method is significant, and the sentence weight assessed by the QA engine is effective. The content evaluation based on subjective assessment also shows that the proposed method is better than the baselines and the other sys-tems, as shown in Figure 7. The differences are statistically significant for Long summaries.

It should also be noted that the baseline IGR + MMR + QB shows a comparatively better performance than other baselines and other participants X  systems. The reason for this is believed to be that a relatively higher number of questions are available in the evaluation set. On the other hand, IGR + + MMR + QB + NE . Since there are many questions under one topic, we did not select NEs on the basis of the question types. Instead, we utilize all the NEs for sentence weighting, which may not be effective.
 In the Long summary, the average precision of IGR + MMR + (0.680) and the average coverage is relatively low (0.391). The reason is that the proposed method tends to extract sentences having content that is similar to the sentences already selected. Figure 11 shows the average number of al-most identical sentences in a summary. It is a part of the subjective assessment for summaries. According to this figure, our proposed method was unable to eliminate, on some occasions, all the obviously redundant sentences. One of the reasons for this is believed to be the manner in which the sentence simi-larity was measured. In the MMI-MS approach, we adopt the cosine similarity measure between the sentence vectors, each element of which is the weight of a noun. Since the weight of a noun may vary across different documents, the similarity between two sentences in different documents may be less than 1 even if they are identical. Therefore, it is necessary to refine the calculation method of sentence similarity. 7.2 Performance in Terms of Coverage of Answers
According to Figure 8, the performance of the proposed method is better than that of the baselines for both Short and Long summaries. It should be noted that the summary set labeled as  X  X UMAN X  has been prepared by five specialists without referring to the questions in the topic information. 7.3 Effect of Mixing Two Types of Sentence Importance
Figures 9 and 10 show that the sentence importance derived from questions, such as the questions themselves and their answers, is superior to sentence importance based on IGR. However, we could detect a peak in each of the curves at  X  = 0 . 6  X  0 . 8. We can thus conclude that the mixing of two types of sentence importance values are effective, to a certain degree, An interesting point is that none of the peaks are located at the point when answer coverage curves. One of the reasons for this could be that the QA system employed by us is not sufficiently accurate. Therefore, the sentence importance based on the probabilistic distribution of words could help compensate for the inaccuracy. 8. CONCLUSION
In order to realize multi-document summarization focused by multiple ques-tions, we introduced the calculation of sentence importance using a QA system. We also proposed an integration of this calculation with a generic multi-document summarization system. The evaluation results showed that the per-formance of the proposed method is better than that of not only several base-lines, but also other participants X  systems at the NTCIR4 TSC3 Formal Run.
As stated earlier, it is observed that the proposed method is not able to elim-inate, on some occasions, all obviously redundant sentences. Therefore, our future work would aim to refine the calculation of sentence similarity. Another issue to be resolved is the computational cost of the QA system. In the current implementation, the system evenly calculates the exact scores for all the words in the documents to be summarized. Approximately tens of seconds are con-sumed per question by average PC hardware. However, it is notable that the
QA engine has a controlled search feature for finding answers and the calcu-lation can, therefore, be terminated after the n best answers are obtained. We plan to utilize this feature and approximate the scores in order to reduce the cost. The proposed method also needs to be improved to allow a more natural situation where a user interactively inputs questions one at a time while in-teracting with the system, and depending on the input, the system gradually outputs a part of summary including the answers by taking into account its relationship with the previously displayed parts of the summary.

We are grateful to the task organizers of NTCIR4 TSC3 and those who managed the NTCIR workshops. We would like to especially thank Mainichi Shimbun and Yomiuri Shimbun for permitting us to use the documents for research. We would also like to thank the anonymous reviewers for their helpful comments.
