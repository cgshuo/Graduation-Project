 Recommendation techniques have been well developed in the past decades. Most of them build models only based on user item rating matrix. However, in real world, there is plenty of auxiliary information available in recommendation systems. We can utilize these information as additional features to improve recommendation performance. We refer to recom-mendation with auxiliary information as context-aware rec-ommendation. Context-aware Factorization Machines (FM) is one of the most successful context-aware recommendation models. FM models pairwise interactions between all fea-tures, in such way, a certain feature latent vector is shared to compute the factorized parameters it involved. In prac-tice, there are tens of context features and not all the pair-wise feature interactions are useful. Thus, one important challenge for context-aware recommendation is how to effec-tively select  X  X ood X  interaction features. In this paper, we focus on solving this problem and propose a greedy interac-tion feature selection algorithm based on gradient boosting. Then we propose a novel Gradient Boosting Factorization Machine (GBFM) model to incorporate feature selection al-gorithm with Factorization Machines into a unified frame-work. The experimental results on both synthetic and real datasets demonstrate the efficiency and effectiveness of our algorithm compared to other state-of-the-art methods. H.1.1 [ Models and Principles ]: Systems and Information Theory; J.4 [ Computer Application ]: Social and Behav-ior Sciences Gradient Boosting, Factorization Machines, Recommender Systems, Collaborative filtering  X 
The work was performed when the first author was an in-tern at Big Data Lab of Baidu Inc..

Recommendation systems have been well studied in the past decades. Most of them mainly focus on context un-aware methods, e.g. only consider user and item interac-tions. Among them, matrix factorization methods [16, 27] have become popular due to their good performance and ef-ficiency in dealing with larger dataset. These methods focus on approximating user item rating matrix using low rank representations, and use them to make further predictions. However, in real world scenarios, plenty of auxiliary informa-tion is available and is proved to be useful especially in large industry datasets. For example, in the Weibo celebrity rec-ommendation scenario, both the user X  X  and celebrity X  X  meta data (such as age and gender), the popularity of a celebrity, the recent following behavior of the user, etc. can help make better recommendations. Recent work in KDDCup 2012 1 [23, 7] show the effectiveness of utilizing auxiliary informa-tion for recommendation.

In regarding of utilizing auxiliary information, several meth-ods have been studied to incorporate meta data (e.g., user profile, movie genre,etc.) [18, 30] and more general auxiliary information such as session information [32, 2, 14, 22, 25]. In these methods, auxiliary information is encoded as fea-tures and together with user and item they are mapped from feature space into a latent space. The Factorization Ma-chine (FM) model [25] is currently a very strong and flexible method which easily incorporates any categorical features. However, it is common that there are tens of context fea-tures in real data. In FM, all features are assumed to be interacted with all other features. For example, assuming there are n features, then for a certain feature i , the latent vector v i is shared with n  X  1 interaction features. It is not always the case that all the feature interactions are useful. Useless feature interactions will introduce noise in learning latent feature vector v i . Thus it is challenging to automati-cally select useful interaction features to reduce noise.
The most recent work in [6] introduced an automatic fea-ture construction method in matrix factorization using gra-dient boosting. In their method, feature functions are con-structed using greedy gradient boosting method and then incorporated into the matrix factorization framework. Dif-ferent from their method, in our paper, we focus on select-ing useful interaction features under factorization machines framework. At each step, we propose a greedy gradient boosting method to efficiently select interaction features, and then we additively optimize the selected latent vector by http://www.kddcup2012.org/ optimizing the residual loss. Another difference is that our method is more efficient in selecting categorical feature in-teractions compared to the binary decision tree construction algorithm in [6]. The contribution of our paper is summa-rized as follows: The rest of the paper is organized as follows. Section 2 introduces the related work. Section 3 gives the details of our methods. Section 4 presents the experiment results and discussions. The paper is concluded in Section 5.
The work present in this paper is closely related to tra-ditional matrix factorization models, context-aware recom-mendation and gradient boosting. In the following, we briefly review the related work.
Matrix factorization techniques [29, 4, 16, 27, 2] have been shown to be particularly effective in recommender sys-tem as well as the well-known Netflix prize competitions 2 They usually outperform traditional item-based methods [28]. The main idea behind matrix factorization is to learn two low rank latent matrices U  X  R k  X  m and V  X  R k  X  n to approximate the observed user item rating matrix R  X  R where m,n are the number of users and items respectively and k is the dimension of low rank matrices.

We assume that the conditional distribution over the ob-served rating is: where N ( x |  X , X  2 ) is the probability density function of the Gaussian distribution with mean  X  and variance  X  2 , and I is the indicator function that is equal to 1 if user u i rated item v j and equal to 0 otherwise. The zero-mean spheri-cal Gaussian priors are also placed on user and item latent feature vectors: p ( U |  X  2 U ) = http://www.netflixprize.com through a Bayesian inference, we have the following objec-tive function: min The above equation can be easily solved either by stochastic gradient descent (SGD) or alternating least squares (ALS).
Most traditional matrix factorization method mainly an-alyze the user item rating matrix thus they are context un-aware. Contextual information has proved to be useful in recommender systems and already have been widely studied. In order to incorporate auxiliary information, several vari-ants of matrix factorization have been proposed. In [17, 33], temporal features are explored to help capture user prefer-ence more precisely. Socical [20, 13] or location information [35, 8, 9] also have been explored. The meta-data like user age or item genre are incorporated into the matrix factoriza-tion model [30, 18]. In addition to the meta-data which is attached to user or item itself, the context features include information attached to the whole recommendation event such as user X  X  mood, day of the week, etc. as well.
The most basic approach for context aware recommen-dation is to conduct pre-filtering or post-filtering where a standard context-unaware method is applied [21, 1]. Bal-trunas et al. [3] proposed a simple model that introduced a basis term for each context feature or item context in-teraction feature. Context information is encoded in these additional parameters. More generally, Karatzoglou et al. [14] proposed a multiverse recommendation model by mod-eling the data as a user-item-context N-dimension tensor. Then Tucker decomposition [31] is applied to factorize the tensor. However, the computation complexity of this model is  X ( k m ) where k is the dimension of low rank vectors and m is the number of features, which is intolerable in practice. Rendle et al. [25] proposed to apply factorization machines (FM) [22] to overcome the problem in Multiverse recommen-dation. They transform the recommendation data into a prediction problem and FM models all interactions between pairs of features with the target. They further proposed to deal with relational data in [24] through block structure within a feature which have repeating patterns.

The idea of tree based random partition has been explored in [36, 19]. Zhong et al. [36] assumed that contextual infor-mation is reflected by user and item latent vectors. In their method, the random tree partition is conducted to split the user item matrix by grouping users and items with simi-lar contexts. Then matrix factorization is applied on the sub-matrices. Liu et al. [19] employ the similar idea but explicitly use context information to split the user item ma-trix into sub matrices according to specific context values. The prediction is the average values of each prediction from T generate decision trees. However, they fail to discuss how to select useful features especially when there are tens of features.
Gradient Boosting have been successfully used in classifi-cations [12] and learning to rank [34, 5]. In each step, gra-dient boosting greedily conducts coordinate descent in the function space to select a feature function. Chen et al. [6] proposed to use gradient boosting method to automatically construct feature function for each user(item) latent vector at each step. Time dependent feature function and demo-graphic feature function construction is discussed in their work. Different from their work, in our paper, we employ gradient boosting algorithm to find the best feature inter-action at each step, then similar to gradient boosting, we additively optimize the latent feature vectors. Besides, in real world, there are many categorical features with large size, the binary tree splitting algorithm in their work is not efficient in deal with such features.
In this section, we first describe the context-aware recom-mendation problem we study in this paper and define the no-tations, and then we briefly review context-aware FMs which is closely related to our work. Then we present the details of our proposed Gradient Boosting Factorization Machines Model followed by complexity analysis and discussions.
Figure 1(a) shows an example of context-aware online rat-ing system. Traditional recommendation systems consider only the user rating matrix 1(b) to make recommendations. However, rich context information is available and easy to obtain in real world. For example, in Fig. 1(a) we can easily get the rating time and the rating comments. These infor-mation provide a new information dimension for recommen-dation. We can encode the context information as well as user and item as either real value or categorical features, and the rating as the target value. In such way, we can trans-form the context aware recommendation into a prediction problem as shown in Fig.1(c). The figure shows an example about users U watch movies I in mood M : Then the first tuple in Fig.1(c) states that user u 1 gave movie i 4 stars in a Happy mood.

Next we give the formal definition of context-aware rec-ommendation problem. We denote the user set as U and the item set as V . Assume there are another m  X  2 context fea-tures, we further denote the context features as C 3 ,..., C In fact, user and item can be regarded as the first and second  X  X ontext X  feature. For simplicity, we denote user set as C and item set as C 2 . In our paper, we consider only categori-cal features for simplicity, since in practice most features are categorical [23] and for real value features can also be seg-mented into categorical features. The training data can be encoded as feature vectors as shown in Fig 1(c) by transform-ing categorical features to indicator variables. We denote n as the number of different feature values for context feature C . Each context feature set is C i = { c i, 0 ,...,c i,n i ther denote the length of feature vector as d which equals to n + ... + n m . Training data is denoted as S = P N i =1 ( x where N is the total training instance number, x i  X  y  X  R are feature vector and target value for instance i re-spectively. Our problem is to estimate the following rating function that minimize the following objective function: where l is a differentiable convex loss function that measures the difference between the prediction rating  X  y i and the tar-get rating y i ,  X  is parameter set to be estimated. The sec-ond term  X  measures the complexity of the model to avoid overfitting.
Factorization Machines [22] is a generic model class that subsumes many well-known recommendation methods in-cluding SVD++[16], matrix factorization [29] and PITF[26]. Rendle et al. [25] proposed to apply FM to solve context-aware recommendation problem and it has proven to be ef-fective in KDDCup 2012 [23] as well.
 In [25], factorization machines is restricted to be 2-way FMs. In such setting, the FM models all interactions be-tween pairs of variables with the target including nested ones, by using factorized interaction parameters. The rating prediction function is: From the perspective of classification problem, w 0 is the global bias, w i is the weight for feature x i and  X  w i,j weight for feature x i x j . We refer feature x i x j as interaction feature which indicates the instance have both feature value x i and x j . For example, the first tuple in Fig. 1(c) one interaction feature is u 1 v 1 since we have user u 1 and item v in the tuple.

The factorized parameters  X  w i,j is defined as:
The model parameters  X  that need to be estimated are: Note that the latent matrix V can be regarded as the con-catenation of all m latent feature matrices V i  X  R n i  X  k 1 ,...,m . The final objective function is where  X  (  X  ) is the regularization parameters. In practice, l can be logit loss for binary classification problems: or least square loss for regression
In practice, it is not surprising that we can have tens of context features 3 and not all interaction features are useful for rating prediction. Note that FM models all pairwise interactions between context features, while the weight of interaction feature is defined in Eq. 8. The latent vector v shared by all other feature vector v j in order to estimate the feature weight  X  w i,j . If the feature interaction feature x is not useful i.e. in practice, the estimate function  X  y does not have the item  X  w i,j x i x j , in such case, the estimation for parameter v j and v j will be affected. In order to effectively select  X  X ood X  interaction features, we propose our Gradient Boosting Factorization Machines model.
In this section, we present our proposed GBFM training algorithm. To relieve the problem in FM discussed in pre-vious section, we borrow the idea of boosting methods [10] to select one interaction feature at each step and additively optimize the target function. The rating prediction function in our model is defined as: where s is the iteration step of the learning algorithm. At step s , we greedily select two interaction features C C q where I is the indicator function, the value is 1 if the condition holds otherwise 0. The feature selection algorithm we will introduce later. V p  X  R n p  X  k and V q  X  R n q  X  k the low rank matrices for feature C p and C q , k is the low rank dimension. After interaction features C p and C selected, we estimate the parameters V p and V q at step s . For example, at step s we select the interaction between user and item, we need to learn the low rank latent matrices U and V . The objective function for estimate V p , V q is: Assume we totally have S steps, we denote C sp and C sq as in-teraction feature selected at step s , then the final prediction function is:  X  y S ( x ) =  X  y 0 ( x ) +
In KDDCup 2012 Task 1 we can easily extract around 20 features, there are more features in real industry world. where  X  y 0 ( x ) is the initialized prediction function. The details of the algorithm are shown in Algorithm 1. Algorithm 1 Gradient Boosting Factorization Machines Model 1: Input : Training Data S = { x i ,y i } N i =1 2: Output :  X  y S ( x ) =  X  y 0 ( x ) + P S s =1  X  v si , v 3: Initialize rating prediction function as  X  y 0 ( x ) 4: for s = 1  X  S do 5: Select interaction feature C p and C q from Greedy Fea-6: Estimate latent feature matrices V p and V q 7: Update  X  y s ( x ) :=  X  y s  X  1 ( x ) + P i  X  X  8: end for
In this section, we show how to effectively select  X  X ood X  interaction features at each step which is the core part of our model. From the view of gradient boosting machine, at each step s , we would like to search a function f in the function space F that minimize the objective function: tion f is set to factorized feature interactions like in FM. However, it is impossible to search all feature interactions to find the best (i.e. decrease the objective function most) due to high computation complexity. In order to find the desirable interaction features, we propose a greedy layer-wised algorithm to find the n -way interaction features. Our idea is as follows: there are n layers in total, at each layer, we greedily select the feature C i that makes the objective function decrease fastest. At the end, we will get the n -way interaction feature. we heuristically assume that the function f has the following form: context feature selected. The function q maps latent feature vector x to real value domain. There are d elements in feature set, for each element we assign a weight w tj to it. The function q C i ( t ) ( x ) is defined as: where I is the indicator function. Although the q looks very complex, in fact, in each instance there is only one non-zero element corresponding to feature C i ( t ) , the function value just takes the weight corresponding to the non-zero element. Take the instances in Fig. 1(c) for example again, suppose we select feature C 1 at layer t , then the q function for first tuple is w t 1 .

Searching function f to optimize the objective function in Eq. 16 can be hard for a general convex loss function l . The most common way is to approximate it by least-square minimization [11]. We denote the negative first derivative and the second derivative at instance i as g i and h i : The first part of Eq. 16 can be approximate as: L = The Eq. 21 is equivalent to: Replace the f s function with the heuristic f defined in Eq. 18, we get the objective function for selecting n -way interaction features. Even using heuristic functions, finding the best interaction feature is still impossible. Instead, we learn the function f l layer by layer. At layer t , we assume that the been learned. Suppose at layer t we select i ( t )-th feature, then we have: Our problem is finalized to find the i ( t )-th feature which: where here we use L2-regularization to control the model complexity. To obtain the feature i ( t ) minimize Eq. 24, we calculate the function q for all features. Without loss of generality, we assume the selected feature at layer t is C The problem is actully transformed to estimating the weight in C i ( t ) , we denote its corresponding weight as w solution for w ij is: w ij = arg min We denote z i = g i /h i and let Then the solution for w ij is: Note that although we need to calculate q function for all features, we can compute a,b for all features at the same time by scanning the training data just once. After we get the q function for all features, it is easy to select the best feature which satisfies Eq. 24. We repeat this process at each layer, at the end we can obtain the heuristic n -way interaction feature. Like FM, in our method, we consider 2-way interaction feature only. The details of the algorithm are shown in Algorithm 2.
 Algorithm 2 Greedy Feature Selection Algorithm 3: for l = 1  X  n do 4: A =  X  // A is the set of context features already selected 5: Maintain two vectors a and b for all categorical values in 8: for j = 1  X  d do 11: end if 12: end for 13: end for 14: Compute weight for all categorical features in C  X  X  ac-17: end for The computation complexity for Greedy Feature Selection Algorithm is O ( n  X  N ), where N is the training data size, n is the number of layers. At each layer, the q function can be computed though scanning the training dataset once as described in Algorithm 2. Then best feature selection according to Eq. 24 can also be carried out by the training dataset once. Usually, n N , in 2-way FM, n = 2. So the computation cost for Algorithm 2 is O ( N ).

In Algorithm 1, the estimation for V p and V q is usually carried out by stochastic gradient descent (SGD). The com-plexity for this part is O ( kN ), where k is number of itera-tions. In total, the complexity for GBFM is O ( SN + kSN ), S is number of boosting steps as stated in the algorithm, the computation complexity is still linear to the number of training dataset.

In addition, GBFM can be speedup by multi-threading and parallelization. The computation of first and second derivative can be decoupled thus can be easily computed though multi-threading and distributed to a cluster of com-puters. The gradient of Eq. 14 also can be decoupled and parallelization is possible for Algorithm 1.
We discuss the insights of our heuristic function f in Eq. 17 which is the key part of Algorithm 2 as well as the relation-ship between the proposed GBFM and other state-of-the-art methods. At last, we discuss some variants of our model.
Insights of heuristic function f : The main idea of our algorithm is that at each layer we greedily select a context feature C i according to Eq. 24 and we compute the corre-sponding weight vector, e.g. q C i . We can regard it as the low rank latent feature matrix for feature C i like in FM with latent dimension k = 1. Then the heuristic function f is an intance of CANDECOMP/PARAFAC (CP) decomposition [15] with k = 1. We greedily use this f function to choose the interaction feature. In practice, for large dataset in in-dustry world, we can additively use this function f as the  X  X eak learner X  instead of  X  V p , V q  X  , to quickly find useful interaction features since the computation cost is relatively low.
 Relation to Factorization Machines : Factorization Machines is a strong baseline method for context-aware rec-ommendation [25]. The main difference between our model and FM is that FM models all interactions between context features while our method only consider part of them. For example, in Fig. 1(c), we have 3 context features, the rating prediction function of FM is: While in our GBFM, we may only consider the (user,item) and (user,mood) interaction pair. If the interaction feature (item,mood) is actually not useful, then the term  X  v i , v which is the weight for feature x i x c 3 will introduce noise for the prediction function. Another difference is that in our algorithm we additively learn the latent feature matrices which are not shared to compute other factorization weights. For example, in first step, we select (user,item) pair, then the second step we select (user,mood) pair, the latent feature matrix V u is not the same. It may lose the advantage of generalization compared to FM, we can regard our GBFM as a feature selection algorithm and only model the interaction on selected features.

Relation to GBMF : Gradient Boosting Matrix Fac-torization [6] is the state-of-the-art model which is a gen-eral functional matrix factorization using gradient boosting. GBMF is under the framework of matrix factorization [27]. They assume that the user/item latent low rank matrix is functional, each time a function f is added to latent di-mension U k . While our model is under the framework of factorization machines, we use gradient boosting to greed-ily select  X  X ood X  interaction features. Another difference is the construction method of high-order categorical features. In our algorithm, we can efficiently find the  X  X est X  features according to Algorithm 2, while the binary splitting tree al-gorithm may fail for categorical features since the cost for finding the best binary split is exponential.

Variants of our GBFM : There are several variants of our proposed GBFM. In our paper, we only use the 2-way interaction feature like in FM. It can be easily extended to n -way FM by selecting n -way interaction feature. Since our model is an additive model, we can first consider linear fea-tures, i.e. 1-way feature, then 2-way interaction and more high order features. Another variant is that we can fully op-timize the selected interaction features instead of additively optimize interaction features one by one, we refer this vari-ant as GBFM-Opt. The difference between GBFM-Opt and FM is that GBFM-Opt only consider some  X  X ood X  selected 2-way interaction features.
In this section, we empirically investigate whether our pro-posed GBFM can achieve better performance compared to other state-of-the-art methods with large number of context features. Furthermore we would like to examine whether the interaction features selected by our algorithm is more effective compared to pairwise interactions in FM.
We conduct our experiments on two dataset: a synthetic dataset and a real world dataset, i.e., the Tencent Microblog Synthetic data : Since there are few public datasets that have many context features. We construct a synthetic dataset for comparison. The data generation process is as follows: assume we have m context features, each context feature C have n i values, we generate the latent context features from zero-mean spherical Gaussian as follows: where j = 1 ,...,n i , 0 K is a K -dimension vector with all elements set to 0, and I K is the K  X  K identity matrix. We also generate the weight vector all categorical feature values w  X  N ( 0 d +1 , X  2 I d +1 ), where d = P m i =1 n i , we incorporate the global bias into the weight vector. Then we select sev-eral 2-way interaction features. We denote the interaction feature set as F . Then the rating is obtained by rescale the sigmoid value to 1 to D by:  X  y ( x ) =  X  y ( x ) = d g ( X  y )  X  D e , where D is the rating scale, g ( x ) = 1 / (1 + exp(  X  x )). In our experiment, we set number of context features m = 10, latent dimension K = 5, rating scale D = 5, feature value size n i = 1000.
 Tencent microblog 2.3 M 6095 73 M
Tencent microblog dataset : Tencent microblog is one of the largest social media services in China like Sina Weibo and Twitter. The dataset is designed for KDDCup 2012 competition and it contains the celebrity recommendation records of about 2.3 million users over a time period of about two months. In this dataset, the celebrities are re-garded as items for recommendation. The system recom-mends a celebrity to a user at a certain time and the user X  X  response is either  X  X ccept X  or  X  X eject X . The dataset contains rich context information such as user X  X  age, gender, item X  X  category, time information, etc.. We can also extract the session information such as the number of recommendation records before current recommendation. The dataset splits into training and testing data by time. The test data fur-thermore splits into public and private set for independent evaluations. The dataset is extremely sparse with only about two positive records (e.g. accept the recommendation) for each user. Besides, nearly 70% of users in the test dataset are never occurred in the training data.

Table 1 shows the statistic for both our synthetic data and real data. http://kddcup2012.org/c/kddcup2012-track1/data
We randomly remove 20% of dataset as testing data, and the remaining 80% data as the training for the synthetic data. We repeat the experiment 5 times and report the av-erage results. For Tencent Microblog data, the dataset is already splitted into training and testing set. We further use 1 / 5 training data as validation data to tune parameters and we conduct the evaluation on pubic test dataset. We ex-tract 18 features from the data, including user, item, tweets number, follower/followee number, tweet time etc. We treat all of the features as categorical features.

For synthetic dataset, we use two metrics, the Mean Abso-lute Error (MAE) and Root Mean Square Error (RMSE), to measure the prediction quality of different methods. MAE and RMSE are defined as follows: where N is the number of training instance.

For Tencent microblog data, MAP@ k is used as the met-ric: where N is the number of users and ap @ k is the average precision at k for the user: ap @ k = where P ( k ) is the precision at cut-off k in the item list.
In our experiments, we compare the following methods: For GBFM, we use 1-way feature linear model as the ini-tialized prediction function. Grid search is applied to find regularization parameter  X  , and we set it to 0 . 1 for synthetic data and 0 . 8 for Tencent microblog data. The latent dimen-sion k is set to 5 and 10 for synthetic data and Tencent microblog data respectively. We use square loss to train synthetic data and logit loss for Tencent microblog data. The detailed comparison results are shown in Table 2 and Table 3.
 From the table, we can observe that: Table 2: Results on Synthetic data in RMSE and MAE Method RMSE MAE PMF 1.9881 1.7650 FM 1.9216 1.6981 GBFM 1.8959 1.6354 GBFM-Opt 1.8611 1.5762 Table 3: Results on Tencent Microblog data in MAP Method MAP@1 MAP@3 MAP@5 PMF 22.88% 34.50% 37.95% FM 24.36% 36.77% 40.32% GBFM 24.62% 37.17% 40.90%
GBFM-Opt 24.66% 37.23% 40.98% In this paper, we have proposed a novel model called GBFM which incorporates feature interaction selection algo-rithm with Factorization Machines into a unified framework to solve context-aware recommendation problems. Exper-iments on both synthetic and real datasets show that our model can effectively select  X  X ood X  interaction features and achieve better performance compared to other state-of-the-art methods.

There are several interesting directions worthy of consid-ering in the further study: 1) we would like to explore how to find high order features, 2) we are interested to extend our GBFM with better high order feature selection algorithm. 3) it is also interesting to explore how to effectively deal with other features apart from categorical features.
The work described in this paper was fully supported by the National Grand Fundamental Research 973 Program of China (No. 2014CB340405 and No. 2014CB340401), the Research Grants Council of the Hong Kong Special Ad-ministrative Region, China (Project No. CUHK 413212 and CUHK 415113), and Microsoft Research Asia Regional Seed Fund in Big Data Research (Grant No. FY13-RES-SPONSOR-036).
