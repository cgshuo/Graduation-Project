 In this tutorial we will present, review, and compare the most popular evaluation metrics for some of the most salient information related tasks, covering: (i) Information Retrieval, (ii) Clustering, and (iii) Filtering. The tutorial will make a special emphasis on the specification of constraints for suit-able metrics in each of the three tasks, and on the system-atic comparison of metrics according to such constraints. The last part of the tutorial will investigate the challenge of combining and weighting metrics.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval Evaluation, Metrics Figure 1 shows a tagcloud of most Information Retrieval (IR) effectiveness metrics: we can count around one hundred IR metrics, let alone user-oriented ones or metrics for related tasks such as filtering, clustering, recommendation, summa-rization, etc. Evaluation metrics are not merely a tool to assess and compare systems. In the space of solutions to a problem, they work like a GPS that tells researchers where is the final destination, providing the operational definition of what systems should do. IR researchers can choose among a set of over one hundred metrics, all pointing at different places in the map, and in general there is no clear procedure to choose the most adequate metric in a specific scenario. And a wrong choice may imply falling off a cliff.

We believe that a better understanding of metrics, and of their conceptual, foundational, and formal properties, would help to avoid wasting time in tuning retrieval systems ac-cording to effectiveness metrics inadequate to specific pur-
