 Frequent itemset mining has been the subject of a lot of work in data mining research ever since association rules were introduced. In this paper we address a problem with frequent itemsets: that they only count rows where all their attributes are present, and do not allow for any noise. We show that generalizing the concept of frequency while pre-serving the performance of mining algorithms is nontrivial, and introduce a generalization of frequent itemsets, dense itemsets. Dense itemsets do not require all attributes to be present at the same time; instead, the itemset needs to define a sufficiently large submatrix that exceeds a given density threshold of attributes present.

We consider the problem of computing all dense item-sets in a database. We give a levelwise algorithm for this problem, and also study the top-k variations, i.e., finding the k densest sets with a given support, or the k best-supported sets with a given density. These algorithms select the other parameter automatically, which simplifies mining dense itemsets in an explorative way. We show that the concept captures natural facets of data sets, and give ex-tensive empirical results on the performance of the algo-rithms. Combining the concept of dense itemsets with set cover ideas, we also show that dense itemsets can be used to obtain succinct descriptions of large datasets. We also discuss some variations of dense itemsets.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database applications X  data mining General Terms: Algorithms, Experimentation Keywords: Frequent itemsets, error tolerance
The concept of frequent patterns has been much explored in data mining research for at least ten years, since the incep-tion of association rule mining in [1, 2]. The basic premise has remained the same: to seek association rules like { onions , carrots } =  X  green peas (  X  = . 03 ,c = . 75) Copyright 2004 ACM 1-58113-888-1/04/0008 ... $ 5.00. meaning that 75% of customers who buy the two vegetables on the left-hand side will also buy the one on the right-hand side, and that 3% of the database rows support this rule.
The usual way to find these rules is to seek first frequent itemsets , i.e., sets of items that often occur together. The concept of frequent itemsets is a useful one, but for under-standing the structure of data it is not perfect. For an item-set to be found frequent, all of its items must co-occur suf-ficiently often X  X n other words, the frequency of an itemset is the result of a full conjunctive database query. Such full conjunctions are rare in real-world data, and connections be-tween items may exist that are manifested by co-occurrence of not the full set of items but of varying subsets.
Generalizing the concept of frequent itemsets turns out to be far from trivial. The most obvious generalization would be to replace the requirement of perfect co-occurrence by the less stringent one of partial co-occurrence: require that an itemset have at least a proportion 1  X   X  of items present in at least a proportion f of database rows, and say that itemsets meeting this more tolerant requirement have  X  -approximate frequency f . This definition leads to two problems: first, any frequent itemset will generate many approximately fre-quent itemsets that do not convey any meaningful informa-tion; second, the usual kind of itemset mining algorithms like Apriori are not easily generalized to the new task.
The first problem is illustrated in Table 1a. There, the itemset ABCDE is obviously a frequent one 1 , and the data has no further structure. However, a multitude of approx-imately frequent sets exist with e.g.  X  =0 . 5: ABCF GH , ABCDF GH , ABCDEF GH etc., and beyond the fact that ABCDE is frequent, these sets give us no new information.
The second problem can be seen in Table 1b. Now the itemset ABCD has 0 . 5-approximate frequency 100%, but the approximate frequencies of its subsets are lower: 50% for A , 83% for AB , and 67% for ABC . Thus a set can be approximately frequent even though none of its nontriv-ial subsets are. This precludes us from pruning the candi-date itemsets in the way that Apriori and other algorithms do. A part of this problem is caused by the discrete nature of item presence: the 0 . 5-approximate frequency of ABC counts only those database rows where two of the three at-tributes are present, since 0 . 5  X  3 =2.

Both problems are avoided by our definition of dense item-sets. An itemset X is (  X ,  X  ) -dense , given two parameters  X  and  X  , if for any subset Y  X  X there is a set r Y of  X  database rows such that in the subdatabase defined by Y and r Y at least a fraction  X  of items are present.
We use ABCDE as shorthand for { A, B, C, D, E } .
With this definition, the first problem cannot occur, be-cause every subset of X must have sufficient co-occurrence. The second problem is also solved, since by the definition, the property of being dense is required to hold also for the subsets. For example, in Table 1b all itemsets are (6 , 0 . 5)-dense, so a levelwise algorithm would find ABCD .
In this paper we formalize the notion of dense itemsets and give some results on their properties. We then give a levelwise algorithm for finding all dense itemsets, and an-other algorithm for finding the top-k dense itemsets for a given threshold  X  , i.e., the sets which are (  X ,  X  )-dense and have the highest values of  X  . We demonstrate the usefulness of the concept and algorithm on a variety of data sets. We also show that the approach can be used to produce intuitive reorderings of the data matrix, and discuss generalizations and variations of the concept of dense itemsets.

We next discuss related work. As far as we are aware, there have been two papers in this direction of research. The first is Pei et al. X  X  paper on fault-tolerant itemsets. [17] They allow a fixed number of missing items on each row in the support of an itemset. This definition leads to an an-timonotone concept, but it suffers from a problem like that in Table 1a. To avoid this, Pei et al. require each item in a fault-tolerant set to be itself sufficiently supported. This still causes problems: for example, add to Table 1a sufficiently many rows with only item F present, and ABCDEF will be a fault-tolerant itemset, even though F has no connection to the other items.
 The other existing publication is Yang et al. X  X  paper [20]. They define strong error-tolerant itemsets ( eti s), which is the same concept that we have here called  X  -approximately frequent sets. They also use the concept of weak eti s ,which correspond to our dense itemsets without the recursive cri-terion. That is, a set is dense if and only if all its subsets are weak eti s. Yang et al. search for weak eti s, which have a partial monotonicity property, and then select strong eti from among the weak ones. They acknowledge the prob-lem illustrated by Table 1a, but the way they deal with it is by searching only a subcollection of all weak eti susinga heuristic algorithm; it is not very clear how exactly to char-acterize the resulting itemset family. Since their objective is to obtain a starting point for mixture modeling, this is perhaps not a serious drawback: it is enough to find some assignment of rows into clusters, and the EM algorithm will refine the solution. In contrast, our goal is to find insights about the data directly from dense itemsets, so it is impor-tant to understand exactly what kind of sets are found.
The rest of this paper is structured as follows. In Section 2 we formally define dense itemsets and describe some of their properties. Then we discuss algorithmic questions related to mining dense sets in Section 3, giving a levelwise algorithm and a top-k algorithm that helps with parameter selection. In Section 4 we show experimental results on dense itemsets and describe a set cover approach for selecting the most in-teresting dense sets. The same approach can also be used to reorder the rows and columns of the data matrix. Section 5 is a brief conclusion.
In this section we first formalize the idea of dense itemsets, then describe some of their properties. We start by defining binary databases and frequent itemsets [2].

Definition 1. Abinary database DB = R, r consists of afiniteset R of attributes ,alsoknownas items ,andafi-nite multiset r = { t 1 ,t 2 ,...,t n } of transactions ,whichare subsets of R .

Binary databases are easily visualized as matrices, as we have already done in Table 1, and so one can naturally refer to transactions as rows, and to items as columns. Itemsets are, of course, subsets of R . The traditional concept of fre-quency (sometimes called support ) is the result of a fully conjunctive database query: only those rows count that in-clude every item in the set.
 Definition 2. The (absolute) frequency of an itemset X  X  R in a database DB = R, r is the number of transactions that include all the attributes of X : An itemset is frequent if its frequency is greater than or equal to a predefined frequency threshold  X  .

As discussed in the introduction, this definition is too strict. It thus needs to be changed to allow some miss-ing attributes: we want to count all transactions, weighting them by the fraction of X  X  X  attributes they have.
Definition 3. The weak density of an itemset X  X  R is The sum in the numerator is taken over all transactions t in r , and the summand is the size of the intersection of X and t ;whatweakdensitymeansisthustheaveragefraction of items present.
 Example 1. In the database ABCDEF GH, r a shown in Table 1a, wdens( ABCDE, r a ) = 1, wdens( DEF, r a )=2 / 3, and wdens( FGH,r a ) = 0. In the database ABCD, r b of Table 1b, wdens( X, r b )=1 / 2 for every (nonempty) item-set X .

We will generally be interested in the weak density of item-sets over some subdatabases, specifically those subdatabases where the weak density is maximized.

Definition 4. Given a number  X  between zero and the size of the relation, the weak density at support  X  of X is where the maximum is taken over all  X  -element submulti-sets r of r . An itemset is weakly (  X ,  X  ) -dense if its weak density at support  X  exceeds  X  ,where  X  and  X  are prede-fined parameters.
Henceforth, we will omit the multiset r from the nota-tion wdens(  X , X, r ). For frequent sets, the definition of fre-quency has the property of antimonotonicity: if X  X  Y , then freq( X )  X  freq( Y ). This property is useful because it enables the Apriori algorithm to prune non-frequent sets. Note that the weakly dense sets do not have this property.
A more fundamental problem with the definition of weakly dense itemsets is the one illustrated in Table 1a: an itemset whose weak density is significantly greater than  X  will attract other items that have lower weak density X  X ree-riders. To ensure a more uniform density, we will use minimization over subsets in our stronger definition.

Definition 5. The density dens(  X , X ) of an itemset X at support level  X  is the minimum of the weak densities of all non-empty subsets of X , i.e., An itemset X is (strongly) (  X ,  X  ) -dense ,ifdens(  X , X )
Antimonotonicity follows as an immediate corollary from this definition, so dense itemsets can be found by a levelwise approach.

Two questions arise: Is this definition too restrictive? Or is it still too loose? We first consider whether the defini-tion of dense sets is too restrictive. It can be shown that every set X that is weakly but not strongly dense includes a strongly dense subset whose weak density is greater than that of X . In other words, every case where a set is weakly dense but not dense is an instance of the free-rider phe-nomenon in Table 1a. The role of the weak density thresh-old  X  then is to define the extent to which free-riders are tolerated.

Next we examine a case where the definition might seem to be too loose X  X ee the left-hand side of Table 2. Since the subsets of a dense set are only required to fulfill the weak density criterion somewhere in the data, not necessarily all on the same rows, we find that X = ABCDEF is (4 , 0 . 5)-dense. Here the connection between the attributes B , C , D , E ,and F is the strongest pattern, and A has a weaker connection that actually only occurs outside the main block where the rest of the attributes are found. This apparent discrepancy between dense sets and intuition is caused by the fact that the support level of 4 rows is too low: perhaps the first half of the database contains only noise, but at this resolution the noise is picked up. The right-hand side of the Table 2: An example database and the supports  X  at which the listed sets are weakly (  X , 0 . 5) -dense table shows the supports  X  at which the listed sets have weak density 0.5: the set ABCDE (like several other sets not shown) has support 4, so if we raise the support threshold above 4, the set ABCDE is no longer dense. The intuitively strong pattern BCDEF is found as the maximal-size (  X ,  X  )-dense set with e.g. (  X ,  X  )=(5 , 0 . 6), or (6 , 0 . 5). Thus, the role of the support threshold  X  is to define the size of the patterns that we seek X  X he algorithm X  X  resolution.
Next we show an inequality for weak density. Yang et al. showed that all weakly dense itemsets are accessible from the empty set by adding single items that keep the intermediate sets weakly dense. [20] Accessibility can be seen as a kind of monotonicity, and it allows weakly dense sets to be found in principle by a levelwise algorithm, whose practical usability is limited by the large number of candidates it needs to consider. The candidates can be pruned to some degree using the following result; however, this does not suffice to make the algorithm usable.

Proposition 1. If X and Y are itemsets with X  X  Y =  X  , wdens(  X , X  X  Y )  X 
Another application for the inequality is filtering the col-lection of dense itemsets: if the gap between the upper bound and the actual density of X  X  Y is large, { X, Y } may be a better collection of itemsets than { X  X  Y } . Space considerations preclude further discussion.
In this section we give simple algorithms for finding all dense itemsets from large collections of binary data. The algorithms are based on the familiar Apriori idea: for each h  X  1, given dense sets of size h , form candidate sets of size h +1, and then do a database pass to verify which candidates indeed satisfy the density condition. We also discuss finding the top-k dense itemsets with respect to density, given the support threshold  X  .
 Algorithm 1, Dense-Sets , performs a levelwise search to find all dense itemsets. The Generate-Candidates sub-routine is exactly the same as in Apriori [2]. As it is written, Algorithm 1 outputs the weak densities of itemsets, but the strong densities can be obtained by simple post-processing. Algorithm 1: Find strongly dense itemsets.
 Input: A binary database DB = R, r , a density thresh-old  X  , a support threshold  X  Output: All (  X ,  X  )-dense itemsets in DB , and their weak densities Dense-Sets ( DB , X , X  ) (1) C  X  X { A }| A  X  R } (2) while C is nonempty (3) D  X  Weak-Densities ( DB ,C, X  ) (4) P  X  X  X  X  C | D ( X )  X   X  } (5) foreach X  X  P (6) print X , D ( X ) (7) C  X  Generate-Candidates ( P ) Algorithm 2: Top-k dense itemsets given  X  .
 Input: A binary database DB = R, r , a support thresh-old  X  , and a number k .
 Output: Some k itemsets in DB such that they are the (  X ,  X  )-dense itemsets for the given  X  and some  X  Top-K-Given-Support ( DB , X ,k ) (1) H  X  empty heap (2) F  X  empty set-family (3)  X  =  X  (4) C  X  X { A }| A  X  R } (5) while k&gt; 0 (6) D  X  Densities ( DB ,C, X  ) (7) foreach X  X  C (8) Heap-Push ( H, D ( X ) ,X ) (9) ( d, X )  X  Heap-Pop ( H ) (10) print X, d (11)  X   X  min(  X , d ) (12) C  X  Candidates ( F, X ) (13) F  X  F  X  X  X } (14) k  X  k  X  1 (15) if consistent answer of  X  k sets is wanted (16) (run similar loop with fixed  X  )
The statement D  X  Weak-Densities ( DB ,C, X  ) assigns to D ( X ), for all X  X  C , the weak density of X . We postpone the description of Weak-Densities for a while.
 Algorithm 1 takes two parameters, a density threshold  X  and a support threshold  X  , whose roles were discussed at the end of Section 2. Selecting the parameters is not always easy; when mining frequent itemsets, one can first try with a high threshold and gradually lower it until a satisfactory number of sets is found. With two parameters, such continual ad-justment becomes more difficult: with too high values, few sets are found, but with too low values, there are so many dense sets that the algorithm takes a prohibitively long time.
However, along the lines of, e.g., [11, 19] there is a way automatically to select one of the two parameters: given the support threshold  X  and the number k of itemsets required, we can find the k itemsets that would have been found by running Algorithm 1 with the given  X  and some  X  .Thisis done by Algorithm 2; an analogous algorithm can be found for the case where  X  is given and  X  unknown.

This algorithm uses a heap H to store pairs ( d, X ), where d is the density of X at support  X  . The subroutine Heap-Push adds such a pair to the heap, and Heap-Pop re-moves and returns a pair ( d, X ) for which d is maximal. The variable F is a family of itemsets that have already been found; in practice, we implement it as a tree structure where the nodes are items, and sets are found by traversing the tree in numerical order of items. The subroutine Candi-dates ( F, X ) (omitted) simply enumerates all sets X  X  X  A with A  X  R \ X and returns the collection of those sets whose all subsets are in F .

A detail about Algorithm 2 is that there may not be a family of exactly k sets that are the dense sets given  X  and some  X  . If exactly k sets are wanted, the algorithm can be stopped at line 15; if a consistent answer is wanted, even though it may include many more than k sets, the algorithm can be continued X  X he last part is essentially similar to the first part, now with a fixed value of  X  .
 For the database pass we need a way to find the weak density of an itemset at a given support. It turns out that one scan through the database is sufficient, using O ( | X | )spacefor each candidate itemset X .

We next introduce a concept that is used here for efficient computation of the weak density of an itemset.

Definition 6. Given an itemset X  X  R with | X | = ,we define the intersection profile H X as an ( + 1)-sized vector, indexed by numbers from 0 to ,where H X ( j )isthenumber of transactions that have exactly j attributes in common with X :
To find wdens(  X , X ) for an itemset X ,itsufficestofind the intersection profile H X and then do some counting. How do we compute the weak density of a set X at support  X  ? We need to take  X  transactions whose intersections with X are maximal. To form the multiset r of these  X  transac-tions t  X  R , we simply start from the top: first the H X transactions whose intersection has size = | X | , then the next H X (  X  1), etc., until finally adding some H X ( j )tu-ples would make the subrelation grow to a size &gt; X  .In this process, the size of the subrelation eventually becomes i = j +1 H X ( i ), where j is the intersection size at which the subrelation would have grown above  X  . Finally, we need to add the remaining transactions, each one having size j .This yields the Weak-Densities subroutine.
We implemented the above algorithms in Python 2.3 using the kjBuckets library 2 for representing sets, and tested on an Athlon XP 1600+ computer running Linux. The reader should keep in mind that Python is an interpreted scripting language, and an optimized implementation of the algorithm would likely be much faster.

We experimented on the Retail data set [5] obtained from the fimi web site [10]. The data contains 16470 attributes, out of which we selected only the 1998 most frequent. The data contains 88162 rows (transactions), and we used sup-port thresholds  X  ranging from 441 (0.5%) to 8816 (10%). Table 3 shows the number of dense itemsets in the pre-processed data; missing values indicate that the run was stopped because it was taking too long. The results show that there are parameter values for which there is a moderate number of dense itemsets. Table 4 summarizes the running times of our implementation: as noted above, the implemen-tation is not optimized. About 8 minutes are needed just to read and preprocess the data.

Next we consider finding top k dense itemsets using Al-gorithm 2. The results are shown in Table 5, for k = 1000. When searching for the top 1000 sets, one usually obtains some more sets, since no collection of exactly 1000 sets might be a coherent result for a single value of  X  : for example, with  X  = 8816, the closest match is a 1007-set collection with  X  =0 . 324.  X  X order X  here means the number of extra sets considered by the algorithm X  X ets that have weak den-sity &lt; X  but whose all immediate subsets are dense. The effect of the border sets is considerable for lower values of  X  , kjbuckets/ Table 3: Number of dense itemsets in Retail data; columns=values of  X  ; rows=values of  X  8816 10.2 10.0 9.9 9.8 9.4 9.9 9.9 4408 29.9 13.0 10.2 10.2 10.0 10.2 9.9 2204 143.3 12.7 10.9 10.7 10.3 10.2 882 80.7 47.9 33.5 25.7 22.3 441 214.9 164.0 111.5
Table 4: Run-times for Retail data in cpu minutes Table 5: Summary of Top-K-Given-Support in Retail data with k = 1000 as can be seen on the last two rows of the table. Of course, the value of k here is too large for most practical uses.
We now move to another data set. The Course data set contains information about course enrollment at the Univer-sity of Helsinki Computer Science Department. This partic-ular data lists only Masters-level courses, and contains in-formation for 1739 students and 102 courses. Mining dense itemsets with  X  = 87 (i.e., 5%),  X  =0 . 5 we found 4657 sets.
We used these sets to obtain a novel representation of the data set. Given a dense itemset, we say that its cov-erage is the number of 1s present in the 87 rows from the data with most attributes within the set. We used a greedy set-cover algorithm: always select the itemset among those found that, when selecting the 87 rows yields the maximal number of 1s not yet covered. The first 8 sets output by this algorithm are shown in Table 6; note that the attributes are listed as rows and the dense itemsets as columns. The den-sities of these sets vary from 0.52 to 0.58.

The first sets have clear meanings: the first one represents programming (both as a process and as low-level systems knowledge), the second information systems, the third algo-rithms, etc. An interesting phenomenon is seen in set 8: it is the same as set 1, but lacking the theory course. In the set cover algorithm, set 1 should already have covered those 5% of students who have taken many programming courses, but now such students are found again. The finding could have an interesting interpretation: many students who are studying Computer Science in order to become professional programmers have not yet passed the theory course. Indeed, many students consider the course a difficult one. frequency Figure 1: Course data: weak density vs. frequency for (87 , 0 . 5)-dense sets X with | X | X  2 course Figure 2: Course data reordered using the 8 itemsets in Table 6.

One of the motivations for the concept of dense itemsets is that the interesting sets will have very low frequency in the classical sense, and would thus not be found by frequent itemset mining. This proved to be the case in the course data: the 8 most useful dense itemsets had frequency 0. As a further illustration, Figure 1 shows the relationship of weak density and frequency for sets with at least two elements. Another motivation is that the family of weakly dense sets is infeasibly large, and this was also confirmed. For example, out of the 4.2 million possible 4-element itemsets in this data, more than 575 thousand had weak density at least 0.5 with  X  = 87. Clearly, finding all weakly dense itemsets even up to size 8 would yield a much larger output than mining the 4657 dense itemsets.

As an illustration of the use of dense itemsets, we also used the greedy approach to reorder the data matrix. We first took the  X  X est X  dense itemset (in the coverage sense de-fined above) and listed its attributes. Then we selected the rows which have at least a frequency of  X  for this itemset. The remaining rows and columns are then iteratively or-dered using the same method. Figure 2 shows the resulting reordered data matrix. Denseset 12345678 Theory of Computation * * * * * * * Software Architectures * * * * Software Processes * * * Software Testing * * Data Communication 2 * * * Computer Architecture * * * Operating Systems 2 * * * Distributed Systems * * * Structured Documents * * * DB Algorithms * * Data Warehouses * Document Collections * * Information Extraction * * Data Mining * * Algorithm Analysis * Artificial Intelligence * Computer Graphics * * * * String Algorithms * User Interfaces * Distributed OS X  X  * Data Management 2 * Database Modeling * Computer-aided Learning * *
Table 6: Set-cover with 8 dense Course itemsets.
In this paper we have introduced the concept of dense itemsets, and argued that the definition is robust by giving some results on the combinatorial behavior of the density functions. We gave algorithms for finding all or the top-k dense itemsets. The experimental results show that the methods work well in practice. We also demonstrated that one can further select the most interesting dense itemsets using a greedy approach: the application of the method to real datasets shows the usefulness of this technique.
Several open problems remain. Most importantly, we be-lieve that filtering and reordering techniques need to be de-veloped for handling large collections of patterns. The re-ordering technique of Section 4 is one possibility; another is applying Proposition 1. Related methods include other greedy covering algorithms [12, 15]; matrix ordering and factorization techniques such as nmf [14] and pca [8]; gener-alizations of the various subclasses of frequent itemsets [4, 7, 16]; and selection using roc curves [18]. We are also investi-gating a generalization of Definition 6 to  X  X xternal intersec-tion counts X , measuring not only how many items of a given set but also how many items of its complement appear in a transaction. These counts lead to filtering techniques rem-iniscent of segmentation problems [13] and tilings [9]. Also of interest are applications to topic models [3] and query approximation [6]. We thank Mohammed Zaki, Aristides Gionis, and Jaakko Hollm  X  en for enlightening discussions, Taneli Mielik  X  ainen for bringing the Pei et al. paper [17] to our attention, and Tom Brijs for donating the Retail data set [5].
