 REGULAR PAPER Ruoming Jin  X  Anjan Goswami  X  Gagan Agrawal Abstract Clustering has been one of the most widely studied topics in data min-ing and k -means clustering has been one of the popular clustering algorithms. K -means requires several passes on the entire dataset, which can make it very expensive for large disk-resident datasets. In view of this, a lot of work has been done on various approximate versions of k -means, which require only one or a small number of passes on the entire dataset.
 tering ( FEKM ), which typically requires only one or a small number of passes on the entire dataset and provably produces the same cluster centres as reported by the original k -means algorithm. The algorithm uses sampling to create initial cluster centres and then takes one or more passes over the entire dataset to ad-just these cluster centres. We provide theoretical analysis to show that the cluster centres thus reported are the same as the ones computed by the original k -means algorithm. Experimental results from a number of real and synthetic datasets show speedup between a factor of 2 and 4.5, as compared with k -means.
 we refer to as DFEKM . This algorithm is suitable for analysing data that is dis-tributed across loosely coupled machines. Unlike the previous work in this area, DFEKM provably produces the same results as the original k -means algorithm. Our experimental results show that DFEKM is clearly better than two other pos-sible options for exact clustering on distributed data, which are down loading all data and running sequential k -means or running parallel k -means on a loosely coupled configuration. Moreover, even in a tightly coupled environment, DFEKM can outperform parallel k -means if there is a significant load imbalance. Keywords k -means clustering  X  Out-of-core datasets distributed k -means  X  Confidence radius  X  Boundary points 1 Introduction Clustering has been one of the most widely studied topics in data mining. Cluster-ing refers to techniques for grouping similar objects in clusters. Formally, given asetof d -dimensional points and a function f : d  X  d  X  that gives the distance between two points in d , we are required to compute k cluster centres, such that the points falling in the same cluster are similar and points that are in different clusters are dissimilar.
 pattern-recognition communities, where the goal was to cluster a modest num-ber of data instances. However, within the data-mining community, the focus has been on clustering large datasets. Developing clustering algorithms to effectively and efficiently cluster rapidly growing datasets has been identified as an impor-tant challenge. For example, Ghosh states  X  X he holy grail of scalable clustering can be to find near linear algorithms that involve only a small number of passes through the database X  [ 13 ].
 and out-of-core datasets, using one or a small number of passes over the data, without compromising on its result . Our work is in the context of k -means clus-tering. The K -means clustering algorithm was developed by MacQueen [ 22 ]in 1967 and later improved by Hartigan [ 16 ]. Bottou and Bengio [ 3 ] proved the con-vergence properties of the k -means algorithm. It has been shown to be very useful for a corpus of practical applications. The original k -means algorithm works with memory-resident data but can be easily extended for disk-resident datasets. converging to a quality solution. This makes it potentially very expensive to use, particularly for large disk-resident datasets. A number of algorithms or approaches focus on reducing the number of passes required for k -means [ 4 , 5 , 10 , 14 ]. How-ever, these approaches only provide approximate solutions, possibly with deter-ministic or probabilistic bounds on the quality of the solutions. A key advantage of k -means has been that it converges to a local minimum [ 15 ], which does not hold true for the approximate versions.
 fewer passes on the entire dataset and can produce the same results as the original k-means algorithm? X  In this paper, we present an algorithm that makes one or a few passes over the data and produces the exact cluster centres as would be generated by the original k -means algorithm. We refer to this algorithm the as fast and exact K -means algorithm, denoted by FEKM .
 run the original k -means algorithm. We store the centres computed after each iter-ationoftherunofthe k -means on the sampled data. We now use this information and take one pass over the entire dataset. We identify and store the points that are more likely to shift from one cluster to another, as the cluster centres could move. These points are now used to try and adjust the cluster centres.
 cluster centres. In the worst case, the algorithm can require the same number of passes as the original k -means. However, our detailed experimental analysis on several synthetic and real datasets shows that it requires at most 3 passes, whereas the average number of passes required is less than 1.5. This results in speedups between 2 and 4.5 as compared with the original k -means.
 we refer to as DFEKM . This algorithm is suitable for analysing data that is dis-tributed across loosely coupled machines. Unlike the previous work in this area, DFEKM provably produces the same results as the original k -means algorithm. Our experimental results show that DFEKM is clearly better than two other pos-sible options for exact clustering on distributed data, which are down-loading all data and running sequential k -means or running parallel k -means on a loosely coupled configuration. Moreover, even in a tightly coupled environment, DFEKM can outperform parallel k -means if there is a significant load imbalance. related work. In Sect. 3 , we present the main ideas of the FEKM algorithm and explain the details of the pseudo code of the algorithm. In Sect. 4 , we provide the theoretical framework for the algorithm. The experimental results from FEKM are provided in Sect. 5 . The distributed version of the FEKM and its evaluation is presented in Sect. 6 . We conclude in Sect. 7 . 2 Related work There has been an extensive study on clustering algorithms in the literature. A comprehensive survey on this subject can be obtained from the book [ 17 ]and papers [ 2 , 13 ]. In this discussion, we limit ourselves to the improvements over k -means.
 data structure to store distance information, which can make each iteration of k -means significantly faster. This algorithm focuses on the in-core datasets. tipass k -means. This algorithm summarises the input points based on their likeli-hood to belong to different centres. Farnstorm and his colleagues [ 11 ]havefurther refined this idea.
 using sampling based on Hoeffding or similar statistical bounds. The algorithm consists of a number of runs of k -means with sample, where, in every iteration, sample size is increased to maintain the loss bound from the multipass k -means. The goal here is to converge to a solution that is close to that of a multipass k -means by a predefined bound with good probability. Motwani, Charikar, and their colleagues [ 1 , 5 , 6 , 14 , 24 ] proposed a series of constant-factor approximation algorithms for one-pass k -centre and k -median problems.
 massive datasets, scanning the dataset only once. Their algorithm splits the entire dataset into chunks, and each chunk can fit into the main memory. Then, it ap-plies k -means on each chunk of data and merges the clustering results by another k -means-type algorithm. Good results are shown for a real dataset; however, no theoretical bounds on the results are established.
 algorithms that cannot maintain the exact result that would be obtained using a multipass k -means algorithm. Developing an algorithm with this goal is the focus of our work. 3 Algorithm description This section describes a new algorithm, fast and exact k -means ( FEKM )thatwe have developed. Initially, we describe the main ideas behind the algorithm. Then, we give some formal definitions, present and explain the pseudo-code and explain some of the choices we have made in our current implementation. 3.1 Main ideas The basic idea behind our algorithm is as follows. We believe that approximate cluster centres computed using sampling can be corrected and moved to exact cluster centres using only one or a small number of passes on the entire data. By exact cluster centres, we mean the cluster centres that are computed by the original k -means algorithm. Thus, we can use sampling to speed up the computation of exact clusters.
 centres are computed using sampling, what information needs to be stored? Sec-ond, how can this information be used to avoid a large number of passes on the entire dataset? Third, how do we know that we have computed the same cluster centres as in the original k -means algorithm? gence criteria and same initial points as we would use for the k -means. The fol-lowing information is stored for future use. After every iteration of k -means on the sampled data, we store the k centres that have been computed. In addition, we compute and store another value, referred to as the confidence radius of each cluster, the computation of which will be described later. This information can be stored in a table with k columns and the number of rows equalling the number of iterations for which k -means was run on the sampled data. Each entry of the table contains a tuple (centre, radius) for each cluster.
 row of the table, we compute the cluster to which this point will be assigned at this iteration, assuming that executing the algorithm on the entire dataset produces the same cluster centres as the initial run on sampled data. Next, we try to estimate algorithm is executed on the entire dataset.
 cluster during any of the iterations. These points are refereed to as boundary points because, intuitively, they fall at the boundary of the clusters. If these points could be identified and stored in memory, we can eliminate any need for any further passes on the entire dataset. However, we can only estimate these points, which means that we could require additional passes if our estimate is not correct. boundary point. If it is, it is stored in a buffer. Otherwise, we update the sufficient statistics tuple , which has the number and sum of the data points for the cluster. following processing. Starting from the first row of the table, we recompute cen-tres using the boundary points and sufficient statistics tuple. If any of the new com-puted centres fall outside the preestimated confidence radius, which means that our computation of boundary points is not valid, we need to take another pass through the data. We use the new centres as new initialisation points and again repeat all the steps. However, if the new computed centres are within the confidence radius, we use these centres for the next iteration and continue. The key observation is that, using cluster centres from sampling, boundary points and sufficient statistics, we are able to compute the same cluster centres that we would have gotten through one pass on the entire dataset. Finally, the algorithm terminates by checking for the same termination condition that one would use in the original algorithm. 3.2 Formal definitions This subsection formalises some of the ideas on which the algorithm is based. new algorithm, FEKM , initially the k -means algorithm is executed on the sampled dataset with the same initialisation. At the i th iteration, let the k centres be denoted means centres and the later k centres are called as the sampling centres . with it. The confidence radius,  X  i j , is based on an estimate of the upper bound of the distance between the sampling centre, s i j , and the corresponding k -means centre, c . Ideally, the confidence radius,  X  i j , should be small, but should still satisfy the condition d ( c i j , s i j )  X   X  i j ,where d is the distance function. cussed before, the sampling centres are stored in a table with k columns, where the i th row represents the i th iteration. To facilitate our discussion, we call the closest centre among a set of k centres for a point as the owner of this point. Definition 1 For any point p in the complete dataset, assuming s i j to be the owner of point p with respect to the sampling centres at the i th iteration, if there exists l , l = j , such that then p is a boundary point for the i th iteration.
 iterations. Thus, the complete set of boundary points includes the points in the entire dataset whose owners with respect to the k -means centres are quite likely to be different from the owners with respect to the sampling centres for one or more of the iterations.
 that are not boundary points for the i th iteration. Usually, for any stable point, the difference between its distance to its owner with respect to the sampling centres and its distance to other sampling centres is quite large. Mathematically, assuming s to be the owner of the point p with respect to the sampling centres at the i th iteration, for any l , l = j ,wehave
Algorithm 1: Pseudo code of fast and exact out of core k -means ( FEKM ). 3.3 Detailed description The detailed algorithm is shown in Fig. 1 . We now explain the algorithm. means run on sample data. We call this table the cluster abstract table or the CAtable . Our algorithm starts with building a CAtable from a sample of the orig-inal dataset. Initially, each entry of the CAtable contains the two tuple, the centre and the confidence radius of each cluster in that iteration. This is done through the function BuildCATable . After this, we take one scan over the complete dataset and find out the likely boundary points for each iteration or for each row of the table. The function IsBndrPoint checks for each data point if it meets the conditions of being a boundary point.
 that the same point also can be a boundary point for the next rows or next itera-tions of the CAtable. We define two lists, one to store the points and another to store the indexes of the rows where these points are found as boundary points. The first list is named as buffer and the second list is the as index . The second list is two dimensional, where each row signifies one specific point and each column has  X  bits, where  X  is the number of iterations or rows in CAtable. If the spe-cific point is found as a boundary point in the j th row of the CAtable, then the j th bit of the corresponding column of the index list is set to 1. We also store the number and sum of the nonboundary points with each CAtable entry. The function UpdateSufficientStats accomplishes this.
 points corresponding to that row and from the sufficient statistics. In the Fig. 1 ,it has been done by the function RecomputeCtrs . We then verify if the new centres are located within the preestimated confidence radius to maintain the correctness. The function IsCtrsWithinRadii is responsible for this verification. If we find that the new centres are located within the confidence radius of corresponding clusters, we update the centres of the CAtable in the next row using the function Update-CAtableCtrs . If any of the new centres are found outside the confidence radius of the corresponding cluster, the initial centres are replaced by those new centres and the algorithm repeats from the creation of CAtable. 3.4 Computation of confidence radius The computation of confidence radius for each cluster and each iteration is an large number of boundary points, which cannot be stored in memory. At the same time, very small confidence radius values could mean that the difference between corresponding sampling centres and k -means centres could be greater than this value, and therefore, an additional pass on the entire dataset may be required. dence radius. Recall that, at iteration i , the confidence radius for the cluster j is denoted by  X  i j .Weuse where, X p denotes a d -dimensional point assigned to the cluster j at the iteration i , X c is the centre of the cluster j at iteration i , N is the number of points assigned to the cluster j and f is a factor that is chosen experimentally. For a fixed f ,the above expression will choose confidence radius value that is proportional to the average distance of a point in the cluster to the cluster centre. This ensures small confidence radius values for a dense cluster, and larger radius values otherwise. be huge and, in the worst case, can also exceed memory size. Thus, in our imple-mentation, we have two conditions on the number of boundary points. First, they should not exceed 20% of all points in the complete dataset. Second, they should not exceed the available memory. If, during an execution, the number of boundary points violate either of the above two conditions, we reduce all the confidence radii by choosing a lower value of f and repeat the computation of boundary points. For our experiments, the value of f wasalwaysfixedat0.05. 4 Theoretical analysis In this section, we initially present a proof of correctness for the FEKM algorithm. Then, we also analyse the execution time of this algorithm. 4.1 Proof of correctness We now show how FEKM computes the same cluster centres as k -means. Our description here builds on the definitions of k -means centres, sampling centres, owners, boundary points, and stable points given in the previous section. We fur-ther add the definition of changing points.
 Definition 2 For the i th iteration, the changing points are defined as the points in the dataset that have different owners with respect to the sampling centres and the k -means centres.
 Lemma 1 Suppose at the i th iteration the following condition holds for each cen-tre j , 1  X  j  X  k: Then, the stable points will have the same owners with respect to the sampling centres and the k-means centres, and the set of changing points is a subset of the set of boundary points.
 Proof Consider any point p in the complete dataset and let s i j be the owner of p with respect to the sampling centres at the i th iteration. For any l , l = j , from the triangle inequality ,wehave Further, applying the condition that is assumed, we can have the following in-equalities: Therefore, Case 1: If p is a stable point, Therefore, we have the inequality This suggests that the centre j is still the owner of the point p .
 Case 2: If the point p changes its owner in the complete dataset, there exists a centre l , such that Therefore, we have This suggests that the point p is a boundary point. Combining both cases, we prove the lemma.
 Lemma 2 If FEKM has computed the k-means correctly at the i th iteration and at the i th iteration the condition holds, then FEKM will compute the k-means centres for the iteration i + 1 cor-rectly.
 Proof This follows the result of Lemma 1 . The stable points will have the same owners with respect to sampling centres and the k -means centres at the i th itera-tion. Therefore, in the i th row in the CAtable , we maintain the correct and suffi-cient statistics to summarise the stable points. Further, after the i th iteration, each boundary point can be assigned to the correct owner because we have the correct k -means centres for the i th iteration. Therefore, each centre in the i th iteration owns the correct partition of the complete dataset, and the k -means centres of the iteration i + 1 can be computed correctly.
 Theorem 1 Suppose that, for each iteration i , 0  X  i  X  m, the condition holds, and at the iteration m + 1 , this condition does not hold. Then, for each iteration i , 0  X  i  X  m + 1 , the k-means centres of the i th iteration can be computed correctly by FEKM.
 Proof This can be proved inductively. For the base case, we use the fact That, at iteration 0, the same initialisation centres are used by k -means and FEKM .Forthe induction step, we use the Lemma 2 . Theorem 2 Assuming the same termination condition, FEKM will iterate the same number of times for the centres as the k-means algorithm, and at each it-eration, will generate the same centres as the k-means algorithm.
 Proof Recall that once the FEKM algorithm finds that the distance between sam-pling centres and k -means centres is greater than the confidence radius, it will sample again and take the k -means centres at that iteration as the initialisation centres. Using this, and Theorem 1 ,wehavetheaboveresult. 4.2 Analysis of performance We now analyse the execution time for our algorithm and compare it with that of original k -means.
 I/O cost for reading the dataset once be C I , and let the computing cost (besides the I/O cost) associated with each pass on the complete dataset be C c . Therefore, the total running time of k -means algorithm, T k -means , can be expressed as need to sample the dataset. Also, let the size of each sample be a fraction SS of the entire dataset. Further, let the execution of k -means on the sampled dataset require an average of m iterations. This suggests that the total number of rows that are maintained in FEKM is m  X  P . Therefore, the total running time of FEKM algorithm, T KEKM , can be expressed as computing cost than the k -means algorithm because usually FEKM has to com-pute more rows ( P  X  m ) than the number for k -means ( n ). For execution on disk-resident datasets, the computing cost of k -means is typically much smaller than the I/O cost. Also, if we have the ability to overlap computation and I/O, the over-all execution time reduces to the maximum of the I/O and computational costs, which is likely to be the I/O cost. In either case, we can see that, if P is small, FEKM will be much faster than the k -means algorithm.
 Furthermore, a sampling fraction of 5% or 10% is usually sufficient. For such cases, the above expressions suggest a clear advantage for the FEKM algorithm. The next section further demonstrates this through experimental results. 5 Experimental results from FEKM This section reports on a number of experiments we conducted to evaluate the FEKM algorithm. Our experiments were conducted using a number of synthetic and real datasets. Our main goal was to compare the execution time of our al-gorithm with that of the k -means algorithm. Additionally, we were interested in seeing how many passes over the entire dataset were required by the FEKM algo-rithm. All our experiments were conducted on 700-MHz Pentium machines with 1-GB memory.
 algorithm are kept the same as those of the k -means algorithm. As the performance of k -means is very sensitive to the initialisation, we considered different initialisa-tions. We used two different initialisation techniques. In the first technique, which could only be applied for the synthetic datasets, we perturbed each dimension in the original centre points of the Gaussians that were used to generate the data sets. Two different initialisations, referred to as good and bad , were obtained by vary-ing the range of perturbation. In the second technique, we randomly selected the initial centre points from a sample of the dataset such that distance between any two points chosen is greater than a threshold . In this case, the good and the bad ini-tialisations corresponded to a large and small value of this threshold, respectively. tres are not sufficiently different from those generated in the previous iteration or (2) it has run for a specified maximum number of iterations. The second criteria is useful with bad initialisations, where the algorithm could run for a large number of iterations. The notation used in the tables containing the results of the experiments are explained in Table 1 . 5.1 Evaluation with synthetic datasets The evaluation with synthetic datasets was done using eighteen 1.1-GB datasets and two 4.4-GB datasets. The datasets involve different number of clusters and di-mensions. For generating each synthetic dataset, points are drawn from a mixture of a fixed number of Gaussian distributions. Each Gaussian is assigned a random weight, which determines the size of each cluster. For each dimension, we kept spectively, to retain the flavour of the datasets used in the experiments by Bradley et al [ 4 ] and in the experiments by Farnstorm et al [ 11 ]. We did the experiments using 5, 10 and 20 clusters and with 5, 10, 20, 50, 100 and 200 dimensions. the execution times for FEKM and original k -means with 5, 10 and 20 clusters, re-spectively. As these tables show, FEKM requires one or at most two passes on the entire dataset. FEKM is faster by a factor between 2 and 4 in almost all cases. The relative speedup of FEKM is higher with bad initialisations, where a larger num-ber of iterations are required. We have considered sample sizes that are 5% and 10% of the entire dataset. FEKM is always faster with 5% sample size, because it reduces the execution time for the k -means and did not require any additional passes on the entire dataset. The number of clusters or dimensions do not make a significant difference to the relative performance of the two algorithms. shows these results. The first set has 20 clusters and 100 dimensions. The sec-ond dataset has 5 clusters and 20 dimensions. The relative speedup of FEKM is between 2 and 4.5. 5.2 Evaluation with real dataset We evaluated our algorithm with three publicly available real datasets. These datasets are KDDCup99, Corel image database, and the Reuters-21578. All these datasets are available from University of Irvine X  X  KDD archive. 1 We preprocessed each of these datasets and generated feature vectors using standard techniques, briefly described below. We then applied k -means and the FEKM algorithm. To be able to experiment with out-of-core datasets, we increased the size of the datasets by random sampling.
 depending on particular preprocessing of the real dataset. In our experiments, we used simple preprocessing techniques, which can be improved upon. We used the Euclidean distance function to compute distance between two points. Different distance metrics may help in obtaining better quality clusters and can also reduce the number of iterations, particularly for the datasets with categorical attributes. connection. This dataset is used for evaluating network-intrusion-detection tech-niques. The size of this dataset is about 743 MB. We enumerated different symbols of each type of categorical attributes. Each attribute is normalized by dividing with the maximum value of that attribute. After the preprocessing step, we obtained normalized continuous-valued feature vectors of 38 dimensions. The number of clusters specified in our experiments was 5. By supersampling the data, we cre-ated 5 million feature vectors, for a resulting dataset size of 1.8 GB. 32 dimensions colour histogram feature vectors of these images, which is available from UCI KDD archive. This dataset is about 20 MB. We thus increased the size of the dataset by randomly selecting vectors from the dataset and created a 1.9-GB dataset containing 6,804,000 continuous-valued feature vectors. We kept the number of clusters at 16. All attribute values are normalized between zero to one. integer-valued feature vectors of 258 dimensions by counting the frequency of the most frequent words in 135 different categories. Following Fayyad, Bradley and Reina [ 4 ], we kept the number of clusters at 25 for our experiments. We then increased the size of the dataset by supersampling and created 4.3 million records of 258 dimensions. The size of the resulting dataset was approximately 2 GB. to what we observed from synthetic datasets, the speedup from FEKM is between 2 and 4.5. The number of passes required by FEKM is either 2 or 3.
 k -means. One question is: How do they compare with the results from clustering the sampled data? We report some data in this regard from the real datasets. The column labeled se shows the squared difference between the final centres and the centres obtained from running k -means on sampled data. The values of all at-tributes were normalized to be between 0 and 1 for the real datasets. In view of this, we consider the reported squared errors to be significant. Further, it should be noted that all datasets were supersampled, which favours sampling. Thus, we believe that using an accurate algorithm like FEKM is required for getting high-quality cluster centres. 6 Distributed version of FEKM This section describes the distributed version of FEKM , which is referred to as DFEKM . 6.1 Motivation With the emergence of the internet, web, and now grid computing, data is in-creasingly being shared through data repositories. Often, data of interest to a data analyser is distributed across multiple data repositories. Analysis of large and geographically distributed scientific datasets has emerged as an important prob-lem [ 7 ]. The challenges of developing mining algorithms for distributed datasets are well recognised [ 20 ]. The biggest issue is that it is typically not feasible to download the entire dataset on a single machine and apply standard algorithms. Because the sites hosting the data are only loosely coupled, the communica-tion latencies are very high and parallel algorithms cannot be directly applied either.
 are no privacy considerations, one can download the data to a local machine and then use any of the standard algorithms for data clustering. Download-ing large volumes of data and then applying a serial clustering algorithm can be very time consuming. Moreover, this approach also requires significant communication, storage and computing resources, which may simply not be available.
 gorithm across the distributed data repositories. The k -means clustering algorithm has been parallelized by many [ 9 , 12 ] and has been shown to give high parallel efficiency on tightly coupled parallel machines. However, these implementations require uniform data distribution across the nodes of the parallel machine and one round of communication after every iteration of the clustering algorithm. If these algorithms are executed on distributed data repositories, uneven data distribution and high communication latencies will likely result in poor performance. algorithm, which is referred to as DFEKM . This algorithm is suitable for analysing data that is distributed across loosely coupled machines. Unlike the previous work in this area [ 18 , 19 , 25 , 27 ], our algorithm provably produces the same results as the original k -means algorithm.
 produces the same clusters as the original k -means algorithm or that maintains k -means property of achieving a local minimum. Samatova, Ostrouchov and their group [ 27 ] has proposed a technique called recursive agglomeration of cluster-ing hierarchies by encircling tactic (RACHET). This technique is based on suf-ficient statistics. It collects local dendograms and then merges them to create a global dendogram. However, this needs to iterate until the sufficient statistics converge to the desired quality. Parthasarathy and Ogihara [ 25 ] provided an al-gorithm where the distance metric is formed applying association rules locally. Kargupta and his group [ 19 ] have applied PCA to do high-dimensional cluster-clustering technique that involves creating local clusters and then deriving global clusters from them [ 18 ]. There have been many efforts on parallelizing k -means or other related clustering algorithms, such as k -harmonic and EM. Key efforts in this area include Dillon and Modha [ 9 ] (for k -means), Kruengkrai et al [ 21 ]and Lopezetal[ 8 ] (both for EM), and Forman and Zhang (for k -means, k -harmonic and EM). All of these approaches require data to be evenly distributed between the nodes and one round of communication after every pass on the data. There-fore, these approaches are not suitable for clustering data resident on distributed repositories. 6.2 Algorithm description Our description in this section assumes that data to be clustered are available at two or more nodes, which are referred to as the data sources . In addition, we have a node denoted as the central site , where the results of clustering are desired. It is also assumed that additional computation for clustering can be performed at the central site. We only consider horizontal partitioning of the data, i.e. each data source has values along all dimensions of a subset of the points.
 each data source and communicate it to the central node. Then, on the central node, we run the k -means algorithm on this sampled data. The main data structure of FEKM, the CATable, is computed and stored. Then, we send the table to all the data sources. Next, at each data source, we take one pass through the portion of the dataset available at that data source. For a given point and row of the table, we determine if this point is a boundary point. If it is, it is stored in a buffer. Otherwise, we update the sufficient statistics tuple , which has the number and sum of the data points for the cluster.
 send their boundary points and sufficient statistics to the central node. The central node then does the following processing. Starting from the first row of the table, it recomputes centres using the boundary points and sufficient statistics tuple. If any of the new computed centres fall outside the preestimated confidence radius, which means that our computation of boundary points is not valid, we need to send the last corrected centres to all other nodes. Using these centres as the new initialisation points, we have to go through another iteration and repeat all the steps. However, if the new computed centres are within the confidence radius, we use these centres for the next iteration and continue. Finally, the algorithm terminates by checking for the same termination condition that one would use in the original algorithm. 6.3 Experimental results We now report on a number of experiments we conducted to evaluate the DFEKM algorithm. We initially discuss the experiments we designed. As we stated ear-lier, two existing approaches for applying k -means-like clustering algorithm on distributed datasets are (1) down loading data on a single node and applying the centralised k -means algorithm and (2) executing parallel k -means algorithm across distributed data sources. Our experiments compare our DFEKM algorithm duces the same results as the exact k -means algorithm applied centrally, we did not compare DFEKM against any of the existing approximate distributed cluster-ing approaches.
 distributed data repositories. As compared with a tightly coupled parallel configu-ration, executing parallel code on distributed data repositories potentially involves large load imbalance and/or high communication latencies. Our experiments were conducted on a parallel machine, and the above two effects were simulated by in-troducing delays during each round of communication. We also considered cases in which data have not been evenly distributed. For such cases, we compare par-allel k -means, sequential k -means and our DFEKM algorithm. Additionally, we were interested in seeing how many passes over the entire dataset and how many rounds of communication were required by the DFEKM algorithm.
 Computing Centre (OSC). Each node of IA-32 cluster has two 900-MHz Itanium-2 processors and 4 GB main memory. These nodes are connected using Myrinet, which is a switched 2.0-GB/s network.
 The method used for generating these was described in the previous section. As in the previous section, a synthetic dataset with n clusters and m dimensions is referred to as c n d m .
 100 dimensions. We considered parallel/distributed configurations with 1, 2, and 4 nodes, and with 0, 50, 100, 150 and 200 second communication delays .This communication delay is a waiting time before any round of interprocessor com-munication. A delay of 0 seconds corresponds to a tightly coupled parallel con-figuration in which data is evenly distributed between the nodes. Nonzero delays simulate high communication latencies in loosely coupled environments, as well as waiting time for the slowest node, when the data are not evenly distributed. k -means and DFEKM . Parallel k -means on each of the four synthetic datasets required 20 iterations, which also means 20 rounds of interprocessor communica-tion. DFEKM required two passes in each case, which also corresponds to three rounds of interprocessor communication. The execution times for these four syn-thetic datasets are presented in Figs. 1 X 4, respectively.
 gle node, DFEKM does better than k -means. This is because this algorithm needs many fewer passes on the data. As expected, the relative speedup of parallel k -means is linear. In comparison, DFEKM sequentialises a part of the computation and does not scale that well. However, because it requires many fewer rounds of communication, increasing delays only have a modest impact on its performance. As communication delays increase, DFEKM outperforms parallel k -means on 2 and 4 nodes.
 tories is to down load all data at one node and apply a centralised clustering algo-rithm. Our results in Figs. 1 X 4 show that DFEKM on 2 and 4 nodes outperforms both k -means and DFEKM on 1 node. Thus, even without including the cost of down loading and storing data, DFEKM has better performance than applying a centralised algorithm.
 mance of parallel k -means and DFEKM . The dataset c20d100 was used for this experiment. On a 4-nodes configuration, we considered three different distribu-tions of data. In the first case, the fraction of data resident on each of the 4 nodes was 40%, 20%, 20% and 20%, respectively. In the second case, the fractions were 30%, 30%, 30% and 10%, respectively. In the third case, the fractions were 50%, 20%, 20% and 10%, respectively. No further delays were introduced.
 parallel k -means did better. This was because the performance of k -means depends on the slowest of the nodes. The fraction of the data with the slowest node was 40%, 30% and 50% for the three cases, respectively. Thus, we can see that, if the load imbalance is significant, DKEKM can outperform parallel k -means even in a tightly coupled configuration.
 databases, which we described in the previous section. Initially, we looked at the number of passes required by k -means and DFEKM . As shown in Table 7 ,the number of iterations for k -means were 18 and 20, whereas the number of passes required by DFEKM was 2 for both the datasets. The execution times with in-creasing communication delays are shown in Figs. 7 and 6 . The results are quite similar to those obtained from the synthetic datasets. As communication delays increase, DFEKM performs better on 2 and 4 nodes. Also, the performance of DFEKM on 2 and 4 nodes is better than sequential execution with either k -means or DFEKM . Again, this shows that, even without including the cost of down load-ing and storing data, DFEKM has better performance than applying a centralised clustering algorithm.
 in Figs. 8 and 9 . When the slowest of the 4 nodes has 30% of the data, parallel k -means performs better. However, in the cases when the slowest node has 40% or 50% of the data, DFEKM is better. 7 Conclusions We have presented, analysed and evaluated an algorithm that provably produces the same cluster centres as the k -means clustering algorithm and typically requires one or a small number of passes on the entire dataset. This can significantly reduce the execution times for clustering on large or disk-resident datasets, with no com-promise on the quality of the results. While a number of approaches existed for approximating k -means or similar algorithms with sampling or using a small num-ber of passes, none of these approaches could provably produce the same cluster centres as the original k -means algorithm. The basic idea in our algorithm is to use sampling to create approximate cluster centres and use these approximate cluster centres for speeding up the computation of correct or exact cluster centres. Our experimental evaluation on a number of synthetic and real datasets have shown a speedup between 2 and 4.5.
 This algorithm is suitable for analysing data that is distributed across loosely cou-pled machines. Unlike the previous work in this area, DFEKM provably produces the same results as the original k -means algorithm. Our experimental results show that DFEKM is clearly better than two other possible options for exact clustering on distributed data, which are down loading all data and running sequential k -means or running parallel k -means on a loosely coupled configuration. Moreover, even in a tightly coupled environment, DFEKM can outperform parallel k -means if there is a significant load imbalance.
 References
