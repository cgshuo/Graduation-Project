 Databased-FAQs (frequently asked questions) are important knowledge sources to many B2C (business-to-customer) companies because the FAQs help customers effi-ciently access various products of the companies. With the emergence of e-commerce systems [11], successful information access on e-commerce websites that accommo-dates both customer needs and business requirements becomes essential. As a useful tool for information access, most commercial sites provide customers with a keyword search. However, sometimes the keyword search does not perform well in sentence retrieval domains like FAQ retrieval, as shown in Fig. 1. 
In Fig. 1, the query  X  X ow can I remove my login ID X  and the FAQ  X  X  method to secede from the membership X  have a very similar meaning, but there is no overlap between the words in the two sentences. This fact often makes keyword search sys-tems misdirect users to irrelevant FAQs. The representative FAQ retrieval systems are FAQ Finder [2], Auto-FAQ [20], and Sneiders X  system [16]. FAQ Finder was designed to improve navigation through aleady existing external FAQ collections. To match users X  queries to the FAQ collec-FAQ Finder performs concept matching using semantic knowledge like WordNet [11]. Auto-FAQ matches users X  queries to predefined FAQs using a keyword com-parison method based on shallow NLP (natural language processing) techniques. Sneiders X  system classifies keywords into three types: required keywords, optional keywords and irrelevant keywords. Sneiders X  system retrieves and ranks relevant FAQs according to the three types. Although the representative systems perform well, high-level knowledge bases or handcrafted rules. 
There have been numerous studies on how clustering can be employed to improve clustering methods [4], [13] and query specific clustering methods [3], [17]. The static clustering methods group entire collections in advance, independent of the user X  X  query. The query specific clustering methods group the set of documents retrieved by an IR system for a query. The main goal of the query specific clustering methods is to improve the rankings of relevant documents on searching time. Some studies have shown that cluster-based retrieval did not outperform document-based retrieval, ex-cept with the small size of collection [1], [17], [18], [21]. In this paper, we propose a cluster-based FAQ retrieval system called FRACT (Faq Retrieval And Clustering Technique). FRACT consists of two sub-systems: FRACT/LSC and FRACT/IR. FRACT/LSC periodically collects and refines users X  query logs that are recorded in files or databases. Then, FRACT/LSC considers each FAQ as an independent category containing a sentence. Unlike ordinary text classifi-ers, FRACT/LSC groups the query logs by predefined FAQ categories only based on sentence-by-sentence similarities between FAQs and query logs without any training processes. Based on this reason, we can consider FRACT/LSC as a kind of unsuper-vised sentence classifier. When a user inputs his/her query, FRACT/IR calculates the similarities between the query and the FAQs smoothed by the query log clusters. According to the similarities, FRACT/IR ranks and returns a list of relevant FAQs. In spite of the skepticism with cluster-based retrieval, FRACT use clusters as a form of document smoothing [8] in order to resolve lexical disagreement problems. We ex-pect that FRACT will outperform traditional document-based retrieval systems be-cause the size of FAQ collections is probably much smaller than the size of ordinary document collections, as mentioned in [18], [21]. In addition, we expect that FRACT will be less sensitive to application domains than the previous FAQ retrieval systems because FRACT uses only statistical methods without additional knowledge sources. The current version of FRACT operates in Korean, but we believe that language con-version will not be a difficult task because FRACT uses only language-independent statistical knowledge. 
This paper is organized as follows. In Section 2, we propose a method of clustering query logs and a cluster-based FAQ retrieval method. In Section 3, we explain ex-perimental results. Finally, we conclude in Section 4. 2.1 Term-Document Matrix Construction In IR (information retrieval), sentences are generally represented as a set of unigrams, but the unigrams do not provide contextual information between co-occurring words. pendency bigrams (i.e. modifier-modified words) in order to provide further control over phrase formation. Unfortunately, automatic syntactic parser of a free-style text is still not very efficient, and the number of dependency bigrams extracted by parsing is not enough to measure similarity between sentences if we do not have a large corpus. Therefore, FRACT uses simple co-occurrence information extracted with a sliding and any of the other content words of the window. The window slides word by word from the first word of the sentence to the last, the size of the window decreasing at the end of the sentence so as not to cross boundaries between sentences. The window size being smaller than a constant, the number of extracted bigrams is linear to the number of unigrams in the sentence. In our experiment, we set the window size to three. Fig. 2 illustrates the process of term extraction with examples. 
After extracting indexing terms, FRACT assigns weight scores to each term ac-(1). query logs as short documents. i dl is the length of the i th document, and avdl is the according to Okapi BM-25 [14]. When sets of weighted terms have been constructed, it is very straightforward to construct an n m  X  term-document matrix n m X  X  , where m is the number of terms, n is the number of documents, and an element ij w indicates the degree of association between the i th term and the j th document. Fig. 3 shows the  X  term-document matrix that consists of q FAQ vectors and n-q query log vec-tors in m -dimensional space. 2.2 Latent Centroid Matrix Construction The similarities between documents can be calculated by popular methods such as the cosine measure, the Dice coefficients, Jaccard coefficients and the overlap coeffi-cients [12]. However, these popular measures may not be effective in calculating the similarities between sentences as there is often very little overlap between the words in the sentences. LSA (latent semantic analysis) is a method of extracting and repre-senting the contextual-usage meaning of words by statistical computations [5]. Some researchers have shown that LSA can bridge some lexical gaps between two words by Based on this fact, we apply the LSA techniques to FRACT/LSC in order to increase the performance of unsupervised sentence classification. 
To classify query logs into FAQ categories, FRACT/LSC first applies SVD (singu-lar vector decomposition) to the term-document matrix n m X  X  , as shown in Equation (2). where m m U  X  is an m m  X  orthonormal matrix, and n n V  X  is an n n  X  orthonormal 
In Fig. 4, the shaded portions of the matrices are what FRACT/LSC uses as the ba-the representation dimension. Therefore, the actual representations of the term and representation dimension, FRACT/LSC transposes the term-document matrix n m X  X   X  that is reconstructed on r -dimensions, as shown in Equation (3). Then, FRACT/LSC multiplies both sides on the right by r m U  X  , as shown in Equation (4). dimensional space called latent semantic space. 
After constructing the pseudo-document matrix, FRACT/LSC compares FAQ vec-tors to query log vectors in the latent semantic space by using cosine similarity meas-ure, as shown in Equation (5) [15]. In this step, we expect FRACT/LSC to calculate space. ql is the j th one among the other n-q query log vectors. is the dimension of the pseudo-document matrix. After calculating the cosine similari-ties, FRACT/LSC classifies each query log vector into categories of the FAQ vectors with the maximum cosine similarities. 
After classifying the query log vectors, FRACT/LSC generates centroid vectors of each FAQ category, as shown in Equation (6). vector of the category i cat . i qln is the number of query logs belonging to i cat , and is a threshold value. If the cosine similarity between a FAQ vector and a query log vector is smaller than the predefined threshold value  X  , FRACT/LSC considers the query log vector an uninformative vector and discards the query log vector. Other-wise, FRACT/LSC sums up all query log vectors belonging to i cat and generates an average vector between the i th FAQ vector and the summed query log vector. As shown in Equation (6), the average vector i c is not a real centroid vector. We expect that this summation method will prevent the cluster centroids from leaning exces-sively toward query logs that may be misclassified. Then, FRACT/LSC constructs the centroid matrix r q C  X  , where q is the number of FAQ categories and r is the reduced dimension, by gathering the centroid vectors. Next, FRACT/LSC restores the repre-gether, as shown in Equation (7). cause term weights should be generally bigger than zero. We call each element value 
C  X   X  latent term weights because the element values represent potential weight scores that are not directly calculated according to the actual occurrence of terms. We vectors in m q C  X   X  latent centroid vectors. 2.3 Cluster-Based Retrieval Using the Vector Space Model The vector space model has been widely used in the traditional IR field. Many search engines also use similarity measures based on this model to rank documents. The model creates a space in which both documents and queries are represented by vec-tors. Then, the model computes the closeness between a document and a query by using a vector similarity function. 
In the vector space model, given a query and FAQs, we can calculate the cosine similarity between the the i th FAQ vector i f and the query vector q in m -dimensional space, as shown in Equation (8). weights, we modify Equation (8), as shown in Equation (9). In Equation (9), ik cf is the estimated term weight of the k th term in the i th FAQ vec-tor. We calculate ik cf according to Equation (10). with the i th FAQ vector i f .  X  is a constant value for smoothing. 
FRACT/IR calculates the similarities between a query and FAQs using Equation (10), and then ranks relevant FAQs according to the similarities. We believe that FRACT/IR can reduce lexical disagreement problems, as FRACT/IR utilizes term weights smoothed by query log clusters including various terms. For example, if FRACT/IR has the latent centroid vector [goods:0.5, merchandise:0.1, method:0.1, return:0.5, send:0.1] associated with the FAQ  X  X ow to return goods X , FRACT/IR may highly rank the FAQ  X  X ow to return goods X  when a user inputs the query  X  X  method to send back merchandise X  because there are three common elements between the query vector and the latent centroid vector although there is no common element between the query vector and the FAQ vector. 3.1 Data Sets a nd Experimental Settings We collected 406 Korean FAQs from three domains; LGeShop (www.lgeshop.com, 91 FAQs), Hyundai Securities (www.youfirst.co.kr, 81 FAQs) and KTF (www.ktf.com, 234 FAQs). LGeShop is an internet shopping mall, Hyundai Securi-ties is a security corporation, and KTF is a mobile communication company. For two months, we also collected a large amount of query logs that were created by previous search engines. After eliminating additional information except users X  queries, we automatically selected 5,845 unique query logs (1,549 query logs from LGeShop, 744 query logs from Hyundai Securities, and 3,552 query logs from KTF) that consisted of two or more content words. Then, we manually classified query logs into the 406 FAQ categories. Finally, we constructed a test collection called KFAQTEC (Korean test collection for evaluation of FAQ retrieval systems). KFAQTEC consists of 6,251 sentences (406 FAQs and 5,845 query logs). The number of content words per query is 5.337, and the number of FAQ categories per content word is 0.342. Table 1 shows a sample of KFAQTEC. 
The manual annotation was done by graduate students majoring in language analy-sis and was post-processed for consistency. In KFAQTEC, we found 150 query logs that did not overlap with original FAQs at all. This fact reveals that these lexical dis-agreement problems can often occur in short document retrieval. 
To experiment with FRACT, we divided KFAQTEC into a seed data (FAQs), 10 training data (nine tenth of query logs per training data), and 10 testing data (a tenth domain. In detail, the seed data is used as both retrieval target sentences for FRACT/IR and category data for FRACT/LSC. The training data is used as both categorization-target sentences for FRACT/LSC and parameter-tuning data for FRACT/IR. As the result of parameter tuning only using the training data, we set the reduced dimension r in Equation (3) to 200, the threshold value  X  in Equation (6) to joint from both the seed data and the training data. In other words, the test data is a set of open sentences that was not used to develop FRACT. 3.2 Evaluation Methods To evaluate the performances of FRACT/LSC, we computed the precisions, the recall rates, and F-measures of query log classification. To evaluate the performances of FRACT/IR, we computed the MRRs (Mean Reciprocal Rank) and the miss rates. The given by each query, as shown in Equation (11) [19]. In Equation (11), i rank is the rank of the first relevant FAQ given by the i th query, FRACT/IR fails to return relevant FAQs, as shown in Equation (12). 3.3 Performance Evaluation To evaluate FRACT/LSC, we calculated the average precisions and average recall rates of two query log classifiers, as shown in Table 2. In Table 2, FRACT/COS is a query log classifier of the same framework as FRACT/LSC except the representation dimension. In other words, FRACT/COS term-document space. As shown in Table 2, FRACT/LSC highly outperforms FRACT/COS in the average recall rates. Based on the high recall rates, we can indi-rectly estimate that FRACT/LSC partially bridges the lexical chasms between sen-tences. In addition, FRACT/IR based on FRACT/LSC showed an increase in average MRR of 0.0919 and a high decrease in average miss rate of 0.16068 on FRACT/IR based on FRACT/COS. This fact reveals that the proposed latent term weights hold more effective information although the average precisions of FRACT/LSC are much lower than FRACT/COS. To evaluate FRACT/IR, it was unfair to directly compare the performances of FRACT/IR with those of the previous FAQ systems like FAQ Finder [2] and Auto-FAQ [20] because the performances of the previous FAQ systems were easily af-fected by high-level knowledge and handcrafted rules. Instead, we compared the av-erage performances of FRACT/IR with those of conventional IR systems because both FRACT/IR and the IR systems were fully automatic systems only using statisti-cal knowledge, as shown in Table 3. 
In Table 3, IDEAL-FRACT means FRACT/IR with ideal FRACRT/LSC of which both the precision and the recall rate are 1.0. TFIDF is the simple vector space model based on TFIDF weights [15]. OKAPI is the Okapi BM25 retrieval model [14], and KL is the KL-divergence language model using JM smoothing [22]. TFIDF, OKAPI, mechanism to introduce extra queries. We implemented these IR systems using Le-Mur Toolkit version 3.0 [7]. As shown in Table 3, FRACT/IR outperforms all com-parison systems except IDEAL-FRACT in both the average MRR and the average miss rate. Specifically, FRACT/IR highly reduced the average miss rate by 0.19568. Based on this experimental result, we conclude that the conventional IR systems only using FAQ sentences have critical lexical disagreement problems and the proposed FAQ retrieval. The difference between the performance of FRACT/IR and the per-formance of IDEAL-FRACT was 0.04968 MRR (0.01235 miss rate). This fact reveals that the more we can increase the performance of FRACT/LSC, the more we can increase the performance of FRACT/IR. Table 4 shows the changes of ranks on the basis of top-10 in comparison with OKAPI. 
As shown in Table 4, FRACT/IR made about 60.3 relevant FAQs ranked into top-10. Moreover, FRACT/IR ranked about 129.3 relevant FAQs in top-10 that OKAPI more than OKAPI. 
We analyzed the cases where FRACT failed to highly rank relevant FAQs. We found some reasons why the relevant FAQs were low ranked or missed. First, there were still the lexical disagreement problems between users X  queries and FAQs. FRACT could resolve some lexical disagreement problems because it used query log clusters in order to smooth the FAQs. However, we found many cases where there was very little overlap between the words in queries and the words in query log clus-ters. To solve this problem at a basic level, we need to study new methods that match users X  queries with FAQs on the semantic levels. Second, there were some cases where only one query was associated with several FAQs. In these cases, we could not information suppliers should accurately construct initial FAQs and should constantly update the FAQs. Finally, there were some cases where several relevant FAQs were much lower ranked, as compared with OKAPI. To solve this problem, we need to study new methods that effectively combine latent term weights with original term weights. We present a cluster-based FAQ retrieval system using LSA techniques. The FAQ retrieval system is divided into two sub-systems: a query log clustering system and a cluster-based retrieval system. During the indexing time, the query log clustering system gathers user X  X  query logs and classifies the query logs into relevant FAQ cate-gories in the latent semantic space. Then, based on the results of the classification, the query log clustering system groups the query logs and generates centroids represent-ing each cluster. During the searching time, the cluster-based retrieval system uses the queries and FAQs. In our experiment, the proposed system outperformed traditional IR systems in FAQ retrieval and resolved some of lexical disagreement problems. For further studies, we will study methods to apply query log information to the other IR models like statistical models and language models. This research was performed for the Intelligent Robotics Development Program, one of the 21st Century Frontier R&amp;D Programs funded by the Ministry of Commerce, Industry and Energy of Korea. 
