 This paper is concerned with the problem of question recom-mendation in the setting of Community Question Answering (CQA). Given a question as query, our goal is to rank all of the retrieved questions according to their likelihood of being good recommendations for the query. In this paper, we pro-pose a notion of public interest, and show how public inter-est can boost the performance of question recommendation. In particular, to model public interest in question recom-mendation, we build a language model to combine relevance score to the query and popularity score regarding question popularity. Experimental results on Yahoo!Answers dataset demonstrate the performance of question recommendation can be greatly improved with considering the public inter-est.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval X  information filtering, selection precess Algorithms, Experimentation Public Interest, Question Recommendation, CQA  X 
This work is supported by State Key Laboratory of Soft-ware Development Environment(SKLSDE-2011ZX-03).

Community Question Answering (CQA), in which users answer questions posted by others, is gaining a large number of audiences in recent years, and has accumulated millions of questions and answers over time. These CQA portals pro-vide an alternative channel for obtaining information on the web: rather than browsing results of search engines, users present detailed information needs and get direct response authored by humans [1].

With the exponential growth in data volume, the problem in CQA portals is how to answer users X  questions efficiently. It was observed that in Yahoo!Answers 1 only 17.6% of ques-tions received satisfied answers within 48 hours and nearly 20% of unresolved questions received no response [4], which showed that the popular CQA portal performs poorly in solving users X  questions directly. This problem gives rise to question recommendation techniques that help users locate interesting questions. To date, most previous work [4] [6] have been done to solve this problem by analyzing users X  information, such as the users X  interests, whether she is an expert or is available to provide an answer. Unfortunately, the fact is that users usually are reluctant to provide their personal information when they browse or search question. Thus, it is relatively hard to predict whether a question is interesting to a specific user when the profile information of the user is not available [6].

In the setting of CQA, many users share the same or similar interests. Usually,  X  X he wisdom of the crowds X  for information needs are expressed by some frequently asked questions. If these popular questions can be identified and be used for recommending, we believe it can save much time for most users either in crafting successful questions or ob-taining satisfied answers. Based on this observation, in this paper, we attempt to employ public interest to boost ques-tion recommendation performance. We try to model the question popularity to measure the public interest. Two perspectives are considered in the popularity measurement  X  possibility of question repeated by other people and users X  response to this question. To the best of our knowledge, this is the first attempt of learning to recommend questions from http://answers.yahoo.com/ this aspect. In the question recommendation framework, we first use the question popularity to model the public interest. Then we build a language model for question recommenda-tion, which combines relevance score to the query and pop-ularity score regarding question popularity.
An ideal question that should be recommended to a query should have 1) a high relevance score to the query; 2) a high question popularity score among related questions. As the question popularity score is independent to the query, it can be calculated offline. In this section, we first present our method on the question popularity measurement. Then we build a language model for question recommendation, which naturally combines the popularity score and relevance score.
We drive the popularity of a question from two observa-tions  X  the probability that the question would be repeated by other people and the people X  X  response to this question. The method is based on two assumptions:
Assumption 1: The higher possibility would be a question repeated by other people, the more popularity the question is.

Assumption 2: The more responses does a question re-ceive, the more popularity the question is.
If a topic is very interesting to people, there can be many lexically similar questions related to the topic in the ques-tion collection. Among those similar questions, the most central questions with the highest repeated possibility can be regarded as the most popular ones regarding the topic. To measure the question repeated possibility, we use the LexRank Method [3]. The LexRank estimates the central-ity(repeated possibility) of a question in a manner similar to the PageRank [2].

First, we construct an undirected graph of questions based on similarities between each two questions. As the question in CQA is usually a short sentence, it may occur sparseness problem when using cosine method to measure the similarity between questions. Since the answers in CQA systems are manually crafted by users, they are usually more comprehen-sive which can provide more context information than ques-tions. Intuitively, two questions should be semantically sim-ilar if their corresponding answers are similar, even though there is little word overlap between the two questions. Based on this observation, we use the answers similarity to improve the question similarity measurement.

Let i and j be two questions, ans ( i )and ans ( j )bethe answer of i and j respectively. The similarity between i and j is defined as: The function sim (  X  ) determines the lexically similarity be-tween two sentences. Clearly, there are many possible selec-tions for the similarity method, we choose cosine here and leave other methods for future work.

An undirected graph is constructed based on the similar-ities between questions. In the graph, each node represents a question and two nodes are connected if the similarity be-tween them exceeds a certain threshold value. In our work, we set the threshold to be 0.5. If the normalized similarity exceed 0.5, then there is an edge between the two questions.
Second, for a question u , the repeated possibility p ( u )is calculated with the following weighting scheme: where N is the total number of nodes in the graph, d is a  X  X amping factor X , which is typically chosen in the interval [0.1, 0.2] [2], and adj [ u ] is the set of questions that adjacent to u . S ( u, v ) is defined in eq.1. Erkan and Radev presented an algorithm for the eq.2. For more details, please see [3].
According to Assumption 2, the more responses does a question receive, the more popularity the question is. In this paper, we explore the answers statistic features to estimate the response information and leave other features such as user ratings or comments for future work. For a question q , the impact of response information w ( q )on q  X  X  popularity is definedas: where C denotes the collection of questions, an q is the num-ber of answers of q ,and f ( p, an p ) is defined as: where cutN is an integer which is supposed to be the largest number of answers of a question in a specific collection. Ac-cording to the statistic information of the question collection in our dataset, we set cutN to be 30.

Considering both impacts of question repeated possibil-ity and responses information, the final function of question popularity is defined as: Pop ( q )= d  X  w ( q )+(1  X  d ) where w ( q ) is defined in eq.3
Given a question as a query q , the recommendation model is defined as the probabilistic of generating query q from the question language model q as follows: As the likelihood P ( q )of q does not affect the ranking of questions so it can be ignored by the rank preserving the principle.

The generation probability P ( q | q ) can be decomposed into a unigram model:
In eq.6, P ( q ) is the prior probability of question q , re-flecting a static rank of the question, independent on query q . As the question popularity Pop ( q ) defined in section 2.1 is the likelihood of a question and independent on a specific query, we can use the question popularity Pop ( q ) as the prior probability of question q . So the probability of question q should be recommended to query q is defined as: where  X  is a constant factor to indicate the importance of question popularity, which will be discussed in experiments. The unigram probability P(t | q X ) is defined as eq.8: where and #( t, q ) denotes the time that term t appeared in q , denotes the length of q , C denotes the question collection,  X  is a smoothing parameter and set to 0.2 in our experiments, the same as [7][5].
We collected 17,680 questions in  X  X iet &amp; fitness X  category from Yahoo!Answers as the experiment dataset. We ran-domly select 50 questions as query questions and use dif-ferent methods to recommend questions to the query. Top 10 results of each query were judged by human. First, we employ different method to recommend questions to each query. For each method, we collect the top 10 results of each query. Then we combined different results together by query. Then an assessor is asked to label it with  X  X elevant X  or  X  X rrelevant X . If a returned result is considered as a good rec-ommendation for the queried question, the assessor will label it  X  X elevant X ; otherwise, the assessor will label it  X  X rrelevant X . A good recommendation of a query means that the recom-mended question should not only be relevant to the query but also be representative among the related questions.
We want to investigate the effectiveness of question popu-larity to the performance of question recommendation. Thus we compare the performance of the following methods. We use MAP (Mean Average of Precision), MRR (Mean Reciprocal Rank), and Precision@10 as metric to evaluate the performance of each method.

MAP calculates the mean of average precisions over a set of queries. MAP is calculated as: where Q r is a set of test queries, R q is the set of relevant questions for q , r is the rank, n is the number of returned results, here n =10, rel (  X  ) is a binary function on the rel-evance of a given function, and P (  X  ) is precision at a given cut-off rank.
 MRR is calculated as : where Q r is a set of test queries, r q is the rank of the first relevant question for q . Precison@n is the fraction of the top n questions recommend that are relevant, it is calculated as: where rel (  X  ) is a binary function on the relevance of a given rank, and here n =10.
In order to evaluate the effect of question popularity, con-trolled by the value of  X  , for question recommendation, we tested different settings of  X  with different question popular-ity estimation methods. Figure 1 gives the performance of Rec QP1, Rec QP2, Rec AP and Rec QAP with different settings of. From Figure 1 (a) and (b) we find that when is setting around 0.1, Rec QP1 performs best, around 0.3, Rec QP2 performs best, around 0.4, Rec AP and Rec QAP performs best. Table 1 reports each method X  X  performance measured by MAP, MRR and Precison@10. Each row reports the re-sults of a method and the improvement compared with other methods. Rec QAP: the combination of method (b) and (c) Table 1: Question recommendation performance on Yahoo! Answers dataset imp. Rec NoP 3.76% -3.52% 5.77% imp. Rec NoP 6.21% 6.03% -7.69% imp. Rec QP1 2.37% 9.90% -12.73% imp. Rec NoP 4.09% -1.21% 1.92% imp. Rec QP1 0.32% -2.39% -3.64% imp. Rec QP1 30.30% 30.00% -1.82% imp. Rec QP2 27.28% 18.29% 12.50%
In this paper, we propose to employ public interest to boost the performance of question recommendation. The question popularity is used to measure the public interest on a question. Two factors, named question repeated possi-bility and questions X  responses information, are examined in assessing question popularity. Experimental results on Ya-hoo!Answers dataset demonstrate that leveraging public in-terest can significantly improve the performance of question recommendation. In addition, utilizing similarity between answers and questions can provide more accurate questions popularity estimation and thus give a better question recom-mendation performance. Furthermore, questions X  responses information can also boost the performance of question rec-ommendation. Experiments demonstrate that the combina-tion of question repeated possibility and questions X  responses information gives a better estimation on question popularity and greatly boosts the performance of question recommen-dation. However, our experiments have conducted with one domain. Additional experiments on larger and heteroge-neous collections are essentially required for deep analysis of our proposed method. It will be our future work. [1] E. Agichtein, C. Castillo, and D. Donato. Finding [2] S. Brin and L. Page. The anatomy of a large-scale [3] G. Erkan and D. R. Radev. Graph-based lexical [4] B. Li and I. King. Routing questions to appropriate [5] A. S. M A Suryanto, Ee-Peng Lim and et al.
 [6] K. Sun, Y. Cao, X. Song, and et al. Learning to [7] C. Zhai and J. Lafferty. A study of smoothing methods
