 Most analysis of web search relevance and performance takes a single query as the unit of search engine interaction. When studies attempt to group queries together by task or session, a timeout is typically used to identify the boundary. How-ever, users query search engines in order to accomplish tasks at a variety of granularities, issuing multiple queries as they attempt to accomplish tasks. In this work we study real sessions manually labeled into hierarchical tasks, and show that timeouts, whatever their length, are of limited utility in identifying task boundaries, achieving a maximum pre-cision of only 70%. We report on properties of this search task hierarchy, as seen in a random sample of user interac-tions from a major web search engine X  X  log, annotated by human editors, learning that 17% of tasks are interleaved, and 20% are hierarchically organized. No previous work has analyzed or addressed automatic identification of interleaved and hierarchically organized search tasks. We propose and evaluate a method for the automated segmentation of users X  query streams into hierarchical units. Our classifiers can improve on timeout segmentation, as well as other previ-ously published approaches, bringing the accuracy up to 92% for identifying fine-grained task boundaries, and 89-97% for identifying pairs of queries from the same task when tasks are interleaved hierarchically. This is the first work to iden-tify, measure and automatically segment sequences of user queries into their hierarchical structure. The ability to per-form this kind of segmentation paves the way for evaluating search engines in terms of user task completion.
 H.3 [ Information Storage and Retrieval ]: Query formu-lation  X  This work was conducted while this author was at Yahoo! Inc Algorithms, Experimentation, Measurement query log segmentation, query session, query session bound-ary detection, search goal
Web search engines attempt to satisfy users X  information needs by ranking web pages with respect to queries. But the reality of web search is that it is often a process of querying, learning, and reformulating. A series of interactions between user and search engine can be necessary to satisfy a single information need [18].

To understand the way users accomplish tasks and sub-tasks using multiple search queries, we exhaustively anno-tated 3-day long query sequences for 312 web searchers. We limited the duration to three days to allow complete anno-tation of every query sequence, with an extremely thorough approach. These spans of time allowed us to identify tasks which result in queries placed over multiple days, as well as multiple tasks which may occur over several days. We manually annotated these query sequences with tasks and subtasks (which we will define as missions and goals ), find-ing that many tasks contained subtasks, and many different tasks and subtasks were interleaved. While previous work has examined the way users interleave tasks [9], no previous work has examined the way tasks contain subtasks.
If we are able to accurately identify sets of queries with the same (or related) information-seeking intent, then we will be in a better position to evaluate the performance of a web search engine from the user X  X  point of view. For example, standard metrics of user involvement with a search engine or portal emphasize visits or time spent [1]. However, each page view can constitute small pieces of the same information need and each visit could encompass some larger task. If we could instead quantify the number of information needs or tasks which a user addresses via a website, we would have a clearer picture of the importance of the site to that user. In particular, we could evaluate user effort in terms of queries issued or time spent on a task, as the user attempts to satisfy an information need or fulfill a more complex objective.
To this end, we built classifiers to identify task and sub-tasks boundaries, as well as pairs of queries which corre-spond to the same task, despite being interleaved with queries from other tasks.
Our contributions include (1) analysis of typical timeouts used to divide query streams into sessions, and demonstra-tion that they are less than optimal for this task (2) hierar-chical analysis of user search tasks into short-term goals and longer-term missions (3) a detailed study of the frequency and patterns of real user queries forming extended and in-terleaved tasks which can be analyzed as missions and goals in this hierarchy (4) a comparison of previously published feature sets on our data and tasks, and (5) a list of fea-tures going beyond timeouts and previously published fea-ture sets that can be used effectively to identify goal and mission boundaries, and pairs of non-adjacent queries be-longing to the same goal or mission.

In Section 2, we discuss related work both in defining  X  X essions X  and automated segmentation of query logs into tasks and sessions. In Section 3 we provide our definitions, detail on the manual annotation of our data, statistics on the missions and goals we find, and show that time-based thresholds are of limited accuracy in identifying task bound-aries. In Section 4 we introduce the supervised classification which we perform to improve task identification, as well as the features and methods we use. In Section 5 we show that a model combining feature types can identify goals and missions with extremely high accuracy, even when they are interleaved. We also discuss performance of the individual features on the classification tasks. Lastly, in Section 6, we discuss conclusions and future directions for the work.
In library search systems,  X  X essions X  were easy to identify: users logged in for a single task then logged out again, so login IDs could be used. Thus historically a session was simultaneously (1) a set of queries to satisfy a single in-formation need (2) a series of successive queries, and (3) a short period of contiguous time spent querying and examin-ing results. On the internet, however, we seldom have users logging in and out on a task-by-task basis. In addition, iden-tifiers such as IP addresses and cookies may be shared by multiple users, as in the case of a shared computer. Thus the term session has necessarily been split between these various meanings, sometimes used for one, sometimes for another.
For web search, there have been a number of conflicting attempts to segment and define sessions, which don X  X  di-rectly address the idea of user information needs, but do rely on a notion of similar context, topic, or temporal char-acteristics. Many of these use the idea of a  X  X imeout X  cutoff between queries. A timeout is the time between two succes-sive activities, and it is used as a session boundary when it exceeds a certain threshold. Often sessions are identified us-ing a 30-minute timeout, apparently following Catledge and Pitkow X  X  1994 work, which claimed to find a 25.5 minute timeout based on user experiments[4]. We will show in Sec-tion 3.3 that this threshold is no better than random for identifying boundaries between user search tasks.
Other time cutoffs have been proposed, from 5 to 120 minutes [11][17][6][2]. Montgomery and Faloutsos [11] tried several cutoff values, but found that the choice of cutoff did not matter. Additionally, a variety of mean session lengths (number of queries in a session) have been found, most rang-ing between 2-3 queries per session[17][8]. Mean session du-rations (amount of time a session lasts) of 5 and 12 minutes have been reported [6][8]. In Section 3.3 we look at all of these timeout thresholds applied to real search engine data, and find that no time threshold is effective at identifying task boundaries.

Jansen et al. [8] defined a session as X  X  series of interactions by the user toward addressing a single information need X , and found experimentally that sessions were better identified by query content  X  a single word in common between queries  X  than by temporal cutoffs. Spink et al. by contrast [19] discuss topic switching and multitasking in two and three query sessions, implicitly defining a session as a sequence of queries from a web-searcher, independent of the information need.

A few researchers have worked on automatically detecting session boundaries. He et al. [6] tried to detect topic shifts in user query streams, devising an algorithm that checks for deleted or added terms in queries thereby specifying a few categories of user behavior, which are also applied in Oz-mutlu and Cavdur [12]. Ozmutlu et al. [13] [14] used the same categories and algorithm but a different classifier. All of these rely solely on the feature of words in common be-tween queries, and rewrite classes such as generalization and specification . There was no manual labeling of ground truth, and none of them addressed the interleaved, nested nature of these topic shifts. Both cases rely solely on the feature of words in common between queries, and used continuous values to predict rewrite classes such as generalization and specification, defined in terms of word insertion and deletion, and they did no manual labeling of ground truth. None of them addressed the interleaved, nested nature of these topic shifts.

A separate body of work models the formal syntax of users X  interactions with the search engine, rather than mak-ing distinctions regarding what they seek. An entire list of these approaches to date may be found in Downey et al. [5], two of which deserve more detail.

Lau and Horvitz [9] manually assigned queries into a few classes, including one to account for interleaving, labeling each query in the context of the previous query. They exam-ined the relationship between the inter-query time interval and these classes in order to better use timeouts to predict topic transitions in a probabilistic Bayesian network.
Radlinski and Joachims [15] identified sequences of queries on the same topic. The authors manually grouped sequences of queries into topics that had no hierarchical or interleaved structure. They built a classifier using features both based on shared words in the queries themselves, and shared words in documents retrieved in response to the queries, but judged a 30-minute timeout to be accurate at over 90%, and thus used only that feature to group their chains. These results were based on a library search engine, however, as we will show in Section 3.3 their results with timeouts do not ap-ply to a general purpose search engine, where users often interleave topics, and time is not a good indicator of task completion. In Section 5.1 we will demonstrate the perfor-mance of their word and time features on our tasks, and show that this is the best of previous methods for our tasks.
We will show in Section 5 that time combined with some of the features proposed by previous authors can be used to classify queries into their hierarchical structure: thus demonstrating for the first time that we can identify user tasks and their sub-tasks, even in the face of task inter-leaving. We will also show that by adding novel query-log features we can perform even better on the original tasks, as well as our novel tasks.
In this section we define search goals and missions, then describe the way we manually annotated them. We perform an exploratory analysis and demonstrate that time-outs are a poor way of identifying boundaries related to user tasks.
Definition 1. A search session is all user activity within a fixed time window.
 A session, for us, is just a slice of user time. Other defini-tions (which conflict among themselves) involve an absence of periods of inactivity, or imply a single information need on the part of the user [4], [8], [2]; ours does not, since we will be more specific with the terms goals and missions , de-fined below, and use inactivity as a predictor, rather than as a definition.

Definition 2. A search goal is an atomic information need, resulting in one or more queries.
 A goal can be thought of as a group of related queries to accomplish a single discrete task. The queries need not be contiguous, but may be interleaved with queries from other goals (e.g., a user who is both looking for work-related in-formation and information for the evening X  X  entertainment).
Definition 3. A search mission is a related set of infor-mation needs, resulting in one or more goals.
 A mission is then an extended information need (such as finding a variety of interesting hiking spots, for a searcher interested in hiking). In Figure 1, we see the resulting hi-erarchy: sessions containing missions, which contain goals. A goal may have multiple queries, and a mission may have multiple goals. Temporally, these can all be interleaved.
We sampled 3 day-sessions for 312 users who submitted a query to the Yahoo! Search engine during a week of mid-2007. The sampling was stratified over days of the week, so we did not have particular days of the week overrepre-sented. The three day time period was deemed long enough to capture extended search patterns for some users, exceed-ing typical 30-minute timeouts, and allowing for goals and missions to extend over multiple days. While it is the case that some missions or goals may start or end before or after the 3 day window of queries, this truncation should occur randomly, and thus introduce no systematic bias in the sub-ject of the mission or goal. Since none of our features depend on the position of the query in the mission or goal, our model should not be affected. However, our density estimates for mission and goal lengths will necessarily be truncated at 3 days.

A group of annotators were instructed to exhaustively ex-amine each session and  X  X e-enact X  the user X  X  experience. 1 The annotators inspected the entire search results page for each query, including URLs, page titles, relevant snippets, and features such as spelling and other query suggestions. They were also shown clicks to aid them in their judgments. They were asked to then use their assessment of the user X  X  Figure 1: Sample hierarchy of user missions and goals, which correspond to some of the goals in the sample session of Table 1. objectives to label each session so that every query belonged to a goal and every goal to a mission. Each unique goal and mission was given an ID number and a description reflecting the user X  X  objective. Each query was labeled with a goal and mission ID number. A guideline for queries to belong to the same goal was that they have the same criteria for  X  X uccess X , in terms of satisfying the user X  X  information need.
This is somewhat similar in nature to the editorial task in [7], though the annotators did not re-submit queries, they examined the query logs as input by the users themselves. It differs somewhat from manual annotation methods in [8] and [15] and [13] in that the criteria for grouping queries is, not only hierarchical in nature, but tied to the users X  intents, as opposed to just clustered in terms of  X  X elatedness X  to an unspecified degree.

A strength of this approach is that the data is recorded without any intervention, and as such can complement lab-oratory and field studies. Additionally, we are able to look at a large number of users at one time, over an extended pe-riod of time. While it is clearly possible that editors may be biased in their interpretation of a user X  X  missions and goals, preliminary results in [7] seem to indicate that this method can achieve reliable results. One of the future directions for this work involves obtaining measures of inter-rater reliabil-ity for the editorial work, as well as studying editorial bias against user self-reporting.

Our aggregate data consists of 312 user sessions, with 1820 missions, 2922 goals and 8226 queries. In Table 1 we see a sample sequence of user queries, annotated with goals and missions. Then timestamps are also given -note that many queries from the same goal are separated by several minutes, while a goal-change (goal 3 to goal 4) takes place within three seconds.
In Table 2 we see a summary of goal and mission lengths, in terms of number of queries, and elapsed time from first to last query in the goal or mission 2 . Median goal and mission length of two queries is in line with the findings of [8] for ses-sion lengths based on segmentation by query topic. Figure 2 in our data 17% of missions were interleaved.
 Table 2: Summary statistics about missions and goals. The distributions of number of queries and task duration can be seen in Figures 2 and 3. shows the density of number of queries per goal and mission. Density plots show Gaussian kernel density estimates, with bandwidths chosen by Silverman X  X  rule [16]. 63% of goals are under one minute, but 15% spanned 30-minute periods of inactivity. This means that a 30 minute time-out will break up 15% of goals. The density of goal and mission durations are in Fig 3.

Most goals and missions have few queries, though a few have many queries. Some missions lasted the entire 3 day session, and those users appeared to make related or re-peated queries an average of every few hours, in some cases looking at baby names, or checking up on favorite television stars. Recall that missions and goals may be interleaved, so these long durations do not necessarily entail continuous engagement at some task. 16% of goals are revisited or inter-leaved with other goals, and 17% of missions are revisited or interleaved with other missions. Of the interleaved missions, 41% contained multiple goals, whiled 59% contained a single goal (which was itself interleaved with a goal from another mission). It is not surprising that users would repeat infor-mation needs; Teevan et al [20] found that 40% of queries are  X  X e-finding X  queries. In addition, we find that 20% of missions contain multiple goals. An example mission con-taining multiple goals consisted of wedding planning queries, for wedding gowns, invitations, and wedding planning lists. We also see the evolution of users X  shopping intent over the course of a mission, with a query for  X  X ridal dresses X  on one day, and another query in the same mission the following day, containing  X  X ridal dresses X  and the name of a bridal dress store. The user is moving from learning about general options to bridal dresses, to looking for a particular store to buy the dress.

Thus any task-segmenting approach which does not con-sider the hierarchical and interleaved nature of search tasks will break up tasks which belong together.

In preliminary experiments we found many queries re-peated immediately after one another, representing either the user re-issuing the query, hitting the  X  X ext X  button, re-freshing the page, or an automatic resubmission on the part of the browser. Removing the repeated queries decreased the total number of queries from 8226 to 6043, thus just over a quarter of all queries were repeats of the previous query.
Most previous work has used temporal features, commonly a  X  X imeout X : an elapsed time of 30 minutes between queries which signifies that the user has discontinued searching. How-ever, on our data time does not appear to be an especially good predictor, particularly of goal boundaries. Precision for different values of inter-query time-lag are shown in Fig-ure 4.

In Table 3 we see that a 30-minute threshold on inter-query interval is more accurate than the baseline (always guess there is no task boundary between each sequential Figure 2: Density for the number of queries per goal, and number of queries per mission, on a log plot. Figure 3: Density for the time span of goals and missions, in minutes, on a log plot.
 Figure 4: As we increase the inter-query interval threshold, the precision at identifying mission and goal boundaries increases, however, we do not see precision much above 70% for identifying missions and above 80% for identifying session boundaries. pair of queries). Training a threshold (thirteen minutes) does not improve the accuracy for mission boundary identi-fication, but the learned threshold of just under 5 minutes improves goal boundary identification from 67% accuracy at a 30 minute threshold, to 71%. All differences are statisti-cally significant. Our conclusions agree with those of [11] that multiple threshold choices give similar accuracy.
Clearly, while using session timeouts can achieve task break-ing with accuracy better than assuming there are no breaks, in general there is no ideal choice of threshold, and us-ing time the precision is capped at 70-80%, depending on whether we seek goal or mission boundaries. The 30-minute standard receives no support from our results. In the next Table 3: For data which includes repeat queries: In-sample accuracy at predicting goal and mission boundaries, as well as same-goal/mission, using inter-query thresholds alone. Trained times for goal and mission boundaries were 5 mins and 13 minutes , respectively. section we will show that we can greatly improve on task segmentation by considering multiple predictors, going be-yond inter-query time to include properties of the queries themselves.
In this section we describe our formulation of automatic detection of search goals and missions as a supervised ma-chine learning task. We then describe the features we use in our experiments for identifying goals and missions, and the classifiers we use to learn to recognise them. In particular, we introduce a way of identifying when queries belong to the same goal or mission, despite being interleaved, something not addressed in any previous work.
If goals and missions are not interleaved, as has been as-sumed by previous work, it suffices to find a boundary be-tween one task and the next. To do this we can look at each sequential pair of queries and ask whether this pair straddles a boundary. Thus we look at task boundary detection . This task has been addressed by previous approaches for identify-ing goals, so we can examine how well previous approaches work on our data. Previous work has not considered this task for higher-level missions, but we can also consider the efficacy of their features on this novel problem.
In order to address interleaved missions and goals, we must consider all possible pairs of queries, and consider whether the pair of queries come from the same task. Cor-rectly performing this task will allow hierarchichally orga-nized interleaved goals and missions to be correctly identi-fied. We call this same-task identification. No previous work has addressed this problem.
Each pair of sequential queries from a user is a possi-ble boundary between goals. Thus we seek to take each such pair and decide whether the pair crosses a boundary between goals, i.e., whether the two queries come from dif-ferent goals. This is the task traditionally addressed using timeouts. Formally we consider the task: { X  q i ,q j  X  : ( t ( q i ) &lt;t ( q j )) ^ ( 6  X  q k : t ( q where t ( q i ) is the time query q i was issued by the user.
Of our original 8226 queries including repeats, some begin and end user sessions, so we trivially ignore these boundaries and wind up with 7914 pairs of sequential queries. Of these 3622 were goal boundaries, so we should guess a position is a boundary 45.8% of the time, and a non-boundary otherwise. Thus our baseline is 54.2% accuracy.

As with goal boundaries, any sequential pair of queries can mark the transition from one mission to another. (Note that a mission boundary is always a goal boundary.) Our baseline is 71% as 5608 of 7914 pairs of sequential queries are not mission boundaries.
Since goals can be inter-leaved, any pair of queries from the same user could be from the same goal. We seek to learn a classifier to take a pair of queries and map it to a 1 if they are from the same goal, and a 0 of they are from different goals. We consider all pairs of queries q i , q j such that q issued before q j : where t ( q i ) is the time query q i was issued by the user.
Since we are considering all pairs of queries, not just se-quential ones, there are many more instances to consider. We see 305,946 pairs of queries, of which 278,152 or 91% are not for the same-goal so our baseline accuracy is 91%. Using the same definitions for same-mission our baseline is 67.5% since 67.5% of 305,946 query pairs were from different mis-sions. Any pair of queries corresponding to the same goal will also correspond to the same mission.

Solving these two problems, same-goal and same-mission for arbitrary pairs of queries from a user X  X  query stream will allow us to identify their complete set of tasks, even those that are nested and hierarchical.
We experimented with many features from the following four general types: temporal, edit-distance, query log and web search. Below we give details of those that contributed to classification efficacy. Temporal features have been com-monly used in previous work ([11][17][6][2]); edit distance has been used in several previous works ([6],[14]) and web search features have also been used in previous work [15]. No previous work has used query log session features, and we will show that combinations of these four types of fea-tures provide superior performance on the boundary detec-tion problem, and superior performance on the previously untackled same-task problem.
While we showed in Section 3.3 that timeouts alone are poor predictors of task boundaries, they may be helpful in conjunction with other features. Temporal features we ex-perimented with are: Table 4: Word and Character Edit Features used for predicting goal and mission boundaries and cooccur-rence on query pairs.
 Table 5: Pairs of queries which occur in user query sequences much more than would be expected by chance, along with the log-likelihood ratio score.
Sequences of queries which have many words and/or char-acters in common tend to be related via a query reformu-lation, for example word insertion or deletion [15, 18]. In addition, related queries from the same goal or mission may have some words in common. Character-edit distance can capture spelling variants and common stems, while word-level features capture common words.

Specific features are shown in Table 4.
Sometimes goals and missions may contain pairs of queries which are semantically related, but which do not share any terms. For example,  X  X ew york hairdresser X  and  X  X ribeca sa-lon X  may be from the same goal: looking for a hair-stylist in New York City. To try to capture these semantic rela-tionships, we use a separate data source 3 to identify pairs of queries,  X  q i ,q p  X  , which occur together much more than chance, which we test using the log-likelihood ratio score [10] (LLR).

Because we test millions of pairs of queries, we can have false positives of coincidentally cooccurring query pairs. A threshold level for the LLR was chosen in order to control false positives, including adjustment for multiple testing, correcting a standard chi-square cutoff for 95% significance. Sample query pairs which pass this threshold are shown in Table 5. After thresholding on the LLR, we use a number of features related to the frequencies and probabilities of seeing the query pair together. Specific features which used rewrite probabilities for the query pair  X  q 1 ,q 2  X  are shown in Table 6.
Two weeks of pairs of sequential queries to the search en-gine, in 2006. Table 6: Query Log Features used to help identify goal and mission boundaries p ( q 1  X  q 2 ) is the proba-bility q 1 is reformulated as q 2 in a large query log.
We include features which depend on the documents re-treived by the search engine for the queries. Similarity be-tween a query pair is measured by commonalities among the terms or characteristics of those documents.
We use a logistic regression model, with 10-fold cross-validation. In order to better handle feature selection for large sets of correlated features, we also tried LASSO, which includes regularization, and CART decision trees, however, we achieved similar results in both cases. When performing feature selection for small subsets, we used an exhaustive search of linear models with Akaike Information Criterion (AIC) to select the best features (also known as all subsets regression).
 Table 7: For data which includes repeat queries: normalized Levenshtein distance dominates other features for most tasks. Thanks to the large number of examples in our tests, all differences are statisti-cally significant.
 Table 8: Prediction accuracy based on features pro-posed in previous work. Trained time thresholds for boundaries were 1.5 mins for goals and 6 mins for missions. For identifying same-goal 17.2 mins and same-mission 47 mins . The best performing previously published feature combination is com-monw+prisma+time. All differences are statisti-cally significant.
Used alone, Levenshtein character edit distance did well in 3 of 4 tasks, especially when repeat queries were included (Table 7). However, a quarter of all sequential query pairs consisted of identical queries, belonging to the same goal or mission. Thus much of our data was easy to classify with Levenshtein distance (a distance of zero). 4 In this section we address the harder problem of classifying those query pairs which are not repetitions. Our baselines, with repeated queries removed, are 63% for goal boundary identification, 60% for mission boundary, 95% for same-goal identification, and 71% for same-mission.
In this section we give results using features proposed in previous work, applied to our data. To compare to ap-proaches using time, we use both a thirty-minute thresh-old, as well as time thresholds learned using cross-validation. To compare with word-insertion and deletion approaches ([6],[14]), we use the word-edit feature commonw , and to compare to word-edit, web-result and time features we use commonw+prisma+time [15]. In all cases we trained a lo-gistic regression function on training data, and used ten-fold cross-validation for testing.

We see in Table 8 that on web search data, Radlinski and Joachims X  result does not hold, that a thirty-minute threshold obtains similar results to a combination of com-mon words and web-search result similarity [15]. We see that our re-implementation of their classification features, using commonw+prisma+time, is the best-performing previously published approach. Table 9: Accuracy at predicting mission and goal boundaries, using the most predictive models, as judged by exhaustive search of logistic regression models with AIC as selection criterion. com-monw+prisma+time is the best performing previ-ously published feature combination for the goal boundary task. All differences are statistically sig-nificant.
We are able to build highly accurate classifiers for goal and mission boundaries as well as identifying pairs of queries from the same goal or mission. When we combine all four types of features we achieve best results, as shown in Tables 9 and 10.

In these tables we summarize features and performance for learned models in the no-repeat cases, as well as folding in the repeated-queries under the assumption that a query repeat is a non-boundary (which is correct in all cases). We see that our best model exceeds the best previously pub-lished feature combination on our data for the task bound-ary problem, as well as providing strong results for the new problem of identifying query pairs as being from the same task, even when interleaved and hierarchically organized.
We see that after removing repeated queries, we clearly gain from a combination of features in all four groups: edit-based, query-log, web-search and time. The models were se-lected via exhaustive search of all models with 8 predictors, using AIC as the selection criterion. While these models contain small subsets of the original features, the accuracy was comparable to that obtained with all features. We re-stricted this exhaustive search to 8 features as when we used subsets larger than 8 features, the feature selection began to become computationally extremely expensive. When we fold back in the instances of repeated queries, we can compare our results to those in Table 7 where we trained with the re-peated queries. While the best accuracy for goal boundaries with timeouts was 62.5%, this combined model gives an ac-curacy of 93.0%, improving on edit distance features alone, which gave an accuracy of 89%. For mission boundaries, the combined model greatly outperforms the time-interval and edit-distance models, lifting from 67.6% and 79.7% to 90.8%.
We showed above that by using a combination of four types of features, we could outperform time-based segmen-tation (which we showed achieved accuracies of around 70%, Table 10: Accuracy at predicting same-mission/goal, using the most predictive models, as judged by exhaustive search of logistic regres-sion models with AIC as selection criterion. All differences are statistically significant.
 Table 11: Prediction accuracy for features based on edit distance between queries alone. All differences are statistically significant.
 Table 12: Prediction accuracy based on edit distance between queries as well as inter-query interval. The latter does not appear to help. All differences are statistically significant. compared to our accuracies of around 90%). We also showed that by using 8 features from the four types edit-distance, web-search, query log and time, we could improve over the best previously published combination of features (commonw+prisma+time). In this section we examine the contribution of each of the feature types separately.
Even after removing repeated queries, Levenshtein char-acter edit distance is the best edit-based feature for iden-tifying goal boundaries (Table 11), and words-in-common (commonw) is the best edit-based feature for identifying mission boundaries as well as same-mission. Ratio of words in common (Jaccard distance) performs best for same-goal. In general, this group of features performs well for all four tasks. Adding time to character edit distance does not help in identifying goal or mission boundaries (Table 12). This is similar to most previous work on identifying task boundaries ([6],[14]). Table 13: Prediction accuracy using best-performing query session cooccurrence features.
 These improve over baseline for boundary identi-fication, but not the same-goal/mission task. All differences are statistically significant.
 Table 14: Prediction accuracy with Prisma vector similarity, the best-performing web search feature we tried.
End-of-session features help with mission boundary iden-tification (Table 13), and probability of being rewritten does slightly better at detecting goal boundaries, but other query log features in isolation do not improve over the baseline. It X  X  possible that the probability of the first query being rewritten may indicate that the second query is more likely to be a reformulation of the first, and thus part of the same goal. One can imagine that  X  X ast query of the day X  might be useful as an indicator for the last query in a task, since often people will finish up a task before turning in for the day. In our external data-source, days were truncated at midnight. We may be able to improve the suitability of query log fea-tures for this task by considering different ways of breaking the data using other markers.
Of the web search features we tested, the Prisma score outperformed all others (Table 14). However, unlike those other features, as well as the query log features, it had not been pre-computed for the more common queries, to save computational cost. This makes it slower to obtain, yet gives it superior coverage.
We have shown that a diverse set of syntactic, tempo-ral, query log and web search features in combination can predict goal and mission boundaries well. (When used inde-pendently, word and character based features perform best). Our classifiers achieve at least 89% accuracy in all four tasks, and over 91% in all but one task, matching within the same goal. Additionally, we X  X e shown that the task of match-ing queries within the same interleaved goal or mission is harder than identifying boundaries. This may indicate that the best approach to clustering queries within the same goal or mission may build on first identifying the boundaries, then matching subsequent queries to existing segments. It may also be effective to use multi-task machine learning to join the tasks of identifying mission and goal boundaries to-gether.

The utility of adopting a hierarchical model for the group-ing of user queries will allow us to more easily model what type of task the user may be doing when querying, e.g. is the user performing a series of searches with information needs which are the same, or are the information needs only peripherally related? This may help us determine when the user is performing a more complicated task, vs. a simpler task. Including the interleaving in the model allows us to more accurately measure the length of time or number of queries a user needs to complete tasks. If we ignore the fact that a more involved task may be interrupted with other needs for information, we lose the ability to model these more involved tasks.

Our work sets the stage for evaluating search engines, not on a per-query basis, but on the basis of user tasks. In fu-ture work we will combine task segmentation with prediction of user satisfaction, which opens up the possibility of truly understanding how web search engines are satisfying their users.
Thanks to Isabella Barbier, Tony Thrall, Ron Lange, Ben-jamin Rey, Dan Fain and the anonymous reviewers. [1] Comscore announces new  X  X isits X  metric for measuring [2] P. Anick. Using terminological feedback for web search [3] P. G. Anick. Automatic Construction of Faceted [4] L. Catledge and J. Pitkow. Characterizing browsing [5] D. Downey, S. Dumais, and E. Horvitz. Models of [6] D. He, A. Goker, and D. J. Harper. Combining [7] S. B. Huffman and M. Hochster. How well does result [8] B. J. Jansen, A. Spink, C. Blakely, and S. Koshman. [9] T. Lau and E. Horvitz. Patterns of search: Analyzing [10] C. D. Manning and H. Schutze. Foundations of [11] A. Montgomery and C. Faloutsos. Identifying web [12] H. C. Ozmutlu and F. Cavdur. Application of [13] H. C. Ozmutlu, F. Cavdur, A. Spink, and S. Ozmutlu. [14] S. Ozmutlu. Automatic new topic identification using [15] F. Radlinski and T. Joachims. Query chains: learning [16] B. W. Silverman. Density Estimation . Chapman and [17] C. Silverstein, M. R. Henzinger, H. Marais, and [18] A. Spink, B. J. Jansen, and H. C. Ozmultu. Use of [19] A. Spink, M. Park, B. J. Jansen, and J. Pedersen. [20] J. Teevan, E. Adar, R. Jones, and M. Potts. History
