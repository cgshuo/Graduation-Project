 We consider the problem of segmenting text documents that have a two-part structure such as a problem part and a solution part. Doc-uments of this genre include incident reports that typically involve description of events relating to a problem followed by those per-taining to the solution that was tried. Segmenting such documents into the component two parts would render them usable in knowl-edge reuse frameworks such as Case-Based Reasoning. This seg-mentation problem presents a hard case for traditional text segmen-tation due to the lexical inter-relatedness of the segments. We de-velop a two-part segmentation technique that can harness a corpus of similar documents to model the behavior of the two segments and their inter-relatedness using language models and translation models respectively. In particular, we use separate language mod-els for the problem and solution segment types, whereas the inter-relatedness between segment types is modeled using an IBM Model 1 translation model. We model documents as being generated start-ing from the problem part that comprises of words sampled from the problem language model, followed by the solution part whose words are sampled either from the solution language model or from a translation model conditioned on the words already chosen in the problem part. We show, through an extensive set of experiments on real-world data, that our approach outperforms the state-of-the-art text segmentation algorithms in the accuracy of segmentation, and that such improved accuracy translates well to improved usability in Case-based Reasoning systems. We also analyze the robustness of our technique to varying amounts and types of noise and em-pirically illustrate that our technique is quite noise tolerant, and degrades gracefully with increasing amounts of noise.
 I.2.7 [ Artificial Intelligence ]: Natural Language Processing X  Text Analysis, Language Models Text, Segmentation, Language Models, Translation Models
Experience reports are prepared in various contexts for various purposes. An example is a bug report in software development which describes a sequence of events starting from events that char-acterize the occurence of the bug, and events involving fixing of the bug. Incident reports in other scenarios comprise descriptions of a temporal sequence of events, some of which characterize the cause of the incident, followed by those that describe the way the incident was dealt with. A real incident report from an airline company is shown below:
A Nexen employee checked his rebreather unit and found it to be past its "Service Due Date". He alerted the other passengers and they found the same situation with their re-breather units. All the units were promptly changed by the heliport staff and the flight proceeded as normal.

Of this, the first two sentences describe the (symptoms of the) incident, and the third indicates how the incident was resolved. In many cases, however, the events describing the problem and those describing the solution could be interleaved, especially at the boundaries. In clinical diagnosis, medical transcriptionists main-tain diagnosis reports; these often have a similar two-part structure with a symptoms part and a diagnosis part.
 School reports continuing difficulties with repetitive questioning. Obsession with cleanness on a daily basis. Inability to relate this well in the classroom. Asperger disorder. Obsessive compulsive disorder.

In the above sample medical transcription report (from MTSam-ples 1 ), the first three sentences talk about symptoms whereas the remaining indicate diagnosis. In most cases where the diagnosis is complete, the two-part structure is very apparent. Root-cause Analysis (RCA) reports generated from service delivery organiza-tions also have a two part structure (problem and solution).
Though such reports are mostly used currently for auditing and tracking purposes, it is easy to see that they provide a wealth of in-formation that could be exploited in knowledge reuse systems such as Case-Based Reasoning 2 . CBR systems make use of problem-solution repositories to find possible solutions to new problems. For a newly posed problem, similar problems are retrieved from the repository and the solutions associated with them are deemed to be usable for the new problem. For example, a CBR system that uses symptom-diagnosis cases would respond to a new symptom report with historical diagnoses of similar symptoms and could help a medical practicioner arrive at a better/faster conclusion regard-ing the diagnosis. Depending on the complexity of the problems and solutions, linguistic processing would have to be performed to be able to choose/adapt the solutions of similar problems for us-h ttp://www.mtsamples.com/ http://en.wikipedia.org/wiki/Case-based_reasoning age to solve the new problem. The CBR framework provides a p owerful platform to exploit such experience and incident reports once they are partitioned into the problem and solution (or similar) components. Problem-solution partitioning is useful beyond CBR too; such paritioning enables better retrieval by usage of translation models [24].

In this paper, we address the problem of segmenting experi-ence/incident reports into problem and solution parts. We will see that this is a hard case for traditional text segmentation approaches that typically rely on inter-segment dissimilarity ; this is because the problem and solution parts may have significantly similar vocabu-lary due to being talking about related incidents. Towards solv-ing our two-part segmentation problem, we develop a technique that harnesses the availability of a set of similar incident reports to build language models (that learn the nature of problem and solu-tion parts using statistical techniques) and translation models (that learn the correlation between words in the problem and those in the solution) that are then used to identify problem and solution seg-ments . We call our technique as Correlation and Cohesion Driven Segmentation ( CCS ). Our specific contributions are as follows:
Section 2 presents an overview of related work. We define our problem in Section 3 and present our approach in Section 4. Our experimental validation comprises Section 5 and we conclude in Section 6.
Segmenting experience/incident reports to problem and solution segments (or similar segmentation, e.g., cause-effect, symptoms-diagnosis) is a special case of the text segmentation problem [3]. Text segmentation involves splitting text into topical segments; the number or nature of the segments are usually not known a pri-ori. Unsupervised text segmentation techniques may use domain-independent assumptions such as lexical cohesion [9] and/or do-main dependent features such as cue words [7]. We briefly review text segmentation methods and knowledge reuse techniques that work on problem-solution data.

Text Segmentation , the problem of splitting text into lexically co-herent or topical segments, started getting attention with the Text-Tiling algorithm [9]. Subsequent works focussed on using word repetetions [20], semantic networks [14], deep semantic features such as presence of co-reference across a candidate segment bound-ary [18] and hidden markov models [23] whereas segmental semi-markov models have been exploited for a related task of change-point detection [8]. A recent text segmentation algorithm, APS [12], uses affinity propagation to identify segment assignments. In con-trast to the above approaches that deal with one document at a time, [15] present an approach that makes use of a collection of similar documents to segment individual documents better. We will compare the approach that we develop against APS, TextTil-ing and [15] .

Using Problem-Solution Repositories: Case-based reasoning [13] or CBR relates to the theory and practice of reusing knowledge available as question-answer (or problem-solution) pairs; a problem-solution pair is referred to as a case in CBR parlance. CBR systems maintain a repository of cases, and respond to a new problem with solutions derived from those of problems related/similar to the new problem. Research in CBR focusses on improving the mechanisms to retrieve similar problems, reuse their solutions by adapting to solve the new problem, revise the adapted solution based on the new problem, and retain the new solution by storing it in the repos-itory. In addition to the large recent interest in textual CBR [16, 2, 10], there has been a good amount of work in improving retrieval in textual question answer repositories [24, 11] of late, mostly from the information retrieval community. Usage of translation mod-els to model question-answer vocabulary correlations was first ex-plored in [6]. [24] builds a translation model using the question set and the answer set as a parallel corpus and uses it to conceptu-ally  X  X xpand X  a new question before posing it to a retrieval system. Apart from being beneficial to knowledge reuse, such segmentation would help more traditional scenarios such as the task of a corpo-rate knowledge-author who seeks to look through incident reports to identify and document new and interesting problems and their solutions.
We now define the problem formally. Given a set D of n docu-ments (e.g., incident/experience reports) of a similar nature, D = {D 1 , D 2 , . . . , D n } , with document D i comprising of l n values [ z 1 , z 2 , . . . , z n ] such that the segmentation of every docu-ment D i into the two segments, approximates as closely as possible the actual segmentation of the document D i into its two inherent segments (e.g., problem and solution, symptoms and diagnosis etc.) that we will generically refer to as problem and solution segments from hereon. For every document D i , we will limit our search space to only those values of z i that are sentence boundaries. In other words, we will ensure that sentences are not broken since it is not very natural to have parts of the sentence in the problem segment and the remaining in the solution. dom ( z i ) denotes all possible values of segmentation points (i.e., all segmentation points that are at a sentence boundary) for D i .
In this section, we outline our approach for two-part segmenta-tion of documents in a corpus of similar documents. We start by outlining certain assumptions that we use and go on to describe our approach in detail.
Our approach is based on some assumptions, some of which are well-known. At the risk of re-stating some of the obvious, we list the assumptions:
As in the case of any statistical/learning technique, we do not require each of the above assumptions to hold in an absolute sense; minor aberrations that do not throw the statistics haywire are typi-cally easily tolerated. In Section 5.7, we will empirically evaluate the performance of our technique in datasets where some of these assumptions do not fully hold. We now describe our approach, Correlation &amp; Cohesion Driven Segmentation (CCS), that exploits the correlation between words in the problem and solution, to arrive at a segmentation of incident reports (or similar text documents) into problem and solution parts. Our approach is fully unsupervised and has no apriori knowledge of problem and solution behavior or the nature of correlatedness between them. CCS relies on the following premises that are based on assumptions in Section 4.1.
We use the popular IBM Model 1 [4] translation model to learn word correlations across segments, and a unigram language model [22] to model behavior of segment types. We first describe the gen-erative model for text documents that we use in our segmentation technique. We then outline an objective function and followed by the CCS approach which is basically an EM algorithm that opti-mizes the objective function.
Consider a unigram problem language model P , a unigram solu-tion language model S and an IBM Model 1 translation model, T , that models translation probabilities from problem words to solu-tion words. The unigram language models for problems and solu-tions are multinomial distributions of words that would favor words that occur more in problems and solutions respectively. The trans-lation model is intuitively a 2-d associative array with T [ w ][ v ] be-ing directly related to the probability of the word v occuring in the solution whenever w is seen to occur in the problem; for consis-tency with previous literature on translation models, we will use T ( v | w ) to refer to this probability. Further, T ( . | w ) refers to the multinomial distribution of words with their value indicating the probability of occurence whenever w occurs in the problem.
Using such a set of models {P , S , T } , our generative model that generates a document D i having the first z i words in the problem and the remaining ( l i  X  z i ) words in the solution, works as follows: 1. For j, 1  X  j  X  z i , 2. For j, z i &lt; j  X  l i ,
Informally, the z i words (those belonging to the problem part) are sampled from the problem language model. The remaining ( l  X  z i ) words that are associated with the solution are then sam-pled from the solution langauge model with a probability of  X  and from the average of the multinomial translation model distributions corresponding to each problem word with a probability of (1  X   X  ) . Disk full reported.
 Some files were deleted to resolve the issue.

Illustrative Example: C onsider a hypothetical two sentence doc-ument from an IT helpdesk-like scenario, as given in Table 3. Let the segmentation point be the one that splits it into these two sen-tences, the first sentence being the problem and the second being the solution. Now, among the words { files, deleted, resolve, issue } in the solution, it is intuitively likely that resolve and issue have high probabilities under the solution language model, especially if D is a dataset of IT helpdesk reports. On the other hand, files and deleted are likely to be better supported by the translation model that is conditioned on very related words such as disk and full that already appear in the problem part. This intuition of the dual origin of solution words is factored into the generative model by allowing it to sample solution words from either the solution language model or the combination of the translation model distributions that are conditioned over the chosen problem words.
Probability Computation: U nder the generative framework out-lined above, the probability of generating a document D x the first z x words belong to the problem, given the models P , S , T is denoted as follows: p ( D x , z x |P , S , T ) = Y
Y where P ( w ) denotes the probability associated with w in the multinomial model P . For each of the problem words, the proba-bility of the word according to the problem language model is used in the product formulation. The solution word X  X  contribution to the product, similar to that in the generative model, is modeled as a weighted sum of its probability from the solution language model and the average of its probabilities from the multinomial translation model distributions for each of the z x problem words.
 For a document corpus D and a segmentation vector Z (Ref. Sec 3), the probability of the [ D , Z ] combination given the models {P , S , T } is then estimated as a product of the probabilities of the separate ( D i , z i ) combinations: L ( P , S , T ; D , Z ) = p ( D , Z|P , S , T ) = Y
For a set of documents D , we can now estimate the models {P , S , T } that are most likely to generate the document set. To-wards this intent, our objective function that is to be maximized denotes the maximum likelihood estimate of the models.

Since the segmentation vector Z is unknown, the maximal like-lihood estimate is calculated by marginalizing over all possible val-ues of Z (i.e., all possible segmentation points over all documents). p ( D|P , S , T ) = X
We will use an iterative EM formulation [5] to find the maximum likelihood estimate outlined above. Each iteration is a sequence of two high-level steps, the E and M steps, that are summarized as follows: In addition to the high-level steps outlined above, the E and M steps involve more detailed processing, especially, those corre-sponding to estimating whether solution words in each document were derived from the language or translation model and using them in re-estimating the models. We describe these steps in greater detail in respective sections.
According to our generative model and the objective function derived from it, the maximum likelihood estimates of the models may be obtained if the following are known:
However, none of these information is available to us. Thus, in the E-step, we estimate this information using the current esti-mates of language and translation models. For each segmentation point in a document D i , we estimate the probability of that being the correct segmentation point. Similarly, for each such segmenta-tion point, we estimate the probabilities that each solution word is derived from either of the sources. These estimates are then used in the M-step to rebuild the language and translation models, as we will show in the next section. We will use  X  as a shorthand to denote the set {P , S , T } for notational convenience, whenever ap-propriate. The estimated values for every ( D i , z 0 ) pair, where z denotes any possible segmentation point for D i , are:
Estimating Segmentation Point Posteriors: The posterior prob-ability of each candidate segmentation point z 0 in a document D obtained easily by conditioning the distribution p ( D i , z puted as shown in Section 4.2.1) over the document D i :
Estimating Posteriors for Solution Word Source: The prob-ability of the language model and translation model generating a given solution word w are assessed separately as follows: p ( T , w |D i , z 0 ,  X  ) = (1  X   X  )  X  avg {T ( w | p ) | p  X  problem ( D
The above construction is derived from the generative frame-work;  X  is the relative weighting used in Section 4.2.1 whereas problem ( D i , z 0 ) denotes the set of words in the problem when the document D i is split at the segmentation point z 0 . The above two values when conditioned over w , give the posterior probabilities of the word w being generated from either sources. p ( Source = S| w, D i , z 0 ,  X  )= p ( S , w |D i , z p ( Source = T | w, D i , z 0 ,  X  ) = 1 . 0  X  p ( Source = S| w, D
In the M-Step, we use the segmentation point posteriors and so-lution word souce assessments from the E-step to (re-)estimate the language and translation models.

Estimating the Language Models: We start with an overview on generating unigram language models. Let V 1 = [ { w 1 = 0 . 8 , w = 0 . 3 } , 0 . 4] and V 2 = [ { w 1 = 1 . 2 } , 0 . 5] denote two conceptual documents; the first one contains the word w 1 with a frequency of 0 . 8 and w 2 with a frequency of 0 . 3 whereas the second one contains just one word w 1 with a frequency of 1 . 2 . The second entry in each document representation is meant to represent the weight associ-ated with the document; in this case, the weights for the documents are seen to be 0 . 4 and 0 . 5 respectively. Though documents do not have fractional word frequencies in reality, our statistical estimates can accomodate such fractional frequencies and weights. The uni-gram language model derived out of a collection of such documents is a multinomial distribution where the value corresponding to any word w is computed as follows: where w 0 is any word in the vocabulary and the weight ( . ) and freq ( ., . ) functions denote the document weight and document-specific word frequency respectively. In a language model gener-ated from V 1 and V 2 , the value corresponding to w 1 would be:
For each segmentation point z 0 for every D i , let P rob ( D and Sol ( D i , z 0 ) denote the set of words in the problem and solu-tion parts respectively. We generate one conceptual document each for the problem and solution part (denoted as V P ( . ) and V spectively) as follows: V
S ( D i , z 0 ) = [ { w = p ( Source = S| w, D i , z 0 ,  X  ) | w  X  Sol ( D i
Informally, the problem document is simply the collection of problem words in P rob ( D i , z 0 ) with the document weight being the posterior probability of the segmentation point. The solution document is also weighted by the posterior probability of the seg-mentation point; however, unlike the problem case, each word in the solution document has a frequency that is determined by the probability of it being generated by the solution model. Each doc-ument D i thus generates as many problem and solution documents as there are possible segmentation points (i.e., | dom ( z Alg. 1 C CS Input. D , a set of documents Output. Z , a vector denoting the segmentation 1. Segment documents in D using 2. Estimate the models P , S and T using Z 3. while ( p ( D|P , S , T ) has not yet converged) 4. E-Step: Estimate the segmentation point posterior 5. M-Step: Re-estimate the language and translation 6.  X D i  X  D 7. return Z problem documents collection is used to estimate P w hereas S is generated from the collection of solution documents; these are models in conformance with the estimates derived in the E-step.
Estimating the translation model: The translation model rep-resents the correlation between words in the problems and those in the solutions. Towards this, it makes use of a corpus of document pairs such as below:
The example above denotes a document pair as a 3-tuple, with the first element denoting the frequencies of words in the problem part, second denoting the frequencies of words in the solution part whereas the third element denotes the weight assigned to the docu-ment pair. Towards estimating the translation model, we create one such 3-tuple for each ( D i , z 0 ) pair as follows: [ { w = 1 . 0 | w  X  P rob ( D i , z 0 ) } , { w = p ( Source = T | w, D i , z 0 ,  X  ) | w  X  Sol ( D i
Analogous to the language model case, each document would generate as many 3-tuples as there are segmentation points. The collection of such 3-tuples are then used to learn a translation model which would then be in accordance with the E-step estimates. In particular, we use the weighted document pairs to derive an IBM Model 1 using a straightforward adaptation of the EM algorithm from [4]; we do not delve into the finer details of the translation model training process since that is tangential to the focus of this paper.
The CCS algorithm is outlined in Algorithm 1. We start in Line 1 by initializing the Z vector according to the segmentation derived from a state-of-the-art segmentation algorithm (e.g., APS [12], [15] or TextTiling [9]); we will use APS to initialize the segmentation in our experiments (Ref. Section 5.4). Segmentations by generic text segmentation algorithms, however, do not necessarily generate ex-actly two segments per document; we use a post-processing step (in Line 2) to convert any multi-segmentation to a two-part segmenta-tion by retaining only the most appropriate segment switch for each document, as determined using a TextTiling-style estimation models are built using the initialized Z and an iterative sequence of
T extTiling estimates the score of each sentence boundary to be a E-step (Ref. Section 4.2.3) and M-step (Ref. Section 4.2.4) o pera-tions follow. We run this sequence of steps until no more changes occur to the objective function in Section 4.2.2, or for 20 iterations, whichever is fewer.

When the iterations are complete, we set the segmentation point for each document as that which maximizes the probability of gen-erating that document, according to the final estimates of the lan-guage and translation models. These form the Z vector that is then output as the final segmentation for the documents. Since there is no apriori evidence to guess the relative importance of the solution language model and the translation model in generating solution words, we weigh them equally by setting  X  = 0 . 5 in our approach. In the remainder of this paper, unless mentioned otherwise, CCS refers to this setting of  X  .

Time Complexity: Consider a corpus of n documents with a vo-cabulary of size m , each document having an average of l sentences or w words. Calculating the posterior probabilities in the E-step costs O ( lnw 2 ) . The M-step operations of learning the translation models (using k iterations) and language models costs O ( k ( nw m )) and O ( nw + m ) respectively. For k 0 iterations of CCS , the to-tal time taken, hence, is of the order of O ( k 0 nw 2 ( k + l ) + k We now describe our experimental study where we compare the CCS algorithm against the state of the art algorithms. We describe the datasets, evaluation measures and baseline algorithms followed by a detailed description of our extensive experimentation.
In the absence of any available segmented incident report data (to the best of our knowledge), we use various datasets that were col-lected as part of a recent IR task and are publicly available lected 8 domains from the training dataset preferring those domains that comprise verbose descriptions of problems and associated so-lutions. These datasets, unlike typical experience/incident reports, often have a first person narrative in the problem part (e.g., I have been in need of a career switch to something related to networking and have a UK work visa ), and a slightly instructional narrative at the solution part (e.g., Most UK companies look for a work visa that is valid for beyond one year ). It may be noted that such style differ-ences between segments are advantageous for all text segmentation segmentation point and uses a threshold to designate all poin ts that have a higher assessed score as segmentation points. In our adapta-tion to derive a two-part segmentation from a multi-segmentation, we simply choose that candidate among the multiple segmentation points that scores best (according to the TextTiling estimate), as the only segmentation point. http://www.isical.ac.in/ clia/faq-retrieval/faq-retrieval.html techniques (including baselines); since CCS is not tapping such style differences explicitly, we argue that it is kosher to attribute its improved performance with respect to the baselines mostly to the CCS formulation. The style differences may be captured in the intra-segment lexical cohesion assumption to some extent; while lexical cohesion of segments at the document level is used by all the baselines that we compare against, CBA (Ref. Sec 5.3) exploits the lexical cohesion of the same segment type across documents in the corpus. The various datasets (named by their domains) are listed in Table 2 and example problem-solution pair is given in Table 3. The documents in our datasets are seen to have 4-6 sentences and 45 to 70 words, on an average. We collate the problem and solution part to create a single document, and run the segmentation algorithms on them; the quality of segmentation is then evaluated with respect to the actual segment boundary (which is known, since the collation of parts to arrive at the single document was performed by us).
We now outline the various evaluation measures that we use in our empirical study:
WindowDiff: We primarily use the well-known segmentation evaluation measure, WindowDiff [19], to evaluate segmentation qual-ity. WindowDiff can be conceptually thought of as moving a sliding window simultaneously over the two segmentations (the created, and the ground truth), capturing the differences in the number of segment boundaries at each step, and then aggregating it across the entire document to arrive at a single measure of segmentation agreement. Consider two segmentations Z 1 and Z 2 of a document comprising of l sentences; the WindowDiff metric is computed as follows: W D ( Z 1 , Z 2 ) = where # SB ( Z , i, w ) counts the number of segment boundaries according to the segmentation Z among the w sentences from the i th sentence in the document, and f ( ., . ) is a function that returns 1 if the two arguments are equal, and 0 otherwise. w is typically chosen as half of the average segment size. The WindowDiff values are then averaged across documents in the corpus. It may be noted that WindowDiff is a penalty measure with lower values indicating better agreement among the segmentations compared .

P K : This metric [3], the precursor to WindowDiff , is very simi-lar to the latter in using a sliding window type approach. Instead of counting the number of segment boundaries within a sliding win-dow for each segmentation, P K checks whether the two ends of the window are in the same segment. For a window, in cases where the segmentations disagree in terms of the membership of the sentences in the two ends, a penalty of 1 . 0 is added to the numerator. These are then averaged across sliding windows and documents similar to that in the WindowDiff measure.

Diff: WindowDiff and P K are metrics that can handle segmenta-tions that segment documents into any number of segments. How-ever, our problem deals with choosing just one segmentation bound-ary for each document, where it would be segmented into two parts. Diff is a simple measure that measures the distance between the boundaries chosen in the segmentations that are compared. For ex-ample, for a document in question, if Z 1 chooses a segmentation point beyond the z th 1 sentence and z 2 denotes the choice by Z Diff ( Z 1 , Z 2 ) is estimated as abs ( z 1  X  z 2 ) , with abs ( . ) denoting the absolute value. This is a very intuitive measure since it gives the number of sentences that a segmentation is off by, when compared with the ground truth.
CBR Usability Measures: S ince our eventual goal is to aid knowledge reuse, we also illustrate the improvements achieved on a CBR system that uses the CCS segmented dataset with respect to the one that uses other segmentations; in particular, we use the max and tot [17] metrics; more details are in Section 5.5.
Statistical Significance: We also present results of statistical significance on various measures using randomization tests [21] with a p-value of &lt; 0 . 05 . A technique being statistically significant over another with a p-value of &lt; 0 . 05 suggests that the probabil-ity that the former achieved superior results by mere chance is less than 0.05; such statistical significance tests are becoming standard practice in evaluating retrieval systems 5 .
TextTiling [9] (TT): Among the earliest algorithms for text seg-mentation, TextTiling relies on lexical frequency and distributional information to identify segment boundaries. This uses the cosine similarity between two blocks of text, one before and another after each candidate segment boundary, to determine whether a segment boundary be placed at the location. We adapt TextTiling by forcing it to choose only one segment boundary per document. This is triv-ial since TextTiling scores each sentence boundary; instead of using a threshold to choose possibly multiple candidate sentence bound-aries as segmentation points, we simply choose the single sentence boundary with the best score as the segmentation point.
Clustering-based Approach [15] (CBA): This, unlike most other algorithms in literature, is similar to CCS in the usage of knowl-edge across a corpus of similar text documents to segment each document. It clusters sentences across documents to arrive at sen-tence clusters, which are then clustered using spatial similarity (in the documents that contain them) to arrive at larger clusters called representative segments . Each document is then segmented using such representative segments . We adapt this approach by running the spatial-similarity based clustering until only two representative segments remain. Under such a setting, given a corpus with two segment types that manifest in many documents in the corpus, the clustering process in CBA is expected to produce one representa-tive segment per segment type. This representative segment model, due to being built across documents, enables CBA to use corpus level signatures per segment type to segment each document; such a corpus-level modeling of segment types is something that algo-rithms that deal with one document at a time are incapable of doing. Even with two representative segments, CBA could produce mul-tipe segment boundaries; in such case, we use TextTiling type scor-ing of such segment boundaries to choose the best segment bound-ary as the segmentation point.

APS [12]: In this recent algorithm, each basic unit (e.g., sen-tence) is treated as a data point. Similarities between data points are estimated using lexical measures unless provided by other means. h ttp://faculty.vassar.edu/lowry/ch4pt1.html An iterative message passing based on the affinity propagatio n for-mulation is then run, until coherent segments (sets of basic units) emerge. This approach could generate segmentations that have more than two segments per document. Similar to earlier scenarios, for such cases, we choose the best segment break from among them using a TextTiling style evaluation.
For each technique, we assess the segmentation generated using the WindowDiff measure when compared against the ground truth segmentation, and present the results in Table 4. WindowDiff be-ing a penalty measure, the technique scoring lesser is considered as being better. Among the two types of models that CCS uses, the language and translation models, the former is more intuitive since it stems from the intra-segment lexical cohesion assumption that is central to most text segmentation algorithms. The language model representation of each segment type is analogous to the rep-resentative segment representation in the CBA approach. Towards illustrating the value of the usage of the translation model over and above the language models, we include the results of the CCS vari-ant that uses only language models. This corresponds to setting  X  = 1 . 0 in the CCS approach; we refer to this variant as CCS and include the results derived from it in Table 4.

The results table presented in Table 4 illustrates the effectiveness of the CCS formulation. The best among the baseline approaches is indicated by an underline. APS is seen to outperform the other base-lines significantly, and was thus chosen to initialize the Z vector in Line 1 of Algorithm 1. CCS beats the baselines by large margins (the best number for each dataset is indicated in boldface) and also fares much better than CCS L on each dataset; this illustrates that the translation models help improve the segmentation considerably over and above the usage of language models alone.

Statistical Significance ( p &lt; 0 . 05 ): Those entries of CCS that were found to be statistically significant over the corresponding en-tries of each of the other techniques (including CCS L ) are marked with a ? . A seen from the Table, CCS performance is statistically significant over every technique on each dataset. We now analyze the performance of the techniques on the P and Diff penalty measures. Since the general trends were similar to the WindowDiff evaluation with APS outperforming the other base-lines, we present a comparison between CCS and APS herein.
As seen from Figure 1, CCS is able to bring down the P K values down by upto three times than that of APS . When averaged across datasets, APS and CCS were seen to score 0 . 27 and 0 . 09 respec-tively. These results were seen to be statistically significant on each dataset at a p -value of less than 0 . 05 . The efficacy of CCS over APS was seen to be much more pronounced under the Diff measure with the techniques scoring 0 . 19 and 0 . 87 respectively. The chart ap-pears in Figure 2. Informally, the CCS segmentation was off from the ground truth by 0 . 19 sentences on an average, whereas the APS segmentation points were seen to be as much as 0 . 87 sentences away from the ground truth segmentation. Under the Diff measure too, the CCS performance was seen to be statistically significant over APS .
Though more accurate segmentation would intuitively be expected to deliver a better performance when used in a CBR system, we use a more direct measure to quantify the actual improvement. The tot measure [17] uses a leave-one-out style evaluation by posing each problem against the repository and measures the similarity between the top-k retrieved solutions against its own (which are known) us-ing cosine similarity. Much like the configuration used in [17], we use cosine similarity as a proxy for the usability of the solution and set k = 3 . tot then denotes the denotes the total usability of the top-k solutions that are retrieved. The tot measure is to be understood as a lower bound on the usability, since the cosine similarity cannot model polysemy, and linguistic processing that could help render-ing the retrieved solution more appropriate. The tot (for which, a higher value indicates better performance, unlike WindowDiff ) value across the datasets for the APS and CCS techniques is pre-sented in Table 5. As usual, the best measure is shown in boldface and CCS segmentation is seen to yield a better CBR system consis-tently, providing an average of 7% gains across datasets. Similar results were obtained on the max measure [17] also. Much like in Table 4, we indicate statistically significant results by a ? ; from Ta-ble 5, it is observed that the usability improvements achieved by CCS are statistically significant over APS on 5 datasets at a p-value &lt; 0 . 05 .
Since we use the best-performing baseline, APS , to initialize the segmentation for the CCS technique (in line 1 in Algorithm 1), part of the credit for the good performance of the latter is likely to be due to the performance of APS . In this section, we analyze the robust-ness of the CCS formulation by subjecting it to a scenario where a good initialization is not available. In particular, we provide a random initialization of Z for CCS to start processing. Somewhat surprisingly, the randomly initialized CCS variant was seen to be highly competetive with that of the APS initialization. The Win-dowDiff measures are plotted in Figure 3; it is seen that the perfor-mance is nearly identical with minor variations. This shows that the CCS technique is extremely robust and can easily recover from bad initializations. However, a good initialization is likely to be more critical while working with datasets of longer documents that may span dozens of sentences each.
Due to the EM formulation, the value of the objective function outlined in Section 4.2.2 is bound to monotonically improve with each iteration. We now analyze the trends of the objective function values across EM iterations; Figure 4 plots the objective function
Solution Words etktcanc misplaced authority values in the Y-axis against the number of iterations in the X-axis. As expected, large gains are achieved between the initialization and the first iteration with the gains becoming smaller in subsequent steps. More importantly, the objective function is seen to stabi-lize very fast with the gains becoming marginal beyond the third iteration, on every dataset. Thus, CCS may be terminated beyond the third iteration in case of a need to optimize on computational expense, since the segmentations are likely to have been relatively stabilized.
Having shown that the usage of translation models in the CCS formulation leads to substantially more accurate segmentations, we now present a few sample translation model correlations to illus-trate the kind of correlations that are learnt by it. We focus on the railways dataset, documents within which mostly start with de-scriptions of specific scenarios in relation to ticketing and traveling in the Indian Railway network, followed by steps to overcome such situations 6 . We pick three words that are commonly found in prob-lem parts in the railways dataset based on a cursory glance through the dataset; these are cancel , duplicate and delivery . For each cho-sen problem word, we mine for the top-10 solution words that are correlated with it, and present a sample from them in Table 6.
We briefly outline intuitions as to why the correlations in Table 6 may be meaningful. cancel was found to be associated with sit-uations involving ticket cancellation, for which remedies included resorting to one of the tdr or etktcanc cancellation processes pro-vided by the railway. The third option, that of walking in to the counter for cancellation involves procuring a cancellation slip or printing it from the internet. Another common circumstance of in-terest is related to fetching a duplicate ticket. Towards getting this done, one needs to report that the ticket has been misplaced and produce ID proofs to assert the authenticity or genuineness of the request. Issuance of a duplicate ticket often involves deduction of a prescribed fee too. When tickets are booked online, Indian Rail-ways provides an option of mailing a printed ticket to the passenger through courier. delivery of such tickets often is often problematic with common problems being unavailability of the passenger at the address location or due to the railways not providing delivery ser-vice in certain cities . The former problem is often easily resolved by leaving an authorization letter with someone who is available at the address provided for delivery.
Unlike APS and other approaches that segment each document independently, CCS uses models created out of the entire corpus to segment each document. This, as illustrated above, leads to sig-nificant improvements in segmentation accuracy. However, it also implies that noise in a few documents in the corpus could affect the
T he choice of dataset was also partially motivated by the domain knowledge of one of the authors; such considerations are inevitable is because understanding and making sense of word pairs is a fairly knowledge intensive process.
 Figure 5: W indowDiff under varying levels of Sentence Swapping Figure 6: W indowDiff under varying levels of Segment Swapping segmentation of even non-noisy documents in the corpus. In this section, we evaluate the robustness of CCS to two different kinds of noise to which incident reports are susceptible.
Incident reports often follow a chronological order of narration; however, an initial solution step could be followed by an incident that discovers something more about the problem, leading to a vio-lation of the assumption of clear separation between the segments. To evaluate the robustness of CCS to this kind of noise, we inject such perturbation in a few documents by swapping the positions of sentences on either side of the true segment boundary (i.e., by making the last sentence in the problem segment as the first state-ment in the solution segment and vice versa); we call such noise as Sentence Swapping . We let CCS operate on the corpus that con-tains varying fractions of such noisy documents, and evaluate the performance with respect to APS on the WindowDiff measure. In accuracy evaluation using the WindowDiff measure, we consider only the non-noisy documents since the real segmentation for the noisy documents is not well-defined (due to the swapping).
We illustrate the WindowDiff evaluation in Figure 5. APS is not affected by the presence of noisy documents since it operates on a per-document level and the noisy documents are not considered in the WindowDiff computation as indicated above. More importantly, it is interesting to note that CCS performance is also seen to be very stable across varying fractions of noisy documents in the corpus; we show values for upto 20% noise (in the X-axis) in the chart.
Certain authors could violate the convention of segment order-ing (e.g., problem segment followed by solution) and may use the opposite ordering. The presence of a few documents with the oppo-site ordering in the collection is detrimental to techniques like CCS that operate at the corpus level. Towards evaluating effects of such noise, we swap the ordering of problem and solution segments in a few documents (i.e., Segment Swapping ). As in the earlier case, we evaluate the performance on corpora containing varying fractions of such noisy documents. Unlike Sentence Swapping , the bound-aries are well defined even in the case of noisy documents since w hole segments are swapped.

While APS is unaffected by such noise due to the per-document formulation, CCS is empirically seen to be fairly sensitive to swap-ping of whole segments in documents in the collection. It may be seen from the WindowDiff evaluation in Figure 6 that CCS de-grades from 0 . 06 to 0 . 13 when the fraction of noisy documents is increased to up to 20% . However, even at 20% noise, CCS is seen to perform twice as better as APS , and is hence, still remains the preferred technique.
We outlined the two-part text segmentation problem to segment documents such as incident reports that contain problem and so-lution segments. Accurate identification of problem solution seg-ments is critical towards making the knowledge in incident reports and diagnosis reports available to knowledge reuse systems. This problem, however, poses an unfriendly scenario to traditional text segmentation algorithms due to the relatedness of the segments. We outlined an iterative technique, CCS , that models the behav-ior of problem and solution segments and word-correlations across them in a corpus of similar documents, and exploits such model-ing to arrive at more accurate segmentations. The CCS genera-tive model formulates each document as being generated by ini-tially sampling words from a problem language model, followed by choosing words from either the solution language model or a trans-lation model conditioned on the words already chosen for the prob-lem part. Our empirical study over a large collection of datasets establish that CCS provides vast and statistically significant im-provements (on the WindowDiff measure) over state-of-the-art tech-niques, including techniques that use corpus-wide knowledge. Such improvements in segmentation accuracy are seen to reflect as im-proved solution usability in Case-based Reasoning systems. We have further shown that our technique can recover from not-so-good initializations, is reasonably noise-tolerant, and degrades very gracefully with increasing noise in the dataset. In short, we have established the utility of corpus-wide statistics and inter-segment word correlation in two-part text document segmentation, when-ever a corpus of similar documents are available.

Since higher order language models (e.g., 2-gram, 3-gram etc.) and translation models (IBM Models 2 and beyond) are designed to rectify some of the drawbacks with the first order models, har-nessing them to improve text segmentation would be an interesting future work. In this work, we have exploited the correlatedness of two segments; generalizing this into a framework that could de-tect and exploit word correlations in scenarios where any number of segments may be expected in a document, would be a useful extension to this work and a logical next step. [1] A. Aamodt and E. Plaza. Case-based reasoning: [2] I. Adeyanju, N. Wiratunga, R. Lothian, and S. Craw. [3] D. Beeferman, A. Berger, and J. Lafferty. Statistical models [4] P. F. Brown, J. Cocke, S. A. D. Pietra, V. J. D. Pietra, [5] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum [6] A. Echihabi and D. Marcu. A noisy-channel approach to [7] J. Eisenstein and R. Barzilay. Bayesian unsupervised topic [8] X. Ge and P. Smyth. Segmental semi-markov models for [9] M. A. Hearst. Multi-paragraph segmentation of expository [10] K. Jayanthi, S. Chakraborti, and S. Massie. Introspective [11] J. Jeon, W. B. Croft, and J. H. Lee. Finding semantically [12] A. Kazantseva and S. Szpakowicz. Linear text segmentation [13] J. L. Kolodner. An introduction to case-based reasoning. [14] H. Kozima. Text segmentation based on similarity between [15] K. Kummamuru, D. P, S. Roy, and L. V. Subramaniam. [16] M. Lenz, A. H X bner, and M. Kunze. Textual cbr. In [17] D. P, S. Chakraborti, and D. Khemani. More or better: on [18] R. J. Passonneau and D. J. Litman. Discourse segmentation [19] L. Pevzner and M. A. Hearst. A critique and improvement of [20] J. C. Reynar. An automatic method of finding topic [21] M. D. Smucker, J. Allan, and B. Carterette. A comparison of [22] F. Song and W. B. Croft. A general language model for [23] P. van Mulbregt, I. Carp, L. Gillick, S. Lowe, and J. Yamron. [24] X. Xue, J. Jeon, and W. B. Croft. Retrieval models for
