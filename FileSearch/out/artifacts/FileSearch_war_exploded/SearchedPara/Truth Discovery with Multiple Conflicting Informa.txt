 The world-wide web has become the most important infor-mation source for most of us. Unfortunately, there is no guarantee for the correctness of information on the web. Moreover, different web sites often provide conflicting in-formation on a subject, such as different specifications for the same product. In this paper we propose a new problem called Veracity , i.e. , conformity to truth , which studies how to find true facts from a large amount of conflicting informa-tion on many subjects that is provided by various web sites. We design a general framework for the Veracity problem, and invent an algorithm called TruthFinder , which uti-lizes the relationships between web sites and their informa-tion, i.e. , a web site is trustworthy if it provides many pieces of true information, and a piece of information is likely to be true if it is provided by many trustworthy web sites .Ourex-periments show that TruthFinder successfully finds true facts among conflicting information, and identifies trustwor-thy web sites better than the popular search engines. Categories and Subject Descriptors: H.2.8 [Database Management]: Database applications  X  Data mining General Terms: Algorithms.
 Keywords: data quality, web mining, link analysis.
The world-wide web has become a necessary part of our lives, and might have become the most important informa-tion source for most people. Everyday people retrieve all kinds of information from the web. For example, when shop-ping online, people find product specifications from web sites like Amazon.com or ShopZilla.com. When looking for inter-
The work was supported in part by the U.S. National Sci-ence Foundation NSF IIS-05-13678/06-42771 and NSF BDI-05-15813. Any opinions, findings, and conclusions or recom-mendations expressed here are those of the authors and do not necessarily reflect the views of the funding agencies.  X  Xiaoxin Yin has joined Google Inc.
 Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. esting DVDs, they get information and read movie reviews on web sites such as NetFlix.com or IMDB.com.  X  Is the world-wide web always trustable?  X  Unfortunately, the answer is  X  no  X . There is no guarantee for the correctness of information on the web. Even worse, different web sites often provide conflicting information, as shown below. Example 1: Authors of books. We tried to find out who wrote the book  X  X apid Contextual Design X  (ISBN: 0123540518). We found many different sets of authors from different online bookstores, and we show several of them in Table 1. From the image of the book cover we found that A1 Books provides the most accurate information. In com-parison, the information from Powell X  X  books is incomplete, and that from Lakeside books is incorrect.
 Table 1: Conflicting information about book authors
The trustworthiness problem of the web has been realized by today X  X  Internet users. According to a survey on credibil-ity of web sites, 54% of Internet users trust news web sites at least most of time, while this ratio is only 26% for web sites that sell products, and is merely 12% for blogs.
There have been many studies on ranking web pages ac-cording to authority based on hyperlinks, such as Authority-Hub analysis [2], PageRank [4], and more general link-based analysis [1]. But does authority or popularity of web sites lead to accuracy of information? The answer is unfortu-nately no. For example, according to our experiments the bookstores ranked on top by Google ( Barnes &amp; Noble and Powell X  X  books ) contain many errors on book author infor-mation, and some small bookstores ( e.g. , A1 Books ) provide more accurate information.

In this paper we propose a new problem called Verac-ity problem, which is formulated as follows: Given a large amount of conflicting information about many objects, which is provided by multiple web sites (or other types of informa-tion providers), how to discover the true fact about each object. We use the word  X  X act X  to represent something that is claimed as a fact by some web site, and such a fact can be either true or false. There are often conflicting facts on the web, such as different sets of authors for a book. There are also many web sites, some of which are more trustworthy than some others. A fact is likely to be true if it is provided by trustworthy web sites (especially if by many of them). A web site is trustworthy if most facts it provides are true.
Because of this inter-dependency between facts and web sites, we choose an iterative computational method. At each iteration, the probabilities of facts being true and the trust-worthiness of web sites are inferred from each other. This iterative procedure is rather different from Authority-Hub analysis [2]. The first difference is in the definitions. The trustworthiness of a web site does not depend on how many facts it provides, but on the accuracy of those facts. Nor can we compute the probability of a fact being true by adding up the trustworthiness of web sites providing it. These lead to non-linearity in computation. Second and more impor-tantly, different facts influence each other. For example, if a web site says a book is written by  X  X essamyn Wendell X , and another says  X  X essamyn Burns Wendell X , then these two web sites actually support each other although they provide slightly different facts.

In summary, we make three major contributions in this paper. First, we formulate the Veracity problem about how to discover true facts from conflicting information. Second, we propose a framework to solve this problem, by defining the trustworthiness of web sites, confidence of facts, and in-fluences between facts. Finally, we propose an algorithm called TruthFinder for identifying true facts using itera-tive methods.

The rest of the paper is organized as follows. We describe the problem in Section 2, and propose the computational model in Section 3. Experimental results are presented in Section 4, and we conclude this study in Section 5.
The input of TruthFinder is a large number of facts about properties of a certain type of objects. The facts are provided by many web sites. There are usually multiple conflicting facts from different web sites for each object, and the goal of TruthFinder is to identify the true fact among them. Figure 1 shows a mini example dataset. Each web site provides at most one fact for an object.

We first introduce the two most important definitions in this paper, the confidence of facts and the trustworthiness of web sites.

Definition 1. (Confidence of facts.) The confidence of a fact f (denoted by s ( f )) is the probability of f correct, according to the best of our knowledge.

Definition 2. (Trustworthiness of web sites.) The trustworthiness of a web site w (denoted by t ( w )) is the expected confidence of the facts provided by w .
 Different facts about the same object may be conflicting. However, sometimes facts may be supportive to each other although they are slightly different. For example, one web site claims the author to be  X  X ennifer Widom X  and another one claims  X  X . Widom X . If one of them is true, the other is also likely to be true.

In order to represent such relationships, we propose the concept of implication between facts . The implication from fact f 1 to f 2 , imp ( f 1  X  f 2 ), is f 1  X  X  influence on dence, i.e. ,howmuch f 2  X  X  confidence should be increased (or decreased) according to f 1  X  X  confidence. It is required that imp ( f 1  X  f 2 ) is a value between  X  1 and 1. A positive value indicates if f 1 is correct, f 2 is likely to be correct. While a negative value means if f 1 is correct, f 2 is likely to be wrong. The details about this will be described in Section 3.1.2.
Please notice that the definition of implication is domain specific. When a user uses TruthFinder on a certain do-main, he should provide the definition of implication be-tween facts. If in a domain the relationship between two facts is symmetric, and the definition of similarity is avail-able, the user can define imp ( f 1  X  f 2 )= sim ( f 1 ,f base sim ,where sim ( f 1 ,f 2 ) is the similarity between f ,and base sim is a threshold for similarity.

Based on common sense and our observations on real data, we have four basic heuristics that serve as the bases of our computational model.
 Heuristic 1: Usually there is only one true fact for a prop-erty of an object.
 Heuristic 2: This true fact appears to be the same or sim-ilar on different web sites.
 Heuristic 3: The false facts on different web sites are less likely to be the same or similar.
 Heuristic 4: In a certain domain, a web site that provides mostly true facts for many objects will likely provide true facts for other objects.
Based on the above heuristics, we know if a fact is pro-vided by many trustworthy web sites, it is likely to be true; if a fact is conflicting with the facts provided by many trust-worthy web sites, it is unlikely to be true. On the other hand, a web site is trustworthy if it provides facts with high confidence. We can see that the web site trustworthiness and fact confidence are determined by each other, and we can use an iterative method to compute both. Because true facts are more consistent than false facts (Heuristic 3), it is likely that we can distinguish true facts from false ones at the end. In this section we discuss the computational model.
We first discuss how to infer web site trustworthiness and fact confidence from each other.
As defined in Definition 2, the trustworthiness of a web site is just the expected confidence of facts it provides. For web site w , we compute its trustworthiness t ( w ) by calcu-lating the average confidence of facts provided by w . where F ( w ) is the set of facts provided by w . Table 2: Variables and Parameters of TruthFinder
In comparison, it is much more difficult to estimate the confidence of a fact. As shown in Figure 2, the confidence of a fact f 1 is determined by the web sites providing it, and other facts about the same object.
Let us first analyze the simple case where there is no related fact, and f 1 is the only fact about object o 1 ( i.e. , f 2 does not exist in Figure 2). Because f 1 is provided by w 1 and w 2 ,if f 1 is wrong, then both w 1 and w 2 are wrong. We first assume w 1 and w 2 are independent. (This is not true in many cases and we will compensate for it later.) Thus the probability that both of them are wrong is (1  X  t ( w 1 ))  X  (1  X  t ( w 2 )), and the probability that f f is the only fact about an object, then its confidence s ( can be computed as where W ( f ) is the set of web sites providing f .
In Equation (2), 1  X  t ( w ) is usually quite small and mul-tiplying many of them may lead to underflow. In order to facilitate computation and veracity exploration, we use log-arithm and define the trustworthiness score of a web site as  X  ( w ) is between and 0 and +  X  , which better characterizes how accurate w is. For example, suppose there are two web sites w 1 and w 2 with trustworthiness t ( w 1 )=0 . 9and t ( w 2 )=0 . 99. We can see that w 2 is much more accurate than w 1 , but their trustworthiness do not differ much as t ( w 2 )=1 . 1  X  t ( w 1 ). If we measure their accuracy with trustworthiness score, we will find  X  ( w 2 )=2  X   X  ( w 1 better represents the accuracy of web sites.

Similarly, we define the confidence score of a fact as
A very useful property is that, the confidence score of a fact f is just the sum of the trustworthiness scores of web sites providing f . This is shown in the following lemma.
Lemma 1.
Proof. According to Equation (2), Take logarithm on both side and we have
The above discussion shows how to compute the confi-dence of a fact that is the only fact about an object. How-ever, there are usually many different facts about an object (such as f 1 and f 2 in Figure 2), and these facts influence each other. Suppose in Figure 2 the implication from f 2 to f 1 very high ( e.g. , they are very similar). If f 2 is provided by many trustworthy web sites, then f 1 is also somehow sup-ported by these web sites, and f 1 should have reasonably high confidence. Therefore, we should increase the confi-dence score of f 1 according to the confidence score of f which is the sum of trustworthiness scores of web sites pro-viding f 2 .Wedefinethe adjusted confidence score of a fact f as  X  is a parameter between 0 and 1, which controls the influ-ence of related facts. We can see that  X   X  ( f )isthesumof confidence score of f and a portion of the confidence score of each related fact f multiplies the implication from f to f . Please notice that imp ( f  X  f ) &lt; 0when f is conflicting with f .

We can compute the confidence of f based on  X   X  ( f )in the same way as computing it based on  X  ( f )(definedin Equation (4)). We use s  X  ( f ) to represent this confidence.
One problem with the above model is we have been assum-ing different web sites are independent with each other .This assumption is often incorrect because errors can be propa-gated between web sites. According to the definitions above, if a fact f is provided by five web sites with trustworthiness of 0.6 (which is quite low), f will have confidence of 0.99! But actually some of the web sites may copy contents from others. In order to compensate for the problem of overly high confidence, we add a dampening factor  X  into Equation (7), and redefine fact confidence as s  X  ( f )=1  X  e  X   X   X   X  where 0 &lt; X &lt; 1.

The second problem with our model is that, the confidence of a fact f can easily be negative if f is conflicting with some facts provided by trustworthy web sites, which makes ( f ) &lt; 0and s  X  ( f ) &lt; 0. This is unreasonable because even with negative evidences, there is still a chance that f is correct, so its confidence should still be above zero. Therefore, we adopt the widely used Logistic function [3], which is a variant of Equation (7), as the final definition for fact confidence.
When  X   X   X   X  ( f ) is significantly greater than zero, When  X   X   X   X  ( f ) is significantly less than zero, s ( f zero but remains positive, which is consistent with the real situation. Equation (8) is also very similar to Sigmoid func-tion [6], which has been successfully used in various models in many fields.
As described above, we can infer the web site trustworthi-ness if we know the fact confidence, and vice versa. As in Authority-hub analysis [2] and PageRank [4], TruthFinder adopts an iterative method to compute the trustworthiness of web sites and confidence of facts. Initially, it has very lit-tle information about the web sites and the facts. At each it-eration TruthFinder tries to improve its knowledge about their trustworthiness and confidence, and it stops when the computation reaches a stable state.

We choose the initial state in which all web sites have a uniform trustworthiness t 0 .( t 0 is set to the estimated average trustworthiness, such as 0.9.) In each iteration, TruthFinder first uses the web site trustworthiness to com-pute the fact confidence, and then recomputes the web site trustworthiness from the fact confidence. It stops iterating when it reaches a stable state. The stableness is measured by the change of the trustworthiness of all web sites, which is represented by a vector an iteration (measured by cosine similarity between the old and the new
In this section we present experiments on a real dataset, which shows the effectiveness of TruthFinder .Wecom-pare it with a baseline approach called Voting , which chooses the fact that is provided by most web sites. We also com-pare TruthFinder with Google by comparing the top web sites found by each of them.

All experiments are performed on an Intel PC with a 1.66GHz dual-core processor, 1GB memory, running Win-dows XP Professional. All approaches are implemented us-ing Visual Studio.Net (C#). The two parameters in Equa-tion (8) are set as  X  =0 . 5and  X  =0 . 3. The maximum difference between two iterations,  X  , is set to 0.001%.
This dataset contains the authors of many books pro-vided by many online bookstores. It contains 1265 computer science books published by Addison Wesley, McGraw Hill, Morgan Kaufmann, or Prentice Hall. For each book, we use its ISBN to search on www.abebooks.com , which returns the book information on different online bookstores that sell this book. The dataset contains 894 bookstores, and 34031 list-ings ( i.e. , bookstore selling a book). On average each book has 5.4 different sets of authors.

TruthFinder performs iterative computation to find out the set of authors for each book. In order to test its accu-racy, we randomly select 100 books and manually find out their authors. We find the image of each book, and use the authors on the book cover as the standard fact.

We compare the set of authors found by TruthFinder with the standard fact to compute the accuracy. For a cer-tain book, suppose the standard fact contains x authors, TruthFinder indicates there are y authors, among which z authors belong to the standard fact. The accuracy of Sometimes TruthFinder provides partially correct facts. For example, the standard set of authors for a book is  X  X raeme C. Simsion and Graham Witt X , and the authors found by TruthFinder may be  X  X raeme Simsion and G. Witt X . We consider  X  X raeme Simsion X  and  X  X . Witt X  as partial matches for  X  X raeme C. Simsion X  and  X  X raham Witt X , and give them partial scores. We assign different weights to differ-ent parts of persons X  names. Each author name has total weight 1, and the ratio between weights of last name, first name, and middle name is 3:2:1. For example,  X  X raeme Simsion X  will get a partial score of 5/6 because it omits the middle name of  X  X raeme C. Simsion X . If the standard name has a full first or middle name, and TruthFinder provides the correct initial, we give TruthFinder half score. For example,  X  X . Witt X  will get a score of 4/5 with respect to  X  X raham Witt X , because the first name has weight 2/5, and the first initial  X  X . X  gets half of the score.

The implication between two sets of authors f 1 and f 2 is defined in a very similar way as the accuracy of f 2 with re-spect to f 1 . One important observation is that many book-stores provide incomplete facts, such as only the first au-thor. For example, if a web site w 1 says a book is written by  X  X ennifer Widom X , and another web site w 2 says it is written by  X  X ennifer Widom and Stefano Ceri X , then w 1 ac-tually supports w 2 because w 1 is probably providing partial fact. Therefore, If fact f 2 contains authors that are not in fact f 1 ,then f 2 is actually supported by f 1 . The implica-tion from f 1 to f 2 is defined as follows. If f 1 has x authors and f 2 has y authors, and there are z shared ones, then imp ( f 1  X  f 2 )= z/x  X  base sim ,where base sim is the threshold for positive implication and is set to 0.5. Figure 3: Accuracies of TruthFinder and Voting Figure 3 shows the accuracies of TruthFinder and Voting . One can see that TruthFinder is significantly more ac-curate than Voting even at the first iteration, where all bookstores have the same trustworthiness. This is because TruthFinder considers the implications between different
For simplicity we do not consider the order of authors in this study, although TruthFinder can report the authors in correct order in most cases. facts about the same object, while Voting does not. As TruthFinder repeatedly computes the trustworthiness of bookstores and the confidence of facts, its accuracy increases to about 95% after the third iteration and remains stable. It takes TruthFinder 8.73 seconds to pre-computes the im-plications between related facts, and 4.43 seconds to finish the four iterations. Voting takes 1.22 seconds.
Figure 4 shows the relative change of the trustworthiness vector after each iteration, which is defined as one minus the cosine similarity of the old and new vectors. We can see TruthFinder converges in a steady speed.
 In Table 3 we manually compare the results of Voting , TruthFinder , and the authors provided by Barnes &amp; Noble on its web site. We list the number of books in which each approach makes each type of errors. Please notice that one approach may make multiple errors for one book.
 Table 3: Compare the results of Voting , TruthFinder , and Barnes &amp; Noble
Voting tends to miss authors because many bookstores only provide subsets of all authors. On the other hand, TruthFinder tends to consider facts with more authors as correct facts because of our definition of implication for book authors, and thus makes more mistakes of adding in incorrect names. One may think that the largest bookstores will provide accurate information, which is surprisingly un-true. Table 3 shows Barnes &amp; Noble has more errors than Voting and TruthFinder on these 100 randomly selected books.

Finally, we perform an interesting experiment on finding trustworthy web sites. It is well known that Google (or other search engines) is good at finding authoritative web sites. But do these web sites provide accurate information? To answer this question, we compare the online bookstores that are given highest ranks by Google with the bookstores with highest trustworthiness found by TruthFinder .Wequery Google with  X  X ookstore X  2 , and find all bookstores that ex-ist in our dataset from the top 300 Google results. The accuracy of each bookstore is tested on the 100 randomly selected books in the same way as we test the accuracy of This query was submitted on Feb 7, 2007.
 TruthFinder . We only consider bookstores that provide at least 10 of the 100 books.
 Table 4: Compare the accuracies of top bookstores by TruthFinder and by Google
Table 4 shows the accuracy and number of books provided (among the 100 books) of different bookstores. TruthFinder can find bookstores that provide much more accurate infor-mation than the top bookstores found by Google. TruthFinder also finds some large trustworthy bookstores, such as A1 Books (not among the top 10 shown in Table 4) which pro-vides 86 of 100 books with accuracy of 0.878. Please notice that TruthFinder uses no training data, and the testing data is manually created by reading the authors X  names from book covers. Therefore, we believe the results suggest that there may be better alternatives than Google for finding ac-curate information on the web.
In this paper we introduce and formulate the Veracity problem, which aims at resolving conflicting facts from mul-tiple web sites, and finding the true facts among them. We propose TruthFinder , an approach that utilizes the inter-dependency between web site trustworthiness and fact con-fidence to find trustable web sites and true facts. Experi-ments show that TruthFinder achieves high accuracy at finding true facts and at the same time identifies web sites that provide more accurate information.
