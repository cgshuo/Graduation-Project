 1. Introduction
The basic idea for TDT was originated in 1996, when the DARPA realized that it needed a technology to television broadcasts, and www sources.
The TDT2000 project embraces five key technical challenges, namely topic segmentation, topic tracking, ( TDT2000, 2000 ). In this paper we report our work on topic tracking and link detection. modeling technique.
 words that are associated with the story even when they do not appear in the story. a new word depends on the previous word, while for a trigram, the probability of a new word depends on have a limitation in handling long-distance dependences. Consider the sentence:
We can recognize a long-distance dependency 1 between  X  X ead X  and  X  X ook X . Recently, there are language ( Charniak, 2001; Chelba &amp; Jelinek, 1998; Roark, 2001; Stolcke, 1995 ).
In this paper, we propose a new dependency structure language model to overcome the limitation of uni-gram and bigram models in TDT and document retrieval. The dependency structure language model is also handle the long-distance dependencies in TDT and document retrieval.
 clusion and future works. 2. Dependency structure language model bility distribution, which is defined by a PCFG directly as a language model ( Stolcke, 1995 ). Another reduce parsers. So these language models are impossible to be applied to TDT and information retrieval retrieval usually do not need such non-terminal symbols or POS tags.

Because of these reasons, we propose a new dependency structure language model for TDT. To capture long-distance dependences, our model adopts the Chow expansion theory and a dependency parse tree, as described below. 2.1. Chow expansion theory and dependency parse tree
Consider a story S = s 1 , s 2 , ... , s n , that is, s i ability P ( S ) can be expressed as follows: If the terms of S are statistically independent, we can write P  X  S  X  X  independent, but we can number the terms so that P ( s i j s s . For example, suppose that in a corresponding dependence tree ( Fig. 1 ).

Then P ( s 1 , s 2 , s 3 , s 4 , s 5 ) can be written as P ( s uct expansion where the function j ( i ) exhibits the limited dependence of one variable on the preceding variables. may be defined by maximizing the sum where I ( i , k ) represents the expected mutual information provided by i about k ,
Van Rijsbergen explored one way of removing the independence assumption using the Chow expansion theory between index terms using a maximum spanning tree approach. The extent to which two index terms depend non-relevant document sets.
 model captures sentence level term dependencies using a maximum spanning tree approach, similar to Van Rijsbergen X  X  modeling of document level term dependencies.

Chow, Rijsbergen, and Nallapati all based their probabilities on a maximum spanning tree using a mutual relations in the syntactic structure, which helps to capture the underlying semantics of a story. (or governor, parent), and another word called a modifier (or dependent, daughter). Dependency grammars word may modify at most one word. The root of the dependency tree does not modify any word. It is also to modifiers.
 We use MINIPAR as a dependency parser, which is a principle-based English parser ( Lin, 1994; MINI-
PAR, 1998 ). MINIPAR represents its grammar as a network where nodes represent grammatical categories and links represent types of dependency relationships. An evaluation with the SUSANNE corpus shows that
MINIPAR achieves about 88% precision and 80% recall with respect to dependency relationships. MINIPAR 1998 ).

Because we use a dependency parse tree generated by a linguistic dependency parser (i.e. MINIPAR) also define j ( i )=0if s i is the root of a dependency parse tree. Finally, we define P ( s write P ( S ) as follows: 2.2. Dependency structure language model for TDT to compute the likelihood that a test story S would be generated from the estimated topic model. 2.2.1. Unigram language model approach to topic tracking where s 1 , s 2 , ... , s n represents the sequence of n terms s exceeds a certain threshold, the tracking system will reject H Because a topic language model is sparse, the topic language model must be with smoothing methods. In
Dirichlet distribution with parameters Thus, the Dirichlet smoothing model is given by where c ( s i ; T ) denotes the count of occurrence of s i
P c  X  s k ; T  X  .

Then the topic language model is augmented with the above smoothing methods with a background language model, respectively: where the subscript J and D mean Jelinek-Mercer and Dirichlet smoothing, respectively. P on the set of training stories for a topic T using maximum likelihood estimator and P ( s large background corpus.
 presented as follows: 2.2.2. Dependency structure language model for topic tracking in the dependency parse tree of the story S . Then we obtain the product expansion as in Chow expansion theory: where the function j ( i ) exhibits the head term of a term s Because of data sparseness problem, we apply two smoothing methods (i.e. Jelinek-Mercer smoothing and Dirichlet smoothing) with a background language model, respectively: where c ( s i s j ( i ) ; T ) denotes the count of occurrence of s parse trees in the topic language model of topic T , c ( s
In the formula, P J ( s i j s j ( i ) , B ) and P D ( s i to P ( s i j s j ( i ) , B ), respectively: where c ( s i s j ( i ) ; B ) denotes the count of occurrence of s parse trees in the background language model and c ( * s j ( i )
We define a likelihood ratio (LR) for dependency structure model like unigram language modeling approach for TDT: story length, we obtain the normalized log likelihood ratio (NLLR): gram model and dependency structure model as follows: forming value is chosen. 2.2.3. Language modeling approach to link detection The link detection task focuses on the core technologies used to compare two stories.
In the language modeling approach to link detection, we build a topic model T ( S , S 2 ). We then compute the probability that the second story S the same method of topic tracking system (e.g. NLLR unigram-J a two-way score to add symmetry to the formula as follows: 3. Experiments and the results following two questions in both application tasks: performance of dependency structure language model with that of the state-of-the-art unigram language model for TDT. answer this question, we compare the results for the dependency structure language model using depen-dency parse tree with the results for the same model using only bigram dependency (i.e. assuming the dependency parse tree is linear).
 3.1. TDT evaluation method in terms of the probability of miss and false alarm errors ( P precision and recall measures, a low P Miss corresponds to high recall and a low P precision.

These error probabilities are then combined into a single detection cost, C false alarm errors: where C Miss and C FA are the costs of a Miss and a False Alarm, P ities of a Miss and a False Alarm, and P target and P non-target C Det is the bottom-line representation of TDT task performance that is used to judge TDT systems. Because this value varies with the application, C Det will be normalized so that ( C one without extracting information from the source data. This is done as follows: system.

There are two reasonable methods of estimating detection error probabilities, called story-weighted and method for evaluation. 3.2. Dataset and topics
All of the following experiments were performed on the TDT-3 dataset. The TDT-3 corpus contains about 30,001 X 30,060 topics) for evaluation.

Stemming is done using MINIPAR ( MINIPAR, 1998 ). A list of 568 high frequency words is used to elim-dependencies in a sentence. 3.3. Topic tracking for the performance of dependency structure language model using the Dirichlet smoothing method (the best parameter values are k = 0.12, l 1 = 10,000, l 2 = 1, and l guage modeling baseline (the parameter value is l = 10,000) in a topic tracking task. A DET curve is a marks the optimal threshold and the corresponding normalized minimum cost ( C the dependency structure language model performs better than the state-of-the-art unigram language model for almost all threshold setting. The normalized minimum cost is reduced from 0.1110 to no difference in performance. Therefore the dependency structure language model may be preferred to the recall).

We also perform the experiment for bigram language model (using the same dependency structure language model but assuming the dependency parse tree is linear, i.e. s l better than the bigram language model. The normalized minimum cost of the bigram language model is 0.1079. For Jelinek-Mercer smoothing, the results are similar to Dirichlet smoothing. 3.4. Link detection Fig. 4 shows the DET curve for the performance of dependency structure language model using the
Dirichlet smoothing method (the best parameter values are k = 0.17, l task. We observe that the dependency structure language model performs better than the unigram language model in this task as well. The normalized minimum cost is reduced from 0.1237 to 0.1162. On the high-in performance.

We also perform the experiment for bigram language model (the parameter values are k = 0.17, l performs better than the bigram language model. The normalized minimum cost of the bigram language model is 0.1188. For Jelinek-Mercer smoothing, the results are similar to Dirichlet smoothing.
From the above experiments, in general, we can verify that the dependency structure language model is more effective than the conventional language modeling (i.e. unigram and bigram language model) for
TDT. 4. Conclusion
In the language model for TDT and information retrieval, the unigram and bigram language models have the weakness of the unigram and bigram language models in TDT. The dependency structure language model experiment results, we drew the following conclusions:
Based on the comparison between the dependency structure language model and traditional language model, we conclude that the dependency structure language model for TDT is an effective method. In our experi-ments, the dependency structure model gives a better performance than the traditional language model.
Based on the comparison between the dependency structure and bigram dependency in language modeling for TDT, we also conclude that the dependency structure is more effective than the bigram in language modeling for TDT.

The disadvantage in using the dependency parser is that the computational cost of the dependency parse tree becomes high (in general, the cost of parsing is O( n to catch the underlying semantics and long-distance dependencies in future. References
