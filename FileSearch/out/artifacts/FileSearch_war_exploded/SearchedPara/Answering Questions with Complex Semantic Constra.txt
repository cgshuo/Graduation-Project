 A knowledge-based question-answering system (KB-QA) is one that answers natural language questions with informa-tion stored in a large-scale knowledge base (KB). Existing KB-QA systems are either powered by curated KBs in which factual knowledge is encoded in entities and relations with well-structured schemas, or by open KBs , which contain as-sertions represented in the form of triples (e.g.,  X  subject; rela-tion phrase; argument  X  ). We show that both approaches fall short in answering questions with complex prepositional or adverbial constraints. We propose using n -tuple assertions, which are assertions with an arbitrary number of arguments, and n -tuple open KB (nOKB), which is an open knowledge base of n -tuple assertions. We present taqa , a novel KB-QA system that is based on an nOKB and illustrate via experiments how taqa can effectively answer complex ques-tions with rich semantic constraints. Our work also results in a new open KB containing 120M n -tuple assertions and a collection of 300 labeled complex questions, which is made publicly available 1 for further research.
A question-answering (QA) system is one that automati-cally answers questions posed in natural languages (NLs). Many of these systems are powered by knowledge bases (KBs), whose information is often encoded as entities and relations with well-structured schemas. With a knowledge-based QA (KB-QA) system, a natural-language question is typically answered in two steps: (1) the free-form natural-language question is transformed into a structured query (e.g., Sparql query [18]); and (2) answers are retrieved by executing the structured query against a KB [4, 5, 13, 12, 3, 18]. Most of existing research on KB-QA systems focuses on answering questions with simple semantic constraints , which are often expressed by the verb phrase of one or multiple bi-nary relations, such as: single relation: Who [ invented ] rel the telephone? [4] c  X  multiple relations: Who [ was married to ] rel 1 an actor that
Relatively few efforts, however, have been spent on an-swering questions with rich semantic constraints, such as those that are expressed via prepositional or adverbial mod-ifiers. As an example, consider the following question: Question: What was the currency of Spain before 2002 ? Correct Answer: Peseta Incorrect Answer: Euro .
 In this example, the prepositional phrase  X  X efore 2002 X  im-poses a temporal constraint of the question. As we will elab-orate shortly, such constraints are not conveniently handled by QA systems built on existing KB models. Very often, such constraints are lost during the transformation of ques-tions into KB queries, resulting in incorrect answers (e.g., the answer  X  X uro X  is returned instead of  X  X eseta X ). The ob-jective of this paper is to study how questions with complex semantic constraints, particularly those expressed via prepo-sitional or adverbial phrases, can be answered on a KB-QA system. For convenient, we call those questions with com-plex semantic constraints  X  X omplex questions X  , while those without  X  X imple questions X  .

To understand the difficulties of existing KB-QA systems in answering complex questions, let us first briefly describe their underlying KBs. KB-QA systems can be categorized into curated KB-QA systems and open KB-QA systems . While the former has a longer history with a number of implemen-tations (e.g., DEANNA [18] and ParaSempre [5]), the lat-ter has been recently proposed by Fader et al. ( [13] and oqa [12]). A curated KB-QA system is built upon a curated KB (e.g., Freebase [6] and DBpedia [1]), which is collaboratively and manually created. Information is mod-eled as entities and relations that strictly follow a predefined schema. Figures 1(a) and 1(c) show two snippets taken from Freebase. Since a curated KB is manually edited, it is gen-erally accurate and precise. There are, however, two aspects of it that weaken its ability in answering complex questions. [Query transformation] An open question has to be first transformed into a structured query (such as Sparql query) that is compatible with the KB schema, after which the query is executed against the KB to obtain answers. For example, Figure 1(b) shows a Sparql query ( Y 1 ) for the question Q 1 :  X  X hat was James K. Polk? X  This query is obtained by observing that government_position is an oc-cupation relation that semantically matches the verb  X  X as X  in Q 1 . The transformation from Q 1 to Y 1 is straightforward considering that Y 1 involves only one relation in the KB. Now consider a complex question Q 2 :  X  X hat was James K. Polk before he was president? X  The additional semantic constraint given in Q 2 leads to a much more complex query Y . Although the temporal information of the occupation that is needed to answer Q 2 has been encoded using Com-pound Value Types (CVTs) in Freebase, as denoted by two  X  nodes in Figure 1(a), existing curated KB-QA systems do not yet have the logic to leverage this information and correctly generate the required queries of complex questions in general. As a result, complex constraints (such as  X  X e-fore he was president X  ) of questions are often neglected and incorrect answers ensue. [Incompleteness] Another problem faced by curated KB-QA systems is the often incomplete KB schema. This of-ten results in insufficient knowledge to resolve the complex constraints posed in a question. For example, the question Q 3:  X  X hat was the currency of Spain before 2002? X  cannot be answered by the Freebase snippet shown in Figure 1(c) because the years of circulation of the currencies are not recorded in the KB. 2
Recently, Fader et al. put forward open KB-QA systems as an alternative approach for KB-QA [12, 13]. An open KB [14] is a large collection of n -tuple assertions that are automatically extracted from web corpora by means of open information extraction (open IE) techniques [2, 11, 8, 17]. An assertion is a string n -tuple ( n  X  3) which contains one subject ( sbj ), one relation phrase ( rel ), and multiple argu-ments ( arg ) in the form:
Each element in the tuple is called a field . Table 1 gives examples of such n -tuple assertions. The first assertion A for example, is a 4-tuple: it consists of 1 subject (field), 1 relation phrase (field) and 2 arguments (fields). Since assertions in an open KB are automatically acquired from unstructured text, they are often unnormalized and noisy. Nevertheless, two unique features of open KB make it poten-tially more suitable for QA. First, as assertions are extracted from text, all of their fields are strings in a natural language. They can therefore naturally be used to answer NL ques-tions by string matchings. In particular, unlike a curated KB, there is not a predefined schema and hence no complex query transformation is needed. Second, assertions in n -tuple form contain rich semantic information, which can be used to answer questions with complex semantic constraints. For example, Question Q 2 , which is difficult to handle by a curated KB, can be easily answered by matching a derived tuple query  X   X  James K. Polk  X ;  X  was  X ; ?x ,  X  before he was pres-ident  X   X  against assertion A 1 in Table 1.

Although open KBs stand out in handling complex ques-tions, the state-of-the-art open KB-QA system, oqa [12], is designed to operate on triplet assertions, each of which contains only one single argument (in addition to a subject and a relation phrase). In fact, the oqa system is pow-ered by such an open KB of triples. This restricted model of assertions (triples instead of n -tuple) limits the ability of oqa in answering questions with complex semantic con-straints. It is non-trivial to adapt the methodology of oqa to an n -tuple open KB for two reasons. First, questions with semantic modifiers exhibit a variety of syntactic forms. oqa , however, parses simple questions into tuple queries us-ing a small number of hand-crafted Part-of-Speech (POS) templates. While this approach works very well with triples under a rigid subject-relation-argument format, it is difficult to extend those templates to cover questions with complex semantic constraints, especially those that require ( n -tuple) assertions with multiple arguments (e.g., assertions A 1 and A ) to derive answers. Second, the query language of oqa only supports querying triplet assertions. The query lan-guage needs to be extended to handle n -tuple open KB, whose assertions contain arbitrary numbers of arguments.
Considering the limitations of existing KB-QA systems in answering complex questions, our goal is to design and im-plement a novel open KB-QA system that can fully leverage the rich semantics of n -tuple assertions for answering ques-tions with complex semantic constraints. To achieve this goal, two key components in an open KB-QA system, ques-tion parsing and open KB querying , have to be redesigned. As an overview, we propose a generic question parsing method that is based on dependency parsing. Our method is able to parse syntactically diverse questions with rich semantic constraints. Moreover, we put forward an alignment-based answer extraction approach, which is capable of efficiently answering tuple queries using assertions with an arbitrary number of arguments. Apart from these two contributions which focus on leveraging n -tuple open KBs, we also im-prove the general quality of KB-QA by designing a question paraphraser powered by rich association features [5]. Ques-tion paraphrasing has been proven important for open KB-QA in bridging the lexical/sytactic gap between user-issued questions and relevant KB assertions. Existing works em-ploy shallow statistical features (e.g., PMI values) to eval-uate each paraphrased question, which is unreliable when the dataset used for paraphrasing is noisy. To improve ac-curacy, we instead use rich association features to learn soft paraphrase rules.

In summary, the main contributions of our paper are: [nOKB] Existing open KBs (such as Open IE [12], Probase [19] and Nell [7]) contain only triples. As we have ex-plained, answering complex questions often require n -tuple assertions of multiple arguments because they are much richer in their semantic contents. Existing open KBs are thus in-adequate. We fill this void by creating an n -tuple open KB (called nOKB) of 120M assertions together with an evalu-ation question set of 300 (and counting) complex questions with labeled reference answers, which is released to the pub-lic to facilitate further research. [TAQA] We designed and implemented an n -Tuple-Assertion-based Question-Answering system (called taqa ) that oper-ates on an nOKB for answering questions with complex se-mantic constraints. We detail the key features of all the major components of taqa and discuss how they were de-signed to work with n -tuple assertions. [Experiments] We conducted extensive experiments to com-pare taqa against state-of-the-art open KB-QA and curated KB-QA systems. Our results show that taqa outperforms other KB-QA systems in answering complex questions. This shows strong evidence that n -tuple assertions are indispens-able in answering questions with rich semantic constraints.
The rest of the paper is organized as follows. Section 2 describes the workflow of taqa and its major components. Section 3 shows our experimental results comparing taqa against other state-of-the-art KB-QA systems. Section 4 further illustrates our proposed system by means of a case study. Section 5 discusses related works. Finally, Section 6 concludes the paper and discusses several future research directions.
In this section we introduce our system taqa . We first give an overview of taqa  X  X  workflow, followed by implemen-tation details of each major component.
Given a natural language question Q , taqa processes the question and outputs a list of ranked candidate answers A . Figure 2 gives an illustrative example that shows the various steps involved. There are four key components:
Question Paraphrasing (Section 2.2), which rewrites the original user-issued question into a set of paraphrased questions. Web users generally use informal and casual wordings and expressions in questions. These questions are difficult to parse and are hard to match with assertions in an open KB. Question paraphrasing reformulates original informal questions into formalized ones, which share simi-lar grammatical structures and vocabulary with open KB assertions. Moreover, each paraphrased question derives tu-ple queries that are executed against the KB for answers. By rewriting the user X  X  question into multiple paraphrased questions, multiple queries are generated. This enhances the recall of the QA system. To improve the quality of question paraphrasing, in taqa , we employ rich association features to help find high-quality paraphrased questions.

Question Parsing (Section 2.3), which parses each para-phrased question into a set of tuple queries . A tuple query (e.g.,  X  ?x ;  X  wrote  X ;  X  Harry Potter  X   X  ) is similar to an n -tuple assertion except that one of its fields is unknown. taqa em-ploys a novel question parser that is able to identify rich semantic constraints expressed in the question by analyzing its dependency structure.

Open KB Querying (Section 2.4), which executes each tuple query against the open KB to obtain a list of candi-date answers. In order to effectively answer tuple queries us-ing semantically rich n -tuple assertions, we propose a novel alignment-based answer extraction approach that is capable of pinpointing answers from open KB assertions that have various numbers of arguments.

Answer Ranking (Section 2.5), which consolidates and ranks candidate answers derived from different tuple queries. Each candidate answer is associated with a set of features that are obtained through the various steps in the whole question-answering process. taqa employs a log-linear model on the features to rank the candidate answers. Due to the large feature space (which consists of over 20 , 000 sparse fea-tures), we use AdaGrad [10] to facilitate fast and effective learning of the ranking model.
We follow the approach taken by oqa [12] of paraphrasing a question using paraphrasing templates . A paraphrasing template ( PT ) has the form where p src and p tgt are the source and target templates, re-spectively. Each such template is a string with wildcard vari-ables. The purpose of a PT is to map a syntactic pattern to an alternative one to facilitate the retrieval of relevant assertions in the open KB. The following is an example PT : PT 1 : What kind of money did they use in  X  NP 7 X  where  X  NP is a wildcard variable that captures a noun phrase. A user-issued question Q that matches the source template of a PT is rewritten according to the target template of the PT . Complex semantic constraints expressed as preposi-tional/adverbial phrases in Q are left out in template match-ing. These phrases are retained in the rewritten questions in their original form. An example of applying PT 1 on question Q 4 is shown in Figure 2. Note that the rewritten question Q 5 allows a perfect match with assertion A 2 (Table 1), which provides the best answer to Q 4 .

We use the WikiAnswers paraphrasing templates dataset released by [12], which contains 5 million templates created by mining related questions tagged by users on WikiAn-swers . The dataset is large but noisy, containing many invalid PTs . Consider the following PT : PT 2 : What kind of money did they use in  X  NP 7 X  In [12], this problem is addressed by scoring each paraphras-ing template using shallow features like its PMI value and Part-of-Speech (POS) tags. However, we found that using those shallow features alone is often inadequate. For exam-ple, PT 1 has identical POS tags as those of PT 2 .
In this paper we propose using rich association features to identify high-quality question paraphrasing templates from noisy data. The intuition is that strong associations between elements in p src and p tgt often indicate good paraphrasing templates. For example, the association between  X  X oney X  and  X  X ational currency X  ( PT 1 , a good PT ) is much stronger than that between  X  X oney X  and  X  X conomic system X  ( PT 2 , a bad PT ). Although the idea of using association features has been explored in curated KB-QA systems [5] to rank gener-ated queries, our work first applies it in the context of open KB-QA for measuring the quality of question paraphrasing.
Specifically, for each PT : p src 7 X  p tgt , we iterate through spans of text s  X  p src and t  X  p tgt to identify a set of association phrases ,  X  s,t  X  . For each  X  s,t  X  , we generate a set of indicative association features using the feature tem-plates listed in Table 3. These features are used in the answer ranker (Section 2.5) to measure the quality of the PT s used in deriving the answers. Following [5], we iden-tify association phrases by looking-up a table that contains 1 . 3 million phrase pairs. For example, an indicator feature, I {  X  X oney X   X  p src  X   X  X ational currency X   X  p tgt } is generated for the association phrases  X   X  X oney X  ,  X  X ational currency X   X  . During training, the answer ranker will learn each feature X  X  weight based on its predictive power. Also, answers derived from high quality PT s will be given more credits. With our training data, the question paraphraser uses over 20,000 as-sociation features.

Algorithm 1: question parsing algorithm
Besides the association features introduced above, we also use a set of statistical features such as the PMI value of the PT s, the log normalized frequency of p src and p tgt , etc. The full set of features used in paraphrasing is listed in Table 3.
Each paraphrased question Q is parsed into a set of struc-tured tuple queries (TQs): T Q = { TQ 1 , TQ 2 ,..., TQ Each tuple query TQ i has the form: TQ i =  X  sbj ; rel ; Arg  X  , which is similar to that of an n -tuple assertion (see Sec-tion 1). For a TQ, either the subject field sbj or one of the arguments in Arg is a wildcard (denoted by ? x ), which in-dicates the hidden answer the question Q seeks for. We call that wildcard, ? x , the query focus of Q , denoted by F Q
Existing open KB-QA systems employ a small number of hand-crafted POS templates to parse questions into tu-ple queries [12], which are limited in coverage and can only handle questions without complex semantic constraints. In this paper we propose a novel approach of parsing questions by dependency analysis . This approach is partially inspired by recent advances in open IE [17, 8] research, where depen-dency parsing has been used to extract open IE assertions, but never to parse questions into tuple queries. Algorithm 1 shows our question parsing algorithm.

Given a question Q , taqa first applies the Stanford parser to construct a dependency tree , DT Q (Line 2). The nodes in DT Q are made up of words in Q . The root of DT Q is usually the main verb of Q and each edge in DT Q denotes a pairwise dependency relation between two words in Q . For example, in the sentence:  X  Bell makes electronic products  X , the word  X  X ell X  is a nominal subject ( nsubj ) of  X  X akes X  , and hence the dependency  X  X ell X  nsubj  X  X  X  X  X  X   X  X akes X  is derived. Table 2 shows three example questions ( Q 5 , Q 6 , Q 7 ) and their respective dependency trees. Given a dependency tree, taqa analyzes the tree to identify the various fields of a tuple query (i.e., sbj , rel , and the arguments in Arg ) as follows: [Identify Relation Phrase] (Line 3) The root word root of DT Q is included in rel together with auxiliary words that are children of root and that satisfy certain dependency rela-tions with root . These dependency relations include copula ( cop ), auxiliary ( aux ), passive auxiliary ( auxpass ), phrasal verb particle ( prt ), etc. For example, the relation phrase of question Q 6 in Table 2 is  X  X s located X  because  X  X ocated X  is the root verb and  X  X s X  is related to  X  X ocated X  by the dependency  X  X s X  auxpass  X  X  X  X  X  X  X   X  X ocated X  . [Identify Subject] (Lines 6-7) First, all nodes that are nominal subjects ( nsubj ) or passive nominal subjects ( nsub-jpass ) of root are collected in a set S . Then, for each node s  X  X  , if the question word 3 , qword , is a descendant of s , the subject sbj is the query focus ?x . Otherwise, sbj includes all the descendants of s . Note that there could be multiple nominal subjects in DT Q . In this case, we generate one tu-ple query for each nsubj . For example, question Q 5 in Table 2 derives 2 tuple queries TQ 1 and TQ 2 . [Identify Arguments] (Lines 9-16) Any child node w of root that is not identified as a subject ( sbj ) or does not belong to the relation phrase ( rel ) in the previous step de-rives an argument in Arg . Specifically, if the question word is a descendant of w , we create an argument that includes a query focus ?x ; otherwise, the argument includes all the descendants of w . [Compound Queries] (Lines 19-22) For each tuple query TQ i obtained from the previous steps, we augment it by de-ducing an answer type , AnsType , forming a compound con-junctive query [ TQ i  X  X  ? x ; X  is-a  X ; AnsType  X  ]. The purpose is to prune the search space for the answer (when the query is later executed against a KB) by restricting the scope of the query focus. For example, Question Q 7 in Table 2 derives the tuple query TQ 7 . Note that the focus for this query can be restricted to the type  X  X ovie X  , which is expressed by the augmenting tuple query  X  ?x ;  X  is-a  X ;  X  movie  X   X  . taqa tifies the noun phrase between qword and the auxiliary 4 or root verb as AnsType . [Generate Relaxed Queries] Finally, if the question Q is a complex question (with prepositional/adverbial con-straints), taqa will remove the complex constraints and ad-ditional objects of root (e.g.,  X  X haracter Lando Calrissian X  in Q ) and derives a relaxed question Q 0 . Algorithm 1 is then applied onto Q 0 to generate (relaxed) tuples queries. The reasons are twofold. First, some complex constraints are re-dundant and uninformative. For example, consider question Q 6 in Table 2. The constraint  X  X n world map X  is uninforma-tive; Q 6 can be answered as well in its relaxed form. Second, using relaxed tuple queries can improve coverage, since there are likely assertions in the KB that do not mention the con-straints given in the original question Q . TQ 6 in Table 2 is a relaxed tuple query of question Q 6 . The use of relaxed tuple queries in answering questions is also illustrated in Figure 2 (see the right branch of the workflow).
After obtaining the set of tuple queries T Q , the next task is to execute each query TQ  X  T Q against the open KB to obtain a set of candidate answers A TQ . This is carried out in two steps. First, a set of assertions, R TQ , in the open KB that are relevant to the query TQ is retrieved. In taqa assertions are indexed by Apache Solr 5 , which retrieves a ranked list of assertions based on the terms mentioned in the assertions and those in the tuple query. We refine R by removing assertions that have fewer fields than TQ , and then retaining the top-50 of the remaining ones.

Next, we need to align the fields of TQ against those of each assertion r  X  X  TQ . This is done to identify the field in r that matches the query focus F Q , which is then taken as the answer of TQ . Unlike oqa , for which queries and asser-tions are both triples, taqa deals with n -tuple queries and assertions with an arbitrary number of arguments. The mul-tiplicity of Arg fields involves a complex alignment problem. We put forward a novel answer extraction approach which models the alignment problem as a matching problem on a weighted bipartite graph. Figure 3 illustrates the alignment between a tuple query TQ 7 (mentioned in Table 2) against the assertion r :  X   X  Billy Dee Williams  X ; ...  X  (shown in the figure). Specifically, in the bipartite graph G , fields in TQ form one type of nodes while fields in r form another. Let TQ .i and r.j denote the i -th field of TQ and the j -th field of r , respectively. The weight of the edge connecting TQ .i and r.j in G is given by the following similarity measure: similarity ( TQ .i,r.j ) =  X   X  text similarity ( TQ .i,r.j ) where  X  is a tuning weight (  X  = 0 . 5 in the experiment). text similarity is the TF-IDF score of two lemmatized strings. pattern similarity is the sum of three indicator functions that utilize POS/prefix/question patterns:
We then seek all optimal matchings for the following con-strained optimization: where x ij = 1 iff TQ .i is aligned to r.j . The constraints x 11 = 1 forces the alignment between two sbj fields. Likewise with x 22 = 1 for the rel fields. The field of r that is aligned to the query focus field of TQ in each optimal matching is added to A TQ as an answer. As an example, in Figure 3, there are two optimal matchings (the query focus field has equal weights with  X  in Star Wars  X  and  X  in the 1980s  X ), deriving two answers ( a 1 and a 2 ).

The tuple query used in the illustration (Figure 3) is actu-ally part of a compound query ( TQ 7 in Table 2). For such cases, taqa verifies the answer by querying the KB using the other part of the query. For example, taqa evaluates the tuples  X   X  Star Wars  X ;  X  is-a  X ;  X  movie  X   X  and  X   X  the 1980s  X ;  X  is-a  X ;  X  movie  X   X  against the KB. Answer a 2 is eliminated because it is not supported by the assertions in the KB.
Generally, a question Q is paraphrased into multiple ques-tions, each such question is parsed into multiple tuple queries and each tuple query can derive multiple answers. taqa col-lects all these candidate answers into an answer set A . The final stage of the QA process is to determine the best answer in A . An answer a  X  X  is associated with many features as it is derived along the paraphrasing-parsing-querying pro-feature vector of a . taqa uses these features to rank the candidate answers. Table 3 lists the features used in taqa
The answer set A is first consolidated . In many cases, the same answers are derived via different paraphrasing-parsing-querying outputs. These answers have the same string value but different feature vectors, which should be combined. Specifically, if a i , a j  X  A are same answers, then their fea-ture vectors are combined by taking the best value for each individual feature. For example, if f k represents the align-ment score (Equation 1) of the assertion that derives an an-swer, then the combined feature value is max( f k ( a i ) ,f because a higher alignment score indicates better answer quality. This simple approach of consolidating features has been proven useful in [15]. Furthermore, we add statistical features that measure the  X  X opularity X  of answers. These
Table 5: Sample questions from WebQ and CompQ features include the occurrence frequency of an answer in A , and the answer X  X  n-gram correlation defined in [9].
After consolidation, we apply a standard log-linear model to estimate the probability p ( a | Q ) for each a  X  X  : where  X  k denotes the k -th feature X  X  weight, and rank candi-date answers accordingly. We limit the size of A to the top 200 candidates when calculating p ( a | Q ).

We tune the feature weights  X  = {  X  k } using a set of N question and gold-standard answer pairs D = { ( Q t ,a t ) } , by maximizing the log-likelihood of the training data: taqa employs AdaGrad [10] to effectively learn the weights of over 20 , 000 sparse features.
In this section we evaluate taqa . We primarily address the following two questions: (1) How does taqa  X  X  perfor-mance compare with the state-of-the-art open KB-QA and curated KB-QA systems, especially in answering complex questions? (2) How do the various components of taqa im-pact the system X  X  performance? We will first describe the open KB taqa uses, the evaluating question sets, the per-formance metrics, and two other KB-QA systems, namely, oqa and ParaSempre , before presenting the results. We also present insightful case study in Section 4. [Open KBs] There are three well known open KBs, namely, Open IE [12], Probase [19] and Nell [7]. Open IE is a large open KB built by performing open IE on ClueWeb . Probase is an open KB of is-a assertions extracted from 1 . 68B web pages. Nell is a relatively small open KB which contains highly precise assertions. Assertions in the above three KBs are all triples. As we have explained, n -tuple assertions are richer in semantics, and are more suitable for answering complex questions. Hence, we build a new open KB of n -tuple assertions (called nOKB) using the latest open IE technique. Here, we briefly discuss how nOKB was built.
First, we need to collect a set of documents D from which assertions are extracted. We collect all English Wikipedia documents into a set D wiki . Also, for each question Q in an evaluation question set QS (we will explain shortly how QS is obtained), we submit Q to a commercial search engine and retrieve top-200 documents. The retrieved documents for all the questions form the set D QS . We get D = D wiki  X  X  QS which contains 5.7M web pages. We use OpenNLP 6 to ex-tract sentences from documents, and filter out non-English sentences and those with more than 100 words. After that, we apply Open IE 4.1 7 to the sentences to obtain n -tuple assertions. We remove assertions whose subject or argument fields are either stop words or more than 10 words in length. The process results in 163M sentences and 120M n -tuple as-sertions. These 120M assertions form nOKB. taqa operates on assertions provided by all 4 open KBs. We denote the combined KB, nOKB+. Table 4 summarizes these KBs. [Question Sets] The experiments were conducted on two sets of questions, WebQuestions and ComplexQuestions. Each question set is a collection of (question, gold-standard-answer) pairs. Table 5 gives example pairs taken from the two sets.
WebQuestions [4] (abbrev. WebQ ) is the de facto ques-tion set used to evaluate curated KB-QA systems. It consists of 5 , 810 question-answer pairs. Questions are collected by the Google Suggest API and manually answered by AMT workers using Freebase. WebQ is moderated for curated KB-QA systems in that all questions are answerable with Freebase (a curated KB). We observe that most of the ques-tions in WebQ are simple questions; only 80 (4%) of the questions in the test set of WebQ (2 , 032 questions) come with complex semantic constraints.

ComplexQuestions (abbrev. CompQ ) is a new dataset created in this work, which focuses on open domain complex questions with prepositional/adverbial constraints. CompQ consists of 300 complex questions obtained as follows: First, all 80 complex questions in WebQ are added to CompQ Next, we follow the approach given in [4] to crawl questions using the Google Suggest API. We start with a seed ques-tion Q  X  :  X  X ho did Tom Hanks play in Toy Story? X  . We google Q  X  to obtain at most 10 suggested questions. We retain those suggested questions that begin with a wh -word and which contain at least one preposition. For each such question, we remove the entities mentioned in the question and submit the resulting string to google to obtain more suggested questions. The process is repeated in a breadth-first manner. Due to space limitation, readers are referred to [4] for the details of the procedure. We collected a total of 100 , 000 questions, and then randomly picked 220 complex questions out of the pool. Of all the questions in CompQ 20 of them have either 2 or 3 complex constraints, the rest have 1 complex constraint each. [Metrics] WebQ comes with a set of answers that are la-beled as correct or incorrect [12]. For CompQ , we manually label the answers returned by the QA systems we tested. Given a question, each QA system computes a ranked list of answers and returns the top-ranked one as the final answer. We measure the performance of a system by its accuracy: [QA systems] We compare taqa against the current state-of-the-art KB-QA systems: oqa [12], which is the latest open KB-QA system based on triple-form assertions, and ParaSempre [5], which is an advanced curated KB-QA sys-tem powered by Freebase. We use nOKB+ as the open KB for both oqa and taqa . For oqa and ParaSempre , we use the trained models provided by their authors. 8 For taqa , we train its feature weights  X  on nOKB+ using the standard training set of WebQ (3,778 questions). Table 6 shows the accuracy ( acc ) of the the three KB-QA systems when presented questions from the two question sets. Readers are reminded that ParaSempre uses a differ-ent KB (Freebase) from that of oqa and taqa (nOKB+). The performance numbers of ParaSempre should not be compared directly with those of the other two systems. Nonethe-less, these numbers provide interesting references.
We first compare oqa and taqa in answering complex questions ( CompQ ). From Table 6, we see that oqa a hard time handling complex questions with an accuracy of only 2%. We studied how oqa handled each of the 300 questions in CompQ . We make the following observations about its poor performance. First, oqa relies on a limited number of hand-crafted POS templates for question pars-ing. These templates are inadequate in parsing questions with complex semantic constraints (see Section 2.3). In fact, only about 10% of the questions in CompQ are successfully parsed by oqa into tuple queries; the other 90% do not result in any tuple queries and thus are not answered by oqa . In sharp contrast, taqa employs dependency analysis in ques-tion parsing, which allows it to successfully parse 98% of the questions. Second, oqa was designed to handle triplets assertions. In particular, only the first three fields ( sbj , rel , and the first argument in Arg ) of assertions in nOKB+ are used. oqa is therefore unable to fully utilize the rich se-mantic information that is often displayed in the additional argument fields of n -tuple assertions. taqa , on the other hand, employs a matching approach to align complex tuple queries to n -tuple assertions. This allows taqa to make full use of the assertions X  argument fields, resulting in a very respectable accuracy of 39.3%.

We also used ParaSempre to answer CompQ questions and registered a 9 . 7% accuracy. Given a question Q , ParaSem-pre first parses Q into a number of Sparql queries. It then evaluates the queries and executes the top-ranked one against Freebase. We found that due to the complexity of questions in CompQ , ParaSempre generates an average of 2,090 candidate queries for each question in CompQ , in an attempt to cover all potential answers. It turns out that most of these queries are false queries , which lead to incor-rect answers. The big pool of queries makes it very difficult for ParaSempre to identify the best query to obtain the correct answers with any less-than-perfect ranking method. In contrast, taqa generates, on average, 25 tuple queries for each question in CompQ . The small query pool makes it easier to identify (rank) the best query for a correct an-swer. Moreover, many of the Sparql queries generated by ParaSempre fail in capturing the questions X  complex con-straints. For example, the query ParaSempre executed for the question:  X  X hat character did Anna Kendrick play in Twilight ? X  is without the underlined constraint. In con-trast, for the same question, taqa executes the tuple query  X   X  Anna Kendrick  X ;  X  play  X ; ?x ,  X  in Twilight  X   X  X  X  X  ?x ;  X  is-a  X ;  X  character  X   X  and extracts the correct answer from assertion  X   X  Anna Kendrick  X ;  X  plays  X ;  X  Jessica Stanley  X ,  X  in Twilight  X   X  .
Another reason why ParaSempre gives a relatively low accuracy for CompQ questions is the restricted coverage of Freebase (a curated KB). For example, ParaSempre fails to answer the question  X  X hen did Canada gain independence from England? X  because Freebase does not contain the fac-tual knowledge about the independence of countries.
Next, we consider questions from WebQ . Recall that ques-tions in WebQ are predominately simple questions and that all are answerable with information on Freebase. ParaSem-pre is very effective in answering them, which is reflected by the very high (45 . 8%) accuracy.

Because of the simplicity of WebQ questions, oqa does not encounter much problem in parsing questions or exe-cuting queries in triples form. Its gives a decent accuracy of 22 . 9% with WebQ . Despite the fact that simple ques-tions do not necessarily require the rich information pro-vided by the additional argument fields in n -tuple asser-tions (which taqa excels in manipulating), taqa still out-performs oqa significantly (with an accuracy of 35 . 6%). Be-sides the sophisticated techniques taqa employs in the para-phrasing, parsing, querying, and ranking, we found that an-other factor that contributes to the gap is the difference
Table 7: Component ablation test for TAQA ( acc ) in the style of the queries generated by the two systems. The POS templates used in oqa for question parsing is de-signed for generating ReVerb style queries [11] (e.g.,  X  ?x ;  X  is national currency of  X ;  X  Spain  X   X  ) instead of OpenIE queries (e.g.,  X  ?x ;  X  is  X ;  X  national currency of Spain  X   X  ), which are generated by the tool we use to build nOKB. In other words, assertions in nOKB, which are used in our experi-ment, match the tuple queries generated by taqa better. This facilitates query execution and hence gives a higher ac-curacy. This observation leads to an interesting question of how open IE assertion styles impact QA systems, which we leave as a future work. taqa  X  X  performance on WebQ is slightly lower than that on
CompQ . We note that this could be attributed to the fact that nOKB+ does not have adequate knowledge to support answering some questions in WebQ that are tailored for Freebase. For example, taqa makes a constant error in an-swering questions about tourist attractions like  X  X hat to see in Paris? X  , a category of question that is not well covered by nOKB+. In contrast, ParaSempre can easily answer these questions using tourist_attractions relation in Freebase. taqa employs a number of techniques along the question-answering process. Our next set of experiments is to eval-uate the effectiveness of each component. This is done by removing these components one at a time and observing the accuracy of the resulting system. Table 7 shows the results.
In the first test (labeled  X  X o complex constraints X ), we stripped all complex constraints (e.g.,  X  X efore 2002 X  ) from the tuple queries generated by the question parser. Each TQ is thus in the basic triple form. For CompQ questions, we see that the accuracy drops by 10% points. This shows that the ability of taqa  X  X  parser in composing queries in the n -tuple form has a significant impact in answering complex questions. One might expect acc to drop even lower than the 29.3% shown, considering that stripping the complex con-straints would  X  X estroy X  the questions in CompQ , which are all complex questions. The answer to this is twofold. First, sometimes complex constraints in questions are redundant and the questions can be answered by simpler triple queries. An example is  X  X ho plays Bilbo Baggins in The Hobbit ? X  , which can be answered by the triple query  X  ?x ;  X  plays  X ;  X  bilbo baggins  X   X  . Second, for cases where dropping the constraints causes ambiguity in answers, sometimes, the correct answers can still be got simply because they happen to be the most popular answers among the assertions in the KB. For ques-tions in WebQ , stripping complex constraints had little ef-fects, since questions are predominately simple questions.
In question parsing, relaxed queries (those with complex constraints removed) are generated in addition to the regu-lar ones. The objective is to improve the coverage of queries (Section 2.3). In our next test, we suppressed relaxed query generation (labeled  X  X o relaxed queries X ). For CompQ , we see an accuracy drop of 11.6% points, which is even more than that in the first test. This shows that considering all the constraints in questions (no relaxed queries) is no bet-ter than ignoring all of them (no constraints); the cover-age of queries is indeed an important factor. taqa model) is able to combine the scattered evidence of answers extracted from both relaxed and  X  X omplete X  queries via an-swer consolidation (Section 2.5), which boosts the rank of the answers that are derived from both forms of queries. An example question is  X  X hat book did Charles Darwin wrote in 1859? X  , which has a relevant assertion A :  X   X  Charles Dar-win  X ;  X  wrote  X ;  X  On the Origin of Species  X ,  X  in 1859  X   X  . With-out relaxed queries, the correct answer was ranked only sec-ond because A has a low frequency in the KB. The full model correctly ranked the answer top-1 because it got more evi-dence for the answer from the relaxed query  X   X  Charles Dar-win  X ;  X  wrote  X ; ?x  X  which is supported by a frequent assertion  X   X  Darwin  X ;  X  wrote the book  X ;  X  On the Origin of Species  X   X  .
While oqa uses shallow features in question paraphrasing, taqa uses 20,000 association features in addition to shallow statistical features (Section 2.2). Our next test removed all association features (labeled  X  X o association features X ). We observed an accuracy drop of 4.0% and 5.3% for CompQ and WebQ , respectively. This shows that association features are very useful in paraphrasing. Finally, taqa performs an-swer consolidation in the answer ranking step (Section 2.5)  X  the same answers (but with different feature vectors) are consolidated into one by combining their feature vectors. The last test turned off answer consolidation (labeled  X  X o answer consolidation X ). In this case, we see significant drops in accuracy of 7.0% and 9.3% for CompQ and WebQ , re-spectively. Answer consolidation, which allows taqa to en-semble scattered evidence in the KB for more confidence in answers, is seen as an important technique.

Our last experiment investigates the effectiveness of taqa answer ranker (Section 2.5). We say that a question Q is an-swerable by taqa if it X  X  correct answer appears in the ranked list of candidate answers generated for Q . Intuitively, for an answerable question, taqa is capable of identifying the cor-rect answer as a candidate. An effective ranker would rank the correct answer first in the list so that it is returned as the final answer. In our experiment, 65.7% of CompQ questions and 56.8% of WebQ questions are answerable. For those an-swerable questions, Table 8 shows the distribution of their ranks. From the table, we see that taqa  X  X  ranker is very effective; the distribution is very skewed towards the top ranks. In particular, for both CompQ and WebQ questions, around 60% of correct answers were ranked top-1 and around 80% of them were ranked top-5. The distributions shown in Table 8 is indeed insightful. For example, for CompQ , 19.8% of correct answers were ranked top-2-to-5, which are about 1/3 of those top-1 ones (59.9%). These correct answers were so close to be picked as the final answers. If we can further improve taqa  X  X  ranker such that all these correct answers are promoted to top-1, we would have effectively improve the accuracy of taqa by 1/3. This discussion indicates that more effort should be spent on further improving the ranker.
In this section we present a few representative questions that illustrate successful and failed cases of taqa
Table 9: Examples of successful and failed cases 9). Through these cases, we obtain insights into the key components of a good KB-QA system.

Open KBs are noisy and unnormalized. A challenging problem is to bridge the lexical/syntactic gap between in-put questions and relevant assertions. When this gap is small, taqa is very successful in finding the correct answer. Example 1 shows such a case. Question Q 9 is paraphrased into Q 0 9 and Q 00 9 , which match perfectly the assertions A and A 10 in the KB, respectively. This results in a correct answer. When the lexical gap is large, paraphrasing is our hope in narrowing the gap. Example 2 shows such a case. Question Q 10 uses informal expressions, which shares little lexical overlap with the relevant assertions ( A 11 ,A 12 In this example, taqa successfully paraphrases Q 10 into for-malized ones ( Q 0 10 , Q 00 10 ), which are parsed into tuple queries ( TQ 0 10 , TQ 00 10 ) that perfectly match the assertions.
While good paraphrasing can help bridge the lexical gaps, poor paraphrasing can adversely affect question parsing. taqa parser employs dependency analysis, which requires ques-tions to follow correct grammatical rules. If the paraphrased questions are grammatically incorrect, the parsed tuple queries will be erroneous, resulting in incorrect answers. Example 3 shows such a case. Question Q 11 is paraphrased as a gram-matically ill-formed question ( Q 0 11 ), which is parsed into a malformed dependency tree, resulting in a meaningless tuple query ( TQ 0 11 ) and an incorrect answer.

Similarity measures (see Section 2.4), which are used to evaluate the relatedness between tuple queries and asser-tions play an important role in filtering incorrect candidate answers. In the current implementation, taqa primarily employs cosine similarity on text, which is insufficient to measure the semantic relatedness between queries and asser-tions. This is illustrated by Example 4. For Question Q taqa returns the incorrect answer  X  X ueen Elizabeth X  instead of the correct one  X  X ing George V X  . The reason is that al-though assertion A 16 gives a larger cosine similarity with the tuple query TQ 12 than does A 15 , the difference in the sim-ilarity scores is marginal. The higher popularity of A 15 the KB over-offsets the similarity difference, giving  X  X ueen Elizabeth X  a higher rank. To improve, one needs to look into similarity measures that can discover deep semantic relat-edness between short-texts, which has been recognized as a fundamental task in AI.
There are two lines of research in KB-QA systems. One focuses on curated KBs and the key challenge is to trans-form the lexicon of NL questions to structured KB queries. Sempre [4] utilizes a set of ReVerb extractions to map NL phrases to KB relations. Kwiatkowski et al. [16] proposed to learn intermediate semantic representations directly from NL questions, with which KB queries are derived on the fly using Wiktionary as synonymy features. Xu et al. [20] proposed to learn an alignment model between NL phrases and Freebase relations from large web corpora. They ap-proached the task of finding corresponding Freebase rela-tions given NL phrases via classification. Recently, Berant et al. [5] proposed ParaSempre , which applies an  X  X ver-generate and re-rank X  strategy. The strategy is to enumer-ate possible KB queries and transform them to synthetic NL questions. They also applied a paraphrase model with rich association and embedding features to rank candidate queries based on the similarity between the input and the synthesized questions.

Another line of research focuses on open KB-QA. Par-alex [13] is the first open KB-QA system, which is based on learned lexicons from a corpus of question paraphrases. More recently, Fader et al. proposed oqa [12], a unified framework for open domain question answering on curated and open KBs. oqa processes questions using a cascaded pipeline of paraphrasing, question parsing, query rewriting and execution. Our work is similar with oqa in decom-posing QA into different sub-components. However, we put forward a number of techniques which are essential in pro-cessing complex questions.
In this paper we presented a novel open KB-QA system, taqa , that is able to answer questions with complex se-mantic constraints using n -tuple open KB, nOKB. We pro-posed new question parsing and answer extraction methods that are adapted to n -tuple assertions. Empirical evaluation showed that our system significantly outperformed state-of-the-art curated KB-QA and open KB-QA systems in an-swering questions with rich semantic constraints.

Through discussions, we have identified a number of key issues that have strong implications to the performance of taqa . These include (1) question parsing that is robust to grammatical errors in questions, (2) similarity measures that can capture the deep semantic relatedness between queries and assertions, (3) answer ranking that can leverage more powerful features. In addition, efficient methods for cleaning and canonicalizing n -tuple assertions would greatly improve the quality of the open KB, and thus the QA accuracy.
Finally, we remark that taqa and ParaSempre excel in handling complex questions and simple questions, respec-tively. In practice, user-issued questions can be of either kind. A natural extension to our work for handling a mixed question workload is ensembling. As an example, we trained an ensemble model using RankBoost with simple statisti-cal features to re-rank the combined top-10 outputs from taqa and ParaSempre . On a mixed question set of 30% complex questions and 70% simple questions, the ensem-ble model achieves an accuracy of 46 . 6%, which is better than running taqa on CompQ (39 . 3%) or ParaSempre on WebQ (45 . 8%). This shows that ensembling is a promising approach for further explore.
 This research is supported by Hong Kong Research Grants Council GRF grant HKU712712E. We would like to thank the anonymous reviewers for their insightful comments.
