 In text mining, we are often confronted with very high di-mensional data. Clustering with high dimensional data is a challenging problem due to the curse of dimensionality . In this paper, to address this problem, we propose an subspace maximum margin clustering (SMMC) method, which per-forms dimensionality reduction and maximum margin clus-tering simultaneously within a unified framework. We aim to learn a subspace, in which we try to find a cluster assign-ment of the data points, together with a hyperplane clas-sifier, such that the resultant margin is maximized among all possible cluster assignments and all possible subspaces. The original problem is transformed from learning the sub-space to learning a positive semi-definite matrix, in order to avoid tuning the dimensionality of the subspace. The transformed problem can be solved efficiently via cutting plane technique and constrained concave-convex procedure (CCCP). Since the sub-problem in each iteration of CCCP is joint convex, alternating minimization is adopted to obtain the global optimum. Experiments on benchmark data sets illustrate that the proposed method outperforms the state of the art clustering methods as well as many dimensionality reduction based clustering approaches.
 I.2.6 [ Artificial Intelligence ]: Learning; I.5.3 [ Pattern Recognition ]: Clustering Algorithms, Experimentations Maximum margin clustering, Dimensionality reduction, Cut-ting plane, Constrained concave-convex procedure
Clustering has long been an important topic in machine learning and data mining. It aims to divide the unlabeled data set into groups of similar data points. Many clustering methods have been proposed up to now, e.g. K-means, spec-tral clustering [19] [17] and maximum margin clustering [22]. In text mining, we are often confronted with very high di-mensional data. Clustering with the high dimensional data is a challenging problem due to the curse of dimensionality . Many dimensions in the data are redundant or irrelevant for clustering, so the clustering performance is usually af-fected. On the other hand, handling the high dimensional data usually leads to high computational cost.

To tackle this problem, dimensionality reduction is a po-tential approach since it may find a subspace which has more separability and discriminative information for clustering. In the past decades, many dimensionality reduction based clustering methods have been proposed [7] [15] [9] [14] [8] [24] [25], which do dimensionality reduction and k-means simultaneously.

In this paper, we present an subspace maximum margin clustering (SMMC), which integrates dimensionality reduc-tion with the state of the art clustering method, i.e. max-imum margin clustering [22] in a joint framework. SMMC aims to learn a subspace, in which it tries to find a cluster as-signment of the data points, together with a hyperplane clas-sifier, such that the resultant margin is maximized among all possible cluster assignments and all possible subspaces. Since the dimensionality of the subspace is also a param-eter which needs careful tuning, we transform the original problem from learning the subspace to learning a positive semi-definite matrix in order to avoid the difficulty of tun-ing it. Then the problem can be solved efficiently by cut-ting plane algorithm [13] and constrained concave-convex programming (CCCP) [21], which owns well-studied con-vergence properties. Furthermore, we will show that in each iteration of CCCP, the sub-problem is joint convex on the variables, hence alternating minimization can be adopted to obtain the global optimum. The convergence and the computational complexity of the whole algorithm is also an-alyzed. Experimental results on benchmark data sets illus-trate that the proposed method outperforms existing dimen-sionality reduction based methods as well as state of the art clustering approaches.
 The remainder of this paper is organized as follows. In Section 2, we will review some existing works related with ours. In Section 3, we will propose binary class subspace maximum margin clustering. In Section 4, we will extend binary class subspace maximum margin clustering to multi-class setting. Experiments on benchmark data sets are demon-strated in Section 5. Finally, we draw a conclusion in Section 6.
In this section, we will review some works closely related with ours. We first introduce dimensionality reduction based clustering approaches, followed with maximum margin clus-tering [22].
Dimensionality reduction is widely used for clustering with high dimensional data. One common strategy is to do un-supervised dimensionality reduction before clustering, e.g. principal component analysis (PCA). However, PCA cannot lead to the separability of the data for clustering because it is unsupervised. Furthermore, since the dimensionality reduc-tion is independent with the subsequent clustering, it does not necessarily achieve improvement in clustering perfor-mance. An alternative strategy is to integrate supervised di-mensionality reduction with clustering in a joint framework, and do them simultaneously. In detail, clustering gener-ates the class label for supervised dimensionality reduction, while supervised dimensionality reduction finds the discrim-inative subspace for clustering. The pioneer works are adap-tive dimension reduction (ADR) [7] and adaptive subspace iteration (ASI) [15]. In [8], the authors proposed to com-bine linear discriminant analysis (LDA) and K-means into a coherent framework to adaptively select the most discrimi-native subspace, which unifies ADR and ASI as its special cases. Similar methods include discriminative cluster anal-ysis (DCA)[14] and adaptive metric learning (AML) [24], which are all optimizing the following objective function max where X = [ x 1 , . . . , x n ]  X  R d  X  n is data matrix, A  X  R linear transformation which projects the d -dimensional in-put data to m -dimensional subspace, and L = F ( F T F )  X  1 R n  X  c is weighted cluster assignment matrix with F ip = 1 if x i belongs to the p -th cluster and F ip = 0 otherwise. Recently, the authors in [25] gave an insightful analysis on the discriminative clustering methods [14] [8] [24] in Eq.(1), showing that the optimal L  X  to Eq.(1) solves the following optimization problem where G = X T X is Gram matrix and I  X  R n  X  n is the iden-tity matrix. Eq.(2) is referred to as discriminative K-means (DisKM). The most appealing property of DisKM is that the linear transformation A is omitted. In other words, the problem in Eq.(2) does not involve explicit dimensionality reduction. And it can be seen as kernel K-means with a specific kernel matrix I  X  ( I + 1  X  G )  X  1 .

All the methods introduced above are all along the line of K-means, while there are many state of the art cluster-ing methods, which outperform K-means significantly, e.g. spectral clustering [19] [17] and maximum margin clustering (MMC) [22] [23]. In the following, we will briefly introduce MMC.
Maximum margin clustering (MMC) [22] [23] is a most recently proposed clustering method which extends maxi-mum margin principle in support vector machine (SVM) [18] to unsupervised learning setting. Since the class labels are unknown in unsupervised learning, MMC tries to find a cluster assignment of the data points, together with a hyper-plane classifier, such that the resultant margin is maximized among all possible cluster assignments.
 Let us recall support vector machine (SVM) [18] first. Given a data set X = { x 1 , . . . , x n }  X  R d , and their class plane f ( x ) = w T x + b by solving the following optimization problem
Different from SVM, where the class labels are given, binary class MMC [22] aims to find the best class assign-ment y = [ y 1 , . . . , y n ] T  X  { X  1 } n such that a SVM trained be formulated mathematically as the following optimization problem where with the data set size. Note that the second constraint is often known as the class balance constraint. It is introduced to avoid the trivially optimal solution which assigns all data points to the same class and thus achieves infinite margin. l is a constant controlling the class imbalance.

According to [23] [6], binary class MMC can be extended to multi-class setting. Multi-class MMC aims to find the that a SVM trained on { ( x 1 , y 1 ) , . . . , ( x n , y largest margin, i.e. where  X  p,q = 1 if p = q and 0 otherwise. Note that the second constraint is the class balance constraint.
In the past years, many algorithms have been proposed for solving MMC, e.g. semi-definite programming (SDP) based [22] [23] [20], support vector regression (SVR) based [26], cutting plane based [27] [28], multiple kernel learning based [16] and stochastic search based [10]. On the other hand, some variants of MMC are also proposed most re-cently, e.g. MMC with pairwise constraints [11]. However, all these methods perform MMC in the input space. When the dimensionality of the data is very high, the curse of the dimensionality mentioned above may occur. Hence in our study, we aim to combine dimensionality reduction with MMC in a unified framework.
In this section, we will present binary class subspace max-imum margin clustering (SMMC).
When the dimensionality of the data is very high, we aim to learn a subspace, in which maximum margin clustering is performed. To achieve this, we introduce a linear trans-formation A  X  R m  X  d , projecting the data points from the input space to a subspace. Rather than doing dimension-ality reduction and clustering successively, we turn to do dimensionality reduction and clustering simultaneously in a joint framework. The objective of binary class SMMC is to learn a linear transformation A , find a cluster assignment y  X  { X  1 } n of the data points in the subspace spanned by A , together with the hyperplane classifier w , such that the resultant margin is maximized among all possible cluster as-signments and all possible subspaces. It is mathematically formulated as the following optimization problem p, q  X  1, ( p, q )-norm of matrix W is defined as || W || ( show that Frobenius norm is (2 , 2)-norm.

In Eq.(6), w and A are coupled together in the con-straints. Moreover, the orthonormal constraint AA T = I is non-convex. To tackle the above problem, we replace the L 2 norm on w with (2 , 1)-norm, and we have the following theorem.
 Theorem 3.1 Eq.(6) using (2 , 1) -norm for w is equivalent to the following problem where u = A T w , D + is the pseudo-inverse of D , D = { D | tr ( D )  X  1 , range ( U )  X  range ( D ) , D  X  S d + denotes the set { x  X  R d | x = Dz , for some z  X  R d } , S notes the set of positive semi-definite real symmetric matri-ces.
 Proof. Please refer to the proof of Theorem 4.1.
 It is worth noting that when D + = I , Eq.(7) degenerates to standard binary-class maximum margin clustering in Eq.(4). The most appealing property of the problem in Eq.(7) is that the linear transformation A is omitted, hence there is no need for tuning the dimensionality of the subspace. And learning the subspace is transformed to learning the positive semi-definite matrix D . To some extent, it is similar with DisKM in Eq.(2) which omits the linear transformation A in Eq.(1).
In the following, we will present an algorithm for solving the problem in Eq.(7).

Eq.(7) is a mixed integer programming, which is difficult to solve. Fortunately, we have the following theorem. Theorem 3.2 Eq.(7) is equivalent to Eq.(8) where y i = sign ( u T x i + b ) .
 Proof. Please refer to [27].
 One challenge in solving Eq.(8) is that the number of vari-ables is very large. To tackle this, we adopt the strategy in [12] to transform the n -slack formulation in Eq.(8) to 1-slack formulation.
 Theorem 3.3 Eq.(8) is equivalent to Eq.(9), with  X   X  = where c = [ c 1 , . . . , c n ] T .
 Proof. Please refer to [27].
 Although the number of slack variables in Eq.(9) is greatly reduced by n  X  1, the number of constraints increases from n to 2 n . Fortunately, cutting plane technique [13] enables us to deal with this problem, which keeps a polynomial sized subset  X  of working constraints and computes the optimal solution to Eq.(9) subject to the constraints in  X . In detail, the algorithm adds the most violated constraint in Eq.(9) into  X  in each iteration. In this way, a successively strength-ening approximation of the original problem is solved. And the algorithm terminates when no constraints in Eq.(9) is violated by more than  X  . The remaining thing is how to find the most violated constraint in each iteration. Since the feasibility of a constraint is measured by the corresponding value of  X  , the most violated constraint is the one which owns the largest  X  . The following theorem gives the calculation of the most violated constraint.
 Theorem 3.4 The most violated constraint could be calcu-lated as follows Proof. Please refer to [27].
 In each iteration of the cutting plane algorithm, we need to solve Eq.(9) to obtain the optimal hyperplanes classi-fier under the current working constraint set  X . However, the first constraint in Eq.(9) is non-convex. Note that the non-convex constraint can be written as the difference of two convex functions. As a result, we can use constrained concave-convex procedure (CCCP) [21] to solve this kind of problem, which owns well-studied convergence properties. a non-smooth function of ( u , b ). In order to use CCCP, we need to use the sub-gradients [3] instead of gradients as fol-lows,
Given an initial point ( u (0) , b (0) ), CCCP computes ( u ) straint with its first order Taylor expansion at ( u ( t ) Thus we obtain the following quadratic programming (QP) problem The following theorem characterizes the joint convexity of the problem in Eq.(12).
 Theorem 3.5 The optimization problem in Eq.(12) is jointly convex on U and D .
 Proof. Please refer to the proof of Theorem 4.5.
 According to this theorem, we can use alternating mini-mization to obtain the global optimum of Eq.(12). In other words, we fix one of the two optimization variables ( u and D ), and solve the other one in terms of the fixed one.
In detail, fixing D ( t ) , u ( t +1) can be solved via QP as fol-lows
Conversely, fixing u ( t +1) , D ( t +1) can be solved by the fol-lowing problem, The following theorem characterizes the optimal solution to Eq.(14) Theorem 3.6 Let C = uu T , the optimal solution of Eq.(14) is and the optimal value equals ( tr ( C 1 2 )) 2 Proof. Please refer to the proof of Theorem 4.6.
 As for the termination criterion of CCCP (alternating mini-mization), we check if the difference in objective values from two successive iterations is less than  X  %. In our experi-ments, we set  X  = 1.
 In summary, we present the whole algorithm of optimizing Eq.(7) in Algorithm 1.
The convergence of Algorithm 1 is theoretically guaran-teed by the following theorem.
 Theorem 3.7 For any  X  &gt; 0 , C &gt; 0 , and any data set X = { x 1 , . . . , x n } , the Algorithm 1 terminates after adding at most CR  X  2 constraints, where R is a constant number in-dependent of n and d .
 Proof. Please refer to [27].

And the computational complexity of Algorithm 1 is char-acterized in the following theorem. Algorithm 1 Binary Class Subspace Maximum Margin Clustering Input: C, l,  X  and  X  ,  X  =  X  ; Output: { u p } ;
Set D = 1 d I , initialize ( u (0) , b (0) ) using K-means; repeat until the newly selected constraint c is violated by no more than  X  Theorem 3.8 The computational complexity of algorithm
Proof. In each iteration of CCCP, the computational complexity of solving the QP in Eq.(13) is O ( |  X  | 2 dn ) where |  X  | is the size of the working set. And the computational complexity of Eq.(15) is O ( d 2 ). Hence the computational complexity is about O ( d 2 ) in each iteration. Moreover, we observed that CCCP takes less than 10 iterations in each round of cutting plane, then according to Theorem 3.7, the overall computational complexity of Algorithm 1 is O ( CRd This completes the proof.
In this section, we will extend binary class subspace max-imum margin clustering to multi-class subspace maximum margin clustering.
When it comes to multi-class clustering setting, the ob-jective of SMMC is to learn a linear transformation A , find a cluster assignment y  X  { 1 , . . . , c } n of the data points in the subspace spanned by A , together with the hyperplane maximized among all possible cluster assignments and all possible subspaces, i.e. Note that and || X || F is Frobenius norm. For p, q  X  1, ( p, q )-norm of matrix W is defined as || W || p,q = ( is the i -th row of W . It is easy to show that Frobenius norm is (2 , 2)-norm.

In Eq.(16), w p and A are coupled together in the con-straints. Moreover, the orthonormal constraint AA T = I is non-convex. Again, we replace the Frobenious norm on W with (2 , 1)-norm, and we have the following theorem. Theorem 4.1 Eq.(16) using (2 , 1) -norm for W is equiva-lent to the following problem where U = [ u 1 , . . . , u c ] = A T [ w 1 , . . . , w c the pseudo-inverse of D , D = { D | tr ( D )  X  1 , range ( U )  X  range ( D ) , D  X  S d + } , range ( D ) denotes the set { x  X  R Dz , for some z  X  R d } , S d + denotes the set of positive semi-definite real symmetric matrices.

Proof. The proof is based on [1]. let D = A T diag( || w i then Hence the optimal value of problem in Eq.(17) is less than or equal to the optimal value of problem in Eq.(16) using (2 , 1)-norm.

Conversely, let D = A T diag(  X  ) A where  X  = [  X  1 , . . . ,  X  R , then The last inequality holds based on the Cauchy-Schwarz In-equality. Hence the optimal value of problem in Eq.(16) using (2 , 1)-norm is less than or equal to the optimal value of problem in Eq.(17).

In summary, the optimal value of problem in Eq.(16) equals to the optimal value of problem in Eq.(17). This completes the proof.
 It is worth noting that when D + = I , Eq.(17) degenerates to standard multi-class maximum margin clustering in Eq.(5). The most appealing property of the problem in Eq.(17) is that the linear transformation A is omitted, hence there is no need for tuning the dimensionality of the subspace. And learning the subspace is transformed to learning the positive semi-definite matrix D .
In the following, we will present an algorithm for solving the problem in Eq.(17). Again, Eq.(17) is a mixed integer programming, which is difficult to solve. Fortunately, we have the following theorem.
 Theorem 4.2 Eq.(17) is equivalent to Eq.(18) where I (  X  ) is the indicator function, z ip = , and the label for data point x i is determined as y = arg max p u T p x i = Proof. Please refer to [28].
 Again, we adopt the strategy in [12] to transform the n -slack formulation in Eq.(18) to 1-slack formulation.
 Theorem 4.3 Eq.(18) is equivalent to Eq.(19), with  X   X  = where e p is a c  X  1 vector with only the p -th element being 1 and others 0 , e 0 is the c  X  1 zero vector and e is the all one vector.
 Proof. Please refer to [28].
 Although the number of variables in Eq.(19) is greatly re-duced by n  X  1, the number of constraints increases from nc to ( c + 1) n . Again, we adopt cutting plane technique [13], which keeps a polynomial sized subset  X  of working constraints and computes the optimal solution to Eq.(19) subject to the constraints in  X . More concretely, the algo-rithm adds the most violated constraint in Eq.(19) into  X  in each iteration. In this way, a successively strengthening approximation of the original problem is constructed and solved. The algorithm terminates when no constraints in Eq.(19) is violated by more than  X  . The key point is how to find the most violated constraint in each iteration. Since the feasibility of a constraint is measured by the correspond-ing value of  X  , the most violated constraint is the one which would lead to the largest  X  . The following theorem gives the computation of the most violated constraint.
 Theorem 4.4 Define p  X  = arg max p ( u T p x i ) and r lated constraint could be calculated as follows Proof. Please refer to [28].

In each iteration of the cutting plane algorithm, we need to solve Eq.(19) to obtain the optimal hyperplane classifier un-der the current working constraint set  X . However, the first constraint in Eq.(19) is non-convex. Again, the non-convex constraint can be written as the difference of two convex functions. And we can use constrained concave-convex pro-cedure (CCCP) [21] to solve this kind of problem. Further-convex, it is a non-smooth function of ( u 1 , . . . , u c CCCP, we need to compute the sub-gradients [3] as follows,
Given an initial point ( u (0) 1 , . . . , u (0) c ), CCCP computes ( u 1 , . . . , u with its first order Taylor expansion at ( u ( t ) 1 , . . . , u we obtain the following quadratic programming (QP) prob-lem The following theorem characterizes the joint convexity of the problem in Eq.(22).
 Theorem 4.5 The optimization problem in Eq.(22) is jointly convex on U and D .

Proof. The proof is based on [5]. It is trivial to show that only if u T p D + u p , 1  X  p  X  c is joint convex on U and D . According to [3], a function f is convex if and only if the epigraph of the function, denoted as epi f is a convex set, where Here f ( u p , D ) = u T p D + u p , so the epigraph of f is = { ( u p , D , t ) | range( U )  X  range( D ) , D  X  S + d According to Schur Complement Theory [3], Since the left-hand-side of the above equality is a positive semi-definite cone, it is convex. Hence the right-hand-side of the equality is convex. Note that the right-hand-side of the equality corresponds to the epigraph of function f ( u p , D ), this completes the proof.
 According to this theorem, we can use alternating mini-mization to obtain the global optimum of Eq.(22). In other words, we fix one of the two optimization variables ( U and D ), and solve the other one in terms of the fixed one.
In detail, fixing D ( t ) , { u ( t +1) p } can be solved via QP as follows,
Conversely, fixing { u ( t +1) p } , D ( t +1) can be solved by the following problem, The following theorem characterizes the optimal solution to Eq.(24) Theorem 4.6 Let C = UU T , the optimal solution of Eq.(24) is and the optimal value equals ( tr ( C 1 2 )) 2
Proof. The proof is based on [1]. Let D = A diag(  X  ) A T where  X  = [  X  1 , . . . ,  X  d ]  X  R d , then The last inequality holds based on Lemma A.1. Next, we have since tr( A )tr( B )  X  tr( AB ) if A and B are positive semi-definite. The equality holds if and only if C 1 2 a i a T of C 1 2 . The optimal  X  is tr( C 1 2 ). Hence we obtain proof.
 As for the termination criterion of CCCP (alternating mini-mization), we check if the difference in objective values from two successive iterations is less than  X  %. In our experi-ments, we set  X  = 1.
 In summary, we present the whole algorithm of optimizing Eq.(17) in Algorithm 2.
The convergence of Algorithm 2 is theoretically guaran-teed by the following theorem.
 Theorem 4.7 For any  X  &gt; 0 , C &gt; 0 , and any data set X = { x 1 , . . . , x n } , the Algorithm 2 terminates after adding at most CR  X  2 constraints, where R is a constant number in-dependent of n and d .
 Proof. Please refer to [28].

And the computational complexity of Algorithm 2 is char-acterized in the following theorem.
 Theorem 4.8 The computational complexity of Algorithm 2 is O ( CRrd 2  X  2 ) , where r is the rank of U . Algorithm 2 Multi-Class Subspace Maximum Margin Clustering Input: C, l,  X  and  X  ,  X  =  X  ; Output: { u p } ;
Set D = 1 d I , initialize { u (0) p } , { z ip } using K-means; repeat until the newly selected constraint c is violated by no more than  X 
Proof. In each iteration of CCCP, the computational complexity of solving the QP in Eq.(23) is O ( |  X  | 2 dn ) where |  X  | is the size of the working set, and the computational complexity of Eq.(25) is O ( rd 2 ), where r is the rank of U . Hence the computational complexity is about O ( rd 2 ) in each iteration. Moreover, we observed that CCCP takes less than 10 iterations in each round of cutting plane, then according to Theorem 4.7, the overall computational complexity of Al-gorithm 2 is O ( CRrd 2  X  2 ). This completes the proof.
In our experiments, we will evaluate the proposed cluster-ing method on benchmark data sets. We compare it with state of the art clustering methods, e.g. normalized cut (NCut) [19] and maximum margin clustering (MMC) [28]. We also compare it with dimensionality reduction based methods, e.g. PCA+K-means (PCAKM), adaptive dimen-sionality reduction (ADR) [7], adaptive subspace iteration (ASI) [15] and discriminative K-means (DisKM) [25].
To evaluate the clustering results, we adopt the perfor-mance measures used in [4]. These performance measures are the standard measures widely used for clustering.
Clustering Accuracy: Clustering Accuracy discovers the one-to-one relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class. Clustering Accuracy is defined as follows: where r i denotes the cluster label of x i , and l i denotes the true class label, n is the total number of documents,  X  ( x, y ) is the delta function that equals one if x = y and equals zero otherwise, and map ( r i ) is the permutation mapping function that maps each cluster label r i to the equivalent label from the data set.

Normalized Mutual Information: The second mea-sure is the Normalized Mutual Information (NMI), which is used for determining the quality of clusters. Given a clus-tering result, the NMI is estimated by where n i denotes the number of data contained in the cluster C (1  X  i  X  c ),  X  n j is the number of data belonging to the L (1  X  j  X  c ), and n i,j denotes the number of data that are in the intersection between the cluster C i and the class L The larger the NMI is, the better the clustering result will be.
In order to evaluate the clustering methods, we use a sub-set of UCI database and three text data sets. These data sets have also been used in [27] [28].

UCI : A subset of UCI [2] machine learning benchmark database is selected to evaluate the binary class clustering, e.g. ionosphere, digits and Letter. Following [27], for the digits data set, we focus on those pairs (3 vs 8,1 vs 7, 2 vs 7, 8 vs 9) that are difficult to differentiate. For the letter data sets, we use their first 2 classes.

Cora 1 : For Cora data set, we select a subset containing the research papers of subfield data structure(DS), hardware and architecture (HA), machine learning (ML), operating system (OS) and programming language (PL).

WebKB 2 : The WebKB data set contains web pages gath-ered from 4 university computer science departments. There are about 8280 documents and they are divided into 7 cat-egories: student, faculty, staff, course, project, department and other, among which student, faculty, course and project are four most populous entity-representing categories. We select a subset of about 6000 web pages. 20Newsgroup 3 : The topic rec containing autos, motor-cycles, baseball and hockey was selected from the version 20news-18828. To evaluate the binary class clustering, we focus on those pairs (autos vs motorcycles (Text1), and base-ball vs hockey (Text2)) that are difficult to differentiate. To evaluate the multi-class clustering, all of them are used, re-ferred to as News4.

Table.1 summarizes the characteristics of the data sets used in this experiment.
 ionosphere 351 34 2 Letter AvB 1555 16 2 http://www.cs.umass.edu/ mccallum/code-data.html http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/ http://people.csail.mit.edu/jrennie/20Newsgroups/
We set the number of clusters equal to the true number of classes for all the clustering algorithms. For NCut [19], the scale parameter of Gaussian kernel for constructing adja-cency matrix is set by the grid { 10  X  3 , 10  X  2 , 10  X  1 For PCA+K-means, the reduced dimension of PCA is set to the minimal number that preserves at least 95% of the in-formation. For ADR and ASI, the dimensionality of the subspace is set to c  X  1 as in [7] [15], where c is the number of clusters. For MMC, it is implemented based on [27] [28]. For MMC and our method, the regularization parameter C 1 , 2 : 1 : 10 } .  X  is fixed to 0 . 1. The balance constant l is set by the grid { 1 , 5 , 10 } . Under each parameter setting, we repeat clustering 20 times, and the average result is com-puted. We report the best average result corresponding to the best parameter setting for each method to compare with each other.
Table 2 shows the clustering accuracy of all the methods on all the data sets, while Table 3 shows the normalized mutual information.

From Table 2 and Table 3, we observe that: (1) Our method (SMMC) outperforms the other methods (includ-ing MMC) significantly on most data sets. This indicates that doing MMC in a proper subspace can improve its clus-ter performance; (2) Among clustering methods without di-mensional reduction, MMC usually outperforms normalized cuts (NC) and K-means (KM); (3) Among dimensionality reduction based clustering methods, the methods which per-form dimensionality reduction and clustering simultaneously are comparable or better than PCA+K-means (PCAKM). DisKM is often better than PCAKM, ADR and ASI. And SMMC outperforms DisKM significantly on most data sets. The reason is probably that SMMC is along the line of MMC, which is superior to K-means.
In this paper, we proposed an subspace maximum mar-gin clustering (SMMC) for high dimensional data, which aims to learn a subspace, in which it tries to find a clus-ter assignment of the data points, together with a hyper-plane classifier, such that the resultant margin is maximized among all possible cluster assignments and all possible sub-spaces. This problem can be solved via cutting plane and constrained concave-convex procedure (CCCP) which owns well-studied convergence properties. Furthermore, we show that the subproblem in each iteration of CCCP is joint con-vex, hence we can use alternating minimization to obtain the global optimum. Empirical studies illustrate that the proposed method outperforms the state of the art clustering methods.
This work was supported by the National Natural Sci-ence Foundation of China (No.60721003, No.60673106 and No.60573062) and the Specialized Research Fund for the Doctoral Program of Higher Education. We thank the anony-mous reviewers for their helpful comments. [1] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task [2] A. Asuncion and D. Newman. UCI machine learning [3] S. Boyd and L. Vandenberghe. Convex optimization . [4] D. Cai, X. He, X. Wu, and J. Han. Non-negative [5] B. Chen, W. Lam, I. Tsang, and T.-L. Wong. A [6] K. Crammer and Y. Singer. On the algorithmic [7] C. H. Q. Ding, X. He, H. Zha, and H. D. Simon. [8] C. H. Q. Ding and T. Li. Adaptive dimension [9] C. Domeniconi, D. Papadopoulos, D. Gunopulos, and [10] F. Gieseke, T. Pahikkala, and O. Kramer. Fast [11] Y. Hu, J. Wang, N. Yu, and X.-S. Hua. Maximum [12] T. Joachims. Training linear svms in linear time. In [13] J. E. Kelley. The cutting plane method for solving [14] F. D. la Torre and T. Kanade. Discriminative cluster [15] T. Li, S. Ma, and M. Ogihara. Document clustering [16] Y. Li, I. Tsang, J. Kwok, and Z. Zhou. Tighter and [17] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral [18] B. Sch  X  olkopf and A. Smola. Learning with Kernels . [19] J. Shi and J. Malik. Normalized cuts and image [20] H. Valizadegan and R. Jin. Generalized maximum [21] A. S. Vishwanathan, A. J. Smola, and S. V. N. [22] L. Xu, J. Neufeld, B. Larson, and D. Schuurmans. [23] L. Xu and D. Schuurmans. Unsupervised and [24] J. Ye, Z. Zhao, and H. Liu. Adaptive distance metric [25] J. Ye, Z. Zhao, and M. Wu. Discriminative k-means [26] K. Zhang, I. W. Tsang, and J. T. Kwok. Maximum [27] B. Zhao, F. Wang, and C. Zhang. Efficient maximum [28] B. Zhao, F. Wang, and C. Zhang. Efficient multiclass Lemma A.1 For any b = [ b 1 , . . . , b n ]  X  R d , we have Proof. The proof is based on [1]. According to Cauchy-Schwarz inequality, we have || b || 1 = ( if and only if
