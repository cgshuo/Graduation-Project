 Community Question Answering (CQA) services enables users to ask and answer questions. In these communities, there are typically a small number of experts amongst the large population of users. We study which questions a user select for answering and show that experts prefer answering ques-tions where they have a higher chance of making a valuable contribution. We term this preferential selection as question selection bias and propose a mathematical model to esti-mate it. Our results show that using Gaussian classification models we can effectively distinguish experts from ordinary users over their selection biases. In order to estimate these biases, only a small amount of data per user is required, which makes an early identification of expertise a possibil-ity. Further, our study of bias evolution reveals that they do not show significant changes over time indicating that they emanates from the intrinsic characteristics of users. H.1.2 [ Information Systems ]: User/Machine Systems -theory and models; H.3.3 [ Information Search and Re-trieval ]: Information Filtering -General Algorithms, Experimentation, Human Factors Expert Identification, Selection Bias, Question Answering
Community Question Answering (CQA) services provide a platform for Internet users to form social communities and exchange knowledge in the form of questions and answers. In these communities, there are typically a small number of highly active users, and an even smaller number of domain experts -users who provide a large number of technically correct, complete and reliable answers. Identifying these experts during initial phase of their engagement can lead to approaches to retain, mentor these users.

In this paper, we examine how users select questions for answering and propose the notion of question selection bias . We hypothesize that experts who aim to provide answers which would be perceived by the community as valuable, tend to prefer answering questions which don X  X  already have good answers. We term this preference as question selection bias and use it to identify experts. The main contributions of this paper are as follows: First we propose a mathemati-cal model to capture this bias in CQA. Second, we propose using a simple machine learning model that uses this bias for the task of expert identification. Third, we show that these biases can be effectively estimated using user X  X  initial inter-actions making early identification of experts a possibility. Fourth, we show that selection biases do not show significant changes over time, leading us to conclude that these biases stem from the intrinsic characteristics of the users.
Expert identification approaches employ a graph analysis in conjunction with text analysis. Graph based approaches model CQA as a graph induced as a result of a users X  inter-actions. Zhang [4] modeled CQA as an expertise graph and proposed Expertise Ranking algorithm. Jurczyk [3] identi-fied authorities using link analysis of the underlying graph.
Other approaches have looked at overall interaction char-acteristics of user. Bouguessa [1] proposed a model to iden-tify authoritative actors based on the number of best an-swers provided by them. Zhang [4] proposed a measure called Z-score which combines the number of answers and questions given by a user to a single value in order to mea-sure the relative expertise of a user. Topic-based models to identify appropriate users to answer a question have recently been proposed by Jinwen [2].
We collected data from TurboTax Live Community 1 (Tur-boTax) which is a Q&amp;A service related to preparation of tax returns. The dataset consists of 633,112 questions provided by 525,143 users and 688,390 answers provided by 130,770 users over the years 2007-2009. TurboTax has employees that manually evaluate an expert candidate on factors, such as correctness and completeness of answers, politeness in re-sponses, language and choice of words used. As of now, they https://ttlc.intuit.com/app/full page have labeled 83 experts and intend to label many more. For our experiment, we selected users who have provided 10 or more answers ( U 10 users). There are 1,367 such users in the dataset. Table 1 summarizes interaction characteristics of Table 1: Dataset description. U 10 are users who pro-amongst them. A , BA stands for answers and best answers, respectively, provided by them.
Answers to a question contain several attributes that re-flect their relative value from the perspective of the commu-nity such as votes given by the community members, answer status (e.g. best answer, helpful answer). Based on this, we can define the value on a question in terms of the value of answers ( V a )providedonit: where status( a )  X  X  1 , 2 } is 2 if answer has a special status (best answer, helpful answer, etc) otherwise 1. Note that the value on a question increases as more answers are provided but the value can decrease if answers get negatively voted. The weight parameters w 0 ,w 1 are scaling constants which are used to adjust the relative priority of one attribute over other. For our experiments, we choose w 0 = w 1 =1(asthis works best). Value is discretized into six buckets using the following strategy: If the value on a question is less than 1 it is assigned bucket 0, if it is larger than 4 it is assigned bucket 5 else it is assigned the respective bucket number from 1-4.
The value as defined here is a simple approximation of the true value on a question. Additionally this value is consis-tent across all users in order to keep the model simple. Our results indicate that the model performance is surprisingly promising even with this simplistic assumption.
We explore question selection bias by measuring the de-gree to which a user prefers answering a question with a given existing value. The existing value gives a clue as to whether the user intends to make a valuable contribution or not. For example, if the user intends to make a valu-able contribution, then with high chances user will choose a question with low existing value. Our hypothesis suggests that this tendency of picking a question with low existing value is prominent in experts. Ordinary users either do not have enough expertise to answer a question, or are not well motivated to put effort in answering the question at length. So it might be the case that some of them prefer to pitch in only when question has received few good answers.
In order to measure user X  X  selection bias, we consider all the answers provided by that user. Let a user u choose to answer a question q with existing value v 0 . Then, we get evidence that u prefers to answer question with value v 0 : isting value of the question q just before u posted his or her answer. A uq is a random variable that takes values { 0 , 1 indicating whether user u answered question q .UsingBayes rule, we estimate the user X  X  selection bias as follows:
P ( V u | A u =1) = ability that u selects q for answering. The prior is considered to be uniform over all the questions answered by u ,hence it has an averaging effect. The bias distribution provides insight into the user X  X  selection process: if P ( V =0 | is high then the user prefers to answer questions with no value; if P ( V =1 | A =1) is high then the user prefers answer-ing questions with some existing value. Choosing a discrete distribution for bias rather than a continuous one has the advantage that the data required to learn a user X  X  bias is not very large.
We use biases to define feature vector for each user: Based on a user X  X  feature vector, that user is classified as ei-ther an expert or an ordinary user. We use Ridge regression and Logistic regression for the task of binary classification. We also use Generative Model based on Gaussian distribu-tion as described below.
This model assumes that users are Gaussian distributed in terms of their selection biases. Figure 1 shows that the scatter plot of first two features for the dataset and the con-tours of the Gaussian distribution fitted based on the MLE estimates. The smaller contours captures contains majority of the experts and some of the ordinary users as well. It also shows that biases of experts are concentrated in a small region and for other users it is more widespread. The model parameters of Gaussian distribution are  X  = {  X ,  X  } .
P ( x |  X  )= 1 The model parameters for the two classes are  X  E = {  X  E ,  X  and  X  O = {  X  O ,  X  O } . We assume that a user X  X  bias is i.i.d which simplifies Maximum Likelihood Estimation (MLE):  X 
MLE = argmax  X  { P ( D |  X  ) } = argmax  X  { Figure 1: Plot of first two features of biases and the contours of Gaussian distribution using MLE. For classification, Bayes rule is used to generate the posterior distribution of class conditioned on feature vector: where P ( E ) is prior probability of a user being an expert. Prior probability is the ratio of experts in the training data. We also compute posterior probability of user belonging to the other class, whichever class has a higher probability. We use 10-fold cross-validation to run the learning models. To compare model performance, we report on the precision, recall and F 1 score of models in prediction of experts .
Figure 2 compares the average biases of experts with those of other users. The biases of experts are significantly differ-ent for v =0 , 5 (t-test, p  X  0). Similarly, for v =1bias means are different (95%CI, p&lt; 0 . 04). It shows that ex-perts are more selective in picking questions with zero ex-isting value (no answers or bad answers on the question).
We consider a few other models that extract different features of users and compare their performance with our model. Following is a brief description of the other models:
Z : This model is based on Z -score [4] ( Z ( a, q )= a  X  q where a and q are the number of answers and questions given by a user). The feature vector for user is [ aqZ ( a, q )]
T : A model based on text analysis of answers. It includes features such as the use of positive/negative emotions, self-reference, big words, and words referring to categories such as religion, gender, etc. We also add other features such as the number of answers and the votes received.

We use the Gaussian model over features extracted from all the different models for classification. Table 2 shows the Table 2: Model performance. B is bias-based model. model performance in predicting experts. We observe that the recall of B is significantly higher than both T and Z , indicating that the experts are tightly clustered over their biases, and our formalism of bias is extremely effective in retrieving them. Z + B model performs better than most models, indicating that biases along with simple features can boost predictive performance significantly.

Low precision of B indicates that there are many other users in the community who show similar selection tenden-cies as experts. On a deeper analysis, we find that several of the TurboTax users have given 10-15 answers which could have led to an inaccurate estimation of biases for these users. If we restrict our focus to users with a higher number of an-swers (Section 5.3), we see that the model performance im-proves. Another perspective is that TurboTax has a manual expert identification process and they have not yet com-pletely labeled all the experts in the community, so several of the false positive can turn out to be experts in future.
In the previous experiment, we established that experts exhibit similar biases, and leveraged that fact to identify ex-perts. The precision of the bias-based model did not turn out to be significantly high. The main reason is that sev-estimation for such small number of answers might not be accurate. In this experiment, we run our model over users who have provided N answers or more. We call N the answer threshold. The answer threshold serves two purposes: First, as we increase N, the ratio of experts to ordinary users in the pool of selected users increases. This is because experts typically give many more answers. Second, inaccurate biases are eliminated from the data. Figure 3 shows that the model Figure 3: Performance of bias-based learning model. performance improves with increase in answer threshold. At high answer threshold, model precision improves by 250% (0.7 vs 0.28). The Gaussian model beats the two regres-sion models indicating that the users are indeed Gaussian distributed over their biases.

Additionally, it shows that by carefully choosing the an-swer threshold, we can strike a balance between model per-formance and amount of data required before predicting a user as an expert or not. Since recall doesn X  X  consider those experts who were discarded due to threshold, an effective tradeoff can be devised by maximize the product of F1 score and the number of experts amongst selected users. This leads us to the answer threshold of 30-40 answers as best.
Here we consider the answers given by users within the month of their first answers to compute their biases. Fig-ure 4 shows the performance of model over this data. It shows that the model performance has improved consider-ably over Figure 3. This presents evidence that biases can be effectively used to identify experts while their associa-tion with the community is in its early stages. This can be pretty useful in identifying potential experts by providing them retentive incentives to avoid them from churning. Figure 4: Performance of bias-based learning model using one-month data.
CQA systems are extremely dynamic in nature -several factors change over time, such as interfaces, functionality, users, user X  X  interests, and activity patterns. Under such dynamics, the selection preferences of a user can change. We study these preferences by dividing the data into 5 equal time-slots and computing biases per slot. We plot the bias mean and deviations for all users (Figure 5). The biases do not show any noticeable changes over time (same result over all other dimensions). This indicates that biases are not influenced by dynamics of CQA even over longer period of time. This provides some evidence to our claim that these biases emanate from the intrinsic characteristics of the user. Figure 5: Bias mean and standard deviation (first dimension only) of users across five time windows. We also performed experiments on dataset collected from Stackoverflow.com , which lead to same conclusion as we get above with the TurboTax dataset. Due to space constraints we do not present our results on these other datasets.
In this paper, we present question selection bias as a new measure to study the behavior of users in CQA. In our set-ting, this bias indicates the degree to which users prefers to answer questions in different stages of answer completeness. This captures the intuition that some users prefer to answer questions where they can create the largest value -the ques-tions with no good answers. On the other hand, some users prefer to answer questions with some existing value, per-haps because they do not have enough expertise to give a complete solution or because they do not want to put much effort into answering. Our results show that experts select with high probability questions with low existing value. We also show that these biases emanate from the intrinsic char-acteristics of user and do not get influenced by the dynamics of the underlying community.

This paper also establishes that a user X  X  selection bias can be effectively used to identify users who have the potential of expertise in their early stages of engagement with the community. Identifying potential users can be extremely useful, as these users can be more effectively motivated or mentored to reach the expertise level faster.

We also show that bias can be mixed with other simple measures to improve the predictive power of expert identifi-cation models ( Z + B model). Though the labeling of experts in the TurboTax dataset is partial, we get significant predic-tive performance. It not only justifies this first formal step towards capturing user biases, but motivates us to explore biases of several types and their effects on the community.
Last but not the least, we show that employing a minimum threshold of 30-40 answers is optimal for measuring biases. This makes the task of expert identification computationally feasible for large datasets.
 Selection biases present an interesting metric in understand-ing the psyche of a user in the community. Q&amp;A inter-faces can be personalized to show questions conforming to each user X  X  bias. Such steps can increase the participation of users. We would also like to remove several simplifying assumptions made in this paper, in order to model user X  X  behavior more accurately.
 We would like to thank F. Maxwell Harper for suggestions towards improving the paper. This work was supported by the National Science Founda tion, under gra nts IIS 08-12148 and IIS 08-08692. [1] M. Bouguessa, B. Dumoulin, and S. Wang. Identifying [2] J. Guo, S. Xu, S. Bao, and Y. Yu. Tapping on the [3] P. Jurczyk and E. Agichtein. Discovering authorities in [4] J. Zhang, M. S. Ackerman, and L. Adamic. Expertise
