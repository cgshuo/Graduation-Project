 kurihara@mi.cs.titech.ac.jp exception.
 be difficult (see Section 3).
 T (see Section 4).
 depth of the tree at which one operates (see Section 6).
 applications. mixture assumes: Given a dataset X = { x component label z  X  ( V ) , and then drawing x n from the corresponding observation model p x ( x n |  X  k ) . We will denote Z = { z interested in computing the posterior over data labels p ( z computed analytically. where q per i ), and q setting q with index higher than the truncation level T , i.e., q consists in estimating a set of T parameters {  X  v energy F (  X  ) = E support only for z only finite sums [7].
 q (see Fig. 4 in [7]). variational distributions for the components q q the limit F = lim free energy reads F = i ,  X  assign nonzero responsibility to components beyond level T , and therefore each q to
T (since for i &gt; T , q v because it allows for optimization with adaptive T starting from T = 1 (see Section 7).  X From the last term of (2) we directly see that the q where Minimization of F over  X  v choices of models for q Evaluation of F requires computing the infinite sum P  X  P j = T +1 E p v [log(1  X  v )] = ( i  X  1  X  T ) E p v [log(1  X  v )] be shown to be terior over data labels can be approximated by p ( z support, in practice it suffices to use the individual q cumulative q be computed analytically even though they involve infinite s ums. assume that p and q In this case, the probabilities q where  X ( ) is the digamma function. The optimal parameters  X  v ,  X   X  can be found to be The update equations are similar to those in [7] except that w e have used Beta (  X  Beta (1 ,  X  ) , and  X  v and (6). In [7] the corresponding sum is finite since q algorithm (a feature that was not present in [7]). given expansion of the kd-tree form a partition of the data se t. accelerated update equations we constrain all x q where S and h x i data in node A , the optimal parameters can be shown to be Finally, using q F = The infinite sums in (17) and (19) can be computed from (6) with S lower F ). Input is a dataset X = { x parameters {  X  v In step 2b we initialized the responsibilities by q figures show speedups and free energy ratios, respectively. partial updates in step 2c, we additionally set q of Finally, in step 2c we monitored convergence of the partial u pdates through F efficiently computed by adding/subtracting terms involvin g the new/old components. datasets. In all experiments we assumed a Gaussian observat ion model p inverse Wishart for p according to size (from top left to bottom right). Moreover, Fast-VDP and VDP were always better than BJ in term s of free energy. with similar plots in [8, 9].
 used the MNIST dataset ( http://yann.lecun.com/exdb/mnist/ ) which consists of 60 , 000 while Fast-VDP computed its results much faster than VDP.
 In a second real data experiment we clustered documents from citeseer ( http://citeseer.ist. y clustered. VDP found six clusters, A X  X . sentation (e.g., priors of the form p representations such as the Chinese restaurant process [3] . Acknowledgments based upon work supported by ONR under Grant No. N00014-06-1 -0734 and the National Science Foundation under Grant No. 0535278
