 TONG XIAO, JINGBO ZHU, and MUHUA ZHU, Northeastern University In recent years many statistical approaches have been developed in Chinese-English Machine Translation (MT), including phrase-based approaches [Koehn et al. 2003] hierarchical phrase-based approaches [Chiang 2005], and syntax-based approaches [Ding and Palmer 2005; Galley et al. 2004; Liu et al. 2006; Yamada and Knight 2001; Zhang et al. 2008]. The translation accuracy on Chinese-English translation has continuously improved in recent MT competitions such as the NIST/DARPA MT Evaluation 1 and the IWSLT Evaluation Campaign 2 .

Although Chinese-English translation quality has improved significantly, pervasive problems remain. One of them is that the output of most machine translation systems is very poor from the grammatical standpoint. To address this issue, it is natural to explore Syntax-based Language Modeling (SBLM) methods that make full use of the syntactic information on the target-language side to overcome the shortcomings of tra-ditional n -gram language modeling. However, several research groups have reported that directly using (Context-Free) Treebank Grammar-based parsers 3 as Language Models (LMs) is not very helpful in improving BLEU scores for Chinese-English MT [Charniak et al. 2001; Och et al. 2004; Post and Gildea 2008]. For example, Charniak et al. [2001] used a syntactic parser to rescore a translation forest generated by a syntax-based MT system [Yamada and Knight 2001]. However, they obtained a lower BLEU score in spite of more grammatical outputs produced by using their parser. Cherry and Quirk [2008] used a parser with a Markovized, parent-annotated Treebank Grammar to distinguish grammatical from ungrammatical text. They found that such a syntax-based language model did not even appear very useful in classifying good and bad sentences.
 While previous efforts on using parsers as language models did not show promising BLEU improvement, syntax-based language modeling is still one of the juiciest prob-lems in MT due to its undoubted advantages over n -gram language modeling, such as the good abilities to model head/modifier dependencies [Knight and Koehn 2009]. In this work we further discuss this issue in the context of syntax-based MT. As Syn-chronous Tree Substitution Grammars (STSG) have been successfully used in syntax-based MT models in recent years 4 , we focus on studying and experimenting with a state-of-the-art MT system based on the STSG model proposed in Galley et al. [2004] and Galley et al. [2006].

We argue that traditional syntax-based language models [Charniak et al. 2001; Och et al. 2004] do not work well in STSG-based MT due to two major reasons.  X  First, treebank grammar-based parsers are somewhat unsuitable for detecting the well-formedness of translations produced by STSG-based MT systems. In general, traditional syntax-based language model is a syntactic parser that is parameterized and trained on all standard height-one subtrees defined in Treebanks, while the output of STSG-based MT systems is in principle an English syntax tree generated by applying a series of TSG rules involving larger tree fragments (i.e., subtrees of height  X  1). Though TSGs are recognized to be weakly equivalent to Context-Free
Grammars (CFGs) which are classic Treebank Grammars, their formalisms are very different from each other. For example, TSGs are able to capture very complicated dependencies, while CFGs have limited capabilities in dealing with dependencies within a relatively large scope. Straightforwardly using the model score produced by CFG-based parsers, therefore, might be inappropriate for evaluating the well-formedness of target-syntax trees generated by STSG-based MT systems.  X  Second, the syntax-based language model is trained on the treebank data with cor-rect English and manual annotation (e.g. the Penn Treebank), but is tested on out-of-domain data with full of incorrect English generated by MT systems. As the underlying probability distribution on Treebank is quite different from that in the
MT environment, it is not expected that the language model could perform well when applied to non-treebank translation domain. A more severe problem is that a traditional syntax-based language model is essentially a syntactic parser whose design is not for the purpose of language modeling and grammaticality detection.
Since the output of MT system is generally far from perfect English, even is incor-rect English, straightforwardly using a general parser to re-score MT outputs might result in unexpected translation results in some cases [Koehn 2010].
 In this article we investigate methods to address the preceding issues.  X  We develop a language model, called TSG-based language model , for syntax-based
MT using tree substitution grammars and demonstrate that it is able to improve the BLEU score of a state-of-the-art Chinese-English MT system.  X  We present three methods to efficiently integrate the TSG-based language model into MT decoding.  X  We present a simple and effective method to adapt the TSG-based language model for syntax-based MT, aiming at guiding the language model training using the data that is relatively more  X  X referred X  by MT systems.

The rest of this article is structured as follows. In Section 2, we briefly describe the baseline MT system used in this study. We then present a TSG-based language model for syntax-based MT in Section 3. In Section 4, we describe three methods for effi-cient language model integration into decoding. Furthermore, in Section 5 we design a method to adapt syntax-based language models for MT. In Section 6, we present the experimental evaluation of the proposed methods. After reviewing the related work, we finally conclude this article with a summary and outlook of further work. In this work, the baseline system is a Chinese-English Statistical Machine Transla-tion (SMT) system based on the string-to-tree model proposed in Galley et al. [2004] and Galley et al. [2006]. This type of translation model can be regarded as a special instance of the framework of synchronous tree substitution grammars and tree trans-ducers [Knight and Graehl 2005], and provides a formalism that can synchronously generate a pair of source-language sentence and target-language syntax tree. Typi-cally, a translation rule used in this model maps a source-language CFG rule (repre-senting a string) into a target-language TSG rule (representing a tree structure). The following example is a sample string-to-tree rule which explains the mapping from the CFG rule  X  S S S  X  NP PP NP PP NP PP  X  X  X  (z) X   X  to the TSG as a  X  X implified X  TSG rule, the string-to-tree rule can also be viewed as a rule from TSG to TSG. In this work, we use terms STSG rule and STSG to name the string-to-tree rule and the grammar in the string-to-tree model, though they are a little different from the definitions in the traditional STSG formalism where tree structures are re-quired to be in both source-language and target-language sides [Chiang and Knight 2006].

According to Chiang and Knight X  X  [2006] definition, an STSG rule consists of three parts, a left-hand side and two right-hand sides (call them the source-side and the target-side ). For example, for the above STSG rule, the left-hand side S S S is the root for the synchronous production. The source-side  X  NP 1 NP 1 NP 1 PP 2 PP 2 PP 2  X  and the target-and the target tree fragment, respectively 5 . The subscript numbers indicate one-to-one alignments that link frontier non-terminals on the source-side to frontier non-and so on. The non-terminal-labeled frontier nodes are generally called substitution nodes that can be rewritten recursively during synchronous parsing. In the rewriting operation (or substitution operation), an aligned pair of substitution nodes is rewritten with an STSG rule, guaranteeing that the substitution nodes must match the root of the STSG rule with which they are rewritten.

In a sense, the target-side of an STSG rule is a subtree in the target-language, having words (terminals) and variables (non-terminals) at leaves. Thus we can draw a target-language TSG rule from the target-side of an STSG rule. For example, from the above STSG rule, we can induce a target-language TSG rule like this:
Obviously, it is a grammar rule of monolingual TSG, which implies that the syn-chronous generation can also explain the generation of the target-language syntax tree. In other words, the string-to-tree model generates the target-tree by recur-sively applying the target-language TSG rules, and meanwhile generates the source-language string correspondingly using the source-side of STSG rules. Typically, the string-to-tree model is trained on the word-aligned bilingual text whose target-side has been parsed using an English syntactic parser. To obtain basic STSG rules, the minimal GHKM extraction method proposed in Galley et al. [2004] is utilized. The basic idea of GHKM extraction is to compute the set of the minimally-sized rules that can explain the mappings between source-language string and target-language tree while respecting the alignment and reordering between the two languages. For example, from the string-tree pair shown at the top of Figure 1, we can extract minimal GHKM rules r 1  X  r 6 . In addition to the GHKM extraction, we com-pose two or more minimal rules having shared states to form larger rules [Galley et al. 2006; Marcu et al. 2006]. For example, rule r 7 in Figure 1 is generated by composing rule r 2 and rule r 4 . When we translate from a Chinese sentence f to an English sentence e using the extracted rules, the system builds a Chinese tree from f , and meanwhile generates an English tree for e . Our primary translation model is based on the joint probability P (  X  respectively. Tuple (  X  e , X  f ) is generally called synchronous tree, and can be explained by a derivation of STSG rules used in decoding. The translation task can be described as: given a Chinese sentence f , we find an optimal English translation e  X  by taking the yield of the highest probability tree  X  e . This process is formulized as the following equations. where yield(  X  e ) is a function that returns the yield of a given tree  X  e . To define P (  X  features. responding to the m -th feature. In our baseline system, the feature design is mainly based on a state-of-the-art system described in Marcu et al. [2006]. There are 14 fea-tures in total, including bidirectional lexical and phrase-based translation probabil-ities (four features), a syntactic feature (root normalized conditional probability), a 5-gram language model, target word penalty, several binary features (lexicalized rule; low-frequency rule; composed rule; name, number, and date translation rules), and a word-deletion feature. All feature weights are optimized using Minimal Error Rate Training [Och 2003].

It is worth noting that in our system the syntactic feature is designed to model the process of synchronous tree generation. Given a synchronous tree, it computes the probability of how likely this synchronous tree is generated. Similar to the tree proba-bility in Context-Free Grammar (CFG) parsing, the probability of a synchronous tree is the product of all synchronous grammar rule probabilities. In this work the rule prob-ability is defined as the root normalized conditional probability which is estimated on the extracted synchronous grammar. We choose this type of rule probability because it has been successfully used in several syntax-based systems [Galley et al. 2006; Marcu et al. 2006; Mi and Huang 2008].

We use a CKY-style decoder with cube pruning [Huang and Chiang 2007] and beam search to decode new Chinese sentences. For efficient integration of n -gram language model into decoding, rules containing more than two variables are binarized using the synchronous binarization method [Zhang et al. 2006].
 As described in Section 2.3, the objective of the baseline model is to find the best En-glish syntax tree for a given Chinese sentence. It is important, therefore, that the model can concentrate more on well-formed English trees, and thus generate more grammatical translation outputs.

An obvious solution to this issue is that we use syntactic parsers as language models X  X all them syntax-based language models X  X o assess how likely the most possi-ble English tree is well-formed. That is, for a tree-structured translation  X  e , a parser is used to calculate the tree probability P(  X  e ) (or model score given by the parser). P(  X  e )is then used to estimate the well-formedness of  X  e . In this article we call the calculation of P(  X  e ) target-tree scoring, or simply tree scoring for short. Applying syntax-based language models to the syntax-based system provides several advantages. First, syn-tactic parsing is the process of analyzing a text to determine its grammatical structure with respect to a given (more or less) formal grammar 6 . Comparing to n -gram lan-guage modeling, it is more suitable for judging the grammaticality in translations, such as detecting subject-verb relations. Moreover, as the design of the parser is gen-erally linguistically motivated, it is believed that parsers can offer reliable tree scoring for syntax-based MT. Second, parsing is a well-studied task in the Natural Language Processing (NLP) community [Jurafsky and Martin 2008]. Many statistical (or partly statistical) models are available to automatically learn tree-scoring functions from a corpus of data that has already been syntactically annotated (by hand or automati-cally). Third, the output of syntax-based MT is already in scorable tree form. It means that we do not need to re-parse the translation candidates or recover their underlying syntactic structures during decoding. Instead, each tree-structured translation candi-date (i.e.,  X  e ) is just rescored by the syntax-based language model.

In the rest parts of this section, we first describe the traditional method of syntax-based language modeling using context-free treebank grammars, and then present a new language modeling method based on tree substitution grammars. To compute P(  X  e ), a simple and straightforward method is to use the tree scoring func-tions of existing parsers that have been trained on treebanks. Typically, these parsers are based on the model derived from parse-annotated training corpora or treebanks, and are parameterized using standard height-one subtrees (or CFG rules) defined in the training data. In this article, we refer to this type of parser as treebank grammar-based parser, or simply treebank parser for short.

To date, treebank parsing models have been intensively investigated. A number of well-developed parsers are available for this study. Among them, we choose two state-of-the-art ones as a starting point to experiment with syntax-based language modeling.
 Probabilistic Context-Free Grammar (PCFG) is one of the simplest grammar for-malisms for statistical parsing. Under PCFG models, the probability of a syntax tree is simply the product of rule probabilities.
 where r is a PCFG rule (i.e., a subtree of height one) induced from Treebanks, and P cfg ( r ) is the rule probability of r . Typically, P cfg ( r ) is estimated on the training data using Maximum Likelihood Estimation (MLE). Because P cfg (  X  e ) has a bias towards trees with fewer rule applications, we use the normalized PCFG probability for syntax-based language modeling in this work.
 where N is the number of rules used in generating  X  e .
 In addition to (non-lexicalized) PCFG models, lexicalized models [Charniak 1997; Collins 1999] are also very good choices for syntax-based language modeling. In this type of parsing model, each rule is specialized for one or more lexical items, which of-fers some nice advantages over non-lexicalized models. For example, lexicalized mod-els are able to express syntactic preferences that are sensitive to lexical words, while non-lexicalized models cannot. Due to these advantages, parsers based on lexicalized models have achieved state-of-the-art parsing performance over the past few years. In this work we also employ a state-of-the-art lexicalized model  X  Collins Model 2  X  to experiment with syntax-based language modeling.

In the Collins parsing model 2, the generation of a syntax tree is decomposed into many sub-steps, using reasonable independence assumptions. More formally, for a lexicalized grammar rule, the generation process can be described as: (1) Choose a head H with probability P H ( H | P ,w h ), where H is the label of generated (2) Choose left and right subcategorization frames, sub cat l and sub cat r , with probabil-(3) Generate all left and right modifiers with probabilities P l ( Li , li | l ( i )) and
All parameters described above can be learned from treebanks. Given a trained model, the probability of a syntax tree  X  e is defined to be the product of probabilities of all these sub-steps.

Like PCFG-based language modeling, the tree probability is finally normalized with the number of grammar rules used in generating  X  e (i.e., N ): As described in Section 2, the string-to-tree decoder seeks the best derivation by pars-ing the input sentence with an STSG. This process is very similar to that used in Data Oriented Parsing (DOP) Models [Bonnema 2002], where a deviation is generated by applying a series of rules (or tree fragments) with height  X  1. It means that the target-tree generated by our MT system is actually produced using the target-side of STSG rules instead of CFG rules (or height-one TSG rules) defined in a Treebank. For example, in our baseline model, more than 81% of the rules are STSG rules whose target-side varies in height from 2 to 10, while only 19% of the rules are STSG rules with a height of one. In the set of rules used during decoding, 71% and 29% of the rules are of height  X  2 and height = 1, respectively. In the set of rules used in generating the final (1-best) translations, the two numbers are 67% and 33%. These observations indicate that the syntax-based MT system is  X  X ominated X  by STSG rules of height  X  2 rather than height-one STSG rules.

To illustrate this point more clearly, Figure 2 compares the same syntax tree gener-ated using different grammars (a CFG and a TSG). We can see that even to generate the same tree, the CFG and TSG rule applications are quite different. It is a risk that CFGs may be not as good as language models for syntax-based MT as often thought, because the grammar formalisms and parameterizations are quite different between within language modeling and MT. For example, the internal nodes of TSG subtrees contribute nothing to STSG-based MT, but affect CFG-based language modeling. A natural question that arises is whether the syntax-based language models can be im-proved using other alternatives such as TSGs rather than CFGs. 3.3.1 Basic Method. In this section, we explore solutions to this issue, using TSGs within language modeling. Instead of exploiting other sophisticated models such as DOP models, we still use the PCFG and lexicalized PCFG (Collins Model 2) models for language modeling. The basic idea is very simple. We train parsing models on the target-side of the bilingual data using TSGs, and then employ the resulting parsers as language models for syntax-based MT. This method provides two obvious advantages over the CFG-based language modeling. First, as target-language TSGs are involved, the language model is more suitable for scoring the translations produced by STSG-based MT systems. Second, the training of language models fits MT better, since lan-guage models are optimized on the same data as that used in MT, rather than on the limited out-of-domain Treebank data (compared to MT). To illustrate the difference between various language modeling methods, Figure 3 shows a comparison.

The only issue we need to address here is how to train these parsing models (in-cluding both the PCFG and the lexicalized PCFG models) on the target-side of bilin-gual training data using TSGs. To deal with it, we use a simple solution inspired by Post and Gildea [2009] which preprocesses the corpus in three ways before conducting the parser training. First, we compute frontier nodes in syntax trees (with respect to word alignment) and then identify all minimal TSG subtrees corresponding to minimal GHKM rules [Galley et al. 2004]. Second, we flatten these TSG subtrees to equivalent height-one CFG trees. As the internal nodes of these TSG subtrees have no use for MT, it will not affect the result under our TSG-based language models. Finally, we introduce dummy preterminals to the words whose tags are removed during subtree-flattening. Figure 4 illustrates this method with a tiny example.

After the three-step preprocessing, each English tree  X  in the bilingual training cor-pus is transformed into a simpler form (  X  ). Consequently, we can collect the statistics for both the PCFG and Collins parsing models from (  X  ) 7 . These models are then used as language models in our syntax-based system. Also, the STSG rules can be extracted using (  X  ) instead of  X  because they are equivalent for both frontier identification and TSG rule extraction. This procedure is illustrated in Figure 5(b), with a comparison with the traditional method (Figure 5(a)) described in Section 3.2. Finally, for each target-tree  X  e generated by the MT system, we score it using P( (  X  e )), where P(  X  ) is the syntax-based language model that is trained using TSGs 8 . The definition of P(  X  ) is the same as that described in Section 3.2. So we omit the detailed description about it in this section. 3.3.2 Beyond Minimal Rules. In general, there are many ways to partition a parse tree into TSG rule applications. As described above, in the training of TSG-based LM, we choose the simplest partition that is constrained by the minimal GHKM extraction. By using such a partition, each resulting TSG rule corresponds to the target-side of a min-imal GHKM rule, and is equivalent to a CFG rule in the transformed tree. Although this method can alleviate the data sparseness in transformation, it may limit the ca-pability of the proposed model to only capture the minimal GHKM rules. Obviously, our model can be enhanced by using more TSG partitions and estimating parameters from the TSG rules in the transformed tree instead of the CFG rules.
 Motivated by this idea, we extend the approach described above to handle more TSG rules beyond the minimal rules. However, as in DOP [Bonnema 2002], there is generally an exponentially large number of TSG partitions for a parse tree. This makes the training of our model very complex. To ease the problem, in this work we only consider some critical TSGs instead of all TSGs. Our method is inspired from the rule composing [Galley et al. 2006] and the SPMT rule extraction [Marcu et al. 2006] in MT, which are two standard methods to improve the limited scope of the GHKM extraction. The idea is very simple: our model uses all TSG rules in the transformed tree that are consistent with the rule composing and the SPMT extraction. That is, instead of training parameters from the CFG rules in the transformed tree structure, we also estimate the TSG rules that are obtained by composing a number of CFG rules (or minimal rules) or SPMT extraction. For example, in Step 3 of Figure 4, three CFG rules (or minimal rules) can be identified, as follows.
 These rules can be further composed into a larger rule which is then involved in the TSG-based LM.
 Note that the above method only considers a small sub-set of all TSG partitions X  TSG rules that are consistent with the translation rule extraction used in MT. Fortu-nately, the completeness of the resulting TSG rules can be guaranteed, because the target-side of any translation rule is estimated in our TSG-based LM. In other words, when an STSG rule is applied during MT decoding, we can always find its target-side (also a TSG rule) and the corresponding probability in the TSG-based LM. This property makes the TSG  X  X ompact X  enough to encode the probabilities of all potential target-trees generated by the MT decoder. In addition, this method has a practical advantage that the TSG rules can be obtained by simply reusing the rule extraction component of our MT system. It means that, in the training of the TSG-based LM, no additional computation is required to enumerate the large number of TSG rules, and the training is nearly as fast as the original version of our method (as described in Section 3.3.1). 3.3.3 Language Modeling Without Removing Internal Nodes. In our proposed method, we remove all internal nodes that are not identified as the frontier nodes in the original trees. This treatment has two advantages: first, it can alleviate the data sparseness problem caused by large-sized and low-frequency TSG rules; and second, removing in-ternal nodes can ease the modeling and training of the TSG-based language model, since they are not really used in our pipeline of string-to-tree SMT 9 . However, this method has a potential risk that TSGs are less discriminative due to the lack of struc-ture difference in TSG rules. For example, for the rule r 7 in Figure 1, the internal node PP PP PP is removed during the tree transformation as described in Section 3.3.1. As resulting rules cannot distinguish the syntactic structures covering IN NP IN NP IN NP and thus somewhat less discriminative for both STSG and LM. Although no studies have shown that the removal of internal nodes could result in decreased performance of MT, it is still worth a discussion on the issue. To this end, we enhance our method by recov-ering the internal structures which are ignored in the tree transformation (Step 2 in Figure 4) for language modeling. The improved method keeps all the information in the original parse trees. We give an empirical comparison of the method described in Section 3.3.1 and the improved method described herein, which will be shown in the experimental evaluation (Section 6). So far we have studied different methods for syntax-based language modeling. A nat-ural issue that arises is how to incorporate syntax-based language models into MT de-coding. So in this section we present three methods for syntax-based language model integration. As our MT decoder is basically a parser that works in a bottom-up fashion, the syntax-based language model can be formulized as a feature that is fit directly into the decod-ing and integrated into the translation model. Hence we can treat the syntax-based language model as a new feature in the (log-)linear model, and re-formulize Equa-tion (2) as: based LM feature. During decoding, the model score of translation hypothesis is cal-culated with the baseline model and the additional syntax-based LM together. Compared to treating syntax-based language model as an integrated feature, a sim-pler method is to re-rank the n -best output of the MT system using a rich feature set containing the syntax-based language model. This method can be regarded as an instance of multi-pass decoding [Jurafsky and Martin 2008]. In the first stage, the baseline system returns an n -best list of translation candidates for a given Chinese sentence. Then, in the second stage, the n -best translation candidates are re-sorted using a more sophisticated model. In this work, our re-ranking feature set consists of the baseline features and the syntax-based language model. The re-ranking system is based on the linear model described in Equation (7) and implemented by reusing the decoder and MERT components of our baseline system. The two methods described above are both rational solutions to the problem, but each of them has its own advantages and disadvantages. On one hand, the traditional re-ranking method is simple to implement and has been successfully utilized in the related studies [Charniak 2001; Koehn et al. 2003; Och et al. 2004]. However, tradi-tional re-ranking is very sensitive to the quality of n -best list (or translation forest). In most cases, the desired translation is not included in the n -best list due to the early stage pruning. Thus, it might be late to apply syntax-based language model in the post-processing stage. On the other hand, as the calculation of P(  X  e ) is very time-consuming, the integration of syntax-based language model in MT decoding is generally not very efficient.

In this section we instead propose a new method, local re-ranking (LR), to take the best use of both traditional re-ranking and feature integration methods. The basic idea is that we locally re-rank the n -best translation hypotheses that share the same properties during decoding. Take CKY-style decoding for instance. For each source span, we first generate the n -best translation hypotheses using the baseline model, and then re-rank them using a sophisticated model that contains the syntax-based language model.

Let f = f 1 ... f m be a Chinese sentence, and Q [ p , q ] be the set of translation hypothe-ses for the source word sequence spanning from position p to q . The below pseudocode formulizes the local re-ranking algorithm within the framework of CKY-style decoding.
The major part of this algorithm is the procedure of standard CKY-style decoding (lines 2 X 7). Line 6 indicates the generation of translation hypotheses from the con-secutive source spans [ p , k ]and[ k +1, q ], and line 7 indicates the update of the n -best list of the source span [ p , q ]. In line 9, the n -best hypotheses are re-ranked using the same model and features as those described in the traditional re-ranking system (Section 4.2). Then, in line 10, a fixed number of hypotheses are kept in the beam for the following stages of decoding. FEASIBLERERANKING() is a function that judges whether local re-ranking can be executed according to the following two constraints.  X  The first constraint (lines 13 X 15): the maximum SBLM score of n -best hypotheses should be above a threshold s min . This is because when the tree score is too low, it makes no sense to apply the syntax-based language model to re-rank the translation hypotheses.  X  The second constraint (line 16): the length of source span for local re-ranking should be greater than l min . It is motivated by the fact that the score of the subtree cov-ering too few words does not always provide very reliable tree scoring results for re-ranking.

Figure 6 illustrates the local re-ranking algorithm. The dashed rounded box at the bottom of Figure 6 shows the case where the local re-ranking is activated, while the other dashed rounded box shows the case where the constraints of local re-ranking are not satisfied. As l min = 3, the local re-ranking is not performed on the spans of length  X  3 (shaded chart cells in Figure 6). In many machine learning (ML) and NLP tasks, it is natural to assume that the training and test data both follow the same distribution. However, for syntax-based language modeling in MT, the situation is not so pleasant. Typically, syntax-based language models are trained on the data of correct English (e.g., the Penn Treebank or sentences translated by human), while is tested on the output of MT system which is far from perfect English, even is incorrect English. This implies very different un-derlying distributions between training data (good English in Treebanks or bilingual data) and test data (poor or even incorrect English in MT output). It is true even when their original texts are very close (e.g., in the same domain and writing-style). This problem probably results in unstable performance of syntax-based language models, especially when the MT output is far from correct English sentences.

The major reason behind this problem lies in that syntactic parsers are actually not designed for detecting the grammaticality in translations generated by MT systems [Koehn 2010]. To handle it, a straightforward solution is to manually annotate the MT outputs with syntactic structures and train syntax-based language models on the annotated MT translations. However, it is very expensive to create such a training data set. In most cases, it is even impractical to annotate enough data for train-ing the language model. In this work we instead use a simpler and more practical method. The basic idea is similar to self-training for parser adaptation [McClosky et al. 2006], where a parser iteratively bootstraps itself using self-generated data. Dif-fering from parser self-training, we bootstrap the syntax-based language model using the target-tree structures generated by MT. Figure 7 shows an intuitive illustration of our method, where initial data (or labeled data) refers to the standard training data used in syntax-language model training, such as Treebanks and the tree structures generated by the MT system or the syntactic parser, and new data (or unlabeled data) refers to the translations of a certain number of source sentences S .

In this method, we first train a base model using the initial data set L .Wethen use the syntax-based MT system to generate tree-formed translations {  X  e } for S . The newly-generated tree structures are treated as truth and combined with the initial data to train a new syntax-based language model. Though {  X  e } might not be well-formed English from the viewpoint of parsing, they are relatively more  X  X referred X  by MT and thus more suitable for training syntax-based language models for translation tasks. If L is initialized with the MT-system-generated tree structures (for the target-side of the bilingual corpus), this procedure can be regarded as training on ill-formed MT output alone; otherwise, the resulting TSG-based LM is affected by the original treebank transformed TSG.

Like self-training, this procedure can be iterated, where the new model is used to generate new data in the next iteration. The below pseudocode summarizes this method, where T is the number of iterations, function TRANSLATE() generates the target-tree structures for a given set of source sentences, and function MODELTRAIN-ING() returns a new syntax-based language model that is learned using the input training data. Note that, using this method, we could bootstrap any syntax-based language model no matter it is initially trained on the date generated using the MT system or on the treebank corpus.
 Previous studies have shown that big STSG rules are not very helpful for syntax-based machine translation [Cohn and Blunsom 2009; Liu and Gildea 2009]. In most cases, rare and big rules are considered to be noisy and generalize poorly on the unseen data (i.e., overfit the training data). This problem is even more severe when the (S)TSG is learned using the EM algorithm where the model is dominated by the biggest rules. Although our method differs sequentially from the EM training with TSG 10 ,itwould be very nice to have a mechanism that controls how likely the model overfits the train-ing data.

A straightforward solution to this issue is to guide the model towards using frequent and small rules. This matches our intuition that good rules are small, with few inter-nal nodes, frontier non-terminals and terminal strings [Cohn and Blunsom 2009]. To realize this idea, we use the variational Bayesian method [Beal 2003] as a replacement of the MLE in the training of TSG-based LM. Take PCFGs for instance. In MLE, the probability of a rule r is estimated according to the following equation: where root ( r ) is a function that returns the root label of r . The variational Bayesian method slightly modifies the above equation and performs an inexact normalization, where the resulting rule probability will add up less than one, by passing counts through function f ( x ) = exp(  X  ( x )).
 where  X  ( x ) is the digamma function [Johnson 2007]. It has the effect of subtracting 0.5 from its argument. The behavior of this estimation is determined by the choice of  X  .When  X  is set to a low value, it can be regarded as a way of  X  X nti-smoothing X . As about 0.5 is subtracted from the rule counts, small counts corresponding to rare rules (i.e., generally big rules) are penalized heavily, while large counts corresponding to fre-quent rules (i.e., generally small rules) are not be affected very much. As a result, low values of  X  make Equation (9) favor the rules which occur frequently and distrust the rules which occur rarely. In this way, the variational Bayesian method could control the overfitting caused by abusing rare and big rules. On the other hand,  X  can be set to a high value if we do not want to rule out any rules and smoothing is required. In this work,  X  is set to zero (the lowest value), since we wish to guide the model towards fa-voring frequent and small rules. In addition, after the normalization using variational Bayes, we perform an additional round of normalization without variational Bayes to add up rule probability to one. We carry out experiments on the NIST Chinese-English MT evaluation tasks. The sys-tem performance is evaluated in terms of the case-insensitive NIST version BLEU[%] (using the shortest reference length). Statistical significant test is conducted with the re-sampling method proposed by Koehn [2004]. The bilingual training corpus consists of about 138K bilingual sentences (3.2M words on the source-language side and 4.1M words on the target-language side) extracted from the FBIS corpus 11 . To filter out noisy data, the bilingual sentences are extracted according to two criteria: first, we only select the sentence-pairs with the length ratio (length of target sentence/length of source sentence) ranging in (1/5, 5/1); and second, we only select the sentence-pairs with reasonable lexicon mapping probabilities 12 to prevent the extraction of incomplete sentences.

To obtain word-aligned corpus, GIZA++ is employed to perform bi-directional word alignment on the bilingual sentences. The  X  X row-diag-final-and X  method [Koehn et al. 2003] is then used to generate the symmetric word alignments. The English side of the bilingual data is parsed using our re-implementation of Collins Model 2. The training data for our parser is Sections 02-21 of the Wall Street Journal (WSJ) Treebank. A 5-gram language model is trained on the English part of the LDC bilingual training data and the Xinhua portion of the Gigaword corpus. Our development set comes from the NIST MT 2003 evaluation corpus in which the sentences of more than 20 words are excluded in order to speed up the MERT. For test data, we choose the NIST MT evaluation corpora of 2004 and 2005. Table I shows the statistics of the data sets used in our experiments. In our syntax-based MT system, both minimal GHKM rules [Galley et al. 2004] and SPMT rules [Marcu et al. 2006] are extracted from the bilingual corpus. Composed rules are generated by composing two or three minimal GHKM and SPMT rules. As described in Section 2.1, we use a CKY-style decoder with cube pruning [Huang and Chiang 2007] and beam search to translate new Chinese sentences. By default, the beam size is set to 30. We use MERT to optimize the weights of the log-linear model. Our MERT uses two stopping criteria: first, the weights do not change by more than 0.001; and second, the BLEU score does not change by more than 0.03%. Also, we stop the MERT run when the iteration number is more than 30 since we observe that the BLEU score is not really improved after the 30 first iterations.

To achieve state-of-the-art performance, we further advance our system in two ways: 1) We use four specialized translation modules to translate date, time, name, and by-line respectively, and then insert their translations into the SMT system. 13 2) We allow the use of explicit deletion rule 14 in decoding. Also, we design a feature (i.e., word-deletion feature) that gives a penalty exp(-1) to this rule, which allows the sys-tem to learn a preference for using more or less word deletion operations.
For language model integration, the size of n -best list (for the entire span) in tra-ditional re-ranking is set to 1000. For local re-ranking, the size of n -best list (for all spans) is set to 30. Other parameters, such as s min and l min (Section 4.3), are optimized on the development set. The treebank grammar-based language models and the TSG-based language models are both based on our in-house re-implementations of the PCFG model [Jurafsky and Martin 2008] and the Collins Model 2 [Collins 1999]. The Treebank Grammar-based language models are trained on the WSJ Treebank Section 01-21, while the TSG-based language models are trained on the target-side parse trees of the bilingual data. As our proposed model has a nice property that all grammar rules used in decoding are observed in the training of the model 15 , it does not suffer from the  X  X nseen X  events and the bad (zero probability) estimates for the probability of grammar rules. Thus, for TSG-based language modeling, we train the PCFG model using MLE, and train the lexicalized model using the method described in Collins [1999] (with back-off estimates for only a few factors). By default, the syntax-based language model is incorporated as an integrated feature into MT decoding.

In addition, since our CKY-style MT decoder requires a binary synchronous gram-mar, a large number of virtual non-terminals are introduced by our rule binarizer. In some cases the decoder generates partial translations with incomplete subtrees root-ing at a virtual non-terminal produced during binarization. This makes it difficult to score the subtree using the syntax-based language model because the non-terminal is never seen in language model training. To solve this problem, we modify the tree scoring function so that it can return a score for such type of subtree while ignoring the unknown root label as well as the corresponding incomplete tree structure. First we evaluate the translations generated using various syntax-based language models, with a primary goal of studying the impact of syntax-based language model upon the translation task. Table II shows the results, where we also report the BLEU score of a state-of-the-art, open-source MT system Moses for comparison. As shown in Table II, stable BLEU improvements are achieved when TSG-based language mod-els are incorporated into the baseline system. By using the lexicalized parsing model, the system generally obtains an improvement of more than 0.4 BLEU points. In con-trast, the traditional Treebank grammar-based language models are not shown to be useful in improving BLEU scores, even leads to a -0.5 BLEU decline in some cases. These results confirm our motivation that the language modeling with TSGs is more appropriate for syntax-based MT than context-free Treebank grammars. We then investigate the impacts of different language model integration methods on BLEU score and translation speed. As the lexicalized parsing model is shown to be relatively more effective for language modeling than the PCFG model (Table II), we choose it for experimenting with language model integration in this set of experiments. Tables III and IV show the results. As expected, traditional re-ranking does not burden the system, while treating syntax-based LM as integrated feature degrades the trans-lation speed greatly (about two times slower than the baseline) 16 . Also, traditional re-ranking seems to be not helpful in improving BLEU scores due to the redundancies in n -best lists. By contrast, local re-ranking finds a good  X  X alance X  between BLEU and speed. As shown in the tables, it is just 1.15  X  1.20 times slower than the baseline. Comparing to employing SBLM as an integrated feature, it is 1.57  X  1.60 times faster. More interestingly, as a  X  X onus X  local re-ranking achieves comparable BLEU scores with its counterpart  X  X ntegrated feature X , even outperforms it in some cases. This re-sult indicates that locally re-ranking translation hypotheses is a promising method for syntax-based language model integration. Thus we choose it as the default method for language model integration for the following experiments. We also study the effect of our language model adaptation method on BLEU improve-ment. The additional (unlabeled) data used for language model adaptation is selected from a combined corpus of LDC2005T10, LDC2003E07 and LDC2005T06, which con-sists of approximately 220K sentence pairs from various news sources. To ease MT, some cleanups are performed. For example, we exclude the sentence pairs with length rate out of the range of (1/5, 5/1) and low lexicon mapping probabilities, as is described in Section 6.1. Also, we filter out hard samples on which our baseline system achieves a sentence-level BLEU score of lower than 35% 17 . We finally select 25K sentences from the source-side of the cleaned corpus. The corpus is then equally divided into five parts to perform five-iteration language model adaptation.

First, we investigate the effectiveness of the TSG-based language modeling on the ill-formed output. That is, we use the MT system to generate the tree structures for the target-side of the bilingual corpus, and train an initial TSG-based LM using these MT-system-generated tree structures. Then, the TSG-based LM is bootstrapped using the new data generated by the MT system, without combining the data with original Treebank transformed TSG grammar. Table V shows the difference in BLEU score when language model adaptation is performed for one iteration or not. We see a sta-ble BLEU improvement when the syntax-based language models are trained on the mixture of initial data and newly-generated data. In most cases, the language model adaptation achieves an improvement of about 0.2 BLEU points. These results indicate that the syntax-based language models are able to benefit from the use of additional MT-preferred data.

We also attempt to find the optimal number of iterations for language model adap-tation. Figure 8 shows that most improvement comes from the first two iterations. However, the improvement does not persist when more additional data are involved. As more iterations are performed, the improvement levels out quickly due to more bias and noises introduced. This observation is similar to that in [McClosky et al. 2006, 2008] where too much unlabeled data does not benefit parsing.

Then, we study the effectiveness of the TSG-based language modeling on the mix-ture of well-formed tree structures and ill-formed MT output. To this end, we initialize the training data for syntax-based LM using parse trees generated by the syntactic parser, as described in Section 3.3.1. We then bootstrap the TSG-based LM using the data combined with the new tree structures generated by the MT system. Table VI and Figure 9 show that language model adaptation is still useful in improving BLEU when mixed data is used. However, similar to the results shown in Figure 8, more iter-ations of bootstrapping does not help. Also as shown in Table VI, the best performance is achieved when the lexicalized TSG-based LM is used. Compared to the baseline, it obtains +0.6 BLEU improvements on all the evaluation sets, which are statistically significant at p &lt; 0 . 05.
 To examine the effect of controlling overfitting on language model adaptation, we apply the method described in Section 5.2 to the (un-lexicalized) TSG-based LM. Figures 10 and 11 show the results where the training set is initialized with the target-side tree structures generated by using the MT system (as in Figure 8) and the syntactic parser (as in Figure 9), respectively. As seen from the figures, the BLEU curves are not changed much when the variational Bayes is employed in the training of LM, which indicates that overfitting is not a very severe problem in our case. A possible reason for this phenomenon is that, in the method presented in Section 5.1, we use the MT system to generate the new tree structures for bootstrapping the TSG-based LM. Since the MT system has the capacity to eliminate the system bias towards derivations that use larger and fewer STSG rules, the training of TSG-based LM suffers little from the overfitting caused by abusing large TSG rules. In addition, as discussed in Section 5.2, another explanation might be that our TSG-based LM can generalize well on the un-seen data since all target-sides of the STSG rules are estimated during the training of the TSG-based LM.
 We also study the effect of using more TSGs within language modeling. As described in Section 3.3.2, we enhance our approach by introducing rules that are consistent with the rule composing and SPMT extraction. That is, instead of training parameters from the CFG rules in the transformed tree structure, we estimate the rule probabilities using the TSG rules. As the Collins model cannot be trivially extended to handle TSGs, we only experiment with the non-lexicalized (PCFG) model for TSG-based language modeling here. Table VII shows that our system can benefit from more TSG rules used in language modeling. By the measure of BLEU, it obtains +0.2 BLEU improvements on the development set and the test set of MT04, while just degrades slightly on the test set of MT05.
 As discussed in Section 3.3.3, recovering internal nodes can lead to a more discrim-inative TSG for language modeling. To study its effect on our string-to-tree system, we compare the BLEU scores of the two TSG-based LMs with and without recovered internal nodes. Table VIII shows that the removal of internal nodes does not affect the baseline MT system very much. However, it slightly degrades in BLEU score when the TSG-based LM (using non-lexicalized PCFG model) is integrated. This result indicates that recovering internal nodes does not really improve the TSG-based LM due to the data sparseness problem.
 To test the effect of our TSG-based LM on larger data sets, we conduct experiments on another data configuration. This configuration uses the NIST portion of the bilingual training data available for the NIST 2008 track translation task 18 . In addition to the FBIS data used in the previous experiments, approximately five million new sentence-pairs are used. As a result, the size of the training corpus is increased by a factor of more than 30. To process the bilingual sentences of this large training corpus, we use the same methods as those described in Section 6.1. We also scale the development set to the full set of MT03 to obtain a stable convergence for MERT. Table IX shows the statistics of the date sets used in this set of experiments. We use all the bilingual data to obtain the STSG for our MT system. For syntax-based language modeling, we use all the target-side parse trees to train the PCFG model and use 358K sentences selected from LDC2003E14, LDC2005T10, LDC2003E07 and LDC2005T06 to train the lexicalized model. For language model adaptation, we randomly select 100K sentences from the data sets provided within CWMT08 19 as the unlabeled data.

Table X shows that the TSG-based LM is still effective when we switch to the large data set. Using all the techniques proposed in this article, the TSG-based LM obtains an improvement of about 0.5 BLEU scores on the development set and the test set of MT04, which is statistically significant at p &lt; 0.05. Moreover, it is observed that the language model adaptation does not seem very helpful in improving the BLEU score. In most cases, its BLEU improvement is less than 0.2 points. The reason of this phenomenon might be due to the relatively small set of unlabeled data used here.
In addition, Table X shows the (normalized) weight of syntax-based language model (  X 
SB L M ) after MERT. For comparison, the (normalized) weight of n -gram language model (  X  NLM ) is also reported. Compared to the TG-based LM, the TSG-based LM gains much more importance in the syntax-based MT system, which further confirms the effectiveness of our approach. We also examine whether the improvements persist when the translation quality is judged by humans. To collect test data, we randomly select 50 sentences from the development set 20 . We develop a tiny program for interactive human evaluation. For each sentence, we first translate it using the baseline MT system and the improved system (TSG-based LM + language model adaptation), respectively. The translation results are then displayed as  X  X T output X  on the screen through the program. We also display one of the reference translations as the third MT output, for the purpose of estimating the upper-bound performance. The three outputs are randomly ordered and displayed to the judges. Finally the other three reference translations are presented as the truth for reference.

Three judges participate in our experiment. They are all qualified translators who are skilled in reading and writing English. In the evaluation, they are required to carefully read the three MT translations and three reference translations, and assign a score from 1 to 5 to each MT output according its translation quality. Furthermore, the assessment of translation quality is required to focus on both grammatical fluency and translation accuracy of the outputs.

Table XI shows the evaluation results. We see that the improvement in human evaluation is consistent with the BLEU improvement. On average score, the improved system outperforms the baseline system more than 0.1 points, which is a statistically significant improvement at p &lt; 0.01 ( t = 3.17, df = 149). More interestingly, it is observed that the reference translation used in this study (provided by LDC) is not perfect as expected. It only achieves a score of 4.70. In some translations, obvious disfluency problems and even a few segments with ungrammatical or incorrect English are found. This observation is similar to that mentioned in Marcu et al. [2006].
We then analyze the data to study what problems and benefits the improved system has. We observe, first of all, that the overall grammaticality is not greatly improved at a glance. Several major problems with the baseline system still exist. For example, for the baseline system, the three most frequent error types are: incorrect noun (phrase) translation (20.1%), incorrect morphological generation (12.0%), incorrect word dele-tion (9.3%) 21 . They are still the most severe problems with the improved system, with nearly unchanged percentage numbers of error rate. We find that our proposed lan-guage model improves the MT output in several major aspects. For example, using our language model, the number of errors in verb sub-categorization is reduced from 13 to 6. Sample 1 shows an example where the baseline system chooses the wrong verb-categorization [ NP VP ] for the verb require , while the improved system chooses the correct verb sub-categorization [ NP to VP ]. Also, the structure movement is handled better. The number of this type of errors is reduced from 7 to 2. For example, in Sam-ple 2, the improved system successfully handles the reordering pattern  X  NP 1 NNS 2  X  propose NNS 2 for NP 1  X . In addition, verb form (e.g., plural or singular verb) is more appropriately generated. Over the test set, 13 out of 31 cases are improved. For example, in Samples 3 and 4, the improved system correctly chooses the plural forms for the verbs have and be according to their subjects which are plural nouns. These results indicate that our language model is more suitable for dealing with some specified problems, rather than handling every grammatical error in MT output. In machine translation, researchers have been concerned for years about syntax-based language modeling. To our knowledge, the earliest attempt is Charniak et al. [2001] in which an English syntactic parser was employed to select the more grammatical translation output from a translation forest generated by a syntax-based MT system [Yamada and Knight 2001]. Charniak et al. X  X  [2001] method was basically a re-ranking method that re-sorted n -best list in terms of the model score produced by a parser. Though very simple and easy to implement, it suffers from the low quality of n -best list (or translation forest) due to the pruning at early stages of decoding. They showed that their method led to more grammatical outputs, but, unfortunately, degraded in BLEU score. As another representative work, Och et al. [2004] introduced the parse tree probability as a new feature into a (log-)linear-model-based re-ranking system for phrase-based MT. They reported that this feature is not helpful in improving BLEU scores of their phrase-based system. Cherry and Quirk [2008] investigated the ques-tion of whether a general syntactic parser is able to distinguish grammatical from ungrammatical texts. They trained a classifier using sentence length and model score produced by a parser with a Markovized, parent-annotated Context-Free Treebank Grammar, and then used the trained classifier to classify the Wall Street Journal sen-tences and self-made  X  X egative X  samples. Their results showed that such a syntax-based language model did not appear to be useful in classifying good and bad texts.
Although no promising BLEU improvements were achieved by using treebank grammar-based parsers as language models, there are studies that successfully integrated dependency parsing techniques into language modeling for machine trans-lation, namely dependency language modeling. For example, Shen et al. [2008] extended Chiang [2005] X  X  hierarchical phrase-based approach with a dependency lan-guage model, and demonstrated a significant BLEU improvement over their strong baseline system. Another example is Post and Gildea [2008]. They advanced a Brack-eting Transduction Grammar (BTG)-based system with the use of both the phrase structure parsing and dependency parsing techniques in language modeling. How-ever, these studies all focused on improving the MT systems based on linguisti-cally uninformed grammar 22 . In contrast, our work is on the basis of explicit syntax approaches in MT.

In the scenario of syntax-based MT, a number of studies have successfully exploited syntactic features for BLEU improvement. For example, several research groups have discussed the issue of synchronous tree scoring [Galley et al. 2006; Marcu et al. 2006; Mi and Huang 2008], where syntax-like-based features were introduced to model the generation of synchronous trees within their syntax-based systems. However, these features were trained using the synchronous grammar extracted from the bilingual data, and did not explicitly provide a mechanism to measure the degree of well-formedness of target-tree. More recently, Mi and Liu [2010] advanced their forest-based system using the dependency language model proposed in Shen et al. [2008], and demonstrated a promising BLEU improvement. On the other hand, Liu and Liu [2010] incorporated both the PCFG and lexicalized PCFG-based features into a tree-to-string system. They reported that the MT system could benefit from the joint learning of parsing and MT models. It is worth noting that, in a sense, Liu and Liu [2010] used a Treebank grammar-based parser to detect the well-formedness of the source-language parse trees yielding the input string. Differing from the results in previous work [Charniak et al. 2001; Och et al. 2004], they showed a great BLEU improvement over their baseline. This is because in Liu and Liu X  X  work the parser X  X  model score was used to select better source parse trees for rule matching rather than directly search-ing for grammatically correct translations. While all these studies present promising results in introducing syntactic features into MT, it is still rare to see work that suc-cessfully advanced syntax-based MT systems using syntax-based language modeling beyond Treebank Grammar-based models.

Our work differs from previous work mainly in that we are concerned more with de-signing an effective language modeling method using tree substitution grammars for the improvement of syntax-based MT systems, and investigating methods of language model integration and adaptation for MT. Perhaps the most related work is Post and Gildea [2009]. They showed that language modeling with tree substitution grammars could achieve lower perplexities on the Penn Treebank than standard Context-Free grammars. Based on their experimental results, they further pointed out that this property was very feasible for applications like machine translation. However, Post and Gildea did not explicitly discuss the issue in the context of syntax-based MT. In addition to Post and Gildea X  X  [2009] work, another line of research tries to improve MT with domain adaptation of n -gram language models [Bacchiani et al. 2004; Foster and Kuhn 2007; Koehn and Schroeder 2007; Snover et al. 2008; Zhao et al. 2004]. These studies showed that language model adaptation could significantly improve BLEU scores when MT systems performed on the out-of-domain data. As a matter of fact, like these works, we adapt the syntax-based language model from the Treebank data to the data used by MT. Beyond this, we bootstrap the syntax-based language model using the additional data generated by a syntax-based MT system. To our knowledge, the only previous work addressing this issue is Nakajima et al. [2002]. They adapted an n -gram language model with the data generated by a word-based MT system. By contrast, we focus on studying the issue in the context of syntax-based language mod-eling and experimenting with an MT system based on a state-of-the-art, string-to-tree model [Galley et al. 2004]. In this work we investigate syntax-based language modeling approaches for Chinese-English machine translation. We have presented a Tree Substitution Grammar-based language model to improve a state-of-the-art Chinese-English syntax-based MT sys-tem. By learning TSGs from the target-side of bilingual data, our model could bet-ter model the well-formedness of MT output than traditional Context-Free Treebank Grammar-based language models that are trained on the limited treebank data. On the NIST Chinese-English evaluation corpora, it achieves promising BLEU improve-ments over the baseline system. Moreover, we have presented three methods for ef-ficient language model integration, as well as a simple and effective method for lan-guage model adaptation. Our experimental results show that these methods are very beneficial to the proposed language model, and consequently further speed-up the sys-tem and improve the translation accuracy. We expect that our promising results could encourage more studies on this interesting topic, such as exploring Tree Adjoining Grammars or other alternatives within language modeling.

